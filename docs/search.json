[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Jung Yeon Lee",
    "section": "",
    "text": "My name is Jung Yeon Lee, and I am deeply passionate about robotics and reinforcement learning. I enjoy exploring innovative learning methods, solving complex problems, and contributing to the advancement of technology that fosters closer collaboration between humans and robots.\nMy guiding principle is Stop Wishing, Start Doing, and I am committed to working toward a future where humans and robots coexist harmoniously and dynamically.\n     \n\nMy Interest Fields are\n\nMachine learning | Deep learning | Reinforcement learning\nBioinspired-Robots | Simulations for Robotics\nOn-device AI | Quantum computing\n\n\nSkills and Tools:\n\n                                                     \n\n\n\n\nEducation\n\nM.Eng. 2022.03~2024.02\n\nM.S. in MECHANICAL ENGINEERING, SungKyunKwan University(SKKU)\nResearcher at Robotics Innovatory Lab. Quadrupedal Walking Robot Team\n\n\n\n2021.01.11~01.29\n\nTU Berlin Winter University Online : Machine learning using Pyhon - Theory and Application\n\nGrade : 1.0(the best score)\n5 credit points according to the ECTS(European Credit Transfer System)\n\n\n\n\nB.Eng. 2018.03~2022.02\n\nB.S. in ENGINEERING MECHANICAL & SYSTEM DESIGN ENGINEERING, HongIk University\nIntegrated Major in Design Engineering\nCompleted Accereditation Board for Engineering Education of Korea(ABEEK)\nUndergraduate research student at Autonomous Navigation Lab"
  },
  {
    "objectID": "posts/paper/2025-06-09-geort.html",
    "href": "posts/paper/2025-06-09-geort.html",
    "title": "📃GeoRT 리뷰",
    "section": "",
    "text": "Project Homepage\nPaper"
  },
  {
    "objectID": "posts/paper/2025-06-09-geort.html#소개-인간-손-동작을-로봇-손으로-직관적으로-전달하려는-노력",
    "href": "posts/paper/2025-06-09-geort.html#소개-인간-손-동작을-로봇-손으로-직관적으로-전달하려는-노력",
    "title": "📃GeoRT 리뷰",
    "section": "2.1 소개: 인간 손 동작을 로봇 손으로 직관적으로 전달하려는 노력",
    "text": "2.1 소개: 인간 손 동작을 로봇 손으로 직관적으로 전달하려는 노력\n로봇 원격 조작(teleoperation) 기술은 사람의 섬세한 손동작을 로봇 손으로 전달함으로써, 복잡한 조작 데이터를 수집하거나 위험한 작업을 대행하는 데 필수적인 요소입니다. 특히 손 기구학 리타게팅(kinematic retargeting)은 사람의 손 제스처를 로봇 손의 자세로 변환하는 핵심 과정으로, 사용자가 로봇을 자연스럽게 제어하도록 해줍니다. 그러나 사람 손과 로봇 손의 형태 및 관절 구조 차이로 인해 효과적인 리타게팅 함수를 정의하기가 매우 어렵습니다. 손가락 길이, 관절 가동범위 등이 다르기 때문에, 어떤 기준으로 사람 손동작을 로봇 손동작에 대응시킬지 명확한 해답이 없습니다. 실제로 수많은 매핑 방법이 가능하지만 그중 어느 것이 인간의 의도를 가장 잘 반영하면서도 로봇의 자연스러운 움직임을 유지하는지 합의된 해법은 없는 상태입니다.\n기존 접근법들은 주로 휴리스틱한 과제 벡터(task vector) 설정에 의존해 왔습니다. 예컨대 사람 손의 특정 키포인트(keypoint)를 로봇 손의 특정 지점에 1대1로 맞추고, 각 축마다 스케일이나 오프셋을 조정하는 선형 매핑 공식을 사용하는 식입니다. 하지만 이런 방식은 조정해야 할 하이퍼파라미터가 매우 많고, 개인별로 값이 달라 일일이 보정(calibration)해야 하는 번거로움이 있습니다. 또한 단순 선형 매핑으로는 사람 손과 로봇 손 공간의 비선형적 차이를 포착하기 어렵습니다. 실제 논문에서도 인간 손가락 끝의 동작 범위와 로봇 손가락 끝의 동작 범위를 비교해보면, 인간 손의 구성 공간이 곡률을 띠며 좁은 반면 로봇 손은 보다 넓고 규칙적인 형태를 보여 선형 관계로 겹치지 않는다고 지적합니다. 이러한 차이 때문에 기존 선형 매핑은 대응 관계를 정확히 재현하지 못하고, 결과적으로 로봇 손의 일부 동작 공간만 제한적으로 활용되는 문제가 있습니다. 요약하면, 사람의 의도를 잃지 않으면서 로봇의 가용 범위를 최대화할 수 있는 보다 원리적인(retargeting) 기준과 기법이 요구되어 왔습니다.\n이번에 소개하는 “Geometric Retargeting” (GeoRT) 알고리즘은 이러한 배경에서 제안된 최신 연구로, 초당 1000Hz 수준의 초고속 동작 변환과 원리에 기반한 명확한 목표 설정을 통해 이 문제를 해결하고자 합니다. 이 리뷰에서는 해당 논문의 주요 기여와 내용을 기술적으로 분석합니다. 먼저 논문의 핵심 기여를 정리한 뒤, GeoRT 알고리즘의 구조와 수학적 원리를 상세히 설명하고, 기존 작업들과의 비교를 통해 혁신성을 평가하겠습니다. 또한 실험 결과를 살펴보고 이 연구의 의의, 한계 및 향후 연구 방향에 대해 논의합니다."
  },
  {
    "objectID": "posts/paper/2025-06-09-geort.html#주요-기여",
    "href": "posts/paper/2025-06-09-geort.html#주요-기여",
    "title": "📃GeoRT 리뷰",
    "section": "2.2 주요 기여",
    "text": "2.2 주요 기여\n논문에서 저자들은 GeoRT를 통해 다음과 같은 두 가지 핵심 기여를 이루었다고 요약합니다:\n\n원칙적인 손 리타게팅 목표 함수 제시: 사람 손동작을 로봇 손으로 변환하는 데 필요한 근본 기준(criteria)들을 기하학적으로 정의하여, 이를 학습형 모델의 손실 함수로 활용할 수 있게 하였습니다. 이로써 인간-로봇 손동작 사이의 대응을 수치적으로 명확히 규정하고, 기존의 복잡한 휴리스틱 대신 체계적인 목표 하에 모델을 학습시킬 수 있습니다.\n초고속 신경망 리타게팅 시스템 구현: 상기한 기하학적 목표들을 기반으로 경량의 신경망 모델을 설계 및 훈련하여, 1kHz(초당 1000회) 수준의 실시간 성능과 최첨단 수준의 정확도를 달성했습니다. 제안된 시스템은 필요한 하이퍼파라미터 수를 크게 줄였으며, 실제 원격 조작 실험에서 기존 방식 대비 향상된 작업 성공률과 속도를 보여주었습니다. 또한 이 접근법은 테스트 시 별도의 복잡한 최적화 절차가 필요 없으므로, 확장성과 실시간 운용성 면에서 뛰어납니다."
  },
  {
    "objectID": "posts/paper/2025-06-09-geort.html#geometric-retargeting-알고리즘-기하학적-목표와-학습-구조",
    "href": "posts/paper/2025-06-09-geort.html#geometric-retargeting-알고리즘-기하학적-목표와-학습-구조",
    "title": "📃GeoRT 리뷰",
    "section": "2.3 Geometric Retargeting 알고리즘: 기하학적 목표와 학습 구조",
    "text": "2.3 Geometric Retargeting 알고리즘: 기하학적 목표와 학습 구조\n\n2.3.1 리타게팅을 위한 기하학적 설계 원칙\nGeoRT 알고리즘의 핵심은 “기하학적 목표 함수”들을 정의하여 사람이 직관적으로 기대하는 동작 대응 특성을 수식으로 표현하고, 이를 신경망 학습의 지도신호로 삼는 것입니다. 저자들은 이상적인 손동작 매핑이 갖추어야 할 다섯 가지 기준을 제시하는데, 이를 각 손가락별로 적용되는 다섯 가지 손실 함수로 구현했습니다. 다섯 가지 기준(criteria)과 그 직관적인 의미는 다음과 같습니다:\n\n운동 보존 (Motion Preservation): 사람 손가락 끝이 어떤 방향으로 움직일 때, 로봇 손가락 끝도 동일한 방향으로 움직여야 한다는 원칙입니다. 사용자가 손가락을 어느 방향으로 움직이면 로봇 손가락도 그 움직임 방향을 따라갈 것이라고 기대하기 때문에, 매핑 함수는 국소적인 운동 방향을 보존해야 합니다. 이를 위해 사람 손가락의 현재 자세에서 작은 변화 \\delta를 주었을 때 로봇 손 끝의 변화 방향이 \\delta와 평행하도록 유도하는 손실 함수를 정의합니다. 이 기준을 통해 미세 조작 시의 직관성을 보장합니다.\n구성 공간 커버리지 (C-space Coverage): 사람 손가락을 최소 위치부터 최대 가동 범위까지 움직였을 때, 로봇 손가락도 전 범위에 걸쳐 대응하도록 한다는 원칙입니다. 즉 사람의 입력 동작 범위 전체가 로봇 출력 공간의 모든 유효 범위에 매핑되어, 로봇 손의 가용 움직임 능력을 남김없이 활용하도록 합니다. 이상적으로 매핑 함수 f가 인간 손 공간 S^h에서 로봇 손 공간 S^r로의 전단사 함수(특히 전사, surjection)가 되길 요구하지만, 이를 직접 달성하기 어렵기 때문에 저자들은 챔퍼(Chamfer) 손실을 사용한 근사 방법을 제안했습니다. 즉, 매 미니배치마다 인간 손 공간에서 샘플링한 점들과 로봇 손 공간에서 샘플링한 점들을 비교하여 양 집합 간 거리를 최소화하는 챔퍼 손실 L_{\\text{cov}}를 계산함으로써, 로봇 공간에 인간 동작의 사상이 고르게 퍼지도록 유도합니다. 이로써 로봇 손가락 끝 구성 공간의 미커버 영역(uncovered space)을 줄이게 되며, 사람 동작이 로봇 손의 전체 범위를 빠짐없이 활용하도록 합니다.\n높은 평탄성 (High Flatness): 매핑 함수의 민감도가 입력 전역에서 일정해야 한다는 원칙입니다. 사람 손의 동일한 움직임 변화가 어느 범위에서든 로봇 손의 유사한 크기의 변화로 이어지도록, 균일한 응답 특성을 추구합니다. 예를 들어 어떤 구간에서는 입력을 조금만 바꿔도 로봇 손이 크게 움직이고, 다른 구간에서는 같은 입력 변화에 로봇이 미세하게 반응한다면 사용자는 어느 구간에서는 로봇이 둔감하고, 다른 구간에서는 과민하다고 느끼게 될 것입니다. 이를 방지하기 위해 GeoRT는 매핑 함수의 곡률(curvature)을 낮추는, 쉽게 말해 2차 미분이 0에 가깝도록 만드는 손실 함수를 도입했습니다. 구현상으로는 각 손가락 자세를 약간씩 변화시킨 두 가지 입력에 대해 유한 차분으로 로봇 출력 변화를 비교하고, 출력의 이차 변위가 0에 수렴하도록 하는 방식으로 평탄성 손실 L_{\\text{flat}}를 계산합니다. 이 지역적 선형성 제약을 통해 매핑이 전 구간에서 예측 가능하고 균일한 비율로 작동하게 됩니다.\n핀치 대응 (Pinch Correspondence): 사람 손가락들 사이에 집는 동작(pinch grasp)이 발생할 때 로봇 손에서도 동일한 핀치 동작이 일어나야 한다는 기준입니다. 예를 들어 사람의 엄지-검지가 집게처럼 모여 물체를 집는다면, 로봇 손도 같은 손가락 쌍으로 집게 동작을 취해야 합니다. 이는 사용자가 로봇을 자기 손처럼 느끼게 하는 에이전시(agency) 감각에 매우 중요하지만, 앞선 기준들만으로는 엄밀히 보장되지 않을 수 있습니다. 따라서 GeoRT는 엄지와 다른 손가락 간 거리를 모니터링하여, 사람이 일정 임계값 이하로 손가락을 모으면(예: 1cm 이하) 로봇에서도 해당 손가락 간 거리가 가까워지도록 강제하는 핀치 손실 L_{\\text{pinch}}를 추가했습니다. 이 제약으로 사람-로봇 손 간 집기 동작의 일치도를 높일 수 있습니다. 핀치 사례의 식별을 위해 사람에게 몇 가지 집기 동작을 미리 해보도록 하여 데이터를 모았으며, 약 5분 이내의 짧은 움직임 기록만으로도 충분했다고 합니다.\n비충돌성 (Collision-Free Retargeting): 사람이 손을 움직이는 동안 손가락들끼리 부딪치지 않는다면, 로봇 손 역시 자체 충돌(self-collision)이 없어야 한다는 기준입니다. 로봇 손가락끼리 엉키거나 충돌하면 작업에 지장을 줄 뿐 아니라 손상 위험도 있으므로, GeoRT는 최종적으로 충돌 억제 손실 L_{\\text{col}}을 포함시켰습니다. 구현상 물리 시뮬레이션을 통해 다양한 로봇 손 관절 구성과 그 충돌 여부를 미리 데이터로 모은 뒤, 신경망 충돌 판별기를 훈련하여 어떤 관절 상태가 충돌을 일으킬 확률인 C(q)를 예측하게 합니다. 학습 중에는 이 사전학습된 충돌 판별기를 통해 현재 로봇 자세 q의 충돌 확률에 비례하는 손실을 추가로 부여하여, 모델이 충돌 위험이 높은 출력을 피하도록 유도합니다. 다만 흥미롭게도, 저자들은 다른 손실들만으로도 어떤 로봇 손(Allegro 등)에서는 자체 충돌이 거의 발생하지 않는 결과가 나오기도 했다고 언급합니다. 그럼에도 완전성을 위해 충돌 회피 항목을 최종 포함했다고 합니다.\n\n以上의 다섯 가지 목표는 서로 독립적이며 리타게팅 문제를 정의하는 최소한의 제약이라고 저자들은 강조합니다. 실제로 일부 기준(I, II, III)을 만족한다고 해서 다른 기준(예: 운동 보존)이 자동 충족되지는 않으므로 각각의 항목이 필요합니다. 이처럼 간단하지만 원리에 충실한 다섯 가지 목표를 세움으로써, 더 이상 사람이 임의로 정한 복잡한 규칙 없이도 손 리타게팅의 품질을 수치적으로 명세화할 수 있게 되었습니다.\n\n\n2.3.2 신경망 구조와 학습 방법\n위 기준들을 실제 모델에 구현하기 위해, GeoRT는 입력으로 사람 손가락들의 위치(keypoint 좌표)를 받아 출력으로 로봇 손의 관절 각도를 내놓는 신경망 함수를 학습합니다. 보다 구체적으로, 각 손가락마다 개별적인 소형 신경망 f_i를 두어 손가락별 매핑을 수행하는 구조를 채택했습니다. 예를 들어 Allegro 로봇 손은 엄지 포함 4손가락으로 이루어져 있는데, 각 손가락마다 독립적인 다층 퍼셉트론(MLP) 네트워크를 할당하여 사람 손가락 끝 좌표를 해당 로봇 손가락의 관절 구동 위치로 변환하도록 합니다. 각 f_i의 출력층에는 Tanh 활성화 함수를 사용하고, 로봇 관절 범위에 맞춰 출력값을 -1~1로 정규화하여 표현합니다. 손가락별로 네트워크를 분리함으로써 학습이 단순해지고 병렬 처리가 가능해 속도 면에서 유리하며, 핀치 동작이나 충돌과 같은 상호작용은 앞서 정의한 제약 손실을 통해 조정합니다 (예: 핀치 손실은 엄지와 검지 네트워크 출력 간 거리를 연결). 이렇게 하면 모델 구조가 가벼워져, 추론시 연산량이 매우 적으므로 결과적으로 초당 1000회 이상의 갱신 주기를 쉽게 달성할 수 있었습니다.\n모델 학습은 완전 비지도학습(unsupervised)으로 이루어집니다. 즉, 사람이 직접 짝지은 입력-출력 데이터셋 없이, 앞서 정의한 다섯 가지 기하학적 손실 항목들의 합만을 최적화 기준으로 삼아 신경망의 가중치를 학습합니다. GeoRT의 전체 손실 함수 L_{\\text{total}}은 아래와 같은 형태로 구성됩니다:\n\nL_{\\text{total}} = \\lambda_1 L_{\\text{motion}} + \\lambda_2 L_{\\text{cov}} + \\lambda_3 L_{\\text{flat}} + \\lambda_4 L_{\\text{pinch}} + \\lambda_5 L_{\\text{col}}\n\n여기서 \\lambda_1 \\sim \\lambda_5는 각 손실의 가중치로, 논문에서는 경험적으로 4개의 하이퍼파라미터만 조정하면 충분하다고 설명합니다 (5개 중 일부는 단위 스케일에 따라 고정). 이는 이전 방식들이 사람 손가락마다 일일이 설정해야 했던 수많은 스케일, 오프셋 등의 조율 변수에 비하면 현저히 단순한 설정입니다. 저자들이 권장한 가중치 조합은 적당한 범위 내에서 결과에 큰 영향 없이 안정적으로 동작하였고, 이는 본 접근법의 매우 높은 실용성을 보여줍니다.\n학습 데이터 준비도 비교적 간단합니다. 로봇 손 공간 쪽은 시뮬레이터에서 로봇 손의 관절들을 무작위로 움직이며 얻은 손가락 끝 위치들의 포인트 클라우드로 샘플링하고, 인간 손 공간 쪽은 사용자에게 자유롭게 손가락을 움직이도록 (쭉 펴고 구부리기, 다양한 집기 동작 등) 5분간 요청하여 모션 캡처로 얻은 손가락 위치 데이터들을 사용합니다. 즉 수 분간 손을 이리저리 놀리며 손가락들의 전체 가용 범위를 탐색한 움직임 기록이 곧 학습에 필요한 인간 손 포인트 클라우드가 됩니다. 이렇게 수집된 인간/로봇 손 공간 표본들을 이용해 앞서 설명한 챔퍼 손실 등을 계산하고, 작은 무작위 자세 변화로 운동 보존 및 평탄성 손실을 계산하며, 일부 핀치 예로 핀치 손실을 적용하는 식으로 각 미니배치마다 손실을 산출합니다. 이때 로봇 손가락 끝 좌표를 계산하려면 출력 관절값에 대해 순방향 기구학(forward kinematics)을 수행해야 하는데, 이를 위해 로봇 손의 해석적 모델을 사용하거나 미리 학습된 미분가능한 신경망 forward 모델을 활용하였습니다. 또한 충돌 여부 판별을 위해 앞서 훈련된 충돌 판별기를 사용하지요. 이러한 부가 모델들(순방향 모델, 충돌 판별기 등)은 오직 학습 단계에서만 사용되고 추론시에는 필요 없으며, 손실의 그래디언트는 이들을 거쳐 신경망 f_i들까지 역전파됩니다. 최종적으로 경사하강법으로 신경망 파라미터를 갱신하면, 각 손실 항목들을 균형 있게 최소화하는 매핑 함수로 수렴하게 됩니다.\n흥미로운 점은, 이러한 학습이 아주 빠르게 완료된다는 것입니다. 저자는 지포스 RTX 3060 단일 GPU에서 3~5분 이내로 최적 학습이 끝났다고 보고합니다. 이는 비교적 간단한 MLP 구조와 소량의 데이터(수분간의 손동작)로 충분히 모델이 학습됨을 보여주며, 상황에 따라 새 사용자나 새 로봇 손에 대해 신속히 재학습하여 적용할 수 있음을 시사합니다. 결과적으로 학습이 완료된 GeoRT 모델은 사람 손의 키포인트 입력을 받아 즉각적으로 로봇 손의 목표 관절각을 출력하며, 별도의 복잡한 계산이나 최적화를 실시간 단계에서 수행하지 않으므로 지연 없이 초고속 응답이 가능합니다."
  },
  {
    "objectID": "posts/paper/2025-06-09-geort.html#기존-연구와의-비교-휴리스틱-매핑-vs.-원리-기반-학습",
    "href": "posts/paper/2025-06-09-geort.html#기존-연구와의-비교-휴리스틱-매핑-vs.-원리-기반-학습",
    "title": "📃GeoRT 리뷰",
    "section": "2.4 기존 연구와의 비교: 휴리스틱 매핑 vs. 원리 기반 학습",
    "text": "2.4 기존 연구와의 비교: 휴리스틱 매핑 vs. 원리 기반 학습\n사람 손에서 로봇 손으로의 동작 매핑은 오랫동안 다양한 방식으로 연구되어 왔습니다. 전통적인 접근법 중 하나는 조인트 공간 매핑(joint-space mapping)으로, 사람 손가락 관절 각도를 센서 장갑 등으로 읽어와 미리 정해둔 대응 관계에 따라 로봇 손 관절 각도로 직접 매핑하는 것입니다. 이러한 방법은 특정 경우(예: 로봇 손 구조가 인간 손과 거의 유사한 경우) 직관적이지만, 일반적으로는 사람 vs 로봇의 기구학 구조 차이 때문에 정밀한 제어가 어렵습니다. 관절 축 개수나 배치가 다르면 1:1 対응이 성립하지 않는 부분이 생겨 오차와 불안정성이 커지기 때문입니다.\n좀 더 보편적으로 쓰이는 방법은 직교 좌표계 매핑(cartesian mapping)으로, 사람 손의 손가락 끝 위치 같은 작업공간 좌표(task vector)를 로봇 손가락 끝이 따라가도록 하는 방식입니다. 예를 들어 사람 검지 끝 좌표를 로봇 검지 끝 좌표에 맞추고, 이 목표 위치에 해당하는 로봇 관절각은 역기구학(IK)으로 풀이하는 식입니다. 최근 많은 원격 조작 연구들이 이 키포인트 기반 매핑을 활용하여 어느 정도 성과를 내왔습니다. 그러나 이런 접근 역시 어떤 키포인트를 어떻게 매칭할지 정하는 일이 까다롭습니다. 단순히 선형 비례식 (Equation 1)으로 각 좌표축을 맞추는 방법이 흔하지만, 앞서 논의했듯이 이 경우 개별 축마다 원점 오프셋과 스케일 계수 등 수많은 파라미터를 보정해야 하고, 그럼에도 사람-로봇 손 공간의 비선형 차이를 커버하지 못해 부자연스러운 포즈가 유발될 수 있습니다. 실제 DexPilot이나 AnyTeleop과 같은 비전 기반 원격조작 시스템에서는 이러한 과제 벡터 매핑을 사용하였는데, 매 실험 전 긴 보정 과정이 필요하고도 세밀한 동작 재현에 한계를 보였습니다. 반면 GeoRT는 사람이 임의로 고안한 매핑 함수를 따르지 않고, 기계적으로 도출된 목표들을 통해 매핑 함수를 자동 학습합니다. 즉, 휴리스틱한 “이 손가락은 여기에 맞춘다”와 같은 규칙을 설계하지 않고도, 로컬 운동학 특성+글로벌 공간 매칭이라는 큰 틀에서 모델이 스스로 최적 매핑을 찾아내도록 한 것입니다. 그 결과 추가적인 튜닝 없이도 인간과 로봇 손의 자연스러운 対응 관계가 부상(emerge)한다는 점을 논문은 강조합니다.\n또 하나의 비교 축은 실시간 성능과 확장성입니다. 기존 많은 방법은 사람이 손을 움직일 때마다 실시간으로 역기구학 계산이나 최적화를 수행하여 로봇 관절각을 결정하므로, 응답 속도가 제한되고 연산 부하가 컸습니다. 일부 최신 연구는 학습 기반으로 매핑을 모색했지만 대개 사람-로봇 데이터 쌍이 필요하거나, 정책(Search) 최적화를 매 시간스텝에 수행하는 등 실용화에 장벽이 있었습니다. GeoRT의 경우 학습 단계에서 모든 계산을 끝마치고, 운영 단계에서는 신경망 순전파(forward)만 수행하므로 현격히 가볍습니다. 논문에서 비교한 DexPilot이나 AnyTeleop 시스템은 리타게팅 속도가 약 60–100Hz 수준인 반면 GeoRT는 1000Hz로 10배 이상 빠르며, Robotic Telekinesis와 같이 오프라인 학습을 거친 방법과 동등한 최고 속도를 유지합니다. 또한 GeoRT는 하드웨어 제약에 대한 의존성이 낮습니다. 센서 장갑, 비전 모션캡처, Leap Motion 등 어떤 손 추적 수단으로 사람 손 키포인트를 얻어도 동일하게 적용 가능하며, 로봇 손도 인간형 오형이라는 가정만 성립하면 모델 구조나 파라미터 수 변경 없이 적용할 수 있습니다. (물론 사람 손가락 수보다 로봇 손가락 수가 현저히 적거나 하면 핀치 対응 등을 새로 정의해야 하므로, GeoRT는 현재로서는 인간형 로봇 손에 초점을 맞춘 해법입니다.)\n마지막으로, GeoRT에서 제시한 기하학적 목표들은 특정 구현에 국한되지 않고 다른 맥락에도 활용 가능하다는 장점을 갖습니다. 예를 들어 본 논문에서는 물체를 직접 다루는 과업지향적(hand-object) 리타게팅은 다루지 않았지만, 저자들은 제안한 규제항(regularization)들을 기존 방법에 추가하여 손-물체 동시 매핑의 품질도 향상시킬 수 있을 것으로 언급합니다. 이는 GeoRT의 철학이 보편적인 형태의 손동작 対응 문제로 확장될 수 있음을 시사합니다. 실제 최근 연구 중에는 형상 대응(shape correspondence) 문제로 손 리타게팅을 바라보는 시도들도 있는데, GeoRT의 원리는 이러한 접근(예: 두 손의 표면을 사상하여 변형 에너지를 최소화하는 방법 등)과도 일맥상통하는 부분이 있습니다. 요컨대 GeoRT는 기존 방식들의 경험적 한계를 인식하고, 이를 체계적인 수학적 원칙으로 극복함으로써 한 단계 진일보한 손 리타게팅 해법이라 평가할 수 있습니다."
  },
  {
    "objectID": "posts/paper/2025-06-09-geort.html#실험-및-결과-분석",
    "href": "posts/paper/2025-06-09-geort.html#실험-및-결과-분석",
    "title": "📃GeoRT 리뷰",
    "section": "2.5 실험 및 결과 분석",
    "text": "2.5 실험 및 결과 분석\n\n2.5.1 시뮬레이션 평가: 부드러운 제어와 공간 활용도\n저자들은 먼저 가상 시뮬레이터에서 제안한 GeoRT의 리타게팅 품질을 정량 평가하였습니다. 이를 위해 두 가지 지표를 정의했는데, 하나는 운동 보존율이고 다른 하나는 공간 커버리지율입니다. 운동 보존율은 앞서 기준 I에 대응하는 지표로, 무작위로 다양한 손 자세와 그 주변의 작은 방향 변화를 샘플링하여 로봇 손끝 움직임이 사람 손끝 움직임과 얼마나 방향 정렬이 잘 되는지를 나타냅니다. 값은 0~1 사이이며 1에 가까울수록 모든 국소 움직임 방향이 완전히 일치함을 의미합니다. 공간 커버리지율은 기준 II에 대응하는 지표로, 충분히 많은 인간 손가락 포즈 표본들을 로봇 손가락 포즈로 변환했을 때 로봇 손 구성 공간 중 얼마나 넓은 영역을 덮었는지를 백분율로 나타낸 것입니다. 0%이면 인간 동작이 로봇 공간의 극히 일부만 사용함을, 100%이면 로봇 손의 전체 가동 범위를 빠짐없이 커버했음을 의미합니다.\nGeoRT와 기존 방식들을 이 두 지표로 비교한 결과, GeoRT는 운동 보존율 약 0.94로 기존 선형 매핑 기반 방법(약 0.73)보다 훨씬 높았으며, 로봇 공간 커버리지도 약 90%로 기존 방식(약 38%)보다 크게 향상되었습니다. 즉 훨씬滑らか(부드럽고 일관된) 제어 감각과 거의 전역에 걸친 로봇 공간 활용이 달성된 것입니다. 이러한 결과는 GeoRT가 명시적으로 최적화한 목표들과 정확히 부합하는 것이어서 놀랍지는 않지만, 제안된 기하학적 손실 설정이 제대로 효과를 발휘함을 입증합니다. 결국 GeoRT를 쓰면 사용자는 로봇 손의 최대 가용 범위를 활용하면서도 미세한 손동작까지 로봇에서 자연스럽게 재현할 수 있음을 시뮬레이션을 통해 확인한 것입니다.\n또한 흥미로운 질적 실험으로, 저자들은 특정 매핑 휴리스틱 없이도 GeoRT가 얼마나 그럴듯한 사람-로봇 対응을 학습하는지 관찰했습니다. 예를 들어 인간 손의 약지-검지 사이 핀치 동작 등은 기존 선형 매핑에서는 잘 구현되지 않았지만, GeoRT 모델은 이러한 세부적인 対응 관계도 목표 손실들만으로 스스로 발견해냈습니다. 그림 7의 사례들을 보면, GeoRT는 작업 벡터 간 일치 항을 전혀 쓰지 않고도 인간과 로봇 손가락 사이에 자연스러운 対응이 형성되는 것을 볼 수 있습니다. 이는 제안한 접근법이 사람의 손동작 의도를 충실히 살려낸다는 점을 보여주는 인상적인 결과입니다.\n\n\n2.5.2 실제 로봇 실험: 물체 잡기 성능 비교\nGeoRT의 성능은 실제 로봇 시스템 상에서도 검증되었습니다. 저자들은 Franka Panda 로봇 팔 끝에 Allegro 로봇 손을 장착하고, 사람은 한 손에 Manus VR 장갑(손가락 위치 트래킹)과 손목에 Vive 트래커(팔 동작 트래킹)를 착용하여 원격 조작을 수행하는 실험을 진행했습니다. 사람의 손가락 움직임은 Manus 장갑으로 읽어 GeoRT 모델의 입력으로 들어가고, 출력된 Allegro 손 관절 위치 명령은 PD 제어를 통해 로봇 손을 구동했습니다. 한편 사람 팔의 움직임은 Vive 트래커로 받아 로봇 팔의 손끝 위치를 따라가도록 제어함으로써, 사용자의 손 위치와 로봇 손 위치도 동기화시켰습니다. 이렇게 구성된 원격 조작 시스템으로 여러 가지 집기(grasping) 실험을 실시하여, GeoRT 방식과 기존 방식의 작업 성공률과 소요 시간을 비교했습니다.\n비교 대상으로는 앞서 언급된 선형 매핑 기반 방법을 두 가지 버전으로 사용했는데, 하나는 매 프레임 실시간으로 보정이 이뤄지는 온라인 버전이고 다른 하나는 고정된 보정값을 쓰는 오프라인 버전입니다. 평가 지표로는 한 번 시도로 물체 잡기에 성공하는 비율(One-trial success)과 성공적인 그립을 이루기까지 걸린 평균 시간을 측정했습니다. 그 결과 GeoRT를 사용한 경우 한 번에 잡기 성공할 확률이 87.5%로, 오프라인 선형 매핑(55.0%)이나 온라인 보정 매핑(42.5%)보다 훨씬 높았습니다. 특히 온라인 방식은 잦은 보정에도 불구하고 성공률이 오히려 떨어졌는데, 이는 프레임 간 가변적인 매핑으로 사용자가 적응하기 어려웠기 때문으로 보입니다. 반면 GeoRT는 항상 일관된 대응을 유지하므로 사용자가 빠르게 숙달되어 높은 성공률을 보인 것입니다. 또한 평균 작업 완료 시간도 GeoRT가 3.2초로, 기존 오프라인(9.0초)이나 온라인 방식(19.3초)에 비해 월등히 짧았습니다. 이는 GeoRT를 쓸 경우 사용자가 여러 번 잡으려고 시도하거나 미세 조정에 시간을 보낼 필요 없이, 한번에 신속하게 물체를 집어 옮길 수 있다는 의미입니다. 사람의 감각으로도 GeoRT 방식은 손끝 움직임이 매끄럽고 직관적이라 작은 물체를 집거나 섬세한 조작을 할 때도 어려움이 적었다고 합니다. 반대로 기존 선형 매핑 기반으로는 손가락 미세 제어가 어색해 작은 물체를 집기 상당히 힘들었다고 관찰되었습니다.\n 알레그로 로봇 손과 프랑카 팔로 구성된 실험 시스템을 이용해 다양한 섬세한 조작 작업을 원격 수행하는 장면. (위 왼쪽) 평면 위 물체를 집어드는 동작, (위 오른쪽) 드라이버로 나사를 조이는 동작, (아래 왼쪽) 작은 블록을 정밀하게 쌓는 동작, (아래 오른쪽) 주사기 형태의 도구를 잡고 제어하는 모습. 제안된 GeoRT 기반 원격 조작을 통해 사용자는 이와 같은 다양한 정밀 작업을 안정적으로 수행할 수 있었다.\n추가로, 저자들은 GeoRT 시스템으로 주어진 작업들을 얼마나 빠르게 연속 수행할 수 있는지도 데모를 보였습니다. 예컨대 여러 가지 물건 12개가 흩어진 테이블에서 이를 한 손으로 집어 모두 통에 담는 과제를 약 100초만에 완료하였는데, 이때 느린 로봇 팔 움직임이 병목이었을 뿐 손 자체의 동작은 대부분 한 번 시도로 성공했다고 합니다. 이는 GeoRT 기반 제어의 효율성을 방증하는 예로 볼 수 있습니다. 종합하면, 실제 로봇 실험에서 GeoRT는 기존 대비 월등히 높은 작업 성공률과 빠른 조작을 구현했으며, 특히 정밀한 그립 동작에서 사용자에게 향상된 제어감과 자신감을 제공함을 확인했습니다."
  },
  {
    "objectID": "posts/paper/2025-06-09-geort.html#연구-의의-한계-및-향후-전망",
    "href": "posts/paper/2025-06-09-geort.html#연구-의의-한계-및-향후-전망",
    "title": "📃GeoRT 리뷰",
    "section": "2.6 연구 의의, 한계 및 향후 전망",
    "text": "2.6 연구 의의, 한계 및 향후 전망\nGeometric Retargeting (GeoRT) 알고리즘은 사람 손동작을 로봇 손으로 전달하는 오랜 문제에 대해 명확한 원리와 실용적인 해법을 제시했다는 점에서 크게 주목받고 있습니다. 우선 이 연구의 의의를 짚어보면 다음과 같습니다:\n\n원리 기반의 정형화: 사람-로봇 손 매핑 문제를 다섯 가지 기하학적 기준으로 정량화함으로써, 감에 의존하던 휴리스틱 설계 대신 과학적이고 재현 가능한 방법론을 마련했습니다. 이는 향후 유사한 문제(예: 인간의 팔 동작을 로봇 팔에 매핑, 인간 걸음새를 로봇에 매핑 등)에도 응용될 수 있는 틀을 제공한 것입니다.\n실시간 성능과 범용성: 간결한 MLP 구조와 사전 학습된 보조 모듈들을 활용하여 1kHz급 실시간 동작 변환을 달성했고, 추가 최적화나 복잡한 연산 없이도 다양한 시나리오에 적용 가능한 경량 프레임워크를 구축했습니다. 이는 원격 조작 시스템을 대규모로 확장하거나, 로봇 제어의 내부 피드백 루프에 통합하는 등 응용 범위를 크게 넓혀줍니다. 실제 저자들은 GeoRT를 자사의 DexterityGen이라는 파운데이션 제어기와 결합하여, 사용자의 거친 원격 조작을 뒷단에서 미세 조정해주는 액션 보정 시스템을 구현하기도 했습니다. 초고속 매핑 덕분에 이러한 상위 제어와의 실시간 연동이 가능해진 사례입니다.\n휴먼-로봇 협업 경험 향상: 실험 결과에서 보았듯 GeoRT는 사용자로 하여금 자신의 손처럼 로봇 손을 직관적으로 조종할 수 있게 해줍니다. 이는 곧 원격 로봇 조작의 학습 부담을 줄이고 생산성을 높이는 효과로 이어집니다. 예컨대 비숙련자도 짧은 시간내에 섬세한 로봇 작업을 수행할 수 있고, 피로도도 낮아질 것으로 기대됩니다. 장기적으로 이런 기술이 발전하면 원격 의료 수술, 원격 제조 등에서 인간과 로봇의 상호작용 효율이 크게 향상될 것입니다.\n\n한편, GeoRT에도 한계와 도전 과제가 존재합니다. 첫째, 인간형 로봇 손에 최적화되어 있다는 점입니다. 논문에서도 전제 조건(A1, A2)으로 로봇 손이 인간 손과 구조적으로 유사하고 손가락 対应관계가 명확함을 가정하고 있습니다. 따라서 사람보다 손가락 수가 적거나 많은 로봇, 혹은 구조가 크게 다른 로봇(hand이 아닌 집게 형태 등)에는 그대로 적용하기 어렵습니다. 이러한 경우 対应관계를 정의하는 추가 연구나 다른 형태의 목표 함수가 필요할 수 있습니다. 둘째, GeoRT는 물체와 상호작용하지 않는 맨손의 매핑에 한정됩니다. 물체를 쥔 상태에서의 손가락 움직임이나, 도구 사용 등의 시나리오에서는 단순 손가락 끝 위치만으로 이상적인 매핑을 정의하기 어려울 수 있습니다. 향후에는 물체의 상태나 힘/촉각 정보까지 포함한 과업 지향적 리타게팅으로의 확장이 필요합니다. 다행히 제안된 기하학적 목표들은 이러한 세팅에서도 규제 항으로 응용될 수 있을 것으로 보입니다. 셋째, 현재 모델은 손가락별 독립적으로 구성되어 엄지-검지 핀치 외의 복잡한 다손가락 협응 동작에 대한 명시적 제약은 부족합니다. 예를 들어 세 손가락으로 동시에 물체를 파지하는 동작 등에서는 보다 전체 손의 통합적인 매핑 전략이 필요할 수 있습니다. 이를 위해 추후에는 손 전체를 입력으로 받아 전체 관절을 출력하는 통합 모델이나, 손가락 간 종속성까지 학습하는 구조로의 발전 가능성도 있습니다.\n또 다른 흥미로운 확장 방향은, GeoRT를 오프라인 모션 사본 생성이나 모방 학습 분야에 활용하는 것입니다. 예컨대 사람의 시연 동작(비디오 혹은 Mocap 데이터)을 로봇으로 재현하는 데에도 동일한 원리의 매핑이 쓰일 수 있습니다. 실제 Robotic Telekinesis 연구는 유튜브 영상의 인간 손동작을 로봇이 모방하도록 학습했는데, GeoRT의 손실 함수를 적용하면 이러한 크로스 도메인 모방 학습의 성능도 개선시킬 여지가 있을 것입니다. 마지막으로, GeoRT의 초고속 성질은 단순히 원격 조작뿐 아니라 휴머노이드 로봇의 실시간 모방 제스처, VR 아바타 손 움직임 자연화 등 다양한 실시간 인터랙티브 시스템에 기여할 수 있습니다.\n요약하면, Geometric Retargeting 알고리즘은 기존의 인간-로봇 손동작 매핑 문제에 이론과 구현 양면에서 혁신적인 솔루션을 제시했습니다. 사람의 감각적인 기대치를 수학적으로 풀어내고 이를 빠른 학습 모델로 실현함으로써, 원격 로봇 조작의 정확성, 속도, 직관성을 모두 끌어올렸습니다. 앞으로 남은 과제들은 이 접근을 보다 다양한 로봇 형태와 시나리오로 확장하고, 인간과 로봇 간 물리적 상호작용까지 포괄하는 방향으로 나아가는 것입니다. 이러한 후속 연구들이 이루어진다면, 한층 자연스러운 인간-로봇 협업 시대를 앞당길 수 있을 것으로 기대됩니다."
  },
  {
    "objectID": "posts/paper/2025-07-03-physics-rl.html",
    "href": "posts/paper/2025-07-03-physics-rl.html",
    "title": "📃Physics Informed RL Survey 리뷰",
    "section": "",
    "text": "Paper Link"
  },
  {
    "objectID": "posts/paper/2025-07-03-physics-rl.html#introduction",
    "href": "posts/paper/2025-07-03-physics-rl.html#introduction",
    "title": "📃Physics Informed RL Survey 리뷰",
    "section": "Introduction",
    "text": "Introduction\nRL은 시행착오를 통해 의사 결정 및 최적화 문제를 해결하는 유망한 접근 방식입니다. 자율 주행, 로봇 공학, 연속 제어 등 다양한 분야에서 성공을 거두었지만, 실제 데이터의 샘플 효율성 부족, 고차원 연속 상태/액션 공간 처리의 어려움, 안전한 탐색, 적절한 보상 함수 정의, 시뮬레이터-실제 환경 간의 차이 등의 문제에 직면해 있습니다. 물리 정보를 ML 모델에 통합하는 PIML(Physics-Informed Machine Learning)은 불완전한 물리 정보와 데이터로부터 더 효율적으로 학습하고, 더 나은 일반화 성능을 보이며, 물리적으로 타당한 솔루션을 제공하는 장점이 있습니다. RL은 대부분 실제 세계 문제와 관련이 있으며 설명 가능한 물리적 구조를 가지고 있기 때문에 물리 정보 통합에 적합한 분야입니다. 최근 연구들은 물리 정보를 RL 파이프라인에 통합하여 이러한 과제를 해결하고 있습니다. 예를 들어, 물리 정보를 사용하여 고차원 연속 상태를 직관적인 표현으로 줄이거나 더 나은 시뮬레이션을 구축하며, 안전한 학습을 위한 물리적 제약 조건을 보상 함수에 통합하는 등의 시도가 이루어지고 있습니다. PIRL 연구는 지난 6년간 증가하는 추세를 보이며 주목받고 있습니다.\n\nTaxonomy: 어떤 물리 지식/프로세스가 모델링되고, 어떻게 표현되며, RL 접근 방식에 어떻게 통합되는지에 대한 통합 분류 체계를 제시합니다.\nAlgorithmic Review: 물리 정보 기반 RL 방법론에 대한 최신 접근 방식을 통일된 표기법과 기능 다이어그램을 사용하여 검토합니다.\nTraining and evaluation benchmark Review: 검토된 문헌에서 사용된 평가 벤치마크를 분석하여 인기 있는 플랫폼/도구를 제시합니다.\nAnalysis: 다양한 도메인에 걸친 model-based 및 model-free RL 애플리케이션에서 물리 정보가 특정 RL 접근 방식에 어떻게 통합되는지, 어떤 물리 프로세스가 모델링/통합되는지, 어떤 네트워크 아키텍처 또는 증강이 사용되는지 상세히 분석합니다.\nOpen Problems: 현재 직면한 과제, 미해결 연구 질문 및 향후 연구 방향에 대한 관점을 제시합니다."
  },
  {
    "objectID": "posts/paper/2025-07-03-physics-rl.html#piml-an-overview",
    "href": "posts/paper/2025-07-03-physics-rl.html#piml-an-overview",
    "title": "📃Physics Informed RL Survey 리뷰",
    "section": "PIML: An Overview",
    "text": "PIML: An Overview\n\n물리 정보를 활용한 기계 학습 개요\n\nPIML은 수학적 물리 모델과 관측 데이터를 학습 과정에 통합하여, 불완전하고 불확실하며 고차원적인 복잡한 시나리오에서도 물리적으로 일관된 솔루션을 찾는 것을 목표로 합니다. 물리 지식을 ML 모델에 추가하는 것은 물리/과학적 일관성 보장, 데이터 효율성 증가, 학습 과정 가속화, 일반화 능력 향상, 투명성/해석 가능성 증진과 같은 이점을 제공합니다. 물리 지식을 통합하는 세 가지 주요 전략은 다음과 같습니다.\n\nObservational bias: 물리적 원리를 반영하는 multi-modal 데이터를 사용하여 DNN을 학습시킵니다. 관측, 시뮬레이션, 물리 방정식 생성 데이터, 지도, 추출된 물리 데이터 등 다양한 소스의 데이터를 활용합니다.\nLearning bias: 손실 함수에 물리 기반의 페널티 항을 추가하여 사전 지식을 강화하는 방식입니다. PINN(Physics-Informed Neural Networks)은 PDE를 신경망의 손실 함수에 포함시키는 대표적인 예입니다.\nInductive biases: custom neural network 구조를 통해 물리 원리를 ‘하드’ 제약 조건으로 통합하는 방식입니다. Hamiltonian NN, Lagrangian Neural Networks (LNNs) 등이 있습니다."
  },
  {
    "objectID": "posts/paper/2025-07-03-physics-rl.html#pirl-fundamentals-taxonomy-and-examples",
    "href": "posts/paper/2025-07-03-physics-rl.html#pirl-fundamentals-taxonomy-and-examples",
    "title": "📃Physics Informed RL Survey 리뷰",
    "section": "PIRL: Fundamentals, Taxonomy and Examples",
    "text": "PIRL: Fundamentals, Taxonomy and Examples\n\n물리 정보를 활용한 강화 학습: 기본, 분류 및 예시\n\nRL 기본 (RL fundamentals)\nRL은 MDP (Markov Decision Process) 프레임워크를 따르는 순차적 의사 결정 문제를 해결합니다. 에이전트(agent)와 환경(environment)이 상호 작용하며, 에이전트는 상태(s_t)를 관찰하고 행동(a_t)을 선택하며, 환경은 다음 상태(s_{t+1})와 보상(r_t)을 제공합니다. 목표는 누적 보상을 최대화하는 정책 \\pi_\\phi(a_t|s_t)의 매개변수 \\phi를 찾는 것입니다. MDP는 튜플 (S, A, R, P, \\gamma)로 표현되며, S는 상태 공간, A는 액션 공간, R은 보상 함수, P(s_{t+1}|s_t, a_t)는 환경 모델(전이 확률), \\gamma \\in [0, 1]는 할인 계수입니다. 목표 함수는 다음과 같습니다. J(\\phi) = \\mathbb{E}_{\\tau \\sim p_\\phi(\\tau)} \\left[ \\sum_{t=1}^T \\gamma^{t-1} R(a_t, s_{t+1}) \\right] 여기서 \\tau는 에피소드의 상태-액션 시퀀스입니다. RL 알고리즘은 model-free (환경 모델 없이 학습)와 model-based (환경 모델을 사용하여 계획/학습)로 나눌 수 있습니다. 또한, online (최신 정책으로 수집한 데이터 사용), off-policy (경험 리플레이 버퍼의 데이터 사용), offline (고정된 데이터셋 사용)으로 분류됩니다.\nPIRL 소개 (PIRL: Introduction)\nPIRL은 물리 구조, 사전 지식(priors), 실제 물리 변수를 정책 학습 또는 최적화 과정에 통합하는 개념입니다. 이는 RL 알고리즘의 효율성, 샘플 효율성, 훈련 가속화에 기여합니다.\nPIRL 분류 체계 (PIRL Taxonomy)\n이 논문은 물리 정보 유형, 물리 정보를 통합하는 PIRL 방법, 그리고 RL 파이프라인의 세 가지 축을 중심으로 PIRL 분류 체계를 제시합니다.\n\nPhysics information (types): representation of physics priors\n\nDifferential and algebraic equations (DAE): PDE/ODE, 경계 조건(BC) 등 시스템 동역학 표현 (예: PINN).\nBarrier certificate and physical constraints (BPC): CLF, BF, CBF/CBC 등 안전 제약 조건 (예: 안전 중요 애플리케이션의 탐색 규제).\nPhysics parameters, primitives and physical variables (PPV): 환경/시스템에서 추출/도출된 물리 값 (예: jam-avoiding distance, dynamic movement primitives).\nOffline data and representation (ODR): 시뮬레이터 기반 학습 개선을 위한 오프라인 데이터 또는 물리적으로 관련된 저차원 표현 학습.\nPhysics simulator and model (PS): RL 알고리즘의 테스트베드 또는 물리적 정확성을 부여하기 위한 시뮬레이터 활용 (예: MBRL에서 시스템 모델 학습).\nPhysical properties (PPR): 시스템 형태, 대칭 등 기본적인 물리 구조/속성 지식.\n\nPIRL methods: physics prior augmentations to RL\n\nState design: 관찰된 상태 공간 수정/확장 (예: 상태 융합, 특징 추출).\nAction regulation: 액션 값에 제약 조건 부과 (예: 안전 필터).\nReward design: 효과적인 보상 설계 또는 보상 함수 증강.\nAugment policy or value N/W: 정책 또는 가치 함수의 업데이트 규칙, 손실, 구조 변경.\nAugment simulator or model: 기초 물리 지식 통합을 통한 시뮬레이터/모델 개선.\n\nRL Pipeline\n\nProblem Representation: 실제 문제를 MDP로 모델링 (상태, 액션, 보상 정의).\nLearning strategy: 에이전트-환경 상호 작용 방식, 학습 아키텍처, 알고리즘 선택 결정.\nNetwork design: 정책/가치 네트워크의 세부 구조 설계.\nTraining: 네트워크 학습 (Sim-to-real 등 훈련 증강 포함).\nTrained policy deployment: 훈련된 정책 배포.\n\n\n추가 분류 (Further categorization)\n이 논문은 추가적으로 두 가지 범주를 사용하여 PIRL 구현을 설명합니다.\n\nBias: PIML에서 사용되는 bias 개념(Observational, Learning, Inductive)과 PIRL 접근 방식의 관계를 분석합니다.\nLearning architecture: 물리 정보 통합을 위해 전통적인 RL 학습 아키텍처에 도입된 변경 사항에 따라 분류합니다.\n\nSafety filter: 안전 제약 조건을 보장하기 위해 에이전트의 액션을 조절하는 모듈 포함.\nPI reward: 보상 함수를 물리 정보로 수정.\nResidual learning: 물리 정보 기반 제어기와 데이터 기반 정책을 결합.\nPhysics embedded network: 정책 또는 가치 함수 네트워크에 시스템 동역학 등 물리 정보 직접 통합.\nDifferentiable simulator: 손실 기울기를 제어 액션에 대해 직접 계산할 수 있는 미분 가능한 물리 시뮬레이터 사용.\nSim-to-Real: 시뮬레이터에서 학습 후 실제 환경으로 전이.\nPhysics variable: 물리 매개변수, 변수, 프리미티브를 상태/보상 등에 추가.\nHierarchical RL: 계층적 또는 커리큘럼 학습 설정에서 물리 정보를 통합.\nData augmentation: 입력 상태를 저차원 표현 등으로 대체/증강하여 물리적으로 관련된 특징 도출.\nPI model identification: MBRL 설정에서 물리 정보를 모델 식별 과정에 통합."
  },
  {
    "objectID": "posts/paper/2025-07-03-physics-rl.html#pirl-review-and-analysis",
    "href": "posts/paper/2025-07-03-physics-rl.html#pirl-review-and-analysis",
    "title": "📃Physics Informed RL Survey 리뷰",
    "section": "PIRL: Review and Analysis",
    "text": "PIRL: Review and Analysis\n\nAlgorithmic review: 위에 제시된 PIRL 방법 및 학습 아키텍처 범주를 기반으로 연구들을 그룹화하여 논의합니다. 예를 들어, State design에서는 CAV 제어에서의 물리 기반 상태 융합, Adaptive cruise control에서의 jam-avoiding distance 활용 등이 논의됩니다. Action regulation에서는 안전 중요 시스템의 CBF/CBC를 활용한 액션 제약이 강조되며, B_\\epsilon(x)와 Lie derivative \\mathcal{L}_f(x, u_{RL}) B_\\epsilon(x)를 이용한 안전 조건이 언급됩니다. Reward design에서는 로봇 보행, 에너지 관리, 유체역학 등 다양한 분야에서 물리 기반 보상 함수 설계 사례가 제시됩니다. Augment simulator or model에서는 LNN을 사용한 시스템 모델 학습, sim-to-real 전이 개선을 위한 시뮬레이터 증강, 미분 가능한 시뮬레이터 사용 등이 포함됩니다. Augment policy and/or value N/W에서는 신경망 정책에 동적 시스템을 미분 가능한 레이어로 통합하는 Neural Dynamic Policies (NDP), 가치 함수를 HJB PDE를 푸는 PINN으로 취급하는 접근 방식 등이 소개됩니다.\nSimulation/ evaluation benchmarks: 연구에서 사용된 다양한 시뮬레이터 및 평가 환경을 OpenAI Gym, MuJoCo, Pybullet, Deep mind control suite와 같은 표준 벤치마크와 SUMO, CARLA, IEEE distribution system benchmarks 같은 도메인별 플랫폼, 그리고 다수의 맞춤형 환경으로 분류하여 제시합니다.\nAnalysis:\n\n연구 동향 및 통계: 가장 많이 사용되는 RL 알고리즘은 PPO이며, 그 뒤를 DDPG, SAC 등이 잇습니다. 물리 정보 유형으로는 물리 시뮬레이터, 시스템 모델, 배리어 인증서/물리 제약이 가장 흔하게 사용됩니다. 학습 아키텍처 중 PI reward와 safety filter는 주로 learning bias를 통해, physics embedded network는 inductive bias를 통해 물리를 통합합니다. 애플리케이션 도메인의 85% 가량이 제어 또는 정책 설계와 관련 있으며, 그 중 Miscellaneous control, Safe control and exploration, Dynamic control이 주를 이룹니다.\nRL 해결 과제: PIRL은 다음과 같은 RL 과제 해결에 기여합니다. Sample efficiency (시뮬레이터/모델 증강), Curse of dimensionality (물리 관련 저차원 표현 학습), Safety exploration (CBF/CLF 등 제어 이론 활용), Partial observability (상태 증강/융합), Under-defined reward function (물리 기반 보상 설계/증강).\n\n\n미해결 과제 및 연구 방향 (Open Challenges and Research Directions)\n\nHigh Dimensional Spaces: 고차원 공간에서 물리적으로 관련된 정보성이 풍부한 저차원 표현을 학습하는 것이 여전히 과제입니다.\nSafety in Complex and Uncertain Environments: 복잡하고 불확실한 환경에서 model-agnostic하며 일반화 가능한 안전한 탐색 및 제어 접근 방식 개발이 필요합니다. 데이터 기반 모델 학습에 물리를 통합하는 일반화된 접근 방식도 중요합니다.\nChoice of physics prior: 문제에 적합한 물리 사전 지식을 선택하는 것은 어렵고 도메인별 전문 지식이 필요합니다. 새로운 물리적 태스크를 다룰 수 있는 포괄적인 프레임워크 구축이 필요합니다.\nEvaluation and bench-marking platform: PIRL 연구를 위한 포괄적인 벤치마킹 및 평가 환경이 부족하여 새로운 방법론의 비교 및 평가가 어렵습니다. 도메인별로 맞춤화된 환경에 의존하는 경향이 있습니다.\n\n결론 (Conclusions): 본 논문은 PIRL 패러다임을 소개하고, 물리 사전 지식 유형 및 물리 정보 통합 방식(RL 방법)에 기반한 분류 체계를 제시합니다. 또한, 학습 아키텍처 및 bias에 따른 추가 분류를 통해 PIRL 구현을 더 잘 이해할 수 있도록 돕습니다. 최신 문헌을 검토하고, 물리 정보가 RL 파이프라인의 다양한 단계에 어떻게 통합되는지 분석하며, 사용된 벤치마크를 요약합니다. 마지막으로, 현재 PIRL 연구의 한계점과 미해결 과제를 논의하며 향후 연구 방향을 제시합니다. PIRL은 물리적 타당성, 정밀도, 데이터 효율성, 실제 환경 적용 가능성을 높여 RL 알고리즘을 향상시킬 잠재력이 있습니다."
  },
  {
    "objectID": "posts/paper/2024-12-22-rotating-without-seeing.html",
    "href": "posts/paper/2024-12-22-rotating-without-seeing.html",
    "title": "📃Rotating without Seeing 리뷰",
    "section": "",
    "text": "이번 포스팅에서 리뷰할 논문은 Rotating without Seeing: Towards In-hand Dexterity through Touch 입니다. RSS(Robotics: Science and Systems) 2023 학회에서 발표된 해당 논문은 사람이 시각 없이 촉각만으로 손안에서 물체를 정교하게 조작하는 능력을 로봇 핸드에 구현하고자, 손바닥, 손가락 관절, 손끝 전체에 넓게 분포된 저비용의 이진 촉각 센서를 활용하여, 시뮬레이션에서 강화학습으로 학습한 정책을 실제 로봇 손에 적용하고, 이를 통해 학습한 물체뿐만 아니라 학습하지 않은 새로운 물체까지 조작할 수 있는 시스템인 Touch Dexterity를 제안합니다."
  },
  {
    "objectID": "posts/paper/2024-12-22-rotating-without-seeing.html#problem-formulation",
    "href": "posts/paper/2024-12-22-rotating-without-seeing.html#problem-formulation",
    "title": "📃Rotating without Seeing 리뷰",
    "section": "2.1 Problem Formulation",
    "text": "2.1 Problem Formulation\nTouch Dexterity는 강화학습 방법으로 제어를 하기 때문에 강화학습의 문제 정의 방식인 MDP(Markov Decision Process)의 요소들, State, Action, Reward 순으로 확인해보겠습니다.\n\n2.1.1 State\nState는 Hand Robot Agent의 상태를 나타내는 요소들로 이루어집니다. Allegro hand 로봇의의 joint position(16 차원), sensor observation(16차원), 이전 position target(16차원), 그리고 rotation axis(2차원)로 구성되어 있습니다. 핸드 로봇의 관절(joint) 부분들이 작은 모터들 16개로 이루어져 있고, FSR(Force Sensing Resistor) 센서들도 총 16개가 아래 그림처럼 손가락과 손바닥에 분포되어 있어 State 벡터의 차원들이 다음과 같이 구성되게 됩니다. 이렇게 구성된 State가 학습 시에 Policy Network의 Input으로 들어가게 되는데 1 time step 정보만으로는 학습하기에 부족한 정보량이기 때문에 현재 시점 기준 이전 스텝 2 time step을 합쳐(concatenation), 총 3 time step 을 쌓아서 policy network에 input으로 넣어줍니다.\n\n\n\nState 구성요소\n\n\n\n\n2.1.2 Action\nHand Agent가 움직이는 Action은 로봇의 각 관절(joint) 모터들이 움직이는 것으로 생각할 수 있습니다. 따라서 Policy network에서는 16차원의 모터와 관련된 어떠한 command 정보가 나오게 됩니다. 하지만 Policy network의 output인 a_t를 바로 쓰는 것이 아닌 PD Controller에 적용하기 위한 값으로 변환하는 과정을 거치게 됩니다. 결과적으로 PD Controller에서 사용하는 값은 \\tilde{q}_{t+1} (현재 time step이 t 이므로 앞으로 제어할 position의 time step 첨자는 t+1)인 것 입니다.\n하지만 여기서 \\tilde{q}_{t+1}을 바로 적용할 경우 생기는 문제가 있습니다. policy network output 값들이 연속적인 시간 순으로 봤을때 갭이 큰 값들이 나타나게 되면 부드러운 움직임을 가질 수 없습니다. 따라서 해당연구에서는 Exponential moving average 방법을 사용하여 smoothing하는 과정을 거치게 됩니다.\n\n\n\nAction이 적용되는 과정\n\n\n아래 그래프는 논문에서 제시한 파라미터(\\eta, 2 consecutive steps)로 랜덤한 포인트들을 가지고 smoothing하는 모습을 보여줍니다.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Parameters\neta = 0.8  # Smoothing factor\nsteps = 2  # Step size for x-axis\nn_points = 200  # Number of points\n\n# Generate data\nx = np.arange(0, n_points, steps)\ndata = np.sin(x / 5) + np.random.normal(0, 0.3, len(x))  # Random data with noise\nema = []\n\n# Calculate EMA\nfor i, point in enumerate(data):\n    if i == 0:\n        ema.append(point)  # Initialize EMA with the first data point\n    else:\n        ema.append(eta * point + (1 - eta) * ema[-1])\n\n# Plot\nplt.figure(figsize=(8, 3))\nplt.plot(x, data, label=\"Data\", marker=\"o\", linestyle=\"--\", alpha=0.6)\nplt.plot(x, ema, label=\"Exponential Moving Average (EMA)\", linewidth=2)\nplt.xlabel(\"Step\")\nplt.ylabel(\"Value\")\nplt.title(f\"Exponential Moving Average (eta={eta}, step={steps}) \")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n이렇게 최종적으로 계산된 Action 값으로 Hand Agent의 모션이 만들어지게 됩니다.\n\n\n2.1.3 Reward\n보상함수는 아래와 같이 6개의 term들로 구성되어 있습니다. 각 6개의 reward term들은 linear weighted sum이 되어 해당 timestep에서의 최종 reward가 됩니다.\n\n\n\nReward Function\n\n\n\nReward of rotation r_{rot}\n\n회전 축 k의 법선 평면 \\Pi에서 샘플링된 단위 벡터의 회전 각도 \\Delta \\theta로 정의된 회전 보상입니다.\n\n\n\nReward Function\n\n\n계산하는 과정\n\n법선 평면 \\Pi에서 단위 벡터 v를 임의로 샘플링하며, 이 벡터는 object에 부착된 것으로 간주할 수 있습니다.\n다음 상태에서의 해당 벡터 v'를 가져와 \\Pi에 투영(projection)합니다: v'_p = \\text{Proj}(v', \\Pi)\n\\Delta \\theta \\in [-\\pi, \\pi)는 축 k에 대해 v'_p와 v 사이의 부호 있는 거리로 정의됩니다.\n\nobject의 움직임이 매우 복잡한 경우, 시뮬레이터가 제공하는 각속도가 매우 노이즈가 심하기 때문에 보상에 이 각속도를 보상함수에 사용할 경우 특정 자세에서 진동하는 등 바람직하지 않은 object 움직임 패턴이 나타날 수 있습니다.\n이 유한 차분(finite difference)을 보상으로 사용하는 것이 서로 다른 실행에서도 일관된 회전 동작을 생성할 수 있습니다.\n\nPenalty of object’s velocity r_{vel}\n\n손이 object를 안정적으로 회전시키도록 장려하며, 훈련된 정책의 transferability을 향상시킵니다.\n\nPenalty of falling r_{fall}\n\nobject가 손바닥에서 떨어질 때 적용되는 negative penalty입니다.\n\nPenalty of the work controller r_{work}\n\n컨트롤러의 일(work)의 양을 패널티로 부과합니다. 이 reward term의 torque \\tau는 t에서 PD 컨트롤러가 출력한 토크입니다. 이 페널티는 손가락 움직임의 부드러움을 향상시키는 데 도움을 줍니다.\n\nPenalty of torque r_{torque}\n\n큰 토크 출력값에 패널티를 부과합니다.\n\nReward of distance r_{dist}\n\n거리 보상으로, 손끝이 객체에 가까이 가서 상호작용하도록 장려합니다.\n\nd(x_{\\text{tip}}, x_{\\text{obj}})는 손끝 위치 x_{\\text{tip}}와 객체 위치 x_{\\text{obj}} 사이의 거리입니다.\n\\epsilon은 작은 양으로, 분모가 0이 되는 것을 방지합니다.\nc_2와 c_3는 보상의 클리핑 범위를 정의하는 하한과 상한입니다.\n\n\n\n\n\n2.1.4 Reset\n불필요한 exploration를 줄이고 학습 과정을 가속화하기 위해 object가 초기 위치(즉, 손바닥의 중심)에서 너무 많이 벗어날 경우 에피소드를 리셋합니다. 또한, object의 주요 축이 회전 축에서 너무 많이 벗어날 경우에도 에피소드를 리셋합니다."
  },
  {
    "objectID": "posts/paper/2024-12-22-rotating-without-seeing.html#domain-randomization",
    "href": "posts/paper/2024-12-22-rotating-without-seeing.html#domain-randomization",
    "title": "📃Rotating without Seeing 리뷰",
    "section": "2.2 Domain Randomization",
    "text": "2.2 Domain Randomization\n강화학습의 Sim2Real Gap을 줄이기 위해 학습 단계에서 Domain Randomization을 적용하는데 해당 연구에서는 2가지 Domain Randomization을 진행합니다.\n\n물리적 랜덤화:\n\nrotation하는 object의 초기 위치, 질량, 형태, 마찰을 랜덤화하여 학습된 정책이 다양한 종류의 객체를 처리할 수 있도록 합니다.\nPD 컨트롤러의 게인을 랜덤화하여 실제 환경에서 PD 컨트롤러의 불확실성을 모델링합니다.\n각 촉각 센서를 랜덤화하는 것도 고려합니다. 활성화된 접촉 센서(출력이 1인 경우)에 대해, 확률 p로 출력을 0으로 뒤집습니다.\n지수 지연 모델(exponential delay)을 통해 접촉 센서의 신호 지연을 모델링합니다.\n\n비물리적 랜덤화\n\npolicy의 observation과 출력된 action에 화이트 노이즈를 주입하여 작은 외란에도 강인하도록 만듭니다."
  },
  {
    "objectID": "posts/paper/2024-12-22-rotating-without-seeing.html#training-procedure",
    "href": "posts/paper/2024-12-22-rotating-without-seeing.html#training-procedure",
    "title": "📃Rotating without Seeing 리뷰",
    "section": "2.3 Training Procedure",
    "text": "2.3 Training Procedure\nProximal Policy Optimization (PPO) 알고리즘을 사용하며 policy 네트워크와 value 네트워크 모두에 다층 퍼셉트론(MLP)을 사용했습니다.\n\n훈련 설정:\n\n이점(advantage) 클립 임계값 ϵ=0.2= 0.2와 KL 발산 임계값 0.020.02를 사용\n네트워크에서 활성화 함수로 ELU를 사용\n정책 네트워크는 학습 가능한 상태 독립적인 표준편차를 가지는 가우시안 분포를 출력\n\n비대칭 관찰(asymmetric observation):\n\n정책 및 가치 네트워크의 학습 난이도를 줄이기 위해 asymmetric observation 을 사용\n\n가치 네트워크: 입력에 접촉력, object의 ground-truth pose, 물리적 파라미터와 같은 특권 정보를 추가\n정책 네트워크: 현재 상태와 함께 3개의 과거 상태를 입력으로 사용하며, 특권 정보는 접근할 수 없음\n\n\n시뮬레이션 설정:\n\nIsaacGym 시뮬레이션에서 시간 간격(dt)은 0.01667초로 설정하고, 2 sub step을 사용\n8192개의 병렬 환경에서 시뮬레이션을 실행\n정책 네트워크가 출력하는 행동(제어 목표)은 6단계 동안 실행되며, 이는 실제 환경에서 10Hz의 제어 주파수에 해당\n\n\n\n\n\nTraining Process"
  },
  {
    "objectID": "posts/paper/2024-12-22-rotating-without-seeing.html#real-world-system-setup",
    "href": "posts/paper/2024-12-22-rotating-without-seeing.html#real-world-system-setup",
    "title": "📃Rotating without Seeing 리뷰",
    "section": "3.1 Real-world System Setup",
    "text": "3.1 Real-world System Setup\n\n\n\nOverview\n\n\n하드웨어 구성은 XArm 로봇 팔과 16자유도(16-DOF)를 가진 Allegro Hand에 접촉 센서 배열을 장착한 형태로 이루어져 있습니다. 손바닥과 손가락 끝을 포함한 Allegro Hand의 여러 부위에 부착된 16개의 접촉 센서로 구성됩니다.\n사용된 접촉 센서는 외부 힘이 표면에 가해질 때 저항이 변하는 Force-Sensing Resistor(FSR) 기반입니다. STM32F 마이크로컨트롤러를 사용하여 각 센서의 아날로그 전압 신호를 수집하고, 이를 디지털 신호로 변환하여 호스트로 전달합니다. 이 접촉 센서는 연속적인 접촉력 측정을 출력할 수 있지만, 신호는 일반적으로 비선형적이고 노이즈가 많습니다. 따라서 이를 제어에 사용하기 전에 적절한 전처리가 필요합니다. 선택된 임계값 \\theta_{\\text{th}}에 따라 이 측정값을 이진화(binarize)하고 이 신호를 사용합니다.\n이진 신호를 사용하는 장점:\n\n시뮬레이션과 실제 로봇 간의 차이를 줄이고, Sim2Real 전이 절차를 단순화할 수 있습니다.\n이진화된 측정값은 임계값을 조정하여 쉽게 보정(calibrate)할 수 있습니다."
  },
  {
    "objectID": "posts/paper/2024-12-22-rotating-without-seeing.html#simulation-setup",
    "href": "posts/paper/2024-12-22-rotating-without-seeing.html#simulation-setup",
    "title": "📃Rotating without Seeing 리뷰",
    "section": "3.2 Simulation Setup",
    "text": "3.2 Simulation Setup\n이 논문에서는 IsaacGym 시뮬레이터 사용합니다. 각 접촉 센서는 손가락과 손바닥 링크의 고정된 링크로 시뮬레이션됩니다. 시뮬레이터는 매 시뮬레이션 단계에서 각 센서 링크에 대한 순 접촉력 F=[Fx,Fy,Fz]F = [F_x, F_y, F_z]를 제공합니다. \\|F\\|을 시뮬레이션된 접촉력 측정값으로 사용한 다음, 이 측정값을 다른 임계값 \\tilde{\\theta}_{\\text{th}}으로 이진화합니다.\n\n\n\n중요한 점은 센서의 부모 링크에서 제공되는 힘은 순 접촉력에 기여하지 않는다는 것입니다. 시뮬레이션에서 실제 환경과 유사한 동작을 보장하기 위해 이 센서들의 임계값 \\tilde{\\theta}_{\\text{th}}을 조정합니다. 시뮬레이션에서는 \\tilde{\\theta}_{\\text{th}} = 0.01N을 사용합니다."
  },
  {
    "objectID": "posts/paper/2024-12-22-rotating-without-seeing.html#benchmark-in-hand-rotation",
    "href": "posts/paper/2024-12-22-rotating-without-seeing.html#benchmark-in-hand-rotation",
    "title": "📃Rotating without Seeing 리뷰",
    "section": "3.3 Benchmark: In-hand Rotation",
    "text": "3.3 Benchmark: In-hand Rotation\n이 논문에서는 시스템의 손재주(dexterity)를 연구하기 위해, 시스템을 사용하여 손 안에서 회전하는 작업(in-hand rotation task)을 목표로 합니다. 이 task는 object가 손바닥에 초기화된 상태로 시작하며, 로봇 손은 주어진 회전 축을 따라 객체를 회전시켜야 합니다. 손 안에서 객체를 회전하는 동안, object의 움직임은 손끝 회전(finger-tip rotation)보다 훨씬 더 복잡하며 특히, 손 안에서 조작하는 동안 객체는 손바닥에서 미끄러지거나 구를 수 있습니다.\n이와 같은 복잡한 움직임 패턴 때문에, 성공적인 조작을 위해 촉각 센서나 비전(vision) 시스템의 명시적인 피드백이 필요합니다. 그렇지 않으면, 현재 객체의 상태를 추론할 수 없으며, 객체를 안전하게 밀거나 회전시키는 데 실패할 수 있습니다."
  },
  {
    "objectID": "posts/paper/2023-07-02-dreamwaq.html",
    "href": "posts/paper/2023-07-02-dreamwaq.html",
    "title": "📃DreamWaQ 리뷰",
    "section": "",
    "text": "이번 포스팅은 DeepMind에서 발표된 DreamWaQ: Learning Robust Quadrupedal Locomotion With Implicit Terrain Imagination via Deep Reinforcement Learning 논문을 읽고 정리한 내용입니다. 최근 ICRA 2023 런던에서 5월 30일부터 6월 1일까지 진행된 Autonomous Quadruped Robot Challenge (QRC)에서 KAIST 연구팀이 1등을 하여 큰 이슈가 되었었습니다. 이번 포스팅에서 리뷰하는 이 논문이 바로 대회에서 사용되었던 강화학습 기반의 보행제어 알고리즘에 대한 내용을 담고 있는 논문입니다."
  },
  {
    "objectID": "posts/paper/2023-07-02-dreamwaq.html#key-contribution",
    "href": "posts/paper/2023-07-02-dreamwaq.html#key-contribution",
    "title": "📃DreamWaQ 리뷰",
    "section": "2.1 Key Contribution",
    "text": "2.1 Key Contribution\n\n\n\nOverview of DreamWaQ\n\n\nDreamWaQ 알고리즘의 전체적인 흐름은 위의 사진과 같습니다. “Dream”이라는 워딩과 알고리즘 개괄도에서 생각 풍선 모양 표현에서 볼 수 있듯이 DreamWaQ 논문의 주요 Contribution으로는 Implicit Terrain Imagination을 할 수 있도록 Context-Aided Estimator Network(CENet)을 도입하였고 안정적으로 Policy가 학습될 수 있도록 Adaptive Bootstrapping(AdaBoot)방법을 제안하여 강화학습 보행 제어기를 설계한 점을 들 수 있습니다."
  },
  {
    "objectID": "posts/paper/2023-07-02-dreamwaq.html#implicit-terrain-imagination",
    "href": "posts/paper/2023-07-02-dreamwaq.html#implicit-terrain-imagination",
    "title": "📃DreamWaQ 리뷰",
    "section": "2.2 Implicit Terrain Imagination",
    "text": "2.2 Implicit Terrain Imagination\n앞서 챌린지에서 사용된 환경에서 볼 수 있듯이 사족보행로봇은 다양한 지형(Terrain)을 극복하며 보행할 수 있는 능력이 중요합니다. 그럼 다양한 지형을 나타낼 수 있는 속성들에는 무엇이 있을까요? 지형의 마찰계수, 반발계수, 놓여져 있는 장애물, 울퉁불퉁한 정도 등등 여러가지 속성들로 지형의 특징을 나타낼 수 있을 것입니다. 그리고 그러한 특징을 어떻게 4개의 다리를 이용하여 보행의 어려운 점들을 극복하며 원하는 방향으로 이동할 수 있도록 하는 것이 관건인 것입니다.\n이런 지형의 특징을 파악하기 위해 많은 연구들이 카메라나 라이다와 같은 비젼센서를 부착하여 환경을 인식한 뒤 극복하기 위한 방법을 고안하는 방향으로 연구되고 있습니다. 하지만 KAIST 연구진이 제안한 DreamWaQ에서는 지형을 인식할 수 있는 부차적인 비젼센서 없이 로봇의 자체의 정보(proprioception)를 이용하여 지형을 극복하기 위해 explicit한 환경 정보가 아닌, implicit한 terrain imagination을 할 수 있는 방법론은 제시했습니다.\n사실 Implicit하게 로봇 주변의 지형이나 환경정보를 강화학습 로봇 에이전트가 인식할 수 있도록 하는 연구는 다양하게 진행되어왔었습니다. 앞선 주요 방법은로는 Teacher-Student Network를 이용하여 모든 환경정보를 학습한 Teacher Network로부터 Student Network가 추후에 따라 학습하는 방식이 있었지만, 해당 방법은 Teacher Network를 학습과 Student Network 학습을 따로 2개의 단계를 거쳐 학습을 해야한다는 데이터 비효율적인 학습 방법이라는 단점이 있었습니다. 따라서 DreamWaQ에서는 Asymmetric Actor-Critic이라는 기존의 Actor-Critic 강화학습 알고리즘에 약간 변형을 준 모델을 사용하여 Teacher-Student Network처럼 두 단계로 나누어서 학습하지 않고도 Implicit하게 Terrain 정보를 Actor-Critic 구조에 녹여들 수 있도록 했습니다."
  },
  {
    "objectID": "posts/paper/2023-07-02-dreamwaq.html#asymmetric-actor-critic",
    "href": "posts/paper/2023-07-02-dreamwaq.html#asymmetric-actor-critic",
    "title": "📃DreamWaQ 리뷰",
    "section": "2.3 Asymmetric Actor-Critic",
    "text": "2.3 Asymmetric Actor-Critic\n기존의 PPO, SAC와 같은 Policy Gradient의 강화학습 알고리즘들의 주요 구성요소로 Actor Network와 Critic(Value) Network가 있습니다. Actor는 강화학습 에이전트가 취해야하는 action 값을 출력하는 네트워크이며 Critic는 에이전트의 학습 방향을 보여주는 value값을 출력하여 이 2개의 네트워크들이 Policy Gradient 알고리즘의 목적식을 따라 Return(누적 보상)값을 최대화하는 방향으로 학습하게 되는 것입니다. 보통 2개의 네트워크 모두에게 같은 state(혹은 observation) 정보가 입력값으로 들어가게 되기 때문에 Actor 네트워크와 Critic 네트워크는 서로 Symmetric하다고 할 수 있습니다.\n하지만 앞서 로봇이 센서 없이는 얻을 수 없는 지형 정보가 강화학습 알고리즘에 사용되는 네트워크의 인풋으로 들어간다면 실제 로봇에서 알고리즘이 돌아갈 때 넣어줄 지형정보가 없기 때문에 제어 알고리즘이 돌아갈 수 없을 것 입니다. 그래서 DreamWaQ에서는 Actor/Critic Network의 상호작용 과정에서 강화학습 에이전트가 얻을 수 있는 시간적 정보들을 기반으로 terrain 정보를 상상할 수 있도록, Actor 네트워크에 들어가는 입력값과 Critic 네트워크에 들어가는 입력값을 다르게 설계하였고 이를 Asymmetric한 구조라고 볼 수 있습니다.\n\n\n\nAsymmetric Actor-Critic\n\n\n위에 보이시는 것처럼 Actor Network에는 Observation o_t, estimated velocity v_t, latent vector z_t가 입력으로 들어가게 됩니다. v_t와 z_t는 다음 파트에서 좀 더 살펴볼 예정이므로 여기에서는 우선 observation vecter인 o_t에 초점을 맞추어서 보겠습니다. observation 정보는 강화학습 MDP를 정의하는 한 요소로 강화학습 에이전트가 학습할 때 관측(혹은 접근 가능한 정보)하는 정보입니다. 따라서 로봇에 특별한 비젼 센서 추가 없이 로봇 자체 하드웨어에서 얻을 수 있는 정보인 proprioceptive 정보를 기반으로 몸체의 각속도 \\omega_t, 중력방향 벡터 g_t 등등의 정보가 observation vector의 요소로 들어가게 됩니다. 반면, Critic Network에는 State s_t가 입력값으로 들어가는 것을 알 수 있는데 이는 위에서 Observation과 State를 비교해놓은 것과 같이 state가 observation보다 많은 정보를 포함한 것을 알 수 있습니다. 여기에서 주목해서 볼 수 있는 점이 바로 지형에 대한 정보인 heightmap scan h_t가 한 요소임을 알 수 있고 이를 통해 implicit한 terrain imagination이 가능한 것 입니다. Heightmap scan에 대해 조금 더 설명을 덧붙이자면, 지형의 heightmap scan 정보는 실제 로봇에서 얻을 수 있는 정보는 아니고 강화학습 에이전트가 학습하게 되는 시뮬레이션에서만 얻을 수 있는 정보로 지형의 z축 방향의 높이 정보를 말합니다.\n환경을 정의하는 변수이고 시뮬레이션에서는 가상공간이기 때문에 프로그램에서 얻을 수 있는 물리적 정보이지만 실제로 로봇이 이용할 수 없는 정보를 privileged observation이라고 부르기도 합니다. 따라서 기존에 강화학습에서 State가 환경에서 에이전트가 놓여있는 상황을 설명할 수 있는 모든 정보를 말하고 Observation이 환경에 놓여있는 에이전트가 관찰할 수 있는 일부 상태 정보를 뜻하기 때문에 State = Observation + Privileged Observation 포함관계로 이해할 수 있습니다.(논문에서는 privileged observation이라는 표기를 state를 뜻하는 것으로 표기하고 있기 때문에 헷갈릴 수 있습니다.)"
  },
  {
    "objectID": "posts/paper/2023-07-02-dreamwaq.html#context-aided-estimator-network",
    "href": "posts/paper/2023-07-02-dreamwaq.html#context-aided-estimator-network",
    "title": "📃DreamWaQ 리뷰",
    "section": "2.4 Context-Aided Estimator Network",
    "text": "2.4 Context-Aided Estimator Network\n이번 파트에서 살펴보게 될 Context-Aided Estimator Network는 센서로 인식할 수 없는 지형 정보를 에이전트가 유추할 수 있도록 하는 일등공신 아이디어 입니다.\n\n\n\nThe architecture of CENet\n\n\nCENet의 구조는 위와 같이 \\beta-VAE구조를 활용하여 구성되어 있습니다. 일정 time horizon H동안 모은 observation이 Encoder에 들어가면 latent vector z와 몸체의 선속도 추정값인 v_t가 출력값으로 나오게 됩니다. Auto-Encoder의 일반적인 구조를 따라 이 값들이 Decoder의 인풋으로 들어가고 Decoder의 출력값으로는 time horizon을 지난 다음 observation vector o_{t+1}을 reconstruction할 수 있도록 학습하게 되는 것 입니다.\n\n\n\nThe loss of CENet\n\n\n그래서 CENet의 loss function은 크게 2개의 파트 L_{est}와 L_{VAE}로 나누어져 있는 것을 확인할 수 있습니다. 먼저 L_{est}는 보행하는 로봇 에이전트의 속도 추정을 CENet에서 할 수 있도록 학습하기 위한 부분으로, 로봇 몸체의 선속도 추정값 \\tilde{v}_t는 실제 정답값 v_t는 시뮬레이션에서는 얻을 수 있는 값이기 때문에 Encoder에서 추정한 값 \\tilde{v}_t와의 MSE(mean square error)를 구할 수 있습니다. 다음으로 L_{VAE}는 time horizon H동안 누적되 여러개의 observation 정보를 가지고 다음 observation o_{t+1}을 오토인코더 구조로 잘 reconstruction한지를 보는 첫번째 term과 추정 분포를 맞추는 부분인 KL-divergence 제약 조건 두번째 term으로 이루어져 있습니다. (VAE loss에 대해서 더 자세한 정보를 알고 싶으신 분은 이전에 VAE 논문을 리뷰한 포스팅을 참고해주세요.)\n이와 같은 loss 구성으로 학습된 CENet은 여러 타임 스텝동안 관찰된 observation 정보들을 기반으로 에이전트가 privileged observation을 유추할 것으로 기대할 수 있는 이유는 privileged observation을 기반으로 가치를 추정하는 Critic(Value) Network를 통해서 Actor Network가 업데이트 되는 Policy gradient과정을 거치기 때문입니다. 이러한 Asymmetric Actor-Critic구조와의 시너지 효과가 기존의 Context RL 분야에서도 사용되는 아이디어 인데(참조논문: AACC) 이와 비교해보았을 때, Critic Network가 deploy되는 과정에서 쓰이지 않기 때문에 Actor보다 더 많은 정보를 받아서 더 정확한 가치를 추정할 수 있게 한다는 기조는 비슷하지만 time-invarient한 context vector를 만드는 Context RL에서의 Asymmetric Actor-Critic과 다르게 DreamWaQ에서는 time-varient한 변수들을 추정하여 implicit하게 추정할 수 있도록 했다는 점이 다릅니다.\nAdaptive Bootstrapping(AdaBoot)\nAdaptive bootstrapping은 policy 학습과정 중에 Estimator network인 CENet이 안정적으로 학습되도록 하기 위해 domain randomized로 다양화된 여러 환경요소에 대해 에피소드별 reward의 평균값에 대한 표준 편차의 비율인 변동 계수(CV)에 의해 제어되는 방법을 말합니다. 핵심 아이디어는 부정확한 가치 추정에 대한 정책을 보다 견고하게 만들기 위해 m개의 에이전트 reward의 CV가 작을 때 부트스트래핑을 하게됩니다. 반대로 에이전트가 충분히 학습하지 않은 경우에는 reward에서 큰 CV로 표시된 것처럼 부트스트랩을 해서는 안하도록 합니다.\n\n\n\nAdaptive Bootstrapping Probability"
  },
  {
    "objectID": "posts/paper/2023-07-02-dreamwaq.html#simultation-result",
    "href": "posts/paper/2023-07-02-dreamwaq.html#simultation-result",
    "title": "📃DreamWaQ 리뷰",
    "section": "3.1 Simultation Result",
    "text": "3.1 Simultation Result\n\n\n\nThe loss of CENet\n\n\nIsaac Gym 시뮬레이터를 이용하여 PPO 강화학습 알고리즘 이용하여 학습과정 동안의 Episodic Reward 그래프 변화를 살펴보면, EstimatorNet은 처음에는 AdaptationNet보다 평균 에피소드 보상이 높지만, 더 많은 training step 후에 더 어려운 지형과 마주치기 때문에 더 많은 반복 후에 성능이 저하됨을 알 수 있습니다. 반대로 DreamWaQ는 학습 지형이 점점 어려워 짐에도 다른 모든 방법들을 능가하는 퍼포먼스를 보여줍니다. 외부 인식 없이 걷는 것임에도 DreamWaQ는 주변 지형의 heightmap을 다 알 수 있는 오라클 policy만큼 성능이 좋은 것을 볼 수 있습니다.\nExplicit Estimation Comparison\n시뮬레이션에서 한번 지형정보를 Implicit가 아닌 Explicit하게 알려주고 학습한다면 어떤 유의미한 차이가 있는지 알아보는 실험도 진행했습니다.\n\n\n\nAdaptive Bootstrapping Probability\n\n\nTimestep이 늘어날 수록 더 어려운 계단지형에서 보행하도록 학습시킨 결과 Explicit하게 지형정보를 학습한 Estimator는 지형이 어려워지자 Foot stumble 현상이 심하게 있었지만 DreamWaQ는 지형이 어려워져도 작은 foot stumble이 있음을 확인하여 오히려 Implicit하게 지형정보를 학습하는 것인 robust한 보행을 하는 것을 확인할 수 있었다고 합니다."
  },
  {
    "objectID": "posts/paper/2023-07-02-dreamwaq.html#real-world-result",
    "href": "posts/paper/2023-07-02-dreamwaq.html#real-world-result",
    "title": "📃DreamWaQ 리뷰",
    "section": "3.2 Real-world Result",
    "text": "3.2 Real-world Result\n\n\n\n실제 로봇 플랫폼을 가지고 Command tracking error를 plot 해보았을 때도 다른 비교 모델들과 비교해보았을 때 error 값이 적은 것을 확인할 수 있습니다. 특히나 AdaBoot방법이 있고 없고에 따라 error값의 크기가 다른 것을 통해 AdaBoot 방법이 policy 학습에 필요한 것을 확인할 수 있습니다."
  },
  {
    "objectID": "posts/paper/2022-09-17-wavenet.html",
    "href": "posts/paper/2022-09-17-wavenet.html",
    "title": "📃WaveNet 리뷰",
    "section": "",
    "text": "이번 포스팅은 Google DeepMind에서 발표한 WaveNet이라는 논문에 대해 리뷰를 하려고 합니다. WaveNet은 Autoregressive한 Generative model로써 Google의 스피커 서비스에 사용되었다고 많이 알려진 모델입니다."
  },
  {
    "objectID": "posts/paper/2022-09-17-wavenet.html#dilated-casual-convolution",
    "href": "posts/paper/2022-09-17-wavenet.html#dilated-casual-convolution",
    "title": "📃WaveNet 리뷰",
    "section": "1. Dilated Casual Convolution",
    "text": "1. Dilated Casual Convolution\n먼저 Dilated Casual Convolution은 µ-law Companding Transformation 처리를 거친 음성 신호를 받아오는 첫번째 부분입니다.\n\n\n\nCasual Convolution 2\n\n\n우선 Casual 이라는 것은 Time-series인 음성 신호의 시간 순서를 고려하여 현재 시점 t를 기준으로 미래 정보는 사용할 수 없고 현재까지의(과거~현재 t) 정보만 사용할 수 있다는 의미입니다. 왼쪽 Causal Convolution 그림에서 Receptive Field는 (레이어 수) + (필터의 length) -1로 계산되어 총 레이어 수는 4개이고 필터 length는 이전 레이어에서 2개의 정보가 모아져서 다음 레이어의 하나의 데이터로 산출되므로 필터 length는 2라고 볼 수 있습니다. 따라서 4+2-1로 Receptive Field는 5가 되며 이를 그림에서 살펴보면 처음 input에서 5개의 음성 정보가 output의 1개의 정보로 나오는 것을 볼 수 있습니다. 이런 Receptive Field는 매우 짧은 시간에 많은 음성신호가 매칭되는 상황에서 매우 좁으며 RF를 늘리기 위해서는 레이어 수를 늘리거나 필터의 length를 늘려야 하는데 이는 모델을 매우 크게 만들게 되고 계산도 많이 요구됩니다.\n\n\n\nDilated Casual Convolution 2\n\n\n그래서 제안이 된 방법이 바로 Dilated Convolution입니다. 이는 convolution with holes로 해석할 수 있는데 위의 그림에서 볼 수 있듯이 이전 레이어에서 데이터가 Dilated되어 데이터가 듬성듬성하게 모아져서 다음 레이어로 넘어가는 것을 볼 수 있습니다. 이는 skip이나 pooling과 유사해보이지만 input과 output의 차원이 유지된다는 점에서 차이가 있습니다. 이때의 RF는 각 레이어의 Dilation 값을 모두 더하고 마지막에 현재 시점의 데이터 1을 더하며 RF가 계산됩니다. WaveNet에서는 Dilation을 총 30개의 레이어에 적용했고 Dilation 값의 패턴은 input에서 부터 1, 2, …, 512 로 2배씩 늘린 10개의 레이어를 총 3번 반복했습니다. 이때, 1 ~ 512 Dilation 값을 가진 10개 레이어의 RF는 1024로 계산됩니다.\n\n\n\nDilated Casual Convolution Process2\n\n\n\n\n\nDilated Convolution Pattern6\n\n\nCode 구현으로 살펴보면 아래와 같이 구현할 수 있습니다. Casual 특성을 반영하기 위해 self.ignoreOutIndex 을 만들어서 dilation 값을 고려하여 (kernel_size - 1) * dilation으로 계산한 후에 잘라내주는 것을 확인할 수 있습니다.\nclass CasualDilatedConv1D(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, dilation, padding=1):\n        super().__init__()\n        self.conv1D = nn.Conv1d(in_channels, out_channels, kernel_size, dilation=dilation, bias=False, padding='same')\n        self.ignoreOutIndex = (kernel_size - 1) * dilation # casual\n\n    def forward(self, x):\n        return self.conv1D(x)[..., :-self.ignoreOutIndex] # casual"
  },
  {
    "objectID": "posts/paper/2022-09-17-wavenet.html#residual-connection-gated-activation-units",
    "href": "posts/paper/2022-09-17-wavenet.html#residual-connection-gated-activation-units",
    "title": "📃WaveNet 리뷰",
    "section": "2. Residual Connection & Gated Activation Units",
    "text": "2. Residual Connection & Gated Activation Units\n다음으로 Dilated Causal Convolution을 거친 후 통과하게 되는 Residual Connection & Gated Activation Units 부분에 대해서 살펴보겠습니다.\n\n\n\nWaveNet에서 사용된 Gated Activation Units는 PixelCNN에서 사용된 매커니즘을 차용했습니다. 아래의 그림에서 보이는 보라색 Dilated Conv가 앞에서 설명한 DCC이며 이를 거친 후 Convoltion layer와 각각 tanh, sigmoid activation을 통과하여 Filter, Gate가 됩니다. 이 2가지 경로로 계산된 값은 elementwise product를 통해 하나의 벡터로 변환됩니다. 이떄 Dilated를 통과하기 전 값을 Residual Connection을 통해 연결함으로써 딥러닝 모델이 레이어를 더 깊게 쌓을 수 있도록 돕고 더 빠르게 학습할 수 있도록 할 수 있었다고 합니다.\n\n\n\nResidual Connection & Gated Activation Units6"
  },
  {
    "objectID": "posts/paper/2022-09-17-wavenet.html#skip-connection",
    "href": "posts/paper/2022-09-17-wavenet.html#skip-connection",
    "title": "📃WaveNet 리뷰",
    "section": "3. Skip Connection",
    "text": "3. Skip Connection\n\n\n\nSkip Connection은 Dilated Convolution을 통해 다양한 Receptive Field를 가진 각 레이어들의 값을 활용하여 output을 만들어낼 수 있도록 했습니다. 앞서 설명했던 대로 각 Residual Block의 Dilation 값이 다 다르기 때문에 각 Residual Block의 output은 서로 다른 Receptive Field를 가지게 됩니다.\n\n\n\nSkip Connection6\n\n\nResidual Connection과 Skip Connection을 Code로 구현하면 다음과 같습니다. 위에서 설명했던 Gated Activation Units의 tanh, sigmoid activation을 각각의 activation function을 거친후 self.resConv1D을 통과하는 것을 확인할 수 있습니다. 또한 Skip Connection을 구현하는 부분은 self.skipConv1D에서 확인할 수 있습니다. 마지막 return에서 resOutput, skipOutput으로 2개의 output이 나오는 것을 알 수 있습니다.\nclass ResBlock(nn.Module):\n    def __init__(self, res_channels, skip_channels, kernel_size, dilation):\n        super().__init__()\n        self.casualDilatedConv1D = CasualDilatedConv1D(res_channels, res_channels, kernel_size, dilation=dilation)\n        self.resConv1D = nn.Conv1d(res_channels, res_channels, kernel_size=1)\n        self.skipConv1D = nn.Conv1d(res_channels, skip_channels, kernel_size=1)\n        self.tanh = nn.Tanh()\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, inputX, skipSize):\n        x = self.casualDilatedConv1D(inputX)\n        x1 = self.tanh(x)\n        x2 = self.sigmoid(x)\n        x = x1 * x2\n        resOutput = self.resConv1D(x)\n        resOutput = resOutput + inputX[..., -resOutput.size(2):]\n        skipOutput = self.skipConv1D(x)\n        skipOutput = skipOutput[..., -skipSize:]\n        return resOutput, skipOutput\n위와 같은 ResBlock은 전체 구조에서 보시다시피 여러개가 stacked 되어 있으므로 StackOfResBlocks class로 구현하여 WaveNet에 넣어주게 됩니다.\nclass StackOfResBlocks(nn.Module):\n\n    def __init__(self, stack_size, layer_size, res_channels, skip_channels, kernel_size):\n        super().__init__()\n        buildDilationFunc = np.vectorize(self.buildDilation)\n        dilations = buildDilationFunc(stack_size, layer_size)\n        self.resBlocks = []\n        for s,dilationPerStack in enumerate(dilations):\n            for l,dilation in enumerate(dilationPerStack):\n                resBlock=ResBlock(res_channels, skip_channels, kernel_size, dilation)\n                self.add_module(f'resBlock_{s}_{l}', resBlock) # Add modules manually\n                self.resBlocks.append(resBlock)\n\n    def buildDilation(self, stack_size, layer_size):\n        # stack1=[1,2,4,8,16,...512]\n        dilationsForAllStacks = []\n        for stack in range(stack_size):\n            dilations = []\n            for layer in range(layer_size):\n                dilations.append(2 ** layer)\n            dilationsForAllStacks.append(dilations)\n        return dilationsForAllStacks\n\n    def forward(self, x, skipSize):\n        resOutput = x\n        skipOutputs = []\n        for resBlock in self.resBlocks:\n            resOutput, skipOutput = resBlock(resOutput, skipSize)\n            skipOutputs.append(skipOutput)\n        return resOutput, torch.stack(skipOutputs)"
  },
  {
    "objectID": "posts/paper/2022-09-17-wavenet.html#conditional-wavenets",
    "href": "posts/paper/2022-09-17-wavenet.html#conditional-wavenets",
    "title": "📃WaveNet 리뷰",
    "section": "4. Conditional WaveNets",
    "text": "4. Conditional WaveNets\n\n\n\n\n\n\nConditional modeling 6\n\n\nConditional Modeling은 Autoregressive model인 WaveNet에 적용하기 쉽고 이 또한 PixelCNN에서의 아이디어와 유사합니다. Feature h 벡터를 조건 부분에 추가하여 음성 데이터에 조건을 추가할 수 있습니다.\n\np(\\mathbf{x} \\mid \\mathbf{h})=\\prod_{t=1}^T p\\left(x_t \\mid x_1, \\ldots, x_{t-1}, \\mathbf{h}\\right)\n\nCondition에는 크게 2가지로 Global과 Local이 있습니다. 먼저 Global은 Time-invariant한 조건으로 시점에 따라 변하지 않는 조건 정보를 추가하는 것을 말합니다. 예를 들어 한 발화자의 음성은 해당 음성 파일의 어떤 시점에서나 똑같은 condition이기 때문에 Global condition이라고 할 수 있습니다. 이때의 Feature vector h는 linear projection을 거친 후 data x와 더하게 됩니다.\n\n\n\n다음으로 Time-variant한 Local condition은 시점에 따라 변하는 조건 정보를 추가하는 것을 말하는데 음성 데이터보다 길이가 짧지만 순서가 있는 일정 길이의 Sequence vector라고 생각할 수 있습니다. 같은 발화자여도 어떤 단어를 말하느냐에 따라 음성학적인 특징(linguistic feature)가 다를 수 있기 떄문에 local한 조건은 한 음성 파일에 여러개가 있을 수 있습니다. 이때 Feature vector h는 음성 파일과 길이가 다르기 때문에 Upsampling을 거친후 1x1 convolution을 거쳐서 data x와 더해집니다."
  },
  {
    "objectID": "posts/paper/2022-08-07-gn-block.html",
    "href": "posts/paper/2022-08-07-gn-block.html",
    "title": "📃GN-Block 리뷰",
    "section": "",
    "text": "이번 post는 Graph Networks as Learnable Physics Engines for Inference and Control 라는 논문을 읽고 리뷰한 내용입니다."
  },
  {
    "objectID": "posts/paper/2022-08-07-gn-block.html#graph-representation-of-a-physical-system",
    "href": "posts/paper/2022-08-07-gn-block.html#graph-representation-of-a-physical-system",
    "title": "📃GN-Block 리뷰",
    "section": "Graph representation of a physical system",
    "text": "Graph representation of a physical system\n물리시스템을 어떻게 그래프로 나타낼 수 있는지 몇가지 용어와 수식들을 정리해보겠습니다.\n\n물리 시스템의 body는 그래프의 node로 표현합니다.\n물리 시스템의 joint는 그래프의 edge로 표현합니다.\n물리 시스템의 global한 속성은 global feature로 표현합니다.\n\n아래 사진에서 보이는 half-cheetah에서 직관적으로 어떻게 그래프가 그려질 수 있는지 알 수 있고 이 그래프를 G로 나타낼 수 있습니다.\n\n\n\n\n\n앞서 설명한 부분을 수식으로 나타내면 다음과 같습니다.\n\nG=\\left(\\mathbf{g},\\left\\{\\mathbf{n}_{i}\\right\\}_{i=1 \\cdots N_{n}},\\left\\{\\mathbf{e}_{j}, s_{j}, r_{j}\\right\\}_{j=1 \\cdots N_{e}}\\right)\n\n\ng : global features 시스템의 중력이나 time step과 같은 속성을 나타내는 벡터입니다.\n\\mathbf{n}_{i} : node features를 나타내는 벡터입니다.\n\\mathbf{e}_{j} : edge features를 나타내는 벡터입니다.\ns_{j} : 이 edge를 통해서 message를 보내는 sender nodes의 인덱스입니다.\nr_{j} : 이 edge를 통해서 message를 받는 receiver nodes의 인덱스입니다."
  },
  {
    "objectID": "posts/paper/2022-08-07-gn-block.html#static-dynamic-properties",
    "href": "posts/paper/2022-08-07-gn-block.html#static-dynamic-properties",
    "title": "📃GN-Block 리뷰",
    "section": "Static & Dynamic properties",
    "text": "Static & Dynamic properties\n여기서 static graph G_s와 dynamic graph G_d 라는 그래프는 2가지 종류가 있습니다. 이 2개의 그래프는 각각 시스템의 속성이 시간에 따라 변화하는지(dynamic/time-variant) 안하는지(static/time-invaritant)에 따라 그래프를 구성하는 정보의 종류가 다릅니다.(자세한 정보는 Appendix G section에서 Mujoco 기반의 어떤 정보로 각 그래프를 구성했는지 나와있습니다.)\n\nA static graph G_s: 시스템의 static한 정보를 가지고 있는 그래프\n\nglobal parameters: the time step, viscosity, gravity, etc\nbody/node parameters: mass, inertia tensor, etc.\njoint/edge parameters: joint type과 properties, motor type and properties, etc\n\nA dynamic graph G_d: 시스템의 일시적인 state 정보를 가지고 있는 그래프\n\nbody/node: 3D Cartesian position, 4D quaternion orientation, 3D linear velocity, 3D angular velocity\njoint/edge: joint에 적용된 action들의 크기"
  },
  {
    "objectID": "posts/paper/2022-08-07-gn-block.html#graph-networks",
    "href": "posts/paper/2022-08-07-gn-block.html#graph-networks",
    "title": "📃GN-Block 리뷰",
    "section": "Graph networks",
    "text": "Graph networks\n\ngraph2graph 모듈을 활용하여 인풋을 그래프로 받고 아웃풋도 그래프로 받는 모델입니다. 따라서 아웃풋의 그래프는 인풋 그래프와 다른 edge, node, global features를 가지게 됩니다.\n\n본 논문의 핵심 아이디어인 GN 블록의 구조에 대해 알아보겠습니다. - A core GN block\n\n\n\n\n\n- 3개의 sub function, MLP로 이루어져 있습니다.\n    - edge-wise $f_e$ : 모든 edge들에 대한 update를 진행합니다.\n    - node-wise $f_n$ : 모든 node들에 대한 update를 진행합니다.\n    - global $f_g$ : 마지막으로 global feature들을 update 합니다.\n하나의 feedforward GN pass는 그래프 상에서 message-passing 단계의 한 스텝으로 간주할 수 있습니다. 이러한 GN-block 내에서의 알고리즘은 아래와 같습니다."
  },
  {
    "objectID": "posts/paper/2022-08-07-gn-block.html#forward-models",
    "href": "posts/paper/2022-08-07-gn-block.html#forward-models",
    "title": "📃GN-Block 리뷰",
    "section": "Forward models",
    "text": "Forward models\nForward model의 목적은 현재 정보를 기반으로 다음 step의 상태를 예측(prediction)하는 것입니다. (이는 영어 단어의 비슷한 의미때문에 다음에 나오는 inference model의 목적과 많이 혼동될 수 있으니 잘 정의하고 넘어가는 것이 좋습니다.) forward model은 RNN(GRU)를 도입했는지 여부에 따라 2가지 타입이 있습니다.\nType1. GNN feed-forward\n\n\n\n\n\n가장 간단한 GNN feed-forward 모델입니다. 그래프는 처음에 GN_1을 거쳐 latent graph인 G'이 됩니다. 그리고 다음 GN_2의 인풋으로는 GN_1을 거치긴 전의 그래프였던 G와 G'를 concatenate를 해서 넣어주게 됩니다. 저자들은 이렇게 디자인한 이유로, 그래프의 모든 노드들과 엣지들이 모두 communicate하게 하기 위함이라고 이야기합니다. 이렇게 GN_1, GN_2를 거쳐 최종적으로 나오는 G^*의 node feature들이 각 body의 상태 prediction 값이 되는 것 입니다.\nType2. RNN+GNN\n\n\n\n\n\n다음으로 앞서 기본이 되는 모델에 G-GRU를 추가한 타입니다. Type 1과 비슷하게 skip connection, latent graph를 모두 사용하는데 GN block의 GRU 버젼인 G-GRU가 들어가면서 G_h라는 RNN에서 hidden vector와 같은 개념의 hidden graph가 추가된 것입니다. 모든 edge, node, global feature들에 대해 각각 RNN이 적용되어 총 3개의 RNN sub-modules이 있습니다.\n두가지 타입의 GNN forward 모델에 공통적인 사항\n\nstate differences를 예측하는 것을 학습해서 state prediction의 절댓값(absolute)을 계산합니다. 이 계산된 absolute state prediction을 가지고 state를 update하게 되는 것입니다.\nlong-range rollout trajectory를 만들어내기 위해서 state prediction 값과 control input을 반복적으로 model에 넣어주어서 여러 스텝의 trajectory를 생성하게 됩니다.\nGN model의 인풋과 아웃풋들은 normalize 됩니다.\n\n사실 리뷰를 하면서 forward model과 inference model 사이의 구분이나 모델의 구체적인 프로세스 이해가 pseudo algorithm을 보기 전까지 잘되지 않았습니다. Appendix에 나와있어서 잘 보지 않을 확률이 높지만 논문의 개념을 대략적으로 이해하고 난 후에는 꼭 line by line으로 보시길 추천합니다.\n먼저 forward model의 학습과정을 보여주는 pseudo algorithm 입니다. 다시한번 이 모델의 목적을 상기시켜보자면, 현재 상태 x^{t_0} 를 기반으로 a^{t_0}와 함께 주어졌을 때, x^{t_0+1}을 예측하는 것입니다. 앞서 설명한 부분들인, state의 잔차를 학습하는 부분이나 normalization 등이 알고리즘내에 잘 나와있습니다.\n\n\n\n\n\n다음은 학습된 forward model을 가지고 다음 상태인 x^{t_0+1}을 어떻게 예측하는지 보여주는 알고리즘입니다.\n\n\n\n\n\n마지막으로 바로 위 알고리즘과 동일하게 학습된 forward model을 가지고 다음 상태인 x^{t_0+1}을 어떻게 예측하는지 보여주는 알고리즘이지만 inference model에서 학습된 GN_p를 가지고 system identification이 추가된 상태에서 어떻게 알고리즘이 흘러가는지 보여줍니다.(이전에 알고리즘에서는 system parameter p라고 표시되었던 부분이 대체된 것입니다.)"
  },
  {
    "objectID": "posts/paper/2022-08-07-gn-block.html#inference-models",
    "href": "posts/paper/2022-08-07-gn-block.html#inference-models",
    "title": "📃GN-Block 리뷰",
    "section": "Inference Models",
    "text": "Inference Models\nInference model의 목적을 한 마디로 표현하자면 System identification이라고 할 수 있습니다. System identification이란 관찰할 수 없는(unobserved) dynamic system의 속성들을 관찰되는(observed) behavior(또는 어떤 양상)를 가지고 추론하는 것을 말합니다. 즉 암시적으로 system을 구성하는 요소들을 (명시적이지 않아) 측정하거나 관찰할 수 없지만 latent representations을 통해 추론할 수 있습니다.\n\n\n\n\n\nInference model도 Recurrent GN-based model 입니다. forward 모델과 다른 점으로는 오직 trajectory의 dynamic states들만 input으로 받습니다. 따라서 dynamic state graph인 G_d와 control input을 받습니다. 아웃풋으로는 일정 time step T이후의 G^*(T)이 되며, 본 논문에서 이후 실험파트에서 20 step을 사용했습니다.\ninference model 학습과정의 pseudo 알고리즘은 아래와 같습니다."
  },
  {
    "objectID": "posts/paper/2022-08-07-gn-block.html#control-algorithm",
    "href": "posts/paper/2022-08-07-gn-block.html#control-algorithm",
    "title": "📃GN-Block 리뷰",
    "section": "Control algorithm",
    "text": "Control algorithm\ncontrol algorithm에서는 그래프 기반이 아니고 앞서 설명한 그래프 기반의 forward model과 inference model을 잘 활용해서 어떻게 control할 수 있을지를 보여줍니다. 본 논문에서는 크게 2가지 control algorithm을 사용했습니다. 강화학습을 주로 연구하는 입장에서 리뷰해보면, 대부분 강화학습은 model-free 기반의 알고리즘이 많이 발전했는데 GN기반의 다음 상태를 예측할 수 있는 model을 만듦으로써 model-based 기반의 강화학습 알고리즘을 적용할 수 있다는 것이 매우 흥미로웠습니다.\n\nMPC(Model Predictive Control)\nGN은 미분 가능하기 때문에 MPC같은 gradient-based trajectory optimization 방법으로 model-based planning을 할 수 있습니다. 대표적으로 MPC가 있고 학습기반이 아니라 최적화 알고리즘이며 알고리즘의 흐름은 아래와 같습니다.\n\n\n\n\n\nSVG(Stochastic Value Gradients)\n강화학습 알고리즘 중 하나이며, GN-based model과 SVG의 policy function을 동시에 학습하는 agent로 control을 하는 방법입니다. SVG(1)은 한 스텝을 예측하는 GN model을 가지고 강화학습 알고리즘으로 control을 한 것이며(model-based) SVG(0)은 예측하는 GN model 없이 model-free 기반으로 control한 것으로 이해하시면 됩니다.\n\n\n사실 MPC와 SVG는 매우 비슷한 측면이 있습니다. MPC에서는 control inputs들이 한 에피소드에서 초기 조건들이 주어졌을 때 최적화 되는 것이라면, SVG에서는 state와 control을 매칭시키는 policy function이 학습과정에서 경험한 states에 대해서 최적화 되는 것입니다."
  },
  {
    "objectID": "posts/paper/2022-08-07-gn-block.html#prediction",
    "href": "posts/paper/2022-08-07-gn-block.html#prediction",
    "title": "📃GN-Block 리뷰",
    "section": "Prediction",
    "text": "Prediction\nLearning a forward model for a single system\n하나의 시스템을 가지고 학습한 forward model의 Prediction 성능 살펴보기\n\nrandom control로 만들어진 데이터들을 가지고 학습된 GN-based model\n\n[Visually] Swimmer6에서 그림에서 처럼 ground truth와 예측 결과가 구분이 안 갈 정도로 흡사하다.(영상에서도 거의 구분이 안 갈 정도로 잘 예측하고 있음을 알 수 있다.)\n\n\n\n\n\n[Quantitatively] 100 step에서 3축 방향으로의 위치, 선속도, 각속도, 쿼터니안 방향 비교\n\n\n\n\n\n\nconstant prediction baseline은 아웃풋으로 인풋을 그대로 복사해서 사용했기 때문에 애러 최대치로 normalization 하기 위해 검은색 점선으로 표기\n우선 검은 점선과 막대기들을 뭉뚱그려서 보면,\n1 step과 100 step의 rollout 결과를 비교했을 때 검은 점선에 비해 파란색 막대기들의 error 값이 낮음을 알 수 있다.\n\n\n\n\n\nGN 모델이 MLP-based 보다 더 낮은 애러를 가지는 것을 알 수 있다. 이는 특별히 Swimmer6처럼 에이전트의 구조가 반복적인 경우에 더욱 눈에 띄게 낮음을 알 수 있었다. 이를 통해 GN-based forward 모델이 다양한 물리 시스템들에서 dynamics를 잘 예측함을 알 수 있다.\n\n\n\n\n\n\n\nGN이 MLP보다 더 generalization이 잘 됨을 확인할 수 있었는데, Swimmer6를 집중적으로 train, valid, test 데이터에 대해 1-step, rollout error를 각각 확인해봤을 때, Best GN의 error 값이 Best MLP보다 낮음을 알 수 있다. 뿐만 아니라 test data의 error 증가율을 봤을 때에도 GN 모델의 test data의 error가 더 적게 증가함을 관찰할 수 있었고 이는 agent의 bodies와 joints들에 대한 inductive bias가 GN을 통해 잘 학습되었음을 증명할 수 있다.\n\n\n\n\n\n\nLearning a forward model for multiple systems\n한 개의 시스템에서의 forward model을 살펴보았으니 이제 여러 시스템에서의 forward model의 성능을 살펴보자. GN을 사용하면 여러 시스템들의 다양한 변수들도 잘 다룰 수 있다는 가정이 있었다. 이를 확인하기 위해 연속적으로 static parameter들(질량, body의 길이, joint의 각도 등)을 바꿔가면서 forward dynamics를 어떻게 학습해가는지 확인했다."
  },
  {
    "objectID": "posts/paper/2022-08-07-gn-block.html#inference",
    "href": "posts/paper/2022-08-07-gn-block.html#inference",
    "title": "📃GN-Block 리뷰",
    "section": "Inference",
    "text": "Inference"
  },
  {
    "objectID": "posts/paper/2022-08-07-gn-block.html#control",
    "href": "posts/paper/2022-08-07-gn-block.html#control",
    "title": "📃GN-Block 리뷰",
    "section": "Control",
    "text": "Control"
  },
  {
    "objectID": "posts/paper/2025-06-13-contact-rich-mpc.html",
    "href": "posts/paper/2025-06-13-contact-rich-mpc.html",
    "title": "📃Contact Trust Region 리뷰(feat.Dextreme)",
    "section": "",
    "text": "CTR vs DeXtreme: 능숙한 접촉 조작을 향한 두 갈래 길 모델 기반 접촉 계획(MPC-CTR)과 강화학습 기반 조작(DeXtreme)의 수학적 원리와 구조를 깊이 분석하고, 두 방법론을 다양한 관점에서 비교"
  },
  {
    "objectID": "posts/paper/2025-06-13-contact-rich-mpc.html#ctr-최적화-프레임워크",
    "href": "posts/paper/2025-06-13-contact-rich-mpc.html#ctr-최적화-프레임워크",
    "title": "📃Contact Trust Region 리뷰(feat.Dextreme)",
    "section": "CTR 최적화 프레임워크",
    "text": "CTR 최적화 프레임워크\n개요: 접촉 신뢰 영역(Contact Trust Region, CTR)은 기존의 타원형 신뢰영역(Ellipsoidal Trust Region, ETR)을 확장하여, 접촉 동역학의 물리 제약 조건을 명시적으로 포함하는 새로운 신뢰영역 모델입니다. 핵심 아이디어는 선형화 오차를 제어하는 작은 타원형 영역뿐 아니라, 접촉 가능성 제약 조건(일방향 접촉력, 마찰 원뿔 제약 등)도 함께 적용하여, 탐색 가능한 지역을 현실적인 물리 범위 내로 제한하는 것입니다.\n\n1. 미분 가능한 접촉 동역학 모델\nCTR은 미분 가능한 접촉 시뮬레이터를 활용합니다. 특히, 이전 연구인 Convex Quasi-Dynamic Contact (CQDC) 모델을 기반으로, 접촉 동역학을 볼록 최적화 문제(SOCP 등)로 표현합니다. 이 모델을 풀면 다음 상태뿐 아니라 접촉력까지 계산되며, 상태와 제어 입력에 대한 감도(Jacobian)도 함께 얻을 수 있습니다. 이는 접촉력을 쌍대변수(dual variable)로 간주한 KKT 조건 민감도 해석을 통해 가능해집니다.\n\n\n2. 상태 및 접촉력의 선형화\n미분 가능한 모델을 기반으로, 다음 상태 $+$와 접촉력 $+$는 다음과 같이 선형 근사됩니다:\n\n상태 업데이트: \\hat{q}_+ = A_\\kappa \\, \\delta q + B_\\kappa \\, \\delta u + f_\\kappa(\\bar{q}, \\bar{u})\n접촉력 응답: \\hat{\\lambda}_{+,i} = C_{\\kappa,i} \\, \\delta q + D_{\\kappa,i} \\, \\delta u + \\lambda_{\\kappa,i}(\\bar{q}, \\bar{u})\n\n이는 표준적인 상태 선형화와 달리, 접촉력 변화까지 함께 근사하므로, 접촉의 1차 응답을 정밀하게 반영할 수 있습니다.\n\n\n3. 접촉 가능성 제약(Contact Feasibility Constraints)\nCTR은 위 선형화 모델에 대해, 다음과 같은 물리 기반 제약을 적용합니다:\n\n비침투 조건 (Primal feasibility): \\hat{J}_i \\, \\hat{q}_+ + \\hat{c}_i \\in K_i → 접촉면에서의 상대 운동이 interpenetration을 유발하지 않도록 제한\n마찰 원뿔 조건 (Dual feasibility): \\hat{\\lambda}_{+,i} \\in K_i^* → 마찰 계수 및 일방향 접촉력 조건(정상 마찰력은 0 이상) 보장\n\n이러한 조건은 2차원 원뿔 제약(SOCP) 형태로 정식화되며, 신뢰 영역 내의 모든 후보해가 접촉 가능성 물리 법칙을 만족하도록 보장합니다.\n\n\n4. 접촉 신뢰 영역의 수학적 정의\nCTR은 다음의 조건을 만족하는 $(q, u)$의 집합으로 정의됩니다:\n\n타원형 제약: \\delta z^T \\Sigma \\delta z \\leq 1 \\quad (\\delta z = [\\delta q; \\delta u])\n선형화된 상태 및 접촉력 식 만족\n비침투 제약: $_+$가 접촉면을 침투하지 않음\n마찰 원뿔 제약: $_{+,i}$가 원뿔 내부에 위치함\n\nCTR은 이러한 제약들의 교집합이며, 이는 볼록 집합(convex set)입니다. 따라서 이후의 최적화 단계도 볼록 최적화 문제(SOCP)로 유지됩니다.\n\n\n5. 변형: A-CTR, R-CTR\n\nA-CTR (Action-only CTR): 상태는 고정하고 입력 $u$만을 탐색하는 경우. 계산량이 줄어 빠른 추론 가능\nR-CTR (Relaxed CTR): 비침투 조건을 제거하고 마찰 제약만 적용하여 보수성 완화 및 탐색 반경 확대\n\n실험 결과 R-CTR이 오히려 더 높은 성능을 보이는 경우가 있었으며, 이는 최적화가 덜 제한적인 방향으로도 유효한 접촉 조작을 계획할 수 있기 때문입니다."
  },
  {
    "objectID": "posts/paper/2025-06-13-contact-rich-mpc.html#ctr-기반-모델-예측-제어mpc-통합",
    "href": "posts/paper/2025-06-13-contact-rich-mpc.html#ctr-기반-모델-예측-제어mpc-통합",
    "title": "📃Contact Trust Region 리뷰(feat.Dextreme)",
    "section": "CTR 기반 모델 예측 제어(MPC) 통합",
    "text": "CTR 기반 모델 예측 제어(MPC) 통합\nCTR은 그 자체로는 하나의 제약 조건 집합이지만, 이를 실질적인 조작 제어기로 사용하려면 MPC(모델 예측 제어) 프레임워크 내에 통합해야 합니다. 본 섹션에서는 CTR이 어떻게 MPC에 통합되고, 접촉-풍부한 조작을 실시간으로 실행 가능한 최적화 문제로 변환하는지를 설명합니다.\n\n1. 접촉 암시적(contact-implicit) MPC\nCTR 논문에서는 접촉-암시적(contact-implicit) MPC 문제를 구성합니다. 즉, 접촉 모드 전이를 미리 명시하지 않고, 접촉 여부 및 접촉력의 발생을 최적화 과정에서 자동으로 결정합니다.\n\n각 시점에서 CQDC 기반 선형화를 통해 상태 및 접촉력에 대한 선형 모델을 생성\nCTR 제약(접촉 가능성, 마찰 등)을 적용한 SOCP 문제를 구성\n일정 시간 지평(horizon) 내에서 최적화한 후, 첫 번째 제어 입력만 적용하고 다시 반복 (Receding Horizon Planning)\n\nCTR의 구조 덕분에 이 MPC 문제는 전 구간에서 볼록 최적화(SOCP)로 유지됩니다.\n\n\n2. 반복 최적화 및 피드백\nCTR-MPC는 일반적인 MPC와 마찬가지로 매 타임스텝마다 새로운 상태를 관측하고, 선형화를 새로 수행한 후 최적화합니다. 이러한 반복 피드백 구조는 다음과 같은 이점을 제공합니다:\n\n모델링 오류나 외란에 대한 강건성 확보\n접촉 변화나 미세한 환경 조건 변화에 대한 실시간 적응\n\n\n\n3. 모드 전이 없이 접촉 처리\nCTR-MPC는 접촉 모드 전이(mode scheduling)를 명시적으로 기술할 필요가 없습니다. 다음의 수식 조건을 통해 접촉의 생성과 소멸을 자연스럽게 포함합니다:\n\n${+,i} K_i^*$ 조건은 ${+,i} = 0$ (접촉 없음)도 허용\n$i + + _i K_i$는 물체와 손가락이 떨어져 있을 때도 비침투 조건을 만족하도록 허용\n\n이러한 설계는 접촉 모드를 명시적으로 분기시키는 기존 방법들보다 훨씬 유연하고 계산 효율적입니다.\n\n\n4. 계산 효율성\nCTR-MPC의 각 최적화는 볼록 문제(SOCP)로 구성되며, 논문에서는 다음과 같은 실험 결과를 보고합니다:\n\nAllegro 핸드로 큐브를 조작하는 작업에서, 온라인 최적화는 수 초 이내에 실행 가능\n전체 조작을 위한 조작 동작 그래프(로드맵)를 구축하는 데 10분 미만 소요\n\n이는 일반적인 강화학습 기반 접근보다 훨씬 낮은 계산 자원으로 유사한 성능을 달성할 수 있음을 의미합니다.\n\n\n5. 예시 작업 및 결과\nCTR-MPC는 두 가지 실제 예시에서 검증되었습니다:\n\n양팔 조작 (Bimanual Manipulation): 두 개의 KUKA iiwa 팔로 큰 원통형 물체를 이동시키는 작업. 복잡한 접촉 협응이 필요하지만, CTR-MPC는 시뮬레이션과 실제 로봇 모두에서 성공적으로 수행.\n손 안 큐브 회전 (In-Hand Manipulation): Allegro 핸드로 큐브를 다양한 방향으로 회전시키는 작업. Relaxed CTR (R-CTR)을 사용한 경우가 가장 높은 성능을 보였으며, 로드맵 기반 전략으로 장거리 목표 회전도 달성 가능했음.\n\n\n\n6. 전역 계획과의 통합\nCTR-MPC는 본질적으로 로컬 최적화 기반이므로, 전체 상태 공간에서의 경로 계획은 어려울 수 있습니다. 이를 보완하기 위해 논문에서는 전역 로드맵 기반 계획(global roadmap planning)을 제안합니다:\n\n큐브의 다양한 안정된 포즈를 노드로 구성\nCTR-MPC를 이용해 이들 노드 간 단거리 조작 궤적(edge)를 생성\n전체 그래프를 탐색하여 멀리 떨어진 목표도 순차적 조작으로 도달 가능\n\n이 방식은 전통적인 샘플링 기반 계획과 유사하지만, MPC 기반 동작 원시(primitive)를 사용하여 접촉-풍부한 경로 생성을 가능케 합니다."
  },
  {
    "objectID": "posts/paper/2025-06-13-contact-rich-mpc.html#dextreme-강화학습-기반-큐브-회전-제어",
    "href": "posts/paper/2025-06-13-contact-rich-mpc.html#dextreme-강화학습-기반-큐브-회전-제어",
    "title": "📃Contact Trust Region 리뷰(feat.Dextreme)",
    "section": "DeXtreme: 강화학습 기반 큐브 회전 제어",
    "text": "DeXtreme: 강화학습 기반 큐브 회전 제어\nDeXtreme(NVIDIA Research, 2022)은 심층 강화학습 기반으로 학습된 정책(policy)을 통해, 저비용 로봇 핸드에서도 정밀한 큐브 회전을 수행한 시스템입니다. 이 접근법은 CTR이 다룬 Allegro 핸드의 조작 문제와 동일한 문제 설정에서, 전혀 다른 방식으로 해결책을 제시합니다.\n\n1. 시뮬레이션 기반 학습\n\nIsaac Gym이라는 GPU 가속 물리 시뮬레이터를 사용해 정책을 학습\n무려 10만 개 이상의 병렬 환경을 GPU에서 동시 실행\n이로 인해 로봇은 초인적인 속도로 시행착오 학습 가능\n\n\n\n2. 정책 구조\n\n정책은 심층 신경망으로 구성되며, 입력은 로봇 상태 및 물체 자세 정보\n비전 기반 정책도 학습됨: RGB 카메라 3대를 사용해 물체 자세 추정 후 입력으로 활용\n별도의 포즈 추정 신경망을 함께 학습시켜, 시각 정보에서 3D 물체 자세를 복원\n\n\n\n3. 도메인 랜덤화(Domain Randomization)\n\n시뮬레이션-현실 간 격차(Sim2Real gap)를 극복하기 위해 물리 속성 및 시각 조건을 광범위하게 랜덤화\n\n질량, 마찰계수, 표면 텍스처, 조명 조건, 카메라 위치 등\n\n이로 인해 정책은 넓은 조건 분포에 대해 강건한 행동 전략을 학습함\n\n\n\n4. 학습 비용 및 계산 자원\n\n약 32시간 동안 고성능 GPU 서버에서 학습\n이 동안 정책은 약 42년치에 해당하는 시뮬레이션 경험을 축적\n이는 강화학습의 대표적인 단점인 샘플 비효율성을 보여주는 지표\n\n\n\n5. 실행 및 실제 로봇 적용\n\n학습 완료 후, 정책은 고속 실시간 제어 가능 (신경망 전방 연산만 수행)\nAllegro 핸드에서 목표 방향으로 큐브를 안정적으로 회전시킴\nOpenAI의 Shadow Hand와 달리, 관절 수가 적고 비용도 낮은 Allegro 핸드에서 성공한 점이 인상적임\n\n\n\n6. 일반화 및 강건성\n\n도메인 랜덤화를 통해, 하드웨어 손상에도 견디는 강건성 확보\n\n예: 엄지 관절이 느슨한 상태에서도 정책이 보상하며 동작 성공\n\n시각 네트워크는 가림(occlusion) 및 모션 블러에도 견딜 수 있도록 학습됨\n\n\n\n7. 정책의 한계\nDeXtreme은 놀라운 성능을 보여줬지만, CTR 접근과 달리 접촉 물리 법칙을 명시적으로 반영하지는 않음:\n\n마찰 원뿔, 비침투 조건 등은 학습을 통해 암묵적으로 습득\n행동은 시뮬레이터의 물리 엔진과 보상 함수 설계를 통해 유도됨\n따라서 정책은 왜 해당 동작을 수행하는지 해석하기 어렵고, 제약 조건 위반 여부도 명시적으로 판단하기 어려움"
  },
  {
    "objectID": "posts/paper/2025-06-13-contact-rich-mpc.html#ctr-vs-dextreme-두-접근-방식의-비교-분석",
    "href": "posts/paper/2025-06-13-contact-rich-mpc.html#ctr-vs-dextreme-두-접근-방식의-비교-분석",
    "title": "📃Contact Trust Region 리뷰(feat.Dextreme)",
    "section": "CTR vs DeXtreme: 두 접근 방식의 비교 분석",
    "text": "CTR vs DeXtreme: 두 접근 방식의 비교 분석\nCTR-MPC와 DeXtreme은 모두 손 안의 큐브 회전과 같은 고난도 접촉 조작을 목표로 하지만, 모델 기반 최적화와 데이터 기반 학습이라는 정반대의 철학을 가지고 접근합니다. 아래는 두 방법론을 주요 관점에서 비교한 내용입니다.\n\n1. 접촉 처리 방식\n\n\n\n\n\n\n\n\n항목\nCTR-MPC\nDeXtreme (RL)\n\n\n\n\n접촉 모델링\n마찰 원뿔, 비침투 조건 등을 명시적 수식으로 모델링하고 최적화에 통합\n시뮬레이션과 보상을 통해 암묵적으로 접촉 전략을 학습\n\n\n접촉력 추론\n접촉력은 최적화 변수로 직접 계산되며, 계획 과정에서 사용됨\n신경망 내부에서 암묵적으로 형성됨 (관측 불가)\n\n\n물리 위반 가능성\n수식 제약으로 인해 물리 법칙 위반 불가능\n학습된 정책이 물리 제약을 위반할 수 있음 (ex. interpenetration)\n\n\n\n\n\n2. 샘플 효율성과 계산 자원\n\n\n\n\n\n\n\n\n항목\nCTR-MPC\nDeXtreme (RL)\n\n\n\n\n사전 학습 필요성\n없음 – 매 실행마다 최적화\n필요 – 수십억 스텝의 시뮬레이션 필요\n\n\n실행 시 계산 비용\n중간 – SOCP 최적화 수행\n매우 낮음 – 신경망 전방 연산만 수행\n\n\n샘플 효율성\n매우 높음 – 모델 기반 추론\n낮음 – 방대한 시행착오 필요\n\n\n\n\n\n3. 일반화와 적응성\n\n\n\n\n\n\n\n\n항목\nCTR-MPC\nDeXtreme (RL)\n\n\n\n\n환경 변화 대응\n모델만 수정하면 즉시 대응 가능\n사전 학습된 분포 외에는 재학습 필요\n\n\n목표 변화 적응\n즉시 가능 (목표 상태만 바꾸면 됨)\n가능하나, 정해진 목표 형식 내에서만 일반화됨\n\n\n외란 대응성\n고 – 재계획 기반\n중 – 일부 외란에는 강건하나 계획 능력은 없음\n\n\n\n\n\n4. 정책 구조와 해석 가능성\n\n\n\n\n\n\n\n\n항목\nCTR-MPC\nDeXtreme (RL)\n\n\n\n\n정책 형태\n최적화 기반 – 현재 상태에서 계획을 계산\n신경망 기반 – 관측 → 행동 매핑\n\n\n해석 가능성\n높음 – 접촉력, 제약 조건 등 확인 가능\n낮음 – 블랙박스 정책\n\n\n제약 조건 추가 용이성\n용이 – 수식 삽입만으로 반영 가능\n어려움 – 네트워크 재학습 필요\n\n\n\n\n\n요약\n\n\n\n항목\nCTR-MPC\nDeXtreme (RL)\n\n\n\n\n접촉 처리\n명시적, 해석 가능\n암묵적, 해석 불가\n\n\n학습 필요성\n없음\n큼 (수십억 스텝)\n\n\n실행 속도\n느리지만 정확\n매우 빠름\n\n\n일반화\n모델 기반 적응\n제한된 목표 내 일반화\n\n\n확장성 및 유지보수\n제약 추가/변경 쉬움\n재학습 필요"
  },
  {
    "objectID": "posts/paper/2025-06-13-contact-rich-mpc.html#결론-및-향후-연구-방향",
    "href": "posts/paper/2025-06-13-contact-rich-mpc.html#결론-및-향후-연구-방향",
    "title": "📃Contact Trust Region 리뷰(feat.Dextreme)",
    "section": "결론 및 향후 연구 방향",
    "text": "결론 및 향후 연구 방향\nCTR과 DeXtreme은 각각 정확하고 물리적으로 해석 가능한 모델 기반 계획과 빠르고 강건한 데이터 기반 제어라는 상반된 강점을 보여줍니다. 이러한 성격의 차이는 오히려 상호보완적인 통합 가능성을 시사합니다.\n\n1. 하이브리드 전략의 가능성\n앞으로의 연구는 다음과 같은 하이브리드 모델을 탐색할 수 있습니다:\n\nCTR으로 생성된 궤적을 imitation learning의 teacher로 활용\n\nRL의 초기 정책을 빠르게 수렴시킬 수 있음\n\nDeXtreme 정책을 warm-start로 사용하여 CTR 최적화를 가속\n\n최적화 초기화를 RL 정책 기반으로 설정해 연산량 감소\n\n접촉 모델의 일부를 학습된 근사 모델로 대체\n\n예: 마찰계수 추정, 감쇠 계수 추정 등 실제 환경 파라미터 보정\n\n\n이처럼 양측의 장점을 조합하는 방식은, 물리 기반 정확성과 학습 기반 유연성을 동시에 확보할 수 있는 유망한 방향입니다.\n\n\n2. 실시간성 향상\nCTR-MPC의 경우, 최적화의 실시간성은 여전히 제한적입니다. 이를 해결하기 위해 다음과 같은 접근이 제안될 수 있습니다:\n\nCTR 기반 정책을 사전 학습해 신경망으로 근사 (Policy Distillation)\nCTR 해를 데이터셋으로 수집 후, offline RL이나 trajectory matching으로 정책 학습\n\n이러한 방식은 제약 조건을 만족하는 정책을 빠르게 실행할 수 있게 해줄 뿐 아니라, 정책의 해석 가능성도 부분적으로 유지할 수 있습니다.\n\n\n3. 보다 복잡한 조작 작업 확장\n향후 연구는 다음과 같은 더 복잡한 작업으로의 확장을 목표로 할 수 있습니다:\n\n비정형 물체 조작 (불규칙한 형상, 연성 물체 등)\n시각 기반 입력 통합 (CTR과 카메라 인식 결합)\n사람과의 협업 조작 (공동 운반, 안전 제약 등 포함)\n\n특히 CTR 기반 접근은 제약 조건 기반의 신뢰성과 안전성을 활용해, 사람과 함께하는 환경에서도 활용 가능성을 보여줍니다."
  },
  {
    "objectID": "posts/paper/2025-06-13-contact-rich-mpc.html#마무리",
    "href": "posts/paper/2025-06-13-contact-rich-mpc.html#마무리",
    "title": "📃Contact Trust Region 리뷰(feat.Dextreme)",
    "section": "마무리",
    "text": "마무리\n“Dexterous Contact-Rich Manipulation via the Contact Trust Region” 논문은 고난도 조작에서 접촉 제약을 어떻게 명시적으로 다루고, 이를 모델 기반 제어 프레임워크에 통합할 수 있는지를 수학적으로 우아하게 풀어낸 작업입니다. 그에 비해 DeXtreme은 대규모 계산 자원을 활용한 전통적인 심층강화학습 방식이지만, 실제 적용성에 있어 매우 강력한 접근임을 보여줍니다.\n이 두 흐름은 서로 경쟁적이라기보다, 다음 세대의 조작 시스템에서 병렬적으로 사용될 수 있는 기술 스펙트럼의 양극단으로 이해될 수 있습니다.\n앞으로의 연구는, 이들 방법론을 상황에 따라 선택하거나 조합함으로써, 보다 유연하고 안전하며 일반화 가능한 로봇 조작 시스템을 구축하는 데 기여할 수 있을 것입니다."
  },
  {
    "objectID": "posts/paper/2022-10-16-recovery.html",
    "href": "posts/paper/2022-10-16-recovery.html",
    "title": "📃Robust Recovery Controller 리뷰",
    "section": "",
    "text": "이번 포스트는 4족보행 로봇이 전복되었을 때 다시 정상적으로 보행하기 위해 자세를 회복하는 모션 제어(이하 Recovery 혹은 Reset task라고 지칭)을 강화학습 방법으로 해결하고자한 Robust Recovery Controller for a Quadrupedal Robot using Deep Reinforcement Learning라는 논문에 대한 리뷰입니다."
  },
  {
    "objectID": "posts/paper/2022-10-16-recovery.html#mpc-vs-rl",
    "href": "posts/paper/2022-10-16-recovery.html#mpc-vs-rl",
    "title": "📃Robust Recovery Controller 리뷰",
    "section": "MPC vs RL",
    "text": "MPC vs RL\n제어(Control) 분야에서 현재 크게 양대 방법론으로 MPC(Model Predictive Control)과 RL(Reinforcement Learning) 방법론이 주목을 받고 있습니다. 이분법적으로 두 방법이 확연하게 나누어지거나 우열이 가려지는 것이 아니기 때문에 두 방법론 간의 공통점, 차이점등을 살펴보며 시작해보려고 합니다.\n아래 그림의 Optimal Control(최적제어), Operations Research(경영과학), Reinforcement Learning(강화학습)을 각각 하나의 집합으로 표시하고 각 관계를 살펴보면 기존의 제어 방식의 발전 흐름을 이어온 MPC는 Optimal Control에 포함되어 있다고 볼 수 있습니다. Operations Research은 좀 더 넓은 범위의 의사 결정 모델들을 연구하는 분야로 주로 산업공학에서 배우는 부분이며 의사결정에 도움을 받기위해 수학적 모델, 통계학, 알고리즘들을 이용하는 연구분야 입니다. 따라서 Optimal Control, Operations Research와 오늘의 주제 방법론인 Reinforcement Learning과의 관계를 살펴보자면 수학적 모델링을 기반으로 한 Model-based Methods가 Optimal Control과 Reinforcement Learning의 교집합 부분으로 볼 수 있고 최근 강화학습 연구들도 Model-based RL으로 연구들이 많이 이루어지고 있습니다. Operations Research와 Reinforcement Learning의 교집합으로는 Large action-space Method로 볼 수 있는데, 기존의 아타리 게임과 같이 간단한 방향 조작키와 같은 action의 선택지가 적은 경우에 비해 의사결정을 내려야하는 상황이 복잡할 수록 즉, 문제가 더 어렵고 복잡할 수록 action의 선택지가 많아지기 때문에 action space가 더 커지는 방향으로 연구가 많이 이루어지고 있다고 볼 수 있습니다.\n그럼 좀 더 자세히 MPC과 RL을 비교해보면, MPC는 그림과 같이 마치 어둠속에서 손전등을 키고 빛을 따라 하나의 길을 보듯이, 하나의 최적의 trajectory를 찾아 그 trajectory의 1 step을 실행시킵니다. 수학적으로 모델링한 최적식의 해를 구해서 계산과 실행을 반복하며 제어를 하는 것 입니다. 반면 RL은 search space를 MDP(Markov Decision Process)를 이용하여 정의하고 학습을 하며 시점 t에서의 최적의 action을 할 수 있도록 합니다. 두가지 방법 모두 Advanced 시킬 부분이 많이 남아있고 MPC는 non-linear 방향으로 나아가고 있으며 RL은 value-based나 policy-based 기반의 방법들과 함께 최근에는 인공신경망의 gradient기법을 이용하여 발전해나가고 있습니다. MPC와 RL의 비교는 최근 ICRA와 같은 로봇 제어 관련 학회들에서는 관심이 많은 주제이며 Michiel van de Panne (UBC) 교수님의 발표에서 좀 더 자세히 확인하실 수 있습니다."
  },
  {
    "objectID": "posts/paper/2022-10-16-recovery.html#locomotion-vs-recovery",
    "href": "posts/paper/2022-10-16-recovery.html#locomotion-vs-recovery",
    "title": "📃Robust Recovery Controller 리뷰",
    "section": "Locomotion VS Recovery",
    "text": "Locomotion VS Recovery\n이런 강화학습 방법론으로 4족보행의 Recovery Task를 풀기 위해서는 Recovery task에 대한 특성을 좀 더 살펴볼 필요가 있습니다. 기본적으로 족형(Legged) 로봇들은 안정적인 보행(Locomotion)을 기반으로 여러가지 task를 하는데에 초점이 맞춰져 있습니다. 예를 들면 보행로봇의 위에 Manipulator를 달아서 원하는 위치로 보행을 한 뒤 Manipulator로 컵을 집는 일과 같은 task를 수행하는 것입니다. 하지만 4족보행 로봇을 사용하기 위해 전제된 안정적인 보행이 실제로 로봇이 운용되는 환경에서는 여러가지 예측할 수 없는 변수들로 인해 만족될 수 없는 조건일 수 있으며 따라서 로봇이 넘어질 수 밖에 없다면 다시 보행을 할 수 있는 정상적인 상태로 돌아오기 위한 능력도 갖추고 있어야 합니다. 바로 이를 위한 로봇의 기능을 Recovery라고 할 수 있고 Recovery를 Locomotion task와 비교하여 특징 몇가지를 살펴보겠습니다.\n\n우선 Recovery는 앞서 이야기한 것과 같이 Locomotion이 메인이 되는 task인데 반해, 로봇의 운용 life cycle을 위한 support task라고 할 수 있습니다. 로봇이 적용되는 현장에서 메인 task들을 끊임없이 수행하기 위해 흐름을 유지시켜주는 장치라고 볼 수 있습니다. 다음으로 Locomotion은 보행을 할 때 로봇 다리의 움직임이 주기(period)를 가지는 Cyclic한 모션을 하는 것에 반해 Recovery는 넘어진(Fall) 자세로부터 다시 걸을 수 있는 서있는(Stand) 자세로 가기 위한 모션을 취해야 하기 때문에 Non-cyclic하고 case-by-case로 다양한 회복 모션들을 할 수 있어야 합니다. 따라서 Locomotion은 환경에 따라(지면의 마찰력이나 외력 등) 보행의 난이도가 달라지긴 하지만 로봇의 joint의 동작 범위, 즉 action의 search space로 생각했을 때 Recovery에 비해 좀 더 좁은 search space를 가지고 있다고 할 수 있습니다. 마지막으로 Locomotion을 수행할 때 참고할 수 있는 동물이나 사람의 모션 데이터들이 있지만 Recovery 같은 경우 참고할 수 있는 모션 데이터들이 많이 없고 일어나는 방법에 대해 메뉴얼 같이 각각의 단계를 명시하기 까다롭다는 점이 있습니다."
  },
  {
    "objectID": "posts/paper/2022-10-16-recovery.html#overview",
    "href": "posts/paper/2022-10-16-recovery.html#overview",
    "title": "📃Robust Recovery Controller 리뷰",
    "section": "Overview",
    "text": "Overview\n논문에서 제시한 전체적인 Control System의 모습은 아래 그림과 같습니다. 강화학습 알고리즘으로는 TRPO(Trust Region Policy Optimization algorithm)와 GAE(Generalized Advantage Estimator)를 사용했으며 제시된 방법에서는 크게 3가지 특징들이 있습니다.\n\nControl Task Decomposed : Recovery하는 task를 3개로 Behavior들로 나누어서 각각의 Behavior를 수행하는 Control Policy를 학습\nHierarchical Structure : 여러개의 Behavior policies를 조율할 수 있는 상위계층의 Behavior Selector를 만들어서 계층적인 구조를 사용\nHeight Estimator : 로봇을 실제 운용할 때 필요한 상태 정보인 Height 값의 부정확한 정보를 보완하기 위해 Neural Network 사용(Regression model)"
  },
  {
    "objectID": "posts/paper/2022-10-16-recovery.html#behavior-policies",
    "href": "posts/paper/2022-10-16-recovery.html#behavior-policies",
    "title": "📃Robust Recovery Controller 리뷰",
    "section": "Behavior Policies",
    "text": "Behavior Policies\nRecovery task를 여러개의 Behavior들로 나누어서 총 3개의 각각의 task, Self-Righting, Standing up, Locomotion가 있고 각 behavior을 담당하는 policy가 있습니다.\n\nSelf-Righting은 임의의 로봇의 넘어진 자세에서 로봇의 몸체(base)가 똑바로(뒤집어져 있거나 옆으로 돌아가 있지 않은 상태) 되어 있고 4개의 발이 지면에 닿아 있는 자세로 움직이는 것을 말합니다. Standing up은 Self-righting에서 만든 자세에서 4개의 다리들을 이용하여 일어선 자세로 만드는 것을 말합니다. 마지막으로 Locmotion은 Controller에서 주는 command를 기반으로 보행을 하는 것을 말하며 이때 command로는 forward velocity, lateral velocity, turing rate(yaw 방향)을 주어 움직이게 됩니다.\n\n(1)Self-righting behavior\nSelf-righting을 학습하기 위해 policy에 들어가는 data(state 정보)로는 아래의 표에서 볼 수 있듯이 총 6개의 data가 있습니다. 그 중 e_g는 몸체 base의 z 방향으로의 단위 벡터로 몸체의 upright를 판단하기 위한 정보로 볼 수 있습니다.\n\n보통 강화학습에서는 reward를 최대화 하는 방향으로 학습(최적화)이 일어나지만 reward의 반대 개념인 cost의 값이 최소화하는 방향으로 학습을 시켰습니다. 아래의 식처럼 여러개의 cost term들이 있지만 그 중 self-righting에서 특징적인 cost term은 orientation cost term 입니다. 이 cost term은 c_o=\\left\\|[0,0,-1]^T-e_g\\right\\|로 계산되는데 이는 위에서 설명한 대로 policy network가 로봇 몸체의 base의 방향을 uprighting 하도록 학습되게 하기 위한 term이라고 볼 수 있습니다.\n\n(2)Standing up behavior\nStanding up은 이전의 Self-righting의 state 정보를 포함하고 더하여 Base linear velocity 정보까지 포함하여 policy의 input으로 들어가게 됩니다. Self-righting과 Standing up policy 간의 state 정보 포함 관계 뿐만 아니라 이후 소개할 Locomotion policy, Behavor Selector의 state 정보의 집합관계를 보면 다른 policy에 들어가는 정보를 포함하고 추가적인 data를 더하여서 Self-righting → Standing up → Locomotion → Behavior Selector 순으로 state space가 점점 더 커져가는 것을 확인할 수 있습니다.\n\nStaning up behavior policy가 학습해야할 cost 최소화 식은 아래와 같이 여러 cost term들이 있지만 그중 height cost term이 특징적인 cost라고 볼 수 있으며 ANYmal 로봇이 서있을 때의 지면으로부터 몸체(base frame)까지의 거리, height이 0.35m보다 작을시에는 1, 아니면 0으로 계산합니다.\n\n(3)Locomotion behavior\nLocomotion task policy에는 input으로 command까지 들어가게 되면서 cost식에는 command를 잘 수행하는지 판단하도록 하는 angular velocity, linear velocity, foot clearance, foot slippage cost가 들어가는 것을 확인할 수 있습니다.\n\n지금까지 policy network들의 input와 objective(=cost 최소화)에 대해서 이야기 했지만 결과적으로 network가 어떤 값들을 output 하는지, 그리고 실제로 그 output으로 어떻게 로봇을 움직이는지에 대해서는 아직 설명하지 않았습니다. 로봇을 움직이기 위해서는 timestep마다 로봇의 각 모터가 움직여야 하는 desired joint position을 구해서 해당 position으로 모터를 돌려주면 됩니다.\n\n각 Behavior policy에서 나오는 output은 o_t이며 이는 로봇을 구동시키는 12개의 joint motor에 대응하는 실수 벡터입니다. 대응이라고 표현한 이유는 해당 실수값을 바로 joint의 desired position으로 사용하는 것이 아니라 좀 더 빠른 학습 수렴을 위해 desired joint position을 구하는 식을 거쳐 계산된 값을 사용하기 때문입니다. 우선 Self-righting과 Standing up task에서는 네트워크에서 나온 값 o_t를 현재 각 모터의 position인 \\phi_t에 더해주어서 최종 desired joint position인 \\phi_d 값으로 제어합니다. Locomotion에서는 현재의 joint position 대신, 로봇이 서있는 자세인 nominal joint configuration \\phi_n에 o_t을 더해주어 최종 desired joint position인 \\phi_d을 사용합니다.(o_t에 곱해지는 k는 하이퍼파라미터처럼 찾아야 하는 상수값입니다.)"
  },
  {
    "objectID": "posts/paper/2022-10-16-recovery.html#height-estimator",
    "href": "posts/paper/2022-10-16-recovery.html#height-estimator",
    "title": "📃Robust Recovery Controller 리뷰",
    "section": "Height Estimator",
    "text": "Height Estimator\ndata-driven RL에서 policy에 들어가는 data의 quality는 매우 중요합니다. 따라서 실제 작동하는 로봇의 state를 추정하는 State estimation 또한 로봇에 RL을 적용하기 위해서 뗄레야 뗄수 없는 중요한 부분이라고 할 수 있습니다. 로봇에 모션 캡쳐와 같은 센서를 부착하지 않는한 실제 로봇의 base height값을 잘 알 수 없고 로봇이 정상적으로 보행할 때는 TSIF(Two State Implicit Filter)와 같은 State Estimation 기법을 통해 어느정도 추정할 수 있지만 넘어져서 base가 거의 바닥과 가까울 경우 추정값이 매우 불안정하게 됩니다. 따라서 해당 논문에서는 Regression Neural Network를 통해 height를 넘어진 상태에서도 잘 추정할 수 있도록 했습니다.\n\nHeight Estimator Network가 input으로 받는 data는 아래의 표와 같습니다. Output으로는 body의 IMU 값과 12개 joint들의 position을 출력하여 해당 값들을 가지고 forward kinematics를 이용하여 구한 height 값을 네트워크에서 추정한 값으로 사용합니다. 이 신경망은 강화학습으로 학습을 하는 것이 아니라 지도학습 방법으로 true 값을 맞춰가는 과정을 통해 학습하게 되는데 이떄 true data는 시뮬레이션 상에서는 쉽게 구할 수 있고 Regression model의 loss 값은 \\sum_{j=0}^K\\left\\|h_j-h_\\psi\\left(s_j\\right)\\right\\|^2으로 계산됩니다. (j: joint index, s_j: joint state, \\psi: network parameter)"
  },
  {
    "objectID": "posts/paper/2022-10-16-recovery.html#behavior-selector",
    "href": "posts/paper/2022-10-16-recovery.html#behavior-selector",
    "title": "📃Robust Recovery Controller 리뷰",
    "section": "Behavior Selector",
    "text": "Behavior Selector\n여러개의 behavior policy들을 어떻게 조율할 것인지는 상위 계층에 Behavior Selector를 NN을 이용하여 만들어서 학습시킵니다.\n\n로봇의 상황에 맞추어서 적절한 behavoir를 하도록 behavior selector는 categorical distribution을 학습하게 됩니다. Behavior selector와 앞서 설명한 Height estimator는 3개의 behavior policy들이 다 학습이 된 후에 아래 그림의 오른쪽에 보이는 Algorithm1의 흐름에 따라 학습되게 됩니다. Behavior selelctor의 state 나cost는 locomotion과 매우 유사하고 해당 포스트의 appendix에 표로 정리되어 있으며 cost식 같은 경우에도 모터의 power efficiency를 위한 term 정도 추가된 것이기 때문에 해당 cost 식이 궁금하신분은 원문 논문에서 확인하실 수 있습니다."
  },
  {
    "objectID": "posts/paper/2022-10-16-recovery.html#simulating-anymal",
    "href": "posts/paper/2022-10-16-recovery.html#simulating-anymal",
    "title": "📃Robust Recovery Controller 리뷰",
    "section": "Simulating ANYmal",
    "text": "Simulating ANYmal\n로봇에 강화학습을 적용할 때 큰 이슈들 중 하나는 Sim-to-Real입니다. 시뮬레이션으로 실제 환경을 단순화하고 모사한 것이기 떄문에 시뮬레이션에서 잘 학습이되고 잘 동작하더라도 실제 로봇을 deploy했을때 잘 작동하지 않는 문제가 생깁니다. 따라서 실제 환경에서도 로봇이 경험하게 되는 noise들을 시뮬레이션에도 random하게 적용시켜 최대한 실제 상황과도 유사하게 만든 환경에서 학습을 하게 됩니다.\n해당 논문에서도 Link length, Intertial property, Link mass, CoM(Center of Mass), Collision geometry, Coefficient of friction 등과 같이 물리적인 값들을 아래 표에서 볼 수 있듯이 일정 범위에서 random하게 값을 넣어주었고 policy의 input data가 되는 observation 값들에도 noise 값을 추가하여 Sim-to-Real 문제를 해결하였습니다. 이외에도 ANYmal 로봇 플랫폼에서 사용하는 SEA motor에 스프링과 같은 기계적 요소들의 변수로 인한 제어 이슈도 해결하기 위해 Actuator Network를 학습시켜 이를 해결하였습니다.\n\n(위 요약에서 언급했던 대로 사실상 Actuator Network나 Height Estimator까지 NN이 추가되므로 4개가 아닌 총 6개의 NN이 사용되었음을 알 수 있습니다.)"
  },
  {
    "objectID": "posts/paper/2022-10-16-recovery.html#the-success-of-recovery",
    "href": "posts/paper/2022-10-16-recovery.html#the-success-of-recovery",
    "title": "📃Robust Recovery Controller 리뷰",
    "section": "The success of recovery",
    "text": "The success of recovery\nRecovery 실험은 총 2가지로 진행되었는데 첫번째는 지면에 로봇을 임의의 position으로 넘어진 상황을 연출한 후 5초 이내로 일어날 수 있는지를 확인했고 두번째로는 로봇이 걷고 있을 때 발로 쳐서 넘어뜨린후 로봇이 다시 자연스럽게(각 behavior들 간의 switching이 자연스럽게) 일어나는지를 확인했습니다. 각각의 실험 모두 50번 이상씩 진행했으며이때 약 100번중 97번을 성공하여 97% 성공률을 보였습니다. (실패한 케이스들의 경우에는 joint의 position이 2\\pi를 넘어가는 값으로 나올 때 잘 작동하지 않았다고 합니다.)"
  },
  {
    "objectID": "posts/paper/2022-10-16-recovery.html#the-effectiveness-of-height-estimator",
    "href": "posts/paper/2022-10-16-recovery.html#the-effectiveness-of-height-estimator",
    "title": "📃Robust Recovery Controller 리뷰",
    "section": "The effectiveness of Height Estimator",
    "text": "The effectiveness of Height Estimator\n기존의 State Estimator TSIF(Two State Implicit Filter)만을 사용했을 때는 로봇이 바닥에 넘어져 있을 경우 error 값이 매우컸지마나 Neural Network를 통해 보정했을 때 오차가 1cm 미만으로 떨어지는 것을 확인할 수 있었습니다. 오른쪽의 height 그래프는 맨 아래 캡쳐되어있는 로봇의 일련의 모션과정 중에 height를 그래프로 plotting한 것인데 simulation(초록색)이 true값이며, TSIF만을 사용했을 때(파란색)는 로봇이 넘어져있을 때 오차가 큰데 반해 neural network(주황색)은 simulation data와 거의 같음을 확인할 수 있습니다."
  },
  {
    "objectID": "posts/paper/2022-10-16-recovery.html#the-competitiveness-of-beahavior-selector",
    "href": "posts/paper/2022-10-16-recovery.html#the-competitiveness-of-beahavior-selector",
    "title": "📃Robust Recovery Controller 리뷰",
    "section": "The competitiveness of Beahavior Selector",
    "text": "The competitiveness of Beahavior Selector\n기존의 제어방법들에서는 여러가지 mode를 조율하기 위해서 FSM(Finite State Machine)을 많이 사용합니다. 특정 조건을 if에 넣어주어서 각 mode를 transition하는 기법인데 논문에서도 강화학습으로 학습한 Behavior Selector의 비교를 위해 기존의 FSM 방식을 활용하여 State Machine을 만들어서 비교했습니다. FSM 방식은 여전히 corner case들이 Behavior Selector에 비해 많이 존재했고 각 behavior들 간의 전환도 자연스럽지 않았습니다.\n\nReview\n\n아직 minor 한 주제인 Recovery task에 대해 집중적으로 잘 분석하고 성과도 확실히 보여준 논문이라고 생각합니다. Control system을 working하게 하기 위해 각 파트들을 어떻게 설계하고 학습해야할 지 많은 고민을 했다는 것을 느낄 수 있었습니다. 여전히 flat ground에서만 진행된 연구이기에 slope가 있는 환경이나 다른 object가 있는 좀 더 실제 상황과 비슷한 상황에 대한 recovery가 해결되기 위해서는 연구되어야 할 부분이 충분히 많은 것 같습니다.\n\n\n\nAppendix\n\n\n\nReference\n\norginal paper : https://arxiv.org/abs/1901.07517\nhttps://youtu.be/veXcohbFxKQ"
  },
  {
    "objectID": "posts/paper/2025-06-11-dex-il-review.html",
    "href": "posts/paper/2025-06-11-dex-il-review.html",
    "title": "📃Dex Imitation Learning 리뷰",
    "section": "",
    "text": "Paper Link"
  },
  {
    "objectID": "posts/paper/2025-06-11-dex-il-review.html#behavioral-cloning-bc",
    "href": "posts/paper/2025-06-11-dex-il-review.html#behavioral-cloning-bc",
    "title": "📃Dex Imitation Learning 리뷰",
    "section": "1.1 Behavioral Cloning (BC)",
    "text": "1.1 Behavioral Cloning (BC)\nBC는 전문가 데모의 state-action 쌍으로부터 직접 학습하여 전문가 행동을 복제하는 지도 학습(supervised learning) 패러다임입니다. 보상 신호나 탐색(exploration) 없이 상태에서 행동으로의 직접 매핑을 특징으로 합니다. 목표 함수는 데모된 액션의 negative log-likelihood를 최소화하는 것입니다:\nL(\\pi) = -E_{(s,a)\\sim p_D}[\\log \\pi(a | s)]\n여기서 D = \\{\\tau_1, \\dots, \\tau_n\\}는 n개의 데모 집합이며, 각 데모 \\tau_i는 길이 N_i의 state-action 쌍 시퀀스 \\{(s_1, a_1), \\dots, (s_{N_i}, a_{N_i})\\}입니다. BC는 푸싱(pushing) 및 grasping과 같은 비교적 간단한 작업에서 효과적인 성능을 보였습니다. 그러나 훈련 중 보지 못한 상태에 직면할 때 전문가 행동에서 벗어나는 액션을 생성할 수 있는 distribution shift 및 sequential decision-making 과정에서 오류가 누적되는 compounding error 문제에 취약합니다. 이를 완화하기 위해 계층적 프레임워크 [29]를 사용하거나, 단계별 액션 대신 전체 액션 시퀀스를 예측하여 유효 결정 시간 범위(effective decision horizon)를 줄이는 접근 방식 [53]이 제안되었습니다. 인간 데모에 흔한 multi-modal 데이터를 모델링하기 위해 에너지 기반 모델링 [26], 가우시안 혼합 모델 [58], 생성 모델 [59] 등이 탐구되었으며, 최근 Diffusion models [32, 60, 61, 62]이 BC 방법의 강건성 및 일반화 향상에 큰 잠재력을 보여주고 있습니다. BC 기반 방법은 일반화 및 multi-modal 액션 분포 모델링에 어려움을 겪지만, Diffusion models는 직접 액션 시퀀스를 생성하거나 고수준 전략을 안내하는 방식으로 유연성을 향상시키고 있습니다."
  },
  {
    "objectID": "posts/paper/2025-06-11-dex-il-review.html#inverse-reinforcement-learning-irl",
    "href": "posts/paper/2025-06-11-dex-il-review.html#inverse-reinforcement-learning-irl",
    "title": "📃Dex Imitation Learning 리뷰",
    "section": "1.2 Inverse Reinforcement Learning (IRL)",
    "text": "1.2 Inverse Reinforcement Learning (IRL)\nIRL은 사전 정의된 보상 함수를 최대화하기 위해 정책을 학습하는 기존 RL 프레임워크를 역전시킵니다. 대신, 전문가 데모 집합 D를 가장 잘 설명하는 기저의 보상 함수 R(s, a)를 추론하는 것을 목표로 합니다. 데모는 최적 또는 거의 최적의 정책을 따르는 전문가에 의해 생성되었다고 가정합니다.\nIRL 문제는 일반적으로 유한 Markov Decision Process M = \\langle S, A, T, R, \\gamma \\rangle 내에서 공식화되며, 여기서 S와 A는 상태 및 액션 공간, T(s'|s, a)는 상태 전이 확률, R(s, a)는 보상 함수, \\gamma \\in [0, 1]는 할인율입니다. 보상 함수는 종종 특징 함수 \\phi(s, a)의 선형 조합 R(s_t, a_t) = w^\\top\\phi(s_t, a_t)으로 표현됩니다. 정책 \\pi 하에서의 기대 특징 카운트는 \\mu_\\phi(\\pi) = \\sum_{t=0}^\\infty \\gamma^t \\psi_\\pi(s_t)\\phi(s_t, a_t)로 정의됩니다. IRL은 보상 함수를 수동으로 정의하기 어려운 DM 시나리오에서 특히 유리합니다.\n최근 연구들은 reward normalization, task-specific feature masking [63], adaptive sampling [64], 사용자 피드백 통합 [65], 비정형 데모로부터 보상 함수 학습 [67], Proximal Policy Optimization [45]과의 통합 [68], 시각 기반 인간-로봇 협업 [69] 등을 통해 IRL 프레임워크를 발전시켰습니다. IRL은 전문가 데모로부터 기저 보상 함수를 추론함으로써 복잡한 행동을 일반화하고 다양한 환경에 적응할 수 있도록 하지만, 고차원 액션 공간이나 희소한 피드백 신호에서 정확한 보상 함수 추정 및 대량의 데모 데이터 요구와 같은 한계에 직면합니다."
  },
  {
    "objectID": "posts/paper/2025-06-11-dex-il-review.html#generative-adversarial-imitation-learning-gail",
    "href": "posts/paper/2025-06-11-dex-il-review.html#generative-adversarial-imitation-learning-gail",
    "title": "📃Dex Imitation Learning 리뷰",
    "section": "1.3 Generative Adversarial Imitation Learning (GAIL)",
    "text": "1.3 Generative Adversarial Imitation Learning (GAIL)\nGAIL은 GAN [102] 프레임워크를 IL 영역으로 확장합니다. 모방 프로세스를 생성자와 판별자 사이의 2인 적대적 게임으로 공식화합니다. 생성자는 전문가 데모와 유사한 행동을 생성하려는 정책 \\pi에 해당하며, 판별자 D(s, a)는 state-action 쌍 (s, a)가 전문가 데이터 M에서 왔는지 또는 \\pi에 의해 생성되었는지 평가합니다. GAIL은 전문가와 생성자의 state-action 분포 사이의 Jensen-Shannon divergence를 최소화합니다.\n판별자는 다음 목표를 최대화하도록 훈련됩니다:\n\\arg \\min_D -E_{d_M(s,a)}[\\log D(s, a)] - E_{d_\\pi(s,a)}[\\log(1 - D(s, a))]\n생성자의 정책 \\pi는 판별자에서 파생된 보상 r_t = -\\log(1 - D(s_t, a_t))을 사용하여 RL로 최적화됩니다. 이 적대적 훈련 과정을 통해 GAIL은 명시적으로 보상 함수를 복구하지 않고도 전문가 데모로부터 복잡한 행동을 효과적으로 학습합니다.\nGAIL은 DM에서 널리 채택되었지만, 데모 데이터의 품질 및 가용성, 그리고 훈련 불안정성(mode collapse, gradient vanishing) 문제에 크게 의존합니다. Hindsight Experience Replay [77], semi-supervised correction [76], Sim-to-real transfer [78] 등이 데이터 문제를 해결하려 시도했으며, Variational Autoencoders [79], Wasserstein GAN [80], self-organizing generative model [82] 등을 사용하여 훈련 안정성을 개선하고 Mode collapse를 완화하려는 노력이 있었습니다. GAIL은 적대적 훈련의 근본적인 한계를 상속받아 훈련 불안정성 및 고차원 액션 공간으로의 확장 어려움에 직면합니다."
  },
  {
    "objectID": "posts/paper/2025-06-11-dex-il-review.html#hierarchical-imitation-learning-hil",
    "href": "posts/paper/2025-06-11-dex-il-review.html#hierarchical-imitation-learning-hil",
    "title": "📃Dex Imitation Learning 리뷰",
    "section": "1.4 Hierarchical Imitation Learning (HIL)",
    "text": "1.4 Hierarchical Imitation Learning (HIL)\nHIL은 복잡한 작업을 계층적 구조로 분해하여 해결하도록 설계된 IL 프레임워크입니다. 일반적으로 2단계 계층 구조를 채택하며, 상위 수준 정책은 현재 상태 및 작업 요구 사항에 따라 하위 작업 또는 원시(primitives) 시퀀스를 생성하고, 하위 수준 정책은 하위 작업을 실행하여 전체 목표를 달성합니다. 이 계층적 분해는 의사 결정 및 제어를 분리하여 장기적인 복잡한 작업을 보다 효과적으로 처리할 수 있도록 합니다.\n상위 정책 \\pi_h는 미리 정의된 원시 집합 \\{p_1, \\dots, p_K\\}에서 원시 p_i를 선택합니다: \\pi_h(s_t) = p_i. 해당 하위 정책 \\pi_{p_i}는 선택된 원시를 실행할 액션을 생성합니다: a_t = \\pi_{p_i}(s_t). 전체 목표는 누적 손실 함수를 최소화하는 것입니다:\nL(\\pi) = \\sum_{t=1}^T E_{(s_t,a_t)\\sim\\pi}[\\ell(s_t, a_t)]\nHIL의 주요 장점은 작업을 계층적 구조로 분해하여 직접적인 액션 공간 탐색의 복잡성을 줄이는 것입니다.\nCompILE [88], HDR-IL [89], ARCH [90], XSkill [91], LOTUS [92] 등의 연구들이 작업 분해, 기술 일반화, 장기적인 작업 처리에 기여했습니다. 최근 연구들은 Play data [93, 94]를 활용하여 두 수준의 정책을 효율적으로 훈련하는 방법을 탐구했습니다. HIL은 작업 분해 및 기술 일반화에서 상당한 이점을 보여주지만, Cross-modal 기술 일반화에서의 적응성 및 동적 환경에서의 모델 강건성 및 연속성 확보에 어려움을 겪고 있습니다."
  },
  {
    "objectID": "posts/paper/2025-06-11-dex-il-review.html#continual-imitation-learning-cil",
    "href": "posts/paper/2025-06-11-dex-il-review.html#continual-imitation-learning-cil",
    "title": "📃Dex Imitation Learning 리뷰",
    "section": "1.5 Continual Imitation Learning (CIL)",
    "text": "1.5 Continual Imitation Learning (CIL)\nCIL은 지속 학습(continual learning)과 IL을 통합하여 에이전트가 동적으로 변화하는 환경에서 전문가 행동을 모방함으로써 기술을 지속적으로 습득하고 적응할 수 있도록 합니다. 에이전트는 초기 단계에서 전문가 데모로부터 기본 기술을 학습하고, 이후 단계에서 점진적으로 지식을 축적하고 새로운 작업이나 환경에 적응하며 이전에 습득한 기술을 잊어버릴 위험을 완화합니다.\nCIL에서 정책 \\pi는 이전에 접한 모든 작업에 대한 누적 모방 손실을 최소화하여 최적화됩니다:\nL(\\pi) = -\\sum_{i=1}^t \\lambda^{(i)} E_{(s^{(i)},a^{(i)})\\sim \\rho^{(i)}_{exp}}[\\log \\pi(a^{(i)} | s^{(i)})]\n여기서 \\lambda^{(i)}는 t개의 각 작업에 할당된 가중치이고 \\rho^{(i)}_{exp}는 작업 i에 대한 전문가 state-action 쌍의 분포입니다.\n초기 연구 [95]는 이전에 습득한 기술을 손상시키지 않고 작업 간 전환을 가능하게 했지만, 상당한 저장 및 계산 리소스가 필요했습니다. Task-specific adapter 구조 [96], 비지도 기술 발견 [92], 행동 증류를 통한 통합 정책 학습 [97], Deep Generative Replay (DGR) [98], 자기 지도 학습 [99] 등 다양한 접근 방식이 제안되었습니다. CIL은 효과적인 멀티태스킹 학습, DGR 기술 적용, 자기 지도 기술 추상화에 중점을 두지만, 생성된 데이터의 품질 및 일관성, 리소스 소비, 현실 세계 응용을 위한 일반화 능력 부족과 같은 실질적인 배포 과제가 남아 있습니다."
  },
  {
    "objectID": "posts/paper/2025-05-05-reskin-anyskin.html",
    "href": "posts/paper/2025-05-05-reskin-anyskin.html",
    "title": "📃Reskin-Anyskin 리뷰",
    "section": "",
    "text": "https://reskin.dev/\n\n\n💡 ReSkin은 머신러닝과 자기 감지를 활용하여 저렴하고 다재다능하며 오래 사용할 수 있는 촉각 소프트 센서를 제공합니다.\n🧲 ReSkin은 자기 센싱을 통해 전자 회로를 수동 인터페이스와 분리하여 다양한 형태로 제작이 용이하며, 머신러닝은 제작 및 시간 변화에 강인한 센서 응답 모델을 학습합니다.\n⚙️ 자가 지도 학습 알고리즘을 통해 작은 데이터 수집 절차로 성능을 향상시켜 기존의 촉각 감지 모듈보다 더 다양하고 확장 가능하며 저렴한 촉각 감지 모듈을 만들 수 있습니다.\n\n\n\nReSkin은 저렴하고 교체 가능하며, 오래 지속되는 다용도 소프트 촉각 스킨입니다.\n핵심 아이디어:\n\n자기장 센싱: ReSkin은 소프트 자성 스킨과 magnetometer 기반 센싱 메커니즘을 사용합니다. 외력에 의해 스킨이 변형되면 자기장 왜곡이 발생하고, 이를 통해 접촉 위치와 힘을 추정합니다. 전자 회로는 수동 인터페이스와 분리되어 있어 인터페이스 교체가 용이하며 다양한 형태로 제작할 수 있습니다.\n머신러닝: 머신러닝 모델을 사용하여 센서 응답 특성을 학습합니다. 이를 통해 제작 과정이나 시간 경과에 따른 변동에 강인한 모델을 구축하고, self-supervised learning 알고리즘을 통해 적은 비용으로 성능을 향상시킵니다.\n\n구체적인 내용:\n\n센서 설계: 스킨은 elastomer 매트릭스 내에 삽입된 자성 미세 입자로 구성됩니다. 힘이 가해져 스킨이 변형되면 magnetometer가 자기장의 변화를 측정합니다. 5개의 magnetometer(중앙 1개, 주변 4개)를 사용하여 자기장 변화를 측정합니다.\n데이터 수집: Dobot Magician 로봇과 ATI Nano 17 force/torque 센서를 사용하여 데이터를 수집합니다. 로봇은 스킨의 다른 위치에 힘을 가하고, 센서 보드는 자기장 측정값을 스트리밍합니다.\n모델 아키텍처: 자기장 변화 \\mathbf{B}에서 접촉력의 위치 \\mathbf{x} = (x, y)와 크기 F를 예측하기 위해 5개의 layer를 가진 multilayer perceptron (MLP)를 사용합니다. 아키텍처는 다음과 같습니다: \\mathbf{B}(15) \\rightarrow \\text{MLP+ReLU}(200) \\rightarrow \\text{MLP}(200) \\rightarrow \\text{MLP}(40) \\rightarrow \\text{MLP+ReLU}(200) \\rightarrow \\text{MLP+ReLU}(200) \\rightarrow xyF(3). 세 번째 activation layer는 bottleneck feature layer이며, \\text{feat}(\\cdot)는 이 3-layer feature extraction network를 나타냅니다. 손실 함수는 \\text{L2-loss}를 \\mathbf{(x, y, F)}에 적용합니다.\nSelf-Supervised Learning (SSL): Ground truth 라벨 없이 새로운 센서에 적응하기 위해 triplet loss를 사용합니다. \\mathcal{L}_{\\text{triplet}} = \\max(0, ||\\text{feat}(\\mathbf{B}_a) - \\text{feat}(\\mathbf{B}_p)||^2 - ||\\text{feat}(\\mathbf{B}_a) - \\text{feat}(\\mathbf{B}_n)||^2). 여기서 \\mathbf{B}_a, \\mathbf{B}_p, \\mathbf{B}_n은 각각 anchor, positive, negative 샘플이며, ||\\mathbf{x}_a - \\mathbf{x}_p|| &lt; ||\\mathbf{x}_a - \\mathbf{x}_n||를 만족합니다.\n실험 결과:\n\n단일 센서 모델은 시간이 지남에 따라 성능이 저하됩니다.\nMulti-sensor 모델은 단일 센서 모델보다 성능이 우수합니다.\nSelf-supervised adaptation은 multi-sensor 모델의 성능을 더욱 향상시킵니다. 특히, 적은 양의 adaptation 데이터로도 상당한 성능 향상을 얻을 수 있습니다.\n\n다양한 응용: ReSkin은 로봇 그리퍼, 개 신발, 장갑, arm sleeve 등 다양한 형태로 적용될 수 있습니다. 물이 채워진 shot glass의 무게를 감지하고, 블루베리를 쥐는 등의 섬세한 조작에도 활용될 수 있습니다.\n\n결론:\nReSkin은 높은 localization 정확도와 힘 감도를 제공하는 저렴하고 컴팩트한 촉각 센서입니다. 머신러닝과 자기장 센싱을 결합하여 시간 경과 및 개별 스킨의 변동에 강인한 모델을 개발하고, SSL adaptation 절차를 통해 새로운 스킨에 대한 모델을 개선합니다."
  },
  {
    "objectID": "posts/paper/2025-05-05-reskin-anyskin.html#brief-review",
    "href": "posts/paper/2025-05-05-reskin-anyskin.html#brief-review",
    "title": "📃Reskin-Anyskin 리뷰",
    "section": "",
    "text": "ReSkin은 저렴하고 교체 가능하며, 오래 지속되는 다용도 소프트 촉각 스킨입니다.\n핵심 아이디어:\n\n자기장 센싱: ReSkin은 소프트 자성 스킨과 magnetometer 기반 센싱 메커니즘을 사용합니다. 외력에 의해 스킨이 변형되면 자기장 왜곡이 발생하고, 이를 통해 접촉 위치와 힘을 추정합니다. 전자 회로는 수동 인터페이스와 분리되어 있어 인터페이스 교체가 용이하며 다양한 형태로 제작할 수 있습니다.\n머신러닝: 머신러닝 모델을 사용하여 센서 응답 특성을 학습합니다. 이를 통해 제작 과정이나 시간 경과에 따른 변동에 강인한 모델을 구축하고, self-supervised learning 알고리즘을 통해 적은 비용으로 성능을 향상시킵니다.\n\n구체적인 내용:\n\n센서 설계: 스킨은 elastomer 매트릭스 내에 삽입된 자성 미세 입자로 구성됩니다. 힘이 가해져 스킨이 변형되면 magnetometer가 자기장의 변화를 측정합니다. 5개의 magnetometer(중앙 1개, 주변 4개)를 사용하여 자기장 변화를 측정합니다.\n데이터 수집: Dobot Magician 로봇과 ATI Nano 17 force/torque 센서를 사용하여 데이터를 수집합니다. 로봇은 스킨의 다른 위치에 힘을 가하고, 센서 보드는 자기장 측정값을 스트리밍합니다.\n모델 아키텍처: 자기장 변화 \\mathbf{B}에서 접촉력의 위치 \\mathbf{x} = (x, y)와 크기 F를 예측하기 위해 5개의 layer를 가진 multilayer perceptron (MLP)를 사용합니다. 아키텍처는 다음과 같습니다: \\mathbf{B}(15) \\rightarrow \\text{MLP+ReLU}(200) \\rightarrow \\text{MLP}(200) \\rightarrow \\text{MLP}(40) \\rightarrow \\text{MLP+ReLU}(200) \\rightarrow \\text{MLP+ReLU}(200) \\rightarrow xyF(3). 세 번째 activation layer는 bottleneck feature layer이며, \\text{feat}(\\cdot)는 이 3-layer feature extraction network를 나타냅니다. 손실 함수는 \\text{L2-loss}를 \\mathbf{(x, y, F)}에 적용합니다.\nSelf-Supervised Learning (SSL): Ground truth 라벨 없이 새로운 센서에 적응하기 위해 triplet loss를 사용합니다. \\mathcal{L}_{\\text{triplet}} = \\max(0, ||\\text{feat}(\\mathbf{B}_a) - \\text{feat}(\\mathbf{B}_p)||^2 - ||\\text{feat}(\\mathbf{B}_a) - \\text{feat}(\\mathbf{B}_n)||^2). 여기서 \\mathbf{B}_a, \\mathbf{B}_p, \\mathbf{B}_n은 각각 anchor, positive, negative 샘플이며, ||\\mathbf{x}_a - \\mathbf{x}_p|| &lt; ||\\mathbf{x}_a - \\mathbf{x}_n||를 만족합니다.\n실험 결과:\n\n단일 센서 모델은 시간이 지남에 따라 성능이 저하됩니다.\nMulti-sensor 모델은 단일 센서 모델보다 성능이 우수합니다.\nSelf-supervised adaptation은 multi-sensor 모델의 성능을 더욱 향상시킵니다. 특히, 적은 양의 adaptation 데이터로도 상당한 성능 향상을 얻을 수 있습니다.\n\n다양한 응용: ReSkin은 로봇 그리퍼, 개 신발, 장갑, arm sleeve 등 다양한 형태로 적용될 수 있습니다. 물이 채워진 shot glass의 무게를 감지하고, 블루베리를 쥐는 등의 섬세한 조작에도 활용될 수 있습니다.\n\n결론:\nReSkin은 높은 localization 정확도와 힘 감도를 제공하는 저렴하고 컴팩트한 촉각 센서입니다. 머신러닝과 자기장 센싱을 결합하여 시간 경과 및 개별 스킨의 변동에 강인한 모델을 개발하고, SSL adaptation 절차를 통해 새로운 스킨에 대한 모델을 개선합니다."
  },
  {
    "objectID": "posts/paper/2025-05-05-reskin-anyskin.html#brief-review-1",
    "href": "posts/paper/2025-05-05-reskin-anyskin.html#brief-review-1",
    "title": "📃Reskin-Anyskin 리뷰",
    "section": "2.1 Brief Review",
    "text": "2.1 Brief Review\n이 문서는 로봇 터치를 위한 새로운 촉각 센서인 AnySkin을 소개합니다. AnySkin은 조립이 용이하고, 다양한 로봇 End-effector와 호환되며, 새로운 Skin instance에 대한 일반화가 가능한 것이 특징입니다.\n핵심 아이디어:\n기존 촉각 센서의 단점(비용, 불편함, 일관성 부족)을 극복하고, 생물학적 움직임 제어에서 중요한 역할을 하는 촉각을 로봇 공학에 더 쉽게 통합하는 것을 목표로 합니다.\n주요 특징 및 기여:\n\n향상된 제작 과정 및 디자인 도구: 접착제가 필요 없고 내구성이 뛰어나며 쉽게 교체할 수 있는 자기 촉각 센서를 위한 간소화된 제작 과정과 디자인 도구를 제시합니다.\nSlip 감지 및 정책 학습: AnySkin 센서를 사용한 Slip 감지 및 Visuo-tactile 정책 학습의 효율성을 입증합니다. USB 삽입과 같은 정밀 작업에 적용 가능합니다.\n교체 용이성: AnySkin은 평균 12초 만에 교체할 수 있으며, 교체 후에도 재사용이 가능합니다.\nZero-shot 일반화: 한 AnySkin instance에서 학습된 모델이 다른 AnySkin instance에 Zero-shot으로 일반화됩니다. ReSkin과 같은 기존 촉각 솔루션과 비교하여 성능 저하가 훨씬 적습니다.\n\n기술적 상세:\n\n센서 작동 원리: AnySkin은 자기장 왜곡을 감지하여 접촉을 감지합니다. 센서 표면에는 자화된 철 입자가 포함되어 있으며, 외부 힘에 의한 표면 변형은 자기장 변화를 일으킵니다.\nFabrication 과정: Smooth-On DragonSkin 10 Slow와 MQFP-15-7(25μm) magnetic particles를 1:1:2 비율로 혼합하여 2-part mold에서 경화시킵니다 (\\text{Fig. 2a}). 경화된 Skin은 Pulse Magnetizer를 사용하여 자화됩니다.\n자기장 강화: Pulse Magnetizer를 사용하여 경화 후 Skin을 자화함으로써, 기존 ReSkin에 비해 더 강력한 자기장을 생성합니다.\n입자 분포 균일성: 더 미세한 Magnetic Particles(MQFP-15-7(25μm))를 사용하여 입자가 경화 전에 가라앉는 현상을 방지하고, Skin 전체에 걸쳐 더 균일한 분포를 얻습니다 (\\text{Fig. 2b}).\nSelf-aligning 디자인: Self-aligning 디자인을 통해 Elastomer와 회로의 상대적 위치 변동성을 줄이고, 신호 일관성을 향상시킵니다.\n\n실험 결과:\n\n신호 특성 비교: Pulse Magnetizer, finer particles, self-aligning 디자인이 신호 강도, 일관성, Misalignment에 대한 민감도에 미치는 영향을 정량적으로 분석합니다. (Table I)\nSlip 감지: Jaco Robot에 장착된 AnySkin 센서를 사용하여 수집된 데이터를 기반으로 LSTM 모델을 훈련하여, unseen object에 대한 Slip 감지 정확도가 92%임을 입증합니다 (\\text{Fig. 3}).\n교체 용이성 비교: ReSkin, DIGIT과 비교하여 AnySkin의 교체 시간과 재사용성을 User Study를 통해 평가합니다 (Table II).\n정책 학습에서의 교체 가능성: Plug insertion, Card swiping, USB insertion의 세 가지 정밀 조작 작업에서 AnySkin의 Cross-instance 일반화 성능을 평가합니다 (\\text{Fig. 5}). ReSkin과 DIGIT에 대한 비교 결과도 제시됩니다 (Table III).\n\n주요 공식:\n\n본 논문에서는 특정 공식을 명시적으로 제시하지는 않습니다. 하지만 실험 결과 분석에서 평균, 표준 편차 등의 기본적인 통계적 개념이 사용됩니다.\nLSTM(Long Short-Term Memory) 모델이 Slip 감지 모델로 사용됩니다.\nBAKU (Bidirectional Action-conditioned Keyframe Transformer) 아키텍쳐가 정책 학습에 사용됩니다.\n\n결론:\nAnySkin은 촉각 데이터를 활용하여 더욱 능숙하고 성능이 뛰어난 모델을 개발하는 데 기여할 수 있는 잠재력을 가진 새로운 촉각 센서입니다. Visuo-tactile 정책을 새로운 Instance에 Zero-shot으로 일반화하는 최초의 센서입니다."
  },
  {
    "objectID": "posts/paper/2025-05-05-reskin-anyskin.html#배경-촉각-센서의-어려움과-요구사항",
    "href": "posts/paper/2025-05-05-reskin-anyskin.html#배경-촉각-센서의-어려움과-요구사항",
    "title": "📃Reskin-Anyskin 리뷰",
    "section": "3.1 배경: 촉각 센서의 어려움과 요구사항",
    "text": "3.1 배경: 촉각 센서의 어려움과 요구사항\n로봇에게 촉각(tactile)은 인간과 마찬가지로 섬세한 상호작용에 필수적이지만, 현실 로봇 시스템에서 촉각은 시각이나 관성 센서 대비 활용이 저조했습니다. 기존 촉각 센서는 비용이 높고, 플랫폼에 특화된 형태로 제작되며, 일관성(consistency) 문제로 인해 센서를 교체하면 센서 출력 특성이 달라지는 문제가 있습니다. 이러한 변동성 때문에 한 센서에서 학습한 모델을 다른 센서에 그대로 적용하기 어렵고, 부드러운 촉각 센서는 사용 중 마모로 잦은 교체가 필요하기에 더 큰 문제를 야기합니다. 예를 들어 카메라와 마이크는 저렴하고 범용적이며 교체 후에도 동일한 성능을 기대할 수 있지만, 촉각 센서는 제작 공정상의 편차로 샘플마다 반응이 달라 많은 경우 새 센서마다 재학습이 필요했습니다. 이러한 이유로 실용적인 촉각 피부(sensor skin)를 만드는 것이 로봇 조작 분야의 오랜 과제로 남아 있었습니다."
  },
  {
    "objectID": "posts/paper/2025-05-05-reskin-anyskin.html#reskin-자기장-기반-저비용-교체형-촉각-센서",
    "href": "posts/paper/2025-05-05-reskin-anyskin.html#reskin-자기장-기반-저비용-교체형-촉각-센서",
    "title": "📃Reskin-Anyskin 리뷰",
    "section": "3.2 ReSkin: 자기장 기반 저비용 교체형 촉각 센서",
    "text": "3.2 ReSkin: 자기장 기반 저비용 교체형 촉각 센서\nReSkin은 이러한 문제를 해결하고자 2021년에 제안된 저비용(&lt; $30), 교체 가능하고 다용도인 촉각 센서입니다. ReSkin의 핵심 아이디어는 자기장 기반 감지(magnetic sensing)입니다. 부드러운 폴리머 피부에 미세한 자성 입자를 섞어 제조한 얇은 패치(두께 2~3mm)를 사용하고, 이 패치를 조금 떨어진 곳에 배치한 자기 센서들(자력계)로 변형을 측정합니다. 물체가 피부를 누르거나 미는 등 변형이 일어나면 자성 입자의 배열이 바뀌어 주변 자기장 B가 변하고, 이를 다수의 자력계가 읽어들여 힘의 크기와 위치를 추정하는 것입니다. 전자 회로부와 센싱 표면을 분리한 이러한 구조 덕분에, 피부 패치가 마모되면 간단히 교체할 수 있고 회로는 재사용 가능합니다. 또한 폴리머 소재이므로 로봇 손가락, 팔 등 곡면에도 부착하기 쉬워 활용 범위가 넓습니다. ReSkin은 400 Hz 이상의 높은 샘플링속도와 약 1 mm의 위치 해상도(90% 정확도 기준)를 보이며, 50,000회 이상의 접촉 사이클에도 성능이 크게 저하되지 않을 만큼 내구성을 갖추었습니다.\nReSkin의 또 다른 특징은 데이터 기반 모델링입니다. 물리 공식을 일일이 사용하지 않고, 자력계들이 출력하는 다차원 신호 ΔB로부터 접촉 지점 (x, y)과 접촉 힘 F를 바로 예측하는 머신러닝 모델을 학습합니다. 이렇게 하면 센서마다 미세한 특성 차이가 있더라도, 학습을 통해 보정된 모델이 복잡한 관계를 학습하게 됩니다. 다만 초기 ReSkin 연구 당시에도 “새로운 센서에도 일반화되는 모델”을 만드는 것이 중요한 목표로 제시되었습니다. 동일한 제작 공정으로 만든 ReSkin이라도 개체별로 raw 자기장 신호 분포에 차이가 있으며 (논문의 Figure 4b에서 센서마다 동일한 힘 입력에 대한 자기장 변화가 다름을 보여줍니다), 장기간 사용하면 시간이 지남에 따른 드리프트도 발생합니다. 따라서 한 센서에서 학습한 모델을 다른 센서에 바로 적용하면 성능 저하가 발생할 수밖에 없습니다. 실제로 ReSkin 논문에서도 단일 센서로 학습한 모델을 새 센서에 적용했을 때 접촉 위치 예측 정확도가 약 25%에 불과해 크게 떨어지는 것을 확인했습니다. 이 문제를 해결하지 못하면 “교체 가능한 촉각 센서”라는 목표를 이룰 수 없기 때문에, ReSkin에서는 별도의 章을 할애하여 “새로운 센서로의 적응(Adapting to New Sensors)” 방법을 제시했습니다."
  },
  {
    "objectID": "posts/paper/2025-05-05-reskin-anyskin.html#새로운-센서로의-적응-reskin의-self-supervised-학습-기법",
    "href": "posts/paper/2025-05-05-reskin-anyskin.html#새로운-센서로의-적응-reskin의-self-supervised-학습-기법",
    "title": "📃Reskin-Anyskin 리뷰",
    "section": "3.3 🔥 새로운 센서로의 적응: ReSkin의 Self-supervised 학습 기법",
    "text": "3.3 🔥 새로운 센서로의 적응: ReSkin의 Self-supervised 학습 기법\nReSkin이 제안한 센서 교체 시 재학습 부담을 줄이는 기법은 크게 두 가지입니다.\n첫째, 다중 센서 학습입니다. 서로 다른 여러 센서(실험에서는 6개의 회로 보드와 18장의 피부 조합)로부터 데이터를 모아 하나의 통합 모델로 학습시킴으로써, 모델이 다양한 분포의 데이터를 학습하고 일반화 능력을 갖추도록 했습니다.\n둘째, 특징 공간 정규화(feature regularization)를 통해 새로운 센서에서도 특징이 일관되게 맵핑되도록 유도했습니다. 이를 위해 저자들은 Self-supervised triplet loss 함수를 도입했는데, 아이디어는 다음과 같습니다. 센서 출력 B (자기장 변화 신호)를 내부 특징 표현 \\text{feat}(B) 공간으로 맵핑할 때, 피부 위에서 가까운 위치의 두 접촉은 특징 공간에서도 가깝게, 멀리 떨어진 접촉은 특징이 멀어지도록 제약합니다. 구체적으로 한 접촉점을 앵커(anchor)로 잡고, 이와 실제 위치가 가까운 접촉을 positive, 먼 접촉을 negative로 삼아 앵커-포지티브 쌍의 특징거리가 앵커-네거티브 쌍의 특징거리보다 작도록 학습합니다. 이러한 triplet loss L_{\\text{triplet}}은 접촉 위치 레이블 없이도 구성할 수 있는 자가 지도 신호로서, 서로 다른 센서 간에 특징 공간의 위치 민감도를 맞춰주는 역할을 합니다.\ntriplet loss을 활용하면 새로운 센서에 대한 fine-tuning도 수월해집니다. ReSkin을 처음 사용할 때 사용자는 간단한 보정 데이터를 수집하여 모델을 해당 센서에 맞게 미세조정할 수 있습니다. 예를 들어, 펜 끝이나 탐침을 이용해 센서 표면을 직선 경로로 따라 누르면서 이동시키고, 움직인 순서에 따라 데이터를 index 하면 위치 레이블 없이도 순서 정보가 부여됩니다. 이렇게 얻은 연속 눌림 데이터에서 임의의 세 점을 뽑아 삼중항(Anchor, Positive, Negative)으로 구성하면, Positive와 Negative의 기준은 index 순서로 결정할 수 있습니다 (예: 앵커에 더 가까운 index를 positive로). 즉, 사람의 손으로 센서 위를 쓸어내린 기록만 있으면 충분하며 별도의 정밀한 라벨링은 필요 없습니다. 이렇게 수집한 unlabeled 데이터로 triplet loss을 적용하여, 앞서 언급한 다중 센서로 미리 학습된 모델을 해당 새로운 센서의 특징 공간에 맞도록 미세조정합니다. 논문에서는 매 스텝 원래 학습 데이터 배치와 삼중항 배치를 함께 사용하여, 기존 예측 손실(L2 손실)과 triplet loss을 동시에 최소화하도록 모델을 훈련하는 절차를 제시했습니다.\n이 Self-supervised 적응 방법의 효과는 실험으로 입증되었습니다. 다중 센서 데이터로 학습한 모델을 새로운 센서에 적용할 때, 아무 추가 적응 없이 바로 쓰면 접촉 위치 정확도가 80% 초반 수준이었으나, 소량의 적응 데이터만으로도 크게 향상되었습니다. 예를 들어 새로운 센서에서 780개 점을 눌러 얻은 비라벨 데이터로 파인튜닝하면 적응 전보다 성능이 눈에 띄게 좋아졌고, 데이터를 더 늘릴수록 점진적 향상이 있지만 1000개 남짓에서 포화 현상을 보였습니다. 이는 수백 개 수준의 값싼 데이터만으로도 모델이 빠르게 해당 센서에 적응함을 보여줍니다. 또한 훈련에 사용한 센서 개수가 적을수록 (즉 처음부터 다양한 데이터를 못 봤을수록) 이 자가 지도 적응이 주는 이득이 더 컸는데, 반대로 충분히 많은 센서 데이터로 학습한 경우에는 기본 성능이 높아 적응의 개선 폭이 약간 줄어드는 양상도 보였습니다. 한편, 매우 간이한 환경에서도 효과가 있음을 보여주기 위해 별도의 실험을 했는데, 사람 손으로 임의 위치 325번 눌러 수집한 데이터로 적응시킨 결과 접촉 위치 정확도가 약 79.9%에서 84.8%로 상승하고 힘 예측 오차도 줄어들어, 통제되지 않은 환경에서도 방법의 효용이 확인되었습니다. 심지어 센서 하드웨어 형태가 다른 경우에도 적용해보았는데, rigid 보드로 학습한 모델을 유연한(flexible) 회로 보드 형태의 ReSkin에 적응시킨 결과 약 75%의 정확도를 달성했습니다. 유연 보드는 구조상 자력계와 피부 간 거리가 훨씬 가까워 신호 세기가 달라지는 어려운 상황이었지만, 제안한 적응 기법으로 일정 수준 성능을 확보한 것입니다.\n정리하면, ReSkin은 다중 센서 학습과 Self-supervised 적응을 통해 새로운 피부를 교체하더라도 간단한 보정만 거치면 일관된 촉각 인식 성능을 유지할 수 있음을 보여주었습니다. (논문 Table 2에서 자가 적응까지 거친 모델의 접촉 위치 정확도는 87% 수준까지 향상되었습니다.) 이러한 접근은 작업 중간에 센서를 교체해야 하는 현실 시나리오에서 특히 유용하며, 논문 저자들은 이를 통해 ReSkin이 “새로운 피부에도 간단한 적응만으로 학습된 모델을 재사용할 수 있는” 교체형 촉각 센서 솔루션의 가능성을 입증했다고 강조합니다."
  },
  {
    "objectID": "posts/paper/2025-05-05-reskin-anyskin.html#anyskin-플러그앤플레이plug-and-play-촉각-피부의-실현",
    "href": "posts/paper/2025-05-05-reskin-anyskin.html#anyskin-플러그앤플레이plug-and-play-촉각-피부의-실현",
    "title": "📃Reskin-Anyskin 리뷰",
    "section": "3.4 AnySkin: 플러그앤플레이(plug-and-play) 촉각 피부의 실현",
    "text": "3.4 AnySkin: 플러그앤플레이(plug-and-play) 촉각 피부의 실현\nReSkin 이후, 2024년에 발표된 AnySkin은 한걸음 더 나아가 교정(calibration)이나 재학습 없이 센서를 교체해 사용할 수 있는 진정한 플러그앤플레이 촉각 센서를 지향합니다. AnySkin은 기본적인 감지 원리나 전자 하드웨어는 ReSkin의 자기장 기반 설계를 계승하지만, 일관성(consistency) 향상을 위해 센서의 구조적 개선과 제조 공정 표준화에 집중했습니다.\n\n첫째로, 센싱 표면과 전자부의 완전 분리를 구현하면서 자체 접착(self-adhering)이 되고 정렬(self-aligning)이 쉬운 부착 메커니즘을 도입했습니다. 이를 통해 사용자가 AnySkin 패치를 로봇 손가락이나 팔에 마치 휴대폰 케이스를 끼우듯 손쉽게 장착할 수 있고, 매번 같은 위치와 orientation으로 정렬되어 센서 간 편차를 줄입니다.\n둘째로, 자기 신호 세기 강화를 위해 새로운 자성물질과 자화 방법을 적용했습니다. 예컨대 이전보다 강한 자계를 만들어내는 자성 입자와 펄스자화 방식을 활용하여 센서 출력을 증폭함으로써, 환경 노이즈나 개체 간 미세한 차이에 덜 민감하게 했습니다.\n셋째로, 접착제 없는 내구성 높은 제조 공정을 개발하고 이를 누구나 따라할 수 있도록 디자인 툴과 CAD 파일을 공개했습니다. AnySkin 패치는 3D 프린터로 출력한 몰드에 실리콘 재료(예: Smooth-On Dragon Skin 10)와 자성 입자를 부어 만들고, 쉽게 떼어낼 수 있도록 설계되어 반복 사용에 용이합니다. 전체 조립 과정은 약 12초 정도밖에 걸리지 않을 만큼 단순하며, 센서를 새 것으로 바꿔 끼운 뒤에도 이전 센서로 수집한 데이터나 모델을 그대로 활용할 수 있습니다.\n\n이러한 개선 덕분에 AnySkin은 저비용·편의성뿐 아니라 데이터 재사용성(data reusability) 면에서 최초의 성과를 보여주었습니다. 연구진은 AnySkin의 효과를 다양한 로봇 플랫폼과 작업에서 실험으로 검증했는데, 몇 가지 예를 들어보면 다음과 같습니다:\n\n범용성: AnySkin 패치는 평평한 그리퍼부터 사람 손가락 모양의 로봇 손까지 다양한 형태에 적용되었습니다. 실제 Franka эм 로봇팔의 그리퍼, 사족보행 로봇의 지팡이형 끝부분(Dobb-E stick), 그리고 4손가락 로봇핸드(LEAP Hand)의 손가락에 부착하여 모두 정상적으로 작동함을 보였습니다. 이는 센서의 물리적 폼팩터 제약이 거의 없고, 간단히 붙였다 떼었다 할 수 있음을 의미합니다.\n학습된 정책의 유지: AnySkin을 장착한 로봇으로 정교한 조작 과제를 학습시킨 후, 센서만 새 것으로 교체해도 성능이 크게 떨어지지 않았습니다. 예를 들어, 카메라 영상 + 촉각센서로 USB 케이블을 꽂는 동작을 배우게 한 뒤 센서를 바꿔끼워 수행해보면, 성공률 감소가 약 13% 정도에 불과했습니다. 반면 동일한 실험에서 이전 세대인 ReSkin 센서는 43% 가량 성능 감소가 나타났고, 다른 대표적인 촉각 센서인 DIGIT(카메라 기반 젤사이트 센서)의 경우는 원래도 성능 기여가 작았을 뿐 아니라 교체 시 오히려 악화되는 경향을 보였습니다. 즉, AnySkin은 한 센서에서 학습된 모델을 새로운 센서에 거의 그대로 쓸 수 있음을 입증한 것입니다. 아래 그림은 플러그 삽입(Plug Insertion) 작업에서 카메라만 쓴 경우, 카메라+ReSkin, 카메라+AnySkin, 카메라+DIGIT의 조합별 성공률을 비교한 결과입니다. AnySkin을 사용하면 원래 학습에 사용된 센서를 새 것으로 바꿔도 성능 저하가 미미하지만, ReSkin은 교체 후 성능이 크게 떨어지고(사실상 카메라만 쓴 것과 비슷해짐), DIGIT 역시 편차가 심한 모습을 볼 수 있습니다. 이러한 특성 덕분에 AnySkin은 “학습된 정책을 보존한 채 센서를 교체(swap)할 수 있는” 최초의 촉각 센서로 평가됩니다.\n미끄러짐 감지: AnySkin은 물체 미끄럼 감지와 같은 섬세한 접촉 변화도 포착할 수 있습니다. 예를 들어 물체를 잡은 그립이 헐거워져 물체가 미끄러지기 시작하는 초기 징후를 LSTM 기반 모델로 학습시킨 결과, 약 92%의 정확도로 미끄러짐 발생을 탐지했습니다. 이는 로봇 손이 사전에 그립을 조정하거나 힘을 높여 떨어뜨리는 것을 방지하는 데 활용될 수 있습니다. 이 실험에서도 여러 AnySkin 센서를 교체하며 수행했지만 일관된 성능을 보였다고 보고됩니다.\n\nAnySkin의 개발 과정에서 무엇보다 강조된 것은 일관된 센서 응답입니다. 연구 결과에 따르면 AnySkin은 ReSkin에 비해 신호의 센서 간 분산이 획기적으로 감소하였고, 개체별 보정 없이도 큰 추가 오차 없이 동작합니다. 사실상 공장 출하시 보정된 카메라를 교체하는 것과 비슷한 수준의 사용자 경험을 제공하는 것이죠. 이러한 특장점으로 인해 AnySkin 연구는 “로봇 촉각 센싱을 카메라처럼 쉽고 흔하게 만들겠다”는 비전을 향한 중요한 진전으로 평가받고 있습니다."
  },
  {
    "objectID": "posts/paper/2025-05-05-reskin-anyskin.html#결론-및-향후-전망",
    "href": "posts/paper/2025-05-05-reskin-anyskin.html#결론-및-향후-전망",
    "title": "📃Reskin-Anyskin 리뷰",
    "section": "3.5 결론 및 향후 전망",
    "text": "3.5 결론 및 향후 전망\nReSkin과 AnySkin으로 이어지는 연구는 로봇을 위한 실용적 촉각 피부 구현에 있어 큰 진전을 보여주었습니다. ReSkin이 소형·저가·교체가능 촉각 센서의 틀을 제시하고 머신러닝을 통해 마모와 개체차를 보정하는 개념을 도입했다면, AnySkin은 한발 더 나아가 센서 인스턴스 간 zero-shot 일반화를 실현함으로써 사실상 보정 없이 바로 교체해 쓸 수 있는 수준에 도달했습니다. 특히 AnySkin의 경우 기존 상용 촉각센서들과 비교해도 범용성, 유지보수 편의성 측면에서 우위를 보여 대규모 적용의 가능성을 높였습니다. 물론 여전히 풀어야 할 과제도 있습니다. 예를 들어 현재 단일 접촉 지점의 힘과 위치를 예측하는 수준에서, 다중 접촉 지점의 동시 인식이나 보다 복잡한 촉각 정보(예: 촉감 질감 분별) 등은 향후 연구로 남아 있습니다. 또한 센서 교체 주기가 늘어날수록 누적되는 미세한 차이나 시간에 따른 재료 특성 변화 등도 추가로 살펴볼 부분입니다. 그럼에도 불구하고, ReSkin/AnySkin 연구는 데이터 취득부터 학습, 교체에 이르는 촉각 센서 활용 전 주기를 크게 개선한 사례입니다. 이를 통해 로봇 연구자들은 마치 카메라를 다루듯 손쉽게 촉각 센서를 다룰 수 있고, 한 번 모은 촉각 데이터를 반복 활용하여 학습 효율을 높일 수 있게 되었습니다. 궁극적으로 이러한 노력들은 로봇에게 인간 수준의 풍부한 촉각능력을 부여하고, 섬세하고 안전한 물체 조작을 가능케 하는 방향으로 나아가고 있습니다. 앞으로 AnySkin처럼 쉽게 교체하고 확장할 수 있는 촉각 피부가 더 발전하고 상용화된다면, 로봇의 촉각이 시각 만큼이나 당연한 시대가 올 것으로 기대됩니다.\n촉각 센서마다 일관성 없이 반응 특성이 제각각이면, 한 센서로 학습한 모델을 다른 동일한 센서에 적용하기 어렵다. 특히 부드러운 촉각 센서는 마모로 교체가 잦아 이 문제가 더욱 중요하다. ReSkin은 자기장 기반의 소프트 촉각 센서로, 자성 입자를 섞은 얇은 피부와 분리된 자기 센서 회로로 구성된다. 이를 통해 센서 인터페이스를 저비용으로 쉽게 교체 가능하게 설계했다. ReSkin에서 학습한 모델이 새로운 센서에서도 일반화되도록 하는 것은 핵심 과제다. 그러나 동일한 힘을 가해도 센서마다 측정되는 자기장 변화 분포(원시 신호)가 달라, 보정 없이 기존 모델을 쓰면 성능 저하가 발생한다. 이를 완화하기 위해 다중 센서의 데이터를 합쳐 학습하고, 삼중항(self-supervised triplet) 손실로 특징 공간을 정규화한다. triplet loss은 접촉 위치가 가까운 샘플들의 특징을 가깝게 유지하도록 유도하여, 센서 간 일관된 표현을 학습시킨다. 새로운 센서를 쓸 때는 간단한 unlabeled 데이터(예: 센서 표면을 펜으로 따라 그으며 눌러본 기록)를 수집해 triplet loss로 모델을 미세조정할 수 있다. 이렇게 Self-supervised 방식으로 추가 학습하면 레이블이 없어도 해당 센서의 특징 분포에 모델을 맞출 수 있다. ReSkin의 Self-supervised 적응 기법은 통제되지 않은 수동 조작 데이터나 센서 하드웨어 종류가 다른 경우에도 성능 향상을 보였다. 예를 들어 사람 손으로 눌러 얻은 데이터로 적응하면 접촉 위치 예측 정확도가 약 79.9%에서 84.8%로 향상되었고, 경직 보드에서 학습한 모델을 유연 보드에 적용할 때도 75% 수준의 정확도를 달성했다. AnySkin은 ReSkin의 디자인을 발전시켜, 센싱 표면과 전자부를 완전히 분리하고 자기장 세기를 강화하는 등으로 센서 간 응답의 일관성을 크게 높였다. 또한 부착 방식도 셀프 정렬되도록 고안되어 교체 시 정확히 같은 위치와 자세로 장착된다. AnySkin 패치는 교체에 12초 정도밖에 걸리지 않으며, 한 센서로 학습한 모델을 새로운 패치에 그대로 적용해도 플러그 삽입 과제 성공률 저하가 13% 이내로 매우 작았다. 반면 이전 세대인 ReSkin은 같은 조건에서 성능이 40% 이상 떨어져, AnySkin의 센서 간 일관성이 월등함을 보여준다.\n# Reference\n\nReskin 논문\nAnySkin 논문"
  },
  {
    "objectID": "posts/storage/2025-05-29-mag-tactile.html",
    "href": "posts/storage/2025-05-29-mag-tactile.html",
    "title": "🧩uSkin vs ReSkin",
    "section": "",
    "text": "Allegro Hand는 인간 손처럼 정교한 조작을 목표로 개발된 로봇 손으로, 섬세한 물체 조작을 위해 촉각 센서의 통합이 중요합니다. 최근 많은 연구에서는 로봇 손가락에 전자 피부를 부착해 접촉 힘과 미끄러짐 등을 감지하려고 시도하고 있습니다. 특히 Allegro Hand에는 XELA Robotics사의 uSkin 센서와 Meta AI가 개발한 ReSkin 센서가 사용되고 있는데, 두 센서는 자기장 기반의 촉각 센서라는 공통점이 있지만 설계 목적과 구현 방식에서 차이가 있습니다. 아래에서는 이 두 센서의 감도, 정확도, 신뢰성, 응답속도, 제조 방식, 자기장 감지 원리 측면에서 특징을 비교하고, 최근 3년간(2022–2025) 해당 센서들을 활용한 최신 연구 사례들을 정리합니다."
  },
  {
    "objectID": "posts/storage/2025-05-29-mag-tactile.html#서론-allegro-hand와-촉각-센서의-중요성",
    "href": "posts/storage/2025-05-29-mag-tactile.html#서론-allegro-hand와-촉각-센서의-중요성",
    "title": "🧩uSkin vs ReSkin",
    "section": "",
    "text": "Allegro Hand는 인간 손처럼 정교한 조작을 목표로 개발된 로봇 손으로, 섬세한 물체 조작을 위해 촉각 센서의 통합이 중요합니다. 최근 많은 연구에서는 로봇 손가락에 전자 피부를 부착해 접촉 힘과 미끄러짐 등을 감지하려고 시도하고 있습니다. 특히 Allegro Hand에는 XELA Robotics사의 uSkin 센서와 Meta AI가 개발한 ReSkin 센서가 사용되고 있는데, 두 센서는 자기장 기반의 촉각 센서라는 공통점이 있지만 설계 목적과 구현 방식에서 차이가 있습니다. 아래에서는 이 두 센서의 감도, 정확도, 신뢰성, 응답속도, 제조 방식, 자기장 감지 원리 측면에서 특징을 비교하고, 최근 3년간(2022–2025) 해당 센서들을 활용한 최신 연구 사례들을 정리합니다."
  },
  {
    "objectID": "posts/storage/2025-05-29-mag-tactile.html#uskin-vs-reskin-주요-특성-비교",
    "href": "posts/storage/2025-05-29-mag-tactile.html#uskin-vs-reskin-주요-특성-비교",
    "title": "🧩uSkin vs ReSkin",
    "section": "uSkin vs ReSkin: 주요 특성 비교",
    "text": "uSkin vs ReSkin: 주요 특성 비교\nAllegro Hand에 통합된 uSkin과 ReSkin의 핵심 사양을 비교하면 다음과 같습니다. uSkin은 다수의 영구자석-홀 센서 배열로 구성된 상용 촉각 피부이고, ReSkin은 자성 입자 기반의 유연한 센서로 개발되어 공개된 저비용 촉각 피부입니다. 두 센서의 특성을 논문 기반 자료로 항목별로 비교하면 다음과 같습니다:\n\n\n\n\n\n\n\n\n비교 항목\nuSkin (XELA Robotics)\nReSkin (Meta AI & CMU)\n\n\n\n\n감도 (Sensitivity)\n약 10 mN(0.45 kPa) 수준의 미세한 힘까지 감지 가능 – 인간 피부에 비하면 떨어지지만, 로봇 촉각 센서로는 매우 높은 감도. 작은 접촉도 검출 가능하여 섬세한 물체 취급에 유리.\n수십 mN ~ 0.1–0.2 N 정도의 힘 변화를 구별 가능 – 예를 들어 약 20 mL 물의 무게 (~0.2 N) 증가도 센서 출력 변화로 포착. 초기 설계 목표는 0.1 N의 힘 분해능이며, 실험적으로도 그에 준하는 작은 힘을 감지함을 시연.\n\n\n정확도 (Accuracy)\n각 촉각 소자(taxel)별 3축 힘 측정의 정확도가 높음. 개별 센서 단위 보정 시 X, Y, Z축 평균 절대오차가 약 0.2 N 수준까지 달성된 사례가 보고됨. 공간 해상도는 taxel 간격 ~4.7 mm로 촘촘하여 접촉 위치도 비교적 정확히 파악 가능.\n머신러닝 보정을 통해 높은 예측 정확도 확보. 예를 들어 자가-보정(self-supervised) 기법 적용 시 접촉 지점 위치 오차 약 0.7 mm, 힘 크기 추정 오차 약 0.44 N 수준까지 성능 향상이 보고됨. 초기 센서 간 편차가 크지만, 다중 센서 학습과 보정으로 84% 이상의 분류 정확도와 낮은 MSE를 달성함.\n\n\n신뢰성 (Reliability)\n일관된 출력과 내구성을 갖춘 편이나, 강한 외부 자기장에 민감하여 교란을 받을 수 있음. 각 센서는 견고하게 패키징되어 장기간 사용 가능하나, 자석-센서의 조립 편차로 센서마다 보정값 차이가 발생할 수 있음. 제조 공정상 수작업 조립으로 인한 개체간 성능 편차를 정밀 보정하여 사용.\n내구성과 교체 용이성을 고려한 설계. 부드러운 센서 층이 마모되면 쉽게 교체할 수 있고, 한 개의 센서 패드가 5만 회 이상의 접촉에도 성능이 크게 저하되지 않음을 검증. 다만 새로운 센서 막 교체 시마다 미세한 특성 차이가 있으므로, 자체 ML기반 보정으로 센서 간 편차와 시간에 따른 변화에 대응함.\n\n\n응답속도 (Response Time)\n전자식 Hall 센서로 실시간 연속 측정이 가능하여 응답속도가 매우 빠름. 이론적으로 kHz 대역까지도 측정 가능하며, 일반적인 비전 기반 촉각센서(30–60 Hz)에 비해 월등히 높은 샘플링 주파수를 지원. 여러 개의 taxel을 동시에 읽을 때도 수백 Hz 이상의 속도를 유지하여 로봇 제어에 활용할 수 있음.\n고속 샘플링 가능 (설계 목표 ≥100 Hz). 실제 응용에서 250 Hz로 데이터 수집을 시연한 바 있으며, 다수 센서를 연결한 경우에도 100–200 Hz 수준으로 안정적으로 동작함. 응답 시간은 수 ms 단위로 인간 촉각보다도 빠른 편이어서 실시간 피드백 제어에 활용할 수 있음.\n\n\n센서 제조 방식  (Fabrication)\n각 촉각 패드마다 영구자석을 포함한 연성 고무층과 그 아래 소형 Hall IC 칩으로 구성. 4×4 격자 등의 모듈 형태로 제작되어 곡면용(손가락 끝 30 taxel)과 평면용(16 taxel 등) 패드로 제공됨. 자석-엘라스토머 부착과 칩 패키징 공정에 수작업 조립이 필요하며, 이로 인해 생산 단가와 개체 간 특성 편차가 발생하는 문제가 지적됨.\n유연한 실리콘 피부에 무작위 자성 입자를 혼합·경화하여 만드는 얇은 패치형 센서. 제작시 3D 프린팅된 몰드에 입자-실리콘 혼합물을 붓고 외부에서 격자 형태로 자화하여 자기 성질을 부여. 경화된 피부를 회로 기판 위에 부착하여 사용하며, 기판에는 소형 3축 자력계 칩(5개 배열)이 장착되어 있음. 전체 설계 파일과 제조법이 오픈소스로 공개되어 있어 손쉬운 제작과 수정이 가능하며, 센서막과 회로를 분리하여 손상 시 피부만 교체하도록 설계됨.\n\n\n자기장 변화 인식 원리  (Magnetic sensing principle)\n영구자석이 외력에 따라 미세 이동하면서 발생하는 자기장 변화를 바로 아래의 홀 효과 센서가 감지하는 방식. 자석이 눌리거나 밀리면 X, Y, Z 방향 자기장 세기가 변하고, 이를 3축 힘 (법선압 + 전단력) 신호로 변환하여 출력함. 각 taxel이 국부적인 접촉력을 벡터 형태로 측정하므로 물체의 미끄러짐 방향이나 접촉 지형을 파악할 수 있음.\n분말 자석들이 포함된 탄성체 막이 변형될 때 주변에 형성된 자기장의 밀도 분포 변화를 자력 센서들이 읽어내는 방식. 말랑한 피부 자체가 자성을 띠고 있어 접촉에 의해 “찌그러지면” 자력계에 읽히는 자기 신호가 변하며, 이를 사전에 학습된 모델이 분석해 힘의 크기와 위치를 추정함. 센서막이 연속적 분포체이므로 넓은 면적에서도 여러 접점의 위치를 계산적으로 추론할 수 있다는 장점이 있음.\n\n\n\n주석: 위 표의 내용은【9】【13】【17】【19】【21】【22】【25】【32】 등의 출처에서 발췌 및 요약한 것입니다."
  },
  {
    "objectID": "posts/storage/2025-05-29-mag-tactile.html#최근-연구-동향-uskin-및-reskin-활용-사례-20222025",
    "href": "posts/storage/2025-05-29-mag-tactile.html#최근-연구-동향-uskin-및-reskin-활용-사례-20222025",
    "title": "🧩uSkin vs ReSkin",
    "section": "최근 연구 동향: uSkin 및 ReSkin 활용 사례 (2022–2025)",
    "text": "최근 연구 동향: uSkin 및 ReSkin 활용 사례 (2022–2025)\n최근 3년간 uSkin 또는 ReSkin 센서를 활용한 대표적인 연구들을 분야별로 정리하면 다음과 같습니다. 각 연구는 촉각 센서의 데이터를 활용하여 로봇의 물체 인지나 조작 능력을 향상시키는 방향으로 이루어지고 있으며, 특히 자기장 기반 촉각센서 + 머신러닝의 결합이라는 공통된 흐름을 확인할 수 있습니다.\n\nuSkin 센서 활용 연구 사례\n\n로봇 그립 미끄럼 감지 (그립 안정성 판단) – “A Model-Free Approach to Fingertip Slip and Disturbance Detection for Grasp Stability Inference” (Kitouni 등, 2023). 이 연구에서는 Allegro Hand의 모든 손가락에 uSkin 촉각 피부를 부착하여 물체를 잡은 상태에서 발생하는 미끄럼(Slip) 및 외부 방해를 감지하였습니다. 총 368개의 3축 촉각 소자가 손바닥과 손가락 마디, 손가락 끝을 덮도록 배치되었으며, 별도의 복잡한 보정 없이 센서 출력 신호의 변화 패턴만으로 미끄럼 여부를 판별하는 모델 프리 접근법을 제안했습니다. 다양한 정밀 쥐기 실험을 통해 제안된 지표가 손가락별 미끄럼 불안정성을 잘 나타냄을 보였고, 이를 활용해 개별 손가락에 능동적인 안정화 피드백을 줄 수 있음을 확인하였습니다. 해당 결과는 로봇이 물체를 놓치기 전에 촉각으로 미끄러짐을 탐지하여 그립을 조정하는 전략에 기여합니다.\n전체 손 촉각 힘 추정 및 제어 – “Interaction force estimation for tactile sensor arrays: toward tactile-based interaction control for robotic fingers” (Chelly 등, 2024). 본 연구는 Allegro Hand에 부착된 다수의 uSkin 센서를 일괄 보정하여 전역적인 3차원 힘 분포를 추정하고, 이를 로봇 제어에 직접 통합한 사례입니다. 저자들은 평면 패드와 곡면 패드가 혼합된 복잡한 배열의 Xela uSkin 촉각 피부를 한 번의 데이터 수집으로 효율적으로 보정하는 데이터 효율적 캘리브레이션 기법을 제안하였습니다. 보정된 촉각센서 배열로부터 얻은 정확한 접촉력 추정치를 로봇 손가락의 상호작용 힘 제어(loop)에 입력하여, 외부 힘을 일정하게 유지하거나 제한하는 힘 제어 작업을 구현하였습니다. 실험 결과, 제안 기법은 센서 배열 전반에 걸쳐 평균 0.1–0.2 N 수준의 오차로 힘을 재구성할 수 있었고, 이를 이용한 힘 조절이 가능한 것을 보여주어 섬세한 힘 조절이 요구되는 작업(예: 깨지기 쉬운 물체 잡기)에 유용한 접근임을 시사했습니다.\n자가 지도학습 기반 촉각표현 학습 – “Self-supervised perception for tactile skin covered dexterous hands (Sparsh-skin)” (Sharma 등, 2025). 이 연구는 자기장 기반 촉각 피부의 복잡한 시계열 신호로부터 의미 있는 표현을 학습하기 위한 자기 지도(self-supervised) 학습기법을 제안했습니다. Allegro Hand의 손바닥, 손가락 마디, 손가락 끝에 걸쳐 Xela uSkin 센서를 분산 배치하여 약 4시간 분량의 다양한 접촉 데이터를 수집한 뒤, 이를 표준화된 표현 공간으로 인코딩하는 프리트레인드(tactile encoder) 모델인 Sparsh-skin을 개발하였습니다. 학습된 촉각 인코더는 unlabeled 데이터로 사전학습되었기 때문에 이후 새로운 작업에 소량의 학습 데이터로 빠르게 적응할 수 있다는 장점을 보였습니다. 실제로 물체 식별 등의 다운스트림 과제에서 기존 엔드투엔드 학습 대비 41% 높은 성능과 향상된 데이터 효율을 달성하여, 촉각 기반 객체 인식이나 미세 동작 제어에 유용한 일반 목적 촉각 표현을 얻을 수 있음을 입증했습니다. 이는 복잡한 자기장 촉각센서 신호를 해석하는 데 있어 학습 기반 접근의 가능성을 보여주며, 향후 인간 수준의 촉각인지 능력을 로봇에 부여하는 데 중요한 단계를 제시합니다.\n\n\n\nReSkin 센서 활용 연구 사례\n\n패브릭(천) 다층 분리 조작 – “Learning to Singulate Layers of Cloth using Tactile Feedback” (Tirumala 등, IROS 2022). 이 연구는 옷감이나 천 여러 장이 포개진 더미에서 로봇이 맨 윗장 한두 장만 집어올리는 어려운 작업에 촉각 센서 ReSkin을 활용한 사례입니다. 프랑카(Franka) 로봇 팔의 그리퍼 손가락 중 하나에 ReSkin 촉각 패드를 부착하고, 해당 센서로부터 얻은 촉감 데이터를 기반으로 현재 잡은 천의 **겹 수(layer 수)**를 판별하는 머신러닝 분류기를 학습했습니다. 학습된 분류기를 로봇 제어에 통합하여, 그리퍼가 너무 많은 층을 잡았을 때 살짝 놓아서 한 층만 잡도록 높이를 자동 조정하는 정책을 실행했습니다. 총 180회의 실제 로봇 실험 결과, 촉각을 활용하지 않은 기존 시각 기반 접근보다 훨씬 높은 정확도로 한 겹 혹은 두 겹의 천을 구분하여 집어올리는 데 성공했고, 보지 못한 새로운 종류의 천에 대해서도 일반화 성능이 향상되었음을 보였습니다. 이는 ReSkin의 미세한 촉각 신호가 옷감의 두께나 결합 상태를 잘 감지함을 보여주며, 기존에 시각으로 어려웠던 섬세한 섬유 조작 작업에 촉각 센서가 유용함을 입증한 사례입니다.\nReSkin의 개선 및 일반화 (AnySkin) – “AnySkin: Plug-and-play Skin Sensing for Robotic Touch” (Bhirangi 등, arXiv 2024). 이 연구는 ReSkin의 개념을 발전시켜 더 강한 자장, 부착 편의성, 센서 간 일관성을 향상시킨 AnySkin이라는 신형 촉각 센서를 소개하였습니다. AnySkin은 ReSkin과 동일하게 자기 입자 기반이지만, 자체 정렬되고 접착제 없이 부착 가능한 설계를 도입하여 로봇 표면 어디에나 붙이기 쉽게 만들었습니다. 또한 센서 간 응답 편차를 줄여, 한 센서에서 학습된 모델이 별도 재보정 없이 다른 센서에도 바로 적용될 수 있음을 시연하였습니다 (cross-instance generalization). 논문에서는 AnySkin을 이용한 미끄러짐 감지와 강화학습 기반 접촉 정책 학습 실험을 수행하여, 이전 ReSkin 대비 향상된 감도와 내구성을 보이며 동시에 데이터 재사용성을 구현함을 보였습니다. 이 결과는 ReSkin이 제시한 저가형 촉각 피부의 가능성을 한층 확장한 것으로, 로봇 촉각 센서의 범용성을 높이는 방향의 중요한 진전으로 평가됩니다."
  },
  {
    "objectID": "posts/storage/2025-05-29-mag-tactile.html#결론-및-시사점",
    "href": "posts/storage/2025-05-29-mag-tactile.html#결론-및-시사점",
    "title": "🧩uSkin vs ReSkin",
    "section": "결론 및 시사점",
    "text": "결론 및 시사점\nuSkin과 ReSkin은 각각 정밀도 높은 상용 촉각센서와 저비용 유연 촉각피부를 대표하며, 최근 연구들은 이들 센서를 로봇에 적용하여 섬세한 조작 기술을 향상시키는 방향으로 전개되고 있습니다. 요약하면, uSkin은 높은 해상도의 3축 힘 측정 능력을 바탕으로 로봇손의 접촉력 제어나 미끄럼 감지와 같은 문제에 활용되고 있으며, ReSkin은 저렴하고 교체가 쉬운 강점을 살려 복잡한 변형 물체의 촉각 인지나 웨어러블 로봇피부 등 확장성 있는 응용에 쓰이고 있습니다. 더욱이 두 센서 모두 머신러닝 기법과 결합되면서 센서 신호 보정, 특징 표현 학습, 정책 학습 등 다양한 측면에서 혁신이 이루어지고 있습니다. 이러한 동향은 향후 로봇이 사람처럼 섬세한 촉각을 활용하고, 여러 형태의 촉각 센서를 효과적으로 통합하여 보다 지능적인 상호작용을 구현하는 데 중요한 밑거름이 될 것으로 기대됩니다.\n참고 자료: 본 답변에서는 arXiv, IEEE, Science Robotics 등 신뢰할 수 있는 출처의 논문 및 기사【9】【11】【13】【17】【19】【21】【22】【25】【27】【31】【32】【37】【39】【42】【49】를 인용 및 참고하였습니다. 각 번호는 해당 출처를 가리키며, 자세한 내용은 인용문 옆 괄호의 번호를 통해 확인할 수 있습니다.\n\n\nEnglish\n\nGot it. I’ll translate the full content into English, including the detailed comparison of uSkin and ReSkin sensors, recent research cases using them, and all structured content like tables and analyses.\nI’ll present the translated report shortly."
  },
  {
    "objectID": "posts/storage/2025-05-29-mag-tactile.html#introduction",
    "href": "posts/storage/2025-05-29-mag-tactile.html#introduction",
    "title": "🧩uSkin vs ReSkin",
    "section": "Introduction",
    "text": "Introduction\nTactile sensing is crucial for dexterous robotic manipulation, providing rich feedback about contact forces, textures, and object properties that vision alone cannot capture. The Allegro Hand – a four-fingered, 16-DOF robotic hand – has become a popular platform for evaluating advanced tactile sensors in research. Two notable tactile skin technologies used with the Allegro Hand are uSkin (developed by XELA Robotics) and ReSkin (developed by Meta AI in collaboration with Carnegie Mellon University). This report presents a detailed comparison of uSkin and ReSkin in terms of sensitivity, accuracy, reliability, response speed, fabrication methods, and their underlying magnetic field sensing principles. It also reviews recent research (2022–2025) that employs each sensor, highlighting applications and outcomes. The goal is to provide robotics researchers with a clear understanding of each sensor’s capabilities and trade-offs in an academic, technical context."
  },
  {
    "objectID": "posts/storage/2025-05-29-mag-tactile.html#overview-of-the-tactile-sensors",
    "href": "posts/storage/2025-05-29-mag-tactile.html#overview-of-the-tactile-sensors",
    "title": "🧩uSkin vs ReSkin",
    "section": "Overview of the Tactile Sensors",
    "text": "Overview of the Tactile Sensors\n\nuSkin Sensor (XELA Robotics)\nuSkin is a high-density 3-axis tactile sensor system packaged in a thin, soft silicone skin. It integrates an array of small sensing units (taxels) that can each detect forces in three dimensions: normal pressure (Z-axis) and shear forces (X and Y axes). The uSkin design embeds tiny magnets in the soft skin and uses underlying magnetometers (or Hall-effect sensors) to track the magnets’ movements under deformation. Each taxel’s magnetic field readings in X, Y, Z change as forces are applied, allowing the system to compute a 3-axis force vector at that point. Because the sensors are distributed in a grid, uSkin provides spatially localized force data across the contact surface (for example, an Allegro fingertip can be covered with ~24 taxel units). The sensor outputs are digital, minimizing noise and eliminating the need for bulky analog wiring or external ADC boards. In practice, uSkin can be integrated into new or existing robots with minimal wiring and straightforward mounting (e.g. glued onto robot fingers or palm). XELA offers flat patch sensors, curved fingertip sensors, and other form factors to cover various robot hand surfaces. Overall, uSkin provides a turnkey tactile sensing solution with high resolution and direct force readouts per taxel, making it suitable for precise manipulation tasks.\n\n\nReSkin Sensor (Meta AI & CMU)\nReSkin is an open-source tactile “skin” that uses a flexible polymer embedded with magnetic particles to sense touch. The ReSkin concept is to create a low-cost, replaceable tactile layer that can be applied to robot hands (or other surfaces) like an electronic skin. The sensor consists of a thin (~2–3 mm) silicone elastomer sheet with randomly distributed microscopic magnetic particles. This magnetic sheet is placed over a small magnetometer chip. When the skin is pressed or deformed, the pattern of magnetic field at the magnetometer changes because the particles move (“squooshed”) within the elastomer. Machine learning is then used to map these field distortions to contact force magnitudes and locations. ReSkin’s design prioritizes simplicity and versatility: the sensing hardware (magnetometer and electronics) is kept separate from the soft skin. The skin contains no wires or electronics; it can be peeled off and replaced like a Band-Aid when worn out. This makes the part most susceptible to damage very easy and cheap to replace (each skin costs on the order of &lt;$30 in materials). ReSkin can be cut or shaped to cover different surfaces – from a robot fingertip to an entire glove or even a dog’s paw – providing a conformal tactile sensing layer. The open-source release includes instructions for fabrication (mixing and curing the magnetic silicone) and pre-trained models for interpreting the sensor signals. In summary, ReSkin offers a flexible, low-cost tactile sensing approach that leverages magnetic field changes and learned models to detect touch across a continuous surface."
  },
  {
    "objectID": "posts/storage/2025-05-29-mag-tactile.html#comparison-of-performance-and-design",
    "href": "posts/storage/2025-05-29-mag-tactile.html#comparison-of-performance-and-design",
    "title": "🧩uSkin vs ReSkin",
    "section": "Comparison of Performance and Design",
    "text": "Comparison of Performance and Design\nTo clearly contrast uSkin and ReSkin, this section compares their key specifications and performance metrics:\n\nSensitivity and Resolution\nSensitivity refers to the smallest force the sensor can reliably detect, and resolution includes the spatial granularity of touch detection. uSkin is engineered for high sensitivity – it can detect forces as light as about 0.1 gram-force (gf) (≈0.001 N). This extremely low force threshold means uSkin can register very slight touches or contact, making it suitable for delicate manipulation where precise force control is needed. In contrast, ReSkin’s target sensitivity is on the order of &lt;0.1 N (≈10 gf) for force detection. However, with careful calibration and machine learning models, ReSkin can achieve very fine force resolution: experiments have demonstrated force measurement errors as low as ~0.005 N (5 mN) in controlled settings. This indicates that ReSkin, despite its simple hardware, can discern minute forces after training, although its native (untrained) sensitivity may be lower than uSkin’s.\nIn terms of spatial resolution, uSkin provides a grid of discrete sensing points (taxels). For example, each uSkin “4x4” patch contains 16 sensing points, and a curved uSkin on an Allegro fingertip uses 24 sensor chips covering the finger pad. The spacing between taxels (a few millimeters apart) defines how finely the contact location can be distinguished – essentially on the order of the taxel pitch. ReSkin, by contrast, behaves like a continuous skin. A single ReSkin patch (roughly the size of a coin) can localize contacts with a spatial accuracy of about 1 mm (with ~90% accuracy) after training. In a benchmark test, ReSkin achieved ~99.6% accuracy in classifying contact locations within ±1 mm on its surface. This suggests that, when calibrated, ReSkin can provide very high spatial detail of where a touch occurs, potentially finer than the discrete spacing of uSkin’s taxels. The trade-off is that uSkin’s taxels give direct physical correspondence to locations, whereas ReSkin’s localization comes from an inference model. In summary, both sensors offer excellent sensitivity and spatial resolution for robotics use: uSkin has an ultra-low force threshold and inherently structured high-density sensing points, while ReSkin achieves comparable force and location resolution through machine-learning-assisted sensing.\n\n\nAccuracy of Force Measurement\nAccuracy encompasses how reliably the sensor can quantify the magnitude and direction of applied forces. uSkin’s design yields direct 3-axis force readings at each taxel, but these raw readings require calibration to map sensor units to physical force values. When properly calibrated, uSkin can measure forces in Newtons and serve in control feedback loops. For instance, a recent study calibrated an Allegro Hand’s uSkin sensors against a force-torque sensor and achieved force estimation errors of around 0.12 N (±0.08 N) in a closed-loop grasping task. This indicates that uSkin can accurately measure and regulate contact forces to within a few hundred millinewtons during manipulation. Its repeatability and linearity benefit from the stable positioning of magnets and sensors in each module. Moreover, uSkin’s on-board digital electronics reduce noise, improving measurement consistency. One challenge for accuracy, however, is dealing with external magnetic interference or drift (addressed later), which XELA mitigates via software compensation for certain models.\nReSkin’s accuracy heavily relies on its learned model. The raw magnetic readings from a ReSkin patch are not directly interpretable as force without a mapping. With a well-trained neural network, ReSkin has demonstrated impressively accurate force reconstruction: in one evaluation, the mean squared error in normal force prediction was on the order of (5 × 10−3 N)2, corresponding to just a few millinewtons error. Additionally, ReSkin is capable of sensing shear forces; a test for dynamic shear contact showed it could predict tangential forces (F_x, F_y) with MSE ~0.0011 N^2, while maintaining normal force error ~0.003 N^2. These results underscore that ReSkin, despite using a single magnetometer, can accurately capture multi-axis force information when aided by machine learning. The limitation is that the accuracy is only as good as the model’s calibration and training data – any change in the skin (replacement or drift over time) can degrade performance if not accounted for. The original ReSkin paper noted that models trained on one sensor did not generalize to other sensors without adaptation, due to instance variability. Recent improvements (see “AnySkin” below) aim to reduce this variability. In summary, uSkin offers direct, hardware-defined accuracy which can be high after one-time calibration, whereas ReSkin offers model-based accuracy which can reach very high levels but requires ongoing calibration and learning algorithms to maintain.\n\n\nReliability and Durability\nReliability covers the sensor’s longevity and consistency of performance over time, especially under repetitive use. uSkin is built as a durable tactile array: its soft silicone and internal structure are designed to handle repetitive contacts and even overload conditions without permanent damage. The silicone skin not only protects the internal sensor elements but also allows slight conformity, distributing stress. XELA specifies that uSkin can sustain up to a certain maximum normal force (e.g. 450 gf for one model, or up to 1500 gf for newer models) without damage. In manipulation tasks, uSkin-covered fingers have been shown to handle fragile objects reliably without harming them. Wear and tear on uSkin is relatively low since the sensor is an integrated unit – there are no loose particles or fluids – and it’s sealed to prevent dust or moisture ingress. Many researchers have used the same uSkin sensors for thousands of grasp cycles; as long as the silicone and wiring remain intact, the performance remains stable. On the other hand, if a uSkin module does fail or break, it is a specialized hardware piece that must be replaced (which can be costly, as high-end tactile sensors often are). XELA advertises their product as more affordable than some competitors like the BioTac (&gt;$1000) while still not compromising performance, but it is certainly more expensive than DIY solutions.\nReSkin emphasizes replaceability as a core feature of reliability. The magnetic skin can undergo many touches: tests showed the machine learning model remained accurate even after 50,000 interactions on the same piece of skin. Eventually, however, the silicone skin will degrade (e.g. tiny cracks, particle loss, or reduced elasticity) after extensive use. Instead of requiring a complex sensor replacement, the worn skin can simply be peeled off and a new one attached, restoring the sensor to like-new performance. This concept makes ReSkin robust in a maintainable way – any damage to the surface (cuts, abrasion) is not catastrophic, because the skin is a cheap consumable. Another aspect of reliability is the sensor’s consistency over time and across units. Early versions of ReSkin saw variability between different fabricated skins and drift in signals over time (as the elastomer properties changed slightly). To combat this, the designers suggest periodic re-calibration (collecting a zero-load magnetic reading occasionally) and have developed improved fabrication methods. In 2024, an improved variant called “AnySkin” introduced a post-curing magnetization process and self-aligning skins to achieve more uniform magnetic particle distribution and secure attachment to the magnetometer. These improvements greatly reduced variability between sensor instances and prevented performance loss due to misalignment or peeling over time. In summary, ReSkin’s reliability comes from its easy renewability and ongoing model adaptation, whereas uSkin’s reliability stems from a robust physical design that maintains performance over a long service life with minimal intervention.\n\n\nResponse Time and Sampling Rate\nRapid response and high sampling frequency are important for capturing dynamic contact events (e.g. slip, impact) and for tight control loops. uSkin provides real-time readings via a digital interface (often CAN or USB converter) and supports high sampling rates. The standard uSkin modules can sample at up to 500 Hz (2 ms interval) on certain models. In many experiments, users run uSkin at 100 Hz due to external constraints or sufficient bandwidth, but the hardware is capable of faster updates for more demanding applications. The internal latency of uSkin’s sensors is low, since it uses direct electrical readings of magnetic field changes with minimal filtering. This allows reactive control – for example, a 100 Hz control loop using uSkin feedback was successfully implemented for force control in a dexterous hand.\nReSkin is also designed for high temporal resolution. The magnetometer can be read at rates up to around 400 Hz (as reported in the initial paper), and potentially faster with optimized hardware. The actual throughput may depend on the microcontroller or interface used, but the goal was to exceed 100 Hz, which ReSkin achieved. Because ReSkin uses a learning pipeline, one consideration is the computational delay for the model to infer forces from magnetic readings. In practice, this inference can be made lightweight (e.g. a small multi-layer perceptron) and run in a few milliseconds, so the end-to-end latency remains low. The ReSkin authors demonstrated real-time use of the sensor (e.g. detecting slips or impacts) without lag, suggesting the response is fast enough for most robotic tasks. Both uSkin and ReSkin thus meet the requirements for real-time tactile feedback, with high-frequency data streams. If comparing, uSkin’s fixed hardware sampling (500 Hz digital output) might offer a slight edge in raw speed and noise immunity, whereas ReSkin’s practical speed (~400 Hz) is comparable and has been validated in closed-loop tasks as well. In either case, both sensors can capture fine contact events (on the order of a few milliseconds), far exceeding slower vision-based tactile sensors (which often run at 30–60 Hz).\n\n\nFabrication Methods and Integration\nThe fabrication and integration process for these sensors differ markedly due to one being a commercial product and the other a DIY solution. uSkin’s fabrication is proprietary to XELA Robotics – it involves assembling small PCBs or sensor chips with embedded magnets and encapsulating them in silicone. Each taxel likely contains a tri-axial magnetometer or Hall sensor aligned with a small magnet in the silicone layer above. The exact manufacturing steps (e.g. how the magnets are embedded and calibrated) are not publicly detailed, but the outcome is a durable sensor sheet with built-in wiring. uSkin modules come with connectors and can be daisy-chained or attached around a robotic finger. Integration is straightforward: the sensors output digital data (e.g. via I2C or SPI through a hub) so one only needs to attach a lightweight cable from the robot hand to a data acquisition board (like XELA’s CAN-to-USB interface). Mechanically, uSkin patches can be glued onto robot surfaces or affixed with screws/brackets depending on the model. The ability to customize shapes (flat, curved, wrap-around) means one can cover complex geometries (fingertips, phalanges, palm) by using multiple uSkin pieces. For example, covering an Allegro Hand might use a curved uSkin on each fingertip and flat patches on each finger link and palm. Because uSkin is a commercial solution, robotics labs often opt for it when they need a plug-and-play tactile array with vendor support, rather than investing time in sensor fabrication.\nReSkin’s fabrication is deliberately simple and accessible. To create a ReSkin sensor, one mixes a two-part silicone rubber with microscopic magnetic particles (like iron oxide or neodymium powder) in a mold to form a thin sheet. After curing, this elastomer is magnetized – early methods involved curing in a magnetic field to align particles, but this was tricky and led to variability. Updated methods use a pulse magnetizer after curing to uniformly magnetize the particles without needing a field during the curing process. The result is a flexible skin with randomly oriented magnetic dipoles. The electronics for ReSkin consist of a small PCB with a magnetometer (typically a 3-axis magnetometer chip) and possibly a microcontroller to read the magnetometer and stream data. This PCB is placed directly under the silicone skin (it can even be embedded or held by a fixture). A critical integration aspect is keeping the skin positioned relative to the sensor – AnySkin research introduced self-adhering skins that clip or snap in place, avoiding glue that can peel. In practice, attaching ReSkin to a robot might involve mounting the tiny magnetometer board on a robot finger and then stretching or securing the silicone patch over it like a thimble or sleeve. The flexibility of ReSkin’s form means it can cover curved or large areas by using multiple magnetometers under one continuous skin, or tiling multiple units. Fabrication time is short (a few hours to mold and cure a batch of skins), and the cost is very low per skin (tens of dollars or less). This makes ReSkin attractive for projects that need many sensors or large areas, as one can fabricate and replace skins as needed. The trade-off is that integrating ReSkin also requires developing or using a ML model for the specific robot application, which adds a layer of complexity in software.\n\n\nSensing Principle: Magnetic Field-Based Detection\nBoth uSkin and ReSkin rely on magnetic field sensing at their core, but the configuration and principles of operation differ:\n\nuSkin: Each taxel in uSkin is essentially a miniaturized magnetic tactile sensor: a small magnet is embedded in the deformable skin, and directly beneath it is a magnetometer that measures the magnet’s field in 3 axes. In the undisturbed state, the magnet’s field at the sensor has a known baseline. When an external force presses on the skin at that taxel, the magnet moves (e.g. gets displaced or tilted) relative to the sensor. This causes changes in the magnetic field readings (ΔB_x, ΔB_y, ΔB_z). These changes are correlated to the force vector applied – for instance, a normal press might move the magnet closer to the sensor (increasing |B_z|), while a shear force might shift it laterally (changing B_x, B_y). Through calibration, uSkin converts the raw magnetic readings into an X, Y, Z force reading for each taxel. The key aspect is direct physical mapping: the sensor is designed so that magnetic field changes correspond in a roughly one-to-one manner with force components. Because the magnets are fixed in known positions and each taxel is independent, the interpretation of the signals is straightforward (often a polynomial or linear map for each axis). uSkin’s use of magnetics provides a contactless sensing mechanism (no electrical contacts at the surface) and allows the sensor to be thin and compliant. However, it also means the readings can be affected by external magnetic fields or nearby ferromagnetic objects. XELA addresses this by offering magnetic interference compensation, using reference sensors or software filters to subtract out background field disturbances. In essence, uSkin’s principle is a localized magnetic displacement sensor at each grid point.\nReSkin: The ReSkin approach uses a distributed magnetic field perturbation principle. Instead of discrete magnets, the entire elastomer sheet contains a random dispersion of tiny magnetic particles. The magnetometer under the skin measures the combined magnetic field from all these particle dipoles. When the skin is not touched, this field has a stable baseline profile. When contact occurs, a region of the skin deforms – particles in that region get slightly closer to the sensor or reorient, altering the field. Crucially, the relationship between a given touch (with certain force and location) and the magnetometer reading is highly complex, since many particles contribute to the field signal. Therefore, ReSkin relies on a learned mapping: a data-driven model (often a neural network) is trained on known indentations to predict the contact location and force from the raw magnetic field readings. The model effectively decodes the pattern of field changes into meaningful tactile information. The benefit of this method is that a single small sensor can cover a relatively large area of skin and sense forces at any point in that area. The drawback is that the magnetic field signal is an entangled representation – without the model, one cannot directly obtain force/position. ReSkin’s magnetic sensing principle is thus a global sensing mode: every touch influences the overall field measurement, but in different ways, and the ML model disentangles them. This principle also means that if the skin shifts relative to the magnetometer or if the magnetic particle distribution changes (due to wear or a new skin), the mapping might need recalibration. Recent efforts like AnySkin aim to make the field more consistent (e.g. uniform particle distribution via post-magnetization) so that the same model can work across replacements. Another consideration is environmental magnetic noise – like uSkin, ReSkin can be affected by strong external magnets or fields. Users must ensure the sensor’s baseline is recorded and possibly apply filtering for stray field fluctuations (some approaches include taking a no-contact reading before each use to serve as a reference). In summary, ReSkin’s magnetic detection principle is a one-to-many mapping (one sensor reading to many possible contacts, resolved by learning), whereas uSkin’s is many one-to-one mappings (each taxel sensor responds to forces mostly at its own location).\n\nThe table below summarizes the key differences between uSkin and ReSkin:\n\n\n\n\n\n\n\n\nAspect\nuSkin (XELA)\nReSkin (Meta/CMU)\n\n\n\n\nSensing Principle\nLocal magnetic displacement at many discrete 3-axis taxels (each taxel: magnet + Hall sensor). Direct mapping from magnet movement to force per taxel.\nGlobal magnetic field distortion measured by one/few magnetometers. Requires ML model to infer contact force and position.\n\n\nSensitivity\n~0.1 gf (0.001 N) threshold for force detection (very light touch). High sensitivity due to precise magnet sensor coupling.\nAimed for &lt;0.1 N detectable force; with calibration, achieved ~0.005 N force resolution in tests. Slightly less sensitive natively, but improved by ML averaging.\n\n\nSpatial Resolution\nDiscrete taxel spacing (e.g. 16–24 sensors per fingertip) gives a few mm resolution; each taxel provides localized 3D force data.\nContinuous skin with ~1 mm contact localization accuracy after training. Can detect multiple contact points if using multiple magnetometers or sequential touches, but typically one contact at a time per patch.\n\n\nAccuracy\nOutputs calibrated force readings per taxel; requires calibration but then reliable (e.g. ~0.1–0.2 N error in practice). Minimal drift; interference compensated via software.\nHigh accuracy with trained model (99% location accuracy, few mN force error in controlled settings). Must retrain or adapt model if skin changes or drifts over long term.\n\n\nResponse Time\nUp to 500 Hz sampling rate (2 ms); low-latency digital output. Suitable for fast control loops (used at 100 Hz in hand control experiments).\nTested up to ~400 Hz update rate; real-time ML inference feasible (few ms). Effective for dynamic tasks (slip detection, impacts) with slight computational overhead.\n\n\nReliability & Durability\nRobust build – soft but resilient; handles overloads without damage. Long-lived hardware; no consumable parts (aside from eventual wear on skin after extensive use). Susceptible to strong external magnetic fields (mitigated by compensation).\nSkin lasts ~50k interactions before degradation. Inexpensive, user-replaceable skins make maintenance easy. Performance consistent if skin is replaced and model updated occasionally. Sensitive to magnetic misalignment or drift; new designs (AnySkin) improve consistency.\n\n\nFabrication & Integration\nProprietary manufacturing; purchase from XELA. Available in flat, curved, bendable formats for integration. Attaches via glue or brackets; requires XELA interface for data. Higher cost per unit, but ready to use out-of-box.\nDIY fabrication from silicone + magnetic powder (open-source specs). Simple electronics (1 magnetometer + microcontroller per patch). Highly affordable (&lt;$30 per sensor). Flexible placement on robot, but requires ML software integration for use.\n\n\n\nTable: Feature comparison of uSkin and ReSkin tactile sensors."
  },
  {
    "objectID": "posts/storage/2025-05-29-mag-tactile.html#recent-research-applications-20222025",
    "href": "posts/storage/2025-05-29-mag-tactile.html#recent-research-applications-20222025",
    "title": "🧩uSkin vs ReSkin",
    "section": "Recent Research Applications (2022–2025)",
    "text": "Recent Research Applications (2022–2025)\nBoth uSkin and ReSkin have been employed in a variety of research projects in robotics, particularly in areas like object manipulation, force control, and haptic perception. Below we summarize selected recent studies that showcase each sensor in use, including the research context, how the sensor was implemented, and key findings:\n\nStudies Utilizing uSkin in Robotics Research (2022–2025)\n\n\n\n\n\n\n\n\n\nStudy & Year\nApplication / Field\nExperimental Setup with uSkin\nKey Outcomes\n\n\n\n\nKulkarni et al., 2024 – “Tactile Object Property Recognition Using Geometrical Graph Edge Features and MT-GCN” (RA-L/IROS 2024)\nObject property recognition (shape/texture stiffness classification)\nAllegro Hand fully covered with uSkin sensors on all fingertips, phalanges, and palm. Tactile readings (1168 channels total) fed into a multi-thread Graph Convolutional Network to learn object features.\nIntegrating high-density uSkin data enabled the GCN to recognize objects’ properties with high accuracy. The proposed method outperformed baseline models, confirming that rich tactile input (3-axis forces from uSkin) improves multi-fingered object classification. It demonstrated effective identification of various object features solely through touch, validating uSkin’s value for complex perception tasks.\n\n\nChelly et al., 2024 – “Tactile-based Force Estimation for Interaction Control with Robot Fingers” (arXiv preprint 2024)\nPrecision force control in dexterous manipulation\nAllegro Hand instrumented with Xela uSkin on each finger. The uSkin taxels were calibrated against an ATI Nano17 force/torque sensor to learn mapping from magnetic readings to actual force (per taxel). Used in a closed-loop admittance controller at 100 Hz.\nAchieved reliable real-time force feedback control: the robot maintained desired contact forces with only ~0.12 N error margin. Demonstrated that uSkin can provide accurate enough force sensing to serve in feedback loops for delicate tasks (e.g., holding an object with constant force). Validated the stability and responsiveness of uSkin-based control, highlighting the sensor’s utility in enhancing manipulation precision.\n\n\nFunabashi et al., 2022 – “Covering a Robot Hand with uSkin for Object Manipulation” (previous study referenced in Kulkarni 2024)\nGeneral grasping and tactile sensing integration\n(Details inferred from context) Allegro Hand with uSkin on all contact surfaces, similar to above. Focus on integrating tactile data into manipulation strategies.\nProvided early evidence that full-hand tactile coverage with uSkin improves manipulation. Likely showed the feasibility of retrofitting Allegro Hand with uSkin and using its readings for tasks like grip adjustment or slip detection. Paved the way for later methods (e.g., graph-based learning) by establishing baseline techniques and highlighting challenges of managing large tactile data streams.\n\n\n\nTable: Selected research using uSkin sensors on Allegro Hand (2022–2025). Each study leveraged uSkin’s dense tactile feedback for perception or control, demonstrating improved performance in manipulation tasks.\n\n\nStudies Utilizing ReSkin in Robotics Research (2022–2025)\n\n\n\n\n\n\n\n\n\nStudy & Year\nApplication / Field\nExperimental Setup with ReSkin\nKey Outcomes\n\n\n\n\nBhirangi et al., 2021 (Meta AI & CMU) – “ReSkin: Versatile, Replaceable, Lasting Tactile Skins” (CoRL 2021, published 2022)\nTactile sensor development & benchmarking\nIntroduced ReSkin and evaluated it in lab tests. A small ReSkin patch (~2 cm) was indented at various locations and forces using a precise indenter and an ATI Nano17 F/T sensor for ground truth. Trained an MLP model to predict contact position (X,Y) and force (Z, and later X,Y) from magnetometer data.\nValidated ReSkin’s core capabilities: contact localization error ~0.5 mm and force error ~5 mN, with 99.6% contact accuracy in controlled conditions. Demonstrated high temporal resolution (up to 400 Hz) and longevity &gt;50,000 presses without model degradation. Established ReSkin as an inexpensive (&lt;$30) yet high-performance tactile sensor, laying groundwork for its adoption in various robot tasks.\n\n\nTirumala, Weng, Seita et al., 2022 – “Learning to Singulate Layers using Tactile Feedback” (IROS 2022)\nDeformable object manipulation (cloth layer separation)\nA Franka arm with a custom two-finger gripper was instrumented with a ReSkin sensor on one fingertip. The robot attempted to pinch and lift one or two layers from a stack of fabrics. A classifier was trained on ReSkin data to infer the number of layers grasped. 180 trials were conducted comparing tactile-informed strategy vs vision-only baselines.\nThe ReSkin-enabled gripper successfully distinguished between one vs. two cloth layers by touch, outperforming vision-only methods that failed on transparent or patterned fabrics. Tactile feedback from ReSkin allowed the robot to adjust its pinch depth in real time, greatly improving reliability in grasping the correct number of layers. This study showcased ReSkin’s thin profile and sensitivity – the sensor could be inserted between layers without bulky hardware, enabling a task (layer separation) that was not feasible with prior optical or larger tactile sensors. It highlighted the potential of ReSkin for fine manipulation in cloth handling and other delicate tasks.\n\n\nSingh et al., 2023 – “AnySkin: Plug-and-play Skin Sensing for Robotic Touch” (arXiv 2023)\nSensor design improvement (robust tactile skin)\nAn extension of ReSkin’s design addressing its drawbacks. Proposed fabrication changes: magnetize the elastomer after curing (using a pulse magnetizer) for uniform particle distribution, use finer magnetic particles to avoid sedimentation, and introduce a self-aligning mount that locks the skin to the magnetometer without adhesives. Evaluated signal consistency across multiple skin instances and under cyclic loading.\nProduced a new “AnySkin” sensor that maintained signal strength and consistency better than original ReSkin. Variability in readings across different skins was greatly reduced (normalized std. deviation ~0.1 vs &gt;0.5 before). The self-adhesive design prevented peeling or shifting during use, enhancing durability. AnySkin preserved ReSkin’s advantages (flexibility, low cost) while improving repeatability and ease-of-use. This research indicates the evolution of ReSkin technology to be more robust for practical deployment, thereby benefiting any robotic applications that rely on such magnetic skin sensors.\n\n\n\nTable: Selected research using ReSkin tactile skins (2021–2025). These works range from the initial demonstration of ReSkin’s capabilities to its application in challenging tasks (fabric manipulation) and further improvements in the sensor design."
  },
  {
    "objectID": "posts/storage/2025-05-29-mag-tactile.html#conclusion",
    "href": "posts/storage/2025-05-29-mag-tactile.html#conclusion",
    "title": "🧩uSkin vs ReSkin",
    "section": "Conclusion",
    "text": "Conclusion\nBoth uSkin and ReSkin represent significant advances in tactile sensing for robotic hands, but they are optimized for different priorities. uSkin offers a ready-made, high-density sensor array with excellent sensitivity and straightforward output of rich 3-axis force data at each contact point. It excels in scenarios demanding precision and reliability, such as controlled manipulation and experiments requiring accurate force measurements out-of-the-box. Its integration into the Allegro Hand has enabled researchers to achieve fine force control and detailed object recognition by leveraging the structured tactile information. The main downsides are the higher cost and the need to manage many sensor channels, as well as careful handling of magnetic interference (though mitigated by design).\nReSkin, on the other hand, offers an incredibly flexible and low-cost approach to spreading touch sensation over robot surfaces. Its strength lies in adaptability: it can conform to unique shapes, be replaced easily, and scale to larger areas without prohibitive cost. ReSkin has enabled innovative applications like sensing multiple layers of cloth – tasks previously impractical – thanks to its thinness and compliance. It leverages data-driven methods to extract a wealth of information (contact point, normal and shear forces) from minimal hardware. The trade-off is the added complexity of model training and the need for occasional recalibration when the “skin” changes or environmental conditions shift. Its accuracy can be superb, but only under a well-trained model’s regime; generalizing that performance broadly is an active area of improvement (e.g., AnySkin making strides in consistency).\nIn sum, uSkin and ReSkin are complementary tactile technologies. uSkin provides a benchmark for precision and ease-of-integration in research labs, delivering high-quality data for each taxel suitable for analytical approaches. ReSkin provides a vision of scalable, affordable tactile coverage, inviting creative uses and rapid prototyping of “sensitive skin” for robots. For the Allegro Hand, which has been used as a testbed, uSkin offers immediate high-resolution touch sensing on each finger, while ReSkin offers a path to cover the entire hand (and arms or tools) with a continuous sensing layer. The choice between them depends on the use-case: if one needs robust, plug-in sensors for fine control (and budget allows), uSkin may be preferable; if one needs wide-area coverage, easy replacement, or has cost constraints, ReSkin is extremely attractive. Future developments may even hybridize these approaches – using structured arrays of ReSkin-like cells, or adding more intelligence to uSkin’s data – to further enhance tactile sensing. What is clear from recent research is that both sensors greatly advance a robot hand’s ability to “feel”, bringing robotic manipulation closer to the dexterity of human touch by different means. Each has proven effective in various studies, and continued improvements (higher density uSkin, more stable ReSkin) are likely to expand their applications. In conclusion, uSkin and ReSkin represent two state-of-the-art solutions in tactile sensing, each with unique strengths, and both are instrumental in the ongoing development of tactile intelligence in robotic hands."
  },
  {
    "objectID": "posts/storage/2021-01-02-GNN-materials.html",
    "href": "posts/storage/2021-01-02-GNN-materials.html",
    "title": "🧩GNN Materials",
    "section": "",
    "text": "GNN에 관심을 가지게 된 계기는 RoboGrammar라는 paper였다. 예전부터 하고 싶었던 Robot design 아이디어를 GNN을 가지고 실현시킨 것이 너무 신기해서 공부해보고 싶었다. 이번 포스팅에서는 GNN과 첫만남인 만큼 공부할 자료들을 정리해보려 한다."
  },
  {
    "objectID": "posts/storage/2021-01-02-GNN-materials.html#materials",
    "href": "posts/storage/2021-01-02-GNN-materials.html#materials",
    "title": "🧩GNN Materials",
    "section": "Materials",
    "text": "Materials\n\nTobigs Graph Study\nCS224W: Machine Learning with Graphs / Videos\nGraph Neural Networks - Penn Engineering\nTF Graph Neural Network Samples\nGraph Neural Networks in TF2\nGraph Representation Learning(Pytorch)\nA Gentle Introduction to Graph Neural Networks (Basics, DeepWalk, and GraphSage)\nInvariant Graph Networks : invariance, equivariance, k-WL GNN 관련 주제\nEnd-to-End, Transferable Deep RL for Graph Optimization : RL + GNN\n\n\nTutorials & Workshops\n\nWWW 18 Tutorial : Representation Learning on Networks\nCIKM 19 Tutorial : Recent Developments of Deep Heterogeneous Information Network Analysis\nWSDM 19 Tutorial : Learning and Reasoning on Graph for Recommendation\nKDD 19 Tutorial : Learning From Networks\nAAAI 20 Tutorial : Graph Neural Networks: Models and Applications\nICML2020 GNN Workshop GRL+\nWWW 20 Hands on Tutorial - Videos\nGraph Neural Networks for Natural Language Processing / PPT\nTutorial on Spectral and Graph ConvNets\n\n\n\nPapers & Survey\n\nGraph Neural Networks: Taxonomy, Advances and Trends\nA Comprehensive Survey on Graph Neural Networks\nDirectional Graph Networks\nGNN KR Paper List\nGraph Meta Learning via Local Sub-graphs\n\nMeta-GNN: On Few-shot Node Classification in Graph Meta-learning\nFew-shot Learning with Graph Neural Networks\nLearning to Propagate for Graph Meta-Learning\n\nSelf-supervised Training of Graph Convolutional Networks\nXGNN: Towards Model-Level Explanations of Graph Neural Networks\nL2-GCN: Layer-Wise and Learned Efficient Training of Graph Convolutional Networks, 2020 CVPR\n\n\n\nVideos\n\nAn Introduction to Graph Neural Networks: Models and Applications\n\n\n\nGraph Convolutional Networks using only NumPy\n\n\n\nGeometric Deep Learning on Graphs and Manifolds\n\n\n\nGraph Nets: The Next Generation\n\nLink\n\n\n\n\nRecent Developments of Graph Network Architectures\n\nSlide\n2019-2020에 발표된 GNN 방법론들을 정리\nGNN의 expressiveness, 그중에서도 invariance and equvariance\n\n\n\n\nDeep learning on graphs: successes, challenges, and next steps\n\n\n\nGraph Representation Learning for Algorithmic Reasoning\n\nSlide\n\n\n\n\nHow Uber uses Graph Neural Networks to recommend you food\n\nPost"
  },
  {
    "objectID": "posts/project/get_off_prediction.html",
    "href": "posts/project/get_off_prediction.html",
    "title": "Self-driving Public Mobility Get-off Safety System",
    "section": "",
    "text": "https://github.com/curieuxjy/Safe_Goodbye"
  },
  {
    "objectID": "posts/project/pick-gpt.html",
    "href": "posts/project/pick-gpt.html",
    "title": "Pick-GPT",
    "section": "",
    "text": "tbd"
  },
  {
    "objectID": "posts/project/pulse_rl.html",
    "href": "posts/project/pulse_rl.html",
    "title": "Deep Reinforcement learning for DME Pulse Design",
    "section": "",
    "text": "tbd"
  },
  {
    "objectID": "posts/project/active_learning.html",
    "href": "posts/project/active_learning.html",
    "title": "Active Learning Algorithm for Object Detection and Segmentation",
    "section": "",
    "text": "tbd"
  },
  {
    "objectID": "posts/code/2023-06-18-chord.html",
    "href": "posts/code/2023-06-18-chord.html",
    "title": "👩‍💻Chord Graph",
    "section": "",
    "text": "이전 Accessibility-Based Clustering for Efficient Learning of Locomotion Skills 논문을 리뷰하면서 로봇의 static pose들을 가지고 K-Acc Clustering하는 과정 이후에 Clustering Analysis에서 Inter-cluster accessibility를 Visulization을 하는 부분이 있었습니다.\n오른쪽에 보이는 그래프가 Chord Graph인데 각 Top-20 cluster에 속한 sample pose들을 하나의 node로 표현하고 각 sample pose들이 다른 pose로 transition되는 시간을 기반으로 계산된 accessiblity 값이 높은 부분은 진한 edge로 accessibility가, 낮은 부분은 옅은 edge로 시각화하여 포즈들 간의 관계성을 보여줍니다. 따라서 이런 시각화를 통해 cluster 간의 inter-cluster accessibility를 파악할 수 있는 것 입니다.(자세한 내용은 이전 논문 리뷰 포스팅을 참고 바랍니다.) 이번 포스팅은 바로 이 Chord graph를 Holoviews라는 파이썬 패키지를 이용해서 시각화 하는 방법에 대해 다룰 것 입니다."
  },
  {
    "objectID": "posts/code/2023-06-18-chord.html#accesstimetable",
    "href": "posts/code/2023-06-18-chord.html#accesstimetable",
    "title": "👩‍💻Chord Graph",
    "section": "AccessTimeTable",
    "text": "AccessTimeTable\n샘플링한 Static poses들을 2000개를 poses.pickle 데이터로 저장해놨습니다. 각 pose-to-pose를 PD tracking을 하며 걸리는 시간을 측정하게 되는데 pose-to-pose로 transition되는 시간은 1초가될 수 있도록 joint trajectory를 만들어주고 PD제어를 하면서 0.0025초 마다 destination pose로 도달했는지(시뮬레이터의 dt)를 체크합니다. 이때 무한정 시간을 잴 수는 없기 때문에 10초로 시간을 제한하여 최대 10초까지만 걸리는 시간을 기록하게 됩니다.\n\n이 정보가 총 2000개 샘플 포즈에 대해서 1:1로 모두 구해야 하기때문에 병렬계산을 해서 저장하여 총 75개의 npy데이터로 나누어 계산하였고 이를 2000 by 2000 매트릭스로 만들어서 AccessTimeTable을 시각화하면 아래와 같이 그려집니다.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport pickle5 \nwith open('../poses.pickle', 'rb') as handle: poses = pickle5.load(handle)\n# 불러올 npy 파일 이름들을 리스트로 만들어줍니다.\naccessTimeFiles = ['accessTimeTableAidin%d.npy'%x for x in range(75)]\n\n\ntimeMat = np.sum([np.load(f) for f in accessTimeFiles], axis=0)\n# 자기자신 pose로부터 자기자신의 pose로의 시간은 0초로 처리하기 위해 대각선의 매트릭스 값은 0으로 일괄 처리합니다.\ntimeMat[ range(len(timeMat)), range(len(timeMat)) ]= 0 \n\n\ntimeMat # shape: (2000, 2000)\n\narray([[ 0.   ,  1.245, 10.   , ...,  1.145, 10.   , 10.   ],\n       [10.   ,  0.   ,  1.555, ...,  0.955,  1.31 , 10.   ],\n       [10.   ,  3.09 ,  0.   , ..., 10.   , 10.   , 10.   ],\n       ...,\n       [10.   ,  0.965,  2.035, ...,  0.   , 10.   , 10.   ],\n       [ 1.345,  1.015, 10.   , ...,  1.145,  0.   , 10.   ],\n       [10.   , 10.   ,  1.4  , ..., 10.   , 10.   ,  0.   ]])\n\n\n\nfig = plt.figure(figsize=(8,8))\nplt.imshow(timeMat)\nplt.title(\"Time Matrix\")\nplt.colorbar()\nplt.show()\nplt.close()\n\n\n\n\n\n\n\n\n10초 이하로 측정되었던 time data와 10초 이상으로 측정된 time data의 수를 살펴보면 아래와 같습니다.\n\nnp.sum(timeMat &lt; 9.999)\n\n907572\n\n\n\nnp.sum(timeMat &gt;= 9.999)\n\n3092428"
  },
  {
    "objectID": "posts/code/2023-06-18-chord.html#accessibility-histogram",
    "href": "posts/code/2023-06-18-chord.html#accessibility-histogram",
    "title": "👩‍💻Chord Graph",
    "section": "Accessibility Histogram",
    "text": "Accessibility Histogram\n이러한 timeMat를 Accessiblity 공식에 맞게 다시 계산하게 됩니다. 이때 10초 이상이 되는 데이터는 1e-8으로 만들어서 가장 낮은 accessiblity 점수를 얻도록 처리합니다.\n\nacc_matrix = (timeMat &lt; 9.999) * np.exp(-timeMat/10) + (timeMat &gt;= 9.999) * 1e-8\n\n히스토그램으로 Accessibility를 시각화하면 다음과 같습니다.\n\nvalues = np.reshape(acc_matrix,(-1,)) # 2000 x 2000 = 4000k\nax = plt.hist(values, bins=50, range=(-1e-4,1.1))\nplt.show()\n\n\n\n\n\n\n\n\n무한대 시간이 걸렸던 부분을 제외하고 히스토그램을 그려보면 아래와 같습니다.\n\nax = plt.hist(values, bins=50, range=(+1e-4,1.1))\nplt.show()"
  },
  {
    "objectID": "posts/code/2023-06-18-chord.html#k-acc-algorithm",
    "href": "posts/code/2023-06-18-chord.html#k-acc-algorithm",
    "title": "👩‍💻Chord Graph",
    "section": "K-Acc algorithm",
    "text": "K-Acc algorithm\n계산한 Accessibility 값을 기준으로 K-Acc 알고리즘으로 centroid pose와 적절한 centroid 수를 결정하게 됩니다. 논문에서 소개된 K-Acc 알고리즘은 원저자가 공개한 코드를 그대로 사용하여 AiDIN-VIII 데이터에 적용했습니다.\n\n\n\n\n\n\nK-Acc 알고리즘을 수행하는 K_access 클래스 살펴보기\n\n\n\n\n\nclass K_access:\n    def __init__(self, access, k=2, seed=123):\n        self.seed = seed\n        np.random.seed(self.seed)\n        self.k = k # 클래스 수 \n        self.access = access # weight_matrix\n        self.node_num = len(access) \n        self.access[range(self.node_num),range(self.node_num)] = 1 # 대각 성분을 1로\n        self.core_index = np.zeros((k,), dtype=int) \n        self.core_index[0] = np.random.randint(0, self.node_num, (1,)) # 클래스 범위 내에 랜덤한 정수를 코어 인덱스로 설정\n\n        for i in range(1, k):\n            ready_core_access = np.sum(self.access[self.core_index[:i],:], axis=0) \\\n                                + np.sum(self.access[:,self.core_index[:i]], axis=1)\n            ready_core_access[self.core_index[:i]] += 999999 # accessible to self\n            self.core_index[i] = np.argmin(ready_core_access) # the one that is the farthest from \n            \n        self.assignment = np.zeros((self.node_num,),dtype=int)\n        self.labels = np.zeros((self.node_num,),dtype=int)\n        self.cores_sorted = np.zeros((self.node_num,),dtype=int)\n        self.max_iter = 10000\n        \n        self.assign()        \n            \n    def assign(self):\n        # from core to nodes\n        core_access = self.access[self.core_index] \n        # argmax access(core,node)\n        self.assignment = self.core_index[np.argmax(core_access, axis = 0)] \n        for c in self.core_index:\n            self.assignment[c] = c  # self belongs to self\n        return\n    \n    def update(self):\n        for i in range(self.k):\n            Si = np.where(self.assignment == self.core_index[i])[0]\n            access_Si = self.access[Si,:][:,Si]\n            minaccess_Si = np.min(access_Si,axis=1)\n            self.core_index[i] = Si[np.argmax(minaccess_Si)]\n        return        \n    \n    def fit(self):\n        pre_assignment = np.zeros((self.node_num,),dtype=int) - 1\n        iter_ = 0\n        while np.sum(np.abs(self.assignment - pre_assignment)) != 0 and iter_ &lt; self.max_iter :\n            pre_assignment = self.assignment\n            iter_ += 1\n            self.update()\n            self.assign()\n        return iter_\n            \n    def predict(self):\n        map_ = {}\n        cnt_Si = []\n        for i in range(self.k):\n            cnt_Si.append(len(np.where(self.assignment == self.core_index[i])[0]))\n        sorted_index = (9999 - np.array(cnt_Si)).argsort()  # descending by # samples in clu\n        for i in range(self.k):\n            map_[self.core_index[sorted_index[i]]] = i+1 # from 1 to # clu\n        self.labels = [map_[c] for c in self.assignment]\n        cores_sorted = self.core_index[sorted_index]\n        return self.labels, cores_sorted \n    \n    def inter_access(self):\n        cnt_Si = []\n        for i in range(self.k):\n            cnt_Si.append(len(np.where(self.assignment == self.core_index[i])[0]))\n        sorted_index = (9999 - np.array(cnt_Si)).argsort()  # descending by # samples in clu\n        all_to_C = self.access[:,:][:,self.core_index[sorted_index]]\n        inter_ = np.zeros((self.k,self.k))\n        for i in range(self.k):\n            Si = np.where(self.assignment == self.core_index[sorted_index[i]])[0]\n            Si_to_C = all_to_C[Si,:]\n            inter_[i,:] = np.mean(Si_to_C,axis = 0)\n            inter_[i,i] = 1\n        return inter_\n    \n    def intra_access(self):\n        intra_ = np.zeros((self.k,))\n        cnt_Si = []\n        for i in range(self.k):\n            cnt_Si.append(len(np.where(self.assignment == self.core_index[i])[0]))\n        sorted_index = (9999 - np.array(cnt_Si)).argsort()  # descending by # samples in clu\n        for i in range(self.k):\n            Si = np.where(self.assignment == self.core_index[sorted_index[i]])[0]\n            intra_[i] = np.min(self.access[self.core_index[sorted_index[i]],Si],axis=0)\n        return intra_\n    \n    def evaluate(self):\n        intra_ = np.mean(np.log(self.intra_access()))\n        inter_ = np.mean(np.log(self.inter_access()))\n        cnt_Si = []\n        for i in range(self.k):\n            cnt_Si.append(len(np.where(self.assignment == self.core_index[i])[0]))\n        num_one_sample_cluster = len(np.where(np.array(cnt_Si)==1)[0])\n        alpha = 1\n        # index I\n        # larger is better\n        score_ = intra_ - inter_ - alpha * num_one_sample_cluster \n        return score_\n\n\n\nK_access 클래스를 가지고 최적의 클래스 수를 선정하기 위해 fit을 클래스 수를 늘려가며 수행합니다. 그랬을 때 155개의 centroid cluster를 가졌을 때 가장 index 점수가 높아 최적의 클래스 수를 선정할 수 있었습니다.\n\nscores = []\nn_cls = range(1,201) # 클래스의 수 1 ~ 200까지 조사\nfor num_class in n_cls:\n    k = K_access(acc_matrix, num_class)\n    k.fit()\n    scores.append(k.evaluate())\n    \nfig,ax = plt.subplots(figsize=(10, 10*9/16))\nmax_ind = np.argmax(scores)\nprint(max_ind+1, scores[max_ind]) # 최적의 클래스 수, 그때의 인데스 점수\n\nplt.plot(n_cls, scores, marker='o', markersize=1)\nplt.plot([max_ind+1],[scores[max_ind]],marker='o',c='r',markersize=2)\nplt.xlabel('Number of Clusters')\nplt.ylabel('Index Value')\nplt.xlim([0,170])\nplt.show()\n\n155 14.17499592680445\n\n\n\n\n\n\n\n\n\n선정된 클래스 수를 가지고 다시한번 클러스터링 작업을 거져 각 centroid pose인 cores에 대한 정보와 inter_access, intra_access 점수를 가져올 수 있습니다.\n\nnum_class = max_ind+1\nk = K_access(acc_matrix, num_class)\nk.fit()\nclusters, cores = k.predict()\ninter_access = k.inter_access()\nintra_access = k.intra_access()\n\n\ncores # centroid pose의 ID\n\narray([1213, 1043, 1150, 1555,  112,  100, 1032,  121,  140, 1097, 1116,\n       1554, 1377, 1623,   33,   41,  250,   29, 1272, 1926, 1401,   22,\n       1888,  809, 1262,  165, 1513,  108, 1248,  333, 1008,  330, 1081,\n        157,  419, 1227, 1231, 1244,  564,  392,  543,   18, 1785,  227,\n       1288, 1644, 1715,  398, 1527, 1017,  169, 1056,  139,  323, 1848,\n       1121, 1067,  225,  476,  450,  898,    8, 1492, 1223,  467, 1844,\n       1608, 1803, 1839, 1913, 1015,   96, 1020,  627, 1526,  691,  268,\n       1899, 1521, 1787,    3, 1597,  210,  643,  502, 1358,  209,  798,\n       1887, 1216,  332,  972, 1122,  404,  343, 1423,  363,  173, 1544,\n          5, 1817,  960,   72, 1832,  853,  446,  479,  395,  650,  313,\n       1587,  677,  239, 1089,  464,  891, 1029, 1491, 1477,  761,  860,\n       1352, 1564,  938,  645, 1254,   50, 1149,  453, 1791,  777, 1990,\n       1737,   47, 1220, 1537,  315, 1180,  162, 1277, 1568,  910,  528,\n        523, 1361,  709, 1718, 1045,  618,   86, 1889, 1250,  455,  814,\n        172])\n\n\n각 클러스터마다 포함하고 있는 샘플들의 수는 어떻게 분포하고 있을까요? 히스토그램으로 시각화를 해보았습니다. 각 샘플들의 수는 이후에 chord diagram의 노드가 될 것 입니다.\n\n# figure 설정\nplt.rcParams['lines.linewidth']=0.7\nplt.rcParams['xtick.direction']='in'\nplt.rcParams['ytick.direction']='in'\nplt.rcParams['xtick.major.width']=0.4\nplt.rcParams['ytick.major.width']=0.4\nplt.rcParams['xtick.major.size']=2\nplt.rcParams['ytick.major.size']=2\nfig,ax = plt.subplots(figsize=(20, 5))\nax.spines['right'].set_visible(False)\nax.spines['top'].set_visible(False)\n\nax_ = plt.hist(np.array(clusters)-0.5,\n               range = [0.5, np.max(clusters)+0.5],\n               bins = np.max(clusters),\n               edgecolor='dimgray',\n               color='deepskyblue',\n               rwidth=1,\n               alpha=0.5)\n\nax = plt.xticks(range(1,1+np.max(clusters),5))\nplt.xlabel('Cluster ID',fontsize=15)\nplt.ylabel('Number of Samples',fontsize=15)\nplt.show()"
  },
  {
    "objectID": "posts/code/2023-06-18-chord.html#get-kacc-poses",
    "href": "posts/code/2023-06-18-chord.html#get-kacc-poses",
    "title": "👩‍💻Chord Graph",
    "section": "Get Kacc poses",
    "text": "Get Kacc poses\n알고리즘을 통해 선정된 centroid pose들에 대한 정보를 한번 확인해보겠습니다. 각 pose에 대한 정보는 아래와 같은 형식으로 정리해서 가장 많은 샘플 수를 포함하고 있는 top-3 centroid pose를 확인해보겠습니다.\n[\n([0,0,height], [ 12 joints], [roll, pitch, 0] ),\n]\n\nkaccess_config = []\nfor i in range(np.max(clusters)):\n    centroid_pose = poses[cores[i]]\n    ele1 = [0,0, centroid_pose['store_height']]\n    ele2 = centroid_pose['store_joints']\n    ele3 = list(centroid_pose['store_roll_pitch'])+[0]\n    ele = (ele1,ele2,ele3)\n    kaccess_config.append(ele)\n\n\n\n\n\n\n\nTop-3 Centroid pose 정보 확인하기\n\n\n\n\n\n[([0, 0, 0.30607859912101715],\n  [0.6053835299386103,\n   1.4824030607413299,\n   -0.0809348638895901,\n   0.3995084366561768,\n   1.2970169955948037,\n   -0.7264673491159409,\n   0.05388458488859648,\n   0.33537195041378626,\n   -0.24038702044129037,\n   -0.501582056954722,\n   -1.5199349541933391,\n   -0.44053023354145876],\n  [3.141592551130548, 0.37124208223469835, 0]),\n ([0, 0, 0.12894579304472167],\n  [0.05857655877237639,\n   -1.4077772309827037,\n   -0.7778225275558831,\n   0.1038327197157696,\n   -0.7770055977003666,\n   -0.9684446036207767,\n   0.06583156564999947,\n   -0.903642551883172,\n   0.3522403885013049,\n   -0.6108687315594281,\n   0.7353928319105865,\n   -0.21053293619245944],\n  [-0.412046597574257, 0.043837823368155976, 0]),\n ([0, 0, 0.2968457946843744],\n  [0.06337843283646641,\n   2.7942809454055157,\n   -0.9737491947994557,\n   -0.2517691066931858,\n   -2.624845950050974,\n   -0.18089990597943467,\n   -0.17501576141383207,\n   0.5877695591500649,\n   -0.06264699708493802,\n   -0.2039006032557608,\n   -1.193152752838448,\n   -0.4222694151347171],\n  [-1.3562253990942323, 0.0010176328562240375, 0])]"
  },
  {
    "objectID": "posts/code/2023-06-18-chord.html#chord-graph",
    "href": "posts/code/2023-06-18-chord.html#chord-graph",
    "title": "👩‍💻Chord Graph",
    "section": "Chord Graph",
    "text": "Chord Graph\n이제 본격적으로 pose 데이터를 가지고 chord graph를 그려보겠습니다. pickle 데이터로 저장되어 있는 pose 데이터들 중 2000개의 데이터를 가지고 dataframe 객체로 만들어줍니다. 마지막으로 clustering 과정에서 구한 각 포즈 데이터가 속해있는 cluster의 ID를 데이터프레임의 열을 추가하여 정보를 추가해줍니다.\n\ndf_poses = pd.DataFrame(poses)\ndf_poses_2000 = df_poses[:2000]\ndf_poses_2000[\"cluster\"] = clusters\n\n\ndf_poses_2000.head(3)\n\n\n\n\n\n\n\n\nstore_height\nstore_roll_pitch\nstore_joints\nstore_links\ncluster\n\n\n\n\n0\n0.300289\n(2.150537560664197, -0.004430010187345012)\n[-0.5810368941034365, -1.206576076316329, -0.0...\n[(-0.17671623826026917, -0.23643925786018372, ...\n22\n\n\n1\n0.067273\n(-0.00028379763432160346, 2.8617441250464685e-05)\n[0.15361879275773346, -2.083050326944349, -1.1...\n[(0.14308467507362366, -0.12229745090007782, 0...\n77\n\n\n2\n0.091594\n(-0.21762631665841028, -1.16469456094937e-05)\n[0.1518267329947646, -1.165616939584498, -0.07...\n[(-0.08804446458816528, -0.04484516754746437, ...\n129\n\n\n\n\n\n\n\n항공편 예제에서도 살펴보았듯이 모든 cluster를 시각화하는 것은 의미가 없기 때문에 Top20 cluster에 속해있는 데이터들만 처리하기 위해서 데이터를 전처리하는 과정이 필요합니다. 우선 각 pose 데이터가 source(출발노드) 가 될수도 있고 target(도착노드) 이 될 수도 있기 때문에 data_id라는 변수를 통해 기준 데이터(pose A) 와 페어 데이터(pose B) 를 튜플로 묶어준 리스트를 생성합니다.\n\ndata_id = [(x+1, y+1) for x in range(2000) for y in range(2000)]\n\n각 기준 데이터와 페어 데이터에 대해서 각 데이터가 속해있는 클러스터 아이디를 확장해서 저장해줍니다. 데이터들을 확인하기 위해 index 998:1004범위에 있는 값들을 확인합니다.\n\n# 기준 데이터(pose A)\nid_list = [x[0] for x in data_id]\nprint(id_list[998:1004])\n\n# 페어 데이터(pose B)\npairs = [x[1] for x in data_id]\nprint(pairs[998:1004])\n\n# 기준 데이터의 클러스터 아이디를 확장 \nclusters_expand = [clusters[x//2000] for x in range(2000*2000)]\nprint(clusters_expand[998:1004])\n\n# 페어 데이터의 클러스터 아이디를 확장\npair_cluster = clusters * 2000\nprint(pair_cluster[998:1004])\n\nassert len(id_list)==len(clusters_expand) == len(pairs) == len(values) == len(pair_cluster) # values는 acc 값\n\n[1, 1, 1, 1, 1, 1]\n[999, 1000, 1001, 1002, 1003, 1004]\n[22, 22, 22, 22, 22, 22]\n[1, 103, 21, 40, 59, 61]\n\n\n다음으로 chord 그래프를 위한 데이터 프레임 객체 df_chord를 만들고 칼럼을 재정렬해줍니다.\n\ndf_chord = pd.DataFrame({\"id\":id_list,\n                         \"cluster\":clusters_expand,\n                         \"pair\":pairs,\n                         \"acc\":values,\n                         \"pair_cluster\":pair_cluster})\n\n# column 재정렬\ndf_chord = df_chord[[\"id\", \"cluster\", \"pair\", \"pair_cluster\", \"acc\"]]\n\n\ndf_chord[998:1004]\n\n\n\n\n\n\n\n\nid\ncluster\npair\npair_cluster\nacc\n\n\n\n\n998\n1\n22\n999\n1\n1.000000e-08\n\n\n999\n1\n22\n1000\n103\n1.000000e-08\n\n\n1000\n1\n22\n1001\n21\n1.000000e-08\n\n\n1001\n1\n22\n1002\n40\n1.000000e-08\n\n\n1002\n1\n22\n1003\n59\n1.000000e-08\n\n\n1003\n1\n22\n1004\n61\n8.589883e-01\n\n\n\n\n\n\n\n기준 데이터의 클러스터 기준으로 상위 20개의 클러스터에 속해 있는 데이터들만 남기기는 과정을 진행합니다. 클러스터의 아이디는 크기순 정렬이기 때문에 1~20까지의 클러스터 아이디만 남기면 상위 20개의 클러스터에 속한 포즈 데이터들만 남게 됩니다.\n\ndf_chord.drop(df_chord[(df_chord['cluster']&gt;20)].index, inplace=True)\n\n페어 데이터의 클러스터 기준으로도 상위 20개의 클러스터에 속해 있는 데이터들만 남기는 과정을 똑같이 진행합니다.\n\ndf_chord.drop(df_chord[(df_chord['pair_cluster']&gt;20)].index, inplace=True)\n\n\ndf_chord.head(5)\n\n\n\n\n\n\n\n\nid\ncluster\npair\npair_cluster\nacc\n\n\n\n\n8004\n5\n12\n5\n12\n1.000000e+00\n\n\n8006\n5\n12\n7\n18\n1.000000e-08\n\n\n8007\n5\n12\n8\n8\n1.000000e-08\n\n\n8011\n5\n12\n12\n13\n1.000000e-08\n\n\n8025\n5\n12\n26\n12\n9.080099e-01\n\n\n\n\n\n\n\nholoviews 패키지를 불러와서 chord graph를 그리기 위한 준비를 합니다. holoviews는 시각화 라이브러리 백엔드를 선택할 수 있는데 interaction이 가능한 bokeh 백엔드를 선택했습니다.\n\nimport holoviews as hv\nfrom holoviews import dim\nhv.extension('bokeh') # backend engine 선택\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n\n\n\n\n\n\n\n항공편 예제에서도 groupby를 이용해서 edge 수를 집계했듯이 pose 데이터셋에 대해서도 acc수를 cluster와 pair_cluster를 기준으로 집계해서 edge 정보를 정리해줍니다.\n\nedges = df_chord.groupby([\"cluster\", \"pair_cluster\"]).acc.count().reset_index()\n\n\nedges\n\n\n\n\n\n\n\n\ncluster\npair_cluster\nacc\n\n\n\n\n0\n1\n1\n1764\n\n\n1\n1\n2\n1764\n\n\n2\n1\n3\n1596\n\n\n3\n1\n4\n1302\n\n\n4\n1\n5\n1218\n\n\n...\n...\n...\n...\n\n\n395\n20\n16\n675\n\n\n396\n20\n17\n650\n\n\n397\n20\n18\n650\n\n\n398\n20\n19\n650\n\n\n399\n20\n20\n625\n\n\n\n\n400 rows × 3 columns\n\n\n\n다음으로 공항정보를 저장했듯이 pose데이터가 속해있는 cluster의 아이디를 각 centroid pose(cp_N) 이름으로 매칭해서 저장해줍니다. 각 노드(pose 데이터)가 속해있는 cluster_id를 기준으로 edge 데이터셋에서 cluster와 매칭되는 것을 알 수 있습니다.\n\n# node\nnodes = pd.DataFrame({\"cluster_id\":[x+1 for x in range(20)],\n                     \"name\":[\"cp_{}\".format(i+1) for i in range(20)]})\nnode_dataset = hv.Dataset(nodes, \"cluster_id\", \"name\" )\n\n\nnodes.head(3)\n\n\n\n\n\n\n\n\ncluster_id\nname\n\n\n\n\n0\n1\ncp_1\n\n\n1\n2\ncp_2\n\n\n2\n3\ncp_3\n\n\n\n\n\n\n\n\nchord = hv.Chord((edges, node_dataset),\n                 [\"cluster\", \"pair_cluster\"],\n                 ['acc'])\n\n\nchord\n\n\n\n\n\n  \n\n\n\n\n마찬가지로 옵션을 추가하여 chord diagram을 좀 더 보기 좋게 만들어서 export 해보겠습니다.\n\n%%opts Chord [height=600 width=600]\n\nchord.opts(\n    cmap='Category20',\n    labels='name',\n    edge_color=dim(\"cluster\").astype(str),\n    edge_alpha=0.7,\n    node_color=dim(\"cluster_id\").astype(str))\n\n\n\n\n\n  \n\n\n\n\n이렇게 해서 완성한 K-Acc Chord Diagram을 각 centroid pose와 함께 시각화를 하면 아래 그림과 같이 됩니다!\n\nConclusion\n그동안 데이터 시각화를 위한 코딩작업을 자주하지 않아서 논문에 있는 그림을 하나 따라하기까지 정말 오랜시간이 걸렸던 것 같습니다. 적절한 패키지를 서칭하는 것부터 시작해서 해당 패키지를 어떻게 사용해야 원하는 그림을 뽑을 수 있는지까지 한 과정마다 많은 고민과 연습이 필요했지만 마지막에 원하는 시각화 자료를 뽑을 수 있어서 뿌듯했던 것 같습니다. 데이터 시각화 과정이 연구를 하는 입장에서는 가장 마지막에 설득과 확인의 과정에 필요한 자료라서 소홀히 하기 쉬운데 논문의 결과를 더 빛낼 수 있는 중요한 과정이라는 것을 이번 기회에 또 한번 느낄 수 있었던 것 같습니다. 생소한 그래프 형식과 적용이 만만치는 않았지만 정말 의미있던 과정이었고 Chord diagram이 필요한 그 누군가에게 도움이 되었기를 바라며 이번 포스팅을 마치겠습니다.\nReference\n\nCHORD DIAGRAM\nChord diagram (information visualization)\nHoloviews - Chord\nHoloviews - Route Chord"
  },
  {
    "objectID": "posts/code/2025-02-16-hydra.html",
    "href": "posts/code/2025-02-16-hydra.html",
    "title": "👩‍💻Hydra로 실험관리 하기",
    "section": "",
    "text": "ML/DL 실험에서는 다양한 실험 파라미터들을 관리해야 합니다. 이를 위해 여러가지 방법이 있지만, 이번 포스팅에서는 FacebookResearch에서 만든 Hydra를 사용해보자 합니다."
  },
  {
    "objectID": "posts/code/2025-02-16-hydra.html#config-초기화",
    "href": "posts/code/2025-02-16-hydra.html#config-초기화",
    "title": "👩‍💻Hydra로 실험관리 하기",
    "section": "Config 초기화",
    "text": "Config 초기화\nHydra에서 설정(Config) 파일을 초기화하는 방법에는 여러 가지가 있습니다. 공식 문서 Initialization methods에서 확인할 수 있듯이, 총 세 가지 방법이 있으며, 각각의 방법을 예제를 통해 살펴보겠습니다.\nHydra의 설정 초기화 방법\n\ninitialize(): 호출하는 코드의 상대 경로를 기준으로 설정 파일을 초기화합니다.\n\ninitialize_config_module(): 절대 경로를 사용하여 설정 모듈(config_module)을 기반으로 초기화합니다.\n\ninitialize_config_dir(): 파일 시스템의 절대 경로를 사용하여 설정 디렉터리(config_dir)를 기반으로 초기화합니다.\n\n이 세 가지 방법은 (1)함수 호출 방식과 (2)컨텍스트(context) 방식으로 사용할 수 있습니다.\n\n함수 호출 방식으로 사용하면 Hydra를 전역적(global)으로 초기화하며, 한 번만 호출해야 합니다.\n반면, 컨텍스트 방식으로 사용하면 특정 블록 내에서만 Hydra를 초기화할 수 있으며, 여러 번 사용할 수도 있습니다.\n\n\n방법1 initialize()\nHydra를 초기화하고 config_path를 설정 검색 경로에 추가합니다.\nconfig_path는 호출하는 코드의 parent 디렉터리를 기준으로 한 상대 경로이며, 이 경우에는 현재 노트북이 위치한 디렉터리를 기준으로 설정됩니다.\n\n%cd /content/\n\nwith initialize(version_base=None, config_path=\"configs\"):\n    # Compose the configuration by selecting the main configuration.\n    cfg_1 = compose(config_name=\"main\")\n\n# full configuration을 YAML 형식으로 출력하여 쉽게 검사할 수 있습니다.\nprint(OmegaConf.to_yaml(cfg_1))\n\n/content\nenv:\n  name: CartPole-v1\n  seed: 42\n  max_steps: 200\nagent:\n  type: DQN\n  hidden_layers:\n  - 64\n  - 64\n  activation: relu\n  learning_rate: 0.001\n  gamma: 0.99\ntraining:\n  episodes: 1000\n  batch_size: 32\n  replay_buffer_size: 10000\n\n\n\n\n\n방법2 initialize_config_module()\nHydra를 초기화하고 config_module을 설정 검색 경로에 추가합니다.\nconfig_module은 반드시 import 가능한 형태여야 하며, 최상위 디렉터리에 __init__.py 파일이 존재해야 합니다. 이번 예제에서는 module이라는 폴더를 만들어서 import 가능한 디렉토리로 만들어 줍니다. 그리고 첫번째 만들었던 main.yaml파일을 복사하여 module/main_2.yaml 파일로 만들어 줍니다.\n\n%cd /content/configs\n%mkdir -p /content/configs/module\n!touch /content/configs/module/__init__.py\n%cd /content\n!cp ./configs/main.yaml configs/module/main_2.yaml\n\n/content/configs\n/content\n\n\n잘 복사가 되었는지 확인해보겠습니다.\n\n!cat /content/configs/module/main_2.yaml\n\n# This is the main configuration file for the RL project.\n# It combines settings for the environment, agent, and training.\n\nenv:\n  name: \"CartPole-v1\"    # Environment name (e.g., Gym environment)\n  seed: 42               # Random seed for reproducibility\n  max_steps: 200         # Maximum steps per episode\n\nagent:\n  type: \"DQN\"            # Agent type (e.g., DQN)\n  hidden_layers: [64, 64] # Network architecture: two hidden layers with 64 units each\n  activation: \"relu\"     # Activation function\n  learning_rate: 0.001   # Learning rate for the optimizer\n  gamma: 0.99            # Discount factor\n\ntraining:\n  episodes: 1000         # Number of training episodes\n  batch_size: 32         # Batch size for learning\n  replay_buffer_size: 10000  # Size of the replay buffer\n\n\n이번에는 2번째 방법인 initialize_config_module()함수를 이용하여 복사했던 main_2.yaml 파일을 이용하여 compose 해보겠습니다.\n\n%cd /content\nwith initialize_config_module(version_base=None, config_module=\"configs.module\"):\n    cfg_2 = compose(config_name=\"main_2\")\n    print(cfg_2)\n\n/content\n{'env': {'name': 'CartPole-v1', 'seed': 42, 'max_steps': 200}, 'agent': {'type': 'DQN', 'hidden_layers': [64, 64], 'activation': 'relu', 'learning_rate': 0.001, 'gamma': 0.99}, 'training': {'episodes': 1000, 'batch_size': 32, 'replay_buffer_size': 10000}}\n\n\n아래와 같이 config의 키들을 확인해볼 수도 있습니다.\n\nlist(cfg_2.keys())\n\n['env', 'agent', 'training']\n\n\n\n\n방법3 initialize_config_dir()\nHydra를 초기화하고 config_path를 설정 검색 경로에 추가합니다. config_path는 파일 시스템 상의 절대 경로여야 합니다. 미리 만들어 놓았던 main.yaml 파일을 이용하여 config를 구성해보겠습니다.\n\nCONFIG_DIR = \"/content/configs\"\n\nwith initialize_config_dir(version_base=None, config_dir=CONFIG_DIR):\n    cfg_3 = compose(config_name=\"main\")\n\n\nprint(cfg_3)\n\n{'env': {'name': 'CartPole-v1', 'seed': 42, 'max_steps': 200}, 'agent': {'type': 'DQN', 'hidden_layers': [64, 64], 'activation': 'relu', 'learning_rate': 0.001, 'gamma': 0.99}, 'training': {'episodes': 1000, 'batch_size': 32, 'replay_buffer_size': 10000}}\n\n\n여기까지 Hydra의 세 가지 방법을 사용하여 설정(Config)을 초기화하는 방법을 살펴보았습니다. 이제 이렇게 생성된 설정이 어떻게 구성되어 있는지 확인해보겠습니다. 대표적인 설정 예제로 cfg_1을 살펴보겠습니다.\n객체의 타입을 확인해보면, 이는 Omegaconf에서 제공하는 DictConfig 객체임을 알 수 있습니다. DictConfig는 딕셔너리 형태의 설정을 계층적으로 관리할 수 있도록 해주는 데이터 구조로, YAML 설정 파일을 로드하거나 동적으로 구성 값을 변경할 때 유용하게 활용됩니다.\n\ntype(cfg_1)\n\n\n    omegaconf.dictconfig.DictConfigdef __init__(content: Union[Dict[DictKeyType, Any], 'DictConfig', Any], key: Any=None, parent: Optional[Box]=None, ref_type: Union[Any, Type[Any]]=Any, key_type: Union[Any, Type[Any]]=Any, element_type: Union[Any, Type[Any]]=Any, is_optional: bool=True, flags: Optional[Dict[str, bool]]=None) -&gt; None/usr/local/lib/python3.11/dist-packages/omegaconf/dictconfig.pyContainer tagging interface\n      \n      \n\n\nConfig 객체의 키들은 다음과 같이 확인할 수 있습니다.\n\nlist(cfg_1.keys())\n\n['env', 'agent', 'training']\n\n\nDictConfig의 키는 config_name.key_name 또는 config_name[\"key_name\"] 형태로 접근할 수 있습니다. 이를 통해 일반적인 딕셔너리처럼 키를 참조하거나 점 표기법(dot notation)을 사용하여 계층적인 설정 값을 쉽게 조회할 수 있습니다.\n\nprint(cfg_1.env == cfg_1[\"env\"])\n\nTrue\n\n\n하위 키들도 동일한 방식으로 접근할 수 있으며, config_name.key_name.sub_key_name 또는 config_name[\"key_name\"][\"sub_key_name\"] 형태로 호출할 수 있습니다. 이를 활용하면 계층적으로 구성된 설정에서 원하는 값을 직관적으로 조회할 수 있습니다.\n\nprint(cfg_1.env.name == cfg_1[\"env\"][\"name\"])\n\nTrue"
  },
  {
    "objectID": "posts/code/2025-02-16-hydra.html#override",
    "href": "posts/code/2025-02-16-hydra.html#override",
    "title": "👩‍💻Hydra로 실험관리 하기",
    "section": "Override",
    "text": "Override\n이번에는 초기화로 만든 config를 override하는 예제를 살펴보겠습니다. 강화학습에서 여러개의 environment를 병렬로 학습하기 위해 env 하위에 num_envs config를 1000개로 추가하는 override를 진행해보겠습니다.\n\n%cd /content/\n\nwith initialize(version_base=None, config_path=\"configs\"):\n    # 기존 설정을 유지하면서 새로운 설정을 추가\n    cfg = compose(config_name=\"main\", overrides=[\"+env.num_envs=1000\"])\n\n# 기존 설정과 오버라이드된 설정이 함께 출력됨\nprint(OmegaConf.to_yaml(cfg))\n\n/content\nenv:\n  name: CartPole-v1\n  seed: 42\n  max_steps: 200\n  num_envs: 1000\nagent:\n  type: DQN\n  hidden_layers:\n  - 64\n  - 64\n  activation: relu\n  learning_rate: 0.001\n  gamma: 0.99\ntraining:\n  episodes: 1000\n  batch_size: 32\n  replay_buffer_size: 10000\n\n\n\noverride는 기존의 main.yaml 파일의 내용을 변경하지 않고 항목을 추가할 수 있습니다. 다시한번 main.yaml 내용을 확인해보면 num_envs가 없음을 알 수 있습니다.\n\n!cat /content/configs/main.yaml\n\n# This is the main configuration file for the RL project.\n# It combines settings for the environment, agent, and training.\n\nenv:\n  name: \"CartPole-v1\"    # Environment name (e.g., Gym environment)\n  seed: 42               # Random seed for reproducibility\n  max_steps: 200         # Maximum steps per episode\n\nagent:\n  type: \"DQN\"            # Agent type (e.g., DQN)\n  hidden_layers: [64, 64] # Network architecture: two hidden layers with 64 units each\n  activation: \"relu\"     # Activation function\n  learning_rate: 0.001   # Learning rate for the optimizer\n  gamma: 0.99            # Discount factor\n\ntraining:\n  episodes: 1000         # Number of training episodes\n  batch_size: 32         # Batch size for learning\n  replay_buffer_size: 10000  # Size of the replay buffer\n\n\n하지만 override된 cfg의 키에는 env.num_envs가 있습니다.\n\ncfg.env.num_envs\n\n1000"
  },
  {
    "objectID": "posts/code/2025-02-16-hydra.html#resolver",
    "href": "posts/code/2025-02-16-hydra.html#resolver",
    "title": "👩‍💻Hydra로 실험관리 하기",
    "section": "Resolver",
    "text": "Resolver\nHydra는 Omegaconf의 Resolver 기능을 활용할 수 있습니다. OmegaConf.register_new_resolver()를 사용하여 커스텀 resolver를 등록하면, 새로운 interpolation 타입을 추가할 수 있으며, 설정(Config) 노드가 접근될 때 해당 resolver가 호출됩니다.\n\nResolver 등록 및 기능\n\neq: 두 문자열을 소문자로 변환한 후, 동일한지 비교합니다.\n\ncontains: 첫 번째 문자열이 두 번째 문자열에 포함되어 있는지 검사합니다.\n\nif: 주어진 조건에 따라 두 값 중 하나를 선택합니다.\n\nresolve_default: 인자가 빈 문자열이면 기본값을 사용하고, 그렇지 않으면 인자 값을 반환합니다.\n\n이러한 resolver를 활용하면 설정 파일 내에서 조건부 로직, 문자열 비교, 기본값 처리 등을 동적으로 적용할 수 있습니다.\n예제로 아래와 같이 Resolver를 등록해보겠습니다.\n\n# Hydra 설정에서 사용할 Resolver들을 등록합니다.\nOmegaConf.register_new_resolver(\"eq\", lambda x, y: x.lower() == y.lower())\nOmegaConf.register_new_resolver(\"contains\", lambda x, y: x.lower() in y.lower())\nOmegaConf.register_new_resolver(\"if\", lambda pred, a, b: a if pred else b)\nOmegaConf.register_new_resolver(\"resolve_default\", lambda default, arg: default if arg == \"\" else arg)\n\n이번 예제에서 사용할 Config는 위의 예제 Config에서 Resolver를 확인하기 위해 아래 내용을 더 추가하여 구성해보겠습니다.\n\nyaml_config = \"\"\"\nenv:\n  name: \"CartPole-v1\"\n  seed: 42\n\nagent:\n  type: \"DQN\"\n  hidden_layers: [64, 64]\n  activation: \"relu\"\n  learning_rate: 0.001 # experiment의 default_lr 기준값으로 설정\n  gamma: 0.99\n\ntraining:\n  episodes: 1000\n  batch_size: ${if:${experiment.is_test}, 128, 32} # 테스트 모드일 경우 128, 아니면 32\n  replay_buffer_size: 10000\n\n# Resolver를 활용한 동적 설정\nexperiment:\n  mode: \"test\"\n  is_test: ${eq:${experiment.mode}, \"test\"}\n  default_lr: ${resolve_default:0.001, ${agent.learning_rate}}\n  is_debug: ${contains:${experiment.mode}, \"debug\"}  # \"debug\" 포함 여부 확인\n\"\"\"\n\nconfig를 로드하고 Resolver가 적용된 값을 출력합니다.\n\ncfg = OmegaConf.create(yaml_config)\n\n# Resolver가 적용된 값 출력\nprint(\"실험 모드:\", cfg.experiment.mode)\nprint(\"is_test:\", cfg.experiment.is_test)\nprint(\"배치 크기:\", cfg.training.batch_size)\nprint(\"default_lr:\", cfg.experiment.default_lr)\nprint(\"디버그 모드 여부):\", cfg.experiment.is_debug)\n\n실험 모드: test\nis_test: True\n배치 크기: 128\ndefault_lr: 0.001\n디버그 모드 여부): False\n\n\n예제 출력을 하나씩 살펴보겠습니다.\n\n${eq:${experiment.mode}, \"test\"} → experiment.mode가 \"test\"이면 True, 아니면 False\n\neq(x, y) Resolver는 두 값을 비교하여 같으면 True, 다르면 False를 반환합니다. ${experiment.mode} 값이 \"test\"인지 확인하는 역할을 합니다.\n\nexperiment:\n    mode: \"test\"\n    is_test: ${eq:${experiment.mode}, \"test\"}\n\n위 설정에서 experiment.mode 값이 \"test\"로 설정되어 있기 때문에, ${eq:${experiment.mode}, \"test\"}는 \"test\"와 \"test\"를 비교하는 형태가 됩니다. eq 함수는 대소문자를 구분하지 않고 두 문자열이 같은지 확인하는 역할을 하므로, \"test\"는 \"test\"와 일치하여 True를 반환합니다. 따라서 experiment.is_test의 값은 True로 설정됩니다.\n반면, 만약 experiment.mode 값이 \"train\"이었다면, ${eq:${experiment.mode}, \"test\"}는 \"train\"과 \"test\"를 비교하게 됩니다. 이 두 값은 서로 다르므로 eq 함수는 False를 반환하게 되고, 결과적으로 experiment.is_test 값은 False로 설정됩니다. 이를 통해 설정 값에 따라 특정 변수를 자동으로 조정할 수 있으며, 이를 활용하면 실험 모드에 따라 설정을 다르게 적용할 수 있습니다.\n\n${if:${experiment.is_test}, 128, 32} → experiment.is_test가 True이면 128, 아니면 32\n\nif(condition, true_value, false_value) Resolver는 condition이 True일 때 true_value를, False일 때 false_value를 반환합니다. experiment.is_test 값이 True인지 확인하여, 이에 따라 다른 값을 할당합니다.\n\ntraining:\n    batch_size: ${if:${experiment.is_test}, 128, 32} # 테스트 모드일 경우 128, 아니면 32\n\n${if:${experiment.is_test}, 128, 32} 구문을 살펴보면, if 함수는 첫 번째 인자로 주어진 조건이 True일 경우 두 번째 인자인 128을 반환하고, False일 경우 세 번째 인자인 32를 반환하는 역할을 합니다. 현재 experiment.is_test가 True이므로 if(True, 128, 32)는 128을 반환하고, 결과적으로 training.batch_size 값이 128이 됩니다.\n반면, 만약 experiment.mode가 \"train\" 등 다른 값으로 설정되어 있다면, eq(\"train\", \"test\")의 결과는 False가 되어 experiment.is_test가 False로 설정됩니다. 이 경우, if(False, 128, 32)는 False에 해당하는 세 번째 값인 32를 반환하게 되며, training.batch_size 값이 32로 설정됩니다.\n이러한 방식은 training과 test에서 배치 크기를 다르게 설정할 때 유용합니다. 예를 들어, 테스트 환경에서는 더 큰 배치 크기를 사용하여 빠르게 결과를 확인하고, 훈련 환경에서는 적절한 배치 크기를 유지하여 안정적인 학습이 가능하도록 조정할 수 있습니다. 이를 통해 설정 파일을 동적으로 관리할 수 있으며, 실험 조건에 따라 유연하게 설정을 변경할 수 있습니다.\n\n${resolve_default:0.001, ${agent.learning_rate}} → agent.learning_rate가 설정되지 않았으면 기본값 0.001 사용\n\nresolve_default(default, arg) Resolver는 arg 값이 비어 있거나 설정되지 않았을 경우 default 값을 반환합니다.\nagent.learning_rate 값이 존재하면 그대로 사용하고, 없다면 기본값 0.001을 사용합니다.\n\nagent:\n    learning_rate: 0.001 # experiment의 default_lr 기준값으로 설정\nexperiment:\n    default_lr: ${resolve_default:0.001, ${agent.learning_rate}}\n\n이 방식은 설정 파일에서 특정 값이 누락되었을 때 기본값을 자동으로 적용하는 데 매우 유용합니다. 예를 들어, learning_rate 값을 실험마다 다르게 설정할 수 있도록 설정 파일에서 값을 명시적으로 지정할 수도 있지만, 실수로 빠뜨렸을 경우에도 resolve_default를 사용하면 안전하게 기본값을 사용할 수 있습니다. 이를 통해 설정을 더욱 견고하게 만들고, 코드의 예외 처리를 간결하게 할 수 있습니다.\n\n${contains:${experiment.mode}, \"debug\"} → experiment.mode에 “debug”라는 글자가 포함되어 있는지 여부 확인\nexperiment:\nmode: \"test\"\nis_debug: ${contains:${experiment.mode}, \"debug\"}  # \"debug\" 포함 여부 확인\n\nexperiment.mode 값이 \"test\"라면 \"debug\"가 포함되지 않았으므로 contains(\"test\", \"debug\")는 False를 반환합니다.\n만약 experiment.mode 값이 \"test_debug\"라면 \"debug\"라는 문자열이 포함되어 있으므로 contains(\"test_debug\", \"debug\")는 True를 반환합니다.\n이를 통해 실험 모드에 따라 자동으로 디버깅 기능을 활성화하거나 로그 출력을 조정할 수 있습니다.\n\n\n이러한 Resolver 기능을 활용하면 설정 파일을 더욱 동적으로 관리할 수 있습니다!"
  },
  {
    "objectID": "posts/code/2024-01-21-quarto-advanced.html",
    "href": "posts/code/2024-01-21-quarto-advanced.html",
    "title": "👩‍💻Quarto Blog + α",
    "section": "",
    "text": "Quarto를 활용하여 Github 블로그를 구축하는 이전 포스팅에 이어서, Github 블로그를 좀 더 보기 좋게, 혹은 원하는 기능을 좀 더 추가하는 몇가지 Customizing 사항들에 대해 소개하려고 합니다. 사실 블로그 Customizing은 삽질의 시작이며, 처음 시작은 가볍게 시작하지만 하나씩 옵션들을 내맘대로 건드리다가 보면 시간이 훌쩍 지나가버리는 마법을 경험하게 됩니다.\n기본적으로 .qmd파일을 작성하면서 글의 configuration을 설정하는 것은 Tutorial: Authoring에서 제목 작성, category 설정등 기본적인 부분을 확인할 수 있습니다. 기본적인 설정들에 더해서 저번 포스팅보다 여러가지 옵션들을 바꿔가면서 확인하는 과정들이 많기 때문에 localhost(로컬 컴퓨터)에서 블로그가 어떻게 바뀌어 가는지 확인하기 위해 아래의 명령어를 VSCode 터미널에 입력하여 창을 띄우며 확인해보세요!"
  },
  {
    "objectID": "posts/code/2024-01-21-quarto-advanced.html#toc",
    "href": "posts/code/2024-01-21-quarto-advanced.html#toc",
    "title": "👩‍💻Quarto Blog + α",
    "section": "ToC",
    "text": "ToC\nToC란 Table of Contents 의 약자로 블로그의 내용들이 여러 제목/소제목들로 구성되어 글이 길어질 경우, 내용을 한눈에 파악하기 쉽도록 목차를 보여주는 기능입니다. Quarto에서는 html 설정 옵션으로 이 기능을 지원하고 있으며 아래 예시 사진에서와 같이 한쪽에 ToC를 보여줘서 포스팅의 독자분들이 쉽게 내용을 파악하고 필요하는 부분만 골라서 확인하기도 용이합니다.\n\n\n\nBlog 포스팅에서 ToC 부분\n\n\nQuarto에서는 다양한 ToC 형식을 지원하고 있는데 작성하는 .qmd파일의 서두에 설정하는 포스팅 설정 부분에 toc: ture라고 추가하면 해당 .qmd파일에 #, ## 등으로 제목으로 작성했던 이름을 기반으로 해당 포스팅의 ToC가 나타나게 됩니다.\n\n\n\nqmd파일에서 ToC 설정하기\n\n\n만약 ToC의 Numbering을 자동으로 나오게 하고 싶다면 아래와 같이 number-sections: true도 포스팅 설정 부분에 추가해준다면 넘버링된 ToC가 나오는 것을 확인할 수 있습니다.\ntoc: true\nnumber-sections: true\n\n\n\nNumbering ToC로 바뀐 모습\n\n\n이 부분도 Quarto Docs에서 더 다양한 옵션들을 확인해보실 수 있으니 다양한 옵션들을 기호에 맞게 바꿔가며 확인해보세요!"
  },
  {
    "objectID": "posts/code/2024-01-21-quarto-advanced.html#이미지-넣기",
    "href": "posts/code/2024-01-21-quarto-advanced.html#이미지-넣기",
    "title": "👩‍💻Quarto Blog + α",
    "section": "이미지 넣기",
    "text": "이미지 넣기\n이미지와 관련된 작성 코드 스니펫을 소개하겠습니다. 이전에 이미지를 Typora를 통해 넣거나 Markdown 문법의 형식으로 이미지를 삽입하게 되면 ![이미지 캡션](이미지 경로)와 같이 코드로 이미지를 넣어주게 됩니다. 하지만 이렇게 이미지들을 넣게 되면 이미지 사이즈 조절이나 정렬 등을 설정해주기가 어렵기 때문에 html 문법형식을 빌려 글에 이미지들을 넣어주는 것을 추천드립니다.\n\nhtml 사용하여 이미지 삽입\n이 코드는 가운데 정렬(centering), 이미지의 (포스팅 대비 상대적인) 너비 조정(width), 이미지 캡션(figcaption)을 html 문법을 빌려 이미지를 삽입해주는 snippet입니다.\n&lt;center&gt;\n&lt;img src=\"../../images/이미지_이름.png\" width=\"50%\" /&gt;\n&lt;figcaption&gt;이미지 캡션&lt;/figcaption&gt;\n&lt;/center&gt;\n포스팅 대표 이미지 설정\n다음으로 작성글의 대표 이미지를 설정하면 블로그 메인 화면에 뜨는 그림을 정해 줄 수 있습니다.\n\n\n\n블로그 메인 화면에서 각 포스팅들의 대표 이미지들\n\n\n.qmd파일의 맨 윗부분에 설정해주는 문서 configuration에 image: ... 라인을 추가하고, ...부분에 이미지 경로를 설정해주면 포스팅의 대표 이미지를 설정할 수 있습니다. 아래는 이번 포스팅의 대표 이미지를 설정하는 예시 configuration 입니다.\n---\ntitle: \"👩‍💻Quarto Blog(2)\"\ntoc: true\ndate: \"2024-01-17\"\ndescription: Quarto로 속편한 Github Blog 구축하기(2)\ncategories: [blog, quarto, code]\nimage: ../../images/2024-01-12-quarto-blog2/blog2.jpg\n---"
  },
  {
    "objectID": "posts/code/2024-01-21-quarto-advanced.html#callout-block",
    "href": "posts/code/2024-01-21-quarto-advanced.html#callout-block",
    "title": "👩‍💻Quarto Blog + α",
    "section": "Callout Block",
    "text": "Callout Block\nCallout Block은 포스팅에서 따로 강조하고 싶거나 토글(Toggling)하고 싶은 내용이 있을 때 사용하면 유용한 작성 방법입니다. Quarto Docs의 Callout Blocks에서 볼 수 있듯이, note, warning, important, tip, caution과 같이 글의 메인 내용과 다른 포인트를 가진 내용이거나 강조하고 싶은 파트를 작성하여 보여줄 수 있습니다.\n::: {.callout-caution collapse=\"true\"}\n# Callout Block 예시\nQuarto의 Callout Block을 활용해봅시다! 생각보다 정말 유용하게 잘 쓰는 기능입니다 :)\n:::\n이 코드 스니펫을 이용하여 글에 삽입하면 아래와 같이 나옵니다. 한번 아래 Block을 클릭해보세요!\n\n\n\n\n\n\nCallout Block 예시\n\n\n\n\n\nQuarto의 Callout Block을 활용해봅시다! 생각보다 정말 유용하게 잘 쓰는 기능입니다 :)\n\n\n\n예시에서 확인할 수 있다시피 collapse=\"true\"등과 같은 Callout Block에 해당하는 다양한 옵션들도 Docs에서 찾아볼 수 있으니 다양하게 시도해보세요!"
  },
  {
    "objectID": "posts/code/2024-01-21-quarto-advanced.html#수학-수식",
    "href": "posts/code/2024-01-21-quarto-advanced.html#수학-수식",
    "title": "👩‍💻Quarto Blog + α",
    "section": "수학 수식",
    "text": "수학 수식\n논문 리뷰와 같은 글들을 작성할 때 가끔식 수학 기호나 수식들을 작성해야 할 때가 있습니다. 그래서 블로그에서 수식이 잘 작성이 되는가도 블로그 플랫폼을 결정하는 중요한 기준이 되기도 합니다. Quarto의 처음 소개에서도 말씀드렸던 것처럼 수학, 과학적 언어와 글들에 친화적인 플랫폼이기 때문에 Quarto는 수식 삽입을 TeX 문법으로 작성하면 예쁘게 랜더링 되도록 만들어진 블로그 플랫폼입니다. 사실 Markdown 문법과 다르지 않습니다. Quarto Docs Equations에서 볼 수 있듯이 $로 수식을 warpping하면 수식이 잘 랜더링 되는 것을 볼 수 있습니다.\nhtml에서 수식을 다양한 형식으로 rendering할 수 있도록 옵션도 제공하고 있습니다. Format Options 테이블에 있는 html-math-method옵션을 활용해서 plain, webtex, gladtex, mathml, mathjax, katex 등과 같은 옵션을 사용할 수 있습니다. 예를들면 아래와 같이 _quarto.yml 설정 파일에 html-math-method: katex 한 줄 추가하면 수식 랜더링 스타일을 바꿀 수 있습니다.\nformat: \n  html:\n    html-math-method: katex"
  },
  {
    "objectID": "posts/code/2024-01-21-quarto-advanced.html#글자색-설정",
    "href": "posts/code/2024-01-21-quarto-advanced.html#글자색-설정",
    "title": "👩‍💻Quarto Blog + α",
    "section": "글자색 설정",
    "text": "글자색 설정\n글자색을 기본 테마에서 설정한 색이 아닌, 파란색, 빨간색 등 다른 색을 사용하고 싶다면 아래와 같이 html 문법을 활용하여 작성하면 됩니다.\n&lt;span style=\"color: blue\"&gt;파란색 글씨를 작성&lt;/span&gt;"
  },
  {
    "objectID": "posts/code/2024-01-21-quarto-advanced.html#toggle",
    "href": "posts/code/2024-01-21-quarto-advanced.html#toggle",
    "title": "👩‍💻Quarto Blog + α",
    "section": "Toggle",
    "text": "Toggle\n토글은 Callout Block을 활용해서 작성할 수도 있지만 기본 html 문법을 활용해서 아래와 같이 작성할 수도 있습니다.\n&lt;details&gt;\n&lt;summary&gt;&lt;b&gt;Toggle 제목&lt;/b&gt;&lt;/summary&gt;\nToggle 내용\n&lt;/details&gt;\n아래와 같이 Toggle 파트가 랜더링 됩니다.\n\n\nToggle 제목\n\nToggle 내용\n\n\nQuarto Blog(1)포스팅에서 Quarto로 기본적인 블로그를 만들고 나서 이번 포스팅의 여러 커스텀 옵션들을 활용하면 각자 원하는 Github Blog를 손쉽게 만들 수 있습니다. 그리고 기본적인 html의 문법을 따라가기 때문에 이번 포스팅에서 몇부분들은 html 문법에 익숙하신 분들에게는 당연한 내용이지 않을까 싶습니다. Quarto를 쓰지 않더라도 블로그를 만들 수 있는 방법은 많지만 처음 소개글에서 이야기 했던 제가 Quarto를 알게 된 배경과 니즈를 듣고 저와 같은 비슷한 상황이시라면 정말 좋은 툴이라고 생각하실 거라고 확신합니다. 만약 아직 Github Blog 초심자라면 Quarto를 한번 고려해보시는 것을 다시 한번 권장해드리며 이만 마치겠습니다.\n\nReference\n\nQuarto Homepage\nHypothesis"
  },
  {
    "objectID": "posts/code/2023-04-04-install-orbit.html",
    "href": "posts/code/2023-04-04-install-orbit.html",
    "title": "👩‍💻Orbit 설치하기",
    "section": "",
    "text": "Orbit은 IsaacLab으로 변경되었습니다.\n\n\n\n\n\n\n이번 포스팅은 Nvidia의 Omniverse Isaac Orbit에 대해 알아보고, Orbit Docs - Installation Guide를 따라 설치과정을 기록해보았습니다. 공식 문서를 참고하여 설치부터 예제 코드들을 살펴보며 시리즈로 포스팅할 예정입니다.\n\n1 Orbit?\n그럼, Orbit 이란 무엇을 말하는 걸까요? 공식 Docs에서는 다음과 같이 설명하고 있습니다.\n\n\n\n\n\n\nOverview\n\n\n\nIsaac Orbit (or orbit in short) is a unified and modular framework, built on top of NVIDIA Omniverse and Isaac Sim, for robot learning. It offers a modular design to easily and efficiently create robot learning environments with photo-realistic scenes, and fast and efficient simulation.\n\n\n\n\nOrbit Github Main Image\n\n\n\n소개글을 보면, 크게 2가지 특징을 살펴볼 수 있습니다. 첫번째로는 Robot learning을 위한 모듈화된 프레임워크라는 점을 알 수 있고 두번째로는 Omniverse와 Isaac Sim이라는 것에 기반한 프로그램이라는 것 입니다. Robot learning을 위한 이라는 말에서 Github 메인 사진에서도 볼 수 있듯이 로봇 작동을 위한 모든 learning 과정을 지원하기 위한 프레임 워크라는 것을 알 수 있습니다. 매니퓰레이터부터 핸드 로봇, 사족보행 로봇에 이르기까지 다양하고 폭넓은 로봇의 작동 시나리오를 지원해주는 프로그램이라는 것을 볼 수 있습니다.\n하지만 Orbit이라는 프레임워크가 Omniverse와 Isaac Sim에 기반한다는 설명은 이전에 Nvidia의 시뮬레이터에 대해 알고 있지 못하신 분들이라면 생소하고 헷갈릴 수 있습니다. Nvidia의 시뮬레이터 개발과정이나 제품에 대해 깊이 알아보고자 하는 것이 이번 포스팅의 목적은 아니므로 간단하게 각 프로그램들의 역학관계를 살펴보자면, Omniverse &gt; Isaac Sim &gt; Orbit의 관계라고 파악해볼 수 있습니다. Omniverse가 가장 큰 범위의 가상세계를 위한 플랫폼이고 그 아래에 로봇틱스 분야를 위한 하위 플랫폼 Isaac Sim이 있고 마지막으로 그 안에 오늘 포스팅의 주인공인 Orbit이 있는 것 입니다.\n여기까지 설명을 들었을 때 명확히 이해가 되지 않는 부분이 있을 수 있습니다. Isaac Sim도 로봇틱스 분야를 위한 하위 플랫폼인데 그 안에 Orbit으로 또 따로 Robot learning이라는 모듈 프레임워크가 더 필요할까 의문이 들 수 있습니다. 심지어 이전에 Nvidia의 시뮬레이터를 조금 아셨던 분들이라면, Isaac Gym이라는 강화학습, 즉 robot learning을 위한 (Issac Sim보다 좀 더 기능이 제한된) 프로그램이 있는데 Orbit은 어떤 위치인지 더 애매모호하게 느껴질 수 있습니다. 실제로 저도 기존에 Isaac Gym으로 강화학습을 진행해왔던 사람으로써 Orbit의 등장은 의아한 점이 있었습니다. 이런 의문을 많은 유저들도 느꼈던 것 같습니다. 포럼에 올라온 Q&A에서 공식 Orbit Maintainer의 답변으로는, Orbit은 Isaac Sim의 진입장벽을 낮추기 위한 내장 프레임워크로 2가지의 목적이 있어서 만들어졌다고 합니다. (Q&A 원본은 링크와 함께 아래에 적어두었으니 참고하시길 바랍니다.)\n\n보다 간소화된 인터페이스를 통해서 로봇 학습을 위한 환경 설계 및 강화학습, 모션 플래닝 등과 같은 로봇틱스 워크 플로우를 지원하는 것\n미리 구축된 환경을 벤치마크 예제로 사용하는 프론트 엔드 프레임워크로서 준비된 에셋과 환경 예제를 통해 warm-start를 할 수 있도록 하는 것\n\n\n\n\n\n\n\nIsaacSim과 Orbit의 차이에 대한 Q&A\n\n\n\n\n\nQ: I have two questions about the Orbit.\nMay I know what is the purpose of orbit? It seems like it is a modular framework for especially robot learning. However, reinforcement learning can be realized by Issac Sim (Issac Gym). So, what is so special about Orbit?\nIn the Orbit tutorial, the example always uses the built-in API, e.g. robots and controllers. Is the Orbit an appropriate platform if we want to use custom robots and controllers?\nThank you.\n\nA: In the following extract from the Orbit maintainer’s public post published in the Omniverse Discord channel dedicated to Isaac Sim\n(…) Isaac Orbit- Batteries included framework to reduce barrier to entry. It serves a dual purpose:\n\nsimplified interface for env design and support for many robotics workflows - RL, Motion planning, teleoperation, imitation learning/behavior cloning, and real robot operation. This unification is the USP of orbit as compared to other interfaces.\nFront end framework for prebuilt environments as benchmark examples. (…) We note that many folks in the community are users of IsaacSim, creating new environments rather than physics solvers. Hence, we hope to provide Orbit to warmstart it with prebuilt assets, and environment examples as benchmarks.\n\nImportantly Orbit is designed such that it can accept community contributions with open licensing. We hope that Orbit will be the environment zoo for IsaacSim with contributions from the community as well as internal development.\nOrbit also has modified env interfaces. However, Orbit is open source, and users can modify & suggest change to these interfaces, as needed. (…)\n\n\n\n정리하자면, Orbit은 Robot Learning을 위한 Nvidia의 오픈 소스 프레임워크인데 가장 사용자에게 친근한(쉽게 시작할 수 있는) 프로그램 이라고 정리해볼 수 있을 것 같습니다. 비유를 들어서 Nvidia 플랫폼의 역학을 정리해보자면, Omniverse는 MS Office, Issac Sim은 Power Point, Orbit은 PPT 디자인 추천 프로그램 정도로 정리하고 넘어갈 수 있을 것 같습니다.\n\n\n2 Orbit을 설치하기 전에\n앞에서 정리한 대로 Orbit은 Omniverse 안에 Isaac Sim을 기반으로 돌아가는 모듈이기 때문에 IsaacSim을 먼저 설치해야 합니다. 엄밀히 말하자면 아직 Orbit을 설치한 것은 아니고, Orbit이라는 기능을 사용하기 위해 준비하는 과정이라고 생각하시면 될 것 같습니다. Isaac Sim은 크게 (1)Workstation Installation과 (2)Container Installation 2가지 방법으로 설치할 수 있습니다. 각자 개인 컴퓨터를 사용하여 이용하고자 한다면 첫번째 Workstation Installation을 따라가면 되므로 이번 포스팅에서는 Workstation Installation 방법을 따라서 설치를 진행하겠습니다.\n\nWorkstation Installation: 공식 홈페이지의 Isaac Sim 설치방법(Workstation Installation)을 따라 Omniverse App을 통해 Isaac Sim 2022.2을 설치합니다. 아래는 제가 설치를 진행했던 스펙입니다. (만약 해당 컴퓨터 자원이 없다면 클라우드 자원을 이용하여 Isaac Sim을 사용할 수 있습니다.)\n\n\n\n\n\n\n\n설치 진행 환경\n\n\n\n\nUbuntu 20.04 LTS\nNvidia RTX 4080\nDriver Version: 525.60.13\n\n\n\nIsaac Sim의 설치과정은 Documentation에 step by step으로 잘 나와있으므로 자세하게 설명하진 않겠습니다. 아래와 같이 Omniverse App에서 Isaac Sim이 보이고 LANCH라는 버튼을 눌러 실행할 수 있으면 설치에 성공한 것 입니다. (23.04.05 기준) 최신 버젼 2022.2.1이 맞는지 꼭 확인해주시고 만약 최신 버젼이 아니라면 LANCH 옆의 list 버튼을 눌러서 최신 버젼을 다운받고 버젼을 선택할 수 있습니다. 아래 사진에서 처럼 2022.2.1 버젼으로 선택되어 있는 것을 확인할 수 있습니다.\n\n\nOmniverse App에서 Isaac Sim 버젼 확인하기\n\n\n\n\nPython Environment Setting: Isaac Sim은 내부(built-in) Python 3.7이 있고 이 default Python 환경을 사용하는 것이 좋습니다. IsaacSim Default Python Environment을 사용하기 위해서는 아래의 예시 command와 같이 Isaac Sim의 Root folder(Isaac Sim 설치시 별도의 경로 설정이 없이 설치했을 경우 ${HOME}/.local/share/ov/pkg/isaac_sim-*, *은 해당 Isaac Sim의 버젼)에 있는 Python 실행파일인 python.sh를 가지고 실행해야 합니다.\n# ${HOME}/.local/share/ov/pkg/isaac_sim-2022.2.1\n./python.sh path/to/script.py\n\n\nIsaac Sim의 Default Folder\n\n\n\n하지만 이와 같이 매번 Isaac Sim의 내부 python 실행파일 경로를 입력하여 python 스크립트를 돌리는 것은 매우 번거로우므로 시스템 환경 변수 설정를 통해 간단히 내부 python을 불러올 수 있습니다. Linux Ubuntu에서는 시스템 환경 변수를 설정하는 파일은 ~/.bashrc 나 ~/.zshrc를 사용합니다. 아래와 같이 리눅스 GUI 환경에서 지원하는 텍스트 에디터인 gedit을 이용하여 시스템 환경 변수를 설정해보겠습니다. Terminal 창을 열어서 command를 입력합니다.\ngedit ~/.bashrc\n팝업되는 .bashrc 파일의 맨 아래에 다음과 같이 입력합니다. #으로 훗날 여러 시스템 환경 변수들을 왜 설정했는지 메모하기 위해서 주석을 달아놓는 것을 권장합니다.(Orbit을 사용하지 않고 Isaac Sim만 설치하는 공식 문서에서는 같은 파이썬 실행파일을 다른 환경 변수로 설치하기 때문에 중복되어 이름을 설치 하지 않도록 주의합니다.) 추가한 후 저장하고 창을 닫습니다.\n# Isaac Sim root directory\nexport ISAACSIM_PATH=\"${HOME}/.local/share/ov/pkg/isaac_sim-2022.2.1\"\n# Isaac Sim python executable\nexport ISAACSIM_PYTHON_EXE=\"${ISAACSIM_PATH}/python.sh\"\n파일을 편집했다고 바로 추가된 환경 변수가 바로 적용되는 것이 아니라 아래의 command까지 터미널에서 실행시키고 나서 적용이 됩니다.\nsource ~/.bashrc\n실행 확인 : 프로그램 설치와 환경변수까지 제대로 설정되었는지 확인하기 위해 Running the simulator에 안내되어 있는 몇 가지 점검 command를 실행시켜 보겠습니다.\n\n시뮬레이터 실행 파일(isaac-sim.sh) 확인\n# note: you can pass the argument `--help` to see all arguments possible.\n$ISAACSIM_PATH/isaac-sim.sh --help\n\n시뮬레이터 내부 파이썬 실행 파일(python.sh) 확인\n# checks that python path is set correctly\n$ISAACSIM_PYTHON_EXE -c \"print('Isaac Sim configuration is now complete.')\"\n\n시뮬레이터 standalone 예제(add_cubes.py)로 실행 확인\n# checks that Isaac Sim can be launched from python\n$ISAACSIM_PYTHON_EXE $ISAACSIM_PATH/standalone_examples/api/omni.isaac.core/add_cubes.py\n\n\nadd_cubes.py 실행 결과\n\n\n\n\n\n\n\n3 Orbit 설치하기\n이제 Orbit을 본격적으로 설치해보려고 합니다. 앞서 Isaac Sim이 제대로 설치가 되고 예제까지 돌아가는지 꼭 확인을 한 후 진행해주시기 바랍니다.\n\nOrbit repository를 git clone 해오기\n# Option 1: With SSH\ngit clone git@github.com:NVIDIA-Omniverse/orbit.git\n# Option 2: With HTTPS\ngit clone https://github.com/NVIDIA-Omniverse/Orbit.git\nSymbolic link 생성하기: 심볼릭 링크란 원본 파일을 가리키도록 하는 윈도우에서의 바로가기 아이콘과 같은 의미로 이해하시면 됩니다. 위에서 git clone한 repo의 위치로 들어가서 다음과 같은 명령어를 통해 심볼릭 링크를 만들어 줍니다.\n# enter the cloned repository\ncd Orbit\n# create a symbolic link\nln -s $ISAACSIM_PATH _isaac_sim\nOrbit 실행파일 명령어 등록하기: orbit을 실행하기 위해서는 매번 git clone 한 repository에 있는 orbit.sh 파일을 써주어야 합니다. 만약 orbit.sh이 있는 위치에서 실행시킨다면 ./orbit.sh 로 간단하겠지만 다른 위치의 파일을 Orbit에서 실행시키고 싶을 때 단축키가 있다면 훨씬 간편하게 실행할 수 있어 편리할 것 입니다. Orbit을 편리하게 실행시킬 수 있는 명령어로는 orbit으로 정해서 즉, ./orbit.sh 와 orbit 이 같도록 하는 작업을 진행해보겠습니다. 앞서 리눅스의 시스템 환경변수를 설정하는 방법에서 .bashrc 파일을 이용하여 명령어 별칭을 등록하는 방법을 이용합니다.\necho -e \"alias orbit=$(pwd)/orbit.sh\" &gt;&gt; ${HOME}/.bashrc\n별칭 등록을 적용하기 위해 source ~/.bashrc도 잊지 않고 실행해주세요.(명령어가 제대로 등록됐는지 확인해보려면 gedit ~/.bashrc를 통해 파일에서 맨 아랫줄에 alias orbit=$(Orbit 레포지토리 위치)/orbit.sh이 써있는지 확인해보면 됩니다.) 터미널에서 아래의 명령어를 입력했을때 다음과 같은 창이 나온다면 명령어 별칭 등록이 잘 된 것 입니다.\norbit --help\n\n\norbit 명령어 별칭 확인\n\n\n\n\n\n\n4 Orbit에서 가상환경 설정\nOrbit에서는 orbit -p라는 명령어를 통해 Isaac Sim의 파이썬 실행파일을 자동으로 가져와서 사용합니다. (혹은 위에서 명령어 별칭을 등록안했다면 Orbit repo 위치에서 ./orbit.sh -p라고 입력해도 됩니다.) 하지만 가상환경을 사용하고 싶다면 Conda를 이용하면 됩니다.\norbit -c (또는 --conda)\n위와 같이 입력하면 자동으로 orbit이라는 이름의 가상환경이 만들어지고 앞으로 이 가상환경을 사용하고 싶다면 conda activate orbit을 통해 활성화 시킬 수 있습니다.\n\n\nConda를 이용하여 가상환경 설치 및 확인\n\n\n\n\n\n5 Extensions 설치\n\napt를 이용해서 우분투에서 디펜던시를 설치합니다.\nsudo apt install cmake build-essential\nOrbit repo에 있는 source/extensions 폴더의 확장프로그램들을 설치합니다. 이때 --editable flag로 편집 가능한 모드로 설치하게 되어 개발자가 확장 기능을 수정하게 되면 변경사항이 즉시 적용됩니다.\norbit --install # or orbit -i\n(rsl-rl 같은)Learning framework 등의 여러 디펜던시 프로그램들을 설치합니다.\norbit --extra # or orbit -e\n\n\n\n6 Closing\n마지막까지 설치과정을 잘 마치셨다면 아래의 명령어를 입력 했을 때 사족보행 로봇들이 나오면서 Orbit이 잘 실행되는 것을 확인하실 수 있습니다!👏👏👏(처음 실행시 시간이 조금 걸릴 수 있으니 기다려주세요.)\norbit -p source/standalone/demo/play_quadrupeds.py\n\n\nplay_quadrupeds.py 실행 결과\n\n\n\n이번 포스팅에서는 Nvidia의 Orbit에 대해 간단히 알아보고 설치까지 진행해보았습니다. 아직 공식 Documentation도 업데이트 중이고 프로그램 자체도 업데이트가 활발히 되고 있어서 이후에 이번 포스팅이 버젼에 따라 도움이 안될 수도 있겠지만 많은 분들께 참고가 되었으면 좋겠습니다. 설치과정에 대해 다른 옵션들이나 자세하게 알고 싶으신 분들은 공식 설치 Documentation을 참고하시면 업데이트 되는 소식들과 함께 더 자세하고 많은 정보를 얻으실 수 있을 것 같습니다. 이어지는 포스팅에서는 각 예제들을 통해서 Orbit에 대해 더 알아보도록 하겠습니다."
  },
  {
    "objectID": "posts/code/2025-02-02-realsense-ros2.html",
    "href": "posts/code/2025-02-02-realsense-ros2.html",
    "title": "👩‍💻ROS2 RealSense C++ Node",
    "section": "",
    "text": "이번 포스팅에서는 Intel RealSense 카메라로 촬영한 비디오를 실시간 스트리밍하고, 영상에서 감지한 AprilTag을 OpenCV를 활용해 시각적으로 표시하는 ROS 2 기능을 C++ 노드로 구현하는 실습을 진행하겠습니다. AprilTag는 로봇 비전, 증강 현실, 마커 기반 위치 추정 등에 활용되는 태그 시스템입니다. 또한, 본 포스팅에서는 RealSense 카메라를 ROS 2에서 사용하기 위해 librealsense2를 설정하는 과정과 일반적으로 발생할 수 있는 빌드 문제 해결 방법도 함께 다룰 예정입니다.\n이 간단한 ROS2 프로젝트를 시작하기 앞서, 아래의 사전 준비 사항들을 확인해서 먼저 환경을 셋팅해주시기 바랍니다."
  },
  {
    "objectID": "posts/code/2025-02-02-realsense-ros2.html#overview",
    "href": "posts/code/2025-02-02-realsense-ros2.html#overview",
    "title": "👩‍💻ROS2 RealSense C++ Node",
    "section": "Overview",
    "text": "Overview\n앞서 설명한 것처럼, 가장 단순한 ROS2 패키지 구조를 가지고 진행할 예정입니다. 2개의 C++ Node들, 즉 Publisher와 Subcriber를 만들어 Realsense Camera D457에서 이미지를 보내주면(Publisher) 받은 이미지를 토대로 Tag ID를 인식하여 OpenCV로 간단한 ploting을 한 창(Subsciber)을 띄워볼 예정입니다. 각 노드들의 역할을 정리해보면 아래와 같습니다.\n\nPublisher Node: RealSense 카메라에서 이미지를 수집하여 ROS 2 토픽으로 퍼블리싱하는 노드\n\nlibrealsense2 패키지를 이용하여 RealSense camera를 연결\n640×480 해상도, 30 FPS 칼라 이미지 프레임을 스트리밍\nROS 2 토픽으로 (image_raw) 이미지 프레임 Publish\n\nSubscriber Node: RealSense 카메라에서 영상을 받아 OpenCV로 처리하고, AprilTag 라이브러리를 활용하여 태그를 감지하는 기능을 수행하는 노드\n\nimage_raw 토픽을 Subscribe\ncv_bridge를 이용하여 OpenCV Mat으로 이미지를 변환\nOpenCV window에 간단한 표시를한 이미지 띄우기"
  },
  {
    "objectID": "posts/code/2025-02-02-realsense-ros2.html#creating-the-ros-2-package",
    "href": "posts/code/2025-02-02-realsense-ros2.html#creating-the-ros-2-package",
    "title": "👩‍💻ROS2 RealSense C++ Node",
    "section": "Creating the ROS 2 Package",
    "text": "Creating the ROS 2 Package\nmy_realsense_example 라는 새로운 ROS 2 package를 만들어 시작해보겠습니다. ROS2의 workspace에 있는 src/폴더에서 C++ 패키지를 생성해줍니다.\ncd ~/ros2_ws/src\nros2 pkg create my_realsense_example \\\n  --build-type ament_cmake \\\n  --dependencies rclcpp sensor_msgs cv_bridge\n그다음, my_realsense_example/src/ 에 각 노드에 대한 C++ 스크립트 파일을 작성합니다. - realsense_publisher.cpp - image_subscriber.cpp"
  },
  {
    "objectID": "posts/code/2025-02-02-realsense-ros2.html#publisher-node-realsense_publisher.cpp",
    "href": "posts/code/2025-02-02-realsense-ros2.html#publisher-node-realsense_publisher.cpp",
    "title": "👩‍💻ROS2 RealSense C++ Node",
    "section": "Publisher Node: realsense_publisher.cpp",
    "text": "Publisher Node: realsense_publisher.cpp\n우선, 카메라 센서에서 이미지를 받아 송출하는 Publisher 노드 코드를 살펴보겠습니다. 먼저, 아래와 같이 필요한 헤더와 패키지들을 포함합니다.\n\nlibrealsense2/rs.hpp: RealSense 카메라를 제어\n\nrclcpp/rclcpp.hpp: ROS 2 노드를 생성 및 관리\n\nsensor_msgs/msg/image.hpp: ROS 2의 이미지 메시지를 처리\n\ncv_bridge/cv_bridge.h: ROS 메시지와 OpenCV 이미지 간 변환\n\nopencv2/opencv.hpp: OpenCV 기능 활용\n\n이러한 요소들을 포함하여, RealSense 카메라에서 영상을 받아 ROS 2 메시지로 변환하고 퍼블리싱하는 노드를 구현할 수 있습니다.\n#include &lt;memory&gt;\n#include &lt;chrono&gt;\n#include &lt;librealsense2/rs.hpp&gt;           // Intel RealSense SDK\n#include \"rclcpp/rclcpp.hpp\"\n#include \"sensor_msgs/msg/image.hpp\"     // ROS 2 Image message\n#include \"cv_bridge/cv_bridge.h\"         // cv_bridge for ROS &lt;-&gt; OpenCV\n#include \"opencv2/opencv.hpp\"            // OpenCV\n본격적으로 RealSensePubliserClass를 살펴보면, RealSense 카메라에서 수집한 영상을 OpenCV 형식으로 변환한 후, 이를 ROS 2의 이미지 메시지로 변환하여 주기적으로 퍼블리싱하는 과정이 진행됩니다. 이 클래스 노드를 통해 송출된 이미지를 다른 ROS 2 노드가 Subscribe하고 활용할 수 있습니다.\n크게 2 부분으로 나누어서 볼 수 있습니다.\n\n노드 초기화 (RealSensePublisher)\n\nrclcpp::Node를 상속받아 \"realsense_publisher\"라는 이름의 ROS 2 노드를 생성합\nrs2::pipeline을 설정하여 RealSense 카메라에서 데이터를 스트리밍할 준비\nrs2::config를 사용해 컬러 영상 스트리밍을 640x480 해상도, BGR8 포맷, 30 FPS로 설정\ncreate_publisher&lt;sensor_msgs::msg::Image&gt;(\"image_raw\", 10);를 통해 \"image_raw\" 토픽으로 영상을 퍼블리싱할 퍼블리셔를 생성\ncreate_wall_timer를 사용하여 100ms(약 10Hz) 간격으로 프레임을 가져와 퍼블리싱하는 타이머를 설정\n\n프레임 캡처 및 퍼블리싱 (publishFrame)\n\npipeline_-&gt;wait_for_frames();를 사용해 RealSense 카메라에서 새로운 프레임을 가져옴\nframeset.get_color_frame();을 통해 컬러 프레임을 추출하고, 존재하지 않으면 경고 메시지를 출력\ncv::Mat을 사용하여 RealSense의 컬러 프레임 데이터를 OpenCV 형식으로 변환\ncv_bridge::CvImage를 활용해 OpenCV 이미지를 ROS 2의 sensor_msgs::msg::Image 형식으로 변환한 후 타임스탬프와 프레임 ID를 추가\npublisher_-&gt;publish(*image_msg);를 통해 변환된 이미지를 \"image_raw\" 토픽으로 퍼블리싱\n\n\nclass RealSensePublisher : public rclcpp::Node\n{\npublic:\n  RealSensePublisher()\n  : Node(\"realsense_publisher\")\n  {\n    pipeline_ = std::make_shared&lt;rs2::pipeline&gt;();\n\n    // Configure and start the pipeline to stream color images\n    rs2::config cfg;\n    cfg.enable_stream(RS2_STREAM_COLOR, 640, 480, RS2_FORMAT_BGR8, 30);\n    pipeline_-&gt;start(cfg);\n\n    publisher_ = this-&gt;create_publisher&lt;sensor_msgs::msg::Image&gt;(\"image_raw\", 10);\n\n    timer_ = this-&gt;create_wall_timer(\n      std::chrono::milliseconds(100),  // ~10 Hz\n      std::bind(&RealSensePublisher::publishFrame, this)\n    );\n\n    RCLCPP_INFO(this-&gt;get_logger(), \"RealSense Publisher Node started.\");\n  }\n\nprivate:\n  void publishFrame()\n  {\n    // Wait for the next set of frames\n    rs2::frameset frameset = pipeline_-&gt;wait_for_frames();\n    rs2::video_frame color_frame = frameset.get_color_frame();\n    if (!color_frame)\n    {\n      RCLCPP_WARN(this-&gt;get_logger(), \"No color frame received.\");\n      return;\n    }\n\n    // Convert RealSense frame to OpenCV Mat\n    cv::Mat color_image(\n      cv::Size(color_frame.get_width(), color_frame.get_height()),\n      CV_8UC3,\n      (void*)color_frame.get_data(),\n      cv::Mat::AUTO_STEP\n    );\n\n    // Convert to ROS Image message via cv_bridge\n    auto image_msg = cv_bridge::CvImage(std_msgs::msg::Header(), \"bgr8\", color_image).toImageMsg();\n    image_msg-&gt;header.stamp = this-&gt;get_clock()-&gt;now();\n    image_msg-&gt;header.frame_id = \"realsense_color_frame\";\n\n    publisher_-&gt;publish(*image_msg);\n  }\n\n  std::shared_ptr&lt;rs2::pipeline&gt; pipeline_;\n  rclcpp::Publisher&lt;sensor_msgs::msg::Image&gt;::SharedPtr publisher_;\n  rclcpp::TimerBase::SharedPtr timer_;\n};\n마지막으로 main에서는 ROS 2 노드 실행의 진입점으로, RealSensePublisher 노드를 생성하고 실행하는 역할을 합니다. 이를 통해 RealSense 카메라에서 받은 영상을 ROS 2의 \"image_raw\" 토픽으로 지속적으로 퍼블리싱하는 노드가 실행됩니다.\nint main(int argc, char *argv[])\n{\n  rclcpp::init(argc, argv); // ROS 2 노드 실행을 위한 초기화\n\n  rclcpp::spin(std::make_shared&lt;RealSensePublisher&gt;()); // `RealSensePublisher` 노드를 생성하고 실행\n  /* \n  `spin()` 함수는 노드가 지속적으로 실행되도록 유지하며, \n  콜백 함수(`publishFrame()`)를 주기적으로 호출하여 RealSense 카메라에서 영상을 가져오고 퍼블리싱합니다. \n  */\n\n  rclcpp::shutdown(); // 프로그램 종료 시 ROS 2를 안전하게 종료\n  return 0;\n}\n각 부분을 살펴본 Publisher 노드를 통합하면 아래와 같이 realsense_publisher.cpp가 완성됩니다.\n\n\n\n\n\n\nrealsense_publisher.cpp\n\n\n\n\n\n#include &lt;memory&gt;\n#include &lt;chrono&gt;\n#include &lt;librealsense2/rs.hpp&gt;           // Intel RealSense SDK\n#include \"rclcpp/rclcpp.hpp\"\n#include \"sensor_msgs/msg/image.hpp\"     // ROS 2 Image message\n#include \"cv_bridge/cv_bridge.h\"         // cv_bridge for ROS &lt;-&gt; OpenCV\n#include \"opencv2/opencv.hpp\"            // OpenCV\n\nclass RealSensePublisher : public rclcpp::Node\n{\npublic:\n  RealSensePublisher()\n  : Node(\"realsense_publisher\")\n  {\n    pipeline_ = std::make_shared&lt;rs2::pipeline&gt;();\n\n    // Configure and start the pipeline to stream color images\n    rs2::config cfg;\n    cfg.enable_stream(RS2_STREAM_COLOR, 640, 480, RS2_FORMAT_BGR8, 30);\n    pipeline_-&gt;start(cfg);\n\n    publisher_ = this-&gt;create_publisher&lt;sensor_msgs::msg::Image&gt;(\"image_raw\", 10);\n\n    timer_ = this-&gt;create_wall_timer(\n      std::chrono::milliseconds(100),  // ~10 Hz\n      std::bind(&RealSensePublisher::publishFrame, this)\n    );\n\n    RCLCPP_INFO(this-&gt;get_logger(), \"RealSense Publisher Node started.\");\n  }\n\nprivate:\n  void publishFrame()\n  {\n    // Wait for the next set of frames\n    rs2::frameset frameset = pipeline_-&gt;wait_for_frames();\n    rs2::video_frame color_frame = frameset.get_color_frame();\n    if (!color_frame)\n    {\n      RCLCPP_WARN(this-&gt;get_logger(), \"No color frame received.\");\n      return;\n    }\n\n    // Convert RealSense frame to OpenCV Mat\n    cv::Mat color_image(\n      cv::Size(color_frame.get_width(), color_frame.get_height()),\n      CV_8UC3,\n      (void*)color_frame.get_data(),\n      cv::Mat::AUTO_STEP\n    );\n\n    // Convert to ROS Image message via cv_bridge\n    auto image_msg = cv_bridge::CvImage(std_msgs::msg::Header(), \"bgr8\", color_image).toImageMsg();\n    image_msg-&gt;header.stamp = this-&gt;get_clock()-&gt;now();\n    image_msg-&gt;header.frame_id = \"realsense_color_frame\";\n\n    publisher_-&gt;publish(*image_msg);\n  }\n\n  std::shared_ptr&lt;rs2::pipeline&gt; pipeline_;\n  rclcpp::Publisher&lt;sensor_msgs::msg::Image&gt;::SharedPtr publisher_;\n  rclcpp::TimerBase::SharedPtr timer_;\n};\n\nint main(int argc, char *argv[])\n{\n  rclcpp::init(argc, argv);\n  rclcpp::spin(std::make_shared&lt;RealSensePublisher&gt;());\n  rclcpp::shutdown();\n  return 0;\n}"
  },
  {
    "objectID": "posts/code/2025-02-02-realsense-ros2.html#subscriber-node-image_subscriber.cpp",
    "href": "posts/code/2025-02-02-realsense-ros2.html#subscriber-node-image_subscriber.cpp",
    "title": "👩‍💻ROS2 RealSense C++ Node",
    "section": "Subscriber Node: image_subscriber.cpp",
    "text": "Subscriber Node: image_subscriber.cpp\n다음으로 위에서 Publisher 노드가 보내준 이미지 데이터를 받는 Subscriber 노드에 대해 살펴보겠습니다.\n우선 아래와 같이 필요한 헤더 파일들을 포함합니다.\n\nC++ 표준 라이브러리 포함 (&lt;memory&gt;, &lt;string&gt;, &lt;iostream&gt;)\n\n스마트 포인터(std::shared_ptr), 문자열(std::string), 표준 입출력(std::cout)을 사용하기 위한 헤더 파일\n\nROS 2 관련 헤더 포함\n\nrclcpp/rclcpp.hpp: ROS 2 노드 및 퍼블리셔/구독자 기능을 사용하기 위한 기본 라이브러리\nsensor_msgs/msg/image.hpp: ROS 2에서 이미지 메시지(sensor_msgs::msg::Image) 를 다루기 위한 메시지 타입\n\ncv_bridge/cv_bridge.h: ROS 이미지 메시지를 OpenCV의 cv::Mat으로 변환하거나, 반대로 OpenCV 이미지를 ROS 메시지로 변환하기 위해 사용\n\nOpenCV 관련 헤더 포함\n\nopencv2/highgui.hpp: OpenCV의 GUI 기능 (cv::imshow(), cv::waitKey())을 사용하여 이미지를 표시할 때 필요\nopencv2/imgproc.hpp: 이미지 처리 기능(예: 필터링, 변형, 색 변환 등)을 수행하기 위한 라이브러리\n\nAprilTag 감지를 위한 라이브러리 포함\n\nextern \"C\" {} 블록을 사용하여 C 스타일로 작성된 AprilTag 라이브러리를 C++ 코드에서 사용할 수 있도록 합니다.\n\n#include &lt;apriltag.h&gt;: AprilTag 감지 시스템의 핵심 헤더\n#include &lt;tag25h9.h&gt;: AprilTag 25h9 태그 세트를 사용하기 위한 헤더 파일(25x9 해상도를 가진 태그 세트)\n#include &lt;tag36h11.h&gt;는 주석 처리되어 있으며, 필요 시 AprilTag 36h11 세트(더 정밀한 태그)를 사용할 수 있도록 변경 가능\n\n\n#include &lt;memory&gt;\n#include &lt;string&gt;\n#include &lt;iostream&gt;\n\n// ROS 2\n#include \"rclcpp/rclcpp.hpp\"\n#include \"sensor_msgs/msg/image.hpp\"\n#include \"cv_bridge/cv_bridge.h\"\n\n// OpenCV\n#include \"opencv2/highgui.hpp\"\n#include \"opencv2/imgproc.hpp\"\n\n// AprilTag library headers\nextern \"C\" {\n#include &lt;apriltag.h&gt;\n// #include &lt;tag36h11.h&gt;\n#include &lt;tag25h9.h&gt;\n}\n다음으로 ImageSubscriber Class를 보겠습니다. RealSense 카메라에서 수신한 영상을 처리하고, AprilTag을 검출하여 화면에 표시하는 ROS 2 노드를 구현합니다. OpenCV를 사용하여 영상을 변환하고, AprilTag 라이브러리를 이용해 태그를 감지 및 시각화하여 검출된 태그의 ID, 꼭짓점, 중심을 화면에 시각적으로 표시하는 과정이 구현되어 있습니다. 크게 세 부분으로 나누어서 보면,\n1. 노드 초기화 (ImageSubscriber)\n\n\"image_raw\" 토픽을 구독(subscription_)하여 카메라 영상을 수신\nOpenCV 창 \"RealSense Camera + AprilTag\"를 생성하여 화면에 영상을 표시할 준비\nAprilTag 검출기 초기화:\n\ntag_family_ = tag25h9_create(); → AprilTag 25h9 태그 세트를 사용하도록 설정\ntag_detector_ = apriltag_detector_create(); → AprilTag 검출기를 생성\napriltag_detector_add_family(tag_detector_, tag_family_); → 특정 태그 패밀리를 검출기에 추가\n검출기의 매개변수(예: quad_decimate, quad_sigma)를 조정하여 성능 최적화 가능\n\n\n2. 이미지 수신 및 AprilTag 검출 (imageCallback): ROS 2의 \"image_raw\" 토픽에서 새로운 이미지가 수신될 때 호출\n\nROS 2 이미지 메시지 → OpenCV cv::Mat 변환\n\ncv_bridge::toCvCopy(msg, \"bgr8\")-&gt;image;를 사용하여 ROS 2의 sensor_msgs::msg::Image를 OpenCV 형식(cv::Mat)으로 변환합니다\n변환 중 오류 발생 시 예외를 처리\n\n그레이스케일 변환 (AprilTag 검출은 흑백 영상이 필요)\n\ncv::cvtColor(frame, gray, cv::COLOR_BGR2GRAY);\n\nOpenCV cv::Mat → AprilTag 전용 이미지 형식 (image_u8_t) 변환\n\nAprilTag 라이브러리는 image_u8_t 구조체를 사용하므로 변환이 필요\ngray.data를 buf로 전달하여 메모리 복사를 최소화\n\nAprilTag 검출 실행\n\napriltag_detector_detect(tag_detector_, &apriltag_image);을 호출하여 AprilTag을 감지\n검출된 태그는 zarray_t* detections에 저장\n\n검출된 AprilTag에 대한 시각적 표시\n\n각 태그의 네 개의 꼭짓점 좌표(p[]) 를 가져와 초록색(cv::Scalar(0, 255, 0))으로 사각형을 그림\n태그의 중앙 좌표(c[]) 에 빨간색 원을 표시(cv::circle())\n태그의 ID를 노란색(cv::Scalar(0, 255, 255)) 텍스트로 화면에 출력\n\n메모리 정리\n\napriltag_detections_destroy(detections);를 호출하여 검출된 태그 리스트를 정리하여 메모리 누수를 방지\n\n결과 이미지 표시\n\ncv::imshow(\"RealSense Camera + AprilTag\", frame);을 통해 검출 결과를 화면에 표시\ncv::waitKey(1);를 호출하여 OpenCV 창이 반응하도록 유지\n\n\n3. 노드 종료 시 AprilTag 리소스 정리 (~ImageSubscriber())\n\napriltag_detector_remove_family(tag_detector_, tag_family_); → 검출기에서 패밀리를 제거\napriltag_detector_destroy(tag_detector_); → 검출기 삭제\ntag25h9_destroy(tag_family_); → 태그 패밀리 메모리 해제\n\nclass ImageSubscriber : public rclcpp::Node\n{\npublic:\n  ImageSubscriber()\n  : Node(\"image_subscriber\")\n  {\n    subscription_ = this-&gt;create_subscription&lt;sensor_msgs::msg::Image&gt;(\n      \"image_raw\",\n      10,\n      std::bind(&ImageSubscriber::imageCallback, this, std::placeholders::_1)\n    );\n\n    // Create an OpenCV window once\n    cv::namedWindow(\"RealSense Camera\", cv::WINDOW_AUTOSIZE);\n\n    RCLCPP_INFO(this-&gt;get_logger(), \"Image Subscriber Node started.\");\n  }\n\nprivate:\n  void imageCallback(const sensor_msgs::msg::Image::SharedPtr msg)\n  {\n    cv::Mat frame;\n    try\n    {\n      frame = cv_bridge::toCvCopy(msg, \"bgr8\")-&gt;image;\n    }\n    catch (cv_bridge::Exception &e)\n    {\n      RCLCPP_ERROR(this-&gt;get_logger(), \"cv_bridge exception: %s\", e.what());\n      return;\n    }\n\n    if (!frame.empty())\n    {\n      cv::imshow(\"RealSense Camera\", frame);\n      cv::waitKey(1);\n    }\n  }\n\n  rclcpp::Subscription&lt;sensor_msgs::msg::Image&gt;::SharedPtr subscription_;\n};\n마지막으로 main을 살펴보면 ROS 2 이미지 구독 노드(ImageSubscriber)를 실행하는 메인 함수로, ROS 2 이벤트 루프를 관리하며 이미지 구독 노드를 실행하면서 30Hz의 주기로 ROS 이벤트를 처리합니다.\nint main(int argc, char *argv[])\n{\n  rclcpp::init(argc, argv); // ROS 2 노드를 실행할 준비\n  auto node = std::make_shared&lt;ImageSubscriber&gt;(); // 노드 생성: `ImageSubscriber` 노드를 생성하여 RealSense 카메라에서 퍼블리싱되는 `\"image_raw\"` 토픽을 구독할 준비\n\n  rclcpp::Rate rate(30); // 초당 30회(30Hz) 실행\n  while (rclcpp::ok()) // 주기적 실행 루프\n  {\n    rclcpp::spin_some(node); // 노드가 메시지를 수신하고, 콜백 함수(`imageCallback`)를 실행할 수 있도록 합니다. \n    rate.sleep(); // 루프가 30Hz의 일정한 주기로 실행되도록 대기\n  }\n\n  rclcpp::shutdown(); // 프로그램 종료 시 ROS 2를 안전하게 종료\n  return 0;\n}\n각 부분을 살펴본 Subsciber 노드를 통합하면 아래와 같이 image_subscriber.cpp가 완성됩니다.\n\n\n\n\n\n\nimage_subscriber.cpp\n\n\n\n\n\n#include &lt;memory&gt;\n#include &lt;string&gt;\n#include &lt;iostream&gt;\n\n// ROS 2\n#include \"rclcpp/rclcpp.hpp\"\n#include \"sensor_msgs/msg/image.hpp\"\n#include \"cv_bridge/cv_bridge.h\"\n\n// OpenCV\n#include \"opencv2/highgui.hpp\"\n#include \"opencv2/imgproc.hpp\"\n\n// AprilTag library headers\nextern \"C\" {\n#include &lt;apriltag.h&gt;\n// #include &lt;tag36h11.h&gt;\n#include &lt;tag25h9.h&gt;\n}\n\nclass ImageSubscriber : public rclcpp::Node\n{\npublic:\n  ImageSubscriber()\n  : Node(\"image_subscriber\")\n  {\n    subscription_ = this-&gt;create_subscription&lt;sensor_msgs::msg::Image&gt;(\n      \"image_raw\",\n      10,\n      std::bind(&ImageSubscriber::imageCallback, this, std::placeholders::_1)\n    );\n\n    // Create an OpenCV window once\n    cv::namedWindow(\"RealSense Camera + AprilTag\", cv::WINDOW_AUTOSIZE);\n\n    RCLCPP_INFO(this-&gt;get_logger(), \"Image Subscriber Node started.\");\n\n    // Initialize the AprilTag detector\n    // tag_family_ = tag36h11_create();  // Create specific tag family (36h11)\n    tag_family_ = tag25h9_create(); // Create specific tag family (36h11)\n    tag_detector_ = apriltag_detector_create();\n    apriltag_detector_add_family(tag_detector_, tag_family_);\n\n    // Optionally adjust detection parameters\n    // e.g., tag_detector_-&gt;quad_decimate = 1.0; // For performance\n    //       tag_detector_-&gt;quad_sigma = 0.0;\n    // etc.\n  }\n\n  ~ImageSubscriber()\n  {\n    // Cleanup AprilTag resources\n    apriltag_detector_remove_family(tag_detector_, tag_family_);\n    apriltag_detector_destroy(tag_detector_);\n    // tag36h11_destroy(tag_family_);\n    tag25h9_destroy(tag_family_);\n  }\n\nprivate:\n  void imageCallback(const sensor_msgs::msg::Image::SharedPtr msg)\n  {\n    // 1. Convert ROS Image to OpenCV Mat\n    cv::Mat frame;\n    try\n    {\n      frame = cv_bridge::toCvCopy(msg, \"bgr8\")-&gt;image;\n    }\n    catch (cv_bridge::Exception &e)\n    {\n      RCLCPP_ERROR(this-&gt;get_logger(), \"cv_bridge exception: %s\", e.what());\n      return;\n    }\n\n    // 2. Convert color to grayscale (AprilTag needs grayscale)\n    cv::Mat gray;\n    cv::cvtColor(frame, gray, cv::COLOR_BGR2GRAY);\n\n    // 3. Convert OpenCV Mat to the format required by AprilTag library\n    //    (AprilTag library uses its own image type: image_u8_t)\n    image_u8_t apriltag_image = {\n      .width  = gray.cols,\n      .height = gray.rows,\n      .stride = static_cast&lt;int&gt;(gray.step),\n      .buf    = gray.data\n    };\n\n    // 4. Detect tags\n    zarray_t* detections = apriltag_detector_detect(tag_detector_, &apriltag_image);\n\n    // 5. For each detection, draw a bounding box on the image\n    for (int i = 0; i &lt; zarray_size(detections); i++)\n    {\n      apriltag_detection_t *det;\n      zarray_get(detections, i, &det);\n\n      // Access the four corners of the AprilTag\n      // p[] is in image coordinates (x=horizontal, y=vertical)\n      cv::Point corners[4];\n      for (int c = 0; c &lt; 4; c++)\n      {\n        corners[c] = cv::Point(det-&gt;p[c][0], det-&gt;p[c][1]);\n      }\n\n      // Draw lines between corners\n      for (int c = 0; c &lt; 4; c++)\n      {\n        cv::line(frame, corners[c], corners[(c+1)%4], cv::Scalar(0, 255, 0), 2);\n      }\n\n      // Draw the tag center\n      cv::circle(frame,\n                 cv::Point(det-&gt;c[0], det-&gt;c[1]),\n                 5, cv::Scalar(0, 0, 255), -1);\n\n      // Put tag ID text near the center\n      std::string text = \"ID: \" + std::to_string(det-&gt;id);\n      cv::putText(frame, text,\n                  cv::Point(det-&gt;c[0] + 10, det-&gt;c[1] + 10),\n                  cv::FONT_HERSHEY_SIMPLEX,\n                  0.5,\n                  cv::Scalar(0, 255, 255),\n                  2);\n    }\n\n    // Always remember to destroy detection array to avoid memory leaks\n    apriltag_detections_destroy(detections);\n\n    // 6. Show the image\n    cv::imshow(\"RealSense Camera + AprilTag\", frame);\n    cv::waitKey(1);\n  }\n\n  rclcpp::Subscription&lt;sensor_msgs::msg::Image&gt;::SharedPtr subscription_;\n\n  // AprilTag fields\n  apriltag_family_t *tag_family_;\n  apriltag_detector_t *tag_detector_;\n};\n\nint main(int argc, char *argv[])\n{\n  rclcpp::init(argc, argv);\n\n  auto node = std::make_shared&lt;ImageSubscriber&gt;();\n\n  // Spin in a loop so that callbacks can be processed\n  rclcpp::Rate rate(30);\n  while (rclcpp::ok())\n  {\n    rclcpp::spin_some(node);\n    rate.sleep();\n  }\n\n  rclcpp::shutdown();\n  return 0;\n}"
  },
  {
    "objectID": "posts/code/2025-02-02-realsense-ros2.html#cmakelists.txt-configuration",
    "href": "posts/code/2025-02-02-realsense-ros2.html#cmakelists.txt-configuration",
    "title": "👩‍💻ROS2 RealSense C++ Node",
    "section": "CMakeLists.txt Configuration",
    "text": "CMakeLists.txt Configuration\n그 다음으로 ROS2 패키지 빌드에 대한 내용을 담고 있는 CMakeLists.txt 파일을 작성해보겠습니다. CMakeLists.txt ( my_realsense_example 루트에 위치)에서 RealSense 카메라에서 이미지를 퍼블리싱하고, AprilTag을 검출하는 ROS 2 노드들을 빌드 및 설치하는 설정을 정의합니다. default로 생성되어있던 내용을 아래 내용으로 업데이트 합니다.\ncmake_minimum_required(VERSION 3.8)\nproject(realsense_apriltag_pubsub)\n\nfind_package(ament_cmake REQUIRED)\nfind_package(rclcpp REQUIRED)\nfind_package(sensor_msgs REQUIRED)\nfind_package(cv_bridge REQUIRED)\nfind_package(OpenCV REQUIRED)\nfind_package(realsense2 REQUIRED)\nfind_package(apriltag REQUIRED)  # &lt;-- Add this!\n\ninclude_directories(\n  include\n  ${OpenCV_INCLUDE_DIRS}\n)\n\n# Publisher\nadd_executable(realsense_publisher src/realsense_publisher.cpp)\nament_target_dependencies(realsense_publisher\n  rclcpp\n  sensor_msgs\n  cv_bridge\n)\ntarget_link_libraries(realsense_publisher\n  ${OpenCV_LIBS}\n  realsense2\n)\n\n# Subscriber with AprilTag\nadd_executable(image_subscriber src/image_subscriber.cpp)\nament_target_dependencies(image_subscriber\n  rclcpp\n  sensor_msgs\n  cv_bridge\n)\ntarget_link_libraries(image_subscriber\n  ${OpenCV_LIBS}\n  realsense2\n  apriltag          # &lt;-- Link AprilTag\n)\n\ninstall(TARGETS\n  realsense_publisher\n  image_subscriber\n  DESTINATION lib/${PROJECT_NAME}\n)\n\n# Install the launch directory\ninstall(DIRECTORY launch/\n  DESTINATION share/${PROJECT_NAME}/launch\n)\n\nament_package()\n이 CMakeLists.txt는 RealSense 카메라에서 이미지를 퍼블리싱하는 노드와, 해당 영상을 구독하여 AprilTag을 검출하는 노드를 빌드할 수 있도록 설정합니다. 특히, apriltag 라이브러리를 추가하여 AprilTag 검출 기능이 포함된 구독자(image_subscriber) 노드를 지원하는 것이 특징입니다.\n1. 필수 패키지 찾기 (find_package)\n\nament_cmake, rclcpp, sensor_msgs, cv_bridge, OpenCV, realsense2 등의 패키지를 포함하여 ROS 2, OpenCV, RealSense SDK를 사용할 수 있도록 설정\nAprilTag 추가 (find_package(apriltag REQUIRED))\n\nAprilTag 검출을 위해 apriltag 라이브러리를 필수적으로 포함\n\n\n2. 헤더 포함 경로 설정 (include_directories)\n\ninclude/ 폴더와 OpenCV의 헤더 파일이 포함된 디렉토리를 추가\n\n3. 퍼블리셔 실행 파일 생성 (realsense_publisher)\n\nsrc/realsense_publisher.cpp 파일을 빌드하여 실행 가능한 퍼블리셔 노드 생성\nament_target_dependencies(realsense_publisher ...)를 통해 ROS 2 라이브러리(rclcpp, sensor_msgs, cv_bridge)와의 의존성을 설정\ntarget_link_libraries(realsense_publisher ${OpenCV_LIBS} realsense2)을 사용해 OpenCV 및 RealSense SDK를 링크\n\n4. 구독자(AprilTag 포함) 실행 파일 생성 (image_subscriber)\n\nsrc/image_subscriber.cpp 파일을 빌드하여 실행 가능한 구독자 노드 생성\nament_target_dependencies(image_subscriber ...)를 통해 ROS 2 라이브러리 의존성 추가\ntarget_link_libraries(image_subscriber ${OpenCV_LIBS} realsense2 apriltag)을 통해 OpenCV, RealSense SDK, AprilTag 라이브러리를 링크하여 AprilTag 검출 기능을 활성화\n\n5. 실행 파일 설치 (install(TARGETS ...))\n\n빌드된 실행 파일(realsense_publisher, image_subscriber)을 ROS 2 패키지의 실행 가능 경로에 배포\n\n6. Launch 파일 설치 (install(DIRECTORY launch/ ...))\n\nlaunch/ 디렉토리를 ROS 2 패키지의 share/ 디렉토리에 복사하여 실행 시 사용할 수 있도록 설정\n이를 통해 ros2 launch realsense_apriltag_pubsub some_launch_file.py 형태로 실행 가능\n\n7. 패키지 선언 (ament_package())\n\nament_package()를 호출하여 ROS 2 빌드 시스템(ament_cmake)과 호환되도록 패키지를 설정\n\n\n⚠️발생할 수 있는 Build 에러 이슈들\n\nUndefined reference to rs2_…: realsense2 library를 링크하지 않았을 때 발생하는 에러입니다. find_package(realsense2 REQUIRED)와 target_link_libraries(... realsense2)가 CMakeLists.txt에 포함되어 있는지 확인하세요.\nCannot find image_subscriber.cpp: CMakeLists.txt에서 지정한 파일 이름과 경로가 src/ 폴더 내의 실제 파일과 정확히 일치하는지 확인하세요.\ncv_bridge or OpenCV not found: package.xml에 작성되어 있는 디펜던시들을 확인하고, 프로그래밍하는 환경 시스템에 cv_bridge 와 OpenCV가 설치되어 있는지 확인하세요."
  },
  {
    "objectID": "posts/code/2025-02-02-realsense-ros2.html#build-실행",
    "href": "posts/code/2025-02-02-realsense-ros2.html#build-실행",
    "title": "👩‍💻ROS2 RealSense C++ Node",
    "section": "Build & 실행",
    "text": "Build & 실행\n이제 패키지를 빌드할 모든 준비는 끝났습니다. 이제 소스 코드들을 작성한 workspace 경로(e.g., ~/ros2_ws)로 이동하여 빌드해줍니다.\ncolcon build --packages-select my_realsense_example\nsource install/setup.bash\n잘 빌드가 되었다면 터미널을 2개 띄워서 각각 publisher 와 subscriber 노드를 실행시켜 줍니다.\n\nTerminal 1:\nros2 run my_realsense_example realsense_publisher\nTerminal 2:\nros2 run my_realsense_example image_subscriber\n\n그러면 “RealSense Camera + AprilTag” 창이 나오면서 인식된 AprilTag ID와 박스가 표시된 창이 아래와 같이 나올 것 입니다!\n\n\n\n최종 결과물"
  },
  {
    "objectID": "posts/code/2025-02-02-realsense-ros2.html#conclusion",
    "href": "posts/code/2025-02-02-realsense-ros2.html#conclusion",
    "title": "👩‍💻ROS2 RealSense C++ Node",
    "section": "Conclusion",
    "text": "Conclusion\n이제 RealSense 카메라에서 컬러 프레임을 퍼블리싱하고, 이를 OpenCV 창에 표시하는 최소한의 ROS 2 시스템을 구축하였습니다. 단순히 영상을 수신하는 것뿐만 아니라, AprilTag 인식을 활용하여 간단한 이미지 후처리도 수행해 보았습니다. 이를 확장하면 로보틱스에서 필요한 위치 추정과 같은 고급 기능도 구현할 수 있으며, 깊이 프레임, 포인트 클라우드 등 RealSense에서 지원하는 다양한 스트림을 활용할 수도 있습니다. 이번 실습을 통해 ROS 2에서 C++ 기반 패키지를 빌드하고 활용하는 과정에 대한 이해를 돕고, 더 복잡한 기능을 구현하는 프로젝트로 나아가는 데 도움이 되기를 바랍니다."
  },
  {
    "objectID": "posts/code/2025-03-14-isaacsim-lab.html",
    "href": "posts/code/2025-03-14-isaacsim-lab.html",
    "title": "👩‍💻IsaacSim과 IsaacLab 알아보기",
    "section": "",
    "text": "이번 포스팅에서는 대표적인 로봇 시뮬레이터 중 하나인 IsaacSim(IsaacLab)에 대해서 알아보고, 이를 Local에 설치하는 과정을 진행해보겠습니다."
  },
  {
    "objectID": "posts/code/2025-03-14-isaacsim-lab.html#isaacgymenvs-및-omniisaacgymenvs",
    "href": "posts/code/2025-03-14-isaacsim-lab.html#isaacgymenvs-및-omniisaacgymenvs",
    "title": "👩‍💻IsaacSim과 IsaacLab 알아보기",
    "section": "2.1 IsaacGymEnvs 및 OmniIsaacGymEnvs",
    "text": "2.1 IsaacGymEnvs 및 OmniIsaacGymEnvs\nNVIDIA가 Isaac Gym과 Isaac Sim을 공개하면서, 이 두 시뮬레이터를 보다 쉽게 활용할 수 있도록 IsaacGymEnvs와 OmniIsaacGymEnvs라는 오픈 소스 환경을 함께 제공했습니다. 이 환경들은 각각의 시뮬레이터 위에서 실행되며, 기본적인 시뮬레이션 기능을 테스트하고 로봇 학습이 가능한 범위를 보여주는 소스코드들을 보여줍니다. 즉, IsaacGymEnvs는 Isaac Gym 기반, OmniIsaacGymEnvs는 Isaac Sim 기반으로 동작하며, 각 시뮬레이터의 장점을 최대한 활용할 수 있도록 설계되었습니다.\n이 환경들을 사용하면, 사전 정의된 로봇 태스크를 빠르게 실행하고 학습 성능을 벤치마킹하는 데 유용합니다. 하지만, 연구자들이 직접 커스텀 환경을 개발하거나 새로운 알고리즘을 테스트하기에는 구조적으로 한계가 많았습니다.\n❌ 특정 태스크에 최적화된 환경 → 새로운 태스크 추가가 어려움\n❌ 환경의 확장성이 부족 → 다양한 로봇 실험을 하기에 한계\n❌ 연구자가 직접 실험을 설계하기 어려움\n결국, 로봇 학습 연구가 점점 복잡해지고 다양한 실험이 필요해지면서, IsaacGymEnvs와 OmniIsaacGymEnvs만으로는 한계를 극복하기 어려워졌고 이 문제를 해결하기 위해 등장한 것이 바로 Isaac Lab입니다."
  },
  {
    "objectID": "posts/code/2025-03-14-isaacsim-lab.html#isaac-lab의-핵심-특징",
    "href": "posts/code/2025-03-14-isaacsim-lab.html#isaac-lab의-핵심-특징",
    "title": "👩‍💻IsaacSim과 IsaacLab 알아보기",
    "section": "3.1 Isaac Lab의 핵심 특징",
    "text": "3.1 Isaac Lab의 핵심 특징\nIsaac Lab은 강화 학습(RL), 시연을 통한 학습(LfD), 모션 플래닝 등 다양한 로봇 연구 워크플로우를 단순화하는 것을 목표로 합니다. 이를 위해 모듈형(modular) 구조를 갖추고 있으며, 연구자가 손쉽게 환경을 변경하거나 새로운 태스크를 추가할 수 있도록 확장성을 고려하여 설계되었습니다.\n✔️ Isaac Lab이 기존 프레임워크와 다른 점\n\n사전 구축된 환경 & 태스크 제공 → 바로 사용할 수 있는 샘플 환경 포함\n맞춤형 커스텀 환경 개발 지원 → 사용자가 직접 환경을 설계하고 실험 가능\n강화 학습 & 모션 플래닝 최적화 → 다양한 학습 방법 적용 가능"
  },
  {
    "objectID": "posts/code/2025-03-14-isaacsim-lab.html#isaac-lab에서-새롭게-추가된-기능",
    "href": "posts/code/2025-03-14-isaacsim-lab.html#isaac-lab에서-새롭게-추가된-기능",
    "title": "👩‍💻IsaacSim과 IsaacLab 알아보기",
    "section": "3.2 Isaac Lab에서 새롭게 추가된 기능",
    "text": "3.2 Isaac Lab에서 새롭게 추가된 기능\n뿐만 아니라, Isaac Sim의 모든 기능을 계승하면서도, 로봇 학습 연구를 위한 새로운 기능들이 추가되었습니다.\n🔹 구동기(Actuator) 동역학 시뮬레이션\n기존 IsaacGymEnvs에서는 단순한 힘 기반 제어가 주로 사용되었지만, Isaac Lab에서는 모터와 액추에이터의 동역학을 보다 정교하게 반영할 수 있습니다. 이는 실제 로봇의 제어 방식과 더욱 유사한 시뮬레이션을 가능하게 합니다.\n🔹 절차적 지형 생성(Procedural Terrain Generation)\n학습을 위한 다양한 환경을 자동으로 생성할 수 있도록 프로시저럴(Procedural) 방식의 지형 생성 기능을 지원합니다. 이는 로봇이 다양한 지형을 학습하는 데 유용하며, 일반화 성능 향상에 기여합니다.\n🔹 사람 시연 데이터 수집 기능\n시연을 통한 학습(LfD: Learning from Demonstration)을 위한 데이터 수집 기능을 포함하여, 사람 시연 데이터 기반의 강화 학습 및 행동 복제(Behavior Cloning)가 가능합니다. 이는 단순한 RL을 넘어, 보다 효율적인 학습 방법론을 연구하는 데 유용합니다."
  },
  {
    "objectID": "posts/code/2025-03-14-isaacsim-lab.html#isaac-sim-설치",
    "href": "posts/code/2025-03-14-isaacsim-lab.html#isaac-sim-설치",
    "title": "👩‍💻IsaacSim과 IsaacLab 알아보기",
    "section": "4.1 Isaac Sim 설치",
    "text": "4.1 Isaac Sim 설치\nIsaac Sim은 4.5.0 버전으로 업데이트되면서 설치 과정이 더욱 간편해졌습니다.\n하지만, 시뮬레이터를 실행하기 위해서는 고성능 그래픽 카드 및 특정 요구 사항을 충족해야 합니다.\n\n4.1.1 시스템 사양 확인: Isaac Sim Compatibility Checker\n\n\n\n설치 요구 사항들\n\n\n💻 최소 요구 사양\n\n운영체제: Ubuntu 20.04 이상\n\nGPU: NVIDIA RTX 3060 이상 (RTX 3090 / 4090 권장)\n\nCUDA 버전: 11.8 이상\n\nVRAM: 최소 8GB (12GB 이상 권장)\n\nRAM: 최소 16GB (32GB 이상 권장)\n\n디스크 공간: 최소 100GB 이상의 여유 공간\n\nIsaac Sim을 실행할 수 있는지 자동으로 점검할 수 있도록 Isaac Sim Compatibility Checker 도구를 제공합니다. 이 도구를 먼저 실행하여 시스템이 적합한지 확인한 후, 설치를 진행하는 것을 추천합니다.\n✅ 설치 및 실행 방법\n\nIsaac Sim Compatibility Checker 다운로드\n압축을 풀어 원하는 폴더에 저장 및 아래 명령어로 실행\n\n./omni.isaac.sim.compatibility_check.sh\n\n검사 결과 확인 (색상 코드 안내)\n\n🟢 Green (최적): 모든 요구 사항 충족\n\n🟢 Light-green (양호): 실행 가능하지만 성능 최적화 가능\n\n🟠 Orange (충분하지만 개선 필요): 실행은 가능하지만 권장 사양보다 낮음\n\n🔴 Red (미지원): 실행 불가능\n\n\n\n\n\n4.1.2 Isaac Sim 설치 (Standalone 방식)\nIsaac Sim은 Omniverse Launcher 없이 독립 실행형(Standalone) 패키지로 다운로드하여 설치해야 합니다.\n✅ 설치 방법\n1. Isaac Sim 다운로드 페이지에서 최신 릴리스 다운로드\n2. 아래 명령어를 실행하여 설치\nmkdir ~/isaacsim\ncd ~/Downloads\nunzip \"isaac-sim-standalone@4.5.0-rc.36+release.19112.f59b3005.gl.linux-x86_64.release.zip\" -d ~/isaacsim\ncd ~/isaacsim\n./post_install.sh\n./isaac-sim.selector.sh\n\n설치 후, Isaac Sim App Selector가 실행되며 실행 모드를 선택 가능\n\n\n\n\n4.1.3 Isaac Sim 실행하기\n설치 후 실행하려면, 아래 명령어를 사용합니다.\n./isaac-sim.sh\n👉 최초 실행 시 셰이더 캐시 로딩(Warmup)이 필요할 수 있습니다. 이 경우, 아래 명령어를 실행하여 셰이더 캐시를 미리 로드합니다.\n./isaac-sim.sh --warmup"
  },
  {
    "objectID": "posts/code/2025-03-14-isaacsim-lab.html#isaac-lab-설치",
    "href": "posts/code/2025-03-14-isaacsim-lab.html#isaac-lab-설치",
    "title": "👩‍💻IsaacSim과 IsaacLab 알아보기",
    "section": "4.2 Isaac Lab 설치",
    "text": "4.2 Isaac Lab 설치\nIsaac Lab은 Isaac Sim이 설치된 환경에서 동작하므로, 먼저 Isaac Sim을 설치한 후 진행해야 합니다. Isaac Sim을 pip 환경에서 사용하기 위해 추가적인 설치 과정이 필요합니다.\n\n4.2.1 설치 전 요구 사항 확인\n✅ GLIBC 버전 확인 (Ubuntu 20.04에서는 추가 조치 필요)\nldd --version\n👉 GLIBC 2.34 이상이 필요합니다. Ubuntu 20.04 기본 버전(2.31)과 호환되지 않을 수 있으므로 확인이 필요합니다.\n\n\n4.2.2 Conda 가상환경 설정\nIsaac Sim을 활용하기 위해 Python 3.10 환경을 갖춘 가상 환경을 생성합니다.\nconda create -n env_isaaclab python=3.10\nconda activate env_isaaclab\n\n\n4.2.3 PyTorch 설치 (CUDA 버전에 맞게 선택)\nCUDA 버전에 맞는 PyTorch를 설치해야 합니다.\n✅ CUDA 11을 사용하는 경우:\npip install torch==2.5.1 --index-url https://download.pytorch.org/whl/cu118\n✅ CUDA 12를 사용하는 경우:\npip install torch==2.5.1 --index-url https://download.pytorch.org/whl/cu121\n\n\n4.2.4 Isaac Sim pip 설치\npip를 이용해 Isaac Sim을 가상 환경에서 설치합니다.\npip install --upgrade pip\npip install 'isaacsim[all,extscache]==4.5.0' --extra-index-url https://pypi.nvidia.com\n✅ 설치 확인\nisaacsim\n⚠️ 첫 실행 시 NVIDIA EULA 동의가 필요합니다.\nDo you accept the EULA? (Yes/No): Yes\n👉 첫 실행 시 모든 확장(extension) 파일이 다운로드되므로 10분 이상 소요될 수 있습니다.\n\n\n4.2.5 Isaac Lab 클론 (소스 코드 다운로드)\ngit clone https://github.com/isaac-sim/IsaacLab.git\ncd IsaacLab\n\n\n4.2.6 의존성 패키지 설치\nIsaac Lab 실행을 위해 필요한 패키지를 설치합니다.\nsudo apt install cmake build-essential\n그 후, Isaac Lab의 의존성을 설치합니다.\n./isaaclab.sh --install\n👉 모든 강화 학습 프레임워크를 함께 설치합니다.\n👉 특정 프레임워크만 설치하고 싶다면 아래와 같이 실행합니다.\n./isaaclab.sh --install rl_games  # 특정 프레임워크 (예: rl_games)만 설치\n✅ 설치 가능한 프레임워크 목록\n\nrl_games\nrsl_rl\nsb3\nskrl\nrobomimic\nnone (학습 프레임워크 제외)\n\n\n\n4.2.7 Isaac Lab 실행 확인\n# Option 1: isaaclab.sh 실행\n./isaaclab.sh -p scripts/tutorials/00_sim/create_empty.py\n\n# Option 2: Python으로 직접 실행\npython scripts/tutorials/00_sim/create_empty.py\n✅ 정상적으로 실행되면, Isaac Lab이 올바르게 설치된 것입니다! 🎉"
  },
  {
    "objectID": "posts/code/2022-12-21-torch-tensor-inverted.html",
    "href": "posts/code/2022-12-21-torch-tensor-inverted.html",
    "title": "👩‍💻torch.Tensor vs torch.tensor",
    "section": "",
    "text": "둘다 data(input)에 대해서 Tensor 객체로 만들어 주는 기능은 동일\n\n\n\n\ntorch.tensor() : function\n\ndocs: https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor\ntorch.tensor(data)에서 data는 필수 argument. (data 없을 시 TypeError)\ntorch.tensor는 항상 data를 복사\nint 입력 시 int 그대로 입력\n입력받은 데이터를 새로운 메모리 공간에 복사해 Tensor 객체 생성 (call by value)\n\ntorch.Tensor() : class\n\ndocs: https://pytorch.org/docs/stable/tensors.html#torch.Tensor\n이미 생성된 객체를 tensor로 바꾸고 싶을 때 사용\n빈 Tensor 객체를 만들 때 사용\nint 입력 시 float로 변환\nTensor 객체로 데이터를 입력할 경우 입력 받은 메모리 공간 그대로 사용 (call by reference)\nlist나 numpy 등 다른 자료형으로 입력 받을 경우 값을 복사하여 Tensor 객체 생성(call by value)\n\n\n\n\n\n\nimport torch\n\n\noriginal_data = torch.tensor([1])\nnew_data = torch.tensor(original_data)\nprint(f\"original : {original_data} {original_data.dtype}\")\nprint(f\"new : {new_data} {new_data.dtype}\")\n\n \n# original data를 수정\noriginal_data[0] = 2\nprint(f\"original : {original_data} {original_data.dtype}\")\nprint(f\"new : {new_data} {new_data.dtype}\")\n\noriginal : tensor([1]) torch.int64\nnew : tensor([1]) torch.int64\noriginal : tensor([2]) torch.int64\nnew : tensor([1]) torch.int64\n\n\n/tmp/ipykernel_847781/1124094978.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  new_data = torch.tensor(original_data)\n\n\n\nint에서 int로\ncall by value, 새로운 Tensor 객체이므로 변화 x\n\n\n\n\n\noriginal_data = torch.Tensor([1])\nnew_data = torch.Tensor(original_data)\nprint(f\"original : {original_data} {original_data.dtype}\")\nprint(f\"new : {new_data} {new_data.dtype}\")\n\n \n# original data를 수정\noriginal_data[0] = 2\nprint(f\"original : {original_data} {original_data.dtype}\")\nprint(f\"new : {new_data} {new_data.dtype}\")\n\noriginal : tensor([1.]) torch.float32\nnew : tensor([1.]) torch.float32\noriginal : tensor([2.]) torch.float32\nnew : tensor([2.]) torch.float32\n\n\n\nint에서 float으로\ncall by reference, shallow copy\n\n\noriginal_data = [1]\nnew_data = torch.Tensor(original_data)\nprint(f\"original : {original_data}\")\nprint(f\"new : {new_data} {new_data.dtype}\")\n\n# original data를 수정\noriginal_data[0] = 2\nprint(f\"original : {original_data}\")\nprint(f\"new : {new_data} {new_data.dtype}\")\n\noriginal : [1]\nnew : tensor([1.]) torch.float32\noriginal : [2]\nnew : tensor([1.]) torch.float32\n\n\n\nint에서 float으로\ncall by value, deep copy"
  },
  {
    "objectID": "posts/code/2022-12-21-torch-tensor-inverted.html#공통점",
    "href": "posts/code/2022-12-21-torch-tensor-inverted.html#공통점",
    "title": "👩‍💻torch.Tensor vs torch.tensor",
    "section": "",
    "text": "둘다 data(input)에 대해서 Tensor 객체로 만들어 주는 기능은 동일"
  },
  {
    "objectID": "posts/code/2022-12-21-torch-tensor-inverted.html#차이점",
    "href": "posts/code/2022-12-21-torch-tensor-inverted.html#차이점",
    "title": "👩‍💻torch.Tensor vs torch.tensor",
    "section": "",
    "text": "torch.tensor() : function\n\ndocs: https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor\ntorch.tensor(data)에서 data는 필수 argument. (data 없을 시 TypeError)\ntorch.tensor는 항상 data를 복사\nint 입력 시 int 그대로 입력\n입력받은 데이터를 새로운 메모리 공간에 복사해 Tensor 객체 생성 (call by value)\n\ntorch.Tensor() : class\n\ndocs: https://pytorch.org/docs/stable/tensors.html#torch.Tensor\n이미 생성된 객체를 tensor로 바꾸고 싶을 때 사용\n빈 Tensor 객체를 만들 때 사용\nint 입력 시 float로 변환\nTensor 객체로 데이터를 입력할 경우 입력 받은 메모리 공간 그대로 사용 (call by reference)\nlist나 numpy 등 다른 자료형으로 입력 받을 경우 값을 복사하여 Tensor 객체 생성(call by value)"
  },
  {
    "objectID": "posts/code/2022-12-21-torch-tensor-inverted.html#torch.tensor",
    "href": "posts/code/2022-12-21-torch-tensor-inverted.html#torch.tensor",
    "title": "👩‍💻torch.Tensor vs torch.tensor",
    "section": "",
    "text": "import torch\n\n\noriginal_data = torch.tensor([1])\nnew_data = torch.tensor(original_data)\nprint(f\"original : {original_data} {original_data.dtype}\")\nprint(f\"new : {new_data} {new_data.dtype}\")\n\n \n# original data를 수정\noriginal_data[0] = 2\nprint(f\"original : {original_data} {original_data.dtype}\")\nprint(f\"new : {new_data} {new_data.dtype}\")\n\noriginal : tensor([1]) torch.int64\nnew : tensor([1]) torch.int64\noriginal : tensor([2]) torch.int64\nnew : tensor([1]) torch.int64\n\n\n/tmp/ipykernel_847781/1124094978.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  new_data = torch.tensor(original_data)\n\n\n\nint에서 int로\ncall by value, 새로운 Tensor 객체이므로 변화 x"
  },
  {
    "objectID": "posts/code/2022-12-21-torch-tensor-inverted.html#torch.tensor-1",
    "href": "posts/code/2022-12-21-torch-tensor-inverted.html#torch.tensor-1",
    "title": "👩‍💻torch.Tensor vs torch.tensor",
    "section": "",
    "text": "original_data = torch.Tensor([1])\nnew_data = torch.Tensor(original_data)\nprint(f\"original : {original_data} {original_data.dtype}\")\nprint(f\"new : {new_data} {new_data.dtype}\")\n\n \n# original data를 수정\noriginal_data[0] = 2\nprint(f\"original : {original_data} {original_data.dtype}\")\nprint(f\"new : {new_data} {new_data.dtype}\")\n\noriginal : tensor([1.]) torch.float32\nnew : tensor([1.]) torch.float32\noriginal : tensor([2.]) torch.float32\nnew : tensor([2.]) torch.float32\n\n\n\nint에서 float으로\ncall by reference, shallow copy\n\n\noriginal_data = [1]\nnew_data = torch.Tensor(original_data)\nprint(f\"original : {original_data}\")\nprint(f\"new : {new_data} {new_data.dtype}\")\n\n# original data를 수정\noriginal_data[0] = 2\nprint(f\"original : {original_data}\")\nprint(f\"new : {new_data} {new_data.dtype}\")\n\noriginal : [1]\nnew : tensor([1.]) torch.float32\noriginal : [2]\nnew : tensor([1.]) torch.float32\n\n\n\nint에서 float으로\ncall by value, deep copy"
  },
  {
    "objectID": "posts/code/2022-12-21-torch-tensor-inverted.html#torch.tensor-2",
    "href": "posts/code/2022-12-21-torch-tensor-inverted.html#torch.tensor-2",
    "title": "👩‍💻torch.Tensor vs torch.tensor",
    "section": "torch.tensor()",
    "text": "torch.tensor()\n\na = torch.tensor([0,1,10])\nprint(f\"Shape of tensor: {a.shape}\")\nprint(f\"Datatype of tensor: {a.dtype}\")\nprint(f\"Device tensor is stored on: {a.device}\")\n\nShape of tensor: torch.Size([3])\nDatatype of tensor: torch.int64\nDevice tensor is stored on: cpu\n\n\n\n~a\n\ntensor([ -1,  -2, -11])\n\n\n\nb = torch.tensor([True, False, True])\nprint(f\"Shape of tensor: {b.shape}\")\nprint(f\"Datatype of tensor: {b.dtype}\")\nprint(f\"Device tensor is stored on: {b.device}\")\n\nShape of tensor: torch.Size([3])\nDatatype of tensor: torch.bool\nDevice tensor is stored on: cpu\n\n\n\n~b\n\ntensor([False,  True, False])\n\n\n\nc = torch.tensor([0.13, 1.5, -.116])\nprint(f\"Shape of tensor: {c.shape}\")\nprint(f\"Datatype of tensor: {c.dtype}\")\nprint(f\"Device tensor is stored on: {c.device}\")\n\nShape of tensor: torch.Size([3])\nDatatype of tensor: torch.float32\nDevice tensor is stored on: cpu\n\n\n\n~c\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nInput In [10], in &lt;cell line: 1&gt;()\n----&gt; 1 ~c\n\nTypeError: ~ (operator.invert) is only implemented on integer and Boolean-type tensors"
  },
  {
    "objectID": "posts/code/2022-12-21-torch-tensor-inverted.html#torch.tensor-3",
    "href": "posts/code/2022-12-21-torch-tensor-inverted.html#torch.tensor-3",
    "title": "👩‍💻torch.Tensor vs torch.tensor",
    "section": "torch.Tensor()",
    "text": "torch.Tensor()\n\na = torch.Tensor([0,1,10])\nprint(f\"Shape of tensor: {a.shape}\")\nprint(f\"Datatype of tensor: {a.dtype}\")\nprint(f\"Device tensor is stored on: {a.device}\")\n\nShape of tensor: torch.Size([3])\nDatatype of tensor: torch.float32\nDevice tensor is stored on: cpu\n\n\n\na\n\ntensor([ 0.,  1., 10.])\n\n\n\n~a\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nInput In [13], in &lt;cell line: 1&gt;()\n----&gt; 1 ~a\n\nTypeError: ~ (operator.invert) is only implemented on integer and Boolean-type tensors\n\n\n\n\nb = torch.Tensor([True, False, True])\nprint(f\"Shape of tensor: {b.shape}\")\nprint(f\"Datatype of tensor: {b.dtype}\")\nprint(f\"Device tensor is stored on: {b.device}\")\n\nShape of tensor: torch.Size([3])\nDatatype of tensor: torch.float32\nDevice tensor is stored on: cpu\n\n\n\nb\n\ntensor([1., 0., 1.])\n\n\n\n~b\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nInput In [16], in &lt;cell line: 1&gt;()\n----&gt; 1 ~b\n\nTypeError: ~ (operator.invert) is only implemented on integer and Boolean-type tensors\n\n\n\n\nc = torch.Tensor([0.13, 1.5, -.116])\nprint(f\"Shape of tensor: {c.shape}\")\nprint(f\"Datatype of tensor: {c.dtype}\")\nprint(f\"Device tensor is stored on: {c.device}\")\n\nShape of tensor: torch.Size([3])\nDatatype of tensor: torch.float32\nDevice tensor is stored on: cpu\n\n\n\nc\n\ntensor([ 0.1300,  1.5000, -0.1160])\n\n\n\n~c\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nInput In [19], in &lt;cell line: 1&gt;()\n----&gt; 1 ~c\n\nTypeError: ~ (operator.invert) is only implemented on integer and Boolean-type tensors\n\n\n\n결론: torch.tensor()로만 ~연산 사용 가능\nReference\n\ntorch.Tensor와 torch.tensor의 차이"
  },
  {
    "objectID": "posts/code/2020-07-13-install-mujoco-win10.html",
    "href": "posts/code/2020-07-13-install-mujoco-win10.html",
    "title": "👩‍💻Install Mujoco in Windows10",
    "section": "",
    "text": "Mujoco\n\nMujoco에서 License tab으로 들어가서 Personal Student Software License를 받는다.\n이메일로 Account number를 받는다.(spam도 확인하자)\n다시 License로 가서 Account number와 Computer Id를 입력한다.\n\n\n\n\n\n다시 이메일로 MuJoCo Pro Personal Activation Key(mjkey.txt)를 받는다.\n.mujoco라는 파일을 C:\\Users\\사용자명\\에 만든다.\nProducts에서 mjpro150 win64을 다운받아 .mujoco에 압축을 풀어준다.(C:\\Users\\사용자명\\.mujoco\\mjpro150)\n\n\n\n\n\n다운받은 mjkey.txt 파일을 C:\\Users\\사용자명\\.mujoco와 C:\\Users\\사용자명\\.mujoco\\mjpro150\\bin 에 옮긴다.\ncmd창을 열어서 cd 명령어로 C:\\Users\\사용자명\\.mujoco\\mjpro150\\bin 경로로 이동한 다음 simulate ../model/humanoid.xml명령어를 입력한다.\nHumanoid Simulation 창이 나오면 성공이다.\n\n\n\n\n\n\n\n\nMujoco-py\n\nSET PATH=C:\\Users\\사용자명\\.mujoco\\mjpro150\\bin;%PATH%;으로 경로설정을 해준다.\nmujoco-py-1.50.1.0 파일을 다운받아 C:\\Users\\사용자명\\.mujoco에 압축을 풀어준다.(C:\\Users\\사용자명\\.mujoco\\mujoco-py-1.50.1.0)\ncmd 창에서 python setup.py install를 입력하여 설치한다.\nC:\\Users\\사용자명\\.mujoco\\mujoco-py-1.50.1.0&gt; python setup.py install\n혹시 여기서 error가 난다면 전에 mujoco-py를 설치해서 버젼이 안 맞아 나는 것일 수도 있다. pip list로 mujoco-py의 버젼을 확인해보고 다른 버젼이라면 pip uninstall mujoco-py를 해준후 다시 설치한다.\ncmd 창에서 python examples\\body_interaction.py를 입력하여 잘 실행되는지 확인한다.\nC:\\Users\\사용자명\\.mujoco\\mujoco-py-1.50.1.0&gt; python examples\\body_interaction.py\n처음에 실행화면이 뜨는 시간이 오래걸리지만 이후에는 실행창이 빨리 나왔다.\n\n\n\n\n\n\n\nReference\n\nMuJoCo 설치 (윈도우 10 version)"
  },
  {
    "objectID": "posts/note/2021-01-31-ML-tu-berlin.html",
    "href": "posts/note/2021-01-31-ML-tu-berlin.html",
    "title": "📘2021 TU Berlin Winter Course 수강 후기",
    "section": "",
    "text": "사실 ML/DL 카테고리에는 공부내용만 적을려고 했으나 관련 수업을 들은 후기도 여기에 정리하면 좋을 것 같아 여기에 분류했다. 후에 수업에서 배운 내용들도 차근차근히 정리해볼 예정이다.\n\n\n학교에서 지원해주는 교환학생이라던가 해외 협업 수업은 언제나 하고 싶었지만 항상 발목을 잡은 건 영어였던 것 같다. 영어..읽고 쓰는 것도 좋아하고 듣는 것도 다 알아듣진 못해도 어느정도는 되는데 항상 ’말하기’가 문제였다. 사실 천성 내향형 인간이라 한국어로도 말은 원래 잘 못하는데 영어는 더더욱 자신이 없었다. 게다가 학교에서 지원하는 프로그램들은 내가 영어를 잘하는지 증명을 해야하는데 토플 준비만했을 뿐 정작 시험은 안봤다. 계속 다른 일에 우선순위가 밀리고 시험비도 그냥 한번 시도해보기엔 만만치 않아서가 변명이라면 변명이었다. 어쨌든 자격증이 없으니 지원자격자체가 안됬었다.\n그런데 영어 자격증이 없어도 지원할 수 있는 프로그램이 하나 생겼고 코로나로 인해 온라인이긴 했지만 지원했다. 마지막 4학년만 남았기에 이번이 마지막일 수도 있다는 생각도 있었다. 들을 수 있는 수업이 4개가 있었는데 데이터 사이언스/시각화분야 수업과 머신러닝 이론/적용분야 수업 중에 고민을 했다. 결과적으로는 머신러닝 이론/적용을 선택했다. 이유는 아래와 같았다.\n\nSyllabus에 이미 알고 있는 내용들이 좀 더 많았기에 혹시 놓치더라도 뒤쳐지지 않을 것 같았음 + 영어로 소통하는 것에 좀 더 집중할 수 있음\nFeature Engineering 부분이나 Unsupervised Learning과 같은 실제 적용은 많이 해보지 않은 부분들을 다지는 기회\nLecturer 중에 전공분야가 Reinforcement Learning이신 분이 있어서 (혹시라도 친해지면) 이것저것 물어볼 수 있지 않을까라는 생각\n\n그래서 학교에 지원을 넣고 합격이 될까 안될까 걱정반 설렘반이었는데, 담당 교수님과 편하게 카톡영상통화로 면접을 보고 난후 흔쾌히 허락해주셨다. 다행이었다. 이제 독일 학교에 지원서를 작성하는데 문제가 하나 있었다. 지원서 영어실력을 증명하라는 칸이 있었는데 위에 써놓은 것과 같이 난 자격증이 없었기에 뭘 제출하라는 말인지 이해하지 못했다. 근데 컨택해주신 교수님이 분명 영어 자격증없어도 가능하다고 하셨던 것 같은데..뭘까…고민을 하다가 학교에서 이수한 과목들 목록을 뽑았다. 그중에는 영국인 교수님이 수업하신 (물론 영어로 진행된) 영국문화 알아가기라는 교양 수업도 있었고, 필수 이수과목인 대학영어, 전공과목이지만 영어로 진행한 수업들도 있었다. 그래서 학기별 내가 들은 ’영어’와 관련된 수업 목록들을 하나하나 설명하는 보고서(?)를 작성하여 “이런 영어로 진행된 수업들을 잘 이수하였으니 난 영어로 소통가능하다”라고 어필했다. 다행히도 그 보고서로 나의 영어 실력이 증명이 되어서 간신히 수업등록을 마칠 수 있었다. 이렇게 우여곡절 끝에 수업등록을 하니 영어자격증을 빨리 따야겠다는 생각만 더 절실해졌다.😂(이제 조금 여유가 생긴 2월달에 토플 시험 볼 예정이다.)\n\n\n\n\n수업이 시작되고 역시나 걱정되는 건 영어로하는 의사소통이었다. 급하게 영어 말하기 세포를 깨우기 위해 Facebook 지인분들께 방법도 여쭤보고 혼자서 엄청 중얼중얼 거렸다. 단기간에 실력적인 향상 효과는 미미했지만 마음의 준비는 할 수 있었다. 근데 확실히 수업이 막상 시작하니 난관은 말하기보다 듣기에 있었다. 다양한 나라의 다양한 억양은 듣기 힘들다는 이야기를 듣긴 했어도 직접 체험해보니 잘 안들리는 황당함이란.. 다양한 나라의 사람들과 소통하고 싶다는 포부로 시작했던 처음 마음과 달리, 수업하는 동안은 ’잘’듣는 것만으로도 나에게 도전이었던 것 같다. 그런데 한편으로는 완벽한 영어를 구성하지 않는, 나와같이 영어가 제2외국어이신 분들과 수업을 하니 말할때는 부담없이 나도 막(?)말했던 것 같다. 어쨌든 걱정했던 ’말하기’는 생각보다 난관이 아니었고, ’듣기’가 난관이었다는 이야기다. 같이 수업을 듣는 분들은 약 20명 정도 였는데 첫시간에 자기소개할 때 들어보니 다들 박사과정이거나 직장인이었다..?!😲 학사도 졸업 못한 나랑은 너무나 대비되는 분들이라 신기하기도 하고 걱정도 됐다. (학교는 왜 이런 프로그램인지 왜 안알려줬지) 그분들의 능력은 나중에 프로젝트 발표하실 때 확연히 느낄 수 있었는데 프로젝트 완성도나 주제가 넘사벽이었다.\n수업은 대부분 강의 시청 + exercise + 질의 응답으로 이루어졌기에 내가 적극적으로 이야기하고 질문할 부분이 많진 않았다. 그리고 앞서 적었던 것처럼 수업내용이 나한테는 완전히 새로운 분야는 아니어서 그런지 그럭저럭 따라갈 수 있었다. (벌써 수업 마쳤다고 기억이 미화된건지는 모르겠지만) 그리고 모르는게 있으면 구글링으로 잘 정리해놓은 훌륭하신 한국인 분들의 포스팅, 가지고 있는 책들로 메꿔갔기에 수업을 따라가는 건 많이 어렵진 않았던 것 같다. 진짜 다른분들이 잘 정리해놓으신 포스팅에 도움을 많이 받아서 항상 드는 생각이지만 언젠가는 나도 저분들처럼 도움되는 포스팅을 하고 싶다라는 생각이 든다.\n사실 등록 전에는 몰랐던 Group Work는 수업계획서에서 처음 봤을 때 당황스러웠다. 그러나 걱정됐던 부분과 달리 팀구성도 잘됐고, 내가 주도해서 그런지 주제도 내가 원하는 걸로 진행하게되었다. 워낙 짧은 시간(3일 정도)에 완성해야하는 프로젝트였기에 마음과 달리 여러가지 기법들을 사용하기 보다는 심플하고 충실하게 ‘배운것만’ 응용해서 프로젝트를 마무리했다. 그래도 팀원분들과 영어로 의견 조율하는게 잠깐이었지만 재밌었고, 이번 경험으로 영어가 더이상 공포의 대상이 아닌 세상 다른 사람들과 ’소통’할 수 있는 도구라는 생각이 확실히 각인된 것 같다. 정말 ’언어’라는 생각을 체험해보는 기회였달까. 그렇게 더듬더듬거리는 영어지만 의견과 생각을 나누는 경험은 소중했고 마지막 프로젝트까지 잘 마칠 수 있었다. 나중에 Lecturer 분들께서 코멘트 해주실 때도 프로젝트 지적을 당했다라는 생각보다, 진정으로 우리 프로젝트를 봐주시고 인정해주신다는 생각이 들어서 감사했다. 그리고 사실은 진짜로 우리팀이 프로젝트에서 중요한 포인트를 빼먹었었는데 짚어주시는 거 보고 많이 배웠다.(한편으로는 말도 안되는 영어로 발표하는 거 들으시고도 이해하셨다는게 신기했다🤣) 아쉽게도 발표할 때가 처음이자 마지막으로 Lecturer 분들과 이야기한 순간이었다.(아 첫시간에 자기소개 빼고!) 프로젝트 깃헙은 여기에 있다.\n수업 마지막에 봤던 시험은 생각보다 어렵지 않았기에 사실 후기에 적을 내용이 별로 없다. 한국 시험에서는 선택형이어도 꼬아서 내거나 복수 정답지들을 만들어서 난이도를 높이는데, 여기 수업은 깔끔하게 중복 답안지 없다라고 말해주고 선택지들도 기본중의 기본이었다. 그나마 분별력을 가질 수 있는 부분은 프로그래밍 파트였으나 이 부분마저 코드를 외워서 치는게 아니라 오픈북이어서 그렇게 어렵진 않았던 것 같다. 게다가 시험시간은 3시간이었기에 타임 리밋도 거의 없는거나 다름없었다. 물론 시험보기 전에는 시험이 어떻게 나올지 예측할 수 없었기에 전날밤을 새워가며 공부하긴 했었다.\n\n\n\n마지막 날에 발표를 마치고 Lecturer분들의 ML/DL 트렌드에 관한 짧은 발표와 같이 수업들은 분들과 약간의 담소를 나누며 훈훈하게 수업이 끝났다. 약 3주간동안 저녁 시간에 열심히 배웠던 수업이 막을 내렸다.\n항상 가장 괴로울 때 가장 많이 배우는 것 같다. 사실 지나고 보면 그 고통이 어느정도 미화되는 것도 있고 위에 ‘어렵지 않았다.’, ’잘 마무리됐다.’라고 써놓기도 했지만, 그걸 하고 있던 순간에는 힘들었다. 아무리 알고 있던 내용이들이라도 그 내용들을 다시 다지는 것도 어렵고, 직접 코드를 써보는 것도 어렵고, 마무리 하기까지 불확실성과 두려움 때문에 괴로웠던 건 팩트다. 근데 항상 그러면서 배우는 것 같다. 그래서 이번 코스를 통해 배운 걸 정리해보자면,\n\nML은 확실히 DL보다 수학적인 이론이 어려웠음\n영어가 시험이나 부담이 아닌 ’언어’라는 것. 나를 표현할 수 있는 툴이라는 것\n전에 조금이라도 공부했던 것들이 나중에 도움이 ’크게’될 수 있다는 것\n내가 알고있는 것들을 꾸준하게 포스팅하는 습관을 길러야 하겠다는 생각\n해보지 않고 추측만으로 하는 생각들로 두려워하지 말 것\n\n수업을 마치고 certificate를 받았다. 사실 certificate에는 옵션이 있었는데, grade를 받을 것인지/pass or fail을 받을 것인지 내가 선택할 수 있었다. 등록 당시엔 근자감으로 grade를 받겠다고 했다가 수업 듣는 동안 과거의 나를 반성했다. 사실 수업하는 동안 그렇게 적극적이지도 않았고, 프로젝트 발표를 하고 지적을 받으면서 든 생각은 ‘최고 grade까지는 못받고 중간이라도 했으면..’ 했다. 그런데 생각지 못했던 최고 grade를 받을 수 있었다. 학점이 다가 아니긴 하지만 그래도 최선을 다했다는 것을 어느정도 뒷받침해줄 수 있기 때문에 뿌듯했다. 그래서 위에 배운 점들에 하나 더 추가하자면, ’최선을 다해서 어찌저찌해보면 뭐라도 한다.’라는 것. 식상한 멘트와 교훈이지만 매번 도전하면서 느끼는 바이기도 하다. 이렇게 2021년도 1월은 TU Berlin의 Machine Learning using Python: Theory and Application 코스와 함께 시간을 보냈다."
  },
  {
    "objectID": "posts/note/2021-01-31-ML-tu-berlin.html#tu-berlin-winter-university-online-2021",
    "href": "posts/note/2021-01-31-ML-tu-berlin.html#tu-berlin-winter-university-online-2021",
    "title": "📘2021 TU Berlin Winter Course 수강 후기",
    "section": "",
    "text": "사실 ML/DL 카테고리에는 공부내용만 적을려고 했으나 관련 수업을 들은 후기도 여기에 정리하면 좋을 것 같아 여기에 분류했다. 후에 수업에서 배운 내용들도 차근차근히 정리해볼 예정이다.\n\n\n학교에서 지원해주는 교환학생이라던가 해외 협업 수업은 언제나 하고 싶었지만 항상 발목을 잡은 건 영어였던 것 같다. 영어..읽고 쓰는 것도 좋아하고 듣는 것도 다 알아듣진 못해도 어느정도는 되는데 항상 ’말하기’가 문제였다. 사실 천성 내향형 인간이라 한국어로도 말은 원래 잘 못하는데 영어는 더더욱 자신이 없었다. 게다가 학교에서 지원하는 프로그램들은 내가 영어를 잘하는지 증명을 해야하는데 토플 준비만했을 뿐 정작 시험은 안봤다. 계속 다른 일에 우선순위가 밀리고 시험비도 그냥 한번 시도해보기엔 만만치 않아서가 변명이라면 변명이었다. 어쨌든 자격증이 없으니 지원자격자체가 안됬었다.\n그런데 영어 자격증이 없어도 지원할 수 있는 프로그램이 하나 생겼고 코로나로 인해 온라인이긴 했지만 지원했다. 마지막 4학년만 남았기에 이번이 마지막일 수도 있다는 생각도 있었다. 들을 수 있는 수업이 4개가 있었는데 데이터 사이언스/시각화분야 수업과 머신러닝 이론/적용분야 수업 중에 고민을 했다. 결과적으로는 머신러닝 이론/적용을 선택했다. 이유는 아래와 같았다.\n\nSyllabus에 이미 알고 있는 내용들이 좀 더 많았기에 혹시 놓치더라도 뒤쳐지지 않을 것 같았음 + 영어로 소통하는 것에 좀 더 집중할 수 있음\nFeature Engineering 부분이나 Unsupervised Learning과 같은 실제 적용은 많이 해보지 않은 부분들을 다지는 기회\nLecturer 중에 전공분야가 Reinforcement Learning이신 분이 있어서 (혹시라도 친해지면) 이것저것 물어볼 수 있지 않을까라는 생각\n\n그래서 학교에 지원을 넣고 합격이 될까 안될까 걱정반 설렘반이었는데, 담당 교수님과 편하게 카톡영상통화로 면접을 보고 난후 흔쾌히 허락해주셨다. 다행이었다. 이제 독일 학교에 지원서를 작성하는데 문제가 하나 있었다. 지원서 영어실력을 증명하라는 칸이 있었는데 위에 써놓은 것과 같이 난 자격증이 없었기에 뭘 제출하라는 말인지 이해하지 못했다. 근데 컨택해주신 교수님이 분명 영어 자격증없어도 가능하다고 하셨던 것 같은데..뭘까…고민을 하다가 학교에서 이수한 과목들 목록을 뽑았다. 그중에는 영국인 교수님이 수업하신 (물론 영어로 진행된) 영국문화 알아가기라는 교양 수업도 있었고, 필수 이수과목인 대학영어, 전공과목이지만 영어로 진행한 수업들도 있었다. 그래서 학기별 내가 들은 ’영어’와 관련된 수업 목록들을 하나하나 설명하는 보고서(?)를 작성하여 “이런 영어로 진행된 수업들을 잘 이수하였으니 난 영어로 소통가능하다”라고 어필했다. 다행히도 그 보고서로 나의 영어 실력이 증명이 되어서 간신히 수업등록을 마칠 수 있었다. 이렇게 우여곡절 끝에 수업등록을 하니 영어자격증을 빨리 따야겠다는 생각만 더 절실해졌다.😂(이제 조금 여유가 생긴 2월달에 토플 시험 볼 예정이다.)\n\n\n\n\n수업이 시작되고 역시나 걱정되는 건 영어로하는 의사소통이었다. 급하게 영어 말하기 세포를 깨우기 위해 Facebook 지인분들께 방법도 여쭤보고 혼자서 엄청 중얼중얼 거렸다. 단기간에 실력적인 향상 효과는 미미했지만 마음의 준비는 할 수 있었다. 근데 확실히 수업이 막상 시작하니 난관은 말하기보다 듣기에 있었다. 다양한 나라의 다양한 억양은 듣기 힘들다는 이야기를 듣긴 했어도 직접 체험해보니 잘 안들리는 황당함이란.. 다양한 나라의 사람들과 소통하고 싶다는 포부로 시작했던 처음 마음과 달리, 수업하는 동안은 ’잘’듣는 것만으로도 나에게 도전이었던 것 같다. 그런데 한편으로는 완벽한 영어를 구성하지 않는, 나와같이 영어가 제2외국어이신 분들과 수업을 하니 말할때는 부담없이 나도 막(?)말했던 것 같다. 어쨌든 걱정했던 ’말하기’는 생각보다 난관이 아니었고, ’듣기’가 난관이었다는 이야기다. 같이 수업을 듣는 분들은 약 20명 정도 였는데 첫시간에 자기소개할 때 들어보니 다들 박사과정이거나 직장인이었다..?!😲 학사도 졸업 못한 나랑은 너무나 대비되는 분들이라 신기하기도 하고 걱정도 됐다. (학교는 왜 이런 프로그램인지 왜 안알려줬지) 그분들의 능력은 나중에 프로젝트 발표하실 때 확연히 느낄 수 있었는데 프로젝트 완성도나 주제가 넘사벽이었다.\n수업은 대부분 강의 시청 + exercise + 질의 응답으로 이루어졌기에 내가 적극적으로 이야기하고 질문할 부분이 많진 않았다. 그리고 앞서 적었던 것처럼 수업내용이 나한테는 완전히 새로운 분야는 아니어서 그런지 그럭저럭 따라갈 수 있었다. (벌써 수업 마쳤다고 기억이 미화된건지는 모르겠지만) 그리고 모르는게 있으면 구글링으로 잘 정리해놓은 훌륭하신 한국인 분들의 포스팅, 가지고 있는 책들로 메꿔갔기에 수업을 따라가는 건 많이 어렵진 않았던 것 같다. 진짜 다른분들이 잘 정리해놓으신 포스팅에 도움을 많이 받아서 항상 드는 생각이지만 언젠가는 나도 저분들처럼 도움되는 포스팅을 하고 싶다라는 생각이 든다.\n사실 등록 전에는 몰랐던 Group Work는 수업계획서에서 처음 봤을 때 당황스러웠다. 그러나 걱정됐던 부분과 달리 팀구성도 잘됐고, 내가 주도해서 그런지 주제도 내가 원하는 걸로 진행하게되었다. 워낙 짧은 시간(3일 정도)에 완성해야하는 프로젝트였기에 마음과 달리 여러가지 기법들을 사용하기 보다는 심플하고 충실하게 ‘배운것만’ 응용해서 프로젝트를 마무리했다. 그래도 팀원분들과 영어로 의견 조율하는게 잠깐이었지만 재밌었고, 이번 경험으로 영어가 더이상 공포의 대상이 아닌 세상 다른 사람들과 ’소통’할 수 있는 도구라는 생각이 확실히 각인된 것 같다. 정말 ’언어’라는 생각을 체험해보는 기회였달까. 그렇게 더듬더듬거리는 영어지만 의견과 생각을 나누는 경험은 소중했고 마지막 프로젝트까지 잘 마칠 수 있었다. 나중에 Lecturer 분들께서 코멘트 해주실 때도 프로젝트 지적을 당했다라는 생각보다, 진정으로 우리 프로젝트를 봐주시고 인정해주신다는 생각이 들어서 감사했다. 그리고 사실은 진짜로 우리팀이 프로젝트에서 중요한 포인트를 빼먹었었는데 짚어주시는 거 보고 많이 배웠다.(한편으로는 말도 안되는 영어로 발표하는 거 들으시고도 이해하셨다는게 신기했다🤣) 아쉽게도 발표할 때가 처음이자 마지막으로 Lecturer 분들과 이야기한 순간이었다.(아 첫시간에 자기소개 빼고!) 프로젝트 깃헙은 여기에 있다.\n수업 마지막에 봤던 시험은 생각보다 어렵지 않았기에 사실 후기에 적을 내용이 별로 없다. 한국 시험에서는 선택형이어도 꼬아서 내거나 복수 정답지들을 만들어서 난이도를 높이는데, 여기 수업은 깔끔하게 중복 답안지 없다라고 말해주고 선택지들도 기본중의 기본이었다. 그나마 분별력을 가질 수 있는 부분은 프로그래밍 파트였으나 이 부분마저 코드를 외워서 치는게 아니라 오픈북이어서 그렇게 어렵진 않았던 것 같다. 게다가 시험시간은 3시간이었기에 타임 리밋도 거의 없는거나 다름없었다. 물론 시험보기 전에는 시험이 어떻게 나올지 예측할 수 없었기에 전날밤을 새워가며 공부하긴 했었다.\n\n\n\n마지막 날에 발표를 마치고 Lecturer분들의 ML/DL 트렌드에 관한 짧은 발표와 같이 수업들은 분들과 약간의 담소를 나누며 훈훈하게 수업이 끝났다. 약 3주간동안 저녁 시간에 열심히 배웠던 수업이 막을 내렸다.\n항상 가장 괴로울 때 가장 많이 배우는 것 같다. 사실 지나고 보면 그 고통이 어느정도 미화되는 것도 있고 위에 ‘어렵지 않았다.’, ’잘 마무리됐다.’라고 써놓기도 했지만, 그걸 하고 있던 순간에는 힘들었다. 아무리 알고 있던 내용이들이라도 그 내용들을 다시 다지는 것도 어렵고, 직접 코드를 써보는 것도 어렵고, 마무리 하기까지 불확실성과 두려움 때문에 괴로웠던 건 팩트다. 근데 항상 그러면서 배우는 것 같다. 그래서 이번 코스를 통해 배운 걸 정리해보자면,\n\nML은 확실히 DL보다 수학적인 이론이 어려웠음\n영어가 시험이나 부담이 아닌 ’언어’라는 것. 나를 표현할 수 있는 툴이라는 것\n전에 조금이라도 공부했던 것들이 나중에 도움이 ’크게’될 수 있다는 것\n내가 알고있는 것들을 꾸준하게 포스팅하는 습관을 길러야 하겠다는 생각\n해보지 않고 추측만으로 하는 생각들로 두려워하지 말 것\n\n수업을 마치고 certificate를 받았다. 사실 certificate에는 옵션이 있었는데, grade를 받을 것인지/pass or fail을 받을 것인지 내가 선택할 수 있었다. 등록 당시엔 근자감으로 grade를 받겠다고 했다가 수업 듣는 동안 과거의 나를 반성했다. 사실 수업하는 동안 그렇게 적극적이지도 않았고, 프로젝트 발표를 하고 지적을 받으면서 든 생각은 ‘최고 grade까지는 못받고 중간이라도 했으면..’ 했다. 그런데 생각지 못했던 최고 grade를 받을 수 있었다. 학점이 다가 아니긴 하지만 그래도 최선을 다했다는 것을 어느정도 뒷받침해줄 수 있기 때문에 뿌듯했다. 그래서 위에 배운 점들에 하나 더 추가하자면, ’최선을 다해서 어찌저찌해보면 뭐라도 한다.’라는 것. 식상한 멘트와 교훈이지만 매번 도전하면서 느끼는 바이기도 하다. 이렇게 2021년도 1월은 TU Berlin의 Machine Learning using Python: Theory and Application 코스와 함께 시간을 보냈다."
  },
  {
    "objectID": "posts/note/2024-12-19-init-vs-call.html",
    "href": "posts/note/2024-12-19-init-vs-call.html",
    "title": "📝__init__ VS. __call__",
    "section": "",
    "text": "Monkey Path를 공부하다가 한번 정리하면 좋을 것 같아 Python의 __init__과 __call__에 대해서 정리하고자 합니다.\n\n__init__은 인스턴스 초기화 시 실행\n__call__은 인스턴스 호출 시 실행\n\n\nclass A:\n\n    def __init__(self):\n        print('init')\n\n    def __call__(self):\n        print('call')\n\n    def myfunc(self):\n        print('my')\n\nprint(\"==== OUTPUT ====\")\n\na = A()\n\na()\n\na.myfunc()\n\n==== OUTPUT ====\ninit\ncall\nmy\n\n\n\nDecorator\n\n데코레이터는 자신이 수식할 함수나 메소드 내부에 받아 놓아야 함. 그러기 위해서 __init__에 데이터 속성 저장.\n데코레이터가 하는 일은 함수를 대리 호출.\n\n\nclass MyDecorator:\n    def __init__(self, data):\n        self.storage = data\n\n    def __call__(self):\n        print('data entered :', self.storage.__name__)\n        self.storage()\n        print('data exited :', self.storage.__name__)\n\n@MyDecorator\ndef printer():\n    print('I print the empty space.')\n\nprint('==== start ====')\nprinter()\n\n==== start ====\ndata entered : printer\nI print the empty space.\ndata exited : printer"
  },
  {
    "objectID": "posts/note/2023-07-05-daily-english-009.html",
    "href": "posts/note/2023-07-05-daily-english-009.html",
    "title": "🌎Casual English Phrases 009",
    "section": "",
    "text": "지지하다/응원하다\nroot for sm/sth\n\nto support or encourage somebody in a sports competition or when they are in a difficult situation\n\n\nMost of the crowd were rooting for the home team. 대부분의 관중들이 홈팀을 응원하고 있었다.\nGood luck, I’m rooting for you! You’ll be amazing. 행운을 빌어, 난 널 믿어! 넌 최고야.\nHis whole hometown was rooting for him as he made his professional boxing debut on live television. 그의 고향 사람들은 그가 생방송으로 전문 복싱 데뷔를 하는 것을 응원했습니다.\nI’ve always rooted for the company to succeed, since they made some of my most cherished games growing up. 나는 어릴 적부터 그 회사가 성공하기를 항상 바래왔습니다. 그들은 내가 가장 소중히 여기는 게임들을 만들어왔기 때문이죠.\n\n\n\n너 T야?(2nd meaning)\ntone-deaf\n\n\nunable to perceive public attitudes or preferences\nlacking emotional insight; insensitive or unsympathetic to others\n\n\n\nThey’re tone-deaf to the opinions of local people; they’re telling them where the new hospital will be without listening to them. 그들은 현지 주민들의 의견을 무시하고 있습니다; 현지 주민들의 의견을 듣지 않고 새로운 병원이 어디에 위치할 것인지 말하고 있습니다.\nThe council’s politically tone-deaf plan would cost lower income residents an extra 100 dollars a year. They simply can’t afford it. 시의 여론을 반영하지 않는(정치적으로 귀담아 듣지 못하는) 계획은 저소득 주민들에게 매년 추가로 100달러의 비용이 들게 할 것입니다. 그들은 당연히 그 비용을 감당할 수 없습니다.\nShe is often tone-deaf to her daughter’s needs. 그녀는 가끔 딸의 도움(정서적 교류의 필요)에 귀기울이지 못한다.\n\n\n\n내 손바닥 안이야\nhave someone in one’s pocket\n\nto have complete control over someone\n\n\nDon’t worry about the mayor. She’ll cooperate. I’ve got her in my pocket.\nJohn will do just what I tell him. I’ve got him and his brother in my pocket.\nI hear that the boss has half the police force in his pocket."
  },
  {
    "objectID": "posts/note/2023-03-29-daily-english-005.html",
    "href": "posts/note/2023-03-29-daily-english-005.html",
    "title": "🌎Casual English Phrases 005",
    "section": "",
    "text": "그거 참 위로가 되네\nWell, that’s very comforting\n\nThat’s very comforting↘️. Thanks a lot. 위안이 되네. 진짜 고마워\nThat’s very comforting↗️. 하나도 위로가 되지 않네요.\n\n\n문맥과 강약을 어떻게 주느냐에 따라 같은 말도 비꼬는 말이 될 수 있다.\n\n\n\n자는 것도 점점 더 어려워지네.\nEven sleeping is getting too complicated.\n\nEven meeting people is getting too complicated. 심지어 사람들 만나는 것도 점점 어려워져\nEven expressing my own feeling literally is getting too difficult. 내 감정을 말 그대로 표현하는 것조차 어려워지고 있어\n\n\n\n실수로\nby mistake\n\nThis filter paper I bought by mistake has only two holes. 실수로 리필 용지는 2구짜리로 샀거든요\nI made a reservation for tomorrow by mistake. 실수로 예약을 내일로 잡았어\n\n\n\n복수하겠어\nI’ll get you for this\n\nAs soon as I find out what’s going on, I’ll get you for this. 무슨 일인지 알아내기만 하면 반드시 복수하겠어\nHow dare you! I’ll get you for this. 어떻게 니가 그럴 수 있어! 내가 갚아줄거야"
  },
  {
    "objectID": "posts/note/2022-09-04-mid-retrospective.html",
    "href": "posts/note/2022-09-04-mid-retrospective.html",
    "title": "📘2022 상반기 회고",
    "section": "",
    "text": "이번 post는 2022년도 상반기 회고록에 관한 내용을 담고 있습니다. 지난 8월 25일에 있었던 글또콘 후기와 글또를 처음 시작했을 때의 다짐을 기반으로 회고를 해보았습니다."
  },
  {
    "objectID": "posts/note/2022-09-04-mid-retrospective.html#section",
    "href": "posts/note/2022-09-04-mid-retrospective.html#section",
    "title": "📘2022 상반기 회고",
    "section": "1",
    "text": "1\n글또에서 활동하기 시작하면서 개인적으로 Github의 커밋도 되도록이면 매일하는 목표를 이루고 싶었는데 상반기에 열심히 노력했으나 벌써 몇몇 빈구석이 있습니다. 최근에 1001일 매일 커밋하신 분을 봤었는데 하반기에는 더욱 더 노력을 해보려고 합니다. 더욱더 부지런하게..!"
  },
  {
    "objectID": "posts/note/2022-09-04-mid-retrospective.html#section-1",
    "href": "posts/note/2022-09-04-mid-retrospective.html#section-1",
    "title": "📘2022 상반기 회고",
    "section": "2",
    "text": "2\n글의 퀄리티는 항상 신경쓰지만 쉽게 올리기 힘든 것 같습니다. 개인적으로 글의 퀄리티라는 것은 내가 타겟으로 하는 독자들의 만족감이라고 생각하는데 우선 스스로 독자가 되어 되돌아봐도 아직 모자른 내용들이기에 아직 많이 모자른 것 같습니다. 개인 기록용으로 처음에 목표를 낮게 잡긴했지만 그래도 좀 더 욕심을 내보아서 다른 누군가에게 도움이 되고 이후에는 입소문도 날 수 있는 블로그가 되기를 기대해봅니다. 또한 글의 콘텐츠는 사실 하고 싶은 것이 많지만 하반기에는 특별히 지금까지 작성해보지 않았던 코드와 함께하는 포스팅을 도전해보려고 합니다."
  },
  {
    "objectID": "posts/note/2022-09-04-mid-retrospective.html#section-2",
    "href": "posts/note/2022-09-04-mid-retrospective.html#section-2",
    "title": "📘2022 상반기 회고",
    "section": "3",
    "text": "3\n삶의 풍요로움과 멘탈관리를 위해 조만간 특별한 취미생활을 시작할 것 같은데 글또 커뮤니티에 용기내서 공유할 수 있었으면 좋겠습니다. 사실 지금 글을 작성하면서도 이 취미생활을 할 수 있을까 많이 고민되기 떄문에 구체적으로 적진 않고 Slack에서 인사드리겠습니다.\n\n\n\n\n\n\n이번 포스팅에서는 글의 서두에 적은 것처럼 일기처럼 2022년도의 상반기를 한가닥 묶어보았습니다. 연말에 이글을 또 다시 읽어보며 조금 더 성장해있기를 바랍니다. :)"
  },
  {
    "objectID": "posts/note/2025-03-02-presentation-review.html",
    "href": "posts/note/2025-03-02-presentation-review.html",
    "title": "📘글또 발표 후기",
    "section": "",
    "text": "2025 데이터/AI 글또 반상회에서 미완성 로보틱스 AI 엔지니어 이야기: Not Cool, Not Chic, Not Chill이라는 제목으로 발표하게 된 일련의 이야기를 이번 포스팅에 남겨보려고 합니다.\n\n동기\n올해를 시작하면서, 으레 모두가 새해의 목표를 정하듯이, 저도 2025년에 도전해볼 무언가를 찾고 있었습니다. 나름 인생의 모토가 Stop Wishing, Start Doing인데 최근에는 도전하는 일들이 없다고 생각이 들었습니다. 그래서 아주 가끔씩 생기는 근거 없는 자신감과 용기를 얻어 머릿속으로만 내가 바라는 걸 생각하지 말고 질러보자라는 생각과 함께 도전 목록들을 생각하고 있었습니다. 마침 글또에서 반상회 발표자 모집을 하고 있었고, 이 기회가 나에게는 좋은 레벨업 스테이지가 될 수 있겠다는 생각이 들어서 바로 지원했습니다. (아직까지는) 글또가 10기가 마지막이라는 아쉬움도 있었고, 이제는 학교에서 벗어나 딱히 어디에서 앞으로 나와 발표할 기회가 없었기에 좋은 기회라는 생각이 들었습니다. 오랜만에 발표 실력도 일깨울겸 나의 이야기도 정리해볼겸 겸사겸사 저에게 주어진 몇 안되는 기회중에 하나라는 생각에 들뜬 마음으로 지원했던 것 같습니다.\n조금 더 솔직하게 발표 지원 동기를 이야기하자면, 저는 학교를 졸업하고 사회에서 일을 시작하면서 정체성에 대한 혼란을 많이 겪었습니다. 연구자라고 하기엔 애매하고, 엔지니어라고 부르기에도 어딘가 모호한. 로보틱스를 한다고 하기엔 부족하고, AI를 한다고 하기에도 어중간한. 회사에서도 저의 역할을 명확하게 정의하기 어려웠고, 모든 사회초년생이 그렇듯 제 실력과 역할에 대한 의구심까지 겹치면서 혼란스러운 시기를 보냈습니다. (솔직히 말하면, 지금도 그 혼란이 완전히 사라졌다고는 자신 있게 말하기 어렵습니다.) 그때 스스로가 정리되지 않아 답답할 때, 어떻게 해결해왔더라? 과거를 돌아보니, 다른 사람들에게 제 이야기를 꺼낼 때 오히려 말하면서 스스로 답을 찾아갔던 순간들이 떠올랐습니다. 그래서 반상회 발표도 마찬가지였습니다. 저를 잘 알지 못하고, 쉽게 판단하지 않을 사람들 앞에서 솔직하게 제 이야기를 풀어놓는다면, 답을 찾을 기회가 되지 않을까 싶었습니다. 솔직히 고백하자면, 반상회 발표는 다른 사람들에게 도움을 주기 위한 목적보다는, 오히려 저 스스로를 끄집어내기 위한 하나의 도구였습니다.\n\n\n준비\n야심찬 동기와 발 빠른 지원 태도와는 달리, 막상 준비를 시작하면서부터 많이 삐걱거리기 시작했습니다. 애초에 발표의 목적은 내 이야기를 꺼내서 스스로 정체성을 확립해보자는 것이었는데, 초안을 작성하고 구성을 고민하다 보니 한 가지 의문이 들었습니다. 사람들이 과연 내 이야기를 듣고 싶어할까? 이 생각이 들자마자, 이야기를 꺼내는 것 자체가 망설여지기 시작했습니다. 이야기의 시작점을 어디로 잡아야 할지, 어디까지 이야기해야 할지, 이러다가 괜히 삼천포로 빠지는 건 아닌지, 스토리의 흐름이 이게 맞는 건지… 머릿속에서 콘텐츠들이 둥둥 떠다니며 정리가 되지 않았습니다. 마치 질서 없는 물 위에서 표류하는 기분이었습니다.\n\n\n\n1달 조금 넘게 남았고, 어차피 내 이야기를 하는 발표이니 별다른 자료조사가 필요 없을 거라 생각했습니다. 그래서 시간 문제는 없을 거라 판단했지만, 불행히도 머릿속에서만 방황하는 데 3주 이상을 써버렸습니다. 발표 리허설을 준비하기 직전까지도 발표의 골자를 계속 수정하며 고민했고, 정작 중요한 내용보다도 발표 제목이나 중간에 넣을 밈(meme) 같은 부수적인 요소에 신경 쓰느라 시간을 허비하기도 했습니다. 그 결과, 준비 시간을 제대로 활용하지 못한 채 시간을 흘려보내고 말았습니다.\n그러는 사이 어느덧 반상회 준비위 분들 앞에서 온라인 리허설을 할 시간이 다가왔고, 그제야 이제 정말 정신 차려야 한다는 생각이 들었습니다. 동시에, 아직 이렇게 준비도 안 됐는데, 대체 왜 발표 지원을 했을까… 하는 후회도 밀려왔습니다. 결국 마음에 들지 않는 발표 자료를 꾸역꾸역 완성해 가며, 그때의 조급함과 혼란스러움을 온몸으로 느꼈던 기억이 납니다. 지금까지 수도 없이 발표를 해왔는데 이렇게까지 자신감이 없었던 적이 처음이었습니다.\n\n\n\n그럼에도 불구하고, 항상 위기를 기회로 만들어주는 글또 분들의 도움 덕분이었을까요. 저는 정말 복이 많은 사람인 것 같습니다. 늘 주변에 대단하고 따뜻한 분들이 계셨고, 이번에도 그 손길 덕분에 무너져가던 마음을 다잡을 수 있었습니다. 발표가 정리되지 않았고, 시간 체크도 한 번도 해보지 않은 상태였으니 리허설이 잘 나올 리 없었습니다. 그런데도 다들 진지하게 제 발표를 들어주시고, 어떤 부분을 개선하면 좋을지 함께 고민해 주셨습니다. 심지어 체크할 수 있는 지표까지 제안해 주시며 혼란스러웠던 제 멘탈을 붙잡아 주셨죠.\n\n\n\n사실 발표를 준비하면서 가장 망설였던 부분이 과연 내 이야기를 듣고 싶어할까?라는 의문이었습니다. 생각해보니 지금까지 발표들은 논문/이론 설명이나 연구 발표 등 정보 전달성이 큰 발표들이라 자료조사와 이해해서 자료를 준비하는게 힘들었지 내용을 구상하는게 힘들지 않았었습니다. 하지만 이번에는 내 이야기를 한다는 측면에서 애초에 발표의 컨텐츠 자체가 망설여지고 갈피가 잡혀지지 않으니 그 고민에 너무 오래 머물러 있었는데, (비록 격려 차원에서 배려의 말씀일 수도 있었겠지만) 이야기를 더 풀어줬으면 좋겠다, 충분히 흥미롭다는 피드백을 들으며, 마치 멈춰 있던 머릿속에 다시 재생 버튼이 눌리는 기분이었습니다. 게다가 중간중간 따뜻한 격려도 해주셔서, 자신감을 잃어가던 저에게 정말 큰 힘이 되었습니다.\n리허설을 마치고 나니 발표의 갈피가 조금씩 잡히기 시작했습니다. 제목도 수정하고, 발표의 방향도 다시 고민해보면서 내 이야기를 편하게 풀어가되, 로보틱스 AI 엔지니어라는 아이덴티티를 내가 만들어가고 싶은 것이니, 이 부분을 좀 더 집중해서 소개해보자는 생각이 들었습니다. 그래서 발표 구성을 다시 정리했습니다. 내 개인적인 이야기는 후반부에 배치하되, 조금 더 사람 냄새가 나는 이야기들을 첨가하기로 했습니다. 그리고 발표 초반에는 로보틱스 AI 엔지니어라는 개념이 생소한 사람들에게 도움이 될 만한, 초심자를 위한 정보성 내용을 담기로 했습니다. 이렇게 발표의 큰 틀을 정리하고 나니, 그때부터는 발표 자료를 다듬는 시간이 더 이상 괴롭지 않았습니다. 오히려 열정을 불태우며 즐겁게 준비할 수 있었던, 정말 시간이 지나가는 지도 모르게 재밌는 시간이었습니다.\n(덧. 사실 회사에서 우연히 한 동료분께서 저에게 로보틱스 개념에 대한 질문을 던지셨는데, 그 질문에 대한 답을 정리하면서 발표 자료를 다듬는 데 큰 도움이 되었습니다. 실명을 거론할 순 없지만, 항상 우연하게도 도움을 주시는 분들이 주변에 있다는 게 감사할 따름입니다. 그 마음을 담아 이곳에 간략히 남겨봅니다.)\n\n\n반상회와 발표\n드디어 데이터/AI 엔지니어 반상회 당일이 되었습니다. 퇴근을 하며 설렘 반, 두려움 반의 마음으로 행사장으로 향했습니다. 혹시 미리 준비할 게 있을까 싶어 일찍 도착했고, 긴장을 풀 겸 스스로에게 응원의 의미로 블루보틀에서 비싼 따뜻한 라떼를 한 잔 시켰습니다. 머릿속으로 발표 내용을 되새기며 마음을 다잡은 채, 천천히 발표장으로 들어섰습니다. 그때, 글또 반상회 준비위 분들이 마련해주신 타로 초콜릿을 받았는데, 그 안에 시킨 대로 한다면이라는 메시지가 적혀 있었습니다. 그걸 보는 순간, “그래… 준비한 대로만 하면 된다.” 그렇게 되뇌며 마음을 다잡았던 것 같습니다.\n\n\n\n발표 시간은 생각보다 떨지 않고, 오히려 편하게 이야기할 수 있었습니다. 그리고 후에 알게된 사실이지만 시간도 딱 맞춰서 발표해서 정말 다행이다라고 생각했습니다. 아마도 청중이 글또 분들이었기 때문에 자연스럽게 편안함을 느꼈던 것 같습니다. 준비하는 동안 함께해주셨던 분들, 글또에서 만나뵈었던 분들, 익숙한 얼굴들이 발표장 곳곳에서 보여서 더욱 안심이 되었던 것 같습니다. 혹시 이 포스팅을 보시면서 제 발표자료가 궁금하신 분들은 여기에서 발표 자료들을 확인하실 수 있습니다. :)\n\n\n\n그렇게 발표를 마치고 나서는, 반상회의 다양한 이벤트들을 마음껏 즐길 수 있었습니다. 그룹 커피챗을 비롯한 여러 프로그램들이 정말 재미있었고, 저뿐만 아니라 멋진 사이드 프로젝트를 공유해주신 장회정님의 발표도 (제 발표가 끝났다는 안도감 덕분에 더욱 편하게) 흥미롭게 들을 수 있었습니다. 또한 8조 그룹에서 만난 모든 분들과 웃으며 편하게 이야기를 나누며, 정말 즐거운 시간을 보낼 수 있었습니다. 그렇게 글또 10기 데이터/AI 반상회는 저에게 또 하나의 잊지 못할 하루로 남게 되었습니다.\n\n\n\n\n\n후기\n본격적인 후기를 시작하기 전에, 반상회 후 설문지에 남겨주신 글또 분들의 따뜻한 발표 후기를 먼저 공유하려 합니다. 이 감사한 글들을 올려두고, 앞으로도 두고두고 꺼내 보며 힘을 얻으려 합니다.(참고로, 중간에 제가 저 스스로에게 쓴 코멘트가 이스터에그처럼 숨어 있으니, 한번 찾아보시는 것도…! 😉) 실제로 발표에서 이야기했던 것처럼, 로보틱스에서 활용하는 AI에 대해 잘 모르셨던 분들이 흥미와 관심을 가지게 되었다는 점에서 무척 뿌듯했습니다. 그리고 사실, 이번 발표의 또 다른 목표였던 제 스스로의 정체성을 좀 더 다지는 것도 이루었다는 생각이 들어 더욱 의미 있는 시간이었습니다.\n\n \n\n글또를 7기부터 시작하면서 지금까지 글또라는 커뮤니티에서 활동하며 저는 개인적으로 가장 많은 특혜를 받은 사람이라고 생각합니다. 글또 지속 기간으로 따져도 가장 오래한 것도 아니고, 가장 글을 많이 썼는가?의 기준으로 보아도 한번도 빠뜨리지 않고 작성한 사람도 아니지만, 저 스스로 글또에서 글을 쓰며 성장해온 자신이 가장 자랑스럽기 때문에 가장 많은 특혜를 받았다고 생각합니다. 어쩌면 가장 많은 이라는 수식어의 비교 대상은 글또의 다른 분들이 아닌, 글또가 아닌 다른 기회 등으로 성장한 것보다 가장 나를 다져준 도구가 글또라는 반증이라서, 비교 대상은 내가 나를 끌어올린 성장의 도구들 중 글또가 가장이라고 볼 수 있을 것 같아요.\n이렇게 또 하나의 도전을 실행했고, 다행히도 좋은 경험으로 남아 앞으로도 큰 자양분이 될 것 같아 뿌듯합니다. 혹시라도 (글또가 지속되고) 나중에 글또 반상회가 다시 열려, 발표를 망설이는 글또 분들이 계신다면, 이 글이 작은 용기가 되었으면 좋겠습니다. 아니면, 꼭 글또가 아니더라도 자신의 이야기를 하는 것을 망설이는 모든 분들에게도 전하고 싶은 응원입니다. “이렇게 준비가 부족했던 사람도, 충분히 성장할 기회로 삼아 발표를 지원했고, 결국 잘 마무리할 수 있었구나.” 그렇게 보면서, 저처럼 한 걸음 내딛는 계기가 되었으면 좋겠습니다.\n그리고, 정말 마지막으로… 수고한 나에게 박수를! 👏👏👏"
  },
  {
    "objectID": "posts/note/2023-07-07-geultto-8th-end.html",
    "href": "posts/note/2023-07-07-geultto-8th-end.html",
    "title": "📘Geultto 8th End",
    "section": "",
    "text": "2번째로 참여한 글또 8기에서 마지막 글을 쓰는 날이 되었습니다. 지금까지 있었던 일들을 되돌아보며 마무리 회고를 해보려고 합니다. 이전에 회고할 때는 활동을 시작하며 작성했던 다짐글을 보며 그때 목표했던 것들을 잘 이루었는지 점검하는 방식으로 했었지만, 이번에는 자유롭게 활동했던 이야기들 중심으로 회고해보려고 합니다. 글또라는 공동체에서 했던 활동들을 주로 돌아보며 작성하겠지만 2023 상반기에 있던 개인의 일상사와 생각도 자연스럽게 담기는 회고가 될 것 같습니다.\n매번 시작할 때 예치금을 넣어놓고 예치금을 절대 까먹지 않으리라 했던 목표는 이뤘습니다. 사실 다짐글에서는 pass권도 안쓰고 모든 제출을 하는 것이 목표였지만 2번의 pass도 다 쓰고, 추가적으로 한번은 제출을 하지 못해서 만원 차감이 되었었습니다. 다행히도 커피드백 환급금으로 만원 손실은 매꿀 수 있었고 결과론적으로는 예치금은 사수할 수 있었습니다. 간단한 활동 결산표이지만 이번 글또 8기에서의 제 활동을 함축적으로 잘 보여주는 것 같습니다."
  },
  {
    "objectID": "posts/note/2023-07-07-geultto-8th-end.html#good-stories",
    "href": "posts/note/2023-07-07-geultto-8th-end.html#good-stories",
    "title": "📘Geultto 8th End",
    "section": "Good Stories",
    "text": "Good Stories\n\nWriting\n역시 글또의 본질, 글쓰기 활동에 대한 점검은 가장 중요한 부분이라고 생각합니다. 블로그 글을 쓰면서 1차로는 당연히 나에게 도움 되는 글을 쓰는 게 목적이었고 2차로는 다른 사람들에게도 도움이 되고 잘 읽힐 수 있는 글이 되는 것이 목표였습니다. 사실 이 생각은 이전에 7기를 마무리하며 썼던 글에서 이야기했던 것처럼 연구자로써 더 좋은 글을 쓰고 싶은 마음이 처음 동기가 되어 지금까지도 변함없는 생각입니다. 이전에 썼던 글들과 다르게, 이번 글또 활동에서부터 정말 청중들에게 발표하듯이 작성해보자라는 생각을 했었기 때문에 개괄식/~이다 문체를 피하고 서술식/~합니다 문체로 작성하는 연습을 했었습니다. 글의 길이가 길어지고 핵심까지 도달하는 시간이 좀 걸리는 글이지만 전체적인 독자의 이해도를 높일 수 있기 때문에 결과적으로 문체를 바꿔서 작성한 글들이 더 좋다는 생각이 들었습니다. 개인적으로 바뀐 문체로 작성한 글들이 퇴고를 할 때도 더 좋았기 때문에 앞으로도 문체를 유지하면서 글을 작성하려고 합니다. 형식 뿐만 아니라 내용과 주제 측면에서도 현재 연구하고 있는 분야와 연관된 논문들과 코드들을 중점적으로 다루면서 원하는 주제들을 재밌게 작성할 수 있었습니다. 이번 8기에서 작성한 총 9개의 글들을 통해 확실히 글쓰는 능력이 한층 성장되었다는 것을 느낄 수 있었습니다.\n스스로 느끼는 성장 히스토리가 가장 중요하지만 다른 사람들에게 어떻게 제 글이 보이는지도 많이 궁금했었습니다. 다른 사람들이 내 글을 볼 때 어떨까?라는 궁금증을 이전 7기 때까지는 없었던 운영진 분들의 감사한 수고로 생긴 큐레이션이라는 파트를 통해 확인해볼 수 있었습니다. 큐레이션을 통해 다른 분야들의 좋은 글들도 볼 수 있었고 객관적으로 제가 쓴 글을 본 독자들의 생각도 알 수 있었습니다. 이번 8기 활동을 하면서 총 4개의 제 글들이 큐레이션이 되었다는 사실이 지금도 얼떨떨하긴 합니다. 큐레이션이 되었다는 사실 자체가 좋은 글이다 라는 절대적인 판단 기준이 될 순 없지만, 글또에 실력 좋고 대단하신 분들이 2주 마다 작성하시는 많은 좋은 글들 중에 조금이라도 눈에 띄었다는 사실에 제가 쓴 글들에 대해 좋은 인정을 받은 것 같아 기분이 좋았습니다.\n\nCurated 1: WASABI\nCurated 2: K-Accessibility for RL\nCurated 3: Github Starstruck 128\nCurated 4: Chord Graph\n\n\n\n\n4번의 큐레이션 선정\n\n\n\n\nNetworking\n커피와 함께한 feedback&chat\n글을 쓰는 습관과 능력을 기를 수 있다는 것이 글또의 주된 매력적인 포인트지만, 글또에서 만나는 멋진 분들이 글또의 또 다른 매력인 것 같습니다. 이번 기수에서는 총 3회의 커피드백에 참여를 했었고 함께하신 분들의 이야기들을 들으면서 배울 점들도 많았고 내 이야기도 나누면서 느끼는 점도 많아서 다 소중한 시간이었습니다. 매번 모임마다 2-3시간은 기본으로 각자의 삶과 진로를 고민하는 이야기들을 나누면서 때론 나만 힘든 게 아니라는 사실에 위로도 받고 힘도 얻었으며, 때론 머릿속에서만 뱅뱅 돌고 있던 고민들을 입 밖으로 꺼내 이야기하면서 새삼 내가 이런 생각들을 가지고 있었구나 놀랐을 때도 있었습니다.\n\n\n\n3번의 커피드백 모임\n\n\n1, 3회차 때는 캐쥬얼하게 각자의 삶의 이야기나 세상 이야기들을 나누었지만, 커피드백이라는 이름에 맞게 2회차 때는 이때까지 글을 쓴 것들을 살펴보며 서로 좋게 생각한 점, 보완하면 좋을 것 같은 점들을 나누었었습니다. 아마 제일 오랫동안 서로 피드백을 주고 받았던 것으로 기억하는데(한 4시간 정도) 감사하게도 같이 모인 모든 분들이 처음부터 끝까지 진지하게 하나하나 각자의 글들을 읽어보고 좋은 생산적인 의견들을 주셔서 어떻게 글의 질을 높일 수 있을 지에 대한 많은 인사이트를 얻을 수 있었습니다. 글또 커피드백 참여가 정말 소중하다는 생각을 했었습니다.\n글또에서의 또 다른 만남들\n사실 오프라인 커뮤니티 모임에서 동기부여를 얻는 것을 매우 좋아합니다. 내가 잘 모르는 분야라도 집중해서 듣다 보면 익숙해지면서 새로운 세상을 알게 되는 것 같아 재밌고, 해당 분야에 열정이 많은 멋진 연사님들이 하시는 발표를 들으면 나도 모르게 좋은 에너지를 받아서 자주 오픈 세미나에 참석하는 편입니다. 그런 저에게 글또에서 하는 채널별 반상회는 정말 기다리던 행사였고 기대했던 만큼 너무나도 좋은 데이터 통합 반상회를 운영진 분들과 반상회 준비위 분들이 만들어 주셔서 정말 감사했습니다. 각자의 분야와 자리에서 데이터, AI 직군/연구에 대한 고민을 치열하게 했던 이야기들을 들으면서 스스로의 한계점에 지쳐있던 참에 힘을 받을 수 있었고, 데이터로 보는 글또 발표에서는 재밌고 공감 되는 부분들도 많아 많이 웃으면서 발표를 들을 수 있었습니다.\n\n\n\n글또 반상회와 글또 안에서의 네트워킹\n\n\n커피드백과 반상회 같은 공식적인 일정 말고도 슬랙을 통해 많은 분들을 만날 수 있었는데, 소심이 I이지만 이번에는 저도 용기 내서 몇 번 활동을 참여하거나 호스트 역할을 했습니다. 해외 대학원/취업을 고민하는 분들을 만나 잠시 접어두었던 꿈과 도전에 대해 생각해볼 수 있었고, 막막했던 코딩 테스트 준비를 위해 스터디 활동도 하고, 혼자 나름 커스텀한 키보드 자랑도 해보고, 진로 고민하는 시기(현재 진행 중..)에 해외 취업도 미쳤다 생각하고 도전해보자는 마음에 공고를 내고 사람들을 모아 보기도 했습니다. 돌아보니 뭔가 제가 이것저것 많이 활동을 했다는 게 신기합니다. 물론 모든 모임에서 제가 생각한 대로 생산적인 결과나 마무리를 하지 못했거나 부족한 점들이 있지만 좋은 사람들을 만나는 용기가 저에겐 쉽지 않은 시도였기 때문에 이후에도 내가 유지해야 할 인생의 태도라고 생각이 들었습니다."
  },
  {
    "objectID": "posts/note/2023-07-07-geultto-8th-end.html#bad-stories",
    "href": "posts/note/2023-07-07-geultto-8th-end.html#bad-stories",
    "title": "📘Geultto 8th End",
    "section": "Bad Stories",
    "text": "Bad Stories\n좋아하는 노래 중에 AJR의 100 Bad Days가 있는데 가끔씩 너무 힘들고 내가 너무 초라해져 보이거나 후회가 되는 일들이 있을 때, 노래 가사를 새겨보며 마음을 다잡습니다. 노래에서 “나쁜 이야기들이 나중에 재밌는 파티에서의 썰을 풀 소재가 된다”라는 메세지가 너무 심각해져있는 마음의 답답함과 긴장감을 풀어주는 느낌이고 “나쁜” 이야기(경험)이 꼭 나쁘지만은 않다는 점이 위로가 정말 많이 됩니다. 그런 면에서 회고 파트에서 Bad Stories를 기록하는 부분도 훗날 재밌는 안줏거리 혹은 지금의 나를 만든 한 페이지의 증거로 소중하다 생각하며 적어봅니다.\n꾸준함에 대한 반성\n글또 다짐글을 다시보며 든 생각은.. 정말 하나도 지킨 다짐이 없다는 생각이 들면서 너무 부끄러웠습니다. 작성했던 다짐들을 살펴보면 미래의 나에 대해 높은 기대감을 가지고 높은 꾸준함을 요구하는 목표들을 나열했었더군요. 마치 당장의 내가 180도 달라질 것을 기대하는 것 같은 느낌이 들 정도로, 지금 생각해보면 목표를 너무 크게 잡은 것이 아닌가하는 생각이 들었습니다. “모든 글 제출”, “제출 기간 외에도 작성하는 꾸준한 습관 기르기”등 목표에서 나에게 바라는 꾸준함의 기준은 굉장히 높은데 이를 조금 낮추어서 하나만 지켰더라도 좋지 않았을까..라는 페이스 조절에 대한 반성을 하게 되었습니다. 누가 강제로 시킨 약속이 아니더라도 스스로에게 한 약속에 대해 좀 더 엄격해질 필요성을 느끼며 지금은 실패한 역사로 남지만 훗날 꾸준히 글을 쓰는 사람이 되어있길 바래봅니다. 글을 쓰는 꾸준함 뿐만 아니라 생활 속에 고치고자 했던 작은 습관들도 포기하지 않고 좋은 방향으로 꾸준히 변화해가는 힘을 기르고 싶네요.\n현재를 살아가기\n저는 사서 고생이 아닌, 걱정을 하는 타입입니다. 걱정도 많고 미래의 불확실성에 대한 두려움이 정말 많은 사람이기 때문에 현재에 집중하지 못해서 상반기에 하고자 했던 일들을 많이 하지 못했습니다. Stop wishing, start doing이라는 좌우명을 설정해서 행동하는 사람이 되자는 취지로 스스로 격려를 하지만, 여전히 필요 없는 생각들로 집중력이 흐트러지고 앞으로 나아가지 못하는 것 같습니다. 내가 오늘, 지금 이 시간에 해야 하는 것을 단순하게 정리하고 집중하는 사람이 될 수 있도록 하반기에 다시 생활 리듬을 돌아보고 해야 할 것과 하지 말아야 할 것 목록을 만들어서 지켜보겠습니다. 그리고 불안한 마음들을 적어보고 이 감정이 현재 내가 개선할 수 있는 action plan으로 바꿀 수 있는 것인지 점검하는 시간도 하루를 마무리하면서 점검을 해보겠습니다."
  },
  {
    "objectID": "posts/note/2025-01-05-Goodbye-2024.html",
    "href": "posts/note/2025-01-05-Goodbye-2024.html",
    "title": "📘Goodbye 2024",
    "section": "",
    "text": "2024 정말 잊지 못할 한해였습니다.\n사람이 살아가면서 많은 마일스톤들이 있고, 그 마일스톤을 한해에 하나 담기도 어려운 법인데 2024는 3가지 마일스톤들이 지나간 해였기에 가장 기억에 남지 않을까 싶습니다.\n하나 하나의 키워드가 굵직굵직해서 솔직히 지금도 이게 저한테 한해동안 일어난 일이었던가 어떨떨하긴 한데, 돌이켜 보면 정말 그렇습니다. 그렇기에 2024년도 회고야 말로 정말 쓸것도 많고 다시 되짚어 볼 것도 많은 것 같습니다. 가장 기억이 나지 않는 1월부터 달력을 짚어보며 회고를 시작해보겠습니다."
  },
  {
    "objectID": "posts/note/2025-01-05-Goodbye-2024.html#월",
    "href": "posts/note/2025-01-05-Goodbye-2024.html#월",
    "title": "📘Goodbye 2024",
    "section": "1월",
    "text": "1월\n12월 폭풍 전야와 같은 논문 심사를 마치고 마지막 논문 편집을 하며 시간들을 보내고 있었습니다. 2년 동안 다사다난했지만 결국 이 논문 한편을 위해 달려왔다는 사실을 다시 깨닫게 되는 시간이었고, 내 자신이 나름 대견하다고 생각하며 마지막 졸업 논문 편집을 달리고 있었습니다. 비록 졸업 논문이기에 많은 사람들이 읽고 피드백을 주는 연구적인 임팩트가 큰 결과물이라고 볼 수는 없지만, 석사 기간동안 내가 공부한 것들과 주장하고자 하는 것들을 정리하는 시간이 지금까지도 큰 거름이 되는 것 같습니다.\n졸업을 하고 한동안은 나에게 주는 휴식 선물로 취직을 바로 할 생각이 없없지만, 말할 수 없는 사정에 의해 바로 취준을 시작했습니다. 그래서 아직 공채 기간이 시작되지 않았기에 스타트업들 위주로 열심히 지원서들도 쓰기 시작했던 기간이었습니다. 물론 석사 연구 내용과 유관한 직무들을 찾긴했었지만 막상 사회생활 전선으로 나아가니 나를 어필할 부분이 많지 않아 당황스럽기도 하고 조금은 좌절스럽기도 했지만, 다른 측면으로 나의 성과들을 돌아보고 정리할 수 있어서 좋았던 것 같습니다. 취준을 하면서 가장 막막했던 점은 코딩 테스트와 같은 스킬적인 측면을 보는 것들이 스트레스가 심했었는데, 아무래도 지금까지 연구 코드만 작성해오던 사람이라 알고리즘 능력에서는 많이 부족하기도 하고 시간 제한이 있는 상황이 익숙하지 않았던 것 같습니다."
  },
  {
    "objectID": "posts/note/2025-01-05-Goodbye-2024.html#월-1",
    "href": "posts/note/2025-01-05-Goodbye-2024.html#월-1",
    "title": "📘Goodbye 2024",
    "section": "2월",
    "text": "2월\n2월은 졸업식이 있었습니다. 대학교 졸업때는 코로나 시기였기에 친구들도 초대하지 못하고 바로 입학할 대학원으로 복귀해야 하는 상황이어서 졸업식을 잘 즐기지 못했던 아쉬움이 있었습니다. 하지만 석사 졸업은 연구실에 단 한명 졸업에다가 친구들도 초대해서 진정한 주인공 놀이를 할 수 있었던 것 같습니다.\n\n\n\n1월부터 여기저기 열심히 알아보던 취준은 한 스타트업에 합격이 되면서 마무리가 되었습니다. 사실 대기업과 스타트업 사이에서 많은 고민들이 있었지만 조금 더 많은 권한과 다이나믹한 환경에서 일하고 싶다는 생각에 스타트업을 선택했었습니다. 직무와 회사 위치 등 나쁘지 않은 조건으로 3월달부터 일하게 되어서 급하게 동생과 함께 일본 여행을 떠났었습니다.\n\n\n\n급하게 준비하는 여행이라 여행을 준비하는 설레는 마음이 크진 않았지만 그동안 긴 시간 공부하느라 수고 많았다고 스스로에게 상을 주는 여행이었기에 행복했던 시간이었습니다. 생각해보니 대학생때 교환학생을 할 수 있는 기회도, 여행을 갈 수 있는 시기들도 코로나 때문에 다 놓쳐버리고 코로나가 조금 잠잠해졌어도 대학원생이라는 신분에 걸맞게(?) 자유롭지 못했기에 해외 여행은 꽤 오랜시간 동안 마음 한켠에 소원으로만 있었습니다. 가까운 일본 조차 한번도 가본적 없었기에 바로 비행기 티켓을 끊고 떠났던 마음은 정말 최고의 상이었던 것 같습니다."
  },
  {
    "objectID": "posts/note/2025-01-05-Goodbye-2024.html#월-2",
    "href": "posts/note/2025-01-05-Goodbye-2024.html#월-2",
    "title": "📘Goodbye 2024",
    "section": "3월",
    "text": "3월\n진짜 직장인이 되었습니다. 사실 초기에 회사에 적응하는 게 쉽지 않았지만 나이대가 비슷하고 에너제틱한 회사 분위기 때문에 즐거웠던 것 같습니다. 물론 대학원생일 때와 차원이 다른 월급에 신났던 것도 사실이지만요. 마치 신학기에 들뜬 설렘과 긴장감이 교차하는 듯한 느낌을 직딩으로써 느끼는 한달이었습니다."
  },
  {
    "objectID": "posts/note/2025-01-05-Goodbye-2024.html#월-3",
    "href": "posts/note/2025-01-05-Goodbye-2024.html#월-3",
    "title": "📘Goodbye 2024",
    "section": "4월",
    "text": "4월\n4월은 회사에서 워크샵을 다녀왔었고 생각보다 대학교 MT같은 분위기에 재밌게 즐길 수 있었던 것 같습니다. 사실 첫 회사가 대학원 같은 성격이었어서 마치 대학원을 다시 입학한 느낌이긴 했습니다. 그래도 회사이기에 내가 어떻게 기여할 수 있고 어떻게 평가받을 것인지 계속 긴장했던 것 같습니다.\n또한 4월에 20년 이상 알고지낸 친한 언니의 결혼식이 있었는데 정말 감회가 새로웠습니다. 4-5살때부터 알고지내온 언니의 결혼식을 마주하고 나니 저도 벌써 그런 나이가 되었구나 새삼 깨달았고, 지금까지 학교생활만 해온 나에게 이후 삶에 대한 진지한 고민도 하게된 날이었습니다. 졸업하고 직장까지 가지게 되었으니 마치 결혼이 다음 숙제인 것 같은 느낌.. 그리고 나는 그런 인생의 동반자를 만날 수 있을까.. 이런 저런 고민들이 머리속에 떠올랐던 것 같습니다. 각설하고, 언니의 결혼식에서는 진심으로 축하와 행복을 기도했습니다."
  },
  {
    "objectID": "posts/note/2025-01-05-Goodbye-2024.html#월-4",
    "href": "posts/note/2025-01-05-Goodbye-2024.html#월-4",
    "title": "📘Goodbye 2024",
    "section": "5월",
    "text": "5월\n5월은 가정의 달이기도 하고 많은 휴일들이 있어서 행복했습니다. 또한 직딩으로써 처음으로 어버이날에 부모님께 용돈을 드릴 수 있어서 뿌듯했습니다. 28살이 되어서야 드린다는 게 죄송하긴 했지만 그래도 지금이라도 드릴 수 있어서 다행이다 싶었습니다.\n5월에 갑자기 대학 동기를 만나게 된 날이 있었는데, 이 날 제 꿈에 대해 다시 복기해보게 되었었습니다. 연말 회고에 적을 정도로 인상이 깊은 날이었는데, 퇴근을 하고 그 친구를 만나는 날 예전의 빛나는 눈빛이 사라진 것 같다라는 말을 듣게 되었습니다. 사실 회사에서 어떠한 이유로 점점 지쳐가고 있었고 어떻게 타파해야할지 몰라 힘든 상황이었는데 그런 상황을 모르는 친구가 그 이야기를 하자 다른 사람을 통해 진짜 내가 지쳐있구나 확인할 수 있었습니다. 그래서 어떤 방향을 찾아봐야겠다는 능동적인 태도를 취하게 되었고 나를 위한 다른 자리를 찾아보고자 이직을 준비하기 시작했습니다. 회사에서 무슨 보람을 찾는 거냐, 낭만을 쫓는 거 아닌가 싶을 수도 있었지만 친구의 말을 통해서 다시 마음속에 열정을 찾고 싶다는 생각이 들었던 것 같습니다."
  },
  {
    "objectID": "posts/note/2025-01-05-Goodbye-2024.html#월-5",
    "href": "posts/note/2025-01-05-Goodbye-2024.html#월-5",
    "title": "📘Goodbye 2024",
    "section": "6월",
    "text": "6월\n첫 회사에서의 첫 결과물을 내는 시기였습니다. 자세히 말할 수는 없지만 처음으로 사회에서 내가 참여한 프로젝트의 결실을 만들어 낸다는 사실이 많이 부담되기도 하고 정말 잘하고 싶다는 욕심도 많이 들어갔던 것 같습니다. 그 과정에서 미숙한 마음과 태도로 결과의 성패와 상관없이 실망한 부분들이 있었고 프로젝트가 마무리가 되는대로 퇴사를 하겠다는 의사를 전달하게 되었습니다. 퇴사와 별개로 마지막까지 제 역할을 잘 마무리하고 나온 것 같아 아쉬움은 없었습니다. 이때 얻은 교훈들은 지금도 많이 도움이 되고 있습니다.\n사실 이직하는 곳을 어느정도 정해놓았었기 때문에 이후 돈을 벌 걱정은 크지 않았습니다. 다만 이직을 처음하다보니 당황스러웠던 점들도, 두려웠던 점들도 많았습니다. 내가 할 수 있는 역할들과 별개로 다른 사람들과 같이 일한다는 건 많이 다르구나를 느꼈던 것 같습니다. 처음에는 이런 점들에 능숙하지 못한 스스로가 실망스러웠지만 이렇게 배워가며 성장하는 거겠지..라고 지금은 생각하고 있습니다."
  },
  {
    "objectID": "posts/note/2025-01-05-Goodbye-2024.html#월-6",
    "href": "posts/note/2025-01-05-Goodbye-2024.html#월-6",
    "title": "📘Goodbye 2024",
    "section": "7월",
    "text": "7월\n7월 중순까지 다니고 나서 퇴직을 하고, 졸업하고도 많이 놀지 못했으니 한달은 놀아야지! 하고 국내에서 전시회등을 돌아다니며 휴식을 취했었습니다. 버킷리스트 중 하나가 혼자 여행하는 것이 있었는데 서울 종로 한옥 숙소에서 약 4일간 여행을 했습니다.\n\n\n\n서울 태생이지만 바빠서 서촌과 같은 한옥의 정취를 느낄 수 있는 곳들을 가보지 못했었는데 이번 기회에 한옥 숙소에서 맘껏 즐기며 나를 돌아보는 시간을 가졌었습니다. 퇴사와 이직을 하면서 스스로도 단단하지 못함을 느꼈었는데, 왜 그랬는지 어떤 상황에서 내가 어떤 모습이었는지 등등 많은 생각들을 할 수 있었습니다. 이 때 내린 결론은 내가 하는 직무에서 너무 이상적인 Hero를 찾을려고 하지 말자는 결론을 내렸습니다. 주니어고 신입이니까 라는 이유로 내가 하는 직무에서 나를 리드해줄 누군가를 계속 찾느라 내 에너지가 고갈되어가고 있는 것 같았습니다. 사실 로보틱스 + AI를 하는 직무 자체가 신기술이고 빛나보여서 멋져보일지는 몰라도 실제 현장과 산업에서 프로젝트를 성공시킨 사례, 성공시켜본 경험이 있는 사람은 많지 않은 것 같습니다. 그래서 내가 보고 배울 수 있는 표본을 찾기 보다는 결국 내가 처음 찾아서 만들어가야 하는구나를 깨달았던 것 같습니다. 그래서 갑자기 뜬금 없지만 최애 밴드인 The Score의 Don’t need a hero 라는 노래가 이 교훈을 잘 담고 있는 노래라는 생각이 드네요."
  },
  {
    "objectID": "posts/note/2025-01-05-Goodbye-2024.html#월-7",
    "href": "posts/note/2025-01-05-Goodbye-2024.html#월-7",
    "title": "📘Goodbye 2024",
    "section": "8월",
    "text": "8월\n두번째 회사에 출근을 시작했습니다. 분위기와 사람들이 첫회사와 정말 정반대였습니다. 우선 거리부터 판교로 거의 2시간 출근 시간을 확보해야 한다는 것도 이전 회사가 40분 거리였다는 점이라 정말 달랐습니다. 마치 밸런스 게임의 정반대 선택지만 다 고른 것처럼 분위기, 사수, 일하는 방식.. 등등 완전 새로웠던 것 같습니다. 두번째 회사는 지금까지 잘 다니고 있습니다 :)"
  },
  {
    "objectID": "posts/note/2025-01-05-Goodbye-2024.html#월-8",
    "href": "posts/note/2025-01-05-Goodbye-2024.html#월-8",
    "title": "📘Goodbye 2024",
    "section": "9월",
    "text": "9월\n9월에는 점점 다가오는 시연날과 함께 현장도 바쁘게 오가며 열심히 배우고 열심히 일했던 것 같습니다. 첫회사에서도 그랬지만 입사하고 거의 3개월 내에 항상 시연해야하는 일이 있었습니다. 그래도 팀원들과 사수의 좋은 팀워크로 일하는 동안 외롭지는 않았었습니다."
  },
  {
    "objectID": "posts/note/2025-01-05-Goodbye-2024.html#월-9",
    "href": "posts/note/2025-01-05-Goodbye-2024.html#월-9",
    "title": "📘Goodbye 2024",
    "section": "10월",
    "text": "10월\n10월.. 잊지 못할 시연이 있었습니다. 시연 전날 새벽까지도 제가 맡은 부분이 잘 작동되지 않아 완전히 멘탈이 나갔었지만 팀장님과 팀원들의 도움과 끝까지 놓을 수 없었던(?) 멘탈을 붙잡아 결국 성공을 시켰습니다. 지금도 그 시연날을 생각하면 아찔하긴 하지만 결과적으로 좋은 결과를 냈기에 지금은 웃으면서 회고를 적을 수 있습니다. 원래 10월은 제 생일있는 달이기에 가장 좋아하는 달인데 24년도 10월은 시연 때문에 순간 최악의 달이 될 뻔 했습니다😅\n\n\n\n생일 말고도 또 한가지 경사가 있는 10월이었습니다! 석사 과정을 시작하면서 시작했던 4족 보행 로봇관련 연구자료들을 모은 awesome list repository가 512개 star를 달성했습니다! 사실 512개까지 모을 수 있을지 모르고 이전에 포스팅으로 자축을 다 한 상태였지만 이후로도 스타가 많이 늘어서 너무 기뻤던 하루였습니다.(현재 회고를 작성하고 있는 순간에는 벌써 606개의 스타가..!)"
  },
  {
    "objectID": "posts/note/2025-01-05-Goodbye-2024.html#월-10",
    "href": "posts/note/2025-01-05-Goodbye-2024.html#월-10",
    "title": "📘Goodbye 2024",
    "section": "11월",
    "text": "11월\n11월은 조금 한숨 쉬어가는 한달이었는데 지금까지 일과 공부에만 전념했던 생활에 조금 tweek을 준 기간이었습니다. 인간관계에 공부를 하는 시간을 가졌었는데 공개되는 회고에 적기에는 조금 민감할 수 있어서 이 부분은 개인적인 다이어리에만 쓰려고 합니다.\n직무적으로는 조금씩 자기효용감이 생기기 시작했던 것 같습니다. 두번째 회사에서의 적응도 어느정도 된 상태에서 내가 어떤 역할을 할 수 있는지, 그리고 회사에서 어떤 점을 바라고 있는지 align을 하면서 내가 성장할 수 있겠다라는 확신이 생겼었습니다. 특히나 회사에서 얻기 쉽지 않은 기회인 Facebook Meta와 협업하는 프로젝트에 내가 involve 할 수 있다는 사실에 감사하며 내가 할 수 있는 부분이 무엇인지 더 능동적인 태도를 가질 수 있게 되어서 좋았던 것 같습니다."
  },
  {
    "objectID": "posts/note/2025-01-05-Goodbye-2024.html#월-11",
    "href": "posts/note/2025-01-05-Goodbye-2024.html#월-11",
    "title": "📘Goodbye 2024",
    "section": "12월",
    "text": "12월\n연말이 다가오는 12월에 크리스마스 분위기를 한껏 느끼며 시간을 보냈었습니다. 첫번째 회사도 5개월, 두번째 회사도 5개월째 되는 달이라서 스스로 각 회사에서 느낀점이 어떤 부분에서 다른지 짚어보았습니다. 그 과정에서 내가 어떤 부분에서 취약하다고 느끼는지 파악해볼 수 있었습니다.\n12월에 갑자기 친구들의 추진력 덕분에 필리핀 여행을 다녀왔습니다. 딱 24년도 말일까지 필리핀에 있는 여행일정이었습니다.\n\n\n\n처음으로 동남아로 가족들이 아닌 친구들끼리 가는 여행이라 긴장도, 걱정도 많이했었지만 엑티비티도 많이하고 추억들을 많이 쌓을 수 있어서 정말 재밌었던 여행이었습니다. 여행을 하면서 내가 막연히 가지고 있는 걱정이 생각보다 많고 경험했을때 깨달을 수 있는 부분들이 많다는 걸 다시한번 짚어볼 수 있었습니다."
  },
  {
    "objectID": "posts/note/2025-01-05-Goodbye-2024.html#마무리",
    "href": "posts/note/2025-01-05-Goodbye-2024.html#마무리",
    "title": "📘Goodbye 2024",
    "section": "마무리",
    "text": "마무리\n한때 회고를 남기는 것에 대해 회의적일 때가 있었습니다. 그런데 기억한다는 점은 다음 단계로 나아가는 첫걸음이라는 생각이 들고 나서 다시 회고를 하고 있습니다. 글의 서두에도 적었듯이 2024년도가 정말 많은 일들이 있던 한해였기에 전체 인생에서도 기억에 남을 한해가 될 것 같습니다. 다가오는 2025년도에 가끔씩 24년도 회고를 보러올 것 같은데 그때에는 조금이라도 성장해있는 또 다른 내가 되어있길 바라며 회고를 이만 맺겠습니다."
  },
  {
    "objectID": "posts/note/2022-10-24-daily-english-004.html",
    "href": "posts/note/2022-10-24-daily-english-004.html",
    "title": "🌎IT English Experssions 004",
    "section": "",
    "text": "영어 회화 표현이나 논문에서 쓸 수 있는 유용한 표현들을 조금씩 익히기 위해 시작한 1000일 시리즈 포스트 입니다. 공부 리소스는 유튜브, 리얼클래스, PN님의 블로그, cake 어플, 책, 논문, 기사 등 다양한 출처에서 보고 기록합니다."
  },
  {
    "objectID": "posts/note/2022-10-24-daily-english-004.html#git으로-세련되게-협업하는-방법",
    "href": "posts/note/2022-10-24-daily-english-004.html#git으로-세련되게-협업하는-방법",
    "title": "🌎IT English Experssions 004",
    "section": "Git으로 세련되게 협업하는 방법",
    "text": "Git으로 세련되게 협업하는 방법\n\nConventional Commits\n\n커밋 메세지는 Subject (Title), Body, Footer로 구분\n구조\n\n&lt;type&gt;[optional scope]:&lt;description&gt;\n[optional body]\n[optional footer(s)]\n\n예시(http://www.conventionalcommits.org/)\n\nfix: prevent racing of requests                           ---&gt; Subject or Title\nIntroduce a request id and a reference to latest request. ---&gt; Body\nReviewed-by : Z                                           ---&gt; Footer(s)\nRefs : #123\n\n\nSubject(Title) 작성법\n커밋 메세지의 제목은 변경사항을 대표하는 텍스트이므로 대표적인 타입들이 있다.\n\nfeat: 코드에 새로운 기능(=feature) 추가\nfix: 버그 수정\nBREAKING CHANGE: 이전 버전과 호환되지 않는 변경 내역. !으로 표시할 수 있음. e.g.) feat!:\ndocs: 개발 문서 변경\nstyle: 들여쓰기, 따옴표, 세미콜론 등 코드 형식 및 스타일 변경\nci: CI/CD(continueout integration and deployment) 관련 코드 변경\nrefactor: 중복된 코드 제거, 변수명 변경, 코드 단순화 등 리팩터링\ntest: 테스트 관련 코드 변경\nbuild: 빌드 시스템 관련 코드 변경\nperf: 성능 개선 관련 코드 변경\nchore: 기타 코드 변경\n\n예시\n\nfix: remove deprecated features 수정: 권장되지 않는 기능 삭제\nfeat: add parameters to getImage 기능: getImage에 매개변수 추가\ndocs(readme): update build instructions 문서(readme): 빌드 지침 업데이트\nchore: update np dependencies to latest version 기타: npm 의존성 최신버전으로 업테이트\n\n나만의 예시 연습\n\ndocs(readme): add new papers \"THE TITLE OF PAPER\"\nfeat: add getJointPosition function\nrefactor: delete overlapped constants and variables of robot model\n\n\n\n5가지 커밋 작성법\n\n동사 원형으로 시작\n\n\n제목은 명령적 어조(Imperative Mood)의 동사원형으로 시작\nBody, Footer는 명령문이 아니어도 됨\n상황에 따라 과거형(e.g. Added) 또는 3인칭 단수 현재형(e.g. Adds)를 사용하기도 함\n커밋 메세지에 자주 등장하는 동사\n\nFix: 수정하다\nImprove: 개선하다\nHandle: 처리하다\nOptimize: 최적화하다\nUpdate: 업데이트하다\nImplement: 구현하다, 적용하다\nRefactor: 리펙터링하다\nAdd: 추가하다\nRevert: 되돌리다\nChange: 변경하다\nReplace: 대체하다\nMerge: 병합하다\nDocument: 문서를 작성하다\nBump: 버전을 올리다\nSimplify: 단순화시키다\nEnable: 가능하게 하다\nRun: 실행하다\nClean: 제거하다, 정리하다\nWrap: 감싸다, 그룹화하다\nDeploy: 배포하다\nModify: 변경하다\nRemove: 제거하다\nRename: 이름을 바꾸다\nMove: 이동하다, 이동시키다\n\n\n\n모두 소문자로 또는 첫 글자만 대문자로\n\n\nConventional Commits 형식에서는 모든 문자를 소문자로 작성. type: description에 맞추어 메세지 작성\nConventional Commits 형식을 적용하지 않는 경우, 일반적으로 앞부분만 대문자 사용하고 type 생략\n예시\n\n[1] docs: update build.md with detailed instructions\n[2] Update build.md with detailed instructions\n\n관사(a, an, the와 같은 Article) 생략\n\n\n커밋 타이틀은 최대 50글자 제한\n핵심 키워드만 활용하여 메세지를 작성하기 위해 관사 생략\n예시\n\n[X] Fix a typo in the header\n[O] Fix typo in header\n\n마침표와 같은 구두점(Punctuation Mark) 생략\n\n\n반드시 필요한 경우가 아니면 쉼표, 하이픈 등 생략\n예시\n\n[X] feat: implement google analytics.\n[O] feat: implement google analytics\n\n변경한 이유, 상세한 설명은 본문(Body)에\n\n\n코드 변경 사유와 상세 설명은 커밋 본문에 씀\n\n\n\nGit 주요 실무 영어\n\nSquash the last 3 commits: 최근 3개 커밋을 합치다\nPush commits to a repository: 리포지터리(코드 저장소)로 커밋을 전달하다\nMerge a feature branch into the base branch: 기능 브랜치를 기본 브랜치에 병합하다\nRevert a pull request: 풀 리퀘스트를 되돌리다(이전 상태로 되돌리다)\nRequest a review: 검토를 요청하다\nComment on a pull request: 풀 리퀘스트에 댓글을 남기다\nResolve a merge conflict: 병합 충돌을 해결하다\nRebase onto another branch: 다른 브랜치로 리베이스(base를 재설정하여 커밋 재적용)하다\nClone a repository: 리포지터리를 복제하다\nClose a pull request without merging it onto the branch: 풀 리퀘스트를 병합하지 않고 종료하다"
  },
  {
    "objectID": "posts/note/2021-01-03-Goodbye-2020.html",
    "href": "posts/note/2021-01-03-Goodbye-2020.html",
    "title": "📘Goodbye 2020",
    "section": "",
    "text": "구글 캘린더의 도움을 받아 조금은 늦은 2020 회고록을 적어봤다. 기억이 희미하고 상기하면서 왜곡된 추억을 회상할 수도 있겠지만, 2020을 보내고 2021을 맞이하기에 충분한 시간을 가지는 건 이 시간이 아니면 할 수 없기에 소중하게 생각하며 한 글자 한 글자 적어봤다. 월마다 1~2개의 이야기를 쓰게 될 것 같으니 한 해를 12개 정도의 이야기로 잘 풀어가봐야 겠다."
  },
  {
    "objectID": "posts/note/2021-01-03-Goodbye-2020.html#월---2월",
    "href": "posts/note/2021-01-03-Goodbye-2020.html#월---2월",
    "title": "📘Goodbye 2020",
    "section": "1월 - 2월",
    "text": "1월 - 2월\n독서실 알바를 했었다. 벌써 1년전 일이라서 잊고 있었던 일이었는데..2020년도 일이었다니 새삼 놀랍다. 아빠의 해외 출장에 마음이 헛헛해지기도 하고, 방학 때 쉽게 쳐질 수 있는 생활 패턴도 잡고, 돈도 벌고자 찾은 아르바이트였다. 그동안 대치동 학원 알바도 해보고 동네 수학학원 보조교사도 해봤었는데 이번에는 처음으로 대면(?)알바가 아니었다. 청소담당을 지원했었는데 당시 실장님 면접 때 여자인 내가 와서 고개를 갸우뚱하셨던게 기억에 남는다. 나름 아침형 인간인지라 사람이 오기전 6~8시 사이에 독서실 모든 공간을 청소기 돌리고 물걸레질을 부지런히 했었다.(부모님이 제일 반대하던 알바였는데.. 생각해보면 은근 말 안듣는 딸인 듯)\n그때 청소를 하면서 독서실 위에 놓여진 공시, 어학 책들을 보며, (삼수생이었던 못난 면모를 아직 벗어던지지 못해서 그런지 겨울만 되면 센치해지는 감성때문일수도) 열심히 공부하는 청춘들의 시간이 왜 안타까워보이기만 했다. 오지랖일순 있지만.. 취업시장이 좁아지고, 스펙과 자격증만 바라보게 되고, 좁은 독서실 자리로 향해야한다는 현실이 너무 답답했다. 각설하고 그때 공짜로 받은 자리에서 공부했던 걸 생각해보면 Udacity RL 코스를 열심히 했던 것 같다. TOEFL도 준비하겠다고 옆에 책을 쌓아두긴 했었지만 제대로 공부 안한건 2021에 그대로 업보로 받아 이어지고 있다.(으이구🤪)\n\n오랜만에 가족들과 속초여행을 갔었다. 당시에 코로나가 조금씩 심해지고 있었는데 그땐 “여름까지 코로나가 계속되면 안되는데..”라고 걱정하고 있었다. 왠걸..2020 한 해를 온전히 코로나랑 함께할 줄은 몰랐다.😥 친구들과도 갔었던 속초였지만 가족들과 함께했던 속초는 또 다른 모습으로 즐길 수 있어서 좋았다. 타임랩스로 바다위로 떠오르는 태양을 찍었던 기억이 생생하다. 물회는 맛있었고, 겨울바다의 소리는 정말 맑고 시원했다.\n\n사실 2019 겨울부터해서 2월까지 KPMG Ideation이라는 대회에 멋진 분들과 함께 준비하고 있었다. 다른 대회보다 달랐던 점은 정말 나에게는 도전 그 자체였기에 더 기억에 남는다. 그 동안 기술적인 성격의 대회들은 대부분 숫자로 표현된 성적으로 판단하는게 대부분이었는데 이 대회는 아이디어 제안 성격을 가지고 있었기에 엔지니어 마인드보다 기획자 마인드를 배우게 되었던 것 같다. 또한 잘 몰랐던 NLP 분야에 대해 공부할 수 있는 기회였고, 특허라는 분야, 변리사라는 직업에 대한 이야기 등 새로운 세상 이야기 좋아하는 나에게는 진짜 재밌었던 시간이었다. 잊지못할 해프닝이 있던 대회이기도 했는데, 본선 대회 당일 대회장에 도착하자 마자 집으로 돌아가라고 통보(?) 받았던 대회였기도 했다. 혹시라도 밝히고 싶지 않으실 수도 있으니까 최대한 말을 아끼고 내 마음속에 저장하겠지만, 정말 감사했던 점은 함께 해주셨던 팀원분들이 정말 다 멋진 분들이었다는 것이다. 밤을 새워가며 함께 나누었던 이야기들이나 생각들 모두 너무 좋았고 평생 남을만한 따뜻한 추억이 생겼다는게 행복했다."
  },
  {
    "objectID": "posts/note/2021-01-03-Goodbye-2020.html#월---6월",
    "href": "posts/note/2021-01-03-Goodbye-2020.html#월---6월",
    "title": "📘Goodbye 2020",
    "section": "3월 - 6월",
    "text": "3월 - 6월\n본격적으로 코로나로 인한 대혼란이 시작됐다. 상황이 좋아질 기미가 보이지 않고 개강이 미뤄졌다. 지금은 익숙하지만 처음접하게 된 온라인 수업은 정말 당황 그 자체였다. 등하교 3시간 통학러인 나에게는 시간을 세이브할 수 있는 장점도 있긴 했지만 집에서 하루종일 노트북만 보고 있으면서 “이게 뭐하는 건가..”싶은 생각이 든게 한 두번이 아니었다. 진짜 대학의 역할에 대해 진지하게 고민도 했었다. 그래도 바쁘게 1학기를 잘 보냈고 랩실 생활도 열심히 했던 것 같다. 아 생각해보니 이때부터 나그네처럼 랩실을 다녔던 생활을 마치고 정식으로 학부연구생으로 인정받아 돈을 받으면서 연구하게 되었다.\n\n종강을 하고 미래연구소 14기 서브튜터를 하게되었다. 내가 처음으로 딥러닝을 공부하게 된 곳에서 서브튜터로 일하게 되었다는 게 정말 신기하고 감사했다. 대학교 가자마자 우연히 보게된 글을 보고 (지금 생각해보면 겁도 없이 혼자 찾아간게 신기하지만) 미래연구소 1기로 딥러닝을 공부했다. 오랜만에 랩장님도 보고 몰라보게 커진 미래연구소 모습을 보면서 괜시리 뿌듯하기도 했다. 사실 메인튜터님이 많이 배려해주시기도 하시고 서브튜터 업무 자체는 크게 부담스럽지는 않았지만 돈을 받고 일하는 자리는 항상 긴장하게 되기 때문에 조금 스트레스를 받았던 것 같긴하다. 14기 분들 중 완전 입문자를 위한 파이썬 기초 스터디는 따로 혼자 운영해야 했기 때문에 좀 더 긴장했던 것 같기도 하다. 그래도 항상 DL 공부 관련해서 INPUT만 했던 입장에서 처음으로 OUTPUT을 하게되는 도전적인 경험이었고, 지식적인 면으로나 태도적인 면으로나 성장할 수 있었던 큰 도약점이 됬었다. 부족한 서브튜터를 만났지만 열심히 공부하셨던 14기 분들이 모두 성공하셔서 나중에 커뮤니티에서 만나뵈면 정말 행복할 것 같다."
  },
  {
    "objectID": "posts/note/2021-01-03-Goodbye-2020.html#월---8월",
    "href": "posts/note/2021-01-03-Goodbye-2020.html#월---8월",
    "title": "📘Goodbye 2020",
    "section": "7월 - 8월",
    "text": "7월 - 8월\n여름방학에 역시 많은 일들이 있었다. 우선 정말 다이나믹했던 2020 국제창작자동차대회 PostNomad팀으로 참가한 이야기.🦈 4학년 분들의 졸업프로젝트였지만 운좋게도 참여할 수 있는 기회가 주어져서 제어팀으로 합류할 수 있었다. 후에는 딥러닝 테스크 비중이 높은 비젼팀으로 옮겨가야만 했지만. 그동안은 학교내에서 팀을 꾸려서 대회에 나가기보다는 대외 스터디나 모임에서 만난 분들과 프로젝트들을 했었기 때문에 또 다른 느낌이었다. 대형학과이다 보니 사실상 같은 학과여도 서로 잘 모르기 때문에 졸업할 때까지 모르는 동기도 많다. 평소에 아싸생활을 하는 나로써는 더더욱 학과 사람들이랑 친해질 기회가 없었는데 처음으로 기계과 사람들과 함께할 수 있었던 시간이 아니었나 싶다. 지금 회고하는 이 시점에서는 대회 결과도 아쉽고😢 준비하는 과정에서 랩뷰만 써야하는 답답함, 협업하면서 느꼈던 어려움과 실망했던 점들도 많지만, 진짜 소중한 시간들이었다. 지하 작업실에서 회의하고, 함께 공구들을 나르고, 덥고 습한 K-City에서 다같이 고생하고 고민했던 모든 시간들이 감사하다. 이 회고록을 빌려 고백을 하자면.. 우리 팀에게 조금 더 잘하지 못한 게 죄송하다. 다들 열심히 하시고 항상 나를 배려해주셨던 것 같은데 그에 비해 나는 잘 따라가는 팀원은 아니었던 것 같다.\n\n랩실 연구를 비롯해서 TOEFL 공부까지 하느라 정말 열심히 살았는데, 사실 이때 제대로 공부하지 않아서 후에 고생하게 된 건 안비밀이다. 항상 느끼는 것이긴 한데 나는 한가지 일을 집중해서 끝내는 능력이 아직도 부족한 것 같다. 이에 더하여 2020 Korea Health Datathon에 참가하여 부비동 데이터셋으로 최종 4위를 했었다. NSML 플랫폼은 할말하않이긴 하지만 꾸준히 데이터톤 경험을 갖게 해주신 것만으로도 감사하다. 2019 대회에도 참여했었는데 그때보다 발전된 성적을 가질 수 있어서 좋았다.\n\n산티아고 순례길을 걷고 싶다는 버킷리스트가 생겼다. 그래서 하나씩 뭘 준비해야 하나 고민하는 중에 체력을 길러야 겠다는 생각을 했다. 이에 더해 코로나로 떨어진 활동성을 보충하고자 등하교를 따릉이로 하기 시작했다. 가는데에만 2시간 걸리는 여정이었지만 한강을 따라 가는 길이 나쁘지 않았기 때문에 충분히 좋은 도전이었다. 처음에는 다들 미친 짓(?)이라고 만류했었고 나도 반신반의 했었지만 막상 해보니 죽을 정도는 아니었고 자전거 타고 보는 한강은 다리아픈 것 따위 다 잊게 만들정도로 예뻤다. 아침은 아침대로, 저녁은 저녁대로, 맑으면 맑은대로, 흐리면 흐린대로. 때로는 상수나들목 공사때문에 당황하기도 했지만 길은 항상 찾으면 되는 거였다."
  },
  {
    "objectID": "posts/note/2021-01-03-Goodbye-2020.html#월---10월",
    "href": "posts/note/2021-01-03-Goodbye-2020.html#월---10월",
    "title": "📘Goodbye 2020",
    "section": "9월 - 10월",
    "text": "9월 - 10월\n2학기도 여전히 코로나로 인해 온라인으로 진행됐다. 학기중에는 거의 수업에 집중하다보니 딱히 회고록에 적을 내용이 없는 것 같아서 한빛미디어에서 “나는 리뷰어다”로 참여했던 경험을 적어볼까 한다. 리뷰어로 활동을 하면서 2개의 도전이 있었다. 첫째는 글쓰는 것 자체에 대한 도전이었는데, 예전에는(그러니까 고2때 까지만 하더라도) 글을 쓰는데 어려움이 없었던 것 같다. 그리고 나름 글을 잘 쓴다고 인정도 받았던 것 같은데 이후에 딱히 글을 쓸 기회가 없었고 쓰지 않다보니 리뷰를 쓴다는 것 자체가 어색했다. 그리고 “독후감”과는 다른 목적이 있는 “리뷰”라는 글에 대한 고민이 있었다. 출판사에서 책을 주면서 나에게 리뷰를 쓰기를 원하는 니즈는 분명 있는 것이고, 다른 사람들에게 책의 내용이 잘 어필이 되길 바라는 것일터였다. 그러면 단순히 책의 장단점을 나 혼자 판단하고 즐기고 끝나는 것이 아닌 다른 사람들에게 잘 전달될 수 있도록 표현해야 한다는 것이었다. 두번째로는 빠르게 기술서를 봐야한다는 도전이었다. 사실 IT전공도 아니고 원래는 1개의 책도 적어도 2~3개월을 봐야하는 거북이 속도인데 리뷰를 하려면 1달에 1권을 무조건 다 보고 리뷰까지 완성해야 했다. 몇개의 책들은 사실 다 보지도 못하고 리뷰 적기에 급급했던 것도 사실이다. 제대로 리뷰어로 활동하지 못해 관계자분들께 죄송하다. 그래도 몇몇 리뷰는 제대로 적었다는 것에 스스로 조금 위안을 삼아본다."
  },
  {
    "objectID": "posts/note/2021-01-03-Goodbye-2020.html#월",
    "href": "posts/note/2021-01-03-Goodbye-2020.html#월",
    "title": "📘Goodbye 2020",
    "section": "11월",
    "text": "11월\n지금까지 인생의 경험들 중에 최악과 최고를 다 뽑으라고 하면 2020.11월에 다 있다. 좋은 것부터 먼저. 우선 최고는 내 인생 처음으로 학회에서 내가 한 연구를 가지고 발표를 완성도 있게 마무리 할 수 있었고 인정도 받아서 우수논문상까지 받게 된 경험이다. 진짜 학회 발표 전 리허설하는 랩미팅에서 울면서 나가기도 했었고 발표 전날까지도 결과가 잘 나오지 않아 정말 힘들었다. “힘들었다”라는 4글자로 밖에 표현 못한다는 게 억울할 정도로 최고의 스트레스를 받은 시간이었다. 그래도 진짜 주변에 천사들을 심어 놓으신 것인지 기적적으로 도움도 받고 몇일 밤을 새서 어찌저찌 마무리 할 수 있었고 IPNT에서 구두발표도 잘 마무리하여 우수논문상도 받게되었다.(지금생각해도 기적이라고 말할 수 밖에 없다.) 진짜 힘들었던 만큼 최고의 성취감은 말로 할 수 없었다.\n다음으로 최악의 경험은 사실 최고의 경험과 관련이 깊다. 앞서 적은 “최고의 스트레스”가 복선이었다. 학회를 마치고 체력이 바닥으로 떨어질대로 떨어졌고 긴장은 완전히 풀어진 상태에서 몸이 엄청 아팠다. 감을 먹고 체한 탓도 있었지만 몸이 정말 아팠고 힘이 빠지면서 “이대로 죽을 수도 있겠구나..” 싶은 생각이 들었다. 인생 처음으로 자다가 새벽에 구급차를 불러 응급실에 갔던 게 최악의 경험이지 않나 싶다. 근데 유감스럽게도 구급대원분들이 오시면서 급속도로 괜찮아져서 정말 난처하고 민망했다. 나중에 새벽 4시쯤 엄마랑 응급실을 나와서 걸어서 집으로 돌아갔다. 이때의 일은 가족들에게도 큰 충격을 주어서 지금도 아빠는 잊을만 하면 이야기를 하시는데 하지말라고 장난스럽게 말을 하면서도 죄송한 마음이 들기도 한다. 웃프긴 하지만 이 일 이후로 내가 제일 좋아하는 “감”은 금지어가 되었다."
  },
  {
    "objectID": "posts/note/2021-01-03-Goodbye-2020.html#월-1",
    "href": "posts/note/2021-01-03-Goodbye-2020.html#월-1",
    "title": "📘Goodbye 2020",
    "section": "12월",
    "text": "12월\n사랑하는 양가의 조부모님들이 몇년전만 해도 다 살아계셨다. 하지만 근 2년 정도 매년 겨울에 사랑하는 분들을 떠나보내게 되었다. 외할아버지, 친할아버지, 그리고 외할머니까지. 올해 후반에 건강이 급속도로 나빠지신 외할머니가 결국 우리곁을 떠나셨다. 4분의 할머니 할아버지 가운데 가장 사랑의 표현도 아끼시지 않고 항상 전화도 먼저 걸어주셨던 멋진 할머니였다. 지금 이렇게 “할머니 사랑해요”라고 말하던게 이렇게 그리워할 것이었다면 살아계셨을때 왜 그렇게 어색해하고 표현하기 부끄러워 했는지. 코로나로 인해 좋았던 점 하나는 장례식에 손님들 없이 식구들끼리 할머니를 추억하면서 얼마나 멋진 분이셨는지 되새길수 있어서 좋았다. 진짜 멋진 분이셨다."
  },
  {
    "objectID": "posts/note/2021-01-03-Goodbye-2020.html#bnm2h",
    "href": "posts/note/2021-01-03-Goodbye-2020.html#bnm2h",
    "title": "📘Goodbye 2020",
    "section": "BNM2h",
    "text": "BNM2h\n2020년도에 감사했던 많은 일들이 있었지만, 지금 나름 뿌듯하고 보람찬 시간들을 회고할 수 있도록 도와준 많은 분들이 있었다. 스터디를 통해서 만나는 인연들이 대부분이다. 물론 많은 스터디에 참여해보고 도전 받을 수 있는 좋은 시간들이었다. 그래도 가장 애정가는 스터디는 아무래도 BNM2h가 아닐까 싶다. 그렇다. 사실 가장 애정가는 이유 중 하나는 아무래도 내가 만든 스터디였기에 가장 책임을 느꼈고 가장 스트레스도 많이 받고 가장 노력했음을 나 스스로도 느꼈기 때문일 것이다.\n처음에는 진짜 가벼운 마음으로 시작했다. 따지고 보면 2019년도에 Kaggle KR에서 스터디 리더를 뽑는다는 글을 보고 마침 캐글에서 Connect-X라는 강화학습 대회가 베타수준으로 시도하기 시작했던 때라 “내가 한번 캐글 강화학습 스터디 리더가 되어보자!”라는 생각과 패기로 시작했던 거였다. 패기는 패기였던 걸로.. 그렇게 시작했으나 함께할 팀원분들이 모집되지 않아 그냥 한 순간의 불꽃으로 끝날 뻔 했다. 다행이도 다른 리더분들 중에 나와 같은 처지였던 분이 계셨었고 감사하게도 같이 공부하자 먼저 손을 내밀어 주셔서 BNM2h라는 스터디가 생기게 되었다!🙌\n그때 그때 마다 인연이 되는대로 지금까지 이어져오고 있는 스터디에서 강화학습을 공부하고 있다. 최고의 스터디라고 자랑할 순 없지만 최애 스터디라고는 할 수 있다. 아직도 강화학습이라는 분야에 대해, 스터디에서 함께 공부하는 방식에 대해, 스터디 매니징하는 것에 대해, 감사함과 겸손함을 표현하는 것에 대해 한참 모자른 애송이이지만 매주 스터디에 나와주셔서 나에게 성장할 기회를 주고 그런 시간들을 함께 보내주시는 BNM2h 스터디원분들은 천사들이신 것 같다. 나도 그런 분들에게 조금이나마 도움이 되고 함께 성장하기 위해 조금 더 노력하는 사람이 되어야겠다고 느낀 한 해였다."
  },
  {
    "objectID": "posts/note/2021-01-03-Goodbye-2020.html#마무리",
    "href": "posts/note/2021-01-03-Goodbye-2020.html#마무리",
    "title": "📘Goodbye 2020",
    "section": "마무리",
    "text": "마무리\n물론 당연히도 여기에 적지 못한 많은 이야기들이 있다. 이야기들은 열정을 담기도 하고, 아쉬움을 담기도 하고, 기쁨을 담기도 하고, 슬픔을 담기도 한다. 그 이야기들은 지금의 나의 마음이나 기억 속 어딘가에 잘 살아있겠지.\n멋있는 개발자분들의 회고록 같은 것을 기대했으나 적고나서 읽어보니 아직은 어디에 내놓기 부끄러운 새벽감성의 회고록이니 그냥 조용히 블로그에 남기기로 생각했다.😂\n잘가요 2020"
  },
  {
    "objectID": "posts/note/2025-03-30-gueltto-10th-end.html",
    "href": "posts/note/2025-03-30-gueltto-10th-end.html",
    "title": "📘글또를 마치며",
    "section": "",
    "text": "글또 활동 마무리 회고글을 작성하는 날이 왔네요.\n2022년 5월, 석사 1학기를 마무리하던 즈음, “글을 쓰는 것이 나를 성장시킬 것이다”라는 믿음 하나로 글또를 시작했습니다. 그리고 어느덧 시간이 흘러 2025년 3월, 사회 초년생이 된 지금, 그 긴 여정의 끝자락에 서 있습니다.\n사실 7기를 시작할 때만 해도, 그 이후의 기수까지 계속 참여하게 될 거라고는 상상하지 못했습니다. 하지만 매 순간, 크고 작은 고민과 방황 속에서 나를 붙잡아줄 무언가가 필요했고, 그럴 때마다 글또가 제게 하나의 버팀목이 되어주었습니다. 그리고 그렇게 이어지다 보니, 어느덧 관성이 습관이 되었고, 습관은 지금의 제가 되었네요.\n세상이란 주관성과 객관성이 공존하는, 나를 둘러싼 환경이라고 생각합니다. 그리고 그 중에서도, 좀 더 주관적으로 해석할 수 있는 세상, 즉 나라는 세상의 무대를 글로써 담아내며, 저만의 방식으로 나의 세상을 바꿔왔습니다. 그런 의미에서 저는 분명히, 제 세상을 바꿔온 사람입니다.\n이제, 그 세상을 바꿔온 제 여정을 회고하며 되돌아보려 합니다."
  },
  {
    "objectID": "posts/note/2025-03-30-gueltto-10th-end.html#그리고-잇기",
    "href": "posts/note/2025-03-30-gueltto-10th-end.html#그리고-잇기",
    "title": "📘글또를 마치며",
    "section": "그리고, 잇기",
    "text": "그리고, 잇기\n마지막으로, 지금까지 글또에서 항상 시작할 때와 마무리할 때 느꼈던 마음들, 그리고 중간중간, 스스로를 되돌아봐야 했던 시기에 적어두었던 작은 일기들을 한데 모아 정리해보았습니다. 그 순간의 감정과 생각들이 고스란히 담긴 기록들입니다. 때로는 방황했고, 때로는 나아갔으며, 언제나 그 안엔 조금씩 성장해온 제가 있었습니다.\n\n\n\n\n📘Geultto 7th Start\n📘2022 상반기 회고\n📘Gueltto 7th End\n📘Geultto 8th Start\n📘Geultto 8th End\n📘Geultto 9th Start\n📘Geultto 10th Start\n📘글또 발표 후기"
  },
  {
    "objectID": "posts/note/2022-10-11-daily-english-002.html",
    "href": "posts/note/2022-10-11-daily-english-002.html",
    "title": "🌎Casual English Phrases 002",
    "section": "",
    "text": "영어 회화 표현이나 논문에서 쓸 수 있는 유용한 표현들을 조금씩 익히기 위해 시작한 1000일 시리즈 포스트 입니다. 공부 리소스는 유튜브, 리얼클래스, PN님의 블로그, cake 어플, 책, 논문, 기사 등 다양한 출처에서 보고 기록합니다.\n\n\n속마음을 보여주다\nwear your heart on your sleeve\n\nIf you wear your heart on your sleeve, you openly show your feelings or emotions rather than keeping them hidden. 속마음을 보여준다는 말은, 너의 감정이나 느낌들을 숨기기보다 보여준다는 거야.\nIt takes courage to wear your heart on your sleeve, which is why few people do it. 속마음을 털어놓는다는 건 용기가 필요한 일이기 때문에 많은 사람들이 하지 못하는 일이지.\nI need someone whom I can wear my heart on my sleeve. 내 속마음을 털어놓을 사람이 필요해.\n\n\n\n고지식하거나 융통성 없는 성격\n약간 모범생 이미지\nsquare\nIf you are a SQUARE: You are an organized, logical, and hardworking person who likes structure and rules.\n\nBe there, or be square. : The expression be there or be square means that if one declines to attend an event, one is considered “uncool.” It implies that the event will be exciting.\nDon’t be square. 고리타분하게 굴지마\nShe is such a square - I’ve never met anyone so boring. 그녀는 진짜 고지식한 사람이야 이때껏 그사람처럼 지루한 사람은 처음봤어.\n\n\n\n위선적인\ntwo-faced\n\nDon’t trust her - I suspect she’s a bit two-faced. 그녀를 믿지마. 그녀가 조금 위선적인 사람인 것 같아.\nHe’s a two-faced cheater. 그는 두얼굴을 사진 사기꾼이야.\nHe had been devious and two-faced. 그는 기만적인고 양면적이었다.\n\ndevious : 정직하지 못한, 기만적인"
  },
  {
    "objectID": "posts/note/2023-03-30-daily-english-006.html",
    "href": "posts/note/2023-03-30-daily-english-006.html",
    "title": "🌎Casual English Phrases 006",
    "section": "",
    "text": "불만족스러운, 불편한\nnot (too) happy\n\nStomach’s not too happy, is it? 배가 고프구나?(아프구나)\nMy stomach’s not happy with me. 배가 아파.(고파)\n\n\n가끔씩 대화를 하다가 맥락을 서로가 알고 있는 상황이면 a 나 your 등을 생략하는 경우가 있음.(Stomach)\n\n\n\n유감스럽게도\nI’m afraid ~\n\nI’m afraid I haven’t heard any cookies calling you, plain or otherwise. 유감이지만 그냥 쿠키든 아니든 널 부르는 쿠키는 없었어.\nI’m afraid your suggestion is not accepted. 유감이지만 네 제안이 받아들여지지 않았어.\nI’m afriad I can’t go to your party. 미안하지만 네 파티에 못 갈 것 같아.\n\n\n\n속이다\nput (someone) on\n\nHe must be wondering if I was just putting him on. 내가 자기를 속이고 있는 건지 궁금해하고 있을 게 뻔해.\nDon’t believe it! She’s putting you on! 믿지마! 그녀는 너를 속이고 있어.\n\n\n\n기다리다 / 견디다, 버티다\nhang on\n\nHang on a few more minutes, I’m looking. 조금만 더 기다려, 찾고 있으니까.\nHang on a moment. 네 잠시만 기다리세요.\nIf you can just hang on for a little longer, we’ll finish soon. 네가 조금만 더 버텨준다면, 우리는 곧 끝낼거야.\n\n\n\n~로 만들어지다\nbe made of\n\nWhat’s it made of, rotten zombie eyes or something? 대체 뭐로 만든거야, 썩은 좀비 눈 같은 건가?\nThis desk is made of baobab wood. 이 책상은 바오밥 나무로 만들어졌다.\n\n\nbe made from : 재료의 형태를 알아 볼수없는 경우. Cheese is made from milk.\n\n\n\n역겨운\nsick\n\nJust looking at it almost makes me sick. 그냥 보기만 해도 구역질 나와.\nHe makes me sick! 그가 하는 짓은 역겨워!\nJust thinking about it makes me sick to the stomach. 그거 생각하는 것만으로도 정말 역겨워."
  },
  {
    "objectID": "posts/note/2025-01-13-ros2-build-issue.html",
    "href": "posts/note/2025-01-13-ros2-build-issue.html",
    "title": "📝ROS2 Build Issue",
    "section": "",
    "text": "Issue\nError Message\nCMake Error at CMakeLists.txt:5 (find_package):\n  By not providing \"Findament_cmake.cmake\" in CMAKE_MODULE_PATH this project\n  has asked CMake to find a package configuration file provided by\n  \"ament_cmake\", but CMake did not find one.\n\n  Could not find a package configuration file provided by \"ament_cmake\" with\n  any of the following names:\n\n    ament_cmakeConfig.cmake\n    ament_cmake-config.cmake\n\n  Add the installation prefix of \"ament_cmake\" to CMAKE_PREFIX_PATH or set\n  \"ament_cmake_DIR\" to a directory containing one of the above files.  If\n  \"ament_cmake\" provides a separate development package or SDK, be sure it\n  has been installed.\n\n\nSolution\nThis error typically appears when CMake cannot locate ament_cmake. In ROS 2, ament_cmake is a core build system package that you must have installed (and sourced) before building any ament-based packages.\nEnsure the ament_cmake package is actually installed. On Ubuntu, for Humble, you’d expect something like:\nIf you have a minimal or custom ROS 2 installation, you might need to install additional packages to get ament_cmake.\nsudo apt-get install ros-humble-ament-cmake"
  },
  {
    "objectID": "posts/note/2020-10-21-on-and-off-policy.html",
    "href": "posts/note/2020-10-21-on-and-off-policy.html",
    "title": "📝On policy VS. Off policy",
    "section": "",
    "text": "강화학습에서 On-Policy와 Off-Policy는 에이전트가 데이터를 수집하고 학습하는 방식에 따라 구분되는 두 가지 접근 방식입니다. 이 문서에서는 두 방식의 차이점, 특징, 장단점 및 주요 개념들을 정리합니다.\n\n\n\n\n\n현재 학습 중인 정책(behavior policy)에서 생성된 데이터를 사용하여 같은 정책을 개선합니다. 즉, 학습에 사용하는 데이터와 정책이 동일합니다.\n\n\n\n\n에이전트는 현재의 정책을 따라 환경과 상호작용하며 데이터를 수집합니다.\n점진적으로 정책이 개선되며, 이전 정책과 크게 다르지 않습니다.\n데이터 효율성은 낮지만, 안정적이고 수렴 성능이 뛰어난 경우가 많습니다.\n\n\n\n\n\nPPO (Proximal Policy Optimization)\nA3C (Asynchronous Advantage Actor-Critic)\nTRPO (Trust Region Policy Optimization)\n\n\n\n\n\n장점:\n\n학습 안정성이 높음\n수렴 속도가 빠를 수 있음\n\n단점:\n\n새로운 데이터를 매번 수집해야 하므로 데이터 효율성이 낮고 계산 비용이 높음\n\n\n\n\n\n\n\n\n\n현재의 정책(target policy)을 학습하지만, 다른 정책(behavior policy)에서 생성된 데이터를 활용합니다. 즉, 학습에 사용하는 데이터와 정책이 다를 수 있습니다.\n\n\n\n\n과거의 경험(리플레이 버퍼 등)에 저장된 데이터를 재활용할 수 있습니다.\n정책이 바뀌어도 이전의 데이터를 계속 활용할 수 있어 데이터 효율성이 높습니다.\n행동 정책(behavior policy)이 반드시 학습 중인 정책(target policy)와 같을 필요는 없습니다.\n\n\n\n\n\nDQN (Deep Q-Network)\nDDPG (Deep Deterministic Policy Gradient)\nSAC (Soft Actor-Critic)\nTD3 (Twin Delayed Deep Deterministic Policy Gradient)\n\n\n\n\n\n장점:\n\n데이터를 재사용할 수 있어 학습이 효율적\n다양한 정책으로 생성된 데이터를 활용 가능\n\n단점:\n\n학습이 불안정할 수 있음\n복잡한 알고리즘 설계가 필요할 수 있음\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n특징\nOn-Policy\nOff-Policy\n\n\n\n\n데이터 수집 방식\n현재 학습 중인 정책에서 데이터를 생성\n이전에 수집한 데이터도 재활용 가능\n\n\n정책과 데이터 관계\n동일한 정책으로 생성된 데이터만 사용\n서로 다른 정책으로 생성된 데이터도 사용\n\n\n데이터 효율성\n낮음\n높음\n\n\n학습 안정성\n상대적으로 안정적\n불안정할 수 있음\n\n\n대표 알고리즘\nPPO, A3C, TRPO\nDQN, SAC, DDPG, TD3\n\n\n\n\n\n\n\n\n\n\n현재 정책(예: 행동 정책)만을 기반으로 데이터를 수집합니다.\n데이터 수집 후 즉시 정책 업데이트에 활용되며, 과거 데이터를 재사용하지 않습니다.\n\n\n\n\n\n목표 정책(예: 학습 중인 정책)과 다른 행동 정책에서 생성된 데이터를 활용합니다.\n과거 데이터를 재활용하기 위해 경험 재생(experience replay)을 자주 사용합니다.\nOff-policy 학습자들은 부트스트래핑(현재 Q 값을 추정하기 위해 다음 상태/동작의 Q 값을 사용하는 방식)을 사용하기 때문에 불안정할 수 있으며, 경험 재생으로 이를 보완합니다.\n\n\n\n\n\n탐험 상수 ε이 0으로 설정되면 Off-policy 방식이 On-policy 방식처럼 작동할 수 있습니다. (예: SARSA vs Q-러닝)\n하지만 ε-탐욕적 정책이 반드시 모든 환경에서 효율적이지는 않습니다.\n\n\n\n\n\n\n\nOn-policy: 현재 정책으로 데이터를 수집하고 학습하며, 데이터 효율성이 낮지만 안정적입니다.\nOff-policy: 과거 데이터를 재활용하며 데이터 효율성이 높지만, 학습이 불안정할 수 있습니다.\n경험 재생: Off-policy 방식에서 더 흔히 사용되며, 특히 함수 근사기와 함께 안정성을 높이는 데 기여합니다.\n\n\n\n\n\nSARSA와 Q-러닝의 비교에서 On-policy와 Off-policy의 차이를 이해할 수 있습니다.\n\nSARSA: On-policy 방식으로 ε-탐욕적 정책을 사용합니다.\nQ-러닝: Off-policy 방식으로 최대화 정책(maximising policy)을 사용합니다."
  },
  {
    "objectID": "posts/note/2020-10-21-on-and-off-policy.html#on-policy",
    "href": "posts/note/2020-10-21-on-and-off-policy.html#on-policy",
    "title": "📝On policy VS. Off policy",
    "section": "",
    "text": "현재 학습 중인 정책(behavior policy)에서 생성된 데이터를 사용하여 같은 정책을 개선합니다. 즉, 학습에 사용하는 데이터와 정책이 동일합니다.\n\n\n\n\n에이전트는 현재의 정책을 따라 환경과 상호작용하며 데이터를 수집합니다.\n점진적으로 정책이 개선되며, 이전 정책과 크게 다르지 않습니다.\n데이터 효율성은 낮지만, 안정적이고 수렴 성능이 뛰어난 경우가 많습니다.\n\n\n\n\n\nPPO (Proximal Policy Optimization)\nA3C (Asynchronous Advantage Actor-Critic)\nTRPO (Trust Region Policy Optimization)\n\n\n\n\n\n장점:\n\n학습 안정성이 높음\n수렴 속도가 빠를 수 있음\n\n단점:\n\n새로운 데이터를 매번 수집해야 하므로 데이터 효율성이 낮고 계산 비용이 높음"
  },
  {
    "objectID": "posts/note/2020-10-21-on-and-off-policy.html#off-policy",
    "href": "posts/note/2020-10-21-on-and-off-policy.html#off-policy",
    "title": "📝On policy VS. Off policy",
    "section": "",
    "text": "현재의 정책(target policy)을 학습하지만, 다른 정책(behavior policy)에서 생성된 데이터를 활용합니다. 즉, 학습에 사용하는 데이터와 정책이 다를 수 있습니다.\n\n\n\n\n과거의 경험(리플레이 버퍼 등)에 저장된 데이터를 재활용할 수 있습니다.\n정책이 바뀌어도 이전의 데이터를 계속 활용할 수 있어 데이터 효율성이 높습니다.\n행동 정책(behavior policy)이 반드시 학습 중인 정책(target policy)와 같을 필요는 없습니다.\n\n\n\n\n\nDQN (Deep Q-Network)\nDDPG (Deep Deterministic Policy Gradient)\nSAC (Soft Actor-Critic)\nTD3 (Twin Delayed Deep Deterministic Policy Gradient)\n\n\n\n\n\n장점:\n\n데이터를 재사용할 수 있어 학습이 효율적\n다양한 정책으로 생성된 데이터를 활용 가능\n\n단점:\n\n학습이 불안정할 수 있음\n복잡한 알고리즘 설계가 필요할 수 있음"
  },
  {
    "objectID": "posts/note/2020-10-21-on-and-off-policy.html#주요-차이점",
    "href": "posts/note/2020-10-21-on-and-off-policy.html#주요-차이점",
    "title": "📝On policy VS. Off policy",
    "section": "",
    "text": "특징\nOn-Policy\nOff-Policy\n\n\n\n\n데이터 수집 방식\n현재 학습 중인 정책에서 데이터를 생성\n이전에 수집한 데이터도 재활용 가능\n\n\n정책과 데이터 관계\n동일한 정책으로 생성된 데이터만 사용\n서로 다른 정책으로 생성된 데이터도 사용\n\n\n데이터 효율성\n낮음\n높음\n\n\n학습 안정성\n상대적으로 안정적\n불안정할 수 있음\n\n\n대표 알고리즘\nPPO, A3C, TRPO\nDQN, SAC, DDPG, TD3"
  },
  {
    "objectID": "posts/note/2020-10-21-on-and-off-policy.html#추가-설명",
    "href": "posts/note/2020-10-21-on-and-off-policy.html#추가-설명",
    "title": "📝On policy VS. Off policy",
    "section": "",
    "text": "현재 정책(예: 행동 정책)만을 기반으로 데이터를 수집합니다.\n데이터 수집 후 즉시 정책 업데이트에 활용되며, 과거 데이터를 재사용하지 않습니다.\n\n\n\n\n\n목표 정책(예: 학습 중인 정책)과 다른 행동 정책에서 생성된 데이터를 활용합니다.\n과거 데이터를 재활용하기 위해 경험 재생(experience replay)을 자주 사용합니다.\nOff-policy 학습자들은 부트스트래핑(현재 Q 값을 추정하기 위해 다음 상태/동작의 Q 값을 사용하는 방식)을 사용하기 때문에 불안정할 수 있으며, 경험 재생으로 이를 보완합니다.\n\n\n\n\n\n탐험 상수 ε이 0으로 설정되면 Off-policy 방식이 On-policy 방식처럼 작동할 수 있습니다. (예: SARSA vs Q-러닝)\n하지만 ε-탐욕적 정책이 반드시 모든 환경에서 효율적이지는 않습니다."
  },
  {
    "objectID": "posts/note/2020-10-21-on-and-off-policy.html#요약",
    "href": "posts/note/2020-10-21-on-and-off-policy.html#요약",
    "title": "📝On policy VS. Off policy",
    "section": "",
    "text": "On-policy: 현재 정책으로 데이터를 수집하고 학습하며, 데이터 효율성이 낮지만 안정적입니다.\nOff-policy: 과거 데이터를 재활용하며 데이터 효율성이 높지만, 학습이 불안정할 수 있습니다.\n경험 재생: Off-policy 방식에서 더 흔히 사용되며, 특히 함수 근사기와 함께 안정성을 높이는 데 기여합니다.\n\n\n\n\n\nSARSA와 Q-러닝의 비교에서 On-policy와 Off-policy의 차이를 이해할 수 있습니다.\n\nSARSA: On-policy 방식으로 ε-탐욕적 정책을 사용합니다.\nQ-러닝: Off-policy 방식으로 최대화 정책(maximising policy)을 사용합니다."
  },
  {
    "objectID": "asset/2025-02-16-hydra-handson.html",
    "href": "asset/2025-02-16-hydra-handson.html",
    "title": "Hydra 소개",
    "section": "",
    "text": "ML/DL 실험에서는 다양한 실험 파라미터들을 관리해야 합니다. 이를 위해 여러가지 방법이 있지만, 이번 포스팅에서는 FacebookResearch에서 만든 Hydra를 사용해보자 합니다."
  },
  {
    "objectID": "asset/2025-02-16-hydra-handson.html#config-초기화",
    "href": "asset/2025-02-16-hydra-handson.html#config-초기화",
    "title": "Hydra 소개",
    "section": "Config 초기화",
    "text": "Config 초기화\nHydra에서 설정(Config) 파일을 초기화하는 방법에는 여러 가지가 있습니다. 공식 문서 Initialization methods에서 확인할 수 있듯이, 총 세 가지 방법이 있으며, 각각의 방법을 예제를 통해 살펴보겠습니다.\nHydra의 설정 초기화 방법\n\ninitialize(): 호출하는 코드의 상대 경로를 기준으로 설정 파일을 초기화합니다.\n\ninitialize_config_module(): 절대 경로를 사용하여 설정 모듈(config_module)을 기반으로 초기화합니다.\n\ninitialize_config_dir(): 파일 시스템의 절대 경로를 사용하여 설정 디렉터리(config_dir)를 기반으로 초기화합니다.\n\n이 세 가지 방법은 (1)함수 호출 방식과 (2)컨텍스트(context) 방식으로 사용할 수 있습니다.\n\n함수 호출 방식으로 사용하면 Hydra를 전역적(global)으로 초기화하며, 한 번만 호출해야 합니다.\n반면, 컨텍스트 방식으로 사용하면 특정 블록 내에서만 Hydra를 초기화할 수 있으며, 여러 번 사용할 수도 있습니다.\n\n\n방법1 initialize()\nHydra를 초기화하고 config_path를 설정 검색 경로에 추가합니다.\nconfig_path는 호출하는 코드의 parent 디렉터리를 기준으로 한 상대 경로이며, 이 경우에는 현재 노트북이 위치한 디렉터리를 기준으로 설정됩니다.\n\n%cd /content/\n\nwith initialize(version_base=None, config_path=\"configs\"):\n    # Compose the configuration by selecting the main configuration.\n    cfg_1 = compose(config_name=\"main\")\n\n# full configuration을 YAML 형식으로 출력하여 쉽게 검사할 수 있습니다.\nprint(OmegaConf.to_yaml(cfg_1))\n\n\n\n방법2 initialize_config_module()\nHydra를 초기화하고 config_module을 설정 검색 경로에 추가합니다.\nconfig_module은 반드시 import 가능한 형태여야 하며, 최상위 디렉터리에 __init__.py 파일이 존재해야 합니다. 이번 예제에서는 module이라는 폴더를 만들어서 import 가능한 디렉토리로 만들어 줍니다. 그리고 첫번째 만들었던 main.yaml파일을 복사하여 module/main_2.yaml 파일로 만들어 줍니다.\n\n%cd /content/configs\n%mkdir -p /content/configs/module\n!touch /content/configs/module/__init__.py\n%cd /content\n!cp ./configs/main.yaml configs/module/main_2.yaml\n\n잘 복사가 되었는지 확인해보겠습니다.\n\n!cat /content/configs/module/main_2.yaml\n\n이번에는 2번째 방법인 initialize_config_module()함수를 이용하여 복사했던 main_2.yaml 파일을 이용하여 compose 해보겠습니다.\n\n%cd /content\nwith initialize_config_module(version_base=None, config_module=\"configs.module\"):\n    cfg_2 = compose(config_name=\"main_2\")\n    print(cfg_2)\n\n아래와 같이 config의 키들을 확인해볼 수도 있습니다.\n\nlist(cfg_2.keys())\n\n\n\n방법3 initialize_config_dir()\nHydra를 초기화하고 config_path를 설정 검색 경로에 추가합니다. config_path는 파일 시스템 상의 절대 경로여야 합니다. 미리 만들어 놓았던 main.yaml 파일을 이용하여 config를 구성해보겠습니다.\n\nCONFIG_DIR = \"/content/configs\"\n\nwith initialize_config_dir(version_base=None, config_dir=CONFIG_DIR):\n    cfg_3 = compose(config_name=\"main\")\n\n\nprint(cfg_3)\n\n여기까지 Hydra의 세 가지 방법을 사용하여 설정(Config)을 초기화하는 방법을 살펴보았습니다. 이제 이렇게 생성된 설정이 어떻게 구성되어 있는지 확인해보겠습니다. 대표적인 설정 예제로 cfg_1을 살펴보겠습니다.\n객체의 타입을 확인해보면, 이는 Omegaconf에서 제공하는 DictConfig 객체임을 알 수 있습니다. DictConfig는 딕셔너리 형태의 설정을 계층적으로 관리할 수 있도록 해주는 데이터 구조로, YAML 설정 파일을 로드하거나 동적으로 구성 값을 변경할 때 유용하게 활용됩니다.\n\ntype(cfg_1)\n\nConfig 객체의 키들은 다음과 같이 확인할 수 있습니다.\n\nlist(cfg_1.keys())\n\nDictConfig의 키는 config_name.key_name 또는 config_name[\"key_name\"] 형태로 접근할 수 있습니다. 이를 통해 일반적인 딕셔너리처럼 키를 참조하거나 점 표기법(dot notation)을 사용하여 계층적인 설정 값을 쉽게 조회할 수 있습니다.\n\nprint(cfg_1.env == cfg_1[\"env\"])\n\n하위 키들도 동일한 방식으로 접근할 수 있으며, config_name.key_name.sub_key_name 또는 config_name[\"key_name\"][\"sub_key_name\"] 형태로 호출할 수 있습니다. 이를 활용하면 계층적으로 구성된 설정에서 원하는 값을 직관적으로 조회할 수 있습니다.\n\nprint(cfg_1.env.name == cfg_1[\"env\"][\"name\"])"
  },
  {
    "objectID": "asset/2025-02-16-hydra-handson.html#override",
    "href": "asset/2025-02-16-hydra-handson.html#override",
    "title": "Hydra 소개",
    "section": "Override",
    "text": "Override\n이번에는 초기화로 만든 config를 override하는 예제를 살펴보겠습니다. 강화학습에서 여러개의 environment를 병렬로 학습하기 위해 env 하위에 num_envs config를 1000개로 추가하는 override를 진행해보겠습니다.\n\n%cd /content/\n\nwith initialize(version_base=None, config_path=\"configs\"):\n    # 기존 설정을 유지하면서 새로운 설정을 추가\n    cfg = compose(config_name=\"main\", overrides=[\"+env.num_envs=1000\"])\n\n# 기존 설정과 오버라이드된 설정이 함께 출력됨\nprint(OmegaConf.to_yaml(cfg))\n\noverride는 기존의 main.yaml 파일의 내용을 변경하지 않고 항목을 추가할 수 있습니다. 다시한번 main.yaml 내용을 확인해보면 num_envs가 없음을 알 수 있습니다.\n\n!cat /content/configs/main.yaml\n\n하지만 override된 cfg의 키에는 env.num_envs가 있습니다.\n\ncfg.env.num_envs"
  },
  {
    "objectID": "asset/2025-02-16-hydra-handson.html#resolver",
    "href": "asset/2025-02-16-hydra-handson.html#resolver",
    "title": "Hydra 소개",
    "section": "Resolver",
    "text": "Resolver\nHydra는 Omegaconf의 Resolver 기능을 활용할 수 있습니다. OmegaConf.register_new_resolver()를 사용하여 커스텀 resolver를 등록하면, 새로운 interpolation 타입을 추가할 수 있으며, 설정(Config) 노드가 접근될 때 해당 resolver가 호출됩니다.\n\nResolver 등록 및 기능\n\neq: 두 문자열을 소문자로 변환한 후, 동일한지 비교합니다.\n\ncontains: 첫 번째 문자열이 두 번째 문자열에 포함되어 있는지 검사합니다.\n\nif: 주어진 조건에 따라 두 값 중 하나를 선택합니다.\n\nresolve_default: 인자가 빈 문자열이면 기본값을 사용하고, 그렇지 않으면 인자 값을 반환합니다.\n\n이러한 resolver를 활용하면 설정 파일 내에서 조건부 로직, 문자열 비교, 기본값 처리 등을 동적으로 적용할 수 있습니다.\n예제로 아래와 같이 Resolver를 등록해보겠습니다.\n\n# Hydra 설정에서 사용할 Resolver들을 등록합니다.\nOmegaConf.register_new_resolver(\"eq\", lambda x, y: x.lower() == y.lower())\nOmegaConf.register_new_resolver(\"contains\", lambda x, y: x.lower() in y.lower())\nOmegaConf.register_new_resolver(\"if\", lambda pred, a, b: a if pred else b)\nOmegaConf.register_new_resolver(\"resolve_default\", lambda default, arg: default if arg == \"\" else arg)\n\n이번 예제에서 사용할 Config는 위의 예제 Config에서 Resolver를 확인하기 위해 아래 내용을 더 추가하여 구성해보겠습니다.\n\nyaml_config = \"\"\"\nenv:\n  name: \"CartPole-v1\"\n  seed: 42\n\nagent:\n  type: \"DQN\"\n  hidden_layers: [64, 64]\n  activation: \"relu\"\n  learning_rate: 0.001 # experiment의 default_lr 기준값으로 설정\n  gamma: 0.99\n\ntraining:\n  episodes: 1000\n  batch_size: ${if:${experiment.is_test}, 128, 32} # 테스트 모드일 경우 128, 아니면 32\n  replay_buffer_size: 10000\n\n# Resolver를 활용한 동적 설정\nexperiment:\n  mode: \"test\"\n  is_test: ${eq:${experiment.mode}, \"test\"}\n  default_lr: ${resolve_default:0.001, ${agent.learning_rate}}\n  is_debug: ${contains:${experiment.mode}, \"debug\"}  # \"debug\" 포함 여부 확인\n\"\"\"\n\nconfig를 로드하고 Resolver가 적용된 값을 출력합니다.\n\ncfg = OmegaConf.create(yaml_config)\n\n# Resolver가 적용된 값 출력\nprint(\"실험 모드:\", cfg.experiment.mode)\nprint(\"is_test:\", cfg.experiment.is_test)\nprint(\"배치 크기:\", cfg.training.batch_size)\nprint(\"default_lr:\", cfg.experiment.default_lr)\nprint(\"디버그 모드 여부):\", cfg.experiment.is_debug)\n\n예제 출력을 하나씩 살펴보겠습니다.\n\n${eq:${experiment.mode}, \"test\"} → experiment.mode가 \"test\"이면 True, 아니면 False\n\neq(x, y) Resolver는 두 값을 비교하여 같으면 True, 다르면 False를 반환합니다. ${experiment.mode} 값이 \"test\"인지 확인하는 역할을 합니다.\n\nexperiment:\n    mode: \"test\"\n    is_test: ${eq:${experiment.mode}, \"test\"}\n\n위 설정에서 experiment.mode 값이 \"test\"로 설정되어 있기 때문에, ${eq:${experiment.mode}, \"test\"}는 \"test\"와 \"test\"를 비교하는 형태가 됩니다. eq 함수는 대소문자를 구분하지 않고 두 문자열이 같은지 확인하는 역할을 하므로, \"test\"는 \"test\"와 일치하여 True를 반환합니다. 따라서 experiment.is_test의 값은 True로 설정됩니다.\n반면, 만약 experiment.mode 값이 \"train\"이었다면, ${eq:${experiment.mode}, \"test\"}는 \"train\"과 \"test\"를 비교하게 됩니다. 이 두 값은 서로 다르므로 eq 함수는 False를 반환하게 되고, 결과적으로 experiment.is_test 값은 False로 설정됩니다. 이를 통해 설정 값에 따라 특정 변수를 자동으로 조정할 수 있으며, 이를 활용하면 실험 모드에 따라 설정을 다르게 적용할 수 있습니다.\n\n${if:${experiment.is_test}, 128, 32} → experiment.is_test가 True이면 128, 아니면 32\n\nif(condition, true_value, false_value) Resolver는 condition이 True일 때 true_value를, False일 때 false_value를 반환합니다. experiment.is_test 값이 True인지 확인하여, 이에 따라 다른 값을 할당합니다.\n\ntraining:\n    batch_size: ${if:${experiment.is_test}, 128, 32} # 테스트 모드일 경우 128, 아니면 32\n\n${if:${experiment.is_test}, 128, 32} 구문을 살펴보면, if 함수는 첫 번째 인자로 주어진 조건이 True일 경우 두 번째 인자인 128을 반환하고, False일 경우 세 번째 인자인 32를 반환하는 역할을 합니다. 현재 experiment.is_test가 True이므로 if(True, 128, 32)는 128을 반환하고, 결과적으로 training.batch_size 값이 128이 됩니다.\n반면, 만약 experiment.mode가 \"train\" 등 다른 값으로 설정되어 있다면, eq(\"train\", \"test\")의 결과는 False가 되어 experiment.is_test가 False로 설정됩니다. 이 경우, if(False, 128, 32)는 False에 해당하는 세 번째 값인 32를 반환하게 되며, training.batch_size 값이 32로 설정됩니다.\n이러한 방식은 training과 test에서 배치 크기를 다르게 설정할 때 유용합니다. 예를 들어, 테스트 환경에서는 더 큰 배치 크기를 사용하여 빠르게 결과를 확인하고, 훈련 환경에서는 적절한 배치 크기를 유지하여 안정적인 학습이 가능하도록 조정할 수 있습니다. 이를 통해 설정 파일을 동적으로 관리할 수 있으며, 실험 조건에 따라 유연하게 설정을 변경할 수 있습니다.\n\n${resolve_default:0.001, ${agent.learning_rate}} → agent.learning_rate가 설정되지 않았으면 기본값 0.001 사용\n\nresolve_default(default, arg) Resolver는 arg 값이 비어 있거나 설정되지 않았을 경우 default 값을 반환합니다.\nagent.learning_rate 값이 존재하면 그대로 사용하고, 없다면 기본값 0.001을 사용합니다.\n\nagent:\n    learning_rate: 0.001 # experiment의 default_lr 기준값으로 설정\nexperiment:\n    default_lr: ${resolve_default:0.001, ${agent.learning_rate}}\n\n이 방식은 설정 파일에서 특정 값이 누락되었을 때 기본값을 자동으로 적용하는 데 매우 유용합니다. 예를 들어, learning_rate 값을 실험마다 다르게 설정할 수 있도록 설정 파일에서 값을 명시적으로 지정할 수도 있지만, 실수로 빠뜨렸을 경우에도 resolve_default를 사용하면 안전하게 기본값을 사용할 수 있습니다. 이를 통해 설정을 더욱 견고하게 만들고, 코드의 예외 처리를 간결하게 할 수 있습니다.\n\n${contains:${experiment.mode}, \"debug\"} → experiment.mode에 “debug”라는 글자가 포함되어 있는지 여부 확인\nexperiment:\nmode: \"test\"\nis_debug: ${contains:${experiment.mode}, \"debug\"}  # \"debug\" 포함 여부 확인\n\nexperiment.mode 값이 \"test\"라면 \"debug\"가 포함되지 않았으므로 contains(\"test\", \"debug\")는 False를 반환합니다.\n만약 experiment.mode 값이 \"test_debug\"라면 \"debug\"라는 문자열이 포함되어 있으므로 contains(\"test_debug\", \"debug\")는 True를 반환합니다.\n이를 통해 실험 모드에 따라 자동으로 디버깅 기능을 활성화하거나 로그 출력을 조정할 수 있습니다.\n\n\n이러한 Resolver 기능을 활용하면 설정 파일을 더욱 동적으로 관리할 수 있습니다!"
  },
  {
    "objectID": "post.html",
    "href": "post.html",
    "title": "Post",
    "section": "",
    "text": "📃 Paper Review | 🧩 Storage | 👩‍💻 Code\n\n\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n    \n  \n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\n\n\n\n\n\nJul 3, 2025\n\n\n📃Physics Informed RL Survey 리뷰\n\n\n\n\n\n\nJul 2, 2025\n\n\n📃Offline RL Survey 리뷰\n\n\n\n\n\n\nJun 13, 2025\n\n\n📃Batch Online RL 리뷰\n\n\n\n\n\n\nJun 13, 2025\n\n\n📃Contact Trust Region 리뷰(feat.Dextreme)\n\n\n\n\n\n\nJun 11, 2025\n\n\n📃Dex Imitation Learning 리뷰\n\n\n\n\n\n\nJun 9, 2025\n\n\n📃GeoRT 리뷰\n\n\n\n\n\n\nJun 9, 2025\n\n\n👩‍💻ROS2 JointState vs. JointTrajectory\n\n\n\n\n\n\nJun 9, 2025\n\n\n👩‍💻ROS2 joint_state_publiser 디폴트 동작 제어하기\n\n\n\n\n\n\nJun 4, 2025\n\n\n📃Neural feels with neural fields 리뷰\n\n\n\n\n\n\nJun 3, 2025\n\n\n📃Digit360 리뷰\n\n\n\n\n\n\nJun 2, 2025\n\n\n📃Sparsh 리뷰\n\n\n\n\n\n\nMay 29, 2025\n\n\n🧩uSkin vs ReSkin\n\n\n\n\n\n\nMay 5, 2025\n\n\n📃Reskin-Anyskin 리뷰\n\n\n\n\n\n\nMar 14, 2025\n\n\n👩‍💻IsaacSim과 IsaacLab 알아보기\n\n\n\n\n\n\nFeb 16, 2025\n\n\n👩‍💻Hydra로 실험관리 하기\n\n\n\n\n\n\nFeb 2, 2025\n\n\n👩‍💻ROS2 RealSense C++ Node\n\n\n\n\n\n\nDec 22, 2024\n\n\n📃Rotating without Seeing 리뷰\n\n\n\n\n\n\nNov 10, 2024\n\n\n📃IPO 리뷰\n\n\n\n\n\n\nMar 17, 2024\n\n\n📃VCGS 리뷰\n\n\n\n\n\n\nJan 21, 2024\n\n\n👩‍💻Quarto Blog + α\n\n\n\n\n\n\nJan 7, 2024\n\n\n👩‍💻Quarto Blog 기본 셋팅\n\n\n\n\n\n\nJul 2, 2023\n\n\n📃DreamWaQ 리뷰\n\n\n\n\n\n\nJun 18, 2023\n\n\n👩‍💻Chord Graph\n\n\n\n\n\n\nMay 7, 2023\n\n\n📃K-Accessibility 리뷰\n\n\n\n\n\n\nApr 23, 2023\n\n\n👩‍💻Orbit Existing Scripts\n\n\n\n\n\n\nApr 4, 2023\n\n\n👩‍💻Orbit 설치하기\n\n\n\n\n\n\nMar 12, 2023\n\n\n📃WASABI 리뷰\n\n\n\n\n\n\nDec 26, 2022\n\n\n👩‍💻class ⟷ dict ⟷ yaml\n\n\n\n\n\n\nDec 21, 2022\n\n\n👩‍💻torch.Tensor vs torch.tensor\n\n\n\n\n\n\nDec 14, 2022\n\n\n👩‍💻Linux GPU 상태 확인하기\n\n\n\n\n\n\nOct 16, 2022\n\n\n📃Robust Recovery Controller 리뷰\n\n\n\n\n\n\nOct 2, 2022\n\n\n📃VAE 리뷰\n\n\n\n\n\n\nSep 17, 2022\n\n\n📃WaveNet 리뷰\n\n\n\n\n\n\nAug 7, 2022\n\n\n📃GN-Block 리뷰\n\n\n\n\n\n\nJun 26, 2022\n\n\n📃Legged Robots that Keep on Learning 리뷰\n\n\n\n\n\n\nJun 10, 2022\n\n\n📃NerveNet 리뷰\n\n\n\n\n\n\nJan 2, 2021\n\n\n🧩GNN Materials\n\n\n\n\n\n\nJul 20, 2020\n\n\n👩‍💻Import custom module\n\n\n\n\n\n\nJul 17, 2020\n\n\n🧩My Reinforcement Learning Roadmap\n\n\n\n\n\n\nJul 13, 2020\n\n\n👩‍💻Install Mujoco in Windows10\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Curieux.JY",
    "section": "",
    "text": "Hello👋 I’m Jung Yeon. Thanks for visiting my blog.\n\n\nThis is where I document everything I explore with curiosity.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n📃Physics Informed RL Survey 리뷰\n\n\n\nrl\n\nphysics-informed\n\nsurvey\n\n\n\nA Survey on Physics Informed Reinforcement Learning - Review and Open Problems\n\n\n\n\n\nJul 3, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃Offline RL Survey 리뷰\n\n\n\nrl\n\noffline-rl\n\nsurvey\n\n\n\nA Survey on Offline Reinforcement Learning - Taxonomy, Review, and Open Problems\n\n\n\n\n\nJul 2, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃Batch Online RL 리뷰\n\n\n\nbatch-online-rl\n\nrl\n\n\n\nWhat Matters for Batch Online Reinforcement Learning in Robotics?\n\n\n\n\n\nJun 13, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃Contact Trust Region 리뷰(feat.Dextreme)\n\n\n\nmpc\n\nrl\n\ndexterous\n\ncontact\n\ntrust-region\n\n\n\nDexterous Contact-Rich Manipulation via the Contact Trust Region\n\n\n\n\n\nJun 13, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃Dex Imitation Learning 리뷰\n\n\n\npaper\n\nimitation\n\ndexterous\n\n\n\nDexterous Manipulation through Imitation Learning / A Survey\n\n\n\n\n\nJun 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n👩‍💻ROS2 JointState vs. JointTrajectory\n\n\n\nros2\n\njoint\n\ncode\n\n\n\nROS2 메세지 JointState와 JointTrajectory 비교\n\n\n\n\n\nJun 9, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n👩‍💻ROS2 joint_state_publiser 디폴트 동작 제어하기\n\n\n\nros2\n\njoint\n\ncode\n\n\n\nROS2 메세지 JointState와 JointTrajectory 비교\n\n\n\n\n\nJun 9, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃GeoRT 리뷰\n\n\n\npaper\n\nteleoperation\n\nretargeting\n\n\n\nGeometric Retargeting - A Principled, Ultrafast Neural Hand Retargeting Algorithm\n\n\n\n\n\nJun 9, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃Neural feels with neural fields 리뷰\n\n\n\npaper\n\ntactile\n\nsdf\n\n\n\nVisuo-tactile perception for in-hand manipulation\n\n\n\n\n\nJun 4, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃Digit360 리뷰\n\n\n\npaper\n\ntactile\n\n\n\nDigitizing Touch with an Artificial Multimodal Fingertip\n\n\n\n\n\nJun 3, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃Sparsh 리뷰\n\n\n\npaper\n\ntactile\n\nsdf\n\n\n\nSelf-supervised touch representations for vision-based tactile sensing\n\n\n\n\n\nJun 2, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n🧩uSkin vs ReSkin\n\n\n\ntactile\n\nsensor\n\nmagneto\n\n\n\nComparison of\n\n\n\n\n\nMay 29, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃Reskin-Anyskin 리뷰\n\n\n\npaper\n\ntactile\n\n\n\n교체가능한 촉각센서\n\n\n\n\n\nMay 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n👩‍💻IsaacSim과 IsaacLab 알아보기\n\n\n\nisaacsim\n\nisaaclab\n\ncode\n\n\n\nIsaacSim 4.5.0 & IsaacLab\n\n\n\n\n\nMar 14, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n👩‍💻Hydra로 실험관리 하기\n\n\n\nhydra\n\ncode\n\n\n\n여러 실험 파라미터들을 관리해주는 Hydra\n\n\n\n\n\nFeb 16, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n👩‍💻ROS2 RealSense C++ Node\n\n\n\nros2\n\nrealsense\n\ncode\n\n\n\nC++로 ROS2 RealSense 카메라 노드 만들기\n\n\n\n\n\nFeb 2, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃Rotating without Seeing 리뷰\n\n\n\npaper\n\ntactile\n\nrl\n\nhand\n\n\n\nTowards In-hand Dexterity through Touch\n\n\n\n\n\nDec 22, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n📃IPO 리뷰\n\n\n\npaper\n\nrl\n\ncmdp\n\n\n\nInterior-point Policy Optimization under Constraints\n\n\n\n\n\nNov 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n📃VCGS 리뷰\n\n\n\ngrasp\n\npointcloud\n\nvae\n\npaper\n\n\n\nVariational Constrained Grasp Sample\n\n\n\n\n\nMar 17, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n👩‍💻Quarto Blog + α\n\n\n\nblog\n\nquarto\n\ncode\n\n\n\nQuarto로 속편한 Github Blog 구축하기(2)\n\n\n\n\n\nJan 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n👩‍💻Quarto Blog 기본 셋팅\n\n\n\nblog\n\nquarto\n\ncode\n\n\n\nQuarto로 속편한 Github Blog 구축하기(1)\n\n\n\n\n\nJan 7, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n📃DreamWaQ 리뷰\n\n\n\ncontext\n\nrl\n\npaper\n\n\n\nLearning Robust Quadrupedal Locomotion With Implicit Terrain Imagination via Deep Reinforcement Learning\n\n\n\n\n\nJul 2, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n👩‍💻Chord Graph\n\n\n\nchord\n\nvisualization\n\ncode\n\n\n\nHoloViews를 이용하여 Chord Graph 그리기\n\n\n\n\n\nJun 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n📃K-Accessibility 리뷰\n\n\n\nrl\n\nclustering\n\nquadruped\n\nrecovery\n\nbackflip\n\npaper\n\n\n\nAccessibility-Based Clustering for Efficient Learning of Locomotion Skills\n\n\n\n\n\nMay 7, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n👩‍💻Orbit Existing Scripts\n\n\n\norbit\n\nisaacsim\n\ncode\n\n\n\nIsaac Orbit Series 002\n\n\n\n\n\nApr 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n👩‍💻Orbit 설치하기\n\n\n\norbit\n\nisaacsim\n\ncode\n\n\n\nIsaac Orbit Series 001\n\n\n\n\n\nApr 4, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n📃WASABI 리뷰\n\n\n\nrl\n\ngan\n\nquadruped\n\nbackflip\n\npaper\n\n\n\nLearning Agile Skills via Adversarial Imitation of Rough Partial Demonstrations\n\n\n\n\n\nMar 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n👩‍💻class ⟷ dict ⟷ yaml\n\n\n\nconfig\n\nyaml\n\ncode\n\n\n\n여러 파라미터들을 기록하기 위한 config 관리\n\n\n\n\n\nDec 26, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n👩‍💻torch.Tensor vs torch.tensor\n\n\n\ntorch\n\ntensor\n\ninverted\n\ncode\n\n\n\ntorch Tensor 객체 생성 방법 비교와 Inverted 연산 확인하기\n\n\n\n\n\nDec 21, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n👩‍💻Linux GPU 상태 확인하기\n\n\n\nlinux\n\ngpu\n\ncode\n\n\n\n리눅스에서 GPU 상태를 확인하는 여러가지 방법을 알아봅니다.\n\n\n\n\n\nDec 14, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n📃Robust Recovery Controller 리뷰\n\n\n\nrl\n\ntrpo\n\ngae\n\nrecovery\n\nquadruped\n\npaper\n\n\n\nRobust Recovery Controller for a Quadrupedal Robot using Deep Reinforcement Learning\n\n\n\n\n\nOct 16, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n📃VAE 리뷰\n\n\n\ngenerative\n\nvae\n\npaper\n\n\n\nAuto-Encoding Variational Bayes\n\n\n\n\n\nOct 2, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n📃WaveNet 리뷰\n\n\n\nautoregressive\n\ngenerative\n\npaper\n\n\n\nA Generative Model for Raw Audio\n\n\n\n\n\nSep 17, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n📃GN-Block 리뷰\n\n\n\ngnn\n\nsystem identification\n\nmpc\n\nrl\n\npaper\n\n\n\nGraph Networks as Learnable Physics Engines for Inference and Control\n\n\n\n\n\nAug 7, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n📃Legged Robots that Keep on Learning 리뷰\n\n\n\nquadruped\n\nrl\n\nredq\n\npaper\n\n\n\nFine-Tuning Locomotion Policies in the Real World\n\n\n\n\n\nJun 26, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n📃NerveNet 리뷰\n\n\n\ngnn\n\nrl\n\npaper\n\n\n\nLearning Structured Policy with Graph Neural Networks\n\n\n\n\n\nJun 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n🧩GNN Materials\n\n\n\ngnn\n\nstorage\n\n\n\nGNN 자료들 모음\n\n\n\n\n\nJan 2, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n👩‍💻Import custom module\n\n\n\npython\n\ncode\n\n\n\ncustom module을 불러오는 방법\n\n\n\n\n\nJul 20, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n🧩My Reinforcement Learning Roadmap\n\n\n\nrl\n\nstudy\n\nstorage\n\n\n\n내가 공부했던 강화학습 Roadmap\n\n\n\n\n\nJul 17, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n👩‍💻Install Mujoco in Windows10\n\n\n\nmujoco\n\ncode\n\n\n\nRL에서 많이 쓰이는 시뮬레이션 Mujoco Windows10에 설치하기\n\n\n\n\n\nJul 13, 2020\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Pick-GPT\n\n\n\nApr 12, 2024\n\n\n\n\n\n\n\n\n\n\nDeep Reinforcement Learning for Fall-Recovery Control on Non-Flat Terrain of Quadruped Robots\n\n\n\nDec 11, 2020\n\n\n\n\n\n\n\n\n\n\nSelf-driving Public Mobility Get-off Safety System\n\n\n\nNov 12, 2020\n\n\n\n\n\n\n\n\n\n\nDeep Reinforcement learning for DME Pulse Design\n\n\n\nNov 11, 2020\n\n\n\n\n\n\n\n\n\n\nActive Learning Algorithm for Object Detection and Segmentation\n\n\n\nNov 12, 2019\n\n\n\n\n\n\n\n\n\n\nSmart Device for Dog Triaining\n\n\n\nOct 12, 2019\n\n\n\n\n\n\n\n\n\n\nTraffic Sign Detection and Recognition Task\n\n\n\nOct 12, 2019\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/note/2025-06-16-linux-cheetsheet.html",
    "href": "posts/note/2025-06-16-linux-cheetsheet.html",
    "title": "📝Linx Cheet Sheet",
    "section": "",
    "text": "git command로 commit\n# If the branch already exists:\ngit checkout [branch-name]\n\n# Otherwise, create it and switch:\ngit checkout -b [branch-name]\ngit add .\ngit commit -m \"commit message\"\ngit push\n\ngit push -u origin [branch-name]\n\n\n다중 GPU에서 사용할 특정 GPU 설정\n\nexport CUDA_VISIBLE_DEVICES=&lt;idx&gt;[,&lt;idx&gt;...] 로 사용할 GPU만 노출\n다중 GPU 환경에서 “어떤 GPU를 쓸지” 지정하는 가장 간단한 방법은 환경 변수 CUDA_VISIBLE_DEVICES 를 설정. 이 변수에 지정한 인덱스(들)만이 애플리케이션에서 “GPU 0,1,2…” 로 인식되기 때문에, 실제 머신에 여러 개 장착된 GPU 중에서 원하는 것만 골라 쓸 수 있음\n\n터미널에서 아래 명령으로 현재 시스템에 붙어 있는 GPU 목록과 각 ID를 확인:\nnvidia-smi # 여기서 좌측의 `GPU` 컬럼이 실제 시스템 인덱스\n\n단일 GPU 사용\nexport CUDA_VISIBLE_DEVICES=1\npython train.py\n이렇게 하면 실제 시스템의 GPU#1 이 “내부적으로 0번 GPU” 로 보이게 됩니다.\n여러 GPU 사용\nexport CUDA_VISIBLE_DEVICES=0,2\npython train.py\n이렇게 설정하면 시스템 GPU 0번과 2번만 보이게 되고, 내부적으로는 [ “가상 GPU0” ← 시스템0, “가상 GPU1” ← 시스템2 ] 로 취급됩니다.\n\n\nTip: 쉼표 없이 CUDA_VISIBLE_DEVICES=0 또는 1, 공백 없이 0,2,3 처럼 지정해야 합니다.\n\n\n\n소스코드 찾기 grep\n\n파일 내에서 또는 입력값으로부터 특정 패턴을 검색\n\n\noption\n\n-v : 일치되는 내용이 없는 라인을 표시\n-c : 일치되는 내용이 있는 행의 개수를 표시\n-l : 일치되는 내용이 있는 파일 이름만 표시\n-h : 일치되는 내용을 찾은 파일의 이름을 표시하지 않음\n-n : 일치되는 내용이 있는 행은 행 번호와 함께 표시\n\n특정 문자열로 찾기: grep -r \"TEXT\" [PATH]\n\n예: grep -r \"TEXT\" ./*\n색 옵션: grep --color=auto \"TEXT\" ./*\n\n파일명만 보기: grep -l \"TEXT\" [PATH]\n특정 경로 및 파일명을 명시하여 탐색: find [PATH] -name \"FILENAME\" | xargs grep \"TEXT\"\n\nAdvanced\n\nripgrep\nfzf\n\n\n\nAppImage icon\nAppImage icon 생성\n\n.AppImage 파일 및 icon 이미지(.png) 경로: /opt\ndesktop 바로가기 설정 경로: /usr/share/applications\n\n\nappimage 파일과 icon image를 /opt로 이동\n\ne.g. /opt/[app_name].AppImage & /opt/[app_name].png\n\n바로가기 설정 파일 [app_name.desktop]을 /usr/share/applications에 만들어줌\n\n[Desktop Entry]\nName=[app_name]\nComment=[AppImage entry]\nExec=/opt/[app_name].AppImage\nIcon=/opt/[app_name].png\nType=Application\nTerminal=false\nEncoding=UTF-8\nCategories=Utility;\n\n\nconda 자동 활성화 해제\nconda config --set auto_activate_base false\n\n\nconda lib error\nImportError: libpython2.x.so.1.0: cannot open shared object file: No such file or directory\n\nSolution: export LD_LIBRARY_PATH=/home/avery/anaconda3/envs/[가상환경이름]/lib\n\n\n\nconda freezing\nconda env export &gt; environment.yaml\nconda env create --file environment.yaml\n\npip list --format=freeze &gt; ./requirements.txt\npip install -r requirements.txt\n\n\nsymbolic link 설정 및 관리\n\n\n심볼릭 링크는 파일 시스템 내에서 다른 파일이나 디렉토리의 경로를 가리키는 “바로 가기”와 같은 역할을 하므로, 실제 파일이 아닌 링크 파일임을 유의\n여러 버전의 라이브러리나 프로그램 관리 시, 심볼릭 링크가 어떤 타겟을 가리키고 있는지 확인하는 것이 중요\n여러 버전의 프로그램(예: python, cuda)을 설치했을 때 어떤 버전이 활성화되어 있는지 관리\n\n\n\n심볼릭 링크 삭제\nrm &lt;symbolic_link&gt;\n예시:\nrm /usr/local/bin/python\n심볼릭 링크 생성\nln -s &lt;target&gt; &lt;symbolic_link&gt;\n예시:\nln -s /opt/python3.9/bin/python /usr/local/bin/python\n심볼릭 링크 변경 (덮어쓰기) 기존 링크를 새 타겟으로 변경할 때 사용합니다.\nln -Tfs &lt;new_target&gt; &lt;symbolic_link&gt;\n예시:\nln -Tfs /opt/python3.10/bin/python /usr/local/bin/python\n심볼릭 링크 확인\nls -l &lt;symbolic_link&gt;\n예시:\nls -l /usr/local/bin/python\nlrwxrwxrwx 1 root root 24 2025-01-22 12:34 /usr/local/bin/python -&gt; /opt/python3.9/bin/python\n\n\n\nlibrary 경로 설정\n\nconda env lib 연결시\n\n$ export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/usr/work/mylib\n\nvi ~/.bashrc (파일명 앞의 .은 숨김파일을 의미)\nexport LD_LIBRARY_PATH=/home/user/work/mylib:${LD_LIBRARY_PATH}\n\n\nGPU Memory 해제\n\n프로세스가 죽지 않을때 강제 종료\n\nps aux | grep python # 딥러닝 학습을 실행시킨 python 파일의 실행 ID를 찾기\nsudo kill -9 [ID_NUMBER]\n\n\nDocker\n\ndocker: Error response from daemon: could not select device driver \"\" with capabilities: [[gpu]].\nnvidia-container-toolkit 설치\n\n$ distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \\\n   && curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - \\\n   && curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list\n\n$ sudo apt-get update && sudo apt-get install -y nvidia-container-toolkit\n\n$ sudo systemctl restart docker\n\nGPU 설정\n\ndocker run -it --gpus '\"device=0,1,2,3\"'\n\ncontainer를 image로 저장\n\ndocker commit [CONTAINER_ID] [IMAGE_NAME]\n\n\n\n\nterminal zsh/bash\nchsh -s bin/bash\nchsh -s bin/zsh\n\n(@terminal)\n/bin/bash\n/bin/zsh\n\n\n용량 확인\ndf -h\n\n해당 경로에 있는 모든 파일과 폴더들을 용량 단위로 표기\n폴더가 1 순위, 파일이 2 순위로 나열\n경로에 있는 모든 폴더들과 각 폴더내에 있는 파일들을 모두 표기\n\ndu -ch [path] | sort -h\n\n해당 경로의 폴더 기준 트리 구조의 깊이 1만큼의 수준까지 탐색\n용량이 큰 폴더가 아래로, 가장 용량이 큰 10개만 보임\n\ndu -ch [path] --max-depth=1 | sort -h | tail -10\n\n현재 경로의 파일 용량\nls -lh\n\n\n\n파일 찾기\nfind [옵션] [경로] [표현식]\n\n# 현재 위치에서 log가 들어가는 파일 모두 찾기\nfind . -name \"*log*\"\n\n# 현재 디렉토리에서 .txt 확장자 파일 검색 후 모두 삭제\nfind . -name \"*.txt\" -delete\n\noptions\n\nP : 심볼릭 링크를 따라가지 않고, 심볼릭 링크 자체 정보 사용.\nL : 심볼릭 링크에 연결된 파일 정보 사용.\nH : 심볼릭 링크를 따라가지 않으나, Command Line Argument를 처리할 땐 예외.\nD : 디버그 메시지 출력.\n\n\n\n\n폴더나 파일 갯수 찾기\n\n현재 위치에서의 폴더 개수 ls -l | grep ^d | wc -l\n현재 위치에서의 파일 개수 ls -l | grep ^- | wc -l\n현재 폴더의 하위 파일 개수 find . -type f | wc -l\n\n\n\nmount 다중 경로 설정\n\n원래 마운트 지점에서 VFS(가상 파일 시스템) 노드를 생성\n\nmount --bind original-dir original-dir\n\n원래 마운트 지점을 공유로 표시\n\nmount --make-shared original-dir\n선택한 마운트 지점과 그 아래의 모든 마운트 지점에 대한 마운트 유형을 변경하려면 --make-shared 대신 --make-r shared 옵션 사용\n\n중복 생성\n\nmount --bind original-dir dupulicate-dir\n\n지우기\n\nsudo umount duplicate-dir\n\n\n\n\n원격과 로컬 사이 파일 복사\n\nscp 사용. scp [options] [source] [target]\n\n\n로컬에서 원격으로\n\nscp [source_path] [User]@[IP]:[target_path]\nscp /media/avery/source/test.txt avery@xxx.xx.x.xxx:mnt/\n\n원격에서 로컬로\n\nscp [User]@[IP]:[source_path] [target_path]\nscp avery@xxx.xx.x.xxx:mnt/test.txt /home/avery/Documents\n\n원격(A User)에서 원격(B User)으로\n\nscp [User]@[IP]:[source_path] [User]@[IP]:[target_path]\nscp ai@xxx.xx.x.xxx:mnt/test.txt avery@xxx.xx.x.xxx:/home/avery/Documents\n\n복수의 파일 전송\n\n여러 경로를 \" \"을 이용하여 묶어줌\nscp [options] [User]@[IP]:\"[file_1] [file_2]\" [target_path]\n\nOptions\n\n-r: 폴더를 복사할 때 사용(전송 대상을 폴더로 지정). 모든 폴더들을 재귀적으로 복사\n-P: ssh 포트 지정\n-i: identity file을 지정해서 사용\n-v: 상세 내용을 보면서 디버깅할 때 사용\n-p: 전송 시 파일의 수정 시간과 권한을 유지\n\n\n\n\n.cache 용량 해결\n\npip와 conda의 .cache 삭제\n\n#pip\npip cache purge\n\n#conda\nconda clean -a\n\n.cache 위치 변경\n\n# .bashrc를 vim으로 열어서 아래 변수를 추가하기\nexport XDG_CACHE_HOME=\"/{disk_location}/{user_name}/.cache\"\n\n# 해당 경로 생성하기\nmkdir -p /{disk_location}/{user_name}/.cache\n\n\nvim 편집\n\n:%d: 전체 내용 삭제 (press Esc to ensure you’re in Normal mode first)\ni 입력 후, Ctrl + Shift + v: 새로운 내용 붙여넣기\n:wq: 저장\nNormal mode shortcuts:\n\ngg (go to the first line of the file)\ndG (delete from current line to the end of the file)\n\n\n\n\npython module 경로 파악\nimport module\nprint(module.__file__)"
  },
  {
    "objectID": "posts/note/2022-05-06-geultto-7th-start.html",
    "href": "posts/note/2022-05-06-geultto-7th-start.html",
    "title": "📘Geultto 7th Start",
    "section": "",
    "text": "전부터 알고 있었던 글쓰는 또라이, 글또는 언젠가 꼭 들어가고 싶은 모임이었다. 내가 아는 지식에 대해 이야기 할 줄 아는 능력이 멋져보이기도 했고 무엇보다 다른 사람들에게 도움이 되기도 하겠지만 글을 쓰면서 오히려 내 머릿속에서 정리되고 나에게 가장 큰 도움이 되기 때문에(과거의 내가 미래의 나를 다시 가르쳐줄거란 생각으로) 글을 쓰고 싶었다. 글을 쓰는 걸 좋아하긴 하지만 IT, 개발관련 글을 쓰는 건 평소에 쉽게 써내려가는 감성적이고 주관적인 글쓰기와는 조금 거리가 있기 때문에 좀 망설였던 부분도 있었다. 혹시나 틀린 정보나 생각을 적으면 어떡하지 등의 걱정과 함께 계속해서 미뤄져온 글쓰기는 이제는 좀 마음의 짐을 덜고 현재에 최선을 다해서 적으면 된다라는 생각으로 시작하려고 한다.\n블로그 방황기에 대해 말하자면 그 역사가 2년 정도 되는데, 그 기간동안 Github pages, 유명한 Jekyll, Go언어 써보기 위해서 도전한 Hugo, 빠르게 포스팅할 수 있어서 잠시 돌아간 Naver blog, 코딩과는 안맞는 것 같아서 그만둔 Brunch, 영어로 포스팅하는 거 연습하기 위해서 해본 Medium, 쉽게 생각했다가 커스텀하기 더 어려워서 그만둔 Tistory, 기록용으로는 좋지만 Github과 연동이 안되어서 아쉬운 Notion까지.. 가능한 블로그 플랫폼들은 다 해봤던 것 같다. 그 중에 글또와 함께 정착할 플랫폼은 Fastai에서 만들어준 Fastpages로 결정했다. 다음은 내가 블로그 플랫폼을 결정할 때 생각했던 우선순위들인데, Fastpages가 가장 잘 만족하는 플랫폼이었다.\n\n작성하는 방법이 어렵지 않은 지\n(해외 검색까지 고려해서)Googling이 가능하도록 한 지\nGithub과 연동(기록)이 되는 지\n수식작성과 Table of Contents(TOC), 태그 설정이 가능한 지\n블로그내 포스팅 검색이 가능한 지\n광고가 붙지 않는지(컨텐츠에 집중할 수 있는 깔끔함)\n\nMarkdown과 Jupyter Notebook 형식의 작성을 모두 지원하면서 검색이나 태그 설정이 만들어져 있었고 형식이 깔끔하다. 다크 템플릿을 위해서 조금 설정을 해준 것 말고는 힘들게 셋팅하는 부분들이 없었다.\n인생은 타이밍이라는 말처럼 어떤 일을 할 때의 시기가 있다고 생각한다. 시간적인 여유를 생각했을 때는 대학생 때 작성하는 습관을 들이는 걸 연습하기에 좋다고 생각할 수 있지만, 그때의 나를 돌이켜보면 다른 사람들의 포스팅을 보면서 배우기에도 급급했던 것 같다. 지금 석사과정을 시작하면서 내가 배운 것들을 최대한 기록하고, 그 기록들을 발판삼아 더 성장하는 공학자가 되기 위한 타이밍이 지금이 아닐까 싶다. 글또를 통해 이전의 블로그 방황기와 간헐적이었던 글쓰기를 멈출 수 있기를 기대한다.\n\n\n글또 7기를 진행하면서 약 10-20여개의 글을 쓰게 되는데, 한 가지 주제에만 집중해서 쭉 포스팅하는 것이 좋겠지만 크게 제약을 두지 않고 시작하려고 한다. 한 가지 주제만 해야한다는 생각에 미루게 되는 핑계를 스스로 만들지 않고 그때 내가 배운, 내가 나누고 싶은 이야기를 적는 걸로 계획했다. 하지만 큰 글감들로는 아래와 같은 내용들을 생각하고 있다.\n\n로봇 제어(이론이나 시뮬레이션 툴 등)\n로봇 연구에서의 강화학습\nJulia 언어로 하는 Robotics\n\n\n\n\n글또 7기에서 꼭 이뤘으면 하는 목표는 다음과 같다.\n\n공식적인 포스팅 작성 횟수 다 채우기\n커피챗 참여\n\n추가적인 목표로는, 글또 7기 Slack에 많은 채널들이 있는데(#멍또, #냥또는 사랑입니다❣️) 그 중에서 #알또와 #헬또를 통해 알고리즘 실력과 운동도 챙기고 싶다.\n\n1주일에 1알고리즘 문제 풀기\n주 3일 운동하기\n\n최근에 한해의 Github commit을 3D로 만들어주는 기능을 알게 되었는데 글또와 함께한 2022 커밋은 대도시의 건물 숲처럼 더 멋지게 만들어졌으면 좋겠다.🏙"
  },
  {
    "objectID": "posts/note/2022-05-06-geultto-7th-start.html#글쓰는-또라이-글또",
    "href": "posts/note/2022-05-06-geultto-7th-start.html#글쓰는-또라이-글또",
    "title": "📘Geultto 7th Start",
    "section": "",
    "text": "전부터 알고 있었던 글쓰는 또라이, 글또는 언젠가 꼭 들어가고 싶은 모임이었다. 내가 아는 지식에 대해 이야기 할 줄 아는 능력이 멋져보이기도 했고 무엇보다 다른 사람들에게 도움이 되기도 하겠지만 글을 쓰면서 오히려 내 머릿속에서 정리되고 나에게 가장 큰 도움이 되기 때문에(과거의 내가 미래의 나를 다시 가르쳐줄거란 생각으로) 글을 쓰고 싶었다. 글을 쓰는 걸 좋아하긴 하지만 IT, 개발관련 글을 쓰는 건 평소에 쉽게 써내려가는 감성적이고 주관적인 글쓰기와는 조금 거리가 있기 때문에 좀 망설였던 부분도 있었다. 혹시나 틀린 정보나 생각을 적으면 어떡하지 등의 걱정과 함께 계속해서 미뤄져온 글쓰기는 이제는 좀 마음의 짐을 덜고 현재에 최선을 다해서 적으면 된다라는 생각으로 시작하려고 한다.\n블로그 방황기에 대해 말하자면 그 역사가 2년 정도 되는데, 그 기간동안 Github pages, 유명한 Jekyll, Go언어 써보기 위해서 도전한 Hugo, 빠르게 포스팅할 수 있어서 잠시 돌아간 Naver blog, 코딩과는 안맞는 것 같아서 그만둔 Brunch, 영어로 포스팅하는 거 연습하기 위해서 해본 Medium, 쉽게 생각했다가 커스텀하기 더 어려워서 그만둔 Tistory, 기록용으로는 좋지만 Github과 연동이 안되어서 아쉬운 Notion까지.. 가능한 블로그 플랫폼들은 다 해봤던 것 같다. 그 중에 글또와 함께 정착할 플랫폼은 Fastai에서 만들어준 Fastpages로 결정했다. 다음은 내가 블로그 플랫폼을 결정할 때 생각했던 우선순위들인데, Fastpages가 가장 잘 만족하는 플랫폼이었다.\n\n작성하는 방법이 어렵지 않은 지\n(해외 검색까지 고려해서)Googling이 가능하도록 한 지\nGithub과 연동(기록)이 되는 지\n수식작성과 Table of Contents(TOC), 태그 설정이 가능한 지\n블로그내 포스팅 검색이 가능한 지\n광고가 붙지 않는지(컨텐츠에 집중할 수 있는 깔끔함)\n\nMarkdown과 Jupyter Notebook 형식의 작성을 모두 지원하면서 검색이나 태그 설정이 만들어져 있었고 형식이 깔끔하다. 다크 템플릿을 위해서 조금 설정을 해준 것 말고는 힘들게 셋팅하는 부분들이 없었다.\n인생은 타이밍이라는 말처럼 어떤 일을 할 때의 시기가 있다고 생각한다. 시간적인 여유를 생각했을 때는 대학생 때 작성하는 습관을 들이는 걸 연습하기에 좋다고 생각할 수 있지만, 그때의 나를 돌이켜보면 다른 사람들의 포스팅을 보면서 배우기에도 급급했던 것 같다. 지금 석사과정을 시작하면서 내가 배운 것들을 최대한 기록하고, 그 기록들을 발판삼아 더 성장하는 공학자가 되기 위한 타이밍이 지금이 아닐까 싶다. 글또를 통해 이전의 블로그 방황기와 간헐적이었던 글쓰기를 멈출 수 있기를 기대한다.\n\n\n글또 7기를 진행하면서 약 10-20여개의 글을 쓰게 되는데, 한 가지 주제에만 집중해서 쭉 포스팅하는 것이 좋겠지만 크게 제약을 두지 않고 시작하려고 한다. 한 가지 주제만 해야한다는 생각에 미루게 되는 핑계를 스스로 만들지 않고 그때 내가 배운, 내가 나누고 싶은 이야기를 적는 걸로 계획했다. 하지만 큰 글감들로는 아래와 같은 내용들을 생각하고 있다.\n\n로봇 제어(이론이나 시뮬레이션 툴 등)\n로봇 연구에서의 강화학습\nJulia 언어로 하는 Robotics\n\n\n\n\n글또 7기에서 꼭 이뤘으면 하는 목표는 다음과 같다.\n\n공식적인 포스팅 작성 횟수 다 채우기\n커피챗 참여\n\n추가적인 목표로는, 글또 7기 Slack에 많은 채널들이 있는데(#멍또, #냥또는 사랑입니다❣️) 그 중에서 #알또와 #헬또를 통해 알고리즘 실력과 운동도 챙기고 싶다.\n\n1주일에 1알고리즘 문제 풀기\n주 3일 운동하기\n\n최근에 한해의 Github commit을 3D로 만들어주는 기능을 알게 되었는데 글또와 함께한 2022 커밋은 대도시의 건물 숲처럼 더 멋지게 만들어졌으면 좋겠다.🏙"
  },
  {
    "objectID": "posts/note/2025-01-07-ros2-cpp-python.html",
    "href": "posts/note/2025-01-07-ros2-cpp-python.html",
    "title": "📝Python과 C++ 노드를 모두 포함하는 ROS2 패키지 생성",
    "section": "",
    "text": "ROS2 공식 문서에서는 C++와 Python Node를 별개로 만드는 튜토리얼과 설명만 있기 때문에 C++와 Python 모두 사용하여 노드를 만들고 하나의 ROS2 패키지로 만들기 위한 방법에 대해 알아보겠습니다. 각 단계마다 변경되는 사항에 대해서는 🟢로 표시되어 있으니 단계를 넘어갈때마다 확인해보시기 바랍니다."
  },
  {
    "objectID": "posts/note/2025-01-07-ros2-cpp-python.html#recap-ros2-package-architecture",
    "href": "posts/note/2025-01-07-ros2-cpp-python.html#recap-ros2-package-architecture",
    "title": "📝Python과 C++ 노드를 모두 포함하는 ROS2 패키지 생성",
    "section": "Recap ROS2 Package Architecture",
    "text": "Recap ROS2 Package Architecture\nmy_cpp_py_pkg/\n🟡 패키지 정보, 구성, 및 컴파일\n├── CMakeLists.txt\n├── package.xml\n🟡 Python 관련 Stuff\n├── my_cpp_py_pkg\n│   ├── __init__.py\n│   └── module_to_import.py\n├── scripts\n│   └── py_node.py\n🟡 Cpp 관련 Stuff\n├── include\n│   └── my_cpp_py_pkg\n│       └── cpp_header.hpp\n└── src\n    └── cpp_node.cpp"
  },
  {
    "objectID": "posts/note/2023-12-01-geultto-9th-start.html",
    "href": "posts/note/2023-12-01-geultto-9th-start.html",
    "title": "📘Geultto 9th Start",
    "section": "",
    "text": "끝과 시작\n7, 8기에 이어 3번째로 글또 9기를 시작하게 되었습니다.\n사실 7기 시작할 때만 하더라도 계속 이어서 참여를 하게 될 것이라고 생각하진 않았었는데, 지금까지 계속 글을 써오면서 재미를 느끼기도 했고 약간의 반강제성이 있어야 블로그에 하나라도 글을 작성하는 제 모습을 보며 나에게 글또가 필요한 존재라는 것을 깨달았습니다.\n2023년 연말을 향해, 그리고 제 인생에서는 대학원 생활의 마무리를 하는 과정에서 끝을 향해 달려가고 있는 듯한 느낌이지만, 글또 9기를 시작하며 끝과 시작이 이어지는 듯한 느낌이라 설레이는 마음도 큰 것 같습니다. 언제나 그랬듯이, 다시 글또라는 커뮤니티 안에서 내가 쓰고자 하는 글들, 하고자 하는 일들에 대해 정리를 해보는 글을 써보려고 합니다. 7기 다짐글은 블로그 삽질했던 이야기와 함께 간단하게 블로그에 적고 싶은 주제들 등을 적었었고, 8기 다짐글은 글또 OT에서 소개해주셨던 Big5 유형 검사와 함께 논문의 형식을 빌려 나름 패러디 형식으로 작성했었던 것 같습니다. 이번에는 어떻게 시작하는 이야기를 할 수 있을까.. 하다가 생각난 어렸을 적 기억남는 에피소드를 하나 회상하는 것으로 시작하려 합니다.\n\n\nDomino Tower\n정확한 시기는 기억이 나지 않지만 아마 초등학교 4-5학년 그쯤이었던 것 같습니다. 학교에서 수련회를 가면 항상 레크레이션 프로그램으로 협동 게임을 진행했었는데, 한번은 도미노 블럭들을 가지고 가장 긴 도미노 줄 세우기, 도미노로 가장 높게 쌓기 등을 반 끼리 대결하는 시간이었습니다.\n\n\n\n세계에서 가장 높은 도미노 타워는 한 대학원생이 세운 5.275m 라고 하네요 (Source: World’s tallest domino tower)\n\n\n위의 사진처럼 도미노로 가장 높게 탑 쌓기로 대결에서 다른 친구들이 각기 다른 여러 모양으로 탑을 쌓아가기 시작했고, 한 반에 약 30명 정도 있었어서 여러 모양의 탑들이 올라가기 시작했습니다. 다양한 아이디어들을 가지고 각기 모여졌다 흩어졌다 하며 무너지고 쌓기를 반복하며 각자 나름대로의 열정으로 미션에 열중하고 있었습니다.\n저도 몇번 시도해보고 친구들이 쌓는 모습들도 보면서 오각형으로 쌓았을 때가 가장 안정적인 것 같다 라는 걸 판단할 수 있었고, 그 때부터 조용히 혼자 하나하나 쌓아 올려갔습니다. 그때도 성격이 조용했던 터라 그냥 생각했던 대로 혼자 만들기 시작했고, 반 친구들은 옆에서 반대표 작품 하나를 크게 만드는 것에 집중하고 있었습니다. 혼자서 만들다보니 생각보다 너무 잘 쌓아 올려져서 의자를 빌려서 까지 올라가서 쌓아갔었고 결과적으로 저희 반이 제가 쌓기 시작했던 탑으로 1등을 했었습니다. 수련회 교관 선생님들도 꽤 많이 놀라셨던 게 기억이 납니다. 탑의 높이가 가장 높았을 뿐만 아니라 나중에 쓰러뜨리기 위해 중간에 한 조각 건드렸음에도 안 무너졌을 정도로 도미노 탑을 잘 만들었었던 걸로 기억합니다.\n가끔씩 지금 하고 있는 일이 제대로 하고 있는 걸까 스스로 물음표가 생길 때마다 무의식적으로 이 도미노 탑 에피소드가 떠오르는데, 아마 그 때의 기억이 나의 관찰과 생각으로 하나씩 쌓다보면 무언가 완성이 될 것이다라는 경험과 교훈을 떠오르게 해서 그런 것 같습니다.\n\n\n지금의 도미노 한 조각\n그렇다면 지금 내 도미노 한 조각은 어디에 올라가고 있는지.\n나의 물음표에 쉽게 답을 하지 못하면, 최대한 단순하게 내 도미노 한 조각이 어디로 향하고 있는지 고민해봅니다. 자세한 고민들과 여러 곁가지 생각들은 23년도 회고록에 남기기로 하고.. 이번에는 글또 9기 시작 다짐글인 만큼, 엔지니어를 꿈꾸는 사람으로써 글을 쓰는 사람이 되고 싶은 마음 한 조각 도미노 블럭에 대한 이야기를 풀어보려고 합니다.\n앞서 이야기 했듯이 글또라는 모임을 통해 반강제적인 글쓰는 스케줄링을 통해서 지금까지 블로그에 어렵게 쌓아온 글들이 나름 뿌듯합니다. 백프로 모든 글들이 맘에 들지 않지만, 써왔던 글들을 가끔 다시 읽어보며 아 맞아 이걸 배울 수 있었지, 이건 이렇게 쓰면 좋았을 껄, 이건 잘 쓴 것 같다등 회고를 하고 한번 더 성장하는 느낌이 듭니다. 아마 도미노 2-3층 짜리는 되지 않았을까? 생각해봅니다. (적어도 이제 블로그 이사는 안 다니고 안정적으로 이 블로그에다 쓸 수 있을 것 같으니 탑을 쌓아 올리기 전에 기반 공사 오래했다고 생각해도 될 듯 하네요.)\n그래서 앞으로 3층 이상의 블로그 도미노 탑을 쌓기 위해 글또 9기에서 다짐해보는 내용은 아래와 같습니다.\n\n글또 마감 지켜서 모든 회차 제출하기: 매 기수 첫 다짐이지만 놀랍게도 한번도 지키지 못했다는 전설의 다짐. 이번에도 솔직히 지킬 수 있을까 두렵지만 삼세판의 기적을 믿어보겠습니다.\n큐레이션 선정 3번 되어보기: 저번부터 생긴 큐레이션 선정이 동기부여가 많이 되었기도 하고 스스로 글을 더 점검하고 퇴고하는 즐거움도 느낄 수 있어서 이번에도 3번 정도는 채워보자!를 목표로 설정해보려고 합니다. 사실 저번 기수에 4번 선정 되었었지만 이번에는 왠지.. 석사 졸업하고 퀄리티 있는 글을 쓸 시간이 줄어들 것 같은 노파심에 1회 줄여서 현실성 있게 3번이면 잘 활동한 거라고 스스로 상주고 싶네요.\n글또에서 더 넓은 세상을 보고 지혜롭게 살아가는 법 배우기: 이전 기수에서 커피챗, 반상회 모임 등을 통해서 이미 멋진 분들을 보고 동기부여를 많이 받았었기에 이번에도 글또에서 얻을 수 있는 혜택인 글또 안에서 멋진 분들의 생각들을 들을 수 있는 기회들을 가지고자 합니다. 그런데 이번에는 횟수보다는 만남의 여운을 좀 더 신경 써보기로 했습니다..! Action plan으로는 만나는 분들과의 대화 속에서 나는 무엇을 느끼고 배웠는지 진지하게 인터뷰이처럼 기록해볼 생각입니다.(물론 이 이야기는 제 개인 소장 노트에만 적을 것 같습니다.)\n\n이 세가지 목표가 이번 글또 9기에서 이상 없이 이루어지길..! 노력해보겠습니다.\n\n\n마무리\n최근에 새벽에 논문을 쓰다가 스쳐 지나가는 생각이 있어서 메모지에 적었는데, 지금 글또 다짐글을 적다가 앞에 메모지가 보여서 옮겨 적어보았습니다.\n머리와 마음이 생각처럼 되지 않을 때는 손이나 발을 움직이자.\n머리가 안돌아가면 손을 움직이고\n생각이 멈춰있으면 발을 움직이자.\n마치 생각을 뻗어나가야 하는 상황에서는 발을 내딛듯한 느낌을 받아야 하고,\n마음을 정리해야 할 때에는 손으로 끄적이며 엉킨 실타래를 풀어나가야 하는 것처럼\n보이지 않는 것과 보이는 것을 무의식적으로 맞춰나갈 때 해결이 되는 것 같다.\n항상 앉아서 공부하고 정숙한 환경에서 눈 앞의 책 위의 활자들을 보며 열심히 머리를 굴리는 방법만 익히다 보면, 가끔씩 몸의 움직임의 중요성을 까먹는 것 같습니다. 요근래 움직이면 오히려 머릿속과 마음속에서 복잡했던 것들이 쉽게 풀리는 걸 경험했었기에 이번에도 글을 쓰거나 공부하면서 막혀서 멍때리고 있을 미래의 나에게 움직이고 다시 생각해보자라는 메세지를 남기며 다짐글을 이만 마치겠습니다."
  },
  {
    "objectID": "posts/note/2023-05-27-twinkle-github-star.html",
    "href": "posts/note/2023-05-27-twinkle-github-star.html",
    "title": "📘Github Starstruck 128",
    "section": "",
    "text": "2023.05.27 또 하나의 작은 성공을 기록하게 된 날이 되었습니다.\n사실 5월달이 들어서면서부터 이 날을 기록할 수 있기를 바라며 로그를 체크하고 있었는데 드디어 새벽 1시 11분에 달성이 되었다고 로그가 떠서 반가운 마음에 이야기를 남기기 위해 블로그 글을 쓰게 되었습니다."
  },
  {
    "objectID": "posts/note/2023-05-27-twinkle-github-star.html#motivation",
    "href": "posts/note/2023-05-27-twinkle-github-star.html#motivation",
    "title": "📘Github Starstruck 128",
    "section": "Motivation",
    "text": "Motivation\n잠깐 이야기가 나왔던 왜 이 repo를 만들게 되었는지에 대해 이야기해보려고 합니다. Github에서는 커밋 히스토리를 추적할 수 있으니까 이 레포를 처음 만들 당시로 돌아가서 first commit을 봤습니다. 사실 저는 막연하게 제가 대학원 과정을 시작하며 만들었겠거니..라고 예상하고 있었는데 그 보다 더 일찍, 그러니까 좀 더 정확하게 말하자면 대학교 4학년 막학기 10월에 만든 것이라는 것을 확인할 수 있었습니다.\n\n제 기억이 맞다면 당시에 졸업작품과 졸업 이후의 진로 고민으로 인해 이런저런 고민을 많이 하는 시기였는데 아마 이때쯤 진학할 대학원들을 알아보고 해당 레포를 만들면서 만약에 내가 로보틱스를 공부한다면 어떤 것을 알아야 하는지 정리해보기 위해 만든 것 같습니다. 그때만 하더라도 4족 보행 로봇이라는 정확한 로봇의 종류에 대해 결정되지 않았기에 더 폭넓게 Robotics라고 정하고 시작했던 것 같습니다.\n대학원을 진학한 이후, 알아야 할 것들은 너무 많고 여러 연구실에서 쏟아지는 논문들을 감당할 수 없어서 정리를 해야겠다 생각했습니다. 처음에는 아무도 관심없지만 괜히 공개되는 인터셋 상에 올리는 것은 부담스러워서 개인적으로 기록하는 곳(notion이나 privite repo)에 모았지만, 지금까지 저도 다른 사람들이 정리한 리스트들과 인사이트들을 통해 많이 성장할 수 있었고 나도 아직 많이 부족하지만 기여하는 부분이 있으면 좋겠다는 생각에 공개 repo로 전환하여 모으기 시작했습니다. 공개된 리스트라고 생각하니 좀 더 신경을 쓰게 되고 좋은 자극 효과가 되서 내가 기록한 내용을 한번 더 보게되고 그런 경험들이 쌓여 인사이트도 성장하게 되어서 결과적으로 제 스스로에게 더 도움이 많이 되었던 것 같습니다."
  },
  {
    "objectID": "posts/note/2023-05-27-twinkle-github-star.html#how-to-write",
    "href": "posts/note/2023-05-27-twinkle-github-star.html#how-to-write",
    "title": "📘Github Starstruck 128",
    "section": "How to write",
    "text": "How to write\n제가 어떻게 해당 repo를 관리하고 작성해오고 있는지 몇가지 팁들을 공유해보려고 합니다.\nSource\n큐레이션이나 어떤 도움이 되는 정보를 리스트화 한다는 것은 많은 소스들이 우선 전제가 되어야 합니다. 관련 분야 연구자로써 같이 연구하는 동료들과 선후배로부터 얻는 정보들도 물론 많습니다. 하지만 이렇게 privite group에서 얻는 정보들 이외에도 제가 source들을 얻는 주요 경로들은 아래와 같습니다.\n\nGoogle Scholar에서 해당 분야에서 활발하게 연구 성과를 내고 있는 연구자들을 follow합니다. 이메일로 해당 연구자가 paper를 내거나 citation되면 알람이 오는데 저는 빠르게 알람온 리스트들을 abstract/result를 살펴보고 공유하기에 좋다고 판단되면 리스팅합니다.\nYoutube 채널 구독을 통해 얻게 되는 정보들도 많습니다. 생각보다 아카이브에 페이퍼를 먼저 올리기 보다 결과 영상이나 설명 영상들을 유튜브에 먼저 올려서 소식을 먼저 알리는 곳들이 많습니다. 제가 repo 카테고리에 youtube 채널을 넣은 이유이기도 합니다.\nGithub도 연구자들이 코드를 올리면서 README에 연구에 대한 설명과 함께 공개합니다. 유튜브와 같은 맥락으로 페이퍼보다 먼저 공개하는 경우들도 많고 오히려 코드를 공개해주다보니 연구를 더 쉽고 빠르게 파악할 수 있을 때도 있습니다. 그래서 관련 연구자들을 follow해서 어떤 커밋을 올리고 있는지, 어떤 레포들을 관심있게 보는지(star/cloning)도 알게 되고 Github organization으로 연구실이나 회사 단위로 활동하는 것을 파악하여 정보를 얻을 때도 있습니다.\n\nEditor\nmarkdown을 작성하는게 복잡한 소스코드도 아니기 때문에 git clone/commit 하는게 어려운 일은 아니지만, awesome list를 작성하기 위해 매번 다른 컴퓨터에서 clone하고 커밋하며 싱크를 맞추는 일이 생각보다 귀찮습니다. 그래서 저는 awesome list를 작성할 때는 온라인으로 특별한 editor 없이 바로 작성하고 정렬도 할 수 있는 github Code space를 사용합니다. README.md 리스트 알파벳순 자동 정렬을 python code(asset/ordering.py)로 하기 때문에 python이 설치되어있지 않은 컴퓨터에서는 정렬이 어려운데 Code space를 사용하면 python이 설치되어 있지 않은 인터넷이 되는 모든 컴퓨터에서 작성할 수 있으니 everywhere, anytime이 가능하고 UI는 VS Code와 같기 때문에 어렵지도 않습니다. 커밋도 바로 하구요!"
  },
  {
    "objectID": "posts/note/2024-01-15-cs-study-001.html",
    "href": "posts/note/2024-01-15-cs-study-001.html",
    "title": "📝Operating System 001",
    "section": "",
    "text": "Originial Repository: https://github.com/gyoogle/tech-interview-for-developer를 공부하며 2차 편집한 내용입니다."
  },
  {
    "objectID": "posts/note/2024-01-15-cs-study-001.html#운영체제의-역할",
    "href": "posts/note/2024-01-15-cs-study-001.html#운영체제의-역할",
    "title": "📝Operating System 001",
    "section": "운영체제의 역할",
    "text": "운영체제의 역할\n\n\n\n\n\n\n\n\n\n\n1. 프로세스 관리\n2. 저장장치 관리\n3. 네트워킹\n4. 사용자 관리\n5. 디바이스 드라이버\n\n\n\n\n프로세스, 스레드 스케줄링 동기화 IPC 통신\n메모리 관리 가상 메모리 파일 시스템\nTCP/IP 기타 프로토콜\n계정 관리 접근권한 관리\n순차접근 장치 임의접근 장치 네트워크 장치\n\n\n\n\n\n각 역할에 대한 자세한 설명\n\n\n\n\n\n\n1. 프로세스 관리\n\n\n\n\n\n운영체제에서 작동하는 응용 프로그램을 관리하는 기능이다.\n어떤 의미에서는 프로세서(CPU) 관리하는 것이라고 볼 수도 있다. 현재 CPU를 점유해야 할 프로세스를 결정하고, 실제로 CPU를 프로세스에 할당하며, 이 프로세스 간 공유 자원 접근과 통신 등을 관리하게 된다.\n\n\n\n\n\n\n\n\n\n2. 저장장치 관리\n\n\n\n\n\n1차 저장장치에 해당하는 메인 메모리와 2차 저장장치에 해당하는 하드디스크, NAND 등을 관리하는 기능이다.\n\n1차 저장장치(Main Memory)\n\n프로세스에 할당하는 메모리 영역의 할당과 해제\n각 메모리 영역 간의 침범 방지\n메인 메모리의 효율적 활용을 위한 가상 메모리 기능\n\n2차 저장장치(HDD, NAND Flash Memory 등)\n\n파일 형식의 데이터 저장\n이런 파일 데이터 관리를 위한 파일 시스템을 OS에서 관리\nFAT, NTFS, EXT2, JFS, XFS 등 많은 파일 시스템들이 개발되어 사용 중\n\n\n\n\n\n\n\n\n\n\n\n3. 네트워킹\n\n\n\n\n\n네트워킹은 컴퓨터 활용의 핵심과도 같아졌다.\nTCP/IP 기반의 인터넷에 연결하거나, 응용 프로그램이 네트워크를 사용하려면 운영체제에서 네트워크 프로토콜을 지원해야 한다. 현재 상용 OS들은 다양하고 많은 네트워크 프로토콜을 지원한다.\n이처럼 운영체제는 사용자와 컴퓨터 하드웨어 사이에 위치해서, 하드웨어를 운영 및 관리하고 명령어를 제어하여 응용 프로그램 및 하드웨어를 소프트웨어적으로 제어 및 관리를 해야한다.\n\n\n\n\n\n\n\n\n\n4. 사용자 관리\n\n\n\n\n\n우리가 사용하는 PC는 오직 한 사람만의 것일까? 아니다.\n하나의 PC로도 여러 사람이 사용하는 경우가 많다. 그래서 운영체제는 한 컴퓨터를 여러 사람이 사용하는 환경도 지원해야 한다. 가족들이 각자의 계정을 만들어 PC를 사용한다면, 이는 하나의 컴퓨터를 여러 명이 사용한다고 말할 수 있다.\n따라서, 운영체제는 각 계정을 관리할 수 있는 기능이 필요하다. 사용자 별로 프라이버시와 보안을 위해 개인 파일에 대해선 다른 사용자가 접근할 수 없도록 해야 한다. 이 밖에도 파일이나 시스템 자원에 접근 권한을 지정할 수 있도록 지원하는 것이 사용자 관리 기능이다.\n\n\n\n\n\n\n\n\n\n5. 디바이스 드라이버\n\n\n\n\n\n운영체제는 시스템의 자원, 하드웨어를 관리한다. 시스템에는 여러 하드웨어가 붙어있는데, 이들을 운영체제에서 인식하고 관리하게 만들어 응용 프로그램이 하드웨어를 사용할 수 있게 만들어야 한다.\n따라서, 운영체제 안에 하드웨어를 추상화 해주는 계층이 필요하다. 이 계층이 바로 디바이스 드라이버라고 불린다. 하드웨어의 종류가 많은 만큼, 운영체제 내부의 디바이스 드라이버도 많이 존재한다.\n이러한 수많은 디바이스 드라이버들을 관리하는 기능 또한 운영체제가 맡고 있다."
  },
  {
    "objectID": "posts/note/2024-01-15-cs-study-001.html#멀티-프로세스",
    "href": "posts/note/2024-01-15-cs-study-001.html#멀티-프로세스",
    "title": "📝Operating System 001",
    "section": "멀티 프로세스",
    "text": "멀티 프로세스\n\n하나의 프로그램을 여러개의 프로세스로 구성하여 각 프로세스가 병렬적으로 작업을 수행하는 것\n\n장점 : 안전성 (메모리 침범 문제를 OS 차원에서 해결)\n단점 : 각각 독립된 메모리 영역을 갖고 있어, 작업량 많을 수록 오버헤드 발생. Context Switching으로 인한 성능 저하\nContext Switching이란?\n\n프로세스의 상태 정보를 저장하고 복원하는 일련의 과정\n즉, 동작 중인 프로세스가 대기하면서 해당 프로세스의 상태를 보관하고, 대기하고 있던 다음 순번의 프로세스가 동작하면서 이전에 보관했던 프로세스 상태를 복구하는 과정을 말함\n→ 프로세스는 각 독립된 메모리 영역을 할당받아 사용되므로, 캐시 메모리 초기화와 같은 무거운 작업이 진행되었을 때 오버헤드가 발생할 문제가 존재함"
  },
  {
    "objectID": "posts/note/2024-01-15-cs-study-001.html#멀티-스레드",
    "href": "posts/note/2024-01-15-cs-study-001.html#멀티-스레드",
    "title": "📝Operating System 001",
    "section": "멀티 스레드",
    "text": "멀티 스레드\n\n하나의 응용 프로그램에서 여러 스레드를 구성해 각 스레드가 하나의 작업을 처리하는 것\n\n스레드들이 공유 메모리를 통해 다수의 작업을 동시에 처리하도록 해줌\n장점 : 독립적인 프로세스에 비해 공유 메모리만큼의 시간, 자원 손실이 감소 전역 변수와 정적 변수에 대한 자료 공유 가능\n단점 : 안전성 문제. 하나의 스레드가 데이터 공간 망가뜨리면, 모든 스레드가 작동 불능 상태 (공유 메모리를 갖기 때문)\n\n멀티스레드의 안전성에 대한 단점은 Critical Section 기법을 통해 대비함\n\n하나의 스레드가 공유 데이터 값을 변경하는 시점에 다른 스레드가 그 값을 읽으려할 때 발생하는 문제를 해결하기 위한 동기화 과정\n상호 배제, 진행, 한정된 대기를 충족해야함"
  },
  {
    "objectID": "posts/note/2024-01-15-cs-study-001.html#외부-인터럽트",
    "href": "posts/note/2024-01-15-cs-study-001.html#외부-인터럽트",
    "title": "📝Operating System 001",
    "section": "외부 인터럽트",
    "text": "외부 인터럽트\n입출력 장치, 타이밍 장치, 전원 등 외부적인 요인으로 발생\n\n전원 이상, 기계 착오, 외부 신호, 입출력"
  },
  {
    "objectID": "posts/note/2024-01-15-cs-study-001.html#내부-인터럽트",
    "href": "posts/note/2024-01-15-cs-study-001.html#내부-인터럽트",
    "title": "📝Operating System 001",
    "section": "내부 인터럽트",
    "text": "내부 인터럽트\nTrap이라고 부르며, 잘못된 명령이나 데이터를 사용할 때 발생\n\n0으로 나누기가 발생, 오버플로우, 명령어를 잘못 사용한 경우 (Exception)"
  },
  {
    "objectID": "posts/note/2024-01-15-cs-study-001.html#소프트웨어-인터럽트",
    "href": "posts/note/2024-01-15-cs-study-001.html#소프트웨어-인터럽트",
    "title": "📝Operating System 001",
    "section": "소프트웨어 인터럽트",
    "text": "소프트웨어 인터럽트\n프로그램 처리 중 명령의 요청에 의해 발생한 것 (SVC 인터럽트)\n\n사용자가 프로그램을 실행시킬 때 발생\n소프트웨어 이용 중에 다른 프로세스를 실행시키면 시분할 처리를 위해 자원 할당 동작이 수행된다."
  },
  {
    "objectID": "posts/note/2024-01-15-cs-study-001.html#인터럽트-발생-처리-과정",
    "href": "posts/note/2024-01-15-cs-study-001.html#인터럽트-발생-처리-과정",
    "title": "📝Operating System 001",
    "section": "인터럽트 발생 처리 과정",
    "text": "인터럽트 발생 처리 과정\n주 프로그램이 실행되다가 인터럽트가 발생했다.\n\n\n\nHandling Interrupts\n\n\n현재 수행 중인 프로그램을 멈추고, 상태 레지스터와 PC 등을 스택에 잠시 저장한 뒤에 인터럽트 서비스 루틴으로 간다.\n(잠시 저장하는 이유는, 인터럽트 서비스 루틴이 끝난 뒤 다시 원래 작업으로 돌아와야 하기 때문)\n만약 인터럽트 기능이 없었다면, 컨트롤러는 특정한 어떤 일을 할 시기를 알기 위해 계속 체크를 해야 한다.\n(이를 폴링(Polling)이라고 한다)\n폴링을 하는 시간에는 원래 하던 일에 집중할 수가 없게 되어 많은 기능을 제대로 수행하지 못하는 단점이 있었다.\n즉, 컨트롤러가 입력을 받아들이는 방법(우선순위 판별방법)에는 두가지가 있다.\n\n폴링 방식\n\n사용자가 명령어를 사용해 입력 핀의 값을 계속 읽어 변화를 알아내는 방식\n인터럽트 요청 플래그를 차례로 비교하여 우선순위가 가장 높은 인터럽트 자원을 찾아 이에 맞는 인터럽트 서비스 루틴을 수행한다. (하드웨어에 비해 속도 느림)\n\n인터럽트 방식\n\nMCU 자체가 하드웨어적으로 변화를 체크하여 변화 시에만 일정한 동작을 하는 방식\nDaisy Chain\n병렬 우선순위 부여\n\n\n인터럽트 방식은 하드웨어로 지원을 받아야 하는 제약이 있지만, 폴링에 비해 신속하게 대응하는 것이 가능하다. 따라서 ‘실시간 대응’이 필요할 때는 필수적인 기능이다.\n즉, 인터럽트는 발생시기를 예측하기 힘든 경우에 컨트롤러가 가장 빠르게 대응할 수 있는 방법이다."
  },
  {
    "objectID": "posts/note/2024-01-15-cs-study-001.html#fork",
    "href": "posts/note/2024-01-15-cs-study-001.html#fork",
    "title": "📝Operating System 001",
    "section": "Fork",
    "text": "Fork\n\n새로운 Process를 생성할 때 사용.\n그러나, 이상한 방식임.\n\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;unistd.h&gt;\n\nint main(int argc, char *argv[]) {\n    printf(\"pid : %d\", (int) getpid()); // pid : 29146\n    \n    int rc = fork();                    // 주목\n    \n    if (rc &lt; 0) {\n        exit(1);\n    }                                   // (1) fork 실패\n    else if (rc == 0) {                 // (2) child 인 경우 (fork 값이 0)\n        printf(\"child (pid : %d)\", (int) getpid());\n    }\n    else {                              // (3) parent case\n        printf(\"parent of %d (pid : %d)\", rc, (int)getpid());\n    }\n}\n\npid : 29146\nparent of 29147 (pid : 29146)\nchild (pid : 29147)\n\n을 출력함 (parent와 child의 순서는 non-deterministic함. 즉, 확신할 수 없음. scheduler가 결정하는 일임.)\n[해석]\nPID : 프로세스 식별자. UNIX 시스템에서는 PID는 프로세스에게 명령을 할 때 사용함.\nFork()가 실행되는 순간. 프로세스가 하나 더 생기는데, 이 때 생긴 프로세스(Child)는 fork를 만든 프로세스(Parent)와 (almost) 동일한 복사본을 갖게 된다. 이 때 OS는 위와 똑같은 2개의 프로그램이 동작한다고 생각하고, fork()가 return될 차례라고 생각한다. 그 때문에 새로 생성된 Process (child)는 main에서 시작하지 않고, if 문부터 시작하게 된다.\n그러나, 차이점이 있었다. 바로 child와 parent의 fork() 값이 다르다는 점이다. 따라서, 완전히 동일한 복사본이라 할 수 없다.\n\nParent의 fork()값 =&gt; child의 pid 값\nChild의 fork()값 =&gt; 0\n\nParent와 child의 fork 값이 다르다는 점은 매우 유용한 방식이다.\n그러나! Scheduler가 부모를 먼저 수행할지 아닐지 확신할 수 없다. 따라서 아래와 같이 출력될 수 있다.\n\npid : 29146\nchild (pid : 29147)\nparent of 29147 (pid : 29146)"
  },
  {
    "objectID": "posts/note/2024-01-15-cs-study-001.html#wait",
    "href": "posts/note/2024-01-15-cs-study-001.html#wait",
    "title": "📝Operating System 001",
    "section": "wait",
    "text": "wait\n\nchild 프로세스가 종료될 때까지 기다리는 작업\n\n위의 예시에 int wc = wait(NULL)만 추가함.\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;unistd.h&gt;\n#include &lt;sys/wait.h&gt;\n\nint main(int argc, char *argv[]) {\n    printf(\"pid : %d\", (int) getpid()); // pid : 29146\n    \n    int rc = fork();                    // 주목\n    \n    if (rc &lt; 0) {\n        exit(1);\n    }                                   // (1) fork 실패\n    else if (rc == 0) {                 // (2) child 인 경우 (fork 값이 0)\n        printf(\"child (pid : %d)\", (int) getpid());\n    }\n    else {                              // (3) parent case\n        int wc = wait(NULL)             // 추가된 부분\n        printf(\"parent of %d (wc : %d / pid : %d)\", wc, rc, (int)getpid());\n    }\n}\n\npid : 29146\nchild (pid : 29147)\nparent of 29147 (wc : 29147 / pid : 29146)\n\nwait를 통해서, child의 실행이 끝날 때까지 기다려줌. parent가 먼저 실행되더라도, wait ()는 child가 끝나기 전에는 return하지 않으므로, 반드시 child가 먼저 실행됨."
  },
  {
    "objectID": "posts/note/2024-01-15-cs-study-001.html#exec",
    "href": "posts/note/2024-01-15-cs-study-001.html#exec",
    "title": "📝Operating System 001",
    "section": "exec",
    "text": "exec\n단순 fork는 동일한 프로세스의 내용을 여러 번 동작할 때 사용함.\nchild에서는 parent와 다른 동작을 하고 싶을 때는 exec를 사용할 수 있음.\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;unistd.h&gt;\n#include &lt;sys/wait.h&gt;\n\nint main(int argc, char *argv[]) {\n    printf(\"pid : %d\", (int) getpid()); // pid : 29146\n    \n    int rc = fork();                    // 주목\n    \n    if (rc &lt; 0) {\n        exit(1);\n    }                                   // (1) fork 실패\n    else if (rc == 0) {                 // (2) child 인 경우 (fork 값이 0)\n        printf(\"child (pid : %d)\", (int) getpid());\n        char *myargs[3];\n        myargs[0] = strdup(\"wc\");       // 내가 실행할 파일 이름\n        myargs[1] = strdup(\"p3.c\");     // 실행할 파일에 넘겨줄 argument\n        myargs[2] = NULL;               // end of array\n        execvp(myarges[0], myargs);     // wc 파일 실행.\n        printf(\"this shouldn't print out\") // 실행되지 않음.\n    }\n    else {                              // (3) parent case\n        int wc = wait(NULL)             // 추가된 부분\n        printf(\"parent of %d (wc : %d / pid : %d)\", wc, rc, (int)getpid());\n    }\n}\nexec가 실행되면,\nexecvp( 실행 파일, 전달 인자 ) 함수는, code segment 영역에 실행 파일의 코드를 읽어와서 덮어 씌운다.\n씌운 이후에는, heap, stack, 다른 메모리 영역이 초기화되고, OS는 그냥 실행한다. 즉, 새로운 Process를 생성하지 않고, 현재 프로그램에 wc라는 파일을 실행한다. 그로인해서, execvp() 이후의 부분은 실행되지 않는다."
  },
  {
    "objectID": "posts/note/2024-01-15-cs-study-001.html#process-management",
    "href": "posts/note/2024-01-15-cs-study-001.html#process-management",
    "title": "📝Operating System 001",
    "section": "Process Management",
    "text": "Process Management\n\nCPU가 프로세스가 여러개일 때, CPU 스케줄링을 통해 관리하는 것을 말함\n\n이때, CPU는 각 프로세스들이 누군지 알아야 관리가 가능함\n프로세스들의 특징을 갖고있는 것이 바로 Process Metadata\n\nProcess Metadata\n\nProcess ID\nProcess State\nProcess Priority\nCPU Registers\nOwner\nCPU Usage\nMemeory Usage\n\n\n이 메타데이터는 프로세스가 생성되면 PCB(Process Control Block)이라는 곳에 저장됨"
  },
  {
    "objectID": "posts/note/2024-01-15-cs-study-001.html#pcbprocess-control-block",
    "href": "posts/note/2024-01-15-cs-study-001.html#pcbprocess-control-block",
    "title": "📝Operating System 001",
    "section": "PCB(Process Control Block)",
    "text": "PCB(Process Control Block)\n\n프로세스 메타데이터들을 저장해 놓는 곳, 한 PCB 안에는 한 프로세스의 정보가 담김\n\n\n\n\nPCB\n\n\n다시 정리해보면?\n프로그램 실행 → 프로세스 생성 → 프로세스 주소 공간에 (코드, 데이터, 스택) 생성 \n→ 이 프로세스의 메타데이터들이 PCB에 저장"
  },
  {
    "objectID": "posts/note/2024-01-15-cs-study-001.html#pcb가-왜-필요한가요",
    "href": "posts/note/2024-01-15-cs-study-001.html#pcb가-왜-필요한가요",
    "title": "📝Operating System 001",
    "section": "PCB가 왜 필요한가요?",
    "text": "PCB가 왜 필요한가요?\n\nCPU에서는 프로세스의 상태에 따라 교체작업이 이루어진다. (interrupt가 발생해서 할당받은 프로세스가 waiting 상태가 되고 다른 프로세스를 running으로 바꿔 올릴 때)\n이때, 앞으로 다시 수행할 대기 중인 프로세스에 관한 저장 값을 PCB에 저장해두는 것이다."
  },
  {
    "objectID": "posts/note/2024-01-15-cs-study-001.html#pcb는-어떻게-관리되나요",
    "href": "posts/note/2024-01-15-cs-study-001.html#pcb는-어떻게-관리되나요",
    "title": "📝Operating System 001",
    "section": "PCB는 어떻게 관리되나요?",
    "text": "PCB는 어떻게 관리되나요?\n\nLinked List 방식으로 관리함\nPCB List Head에 PCB들이 생성될 때마다 붙게 된다. 주소값으로 연결이 이루어져 있는 연결리스트이기 때문에 삽입 삭제가 용이함.\n즉, 프로세스가 생성되면 해당 PCB가 생성되고 프로세스 완료시 제거됨\n\n이렇게 수행 중인 프로세스를 변경할 때, CPU의 레지스터 정보가 변경되는 것을 Context Switching이라고 한다."
  },
  {
    "objectID": "posts/note/2024-01-15-cs-study-001.html#context-switching",
    "href": "posts/note/2024-01-15-cs-study-001.html#context-switching",
    "title": "📝Operating System 001",
    "section": "Context Switching",
    "text": "Context Switching\n\nCPU가 이전의 프로세스 상태를 PCB에 보관하고, 또 다른 프로세스의 정보를 PCB에 읽어 레지스터에 적재하는 과정\n\n보통 인터럽트가 발생하거나, 실행 중인 CPU 사용 허가시간을 모두 소모하거나, 입출력을 위해 대기해야 하는 경우에 Context Switching이 발생\n즉, 프로세스가 Ready → Running, Running → Ready, Running → Waiting처럼 상태 변경 시 발생!"
  },
  {
    "objectID": "posts/note/2024-01-15-cs-study-001.html#context-switching의-overhead란",
    "href": "posts/note/2024-01-15-cs-study-001.html#context-switching의-overhead란",
    "title": "📝Operating System 001",
    "section": "Context Switching의 OverHead란?",
    "text": "Context Switching의 OverHead란?\noverhead는 과부하라는 뜻으로 보통 안좋은 말로 많이 쓰인다.\n하지만 프로세스 작업 중에는 OverHead를 감수해야 하는 상황이 있다.\n프로세스를 수행하다가 입출력 이벤트가 발생해서 대기 상태로 전환시킴\n이때, CPU를 그냥 놀게 놔두는 것보다 다른 프로세스를 수행시키는 것이 효율적\n즉, CPU에 계속 프로세스를 수행시키도록 하기 위해서 다른 프로세스를 실행시키고 Context Switching 하는 것\nCPU가 놀지 않도록 만들고, 사용자에게 빠르게 일처리를 제공해주기 위한 것이다."
  },
  {
    "objectID": "posts/note/2024-01-15-cs-study-001.html#ipc-종류",
    "href": "posts/note/2024-01-15-cs-study-001.html#ipc-종류",
    "title": "📝Operating System 001",
    "section": "IPC 종류",
    "text": "IPC 종류\n\n익명 PIPE\n\n파이프는 두 개의 프로세스를 연결하는데 하나의 프로세스는 데이터를 쓰기만 하고, 다른 하나는 데이터를 읽기만 할 수 있다.\n한쪽 방향으로만 통신이 가능한 반이중 통신이라고도 부른다.\n따라서 양쪽으로 모두 송/수신을 하고 싶으면 2개의 파이프를 만들어야 한다.\n매우 간단하게 사용할 수 있는 장점이 있고, 단순한 데이터 흐름을 가질 땐 파이프를 사용하는 것이 효율적이다. 단점으로는 전이중 통신을 위해 2개를 만들어야 할 때는 구현이 복잡해지게 된다.\n\nNamed PIPE(FIFO)\n\n익명 파이프는 통신할 프로세스를 명확히 알 수 있는 경우에 사용한다. (부모-자식 프로세스 간 통신처럼)\nNamed 파이프는 전혀 모르는 상태의 프로세스들 사이 통신에 사용한다.\n즉, 익명 파이프의 확장된 상태로 부모 프로세스와 무관한 다른 프로세스도 통신이 가능한 것 (통신을 위해 이름있는 파일을 사용)\n\n\n하지만, Named 파이프 역시 읽기/쓰기 동시에 불가능함. 따라서 전이중 통신을 위해서는 익명 파이프처럼 2개를 만들어야 가능\n\nMessage Queue\n\n입출력 방식은 Named 파이프와 동일함\n다른점은 메시지 큐는 파이프처럼 데이터의 흐름이 아니라 메모리 공간이다.\n사용할 데이터에 번호를 붙이면서 여러 프로세스가 동시에 데이터를 쉽게 다룰 수 있다.\n\n공유 메모리\n\n파이프, 메시지 큐가 통신을 이용한 설비라면, 공유 메모리는 데이터 자체를 공유하도록 지원하는 설비다.\n프로세스의 메모리 영역은 독립적으로 가지며 다른 프로세스가 접근하지 못하도록 반드시 보호돼야한다. 하지만 다른 프로세스가 데이터를 사용하도록 해야하는 상황도 필요할 것이다. 파이프를 이용해 통신을 통해 데이터 전달도 가능하지만, 스레드처럼 메모리를 공유하도록 해준다면 더욱 편할 것이다.\n공유 메모리는 프로세스간 메모리 영역을 공유해서 사용할 수 있도록 허용해준다.\n프로세스가 공유 메모리 할당을 커널에 요청하면, 커널은 해당 프로세스에 메모리 공간을 할당해주고 이후 모든 프로세스는 해당 메모리 영역에 접근할 수 있게 된다.\n\n중개자 없이 곧바로 메모리에 접근할 수 있어서 IPC 중에 가장 빠르게 작동함\n\n\n\n메모리 맵\n\n공유 메모리처럼 메모리를 공유해준다. 메모리 맵은 열린 파일을 메모리에 맵핑시켜서 공유하는 방식이다. (즉 공유 매개체가 파일+메모리)\n주로 파일로 대용량 데이터를 공유해야 할 때 사용한다.\n\n소켓\n\n네트워크 소켓 통신을 통해 데이터를 공유한다.\n클라이언트와 서버가 소켓을 통해서 통신하는 구조로, 원격에서 프로세스 간 데이터를 공유할 때 사용한다.\n서버(bind, listen, accept), 클라이언트(connect)\n\n\n이러한 IPC 통신에서 프로세스 간 데이터를 동기화하고 보호하기 위해 세마포어와 뮤텍스를 사용한다. (공유된 자원에 한번에 하나의 프로세스만 접근시킬 때)"
  },
  {
    "objectID": "posts/note/2025-06-23-ros-errors.html",
    "href": "posts/note/2025-06-23-ros-errors.html",
    "title": "📝ROS1/2 Errors",
    "section": "",
    "text": "gzclient: /usr/include/boost/smart_ptr/shared_ptr.hpp:728: typename boost::detail::sp_member_access&lt;T&gt;::type boost::shared_ptr&lt;T&gt;::operator-&gt;() const [with T = gazebo::rendering::Camera; typename boost::detail::sp_member_access&lt;T&gt;::type = gazebo::rendering::Camera*]: Assertion `px != 0' failed.\n[ERROR] [gzclient-2]: process has died [pid 2979, exit code -6, cmd 'gzclient --gui-client-plugin=libgazebo_ros_eol_gui.so'].\n\n\n\n$ source /usr/share/gazebo/setup.sh\n\nGazebo 환경변수(source /usr/share/gazebo/setup.sh)가 제대로 설정되지 않아 GAZEBO_PLUGIN_PATH·GAZEBO_MODEL_PATH 등에서 플러그인·모델 로드에 실패해 shared_ptr이 null이 되는 경우가 많음\n또한 그래픽 드라이버/OpenGL 컨텍스트 문제로 객체 초기화가 실패하거나, ROS–Gazebo 버전 불일치로 플러그인이 정상 로드되지 않아 내부 객체가 null로 반환되는 경우가 주요 원인\n\n또 같은 에러가 발생하면 gazebo 필요 모델이나 플러그인 로드되는지 확인\necho $GAZEBO_PLUGIN_PATH\necho $GAZEBO_MODEL_PATH"
  },
  {
    "objectID": "posts/note/2025-06-23-ros-errors.html#error",
    "href": "posts/note/2025-06-23-ros-errors.html#error",
    "title": "📝ROS1/2 Errors",
    "section": "",
    "text": "gzclient: /usr/include/boost/smart_ptr/shared_ptr.hpp:728: typename boost::detail::sp_member_access&lt;T&gt;::type boost::shared_ptr&lt;T&gt;::operator-&gt;() const [with T = gazebo::rendering::Camera; typename boost::detail::sp_member_access&lt;T&gt;::type = gazebo::rendering::Camera*]: Assertion `px != 0' failed.\n[ERROR] [gzclient-2]: process has died [pid 2979, exit code -6, cmd 'gzclient --gui-client-plugin=libgazebo_ros_eol_gui.so']."
  },
  {
    "objectID": "posts/note/2025-06-23-ros-errors.html#solution",
    "href": "posts/note/2025-06-23-ros-errors.html#solution",
    "title": "📝ROS1/2 Errors",
    "section": "",
    "text": "$ source /usr/share/gazebo/setup.sh\n\nGazebo 환경변수(source /usr/share/gazebo/setup.sh)가 제대로 설정되지 않아 GAZEBO_PLUGIN_PATH·GAZEBO_MODEL_PATH 등에서 플러그인·모델 로드에 실패해 shared_ptr이 null이 되는 경우가 많음\n또한 그래픽 드라이버/OpenGL 컨텍스트 문제로 객체 초기화가 실패하거나, ROS–Gazebo 버전 불일치로 플러그인이 정상 로드되지 않아 내부 객체가 null로 반환되는 경우가 주요 원인\n\n또 같은 에러가 발생하면 gazebo 필요 모델이나 플러그인 로드되는지 확인\necho $GAZEBO_PLUGIN_PATH\necho $GAZEBO_MODEL_PATH"
  },
  {
    "objectID": "posts/note/2025-06-23-ros-errors.html#error-1",
    "href": "posts/note/2025-06-23-ros-errors.html#error-1",
    "title": "📝ROS1/2 Errors",
    "section": "error",
    "text": "error\n패키지 빌드시 마주친 에러 로그\nCould not find a package configuration file provided by \"moveit_servo\" with\n  any of the following names:\n\n    moveit_servoConfig.cmake\n    moveit_servo-config.cmake\n\n  Add the installation prefix of \"moveit_servo\" to CMAKE_PREFIX_PATH or set\n  \"moveit_servo_DIR\" to a directory containing one of the above files.  If\n  \"moveit_servo\" provides a separate development package or SDK, be sure it\n  has been installed.\nROS 2 패키지를 APT로 올바르게 가져오지 못해 moveit_servo 설치 시 404 오류가 발생하고, 동시에 GPG 키 만료로 인해 apt update 단계부터 경고가 뜨는 상황:\n\nGPG 키 만료로 인해 ROS 2 저장소가 신뢰되지 않아 패키지 색인이 최신으로 갱신되지 않습니다.\n오래된 색인을 기반으로 설치를 시도하므로 서버에 없는 .deb 파일을 내려받으려 해 404가 납니다.\n해결을 위해서는 ① 새로운 GPG 키를 가져와, ② APT 소스에 signed-by 옵션을 넣어 제대로 설정한 뒤, ③ 캐시를 정리하고 갱신, ④ 다시 패키지를 설치하는 순으로 진행해야 합니다."
  },
  {
    "objectID": "posts/note/2025-06-23-ros-errors.html#solution-1",
    "href": "posts/note/2025-06-23-ros-errors.html#solution-1",
    "title": "📝ROS1/2 Errors",
    "section": "solution",
    "text": "solution\n\nROS 2 GPG 키 갱신\n\nROS 2용 새 키 다운로드\nsudo apt install curl gnupg2 lsb-release        # 키링 관리용 도구 설치\nsudo curl -sSL https://raw.githubusercontent.com/ros/rosdistro/master/ros.key \\\n  -o /usr/share/keyrings/ros-archive-keyring.gpg\n\n만료된 키를 대체하며, 이후 APT가 신뢰할 저장소 키로 사용\n\n기존 ROS 2 리스트 제거\nsudo rm /etc/apt/sources.list.d/ros2*.list\n\n예전 설정이 남아 있으면 signed-by 옵션이 적용되지 않음\n\n\nAPT 소스 설정\n\nOfficial 문서 권장 방식에 따라 저장소를 설정:\n\nsudo apt update\nsudo apt install software-properties-common\nsudo add-apt-repository universe\n\necho \\\n  \"deb [arch=$(dpkg --print-architecture) \\\n        signed-by=/usr/share/keyrings/ros-archive-keyring.gpg] \\\n   http://packages.ros.org/ros2/ubuntu \\\n   $(. /etc/os-release && echo $UBUNTU_CODENAME) main\" \\\n  | sudo tee /etc/apt/sources.list.d/ros2.list &gt; /dev/null\n\nsigned-by 옵션으로 키링 파일을 지정해 보안 경고를 방지\n\n캐시 정리 및 최신 색인 가져오기\n\n만약 키 설정 이후에도 색인이 갱신되지 않는다면, APT 리스트 캐시를 지운 뒤 다시 시도:\nsudo rm -rf /var/lib/apt/lists/*\nsudo apt update\n이렇게 하면 저장소에서 최신 패키지 목록을 내려받음\n\nmoveit_servo 설치\n\n이제 설치가 가능:\nsudo apt install ros-humble-moveit-servo\n\n패키지명은 ros-humble-moveit-servo이고, 설치 후 /opt/ros/humble/share/moveit_servo에 관련 파일이 생김"
  },
  {
    "objectID": "posts/note/2023-03-31-daily-english-007.html",
    "href": "posts/note/2023-03-31-daily-english-007.html",
    "title": "🌎Casual English Phrases 007",
    "section": "",
    "text": "독차지하다\nhog\n\n명사로는 야생 돼지, 큰 돼지 라는 뜻\n\n\nThey’ll hog the court all day! 그들이 하루 종일 테니스장 코트를 독차지하고 있을거야!\nDon’t hog the toys! 장난감을 독차지 하지마!\nQuit hogging all my time! 내 시간을 독차지하려 하지마!\n\n\n\n뭘 망설이고 있어?(용기를 북돋는)\nWhat are you waiting for?\n\n진짜 노래가사에서 많이 들을 수 있음\n\n\n\n떠나다\nget off\n\nYou big kids get off that court right now! 너네 코트에서 당장 나와!\nI need to get off this island. 난 이 섬에서 떠나야 한다\nGet off the bridge! 그 다리에서 떨어져!\n\n\n\n어떻게 생각해?\nWhat do you say?\n\nWhat do you say to going to the movies tomorrow? 내일 영화 보러 가는 것에 대해서 어떻게 생각해?\n\n\n어떤 제안을 하고 나서 그 제안에 대한 상대방의 견해를 물어볼 때 사용 / 일상이나 업무 둘다 사용\n\n\n\n지루하게 자세한 걸 다 읽지는 않겠습니다.\nI won’t bore you by reading all of the details.\n\nI won’t bore you by telling all of the stories. I’ll make a point. 모든 이야기를 다해서 지루하게 하진 않을게요. 요점을 말하겠습니다.\n\n\n\n나 ~ 못하는 거 알잖아.\nYou know I have trouble (with) ~\n\nYou know I have trouble with the can opener! 나 캔 따개 못 따는 거 알잖아!\nYou know I have trouble speaking out loud in front of a big crowd. 제가 많은 관중들 앞에서 이야기 못하는 거 알잖아요."
  },
  {
    "objectID": "posts/note/2023-04-05-daily-english-008.html",
    "href": "posts/note/2023-04-05-daily-english-008.html",
    "title": "🌎Casual English Phrases 008",
    "section": "",
    "text": "내가 그럴 줄 알았어!\nI knew it!\n\n어쩐지..이상하다 그랬어. 라는 뉘앙스\n\n\nI knew I had the wrong code. (어쩐지..) 내가 잘못된 코드를 가지고 있었던 걸 알고있었어.\nI knew it! It was the wrong size! 역시! 사이즈가 잘못된 거였어!\n\n\n\n생각해보니\nUpon reflection\n\nUpon reflection, I think Charlie Brown was wrong. 생각해보니 찰리가 틀린 것 같아.\nUpon reflection, I think we have to change the main color of this banner. 다시 생각해보니 내 생각에 이 배너의 메인 색을 바꿔야 할 것 같아.\nUpon reflection, I prefer this option. 다시 생각해보니 이 옵션이 좋은 것 같아요.\n\n\n\n배짱이 좋다./뻔뻔하다.\nhave(got) the(a/some/a lot) nerve\n\nnerve: 용기, 대담성, 뻔뻔스러움\n\n\nhave the nerve + to 동사원형\n\n\nHe had the nerve to say that I’m not perfect! 내가 완벽하지 않다고 하다니 배짱도 좋지!\nWow, you have the nerve to criticize my report. 와, 당신이 내 리포트를 비평하다니 배짱이 있으시네요.\nYou really have a nerve, arriving an hour late for the event. 넌 그 행사에 1시간이나 늦게 도착하다니 참 뻔뻔하다.\n\n\n\n~을 정리하다, 정돈하다\nstraighten up\n\nInstead of watching TV, you could be straighten up your room. TV 보는 대신에 방 청소를 할 수도 있지.\nI’m going to straighten up my room. 내 방 정리정돈 할거야.\nStraigthen up your back! 너 허리 똑바로 펴!\n\n\n\n진심이야?\nDo you mean it?\n\nYes, I mean it. 응 진심이야.\n\n\n\n넌 절대 마음 바꿀 생각 없나 보네?\nI suppose you’re not going to change your mind.\n\n\n평소와 달리/기분 전환으로\nfor a change\n\nYou look pretty clean today for a change. 오늘은 평소와 달리 깨끗해 보여.\nWe always go to the movies. Let’s go shopping for a change! 우리 맨날 영화만 보러가니까 오늘은 평소와 다르게 쇼핑가자!\nI think I’m going to do an intense workout for a change. 나 오랜만에 강도 높은 운동을 해야 할 것 같아.\nI want to go my hometown and visit my parents for a change. 기분 전환하게 오랜만에 고향에 가서 부모님을 보고 싶어."
  },
  {
    "objectID": "posts/note/2022-10-29-geultto-7th-end.html",
    "href": "posts/note/2022-10-29-geultto-7th-end.html",
    "title": "📘Gueltto 7th End",
    "section": "",
    "text": "이번 post는 지난 2022년 5월 15일부터 10월 16일까지 활동했던 글쓰는 또라이 7기 활동을 마치고 회고한 글입니다. 처음 시작을 다짐으로 시작하여 9월 14일에 중간 점검글도 잠깐 쓰고 이제 마지막 이글로 회고를 하면서 마무리 지어보려고 합니다."
  },
  {
    "objectID": "posts/note/2022-10-29-geultto-7th-end.html#the-bottom-line",
    "href": "posts/note/2022-10-29-geultto-7th-end.html#the-bottom-line",
    "title": "📘Gueltto 7th End",
    "section": "The bottom line …",
    "text": "The bottom line …\n글또 활동 보증금으로 10만원을 내고 시작했고 모든 권장 활동을 충실히 지킨 결과, 굿즈 구입비 1만원도 커피챗 2회 보상(5000원x2)으로 보충되어 그대로 10만원을 받을 수 있었습니다."
  },
  {
    "objectID": "posts/note/2022-10-29-geultto-7th-end.html#본질",
    "href": "posts/note/2022-10-29-geultto-7th-end.html#본질",
    "title": "📘Gueltto 7th End",
    "section": "본질",
    "text": "본질\n처음에 글또에 참여하겠다고 설문지를 작성하던 그때를 떠올리고 오리엔테이션을 마치고 처음 다짐글을 다시 읽으면서 되새겨보면, 제게 가장 강력했던 동기는 글쓰기 힘 기르기였습니다. 글은 장르와 목적에 따라 어떻게 작성하는게 명확하고 전달력이 좋은지 차이가 크고 콘텐츠를 보는 사람의 즉각적인 피드백을 얻기 힘들기 때문에 머릿속을 빙빙도는 스토리들을 잘 만드는 사람이 되고 싶었습니다. 처음 다짐글에 감상적인 글보다는 IT, 개발관련 글을 쓰고 싶었다고 생각한 이유는 생각이나 감정을 배제하고 싶다는 생각보다는 일기와 같은 글에서는 내 이야기 중심이다보니 나 이외의 독자를 배려하거나 고려할 수 있는 힘을 기를 수 없기 때문이었습니다. 같은 맥락으로 사실 블로그에 내가 공부한 것들을 정리한 정리노트 서랍장이 되지 않기를 바랐던 것도 있었습니다. 나 이외의 독자(들)도 이해를 하고 도움을 받을 수 있을까? 나는 내 글로 다른 사람들이 이 글을 읽을 가치가 있다고 설득할 수 있을까? 고민들을 했었습니다. 물론 지금도 이 질문들에 완벽한 답변이 될 만한 포스트들을 작성하고 있지 못하지만 글또에서 조금은 나아졌다고 생각합니다. 단순히 느낌만 그런것이 아니라, 같은 팀내에서 피드백들을 받고 저보다 글을 더 논리정연하게 쓰고 설득할 줄 아는 분들의 어깨너머로 확실이 배울 수 있었습니다.\n활동 보증금이 있어서 정말 완벽한 의지로 글을 쓰는 습관을 들인 것은 아니지만 그래도 활동 보증금을 다 받을 수 있을 정도 만큼은 성장했다는 것에 만족합니다. 일단 1차 목표였고 한 단계를 밟았기 때문입니다. 사실 이 작은 성장을 시작으로 본질 밑바닥에 깔려있던 흑심은 블로그 포스팅으로 서술하는 능력을 길러서 좋은 논문을 내는 연구자가 되고 싶었던 마음이 있었습니다. 연구가 아직 내 적성에 맞는지는 확신이 서지 않지만 일단 석사과정을 시작한 한 미약한 연구자로써 글을 잘쓰고 싶었기 때문입니다. 큰 학술지는 아니었지만 학부과정에서 2번정도 논문들을 쓸 기회가 있었는데 솔직히 CV에는 한줄이라도 쓰기 위해 논문들을 적어놓지만 남보여주기에는 부끄러웠습니다. 저 이외의 연구자들을 전혀 고려하지 않은 서술들이었기 때문이었습니다. 또한 여러 논문들을 읽다보면 어떤 논문은 정말 감탄이 나올정도로 설계도 깔끔하고 서술도 완벽에 가까운데 반해 어떤 논문은 뭘 말하고자 하는지 왜 이렇게 써야했는지 이해하기 어려운데 적어도 저의 석사논문 만큼은 같은 동료 연구자들에게 고개를 끄덕일 수 있도록 퀄리티가 좋았으면..하는 희망이 있습니다. 이 마음 저 생각이 모여 글또로 시작을 열 생각을 하게된 것 같고 그 본질과 흑심을 표면 밖으로 끄집에 내게 해준 글또 활동들이 소중했습니다.\n본질과는 조금 동떨어지지만 글쓰는 작은 변화가 근래에 있었습니다. 영어공부도 할겸 영어 실력을 향상시키기 위해 daily english 시리즈로 글을 되도록 자주 올리도록 노력하게 되었습니다. 이야말로 위에서 말했던 정리노트 서랍장을 만들 수도 있을 위험이 크지만 그래도 영어표현이나 팁들을 다른 사람들도 보기 쉽게 소비하고 도움을 받을 수 있도록 정리하려고 합니다. 실제로 제가 다른분들의 짧은 영어 포스팅들로 도움을 많이 받기 때문에 이와 같은 일환으로 시작하게 되었습니다."
  },
  {
    "objectID": "posts/note/2022-10-29-geultto-7th-end.html#처음-다짐",
    "href": "posts/note/2022-10-29-geultto-7th-end.html#처음-다짐",
    "title": "📘Gueltto 7th End",
    "section": "처음 다짐",
    "text": "처음 다짐\n\n글의 주제\n\n달성정도: 2/5\n처음 다짐에 로봇 제어(이론이나 시뮬레이션 툴 등), 로봇 연구에서의 강화학습, Julia 언어로 하는 Robotics로 글을 쓰고 싶다고 적었었습니다. 하지만 거의 논문 리뷰 위주로 글을 쓰다보니 해당 주제들에서 많이 벗어난 글들도 있었고 로봇 연구에서의 강화학습에 해당하는 글들만 조금 쓸 수 있었던 것 같습니다.\n\n활동 목표\n\n달성정도: 5/5\n활동 목표로 처음 활동 보증금을 모두 회수할 수 있는 미니멈으로 목표를 정했었습니다. 그리고 중간중간 위험하게 마감을 못지킬 것 같은 순간들도 있었지만 결과적으로 100% 회수할 수 있는 활동기록으로 마무리했습니다. 사실 이게 가장 중요하게 생각한 목표였기 때문에 정말 뿌듯했습니다.\n\n생활 목표\n\n달성정도: 2/5\n알고리즘 문제 풀기와 운동 꾸준히 하기를 생활목표로 정했었는데 우선 알고리즘 문제 풀기는 중간에 연구장학생 준비를 위해 조금 했기 때문에 1점 + 운동은 꾸준히는 아니지만 간헐적으로 시간있을 때 챙길려고 노력하기 때문에 1점 해서 총 2점입니다."
  },
  {
    "objectID": "posts/note/2022-10-29-geultto-7th-end.html#중간-점검",
    "href": "posts/note/2022-10-29-geultto-7th-end.html#중간-점검",
    "title": "📘Gueltto 7th End",
    "section": "중간 점검",
    "text": "중간 점검\n\n매일 커밋하기\n\n달성정도: 3/5\n커뮤니티에서 1001일 매일 커밋하신 분을 보고 모티베이션을 얻어서 정한 목표였는데 역시 매일 커밋은 정말 어려운 일이었습니다. 솔직히 하루 무너지니 다짐이 도미노처럼 우르르 무너져 3-4일간 하기 싫은 적도 있었는데 글또에서 피드백 주셨던 것처럼 의미없는 커밋에 마음 뺏기지 말고 의미있는 커밋에 좀 더 중점을 두고 꾸준히 노력해봐야겠습니다. 본질을 생각해야겠죠.\n\n코드가 들어가는 포스팅 작성하기\n\n달성정도: 0/5\n지금까지 작성해보지 않았던 코드와 함께하는 포스팅을 도전하겠다 해놓고 아직까지도 도전 준비중 상태입니다. 코드를 글에 적기 위해서는 실습을 해보는 시간 + 적절한 편집 실력도 필요해서 더 망설여지는 것 같습니다. 그렇지만 아직 하지 않았을 뿐이지 하지 않을 것은 아니기 때문에 조만간 학교 수업에서 듣는 코드들도 조금씩 작성해보려고 합니다.\n\n멘탈관리를 위해 딴짓하기\n\n달성정도: 3/5\n조만간 특별한 취미생활이라며 중간 다짐글에 적어놓지 않았었는데 그 취미는 클라리넷이었습니다. 사실 어렸을 때부터 악기를 매우매우 좋아해서 이것저것 많이 했었는데 그 중 고1때까지 했던 클라리넷 상자가 어느날 제 눈을 사로잡았습니다. 입시만 끝나면 다시 끄낼 줄 알았던 그 상자가 어느새 대학 졸업하고 대학원에 들어왔는데도 열리지 않았는데 멘탈을 위해, 딴짓을 위해 이것저것 생각하다보니 떠올라서 다시 열게되었습니다. 다행히도 학교 근처에서 렛슨을 하는데가 있어서 다시 시작할 수 있었고 2011.00.00 렛슨 날짜가 적혀있는 교본을 다시 꺼내서 2022.00.00날짜를 적으니 감회가 새로웠습니다. 이에 더해 몸이 기억하고 있는 운지법은 정말 신기했던 것 같습니다.\n\n\n마지막 한 줄로 글또 7기를 정리하자면,\n\n좋은 시도, 경험, 마무리였다."
  },
  {
    "objectID": "posts/note/2022-10-17-daily-english-003.html",
    "href": "posts/note/2022-10-17-daily-english-003.html",
    "title": "🌎Casual English Phrases 003",
    "section": "",
    "text": "영어 회화 표현이나 논문에서 쓸 수 있는 유용한 표현들을 조금씩 익히기 위해 시작한 1000일 시리즈 포스트 입니다. 공부 리소스는 유튜브, 리얼클래스, PN님의 블로그, cake 어플, 책, 논문, 기사 등 다양한 출처에서 보고 기록합니다.\n\n\n속마음을 보여주다\nwear your heart on your sleeve\n\nIf you wear your heart on your sleeve, you openly show your feelings or emotions rather than keeping them hidden. 속마음을 보여준다는 말은, 너의 감정이나 느낌들을 숨기기보다 보여준다는 거야.\nIt takes courage to wear your heart on your sleeve, which is why few people do it. 속마음을 털어놓는다는 건 용기가 필요한 일이기 때문에 많은 사람들이 하지 못하는 일이지.\nI need someone whom I can wear my heart on my sleeve. 내 속마음을 털어놓을 사람이 필요해.\n\n\n\n기억력이 좋은\na memory like an elephant\n\nShe has a memory like an elephant. 그녀는 기억력이 굉장히 좋아\nYou can remember his name even if you don’t have a memory like an elephant. 니가 기억력이 엄청 좋지 않더라도 그의 이름은 기억할껄?\nThe menual is so simple. Don’t worrry that you don’t have a memory like an elephant. 메뉴얼이 엄청 간단해. 기억력이 좋지 않다고 걱정할 필요 없어.\n\n\n\n입장바꿔 생각해봐\nput yourself in one’s shoes\n\nPlease put yourself in his shoes before blurting it out. 그냥 말을 하기전에 그의 입장도 생각해봐\n\nblurt out : 말을 내뱉다\n\nIt’s hard to put myself in other people’s shoes from the my heart. 진심으로 다른 사람들의 입장이 되어보는 것은 어려워\n\nfrom the (bottom of one’s) heart : 진심으로\n\nWould you mind putting yourself in my shoes? 제 입장에서 한번만 생각해주실 수 있으실까요?\n\n참고 기억력 상징이 코끼리가 된 이유"
  },
  {
    "objectID": "posts/note/2021-01-03-Hello-2021.html",
    "href": "posts/note/2021-01-03-Hello-2021.html",
    "title": "📘Hello 2021",
    "section": "",
    "text": "2021에 이루고 싶은 일들, 하고 싶은 일들, 바라는 일들, 2021 회고록을 쓰면서 “그땐 이런 생각을 하며 살려고 노력했었구나” 할 만한 이야기를 적어보자."
  },
  {
    "objectID": "posts/note/2021-01-03-Hello-2021.html#해야하는하고싶은-공부",
    "href": "posts/note/2021-01-03-Hello-2021.html#해야하는하고싶은-공부",
    "title": "📘Hello 2021",
    "section": "해야하는/하고싶은 공부",
    "text": "해야하는/하고싶은 공부\n구체적인 실천 목표들은 다이어리에 적고 여기에는 크게 대략적인 목표지점들을 적어보려고 한다.\n\n영어\n이젠 피하고 싶다는 생각보다는 해내야만 한다는 생각이 더 크다. 다른 공부들도 그렇지만 영어야 말로 이제는 output을 내야하기 때문에 간절함이 더 커졌다.\n\n영어루틴\n토플 90점 이상\n외국인 친구 만들기\n\n\n\nRL\n아직도 잘 모르겠지만 아직도 내 호기심을 자극시키는 분야라 계속 공부하고 싶다.\n\n스터디(기초이론과 논문읽기)\n구현능력 올리기\n연구에 활용하는 능력 기르기\n\n\n\nGNN\n\n기초 이론 입문하기\n\n\n\nQC\n\n기초 이론 입문하기(feat. 모두연)\nQiskit Challenge 도전하기"
  },
  {
    "objectID": "posts/note/2021-01-03-Hello-2021.html#하고싶은-취미",
    "href": "posts/note/2021-01-03-Hello-2021.html#하고싶은-취미",
    "title": "📘Hello 2021",
    "section": "하고싶은 취미",
    "text": "하고싶은 취미\n2020에 회고하면서 한 가지 느꼈던 게 있었다. 취미라 할 만한 활동을 하질 않았었다. 그래서 더 쉽게 지쳤고 멀리 나가지 못했던 것 같아 2021에는 취미도 생각하면서 살고 싶어서 한번 적어보았다.\n\n피아노\n부모님의 반대를 무릅쓰고 산 전자 피아노가 하나 있는데 많이 쳐보지를 못했다. 원래 쳤던 곡들도 좋고 재즈 피아노나 마피아에서 산 몇곡 악보를 제대로 연습해보고 싶다. 예전에는 피아노한테 힐링도 많이 받았었는데..왜 이렇게 멀어졌을까🤔\n\n박터틀의 재즈 피아노\nTido Kang의 노래\n피아노 치는 이정환님의 노래 이건 불가능하겠지만🤣\n\n\n\n책 읽기\n유튜브 보는 시간을 좀 줄여보고 한 달에 딱 1권은 인문이나 사회 분야 등과 같이 내가 쉽게 접할 수 없는 분야의 책들을 읽어보려 한다. 기술서 같은 경우에는 필요에 의해 손이 많이 가지만 이외의 책들은 정말 안 읽게 되는 것 같다. 간략히 남기고 싶은 문구나 독후감은 다이어리에 남겨보자."
  },
  {
    "objectID": "posts/note/2021-01-03-Hello-2021.html#만들고-싶은-습관",
    "href": "posts/note/2021-01-03-Hello-2021.html#만들고-싶은-습관",
    "title": "📘Hello 2021",
    "section": "만들고 싶은 습관",
    "text": "만들고 싶은 습관\n\n운동\n체력과 건강을 무시 못할 시기가 온 것 같다. 지금은 코로나로 인해 헬스장이 언제 열릴지는 모르겠으나 일단은 기본적인 걷기나 자전거, 간단한 홈트 동작들을 꾸준히 아침 루틴으로 하는 것이 목표다.\n\n\n블로깅\n지금 쓰고 있는 github 블로그에 꾸준히 기술관련 포스팅을 하는 것이 목표다. 아직은 개발자도 아니고 유의미한 output 포스팅을 하기엔 실력이 없지만, 내가 그때 그때 마다 알게 된 것, 이후의 나에게 도움이 될 만한 내용이라고 판단되면 적어보려고 한다. 나부터 나를 가르쳐보면서 어떤 글이 잘 쓰여진 글인지 스스로 체크해보며 블로깅하는 실력을 길러보는 한 해가 되었으면 좋겠다.\n\n\n다이어리\n블로깅이 주로 기술관련(특히 it 분야겠지만)이라면 다이어리는 내 내면과 생각을 정리해보고 싶어서 만들고 싶은 습관이다. 생각보다 하루하루 내가 느낀 것들은 많은데 막상 돌이켜 보면 잘 기억이 나지 않고 내면의 단단함이 쉽게 물러지는 느낌이 든다. 그냥 생각없이 산 것 같기도 하고 내 삶을 소중히 생각하지 않은 것 같은 느낌이 들 때. 순간의 나도 이런 생각을 하고 고민을 하며 살아왔다는 흔적을 남기는 연습을 하고 싶다. 예전에 삼수를 할 때 비교적 생활 패턴이 규칙적이고 고요할 때(?)는 다이어리에 기록하는게 어렵지 않았는데 지금은 다이어리 루틴을 만든다는 게 쉽지 않은 것 같다. 2021은 남겨보려고 노력해보고 싶다."
  },
  {
    "objectID": "posts/note/2021-01-03-Hello-2021.html#신년다짐과-생각",
    "href": "posts/note/2021-01-03-Hello-2021.html#신년다짐과-생각",
    "title": "📘Hello 2021",
    "section": "신년다짐과 생각",
    "text": "신년다짐과 생각\n송구영신 예배와 신년 축복예배에서 받은 말씀은 기대의 차원을 높이자와 대장부가 되자였다. 남에게 기대하기 보다 나에게 기대하는 삶을, 나보단 하나님께 근거있는 삶을 바라며 나에게 주어진 새로운 시간들을 잘보냈으면 좋겠다. 그리고 지금까지 살면서 느껴왔던 것처럼 삶을 살아가면서 내가 생각지 못한 경험들의 스펙트럼은 점점 넓어진다는 걸 알았다. 좋은 쪽으로만 범위가 넓어지는 것이 아니기에. 내가 감당치 못할 어려움을 주시는 분이 아니시기에 당장 눈 앞에 커보이는 두려움을 담대하게 맞설 수 있다는 믿음을 더 확고히 할 수 있는 한 해가 될 수 있도록.\n어쩌다보니 대학생활도 마지막에 접어들었다. 생각없이 보내기엔 다시 돌아오지 않을 시간들이기에 좀 더 정신차리고 최선을 다해보고 싶다. 내 것이 되기전에는 한 없이 미웠었지만 지금은 그 어떤 대학보다 나의 가능성을 확인시켜 준 내 학교에서 마지막 생활은 어떨까. 안녕 2021"
  },
  {
    "objectID": "posts/note/2024-10-09-geultto-10th-start.html",
    "href": "posts/note/2024-10-09-geultto-10th-start.html",
    "title": "📘Geultto 10th Start",
    "section": "",
    "text": "나야, 글또\n한창 흑백요리사가 유행하는 요즘. 많은 밈들이 만들어지고 있습니다. 그 중 하나 “나야, 들기름”을 조금 응용하여, 글또가 나에게 어떤 존재인지 표현해보면 “나야, 글또”로 표현해볼 수 있지 않을까 싶습니다. 글또는 대학원 석사 2학기 무렵 글을 쓰며 성장하고 싶다라는 생각으로 7기에 시작하여 지금 10기까지 참여하고 있는데, 어느덧 졸업과 취업이라는 2개의 산을 넘는 동안 참여 해온 커뮤니티라는 사실이, 은은한 들기름 향처럼 제 생활에 성장의 고소함을 주었다는 것이, 이렇게 표현이 되지 않나 싶습니다. 대학원생 신분일 동안에는 어디에 표현되지도 않고 드러나지도 않을 방구석 연구자인 것처럼 내 자신이 초라하게 느껴질 때 글또에서 글을 쓰며 힘을 얻을 수 있었습니다. 그리고 비교적 짧은 시간이었지만 그 어느 때보다 불안정함을 느꼈던 취준 시기에는 글또에서 만난 분들에게 직간접적으로 영감을 받으며 버틸 수 있었습니다. 이제는 아직 1년도 안된 사회 초년생으로, 글또에서 이렇게 많은 선배/동료 개발자들을 보고 배울 수 있기에 얼마나 감사한지 모릅니다.\n사실 매번 글또를 마치는 시기마다 “다음 기수엔 신청이 어렵겠다. 이제 혼자서 글을 꾸준히 써보자.”라고 생각하며 “아마도 다음에 신청을 하지 않을 것 같다.” 생각하게 됩니다. 하지만 또 다시 새로운 기수가 시작되는 시기가 오고 신청을 받는 기간이 돌아오면 다시 신청서를 작성하고 “글또 없이는 블로그 글이 전혀 안써지네.”라고 생각하며 시작하게 됩니다. “나야, 글또”라고 말하며 다시 다가오는 글또 활동 시기에 감사하고, 또 한번 저에게 글 쓰는 힘을 기를 수 있는 커뮤니티의 힘을 빌릴 수 있다는 것이 안도가 되는 것 같습니다.\n\n\n이번에는\n스스로도 아이러니 한 것은, 사실 꾸준히 글을 쓰며 성장하고 싶은 마음이 충분히 있음에도 동시에 글을 쓰면서 느끼는 저항감과 피로감이 있기에 글또 활동이 마냥 편하지 않습니다. 그렇기에 마지막 글 제출까지 하고 나면 다음 기수 글또를 기약하기 어렵겠다는 생각과 함께 다시 글을 안 쓰던 안일한 마음으로 되돌아가려고 하는 것 같습니다.\n이번에는 글또가 10기로 마지막으로 운영되는 기수입니다. 마지막으로 커뮤니티의 힘을 빌려 글을 쓰며 성장하는 기회이기도 하기에, 본질로 돌아가 10기에서의 목표로 단 하나만 지켜보려고 합니다.\n매주 글쓰기\n2주 간격 제출이 원칙적인 정기 제출 주기이지만 건너뛰게 되는 한 주에도 스스로 글 쓰는 마감일을 지정해서 작성하는 목표를 잡았습니다. 7기 때부터 스스로 글 쓰는 습관을 만들고 싶었는데 매번 정기 제출 마감일에 급급하며 썼었기에 제대로 습관을 기르지 못했다는 판단 하에 이번에는 가장 단순하고도 본질적인 목표 하나만 잡았습니다. 마지막 글또에서 가장 단단한 습관을 만들어 놔야 이후에 지속적인 성장이 가능하지 않을까 라고 생각하기 때문에 도전해보려고 합니다.\n이전에는 학생/연구자 신분이었기에 ai 연구 코어 채널에서 활동을 했었지만, 이번에는 첫 직장에서 AI/Robotics 엔지니어로써 일을 시작하게 되어 ml-ai-엔지니어 코어 채널에서 활동하게 되었습니다. 비슷하지만 다른 분위기에서 영감을 받을 수 있을 것 같아 많이 기대가 됩니다.\n\n\n실험을 받아들이는 자세\n마지막으로 다시 흑백요리사 이야기로 돌아가서, 경연에서 가장 인상이 깊었던 분은 에드워드 리 셰프님이었습니다. 다 쟁쟁한 요리사들이 나왔고 존경스러운 점들이 많았지만 , 에드워드 리 셰프의 경력과 연륜에서 유지하기 힘든 도전에 대한 갈망과 그런 지향점을 유지하게 해주는 삶의 태도에서 “나도 저렇게 살아가고 싶다.”라는 생각이 들었던 것 같습니다.\n\n\n\n에드워드 리의 인터뷰 중 1\n\n\n사실 2024년도에 많은 변화가 일어나면서 내가 좋아하는 게 뭔지 조차 헷갈리는 시기가 있었습니다. 나도 나를 모르겠는 혼란스러운 순간들 가운데 처음 찐 사회생활을 하다 보니 흔들림의 진폭이 더 커져 괴로웠었고 “내 선택이 맞는 걸까”라는 고민에 정체되어 있던 시간이었습니다. 특히나 내가 원하지 않았던 것들을 하고 있다고 생각이 들 때가 가장 괴롭고 시간이 허비되고 있다는 생각과 이러다가 내 커리어가 꼬이는 거 아닐까라는 불안함으로 앞으로 나아가지 못하고 있었습니다. 하지만 내가 원하는 것에만 너무 초점을 맞춰서 생각하다보니 원하지 않는 것들이 도움이 되지 않을 것이다 라는 얕은 생각으로 괴로워하지 말고, 셰프님의 인터뷰에서 처럼 내가 원하지 않는 것에 대해 알아가는 시간 또한 귀하다고 생각하고 나아갈 수 있구나 라는 걸 깨달을 수 있었습니다.\n\n\n\n에드워드 리의 인터뷰 중 2\n\n\n글또는 내가 나를 가지고 어떤 실험이든 할 수 있도록 판을 깔아주는 소중한 연구실이었기에, 이번 10기에도 글쓰는 성장의 힘을 실험해볼 수 있는 좋은 시간을 만들어 갈 수 있길 바래봅니다."
  },
  {
    "objectID": "posts/note/2023-02-02-geultto-8th-start.html",
    "href": "posts/note/2023-02-02-geultto-8th-start.html",
    "title": "📘Geultto 8th Start",
    "section": "",
    "text": "Introduction\n나는 나를 안다라고 말할 수 있는 사람이 많을까요? 사실 그런 사람들이 많지 적은지는 관심이 없고 적어도 저는 내가 그런 사람이길 바랐습니다.\n나라는 것에 대한 정의와 경계가 있는 건 분명한데 그 안에 있는 공간이 너무 광활해서 마치 작은 박스안에 블랙홀이 있는 것 같은 느낌이랄까요. 어렸을 때는 누가 나에게 꿈과 하고 싶은게 무엇이냐라고 물으면 대화의 흐름에 방해되지 않을 정도로만 뜸을 들인 뒤 대답을 할 수 있었던 것 같습니다. 흔히들 대답하는 선택지인 직업과 하는 일, 그리고 거기에 약간의 신념을 섞어서요. 걱정과 근심, 고민이 없진 않았지만 대답을 할 수 있다는 것만으로 나는 나를 안다고 생각했습니다. 지금, 물론 지금도 사람들이 청춘이다라고 말하는 시기에 서있지만, 적어도 과거의 나보다 지나온 시간이 많은 지금의 저는 많은 의문이 듭니다. 나는 나를 아나? 내가 모르면 누가 나를 아나라며 누구보다 나에 대해서는 잘 안다고 생각했는데.. 그 주장이 약해지는 것이 느껴집니다.\n이렇게 내가 나에 대한 갈피를 잡지 못할 때 글또가 하나의 도움이 되기 때문에 이번 8기에도 참여하게 되었습니다.\n\n\nRelated Works\n이질감이 드는 나를 바라볼 때 도움이 되었던 몇가지 방법이 있습니다. 우선 한동안 유명했던 MBTI가 그 중 하나였는데 고등학생 때 진로적성 검사로 몇번 해보고 최근 1-2년 동안 대한민국에서 자기소개의 도구가 되어버린 MBTI는 나를 알아가는데 재밌는 도구였습니다. 저는 INFJ입니다. 검사지에 나온 몇가지 눈에 띄는 서술어들로는 통찰력 있는 선지자. 극소수의 유형, 내향적인 이상주의자, 감정형(F) 중에서는 대체로 사고(T) 성향이 높은 편 등이 있고 이외에 SNS등에서 유형별 특징들에서는 아무도 신경안쓰는데 혼자 눈치봄, 생각이 너무 많아 집중이 안됨(아무생각 안하는 거 할 줄 모른다), 글로 적으면 논리력 기가막힌데 말로하면 어버버하고 안나옴, 뭐든지 파악하고 정리하고 싶어한다, 구구절절 (괄호까지 쳐가며) 설명하는 습관이 있다 등이 있습니다. 공감의 차원을 넘어서서 누가 나를 이렇게 잘 아나 흠칫 놀랄 정도였으니 이 검사가 확증편향을 가져온다고 하더라도 나를 알아가는데 정말 많이 도움이 되었던 것 같습니다.\n\n\n\n매우 공감되는 INFJ의 모습들\n\n\n다음으로, 글쓰는 또라이라는 글쓰는 모임에서 소개받은 Big5 검사라는 걸 한번 해봤는데 공감력, 책임감, 협조성이 대표적인 키워드로 나왔고 강직함, 지적 호기심, 상상력, 걱정 등이 내가 강한 성향으로 친밀감, 자제력, 사교성, 분노, 리더십 등이 내가 약한 성향으로 나왔습니다. MBTI 검사와 크게 다르지 않아 새롭게 알게 된 사실이 딱히 많진 않았지만 추가적으로 일적인 측면에서 현재 주어진 상황에 만족하기보다 새로운 지식과 기술을 익히기 위해 노력하는 변화 주도형 리더라는 워딩에 공감하며 어쩌다 현재 내가 공학도가 되어있는가에 대해 생각해보게 된 것 같습니다.\n\n\n\nBig5 검사 결과\n\n\n성향, 성격도 나의 일부지만 앞서 이야기했던 꿈과 하고 싶은 것, 싫어하는 것, 좋아하는 것 등 다른 부분들도 나라는 경계 안에 있다는 것을 압니다. 요즘 부쩍 하고 싶은 것에 대한 확신은 불투명해지고 싫어하는 것과 좋아하는 것을 구분짓는 표현은 하기가 힘들어집니다. 그래서 아마 이 글도 이런 맥락 속에서 나를 안다는 것에 대한 고찰로 시작된 것 같습니다. 학창시절 때는 친구들이 저에게 너는 정말 네가 하고 싶어하는 게 분명한 것 같아. 멋있다. 라고 많이 이야기 했습니다. 저 또한 그런 나의 모습에 뿌듯함과 자랑스러움을 느끼고 있었습니다. 그때는 나를 안다고 생각했습니다. 지금은 잘 모르겠습니다. 여전히 저를 좋게 봐주는 좋은 사람들이 있지만 이제 더 이상 제 안에서 저를 뿌듯하게 바라보기 보다 스스로를 걱정스럽게 바라보는 눈길이 더 느껴집니다. 이런 불안함이 일시적인거다. 성향적인 이유에서 그런거다.라고 생각할 수도 있겠지만 그렇게 넘어갈 수 없을 것 같다는 느낌이 들었습니다. 여전히 괴롭지만 최근에 엄마와의 통화를 통해 힌트를 얻고 다른 관점으로 보려고 합니다. 그 괴로움이 내가 살아있다는 증거라고 하시더라구요. 그렇게 말씀하시면서 너를 위해 기도하고 있다고 덧붙여서 말씀하실 때 솔직히 반항심도 들었습니다. 불안함과 고통이 살아있다는 증거라면 솔직히 살고 싶지 않다라는 생각을 하게 되는 나를 하나님은 봐주시지 않나라는 생각이 들었습니다. 각설하고, 여기서 힌트를 얻었다는 것은 내가 살아가는 동안 완전히 해결할 수 있는 고민은 아니구나 싶었습니다. 해결하고 싶었지만 해결할 수 없다는 걸 깨달았다고 마무리 짓겠습니다.\n나의 성향과 나의 고민을 펼쳐보며 든 생각은 글쓰기가 유일한 내 창구일 수도 있겠다 싶었습니다. 나는 나를 표현하기 힘들어하고 다른 사람들에게 잘 말하지도 못하는 사람이라는 걸 알아서 글을 쓰면서 나를 돌아보고 내가 나에게 말해보는 시간이 도움이 될 것 같았습니다. 시간이 지난 후에 이 블로그에 들어와서 과거 시점의 나를 한번 더 들여다 볼 수 있는 것이 때론 힘이 될 것 같기도 했습니다. 글쓰는 또라이 모임 8기에 참여하면서 쓰게 되는 글들은 물론 이런 종류의 글들이 아닙니다. 감정적인 이야기보다는 기술적이고 어떤 정보나 지식을 찾던 사람들을 위한 글을 쓰는 것이 취지에도 맞고 제가 기르고 싶은 실력에도 도움이 되기 때문에 참여하게 된 것 입니다. 아마 이 글을 마지막으로 글또에 제출하는 글들에는 더 이상 저의 잡다한 이야기가 들어가지 않을 것 같습니다.\n\n\nMethod\n글또에서 쓰는 기술적인 글들 또한 제가 쓴 글이고 나를 만들어가는 글들이 되기 때문에 앞서 장황한 도입을 썼습니다. 7기에서 썼던 글들이 지금 연구하고 있는 나를 만들어준 동력원이기도 했고 다시 읽으면서 도움이 되기도 했습니다. 이전에 다짐한 것처럼 지식과 정보의 차원에서 다른 사람들에게 도움이 되는 글을 쓰고 싶은 것은 변함없지만 너무 이 목표에 사로잡히지는 않으려고 합니다. 일단 내가 정리하고 글을 쓰면서 나에게 도움이 된 것 만으로도 나는 또 다른 성장을 했다고 나를 격려하며, 이번 8기에서는 이런 마음으로 임하려고 합니다. 쉽게 말하자면 부담을 좀 덜어내고 글을 자주 쓰는 습관이 이번 기수에서 제가 설정한 목표라고 할 수 있겠습니다.\n정리를 좋아하니 개괄식으로 글또에서 이루고자한 목표를 정리해보겠습니다.\n\n글또에서 설정한 글 마감일에 모든 글 제출하기(패쓰권 안쓰기)\n커피드백에서 성장할 수 있는 모든 점들을 파악하고 더 나아지기\n코드가 들어가는 글 50%이상 작성하기\n글또에 제출하지 않는 주간에는 자유글 쓰기(매주 1포스팅 목표)\n\n이전 7기에서 목표와 다짐을 작성할 때도 글을 쓰는 목표말고도 나를 이루는 생활 목표도 적었었는데 잘 지키진 못했어도 다시한번 환기시킬 수 있어서 좋았기에 이번에도 생활목표들을 덧붙여볼까 합니다. 생활 목표들은 기본적으로 매일한다는 생각으로 지금 이미 만들고 있는 습관들에 업그레이드 하는 수준으로 설정해봤습니다.\n\n매일 아침 해야할 것과 하지 말아야 할 것 적기: 생각이 많아서 실천을 못하기 때문에 아침에 정리를 해서 나를 좀 더 빨리 움직여보려고 합니다.\n매일 운동하기(저녁 만보 걷기/저녁 30분 러닝/아침 필라테스 중 하나 골라서): 우울감과 나태함은 생각보다 쉽게 신체활동으로 극복된다는 깨달음이 계기가 되었습니다.\n매일 영어표현 3개 내 문장으로 만들기(with RealClass): 문어체보다 쉽게 다가갈 수 있는 대화 영어가 부족하기 때문에 부담없이 반복해보려고 합니다.\n\n\n\nConclusion\n결론은 광활한 우주에서 먼지에 불과한 고민많은 한 작은 인간이 커피와 음악으로 해결되지 않는 자아관찰을 이번 글또 8기와 함께 잘 이뤄나가고 싶다는 것이었습니다. 글을 쓸때 만큼은 한없이 진지해지고 새벽이 아님에도 새벽감성 충만하게 되는 저의 모습을 또 하나 알아가며 이번 글또 8기도 잘 활동해보고 싶습니다."
  },
  {
    "objectID": "posts/note/2022-10-03-daily-english-001.html",
    "href": "posts/note/2022-10-03-daily-english-001.html",
    "title": "🌎Casual English Phrases 001",
    "section": "",
    "text": "영어 회화 표현이나 논문에서 쓸 수 있는 유용한 표현들을 조금씩 익히기 위해 시작한 1000일 시리즈 포스트 입니다. 공부 리소스는 유튜브, 리얼클래스, PN님의 블로그, cake 어플, 책, 논문, 기사 등 다양한 출처에서 보고 기록합니다.\n\n\n갑자기\nout of the blue\n\nA cockroach appeared out of the blue. And then I really wanted to get out of there. 갑자기 바퀴벌레가 나타나서 진짜 거기서 나오고 싶었어.\nI just thought about that out of the blue. 그냥 갑자기 생각나서.\nIt happened out of the blue, so I could not handle that situation. 그 일이 너무 갑자기 일어나서 어떻게 할 수가 없었어.\n\n\n\n듣고있어\nI’m all ears\n\nJust keep talking about it. I’m all ears. 계속 말해. 나 듣고 있어.\nWhen you have a conversation with someone, you should be all ears. 누군가와 대화를 하면 귀기울여서 들어야해.\nDon’t pretend to be all ears. 듣고 있는 척 하지마.\n\n\n\n들킬것 같이 위태롭고 불안한\nprecarious\n\nI hate precarious situations. They make me very vexed. 난 진짜 조마조마한 상황들이 싫어. 진짜 나를 초조하게 만들거든.\n\nvexed : 초조한\n\nReal liars don’t have a precarious feelings. Instead, They enjoy the situations and make more fakes. 진짜 거짓말 쟁이들은 불안한 감정을 느끼지 않아. 오히려 그들은 그 상황을 즐기고 더 많은 거짓들을 만들어내지.\nJust tell the truth to her, not make yourself more precarious. 그냥 그녀에게 진실을 털어놔 더 너 자신을 위태롭게 만들지 말고."
  },
  {
    "objectID": "posts/note/2023-08-31-daily-english-010.html",
    "href": "posts/note/2023-08-31-daily-english-010.html",
    "title": "🌎Casual English Phrases 010",
    "section": "",
    "text": "최고야\nNothing beats ~\n\n가장 좋아하는 것을 말할 때 유용한 표현\n\n\nNothing beats “About Time” \"About Time\" 영화가 최고야.\nNothing beats a good nap. 좋은 낮잠이 최고야.\nNothing beats pizza. 피자보다 좋은 건 없어.\n\n\n\n완전 동의/공감해!\nI couldn’t agree more!\nIt’s hard to go wrong with that choice!\n\n\n꼭 명심해야 할 점은\nRule number one,\n\nRule number one, don’t get yourself into this situation again. 첫 번째 규칙, 다시는 이런 상황을 만들지 마.\nRule number one. Don’t be late. Our boss hates being late. 꼭 명심해야 될 게 있어. 절대 늦으면 안돼. 우리 상사가 늦는 거 진짜 싫어해.\nRule number one. The customer is always right. 첫번째 규칙, 가장 중요한 원칙: 손님은 언제나 옳다.\n\n\n\n정신 차리고 제대로 살기 시작하다, 제대로 돌아가기 시작하다\nget oneself together\n\nGot myself together. So now we’ll see. 정신 차렸다고요. 이젠 어떻게 될지 두고봐야죠.\nYou’ve got to stop drinking every day. Get yourself together! 너 매일 술 마시는 것 좀 그만하고 정신 좀 차려!\nThe company got itself together and turned a profit. 그 회사는 재정비를 마치고 제대로 돌아가기 시작하면서 이윤을 내기 시작했다.\n\n\n\n감정을 추스리다, 감정을 컨트롤하다\npull oneself together\n\nPull yourself together. There’s no point getting angry about it. 진정하고 감정 추스려. 화내 봤자 소용없어.\nPull yourself together. You can’t just sit here and cry. 감정 추스려. 여기 앉아서 울고만 있을 순 없잖아."
  },
  {
    "objectID": "posts/code/2022-12-26-class-dict-yaml.html",
    "href": "posts/code/2022-12-26-class-dict-yaml.html",
    "title": "👩‍💻class ⟷ dict ⟷ yaml",
    "section": "",
    "text": "Config Class\n기준이 되는 부모 class AConfig\n\n자식 class가 같은 하위 class에 override 할 때는 이름을 바꾸지 않음\n부모 class에 없었던 하위 class를 만들 경우 자식Class의 이름(e.g. B, C) 활용\n대문자는 Config 객체의 ID\n소문자는 한개의 Config 클래스 객체 내에서의 하위 class 순서\n\n\nclass AConfig():\n    class A_a_Config:\n        element01 = \"element01\"\n        element02 = True\n    class A_b_Config:\n        element03 = 0\n        element04 = [2,4,6]\n\nBConfig는\n\nAConfig의 A_a_Config를 override\nAConfig의 A_b_Config를 override 하지 않음\nB_a_Config 추가\n\n\nclass BConfig(AConfig):\n    class A_a_Config:\n        element02 = False\n    # class A_b_Config:\n    #     pass\n    class B_a_Config:\n        element05 = {\"key\":\"value\"}\n\nCConfig는\n\nAConfig의 A_a_Config를 override\nAConfig의 A_b_Config를 pass로 override\nC_a_Config 추가\n\n\nclass CConfig(AConfig):\n    class A_a_Config:\n        element02 = False\n    class A_b_Config:\n        pass\n    class C_a_Config:\n        element05 = {\"key\":\"value\"}\n\nDConfig는\n\nAConfig의 A_a_Config를 인자로 받아 override\nAConfig의 A_b_Config를 인자로 받아 override\n\n\nclass DConfig(AConfig):\n    class A_a_Config(AConfig.A_a_Config):\n        element02 = False\n    class A_b_Config(AConfig.A_b_Config):\n        pass\n\nEConfig는\n\n__init__(super)로 Aconfig 상속\n\n\nclass EConfig(AConfig):\n    def __init__(super): pass\n    class E_a_Config:\n        element06 = (1,3,5)\n\n\n\nClass to Dict\nclass 객체를 dictionary로 만들기\n\ndef class_to_dict(obj) -&gt; dict:\n    if not hasattr(obj, \"__dict__\"):                # __dict__ : 클래스 객체의 속성 정보를 확인하기 위해 사용. 객체가 가진 여러가지 속성들을 딕셔너리 형태로 편하게 확인\n        return obj\n    result = {}                                     # 빈 dict 객체 만들기\n    for key in dir(obj):                            # dir(): 어떤 객체를 인자로 넣어주면 해당 객체가 어떤 변수와 메소드(method)를 가지고 있는지 나열\n        if key.startswith(\"_\"):                     # startswith: str.startswith(str or tuple) 형식으로 사용하면 되고, 반환 값으로는 True, False를 반환\n            continue\n        element = []                                # 리스트 요소에 대비\n        val = getattr(obj, key)                     # getattr: object에 존재하는 속성의 값을 가져옴\n        if isinstance(val, list):                   # isinstance로 list 객체인지 확인 e.g.) isinstance(object, type)\n            for item in val:\n                element.append(class_to_dict(item))\n        else:                                       # 리스트가 아닌 모든 요소들 처리\n            element = class_to_dict(val)\n        result[key] = element                       # 결과 dict에 저장\n    return result\n\n\nAConfigDict = class_to_dict(AConfig)\nAConfigDict\n\n{'A_a_Config': {'element01': 'element01', 'element02': True},\n 'A_b_Config': {'element03': 0, 'element04': [2, 4, 6]}}\n\n\n\nBConfigDict = class_to_dict(BConfig)\nBConfigDict\n\n{'A_a_Config': {'element02': False},\n 'A_b_Config': {'element03': 0, 'element04': [2, 4, 6]},\n 'B_a_Config': {'element05': {'key': 'value'}}}\n\n\n\nCConfigDict = class_to_dict(CConfig)\nCConfigDict\n\n{'A_a_Config': {'element02': False},\n 'A_b_Config': {},\n 'C_a_Config': {'element05': {'key': 'value'}}}\n\n\n\nDConfigDict = class_to_dict(DConfig)\nDConfigDict\n\n{'A_a_Config': {'element01': 'element01', 'element02': False},\n 'A_b_Config': {'element03': 0, 'element04': [2, 4, 6]}}\n\n\n\nEConfigDict = class_to_dict(EConfig)\nEConfigDict\n\n{'A_a_Config': {'element01': 'element01', 'element02': True},\n 'A_b_Config': {'element03': 0, 'element04': [2, 4, 6]},\n 'E_a_Config': {'element06': (1, 3, 5)}}\n\n\n\n\n\nDict to Yaml\n\nimport yaml\nfrom pprint import pprint\n\n\nAConfigDictYaml = yaml.dump(AConfigDict)\nprint(AConfigDictYaml)\nprint(type(AConfigDictYaml))\n\nA_a_Config:\n  element01: element01\n  element02: true\nA_b_Config:\n  element03: 0\n  element04:\n  - 2\n  - 4\n  - 6\n\n&lt;class 'str'&gt;\n\n\n\n\nYaml to Dict\n\nAYamlDict = yaml.safe_load(AConfigDictYaml)\npprint(AYamlDict)\n\n{'A_a_Config': {'element01': 'element01', 'element02': True},\n 'A_b_Config': {'element03': 0, 'element04': [2, 4, 6]}}\n\n\n\n\nDict to Class\n\n속성값으로 만들어주는 것이기 때문에 내부 메소드나 내부 클래스가 아닌, 내부 변수와 같음\n\n\nclass Dict2Class(object):\n    def __init__(self, input_dict):\n        for key in input_dict:\n            print(\"Set arribute: \", key)\n            setattr(self, key, input_dict[key]) # object에 존재하는 속성의 값을 바꾸거나, 새로운 속성을 생성하여 값을 부여\n\n\nAYamlDictClass = Dict2Class(AYamlDict)\n\nSet arribute:  A_a_Config\nSet arribute:  A_b_Config\n\n\n\nAYamlDictClass.A_a_Config\n\n{'element01': 'element01', 'element02': True}\n\n\n\nAYamlDictClass.A_b_Config\n\n{'element03': 0, 'element04': [2, 4, 6]}"
  },
  {
    "objectID": "posts/code/2025-06-09-ros2-pub-tip.html",
    "href": "posts/code/2025-06-09-ros2-pub-tip.html",
    "title": "👩‍💻ROS2 joint_state_publiser 디폴트 동작 제어하기",
    "section": "",
    "text": "ROS2 애플리케이션에서 URDF 기반 로봇 모델 시각화 시, joint_state_publisher 노드는 기본적으로 정의된 모든 관절의 위치를 0으로 퍼블리시합니다. 하지만 실제 센서나 외부 노드에서 들어오는 관절 상태만 사용하고 싶을 때도 있는데, 단순 토픽 리매핑(remapping)만으로는 이 기본 동작을 해제할 수 없습니다.\n이번 포스팅에서는:\n등을 살펴보며, 가장 깔끔하게 외부 JointState 메시지만 반영하는 설정법을 자세히 알아봅니다.\n두 가지 방식의 차이를 정리하면 다음과 같습니다:"
  },
  {
    "objectID": "posts/code/2025-06-09-ros2-pub-tip.html#remapping만으로는-디폴트-퍼블리시를-끌-수-없는-이유",
    "href": "posts/code/2025-06-09-ros2-pub-tip.html#remapping만으로는-디폴트-퍼블리시를-끌-수-없는-이유",
    "title": "👩‍💻ROS2 joint_state_publiser 디폴트 동작 제어하기",
    "section": "3.1 remapping만으로는 디폴트 퍼블리시를 끌 수 없는 이유",
    "text": "3.1 remapping만으로는 디폴트 퍼블리시를 끌 수 없는 이유\n\npublish_default_positions 기본값이 True\n\njoint_state_publisher 문서에 따르면, 파라미터 publish_default_positions 의 기본값은 True 이고, 이 경우 “외부에서 들어오는 메시지가 없으면 URDF에 정의된 모든 관절 위치를 디폴트(0값)로 퍼블리시” 합니다.\n\n리매핑(remapping)만으로는 이 내부 동작을 변경 불가\n\n토픽 remapping(('sim_joint_states', '/cmd_joint_states') 등)을 적용해도, 노드는 여전히 내부적으로 “디폴트 위치 퍼블리시”를 수행합니다.\n실제 사례로, Robotics Stack Exchange 답변에서도 “publish_default_positions 를 False 로 설정하지 않으면 JSP는 (source_list 유무와 상관없이) 항상 모든 관절의 값을 퍼블리시한다” 고 명시하고 있습니다."
  },
  {
    "objectID": "posts/code/2025-06-09-ros2-pub-tip.html#해결책-publish_default_positions-옵션-사용",
    "href": "posts/code/2025-06-09-ros2-pub-tip.html#해결책-publish_default_positions-옵션-사용",
    "title": "👩‍💻ROS2 joint_state_publiser 디폴트 동작 제어하기",
    "section": "3.2 해결책: publish_default_positions 옵션 사용",
    "text": "3.2 해결책: publish_default_positions 옵션 사용\n\n파라미터 설정\npublish_default_positions: False\nsource_list: ['cmd_joint_states']\n\n이렇게 하면 /cmd_joint_states 로부터만 JointState 메시지를 받아 /sim_joint_states 로 내보내며, 디폴트(0값) 퍼블리시는 완전히 끌 수 있습니다.\n\nROS 2 런치 예시\nNode(\n  package='joint_state_publisher',\n  executable='joint_state_publisher',\n  name='joint_state_publisher',\n  output='screen',\n  parameters=[\n    {'use_sim_time': False},\n    {'publish_default_positions': False},\n    {'source_list': ['cmd_joint_states']},\n  ],\n),\n\n이렇게 publish_default_positions 를 False 로 명시적으로 꺼야만, remapping만으로는 제어할 수 없는 기본 퍼블리시 동작을 비활성화할 수 있습니다."
  },
  {
    "objectID": "posts/code/2022-12-14-gpu-status.html",
    "href": "posts/code/2022-12-14-gpu-status.html",
    "title": "👩‍💻Linux GPU 상태 확인하기",
    "section": "",
    "text": "1. nvidia-smi\nwatch -d -n 0.5 nvidia-smi\n\nwatch : 명령어를 주기적으로 실행\n-d : 차이를 보여줌\n-n : 주기적으로 실행할 시간 간격\n\n\n\n2. gpustat\nsudo apt install gpustat\ngpustat -i\noptions\n--color : Force colored output (even when stdout is not a tty)\n--no-color : Suppress colored output\n-u, --show-user : Display username of the process owner\n-c, --show-cmd : Display the process name\n-p, --show-pid : Display PID of the process\n-P, --show-power : Display GPU power usage and/or limit (draw or draw,limit)\n-i, --interval : Run in watch mode (equivalent to watch gpustat) if given. Denotes interval between updates.\n--json : JSON Output (Experimental, #10)\n\n\n3. gpumonitor\n\nGithub mountassir/gmonitor에서 설치방법 확인\n\n$ cd gmonitor\n$ mkdir build\n$ cd build\n$ cmake ..\n$ make\n$ sudo make install\n\\# use default\ngmonitor\n\n\\# Monitor the most recent state only\ngmonitor -d 1\n\n\\# Monitor current and history states for 4 GPUs.\ngmonitor -d 0 -g 0123\n\n\\# Monitor both current and previous states for all GPUs, refresh every 3 seconds.\ngmonitor -d 0 -r 3\n\n\n4. glance\n\n설치 sudo apt-get install -y python-pip; sudo pip install glances[gpu]\n실행 sudo glances\n\n\n\n[+] Jupyter Lab에서 GPU 상태 확인하기\n\nhttps://developer.nvidia.com/blog/gpu-dashboards-in-jupyter-lab/\n\n$ pip install jupyterlab-nvdashboard\n\n# If you are using Jupyter Lab 2 you will also need to run\n$ jupyter labextension install jupyterlab-nvdashboard"
  },
  {
    "objectID": "posts/code/2023-04-23-orbit-existing-scripts.html",
    "href": "posts/code/2023-04-23-orbit-existing-scripts.html",
    "title": "👩‍💻Orbit Existing Scripts",
    "section": "",
    "text": "Orbit은 IsaacLab으로 변경되었습니다.\nOrbit 시리즈의 두번째 포스팅으로 이번에는 Orbit에서 제공하는 scripts를 살펴보며 Orbit으로 어떤 프로그래밍을 할 수 있을지 살펴보겠습니다. 공식 Documents에서 Running existing scripts를 따라가보며 진행될 예정입니다."
  },
  {
    "objectID": "posts/code/2023-04-23-orbit-existing-scripts.html#modules",
    "href": "posts/code/2023-04-23-orbit-existing-scripts.html#modules",
    "title": "👩‍💻Orbit Existing Scripts",
    "section": "2.1 Modules",
    "text": "2.1 Modules\nOrbit이 기반하고 있는 Isaac Sim Simulator을 먼저 키기 위해 SimulationApp class를 이용하여 시뮬레이터 앱을 설정해줍니다. 이때 headless는 시뮬레이터의 GUI를 띄우지 않고 실행하는 옵션을 가리킵니다.\n\"\"\"Launch Isaac Sim Simulator first.\"\"\"\n\nfrom omni.isaac.kit import SimulationApp\n\nimport argparse\n# add argparse arguments\nparser = argparse.ArgumentParser(\"Welcome to Orbit: Omniverse Robotics Environments!\")\nparser.add_argument(\"--headless\", action=\"store_true\", default=False, help=\"Force display off at all times.\")\nargs_cli = parser.parse_args()\n\n# launch omniverse app\nconfig = {\"headless\": args_cli.headless}\nsimulation_app = SimulationApp(config)\n다음으로 import하는 orbit의 core모듈을 살펴보겠습니다.\n\nprim_utils : 현재 USD stage에 prim을 생성하기 위한 모듈. (자세한 prim에 대한 내용은 다음 포스팅에서 다루도록 하겠습니다. 우선 간단하게 시뮬레이터의 오브젝트들을 prim으로 본다고 생각하고 진행하겠습니다.)\nSimulationContext : 타임라인 관련 이벤트를 처리하고(simulator 일시 중지, 재생, 단계적 실행 또는 중지 등), stage를 구성하며(stage 단위나 상/하 방향과 같은 설정), physicsScene prim을 생성합니다(중력 방향 및 크기, 시뮬레이션 시간 간격 크기, 고급 솔버 알고리즘 설정과 같은 물리 시뮬레이션 매개 변수를 제공). 여기서 physicsScene prim은 물리 시뮬레이션을 위한 초기화 및 설정을 담당하는 객체를 말하며, SimulationContext는 이러한 physicsScene prim을 생성하고 물리 시뮬레이션 설정을 관리하며, 타임라인 관련 이벤트 처리와 스테이지 설정을 담당합니다.\nset_camera_view : stage에서 카메라 prim의 위치와 대상을 설정하고, 해당 prim의 경로를 지정합니다. 여기서 카메라 prim은 stage 카메라로 사용될 객체를 의미하며, 위치(location)와 대상(target)은 각각 카메라의 위치와 카메라가 바라보는 대상의 위치를 지정하는 것을 의미합니다. 경로(path)는 스테이지에서 해당 카메라 prim을 가리키는 이름이나 경로를 의미합니다.\n\n이외의 모듈중에서 kit_utils는 시뮬레이터에서 제공하는 ground를 불러오기 위한(create_ground_plane) 모듈이며 로봇의 발 위치 마커를 표시하기 위해 markers 모듈에서 PointMarker, StaticMarker를 불러옵니다.\n다음으로 robot 모듈에서 4족보행로봇들을 위한 configuration들을 불러올 수 있습니다.\n# from core\nimport omni.isaac.core.utils.prims as prim_utils\nfrom omni.isaac.core.simulation_context import SimulationContext\nfrom omni.isaac.core.utils.viewports import set_camera_view\n\nimport omni.isaac.orbit.utils.kit as kit_utils\nfrom omni.isaac.orbit.markers import PointMarker, StaticMarker\n\nfrom omni.isaac.orbit.robots.config.anymal import ANYMAL_B_CFG, ANYMAL_C_CFG\nfrom omni.isaac.orbit.robots.config.unitree import UNITREE_A1_CFG\nfrom omni.isaac.orbit.robots.legged_robot import LeggedRobot"
  },
  {
    "objectID": "posts/code/2023-04-23-orbit-existing-scripts.html#helpers",
    "href": "posts/code/2023-04-23-orbit-existing-scripts.html#helpers",
    "title": "👩‍💻Orbit Existing Scripts",
    "section": "2.2 Helpers",
    "text": "2.2 Helpers\nMain 코드에서 시뮬레이터의 Scene을 구성하기 위해서 Ground-plane, Lights들을 편하게 구성하기 위해서 helper 함수 design_scene()를 만들어줍니다. 앞서 import했던 kit_utils를 이용해서 ground를 불러오고 prim_utils를 이용하여 빛 설정을 해줍니다.\ndef design_scene():\n    \"\"\"Add prims to the scene.\"\"\"\n    # Ground-plane\n    kit_utils.create_ground_plane(\n        \"/World/defaultGroundPlane\",\n        static_friction=0.5,\n        dynamic_friction=0.5,\n        restitution=0.8,\n        improve_patch_friction=True,\n    )\n    # Lights-1\n    prim_utils.create_prim(\n        \"/World/Light/GreySphere\",\n        \"SphereLight\",\n        translation=(4.5, 3.5, 10.0),\n        attributes={\"radius\": 2.5, \"intensity\": 600.0, \"color\": (0.75, 0.75, 0.75)},\n    )\n    # Lights-2\n    prim_utils.create_prim(\n        \"/World/Light/WhiteSphere\",\n        \"SphereLight\",\n        translation=(-4.5, 3.5, 10.0),\n        attributes={\"radius\": 2.5, \"intensity\": 600.0, \"color\": (1.0, 1.0, 1.0)},\n    )"
  },
  {
    "objectID": "posts/code/2023-04-23-orbit-existing-scripts.html#main",
    "href": "posts/code/2023-04-23-orbit-existing-scripts.html#main",
    "title": "👩‍💻Orbit Existing Scripts",
    "section": "2.3 Main",
    "text": "2.3 Main\n메인 코드에서는 먼저 SimulationContext를 이용하여 시뮬레이터의 시간관련 설정 등을 진행합니다. dt가 0.005라는 것은 시간 단위가 second로 시간간격이 0.005초로 설정하는 것을 말합니다. 또한 backend는 이후 포스팅에서도 설명하겠지만 물리 시뮬레이터에서 반환되는 tensor들을 어떤 backend로 캐스팅할 것인지 설정하는 부분입니다. 현재 orbit에서는 torch만 지원하고 있습니다.\n시뮬레이터의 시점이 되는 카메라를 설정하고 각 로봇을 어디에 놓을지 정하는 translation 파라미터와 함께 spawning하고 마지막으로 helper 함수로 만들어주었던 design_scene()을 이용하여 ground와 light를 설정합니다.\n\"\"\"Imports all legged robots supported in Orbit and applies zero actions.\"\"\"\n\n# Load kit helper\nsim = SimulationContext(stage_units_in_meters=1.0, physics_dt=0.005, rendering_dt=0.005, backend=\"torch\")\n# Set main camera\nset_camera_view(eye=[3.5, 3.5, 3.5], target=[0.0, 0.0, 0.0])\n\n# Spawn things into stage\n# -- anymal-b\nrobot_b = LeggedRobot(cfg=ANYMAL_B_CFG)\nrobot_b.spawn(\"/World/Anymal_b/Robot_1\", translation=(0.0, -1.5, 0.65))\nrobot_b.spawn(\"/World/Anymal_b/Robot_2\", translation=(0.0, -0.5, 0.65))\n# -- anymal-c\nrobot_c = LeggedRobot(cfg=ANYMAL_C_CFG)\nrobot_c.spawn(\"/World/Anymal_c/Robot_1\", translation=(1.5, -1.5, 0.65))\nrobot_c.spawn(\"/World/Anymal_c/Robot_2\", translation=(1.5, -0.5, 0.65))\n# -- unitree a1\nrobot_a = LeggedRobot(cfg=UNITREE_A1_CFG)\nrobot_a.spawn(\"/World/Unitree_A1/Robot_1\", translation=(1.5, 0.5, 0.42))\nrobot_a.spawn(\"/World/Unitree_A1/Robot_2\", translation=(1.5, 1.5, 0.42))\n\n# design props\ndesign_scene()\n시뮬레이터를 초기화하는 reset을 먼저 진행해줍니다. 각 로봇의 handle 또한 초기화를 시켜줍니다. 로봇의 각 정보를 담기 위한 buffer도 reset을 해서 본격적인 실행을 준비합니다.\n# Play the simulator\nsim.reset()\n# Acquire handles\n# Initialize handles\nrobot_b.initialize(\"/World/Anymal_b/Robot.*\")\nrobot_c.initialize(\"/World/Anymal_c/Robot.*\")\nrobot_a.initialize(\"/World/Unitree_A1/Robot.*\")\n# Reset states\nrobot_b.reset_buffers()\nrobot_c.reset_buffers()\nrobot_a.reset_buffers()\n4족 보행 로봇은 제어를 할 때 발의 움직임이 매우 중요합니다. 발의 위치에 마커를 위치시켜서 발에 대한 정보를 얻기 위해 marker를 설정해줍니다. 발의 x, y, z 축을 시각화하기 위해서 StaticMarker를 이용하여 설정하고 발이 contact 포인트를 보기 위해 PointMarker를 설정합니다.\n# Debug visualization markers.\n# -- feet markers\nfeet_markers: List[StaticMarker] = list()\nfeet_contact_markers: List[PointMarker] = list()\n# iterate over robots\nfor robot_name in [\"Anymal_b\", \"Anymal_c\", \"Unitree_A1\"]:\n    # foot\n    marker = StaticMarker(f\"/World/Visuals/{robot_name}/feet\", 4 * robot_c.count, scale=(0.1, 0.1, 0.1))\n    feet_markers.append(marker)\n    # contact\n    marker = PointMarker(f\"/World/Visuals/{robot_name}/feet_contact\", 4 * robot_c.count, radius=0.035)\n    feet_contact_markers.append(marker)\n각 로봇은 action을 하게되고 이를 로봇을 제어한다고 볼 수 있습니다. get_default_dof_state()은 로봇의 각 객체에 있는 method로 각 로봇의 standing 자세에 대한 joint position(dof) 정보가 들어있습니다.\n# dummy action\nactions = torch.zeros(robot_a.count, robot_a.num_actions, device=robot_a.device)\n\n# Define simulation stepping\nsim_dt = sim.get_physics_dt()\nsim_time = 0.0\ncount = 0\n# Simulate physics\nwhile simulation_app.is_running():\n    # If simulation is stopped, then exit.\n    if sim.is_stopped():\n        break\n    # If simulation is paused, then skip.\n    if not sim.is_playing():\n        sim.step(render=not args_cli.headless)\n        continue\n    # reset\n    if count % 1000 == 0:\n        # reset counters\n        sim_time = 0.0\n        count = 0\n        # reset dof state\n        for robot in [robot_a, robot_b, robot_c]:\n            dof_pos, dof_vel = robot.get_default_dof_state()\n            robot.set_dof_state(dof_pos, dof_vel)\n            robot.reset_buffers()\n        # reset command\n        actions = torch.zeros(robot_a.count, robot_a.num_actions, device=robot_a.device)\n        print(\"&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Reset!\")\n    # apply actions\n    robot_b.apply_action(actions)\n    robot_c.apply_action(actions)\n    robot_a.apply_action(actions)\n    # perform step\n    sim.step()\n    # update sim-time\n    sim_time += sim_dt\n    count += 1\n시뮬레이터 초기화할 때 reset했던 buffer에 각 로봇의 정보를 담습니다. marker로 설정해두었던 foot_marker와 contact_marker도 각 로봇에서 불러와서 state update를 진행합니다.\n# note: to deal with timeline events such as stopping, we need to check if the simulation is playing\nif sim.is_playing():\n    # update buffers\n    robot_b.update_buffers(sim_dt)\n    robot_c.update_buffers(sim_dt)\n    robot_a.update_buffers(sim_dt)\n    # update marker positions\n    for foot_marker, contact_marker, robot in zip(\n        feet_markers, feet_contact_markers, [robot_b, robot_c, robot_a]\n    ):\n        # feet\n        foot_marker.set_world_poses(\n            robot.data.feet_state_w[..., 0:3].view(-1, 3), robot.data.feet_state_w[..., 3:7].view(-1, 4)\n        )\n        # contact sensors\n        contact_marker.set_world_poses(\n            robot.data.feet_state_w[..., 0:3].view(-1, 3), robot.data.feet_state_w[..., 3:7].view(-1, 4)\n        )\n        contact_marker.set_status(torch.where(robot.data.feet_air_time.view(-1) &gt; 0.0, 1, 2))"
  },
  {
    "objectID": "posts/code/2020-07-20-import-custom-module.html",
    "href": "posts/code/2020-07-20-import-custom-module.html",
    "title": "👩‍💻Import custom module",
    "section": "",
    "text": "실제 폴더와 파일들은 아래와 같다.\n\n\n\ngym_foo folder에 반드시 __init__.py를 만들어야 한다.\n\n\n\n파일 구조"
  },
  {
    "objectID": "posts/code/2020-07-20-import-custom-module.html#structure",
    "href": "posts/code/2020-07-20-import-custom-module.html#structure",
    "title": "👩‍💻Import custom module",
    "section": "",
    "text": "실제 폴더와 파일들은 아래와 같다.\n\n\n\ngym_foo folder에 반드시 __init__.py를 만들어야 한다.\n\n\n\n파일 구조"
  },
  {
    "objectID": "posts/code/2020-07-20-import-custom-module.html#code",
    "href": "posts/code/2020-07-20-import-custom-module.html#code",
    "title": "👩‍💻Import custom module",
    "section": "Code",
    "text": "Code\n\nutils_foo.py\n\ndef utils_test():\n    print(\"utils_foo\")\n\nprint(\"HERE: utils_foo\")\n\nHERE: utils_foo\n\n\nenv_foo.py\nfrom gym_foo import utils_foo\n\nutils_foo.utils_test()\n\nprint(\"HERE: env_foo\")\n\ndef env_test():\n    print(\"env_foo\") \nmain_foo.py\nfrom gym_foo import utils_foo\nfrom gym_foo import env_foo\n\nutils_foo.utils_test()\nenv_foo.env_test()"
  },
  {
    "objectID": "posts/code/2020-07-20-import-custom-module.html#goal",
    "href": "posts/code/2020-07-20-import-custom-module.html#goal",
    "title": "👩‍💻Import custom module",
    "section": "Goal",
    "text": "Goal\n\n실행 파일: main_foo.py\nimport하는 파일: env_foo.py\nimport하는 파일 내에서(=env_foo.py) import하는 파일: utils_foo.py"
  },
  {
    "objectID": "posts/code/2020-07-20-import-custom-module.html#how",
    "href": "posts/code/2020-07-20-import-custom-module.html#how",
    "title": "👩‍💻Import custom module",
    "section": "How",
    "text": "How\n\nmain_foo.py에서 env_foo.py를 import한다.\nenv_foo.py에서 utils_foo.py를 import 한다.\n이때 env_foo.py에서 import utils_foo로 utils_foo를 불러오면,\n\npython env_foo.py 실행시 잘 작동되지만(같은 위치)\npython main_foo.py 실행에서는 from gym_foo import *코드를 읽을 때 env_foo.py내의 from gym_foo import utils_foo를 불러올 수 없다고 error가 난다.(상위 위치)\n\n$ python main_foo.py \nTraceback (most recent call last):\nFile \"main_foo.py\", line 4, in &lt;module&gt;\n    env_foo.env_test()\nNameError: name 'env_foo' is not defined\nenv_foo.py에서 utils_foo module을 불러올 때, from gym_foo import utils_foo로 불러온다. 상위 위치인 gym_foo를 거쳐서 import해야한다는 뜻이다. 그러면 python main_foo.py 실행시 잘 작동한다.\n$ python main_foo.py \nHERE: utils_foo\nutils_foo\nHERE: env_foo\nutils_foo\nenv_foo\n한 가지 더 주의해야 할 점이 있다. main_foo.py에서 utils_foo와 env_foo를 import 할 때이다.\nfrom gym_foo import * 코드로 utils_foo와 env_foo가 모두 불러와질 것이라고 생각했으나, main_foo.py를 실행했을 때 import 하지 못한다. 따라서 위에 main_foo.py에서 볼 수 있듯이 from gym_foo import utils_foo, from gym_foo import env_foo각각 따로 import 해줘야 한다."
  },
  {
    "objectID": "posts/code/2024-01-07-quarto-blog.html",
    "href": "posts/code/2024-01-07-quarto-blog.html",
    "title": "👩‍💻Quarto Blog 기본 셋팅",
    "section": "",
    "text": "이번 포스팅은 Github Blog를 만들어 갔던 경험과 그 경험들을 통해 Quarto라는 좋은 툴을 소개하기 위한 글로 준비를 했습니다. 본격적인 Quarto로 속편한 Github Blog를 구축하는 방법을 소개하기 전에 어떻게 Quarto를 알게 되었고 적극적으로 소개하게 되었는지 이야기를 드릴려고 합니다.\n이전에 글또 7기를 시작하는 다짐글에서 잠깐 기술 블로그 방황기에 대해 언급을 한 적이 있었습니다. 하지만 기술 블로그를 작성해보고 싶은 계기부터 잠깐 이야기해보려고 합니다. 학부 전공은 기계공학과인지라 컴퓨터, IT 관련글들을 볼 기회가 많았던 것은 아니지만 어쩌다보니 AI, 인공지능의 매력에 빠져서 공부를 하다보니 책보다는 구글링과 유튜브로 많은 지식들을 배우게 되는 경험들이 자연스럽게 기술 블로그를 작성하는 문화에 빠져들게 되었습니다. 저와 같이 아무것도 모르는 사람들을 위해 친절한 설명들과 단계별 캡쳐 사진들을 따라가다보면 원하는 프로그램을 실행시킬 수 있었고, 때론 내가 막혀있던 점들을 푼 이전의 누군가의 간단히라도 적어둔 팁을 확인하고 도움을 얻을 수 있었기 때문에 잘 키워가는 기술 블로그 하나 열 책 안 부러울 정도로 기술 블로그에 대한 막연한 동경이 있었습니다.\n그러다가 어느 정도 공부를 하다보니 \"나도 한번 내 기술 블로그를 운영해볼까?\"라는 호기가 생겼습니다.\n시작도 하기 전에 (살짝은) 비장했던 마음도 있었고 설렜던 마음도 있었습니다. 아직 어떻게 블로그를 만들지 생각도 없었지만 나도 무언가를 아카이빙하고 그 글들이 다른 많은 사람들에게 도움이 되면 좋지 않을까라고 생각하며 유명해지면 어떨까..?라는 행복회로를 돌리며 다들 어떻게 기술 블로그를 만드는지 찾아보기 시작했습니다. 나름 AI 공부를 하며 코딩을 할 줄 아니까 Markdown까지는 문서를 작성하는 규칙이니 쉽게 적응을 했지만, 웹 개발은 정말 잘 모르는 분야에다가 웹 개발 공부에 시간을 투자하면서 까지 Github 블로그를 만들 수는 없었기에 눈 앞이 깜깜해지는 기분이었습니다.\n물론..! 이전에 Hugo, Notion 개인 도메인, Velog 등등 여러 블로그 구축 방법들도 있었지만 플랫폼마다 커스텀하기가 어려운 점도 있었고 새로운 언어를 새롭게 공부해야 하는 경우들도 있었기 때문에, 저는 최대한 블로그의 내가 쓸 내용 Contents에 집중할 수 있도록 내용이 Compile/Rendering 되는 건 알아서 해주면 좋겠다는 바람이 있었습니다. (왜 Github Blog를 선택했는지는 여러 조건과 개인적 선호를 반영한 내용이 있으니 이전글을 참고해주세요!) 그러던 중에 FastAI에서 딥러닝 기술 블로그를 가장 심플하고 빠르게 작성할 수 있는 FastPages를 만들었다는 걸 알게 되었고 이 FastPages를 활용해서 잘 작성하고 있었습니다. FastPages의 최대 장점은 AI에서 많이 사용하는 Jupyter Notebook 형식의 파일을 그대로 포스팅을 할 수 있는 기능 등을 통해 원래 제가 원했던 점인 정말 Contents에 집중할 수 있는 환경을 만들어주는 Build 프로그램이었기에 정말 마음에 쏙 들었고 그대로 기술 블로그를 만들었습니다.\n그렇게 이제는 더이상 블로그 이사를 안할 줄 알았지만..(블로그 이사는 정말 품이 많이 드는 것 같아요🥲) fastpages repository에 청천벽력과 같은 공지가 붙게 되었습니다.\n아예 Fastpages 서비스를 닫아서 블로그가 아예 안열리는 것은 아니었지만, 블로그 지속성을 위해 마지막 블로그 이사를 Quarto로 하게 되었고 결과적으로는 Fastpages보다 Quarto가 훨씬 사용하기도 편하고 적당한 자유도가 있는 플랫폼이라는 생각이 들어서 오늘 이 포스팅을 통해 많은 분들께 알리고 싶어서 글을 작성하게 되었습니다. 생각보다 많은 분들이 알고 계시지 않은 것 같아 앞으로 주변에 더더욱 열심히 홍보하려고 합니다. Quarto를 중심으로 VSCode(+Copliot), Typora를 사용하면 더욱 쾌적한 블로그를 만들 수 있기 때문에 Quarto를 메인으로 다른 프로그램들은 가볍게 서포트 프로그램이라고 생각해주시면 될 것 같습니다.\n그럼 프로그램들을 하나씩 살펴본 후에 Github repository를 하나 만들어서 블로그(Website)를 하나 만들어보는 실습까지 가보도록 하겠습니다!"
  },
  {
    "objectID": "posts/code/2024-01-07-quarto-blog.html#repository-하나-만들기",
    "href": "posts/code/2024-01-07-quarto-blog.html#repository-하나-만들기",
    "title": "👩‍💻Quarto Blog 기본 셋팅",
    "section": "1. Repository 하나 만들기",
    "text": "1. Repository 하나 만들기\n블로그를 만들 Repository를 Github에 하나 만들어 줍니다. Repository는 어떤 이름을 가지고 있어도 상관없지만 보통 Github에서 개인 블로그 주소를 본인의 nickname.github.io로 만들기 때문에 저는 아래와 같이 블로그를 위한 Repository를 만들었습니다.\n\n\n\n다음으로 해당 Repository의 Settings에 들어가서 블로그의 웹페이지들이 랜더링될 폴더를 /docs로 설정하는 과정을 아래와 같이 설정해줍니다.\n\n\n\n다음으로 해당 repository를 원하는 경로에 clone 해서 블로그를 작성할 폴더를 준비합니다."
  },
  {
    "objectID": "posts/code/2024-01-07-quarto-blog.html#quarto-project-시작",
    "href": "posts/code/2024-01-07-quarto-blog.html#quarto-project-시작",
    "title": "👩‍💻Quarto Blog 기본 셋팅",
    "section": "2. Quarto Project 시작",
    "text": "2. Quarto Project 시작\n이제 VSCode를 열어서 확장 프로그램에서 Quarto를 검색한 후 해당 확장 프로그램을 VSCode에 깔아줍니다.\n\n\n\nVSCode에서 Ctrl+Shift+P 단축키로 명령어 팔레트를 열어서 Qaurto:Create Project를 실행합니다.\n\n\n\n출처: Quarto 공식 홈페이지\n\n\nBlog Porject를 선택합니다. 이때 프로젝트 이름은 아무거나 입력헤도 됩니다. 프로젝트를 생성한 후, 프로젝트 하위에 있는 모든 파일들을 Github repository 이름과 동일한 폴더로 옮겨줍니다.\n\n\n\n출처: Quarto 공식 홈페이지\n\n\n그러면 Github Repository의 이름이 test_blog라고 했을 때 아래와 같이 폴더와 파일들의 구성이 되는 것을 확인할 수 있습니다.\n\n\n\nBlog project 시작"
  },
  {
    "objectID": "posts/code/2024-01-07-quarto-blog.html#default-blog-localhost로-확인",
    "href": "posts/code/2024-01-07-quarto-blog.html#default-blog-localhost로-확인",
    "title": "👩‍💻Quarto Blog 기본 셋팅",
    "section": "3. Default Blog localhost로 확인",
    "text": "3. Default Blog localhost로 확인\n이제 VSCode에서 Blog Repository(test_blog)가 열린 프로젝트 창에서 터미널을 열어서 아래의 명령어를 입력해봅니다.\nquarto preview\n\n\n\n그러면 Default로 생성된 블로그 페이지가 localhost로 열리는 것을 확인할 수 있습니다.\n\n\n\nDefault Blog"
  },
  {
    "objectID": "posts/code/2024-01-07-quarto-blog.html#blog-publish",
    "href": "posts/code/2024-01-07-quarto-blog.html#blog-publish",
    "title": "👩‍💻Quarto Blog 기본 셋팅",
    "section": "Blog Publish",
    "text": "Blog Publish\n이전 단계에서는 localhost로 페이지를 랜더링해서 확인한 것이기 때문에 publish가 되지 않았습니다. quarto에서 프로젝트를 rendering하기 위해서는 publish를 하는 명령어를 실행해야 합니다.\n하지만 그전에!\npage들을 publish할 directory를 /docs로 따로 설정을 해주었기 때문에 해당 폴더에 랜더링된 html 파일들을 만들 수 있도록 아래와 같이 ._quarto.yml 설정파일을 아래와 같이 작성해줍니다.\nproject:\n  type: website\n  output-dir: docs\n\n\n\n위와 같이 render이 잘 완료되었다는 메세지가 확인이 되면 아래의 명령어를 실행하여 publish 합니다.\nquarto render\n마지막으로 Github에 Push를 하게 되면 이제 website를 확인할 수 있습니다!(build되는데 약 2-3분 걸릴 수 있습니다.)\n이번에는 Quarto Engine을 이용해서 간단하게 Blog Website를 올리는 부분을 진행했습니다. 좀 더 본격적으로 조정할 수 있는 옵션들에 대해서는 다음 포스팅에서 살펴볼 예정입니다.\n\nReference\n\nQuarto Homepage\nTypora Homepage"
  },
  {
    "objectID": "posts/code/2025-06-09-ros2-joint-state-trajectory.html",
    "href": "posts/code/2025-06-09-ros2-joint-state-trajectory.html",
    "title": "👩‍💻ROS2 JointState vs. JointTrajectory",
    "section": "",
    "text": "ROS에서 sensor_msgs/JointState와 trajectory_msgs/JointTrajectory는 목적과 사용 시점이 뚜렷하게 다릅니다.\n📌 비교 요약"
  },
  {
    "objectID": "posts/code/2025-06-09-ros2-joint-state-trajectory.html#상태-피드백용-jointstate",
    "href": "posts/code/2025-06-09-ros2-joint-state-trajectory.html#상태-피드백용-jointstate",
    "title": "👩‍💻ROS2 JointState vs. JointTrajectory",
    "section": "1.1 1. 상태 피드백용 – JointState",
    "text": "1.1 1. 상태 피드백용 – JointState\n\n하드웨어나 Gazebo 플러그인이 조인트 센서 데이터를 실시간으로 발행\n퍼블리셔 예: joint_states 토픽\n소비처: robot_state_publisher/JointStatePublisher, TF 시스템, Rviz, 모니터링 툴 등"
  },
  {
    "objectID": "posts/code/2025-06-09-ros2-joint-state-trajectory.html#궤적-명령용-jointtrajectory",
    "href": "posts/code/2025-06-09-ros2-joint-state-trajectory.html#궤적-명령용-jointtrajectory",
    "title": "👩‍💻ROS2 JointState vs. JointTrajectory",
    "section": "1.2 2. 궤적 명령용 – JointTrajectory",
    "text": "1.2 2. 궤적 명령용 – JointTrajectory\n\nMoveIt! 또는 다른 경로 생성기가 계획된 조인트 궤적을 생성\n메시지 구성 예: joint_names = [“joint1”, “joint2”], points는 여러 스텝으로 구성\n컨트롤러에 전달: FollowJointTrajectory 액션 서버 호출"
  },
  {
    "objectID": "posts/code/2025-06-09-ros2-joint-state-trajectory.html#제어-사이클-내-캡처",
    "href": "posts/code/2025-06-09-ros2-joint-state-trajectory.html#제어-사이클-내-캡처",
    "title": "👩‍💻ROS2 JointState vs. JointTrajectory",
    "section": "1.3 3. 제어 사이클 내 캡처",
    "text": "1.3 3. 제어 사이클 내 캡처\n\n궤적 실행 중 컨트롤러는 JointTrajectory 명령을 받아들이고,\n실제 하드웨어는 현재 상태를 다시 JointState로 발행하여\n피드백 루프 및 모니터링 가능 (e.g. FollowJointTrajectoryFeedback)\n\n❗ 자주 묻는 실수\n\nJointState를 명령용으로 사용하는 것은 바람직하지 않습니다. 내부 상태 보고용 메시지입니다\n반대로, JointTrajectory는 단순 상태 보고용으로는 적합하지 않으며, 시간 기반 waypoint 전달용입니다.\nJointState는 시간 시리즈 데이터를 여러 번 발행할 수 있고,\nJointTrajectory는 단 한 번의 메시지로 전 궤적을 함께 전달합니다"
  },
  {
    "objectID": "posts/project/traffic_signal_od.html",
    "href": "posts/project/traffic_signal_od.html",
    "title": "Traffic Sign Detection and Recognition Task",
    "section": "",
    "text": "https://github.com/curieuxjy/traffic_sign_object_detection"
  },
  {
    "objectID": "posts/project/kompanion.html",
    "href": "posts/project/kompanion.html",
    "title": "Smart Device for Dog Triaining",
    "section": "",
    "text": "https://github.com/curieuxjy/K-ompanion"
  },
  {
    "objectID": "posts/project/quad_recovery.html",
    "href": "posts/project/quad_recovery.html",
    "title": "Deep Reinforcement Learning for Fall-Recovery Control on Non-Flat Terrain of Quadruped Robots",
    "section": "",
    "text": "tbd"
  },
  {
    "objectID": "posts/storage/2020-07-17-rl-study-roadmap.html",
    "href": "posts/storage/2020-07-17-rl-study-roadmap.html",
    "title": "🧩My Reinforcement Learning Roadmap",
    "section": "",
    "text": "강화학습을 공부했던 roadmap을 기록하고자 글을 쓰게 되었다.\n짧게 기록할 수 있는 정보들과 내 느낌들을 간략하게 적어보고자 한다."
  },
  {
    "objectID": "posts/storage/2020-07-17-rl-study-roadmap.html#my-rl-study-road-map",
    "href": "posts/storage/2020-07-17-rl-study-roadmap.html#my-rl-study-road-map",
    "title": "🧩My Reinforcement Learning Roadmap",
    "section": "My RL study road map",
    "text": "My RL study road map\n\n아래는 시간 순서대로 내가 공부했던 강의나 책, 스터디 모임\n\n\n모두의 rl\n케라스로 시작하는 rl 책 스터디\n모두연 starcraft project - 논문 리딩\nUdacity\nConnect-X\npytorch 책\n수학 강화학습 책\nOpen AI Gym\nfast campus\nBNM2h 스터디\nRL 논문 스터디 3기\nUnity Ml agent 스터디\nHugging face 강화학습 강의"
  },
  {
    "objectID": "posts/storage/2020-07-17-rl-study-roadmap.html#materials",
    "href": "posts/storage/2020-07-17-rl-study-roadmap.html#materials",
    "title": "🧩My Reinforcement Learning Roadmap",
    "section": "Materials",
    "text": "Materials\n\n무료강의\n\n\nHuggiing Face - DRL course: 허깅페이스에서 제공하는 강화학습 강의. 역시 허깅 페이스. 개인적으로 Udacity보다 더 좋은 것 같다.\n모두의 RL: 강화학습 입문용 강의. 가볍게 듣고 시작하는 것을 추천.\n팡요랩 : 강화학습 강의로 유명한 데이비드 실버 교수님 강의 한글 버전\n혁펜하임 : 수식적으로 파고 들어가고 싶다면 추천.\nCS285 : 입문이라기에 조금 어려울 수는 있으나 만약 로봇틱스쪽을 생각하고 있다면 꼭 들어야하는 강의.\nDeepMind - Reinforcement Learning Lecture Series 2021 (2022.05.06 updated)\n\n\n유료강의\n\n\nUdacity : 엄청 비쌈, 프로그램이 좋긴한데 nano degree가 목적이 아니라면 추천하지 않음.\nUdemy : 코드 위주로 빨리 배워보고 싶을 때. 어느정도 강화학습에 대한 기본 지식이 있을 때 듣는 것이 좋음.\nFast campus 박준영 강사님 : 무엇보다 한국어로 자세히 설명도 해주시고 코드도 있어서 좋음. 개인적으로 무료 강의들로 기초를 한번 다지고 난 후 이 강의를 들으면서 불명확했던 부분을 짚으면 좋은 것 같다.\n\n\n책\n\n\n파이썬과 케라스로 배우는 강화학습\nPyTorch를 활용한 강화학습/심층강화학습 실전 입문\n수학으로 풀어보는 강화학습 원리와 알고리즘 : 수학적 기반 다지기. 모델 베이스드 RL 쪽으로 내용이 좋음.\n단단한 강화학습 : 강화학습 대가 리처드 서튼 교수님의 바이블 원작을 번역.\n단단한 심층강화학습 (2022.05.06 updated)\n\n\n2022.05.06 기준 강화학습 관련된 많은 책들이 나와서 책에 대한 추천은 최근 출간된 책들을 목차들을 보고 본인에게 필요한 부분을 공부하는 것을 추천.\n\n\n웹사이트(입문으로는 조금 힘들 수 있으나 어느정도 공부한 후, 트렌드나 흐름 잡기에 좋음)\n\n\nDeepMind blog\nOpenAI blog\nOpenAI Spinning up\n\n\nGithub\n\n\nRL 가계도\nHuggingface DRL: 위에 무료강의 중 허깅페이스 강의에서 사용하는 코드들이 올라와 있음.\npg-is-all-you-need: Policy Gradient 계열의 알고리즘들 코드. 최근 25.01 기준 리펙토링 되었음\n\n\n커뮤니티\n\n\nRL Korea : 페이스북\nRL.start() : 오픈 카카오톡"
  },
  {
    "objectID": "posts/paper/2024-03-17-vcgs.html",
    "href": "posts/paper/2024-03-17-vcgs.html",
    "title": "📃VCGS 리뷰",
    "section": "",
    "text": "이번 포스팅은 Variational Constrained Grasp Sample 논문을 읽고 정리한 내용입니다. 해당 논문은 IROS 2023 학회에 Accept된 논문으로, 특정 대상 영역에 대한 제약을 가진 6자유도(DoF) Grasp을 샘플링하기 위한 새로운 생성적 그리핑 샘플링 네트워크, VCGS를 소개합니다. 뿐만 아니라 1,400만 개 이상의 훈련 샘플을 포함하는 새로운 데이터셋 CONG를 구축한 내용을 발표했습니다. 제안된 VCGS가 시뮬레이션 및 실제 테스트에서 비교 모델인 GraspNet보다 10-15% 높은 그리핑 성공률을 보이며, 2-3배 더 효율적인 것을 보여준 논문입니다."
  },
  {
    "objectID": "posts/paper/2024-03-17-vcgs.html#grasp-sampler",
    "href": "posts/paper/2024-03-17-vcgs.html#grasp-sampler",
    "title": "📃VCGS 리뷰",
    "section": "3.1 Grasp Sampler",
    "text": "3.1 Grasp Sampler\nVCGS의 Grasp Sampler의 기본적인 틀은 Conditional Variational Autoencoder (CVAE) 구조를 차용해서 아래와 같이 만들었습니다. Grasp pose를 다양하게 Sampling하기 위해 Encoder와 Decoder를 VAE 구조를 차용하여 Gaussian Prior Distribution을 이용해서 가능한 다양한 Grasp pose를 생성할 수 있도록 설계했습니다.\n\n\n\nC-VAE 구조\n\n\n\n\n\nLoss Function"
  },
  {
    "objectID": "posts/paper/2024-03-17-vcgs.html#grasp-evaluator",
    "href": "posts/paper/2024-03-17-vcgs.html#grasp-evaluator",
    "title": "📃VCGS 리뷰",
    "section": "3.2 Grasp Evaluator",
    "text": "3.2 Grasp Evaluator\n학습동안에 좋은 Grasp data만 학습하는 Encoder가 더 다양한 Grasp data를 경험할 수 있도록 Evaluator Network를 추가하여 Bad Grasp에 대한 경험도 할 수 있도록 만들었습니다.\n\n\n\nInput data 형태와 Evaluator Network"
  },
  {
    "objectID": "posts/paper/2024-03-17-vcgs.html#cong-dataset",
    "href": "posts/paper/2024-03-17-vcgs.html#cong-dataset",
    "title": "📃VCGS 리뷰",
    "section": "3.3 CONG Dataset",
    "text": "3.3 CONG Dataset\n\n\n\nCONG Dataset 구축과정\n\n\n구성 요소\n\nO: object point cloud\nG*: target area A에서 랜덤하게 샘플링된 successful grasp\n\n데이터셋 구축 과정\n\nobject를 원점에 랜덤한 orientation으로 놓고 O[N x 3] rendering\nO에서 query point I[K x 3]를 샘플링(K &lt;&lt; N) - Farthest Point Sampling 사용\n각 query point xi(∈I)에 대해서 반경 ri(~U[0, R]) 이웃한 point Ai들을 모두 찾음\n\n이때 R은 mesh bounding box의 대각선 길이\n\n[grasp center point]와 [Ai의 어떤 점]이라도 최대 d인 모든 G를 찾아냄\n\n\n\n\nmesh 데이터에서 grasp data를 추출하는 과정"
  },
  {
    "objectID": "posts/paper/2024-03-17-vcgs.html#a.-simulated-robotic-grasping",
    "href": "posts/paper/2024-03-17-vcgs.html#a.-simulated-robotic-grasping",
    "title": "📃VCGS 리뷰",
    "section": "4.1 A. Simulated Robotic Grasping",
    "text": "4.1 A. Simulated Robotic Grasping\n\nbest grasp, NOT the best reachable\ngripper와 object 둘 다 free-floating 상황\nIsaacGym simulator 사용\n\nAcronym dataset에서 123개의 random object\n물체의 observation data로는 depth sensor 사용\n\n시뮬레이터에서 2개 실험 진행\n\nUnconstrained sampling: target area 없이 그냥 grasp을 샘플링. A=O\nConstrained sampling: target area에서만 grasp 생성\n\n비교군\n\nGraspNet: SOTA\nGraspNetTaI: Target as Input. target area만 grasp sampling network에 넣어준 모델\n\n\n\n\n\n13\n\n\n\nVCGS는 GraspNet보다 3배 이상의 Ratio of grasps kept %를 보여줌\n\n네트워크 입력으로 Constrained grasp sampling을 넣어주는 것의 이점에 대한 증거\n\nGraspNetTaI는 GraspNet보다 Success Rate가 낮음\n\n물체의 전체 정보(global)를 사용하는 것이 특정한 target area에 대한 정보(local)를 사용하는 것보다 좋음을 알 수 있음\n\nGraspNet은 Success Rate가 # of grasps sampled에 영향을 받음\n\n만약 Unconstrained 경우라면 더 많은 sampling이 필요하다고 볼 수 있음\nRGK가 #GS에 영향을 받지 않은 결과를 보고도 확인할 수 있는 가설임\n\n\n\n\n\n14\n\n\n\n\n\n15\n\n\n마지막으로 해당 논문의 발표영상을 마지막으로 이번 포스팅을 마무리하도록 하겠습니다."
  },
  {
    "objectID": "posts/paper/2025-07-02-offline-rl-review.html",
    "href": "posts/paper/2025-07-02-offline-rl-review.html",
    "title": "📃Offline RL Survey 리뷰",
    "section": "",
    "text": "Paper Link"
  },
  {
    "objectID": "posts/paper/2025-07-02-offline-rl-review.html#종류",
    "href": "posts/paper/2025-07-02-offline-rl-review.html#종류",
    "title": "📃Offline RL Survey 리뷰",
    "section": "종류",
    "text": "종류\n논문은 오프라인 RL 방법론을 분류하기 위한 새로운 Taxonomy (분류체계)를 제안합니다. 상위 수준에서는 학습 대상을 기준으로 Model-Based (모델 기반), One-step (원스텝), Imitation Learning (모방 학습) 방법으로 나뉩니다. 또한, 손실 함수나 훈련 절차에 대한 변형인 Policy Constraints (정책 제약), Regularization (정규화), Uncertainty Estimation (불확실성 추정)을 부가적인 특성으로 설명합니다.\n\nPolicy Constraints: 학습된 정책\\pi_{\\theta}를 행동 정책\\pi_{\\beta}에 가깝게 제약합니다.\n\nDirect (직접):\\pi_{\\beta}를 명시적으로 추정하고\\mathcal{D}(\\pi_{\\theta}(\\cdot|s), \\hat{\\pi}_{\\beta}(\\cdot|s)) \\le \\epsilon와 같은 제약 조건(e.g.,f-divergence 사용)을 부여합니다 (BCQ, BRAC). 추정 오류에 민감합니다.\nImplicit (암묵적):\\pi_{\\beta} 추정 없이 수정된 목적 함수를 통해 암묵적으로 제약합니다. 아래와 같은 Advantage-weighted regression 형태가 대표적입니다 (BEAR, AWR, AWAC, TD3+BC). J(\\theta) = \\mathbb{E}_{s,a \\sim \\mathcal{D}}[\\log \\pi_{\\theta}(a|s) \\exp(\\frac{1}{\\lambda} \\hat{A}^{\\pi}(s, a))]\n\nImportance Sampling (IS): Off-policy 정책 평가를 위해 사용됩니다. 트라젝토리 확률 비율의 곱(w_{i:j})으로 인해 분산이 매우 높습니다. Variance Reduction (분산 감소) 기법(Per-decision IS, Doubly Robust Estimator, Marginalized IS)이 제안되었습니다. Marginalized IS는 상태 한계 분포 비율(\\rho_{\\pi}(s)) 또는 상태-행동 한계 분포 비율(\\rho_{\\pi}(s, a))의 벨만 방정식d^{\\pi_\\beta}(s')\\rho_\\pi(s') = (1-\\gamma)d_0(s') + \\gamma \\sum_{s,a} d^{\\pi_\\beta}(s)\\rho_\\pi(s)\\pi(a|s)T(s'|s,a)을 활용하여 분산 문제를 완화합니다 (GenDICE).\nRegularization: 정책 또는 가치 함수에 페널티 항을 추가하여 바람직한 속성을 부여합니다.\n\nPolicy Regularization: 정책의 엔트로피(entropy)를 최대화하여 확률성(stochasticity)을 높입니다 (SAC).\nValue Regularization: OOD 행동에 대한 Q-값 추정을 낮게 강제하여 보수적인 가치 추정을 수행합니다. CQL은 \\max_{\\mu} \\mathbb{E}_{s \\sim \\mathcal{D}, a \\sim \\mu(\\cdot|s)}[Q^{\\pi}_{\\phi}(s, a)] - \\mathbb{E}_{s \\sim \\mathcal{D}, a \\sim \\hat{\\pi}_{\\beta}(\\cdot|s)}[Q^{\\pi}_{\\phi}(s, a)] + \\mathcal{R}(\\mu) 와 같은 정규화 항을 통해 데이터셋의 가치 함수가 참 값의 하한(lower bound)이 되도록 학습합니다.\n\nUncertainty Estimation: 학습된 정책, 가치 함수 또는 모델의 불확실성을 추정하여 보수성의 정도를 동적으로 조절합니다. 보통 앙상블(ensemble)을 사용하여 예측 분산 등으로 불확실성을 측정합니다 (REM).\nModel-Based Methods: 데이터셋\\mathcal{D}로 전이 동역학(T)과 보상 함수(r)를 학습합니다. 학습된 모델은 계획(planning)에 사용되거나 모델 롤아웃(model rollout)을 통해 합성 데이터 생성에 사용됩니다. 모델 분포 변화 문제를 피하기 위해 불확실성을 기반으로 보상에 페널티를 주는 보수적인 모델(\\tilde{r}_{\\psi_r}(s, a) = r_{\\psi_r}(s, a) - \\lambda U_r(s, a))을 학습하는 접근 방식이 있습니다 (MOReL, MOPO, COMBO). COMBO는 모델 기반 환경에서의 가치 정규화(value regularization)를 통해 불확실성 정량화 없이도 보수성을 확보합니다.\nOne-Step Methods: 정책 평가 및 정책 개선 단계를 반복하지 않고, 행동 정책(\\pi_{\\beta})의 가치 함수(Q^{\\pi_{\\beta}})를 정확하게 학습한 후 단일 정책 개선 단계만 수행합니다. 이를 통해 OOD 행동에 대한 가치 평가를 피합니다. IQL(Implicit Q-Learning)은 가치 함수(V^{\\pi}) 학습에 Expectile Regression (분위 회귀) 손실 함수를 사용하여 데이터 분포 내의 ‘좋은’ 행동들에 대한 Q값의 상한에 근사합니다.\nImitation Learning: 행동 정책을 모방(mimic)합니다. 단순 Behavior Cloning (행동 복제, BC)은 전체 데이터를 복제합니다. 고급 기법은 가치 함수 등을 사용하여 차선 행동을 필터링하거나(BAIL, CRR) 원하는 결과(목표, 보상 등)에 조건화된 정책을 학습합니다(RvS).\nTrajectory Optimization (트라젝토리 최적화): 전체 트라젝토리(\\tau = (s_0, a_0, \\dots, s_H))에 대한 결합 상태-행동 분포(p_{\\pi_{\\beta}}(\\tau))를 시퀀스 모델(Sequence Model, 예: Transformer)로 학습합니다. 학습된 분포를 기반으로 원하는 수익(Return-to-Go) 등에 조건화하여 계획을 수행합니다(TT, DT). 희소 보상 문제에 강점을 보입니다."
  },
  {
    "objectID": "posts/paper/2025-07-02-offline-rl-review.html#평가",
    "href": "posts/paper/2025-07-02-offline-rl-review.html#평가",
    "title": "📃Offline RL Survey 리뷰",
    "section": "평가",
    "text": "평가\nOff-policy Evaluation (OPE, 오프-폴리시 평가)는 오프라인 RL의 중요한 Open Problem 중 하나입니다. 환경과의 상호작용 없이 오프라인으로 정책의 성능을 정확히 추정하고 하이퍼파라미터를 튜닝하는 것은 실용적인 오프라인 RL에 필수적입니다. 주요 OPE 방법에는 Model-Based 접근법, Importance Sampling, Fit Q Evaluation (FQE)가 있습니다. 경험적 연구들에 따르면 FQE가 종종 좋은 성능을 보이지만, 모든 설정에서 일관적으로 우수한 방법은 아직 없습니다 (DOPE 벤치마크).\n\n오프라인 RL Benchmark로는 D4RL과 RL Unplugged가 널리 사용됩니다.\n\n이들은 Narrow and Biased Data Distributions (좁고 편향된 데이터 분포), Undirected and Multitask Data (지향되지 않은 다중 작업 데이터), Sparse Rewards (희소 보상), Suboptimal Data (차선 데이터), Nonrepresentable Behavior Policies (표현 불가능한 행동 정책), Non-Markovian Behavior Policies (비 마르코프 행동 정책), Realistic Domains (현실적인 도메인) 등 실제 응용에 중요한 Dataset Design Factors (데이터셋 설계 요소)를 포함하는 다양한 환경과 데이터셋을 제공합니다.\n하지만 Stochastic Dynamics (확률적 동역학), Nonstationarity (비정상성), Risky Biases (위험한 편향), Multiagent 환경 등은 여전히 부족한 실정입니다. D4RL 벤치마크 성능 분석에 따르면 최근 방법(TT, IQL)과 트라젝토리 최적화 및 원스텝 방법이 희소 보상이나 다중 작업 데이터에서 강점을 보이며 유망한 분류로 나타납니다.\n미래 연구 방향으로는 OPE의 신뢰성 향상, Unsupervised RL 기법을 활용한 레이블 없는 데이터 활용, Incremental RL을 통한 온라인 Fine-tuning 전략 개발, Safety-critical RL (안전 필수 강화학습, 예: CVaR) 분야 연구 등이 제안됩니다. 효과적인 데이터 수집 및 curation 또한 알고리즘 개발만큼 중요합니다."
  },
  {
    "objectID": "posts/paper/2025-06-12-batch-online-rl.html",
    "href": "posts/paper/2025-06-12-batch-online-rl.html",
    "title": "📃Batch Online RL 리뷰",
    "section": "",
    "text": "Paper Link"
  },
  {
    "objectID": "posts/paper/2025-06-12-batch-online-rl.html#연구-동기-및-문제-설정",
    "href": "posts/paper/2025-06-12-batch-online-rl.html#연구-동기-및-문제-설정",
    "title": "📃Batch Online RL 리뷰",
    "section": "연구 동기 및 문제 설정",
    "text": "연구 동기 및 문제 설정\n현대 딥러닝은 대규모 데이터 활용을 통해 발전해왔지만, 로봇 공학 분야는 현실적으로 사용할 수 있는 데이터의 양이 현저히 부족합니다. 로봇에게 일일이 사람이 시연하여 데이터를 모으는 것은 큰 노력과 시간이 들기 때문에, 사람의 개입을 최소화하면서 로봇이 스스로 학습 데이터를 모으고 성능을 향상시키는 방법에 대한 관심이 높습니다. 이를 위해 과거에는 사람이 제공한 시연 데이터를 모방하는 모방학습(Imitation Learning, IL)을 주로 활용했지만, 이 방식은 주어진 데이터 범위 이상으로 성능을 끌어올리는 데 한계가 있습니다.\n강화학습(RL)을 적용하면 로봇이 스스로 시행착오를 겪으며 향상될 수 있다는 점에서 매력적이지만, 온라인 RL(online RL), 즉 로봇이 실시간 상호작용하며 동시에 학습을 진행하는 방법은 현실 로봇에게 적용하기에 어려움이 많습니다. 예를 들어, 학습 도중 로봇이 계속 움직이며 학습해야 하므로 물리적 피로도나 안전 문제, 그리고 학습 과정에서 초기 정책이 망가지는 분포 이동(distribution shift) 문제가 나타날 수 있습니다.\n이에 대한 중간 해결책(middle ground)으로 제시된 개념이 “배치 온라인 RL”(batch online RL)입니다. 이 방법에서는 초기에는 오프라인 데이터로 정책을 학습시키고 (예: 시연 데이터로 초기 정책 \\pi\\_0 학습), 이 정책을 로봇에 배치하여 일정량의 경험을 한꺼번에 수집한 뒤, 로봇을 멈추고 모은 데이터를 오프라인으로 학습하여 정책을 갱신합니다. 이렇게 새 정책 \\pi\\_1을 얻으면 다시 로봇을 사용해 자율적으로 데이터를 모으고(rollout), 축적된 데이터셋으로 다시 학습을 진행하는 과정을 반복합니다. 요컨대, 실험 환경에서 정책의 실행(데이터 수집)과 정책의 학습(업데이트) 단계를 교대로 배치 단위로 수행함으로써, 온라인 RL의 자기 향상 능력을 유지하면서도 실제 배치 시에는 학습을 하지 않으므로 안정성과 효율성을 높인 접근입니다. 이러한 Batch online RL은 스스로 모은 대량의 데이터로 정책을 향상시킬 수 있어 잠재적으로 대규모 로봇 학습을 가능하게 할 것으로 기대됩니다.\n하지만 배치 온라인 RL을 실제로 성공적으로 구현하는 데에는 여전히 도전과제가 있습니다. 과거 일부 연구들은 배치 온라인 RL 환경에 모방학습이나 변형된 모방학습(filtered IL) 알고리즘을 적용해 보았으나, 자율 수집한 데이터로부터 효과적으로 성능을 높이지 못하거나 금세 어느 수준에서 성능이 정체되는 문제가 보고되었습니다. 이에 따라 이 논문에서는 “로봇 배치 온라인 강화학습에서 무엇이 성능 향상을 결정짓는 핵심 요소인가?”라는 질문을 제기하고, 체계적인 실험을 통해 답을 찾고자 합니다."
  },
  {
    "objectID": "posts/paper/2025-06-12-batch-online-rl.html#기존-강화학습-방식과의-차별점",
    "href": "posts/paper/2025-06-12-batch-online-rl.html#기존-강화학습-방식과의-차별점",
    "title": "📃Batch Online RL 리뷰",
    "section": "기존 강화학습 방식과의 차별점",
    "text": "기존 강화학습 방식과의 차별점\n배치 온라인 RL은 기존의 오프라인 RL 및 온라인 RL과 비교해 몇 가지 측면에서 차별화됩니다.\n\n온라인 RL 대비: 전통적인 온라인 RL에서는 로봇이 환경과 상호작용하며 즉시 학습을 업데이트하지만, 배치 온라인 RL에서는 로봇이 데이터만 수집하고 학습은 나중에 오프라인으로 진행됩니다. 이로 인해 온라인 RL에서 문제가 되었던, 학습 중 실제 로봇을 지속 운용해야 하는 부담과 훈련 과정에서의 불안정성을 줄일 수 있습니다. 논문에서도 “배치 온라인 RL은 정책 훈련과 데이터 수집을 분리함으로써 온라인 RL의 복잡한 문제 없이도 자체 롤아웃 데이터를 활용한 향상을 가능케 한다”고 설명합니다. 또한 온라인으로 바로 미세조정(fine-tuning)하는 경우 나타날 수 있는 분포 차이로 인한 성능 붕괴나 초기 정책의 망각 현상을 배치 방식은 완화해 줍니다.\n오프라인 RL 대비: 오프라인 RL은 고정된 정적 데이터셋을 가지고 한 번에 학습을 끝내는 반면, 배치 온라인 RL에서는 새로운 데이터를 지속적으로 추가하며 여러 단계에 걸쳐 정책을 개선합니다. 즉, 오프라인 RL이 초기 주어진 데이터 품질에 전적으로 의존하는 데 반해, 배치 온라인 RL은 정책이 개선됨에 따라 새로운 (잠재적으로 더 나은) 데이터를 모아 학습함으로써 성능을 점진적으로 높일 수 있습니다. 한편 배치 온라인 RL도 오프라인 RL처럼 훈련과 데이터 수집이 분리된 상태에서 학습하므로, 순수 온라인 RL보다 안정적으로 정책을 향상시킬 수 있다는 장점이 있습니다.\n\n정리하면, 배치 온라인 RL은 오프라인과 온라인의 절충 방식으로, 자율주행 데이터 수집의 이점과 오프라인 학습의 안정성을 결합한 프레임워크입니다. 이 접근은 로봇이 스스로 데이터를 모으면서도, 모은 데이터로 학습을 별도로 수행함으로써 실시간 학습의 부담 없이도 자기 개선을 달성하려는 것입니다."
  },
  {
    "objectID": "posts/paper/2025-06-12-batch-online-rl.html#제안된-접근-방식-및-프레임워크-설명",
    "href": "posts/paper/2025-06-12-batch-online-rl.html#제안된-접근-방식-및-프레임워크-설명",
    "title": "📃Batch Online RL 리뷰",
    "section": "제안된 접근 방식 및 프레임워크 설명",
    "text": "제안된 접근 방식 및 프레임워크 설명\n이 논문에서는 배치 온라인 RL 문제에서 어떤 접근이 가장 효과적인지를 실험적으로 규명한 뒤, 그 결과를 토대로 일반적인 해법(recipe)을 제안합니다. 연구진은 배치 온라인 RL에 적용될 수 있는 방법들을 세 가지 축으로 분류하여 실험하였는데, 그 축은 (i) 알고리즘 유형, (ii) 정책 추출 방법, (iii) 정책의 표현력입니다. 각 축에서 어떤 선택을 하느냐가 성능에 큰 영향을 미친다는 것을 확인한 후, 최적의 조합을 일반 원칙으로 정리한 것이 이 논문의 핵심 제안입니다.\n① 알고리즘 클래스: 먼저 정책을 개선하는 알고리즘으로 세 가지를 비교했습니다. 하나는 모방학습(IL) 기반으로, 수집된 데이터를 행동클로닝(Behavior Cloning) 방식으로 학습하는 가장 단순한 방법입니다. 다른 하나는 필터링된 모방학습(filtered IL)으로, 자율주행으로 모은 데이터 중 성공적이거나 양질의 부분만 선별하여 모방학습에 사용하는 기법입니다. 마지막은 가치 기반 강화학습(value-based RL)으로, 수집 데이터에서 보상신호를 활용하여 Q-함수 등 가치함수를 학습하고 이를 통해 정책을 업데이트하는 방법입니다. 이 때 가치 기반 방법은 실패 사례 등도 학습에 활용하고 Bellman 업데이트를 수행하기 때문에, 오직 성공 사례에 의존하는 IL보다 잠재적으로 더 풍부한 학습이 가능합니다.\n② 정책 추출 방법: 다음으로, 가치 기반 RL을 쓸 경우 정책을 얻는 방식을 두 가지 고찰했습니다. 명시적 정책 추출(explicit policy extraction)은 오프라인 RL에서 흔히 쓰이는 접근으로, Q함수를 최대화하면서도 행동 분포가 기존 데이터에서 크게 벗어나지 않도록 정책을 학습하는 기법입니다. 예를 들면 Advantage-Weighted Regression (AWR) 같은 알고리즘이 이에 해당하며, Q함수의 신호를 정책 학습에 직접 반영하지만 동시에 행동클로닝 제약을 줘서 기존 데이터 분포와의 거리를 유지하게 합니다. 반면 암시적 정책 추출(implicit policy extraction)은 정책을 별도로 최적화하지 않고, 실행 시에 Q함수를 이용해 최선의 행동을 고르는 방식입니다. 구체적으로는 매 스텝 상태에서 현재 정책으로 여러 후보 행동을 샘플링한 후, Q값이 가장 높은 행동을 실제 실행에 선택하는 방식입니다. 암시적 방법은 Q함수의 정보를 정책 파라미터 학습에 직접 반영하지 않기 때문에 학습이 더 안정적일 수 있지만, 그 대신 Q함수의 유용한 신호를 정책 업데이트에 활용하지 못한다는 trade-off가 있습니다.\n③ 정책의 표현력(Expressivity): 마지막으로 정책 모델의 클래스에 따른 표현력 차이를 실험했습니다. 일반적으로 강화학습에서 많이 쓰이는 정책 모델은 가우시안 정책으로, 상태를 입력받아 행동의 평균과 분산을 출력하고 정규분포로 행동을 샘플링하는 비교적 단순한 형태입니다. 이런 가우시안 정책은 경험이 풍부하지 않은 초기 단계에서는 안정적이고 추론 속도가 빠른 장점이 있지만, 복잡한 여러 모달리티의 행동분포를 표현하기에는 한계가 있습니다. 이에 비해 논문에서 표현력이 높다고 지칭하는 정책은 디퓨전 모델 기반 정책(diffusion-based policy)으로, 마르코프 확률과 노이징/디노이징 과정을 활용하여 주어진 데이터를 생성 모델 형태로 학습하는 방식입니다. 디퓨전 정책은 다중 모달의 행동 분포를 더 잘 모델링할 수 있고, 특히 암시적 정책 추출 방식과도 잘 맞는 특성이 있습니다 (정책 자체가 다양한 후보 행동을 샘플링해낼 수 있으므로, 거기서 Q값이 높은 것을 고르는 전략과 시너지가 좋습니다).\n이상 세 가지 축에서 찾은 시사점을 바탕으로, 저자들은 배치 온라인 RL을 효과적으로 수행하기 위한 일반 해법(recipe)을 제안합니다:\n\n가장 첫 단계로 표현력이 높은 정책 모델을 IL(모방학습)로 초기 학습하여 배치 온라인 RL의 배우(행동자)로 사용합니다. (논문에서는 Diffusion 기반 정책네트워크를 시演 데이터로 학습하여 사용했습니다.)\n병행해서, 동일한 자율수집 데이터에 대해 Q-함수를 학습합니다 (논문 구현에서는 IQL 알고리즘, 즉 오프라인 RL 방식으로 Q를 업데이트).\n정책을 업데이트할 때는 암시적 정책 추출 방식을 활용합니다. 즉, 새로운 데이터를 모을 때 방금 학습한 Q함수를 이용해 현재 정책에서 샘플한 여러 행동 중 Q값이 가장 높은 행동을 실행하도록 합니다. 이렇게 하면 명시적으로 정책을 업데이트하지 않고도 Q함수가 높은 방향으로 행동이 유도되어 정책 향상이 이뤄집니다.\n위 과정에서 필요한 경우 정책의 탐색을 돕기 위해 시간적으로 상관된 노이즈(Ornstein–Uhlenbeck 과정 등을 통한 노이즈)를 행동에 추가로 주입할 수 있습니다. 이는 로봇이 좀 더 다양한 시도를 해보도록 유도하여 데이터 다양성을 높이기 위함입니다.\n\n이러한 레시피를 요약하면, “표현력 높은 정책 + Q함수 기반 가치학습 + 암시적 정책 추출”의 조합으로 정책을 반복 개선하는 것입니다. 논문에서는 이 조합을 IDQL (Implicit Diffusion Q-Learning) 알고리즘으로 구현했으며, 이는 배치 온라인 RL 1회 반복에 해당하는 알고리즘으로 볼 수 있습니다. 전체 배치 온라인 RL 프로세스에서는 이 과정을 여러 iteration 수행하면서 정책을 향상시킵니다. 마지막으로, 저자들은 레시피의 실용적 보완책으로 행동 노이즈 주입을 제안하였는데, 실험 결과 약간의 노이즈 추가는 적은 데이터 환경에서 성능 향상을 가져오지만 데이터가 충분히 많아지면 자연스럽게 탐색 다양성이 확보되기 때문에 노이즈의 효과는 제한적임을 확인했습니다. 즉, 노이즈 추가는 선택 사항이지 필수 요소는 아니며, 다만 데이터가 부족한 초기 단계에서는 소량의 노이즈로도 성능을 높이는 보탬이 될 수 있습니다."
  },
  {
    "objectID": "posts/paper/2025-06-12-batch-online-rl.html#실험-환경-및-평가-지표",
    "href": "posts/paper/2025-06-12-batch-online-rl.html#실험-환경-및-평가-지표",
    "title": "📃Batch Online RL 리뷰",
    "section": "실험 환경 및 평가 지표",
    "text": "실험 환경 및 평가 지표\n저자들은 제안한 접근과 다른 대안들을 여러 시뮬레이션 환경과 실제 로봇 작업에 걸쳐 평가했습니다. 시뮬레이션 실험으로는 총 6개의 복잡한 로봇 조작 과제를 선택했는데, 구체적으로 RoboMimic 데이터셋의 Lift, Can, Square 작업, MimicGen의 Stack 및 Threading 작업, 그리고 Adroit의 Pen 작업입니다. 이들 과제는 연속적 제어가 필요한 까다로운 작업들로, 난이도에 따라 초기 시 데이터(D₀)의 크기를 5개에서 최대 100개까지 제공하여 초기 정책 \\pi_0의 성공률이 약 30~65% 수준이 되도록 설정했습니다. 이렇게 비교적 미완성된 초기 성능에서 시작함으로써, 향후 스스로 개선할 여지가 충분한 현실적인 시나리오를 만들었습니다. 각 과제마다 배치 온라인 RL의 반복(iteration)을 10~20회 수행하며, 매 iteration마다 200개의 에피소드(rollouts)를 수집하도록 설정했습니다. 학습 중 성능 평가를 위해 각 반복 단계마다 정책을 고정시키고 여러 에피소드를 실행하여 얻은 누적 보상(returns)이나 성공률을 측정하였으며, 이러한 지표를 초기 성능 대비 정규화(normalize)하여 비교하였습니다. (그래프 상에서 100%는 과제 성공률 100% 또는 주어진 목표 대비 최고의 성능으로 정규화되어 있습니다.)\n한편 실제 로봇 실험으로는 비전 기반의 로봇 조작 작업을 하나 선정했습니다. 7자유도 Franka 암 로봇으로 테이프 롤을 집어서 고리에 거는 작업을 수행했으며, 로봇 손목 카메라와 외부 카메라에서 얻은 RGB 영상 및 로봇의 관절각/엔드이펙터 상태 등의 proprioceptive 정보를 입력으로 사용했습니다. 이 작업은 초기 정책으로 성공하기 어렵도록 설계된 어려운 과제로, 배치 온라인 RL을 통한 성능 개선 여부를 확인하기에 적합합니다."
  },
  {
    "objectID": "posts/paper/2025-06-12-batch-online-rl.html#주요-실험-결과와-분석",
    "href": "posts/paper/2025-06-12-batch-online-rl.html#주요-실험-결과와-분석",
    "title": "📃Batch Online RL 리뷰",
    "section": "주요 실험 결과와 분석",
    "text": "주요 실험 결과와 분석\n \n\n\n\nFigure: 배치 온라인 RL에서 서로 다른 알고리즘 클래스의 성능 비교(여러 작업에서 반복을 거듭하며 얻은 평균 누적 성과)\n\n\n\n주황색 곡선은 가치 기반 RL 방법, 회색은 필터링된 IL, 연한 회색은 기본 IL을 나타냅니다. 가치 기반 RL 방법이 대부분의 작업에서 반복이 진행될수록 가장 높은 성능 향상을 보이고 있으며, IL 기반 방법들은 초기 성능에서 크게 개선되지 못하거나 일부 향상 후 정체되는 양상을 보입니다.\n\n1) 알고리즘 클래스 비교: 가치 기반 RL 방법이 두드러지게 우수한 성능을 보였습니다. 위 그래프에서 보이듯 IL(모방학습)만으로 학습하는 경우 자율적으로 모은 실패 데이터까지 그대로 모방해버리기 때문에 오히려 성능이 떨어지거나 거의 향상되지 못했습니다. 필터링된 IL의 경우 초기에는 약간의 개선을 보였지만, 곧 아직 최적이 아닌 성능 수준에서 정체되어 더 이상 나아지지 못하는 경향이 있습니다. 반면 가치 기반 RL은 보상 신호를 통해 성공/실패 사례를 모두 학습에 활용함으로써, iteration을 거듭할수록 지속적인 성능 향상을 이루었고 최종적으로 가장 높은 성과를 달성했습니다. 이처럼 RL 방법이 우수한 이유는, 자율 수집한 데이터에 내재한 다양한 상황과 실패 사례로부터도 교훈을 얻어 정책을 수정할 수 있기 때문입니다. 실제로 논문에서는 RL 기법이 배치 데이터를 더 다양하게 활용하여 IL에 비해 정책 탐색 범위를 넓힌다고 분석합니다. 예를 들어, 성공한 경로들의 상태 방문 분포를 시각화한 결과 RL 알고리즘은 IL에 비해 훨씬 폭넓은 상태 공간을 커버하고 있었는데, 이는 RL이 다양한 시도를 통해 새로운 성공 경로를 개척한 반면 IL은 주어진 시演 궤적 주변만 반복했음을 보여줍니다. 또한 데이터 규모를 확장했을 때도 RL의 강점이 확인되었습니다. 배치당 수집하는 에피소드 수(M)를 소/중/대로 늘려가며 성능을 비교한 결과, 가치 기반 RL은 데이터가 많아질수록 성능이 꾸준히 향상된 반면 IL 기반 방법들은 일정 수준 이상에서 추가 데이터가 효과를 발휘하지 못하고 정체되었습니다. 즉 배치 온라인 RL에서 더 큰 데이터로 스케일하려면, IL로는 한계가 있고 RL 알고리즘이 필수적임을 알 수 있습니다. 다만 저자들은 “가치 기반 RL만으로 충분하지 않다”고 강조하는데, 이는 아래의 다른 요소들(정책 추출 방식, 정책 표현력)도 적절히 선택해야 진정한 효과를 볼 수 있다는 의미입니다.\n2) 정책 추출 방법 비교: 명시적 vs. 암시적 두 방식의 성능 차이도 뚜렷했습니다. 실험에 따르면 암시적 정책 추출이 반복 학습 후의 최종 성능에서 항상 우월했습니다. 흥미로운 점은, 학습 초반에는 명시적 방법(AWR 등으로 정책을 함께 최적화한 경우)이 다소 높은 성능으로 시작하는 경향이 있었으나, 여러 iteration을 거친 후에는 암시적 방법이 훨씬 더 크게 성능을 끌어올렸다는 것입니다. 명시적으로 정책을 업데이트하는 방법은 Q함수의 신호를 직접 활용한다는 장점 때문에 초기에 빠르게 성능을 올릴 수 있지만, 새로운 자율 데이터가 누적되면서 기존 행동 분포와 달라지는 변화에 적응하지 못하고 발목을 잡히는 것으로 해석됩니다. 실제로 논문에서는, 명시적 방법에서는 매 iteration마다 정책 학습 시 이전 데이터 분포에 묶이는 제약이 존재하여 새로운 데이터 분포로의 이동이 어렵다고 지적합니다. 그 결과 배치 온라인 RL 과정에서 데이터 분포가 다양해질수록 명시적 방법의 정책은 이를 제대로 따라가지 못해 성능이 저하되는 반면, 암시적 방법은 Q함수와 정책 학습을 분리하여 이러한 분포 변화에 유연하게 대응할 수 있기 때문에 안정적으로 성능 개선을 이루는 것으로 나타났습니다. 결론적으로 배치 단위 자기학습 시에는 암시적 정책 추출이 일관되게 효과적이며, 이는 오프라인 RL 방식의 정책 학습이 항상 최선은 아닐 수 있음을 시사합니다.\n3) 정책 표현력(모델) 비교: 정책 모델의 표현력도 결과에 큰 영향을 미쳤습니다. 비교 실험에서는 표현력이 낮은 가우시안 정책과 표현력이 높은 디퓨전 정책을 각각 사용해 보았습니다. 먼저 두 정책을 각각 최적의 방식으로 조합했을 때 (가우시안+명시적, 디퓨전+암시적) 결과는 명확했습니다: 디퓨전 기반의 고표현력 정책이 모든 작업에서 일관되게 가우시안 정책보다 뛰어난 최종 성능을 보였습니다. 특히 배치 RL 반복 전후의 성능 향상폭을 보면, 가우시안 정책은 개선 폭이 제한적인 반면 디퓨전 정책은 크게 향상하여 최종 성능 격차가 벌어졌습니다. 이는 고차원 복잡한 행동 분포를 모델링할 수 있는 능력이 있을 때 배치 RL 과정에서 더 다양한 시도를 하고 학습할 수 있음을 보여줍니다. 흥미로운 점으로, 저자들은 앞의 정책 추출 실험과 연관지어 “표현력 높은 정책은 암시적 추출과 특히 궁합이 좋다”고 분석합니다. 추가로 가우시안 정책에도 암시적 추출을 적용해보는 대조 실험을 했는데, 이 경우 성능이 어느 정도 개선되었지만 여전히 디퓨전 정책 조합에는 못 미쳤습니다. 결국 표현력 그 자체의 차이가 성능 격차의 중요한 원인이라는 것입니다. 왜 온라인 RL에서는 가우시안 정책으로도 잘 되는데 배치 RL에서는 부족한가? 라는 질문에 대해, 논문에서는 “온라인 RL에서는 정책이 매 스텝 업데이트되므로 초기 분포를 잘 모델링하지 않아도 새로운 행동을 계속 발굴하지만, 배치 RL에서는 각 iteration 동안 정책이 고정되므로 초기 정책이 충분한 다양성을 가지지 못하면 새 데이터를 모으기 어렵다”고 설명합니다. 즉 배치 온라인 RL에서는 초기에 정책의 표현력이 높아야만 충분히 탐색적인 데이터 수집이 이루어지고, 이는 곧 향상으로 이어진다는 통찰입니다.\n이상의 결과들을 종합하면, 배치 온라인 RL에서 성능을 극대화하기 위한 핵심 요소는\n(a) 가치 기반 RL 알고리즘의 활용,\n(b) 암시적 정책 추출 방식,\n(c) 표현력 높은 정책 모델 사용으로 요약됩니다.\n저자들은 이 세 가지를 모두 만족하는 조합(앞서 설명한 레시피)을 사용했을 때 기존 방법들 대비 크게 향상된 성능과 데이터 스케일링 효과를 거두었음을 보고합니다. 또한 추가로 제안된 행동 노이즈 주입은 초기 학습 단계에서 성과를 약간 높여주는 실용적 트릭으로 작용했지만, 핵심은 아니었다고 언급됩니다. 결국 이 연구는 어떤 방법으로 로봇이 스스로 모은 데이터로 학습해야 효율적인가에 대한 경험적 해답을 제시한 것입니다."
  },
  {
    "objectID": "posts/paper/2025-06-12-batch-online-rl.html#논문에서-제안하는-실질적인-로봇-적용-시-고려사항",
    "href": "posts/paper/2025-06-12-batch-online-rl.html#논문에서-제안하는-실질적인-로봇-적용-시-고려사항",
    "title": "📃Batch Online RL 리뷰",
    "section": "논문에서 제안하는 실질적인 로봇 적용 시 고려사항",
    "text": "논문에서 제안하는 실질적인 로봇 적용 시 고려사항\n배치 온라인 RL 레시피의 실용성은 실제 로봇 실험을 통해 검증되었습니다. 저자들은 프랑카 암(arm) 로봇에게 테이프 걸기 작업을 학습시키는 실험을 3회 반복하여, 앞서 도출한 방법이 현실 로봇에도 통하는지 확인했습니다. 이때 고려해야 할 실질적 요소 몇 가지가 있습니다:\n\n\n\nFigure: 실험에 사용된 실제 로봇 및 작업 예시.\n\n\n\n7자유도 로봇 암이 초기에는 집게에 테이프 롤을 쥐고 목표물 앞에 위치합니다 (좌측). 이후 정책이 개선되면 로봇이 테이프를 정확히 갈고리 모양의 걸이(hook)에 거는 최종 동작(우측)을 성공적으로 수행합니다. 이 과제는 컴퓨터 비전(RGB 이미지 입력)과 정교한 동작 제어가 모두 필요한 어려운 작업으로, 배치 온라인 RL의 효과를 시험하기 위한 현실적인 시나리오입니다.\n\n로봇 실험에서는 초기 시연을 단 5개만 제공하여 시작한 후, N=3회의 batch 자율학습 반복, 매회 30개의 롤아웃을 수집하였습니다. 실험 결과는 고무적이었는데, 제안된 레시피를 따른 경우 단 3번의 반복만에 초기 정책 대비 성공률 30%p 상승이라는 유의미한 향상을 달성했습니다. 아래 그래프의 녹색 선(Recipe)이 보여주듯, 약 45% 정도이던 초기 성공률이 세 번의 학습을 거쳐 약 70%까지 상승했습니다. 반면 회색 선(Filtered-IL)으로 표시된 필터링 모방학습 방법은 초기에 60% 수준에서 시작했으나 추가 향상이 거의 없었으며, 자주색 점선(Steering)으로 표시된 비교 기준 방법(기존 연구 [11]의 접근을 응용한, 새로운 데이터로 Q함수를 학습하되 정책은 갱신하지 않는 일종의 “조종” 방법)은 오히려 성능이 정체되어 약 50% 언저리에 머물렀습니다. 이는 새로운 롤아웃 데이터로 정책 자체를 계속 재훈련하는 것이 중요하며, 단순히 Q함수로 기존 정책의 행동을 보정해주는 것만으로는 충분치 않음을 의미합니다. 또한 필터링된 IL 방식이 초기에 높은 성능으로 시작하고도 추가 향상에 실패한 것은, 초기 정책이 이미 시演 데이터 분포를 거의 표현하고 있었기 때문으로 분석됩니다. 이 경우 새로운 데이터가 추가되어도 배울 새로운 내용이 거의 없어 개선이 이루어지지 않은 것입니다.\n\n\n\nFigure: 실제 로봇 실험 “테이프 걸기”의 학습 성과 비교 (성공률)\n\n\n\n녹색 선(Recipe)은 본 논문의 레시피 적용 결과, 회색 선(Filtered IL)은 필터링된 모방학습, 자주색 점선(Steering)은 정책을 업데이트하지 않고 Q함수로만 보조하는 기준 방법을 나타냅니다. 배치 온라인 RL 레시피를 적용한 경우(iteration 3) 성공률이 약 70%로 크게 향상되었으며, 다른 방법들은 초기 수준을 크게 넘지 못한 것을 볼 수 있습니다.\n\n실제 로봇에 배치 온라인 RL을 적용할 때 고려할 현실적 사항으로, 안전과 탐색의 균형이 있습니다. 예를 들어 성능 향상을 위해 탐색 노이즈를 추가하는 것이 유용하지만, 자칫하면 로봇에게 물리적으로 위험한 행동을 유발할 수 있습니다. 다행히 이 실험에서는 소량의 Ornstein-Uhlenbeck 노이즈를 추가해도 테이프를 놓친다거나 로봇이 이상 동작을 하는 등의 문제 없이 안정적으로 다양한 시도가 이루어졌습니다. 하지만 일부 실제 환경에서는 임의의 노이즈 주입이 어려울 수 있으므로, 다른 안전 탐색 기법이나 시뮬레이션을 병행한 사전 학습 등이 필요할 수 있습니다. 또한 현실 로봇 실험에서는 센서 노이즈, 시스템 지연 등 시뮬레이션에 없던 변수가 존재하므로, 배치 온라인 RL의 알고리즘이 이러한 비이상적 상황에서도 강인하게 작동하도록 설계되어야 합니다. 논문의 실험은 비교적 안정적인 실내 환경에서 이루어졌지만, 향후에는 더 복잡한 실제 시나리오(예: 이동 로봇, 협동 로봇 등)에서 이 접근법의 성능과 한계를 추가 검증해볼 필요가 있습니다."
  },
  {
    "objectID": "posts/paper/2025-06-12-batch-online-rl.html#본-논문의-한계점-및-향후-연구-방향",
    "href": "posts/paper/2025-06-12-batch-online-rl.html#본-논문의-한계점-및-향후-연구-방향",
    "title": "📃Batch Online RL 리뷰",
    "section": "본 논문의 한계점 및 향후 연구 방향",
    "text": "본 논문의 한계점 및 향후 연구 방향\n이 연구는 배치 온라인 RL의 효과를 좌우하는 요소들을 밝혀내고 일반적 지침을 제시하였지만, 저자들이 언급한 한계점도 몇 가지 존재합니다:\n\n적용 범위의 한계 (연속 vs. 이산): 본 논문의 모든 실험은 연속적 행동 공간을 갖는 로봇 제어 작업에 대해 이루어졌습니다. 만약 이산(discrete) 행동 공간을 가지는 문제 (예: 강화학습의 Atari 게임이나 로봇의 관절 제어가 아닌 개별 명령 선택 문제 등)에 이 방법을 적용하면 그대로 성립하지 않을 수 있습니다. 이산 환경에서는 Q함수를 직접 정책으로 사용할 수도 있고, 또 “표현력 높은/낮은 정책 클래스”라는 구분도 모호해지기 때문에, 배치 온라인 RL의 핵심 요소가 다르게 나타날 가능성이 있습니다. 향후에는 이산 행동을 포함한 다양한 영역에서 본 레시피의 유효성을 연구할 필요가 있습니다.\n노이즈 주입에 대한 고려: 본 논문에서 제안한 OU 노이즈 등 탐색 전략은 시뮬레이션과 제한된 실제 실험에서 일정 수준 효과를 보였지만, 모든 환경에서 통용될 수 있는지는 미지수입니다. 경우에 따라서는 노이즈로 인한 탐색보다 안전이 우선인 상황도 있고, 노이즈가 성능에 거의 영향을 주지 못할 수도 있습니다. 저자들도 실험에서 소량의 노이즈 추가가 정책 성공률을 극적으로 높이지는 못했고 약간의 향상만 주었다고 밝혔습니다. 따라서 미래 연구에서는 노이즈를 쓰지 않고도 탐색 다양성을 확보할 방법이나, 더 효과적인 탐색 노이즈 전략을 모색할 필요가 있습니다.\n초기 정책 성능에 대한 가정: 본 연구는 초기 정책이 어느 정도의 성공률은 갖춘 상태(약 30~60%)에서 시작하는 시나리오에 초점을 맞추었습니다. 그렇다면 만약 초기 정책이 거의 실패만 하는 수준(성공률 ~0%)이라면, 배치 온라인 RL로 개선이 가능할까요? 논문에서는 이 부분이 열린 문제로 남아 있다고 지적합니다. 초기 정책이 전혀 성능이 없다면 자율적으로 수집하는 데이터도 실패 투성이일 것이고, 아무리 RL이라도 완전한 탐색부터 시작해야 하는 어려움이 있습니다. 향후에는 초기 데모 없이도 학습을 시작하거나, 완전 무작위 정책으로부터 배치 RL을 안정적으로 수행하는 기법에 대한 연구가 필요합니다.\n\n이상의 한계들을 바탕으로, 향후 연구 방향으로는: (1) 다른 형태의 문제들(특히 이산적 결정 문제)에 대한 배치 온라인 RL 전략 확장, (2) 탐험-탐색(exploration-exploitation) 전략 개선 – 예를 들어 안전한 범위 내에서의 노이즈 주입, 혹은 모델 기반 탐색 기법과의 결합 등, (3) 초기 모델 없이 자율학습을 시작하는 방법 – 예를 들어 프리트레이닝된 세계 모델 활용이나 보상 신호 설계를 통해 완전 새로운 정책도 스스로 발전하게 하는 연구 등이 제시될 수 있습니다. 더 나아가, 본 논문의 레시피 자체도 각각의 구성요소(가치 학습 알고리즘, 정책 모델 구조, 암시적 추출 기법 등)를 최적화하거나 대체하여 성능을 더욱 높일 여지가 있습니다. 저자들은 이러한 후속 연구가 이루어진다면 스스로 개선하는 로봇의 성능 한계를 훨씬 끌어올릴 것으로 기대하고 있습니다."
  },
  {
    "objectID": "posts/paper/2025-06-12-batch-online-rl.html#전체적인-기여도와-의의",
    "href": "posts/paper/2025-06-12-batch-online-rl.html#전체적인-기여도와-의의",
    "title": "📃Batch Online RL 리뷰",
    "section": "전체적인 기여도와 의의",
    "text": "전체적인 기여도와 의의\n본 논문은 로봇 강화학습 분야에서 “스스로 모은 데이터로 점진적 학습”이라는 중요한 문제에 답하기 위해 체계적인 실증 연구를 수행했다는 점에서 큰 의미가 있습니다. 이전까지 여러 연구들이 시도했지만 명확한 결론을 얻지 못했던 배치 온라인 RL의 성공 요인을 세 가지 측면에서 분석해주었고, 이를 종합하여 실제로 효과가 입증된 학습 레시피를 제안하였습니다. 이 레시피는 복잡한 알고리즘이 아니라도 기존 기법들의 조합과 운영 방법을 조금 바꾸는 것만으로도 로봇의 자기학습 성능을 크게 높일 수 있다는 실용적 지침을 제공합니다. 특히 모방학습만으로 부족했던 이유와 강화학습을 도입할 때 주의할 점(정책 추출 방법, 모델 표현력)을 밝혀줌으로써, 향후 로봇 학습 실험을 설계하는 연구자들과 현장에서 실제 로봇을 학습시켜야 하는 실무 엔지니어 모두에게 유용한 인사이트를 준다고 볼 수 있습니다.\n또한 이 연구는 로봇 학습에서 데이터 효율성과 자동화에 대한 희망을 보여주었습니다. 사람이 일일이 데이터를 모으지 않아도, 로봇이 처음 배운 정책으로 스스로 실행하고 데이터를 모아 더 똑똑해질 수 있음을 실제 실험으로 증명하였습니다. 이는 마치 자율주행차가 주행하며 경험으로 학습하는 그림을 떠올리게 하며, 향후 로봇공학이 나아갈 방향이 인간의 지도 최소화와 자율성 극대화에 있음을 시사합니다. 물론 완전한 자율학습으로 가기까지 해결해야 할 과제들(안전, 초기 성능 등)은 남아 있지만, 본 논문은 그 실마리를 제공하고 있다고 평가할 수 있습니다.\n마지막으로, 학술적 기여도 측면에서 이 논문은 배치(off-policy) 환경에서의 강화학습 연구에 새로운 데이터를 제시했습니다. 과거에는 온라인 상에서 바로 학습하는 RL과, 고정 데이터로 학습하는 offline RL이 별개로 연구되는 경향이 있었는데, 본 연구는 오프라인-온라인의 경계를 넘나드는 배치 학습에 초점을 맞추어 중요한 통찰을 얻었습니다. 이로써 오프라인 RL 알고리즘을 반복 적용하면 어떨까?라는 물음에 답을 주고, 나아가 더 나은 오프라인 RL 기법 개발이나 효과적인 탐색 데이터 수집 전략에 대한 후속 연구를 촉발할 수 있습니다. 저자들이 언급했듯, 이 레시피의 각 요소를 더 발전시키는 연구를 통해 자기 개선형 로봇 모델을 한층 더 향상시킬 여지가 큽니다. 또한 저자들은 이번 연구의 알고리즘 구현 코드를 공개할 예정으로, 이를 통해 다른 연구자들이 쉽게 접근하여 배치 온라인 RL 분야의 발전을 가속할 수 있을 것으로 기대됩니다.\n요약하면, “What Matters for Batch Online RL in Robotics?” 논문은 로봇 강화학습에서 데이터 효율적 자기 학습을 실현하는 데 필요한 조건을 밝혀내고 검증함으로써, 로봇공학 및 강화학습 커뮤니티에 중요한 지식과 실용 해법을 제공한 작품이라 할 수 있습니다. 이는 궁극적으로 더 적은 인간 개입으로도 점점 똑똑해지는 로봇을 만드는 길을 여는 의미있는 걸음입니다."
  },
  {
    "objectID": "posts/paper/2022-06-26-keep-learning.html",
    "href": "posts/paper/2022-06-26-keep-learning.html",
    "title": "📃Legged Robots that Keep on Learning 리뷰",
    "section": "",
    "text": "Legged robots are physically capable of traversing a wide range of challenging environments but designing controllers that are sufficiently robust to handle this diversity has been a long-standing challenge in robotics. Reinforcement learning presents an appealing approach for automating the controller design process and has been able to produce remarkably robust controllers when trained in a suitable range of environments. However, it is difficult to predict all likely conditions the robot will encounter during deployment and enumerate them at training-time. What if instead of training controllers that are robust enough to handle any eventuality, we enable the robot to continually learn in any setting it finds itself in? This kind of real-world reinforcement learning poses a number of challenges, including efficiency, safety, and autonomy. To address these challenges, we propose a practical robot reinforcement learning system for fine-tuning locomotion policies in the real world. We demonstrate that a modest amount of real-world training can substantially improve performance during deployment, and this enables a real A1 quadrupedal robot to autonomously fine-tune multiple locomotion skills in a range of environments, including an outdoor lawn and a variety of indoor terrains."
  },
  {
    "objectID": "posts/paper/2022-06-26-keep-learning.html#system-process",
    "href": "posts/paper/2022-06-26-keep-learning.html#system-process",
    "title": "📃Legged Robots that Keep on Learning 리뷰",
    "section": "System Process",
    "text": "System Process\n\n\n\n\n\n\n위의 사진에 보이는 공원과 같은 새로운 환경에서 먼저 로봇 agent가 첫번째 시도로 locomotion task를 진행한다.\n만약에 땅이 고르지 못해서 agent의 학습된 policy를 활용할 수 없는 상황이 되어서 넘어지게 되는 상황이 될 수 도 있다.\n이때 reset controller를 이용해서 빠르게 다시 일어난다.\n실제 task에서 좀 더 몇 번 시도를 하면서 1~3의 과정을 몇 번 반복하게 되고 이 과정에서 policy가 업데이트 되게 된다.\n업데이트가 되면서 policy는 새로운 test 환경에서 제대로 작동할 수 있게 된다."
  },
  {
    "objectID": "posts/paper/2022-06-26-keep-learning.html#how",
    "href": "posts/paper/2022-06-26-keep-learning.html#how",
    "title": "📃Legged Robots that Keep on Learning 리뷰",
    "section": "How",
    "text": "How\n\n강화학습의 reward 가 robot의 on-board 센서로 측정되는 값들로만 디자인 되어야 실제 Real-world에서 작동하면서 fine tuning을 할 수 있다.\nAgile한 behavior를 학습하기 위해서 Motion imitation 기법을 활용했다.\n로봇의 넘어지고 나서 빠르게 정상자세로 회복할 수 있도록 Recovery policy를 학습했다.\n강화학습 알고리즘들 중에서 REDQ(Randomized Ensembled Double Q-Learning) 라는 알고리즘을 사용했는데, 이 알고리즘은 여러개 Q-network들의 앙상블을 통해 randomization을 해서 Q-learning 계열의 알고리즘들의 sample-efficiency와 안정성을 향상시킨 알고리즘이다."
  },
  {
    "objectID": "posts/paper/2022-06-26-keep-learning.html#main-contribution",
    "href": "posts/paper/2022-06-26-keep-learning.html#main-contribution",
    "title": "📃Legged Robots that Keep on Learning 리뷰",
    "section": "Main Contribution",
    "text": "Main Contribution\n\n본 논문의 주요 contribution은 다음과 같다.\n\n\n4족 보행 로봇의 agile한 locomotion skill을 real-world에서 학습하기 위한 fine-tuning 자동화 시스템을 제안하였다.\n처음으로 자동화 reset과 on-board 상태 추정을 통해 real-world에서 fine-tuning이 될 수 있음으로 보였다.\nA1 로봇을 가지고 dynamic skill들을 학습해서 외부 잔디에서 앞으로, 뒤로 pacing을 하고 3가지 다른 지형 특징을 가진 환경에서 side-stepping을 할 수 있었다."
  },
  {
    "objectID": "posts/paper/2022-06-26-keep-learning.html#details-with-hash-tags",
    "href": "posts/paper/2022-06-26-keep-learning.html#details-with-hash-tags",
    "title": "📃Legged Robots that Keep on Learning 리뷰",
    "section": "Details with Hash tags",
    "text": "Details with Hash tags\n\n원 논문의 II. Related Work section 참고\n\n#Cumbersome controller designs\n\n이전의 로봇 controller들은 footstep planning, trajectory optimization, model-predictive control (MPC) 등의 조합으로 만들어지고 있었다. 그러나 이런 방법들은 로봇의 동역학과 각 로봇마다 다르고 각 skill마다 다른 많은 요소들을 고려해야 하기 때문에 정말 어려웠다.\n\n#Sim2Real\n\ntrial-and-error라는 데이터에 매우 의존성이 높은 강화학습 알고리즘의 특성과 하드웨어의 safety 이슈 때문에 보통 로봇 강화학습 agent는 시뮬레이션 기반으로 학습된다. 하지만 시뮬레이션에서 학습하면서 실제로 만나보지 않은 real-world의 모든 조건들을 예상하고 학습하기란 사실상 불가능하며 가장 robust한 policy라고 할지라도 모든 상황에 대해 generalization 되었다고 할 수 없다.\n\n#Real-world\n\n이전에 복잡한 motion들을 학습하게 하기 위해서 environment의 다양한 장치들로 다양한 상태 정보를 만들어서 사용했지만 본 연구에서는 real-world에서 작동하고 있는 로봇에서 fine-tuning을 해야 하기 때문에 로봇의 on-board에서 받을 수 있는 모든 state estimation 정보들을 가지고만 진행했으며 motion capture나 외부 장치들을 별도로 사용하지 않았다.\nscratch부터 실제 환경에서 단순한 구조의 로봇들로 walking gaits들을 학습하는게 아니라, A1 로봇으로 pacing, side stepping 등 매우 자연스럽고 조금은 불안정하고 세밀한 balancing이 요구되는 skill들을 학습할 수 있었다. (기존의 연구들은 balancing에 매우 신경쓴 나머지 느리고 부자연스러운 walking gaits 에 치중한 면이 있었다.) 본 논문의 연구에서 motion imitation과 실제 환경에서의 fine-tuning 이 이런 다이나믹한 task들을 성공시키는데 매우 중요한 역할을 했다. 또한 실제 환경에서 로봇이 작동하면서 넘어질 때, manual하게 로봇의 reset하거나 recovery시키지 않고 강화학습으로 자동적으로 reset 할 수 있는 controller를 만들어서 사용했다.\n\n#Few-shot adaptation\n\n기존의 Adaptation structure라는 구조를 만들어서 학습시켜서 latent 또는 explicit한 환경에 대한 descriptor로 adaptive한 policy를 만드는 연구들이 있었으나, 이 기법들 또한 결국 training에서 경험했던 것들을 기반으로 adaptive함을 보이는 것이므로 실제 test 환경이 이 허용 범위에서 많이 벗어날 경우 제대로 작동안되는 것은 똑같다. 따라서 강화학습으로 지속적인 적응적인 학습능력을 보장해서 어떤 test 환경에서든 잘 작동할 수 있도록 했다.\n\n#RL Algorithm\n\n강화학습 알고리즘으로는 기존의 vision 기반 매니퓰레이터들에서 grasping 작업을 하는 task들에서 많이 쓰인 off-policy model-free RL 기법들을 참고하여 fixed되어 있는 매니퓰레이터들보다 더 challenging한 floating-based 보행 로봇의 locomotion에 적용해서 성공시켰다."
  },
  {
    "objectID": "posts/paper/2022-06-26-keep-learning.html#overview",
    "href": "posts/paper/2022-06-26-keep-learning.html#overview",
    "title": "📃Legged Robots that Keep on Learning 리뷰",
    "section": "Overview",
    "text": "Overview\n아래 사진의 전체 시스템의 개략도에서 볼 수 있듯이 각각의 policy는 하나의 desired skill을 학습하게 된다. 즉 하나의 policy는 forward를, 다른 policy는 backward를, 마지막 다른 policy는 reset을 담당하여 학습하게 된다. 이렇게 다양한 task를 수행할 수 있도록 만든 프레임워크 이기 때문에 Multitask framework인 것이다.\n\n\n\n\n\n\nPseudo Algorithm\n시스템 개략도에서 봤듯이 논문에 나와있는 시스템 전체를 보여주는 Algorithm2 알고리즘은 크게 2개의 과정으로 진행된다.\n\n\n\n\n\n\nAgent의 policy는 시뮬레이션에서 pretrained 한다. (Algorithm 2 line 2~7)\n\n각 에피소드가 끝날 때마다 학습된 recovery policy가 로봇을 다음 rollout을 할 수 있도록 준비시켜준다.\n각 skill을 위한 policy들은 독립적으로 학습되고 recovery policy도 마찬가지로 독립적으로 학습된다.\n\nFine-tuning을 실제 물리적인 환경에서 진행하면서 training process를 계속 이어나갈 수 있다. (Algorithm 2 line 8~14)\n\n시뮬레이션과 실제 환경의 차이를 고려하여 각 policy들의 replay buffer는 초기화 시켜준다.(Algorithm 2 line 12)"
  },
  {
    "objectID": "posts/paper/2022-06-26-keep-learning.html#motion-imitation-off-policy-rl",
    "href": "posts/paper/2022-06-26-keep-learning.html#motion-imitation-off-policy-rl",
    "title": "📃Legged Robots that Keep on Learning 리뷰",
    "section": "Motion Imitation & Off-policy RL",
    "text": "Motion Imitation & Off-policy RL\n\n\n\n\n\nMotion Imitation\nMotion Imiation 방법을 이용하여 reference motion clip들의 skill들을 모방 학습하도록 했는데 이는 Learning Agile Robotic Locomotion Skills by Imitating Animals라는 논문에서 제시한 방법을 따라했다. (Algorithm 1 line1~4)\nReference motion M이 주어지면 agent의일련의 pose들과 비교하여 section III-B에서 소개될 reward function을 기반으로 학습한다. - 이 방법을 통해 reference motion data만 바꿔주면 바로 다른 여러 skill들을 배울 수 있다. - recovery policy를 학습하기 위해서 standing pose를 모방하도록 할 수 있다.(III-C 참고)\nOff-policy RL\noff-policy 알고리즘인 REDQ algorithm 사용했다.(Algorithm 1 line5~9) - SAC 알고리즘을 더 발전시킨 알고리즘 - time step에 대한 gradient step비율을 증가시켜서 강화학습 알고리즘의sample efficiency를 높였다. - 너무 많은 gradient step을 할 경우에 일어날 수 있는 overestimation issue를 앙상블 기법을 이용해서 완화할 수 있었다."
  },
  {
    "objectID": "posts/paper/2022-06-26-keep-learning.html#a.-state-action-spaces",
    "href": "posts/paper/2022-06-26-keep-learning.html#a.-state-action-spaces",
    "title": "📃Legged Robots that Keep on Learning 리뷰",
    "section": "A. State & Action Spaces",
    "text": "A. State & Action Spaces\n\nState space\n\nState는 연속적인 3 timesteps에서 얻은 아래 정보들로 정의했다.\n\nRoot orientation (read from the IMU)\nJoint angles\nPrevious actions\n\nPolicy는 위에서 말한 Proprioceptive input 뿐만 아니라 a goal g_t에 대한 정보도 input으로 받게 된다.\n\ng_t는 future timesteps에서의 reference motion에서 계산된 Target pose (root position, root rotation, joint angles)의 정보를 포함한다.\n4 future target poses 는 현재 timestep에서 약 1초 정도 이후의 pose들이다.\n\n\nAction space\n\nAction은 12 joints들에 대한 PD position targets 이다.\n33Hz의 주파수로 command가 적용된다.\n자연스러운 움직임을 위해 PD targets을 low-pass filter를 로봇에 적용하기 전에 통과시켜준다."
  },
  {
    "objectID": "posts/paper/2022-06-26-keep-learning.html#b.-reward-function",
    "href": "posts/paper/2022-06-26-keep-learning.html#b.-reward-function",
    "title": "📃Legged Robots that Keep on Learning 리뷰",
    "section": "B. Reward Function",
    "text": "B. Reward Function\n\n\\begin{gathered}r_{t}=w^{\\mathrm{p}} r_{t}^{\\mathrm{p}}+w^{\\mathrm{v}} r_{t}^{\\mathrm{v}}+w^{\\mathrm{e}} r_{t}^{\\mathrm{e}}+w^{\\mathrm{rp}} r_{t}^{\\mathrm{rp}}+w^{\\mathrm{rv}} r_{t}^{\\mathrm{rv}} \\\\w^{\\mathrm{p}}=0.5, w^{\\mathrm{v}}=0.05, w^{\\mathrm{e}}=0.2, w^{\\mathrm{rp}}=0.15, w^{\\mathrm{rv}}=0.1\\end{gathered}\n\n\n\nr_{t}^{\\mathrm{p}} : 로봇의 joint rotation 값들을 reference motion의 joint rotation과 맞추도록 하는 reward term\n\n  r_{t}^{\\mathrm{p}}=\\exp \\left[-5 \\sum_{j}\\left\\|\\hat{q}_{t}^{j}-q_{t}^{j}\\right\\|^{2}\\right]\n  \n\n\\hat{q}_{t}^{j} : 시점 t에 reference motion의 j번째 joint의 local rotation\nq_{t}^{j} : 로봇의 j번째 joint local rotation\n\nr_{t}^{\\mathrm{v}} : joint velocities\nr_{t}^{\\mathrm{e}} : end-effector positions\n로봇이 reference root motion을 잘 tracking 하게 하기 위한 reward term\n\nr_{t}^{\\mathrm{rp}} : root pose reward\nr_{t}^{\\mathrm{rv}} : root velocity reward\n\n\n\n이전부터 강조해왔듯이, 실제 환경에서 fine-tuning과정을 진행하기 위해서 on-board 센서들의 값을 이용해서 reward function을 디자인하였고 실제 물리적인 환경에서 구동할 때 이를 상태 추정 기법을 이용해서 reward를 구하게 된다. 따라서 아래의 상태 추정 방법(State Estimation)이 fine-tuning의 성능을 결정하는 중요한 부분이 된다.\n\nReal-world에서 로봇의 linear root velocity를 잘 추정하기 위해서 Kalman filter를 사용했다.\n\n칼만 필터는 IMU 센서에서 acceleration과 orientation 값들을 읽어서 foot contact sensors로 값들을 보정한다.\n처음에 발 끝의 속도를 0으로 생각해서 각 다리의 joint velocities를 고려하여 몸체의 속도를 계산하고 IMU으로부터 추정했던 값을 보정한다.\n\n이렇게 계산된 linear velocity를 로봇의 position 추정값에 통합시킨다.\n\n\n\n\n\n\n위의 그래프들에 볼 수 있듯이(아래에서 위 방향으로),\n\nangular velocity와 orientation 센서 값들은 매우 정확했다.\nlinear velocity는 매우 정확하진 않았지만 허용가능했다.(reasonable)\nposition drifts는 상당히 벗어나는 부분이 있었지만, 각 에피소드에서 reward function을 계산할 정도로의 적합한 값들을 보여주었다."
  },
  {
    "objectID": "posts/paper/2022-06-26-keep-learning.html#c.-reset-controller",
    "href": "posts/paper/2022-06-26-keep-learning.html#c.-reset-controller",
    "title": "📃Legged Robots that Keep on Learning 리뷰",
    "section": "C. Reset Controller",
    "text": "C. Reset Controller\n\nreset policy를 시뮬레이션에서 학습하기 위해 다양한 initial states에서 시작하도록 했다.\n\n→ 로봇을 random한 height & orientation에서 떨어뜨려서 아래 사진에서 볼 수 있듯이 다양한 initial states를 설정\n\n\n\n\n\n\nMotion imitation 목적함수를 수정해서 single, streamlined reset policy를 학습시켰다.\nReference motion을 가지고 로봇이 정확히 어떻게 일어나야 할지를 알려주는 것이 아니라, 아래와 같은 방법으로 reset policy를 학습시켰다.\n\n\npolicy가 rolling right side up을 위한 reward만을 가지고 학습한다.\n만약 로봇이 upright하는데 성공하면 이후에 motion imitation reward를 추가시켜서 학습니다.\n\n이때의 reference motion은 standing pose가 되고 로봇이 똑바로 설 수 있도록 학습시킨다.\n\n\n\n이런 방식으로 학습된 reset policy는 다양한 test 지형에서 fine-tuning 없이도 잘 동작했다.(tranfered well)"
  },
  {
    "objectID": "posts/paper/2022-06-26-keep-learning.html#a.-simulation-experiments",
    "href": "posts/paper/2022-06-26-keep-learning.html#a.-simulation-experiments",
    "title": "📃Legged Robots that Keep on Learning 리뷰",
    "section": "A. Simulation Experiments",
    "text": "A. Simulation Experiments\n\nagent의 policy를 먼저 특정 시뮬레이션 셋팅에서 학습시킨 후에 학습된 시뮬레이션과 또 다른 시뮬레이션 환경 셋팅에 “deployed”한 후 결과를 살펴보았다.\nLearned forward pacing gait가 테스트 환경들에서 얼마나 빨리 적용되는지 확인해보았다.\nStandard dynamics randomization (mass, inertia, motor strength, friction, latency 변동)으로 Pre-train을 flat ground에서 진행했다.\n\nThe test terrains\ntest 환경들로는 총 3가지로 실험하였으며 pre-training 과정의 시뮬레이션 셋팅과 유사한 test 환경 [1]과 pre-training 과정의 시뮬레이션 셋팅과 다소 다른 test 환경 [2], [3]에서 진행됐다.\n\na flat ground\nrandomized heightfield : 랜덤하게 지형의 높이를 설정한 울퉁불퉁한 지형\na low friction surface : 낮은 마찰계수를 가지는 지형, 빙판길과 같은 미끄러운 지형(Training 과정에서 경험한 마찰계수 분포와 한참 동떨어진 마찰계수를 가지고 있음)\n\n비교군\n\nlatent space : 호율적인 다양한 dynamics parameters에 대한 학습을 하기 위해 latent space에 표현된 behaviors을 학습\nRMA: dynamics randomization한 모델. 위에서 언급한 Adaptation Module을 가지고 학습\nVanilla SAC : Soft Actor-Critic 알고리즘으로 학습\nOurs(REDQ): 10개의 Q-functions을 가지고 randomly sample 2로 학습\n\n\n\n\n\n\n\n실험 결과를 살펴보면, RMA는 training 환경에서만 높은 성능을 보여주어 Adaptation Module의 한계점을 명확히 보여주었다. SAC에 비해서 REDQ(Ours)가 sample efficiency가 좋을 뿐만 아니라 수렴하는 Return 값도 높았다."
  },
  {
    "objectID": "posts/paper/2022-06-26-keep-learning.html#b.-real-world-experiments",
    "href": "posts/paper/2022-06-26-keep-learning.html#b.-real-world-experiments",
    "title": "📃Legged Robots that Keep on Learning 리뷰",
    "section": "B. Real-World Experiments",
    "text": "B. Real-World Experiments\n시뮬레이션에서 학습된 Agent를 4개의 real-world 환경(Outdoor 1개, Indoor 3개)에서 test 했다. 모든 (real-world) test 지형 실험은 시뮬레이션의 flat ground에서 pre-training된 agent로 실험한 것이었으며, 처음에 buffer를 5000 samples로 초기화 해주고 시작한 다음 test real world 환경에서 policy를 fine-tuning 해주었다.\n\nOutdoor grassy lawn:\n\nslippery surface를 가지고 있어서 발이 잔디에서 미끄러지거나 흙에 빠질 수 있다.\n앞 혹은 뒤로 움직이는 pacing gait를 fine-tuning 하도록 했다.(pacing gait: 좌나 우의 2개의 다리가 한번에 움직이는 걸음새)\nPre-trained forward pacing policy는 매우 조금만 앞으로 갈 수 있었고, pre-trained backward pacing policy는 잘 넘어지는 경향이 있었다.\n작동한 지 약 2시간 만에, 로봇은 (아주 조금의 넘어짐은 있었지만) 지속적이고 안정적으로 앞 혹은 뒤로 pacing gait를 할 수 있었다.\n\n\n\n\n\n\nIndoor\n\nCarpeted room: 높은 마찰계수를 가지는 지형으로 (카펫이 푹신하므로) 로봇의 고무로 마감되어 있는 발이 시뮬레이션에서 학습된 것과 다르게 안정적이지 않은 컨택을 하게 될 수 있다.\n\n\n\n\n\n\n\nDoormat with crevices: 매트 표면에 발이 빠질 수도 있는 환경이다.\n\n\n\n\n\n\n\nMemory foam: 4cm 정도의 두께의 메모리폼으로 발이 매트리스에 빠지고 평평하고 딱딱한 바닥과 비교했을 때 이 환경에서는 gait(걸음새)가 상당히 변화가 많이 일어날 수 있다.\n\n\n\n\n\n\n\nIndoors에서는, pre-trained side stepping policy가 움직일 때 매우 불안정했고 motion을 끝내기 전에 넘어졌다.\n그러나 각 지형 셋팅에서 2.5 시간 이내로 로봇이 비틀거림 없이 skill을 수행할 수 있었다."
  },
  {
    "objectID": "posts/paper/2022-06-26-keep-learning.html#c.-semi-autonomous-training",
    "href": "posts/paper/2022-06-26-keep-learning.html#c.-semi-autonomous-training",
    "title": "📃Legged Robots that Keep on Learning 리뷰",
    "section": "C. Semi-autonomous training",
    "text": "C. Semi-autonomous training\n\n\n\n\n\n\n전반적인 모든 실험들에서, the recovery policy는 100% 성공적이었다.\n본 논문에서 제시된 방법으로 학습된 reset controller와 Unitree에서 제공한 built-in rollover controller를 비교해보았다.\n\nOn hard surfaces : 두 가지 controllers 모두 효과적으로 잘 작동했지만 built-in 컨트롤러는 learned policy에 비해 상당히 느렸다.\nOn the memory foam : built-in 컨트롤러는 더 성능이 좋지 못했다."
  },
  {
    "objectID": "posts/paper/2022-10-02-vae.html",
    "href": "posts/paper/2022-10-02-vae.html",
    "title": "📃VAE 리뷰",
    "section": "",
    "text": "이번 포스트는 생성모델에서 유명한 Variational Auto-Encoder(VAE)를 다루고 있는 Auto-Encoding Variational Bayes라는 논문 리뷰입니다. 이번 포스트를 정리하면서 가장 많이 인용하고 도움을 받은 오토 인코더의 모든 것를 보시면 훨씬 더 자세하고 깊은 이해를 하실 수 있습니다. 포스트의 순서는 아래와 같이 진행됩니다."
  },
  {
    "objectID": "posts/paper/2022-10-02-vae.html#regularization-term",
    "href": "posts/paper/2022-10-02-vae.html#regularization-term",
    "title": "📃VAE 리뷰",
    "section": "2.1 Regularization term",
    "text": "2.1 Regularization term\nELBO term을 나누었을 때 나왔던 첫번째 Regularization term에 대해 보겠습니다. True posterior를 추정하기 위한 q_\\phi(\\mathrm{z} \\mid \\mathrm{x})은 KL 값을 계산하기 쉽도록 하기 위해 Multivariate gaussian distribution으로 설계합니다. 또한 앞서 이야기했던 것 처럼 controller 부분인 p(z)는 다루기 쉬운 분포이어야 하기 때문에 정규분포로 만들어 줍니다. 그러면 논문의 Appendix F.1에서 볼 수 있듯이 가우시안 분포들 사이의 KL 값은 mean과 std를 사용하여 다음과 같이 쉽게 계산될 수 있습니다.\n\n\n\nRegularization term6"
  },
  {
    "objectID": "posts/paper/2022-10-02-vae.html#reconstruction-error-term",
    "href": "posts/paper/2022-10-02-vae.html#reconstruction-error-term",
    "title": "📃VAE 리뷰",
    "section": "2.2 Reconstruction error term",
    "text": "2.2 Reconstruction error term\nELBO의 두번째 term인 Reconstruction error에 대해 살펴보겠습니다. Reconstruction error의 expectation 표현을 integral로 표현하면 다음과 같고 이는 몬테카를로 샘플링을 통해 L개의 z_{i, l}를 가지고 평균을 내서 구할 수 있습니다. 여기에서 index i는 데이터 x의 넘버링이고 index l은 generator의 distribution에서 샘플링하는 횟수에 대한 넘버링입니다. VAE는 한정된 몬테카를로 샘플링을 통해 효과적으로 optimization을 수행합니다.\n\n\n\nRecontruction error term6\n\n\n\n2.2.1 Reparametrization Trick\n위에서 Reconstruction error를 구하기 위해 샘플링하는 과정에서 backpropation을 하기 위해 Reparametrization trick을 사용하게 됩니다. 단순히 정규분포에서 샘플링 하면 random node인 z에 대해서 gradient를 계산할 수 없기 때문에 random성을 정규분포에서 샘플링 되는 ϵ으로 만들어주고 이를 reparametrization을 해주어서 deterministic node가 된 z를 backpropagation 할 수 있게 됩니다.\n\n\n\nReparametrization trick6\n\n\n# sampling by re-parameterization technique\nz = mu + sigma * tf.random_normal(tf.shape(mu), 0, 1, dtype=tf.float32)\nz를 샘플링하는 generator의 distribution은 Bernoulli로 디자인할 경우 NLL이 Cross Entropy가 되며 Gaussian 분포로 디자인할 경우 MSE가 되어서 보통 계산하기 용이한 2개의 분포 중 하나를 사용하게 됩니다. 모델 디자인의 조건은 데이터의 분포에 따라 결정되는데 데이터의 분포가 continuous 하다면 Gaussian 분포에 가깝기 때문에 Gaussian으로 디자인하고, 데이터의 분포가 discrete 하다면 Bernoulli분포에 가깝기 때문에 Bernoulli로 디자인합니다.\n\n\n\nTypes of generator distributions6"
  },
  {
    "objectID": "posts/paper/2022-06-10-NerveNet.html",
    "href": "posts/paper/2022-06-10-NerveNet.html",
    "title": "📃NerveNet 리뷰",
    "section": "",
    "text": "We address the problem of learning structured policies for continuous control. In traditional reinforcement learning, policies of agents are learned by multi-layer perceptrons (MLPs) which take the concatenation of all observations from the environment as input for predicting actions. In this work, we propose NerveNet to explicitly model the structure of an agent, which naturally takes the form of a graph. Specifically, serving as the agent’s policy network, NerveNet first propagates information over the structure of the agent and then predict actions for different parts of the agent. In the experiments, we first show that our NerveNet is comparable to state-of-the-art methods on standard MuJoCo environments. We further propose our customized reinforcement learning environments for benchmarking two types of structure transfer learning tasks, i.e., size and disability transfer, as well as multi-task learning. We demonstrate that policies learned by NerveNet are significantly more transferable and generalizable than policies learned by other models and are able to transfer even in a zero-shot setting.\n\n보통 강화학습에서 agent들의 policy는 multi-layer perceptrons (MLPs)으로 네트워크를 만들기 때문에 agent가 environment에서 받은 observation들을 단순히 쌓아서(concatenation) policy network에 입력으로 들어가게 된다. 하지만 손의 속도 정보와 발의 속도 정보가 같은 속도 범주이지만 위치가 다르기 때문에 구분이 있을 수 있듯이 agent의 이런 구조적인 특성을 반영해서 policy를 만든다면 observation 정보들간의 구분을 할 수 있을 것이다. 이런 agent의 구조적 관계성을 나타내기 위해서 MLP대신 그래프를 활용하게 되었고 NerveNet을 고안하게 되었다. NerveNet은 그래프 구조로 되어 있는 policy network에서 각 노드들의 정보들이 전파(propagation)되며 agent의 부분들을 나타내는 노드마다 action을 prediction 하게 된다. MuJoCo 환경에서 MLP 기반의 벤치마크들과 비등한 학습결과를 보여주었으며, transfer learning task로 agent의 크기(size)와 agent의 일부 파트가 작동하지 않는(disability) variation을 주었을 때도 잘 학습되었으며 multi-task learning으로 walker 그룹의 다양한 환경에서의 학습 결과들도 좋았다. 이런 결과들을 통해 NerveNet이 transferable할 뿐만 아니라 zero-shot setting도 가능함을 보여주었다.\n\ntransferable - A task를 학습한 네트워크(weights)를 활용하여 B task 학습에도 적용하여 scratch에서 B task를 학습하는 것보다 더 빠르고 효율적인 학습을 가능하게 할 수 있다는 의미. A task 학습에서 습득한 논리체계를 B task에도 적용할 수 있음으로 볼 수 있다.\nzero-shot - Meta learning에서 사용되는 용어로 A task에 대해서 학습된 네트워크가 fine tuning이 없이 바로 unseen new task B에 대해서 좋은 성능을 내는 것을 의미."
  },
  {
    "objectID": "posts/paper/2022-06-10-NerveNet.html#graph-construction",
    "href": "posts/paper/2022-06-10-NerveNet.html#graph-construction",
    "title": "📃NerveNet 리뷰",
    "section": "Graph Construction",
    "text": "Graph Construction\n본 논문에서는 사용한 MuJoCo의 agent들은 이미 구조적으로 tree 구조를 가지고 있다. NerveNet의 핵심 아이디어인 그래프를 구성하기 위해 body와 joint, root라는 3가지 종류의 노드를 설정했다. body 노드는 로봇공학에서 말하는 link 기준의 좌표시스템을 나타내는 노드이고, joint 노드는 모션의 자유도(freedom of motion)을 나타내며 2개의 body 노드들을 연결해주는 노드이다.\n아래는 Ant 환경의 예시인데, 한 가지 그림에서 헷갈리지 말아야 할 점은 그림에서는 마치 body와 root 노드만 노드로 만든것 처럼 보이지만 root와 body, body와 body를 연결하는 엣지들도 실제로는 joint 노드들이다.(we omit the joint nodes and use edges to represent the physical connections of joint nodes.)root라는 노드는 agent의 추가적인 정보들을 담을 부분으로 사용하기 위해 추가한 노드 종류로, 예를 들어 agent가 도달해야 하는 target position에 대한 정보 등이 담겨있다."
  },
  {
    "objectID": "posts/paper/2022-06-10-NerveNet.html#nervenet-as-policy",
    "href": "posts/paper/2022-06-10-NerveNet.html#nervenet-as-policy",
    "title": "📃NerveNet 리뷰",
    "section": "NerveNet as Policy",
    "text": "NerveNet as Policy\n크게 3가지 파트로 NerveNet을 살펴볼 것인데 우선 (0) Notation을 보고 난뒤, (1) Input model (2) Propagation model (3) Output model 순으로 살펴볼 예정이다.\n\n0. Notation\n그래프에서의 노테이션은 다음과 같이 G 라는 그래프는 노드 집합 V와 엣지 집합 E로 구성된다.\n\nG=(V, E)\n\nNervenet policy를 구성하는 그래프는 Directed graph(유향 그래프)이기 때문에 각 노드에서의 in과 out이 따로 명시되게 된다.\n\n노드 u를 중심으로 노드 u로 들어오는 이웃 노드이면 \\mathcal{N}_{in}(u)\n노드 u를 중심으로 노드 u에서 나가는 이웃 노드이면 \\mathcal{N}_{out}(u)\n\n그래프의 모든 노드 u는 타입을 가지게 되고 이를 p_{u} \\in\\{1,2, \\ldots, P\\} (associated note type)로 나타내며 여기에서는 위에 설명한 것과 같이 body, joint, root 3가지 타입이 있다.\n노드들 뿐만 아니라 엣지들도 타입을 정할 수 있는데 c_{(u, v)} \\in\\{1,2, \\ldots, C\\} (associate each edge)로 표기하여 노드쌍 (u, v) 사이의 엣지 타입을 정의할 수 있다.(하나의 엣지에 대해서 여러 엣지 타입을 정의할 수 있지만 여기에서는 심플 이즈 더 베스트 철학으로 하나의 엣지는 하나의 타입만 가지도록 했다)\n이렇게 노드별, 엣지별 타입을 나눔으로써,\n\n노드 타입은 노드들간의 다른 중요도를 파악하는데 도움이 되고\n엣지 타입은 노드들간의 서로다른 관계들을 나타내고 이 관계의 종류에 따라 정보를 다르게 propagation 하게 된다.\n\n이제 시간 노테이션에 대한 부분을 살펴보자. NerveNet에는 시간(time step)의 개념이 2가지 존재한다.\n\n기존 강화학습에서 환경과 agent 사이의 interaction time step을 나타내는 \\tau\nNerveNet의 내부 graph policy에서의 propagation step을 나타내는 t\n\n다시 풀어서 생각해보면, 강화학습의 시간 개념 \\tau 스텝에서 환경으로부터 observation을 받고, 받은 observation을 기반으로 t 스텝동안 NerveNet의 내부의 그래프의 propagation이 일어난다.\n\n\n1. Input model\n위에서 말했듯이 환경과 상호작용으로 observation s^{\\tau} \\in \\mathcal{S}을 받게 된다(time step \\tau). 이 s^{\\tau}는 concatenation된 각 노드의 observation이라고 볼 수 있다. 이제 강화학습 interaction 수준의 \\tau 스텝은 잠시 멈춰두고 그래프 내부의 타임 스텝인 t 수준에서 생각해보자. observation은 node u에 해당하는 x_{u}로 표현할 수 있고 x_{u}는 input network F_{\\mathrm{in}}(MLP)를 거쳐서 고정된 크기의 state vector인 h_{u}^{0}가 된다. h_{u}^{0}의 노테이션을 풀어서 해석하면 노드 u의 propagation step 0 에서의 state vector인 것이다. 이때 observation vector x_{u}가 노드마다 크기가 다를 경우 zero padding으로 맞춰서 input network에 넣어주게 된다.\n\nh_{u}^{0}=F_{\\text {in }}\\left(x_{u}\\right)\n\n\n\n2. Propagation model\nNerveNet의 propagation 과정 노드들 간에 주고 받는 정보를 message라고 하게 되고 이는 노드들 간에 주고 받는 상호작용이라고 생각할 수 있다. Propagation model은 3가지 단계로 나누어서 볼 수 있다.\n\nMessage Computation\n\n전달할 메세지를 계산한다.\npropagation step인 t에, 모든 노드들 u에서 state vector h_{u}^{t}를 정의할 수 있다.\n노드 u로 모아지는(in-coming) 모든 엣지들을 가지고 메시지를 구하게 되는데, 이때 M은 MLP이고 M의 아래첨자 c_{(u, v)} 노테이션에서 알 수 있듯이 같은 종류의 엣지에 대해서는 같은 message function M을 쓴다.\n\n  m_{(u, v)}^{t}=M_{c_{(u, v)}}\\left(h_{u}^{t}\\right)\n  \n예를 들어 아래 그림은 CentipedeEight 의 모습인데, 왼쪽은 실제 agent의 모습을 나타내고 있으며 오른쪽은 agent를 그래프로 나타냈을 때의 모습이다. 여기에서 2번째 torso에서 첫번째 세번째 torso에서 보낼 때 같은 메세지 펑션 M_{1} 을 사용하고, LeftHip과 RightHip으로 보내는 메세지 펑션 M_{2}를 사용하게 되는 것이다.\n\n\n\n\n\n\nMessage Aggregation\n\n앞 단계에서 모든 노드들에 대해서 메세지 계산이 끝난 후에 in-coming 이웃 노드들로부터 온(계산된) 메세지를 모으게 된다. 이때 summation, average, max-pooling 등 다양한 aggregation 함수를 사용할 수 있다.\n\n  \\bar{m}_{u}^{t}=A\\left(\\left\\{h_{v}^{t} \\mid v \\in \\mathcal{N}_{i n}(u)\\right\\}\\right)\n  \n\nStates Update\n\n이제 모은 메세지를 기반으로 state vector를 업데이트 하면 된다!\n\n  h_{u}^{t+1}=U_{p_{u}}\\left(h_{u}^{t}, \\bar{m}_{u}^{t}\\right)\n  \n여기서 업데이트 함수 U 는 a gated recurrent unit (GRU), a long short term memory (LSTM) unit 또는 MLP가 될 수 있다.\nUpdate function의 아래첨자 p_{u}에서 볼 수 있다시피 같은 노드 타입이면 같은 update function U를 쓰게 된다. 이렇게 업데이트된 state vector는 타임 스텝 t가 하나 올라간 t+1 이 된 h_{u}^{t+1}가 된다.\n\n\n이렇게 내부 propagation 과정 3단계(Message Computation, Message Aggregation, States Update)가 T 스텝동안 일어나게 되고 각 노드의 최종 state vector는 h_{u}^{T} 가 된다.\n\n\n3. Output model\n전형적인 RL의 MLP 폴리시에서는 네트워크에서 각 action의 gaussian distribution의 mean을 뽑아내게 된다. std는 trainable한 벡터이다. NerveNet에서도 std는 비슷하게 다루지만 각 노드에 마다 action prediction을 만들게 된다.\nactuator와 연결되어 있는 노드들의 집합을 O라고 하자. 이 집합에 있는 노드들의 최종 state vector h_{u \\in \\mathcal{O}}^{T}는 MLP인 Ouput model O_{q_{u}}에 인풋으로 들어가게 되고 아웃풋으로 각 actuator의 action distribution인 gaussian distribution의 mean \\mu을 출력하게 된다. 여기에서 새로운 노테이션 q_{u}를 볼 수 있는데 q_{u}는 아웃풋 타입, 즉 아웃풋을 내놓는 노드 u의 타입으로 아웃풋 펑션의 아래첨자에 q_{u}에 따라 아웃풋 노드의 타입이 같으면 Output function을 공유할 수 있다. 다시말해 아웃풋 노드 타입에 따라 컨트롤러를 공유할 수도 있는 것이다. 위의 Centipedes의 예시로 보면, 같은 LeftHip 끼리는 컨트롤러를 공유할 수 있다는 것이다.\n\n\\mu_{u \\in \\mathcal{O}}=O_{q_{u}}\\left(h_{u}^{T}\\right)\n\n논문에서 실제로 실험을 해봤을 때 다른 타입의 컨트롤러들을 하나로 통합했더라도(O function을 다 같은 MLP로 사용) 퍼포먼스가 그렇게 해쳐지지 않음을 확인할 수 있었다고 한다.\n여기까지해서 그래프 노테이션을 빌려 그래프 기반 가우시안 stochastic policy를 나타내면 아래의 수식과 같다.\n\n\\pi_{\\theta}\\left(a^{\\tau} \\mid s^{\\tau}\\right)=\\prod_{u \\in \\mathcal{O}} \\pi_{\\theta, u}\\left(a_{u}^{\\tau} \\mid s^{\\tau}\\right)=\\prod_{u \\in \\mathcal{O}} \\frac{1}{\\sqrt{2 \\pi \\sigma_{u}^{2}}} e^{\\left(a_{u}^{\\tau}-\\mu_{u}\\right)^{2} /\\left(2 \\sigma_{u}^{2}\\right)}\n\n\n여기까지 NerveNet의 각 단계를 Walker-Ostrich 환경에서 예시로 한눈에 보기 쉽게 정리한 그림은 아래와 같다."
  },
  {
    "objectID": "posts/paper/2022-06-10-NerveNet.html#learning-algorithm",
    "href": "posts/paper/2022-06-10-NerveNet.html#learning-algorithm",
    "title": "📃NerveNet 리뷰",
    "section": "Learning Algorithm",
    "text": "Learning Algorithm\n이전 파트에서 NerveNet의 내부에서 propagation 스텝 t 단위에서 각 단계들을 자세히 살펴보았다면 이제 강화학습 타임 스텝 \\tau 단위에서 학습의 목적함수와 알고리즘을 살펴보자. 목적함수는 전형적인 RL과 다른 점이 없이 policy의 파라미터 \\theta를 가지고 Return 값을 maximization하는 것으로 한다.\n\nJ(\\theta)=\\mathbb{E}{\\pi}\\left[\\sum{\\tau=0}^{\\infty} \\gamma^{\\tau} r\\left(s^{\\tau}, a^{\\tau}\\right)\\right]\n\n강화학습 알고리즘으로는 PPO과 GAE를 사용했으며 해당 알고리즘들의 내용은 각각 알고리즘들의 원래 수식과 내용들과 상이한 점이 없으므로 각 논문으 참고하면 되기 때문에 이번 논문 리뷰에서는 생략한다.\nPPO와 GAE 알고리즘을 참고하여 위의 목적함수 J를 정리하면 NerveNet의 목적함수는 다음과 같다.\n\n\\begin{aligned}\n\\tilde{J}(\\theta)=& J(\\theta)-\\beta L_{K L}(\\theta)-\\alpha L_{V}(\\theta) \\\\\n=& \\mathbb{E}_{\\pi_{\\theta}}\\left[\\sum_{\\tau=0}^{\\infty} \\min \\left(\\hat{A}^{\\tau} r^{\\tau}(\\theta), \\hat{A}^{\\tau} \\operatorname{clip}\\left(r^{\\tau}(\\theta), 1-\\epsilon, 1+\\epsilon\\right)\\right)\\right] \\\\\n&-\\beta \\mathbb{E}_{\\pi_{\\theta}}\\left[\\sum_{\\tau=0}^{\\infty} \\operatorname{KL}\\left[\\pi_{\\theta}\\left(: \\mid s^{\\tau}\\right) \\mid \\pi_{\\theta_{o l d}}\\left(: \\mid s^{\\tau}\\right)\\right]\\right]-\\alpha \\mathbb{E}_{\\pi_{\\theta}}\\left[\\sum_{\\tau=0}^{\\infty}\\left(V_{\\theta}\\left(s^{\\tau}\\right)-V\\left(s^{\\tau}\\right)^{\\operatorname{target}}\\right)^{2}\\right]\n\\end{aligned}\n\n위의 수식에서 볼 수 있는 value network V를 어떻게 디자인할 것인지가 이번 논문의 다른 포인트로 볼 수 있다. 논문의 기본 아이디어는 policy network를 그래프로 표현하는 것이고, value network는 어떻게 할지 여러 선택지들이 남아있다. 그래서 본 논문에서는 value network의 디자인을 두고 크게 3가지 NerveNet의 변형 알고리즘들을 실험해보았다.\n\nNerveNet-MLP : policy network를 1개의 GNN으로 구성하고 value network는 MLP로 구성\nNerveNet-2 : policy network를 1개의 GNN으로 구성하고 value network는 또 다른 GNN으로 구성(총 GNN 2개 - without sharing the parameters of the two GNNs)\nNerveNet-1 : policy network와 value network 모두 1개의 GNN으로 구성(총 GNN 1개)"
  },
  {
    "objectID": "posts/paper/2022-06-10-NerveNet.html#comparison-on-standard-benchmarks-of-mujoco",
    "href": "posts/paper/2022-06-10-NerveNet.html#comparison-on-standard-benchmarks-of-mujoco",
    "title": "📃NerveNet 리뷰",
    "section": "1. Comparison on standard benchmarks of MuJoCo",
    "text": "1. Comparison on standard benchmarks of MuJoCo\n\n비교군으로 MLP, TreeNet(모든 노드들이 연결 되어 있는 그래프, depth 1)을 사용\n총 8개의 환경에서 실험 - Reacher, InvertedPendulum, InvertedDoublePendulum, Swimmer, HalfCheetah, Hopper, Walker2d, Ant\n충분히 학습하는 스텝을 주기 위해서 1 million을 max로 둠\n하이퍼 파라미터의 경우 그리드 서치로 찾았으며(Appendix 참고) 각 알고리즘의 퍼포먼스를 측정할 때 3번의 run을 랜덤 시드를 바꿔가며 실행시킨 후 평균을 구해서 기록\n대부분의 환경에서 MLP가 잘됐고 NerveNet도 이와 비등한 퍼포먼스를 냈다.\n\n(3가지 케이스에 대한 learning curve, 다른 케이스들에서는 대체로 NerveNet과 MLP가 비슷했다.)\n\n\n\n\n\n\n\n\nHalfCheetah\nInvertedDoublePendulum\nSwimmer\n\n\n\n\n\n\n\n\n\nMLP와 NerveNet이 비슷하고 TreeNet이 많이 안좋았음\nMLP가 좀더 좋은 결과를 냄\nNerveNet이 MLP보다 좋은 성능을 냄\n\n\n\n\n대부분 환경들에서 TreeNet이 NerveNet보다 좋지 않았고 이를 통해서 물리적인 그래프 구조를 가져가는 것이 얼마나 중요한지 알 수 있다."
  },
  {
    "objectID": "posts/paper/2022-06-10-NerveNet.html#structure-transfer-learning",
    "href": "posts/paper/2022-06-10-NerveNet.html#structure-transfer-learning",
    "title": "📃NerveNet 리뷰",
    "section": "2. Structure transfer learning",
    "text": "2. Structure transfer learning\n\nMuJoCo의 환경 하나를 커스텀해서 size와 disability의 변화가 있을 때 transferable 함을 검증\n\nsize transfer - 작은 사이즈의 그래프를 가진 agent를 학습 시킨 후 더 큰 사이즈의 그래프를 가진 agent로 transferable 한지\ndisability transfer - 모든 파트들이 정상작동하는 agent로 학습한 후 일부 파트들이 작동하지 않는 상황의 agent로 transferable 한지\n\n2개 종류의 환경을 커스텀하여 실험 - centipede와 snake\n\ncentipede - 지네와 같이 생긴 agent로 torso body들이 여러개 체인처럼 연결 되어 있고 torso를 중심으로 양쪽에 다리가 1쌍으로 붙어 있다. 하나의 다리는 thigh와 shin으로 구성되어 있고 hinge actuator로 구현되어 있다. 커스텀은 다리의 갯수를 다양하게 해서 여러 커스텀 환경들을 만들었는데, 가장 짧은 agent로는 CentipedeFour 부터 가장 긴 agent로는 CentipedeFourty 로 다리가 40개까지(20쌍) 있는 환경을 만들수 있었다. disability로 일부 파트가 작동하지 않는 환경은 Cp(Cripple)로 따로 표기했다. 이 환경에서 y-direction으로 빨리 앞으로 가는게 목표다.\nsnake - swimmer 환경을 기반으로 커스텀했으며 가장 빨리 진행방향으로 움직이는 게 목표다.\n\n\n\n\n\n\n\n비교군\n\nNerveNet : small agent가 학습한 모델을 바로 large agent에 적용할 수 있었다. agent의 구조가 반복적이기 때문에 반복되는 부분을 더 늘리기만 하면 되기 때문이다.\nMLP Pre-trained (MLPP): agent의 크기가 커짐에 따라 input size가 달라지므로 가장 straightforward하게 첫번째 hidden layer를 그대로 output layer로 사용하고 input layer의 사이즈만 키워서 추가하고 이 input layer는 랜덤 초기화를 해준다.\nMLP Activation Assigning (MLPAA): small agent의 weight들을 바로 large agent의 모델에 넣어주고 weight들의 남는 부분들을 0으로 초기화 해준다.\nTreeNet: MLPAA처럼 스케일을 키워서 0으로 초기화 해준다.\nRandom : action space에서 uniformly하게 샘플링을 하는 policy이다.\n\nResult\n\nCentipede\n1-1. Pretraining\n\n6-다리 모델과 4-다리 모델로 NerveNet, MLP, TreeNet 에서의 퍼포먼스를 비교했다. 여기서 3개의 모델은 앞서 benchmark 비교 실험에서 사용한 비교군들과 동일하다.\n\n\n\n\n\n\n\n4-다리 모델에서는 NerveNet이 가장 Reward가 높고, 6-다리 모델에서는 MLP가 가장 Reward가 높음을 알 수 있다. TreeNet은 두 환경 모두에서 가장 낮다.\n6-다리 모델과 4-다리 모델로 pretraining을 진행한 후 transferable을 실험했다.\n\n1-2. Zero-shot\n\nfine tuning 없이 퍼포먼스를 측정했다.\n퍼포먼스를 쉽게 비교할 수 있도록 average reward와 average running-length를 normalization해서 색으로 아래와 같이 표현했다.(green-good, red-bad)\n\n\n\n\n\n\n\n눈으로 확실히 확인할 수 있듯이 NerveNet의 퍼포먼스가 다른 비교군에 비해 월등히 transferable함을 알 수 있었다.\n또한 learning curve에서 볼 수 있듯이 NerveNet+Pretrain 이 다른 Pretrain 비교군들에 비해 훨씬 높은 reward 시작점에서 시작하고 더 적은 timestep으로 solved 점수에 도달하는 것을 보아 그래프의 구조적 이점을 확실히 활용하고 있음을 알 수 있다.\n\n\n\n\n\n\n\nNerveNet의 agent들은 다른 비교군 agent들에서 보이지 않는 walk-cycle을 가지고 있음을 확인할 수 있었는데, 이는 보행 로봇들은 걸음새에서 반복적인 움직임을 하게 되어 있기 때문에 자연스럽게 cycle을 가지게 되는 것을 agent가 학습했음을 알 수 있다. (반면 MLP는 8-다리 모델에서 모든 다리를 움직이지 않는 모습을 보이기도 했다.)\n\nSnake\n\nsnake환경에서도 NerveNet이 다른 비교군들에 비해 뛰어난 reward 점수를 보여주며 transferable 함을 아래의 도표에서처럼 보여주었다.\n350점 정도가 snakeThree에서 solved된 상태라고 볼 수 있는데 NerveNet의 시작 점수들이 대부분 300점대에서 시작한 것으로 보아 이는 상당한 zero-shot 역량이 있음을 알 수 있다.\n다른 비교군들은 overfitting이 심해서 Random보다 안좋은 결과를 보여주는 점도 흥미롭다.\n\n\n\n\n\n\n\nzero-shot 뿐만 아니라 fine tuning을 하는 learning curve에서도 NerveNet은 Pretrain의 이점을 다른 비교군들에 비해 잘 활용하고 있음을 볼 수 있었다. NerveNet+Pretrain의 시작 reward가 높으며, 특정 size transfer 실험에서는 scratch NerveNet이 넘지 못한 MLP 점수를 NerveNet+Pretrain이 따라잡았다."
  },
  {
    "objectID": "posts/paper/2022-06-10-NerveNet.html#multi-task-learning",
    "href": "posts/paper/2022-06-10-NerveNet.html#multi-task-learning",
    "title": "📃NerveNet 리뷰",
    "section": "3. Multi-task learning",
    "text": "3. Multi-task learning\nNerveNet은 네트워크에 structure prior를 포함한 것이기 때문에 multi-task learning에 유리할 수 있다. 따라서 이를 실험하기 위해 Walker multi-task learning을 진행했다.\n\n2d-walker 환경들 5개 - Walker-HalfHumanoid, Walker-Hopper, Walker-Horse, Walker-Ostrich, Walker-Wolf\n1개의 통합된 network로 학습\n\n비교군\n\nNerveNet : agent들의 형태가 달라 weight들이 다를 수 밖에 없기 때문에 propagation과정에서의 weight matrices와 output만 공유했다.\nMLP Sharing : hidden layer들 간의 weight matrices 를 공유\nMLP Aggregation : 차원이 다른 observation들을 aggregation과정을 통해 첫번째 hidden layer의 크기로 다 맞춰주어서 input으로 넣어줌\nTreeNet: TreeNet도 weight를 공유를 할 수 있지만 agent의 구조적인 정보는 알 수 없다. 단순히 root node를 중심으로 모든 노드의 정보다 aggregation 되기 때문이다.\nMLPs: 각 agent마다 따로 MLP policy를 만들어서 학습(single-task)\n\nResult - multi-task learning 실험이기 때문에 한 두개 러닝 그래프만 볼 수 없고 5개의 러닝 그래프를 같이 봐야 한다. - Single-task policy를 제외하고 모든 환경에서 NerveNet의 퍼포먼스가 좋음을 알 수 있다.\n\n\n\n\n\n\n테이블에서 Ratio가 single-task policy에 비해 multi-task policy의 성능을 percentage로 나타낸 수치인데, MLP의 퍼포먼스가 single-task에서 multi-task로 넘어갔을 때 42%나 퍼포먼스가 줄어드는 것을 확인할 수 있다. (Average-58.6%) 반면에 NerveNet은 성능이 전혀 떨어지지 않는 결과를 보여주었다."
  },
  {
    "objectID": "posts/paper/2022-06-10-NerveNet.html#robustness-of-learnt-policies",
    "href": "posts/paper/2022-06-10-NerveNet.html#robustness-of-learnt-policies",
    "title": "📃NerveNet 리뷰",
    "section": "4. Robustness of learnt policies",
    "text": "4. Robustness of learnt policies\n강화학습 제어에서 robustness는 중요한 지표인데 질량이나 힘과 같은 물리적인 값들의 오차 범위가 어느정도까지 policy가 허용하고 잘 작동하는지를 확인해야 한다.\n\n5개의 Walker 그룹의 환경에서 실험\npretrained agent를 가지고 agent의 질량과 joint의 strength을 변경한 뒤 퍼포먼스 측정\n대부분의 환경과 variation에서 NerveNet의 robustness가 MLP보다 좋음을 알 수 있다."
  },
  {
    "objectID": "posts/paper/2022-06-10-NerveNet.html#interpreting-the-learned-representations",
    "href": "posts/paper/2022-06-10-NerveNet.html#interpreting-the-learned-representations",
    "title": "📃NerveNet 리뷰",
    "section": "5. Interpreting the learned representations",
    "text": "5. Interpreting the learned representations\n실제 폴리시들이 어떤 representation들을 학습했는지 알아보기 위해 CentipedeEight 환경에서 학습된 agent의 final state vector를 가지고 2D, 1D PCA를 진행했다.\n각 다리쌍들(Left Hip-Right Hip)들은 agent의 전체 몸체에서 각기 다른 위치에 있음에도 불구하고 invariant representation을 배울 수 있었음을 PCA를 통해서 알 수 있었다.\n\n\n\n\n\n또한 앞서 Centipede transfer learning 실험 결과에서도 잠깐 언급했던 walk-cycle이 주기성이 뚜렷하게 보였다."
  },
  {
    "objectID": "posts/paper/2022-06-10-NerveNet.html#comparison-of-model-variants",
    "href": "posts/paper/2022-06-10-NerveNet.html#comparison-of-model-variants",
    "title": "📃NerveNet 리뷰",
    "section": "6. Comparison of model variants",
    "text": "6. Comparison of model variants\nValue Network를 어떻게 할 것인지에 따라 NerveNet의 여러 변형이 있을 수 있는데 Swimmer, Reacher, HalfCheetah에서 비교해본 결과, Value Network는 MLP로 한 NerveNet-MLP의 퍼포먼스가 가장 좋았고 NerveNet-1의 퍼포먼스가 2등으로 NerveNet-MLP 와 비슷했다. 이에 대한 잠재적인 이유로 value network와 policy network가 weight를 공유하는 것이 PPO 알고리즘에서의 trust-region based optimitaion에서의 weight \\alpha를 더 sensitive하게 만들기 때문이라고 추론할 수 있다."
  },
  {
    "objectID": "posts/paper/2023-03-12-wasabi.html",
    "href": "posts/paper/2023-03-12-wasabi.html",
    "title": "📃WASABI 리뷰",
    "section": "",
    "text": "이번 포스팅은 WASABI: Learning Agile Skills via Adversarial Imitation of Rough Partial Demonstrations 논문을 읽고 정리한 내용입니다. 4족 보행 로봇 연구에서 많은 연구 성과들을 발표하는 스위스의 ETH Robotic System Lab과 독일의 Max Plank Institude for Intelligent Systems에서 발표한 논문으로, 강화학습에서 중요한 부분들 중 하나인 reward design에 대한 고민을 generatvie adversarial method(WGAN, Wasserstein GAN)를 통해 해결할 수 있음을 보여주었습니다.\n보행 로봇의 모션 제어에서 기본적인 보행뿐만 아니라 다양한 다이나믹한 모션을 수행하도록 로봇의 퍼포먼스를 끌어올리는 방향으로 연구가 활발하게 진행되고 있습니다. 여기서 말하는 다이나믹한 모션들로는 로봇이 공중에서 한바퀴 돌아야 하는 backflip과 같은 기존의 전통적인 보행 제어 연구를 기반으로 rule-based로 제어하기에는 매우 어려운 모션들을 말합니다. 로봇이 이런 모션들을 수행하도록 수학적으로 자세히 명시하고 그리고 모든 물리적 환경요소들을 고려하여 제어하기 어려울 때, 강화학습이라는 인공지능 프레임 워크를 이용하여 reward라는 보상체계를 기준으로 trial-and-error를 통해 모션을 학습하도록 하는 것이 직관적으로 매우 좋은 해결책으로 보입니다.\n하지만 다이나믹한 모션을 각 task로 정의하고 우리가 원하는 방향대로 로봇이 모션들을 학습되기 위해서는 reward를 잘 정의해주어야 하는데 이 과정이 만만치 않게 까다롭고 어려우며, 오히려 수학적인 동역학 모델을 기반으로 제어할 때보다 분석적인 접근이 어렵기 때문에 reward design이라는 과제를 해결해야만 우리가 원했던 다이나믹 모션들을 강화학습을 이용하여 로봇이 수행할 수 있을 것 입니다. 바로 이 부분을 생성모델로 유명한 GAN 모델들 중 하나인 WGAN을 이용하여 해결하고자 했으며 해당 논문에서 가장 흥미로웠던 접근법은 강화학습의 policy를 GAN의 generator 관점으로 바라보고 reward를 추론하도록하는 프레임 워크를 만들었다는 점이었습니다. (이후 관련해서 더 논문들을 찾아보니 생성모델과 강화학습은 닮은 점이 많은 것 같습니다. 관련해서 흥미롭게 읽었던 다른 논문 Connecting Generative Adversarial Networks and Actor-Critic Methods도 관심이 있으시다면 가볍게 읽어보시는 것을 추천드립니다.)"
  },
  {
    "objectID": "posts/paper/2023-03-12-wasabi.html#gan",
    "href": "posts/paper/2023-03-12-wasabi.html#gan",
    "title": "📃WASABI 리뷰",
    "section": "GAN",
    "text": "GAN\n적대적 신경망에 대해 기본적인 이론부터 시작해보겠습니다. GAN은 생성 모델을 학습하기 위한 방법론 중 하나로 Generative, 어떠한 새로운 데이터 생성을 하는, Adversarial 게임과 같이 Discriminator와 Generator라는 2개의 알고리즘 모듈이 경쟁을 하며 학습을 하는 방법론 입니다. 아래 사진에서 보이는 예시로 보면 진짜 모나리자 그림이라는 Real example을 보고 이를 모사한 작품을 파는 화가를 Generator라고 생각해볼 수 있습니다. 그러면 미술 작품 감별사인 Discriminator는 이 작품이 진짜 모나리자 그림인지 아니면 화가가 모사한 가짜 모나리자 인지 판단하게 됩니다. 당연히 Generator 입장에서는 Discriminator가 감별하기 어렵게 점점 더 진짜같은 모나리자를 그리게 되고(new data) Discriminiator 입장에서는 진짜와 가짜 사이에 더 자세하고 민감한 차이를 찾아내어 Generator의 모사품을 찾아내려고 할 것 입니다.\n\n이러한 GAN의 학습 과정에는 지도 학습과 비지도 학습이 모두 들어있습니다. 우선 Discriminator 입장에서는 진짜와 가짜 라벨을 가진, 인풋 데이터가 들어오면 2개의 카테고리들 중 하나를 선택하는 지도학습을 하게 됩니다. Generator는 비지도 학습으로 latent code라는 일종의 trigger 요소인 어떤 벡터를 인풋으로 받으면 진짜 data distribution과 가까운 데이터인 new data를 생성하게 됩니다.\n\n잠깐 data distribution이라는 개념이 GAN에서는 중요한 개념이므로 Probability Distribution(확률 분포)을 간단하게 짚고 넘어가겠습니다. 확률 분포란 어떤 사건을 대변하는 랜덤 변수들의 확률 분포라고 볼 수 있습니다. 주사위를 총 6번 던져서 1, 2, 3, 5가 각각 1번씩 그리고 6이 2번 나왔다면 아래와 같은 확률 분포 그래프를 그릴 수 있고, 이때의 Expectation(기댓값)을 구해보면 1 \\cdot \\frac{1}{6} + 2 \\cdot \\frac{1}{6} + 3 \\cdot \\frac{1}{6} + 4 \\cdot \\frac{0}{6} + 5 \\cdot \\frac{1}{6} + 6 \\cdot \\frac{2}{6} = \\frac{23}{6} \\eqsim 3.8 임을 알 수 있습니다.\n\n이미지를 데이터 포인트 x라고 하고 우리가 가지고 있는 사람 얼굴 이미지 데이터 셋의 분포가 왼쪽의 분포와 같다고 한다면, 여러개의 모드(mode)가 있는데 가장 높은 확률의 mode에서는 금발 여성의 얼굴이 있고 상대적으로 낮은 확률로 흑발의 안경 쓴 남자의 얼굴 이미지가 있음을 알 수 있습니다. 또한 mode가 아닌 매우 낮은 확률을 보이는 분포의 꼬리 부분을 보면 매우 이상한 얼굴 이미지들이 나오는 것을 알 수 있습니다.\n\n바로 우리가 가지고 있는 이미지 데이터 셋 분포(빨강색)과 유사한 데이터 분포(파란색)를 학습하는 것이 생성 모델의 목표이고 이를 Discriminator와 Generator를 가지고 학습하도록 하는 것이 GAN입니다.\n\nDiscriminator의 Objective Function(V)을 보면, 먼저 첫번째 term은 데이터 x는 true dataset distribution인 p_{data}에서 샘플링 되었을 때 Discriminator는 이를 진짜라고 판별해야 하고 이는 output 1(true label)을 출력해야하는 방향으로 학습되어야 합니다. 두번째 term은 fake dataset distribution인, 즉 generator가 만든 데이터일 경우에 가짜라고 판별해야 하고 output 0(fake label)을 출력해야 합니다. 따라서 2개의 term을 모두 maxmization하는 것이 Discriminator의 목표이기 때문에 \\text{max}_DV(\\cdot)이 됩니다.\n\nGenerator의 Objective Function을 보면, 첫번째 true dataset distribution에서 샘플링 되는 부분은 Generator와 상관이 없습니다. 두번째 term에서 Generator에서 나온 ouput new data를 Discriminator에게 넘겨주었을 때 1(true label)로 착각하도록 만들어야 하므로 \\text{min}_GV(\\cdot)이 됩니다."
  },
  {
    "objectID": "posts/paper/2023-03-12-wasabi.html#wgan",
    "href": "posts/paper/2023-03-12-wasabi.html#wgan",
    "title": "📃WASABI 리뷰",
    "section": "WGAN",
    "text": "WGAN\n위에서 설명한 기본적인 GAN을 잘 학습했을 때 확률분포를 그려보면 다음과 같이 Discriminator의 판별 분포가 빨간색 그래프처럼 그려지는 것을 알 수 있습니다. 완벽하게 true distribution인 p_{data}에 대해서는 1을, generated distribution p_G에 대해서는 0을 보여주고 있지만 이런 상황에서는 유의미한 학습이 일어나기 힘듭니다.\n\nOptimal한 Discriminator를 가정하고 Objective function을 다시보면 p_{data}와 p_G가 너무 멀리 떨어져 있어서 사실상 계산된 V(\\cdot)값이 0이기 때문입니다. 따라서 Generator가 두 분포가 가깝도록 만드는 방향으로 학습을 해야 하는데 Classic GAN의 Objective Function에는 이러한 정보를 알려줄 수 있는 부분이 수학적으로 모델링이 되어 있지 않습니다.\n\n따라서 분포들간의 먼 정도를 모델링할 수 있는 WGAN(Wasserstein GAN)이 제안되었고 이에 대해서는 수학적으로 매우 딥한 내용이 있지만 본 포스팅에서는 간단하게 개념적으로 공사장의 포크레인을 이용하여 이해하고 넘어가겠습니다. Wassertein Distance는 Earth mover’s distance라고도 불리는데 이름에서 직관적으로 이해할 수 있듯이, 두 분포를 어떤 흙더미라고 생각하고 우리가 Generated Distribution에 있는 흙들을 Real Distribution의 모양대로 흙들을 옮긴다고 했을 때 드는 cost가 distance로 정의된다고 볼 수 있습니다. (수학적으로 더 궁금하신 분들은 Implicit DGM 29 | Wasserstein Distance with GAN을 추천합니다.) 본 연구에서는 이 WGAN을 이용하여 reward 디자인을 했습니다."
  },
  {
    "objectID": "posts/paper/2023-03-12-wasabi.html#rl-with-gan",
    "href": "posts/paper/2023-03-12-wasabi.html#rl-with-gan",
    "title": "📃WASABI 리뷰",
    "section": "RL with GAN",
    "text": "RL with GAN\nGAN 내용을 설명할 때 이미지 생성 분야의 예시가 직관적이고 쉽기 때문에 이를 가지고 설명하다 보니 문득 그래서 강화학습에서 어떻게 GAN을 사용하는데? 라는 의문이 생길 수 있습니다. 다시 강화학습에서의 여러 어려움들 중 Task reward를 잘 정의해주기가 어렵다는 점을 상기시켜보면 Task reward를 Discriminator가 결정해줄 수 있지 않을까라는 아이디어를 떠올려볼 수 있습니다. 모션의 reference가 될 수 있는 demonstration의 일련의 state들이 true distribution이 되고, policy에서 나오는 일련의 state들이 generated distribution이 되어서, Discriminator가 두 분포를 못 구분할 정도를 task reward로 정의한다면 policy가 demonstration에서 나타난 다이나믹한 모션들을 따라하도록 학습할 수 있는 지표가 될 수 있을 것 입니다. 이전에 locomotion이나 backflip 등의 각각의 모션마다 task reward를 hand design 할 때는 각 모션에서 보행 로봇의 발이 어떻게 움직여야 하는지, 몸체의 속도가 어떠해야 하는지 일일이 reward로 고려하고 여러 reward term들을 weighted sum하는 방식이었지만 이 GAN 방식을 이용하면 각 모션에 대한 demonstration의 state들을 보고 어떤 모션을 어떻게 따라해야하는지 agent의 policy가 알아서 task reward를 높이는 방향으로 학습할 수 있는 것 입니다."
  },
  {
    "objectID": "posts/paper/2023-03-12-wasabi.html#problem-definition",
    "href": "posts/paper/2023-03-12-wasabi.html#problem-definition",
    "title": "📃WASABI 리뷰",
    "section": "Problem Definition",
    "text": "Problem Definition\n이전에 AMP 방식에서 모션의 자연스러움을 학습하기 위해 Motion data가 매우 well-defined 되어 있어야 한다고 했습니다. 하지만 이러한 Motion data(혹은 demonstration)을 얻기는 어렵고 특히나 보행과 같이 이미 많이 연구가 되어왔고 동물들의 모습에서도 많이 관찰될 수 있는 task와는 다르게 다이나믹한 backflip하는 모션 task들은 참고할 데이터들도 매우 적고 만들어내기도 어렵습니다. 이런 문제 상황을 본 연구에서는 Rough하고 Partial한 demonstration만 있는 문제로 파악하고 Rough한 모션 데이터라는 것은 실제 로봇이나 동물이 움직여서 얻은 데이터가 아닌 사람이 로봇을 단순히 들고 움직여서 얻은 데이터를 말하며 Partial하다는 것은 로봇의 모션 데이터라고 해서 로봇을 구성하고 있는 모든 joint들의 움직임에 대한 데이터가 아닌 로봇의 몸체에 대한 정보만 있는 모션데이터만 있는 것을 말합니다.\n\n말로만 들으면 잘 와닿지 않기 때문에 위에 사진에서 한 연구자가 backflip하는 demonstration 데이터를 얻기 위해 로봇을 들고 손으로 그냥 한번 뒤집어주는 모습을 보면서 다시한번 설명을 해보겠습니다. 앞서 설명했듯이 로봇이 backflip하는 작동을 해서 데이터를 얻지 않고 사람이 단순히 로봇을 들고 원하는 모션의 demonstration 데이터를 얻습니다. 여기서 Backflip demonstration 데이터는 로봇의 12개의 joint들에 대한 정보는 없이 몸체에 대한 정보(base linear, angular velocity, projected gravity, base height)만을 포함하게 됩니다. 여기서 demonstration 데이터에 대한 놀라운 점은 로봇이 직접 움직여서 얻은 데이터도 아니고 실제 동물의 모션 데이터도 아니기 때문에 물리적으로도 시간적으로도 로봇 플랫폼에서는 사실상 따라하기 어려운 데이터라는 것입니다. 이런 demo 데이터만 있다고 문제상황을 가정한 이유는 backflip과 같이 다이나믹하고 다양한 모션에 대해서는 reference가 될 만한 motion data를 well-defined하기 어렵기 때문입니다.\n이쯤에서 다시한번 AMP와 WASABI를 다시 비교해보면, 두가지 방법 모두 expert의 action이 없이도 reference가 될 수 있는 motion data(혹은 demonstration)를 가지고 reward engineering을 잘해서 모션 제어를 할 수 있었다는 점에서 공통점이 있습니다. 하지만 AMP는 well-defined한 모션 데이터가 있어야 가능한 방법론인 반면 WASABI는 로봇의 몸체에 대한 partial한 모션 데이터만 있으면 학습할 수 있었고 AMP는 모션의 주요 reward를 디자인한 것이 아니라 자연스러움을 위한 보조적인 style reward 디자인을 했고 WASABI는 각 모션에 대한 task reward를 디자인 한 것이 큰 차이점이라고 볼 수 있습니다."
  },
  {
    "objectID": "posts/paper/2023-03-12-wasabi.html#reward-design",
    "href": "posts/paper/2023-03-12-wasabi.html#reward-design",
    "title": "📃WASABI 리뷰",
    "section": "Reward Design",
    "text": "Reward Design\nPartial하고 Rough한 모션 demo들을 가지고 어떻게 하면 다이나믹한 모션에 대한 reward를 정의할 수 있을까요?\n\nWASABI에서 제안한 전체적인 알고리즘 구조는 아래와 같습니다. r^I, r^R, r^T 라는 각각의 reward가 합쳐지는 것을 볼 수 있는데요 이제부터 각각의 reward가 어떤 의미와 목적을 가지고 있는 것인지 살펴보겠습니다.\n\n\nImitation(Task) Reward\n우선, task reward는 다이나믹 모션의 demo를 잘 모방(imitate)할 수 있도록 해야할 것 입니다. 그래서 imitation reward 혹은 task reward로 불리며 여기서 WGAN 방법을 이용해서 정의하게 되는 부분입니다. 다시한번 이야기하지만 우리가 backflip을 하는 학습을 하기 위해서 로봇의 몸체를 공중에 올리고 pitch 방향으로의 회전을 360도 해야해!라고 말해주는 imitation reward function(hand-designed)을 사용하는 것이 아니라 demo(true) distribution을 보고 이를 따라가는 generated distribution을 policy가 학습할 수 있도록 하는 것이 이 방법의 핵심입니다.\n\n잠깐 앞에서 이야기 했듯이 우리가 사용하는 demo 데이터는 well-defined한 데이터가 아닌 사람이 로봇을 들고 모은 데이터이기 때문에 로봇의 base에 대한 데이터(O)로 한정적입니다. 하지만 policy에서 generated된 observation 데이터(S)는 로봇의 각 joint에 대한 정보 등 더 많은 정보가 있는 vector space이기 때문에 true distribution과 generated distribution을 비교가능한 상태로 만들어주기 위해 Mapping function \\phi를 사용하여 맞춰줍니다. 쉽게 생각하자면 정보량이 더 많은 S를 차원이 적은 O로 맞춰주기 위해 joint position, velocity, last action과 같은 부분을 가리고 data distribution을 Discriminator에게 넘겨주는 것으로 볼 수 있습니다.\n\nmapping function을 통해 차원을 맞춘 \\phi(s) 와 o는 GAN의 objective function에서 Discriminator의 인풋으로 들어가는 seq. of states(observations)이며 아래와 같이 일정 time horizon H동안 모아진 states 벡터들로 볼 수 있습니다. 이러한 seq. of states들을 가지고 Discriminator가 만든 reward distribution을 각각 LSGAN(Least Squares GAN)과 WGAN의 objective function으로 아래와 같이 나타내 볼 수 있습니다. 여기서 LSGAN은 WGAN의 비교군이 되는 또 다른 GAN의 알고리즘이며 LSGAN의 Objective function을 해석해보면, policy에서 나온 state history를 가지고 나온 reward distribution은 -1에 가깝도록 demo를 통해 나온 reward distribution은 +1에 가깝도록 하는 것으로 볼 수 있습니다. 반면, WGAN은 이 두 분포간의 wasserstein distance 줄이도록하는 방향으로 학습합니다. 두 가지 GAN 모두 policy에서 나온 seq. of states로 나온 task reward distribution을 demo의 seq. of states로 나온 task reward distribution을 맞춰가도록 학습하는 것은 공통적입니다.\n\n\n이렇게 Discriminator를 통해 나온 task reward는 바로 사용되는 것이 아니고 zero-mean unit-variance로 만들어주는 과정을 한번 거친 후 비로소 Task(Imitation) Reward로 만들어집니다.\n\n\n\nRegularization Reward\n이전에 AMP에서의 Style reward의 역할을 WASABI에서는 Regularization Reward가 대신한다고 볼 수 있습니다. 이 reward는 task-dependent하지 않은 task-agnostic한 term들로 이루어져 있어서 backflip 모션을 하든 locomotion 모션을 하든 로봇의 자연스럽고 에너지 효율적인 모션을 위해 부가적으로 더해지는 reward라고 볼 수 있습니다.\n\n\n\nTermination Reward\n마지막으로 agent가 모션을 충분히 학습하기도 전에 episode를 더 빨리 끝내는 것이 이득이라 판단하고 학습이 잘 이루어지지 않는 경우를 방지하고자 Termination Reward를 추가해주었습니다. T는 episode를 너무 빨리 끝내버린 경우에 대해서 0 또는 1로 판단하는 인디케이터 역할을 하게 되고, termination에 대한 고려는 Imitation reward의 분포에서 나온 \\sigma와 할인율 \\gamma를 고려하여 다음과 같이 정해주게 됩니다.\n\n\n\nTotal Reward\n앞서 설명한 Imitation reward r^I, Regularization reward r^R, Termination reward r^T를 모두 합산하여 Total reward가 계산되게 되고 이를 Agent에게 학습 피드백으로 보내주게 됩니다. 이때 r^I와 r^T는 모션 task 마다 다르게 정의될 수 있는 부분이므로 task-related한 부분이라고 볼 수 있으며 r^R는 어떤 모션 task인지 상관없이 항상 동일한 reward term이기 때문에 task-agnostic한 부분이라고 할 수 있습니다. 물론 여기서 해당 연구의 contribution이 두드러진 부분은 Imitation reward r^I이라고 할 수 있습니다."
  },
  {
    "objectID": "posts/paper/2023-03-12-wasabi.html#induced-imitation-reward-distributions",
    "href": "posts/paper/2023-03-12-wasabi.html#induced-imitation-reward-distributions",
    "title": "📃WASABI 리뷰",
    "section": "Induced Imitation Reward Distributions",
    "text": "Induced Imitation Reward Distributions\n우선 Imitation Reward Distribution이 정말 의미있게 학습을 했는가(Informative한 reward distribution을 만들어 냈는가)를 보기 위해 reward distribution을 시각화해보았습니다. 먼저 Informative한 분포라는 것은 어떤 분포를 말하는가를 짚어볼 필요가 있습니다. 아래 사진의 오른쪽 2개의 분포 그래프에서 평평한 분포(파란색)보다는 뾰족한 분포(초록색)가 여러 x값들에 대해 분별적인 y값(확률)을 가지고 있기 때문에 더 informative하다고 할 수 있습니다.(더 자세한 내용은 정보이론을 살펴보셔도 좋을 것 같습니다.)\n왼쪽의 2개의 그래프는 각각 LSGAN과 WGAN(WASABI)를 가지고 학습했을 때, O의 요소들 중 고정된 pitch rate(\\dot\\theta)와 height(z)를 가지고 Imitation reward 분포를 시각화한 그래프입니다. LSGAN보다 WGAN으로 학습한 분포가 reward range도 더 넓고 더 구분되는 분포를 가지고 있는 것을 볼 수 있습니다. 마지막으로 세번째 그래프는 학습 과정 중에 r^I의 분포를 그린 것으로 LSGAN은 -1과 1, 각각으로 reward targeting을 하게 되는 objective function을 가지고 있었기 때문에 넓고 다양한 reward distribution을 가지지 못한 모습을 볼 수 있고 그에 반해 WGAN은 약 -5~2 정도의 range를 가지는 넓은 reward distribution을 가지고 있는 것을 확인할 수 있었다고 합니다."
  },
  {
    "objectID": "posts/paper/2023-03-12-wasabi.html#learning-to-mimic-rough-demonstrations",
    "href": "posts/paper/2023-03-12-wasabi.html#learning-to-mimic-rough-demonstrations",
    "title": "📃WASABI 리뷰",
    "section": "Learning to Mimic Rough Demonstrations",
    "text": "Learning to Mimic Rough Demonstrations\n그럼 정말로 Demo 모션 데이터들을 얼만큼 잘 따라 학습할 수 있었을까요? 이에 대한 지표는 단순히 reward가 높다고 판단할 수 있는 것이 아니라 모션의 유사성을 판단할 수 있는 다른 metric이 필요합니다.\n\nDynamic Time Warping\nDynamic Time Warping이란 각 데이터의 시간의 길이도 다르고 데이터 포인트의 수도 다른 2개의 시계열 데이터를 비교할 때 사용하는 방법으로 기존의 Euclidean distance라면 측정할 수 없거나 정확한 비교가 어려운 점을 DTW를 이용하면 시간적인 밀림이나 소실된 데이터 포인트까지 고려하여 시계열 데이터 간의 유사도를 판단할 수 있습니다. 바로 이 방법을 이용해서 사람이 들고 만들었던 demo의 모션 데이터와 실제 학습 후 policy에서 만들어낸 모션 데이터 간의 차이를 측정해보았습니다.\n\n\\tau_\\pi는 policy에서 만들어진 trajectory를, \\tau_M은 demo에서 따온 trajectory를 말하며 아래의 실험 결과표는 각각 WASABI와 LSGAN에서의 4 task에 대한 DTW를 구한 값을 나타내고 있습니다. DTW가 낮을수록 demo 데이터와의 유사성이 높은 것이며 잘 모션을 따라 학습했다고 볼 수 있습니다.(아래 Stand Still은 단순히 가만히 서 있는 모션의 데이터와 demo 데이터 간의 DTW 값을 나타낸 것이며 비교를 위한 DTW의 최대 상한선을 나타낸 것으로 볼 수 있습니다.)\n\n\n\nHandcrafted Task Reward\n또 다른 지표로는, 해당 모션 task에 대한 Handcrafted task reward로 점수를 매겼을 때 그 점수가 더 높다면 해당 모션을 잘 학습했다고 판단하는 지표가 있습니다. 예를 들어 STANDUP은 몸체의 pitch angle이 90도에 가깝고 몸체의 높이가 높고 몸체의 z축이 중력방향에 수직이 되는 상태라면 해당 모션을 잘 수행하고 있다고 볼 수 있을 것 입니다. 이처럼 우리가 원하는 모션에 대한 Handcrafted task reward를 계산해서 학습 iteration 마다 그려보면 오른쪽 그림과 같이 WASABI를 가지고 학습한 reward 점수가 대체적으로 LSGAN에 비해 높은 것을 알 수 있습니다. 아래 표에서는 학습을 끝낸 후 각 task에 대한 handcrafted reward 점수이며 맨 아래 점수는 최고 상한 기준 점수로 볼 수 있습니다. 표에서 볼드체로 표시된 부분은 roll-out을 했을 때 모션을 눈으로 확인한 결과 잘 수행했다고 판단한 경우를 나타내면 WASABI로 학습한 4가지 task 모두에서 성공적인 학습 결과를 볼 수 있었다는 것을 볼 수 있습니다."
  },
  {
    "objectID": "posts/paper/2023-03-12-wasabi.html#evaluation-on-real-robot",
    "href": "posts/paper/2023-03-12-wasabi.html#evaluation-on-real-robot",
    "title": "📃WASABI 리뷰",
    "section": "Evaluation on Real Robot",
    "text": "Evaluation on Real Robot\n학습이 시뮬레이션에서만 멈춘다면 당연히 의미가 없는 것이므로 실제 로봇을 가지고 해당 policy의 학습 결과를 확인해봐야 합니다. 따라서 WASABI로 학습한 policy를 가지고 실제 로봇으로 작동을 해보고 이때 10개의 marker를 이용해서 모션 데이터를 얻어 DTW를 측정해보았습니다. 그 결과 표에서 볼 수 있듯이 Sim-to-Real의 퍼포먼스 차이가 거의 없었고 실제 로봇에서도 4가지 task 모두 다 잘 수행하는 것을 확인할 수 있었습니다. 이 부분은 실험영상에서 직접 확인할 수 있습니다.\n\nLeap\n\nWave\n\nStand up\n\nBackflip"
  },
  {
    "objectID": "posts/paper/2023-03-12-wasabi.html#cross-platform-imitation",
    "href": "posts/paper/2023-03-12-wasabi.html#cross-platform-imitation",
    "title": "📃WASABI 리뷰",
    "section": "Cross-platform Imitation",
    "text": "Cross-platform Imitation\n사실 강화학습은 특정 로봇 플랫폼에서 학습한 결과를 다른 configuration을 가진 로봇 플랫폼에 바로 적용하기 어렵습니다. 하지만 WASABI 알고리즘은 처음에 Rough하고 Partial한 demo 데이터를 가지고 학습했기 때문에 다른 로봇 플랫폼에 적용해보는 것이 가능했으며 기존에 Solo 8 로봇 플랫폼을 가지고 학습한 policy를 단순히 로봇 플랫폼의 크기 차이만을 고려하여 base height를 0.25m 조금 더 큰값으로 수정해서 Anymal-C 로봇 플랫폼에 적용했을 때 특별한 추가적인 학습 과정없이도 적용할 수 있었다고 합니다. 이때에도 DTW 값을 찍어서 확인한 결과, 낮은 DTW 값과 함께 시뮬레이션으로 roll-out을 했을 때에 로봇이 잘 작동되는 것을 확인할 수 있었다고 합니다."
  },
  {
    "objectID": "posts/paper/2023-05-07-accessibility.html",
    "href": "posts/paper/2023-05-07-accessibility.html",
    "title": "📃K-Accessibility 리뷰",
    "section": "",
    "text": "이번 포스팅은 최근 ICRA(International Conference on Robotics and Automation) 2022에서도 발표된 Accessibility-Based Clustering for Efficient Learning of Locomotion Skills 논문을 읽고 정리한 내용입니다. 강화학습으로 로봇 제어를 학습할 때 어떻게 효율적으로 initial state distribution을 탐색하도록 만들어 줄 수 있을까?라는 질문을 K-means++ 알고리즘과 유사한 K-Access라는 알고리즘을 고안하여 해결한 논문입니다. 해당 논문에서는 quadruped robot의 Recovery와 Backflip 모션 학습을 보여주었습니다."
  },
  {
    "objectID": "posts/paper/2023-05-07-accessibility.html#initial-state-distrubutions",
    "href": "posts/paper/2023-05-07-accessibility.html#initial-state-distrubutions",
    "title": "📃K-Accessibility 리뷰",
    "section": "1.1 Initial state distrubutions",
    "text": "1.1 Initial state distrubutions\n앞서 Locomotion과 Recovery를 비교하며 살펴보았는데 강화학습으로 하는 로봇 제어의 관점에서 매우 큰 차이점 하나가 더 있습니다. 바로 Initial State Distribution, 강화학습의 Robot Agent가 학습 Episode를 시작하는 맨 처음의 State들의 분포입니다. Locomotion에서는 command(컨트롤러로 조작하는 로봇의 desired velocity 혹은 간단하게 방향키 조작으로 생각할 수 있음)를 따라 움직이는 것이기 때문에 Initial State로 로봇의 standing 자세를 가지고 학습 Episode를 시작하게 됩니다. 반면 Recovery는 로봇이 넘어진 상황(자세)가 각 Episode의 Initial State가 됩니다. 넘어진 자세는 매우 다양하기 때문에 어떤 넘어진 자세는 정상상태로 회복하기가 상대적으로 쉬운 반면, 어떤 자세는 정상상태로 회복하기가 어렵기 때문에 Recovery task에서는 RL(Reinforcement Learning) agent가 Initial State Distribution을 잘 탐색하고 학습할 수 있도록 만들어주는 것이 매우 중요합니다.\n\n위 사진에서 처럼 RL Agent가 탐색해야하는 전체 State Space와 어떤 한 Initial state(혹은 Initial pose, orange dot)와 유사한 state들의 집합 영역 Effective Exploration Region(EER)을 주황색 원 영역으로 표시할 수 있습니다. 여기서 주황색 원 안의 영역의 State들은 원 중심의 하나의 Initial State를 탐색하고 학습하고 나면 어렵지 않게 강화학습 Policy가 잘 학습할 수 있는 State들이라고 볼 수 있습니다. Case 1은 전체 탐색해야 하는 State Space를 빈틈의 최소화하도록 많은 Initial state를 학습하지만 각 EER들이 많이 중복되어 학습되기 때문에 학습 효율이 매우 떨어지는 것을 알 수 있습니다. Case 2에서는 적은 Initial state로 학습해서 State Space가 잘 커버되지 않았을 뿐만 아니라 목표로하는 Target State도 잘 학습되지 않아 학습 Policy의 성능이 매우 떨어지는 것을 알 수 있습니다. Case 3는 목표로하는 Target State는 EER에 들어가서 Policy가 학습한 state라고 할 수 있지만 전체 State Space에서 커버되지 못한 state들이 있기 때문에 Corner case들(Policy가 잘 작동되지 않는 경우)이 있어 Policy의 robustness가 떨어진다고 볼 수 있습니다. 따라서 가장 이상적인 상황은 Case 4에서처럼 Target State도 EER의 범주에 들어가 있고 전체 State Space도 적절한 수의 Initial State들로 탐색되어 Policy의 Robust한 상황이라고 할 수 있습니다."
  },
  {
    "objectID": "posts/paper/2023-05-07-accessibility.html#pose-of-quadruped-robots",
    "href": "posts/paper/2023-05-07-accessibility.html#pose-of-quadruped-robots",
    "title": "📃K-Accessibility 리뷰",
    "section": "1.2 Pose of Quadruped Robots",
    "text": "1.2 Pose of Quadruped Robots\n그렇다면 Initial State, 즉 4족 보행 로봇의 자세(pose)는 어떻게 표현할 수 있을까요? 전복된 상황은 넘어져 있는 로봇의 자세로 표현할 수 있을 것 입니다.\n\n전복된 상황은 움직임이 없는 넘어진 정적(Static) 상황이라 가정하고 로봇의 상황을 다음과 같이 2가지 정보로 표현할 수 있습니다. 첫번째로는 몸체의 기울어짐을 표현하는 Projected gravity vector로 지구 중력 방향의 벡터를 (0, 0, -1)이라고 했을 때, 로봇 몸체의 프레임에 gravity vector를 projection하고 normalized한 3차원의 벡터 정보는 몸체의 기울어짐을 표현할 수 있습니다. 두번째 요소는 로봇의 각 다리에 3개씩 배치되어 관절이 되는 12개의 revolute joint(motor) angle 입니다."
  },
  {
    "objectID": "posts/paper/2023-05-07-accessibility.html#distance-between-poses",
    "href": "posts/paper/2023-05-07-accessibility.html#distance-between-poses",
    "title": "📃K-Accessibility 리뷰",
    "section": "1.3 Distance between poses",
    "text": "1.3 Distance between poses\nPose를 정의한 다음으로 살펴볼 부분은 여러 pose들 간의 관계를 어떻게 정의할 수 있을까에 대한 부분을 고민해볼 수 있습니다. 여러 pose들 간에 가깝다(비슷하다), 멀다를 파악하기 위해서는 거리(Distance)를 정의할 수 있어야 합니다. 가장 직관적으로 pose를 이루고 있는 요소들 간의 유클리디안 거리를 생각해볼 수 있습니다. 앞서 정적인 자세를 구성하는 Projected gravity vector 와 Joint angles의 유클리디안 거리를 계산해서 나온 수치를 기반으로 pose가 서로 비슷하다, 많이 다르다를 판단할 수 있을 것 입니다.\n\n하지만 그림에서의 예시를 통해 유클리디안 거리가 Non-sense하다는 것을 볼 수 있습니다. 3가지 자세, Backward Leaning(B), Forward Leaning(F), Lying(L)를 가지고 유클리디안 거리를 계산해보면 B-F의 거리가 F-L의 거리보다 큰 수치인 것을 확인해볼 수 있습니다. 하지만 로봇을 직접 제어해서 자세를 transition한다고 생각했을 때, F에서 B로의 transition이 F에서 L로의 transition이 훨씬 어렵기 때문에 단순하게 구성 요소들의 유클리디안 거리로 pose들 간의 거리를 정의하는 것은 제어적인 측면에서 말이 된다고 볼 수 없습니다."
  },
  {
    "objectID": "posts/paper/2023-05-07-accessibility.html#sampling-static-poses",
    "href": "posts/paper/2023-05-07-accessibility.html#sampling-static-poses",
    "title": "📃K-Accessibility 리뷰",
    "section": "3.1 Sampling Static Poses",
    "text": "3.1 Sampling Static Poses\n전복된 다양한 자세들을 샘플링하기 위해서 로봇의 base frame의 roll, pitch 각도를 일정 범위에서 랜덤하게 샘플링하고 12개의 joint position도 로봇의 configuration을 고려하여 upper/lower limit range에 있는 각도로 자세를 set해서 전복된 자세를 만듭니다. (이때 yaw 방향은 flat terrain에선 의미가 없기 때문에 0으로 셋팅합니다.) 샘플링된 자세로 pose를 set 했을 때 self-collision을 확인한 뒤 self-collision이 되지 않은 자세 2.4k개를 sampling 합니다.\n\n\n\n예시 사진은 해당 논문의 코드를 연구실에서 개발된 AiDIN-VIII 로봇에 적용한 모습입니다"
  },
  {
    "objectID": "posts/paper/2023-05-07-accessibility.html#estimating-accessibility-values",
    "href": "posts/paper/2023-05-07-accessibility.html#estimating-accessibility-values",
    "title": "📃K-Accessibility 리뷰",
    "section": "3.2 Estimating Accessibility Values",
    "text": "3.2 Estimating Accessibility Values\n앞 단계에서 샘플링한 2.4k개의 pose들 중 1000개만 가지고 Accessibility를 측정하게 됩니다. 우선 2.4k개 중 1000개만 가지고 진행하는 이유는 학습 이후 Policy를 테스트하기 위한 Initial state로 사용하기 위해서 1.4개의 pose는 남겨놓는 것 입니다. 앞서 유클리디안 거리가 metric으로써 좋지 않은 점을 예시를 보며 확인할 수 있었기에 논문에서는 이를 대체할 metric으로 Accessibility라는 개념을 제안합니다.\n\n위의 예시는 어떤 pose A에서 pose B로의 Accessibility를 계산하는 과정을 보여줍니다. 특정 pose에서 다른 특정 pose로 transition하는 과정을 progress라는 작은 timestep들로 쪼개고 각 timestep에 해당하는 transition angle을 PD controller로 제어하게 됩니다. pose를 구성하는 12개의 joint position(angle)은 continuous value이기 때문에 처음과 끝 pose의 angle을 안다면 linear interpolation을 할 수 있습니다. progress를 scaled timeline(0~1로 normalized)이라고 하고 쪼갠 timestep 하나를 변수 t로 본다면 매 순간의 desired transition angle 은 t \\cdot \\text{[joint angle of B]} + (1-t) \\cdot \\text{[joint angle of A]}으로 계산될 수 있습니다. 이렇게 계산된 desired transition angle을 따라가도록 PD제어를 하면서 충분히 pose B에 가까워졌는가?를 판단하게 되는데 이때의 기준은 유클리디안 거리로 계산된 joint position distance, base의 height distance, gravity vector distance이 매우 작은 오차 범위내에 들어갔는지가 됩니다. pose A에서 pose B로 충분히 가까워진 해당 시간 t를 기록하게 되는데, 3초 내에 pose B에 가까워진 상태로 평형상태에 도달하는지 체크하게 됩니다. 저자가 공개한 코드에서 확인해봤을 때 20초를 상한선으로 설정하고 1000 pose \\times 1000 pose Time 매트릭스로 평형상태에 도달한 시간을 기록합니다.\n\n앞서 유클리디안 거리로 판단하는 것이 좋지 않다고 주장할 때는 pose들이 충분히 달랐을 때 pose들 간의 관계 정의로 사용하기에 부적절함을 들어 타당하지 않다고 주장한 것이었고, 현재 pose가 transition이 되었는가를 판단하기 위한 기준으로 유클리디안 거리가 매우 작은지로 판단하는 것은 similarness를 판단하는 것이기에 motivation을 해치지 않는다고 볼 수 있습니다.\n\n\n이렇게 측정한 transition time을 가지고 State Space를 해석해본다면 pose A(s_0)에서 pose B(s_1)으로의 시간 t(s_0, s_1)이 어떤 특정 시간 t_0이하라면 두 pose 사이 관계는 High Accessibility를 가지고 있다고 볼 수 있습니다. 반면, 만약 t(s_0, s_1)이 어떤 특정 시간 t_0 초과라면 Low Accessibility 라고 할 수 있고 이때의 기준이 되는 특정 시간 t_0가 EER R의 경계를 결정합니다. 따라서 이러한 Radial Boundary를 만들기 위해 앞서 계산한 Time 매트릭스(t(s_i, s_j))를 가지고 e^{-t(s_i, s_j)}을 계산한 것을 바로 Accessibility라고 정의하게 됩니다."
  },
  {
    "objectID": "posts/paper/2023-05-07-accessibility.html#clustering",
    "href": "posts/paper/2023-05-07-accessibility.html#clustering",
    "title": "📃K-Accessibility 리뷰",
    "section": "3.3 Clustering",
    "text": "3.3 Clustering\nK-Access Algorithm\n이제 State Space 상의 pose들간의 거리를 정의하는 Accessibility 값을 구한 다음에 어떻게 하면 클러스터링을 잘할 수 있을 것인가?에 대한 고민으로 넘어가게 됩니다. 각 cluster의 centroid가 되는 pose를 정할 수 있어야 하고 몇개의 cluster 갯수가 적절할 지 판단하는 알고리즘으로 K-Access알고리즘을 제안합니다.\n\n우선 결론적으로 cluster의 갯수의 적절성을 Index 지수가 최대가 되는 값로 판단하게 되는데, 이 Index 지수는 Intra-cluster Accessibility와 Inter-cluster Accessibility, 마지막으로 Regularization Term까지 합산하여 결정하게 됩니다.\n\nIntra-cluster Accessibility: 이름에서도 볼 수 있듯이 특정 클러스터에 속해있는(=내부에 있는) sample들(각 pose를 지칭)과 centroid sample간의 Accessibility 값들 중 최소값입니다. 이 값은 Index 지수에 positive sum이 되기 때문에 의미를 해석해본다면 한 클러스터에 속해있는 sample들의 centroid로 향하는 응집력이라고 볼 수 있습니다. Intra-cluster accessibility의 차원은 1000개 샘플이 자신이 속한 클러스터 centroid와의 값을 계산하므로 1000 dimension을 가지고 있습니다.\nInter-cluster Accessibility: 클러스터들 간에 overlapping이 되지 않고 적절히 거리를 유지하며 각 EER이 전체 State Space를 커버할 수 있도록 하기 위해서 centroid sample 간의 Accessibility의 평균을 구한 값\nRegularization Term: 클러스터의 개수가 너무 커지지 않도록 하는 부분으로 Index에 negative sum이 되는 부분입니다. \\alpha 값으로 Regularization의 비중을 높일 수 있는데 논문에서는 1을 사용했습니다.\n\n\nK-means++ VS. K-Access\nK-Access 알고리즘은 기존에 ML에서 자주 사용되는 클러스터링 알고리즘인 K-means++ 알고리즘을 기반으로 만들어진 알고리즘입니다. K-means++ 알고리즘처럼 (1) Initialize the centroids (2) Assignment step (3) Update step 단계를 거치는 것은 비슷하지만 K-means++ 알고리즘에서는 (3)단계에서 평균값을 기반으로 클러스터링이 진행되는 반면 K-Access 알고리즘에서는 robustness를 보장하기 위해 Maximal neighborhood accessibility를 사용합니다.\n좀 더 자세한 알고리즘 과정을 알아보고 싶으신 분들은 아래 Pseudo Code를 확인해주세요.\n\n\nPseudo Code of K-Access\n\n\n\n\nClustering Analysis\n논문에서 사용한 Bittle 로봇 플랫폼으로 clustering을 진행했을 때 43개의 cluster가 최적의 갯수로 정해집니다. 각 클러스터에 속하는 샘플 수를 히스토그램으로 확인해보면 아래 왼쪽 그래프같이 그려지며 이중 해당 클러스터에 속한 샘플 수가 많은 순서대로 top 20개의 클러스터들 간의 inter-cluster accessibility를 Chord graph를 가지고 시각화를 해보면 오른쪽 그래프와 같이 그려집니다. Chord graph에서 강조된 부분들은 0.15 이상의 Accessibility(약 1.9초 이내의 transition time)를 가진 부분들이며 옅게 표시된 부분들은 0.05 이하의 Accessibility(약 3초 이상 transition time)를 가지는 부분들입니다.\n\n\nChord graph 시각화 방법에 대해서는 해당 논문을 기반으로 실제 제가 연구하고 있는 로봇 플랫폼을 이용하여 적용한 코드 실습은 다음 포스팅 에서 확인할 수 있습니다."
  },
  {
    "objectID": "posts/paper/2023-05-07-accessibility.html#reinforcement-learning-process",
    "href": "posts/paper/2023-05-07-accessibility.html#reinforcement-learning-process",
    "title": "📃K-Accessibility 리뷰",
    "section": "3.4 Reinforcement Learning Process",
    "text": "3.4 Reinforcement Learning Process\n마치 Machine Learning에서 Feature Engineering이 많은 주의를 요하는 작업이듯이 앞서 Initial State를 정하는 작업을 진행하고 드디어 강화학습 과정에 들어오게 되었습니다. 유명한 강화학습 알고리즘인 SAC(Soft-Actor-Critic)을 단순한 MLP 레이어로 만들어서 사용했고 Policy Network의 Input과 Ouput 설계도 관련 연구들의 convention과 크게 다르지 않기 때문에 자세한 설명은 생략하고 특징적인 부분을 살펴보겠습니다.\nReward Functions w/ RBF\n\n해당 논문에서 다른 논문들의 강화학습 MDP 설계와 다른 특징적인 부분은 보상함수 설계 부분이었습니다. 일반적으로 Reward function은 각 Reward Term들을 Linear Weigthed Sum형식을 가집니다. 하지만 해당 논문에서는 RBF(Radial Basis Function)를 사용하여 각 Reward를 weighted sum한 값으로 최종 reward를 계산한 것을 알 수 있습니다. 사실 Reward Function을 설계하는 부분은 강화학습 연구에서 Reward Engineering 이슈가 큰 것처럼, 다분히 설계자의 의도와 설명이 필요한 부분이지만 논문에서 자세히 설명이 되어 있지 않고 Main Contribution이 아니라고 생각해서 그런지 Linear sum과 비교한 실험값도 있지 않아서 RBF를 사용한 이유를 파악하기 어려웠습니다.\n\n \n\nLecture 16 - Radial Basis Functions Slides(Caltech)\n\n\n따라서 이 부분은 RBF 커널에 대해 공부하고 나서 제가 생각한 이유를 덧붙이겠습니다. RBF 커널은 기본적으로 Gaussian Distribution 모습으로 target value와 data 간의 radial한 거리 가중치를 주게 되는데, linear sum과 비교했을 때 무한 차원 영역에서 매우 멀리 떨어져 있는 data로부터 영향을 덜 받을 수 있는 장점을 가지고 있습니다. 따라서 Reward를 계산하는 데에 RBF 커널을 통해 계산한 의도는 Maximization해야 하는 Reward term들을 단순히 Linear sum하는 것보다 여러 카테고리의 Reward target 값들에 민감하게 반응할 수 있는 정도를 \\alpha값(Slide에서는 \\gamma로 표현)을 이용하여 학습의 좋은 지표가 될 수 있는 Reward space를 설계한 것으로 보입니다.\n\nReward Term에서 사용된 Symbol의 의미가 궁금하신 분들은 아래 table을 확인해주세요.\n\n\n\nSymbols of Reward Terms for DRL\n\n\n\n\nOther Tasks - Backflip\n해당 논문에서는 Recovery 뿐만 아니라 Locomotion 보다 더 다이나믹한 모션도 학습하는 것을 보여주기 위해 Backflip 학습도 K-Accessibility 알고리즘을 이용하여 학습을 진행하였습니다. (이전에 리뷰했던 WASABI 논문에서도 다이나믹한 모션 4가지 중 하나를 Backflip으로 학습 결과를 보여주었던 것과 같은 맥락으로 해당 모션 Task를 설정했다고 보시면 됩니다.)"
  },
  {
    "objectID": "posts/paper/2025-06-02-sparsh.html",
    "href": "posts/paper/2025-06-02-sparsh.html",
    "title": "📃Sparsh 리뷰",
    "section": "",
    "text": "Sparsh는 시각 기반 촉각 센서를 위한 자기 지도 학습 기반의 일반적인 촉각 표현 모델을 제안합니다.\n본 연구는 힘 추정, 슬립 감지, 자세 추정 등을 포함하는 6가지 촉각 중심 작업으로 구성된 표준 벤치마크인 TacBench도 함께 제시합니다.\n평가 결과, 자기 지도 학습으로 사전 학습된 Sparsh는 제한된 라벨 데이터 환경에서 작업/센서별 모델보다 월등히 우수한 성능을 보이며 다양한 작업과 센서에 걸쳐 일반화됨을 입증했습니다."
  },
  {
    "objectID": "posts/paper/2025-06-02-sparsh.html#연구-목표-및-주요-기여",
    "href": "posts/paper/2025-06-02-sparsh.html#연구-목표-및-주요-기여",
    "title": "📃Sparsh 리뷰",
    "section": "2.1 연구 목표 및 주요 기여",
    "text": "2.1 연구 목표 및 주요 기여\n이 논문의 핵심 목표는 여러 종류의 촉각 센서에 공통으로 적용 가능한 일반적인 촉각 표현을 학습하는 것입니다. 즉, 한 번 거대한 촉각 이미지 데이터로 사전학습한 백본 신경망을 갖추면, 개별 작업별로 일일이 센서 특성에 맞춘 모델을 새로 설계하거나 대량의 레이블을 모으지 않아도, 다양한 로봇 조작 과제에 촉각 정보를 활용할 수 있게 하자는 것입니다. 저자들은 이 목표를 달성하기 위해 자기 지도학습 기법을 촉각 데이터에 적용하고, 학습된 표현의 효과를 체계적으로 검증할 벤치마크를 구축하였습니다.\n논문에서 밝힌 주요 기여는 다음 세 가지로 요약됩니다:\n\n**범용 촉각 표현 모델 (Sparsh)**을 제시하였습니다. 다양한 비전 기반 촉각 센서 (예: DIGIT, GelSight 등)가 생성하는 이미지를 한데 모은 460,000장 이상의 촉각 이미지 데이터셋으로 SSL 사전학습한 공통 인코더를 개발하였습니다. 별도의 작업별 레이블 없이 마스킹 복원과 자기 증류 등의 자기 지도 학습 기법으로 학습된 이 모델들은 여러 센서에 걸쳐 사용할 수 있는 일반적인 촉각 특징 표현을 제공합니다.\nTacBench 벤치마크를 구축하였습니다. 이는 다양한 센서로부터 수집된 6가지 촉각 관련 과제에 대한 데이터와 평가 프로토콜로 구성된 벤치마크로, 촉각 표현의 성능을 표준화된 방식으로 평가할 수 있습니다. TacBench의 과제들은 접촉 힘 추정, 미끄럼 감지 등의 촉각 물리량 이해 과제부터 물체 위치 추적, 그립 안정성 예측, 재질 인식 등의 물체 인지 과제, 그리고 비드 미로 조작 같은 로봇 조작 계획 과제까지 폭넓게 포함하고 있습니다.\n대규모 촉각 데이터셋의 수집 및 통합을 수행하였습니다. 저자들은 기존에 공개된 여러 촉각 센서 데이터셋(YCB-Slide, Touch-and-Go, ObjectFolder 등)에 더해, 자체적으로 Touch-Slide라는 새 데이터셋을 수집하여 총 66만장 이상의 촉각 이미지 데이터를 확보했습니다. 이 중 **약 46만장(70%)**을 사전학습에 사용하고 나머지는 검증에 사용하였으며, 별도로 TacBench의 각 과제에 맞는 레이블된 데이터셋도 구축하여 공개하였습니다. 이러한 노력은 촉각 표현 학습을 위한 데이터 규모를 한층 끌어올려, 이전의 어떤 연구보다 1자리수 이상 많은 이미지로 모델을 훈련할 수 있게 하였다는 의의를 갖습니다.\n\n요약하면 Sparsh 연구는 (1) 다양한 촉각 센서에 일반화되는 대규모 사전학습 촉각 표현 모델을 만들고, (2) 그 성능을 평가할 수 있는 표준 벤치마크와 데이터셋을 제시했으며, (3) 이를 통해 한정된 레이블 데이터로도 높은 성능을 달성할 수 있음을 보여준 것입니다. 실제로 TacBench 실험 결과, 사전학습한 Sparsh 모델이 작업별로 처음부터 끝까지 학습한 모델보다 평균 95.1% 높은 성능을 보였으며(레이블 데이터 33~50% 사용 시), 특히 잠재 공간 표현(latent representation)을 학습하는 **Sparsh (DINO)**와 Sparsh (IJEPA) 변형이 가장 우수했다고 합니다. 이는 촉각 이미지의 미묘한 변화와 노이즈를 직접 복원하는 픽셀 공간 학습보다, 잠재 특징 공간에서의 예측 학습이 효과적임을 시사합니다."
  },
  {
    "objectID": "posts/paper/2025-06-02-sparsh.html#방법론-자기-지도학습-기반-촉각-표현-학습",
    "href": "posts/paper/2025-06-02-sparsh.html#방법론-자기-지도학습-기반-촉각-표현-학습",
    "title": "📃Sparsh 리뷰",
    "section": "2.2 방법론: 자기 지도학습 기반 촉각 표현 학습",
    "text": "2.2 방법론: 자기 지도학습 기반 촉각 표현 학습\nSparsh의 핵심은 **자기 지도학습(SSL)**을 통해 레이블 없는 촉각 이미지 데이터에서 유용한 표현을 학습하는 것입니다. 이를 위해 저자들은 최신 컴퓨터 비전 SSL 기법들을 촉각 도메인에 맞게 변형하여 활용하였습니다. 전체 프레임워크는 크게 대규모 촉각 이미지 사전학습 단계와 다운스트림 과제 평가 단계로 나뉩니다. 사전학습 단계에서 하나의 비전 트랜스포머(ViT) 인코더가 여러 센서의 촉각 이미지를 입력으로 공통된 잠재 표현을 학습하고, 이후 다운스트림 단계에서는 이 **인코더를 고정(freeze)**시킨 채 각 과제별로 간단한 디코더나 **프로브(probe)**를 학습시켜 표현의 품질을 평가합니다.\n 그림 2: Sparsh 모델의 자기 지도학습 기법들. (a) 이번 연구에서 통합한 대규모 촉각 이미지 데이터셋의 구성: 새로운 Touch-Slide(인간이 장난감 부품을 문지르는 데이터)와 기존 YCB-Slide, Touch-and-Go, ObjectFolder 데이터셋 등으로 총 약 66만 장의 촉각 이미지가 사용되었다. 이 중 70%에 해당하는 약 46만 장을 자기 지도 사전학습에 활용하였다. (b) Sparsh (MAE) – 마스킹 자동인코더(Masked Autoencoder) 방식: 입력 이미지의 상당 부분(예: 75%)을 무작위로 가린 후, 남은 일부분만을 보고 전체 이미지를 복원하도록 인코더-디코더를 학습시킨다. 인코더는 ViT 기반이며, 마스크된 영역 복원은 간단한 디코더를 통해 수행되고 L2 픽셀 손실로 학습된다. (c) Sparsh (DINO) – 자기 증류(self-distillation) 방식: 동일한 구조의 학생 네트워크와 EMA로 업데이트되는 교사 네트워크 두 개를 두고, 하나의 촉각 이미지에 서로 다른 변환(crop 등)을 가한 두 입력을 통해 각각 특징을 추출한다. 각 특징을 별도의 MLP 헤드에 통과시켜 임의의 범주 분포(클러스터 확률)를 예측하고, 교사 출력에 학생 출력이 맞춰지도록 크로스엔트로피 손실로 학습한다. 이렇게 하면 레이블 없이도 학생 네트워크가 교사의 군집화된 표현을 모방하며 의미 있는 잠재 표현을 얻게 된다. (d) Sparsh (I-JEPA) – Joint Embedding Predictive Architecture 방식: 학생(컨텍스트 네트워크)과 교사(타겟 네트워크)로 구성되며, 교사 네트워크는 EMA로 갱신된다. 학생 네트워크는 이미지의 일부 영역만 관찰하여 특징을 만들고 작은 예측기(predictor)를 통해 교사 네트워크의 특징을 맞춰보는 작업을 한다. 구체적으로, 학생은 이미지의 상당 부분을 마스킹한 글로벌 시야를 가지고, 교사는 이미지의 국소 패치들만을 본다. 학생의 컨텍스트 특징을 통해 교사의 지역 특징을 예측하도록 L1 손실로 학습함으로써, 부분 관찰을 기반으로 전체 정보를 추론하는 능력이 향상된다. Sparsh에는 정적인 이미지에 적용한 I-JEPA뿐 아니라, 연속적인 비디오 프레임에 적용한 V-JEPA도 포함되어 있다.\n사전학습에 사용된 모델 아키텍처는 기본적으로 ViT-Base (패치 크기 14) 구성입니다. 입력으로 들어오는 촉각 이미지는 배경 차감(background subtraction)을 거치는데, 이는 각 센서에서 접촉이 없을 때의 기본 영상을 빼줌으로써 조명이나 마커 패턴 차이 등 센서별 잡음을 줄이기 위함입니다. 이렇게 하면 같은 종류의 센서 내에서는 조금 다른 제조 공정으로 인한 배경 차이도 보정되어, 모델이 진짜 접촉에 의한 변화에 집중할 수 있게 됩니다. 또한 촉각 센싱은 시간적인 맥락이 중요한 경우가 많기 때문에, 저자들은 시계열 정보를 입력에 반영하였습니다. 구체적으로, 정적인 이미지 기반 SSL 기법들(MAE, DINO 등)에는 한 센서 프레임에서 약 80ms 간격으로 떨어진 두 장의 이미지를 채널 차원으로 붙여서 한 입력으로 사용하였습니다. 보통 촉각 센서가 60 FPS로 동작함을 감안하여 5프레임 차이를 둔 것으로, 이는 인간이 미끄러짐을 감지하고 힘을 조절하는 반응 시간과 비슷한 수준입니다. 한편 비디오 기반 SSL 기법(V-JEPA)에는 4프레임 길이의 짧은 클립(동영상 조각)을 입력으로 사용하여 시간에 따른 변화까지 학습하도록 하였습니다. 이렇게 두 장 또는 여러 장의 연속 이미지를 활용함으로써, 촉각 변화의 동적 패턴(예: 미끄러지기 전의 미세한 움직임)을 포착할 수 있도록 한 것이 특징입니다.\nSSL 학습은 무엇을 예측하느냐에 따라 픽셀 공간 또는 잠재 공간에서 이루어지는데, 앞서 언급했듯이 저자들은 잠재 공간에서의 학습이 더 효율적일 것으로 가설을 세웠습니다. 촉각 이미지는 센서의 국소적 접촉만 담고 있어 한 장면만으로는 모호성이 존재할 수 있고, 촬영 조건에 따라 마커나 조명 변화 같은 방해 요소도 큽니다. 따라서 모든 작은 디테일까지 복원하려는 픽셀 재구성보다는, 더 추상화된 특징에 집중하는 방법(예: DINO, JEPA)이 이런 불확실성과 잡음을 무시하고 유용한 패턴을 학습하는 데 유리할 것이라 본 것입니다. 실제 실험에서도 나중에 보겠지만, DINO나 I-JEPA처럼 잠재 표현 학습을 한 모델이 MAE같이 픽셀 복원을 한 모델보다 전반적으로 우수한 다운스트림 성능을 보였습니다.\nSparsh 사전학습은 150 에포크 동안 진행되었으며, AdamW 옵티마이저와 코사인 학습률 스케줄 등이 사용되었습니다. 모든 모델은 학습 시 [CLS] 토큰을 사용하지 않고, 패치 임베딩들의 풀링이나 별도 헤드를 통해 학습되었습니다. 특히 DINO의 경우 [CLS] 토큰 대신 ViT의 특정 레지스터를 활용하여 클러스터 확률 출력을 얻는 등, 약간의 구조 변형이 있었습니다. 하이퍼파라미터로는 DINO와 JEPA 방식에서 EMA 모멘텀 계수(예: DINO 0.998, IJEPA 0.996)와 학습률 등이 조정되었고, 배치 크기는 MAE 100, 나머지 150으로 세팅되었습니다. 최종적으로 학습된 Sparsh 인코더는 약 0.86억 개 파라미터(ViT-Base 수준)이며, GPU 기준으로 100 FPS 이상의 추론 속도를 보여 실시간 활용에도 무리가 없음을 확인하였습니다 (예: Sparsh (DINO) ~112FPS)."
  },
  {
    "objectID": "posts/paper/2025-06-02-sparsh.html#실험-tacbench-벤치마크-평가",
    "href": "posts/paper/2025-06-02-sparsh.html#실험-tacbench-벤치마크-평가",
    "title": "📃Sparsh 리뷰",
    "section": "2.3 실험: TacBench 벤치마크 평가",
    "text": "2.3 실험: TacBench 벤치마크 평가\nSparsh의 효과를 검증하기 위해 저자들은 TacBench라 불리는 벤치마크를 구축하여 일련의 다운스트림 과제 실험을 수행했습니다. TacBench에는 앞서 소개한 대로 6개의 대표적인 촉각 과제(T1–T6)가 포함되어 있습니다. 각 과제는 서로 다른 센서와 데이터셋으로 구성되며, 구체적인 내용은 다음과 같습니다:\n\n[T1] 힘 추정 (Force Estimation): 촉각 센서의 젤에 가해지는 **3축 힘(수직+두 축의 전단력)**을 추정하는 회귀 문제입니다. 저자들이 구축한 Shear Load 데이터셋을 사용하며, 로봇 팔에 장착된 촉각 센서(DIGIT 또는 GelSight Mini)로 반구형, 뾰족한, 평평한 모양의 인덴터(indenter)를 눌러가며 동시에 힘 센서 값과 촉각 영상을 기록하였습니다. DIGIT은 60fps, GelSight Mini는 25fps로 촬영되어 총 각 7.5만 샘플의 정렬된 영상-힘 데이터가 얻어졌습니다. 평가 지표는 **3축 힘의 RMSE(평균 제곱근 오차)**이며, 학습 시 실제 힘값을 정규화하여 L1 손실로 예측하도록 디코더(작은 MLP)를 학습시켰습니다.\n[T1A] 힘장 시각화 (Force Field Visualization): T1의 부가 실험으로, 촉각 이미지로부터 젤 표면 전체의 접촉 힘 분포 (정규력 분포 및 전단력 벡터장)를 추정하는 과제입니다. 마커가 있는 센서의 경우 마커 움직임 추적으로 전단 변형장을 얻는 것이 가능하지만, 마커가 없는 센서에서는 전체 장(field)의 ground truth를 얻기 어려워 이 작업이 잘 수행되어오지 않았습니다. 저자들은 Sparsh 표현을 이용해 마커 없는 센서의 힘장도 추정이 qualitatively 가능함을 보였는데, **정규력장은 깊이지도(depth map)**로, 전단력장은 옵틱 플로우 문제로 프레임 간 변위를 예측하도록 설정하여 감독없이 디코더(CNN+DPT 구조)를 학습시켰습니다. 그 결과 Sparsh (DINO) 모델을 활용하면, 접촉 패치의 움직임 방향, 예를 들어 미끄럼 방향이나 비틀림(torsion), 접촉 시의 퍼져나가는 형태 등을 유의미하게 보여주는 전단력 벡터장을 그려낼 수 있었습니다. 이는 **그림 4 (vi)**에 시각화 예시가 제시되어 있습니다.\n[T2] 미끄럼 감지 (Slip Detection): 물체가 손가락에서 미끄러지는지 여부를 이진 분류하는 과제입니다. 실험을 위해 T1과 동일한 장비로 반구형 인덴터를 사용하되, 일부 구간에서는 센서 표면이 미끄러지도록 힘의 방향을 조절하여 stick-slip 동작을 만들어냈습니다. 마찰 원뿔 모델에 따라, 정적 마찰 한계를 넘은 경우를 미끄럼 발생으로 레이블링 하였고 (정적 마찰계수는 실험적으로 추정), 총 12.5만 샘플 중 약 **13%**가 미끄럼으로 레이블되었습니다. 데이터 불균형이 있기 때문에 평가에는 F1-score를 주로 사용했습니다. 학습 시에는 미끄럼 유무 분류 MLP와 힘 변화량(regression) MLP를 멀티태스크로 동시에 학습시켰는데, 힘 변화 (특히 전단력의 변화)가 미끄럼과 밀접한 관련이 있어 두 값을 함께 예측하면 성능이 좋아짐을 관찰했기 때문입니다.\n[T3] 물체 자세 추적 (Pose Estimation): 촉각 이미지를 통해 센서에 접촉한 물체의 상대 자세(SE(2) 변환) 변화를 추정하는 과제입니다. 이는 손가락과 물체 사이의 미끄러짐 정도(평면 이동 및 회전)를 추적하는 것으로 볼 수 있습니다. 데이터셋은 DIGIT 센서로 사람이 물체(예: YCB 물체)를 살짝 미끄러뜨리는 장면을 촬영하고, 동기화된 모션 캡처 시스템으로 물체의 실제 자세 변화를 기록하여 만들었습니다. 총 4.9만 쌍의 (이전 이미지, 현재 이미지)와 그 사이의 물체 변환 레이블이 있으며, 학습 시 분류를 통한 회귀(regression-by-classification) 방식을 채택했습니다. 즉, 평면 상의 x, y 이동 및 회전 변화를 **연속값 대신 여러 구간으로 양자화(binning)**하여 다중 클래스 분류 문제로 변환한 것입니다. 이동은 수백 μm 정도의 해상도로, 회전은 수 도(degree) 단위로 구간화하여, 각 자유도에 대해 별도의 Softmax 출력을 내도록 하였습니다. 평가 지표로는 분류 정확도(정답 구간 맞춘 비율)를 사용했습니다.\n[T4] 그립 안정성 예측 (Grasp Stability): 로봇 손가락으로 물체를 잡았을 때 성공적으로 들었는지(안정적으로 파지되었는지) 여부를 맞추는 이진 분류 과제입니다. 이는 촉각 연구에서 오랫동안 다뤄진 주제인데, 본 논문에서는 기존 Feeling of Success 데이터셋을 활용했습니다. 이 데이터셋은 병 따기용 로봇 손가락에 GelSight 센서를 부착하여 100여 개 물체를 잡은 후 성공/실패를 기록한 것으로, 하나의 시도마다 잡기 전, 잡는 중, 잡은 후의 세 장의 촉각 이미지가 제공됩니다. 총 약 9300회 시도 중 성공과 실패 사례가 모두 포함되어 있습니다. 본 연구에서는 한 손가락의 촉각 정보만 사용하므로, 세 개 이미지 중 잡기 전과 잡는 중 두 이미지를 히스토리로 입력으로 주었고, 레이블은 그립 성공 여부(예/아니오)입니다. 데이터셋에 공식 분할이 없어서 임의로 8000여 개 시도를 학습에, 1300여 개를 테스트에 사용하였습니다. 성능 지표는 분류 정확도입니다.\n[T5] 직물 재질 인식 (Textile Recognition): 촉각만으로 다양한 직물의 재질을 분류하는 과제입니다. 기존 연구에서 정의한 Clothing Dataset을 사용하였는데, 이는 GelSight 2017 센서(마커 패턴 있음)로 가죽, 면, 폴리에스터 등 20가지 직물을 잡았을 때의 촉각 영상이 담긴 짧은 비디오 클립 4467개로 이루어져 있습니다. 각 클립은 10~25프레임 길이이며, 로봇이 옷감을 살짝 쥐었다 놓는 동작 등을 포함합니다. 우리는 제공된 메타데이터의 학습/시험 분할을 따랐습니다. 이 문제는 20-class 분류이며 평가 지표는 정확도입니다.\n[T6] 비드 미로 (Bead Maze) 조작: 장난감인 비드 미로(고리 모양의 구슬을 막대 기둥을 따라 움직이는 퍼즐)를 로봇이 촉각 피드백으로 풀도록 하는 강화 학습적 과제입니다. 한쪽 끝에 구슬이 걸려 있는 구불구불한 막대 경로를 따라 구슬을 다른 끝까지 움직이는 것이 목표로, 로봇은 구슬을 집은 손가락에 달린 촉각 센서로부터 저항을 느끼며 힘을 조절해야 합니다. 시각적으로는 손가락에 가려 잘 안 보이고, 마찰로 인한 미세한 막힘 등을 감지해야 하기 때문에 촉각에 의존하는 문제 설정입니다. 저자들은 프랑카(Franka) 로봇팔과 DIGIT 센서를 이용하여 다양한 모양의 미로에서 50회 데모 시연을 모았습니다. 절반은 VR 장치를 활용한 원격 조작으로, 절반은 사람의 수동 조작으로 수행되었습니다. 총 3.4만 개의 (촉각 이미지 시퀀스, 로봇 관절 명령) 쌍이 데이터로 확보되었고, 이를 이용해 Diffusion Policy 라는 최신 Behavior Cloning 알고리즘으로 정책(policy) 학습을 합니다. 이때 시각용으로 설계된 원래 Diffusion Policy의 인코더를 Sparsh 사전학습 인코더로 교체하여, 촉각 관측을 처리하도록 만들었습니다. 학습된 정책은 현재까지의 촉각 이미지 히스토리(예: 2~3장)와 현재 로봇 관절각을 입력으로 받아, 다음 관절 각 변화량을 출력합니다. 평가에서는 데모와 로봇의 실행 결과 간의 거리 차이(trajectory error)를 짧은 구간별(3cm 이동당)로 측정하여 누적 오차를 계산하였으며, 최종적으로 실제 로봇에 정책을 실행하여 얼마나 먼 거리까지 구슬을 운반하는지도 측정했습니다.\n\n각 과제마다, Sparsh로 사전학습된 인코더는 동결하고, 그 위에 작은 태스크별 디코더/프로브만 학습시켰습니다. 구체적으로, [T1]~[T5] 대부분의 과제는 어텐션 풀링 기반 디코더를 사용했습니다. 이는 Sparsh 인코더의 패치별 출력에 **교차 어텐션(cross-attention)**을 적용해 전역적인 특징을 모은 후, 2계층 MLP를 통해 최종 예측을 산출하는 소규모 네트워크입니다. 이렇게 하면 사전학습 표현이 얼마나 해당 과제의 정보를 담고 있는지 선형 분류기 수준에서 평가할 수 있습니다. 한편 힘 분포 필드 재구성(T1A) 같은 Dense 예측이 필요한 경우에는, Sparsh 인코더의 중간 특징들을 받아 DPT(Dense Prediction Transformer) 디코더로 픽셀 단위 출력을 복원하도록 설계했습니다. [T6] 비드 미로의 경우는 강화학습 정책 특성상 end-to-end로 학습이 이뤄지지만, 비교를 위해 Sparsh 인코더를 고정한 버전과, 처음부터 정책과 인코더를 함께 학습한 버전을 모두 시험했습니다.\n이제 TacBench 실험 결과를 살펴보겠습니다. 전체적인 결론부터 말하면, Sparsh 사전학습 표현은 다양한 촉각 과제에서 매우 뛰어난 성능 향상을 보여주었습니다. 우선 공통적으로, 레이블된 데이터가 적을수록 Sparsh의 효과는 더욱 두드러졌습니다. 그림 1 가운데 그래프에서 볼 수 있듯이, 사전학습 없이 **개별 과제 전용으로 학습한 모델(E2E)**들은 학습 데이터가 충분할 때는 일정 수준 성능을 내지만, 데이터가 줄어들수록 성능이 급격히 떨어졌습니다. 반대로 Sparsh 인코더를 사용한 모델들은 레이블 1/3만으로도 준수한 정확도를 유지했고, **10%**나 1% 수준의 극소량 레이블로도 어느 정도 유의미한 결과를 냈습니다. 예를 들어 힘 추정의 경우, Sparsh (DINO) 모델은 **레이블의 10%**만 써도 0.1N 이하의 오차를 유지했으며, 미끄럼 감지도 1% 데이터로 학습해도 F1-score가 꽤 유의미하게 나왔습니다. 반면 E2E 모델은 33% 이하에서는 아예 출력이 한쪽으로 치우치는 등 제대로 학습되지 않는 현상이 관찰되었습니다. 이러한 경향은 정량적인 평균 지표로도 확인되는데, 저자들이 명시하였듯이 Sparsh 모델들은 레이블 33% 조건에서 E2E 대비 평균 95.1% 성능 향상을 이루었습니다. 다시 말해 레이블이 부족할 때 두 배 가까운 성능 격차를 낸 것입니다. 심지어 모든 과제가 레이블 충분 조건(Full data)에서도 Sparsh가 E2E보다 대체로 우수했는데, 이는 사전학습 표현이 학습 효율뿐 아니라 일반화 성능 측면에서도 이점이 있음을 시사합니다.\n각 과제별 상세 결과를 요약하면 다음과 같습니다:\n\n힘 추정 [T1]: Sparsh 표현을 이용하면 정규화된 힘 추정 오차를 매우 낮게 유지할 수 있었습니다. 특히 GelSight Mini 센서의 경우 해상도가 높고 배경 대비 접촉 영역이 작아서 E2E 모델은 충분한 데이터가 있어도 과적합 등의 문제로 정확도가 낮았지만, Sparsh 인코더를 쓴 모델은 안정적으로 작은 RMSE를 기록했습니다. DIGIT의 경우는 데이터가 충분하면 E2E도 어느 정도 성능을 냈으나, 레이블이 적을 때는 Sparsh (특히 DINO 변형)이 훨씬 견고하게 낮은 오류를 보였습니다. Sparsh (DINO)는 소량의 학습데이터로도 강건하게 힘을 예측하여, 레이블 부족 상황에서 두드러진 성능을 발휘했습니다.\n힘장 시각화 [T1A]: 정량적 평가가 어려운 과제이지만, Sparsh 표현을 사용해 추정한 전단력 벡터장은 마커 없는 GelSight 센서에서도 접촉 패치의 움직임 방향을 잘 나타내주었습니다. 예컨대 미끄러지는 방향으로 화살표가 일정하게 그려진다든지, 비틀리는 경우 회전형 패턴이 나타난다든지, 새로운 접촉이 일어날 때 퍼져나가는 형태가 보이는 등, 사람의 촉각적 직관과 맞아떨어지는 시각화를 얻었습니다. 이는 기존에 마커가 없으면 얻기 힘들었던 정보를 사전학습 표현으로 보완한 흥미로운 결과입니다.\n미끄럼 감지 [T2]: 불균형 데이터(미끄럼 적음)와 센서 노이즈로 인해 DIGIT 센서에서는 특히 어려운 문제였지만, Sparsh의 V-JEPA 변형이 가장 뛰어난 성능(F1-score)을 달성했습니다. Sparsh (VJEPA)는 비디오 클립 입력을 활용해 시간에 따른 변화를 직접 학습한 덕분에, 정적인 방법들보다 미끄럼 징후 포착에 유리했습니다. 그 결과 DIGIT처럼 표면에 마커 패턴이 없어 전단력 방향 파악이 어려운 센서에서도, 미끄럼 여부를 훨씬 정확히 분류해냈습니다. 특히 학습데이터의 50% 이하만 사용할 경우 Sparsh 모델과 E2E 사이 성능 격차가 크게 벌어졌는데, Sparsh (VJEPA)는 50% 데이터로 학습해도 E2E의 100% 데이터 성능을 능가할 정도였습니다.\n자세 추정 [T3]: 이 과제에서는 클래스가 촘촘하게 양자화되어 있어, 적은 데이터로 학습하면 인접한 각도나 이동 구간을 구별하기 어려워집니다. 실제로 E2E 모델은 레이블이 줄어들수록 혼동이 심해져, 예컨대 0도와 5도의 회전 차이도 맞히지 못하고 엉뚱한 큰 값만 예측하는 경향(극단값으로 수렴)이 나타났습니다. 반면 Sparsh 인코더 기반 모델은 1/3 수준의 데이터로도 꽤 높은 정확도를 유지했고, 적은 데이터에서 극단값으로 치우치는 현상도 덜했습니다. 충분한 데이터가 있을 때는 E2E와 큰 차이가 없었지만, 저데이터 시나리오에서 Sparsh의 이점이 두드러져, 미세한 각도 차이까지 비교적 잘 예측하였습니다.\n그립 안정성 [T4]: 학습 데이터가 충분한 (8000개 이상) 경우에는 Sparsh나 E2E나 모두 유사한 높은 정확도를 보였습니다. Sparsh (IJEPA)와 Sparsh (VJEPA)는 약 90%에 가까운 정확도를 달성하여, 시각+촉각 멀티모달 정보를 함께 쓴 이전 연구보다도 오히려 높은 성능을 나타냈습니다. 주목할 점은 여기서 Sparsh는 한 손가락의 촉각 정보만 사용하고도, 과거에 시각까지 동원한 모델을 앞질렀다는 것입니다. 또한 Sparsh 기반 모델은 **데이터 33%**로 학습해도 큰 성능 저하 없이 80% 이상 정확도를 유지했고, **10% (800개 미만)**만으로도 70%대 수준을 보여 실용적인 성능을 발휘했습니다. 그러나 80개 (1%) 정도로 극단적으로 줄이면 확연히 떨어져서, 저자들은 이 경우엔 무작위 추측 수준으로 전락함을 관찰했습니다. 그럼에도 불구하고 전반적으로 Sparsh 표현은 촉각만으로 그립 성공 여부를 신뢰성 있게 예측할 수 있음을 검증한 셈입니다.\n직물 인식 [T5]: 이 과제는 20-way 분류로 난이도가 높고, 기존 연구에서도 E2E 학습이 어렵다고 보고된 바 있습니다. 본 연구 결과 역시 E2E로는 좋은 성능을 내기 힘들었지만, 사전학습 표현을 사용하자 성능이 크게 향상되었습니다. 특히 Sparsh (MAE) 모델이 두드러졌는데, 픽셀 수준의 질감 특징을 복원하도록 학습된 덕분에 촉감으로 재질을 구분하는 데 유리했던 것으로 보입니다. Sparsh (MAE)를 사용하면 전체 데이터로 학습할 때 정확도가 크게 높아졌을 뿐 아니라, 10% 이하의 데이터로도 E2E의 풀데이터 성능에 맞먹는 결과를 얻었습니다. 흥미로운 추가 실험으로, 저자들은 센서 간 전이 학습을 시도했는데, 예컨대 GelSight로 사전학습한 Sparsh를 이용해 DIGIT 센서의 직물 인식을 몇 개 샘플로 파인튜닝하면 금방 적응한다는 것을 보였습니다. 단 10-shot(직물별 10장) 정도의 소량 학습으로도 새로운 센서에 맞게 분류기가 조정되어, 사전학습 표현의 범용성을 확인시켜주었습니다.\n비드 미로 [T6]: 강화학습(행동 클로닝) 문제에서도 Sparsh 표현이 도움을 주는지 검증했습니다. 우선 시뮬레이션 환경(데모 경로 추종)에서의 결과를 보면, **Sparsh (DINO)**와 Sparsh (IJEPA) 기반 정책이 E2E 정책보다 궤적 오차가 약 16% 적게나타났습니다. 같은 데이터로 학습해도 사전학습 인코더를 쓴 쪽이 훨씬 데모를 정확히 재현한다는 의미입니다. 또 학습에 쓰인 데모 개수를 줄여보는 실험에서도 Sparsh 모델은 적은 시연으로도 괜찮은 정책을 학습했지만, E2E는 데모가 줄면 성능 급하락을 보여 표현 학습의 데이터 효율성을 재확인했습니다. 마지막으로 실제 로봇에 정책을 실행하여 테스트한 결과, 모든 모델이 미로 완주는 실패했지만 Sparsh 기반 정책들이 E2E보다 20~53% 더 먼 거리까지 구슬을 움직이는 데 성공했습니다. 즉, 완전한 성공률 측면에서는 한계가 있었으나, 사전학습 표현이 부분적인 성능 향상과 안정적인 조작에 기여함을 알 수 있습니다. 다만 실제 로봇 실험에서는, 시뮬레이션과 달리 로봇의 미세한 제어 한계, 한번 미끄러지면 재잡기 어려운 물리적 상황 등이 작용하여 사전학습 효과가 완전히 발휘되지 못했습니다. 그럼에도 Sparsh를 사용한 정책이 전반적으로 더 나은 결과를 보였다는 점에서, 촉각 표현 학습이 로봇 정책 학습에도 유용할 수 있다는 가능성을 보여주었습니다.\n\n以上의 실험들을 종합하면, Sparsh로 학습된 촉각 표현은 여러 센서와 여러 과제를 아우르며 성능을 향상시킵니다. 특히 레이블이 제한적인 현실에서, 사전학습 모델을 쓰면 훨씬 안정적이고 높은 성능을 얻을 수 있음을 확인했습니다. 또한 Sparsh의 다양한 변형들 중 DINO 기반 모델은 힘/자세 추정 같은 물리량 회귀 과제에 강했고, I-JEPA 기반 모델은 미끄럼, 안정성, 재질 분류 같은 의미론적 분류 과제에 좀 더 성능이 우수한 경향을 보였습니다. 이는 픽셀 복원 vs 특징 예측의 차이뿐 아니라, 학습 목표에 따른 특화 효과도 일부 있음을 시사합니다. 재미있게도, 두 모델은 비드 미로 정책처럼 복합적인 과제에서는 비슷한 성능을 냈는데, 이는 그 과제가 힘과 미끄럼에 대한 종합적 이해를 요구하기 때문일 것입니다."
  },
  {
    "objectID": "posts/paper/2025-06-02-sparsh.html#기존-연구와의-비교",
    "href": "posts/paper/2025-06-02-sparsh.html#기존-연구와의-비교",
    "title": "📃Sparsh 리뷰",
    "section": "2.4 기존 연구와의 비교",
    "text": "2.4 기존 연구와의 비교\nSparsh 이전의 비전 기반 촉각 인지 연구들은 주로 특정 작업에 초점을 맞춰 개별적인 모델을 개발하곤 했습니다. 초창기 접근법으로는, 촉각 이미지를 해석하기 위해 마커 추적이나 물리 시뮬레이션(FEM) 등을 활용하여 접촉 지형과 힘을 계산하는 방식이 있었습니다. 그러나 이러한 방식은 센서마다 마커 패턴 보정이 필요하거나 계산 비용이 크다는 한계가 있었습니다. 이후 학습 기반 기법들이 등장하면서, 개인별로 커스텀 디자인한 신경망을 사용하여 촉각 정보를 처리하는 시도가 많았습니다. 예를 들어, 어떤 연구에서는 특정 촉각 센서에 맞춘 CNN 인코더를 설계해 종이 질감 분류를 학습하거나, BioTac이나 GelSight 등의 센서 데이터를 전이학습하여 패브릭의 여러 성분(실, 질감 등)을 알아내는 작업을 하기도 했습니다. 심지어 최근에는 XELA라는 센서의 출력을 최근접 이웃 비교만으로 조작에 활용한 사례도 있었는데, 이는 촉각 표현을 명시적으로 학습하지 않고 임베딩 공간에서 비슷한 촉각을 찾아내는 방식이었습니다. 하지만 이렇듯 파편화된 접근들로 인해, 서로 다른 연구 간에 결과를 비교하거나 표준 모델을 공유하기가 어려웠습니다. 표준화된 사전학습 모델이 없었기 때문에, 매번 새로운 문제나 센서가 나오면 다시 데이터 수집에서 모델 디자인까지 처음부터 해야 하는 비효율이 존재했습니다.\n자기 지도 표현 학습에 대한 관심도 점차 높아져, Sparsh 이전에도 몇 가지 관련 연구가 등장했습니다. 한 예로, MAE 기법을 촉각에 적용하여 재질 분류 등에 효과적임을 보인 초기 실험이 있었고, **다양한 촉각 센서(BioTac 등)**에 대해 사전학습 후 미세조정(fine-tuning)을 하여 성능을 높인 시도도 있었습니다. 그러나 이들 역시 단일 센서 혹은 한정된 범위의 작업에 초점을 맞추고 있어 일반성이 부족했습니다.\n한편, 멀티모달 학습 측면에서는 시각-촉각 간의 연관성을 학습하려는 노력들도 있어 왔습니다. 예를 들어, 동일한 물체를 만졌을 때의 촉각 이미지와 시각 이미지 쌍을 모아 **대조 학습(contrastive learning)**으로 공통 임베딩 공간에 매핑하거나, 촉각 정보로 시각적 스타일을 변환하는 등의 실험이 이루어졌습니다. 이러한 연구들은 재질 분류나 그립 성공 예측 등에 있어서 촉각+시각 결합의 가능성을 보여주었지만, 주로 표면의 질감이나 물체의 시각적 특징에 집중되었습니다. 즉, 힘, 미끄럼, 자세 변화와 같은 물리적 접촉 특성까지 다루지는 못하였습니다. 이러한 부분은 여전히 멀티모달 접근법의 도전 과제로 남아 있습니다.\nSparsh와 가장 유사하거나 병행된 연구로 논문에서 언급한 것은 T3와 UniT라는 두 가지입니다. T3는 여러 촉각 센서에 대해 공유 인코더(shared trunk)를 학습하되, 센서별로 **개별 가지(branch)**를 둔 구조입니다. MAE 방식의 복원 과제와 함께 몇 가지 레이블된 작업을 혼합해 학습한 점이 특징이지만, 결국 센서마다 별도 인코더 파라미터를 유지해야 하므로 완전히 하나의 통합 모델이라고 보긴 어렵습니다. 또한 T3는 제한된 데이터로 학습되어 Sparsh만큼 광범위한 검증은 이루어지지 않은 것으로 보입니다. UniT는 GelSight Mini (마커 있음) 센서를 대상으로 한 연구로, VQ-GAN을 활용한 생성모델 기반 표현 학습을 시도했습니다. 이는 패치 단위로 이미지를 압축하며 디스크리미네이터로 훈련하는 방식인데, 특정 센서 한 종류에만 국한되어 있고 주로 정적 특성에 초점을 맞추었습니다.\n이들에 비해 Sparsh의 차별점은 분명합니다. 첫째, Sparsh는 여러 대표적인 촉각 센서(DIGIT 계열, GelSight 계열 등)의 데이터를 모두 활용하여 하나의 공용 모델로 학습했다는 점입니다. 이로써 센서 교체나 추가에 대한 일반화 가능성을 처음으로 본격 탐구했다는 의의가 있습니다. 둘째, 최신 SSL 알고리즘들을 적극 도입하여, 픽셀 복원(MAE), 지식 증류(DINO), 잠재 예측(JEPA) 등 다양한 패러다임을 비교·분석했습니다. 이는 이전 연구들이 주로 하나의 SSL 기법만 시험한 데 반해, 어떤 자기 지도 방식이 촉각에 가장 적합한가라는 중요한 질문에 답을 제시하려 한 것입니다. 셋째, Sparsh는 단순 모델 제시를 넘어 TacBench라는 벤치마크를 구축함으로써, 촉각 표현 학습의 평가 표준을 마련했습니다. 이 벤치마크에는 물리적, 인지적, 조작 측면을 망라한 과제들이 포함되어, 앞으로 다른 연구들과 직접 성능 비교를 할 수 있는 장을 열었다는 점에서 큰 의미가 있습니다. 넷째, 데이터 규모 측면에서도 Sparsh는 이전보다 압도적으로 큰 데이터로 학습되었고, 이를 통해 모델 용량을 키우고도 과적합 없이 학습이 가능함을 보여주었습니다. 요컨대 Sparsh는 **“촉각계의 Foundation Model”**을 지향한 최초의 본격 시도로서, 이전 개별 연구들의 한계를 한 단계 뛰어넘은 통합적 접근이라 평가할 수 있습니다."
  },
  {
    "objectID": "posts/paper/2025-06-02-sparsh.html#장점과-한계",
    "href": "posts/paper/2025-06-02-sparsh.html#장점과-한계",
    "title": "📃Sparsh 리뷰",
    "section": "2.5 장점과 한계",
    "text": "2.5 장점과 한계\nSparsh의 성공 요인과 강점을 정리하면 다음과 같습니다:\n\n레이블 프리(self-supervised) 학습의 이점: 거대한 비지도 데이터로 학습했기에, 기존에 힘들었던 접촉 현상의 희귀 패턴까지 모델이 학습할 수 있었습니다. 예컨대 미끄럼이나 비틀림 같은 이벤트는 레이블링하기 어렵고 드물지만, SSL로 방대한 데이터를 학습하면서 이런 특징까지 잠재적으로 포착하게 되었습니다. 그 결과 일반적인 E2E 모델이 놓치기 쉬운 부분에서도 Sparsh 표현은 의미 있는 신호를 내재화하고 있어, 적은 레이블로도 높은 성능을 내는 밑바탕이 되었습니다.\n범용성 (Generalization): Sparsh 인코더 하나로 여러 센서와 여러 과제를 커버할 수 있다는 점이 입증되었습니다. 실제 TacBench 과제들을 보면, 센서 종류(DIGIT vs GelSight 등)나 출력 형태(회귀 vs 분류 vs 강화학습)가 제각각인데, 동일한 Sparsh 표현이 모두에 적용되어 좋은 결과를 냈습니다. 이는 로봇 시스템에서 센서를 교체하거나 새로운 작업에 투입할 때 기존 모델을 재사용할 수 있는 길을 열어줍니다. 예를 들어, Sparsh로 학습된 표현을 쓰면 새로운 GelSight 센서를 장착하거나 새로운 물체 잡기 과제가 주어져도, 일부터 학습하지 않고 10-shot 정도의 경미한 추가 학습만으로 적응이 가능함을 보였습니다.\n잠재 표현 학습의 효과: Sparsh (DINO)와 (IJEPA)가 MAE보다 좋았듯이, 노이즈와 센서 특수성을 걸러낸 잠재 공간 표현이 촉각 문제에 유리함이 확인되었습니다. 이는 촉각 센싱의 현실적인 문제, 즉 조명 변화, 카메라 잡음, 마커 패턴 등의 요소를 추상화하여, 핵심 접촉 정보에 집중할 수 있게 해줍니다. 그 결과 DIGIT이나 GelSight Mini같이 마커가 없는 센서에서도 전에는 풀기 어려웠던 전단력 분포 추정이나 미끄럼 감지 등이 Sparsh 표현으로는 가능해졌습니다.\n실시간성과 활용 가능성: ViT-Base 수준의 Sparsh 모델은 고성능 GPU에서 100 FPS 이상의 추론 속도를 보였습니다. 이는 로봇 제어에 충분히 투입할 만한 속도이며, 실제로 비드 미로 과제에서도 실시간 촉각 피드백으로 정책을 실행할 수 있음을 시연했습니다. 따라서 Sparsh는 실험실 프로토타입을 넘어 실시간 로봇 제어에 투입 가능한 준비된 모델이라고 할 수 있습니다.\n표준 벤치마크 제공: TacBench의 등장은 촉각 연구 커뮤니티에 크게 기여하는 점입니다. 이제 연구자들은 새로운 촉각 표현 모델이 나오면 이 6개 과제에서의 성능 지표로 비교 평가할 수 있습니다. 논문 저자들도 TacBench를 **“초기 벤치마크”**로 칭하며, 향후 과제들을 더 추가하거나 데이터셋을 확장함으로써 지속적으로 발전시킬 수 있다고 언급했습니다. 이런 표준화된 플랫폼은 공동 발전을 가속할 것입니다.\n\n물론 한계나 개선점도 존재합니다:\n\n실제 로봇 응용에서의 과제: Sparsh 표현을 사용한 정책도 복잡한 실제 조작 문제(비드 미로 완주 등)에서는 아직 완벽한 성과를 내지 못했습니다. 이는 꼭 Sparsh의 잘못이라기보다, 행동 복제 방식의 한계와 환경적 요인 때문입니다. 실제 로봇에서는 미세한 힘 제어의 어려움, 오차가 누적되면 복구 불가 등의 문제가 있어, 사전학습 표현이 있어도 추가적인 대책이 필요함이 드러났습니다. 이는 향후 강화학습과의 연계나 실시간 보정 기법 연구가 필요함을 시사합니다.\n데이터의 편중: 이번 연구에 사용된 공개 촉각 데이터들은 주로 불연속적인 접촉(물체를 뗐다 붙였다 하는) 위주입니다. 지속적으로 미끄러지는 상호작용이나 충격/진동 같은 역동적 데이터는 부족한 편입니다. 저자들도 전단이 풍부한 데이터를 추가로 모은다면 표현 학습이 더 좋아질 것이라고 예상했습니다. 따라서 현 단계 Sparsh는 저속, 준정적 접촉 상황에 최적화되어 있을 수 있으며, 고속 충돌이나 진동 감지 등에는 성능이 검증되지 않았습니다.\n히스토리 길드에 대한 미실험: Sparsh는 입력으로 두 프레임(또는 4프레임)을 사용하는 것으로 고정하였는데, 이 이력 길이를 늘리거나 줄이는 것에 대한 실험이 없습니다. 더 긴 시간 창의 입력이 잠재표현에 유리할지, 혹은 불필요한 잡음을 줄지에 대한 분석이 없어서, 현 설정이 최적인지는 추가 연구가 필요합니다.\n타 센서 유형에의 일반화: Sparsh는 비전 기반 촉각 센서들만 다룹니다. 압력 분포를 이미지로 변환하지 않는 다른 종류의 촉각 센서(예: 바이오태크의 전도도 변화, 힘 토크 센서 등)에 대해서는 이 접근법이 바로 통하지 않을 수 있습니다. 물론 비전 기반이라는 동일 원리에 속하는 센서들 사이에서는 일반화가 잘 되었지만, 그 외 방식까지 포괄하는 멀티모달 촉각 표현으로 확장하려면 추가 연구가 필요합니다.\n대규모 자원 필요: 장점으로 꼽았던 사전학습이란 과정은, 거꾸로 보면 상당한 계산 자원과 데이터 수집 노력이 필요한 작업입니다. 본 연구는 Meta 및 대학 협력을 통해 데이터를 모으고 수 주간 대용량 모델을 학습시켰는데, 모든 연구팀이 이를 수행하기는 어렵습니다. 다행히 학습된 가중치 공개(프로젝트 깃허브 등)가 이루어진다면 이를 활용하면 되겠지만, 향후 더 큰 모델들은 더 많은 데이터/연산이 필요할 것이기에 규모의 경제를 누릴 수 있는 기관 중심으로 연구가 진행될 우려도 있습니다.\n부분 파인튜닝 효과 미미: 본문에서는 다루지 않았지만, 부록에서 Sparsh 인코더를 부분적으로 미세조정(fine-tuning)해도 성능 향상은 크지 않고 거의 고정된 상태와 비슷했다고 합니다. 전체 파인튜닝하면 더 성능이 좋아졌지만 특히 latent기반 모델이 그렇다는 것이고, 부분(예: 마지막 몇 계층만)만 훈련하는 건 큰 의미가 없었는데요. 이는 Sparsh 표현이 이미 충분히 일반적 특성을 뽑아내 주기 때문일 수도 있고, 반대로 보면 표현에 잠재된 한계가 있어서 더 튜닝해도 극복하기 어렵다는 의미일 수도 있습니다. 이 부분은 추후 연구로 사전학습 표현을 적절히 보완/미세조정하는 기법이 개발될 여지가 있습니다."
  },
  {
    "objectID": "posts/paper/2025-06-02-sparsh.html#향후-전망-및-응용",
    "href": "posts/paper/2025-06-02-sparsh.html#향후-전망-및-응용",
    "title": "📃Sparsh 리뷰",
    "section": "2.6 향후 전망 및 응용",
    "text": "2.6 향후 전망 및 응용\nSparsh 연구는 촉각 센싱 분야의 새로운 장을 여는 출발점으로 볼 수 있습니다. 저자들도 **“시작에 불과”**하다는 투으로 앞으로의 발전 방향을 제시하고 있습니다. 우선, 더 큰 규모의 촉각 데이터셋 구축이 주요 과제로 언급됩니다. 현재까지는 연구팀이 자체 수집하거나 공개된 데이터를 모았지만, 전세계적으로 촉각 데이터를 모으는 노력이 결집된다면 컴퓨터 비전이나 NLP처럼 거대하고 다양한 데이터로 모델을 학습시킬 수 있을 것입니다. 특히 다양한 센서(예: 새로운 저가 촉각센서, 질감 센서 등)와 다양한 환경(산업용, 의료용 등)의 데이터를 포함시키면 Sparsh의 후속 모델은 더욱 범용적으로 진화할 것입니다.\n**모델의 스케일 업(scale-up)**도 기대해볼 수 있습니다. Vision Transformer 기반의 Sparsh는 Base 수준이지만, 향후 ViT-Large나 Hybrid 모델로 키운다면 더 정교한 표현을 얻을 수 있을 것입니다. 물론 데이터도 비례해서 늘어나야 하겠지만, 컴퓨터 비전 분야의 경험상 모델이 커질수록 성능이 올라가는 추세가 촉각에도 적용될 가능성이 높습니다. 이러한 대형 사전학습 촉각 모델은 진정한 의미의 촉각 Foundation Model로 자리매김하여, 사람 손의 촉각처럼 범용적인 지능을 보여줄지도 모릅니다.\n또 하나의 방향은 다중 모달 통합입니다. 앞서 비교한대로 Sparsh는 오직 촉각 모달리티만 사용했는데, 이후에는 시각+촉각 공동 학습이나 더 나아가 언어까지 결합한 멀티모달 학습으로 확장할 수 있습니다. 예를 들어, 동일한 상황에서 촉각과 시각 정보를 모두 사전학습하여 공용 임베딩을 만들면, 로봇이 보거나 만지는 것에 대해 일관된 표현 공간에서 이해할 수 있게 될 것입니다. 이는 인간이 보고 느끼는 감각을 결합하여 물체를 인지하는 방식과도 유사합니다. 실제로 이전 연구들에서 멀티모달 시도가 있던 만큼, Sparsh를 기반으로 그런 방향을 모색한다면 촉각-시각 동시 활용 작업(예: 물체 식별, 재질 탐색)에 혁신을 가져올 수 있습니다.\n강화학습 및 로봇 제어 측면에서는, 표현 학습과 정책 학습의 접목이 중요할 것입니다. Sparsh는 Behavior Cloning 예제로 비드 미로를 시도했지만, 앞으로는 모델 기반 강화학습이나 온-정책(on-policy) 방법에서 사전학습 표현을 활용하는 연구가 필요합니다. 특히 실시간 상호작용 중에 표현을 계속 적응시켜나가는 표현 강강학습 같은 개념도 생각해볼 수 있습니다. 이는 Sparsh 표현을 동적으로 업데이트하거나, 혹은 프로브 네트워크에 피드백을 주는 방식으로 이루어질 수 있을 것입니다. 궁극적으로는, 사전학습된 표현을 써서 학습 효율을 높이면서도, 실제 환경에서의 피드백으로 지속 개선하는 방향이 바람직할 것입니다.\n응용 분야를 살펴보면, Sparsh는 다양한 산업 및 연구 도메인에 파급효과를 가져올 수 있습니다. 예를 들어:\n\n정밀 조립 및 삽입 작업: 촉각 센서를 이용한 미세 조립(기계 부품 끼우기 등)에서는 힘과 위치를 섬세히 감지해야 합니다. Sparsh 표현이 있으면 작은 접촉 변화를 놓치지 않고도, 다양한 부품에 대한 범용 조립 모듈을 구축할 수 있을 것입니다.\n의료 로봇 및 의수(의족): 사람의 촉각을 대체하거나 보조하는 의수/의족에 Sparsh 같은 모델이 들어가면, 물건을 쥘 때 미끄러짐을 자동으로 감지해 힘을 조절하거나, 촉각으로 물체의 재질을 식별하여 사용자에게 피드백을 줄 수도 있습니다. 이는 촉각이 결여된 로봇이나 보조장치에 사람 비슷한 촉감 능력을 부여하는 방향입니다.\n재료 및 품질 검사: 촉각 센싱은 표면의 거칠기나 소재 특성을 파악하는 데 쓰일 수 있습니다. Sparsh 모델로 학습된 임베딩 공간은 질감이나 마찰계수 등의 정보를 내포하고 있으므로, 산업 공정에서 제품의 표면 품질 검사, 직물이나 종이의 분류 작업 등에 바로 활용될 수 있을 것입니다.\n연구 플랫폼으로서의 활용: TacBench의 공개로, 이제 연구자들은 새로운 촉각 센서만 개발되면 Sparsh에 추가 학습시키고 동일한 TacBench에서 시험해볼 수 있습니다. 이는 새로운 센서 아키텍처 평가에도 표준을 제공하여, 어떤 센서 설계가 더 다양한 촉각 정보를 잘 담아내는지 비교할 수도 있게 됩니다. 즉, Sparsh는 촉각 센서 하드웨어 연구에도 정량적 평가 도구로 기여할 수 있습니다.\n\n마지막으로, Sparsh 연구진은 논문을 통해 *“Sparsh는 범용 촉각 백본을 향한 중요한 발걸음이다”*라고 강조합니다. 그리고 *“우리의 목표는 더 큰 촉각 데이터셋과 더 큰 SSL 백본을 활용하는 것”*이라고 밝히고 있습니다. TacBench는 시작일 뿐이며, 부족한 부분(예: 데이터 다양성, 시간 창 길이 등)은 향후 보완하고, 예컨대 액체를 따를 때 질량 변화 추적 같은 더 흥미로운 촉각 과제도 추가할 수 있다고 제안합니다. 이러한 청사진을 볼 때, 앞으로 촉각 연구 커뮤니티의 협업 하에 Sparsh를 잇는 더 발전된 모델들이 나올 것으로 기대됩니다. **시각과 언어에 이은 “촉각의 GPT”**가 등장하는 날도 머지않을지 모릅니다. Sparsh는 그 첫걸음을 내딛었으며, 향후 로봇에게 인간 수준의 촉각 지능을 심어주는 여정에 중요한 이정표가 될 것입니다."
  },
  {
    "objectID": "posts/paper/2025-06-03-digit360.html",
    "href": "posts/paper/2025-06-03-digit360.html",
    "title": "📃Digit360 리뷰",
    "section": "",
    "text": "✨ 이 논문은 시각, 오디오, 진동, 힘, 열, 냄새 등 다양한 감각을 동시에 측정하는 고해상도 인공 촉각 센서인 ’Digit 360’을 제시합니다.\n🔬 이 손끝 센서는 7 마이크로미터의 공간 해상도와 1 밀리뉴턴 수준의 힘 감지 능력을 포함하여 인간의 촉각 성능을 능가하는 결과를 보였습니다.\n🤖 장치 내 AI 프로세싱을 통해 빠른 반응 속도를 구현했으며, 로봇 공학, 가상 현실, 원격 조작 등 광범위한 응용 분야를 위한 유망한 플랫폼입니다."
  },
  {
    "objectID": "posts/paper/2025-06-03-digit360.html#기술-분석-인공-멀티모달-손끝의-구성과-작동-원리",
    "href": "posts/paper/2025-06-03-digit360.html#기술-분석-인공-멀티모달-손끝의-구성과-작동-원리",
    "title": "📃Digit360 리뷰",
    "section": "2.1 기술 분석: 인공 멀티모달 손끝의 구성과 작동 원리",
    "text": "2.1 기술 분석: 인공 멀티모달 손끝의 구성과 작동 원리\n\n2.1.1 센서 설계 및 주요 구성 요소\n이 인공 손끝(Digit 360이라 명명)은 인간 손가락과 유사한 반구형의 유연한 촉각 표면을 갖추고 있으며, 그 내부에 복합 센서들이 층상 구조로 통합되어 있습니다. 가장 핵심이 되는 것은 시각 촉각 센서(vision-based tactile sensor)로, 표면의 미세한 변형을 고해상도로 포착하는 역할을 합니다. 반구형 엘라스토머 재질의 젤이 손가락 표면을 이루고, 그 표면 아래에는 매우 얇은 은(Ag) 반사막 코팅층이 입혀져 있어 접촉 시 움푹 패이는 변형을 시각적으로 표현합니다. 센서 내부에는 초광각 어안 렌즈(hyper-fisheye lens)를 장착한 고해상도 카메라가 배치되어 있는데, 이 특수 렌즈 덕분에 하나의 카메라로 손가락 끝 부분부터 옆면까지 전방위(360°) 촉각 이미지를 수집할 수 있습니다. 이는 기존에 다수의 카메라를 사용하거나 평면 부위만 감지하던 접근과 달리, 하나의 카메라로 전방위 접촉을 감지하는 혁신으로 센서 통합과 데이터 처리가 한층 단순화됩니다.\n이 시각 촉각 모듈은 동작 시 구조화 조명(structured light) 기법을 활용하여 표면 변형 정보를 획득합니다. 구체적으로, 내부에 배치된 다색 LED 조명(예: 360도 방향에 고르게 배열된 RGB LED들)을 통해 젤 내부를 조명하고, 반사막 표면의 압흔(depression)이 만들어내는 빛 반사 변화를 카메라가 포착합니다. 연구진은 조명의 세기와 색상 스펙트럼을 동적으로 제어함으로써, 조명 불균일로 인한 영상 왜곡(glare, hotspot)을 최소화하고 표면 눌린 자국과 배경을 높은 대비로 분리해내는 최적의 환경을 구축했습니다. 또한 내부 구조물을 두지 않고 고체형 실리콘 볼륨으로 일체화된 젤을採用하여, 빈 공간에서 발생하는 빛 산란 노이즈를 줄이고 깨끗한 촉각 영상을 얻었습니다. 이러한 설계 덕분에 시각 촉각 센서는 약 8백만 개 이상의 촉각 화소(taxel)로 이루어진 고해상도 영상을 제공하며, 7μm에 불과한 미세 공간 구조까지 구분할 수 있는 초정밀 공간 해상도를 달성했습니다. 이는 인간 손끝이 인지하는 수십 μm 수준의 촉감 해상도를 뛰어넘는, 말 그대로 “초인적(superhuman) 성능”입니다. 더불어, 촉각 영상의 프레임률을 240Hz까지 높여 빠르게 변하는 접촉 사건도 포착할 수 있도록 하였는데, 이는 일반 카메라 기반 촉각센서(보통 30~60Hz)의 한계를 크게 확장한 것입니다. 시각 정보만으로도 접촉 지형(contact geometry)과 힘의 방향/세기 추정이 가능하지만, 이 논문의 센서는 여기서 그치지 않고 다양한 물리 신호를 추가로 감지하도록 설계되었습니다.\n멀티모달 감각 통합을 위해 손끝 내부에는 카메라 외에도 여러 센서들이 융합되어 있습니다. 우선, 소형 마이크로폰 및 압력 기반 진동 센서가 내장되어 손가락이 표면을 스칠 때 나는 마찰음이나 미세 진동 정보를 포착합니다. 덕분에 이 손끝은 240Hz 카메라로는 포착하기 어려운 수 kHz 대역의 빠른 진동까지도 감지할 수 있으며, 실제 10kHz에 이르는 고주파 촉각 진동까지 인지할 수 있음이 확인되었습니다. 이는 인간의 진동 감각 수용기(Pacinian corpuscle)가 약 수백 Hz 정도까지 반응하는 것과 견줘볼 때 월등히 높은 수치입니다. 높은 대역의 진동 및 음향 정보는 재질 특성 파악에 유용한데, 예를 들어 이 센서는 손가락으로 가볍게 두드리거나 문지르는 동작만으로도 나무와 플라스틱, 직물과 거친 면과 같은 다양한 표면의 질감 차이를 음향/진동 스펙트럼으로 구별해낼 수 있었습니다. 또한 연구진은 이 진동 센서를 활용하여 불투명 용기의 내용물량을 파악하는 데 응용했는데, 손끝으로 병을 톡톡 두드릴 때 발생하는 공명 소리의 특징으로부터 병에 든 액체의 높이를 가늠하는 데 성공하였습니다. 이러한 사례들은 시각+청각 촉각의 융합이 제공하는 정보를 잘 보여줍니다.\n다음으로, 이 손끝에는 온도 센서가 포함되어 접촉 물체의 온도 변화를 감지할 수 있습니다. 이를 통해 단순한 촉감 이상으로, 물체가 차가운지 따뜻한지, 혹은 온도 상승/하강 추세에 있는지까지 파악이 가능하며, 뜨거운 표면이나 냉각 중인 물체 등 안전과 관련된 정보를 얻을 수 있습니다.\n나아가 화학 물질 센서(가스 센서)도 내장되어 있어, 접촉 중이거나 주변 공기 중에 휘발성 화합물이 있는지를 감지합니다. 일종의 “후각”에 해당하는 이 능력은 일반적인 촉각 센서에서는 찾아보기 힘든 독특한 모달리티로서, 예를 들어 손가락이 젖었을 때 비누 냄새나 기름기 냄새를 감지하여 표면에 비누나 윤활유가 묻어 미끄럽게 할 요인이 있는지 사전에 인지하는 활용이 가능합니다. 이처럼 힘/형상(시각), 진동/음향, 온도, 화학에 이르는 다채로운 센싱을 하나의 손가락 팁에 통합한 것은 해당 장치의 가장 큰 강점입니다. 인간 피부 역시 촉각 수용기가 여러 종류(기계적, 열, 통증 등)로 이루어져 다양한 자극을 느끼는데, 기존의 인공 촉각들은 그 중 일부 신호만 개별적으로 모방했을 뿐 이처럼 통합적인 멀티모달 센싱을 구현한 예는 전무했습니다. Digit 360 인공 손끝은 사람 손끝의 감각 스펙트럼을 거의 전부 (또는 그 이상으로) 디지털화했다는 점에서 의미가 큽니다.\n한편 이러한 방대한 센서 데이터를 효과적으로 활용하기 위해, 손끝 내부에는 온장치 인공지능 가속기(on-device neural network accelerator)를 내장하고 있습니다. 이 작은 AI 전용 프로세서는 RISC-V 기반 9코어 NPU로, 일종의 “로봇 말초신경계” 역할을 수행합니다. 즉, 손끝에서 취득한 멀티모달 데이터를 로컬에서 즉각적으로 신경망으로 처리하여 유의미한 고레벨 정보(접촉 여부, 힘 추정치, 미끄럼 발생 등)를 뽑아내고, 필요한 경우 로봇의 구동기로 바로 피드백 제어명령을 내리는 것입니다. 마치 인간이 뜨거운 물체를 손끝으로 짚었을 때 뇌까지 생각하지 않고 즉각 손을 떼는 척수 반사(reflex arc)처럼, 이 손끝은 중요한 촉각 이벤트를 자체적으로 판별해 로봇 팔에 즉각 신호를 줄 수 있습니다. 연구 결과에 따르면, 기존 방식처럼 모든 센서 데이터를 중앙 컴퓨터로 보내 처리하는 경우 약 5~6ms 소요되던 이벤트 감지부터 반응 출력까지의 지연이, 온보드 처리 시 약 1.2ms로 대폭 단축되었습니다. 이는 처리 지연을 5배 이상 줄인 것으로, 로봇 제어의 민첩성을 크게 높여줍니다. 요컨대 Digit 360 플랫폼은 센서 하드웨어부터 즉각적 AI 처리까지 한 몸체에 넣음으로써, 방대한 촉각 데이터를 다루는 복잡성을 줄이고 실시간성을 극대화한 설계라 할 수 있습니다.\n\n\n2.1.2 센서 데이터 융합 및 학습 모델 구조\n이 인공 손끝이 생성하는 멀티모달 데이터 스트림(초당 240장의 촉각 이미지와 여러 실시간 센서신호)을 어떻게 유의미한 정보로 바꿀 것인가도 중요한 기술 요소입니다. 연구팀은 전통적 공학 모델 대신 딥러닝 기반의 학습 모델을 활용하여, 복잡한 센서 패턴을 힘이나 물체 특성으로 변환했습니다. 먼저 촉각 이미지로부터 접촉 힘을 추정하기 위해 심층 합성곱 신경망(CNN)을 학습시켰습니다. 구체적으로, 시각 촉각 이미지(3채널 RGB)를 입력 받아 해당 접촉의 법선력(normal force)과 전단력(shear force) 값을 회귀 출력하는 CNN 모델을 만들었는데, 이는 ResNet-50 백본을 변형하여 다중 분류 대신 2개의 연속값(힘)을 내도록 구성했습니다. 이 모델을 수천 건의 교정 데이터를 통해 훈련한 결과, 손끝 이미지만 가지고도 법선 및 전단 힘을 각각 오차 약 1.0~1.3 mN 이내로 예측할 수 있었습니다. 이는 앞서 언급한 센서의 힘 분해능(≈1 mN) 수준에 근접하는 정확도로서, 광학식 촉각센서로 힘을 측정하는 정량적 캘리브레이션의 새로운 기록이라 할 수 있습니다. 특히 전단력 추정의 경우, 기존 시각 촉각센서들은 표면에 마커(marker)점을 찍어 변형 추적을 도와주지 않으면 어려움이 있었으나, 본 센서는 고해상도 영상 자체에 풍부한 표면 특징이 담겨있어 마커 없이도 정밀한 전단 추정이 가능했다고 보고됩니다. 그만큼 고해상도의 이점이 학습 모델에도 기여한 것입니다.\n또한 연구진은 이 손끝이 산출하는 다양한 모달 데이터를 종합적으로 해석하기 위해 멀티모달 딥러닝 모델을 구성하였습니다. 하나의 예로, 촉각 센서를 이용한 행동 분류(예: 밀기, 두드리기, 저어섞기)와 재질 분류(예: 물체가 나무인지 플라스틱인지) 문제를 학습시켰습니다. 여기에는 이미지, 진동(음향), 온도, 가스, 관성 등 여러 종류의 센서 값들이 입력으로 사용되었으며, 각 모달리티에 따라 다른 신경망 구조를 결합하는 방식을 취했습니다. 예를 들어 시각 및 음향 정보는 시계열 데이터를 주파수-시간 스펙트럼 이미지로 변환하여 ResNet-18 기반 CNN 인코더로 처리하고, 관성(IMU), 온도, 가스와 같은 스칼라 신호는 별도의 다층퍼셉트론(MLP) 인코더로 처리한 후, 최종적으로 이들 특징을 하나의 잠재 벡터로 융합하여 행동 및 재질을 예측하도록 했습니다. 이러한 멀티모달 신경망을 통해 각 모달리티의 기여도와 상호 작용도 분석할 수 있었는데, 연구 결과 여러 모달의 결합이 단일 모달 대비 분류 정확도를 향상시켰으며 특히 동작 종류에 따라 핵심 단서가 되는 센서가 다름을 관찰했습니다. 예를 들어 슬라이딩하는 동작에서는 진동/음향 신호가 중요하고, 물체의 재질 식별에는 시각적인 접촉 패턴이 주요하게 작용하는 식입니다. 이처럼 딥러닝 모델을 통해 센서 융합의 유효성을 정량적으로 확인하였지만, 동시에 복잡한 모달 조합에서 최적의 특징을 학습하는 것은 난이도가 높습니다. 여기서 자연히 “센서 융합이 학습에 어떤 영향을 주는가?” 라는 질문이 따라옵니다. 여러 이질적인 센서 데이터를 함께 사용할 때 발생하는 모델 복잡도 증가, 학습 데이터 요구량 증가, 그리고 모달 간 동기화와 상관관계 파악 등의 이슈가 존재합니다. 논문의 접근처럼 각 모달별 특징 추출 후 융합하는 구조는 한 해결책이지만, 어떤 모달이 얼마나 기여하도록 모델을 설계해야 최적인지는 향후 추가 연구가 필요한 영역입니다. 다행히 저자들은 멀티모달 모델의 cross-modal significance를 분석하여 촉각 행동 인식에 기여하는 센서들의 상관관계를 탐구하였고, 이는 향후 센서 선택 및 모델 경량화에 참고되는 통찰을 제공합니다.\n이 인공 손끝 플랫폼의 또 다른 기술적 특징은 모듈식 설계입니다. 센서와 프로세서 전체를 하나로 묶었음에도 불구하고, 내부 구성은 카메라 모듈, 마이크/IMU 모듈, AI 가속기 모듈 등 기능별로 분리된 소형 회로 블록들로 구성되어 있습니다. 이러한 모듈식 구조의 장점은 연구 개발 시 구성요소의 교체나 확장이 용이하다는 것입니다. 실제로 저자들은 필요에 따라 특정 센서를 빼서 비용을 낮추거나, 새로운 센서로 교체하여 성능을 높이는 식의 유연한 플랫폼을 지향하고 있습니다. 예를 들어 마커가 있는 손끝 젤로 교체하여 전단력 추정을 더욱 용이하게 하거나, 젤 재질의 경도나 두께를 변경해 감도를 조절하는 것도 쉽게 시도해볼 수 있습니다. 이러한 유연성은 학계와 산업계 연구자들이 자신들의 용도에 맞게 손끝 센서를 재구성(reconfigure)하여 실험해볼 수 있게 하며, 신기술 도입 시 전체를 새로 설계하지 않고 해당 모듈만 교체하면 되므로 개발 사이클을 단축시켜 줍니다. 실제 논문 저자들은 촉각 연구 활성화를 위해 이 손끝의 설계 도면과 소프트웨어를 오픈소스로 공개해 두었고, 이를 통해 더 많은 연구자들이 해당 플랫폼을 기반으로 촉각의 본질과 멀티모달 처리 기법을 탐구하기를 기대하고 있습니다."
  },
  {
    "objectID": "posts/paper/2025-06-03-digit360.html#응용-가능성-산업부터-vr까지의-활용-전망",
    "href": "posts/paper/2025-06-03-digit360.html#응용-가능성-산업부터-vr까지의-활용-전망",
    "title": "📃Digit360 리뷰",
    "section": "2.2 응용 가능성: 산업부터 VR까지의 활용 전망",
    "text": "2.2 응용 가능성: 산업부터 VR까지의 활용 전망\n이처럼 다중 센서융합 인공 손끝이 제공하는 풍부한 촉각 데이터와 정밀 제어 능력은 다양한 분야에서 혁신적인 응용을 가능케 합니다. 저자들은 논문에서 본 기술의 잠재적 적용 분야로 로보틱스(산업, 의료, 농업, 소비자 영역), 가상현실/텔레프레즌스, 의수(보철), 전자상거래 등을 직접 언급하고 있습니다. 이하에서는 주요 분야별로 기대되는 활용 사례와 기존 기술 대비 이점 및 한계를 살펴봅니다.\n\n2.2.1 로보틱스 및 자동화 분야\n산업용 로봇에서 정밀 촉각 센싱은 매우 큰 가치를 갖습니다. 예를 들어 제조 조립 공정에서 로봇팔이 작은 부품을 끼우거나 나사를 조이는 작업을 생각해보면, 육안으로는 보이지 않는 미세한 맞촉감(fit)이나 나사선의 걸림 여부 등을 사람이 손끝으로 느끼며 작업하는 경우가 많습니다. 인공 멀티모달 손끝을 장착한 로봇은 이러한 작업에서 사람 못지않은 (어떤 면에서는 사람 이상의) 섬세한 촉각 피드백을 얻을 수 있습니다. 7μm 수준의 공간 분해능이라면 머리카락 굵기의 흠집이나 표면 거칠기 변화도 감지할 수 있어, 부품 간 결합의 미세한 오차나 표면 결함 검사에도 활용될 수 있습니다. 또한 1mN 단위의 힘 제어가 가능하다는 것은, 로봇이 매우 부드럽게 힘을 조절하여 쉽게 깨지는 부품이나 연성 재료를 다루는 정밀 조작에 강점을 지닐 것임을 시사합니다. 기존 로봇은 주로 관절 토크센서나 간단한 그립센서로 힘을 제어해왔는데, 이러한 저해상도 정보로는 구현하기 어려웠던 미끄러짐 감지, 접촉면 마찰 측정, 물체 무게 파악 등을 이제는 하나의 손끝 센서로 모두 수행할 수 있습니다. 실제 연구 결과, 본 센서는 물체가 손에서 미끄러지는 초기 징후를 고속 진동 센싱으로 탐지하여 즉각적으로 그립을 조정할 수 있음을 보였는데, 이는 산업 로봇의 자동 그립 조절(reflex) 기능 구현에 응용될 수 있습니다. 또한 제조 라인에서 제품 표면을 이 센서로 스캔하면, 시각 검사로 놓치기 쉬운 미세 흠집이나 표면 텍스처 이상까지도 촉각으로 검출하여 품질 관리에 활용할 수 있습니다.\n물류 창고나 서비스 로봇에서도 응용 가능성은 큽니다. 예를 들어 소비자용 가정 로봇이 있다면, 섬세한 촉각이 있으면 물건을 집거나 정리할 때 물체의 종류와 상태를 파악하여 알맞게 다룰 수 있습니다. 옷가지나 식기, 전자제품 등 각각 다른 취급이 필요한 물건들을 손끝의 촉감으로 구분하고, 힘 조절을 다르게 할 수 있습니다. 특히 농업 로봇의 경우 과일 수확이나 식물 다루는 작업에서, 과일의 숙성도를 촉감으로 느끼거나 (단단함, 표면 질감), 식물 줄기의 두께와 강도를 파악하여 적절한 힘으로 잡는 등의 활용이 가능합니다. 토마토처럼 살짝 눌러봐야 익은 정도를 아는 작물도 이 센서로는 익은 정도에 따른 미세한 탄성 변화를 정량화할 수 있을 것으로 기대됩니다. 나아가 젖은 흙과 마른 흙의 감촉 차이나, 병해 충격 받은 잎의 질감 변화 등을 감지하는 등 농업 환경에서의 촉각 데이터 수집도 가능할 것으로 보입니다. 산업 및 서비스 로봇 분야에서 이 기술의 기술적 이점은 결국, 사람 수준의 촉각 피드백을 로봇이 얻음으로써 자율작업의 신뢰성과 정밀도가 높아진다는 점입니다. 기존에는 힘을 세게 가해 실험적으로 실패를 겪으며 수행하던 작업도, 섬세한 촉감으로 사전에 상황을 인지하고 적응함으로써 보다 안정적인 자동화가 가능해질 것입니다.\n물론 이러한 응용을 위해서는 센서의 견고성과 실시간 처리능력이 뒷받침되어야 합니다. 산업 현장은 온도, 먼지, 전자기 노이즈 등이 존재하므로, 광학식 촉각센서가 그런 환경에서도 유지보수가 용이할지 검증이 필요합니다. 이는 뒤의 한계점 부분에서 논의하겠지만, 간단히 말해 “이 방식이 실제 환경에서 견딜 수 있는가?”라는 실용적 질문이 남아 있습니다. 그럼에도 불구하고, Digit 360 센서가 보여준 성능 지표(공간/힘 해상도, 다중감각)는 현재까지 보고된 어떤 촉각 센서보다 뛰어나 인간 수준에 근접하거나 초과하는 것으로 평가되므로, 로보틱스 응용에 있어서 게임 체인저가 될 잠재력이 충분하다고 하겠습니다.\n한편 의료 로봇틱스 분야에서도 정밀 촉각은 매우 중요합니다. 예를 들어 외과 수술 로봇의 수술 도구 끝에 이러한 센서를 장착하면, 조직의 경도나 질감, 맥박 진동까지 느끼면서 수술을 진행할 수 있습니다. 종양과 정상 조직의 미세한 경도 차이나, 동맥의 박동, 미세 calcification 등을 촉각으로 구분하여 더 정교한 수술이 가능해질 수 있습니다. 또한 로봇 간호 및 재활 기기에서도 환자와 접촉하는 부분에 정밀 촉각 센서가 있다면, 환자의 피부상태, 근육 긴장도, 반응 등을 감지하여 안전하고 개인화된 케어를 제공할 수 있습니다. 예를 들어 재활 로봇 손이 환자의 관절을 굽힐 때, 환자가 과도한 통증으로 미세한 저항을 보이면 (근육 경직 등 촉각 피드백) 이를 감지해 동작을 완화한다든지 하는 피드백 제어가 가능해집니다. 이러한 의료/복지 로봇 응용에서도 중요한 것은 신뢰성과 안전성이며, 멀티모달 손끝 센서는 촉각을 통한 이중 안전장치 역할을 하여 인간에게 위해가 가는 상황을 미연에 방지하는 데 도움을 줄 것입니다.\n\n\n2.2.2 가상현실(VR) 및 텔레프레즌스 분야\n가상현실(VR)과 증강현실(AR)에서는 시각, 청각 기술은 크게 발전했지만 촉각 경험의 부재가 한계로 지적되어 왔습니다. 본 기술은 현실 세계의 촉감 데이터를 디지털 콘텐츠로 녹여낼 수 있는 수단을 제공함으로써, VR/AR의 리얼리티를 한층 높일 수 있습니다. 예를 들어 이 손끝 센서를 이용해 다양한 물체의 질감, 표면 구조, 단단함을 데이터화하여 가상 객체에 부여하면, 사용자가 VR 장비를 통해 인터랙션할 때 사실감 있는 촉각 피드백을 줄 수 있을 것입니다. 현재도 VR용 촉각장갑 등이 연구되고 있으나 주로 진동 정도의 피드백만 제공하는데, 본 센서로 측정된 정교한 촉각 프로파일을 기반으로 하면 사용자가 실제 나뭇결을 손으로 문지르는 듯한 미세한 질감까지도 재현하는 것이 궁극적으로 가능해집니다. 또한 AR 환경에서 로봇이나 장치가 현실 물체를 만졌을 때 그 촉감 정보를 실시간으로 사용자에게 전달한다면, 원격 현실감(remote presence)이 크게 향상될 것입니다. 이를테면, 사용자가 원격 로봇을 통해 박물관의 유물을 만져보는 AR 투어를 한다면 로봇 손끝의 촉각이 사용자의 장갑에 피드백되어 “만지는 경험”을 전달할 수 있을 것입니다.\n텔레프레즌스(원격현존)와 원격 조작(teleoperation) 분야는 이 기술과 특히 직접적인 관련이 있습니다. 원격조작 로봇 (예컨대 해저 로봇 팔, 우주 로봇 팔, 원격 의료수술 로봇 등)에서 조종자는 카메라 화면과 약간의 힘 피드백만으로 작업하는 경우가 많은데, 만약 로봇 손끝에 Digit 360 센서를 달아두면 원격지의 미세한 촉각 정보를 실시간 전송받아 느낄 수 있습니다. 예를 들어 원격 수술에서 조직의 질감이나 경계를 손끝 감각으로 느끼면서 수술할 수 있게 되며, 위험물 처리 로봇이 폭발물의 표면 상태나 흔들림을 손끝으로 감지하여 더 섬세하게 다룰 수 있습니다. 이러한 고품질 촉각 정보는 양방향(haptic feedback) 장치와 결합되어 비로소 인간 조종자가 느낄 수 있게 되므로, 향후 촉각 피드백 장치 기술과 함께 발전이 필요합니다. 하지만 일단 정보를 획득하는 측면에서의 돌파구로서, 본 센서가 원격 촉각의 눈 역할을 할 수 있다는 의의가 있습니다. 특히 인터넷을 통한 원격작업에는 지연 문제가 큰데, 앞서 언급했듯 온보드 AI가 1ms 수준으로 로컬 반사를 수행한다면, 원격 조작에서도 로봇 자체가 위험을 감지해 즉각 대처함으로써 네트워크 지연으로 인한 사고를 줄일 수 있습니다. 예컨대 원격 로봇 손이 미끄러운 물체를 잡았을 때, 사람이 느끼기도 전에 센서가 미끄러짐을 감지해 그립을 조정해준다면 원격 조작의 안정성이 크게 향상될 것입니다. 이처럼 VR/텔레프레즌스 분야에서 인공 멀티모달 손끝은 현실의 촉감을 디지털로 복제하고 전송하는 역할을 하며, 이는 궁극적으로 현실-가상의 경계를 허무는 몰입형 경험으로 이어질 것입니다.\n\n\n2.2.3 의료 및 보조공학 분야\n본 기술은 의수나 의족 등의 보조공학 분야에도 직접적인 파급 효과가 있을 것으로 기대됩니다. 현존하는 의수의 손가락에는 간단한 압력센서나 가속도센서 등이 달려 있을 뿐, 인간 고유의 풍부한 촉각을 제공하지 못합니다. 사용자(절단 장애인)는 물체를 잡아도 어느 정도 힘으로 잡는지, 미끄럽게 생겼는지 등의 정보를 거의 받지 못해 섬세한 작업이 어렵고, 심지어 뜨거운 물체를 만져도 느끼지 못해 위험하기도 합니다. 인공 멀티모달 손끝을 의수에 적용한다면 이러한 제약을 크게 줄일 수 있습니다. 우선 의수가 물체를 쥐는 힘을 1mN 단위로 제어할 수 있게 되어 달걀처럼 약한 물체도 깨뜨리지 않고 집을 수 있고, 반대로 무거운 물건도 미끄러지지 않을 적절한 힘을 자동 조절할 수 있습니다. 또, 잡은 물체의 질감과 온도, 맥동까지 감지하여 사용자에게 피드백해줄 수 있다면, 사용자는 거의 실제 손과 유사한 촉감 경험을 얻을 수 있을 것입니다. 예를 들어 따뜻한 커피잔을 쥐었을 때 온기가 느껴지고, 사과를 쥐었을 때 표면의 매끈함과 단단함이 느껴진다면 사용자는 훨씬 자연스럽게 의수를 자신의 일부처럼 여길 수 있을 것입니다. 실시간 촉각 피드백을 사용자에게 전달하는 기술(신경 인터페이스나 촉각 자극 디스플레이)은 별도의 연구과제지만, 본 센서 기술의 존재로 인해 이식형 촉각 피드백 시스템도 구체적으로 논의할 수 있게 됩니다. 최소한의 단기 응용으로는, 의수에 멀티모달 손끝을 달아 물체가 미끄러질 때 자동으로 꽉 쥐거나 너무 뜨거울 때 즉각 놓아버리는 자동 반사 동작을 구현해줄 수 있습니다. 이는 감각이 없는 의수를 사용하는 사람에게 안전망이 되어 줄 것입니다. 더 나아가 의수 사용자에게 손끝 센서 데이터를 진동이나 전기 자극으로 환원(haptic feedback)해 주면, 사용자 스스로 힘 조절을 학습하여 더욱 섬세한 작업 (예: 계란 깨지 않게 들기, 옷감의 두께 느끼기 등)을 수행할 수도 있습니다. 이러한 보조공학 응용에서 가장 큰 기술적 이점은, 사용자가 잃어버린 촉각 감각을 기계가 대폭 복원 또는 증강해줄 수 있다는 점입니다. 기존 의수 개발은 주로 모터 제어에 집중되어 촉각은 등한시되었는데, 이제는 센서 측면에서 인간 촉각에 필적하는 장치를 활용할 수 있게 된 것입니다.\n이 밖에도 전자상거래(e-commerce) 분야에서의 잠재적 활용도 흥미롭습니다. 온라인 쇼핑에서는 소비자가 물품을 직접 만져볼 수 없다는 한계가 있는데, 장차 촉각 센싱 데이터베이스를 구축하여 이를 부분적으로나마 해소할 수 있습니다. 예를 들어 원단이나 의류 상품의 경우, Digit 360 센서로 천의 질감, 두께, 뻣뻣함, 부드러운 정도 등을 측정한 데이터를 제공함으로써 소비자가 촉감 특성을 유추하도록 할 수 있습니다. 또 전자제품 버튼의 조작감이나, 가구 표면의 마감 촉감 등을 정량화해 보여주는 것이 가능합니다. 나아가 매장에 원격 로봇을 배치하고 소비자가 집에서 VR 장비를 통해 로봇을 조작하면서 상품을 만져볼 수 있게 하는 시나리오도 생각해볼 수 있습니다. 이때 본 손끝 센서가 부착된 로봇이 상품의 촉감을 실시간 전송해주면, 온라인과 오프라인의 체험 격차를 크게 좁힐 수 있을 것입니다. 또한 물류 분야에서 로봇이 상품을 집을 때 얻는 촉감 정보를 바탕으로 상품의 파손 여부나 상태 이상을 자동으로 판별하는 등의 응용도 가능합니다. 예컨대 택배 상자를 로봇팔이 쥐었을 때 살짝 눌러본 촉감으로 내부 내용물이 깨졌는지(딸그락 거리는 진동? 균일하지 않은 단단함?)를 감지한다면 비대면 자동 검사가 될 수 있습니다. 이러한 전자상거래 및 물류 응용은 아직은 개념적인 단계이지만, 촉각 데이터의 축적과 활용이라는 새로운 영역을 개척할 수 있다는 점에서 주목할 만합니다."
  },
  {
    "objectID": "posts/paper/2025-06-03-digit360.html#한계점-및-향후-과제-도전-과제와-비판적-고찰",
    "href": "posts/paper/2025-06-03-digit360.html#한계점-및-향후-과제-도전-과제와-비판적-고찰",
    "title": "📃Digit360 리뷰",
    "section": "2.3 한계점 및 향후 과제: 도전 과제와 비판적 고찰",
    "text": "2.3 한계점 및 향후 과제: 도전 과제와 비판적 고찰\n혁신적인 인공 멀티모달 손끝 기술이지만, 실제 활용을 위해서는 몇 가지 극복해야 할 한계와 고려사항이 존재합니다. 여기서는 실험적 제약, 내구성 및 환경적 요인, 비용/복잡성, 데이터 처리 및 확장성 측면에서 주요 한계를 지적하고, 동시에 연구 개발자들이 관심을 가질 만한 후속 연구 방향과 질문을 제시합니다.\n내구성 및 환경 적응성:\n현재 공개된 결과는 실험실 환경에서 얻어진 성능 수치입니다. “이 방식이 실제 환경에서 견딜 수 있는가?” 라는 의문은 여전히 남습니다. 예컨대 산업 현장의 먼지, 기름기, 온도 변화, 충격 등에 노출되었을 때 광학식 + 젤 구조의 센서가 얼마나 유지될지 알 수 없습니다. 젤 표면의 얇은 은 코팅은 반복 접촉으로 마모되거나 긁힐 우려가 있고, 젤 자체도 날카로운 모서리에 여러 번 눌리면 찢어질 가능성이 있습니다. 논문에서는 일정 힘 이상 가해지면 손끝이 본체에서 분리되는 한계(force threshold)도 실험으로 보였는데, 이는 최대 하중 이상에서는 센서가 물리적으로 파손될 수 있음을 의미합니다. 따라서 거친 산업 작업이나 돌발적인 큰 충격이 가해지는 상황에서 센서를 보호할 수 있는 외장 커버나 오버레이 등에 대한 추가 설계가 필요할 것입니다. 그러나 보호층을 추가하면 감도가 낮아질 수 있으므로, 내구성과 성능의 트레이드오프를 최적화해야 합니다. 또, 실외 환경 (예: 농업 로봇)에서는 온습도 변화로 젤의 탄성이 바뀌거나, 광학 부품에 습기가 차는 등의 변수가 있습니다. 이러한 환경 변수에 자동 보정(calibration)할 수 있는 기법이나 자가진단 기능도 후속 연구 과제입니다. 장시간 사용에 따른 노화도 고려해야 하는데, 센서 재료(실리콘 젤, 코팅 등)가 수백 시간의 사용 후에도 초기 성능을 유지하는지, 혹은 일정 주기마다 젤 팁 교체가 필요한 소모품으로 봐야 할지 평가가 필요합니다. 다행히 모듈식 설계로 손쉽게 팁을 교체 가능하므로 현장에서의 유지보수는 큰 어려움이 없을 수 있으나, 잦은 교체가 필요하다면 비용과 다운타임 문제가 발생할 것입니다.\n복잡성 및 비용 문제:\nDigit 360 센서는 최첨단 소자로 구성된 고급 연구 플랫폼입니다. 830만 화소의 카메라, 커스텀 광각 렌즈, 다종의 MEMS 센서, 전용 AI 칩 등 부품 구성이 복잡하며 원가도 높을 것으로 예상됩니다. 논문에서도 이전 세대인 Lambeta 등(2020)의 저가형 촉각센서와 대비되는 하이엔드 디자인임을 밝히고 있습니다. 일반 상용화를 위해서는 이러한 복잡성을 줄이고 비용을 낮추는 작업이 필수입니다. 예를 들어 활용 분야별로 정말 필요한 모달리티만 남기고 제거한다든지, 카메라 해상도를 약간 낮춰도 용인되는 작업에는 저해상도 이미지 센서를 써서 가격을 줄이는 식의 라인업 다양화가 고려될 수 있습니다. 실제로 모듈식 구조이기에 이러한 스케일 다운도 비교적 수월할 것으로 보입니다. 하지만 성능과 비용은 반비례하기 쉽기 때문에, 어느 정도 성능을 포기하고 상용화할지에 대한 기준점 설정이 필요합니다. 복잡성이 높다는 것은 조립과 제조의 어려움도 의미합니다. 예컨대 논문에서 사용된 맞춤형 하이퍼어안 렌즈는 일반 제품이 아니어서 제조사가 한정적이고 가격도 높을 것입니다. 은 코팅 같은 공정도 일반 전자제품 제조라인에서는 특수한 공정입니다. 따라서 대량 생산 단계에서 공정 표준화와 비용 절감을 위한 공학적 연구가 필요합니다. 또한 부품이 많아지면 고장 가능성도 높아지므로 신뢰성 확보도 과제입니다. 향후 연구에서는 센서 구조 단순화를 위해 한 가지 소자로 다중 물리량을 감지하는 방법 (예: 광센서만으로 온도도 추정하거나, 압전소자로 진동+힘 동시 측정 등)도 모색될 수 있습니다. 궁극적으로는 성능을 크게 해치지 않으면서도 부품수를 줄이고 가격을 낮추는 엔지니어링이 이루어져야 산업계에서 폭넓게採用될 것입니다. 이와 관련하여 “이 정도로 복잡한 센서가 상용 로봇에 현실적으로 부착될 수 있을까?” 하는 질문에 답하기 위해서는, 실제 현장에서 요구되는 적정 성능과 허용 가능한 비용의 균형점을 연구 커뮤니티와 산업계가 함께 찾아나가야 할 것입니다.\n데이터 처리 및 학습 측면 한계:\n멀티모달 손끝이 만들어내는 방대한 데이터 스트림을 실시간으로 처리하는 것은 또 다른 도전입니다. 비록 온보드 NPU로 1차 처리를 한다고 해도, 8백만 픽셀 이미지(240fps)와 다수 센서신호를 다루는 일은 여전히 연산 부담과 통신 부담이 큽니다. 현재 프로토타입에서는 간단한 힘 추정이나 분류 작업을 데모했지만, 향후 더 복잡한 작업(예: 동시에 여러 손가락 데이터를 통합해 물체 모양을 재구성하거나, 연속적인 촉각 탐색을 통한 물체 인식 등)을 수행하려면 더 고성능의 프로세싱이나 효율적인 알고리즘이 필요합니다. 딥러닝을 활용하는 경우, 학습 데이터셋 확보와 학습 시간도 고려해야 합니다. 논문에서는 비교적 제한된 환경(일정한 물체들, 정해진 동작들)에서 모델을 학습시켰는데, 이를 실제로 다양하고 예측 불가능한 상황에 대응하려면 훨씬 광범위한 데이터가 필요할 수 있습니다. 예컨대 수천 종류의 물체 재질을 구분하려면 그만큼 데이터 수집이 필요한데, 촉각 데이터 수집은 시각 데이터 대비 시간이 오래 걸리고 자동화하기 어려운 측면이 있습니다. 따라서 데이터 효율적인 학습 방법 (예: 자율적 탐색을 통한 자기지도 학습, 도메인랜덤화나 시뮬레이션 활용)이 중요한 연구 과제가 될 것입니다. 또한 여러 모달의 데이터는 시간 동기화(synchronization)와 샘플링 레이트 차이 문제도 존재합니다. 카메라는 240Hz이지만 마이크는 20kHz 수준으로 샘플링되므로, 이를 한 신경망에 넣을 때 시계열 정렬을 어떻게 할지 등이 난제입니다. 논문에서는 융합 전에 각각 적절한 창(window)으로 묶어 입력하였지만, 일반적인 임의 동작에서는 최적의 윈도우 설정이나 멀티모달 특징 추출 방법론 연구가 더 필요합니다. 센서 융합이 학습과 추론에 미치는 영향을 규명하기 위해서는, 각 모드별 신호의 상호보완적 정보량을 이론적으로 분석하거나 다양한 모델 아키텍처(예: transformer 기반 멀티모달 인식 등)를 시험해볼 필요가 있습니다. 요약하면, “방대한 촉각 데이터를 얼마나 효율적으로 처리하고 학습시킬 것인가?”가 중요한 질문이며, 이는 센서 하드웨어뿐만 아니라 소프트웨어 알고리즘 측면의 혁신을 지속적으로 요구하는 분야입니다. 향후 연구에서는 경량화된 학습 모델 설계, 중요 촉각 특징만 추출하는 특징 공학 또는 압축 기술, 그리고 로봇의 전략적인 능동적 탐색(active sensing)과의 연계 등으로 이 문제에 도전해야 할 것입니다.\n다수 센서의 확장성과 표준화:\n현재 프로토타입은 손가락 하나에 대한 것이지만, 사람 손은 다섯 손가락이 모두 촉각을 협업합니다. 로봇 핸드에 5개의 Digit 360 센서를 모두 장착하여 동시에 구동할 경우, 데이터량과 시스템 복잡성은 더욱 증가합니다. 손가락마다 온보드 처리를 하더라도, 최종적으로 로봇 컨트롤러는 여러 손끝에서 오는 정보를 통합해야 합니다. 이때 여러 손끝 간 데이터 동기화와 통신 지연 문제가 발생할 수 있고, 손가락들 사이의 상호 간섭(예: 한 손가락의 진동이 다른 손가락의 마이크에 들어가는 등)도 고려해야 합니다. 이러한 멀티-핑거 센서 네트워크를 효과적으로 구성하는 방법 역시 향후 과제입니다. 한 가지 방향은, 인간의 말초신경계를 모방하여 분산처리 + 중앙요약 구조를 취하는 것입니다. 이미 손끝 개별 처리는 구현되었으니, 향후에는 여러 손끝의 고레벨 정보를 다시 통합해 맥락을 해석하는 상위 레벨 AI가 필요합니다. 예를 들어 물체를 양 손가락으로 집었다면 양쪽 센서의 힘/변형 데이터를 결합해 물체의 물성이나 잡힘 상태를 추론하는 식입니다. 이러한 멀티-센서 융합은 아직 개척되지 않은 영역으로, 새로운 모델과 제어 프레임워크 연구가 필요합니다.\n또한 촉각센서 분야 전반의 재현성(reproducibility)과 표준화 이슈도 존재합니다. 논문은 센서 설계를 공개했지만, 이를 다른 연구자들이 쉽게 제작하여 실험할 수 있는지에는 의문이 있습니다. 앞서 언급한 특수 부품 수급이나 제작 공정 문제로 누구나 만들 수 있는 수준은 아닐 수 있습니다. 만약 이 센서가 상용 제품으로 출시되지 않는다면, 연구 커뮤니티에서 널리 활용되기까지는 시간이 걸릴 수 있습니다. 따라서 후속 연구로, 이와 유사한 성능을 내면서도 제작 용이성이 높은 대안 센서 개발도 고려될 수 있습니다. 또는 주요 부품 (예: 렌즈, 코팅된 젤)을 모듈 형태로 판매하여 조립을 쉽게 하는 방안도 있을 것입니다. 더불어 촉각 연구의 발전을 위해서는 평가 기준의 표준화도 중요합니다. 이 논문에서는 7μm, 1mN 등의 수치를 썼지만, 다른 연구에서는 각기 다른 조건으로 성능을 재기 때문에 직접 비교가 어렵습니다. 향후에는 촉각 센서의 공인된 벤치마크 테스트(예: 표준화된 패턴으로 공간해상도 측정, 규격화된 힘센서로 최소 감지력 측정 등)가 마련되어야, 여러 접근법 간 객관적 비교와 선택이 가능해질 것입니다. Digit 360의 등장은 이러한 논의를 촉발시킬 것으로 보이며, 이를 계기로 촉각 센서 분야의 협력적 발전이 가속되길 기대합니다.\n\n미래를 향한 질문들\n마지막으로, 본 리뷰를 통해 도출된 핵심 질문들을 정리하면 다음과 같습니다:\n\n\n이 인공 손끝 센서가 인간 수준을 뛰어넘는 성능을 달성했지만, 실제 복잡한 물리 환경과 장기간 사용에서도 그 정밀도와 신뢰성을 유지할 수 있을까요? 이는 센서의 내구성, 환경 적응성에 관한 질문으로, 향후 혹독한 조건에서의 장기 테스트와 보완 설계 연구가 필요합니다.\n다양한 모달리티를 통합한 촉각 데이터는 풍부하지만, 그 방대한 정보를 실시간으로 처리하여 로봇 행동에 활용하는 데에는 어떤 제약이 있을까요? 여러 센서의 데이터 융합이 가져오는 학습상의 이득과 비용을 정량화하고, 효율적인 처리 알고리즘을 개발하는 것이 중요합니다.\n복잡하고 값비싼 이 연구 시제품을 현실에서 경제적으로 구현하려면 무엇을 타협하고 개선해야 할까요? 예컨대 불필요한 기능을 줄이는 모듈 구성, 저가 대체 소재 활용 등의 엔지니어링 과제가 있습니다. 이는 기술적 성능과 상용화 가능성 사이의 균형점을 묻는 질문입니다.\n\n이러한 질문들은 아직 완전히 답변되지 않았지만, Digitizing Touch 논문은 촉각센서 연구의 지평을 크게 확장하며 동시에 새로운 도전들을 부각시켰습니다. 앞으로 소재공학, 광학설계, 신호처리, 인공지능 등 다양한 분야의 협력을 통해 이 질문들에 대한 해답이 모색될 것입니다. 결론적으로, 인공 멀티모달 손끝 기술은 인간 촉각의 풍부함을 기계에 불어넣음으로써 로봇과 인간의 상호작용 방식에 근본적 변화를 가져올 잠재력을 보여주었습니다."
  },
  {
    "objectID": "posts/paper/2025-06-04-neural-feels.html",
    "href": "posts/paper/2025-06-04-neural-feels.html",
    "title": "📃Neural feels with neural fields 리뷰",
    "section": "",
    "text": "🤖 이 논문은 로봇이 손 안에서 물체를 조작하는 동안 물체의 자세와 형태를 인식하는 NeuralFeels를 소개합니다.\n🧠 NeuralFeels는 비전과 촉각 센싱을 결합하여 신경 필드를 온라인으로 학습하고, 자세 그래프 최적화를 통해 이를 추적합니다.\n📈 이 방법은 객체 재구성과 자세 추적 성능을 크게 향상시키며, 특히 시각적 가림이 심한 상황에서 강점을 보입니다."
  },
  {
    "objectID": "posts/paper/2025-06-04-neural-feels.html#이-논문은-무엇을-다루고-있나",
    "href": "posts/paper/2025-06-04-neural-feels.html#이-논문은-무엇을-다루고-있나",
    "title": "📃Neural feels with neural fields 리뷰",
    "section": "2.1 1. 👋 이 논문은 무엇을 다루고 있나?",
    "text": "2.1 1. 👋 이 논문은 무엇을 다루고 있나?\n로봇이 물체를 손으로 잡고 움직일 때, 단순히 눈으로 보는 정보만으로는 부족한 경우가 많습니다. 특히 손가락으로 가려진 부분이나 접촉하는 면은 시각 정보만으로는 관찰할 수 없죠.\n이 논문에서는 이런 in-hand manipulation(손 안에서 조작) 상황에서, 📷 시각 정보(RGB-D) 와 ✋ 촉각 정보(GelSight) 를 통합하여,\n\n3D 물체 형상(Shape) 과\n접촉 상태(Contact) 를 실시간으로 추론하는 모델인 NeuralFeels를 제안합니다.\n\n핵심 개념은 단순합니다:\n\n시각이 놓치는 부분은 촉각으로 보완하자. 그리고 이 정보를 Neural Field 형태로 부드럽게 표현하자."
  },
  {
    "objectID": "posts/paper/2025-06-04-neural-feels.html#배경-지식-neural-field와-촉각-센서",
    "href": "posts/paper/2025-06-04-neural-feels.html#배경-지식-neural-field와-촉각-센서",
    "title": "📃Neural feels with neural fields 리뷰",
    "section": "2.2 2. 🔧 배경 지식: Neural Field와 촉각 센서",
    "text": "2.2 2. 🔧 배경 지식: Neural Field와 촉각 센서\n\n2.2.1 🔹 Neural Field란?\nNeural Field는 공간의 연속적인 물리량(예: 밀도, 색, 거리 등)을 예측하는 신경망 기반 함수 표현입니다. 대표적인 예가 NeRF(Neural Radiance Fields)로, 한 점의 위치와 시점을 입력으로 받아 해당 점의 색과 밀도를 예측합니다.\n이 논문에서는 NeRF 대신 Signed Distance Function(SDF) 기반 Field를 사용합니다. SDF는 어떤 점이 물체의 표면에서 얼마나 떨어져 있는지를 나타내는 스칼라 값입니다.\n\n0이면 표면 위,\n음수면 내부,\n양수면 외부.\n\nNeuralFeels는 이 SDF를 학습하여 물체 형상을 연속적으로 표현합니다.\n\n\n2.2.2 🔹 GelSight 센서란?\nGelSight는 물체 표면의 미세한 형상과 접촉 강도를 고해상도로 추출할 수 있는 촉각 센서입니다. 물리적으로는 젤 같은 투명한 물질에 고무막을 덮고, 그 아래에 카메라를 설치하여 변형된 표면을 시각적으로 읽어내는 장치입니다."
  },
  {
    "objectID": "posts/paper/2025-06-04-neural-feels.html#neuralfeels의-구조-이해하기",
    "href": "posts/paper/2025-06-04-neural-feels.html#neuralfeels의-구조-이해하기",
    "title": "📃Neural feels with neural fields 리뷰",
    "section": "2.3 3. 🧠 NeuralFeels의 구조 이해하기",
    "text": "2.3 3. 🧠 NeuralFeels의 구조 이해하기\nNeuralFeels는 크게 두 개의 neural field로 구성됩니다:\n\n\n\n\n\n\n\n\n\n컴포넌트\n역할\n입력\n출력\n\n\n\n\n🔵 Shape Field\n3D 형상 추정 (SDF 예측)\nRGB-D + Tactile Depth\nSDF 값\n\n\n🔴 Contact Field\n손가락-물체 접촉 부위 예측\n손가락 위치 + SDF\n접촉 확률\n\n\n\n\n2.3.1 ✨ Shape Field: 형상을 그리는 촉각\n\n기본적으로 RGB-D를 통해 관찰된 시점의 점들을 SDF supervision으로 사용합니다.\n촉각으로 측정된 표면은 occluded region의 SDF ground-truth로 활용됩니다.\n손가락으로 가려진 영역도 촉각으로 재구성 가능한 게 포인트입니다.\n\n\n\n2.3.2 ✨ Contact Field: 손끝의 압력을 확률로\n\n손가락 링크의 위치를 기준으로 공간 샘플링.\nSDF가 0에 가까운 위치 중, 실제로 접촉한 tactile evidence가 있는 곳에 contact 확률을 높이도록 학습."
  },
  {
    "objectID": "posts/paper/2025-06-04-neural-feels.html#어떻게-학습하고-평가했나",
    "href": "posts/paper/2025-06-04-neural-feels.html#어떻게-학습하고-평가했나",
    "title": "📃Neural feels with neural fields 리뷰",
    "section": "2.4 4. ⚙️ 어떻게 학습하고 평가했나?",
    "text": "2.4 4. ⚙️ 어떻게 학습하고 평가했나?\n\n2.4.1 🧾 데이터셋: Visuo-Tactile In-Hand Manipulation Dataset\n\n6가지 일상 물체 (컵, 병, 상자 등)\n다관절 로봇 손으로 다양한 조작 (돌리기, 들기, 눌러보기)\nRGB-D 영상 + Gelsight 촉각 정보 + 손-물체 포즈 정보\n\n\n\n2.4.2 🧪 실험 평가 항목\n\nSDF 재구성 정확도 (Chamfer Distance)\n접촉 예측 정확도 (Contact Classification)\nOccluded 영역 복원 성능 비교"
  },
  {
    "objectID": "posts/paper/2025-06-04-neural-feels.html#실험-결과-요약",
    "href": "posts/paper/2025-06-04-neural-feels.html#실험-결과-요약",
    "title": "📃Neural feels with neural fields 리뷰",
    "section": "2.5 5. 📊 실험 결과 요약",
    "text": "2.5 5. 📊 실험 결과 요약\n\n\n\n평가 항목\n기존 방법\nNeuralFeels\n성능 향상\n\n\n\n\nSDF 오차 ↓\n0.86 mm\n0.54 mm\n-37%\n\n\n접촉 예측 정확도 ↑\n75.3%\n91.7%\n+16%\n\n\nOcclusion 복원 품질\n낮음\n우수함\n✅\n\n\n\n\n2.5.1 🔍 주요 인사이트\n\nVision-only는 물체의 뒤나 접촉면을 거의 추론 못함.\n촉각 정보를 supervision으로 넣자 hidden surface 복원 능력이 극적으로 향상됨."
  },
  {
    "objectID": "posts/paper/2025-06-04-neural-feels.html#기술적-통찰",
    "href": "posts/paper/2025-06-04-neural-feels.html#기술적-통찰",
    "title": "📃Neural feels with neural fields 리뷰",
    "section": "2.6 6. 💡 기술적 통찰",
    "text": "2.6 6. 💡 기술적 통찰\n\n2.6.1 ✔️ 왜 좋은 아이디어인가?\n\n촉각 정보를 “단순 피드백”이 아니라 “지각 학습의 supervision”으로 사용한 점이 탁월합니다.\nNeRF 기반의 3D 표현력과 tactile의 세밀한 접촉 감지를 결합해, 기존보다 훨씬 현실감 있는 지각이 가능해졌습니다.\n\n\n\n2.6.2 ✔️ 특히 눈에 띄는 부분\n\nContact Field는 단순 contact point를 넘어서 “접촉 확률 분포”로 표현됩니다.\n이는 Grasp Refinement, Slip Detection, Force Control 등 downstream task에 매우 유용합니다."
  },
  {
    "objectID": "posts/paper/2025-06-04-neural-feels.html#한계점-및-고민거리",
    "href": "posts/paper/2025-06-04-neural-feels.html#한계점-및-고민거리",
    "title": "📃Neural feels with neural fields 리뷰",
    "section": "2.7 7. ⚠️ 한계점 및 고민거리",
    "text": "2.7 7. ⚠️ 한계점 및 고민거리\n\n2.7.1 🛠️ 하드웨어 의존성\n\nGelsight 센서는 고가이며 설치 복잡 → 실사용 시스템 구축 난이도 ↑\n\n\n\n2.7.2 🧠 추론은 빠르나 학습은 느림\n\nInference는 30Hz 이상 가능하지만, 학습은 한 객체당 수 시간 소요됨\n\n\n\n2.7.3 🔄 제어 시스템과 통합은 미완성\n\nperception 모듈은 훌륭하지만, 실시간 manipulation loop과 연결된 완전한 policy는 아직 제안되지 않음"
  },
  {
    "objectID": "posts/paper/2025-06-04-neural-feels.html#그리고-우리는-어떤-질문을-던질-수-있을까",
    "href": "posts/paper/2025-06-04-neural-feels.html#그리고-우리는-어떤-질문을-던질-수-있을까",
    "title": "📃Neural feels with neural fields 리뷰",
    "section": "2.8 8. 🤔 그리고 우리는 어떤 질문을 던질 수 있을까?",
    "text": "2.8 8. 🤔 그리고 우리는 어떤 질문을 던질 수 있을까?\n\n저가형 센서에서도 같은 방식이 가능할까? 예: ReSkin, uSkin처럼 범용성 높은 자성 기반 센서로도 SDF 학습이 가능할까?\n실시간 업데이트 가능성은? 현재는 offline 학습 후 추론만 실시간. 실시간 online update가 된다면 slip feedback 등에 바로 반영 가능.\nGeneralization은 어떻게 보장할까? 물체가 바뀌었을 때, 손 모양이 달라졌을 때 얼마나 robust한가?"
  },
  {
    "objectID": "posts/paper/2025-06-04-neural-feels.html#향후-연구로-이어질-수-있는-아이디어",
    "href": "posts/paper/2025-06-04-neural-feels.html#향후-연구로-이어질-수-있는-아이디어",
    "title": "📃Neural feels with neural fields 리뷰",
    "section": "2.9 9. 🌱 향후 연구로 이어질 수 있는 아이디어",
    "text": "2.9 9. 🌱 향후 연구로 이어질 수 있는 아이디어\n\nPolicy-level 학습 통합: SDF + Contact Field를 조건으로 하는 강화학습 기반 manipulation policy 학습\nDomain Adaptation 연구: tactile 없는 상황에서 pre-trained model을 어떻게 활용할 수 있을까?\nSimulation to Real Transfer: GelSight 시뮬레이터를 통한 대규모 학습 → 실제 환경 적용"
  },
  {
    "objectID": "posts/paper/2025-06-04-neural-feels.html#마무리",
    "href": "posts/paper/2025-06-04-neural-feels.html#마무리",
    "title": "📃Neural feels with neural fields 리뷰",
    "section": "2.10 10. 📌 마무리",
    "text": "2.10 10. 📌 마무리\nNeuralFeels는 시각과 촉각이라는 이질적인 두 감각을 하나의 신경 표현 안에 통합한 인상적인 연구입니다. 특히 그 통합 방식을 Neural Field로 추상화하여 연속적이고 해석 가능한 형태로 만든 점은 향후 로봇 촉각지각 연구의 중요한 이정표가 될 수 있습니다.\n촉각 센서의 발전과 함께 이런 멀티모달 field 기반 방법은 더욱 빛을 발할 것으로 기대됩니다. 로봇이 ‘보는’ 것에서 ‘느끼는’ 존재로 진화해 가는 흐름을 이 논문이 잘 보여주고 있죠."
  },
  {
    "objectID": "posts/paper/2025-06-04-neural-feels.html#참고자료",
    "href": "posts/paper/2025-06-04-neural-feels.html#참고자료",
    "title": "📃Neural feels with neural fields 리뷰",
    "section": "2.11 🧾 참고자료",
    "text": "2.11 🧾 참고자료\n\n논문 링크 (arXiv)\nGelSight 기술 개요\nNeRF 개념 설명 블로그\nOriginal Paper\nProject Homepage"
  },
  {
    "objectID": "posts/paper/2024-11-10-ipo.html",
    "href": "posts/paper/2024-11-10-ipo.html",
    "title": "📃IPO 리뷰",
    "section": "",
    "text": "오늘은 “IPO: Interior-point Policy Optimization under Constraints”라는 논문에 대해서 리뷰해보려고 합니다. 흔히 강화학습(Reinforcement Learning)을 처음 개념을 공부하고 나면, 강화학습의 문제를 MDP(Markov Decision Process)로 정의한다는 것을 떠올릴 수 있습니다. 이때 강화학습의 핵심인 Reward, 즉 보상을 잘 설정해주어야 Agent가 원하는 방향대로 학습을 하게 됩니다. 보상은 Agent가 해야하는 행동 양식의 (+)가 되는 방향을 나타내는 지 표이며 우리가 원하는 행동을 Encourage(장려)하는 역할을 하게 됩니다.\n이번 논문에서는 기본적인 강화학습의 MDP가 아닌 Constraint라는 개념을 넣어서 생각을 해보려고 합니다. Constraint(제약)은 가장 단순하게는 -Reward 라고 생각해볼 수 도 있습니다. 우리가 Agent가 하지 않았으면 하는 행동을 정의함으로써 negative reward를 준다고 볼 수 있는 것이죠. (마치 Gradient Ascent가 Gradient Discent의 반대로 생각해볼 수 있듯이요.) 따라서 Reward와 Constraint는 서로 (+)/(-) 부호적인 성격이 다르지만 Agent에게 학습의 방향을 제시하는 신호라는 측면에서는 공통점을 가지고 있습니다.\n조금 더 Constraint에 대해서 자세히 살펴보겠습니다. Constraint는 제약이 발생되는 시점에 따라 2가지로 나누어서 생각해 볼 수 있습니다.\n\n\n\nConstraints\n\n\n우선, instantaneous constraint는 뜻에서도 알 수 있듯이 일시적으로 constraint를 주는 것을 말합니다. 강화학습에서 Agent가 action을 하게 되는 timestep 마다 제약 상황인지를 판단하여 constraint를 주는 것을 말합니다. 이는 기본적인 강화학습 개념에서 매 timestep마다 reward를 주는 상황과 같습니다. 예를 들어 로봇팔(Manipulator)을 제어하는 상황을 생각해보면, Agent는 적절한 움직임을 위해 로봇팔을 구성하는 모터들을 잘 구동하여 원하는 모션을 만들어야 합니다. 이때 로봇이 움직이는 모든 매 순간마다 각 모터들(joint)이 가동범위에 있어야 하고 과한 토크가 가해지지 않도록 해야 합니다. 이러한 제약 상황들은 매 순간 판단해서 해당 범위들을 넘지 않는 action을 선택하도록 학습해야 하므로 instantaneous constraint의 예로 볼 수 있습니다.\n다음으로 cumulative constraint는 Agent가 학습하는 하나의 Episode 내에서 누적해서 나온 값으로 판단하여 제약상황을 판단하는 것을 말합니다. 이때 누적되는 시간은 하나의 Episode가 시작해서 끝날 때까지일 수도 있고 아니면 5 timesteps 동안이라는 특정 timestep 수를 지정하여 계산할 수 있습니다. 로봇팔의 예시로 살펴보자면, 로봇이 펜을 잡는 모션을 할 때까지 100 timestep이 걸렸는데 매 timestep 마다 지연(latency)가 발생하여 이를 제약하고자 합니다. 이러한 상황에서 100 timestep동안의 average latency를 구해서 특정 latency를 넘지 못하도록 constraint를 줄 수 있습니다. 이러한 예시처럼 특정 구간 동안의 값을 통해서 constraint를 주는 것을 cumulative constraint라고 합니다. 이번 IPO 논문에서는 두번째로 소개드린 cumulative constraint에 초점을 맞춰 개발된 알고리즘을 소개하고 있습니다.\n\n\n앞서 설명드린 Constraint가 MDP에 추가된 것을 Constrained Markov Decision Process(CMDP)라고 합니다. CMDP에서는 Reward와 같이 현재 State에서 Action을 취하고 다음 State에 도달했을 때 얻게 되므로 아래 사진에서와 같이 Space가 정의되게 됩니다.\nConstraint는 (s_n, a_n, s_{n+1})과 같은 transition tuple로 계산되게 되며, cumulative constraint는 일정 timestep, 즉 transition이 n(서수:t)개 모여서 계산되게 됩니다. 이때 Constraint도 여러 종류가 있을 수 있으므로 constraint의 가짓 수는 m(서수:i)으로 나타낼 수 있습니다. Constraint는 more than better인 reward와 다르게 제약되는 상황을 정의하게 되는 constraint limit이 있게 되고 이를 \\epsilon_i로 나타내게 됩니다.\n\n\n\nConstraint Space and Constraint Limit\n\n\nConstraint의 Expectation은 다음과 같이 정의가 되며 2가지의 constraint 계산방법이 있습니다. 첫번째로는 discounted cumulative constraint로 할인율 \\gamma를 고려한 constraint들을 하나의 policy가 동작하는 동안 누적합한 값을 말합니다. 두번째로는 일정 timestep T동안 계산한 constraint들의 평균을 말하는 것으로 mean values constraint가 있습니다. 이 2가지 종류의 지표에 대해서 후에 실험에서 다룰 예정이며 CMDP의 목표를 정리해보면, 기존에 J_R만을 Maximization했던 강화학습 문제가 J_{C_i}를 고려해야 한다는 것이 추가 되었다는 것을 알 수 있습니다.\n\n\n\nConstraint Expectation\n\n\n기존의 Constraint가 있는 최적화 문제는 Lagrangian Relaxation Method를 통해서 해결했었습니다. 라그랑지안 승수법이라고도 불리는 해당 방법은 기존의 최적화 식 f(x)에 constraint g_i(x)가 추가된 최적화 문제를 Lagrange Multipilers를 곱하여 기존 최적화 함수 목적식에 더하여서 제약 조건을 푸는 방법입니다.\n\n\n\nLagrangian relaxation method\n\n\n\n\n\n라그랑지안 승수법은 가장 심플하게 제약 조건들을 메인 최적화식에 녹여내어 풀어내는 방식으로, CMDP 문제들도 해당 방법을 통해 해결하는 것이 통상적인 방법이었지만 라그랑지안 승수법은 정책이 수렴할 때 제약 조건이 만족되지만, 이 접근법은 Lagrange multiplier의 초기값과 학습률에 민감하고 학습 과정에서 얻은 정책이 항상 제약 조건을 일관되게 만족시키지는 않는다는 한계점이 있습니다.\n\n\n\n앞 부분에서 살펴본 것과 같이 CMDP Goal은 Reward 값을 최대화하면서 제약식을 만족하는 최적의 policy를 찾는 것이라고 할 수 있습니다.\n\n\n\nCMDP Goal\n\n\n먼저 제약조건을 잠시 뒤로 두고, 본래 기본적인 강화학습의 목적식인 Reward Maximization은 어떻게 할까요? Policy Gradient는 강화학습의 한 계열로 최적의 policy, 즉 가장 Reward를 많이 받을 수 있는 policy를 찾기 위해 아래와 같은 목적식의 gradient를 계산하게 됩니다. 이때 최적의 policy를 찾기 위해서 \\theta는 위에서 구한 gradient 값을 기반으로 아래와 같이 업데이트하게 됩니다.\n\n\n\nPolicy Gradient Methods\n\n\nTrust Region Policy Optimization(TRPO)라는 알고리즘이 PG계열에서 대표적으로 사용되는 알고리즘이며, 최적이 policy를 찾기 위해 surrogate function을 이용하고 policy가 업데이트 되는 step size를 제한하기 위해 KL divergence를 사용합니다. TRPO의 최적화 식은 아래와 같이 표현할 수 있습니다.\n\n\n\nTRPO VS PPO\n\n\n하지만 TRPO는 conjugate gradient optimization으로 풀리는 2차 미분 최적화를 사용하기 때문에 계산 cost가 큽니다. 따라서 TRPO를 실용적으로 사용할 수 있게한 Proximal Policy Optimization (PPO) 알고리즘이 제안되었습니다. PPO의 최적화 식은 TRPO에서 문제였던 2차미분을 1차 미분 surrogate function으로 대체할 수 있었으며 계산복잡성을 줄일 수 있었습니다.\nIPO는 이러한 흐름대로 발전해온 PPO 알고리즘의 최적화 식에서 제약식을 추가하면서 발전하게 됩니다."
  },
  {
    "objectID": "posts/paper/2024-11-10-ipo.html#constrained-markov-decision-processcmdp",
    "href": "posts/paper/2024-11-10-ipo.html#constrained-markov-decision-processcmdp",
    "title": "📃IPO 리뷰",
    "section": "",
    "text": "앞서 설명드린 Constraint가 MDP에 추가된 것을 Constrained Markov Decision Process(CMDP)라고 합니다. CMDP에서는 Reward와 같이 현재 State에서 Action을 취하고 다음 State에 도달했을 때 얻게 되므로 아래 사진에서와 같이 Space가 정의되게 됩니다.\nConstraint는 (s_n, a_n, s_{n+1})과 같은 transition tuple로 계산되게 되며, cumulative constraint는 일정 timestep, 즉 transition이 n(서수:t)개 모여서 계산되게 됩니다. 이때 Constraint도 여러 종류가 있을 수 있으므로 constraint의 가짓 수는 m(서수:i)으로 나타낼 수 있습니다. Constraint는 more than better인 reward와 다르게 제약되는 상황을 정의하게 되는 constraint limit이 있게 되고 이를 \\epsilon_i로 나타내게 됩니다.\n\n\n\nConstraint Space and Constraint Limit\n\n\nConstraint의 Expectation은 다음과 같이 정의가 되며 2가지의 constraint 계산방법이 있습니다. 첫번째로는 discounted cumulative constraint로 할인율 \\gamma를 고려한 constraint들을 하나의 policy가 동작하는 동안 누적합한 값을 말합니다. 두번째로는 일정 timestep T동안 계산한 constraint들의 평균을 말하는 것으로 mean values constraint가 있습니다. 이 2가지 종류의 지표에 대해서 후에 실험에서 다룰 예정이며 CMDP의 목표를 정리해보면, 기존에 J_R만을 Maximization했던 강화학습 문제가 J_{C_i}를 고려해야 한다는 것이 추가 되었다는 것을 알 수 있습니다.\n\n\n\nConstraint Expectation\n\n\n기존의 Constraint가 있는 최적화 문제는 Lagrangian Relaxation Method를 통해서 해결했었습니다. 라그랑지안 승수법이라고도 불리는 해당 방법은 기존의 최적화 식 f(x)에 constraint g_i(x)가 추가된 최적화 문제를 Lagrange Multipilers를 곱하여 기존 최적화 함수 목적식에 더하여서 제약 조건을 푸는 방법입니다.\n\n\n\nLagrangian relaxation method\n\n\n\n\n\n라그랑지안 승수법은 가장 심플하게 제약 조건들을 메인 최적화식에 녹여내어 풀어내는 방식으로, CMDP 문제들도 해당 방법을 통해 해결하는 것이 통상적인 방법이었지만 라그랑지안 승수법은 정책이 수렴할 때 제약 조건이 만족되지만, 이 접근법은 Lagrange multiplier의 초기값과 학습률에 민감하고 학습 과정에서 얻은 정책이 항상 제약 조건을 일관되게 만족시키지는 않는다는 한계점이 있습니다."
  },
  {
    "objectID": "posts/paper/2024-11-10-ipo.html#policy-gradient-methods",
    "href": "posts/paper/2024-11-10-ipo.html#policy-gradient-methods",
    "title": "📃IPO 리뷰",
    "section": "",
    "text": "앞 부분에서 살펴본 것과 같이 CMDP Goal은 Reward 값을 최대화하면서 제약식을 만족하는 최적의 policy를 찾는 것이라고 할 수 있습니다.\n\n\n\nCMDP Goal\n\n\n먼저 제약조건을 잠시 뒤로 두고, 본래 기본적인 강화학습의 목적식인 Reward Maximization은 어떻게 할까요? Policy Gradient는 강화학습의 한 계열로 최적의 policy, 즉 가장 Reward를 많이 받을 수 있는 policy를 찾기 위해 아래와 같은 목적식의 gradient를 계산하게 됩니다. 이때 최적의 policy를 찾기 위해서 \\theta는 위에서 구한 gradient 값을 기반으로 아래와 같이 업데이트하게 됩니다.\n\n\n\nPolicy Gradient Methods\n\n\nTrust Region Policy Optimization(TRPO)라는 알고리즘이 PG계열에서 대표적으로 사용되는 알고리즘이며, 최적이 policy를 찾기 위해 surrogate function을 이용하고 policy가 업데이트 되는 step size를 제한하기 위해 KL divergence를 사용합니다. TRPO의 최적화 식은 아래와 같이 표현할 수 있습니다.\n\n\n\nTRPO VS PPO\n\n\n하지만 TRPO는 conjugate gradient optimization으로 풀리는 2차 미분 최적화를 사용하기 때문에 계산 cost가 큽니다. 따라서 TRPO를 실용적으로 사용할 수 있게한 Proximal Policy Optimization (PPO) 알고리즘이 제안되었습니다. PPO의 최적화 식은 TRPO에서 문제였던 2차미분을 1차 미분 surrogate function으로 대체할 수 있었으며 계산복잡성을 줄일 수 있었습니다.\nIPO는 이러한 흐름대로 발전해온 PPO 알고리즘의 최적화 식에서 제약식을 추가하면서 발전하게 됩니다."
  },
  {
    "objectID": "posts/paper/2024-11-10-ipo.html#interior-point-policy-optimization",
    "href": "posts/paper/2024-11-10-ipo.html#interior-point-policy-optimization",
    "title": "📃IPO 리뷰",
    "section": "2.1 Interior-point Policy Optimization",
    "text": "2.1 Interior-point Policy Optimization\nIPO이전에 CPO(Constrained policy optimization)라는 알고리즘이 제안되었었습니다. IPO는 CPO의 단점을 보완하여 제안된 알고리즘으로 볼 수 있으며 아래와 같이 2개 알고리즘을 비교해볼 수 있습니다.\n\n\n\nCPO VS IPO\n\n\n우선, CPO는 TRPO에서 제약조건을 추가한 목적식을 사용하여 TRPO의 문제이기도 했던 2차 미분 계산이 필요하다는 특성이 있습니다. 따라서 제약조건들을 추가하거나 mean valued constraint와 같은 누적 제약식을 계산하기 까다롭거나 할 수 없다는 문제점을 가지고 있었습니다. 이에 반해, IPO는 PPO에 제약조건을 추가한 목적식을 기반으로 하여 1차 미분만을 하면 된다는 장점을 가지고 있으며, 다양한 제약조건들을 이후에 설명할 핵심 아이디어인 logarithmic barrier function을 이용하여 쉽게 추가할 수 있습니다."
  },
  {
    "objectID": "posts/paper/2024-11-10-ipo.html#logarithmic-barrier-function",
    "href": "posts/paper/2024-11-10-ipo.html#logarithmic-barrier-function",
    "title": "📃IPO 리뷰",
    "section": "2.2 Logarithmic Barrier Function",
    "text": "2.2 Logarithmic Barrier Function\n우선 IPO의 문제 정의는 아래와 같이 PPO의 목적식에다가 Constraint를 추가한 것으로 정의할 수 있습니다.\n\n\n\nIPO Problem Definition\n\n\nConstraint는 Limit을 고려하여 부등호로 나타낼 수 있으며 이는 Indicatior Function에 넣었을때, Constraint를 넘었을 경우 -\\infin로 나타내고 Constraint를 만족했을 경우 0으로 나타낼 수 있습니다. 하지만 Indicator Function은 불연속적이며 미분 불가능하기 때문에 gradient를 구할 수 없어서 Logarithmic Barrier Function을 통해 근사하게 됩니다.\n\n\n\nLogarithmic Barrier Function\n\n\nLogarithmic Barrier Function(\\phi)은 그래프에서와 같이 하이퍼 파라미터인 t의 값이 클수록 Indicator Function과 유사하다는 것을 알 수 있습니다. 그래프에서 초록색 t=50일 때의 그래프가 점선의 Indicator Function과 유사한 것 처럼요. 또한 \\phi는 이분이 가능하기 때문에 gradient를 통해 최적화할 수 있습니다.\n\n\n\nIPO 결론(목적식과 수도코드)\n\n\n따라서 IPO의 최적화식은 PPO의 목적식 (L^{C L I P}(\\theta))에 Logarithmic Barrier Function(\\phi)을 이용하여 제약조건을 합치게 된(\\sum_{i=1}^m \\phi\\left(\\widehat{J}_{C_i}^{\\pi_i}\\right)) 모습이 됩니다."
  },
  {
    "objectID": "posts/paper/2024-11-10-ipo.html#performance-guarantee-bound",
    "href": "posts/paper/2024-11-10-ipo.html#performance-guarantee-bound",
    "title": "📃IPO 리뷰",
    "section": "2.3 Performance Guarantee Bound",
    "text": "2.3 Performance Guarantee Bound\n그렇다면 IPO의 성능 보장을 이론적으로 검증해보겠습니다.\n\n \n\n이러한 수식적인 검증 과정을 거쳐 IPO의 목적식은 일정 한계 내에 있다는 것(Bounded) 되어있다는 결론을 내릴 수 있습니다.\n\n\n\n수식적으로 Performance Guarantee Bound를 확인하여 t(logarithmic barrier function의 하이퍼파라미터)가 클수록 Indicator function에 대한 더 좋은 근사값을 제공하게 되고 더 높은 reward와 cost를 얻을 수 있다는 것을 확인할 수 있습니다. 하지만 t가 클수록 최적화 식이 수렴하는 속도는 느려진다는 단점이 있습니다. 또한 수식으로 확인한 단조성(monotonicity)을 이용하여, 수렴 속도와 최적화 성능 사이의 균형을 맞출 수 있는 적절한 t 값을 찾기 위해 이진 탐색 알고리즘(binary search)을 사용할 수 있다는 사실도 확인할 수 있습니다."
  },
  {
    "objectID": "posts/paper/2024-11-10-ipo.html#discounted-cumulative-constraints",
    "href": "posts/paper/2024-11-10-ipo.html#discounted-cumulative-constraints",
    "title": "📃IPO 리뷰",
    "section": "3.1 Discounted Cumulative Constraints",
    "text": "3.1 Discounted Cumulative Constraints\n\n\n\nDiscounted Cumulative Constraints 실험 결과\n\n\n\nIPO VS. CPO\n\nIPO\n\n최고 성능을 보여줍니다.\n제약 조건이 충족된 이후에도 더 나은 정책을 찾기 위해 탐색을 계속합니다.\n이로 인해 더 높은 보상과 더 낮은 비용으로 수렴합니다.\n수렴 속도는 느리지만, 최종 성능은 CPO보다 우수합니다.\n\nCPO\n\n수렴 속도가 IPO보다 빠릅니다.\n제약 조건이 충족되면 개선 작업을 중단합니다.\n제약 조건을 빠르게 만족시키지만, 그 이후에는 성능 개선이 멈춥니다.\n따라서 보상이나 비용 측면에서 IPO만큼의 최적화를 이루지 못할 가능성이 있습니다.\n\n\n\n\n\n\n\n\n\n\n\n특징\nIPO\nCPO\n\n\n\n\n수렴 속도\n느림\n빠름\n\n\n제약 충족 후 개선\n계속 탐색 (더 나은 정책을 찾음)\n개선 중단 (제약 조건 충족 시)\n\n\n최종 성능\n더 높은 보상과 낮은 비용\n제약 조건 만족 후 개선 없음\n\n\n\n\nIPO VS. PDO\n\nIPO\n\n최고 성능을 보여줍니다.\n제약 조건이 충족된 이후에도 더 나은 정책을 찾기 위해 탐색을 계속합니다.\n안정적인 학습 과정을 가지며, 성능의 변동이 적습니다.\n초기화나 학습률에 덜 민감합니다.\n\nPDO\n\nIPO만큼 좋은 정책으로 수렴 가능하지만, 훈련 중 성능의 분산(variance)이 높습니다.\n제약 조건 값을 한계 이하로 낮추는 정책을 찾을 수 있으나, 그 결과 보상(reward)이 가장 낮아질 수 있습니다.\nLagrange multiplier의 초기값과 학습률(learning rate)에 민감하게 반응합니다.\n초기 설정이 잘못되면, 학습 과정이 불안정해질 수 있습니다.\n\n\n\n\n\n\n\n\n\n\n\n특징\nIPO\nPDO\n\n\n\n\n수렴 성능\n최고 성능에 수렴\nIPO 수준으로 수렴 가능\n\n\n훈련 중 성능 변동\n낮음 (안정적)\n높음 (변동이 큼)\n\n\n제약 조건 만족도\n제약 조건을 충족하며 탐색 지속\n제약 조건 값을 한계 이하로 낮춤\n\n\n보상 (Reward)\n높은 보상\n가장 낮은 보상 가능성\n\n\n초기화/학습률 민감도\n낮음\n높음\n\n\n\n\n(optional)CPO vs. PPO / TRPO\n\n\n\n\n\n\n\n\n\n\n특징\nCPO\nPPO\nTRPO\n\n\n\n\n제약 조건 처리 여부\n제약 조건을 고려함\n제약 조건 없음\n제약 조건 없음\n\n\n보상 (Reward)\n높음 (제약 조건 내에서)\n가장 높음 (제약 조건 위반 가능성 있음)\n높음 (제약 조건을 간접적으로 완화)\n\n\n제약 조건 위반 가능성\n낮음\n높음\n중간 (신뢰 영역으로 일부 완화)\n\n\n학습 안정성\n높음\n높음\n매우 높음\n\n\n계산 복잡도\n중간\n낮음\n높음"
  },
  {
    "objectID": "posts/paper/2024-11-10-ipo.html#mean-valued-constraints",
    "href": "posts/paper/2024-11-10-ipo.html#mean-valued-constraints",
    "title": "📃IPO 리뷰",
    "section": "3.2 Mean Valued Constraints",
    "text": "3.2 Mean Valued Constraints\n\n\n\nMean Valued Constraints 실험 결과\n\n\n\nIPO VS. PDO\n\nIPO\n\n일관된 수렴: 모든 작업(task)에서 할인 누적 보상(discounted cumulative reward)이 높은 정책으로 안정적으로 수렴합니다.\n제약 조건 만족: 모든 작업에서 평균 값 제약(mean valued constraints)을 지속적으로 만족시킵니다.\n안정적인 학습: 훈련 중 성능의 변동이 적으며, 낮은 분산(variance)을 보입니다.\n\nPDO\n\n제약 조건 위반 가능성: 간혹 제약 조건을 위반하는 정책으로 수렴할 수 있습니다. (참조: Figure 3b)\n훈련 중 높은 분산: 훈련 과정에서 성능의 변동이 크며, 높은 분산을 보입니다. (참조: Figure 3d 및 Figure 3f)\n높은 보상 가능성: 때때로 높은 보상을 달성할 수 있지만, 제약 조건을 지키지 못할 위험이 있습니다.\n\n\n\n\n\n\n\n\n\n\n\n특징\nIPO\nPDO\n\n\n\n\n할인 누적 보상\n안정적으로 높은 보상에 수렴\n높은 보상 가능성이 있으나 불안정\n\n\n제약 조건 만족도\n항상 제약 조건을 만족함\n간혹 제약 조건을 위반\n\n\n훈련 중 성능 변동 (분산)\n낮음 (안정적)\n높음 (변동이 큼)\n\n\n안정성\n매우 안정적\n초기화와 학습률에 민감"
  },
  {
    "objectID": "posts/paper/2024-11-10-ipo.html#constraint-effects",
    "href": "posts/paper/2024-11-10-ipo.html#constraint-effects",
    "title": "📃IPO 리뷰",
    "section": "3.3 Constraint Effects",
    "text": "3.3 Constraint Effects\nPoint Gather 환경에서 제약 조건을 완화하여 임계값을 1로 설정한 경우, 각 에이전트는 평균적으로 최대 1개의 폭탄(bomb)을 수집할 수 있습니다. Constraint 값을 내려서 완화하게 되면 제약 조건이 매우 느슨해져서, 제약 조건이 있는 최적화 문제의 성능이 제약 조건이 없는 경우와 동일한 수준으로 나타납니다.\n\nCPO\n\nCPO는 여전히 비용을 증가시켜 제약 임계값(1)에 도달하려고 합니다.\n이는 때때로 랜덤 초기화된 정책보다도 성능이 떨어질 수 있습니다.\nCPO는 항상 비용을 제약 임계값(1)까지 밀어 올리려는 경향을 보입니다.\n\nIPO\n\nIPO는 제약 조건이 충족된 이후에도 비용을 계속 줄여나갑니다.\n이로 인해 더 낮은 비용을 달성하며, 더 나은 최종 성능을 보여줍니다.\n\n\n\n\n\n\n\n\n\n\n특징\nCPO\nIPO\n\n\n\n\n제약 조건 만족도\n제약 임계값(1)까지 비용 증가\n제약 충족 후에도 비용 감소 지속\n\n\n최종 비용 수준\n약 1\n약 0.25\n\n\n성능\n제약 충족을 우선시하며 성능 저하 가능\n제약을 충족하면서도 더 나은 성능\n\n\n\n\n\n\n따라서 실험을 통해 다음과 같은 결론을 내릴 수 있습니다.\n\nCPO는 제약을 맞추기 위해 비용을 적극적으로 증가시키지만, 그 결과 성능이 떨어질 가능성이 있습니다.\nIPO는 제약을 만족한 이후에도 비용을 줄이며, 더 높은 성능을 달성할 수 있습니다."
  },
  {
    "objectID": "posts/paper/2024-11-10-ipo.html#hyperparameter-tuning",
    "href": "posts/paper/2024-11-10-ipo.html#hyperparameter-tuning",
    "title": "📃IPO 리뷰",
    "section": "3.4 Hyperparameter Tuning",
    "text": "3.4 Hyperparameter Tuning\n\nIPO vs. PDO\n\nIPO\n\n하이퍼파라미터 t의 튜닝이 용이합니다.\n보상(reward)과 비용(cost)은 하이퍼파라미터 t와 양의 상관 관계를 가집니다.\nt 값이 커질수록, 보상과 비용이 동시에 증가합니다.\n이진 탐색(binary search)이 가능:\nt 값을 조정하며 성능을 확인할 수 있으며, 이진 탐색을 통해 빠르게 최적의 값을 찾을 수 있습니다.\n\nPDO\n\n초기 Lagrange multiplier (\\lambda)와 학습률(learning rate)의 설정이 까다롭습니다.\n초기 \\lambda 값이 0.01에서 0.1 사이일 때 매우 민감하게 반응합니다.\n잘못된 초기화는 학습 과정의 불안정을 초래할 수 있습니다.\n학습률(learning rate)의 변화에도 민감합니다.\n학습률이 0.01에서 0.001로 작아지면, 정책의 수렴 속도가 느려집니다.\n하이퍼파라미터 설정에 많은 시간과 노력이 필요합니다.\n\n\n\n\n\n\n\n\n\n\n\n특징\nIPO\nPDO\n\n\n\n\n하이퍼파라미터 튜닝 용이성\n쉬움\n어렵고 복잡함\n\n\n보상과 비용의 관계\nt와 양의 상관 관계\n초기 \\lambda와 학습률에 민감\n\n\n초기 설정 민감도\n낮음\n높음\n\n\n튜닝 방법\n이진 탐색 가능\n초기화와 학습률 설정에 많은 노력 필요\n\n\n\n\n\n\n따라서 실험을 통해 다음과 같은 결론을 내릴 수 있습니다.\n\nIPO는 하이퍼파라미터 t의 튜닝이 쉽고, 보상과 비용이 t 값에 따라 예측 가능하게 변화하기 때문에 안정적인 최적화가 가능합니다.\nPDO는 초기화와 학습률에 민감하여 튜닝이 까다롭고 학습 과정이 불안정할 수 있습니다. 특히 초기 \\lambda와 학습률 설정이 중요한 역할을 합니다."
  },
  {
    "objectID": "posts/paper/2024-11-10-ipo.html#multiple-constraints",
    "href": "posts/paper/2024-11-10-ipo.html#multiple-constraints",
    "title": "📃IPO 리뷰",
    "section": "3.5 Multiple Constraints",
    "text": "3.5 Multiple Constraints\nIPO (Interior Point Optimization)는 제약 조건을 다룰 때 유연하고 확장 가능한 방식으로 설계되어 있습니다. 특히, logarithmic barrier function을 사용하여 제약 조건을 쉽게 추가할 수 있습니다. IPO에서는 새로운 제약 조건이 필요할 때, 기존 최적화 함수에 로그 배리어 항을 추가하기만 하면 됩니다. 이 방식은 CPO보다 간단하게 제약 조건을 추가할 수 있는 이점이 있습니다. IPO는 logarithmic barrier function을 사용하여 제약 조건을 쉽게 추가할 수 있어, 확장성과 유연성 측면에서 CPO보다 유리합니다.\n\nCPO와의 비교\n\nCPO (Constrained Policy Optimization)는 제약 조건을 직접적으로 다루지만, 새로운 제약 조건이 추가될 때마다 문제의 복잡도가 증가하고, 튜닝이 어려워질 수 있습니다.\n반면, IPO는 logarithmic barrier function을 사용하기 때문에, 제약 조건을 쉽게 확장할 수 있으며 구현과 튜닝이 더 간단합니다.\n\nPoint Gather 실험에서의 제약 조건 확장\n\nPoint Gather 환경에서는 에이전트가 보상을 얻는 과정에서 다양한 제약 조건을 추가할 수 있습니다.\n실험에서 다양한 제약 조건을 추가하기 위해, 새로운 타입의 ball (제약 조건에 해당하는 오브젝트)을 도입할 수 있습니다.\n예를 들어, 기존의 bomb 외에 새로운 제약 조건을 나타내는 여러 종류의 ball을 추가하여, 에이전트가 이들을 피하면서도 최대한 많은 보상을 얻는 정책을 학습할 수 있습니다.\n이를 통해 다중 제약 조건 환경에서도 IPO의 성능을 평가할 수 있습니다.\n\n\n\n\n\n\n\n\n\n\n\n\n\n특징\nIPO\nCPO\n\n\n\n\n제약 조건 추가 용이성\n로그 배리어 항 추가만으로 가능\n복잡한 추가 작업과 튜닝 필요\n\n\n확장성\n간단하게 여러 제약 조건 확장 가능\n제약 조건 추가 시 복잡도 증가\n\n\nPoint Gather 실험 적용\n다양한 제약 조건 ball 추가 가능\n제약 조건 추가 시 성능 저하 위험"
  },
  {
    "objectID": "posts/paper/2024-11-10-ipo.html#stochastic-environment-effects",
    "href": "posts/paper/2024-11-10-ipo.html#stochastic-environment-effects",
    "title": "📃IPO 리뷰",
    "section": "3.6 Stochastic Environment Effects",
    "text": "3.6 Stochastic Environment Effects\n실세계 환경에서의 불확실성 및 랜덤 노이즈 추가 실험 실제 환경에서는 항상 불확실성(uncertainty)이 존재합니다. 에이전트의 행동 결과는 종종 랜덤 노이즈(random noise)에 의해 영향을 받습니다. 예를 들어, 바람, 센서 오류, 마찰 등의 예기치 못한 요인들이 시스템에 영향을 줄 수 있습니다. 해당 실험에서 행동(action)은 속도(velocity)와 진행 방향(heading)의 벡터로 정의되며, 값의 범위는 -1에서 1 사이입니다. (-1, 1) 범위의 벡터는 에이전트가 움직일 방향과 속도를 나타냅니다.\n실험에서는 평균 0의 랜덤 노이즈를 행동(action)에 추가하여 환경의 불확실성을 모사했습니다.\n\n노이즈의 분산(variance)은 세 가지 값으로 설정되었습니다:\n\n\\sigma^2 = 0.2\n\\sigma^2 = 0.5\n\\sigma^2 = 1.0\n\n\n\n\n\n\n\\sigma^2 = 0.5일 때도 학습이 성공적으로 수렴하는 것을 확인할 수 있었습니다.\n\n이는 에이전트가 일정 수준의 환경 불확실성에서도 안정적으로 정책을 학습할 수 있음을 보여줍니다.\n\n\\sigma^2 = 1.0의 경우, 노이즈가 커져 학습이 불안정해질 가능성이 있으며, 이는 추가 실험에서 확인할 필요가 있습니다.\n실제 환경의 불확실성을 반영하기 위해 랜덤 노이즈를 추가하는 것은 강화 학습의 강건성(robustness) 평가에 중요한 역할을 합니다.\n적절한 수준의 노이즈(\\sigma^2 = 0.5)에서는 학습이 안정적으로 진행되었으며, 에이전트가 다양한 환경 변동에도 잘 적응할 수 있음을 확인했습니다."
  },
  {
    "objectID": "note.html",
    "href": "note.html",
    "title": "Note",
    "section": "",
    "text": "📘 Diary | 🌎 Language | 📝 Study\n\n\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n    \n  \n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\n\n\n\n\n\nJun 23, 2025\n\n\n📝ROS1/2 Errors\n\n\n\n\n\n\nJun 16, 2025\n\n\n📝Linx Cheet Sheet\n\n\n\n\n\n\nMar 30, 2025\n\n\n📘글또를 마치며\n\n\n\n\n\n\nMar 2, 2025\n\n\n📘글또 발표 후기\n\n\n\n\n\n\nJan 13, 2025\n\n\n📝ROS2 Build Issue\n\n\n\n\n\n\nJan 7, 2025\n\n\n📝Python과 C++ 노드를 모두 포함하는 ROS2 패키지 생성\n\n\n\n\n\n\nJan 5, 2025\n\n\n📘Goodbye 2024\n\n\n\n\n\n\nDec 19, 2024\n\n\n📝__init__ VS. __call__\n\n\n\n\n\n\nOct 9, 2024\n\n\n📘Geultto 10th Start\n\n\n\n\n\n\nJan 15, 2024\n\n\n📝Operating System 001\n\n\n\n\n\n\nDec 1, 2023\n\n\n📘Geultto 9th Start\n\n\n\n\n\n\nAug 31, 2023\n\n\n🌎Casual English Phrases 010\n\n\n\n\n\n\nJul 7, 2023\n\n\n📘Geultto 8th End\n\n\n\n\n\n\nJul 5, 2023\n\n\n🌎Casual English Phrases 009\n\n\n\n\n\n\nMay 27, 2023\n\n\n📘Github Starstruck 128\n\n\n\n\n\n\nApr 5, 2023\n\n\n🌎Casual English Phrases 008\n\n\n\n\n\n\nMar 31, 2023\n\n\n🌎Casual English Phrases 007\n\n\n\n\n\n\nMar 30, 2023\n\n\n🌎Casual English Phrases 006\n\n\n\n\n\n\nMar 29, 2023\n\n\n🌎Casual English Phrases 005\n\n\n\n\n\n\nFeb 2, 2023\n\n\n📘Geultto 8th Start\n\n\n\n\n\n\nOct 29, 2022\n\n\n📘Gueltto 7th End\n\n\n\n\n\n\nOct 24, 2022\n\n\n🌎IT English Experssions 004\n\n\n\n\n\n\nOct 17, 2022\n\n\n🌎Casual English Phrases 003\n\n\n\n\n\n\nOct 11, 2022\n\n\n🌎Casual English Phrases 002\n\n\n\n\n\n\nOct 3, 2022\n\n\n🌎Casual English Phrases 001\n\n\n\n\n\n\nSep 4, 2022\n\n\n📘2022 상반기 회고\n\n\n\n\n\n\nMay 6, 2022\n\n\n📘Geultto 7th Start\n\n\n\n\n\n\nJan 31, 2021\n\n\n📘2021 TU Berlin Winter Course 수강 후기\n\n\n\n\n\n\nJan 3, 2021\n\n\n📘Goodbye 2020\n\n\n\n\n\n\nJan 3, 2021\n\n\n📘Hello 2021\n\n\n\n\n\n\nOct 21, 2020\n\n\n📝On policy VS. Off policy\n\n\n\n\n\n\nNo matching items"
  }
]