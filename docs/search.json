[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Jung Yeon Lee",
    "section": "",
    "text": "My name is Jung Yeon Lee, and I am deeply passionate about robotics and reinforcement learning. I enjoy exploring innovative learning methods, solving complex problems, and contributing to the advancement of technology that fosters closer collaboration between humans and robots.\nMy guiding principle is Stop Wishing, Start Doing, and I am committed to working toward a future where humans and robots coexist harmoniously and dynamically.\n     \n\nMy Interest Fields are\n\nMachine learning | Deep learning | Reinforcement learning\nBioinspired-Robots | Simulations for Robotics\nOn-device AI | Quantum computing\n\n\nSkills and Tools:\n\n                                                     \n\n\n\n\nEducation\n\nM.Eng. 2022.03~2024.02\n\nM.S. in MECHANICAL ENGINEERING, SungKyunKwan University(SKKU)\nResearcher at Robotics Innovatory Lab. Quadrupedal Walking Robot Team\n\n\n\n2021.01.11~01.29\n\nTU Berlin Winter University Online : Machine learning using Pyhon - Theory and Application\n\nGrade : 1.0(the best score)\n5 credit points according to the ECTS(European Credit Transfer System)\n\n\n\n\nB.Eng. 2018.03~2022.02\n\nB.S. in ENGINEERING MECHANICAL & SYSTEM DESIGN ENGINEERING, HongIk University\nIntegrated Major in Design Engineering\nCompleted Accereditation Board for Engineering Education of Korea(ABEEK)\nUndergraduate research student at Autonomous Navigation Lab"
  },
  {
    "objectID": "posts/paper/2025-08-15-anyteleop.html",
    "href": "posts/paper/2025-08-15-anyteleop.html",
    "title": "📃AnyTeleop 리뷰",
    "section": "",
    "text": "Paper Link\nProject Link"
  },
  {
    "objectID": "posts/paper/2025-08-15-anyteleop.html#기술적-기여-및-주요-아이디어-분석",
    "href": "posts/paper/2025-08-15-anyteleop.html#기술적-기여-및-주요-아이디어-분석",
    "title": "📃AnyTeleop 리뷰",
    "section": "2.1 기술적 기여 및 주요 아이디어 분석",
    "text": "2.1 기술적 기여 및 주요 아이디어 분석\n사람의 섬세한 손동작을 로봇 팔-손 시스템으로 원격 조작(teleoperation)하는 것은 로봇에게 인간 수준의 지능적인 물체 조작 능력을 부여하는 유망한 접근법입니다. 특히 비전 기반 원격 조작은 값비싼 장비 없이 카메라만으로 이러한 목표를 달성할 수 있어 큰 주목을 받고 있습니다. 그러나 기존의 비전 기반 원격 조작 시스템들은 대개 특정 로봇 모델이나 정해진 환경만을 염두에 두고 설계되어 왔습니다. 이에 따라 새로운 로봇이나 다양한 작업 환경이 추가되면 시스템을 확장하기 어려운 한계가 있었습니다. 또한 대부분 한 가지 현실(시뮬레이션 또는 실제)에만 국한되거나, 단일 조종자-로봇 상호작용만 지원하여 다중 로봇 협업에는 대응하지 못했습니다. 이러한 한계를 극복하기 위해 이 논문에서는 AnyTeleop이라는 범용 비전 기반 원격 조작 시스템을 제안합니다. AnyTeleop의 핵심 아이디어와 기술적 기여는 다음과 같습니다:\n\n범용성과 통합성: AnyTeleop은 하나의 시스템으로 다양한 로봇 팔 및 다지(多指) 로봇 손 조합을 지원하고, 여러 종류의 현실(여러 시뮬레이터 플랫폼 또는 실제 환경)에서 동작할 수 있으며, 원격지에서도 웹 브라우저를 통한 실시간 시각 피드백으로 조작이 가능하고, RGB 또는 RGB-D 카메라 여러 대를 활용한 유연한 카메라 구성을 허용하며, 나아가 다수의 운영자가 각기 다른 로봇을 동시에 조작하여 협업할 수 있습니다. 즉 하나의 통합 시스템으로 “모든(Any) 테레오퍼레이션” 상황을 아우르는 유연성을 달성했습니다.\n\n\n\n\n\nAnyTeleop 시스템이 다양한 시나리오에서 조작 작업을 수행하는 예시. 상단 행은 IsaacGym 등 가상 시뮬레이터 상에서의 로봇 조작, 중단 행은 또 다른 시뮬레이터(SAPIEN) 상의 조작, 하단 행은 실제 로봇(XArm6+Allegro)이 컵 쌓기 등 일상 물체를 다루는 모습입니다. 이러한 다양한 환경과 작업에서 일관된 원격 조작 프레임워크를 제공하는 것이 AnyTeleop의 목표입니다.\n\n\n모듈화된 아키텍처: AnyTeleop은 카메라로부터 사람 손의 동작을 추적하고 이를 로봇 동작으로 변환하는 일련의 파이프라인을 모듈화하여 설계했습니다. 구체적으로, ① 손 자세 인식 모듈(Hand Pose Detection)은 단일 혹은 복수의 RGB/RGB-D 카메라 입력으로부터 사람 손의 3차원 관절 위치(keypoint)를 추정합니다. 여러 카메라를 사용할 경우 ② 다중 카메라 융합 모듈(Detection Fusion)이 각 카메라에서 얻은 손 추적 결과를 통합하여, 한 카메라에서 손이 가려져도 다른 시점 정보를 활용해 자기 가림(self-occlusion) 문제를 완화합니다. 그리고 ③ 손 자세 리타게팅 모듈(Hand Pose Retargeting)이 사람 손의 관절각을 로봇 손의 관절각으로 변환하는데, 로봇의 URDF(기구학 모델) 정보만 있다면 학습된 모델 없이도 임의의 로봇 손 구조에 대응할 수 있도록 최적화 기반 알고리즘을 사용했습니다. 마지막으로 ④ 모션 생성 모듈(Motion Generation)은 목표로 한 손목(End-effector)의 위치와 자세를 로봇 팔이 부드럽고 충돌 없이 따라가도록 실시간 경로 생성을 수행합니다. 본 논문에서는 NVIDIA의 GPU 가속 모션 계획 라이브러리인 CuRobo를 활용하여 120Hz 이상의 고주파수로 자연스러운 로봇 팔 움직임을 생성하며, 로봇의 자기 충돌이나 관절 한계 위반이 없도록 제어합니다.\n웹 기반 원격 조작: AnyTeleop은 웹 시각화 인터페이스를 제공하여, 원격지에 있는 운영자도 손쉽게 브라우저를 통해 로봇의 3D 환경을 실시간으로 모니터링하고 조작할 수 있게 했습니다. MeshCat/Three.js 기반으로 구현된 뷰어는 시뮬레이터나 실제 로봇으로부터 주기적으로 장면 정보를 받아와 동기화된 영상을 제공합니다. 이로써 공간적으로 떨어진 다수의 사용자들도 동일한 가상 환경을 보며 협동 작업을 수행할 수 있습니다. 또한 이러한 웹 기반 접근은 특정 시뮬레이터에 종속되지 않는 시뮬레이터 불가지론적 시각화를 가능케 하여, IsaacGym이든 SAPIEN이든 동일한 방식으로 원격 조작을 지원합니다.\n\n요약하면, AnyTeleop은 모든 구성요소를 표준 인터페이스로 분리하여 필요에 따라 손쉽게 교체하거나 확장할 수 있고, 소프트웨어적으로 도커(container)로 배포되어 복잡한 의존성 설정 없이 손쉽게 설치 및 실행할 수 있도록 구현되었습니다. 이러한 범용적이면서도 성능을 놓치지 않은 설계 철학 덕분에, AnyTeleop은 다양한 조건에서 안정적으로 동작하면서도 인간 조종자의 섬세한 동작을 로봇에 충실히 재현해냅니다."
  },
  {
    "objectID": "posts/paper/2025-08-15-anyteleop.html#기존-연구와의-차별점",
    "href": "posts/paper/2025-08-15-anyteleop.html#기존-연구와의-차별점",
    "title": "📃AnyTeleop 리뷰",
    "section": "2.2 기존 연구와의 차별점",
    "text": "2.2 기존 연구와의 차별점\n과거의 로봇 원격 조작 연구들은 주로 특수 장비에 의존하거나 한정된 기능만을 제공했습니다. 예를 들어 초기의 섬세한 손 조작 원격 제어는 데이터 장갑, 모션 캡처 수트, VR 컨트롤러, 햅틱 장치 등의 고가 하드웨어를 착용한 상태에서 수행되는 경우가 많았습니다. 반면 AnyTeleop은 카메라 하나만으로 사람 손의 움직임을 인식함으로써 장비 비용과 사용자 부담을 크게 낮추었습니다. 최근 등장한 비전 기반 원격 조작 시스템들도 있었지만, 이들 역시 각자 특정 로봇에 맞게 튜닝되어 범용성이 떨어지는 단점이 있었습니다. 예컨대 DexPilot (2020)은 KUKA LBR 팔 + Allegro Hand 조합을 비전으로 조작했지만 특정 하드웨어에 맞춘 보정(calibration)과 심도 카메라 등을 필요로 했고, Robotic Telekinesis (2023)와 같은 시스템은 XArm6 + Allegro에 특화된 학습된 리타게팅 모델과 충돌회피 모델을 사용하여 다른 로봇에는 적용이 어려웠습니다. 또한 Holo-Dex (2022)나 DIME (2022) 등의 연구는 AR/VR 장치를 통해 몰입감 있게 조작하거나, 학습 기반으로 효율적인 모방을 꾀했지만, 로봇 손만 제어하고 로봇 팔은 고려하지 않는 등 시스템 범위가 제한적이었습니다. 이처럼 기존 연구들은 각각 일부 기능에 초점을 맞추었을 뿐, 여러 로봇과 환경을 통합적으로 지원하거나 협업 시나리오까지 포괄하지는 못했습니다.\n이에 비해 AnyTeleop의 가장 큰 차별점은 범용적 모듈성과 확장성입니다. 사람 손 추적부터 로봇 제어까지의 전 과정을 모듈화하고 표준화함으로써, 새로운 로봇 팔-손 시스템이나 새로운 환경에도 즉각 대응할 수 있습니다. 예를 들어, 다른 형태의 로봇 손을 사용하려면 해당 URDF 기구학 모델만 제공하면 되고, 별도의 신경망 재학습 없이 곧바로 리타게팅과 제어가 가능합니다. 충돌 회피 역시 특정 로봇에 특화된 모델 대신 CUDA 기반 기하 계산으로 처리하여, 로봇 모양만 주어지면 즉석에서 충돌을 검사하고 회피경로를 산출합니다. 이러한 학습 비의존적 접근 덕분에 새로운 로봇 추가나 환경 변경 시 발생하는 데이터 수집 및 모델 재교육 비용이 거의 들지 않으며, 시스템이 자동적으로 일반화될 수 있습니다. 반면 기존 시스템들은 새로운 로봇마다 모션 변환기를 다시 학습시키거나 환경별로 손동작 데이터셋을 다시 수집해야 했기에 확장성이 낮았습니다.\n또 다른 차별화 지점은 협업 및 멀티-로봇 지원입니다. 이전의 비전 기반 원격 조작 연구들은 대부분 한 명의 인간 조작자가 한 대의 로봇만 다루는 시나리오에 머물렀습니다. AnyTeleop은 처음으로 두 명 이상의 사람이 각자의 로봇을 동시에 조작하여 공동의 작업을 수행하는 비전 기반 시스템을 선보였습니다. 예컨대 한 운영자가 물체를 집어 다른 로봇에게 건네주고, 다른 운영자가 그 로봇을 통해 이를 받아 최종 목표 지점에 두는 식의 다중 로봇 협동도 구현됩니다 (뒤의 실험 결과 참조). 이러한 기능은 이전 연구에서는 보고된 바 없어 AnyTeleop의 독보적인 기능적 차별성이라 할 수 있습니다.\n종합하면, AnyTeleop은 기존의 원격 조작 연구들이 개별적으로 추구하던 목표들(예: 저비용 비전 기반 추적, 범용 로봇 지원, 시뮬레이션 활용, 협업 등)을 단일 시스템에 통합함으로써 실현했다는 점에서 의미가 큽니다. 이를 통해 성능 저하 없이도 더 넓은 적용 범위를 커버하며, 실제로 실험을 통해 이전 특수목적 시스템들보다 더 나은 성능을 입증하였습니다. 즉 AnyTeleop은 범용성 vs. 성능의 트레이드오프를 극복한 사례로 평가됩니다."
  },
  {
    "objectID": "posts/paper/2025-08-15-anyteleop.html#실험-설정-및-결과-평가",
    "href": "posts/paper/2025-08-15-anyteleop.html#실험-설정-및-결과-평가",
    "title": "📃AnyTeleop 리뷰",
    "section": "2.3 실험 설정 및 결과 평가",
    "text": "2.3 실험 설정 및 결과 평가\n논문 저자들은 AnyTeleop의 성능을 정량적 실험을 통해 검증하기 위해, 실제 로봇 실험과 가상 시뮬레이션 실험을 모두 수행했습니다.\n\n실제 로봇 실험:\n\n기존 Robotic Telekinesis 시스템과의 성능 비교를 위해, 동일한 로봇 하드웨어인 HxArm6 로봇 팔과 Allegro 로봇 손을 사용했습니다. 실험에 사용된 작업(task)들은 Telekinesis 논문에서 제시된 10가지 조작 작업으로, 작은 상자를 집어 옮기기, 천으로 만든 장난감 집기, 상자 회전시키기, 가위 잡기, 컵 포개 쌓기, 이중 컵 쌓기, 상자에 든 큐브를 접시로 붓기, 컵을 접시에 옮겨 담기, 서랍 열기, 서랍 열고 물건 집어내기 등이 포함됩니다. 각 작업에 대해 숙련된 조작자가 AnyTeleop을 통해 10번씩 시도를 하였고, 단일 Intel RealSense RGB-D 카메라로 손동작을 추적했습니다. Telekinesis (기존 시스템)의 경우 해당 논문에 보고된 성공률을 그대로 비교 대상으로 사용했습니다. 성공률 측정은 10회 시도 중 작업 목표를 완수한 비율로 정의되었습니다.\n결과:\n비교 결과 AnyTeleop은 10개 중 8개 작업에서 더 높은 성공률을 보였고, 나머지 2개 작업에서는 대등한 성능(동일 성공률)을 나타냈습니다. 구체적으로, 예를 들어 두 개의 컵 쌓기 작업의 경우 기존 시스템 성공률이 30%에 그쳤던 반면 AnyTeleop은 70%의 성공률을 달성했고, 컵을 접시에 옮기기 작업도 기존 80%에서 AnyTeleop은 100% 성공으로 향상되었습니다. 대부분의 작업에서 AnyTeleop이 향상된 성과를 보였으며, 상자 회전이나 큐브 붓기 같은 작업에서는 두 시스템 모두 유사한 성공률을 보였습니다. 흥미롭게도 저자들은 AnyTeleop의 우수한 성능 요인으로, 얇은 벽 구조의 물체(컵 등)를 다루는 작업에서의 강점을 꼽았습니다. AnyTeleop의 최적화 기반 손 리타게팅 모듈은 사람 손가락 사이 거리를 로봇 손가락도 최대한 좁혀서 섬세하게 재현하기 때문에, 얇은 컵을 단단히 쥐는 동작을 성공적으로 모사했습니다. 반면 Telekinesis의 신경망 기반 리타게팅은 이러한 미세한 그립(grip) 동작을 정확히 전달하지 못해 컵을 놓치는 경우가 있었다고 분석합니다. 요컨대 AnyTeleop은 더 범용적인 시스템임에도 불구하고 특정 하드웨어에 맞춰진 기존 시스템보다 오히려 높은 조작 성공률을 보여주었고, 특히 정밀한 손가락 제어가 필요한 작업에서 두드러진 강점을 입증했습니다.\n시뮬레이션 및 모방학습 실험:\nAnyTeleop의 범용성은 가상 환경에서도 시험되었습니다. 저자들은 로봇 학습에서 중요한 인간 시연 데이터 수집 용도를 평가하기 위해, 시뮬레이터 상에서 원격 조작으로 모방 학습용 데이터를 모으고 그 품질을 비교했습니다. 비교 대상으로는 최근 발표된 비전 기반 원격 조작 시스템인 Qin 등 (2022)을 선정했는데, 이 시스템은 단일 RGB-D 카메라로 부유하는 손(floating hand)만 조작할 수 있는 제한이 있었습니다. 실험에 사용된 조작 작업들은 해당 기존 연구와 동일하게 3가지로 설정되었으며, (i) 탁자 위 물체를 집어 다른 위치로 옮기는 위치 이동(Relocate), (ii) 뒤집혀 놓인 머그잔을 90도 회전시켜 올려놓는 머그 뒤집기(Flip Mug), (iii) 문 손잡이를 돌려 잠금을 풀고 문을 당겨 여는 문 열기(Open Door) 입니다. 각 작업은 부유 손(floating hand) 버전과 팔이 달린 손(arm-hand) 버전 두 가지 변형으로 수행되었습니다. 부유 손은 로봇 팔이 없는 자유롭게 움직이는 손만으로 조작하는 경우이고, 팔-손 버전은 로봇 팔 끝에 손이 달려있어 고정된 기반(base)을 갖는 현실적인 경우입니다.\n데이터 수집 및 학습:\n우선, 기존 시스템과 AnyTeleop을 이용하여 각 작업당 50개 시연(trajactory) 데이터를 수집했습니다. 기존 시스템은 부유 손 형태만 직접 조작 가능했기에, 팔-손 버전 시연을 얻기 위해 저자들이 별도의 시연 변환 과정을 적용했습니다. 반면 AnyTeleop은 처음부터 팔-손 조작을 지원하므로, 팔-손 형태로 50개 시연을 수집한 뒤 이를 부유 손 형태로 변환하여 두 방식 모두에 활용했습니다. 이렇게 얻은 시연 데이터로 Demo Augmented Policy Gradient (DAPG) 알고리즘을 이용해 모방 학습 정책을 훈련하였고, 비교를 위해 순수 강화학습(RL)으로만 훈련한 정책도 함께 평가했습니다. 각 방법에 대해 서로 동일한 신경망 구조와 보상 설계를 사용하여 공정성을 유지했습니다. 최종 정책들은 3개의 시드로 학습되어, 각각 100회 에피소드 평가를 통해 성공률을 산출했습니다.\n결과:\nTable VI에 보고된 결과에 따르면, AnyTeleop을 통해 수집한 시연으로 학습한 정책이 6개의 평가 시나리오 중 5개에서 기존 시스템 시연으로 학습한 정책보다 높은 성공률을 기록했습니다. 또한 이 정책들은 사람 시연 없이 강화학습만으로 훈련한 정책보다도 월등히 높은 성능을 보였습니다. 특히 팔-손 조작이 포함된 과제들에서 AnyTeleop 기반 정책의 성능 향상이 두드러졌는데, 그 이유는 다음과 같습니다. (i) AnyTeleop으로 얻은 시연 궤적은 매우 부드럽고 자연스러운 연결 동작으로 이루어져 있어, 상태-행동 쌍이 일관되고 학습하기 용이했습니다. 반면 기존 시스템의 시연 데이터는 손동작이 불연속적으로 추정되거나 팔 움직임 생성 시 다소 튐(jitter)이 있어 학습 난이도가 높았습니다. (ii) AnyTeleop은 팔-손 시스템을 직접 조작하면서 자기 충돌이 발생하지 않도록 제약을 거는 등 물리적으로 유효한 시연만을 수집합니다. 하지만 기존 시스템은 팔이 없는 손 시연을 사후에 로봇 팔에 재현(retargeting)하는 방식을 쓰다 보니, 그 과정에서 로봇 팔이 자기 자신을 때리는 등의 충돌이 포함된 시연이 여럿 발생했습니다. 이러한 차이로 인해 팔-손 조작 과제들에서 AnyTeleop 시연으로 학습한 정책이 훨씬 높은 성공률을 얻은 것입니다. 한 가지 예외적으로, 팔을 이용한 머그 뒤집기 과제에서는 두 시스템의 학습 정책 성능이 비슷했는데, 저자들은 팔이 있는 상태로 머그를 뒤집는 시연 자체가 매우 어렵고 AnyTeleop으로도 해당 데모의 품질이 떨어졌기 때문이라고 분석했습니다.\n종합하면, AnyTeleop을 통한 원격 조작 데모가 기존 시스템보다 질적으로 우수하며, 이는 다운스트림 로봇 학습 성능 향상으로 직접 이어짐을 보였습니다. 이로써 AnyTeleop의 범용성이 단순히 여러 로봇을 조작할 수 있다는 것뿐만 아니라, 실제 활용 가치 측면에서도 더 나은 데이터를 제공한다는 점이 입증되었습니다.\n\n협동 조작 시연: 앞서 언급한 다중 운영자 협업 시나리오의 가능성도 실험적으로 보여주었습니다. 저자들은 예시로 한 명의 사람 조종자가 로봇 팔 A를 이용해 물체를 집어 다른 로봇 팔 B에게 건네주고, 두 번째 조종자가 그 로봇 B를 제어해 물체를 받는 “사람→로봇 핸드오버” 과제를 수행했습니다. 구체적으로, 운영자 1은 UR10 팔 + Schunk 5지 로봇 손을 AnyTeleop으로 조작하여 테이블 위 물체를 집어 들고, 운영자 2는 KUKA 팔 + Shadow 로봇 손을 조작하여 그 물체를 건네받았습니다. 이 협업 시나리오는 두 운영자가 같은 가상 환경을 실시간으로 바라보면서 동작을 조율해야 했는데, AnyTeleop의 웹 기반 동기화 뷰어를 통해 두 사용자 화면에 항상 동일한 장면이 표시되고, 각자의 손동작이 중앙 시뮬레이션 서버에서 통합되어 두 로봇의 움직임으로 반영됨으로써 가능했습니다. 그림 4는 이 핸드오버 작업의 한 장면을 보여주는데, 왼쪽의 흰색 UR10-슁크 로봇이 물체를 건네고 오른쪽 주황색 KUKA-섀도우 로봇이 그것을 받는 모습입니다.\n\n\n\n\n\n\n\n\n두 운영자가 원격으로 각기 다른 로봇 팔-손을 조작하여 물체를 주고받는 협동 조작 예시. AnyTeleop의 중앙 서버-다중 유닛 구조를 활용하면 물리적으로 떨어진 장소에 있는 사용자들도 마치 한 공간에서 두 로봇을 함께 조작하는 것처럼 협응(coordinate)할 수 있습니다. 해당 데모는 정량적 수치로 평가되지는 않았지만, AnyTeleop이 협업 데이터 수집에도 유용하게 쓰일 수 있음을 보여줍니다. 저자들은 사람이 둘 이상 필요한 로봇 작업(예: 무거운 물체 공동 운반, 조립 작업 등)의 시연 데이터를 얻는 것이 얼마나 어려운지 언급하며, AnyTeleop이 이러한 복잡한 시나리오의 데이터 수집을 손쉽게 해주는 도구가 될 수 있다고 강조합니다."
  },
  {
    "objectID": "posts/paper/2025-08-15-anyteleop.html#한계점-및-향후-연구-방향",
    "href": "posts/paper/2025-08-15-anyteleop.html#한계점-및-향후-연구-방향",
    "title": "📃AnyTeleop 리뷰",
    "section": "2.4 한계점 및 향후 연구 방향",
    "text": "2.4 한계점 및 향후 연구 방향\nAnyTeleop은 현재까지의 원격 조작 시스템 중 가장 범용적이고 성능도 뛰어났지만, 여전히 몇 가지 한계점과 도전 과제가 존재합니다.\n\n비전 기반 손 추적의 한계: AnyTeleop은 상업용 카메라와 딥러닝 기반 손 인식에 의존하기 때문에, 사람 손이 너무 빠르게 움직일 경우 추적이 따라가지 못해 손실되는 문제가 있습니다. 실제 실험 중에도 조종자가 손을 급격히 휘두르면 시스템이 일시 정지한 뒤 손 위치를 재인식(re-detection)해야 하는 상황이 발생했습니다. 또한 단일 카메라로 손을 볼 때 손의 일부가 자기 몸으로 가려지면 인식된 손 자세의 정확도가 떨어지는 현상이 있습니다. 이러한 문제를 완화하기 위해 현재는 “천천히 움직이라”고 조종자에게 안내하거나, 여러 대의 카메라를 배치하여 다양한 각도에서 손을 관찰하는 방식을 사용합니다. 그러나 이는 근본적인 해결책이 아니므로, 향후 연구로 더 향상된 손 추적 비전 알고리즘을 도입하거나 딥러닝 모델을 개선하여 속도와 정확도를 높이는 방향이 필요합니다. 예컨대, 빠른 동작 예측을 위한 추론 프레임 보간이나, 자기 가림 상황을 극복할 수 있는 3D 모델 기반 추적 기법 등을 적용해볼 수 있을 것입니다.\n실시간 시스템 성능: AnyTeleop이 원활한 조작을 위해 요구하는 일부 기술적 사양도 한계로 지적됩니다. 논문에 따르면 손 추적 모듈은 GPU 상에서 25Hz 정도로 동작하고, 로봇 팔 모션 생성은 100Hz 이상으로 구동되어야 가장 자연스러운데, 모든 모듈을 단일 PC에서 동시에 실행하면 성능이 저하될 수 있다고 합니다. 이를 해결하기 위해 손 추적/제어 모듈과 시뮬레이터를 별도의 컴퓨터로 분리하여 운용하면 성능을 유지할 수 있었지만, 일반 사용자 입장에서는 여러 대의 고성능 장비를 요구하는 셈이어서 부담일 수 있습니다. 따라서 향후에는 더 경량화된 추적 모델이나 최적화된 병렬 처리를 통해 단일 시스템에서도 충분한 주파수로 동작할 수 있도록 개선하는 연구가 기대됩니다.\n감각 피드백의 부재: 현재 AnyTeleop 조종자는 시각 정보에만 의존하여 원격 환경을 느낄 수 있습니다. 반면 기존의 장갑 기반 시스템들은 일부 촉각/힘 피드백을 제공하여 사용자가 물체를 쥐는 힘이나 질감을 느낄 수 있게 했습니다. AnyTeleop은 이러한 햅틱 장비를 배제함으로써 간편성과 범용성을 얻었지만, 동시에 조종자가 촉각적 단서 없이 화면만 보고 조작해야 한다는 제약이 있습니다. 이는 매우 섬세한 힘 조절이 필요한 작업(예: 계란을 잡기 등)에서 어려움을 초래할 수 있습니다. 향후 연구로 증강현실(AR)이나 햅틱 피드백 장치를 선택적으로 결합하여, 필요한 경우 조종자에게 추가적인 감각 피드백을 제공하는 방향을 모색해볼 수 있습니다. 예를 들어 AR 헤드셋을 사용해 원격 장면을 1인칭으로 보여주거나, 로봇 손끝에 힘 센서를 달아 조종자 손에 진동 피드백을 주는 방안 등이考해볼 수 있습니다 (물론 이는 시스템 복잡도를 높여 AnyTeleop의 장점인 저비용성을 희석시킬 수 있으므로 균형이 필요합니다).\n복잡한 작업 및 다중 로봇 시나리오: AnyTeleop으로 시연된 과제들은 주로 단일 로봇 팔-손이 하나의 물체를 다루는 작업 또는 두 로봇 간의 간단한 물체 전달 작업이었습니다. 향후에는 보다 복잡한 협업 시나리오 (예: 두 로봇이 동시에 서로 다른 도구를 사용해 조립 작업을 한다거나, 다수의 로봇과 인간이 섞인 팀 협업)으로 확장하는 도전이 있습니다. 이를 위해서는 여러 조종자의 동작 타이밍을 자동으로 조율해주는 공동 제어 알고리즘, 물체를 주고받는 이상의 정교한 상호작용 프로토콜 등이 필요할 것입니다. 또한 현 단계에서는 한 명의 조종자가 하나의 로봇만 제어하지만, 장차 한 사람이 다수의 로봇을 제어하거나 한 로봇을 여러 명이 제어하는 특별한 경우(예: 거대한 로봇을 두 사람이 각각 다른 부분을 조작)도 고려해볼 수 있습니다. 이러한 확장은 AnyTeleop의 모듈식 구조를 한층 발전시켜, 고차원의 공동 제어 문제를 다루는 연구로 이어질 수 있습니다.\n\n요약하면, AnyTeleop은 훌륭한 성능과 범용성을 달성했지만 비전 센싱의 한계, 시스템 성능 요구사항, 피드백 부족, 복잡한 확장 시나리오 등의 면에서 향후 개선의 여지가 있습니다. 다행히 저자들은 이 시스템을 오픈 소스로 공개하여 연구자들이 자유롭게 개선·활용하도록 할 계획이며, 이를 통해 다양한 후속 연구가 활발히 전개될 것으로 기대됩니다."
  },
  {
    "objectID": "posts/paper/2025-08-15-anyteleop.html#종합-정리",
    "href": "posts/paper/2025-08-15-anyteleop.html#종합-정리",
    "title": "📃AnyTeleop 리뷰",
    "section": "2.5 종합 정리",
    "text": "2.5 종합 정리\n“AnyTeleop: A General Vision-Based Dexterous Robot Arm-Hand Teleoperation System” 논문은 로봇 원격 조작 분야에서 중요한 이정표를 세운 연구로 평가할 수 있습니다. 이 연구는 하나의 통합된 시스템으로 다양한 로봇과 환경을 지원하면서도, 개별 특화 시스템에 비견하거나 뛰어넘는 성능을 입증하였습니다. 범용성과 성능을 동시에 잡았다는 점에서 학술적 독창성이 돋보이며, 특히 다중 로봇 협업까지 아우르는 비전 기반 테레오퍼레이션은 처음 시도되었다는 점에서 신규성이 있습니다.\nAnyTeleop이 미칠 실용적 영향도 상당합니다. 우선 로봇 학습을 위한 데몬스트레이션 수집을 규모 확장하는 데에 큰 기여를 할 것으로 보입니다. 과거에는 많은 로봇 시연 데이터를 모으기 위해 복잡한 장비와 인력이 필요했지만, AnyTeleop을 활용하면 저비용 카메라와 인터넷만으로도 어디서든 사람이 로봇에게 시연을 가르칠 수 있습니다. 이는 곧 대규모 모방 학습 데이터셋 구축이나, 원격 크라우드소싱을 통한 로봇 학습으로 이어질 수 있습니다. 또한 시뮬레이터와 실제를 한 시스템에서 모두 지원하므로, 시뮬레이션-현실 격차(sim-to-real gap)를 줄이고 보다 빠르게 현실 로봇에 학습 결과를 적용하는 사이클을 구축할 수 있습니다.\n산업 및 사회적으로도 응용 가능성이 무궁무진합니다. 예를 들어, 위험한 환경에서 인간을 대신해 작업해야 하는 로봇을 AnyTeleop으로 조종한다면, 작업자는 안전한 장소에서 카메라 원격장비로 로봇을 제어할 수 있습니다. 이는 원자력 발전소 점검, 화학 물질 취급, 화재 진압, 재난 현장 수색 등의 분야에서 원격 존재(telepresence) 로봇의 활용을 용이하게 할 것입니다. 또한 의료/복지 분야에서도 숙련된 전문가가 멀리 떨어진 곳의 로봇 팔을 원격 조종해 수술을 보조하거나, 장애인이 집에 있는 로봇 손을 원격 조작해 일상 작업을 수행하는 등 원격 협력 시나리오가 실현될 수 있습니다. 두 명 이상의 사람이 협동하여 거리를 초월해 로봇들을 움직이는 기술은 향후 원격 제조나 원격 협업 연구실 등에도 적용되어, 공간의 제약 없이 인력이 협업하는 새로운 형태의 작업 환경을 만들어낼 수 있습니다.\n마지막으로, AnyTeleop은 오픈 소스로 공개됨으로써 연구 커뮤니티와 산업계 모두에 플랫폼을 제공할 것입니다. 이제 개별 로봇 회사나 연구실이 각자 원격 제어 시스템을 처음부터 만들 필요 없이, AnyTeleop을 기반으로 자신들의 로봇에 맞춰 확장하고 개선하는 식으로 발전이 가속화될 것입니다. 이러한 공유 인프라의 등장은 로봇공학 분야에서 표준을 정립하고 중복 노력을 줄이는 효과도 기대됩니다.\n결론적으로, AnyTeleop은 비전 인식, 최적화 제어, 웹기술을 융합하여 로봇 원격 조작의 새로운 지평을 연 시스템입니다. 사람의 능숙한 손동작을 전례 없이 다양한 로봇에 이식함으로써, 인간과 로봇의 거리를 한층 좁혔습니다. 향후 이 연구를 바탕으로 보다 향상된 추적 기술, 사용자 피드백 통합, 그리고 인간-로봇 협업의 복잡한 문제들에 대한 추가 연구가 이어진다면, 언제 어디서나 누구나 로봇을 조종할 수 있는 시대가 성큼 다가올 것으로 기대됩니다. AnyTeleop이 구축한 탄탄한 토대 위에서, 로봇 원격 조작의 스케일 업(scale-up)과 실용화가 한층 가속화될 것입니다."
  },
  {
    "objectID": "posts/paper/2025-08-06-seq-multi-obj.html",
    "href": "posts/paper/2025-08-06-seq-multi-obj.html",
    "title": "📃SeqMultiGrasp 리뷰",
    "section": "",
    "text": "Paper Link\nProject Link\n\n\n🤖본 논문은 Allegro Hand를 사용하여 여러 객체를 한 손으로 순차적으로 파지하는 로봇 시스템인 SeqMultiGrasp을 제안합니다.\n✋이 시스템은 먼저 손의 특정 링크에 제약된 단일 객체 파지 후보를 합성하고 시뮬레이션에서 검증한 후, 이를 병합하여 다중 객체 파지 구성을 생성합니다.\n✅실제 환경 배포를 위해 Point Cloud 기반의 Diffusion Model이 파지 자세를 제안하고 휴리스틱 기반의 실행 전략을 통해 시뮬레이션에서 65.8%, 실제 환경에서 56.7%의 평균 성공률을 달성했습니다.\n\n\n\n\n\n\n1 Brief Review\n본 논문은 하나의 민첩한 손으로 여러 객체를 순차적으로 파지하는 문제를 다루며, 이를 위한 시스템인 SeqMultiGrasp를 제안합니다. 인간은 손의 뛰어난 민첩성을 활용하여 여러 객체를 동시에 또는 순차적으로 파지할 수 있지만, 로봇에게 이는 객체의 다양한 형상과 높은 자유도(high-DOF) 손의 복잡한 접촉 상호작용으로 인해 어려운 도전 과제입니다. 특히 하나의 객체를 파지한 상태에서 다른 객체를 파지해야 하는 순차적 시나리오에서 난이도는 더욱 증가합니다.\nSeqMultiGrasp는 네 손가락을 가진 Allegro Hand를 사용하여 두 개의 객체를 순차적으로 파지하는 데 초점을 맞춥니다. 이 시스템은 첫 번째 객체를 완전히 감싸 들어 올린 후, 첫 번째 객체를 떨어뜨리지 않으면서 두 번째 객체를 파지하는 것을 목표로 합니다.\n핵심 방법론은 다음과 같은 단계로 구성됩니다.\n\n단일 객체 그랩 후보 합성:\n\n우선, Differentiable Force Closure (DFC) [13] 알고리즘을 기반으로 단일 객체 그랩 포즈를 합성합니다. 이 과정은 파지 문제를 에너지 함수의 최적화로 정식화하여 해결합니다.\n손 구성 H = (\\theta , T)는 로봇 손의 관절 구성 \\theta \\in \\mathbb{R}^d와 객체 O에 대한 상대 포즈 T \\in SE(3)를 나타냅니다.\n에너지 함수는 다음과 같습니다: E = E_{fc} + w_{dis}E_{dis} + w_pE_p + w_{sp}E_{sp} + w_qE_q 여기서 E_{fc}는 force closure 항, E_{dis}는 접촉점과 객체 표면 간의 거리에 대한 페널티, E_p는 손, 객체, 탁자 간의 침투(penetration)에 대한 페널티, E_{sp}는 손의 자기 침투(self-penetration)에 대한 페널티, E_q는 관절 한계 위반에 대한 페널티를 나타냅니다. w 항들은 각 구성 요소의 가중치 계수입니다.\n합성 과정에서는 손 표면의 접촉 후보점에서 접촉점을 샘플링하고 초기 구성을 설정합니다. 이후 Metropolis-Adjusted Langevin Algorithm (MALA)과 결합된 경사 기반 접근 방식을 사용하여 최적화합니다. 특정 임계값을 초과하는 에너지를 가진 구성은 필터링됩니다.\n순차적 다중 객체 파지를 위해, 첫 번째 객체는 엄지, 검지, 중지를 사용하는 pinch-like grasp, 두 번째 객체는 약지와 손바닥을 사용하는 side grasp에 접촉 후보점을 제한하는 등 기존 DFC 파이프라인에 여러 수정 사항이 적용되었습니다.\n\n물리 시뮬레이션 기반 그랩 유효성 검증:\n\n합성된 그랩 후보들은 GPU 가속 물리 시뮬레이터인 ManiSkill [39]에서 그랩을 실행하여 안정성과 실행 가능성을 검증합니다.\nRotation Robustness: 객체가 6가지 축 정렬 중력 방향(±x, ±y, ±z) 하에서 2.5초 시뮬레이션 후에도 손과 접촉을 유지하는지 평가합니다.\nExecution Feasibility: 그랩이 환경과의 충돌 없이 성공적으로 실행될 수 있는지 확인합니다.\n\n다중 객체 그랩 구성 병합:\n\n검증된 단일 객체 그랩 포즈들을 병합하여 다중 객체 그랩 구성을 생성합니다. 이 과정은 관련 손 링크와 관절이 완전히 분리되어 있을 때만 가능합니다.\n병합 시, 각 손가락의 관절 각도는 해당 손가락이 잡는 객체의 접촉점에 따라 설정됩니다. 어떤 객체도 잡지 않는 손가락의 관절 각도는 단일 객체 그랩 중 하나에서 무작위로 상속받아 비겹침 제어 제약 조건을 유지합니다.\n\nDiffusion-based 포즈 생성:\n\n그랩 포즈 생성의 계산 비용을 줄이기 위해, 객체의 point cloud P = \\{P_j\\}_{j=1}^{N_o}에 조건화된 diffusion model [40]을 훈련하여 손 포즈를 제안합니다.\nForward Process (노이즈 추가): q(H_t |H_{t-1}) = \\mathcal{N} \\left( H_t ; \\sqrt{1 - \\beta_t} H_{t-1}, \\beta_t \\mathbf{I} \\right) 여기서 \\beta_t는 노이즈 레벨을 제어하고 \\mathbf{I}는 항등 행렬입니다.\nReverse Process (노이즈 제거 및 재구성): p_\\phi (H_{t-1}|H_t , P) = \\mathcal{N} \\left( H_{t-1}; \\mu_\\phi (H_t ,t, P), \\Sigma_\\phi (H_t ,t, P) \\right) 여기서 \\mu_\\phi와 \\Sigma_\\phi는 각각 예측된 평균과 공분산입니다.\n네트워크는 PointNet++ [43]를 사용하여 point cloud 특징을 추출하고, 회전 행렬로 객체 방향을 표현하며, singular value decomposition (SVD) [44]를 적용하여 직교성을 보장합니다.\n\n휴리스틱 기반 실행 전략:\n\n복잡한 reinforcement learning (RL) 정책 대신, simple squeeze-and-lift 절차를 채택합니다.\nCuRobo [45]를 사용하여 엔드 이펙터를 그랩 포즈에서 오프셋된 충돌 없는 포즈로 모션 플래닝합니다.\n이후 충돌 검사 없이 그랩 포즈로 느리게 이동하며, 손 관절 위치를 두 단계로 조정합니다. 첫 번째는 pre-grasp joint position으로 손가락 끝을 후퇴시키고, 두 번째는 target joint position으로 손가락을 닫습니다.\n\n\n시뮬레이션 및 실제 환경에서 광범위한 실험이 수행되었습니다. 시뮬레이션에서는 8x8 객체 조합에 대해 Synthesized Grasp (SG) 방식이 평균 82.7%의 성공률을 보였으며, diffusion model 기반 Learned Grasp (LG) 방식은 65.8%의 성공률을 달성했습니다. 실제 로봇 시스템을 사용한 실험에서는 6x3 객체 조합에 대해 SG가 64.4%, LG가 56.7%의 평균 성공률을 기록했습니다. 실제 환경 point cloud 획득을 위해 Nerfstudio [50], COLMAP [51], Stable Normal [52], 2D Gaussian Splatting [53] 등의 기술이 활용되어 sim-to-real gap을 줄였습니다.\nSeqMultiGrasp는 여전히 두 개의 객체만 다루며 데이터셋 크기와 다양성, 그리고 휴리스틱에 대한 의존성 등 몇 가지 한계를 가지고 있지만, 다재다능한 다중 객체 파지 분야의 미래 연구를 위한 유망한 기반을 제공합니다.\n\n\n\n2 Detail Review"
  },
  {
    "objectID": "posts/paper/2025-07-25-catching.html",
    "href": "posts/paper/2025-07-25-catching.html",
    "title": "📃Catching Objects in Flight 리뷰",
    "section": "",
    "text": "Paper Link\nYoutube Link"
  },
  {
    "objectID": "posts/paper/2025-07-25-catching.html#서론-비행-물체-잡기의-어려움과-연구-목표",
    "href": "posts/paper/2025-07-25-catching.html#서론-비행-물체-잡기의-어려움과-연구-목표",
    "title": "📃Catching Objects in Flight 리뷰",
    "section": "2.1 서론: 비행 물체 잡기의 어려움과 연구 목표",
    "text": "2.1 서론: 비행 물체 잡기의 어려움과 연구 목표\n고속으로 비행하는 물체를 로봇이 공중에서 잡아내는 것은 로봇공학에서 매우 도전적인 과제입니다. 이 작업을 위해서는 (1) 빠르게 움직이는 물체의 궤적을 정확히 예측하고, (2) 물체를 잡기 위한 최적의 로봇 팔-손 자세(잡기 구성)를 찾아내며, (3) 제한된 시간 내에 로봇 팔의 궤적을 계획하여 목표 지점에 도달하도록 제어하는 것, 이 세 가지 문제를 모두 풀어야 합니다. 이러한 모든 처리를 밀리초(ms) 단위의 시간 안에 수행해야 한다는 점에서 기술적 난도가 매우 높습니다. 특히 공중에서 잡으려는 대상이 망치, 라켓, 병과 같이 무게 중심이 한쪽으로 치우치거나 비대칭인 물체인 경우, 단순한 포물선 모델로는 운동을 설명하기 어렵고 물체의 자세(orientation)까지 고려해야 하므로 문제가 더욱 복잡해집니다.\n이 논문의 저자들은 이러한 문제를 해결하기 위해 프로그래밍-바이-데몬스트레이션(programming by demonstration) 접근을 활용하였습니다. 즉, 사람이나 로봇이 물체를 던지고 잡는 시범 데이터로부터 학습함으로써, 물체의 동특성과 로봇 팔의 움직임을 모델링하였습니다. 이를 통해 물체 비행 궤적의 학습 기반 예측 모델을 얻고, 확률적인 방법으로 로봇의 잡기 자세를 결정하며, 동적 시스템(Dynamical System, DS) 기반의 제어기로 로봇 팔을 빠르고 유연하게 움직이도록 하였습니다. 이러한 통합 프레임워크를 통해 센서 노이즈나 예기치 않은 변화에도 실시간으로 대응하면서 로봇 팔이 공중의 물체를 잡을 수 있도록 하였습니다. 본 리뷰에서는 해당 논문의 기술적 기여를 중심으로, 제안된 방법론(알고리즘, 실험 설정, 하드웨어/소프트웨어 구성, 데이터 흐름 등)을 상세히 분석하고, 기존 연구들과의 차별점을 함께 논의합니다.\n\n\n\n\n그림 1: KUKA LWR 4+ 로봇 팔과 Allegro 로봇 손을 이용해 공중에 던져진 우유병 형태의 물체를 잡는 장면. 해당 연구에서 로봇은 테니스 라켓, 망치, 빈 병, 내용물이 일부 든 병, 박스 등 다양한 형태의 물체를 0.5초 이내의 비행 시간 동안 추적하여 잡아낼 수 있음을 입증하였다."
  },
  {
    "objectID": "posts/paper/2025-07-25-catching.html#기존-연구와의-차별성",
    "href": "posts/paper/2025-07-25-catching.html#기존-연구와의-차별성",
    "title": "📃Catching Objects in Flight 리뷰",
    "section": "2.2 기존 연구와의 차별성",
    "text": "2.2 기존 연구와의 차별성\n고속 비행 물체 포획에 관한 선행 연구들은 주로 이론적 모델이나 사전 정의된 궤적에 기반한 접근을 취해왔습니다. 예를 들어, 공을 잡는 문제에서는 물체의 궤적을 포물선(탄도)으로 가정하고 최소자승 등의 기법으로 실시간 추정하거나, 공기 저항을 포함한 탄도 모델에 확장 칼만 필터(EKF)를 결합하여 궤적을 예측하는 시도가 있었습니다. 이러한 방법들은 물체의 동역학 모델이 정확히 알려져 있는 경우에는 궤적 추정 정확도가 높았지만, 모델에 대한 사전 지식(예: 물체의 질량, 무게중심 위치, 관성 모멘트 등)이 필요하고, 주로 구 형태의 물체(공)를 전제로 질점(COM) 궤적만 추적하기 때문에 망치나 라켓처럼 복잡한 형태의 물체에는 적용하기 어렵다는 한계가 있습니다. 반면 이 논문에서는 임의 형태의 물체에도 적용 가능하도록, 데이터 기반으로 물체의 운동을 학습하여 모델을 구축함으로써 사전에 물리적 매개변수를 알 필요 없이 궤적을 예측할 수 있게 했습니다. 실제로 저자들의 이전 연구에서는 사람의 던지기 시演으로부터 물체 운동의 동역학을 학습하여, 물체의 질량이나 관성 등의 정보 없이도 6자유도(위치+자세) 궤적을 예측하는 자율 동적 시스템 모델을 구현한 바 있습니다.\n또 다른 차별점은 실시간성과 불확실성 대응입니다. 과거의 여러 로봇 catching 연구들은 사전에 계산된 시간-의존 궤적(예: 다항식 보간, 최소 가속도 궤적 등)으로 로봇을 움직이도록 했기 때문에, 일단 궤적 생성 후에 시간상의 교란이나 목표 위치의 변화가 생기면 대응하기 어려웠습니다. 즉, 잡기 동작이 시작된 후에 물체의 운동에 변화가 생겨도 로봇 궤적을 즉각 재계산하기 어렵다는 문제입니다. 본 연구에서는 이러한 한계를 극복하기 위해 시간-불변적 동적 시스템을 기반으로 한 반응형 제어기를 사용하고, 여기에 시간 조율 기법(fast-forward integration & scaling)을 적용하여 잡는 순간에 정확히 도달하도록 궤적을 실시간으로 조절합니다. 그 결과, 예측된 catching 자세가 수시로 변경되어도 로봇 팔의 DS 제어 궤적을 그에 맞게 고속 재계획(약 10ms 주기)할 수 있어, 비행 중 발생하는 센서 지연이나 잡기 시점 변화에도 대응할 수 있습니다. 특히 논문에서는 제안한 확률적 모델 덕분에 전체 비행 시간이 0.7초 이하인 경우에도 로봇이 물체를 잡을 수 있었고, 최적 잡기 자세 계산에는 0.2ms 정도밖에 걸리지 않았다고 보고합니다. 이는 이전 연구들과 비교해 압도적으로 빠른 계산 및 적응 속도를 보여주는 것으로, 과거에는 이 정도 속도의 궤적 재계획을 위해 32코어 병렬컴퓨팅을 활용한 복잡한 최적화 기법을 써야 했던 사례도 있었던 것과 대비됩니다.\n마지막으로, 로봇 손가락 제어 측면에서도 본 연구는 기존 방식과 차별됩니다. 전통적으로 로봇이 물체를 잡을 때는 엔드이펙터(손바닥)와 물체 사이 거리가 일정 임계값 이하로 좁혀지는 순간 손가락을 닫는 간단한 트리거 방식이 주로 사용되었습니다. 이러한 방식은 손가락 닫는 타이밍, 속도 등의 매개변수를 사람이 튜닝해야 하고 물체의 접근 방향이나 속도 변화에 따라 최적의 타이밍을 보장하기 어렵습니다. 반면 이 논문에서는 팔과 손가락을 하나의 연합된 동적 시스템(CDS)으로 모델링하여, 팔의 운동 궤적과 손가락의 닫힘 동작이 자연스럽게 동기화되도록 만들었습니다. 즉, 팔의 움직임 상태에 따라 손가락 동작이 자동으로 결정되므로, 별도의 임계값 트리거 없이도 물체가 손바닥에 닿는 정확한 순간에 손가락이 닫히게 됩니다. 이는 팔 운동이 예측과 달리 지연되거나 가속되더라도 손가락 타이밍이 함께 조절되므로, 물체를 흘리거나 튕겨내지 않고 안정적으로 잡을 확률을 높여줍니다. 이러한 팔-손 연합 제어 기법은 저자들의 선행 연구에서 제시된 것으로, 본 논문에서는 이를 실제 7자유도 KUKA 팔과 4-손가락 Allegro Hand 플랫폼에 적용하여 유효성을 검증하였습니다."
  },
  {
    "objectID": "posts/paper/2025-07-25-catching.html#물체-궤적-학습과-예측-dynamics-learning",
    "href": "posts/paper/2025-07-25-catching.html#물체-궤적-학습과-예측-dynamics-learning",
    "title": "📃Catching Objects in Flight 리뷰",
    "section": "2.3 물체 궤적 학습과 예측 (Dynamics Learning)",
    "text": "2.3 물체 궤적 학습과 예측 (Dynamics Learning)\n이 논문에서는 비행 물체의 궤적을 모델링하기 위해 비선형 동역학 모델을 학습하는 접근법을 사용했습니다. 물체의 상태를 \\xi = [p,; q]로 정의하는데, 여기서 p는 물체에 고정된 특정 관심 지점(point of interest)의 3차원 위치이고, q는 물체의 자세(orientation)를 나타내는 사원수(quaternion)입니다. 물체의 운동 방정식은 2차 자율 동적 시스템(이차 미분방정식)으로 가정되며, 형태는 다음과 같습니다:\n\n\\ddot{\\xi} = f(\\xi,; \\dot{\\xi}), 즉 가속도 \\ddot{\\xi}가 현재의 위치와 속도 (\\xi, \\dot{\\xi})의 함수로 표현됩니다. 이때 f(\\cdot)는 비선형 함수로서 명시적 수식을 알 수 없으므로, 머신러닝 기반 회귀로 근사하게 됩니다.\n\n저자들은 서포트 벡터 회귀(Support Vector Regression, SVR) 기법을 활용하여 이 함수를 학습시켰습니다. N개의 던지기 실험으로부터 얻은 물체 궤적 데이터(각 궤적에 시간순으로 T개의 상태 데이터 포함)를 사용하여, 입력 \\zeta = [\\xi; \\dot{\\xi}] (위치+자세와 그 미분을 이어붙인 벡터)을 받고 출력 \\ddot{\\xi} (가속도)을 예측하는 SVR 모델을 훈련합니다. 각 출력 차원(\\xi의 각 성분)에 대해 별도의 SVR을 학습하여 총 D개의 회귀 모델 f_{SVR}^d를 얻으며, RBF 커널을 사용하여 비선형 특성을 모델링했습니다. 최적의 커널 매개변수 및 정규화 파라미터 C는 교차검증(grid search)을 통해 결정되었고, 학습 완료 후 SVR 모델은 다음과 같은 회귀 함수를 제공합니다:\n\n\\hat{\\ddot{\\xi}} = f_{SVR}(\\zeta) = \\sum_{m=1}^{M} \\alpha_m K(\\zeta, \\zeta_m) + b,\n\n여기서 M은 서포트 벡터의 수, \\alpha_m은 학습된 계수, K는 RBF 커널 함수입니다. 이 회귀식이 곧 물체 운동 방정식 f(\\cdot)에 대한 근사이며, 임의의 현재 상태 (\\xi, \\dot{\\xi})를 넣으면 다음 순간의 가속도를 예측할 수 있습니다.\n이렇게 학습된 물체 동역학 모델을 이용하면, 로봇은 비행 물체의 현재 상태 측정값(위치, 자세, 속도)을 입력하여 미래의 궤적을 실시간으로 예측할 수 있습니다. 논문에서는 이 예측 모듈을 주기적으로 갱신하도록 설계하였는데, 카메라 등의 센서로부터 새로운 물체 위치/자세 데이터가 들어올 때마다 SVR 모델로 남은 비행 경로를 빠르게 계산합니다. 또한 앞서 언급한 대로 예측 과정에 EKF 기반의 보정을 결합하여, 센서 노이즈나 외란으로 인한 불확실성을 줄였습니다. 요컨대, SVR 회귀가 이상적인 운동을 내다보는 역할을 하고, EKF 추정기가 새로운 관측값으로 그 예측을 미세 조정함으로써, 노이즈에 강인하면서도 빠른 궤적 예측을 구현한 것입니다.\n저자들은 물체 동역학 모델을 학습하기 위해 다양한 초기 조건으로 물체를 던지는 실험을 수행하여 데이터셋을 구축했습니다. 예를 들어, 시뮬레이션 상의 iCub 로봇을 이용해 망치와 테니스공을 20회씩 던져 서로 다른 궤적을 만들고, 실제 실험에서도 빈 병, 물이 절반 든 병, 라켓, 상자 등을 각각 여러 차례 던져 데이터로 활용했습니다. 각 던지기 궤적은 최대 100Hz로 기록되었고, 사전에 저역 통과 필터(Butterworth, 25Hz)로 노이즈를 제거한 후 속도와 가속도를 계산하여 학습에 사용했습니다. 이렇게 수집한 약 20개 내외의 궤적 데이터로 SVR을 훈련한 결과, 불과 수십 개의 시연만으로도 물체 운동의 비선형 패턴을 학습할 수 있었습니다. 학습된 모델은 텍스트 파일로 저장하여 실시간 실행 시 불러오도록 구성하였으며, C++ 기반 제어 소프트웨어에서 해당 모델을 활용해 매 주기마다 물체의 다음 위치와 자세를 예측하게 됩니다.\n예측 성능 평가를 위해 진행된 시뮬레이션 실험에서, 제안한 SVR-RBF 기반 모델은 매우 정확한 궤적 예측을 보여주었습니다. iCub 시뮬레이터 상에서 망치와 라켓을 무작위 초기조건으로 50회 던져본 결과, 그 중 로봇 작업공간 내로 들어온 시도 47회에 대해 로봇이 100%의 성공률로 물체를 잡았습니다. (나머지 3회는 물체가 로봇이 닿을 수 없는 범위를 벗어나 아예 잡을 수 없는 경우였습니다.) 특히 시뮬레이션에서는 공기저항이나 센서 오차 등의 불확실성이 없었기 때문에, 처음 몇 프레임의 관측만으로도 궤적 예측이 실제 궤적에 거의 수렴하였고 예측 오차도 매우 작았습니다. 한편, 부분적으로 물이 든 병과 같이 질량 분포가 변화하는 복잡한 물체의 경우, SVR로 학습한 모델과 전통적인 강체 동역학 모델의 예측 성능을 비교한 결과가 흥미롭습니다. 강체 모델을 위해 병의 초기 무게중심과 질량, 관성 등을 계측하여 포물선+회전 운동 방정식을 구성해 보았으나, SVR 학습 모델은 실제 궤적을 정확히 맞춘 반면, 강체 모델은 특히 물체의 회전(자세) 예측에서 큰 오차를 보였습니다. 논문에 제시된 부분채워진 물병의 궤적 그래프(Fig. 13)를 보면, SVR 기반 추정은 물체의 복잡한 운동(비선형 공중회전까지 포함)을 잘 따라가는 반면, 강체 역학 모델은 시간 경과에 따라 예측 자세가 실제와 동떨어져 결국 물체를 놓치는 상황으로 이어짐을 알 수 있습니다. 이 결과는 사전 모델링 없이 데이터로부터 학습한 접근법의 우수성을 보여주는 사례로, 특히 물체의 물리적 특성이 불완전하거나 변화하는 상황에서 유용함을 시사합니다."
  },
  {
    "objectID": "posts/paper/2025-07-25-catching.html#최적-잡기-자세-결정-가용-공간-파지-공간-모델링",
    "href": "posts/paper/2025-07-25-catching.html#최적-잡기-자세-결정-가용-공간-파지-공간-모델링",
    "title": "📃Catching Objects in Flight 리뷰",
    "section": "2.4 최적 잡기 자세 결정: 가용 공간 & 파지 공간 모델링",
    "text": "2.4 최적 잡기 자세 결정: 가용 공간 & 파지 공간 모델링\n비행 물체를 잡기 위해서는 로봇 팔이 어떤 위치에서 어떤 자세로 물체를 포착할 것인지 결정해야 합니다. 일반적인 로봇공학 용어로, 이는 로봇의 작업 공간(reachable space)과 물체의 파지(grasp) 가능한 자세 공간이 교집합을 이루는 지점을 찾는 문제로 볼 수 있습니다. 본 논문에서는 이 문제를 풀기 위해 로봇의 가용 공간 모델과 물체의 파지 공간 모델을 확률적 방식으로 학습하고, 두 모델을 결합하여 실시간으로 최적의 잡기 자세(인터셉트 지점)를 결정하는 새로운 방법론을 제안했습니다.\n먼저 로봇의 가용 공간(reachable space) 모델링부터 살펴보겠습니다. 로봇 팔이 도달 가능한 모든 손끝 자세(위치+방향)의 집합은 일반적으로 6차원(SE(3)) 공간에서 매우 복잡한 형태의 영역을 이룹니다. 이를 분석하기 위해 과거 연구들에서는 로봇의 작업공간 경계를 기하학적으로 해석하거나, 격자화된 3D 공간의 점들에 대해 역기구학(IK)으로 도달 가능 여부를 일일이 검사하여 데이터베이스를 만드는 접근이 시도되었습니다. 그러나 이러한 방법들은 불연속적(discrete)인 근사만 제공하거나 로봇 종류마다 개별 설계가 필요하며, 실시간으로 활용하기 어렵다는 단점이 있습니다. 이에 반해 본 논문에서는 로봇의 6D 가용 공간을 하나의 확률분포로 모델링하는 아이디어를 취했습니다. 구체적으로, 로봇이 취할 수 있는 손끝의 위치 및 방향 데이터를 양성 표본(positive examples)으로 수집한 뒤, 이를 학습시켜 확률 밀도 함수로 표현합니다. 저자들은 이러한 one-class 분류 접근법을 사용하면, 오로지 도달 가능한 경우의 데이터만으로도 로봇 작업공간을 충분히 묘사할 수 있고, 도달 불가능한 영역에 대해서는 모델의 밀도 값이 자연스럽게 0에 가까워져 거짓 양성(false positive)을 거의 내지 않는다고 설명합니다.\n가용 공간 모델 학습을 위한 데이터 획득은 두 가지 방식으로 이루어졌습니다. 첫째, 인간 조작을 통한 로봇 팔 시범입니다. 연구진은 KUKA LWR 4+ 로봇팔을 사람이 직접 이끌어(kinesthetic teaching) 다양한 위치로 움직이는 시범 조작을 수행하여, 로봇이 닿을 수 있는 끝부분 자세들을 수집했습니다. 특히 로봇이 매번 동일한 초기 자세(대기자세)에서 출발하여, 작업공간 내 여러 지점을 향해 팔을 뻗는 약 20개의 데모를 진행함으로써, 로봇이 어디까지 팔을 뻗을 수 있는지 다양한 방향으로 데이터를 얻었습니다. 각 데모의 말단 자세(손끝 위치와 자세)를 모으면 로봇 reachable space 경계 부근의 표본을 얻을 수 있는데, 연구진은 20개 정도의 데모로도 충분히 안정적인 작업공간 모델을 학습할 수 있었다고 합니다. 둘째, 이러한 인간 시범 외에 알고리즘적 표본 생성도 활용되었습니다. 예를 들어 로봇의 관절공간을 임의로 샘플링하여 손끝의 6D 자세를 계산하고, 자가 충돌이나 특이점 등을 걸러낸 유효 자세들을 모을 수도 있습니다. 논문에서는 자세한 구현을 밝히진 않았으나, 언급된 Section II-B2의 방법으로 LWR 4+ 로봇의 reachable space를 추가로 모델링했다고 되어 있습니다. 결과적으로 이렇게 수집된 수천 개 이상의 손끝 자세 데이터에 대해 가우시안 혼합 모델(GMM)을 학습함으로써, 로봇 팔이 어떤 위치에서 어떤 방향으로 손목을 향하게 할 수 있는지에 대한 확률 모델을 만들었습니다.\n\n\n\n예를 들어, 논문에 제시된 Fig. 7은 7-DOF LWR 로봇의 reachable space를 나타낸 그림으로, 로봇 손끝의 3차원 위치를 둘러싼 등고선(plots)과 대응 가능한 손목 방향의 분포를 시각화하고 있습니다. 이 모델을 이용하면, 어떤 3D 위치가 로봇으로 reachable 한지를 밀도 값으로 판단할 수 있을 뿐 아니라, 특정 위치에서 로봇이 취할 수 있는 최적 손목 방향이 무엇인지도 조건부 확률 질의로 얻을 수 있습니다. 즉, 학습된 GMM으로부터 “만약 손끝 위치가 (x,y,z)라면, 로봇 손목은 어떤 방향 쪽으로 향할 수 있는가?”를 질의하면 가장 가능도 높은 방향(orientation)을 계산할 수 있고, 반대로 “손목 방향을 특정 각도로 정하면 reachable 위치가 어디인가?”도 추론할 수 있는 것입니다.\n한편, 물체의 파지 공간(graspable space) 모델링은 물체를 어떤 자세로 쥘 수 있는지를 확률적으로 표현한 것입니다. 로봇이 공중에서 물체를 잡는 경우, 아무 위치나 잡는 것이 아니라 정해진 부분(예: 라켓의 손잡이, 망치의 손잡이 부분 등)을 잡아야 하며, 또 로봇 손가락이 물체를 둘러싸는 방향도 제한됩니다. 따라서 물체마다 잡기 좋게 손을 가져갈 수 있는 위치와 자세의 집합이 존재하며, 이를 Graspable space라 정의합니다. 이 또한 6차원 공간(손끝의 상대적인 위치+자세)에서 복잡한 분포를 가지므로, 시범 학습을 통해 모델링하였습니다. 연구진은 각 물체에 대해 인간이 직접 로봇 팔과 손을 움직여서 물체를 잡는 시범을 제공했고, 이 과정에서 물체와 로봇 손의 상대적인 위치/자세를 캡쳐하여 데이터로 활용했습니다. 예를 들어 Allegro Hand로 병을 잡는 파지 공간을 학습하기 위해, 사람 연구자가 로봇 팔을 수동으로 이끌어 약 15초 동안 여러 각도에서 병을 쥐어보는 시범을 보였습니다. 이 동안 240Hz로 모션 캡쳐 시스템이 물체의 좌표와 로봇 손끝의 좌표를 동기화하여 기록하였고, 총 약 3600개의 순간 자세 표본을 얻었습니다. 이렇게 모인 데이터 중 대표적인 300개 표본을 무작위 추출하여 GMM 학습에 사용함으로써, 해당 물체를 잡을 수 있는 손 위치/방향의 확률 모델을 구축했습니다. Fig. 4에 그 예시가 제시되어 있는데, (a)는 로봇 손, (b)는 시범 과정, (c)는 시범으로 얻은 파지 자세 표본 분포, (d)는 GMM으로 모델링한 후 특정 위치에서의 손목방향 가능도 분포 등으로 구성되어 있습니다. 이를 통해 물체에 대해 “어떤 상대적 자세로 로봇 손을 가져가면 잡을 수 있다”를 확률적으로 표현할 수 있게 되었습니다.\n가용 공간 모델과 파지 공간 모델이 모두 준비되었다면, 이제 비행 물체를 언제 어디서 잡을지 결정하는 일만 남습니다. 저자들은 두 모델을 통합적으로 활용하여 최적의 catching 자세와 시점을 산출하는 알고리즘을 구현했습니다. 개략적인 과정은 다음과 같습니다. 우선 앞서 설명한 SVR 기반 물체 궤적 예측 모듈을 통해, 물체가 앞으로 비행할 경로(연속적인 위치와 자세의 시간 함수)를 예측합니다. 이 예측 궤적 상에서 일정한 시간 간격으로 후보 포인트(미래 시각 t_i에서의 물체 자세)를 샘플링합니다. 그런 다음 각 후보 (p_i, q_i)에 대해 로봇이 그 지점에서 잡을 수 있는지를 평가하는데, 이것이 바로 reachable-space 모델과 graspable-space 모델의 결합 판단입니다. 구체적으로, 후보 물체 자세 (p_i, q_i)가 주어지면 우선 로봇 가용 공간 모델을 이용해 그 위치 p_i에서 로봇 손끝이 취할 수 있는 방향 분포를 계산할 수 있습니다. 동시에 파지 공간 모델로부터는 물체가 자세 q_i일 때 로봇 손이 취해야 할 상대 방향을 얻습니다. 이 둘이 일치한다면 (즉, 물체의 그 자세에서 로봇이 손을 넣어 잡을 수 있는 경우), 해당 지점은 유효한 catching 자세가 됩니다. 논문에서는 이 과정을 더욱 효율화하기 위해, 아예 reachable-space와 graspable-space를 하나의 결합 확률모델 M_{\\text{joint}}로 간주하여, 특정 물체 자세에 대한 공동 확률밀도를 계산하는 방법을 취했습니다. 예를 들어 “시간 t_i에 물체가 자세 (p_i, q_i)일 확률”과 “그때 로봇 손이 자세 (p_i, o) (어떤 방향 o)로 도달할 수 있을 확률”을 함께 고려하는 식입니다. 이 joint 모델의 밀도가 가장 높은 (p, q, o, t) 조합이 바로 최적 잡기 자세 및 시점으로 선택됩니다. 저자들은 여기에 하나의 추가적인 휴리스틱을 적용했는데, 잡기 직전 로봇 손바닥의 방향이 날아오는 물체의 운동 방향에 정확히 반대가 되도록 자세를 정제하였습니다. 이는 물체를 받쳐서 충격을 흡수하고 흘리지 않기 위한 것으로, 예를 들어 공이 날아올 때 손바닥이 공의 진행 방향을 향해 있어야 튕겨내지 않고 품을 수 있는 것과 같은 원리입니다.\n위와 같은 절차를 통하면, 매 새로운 센서 관측 시점마다 로봇 시스템은 현재 추정된 물체 상태로부터 미래의 잡기 지점과 시간을 지속적으로 업데이트하게 됩니다. 논문에서는 이 과정을 Thread 1로 명명하여, 물체 궤적 예측과 최적 자세 계산이 병렬 쓰레드로 상시 실행됨을 설명합니다. 이 쓰레드는 새로운 카메라 측정이 들어올 때마다 물체의 남은 궤적을 다시 예측하고, 최적의 catching 시점/자세를 다시 계산하여 갱신합니다. 이렇게 계산된 목표 catching 자세는 Thread 2인 로봇 팔 제어기에 실시간으로 전달되어, 로봇 팔이 그 지점으로 이동하도록 유도됩니다. 두 쓰레드의 상호작용은 논문의 Fig. 2 블록 다이어그램에 잘 나타나 있으며, 이러한 계속적인 재계획(replanning) 덕분에 센서 지연이나 예측 오차가 조금씩 누적되더라도 로봇이 잡기 직전까지 궤적을 수정하여 정밀한 포획이 가능해집니다. 다만, 실제 시스템에서는 물체가 로봇에 너무 가까이 접근한 마지막 0.09초 정도 전부터는 더 이상 계획을 변경하지 않고 최종 동작에 집중하도록 하였습니다. 이는 물체가 눈앞에 근접했을 때 후방 카메라 시야에서 부분적으로 사라져 위치 추정이 흔들릴 수 있고, 임계시간 내에 새로운 계산을 반영하기 어려워질 수 있기 때문입니다. 따라서 잡기 90ms 전을 최종 커트오프로 두고, 그 이후에는 현재 계획대로 손가락을 닫는 동작까지 수행하도록 설계되었습니다.\n요약하면, 제안된 방법은 확률적 모델링을 통해 로봇의 가능한 자세 공간과 물체의 잡기 가능한 자세를 하나로 통합한 뒤, 실시간 궤적 예측과 결합하여 밀리초 단위로 최적 인터셉트 지점을 찾아내는 알고리즘입니다. 이러한 접근은 기존의 기하학적/격자 기반 방법에 비해 훨씬 매끄럽고 연속적인 해결책을 주며, 계산 비용도 낮아서 2.7GHz CPU에서 0.2밀리초 만에 잡기 자세를 결정할 수 있었던 것으로 보고됩니다. 이는 로봇이 공중에 떠있는 물체를 잡기 위해 쓸 수 있는 시간 여유가 많지 않을 때(보통 &lt;0.5–0.7초)에 큰 강점이 됩니다."
  },
  {
    "objectID": "posts/paper/2025-07-25-catching.html#로봇-팔-손-운동-계획과-제어-reactive-arm-hand-control",
    "href": "posts/paper/2025-07-25-catching.html#로봇-팔-손-운동-계획과-제어-reactive-arm-hand-control",
    "title": "📃Catching Objects in Flight 리뷰",
    "section": "2.5 로봇 팔-손 운동 계획과 제어 (Reactive Arm-Hand Control)",
    "text": "2.5 로봇 팔-손 운동 계획과 제어 (Reactive Arm-Hand Control)\n잡기 시점과 목표 자세가 정해지면, 이제 로봇은 그에 맞추어 팔을 움직이고 손가락을 제어해야 합니다. 이 논문에서는 로봇 팔의 궤적을 생성하기 위해 안정적인 동적 시스템 기반 제어기를 사용하였습니다. 이는 저자들이 이전 연구들에서 개발한 기법으로, 여러 시연 궤적을 일반화하여 종료 안정(equilibrium stable)한 벡터장 형태로 학습하는 시간-불변 DS 기술입니다. 간단히 말해, 사람이 가르쳐준 몇 가지 경로를 자유롭게 뒤섞어도 목표점에 수렴하도록 보장되는 연속 함수 형태로 모델링하는 것으로, SEDS (Stable Estimator of Dynamical Systems) 등의 알고리즘을 응용한 방법입니다. 이 DS 제어기의 장점은 현재 상태를 입력하면 다음에 어느 방향으로 움직일지 즉각 출력해주는 형태이므로, 로봇이 목표에 도달할 때까지 닫힌 형식(closed-form)으로 쭉 따라가는 연속 궤적을 만들어준다는 점입니다. 특히 DS는 시간 매개변수가 없어 목표점만 향하도록 설계되므로, 예측되는 목표 위치나 팔의 경로가 실시간으로 바뀌어도 그때그때 새로운 벡터장을 따라가면 됩니다.\n논문에서는 이 DS 기반 팔 제어기를 학습하기 위해 20개의 팔 움직임 시범을 사용했습니다. 앞서 가용 공간 모델을 수집할 때와 동일한 방법으로, 연구자는 로봇 팔을 초기자세에서 시작하여 여러 가지 잡기 목표 자세까지 이끄는 데모들을 수행했습니다. 각 데모는 시작 (대기자세)부터 끝 (잡기자세)까지 팔의 궤적을 제공하며, 이렇게 모인 다수의 경로를 Gaussian Mixture Model로 학습한 후 Dynamical System으로 변환하여 팔 움직임을 생성합니다. 결과적으로, 로봇 팔은 현재 자신의 손끝 위치가 어디에 있든지 간에 DS가 지시하는 방향으로 움직이면 자연스럽게 목표 잡기 자세로 수렴하게 됩니다. 또한 잡기 목표 자체가 움직이는 경우(예: 예측이 변경된 경우)에도, DS는 새로운 목표를 향해 연속적으로 방향을 수정해주므로, 마치 목표를 추적하는 유도장치처럼 팔을 몰고 가는 효과를 냅니다.\n하지만 물체 잡기 동작에서는 “언제 도착하는가” 또한 매우 중요합니다. DS 단독으로는 시간에 대한 개념이 없기 때문에 목표점에는 수렴하지만 주어진 시간 내 도달을 보장할 수 없습니다. 이를 위해 논문에서는 타이밍 조절기(timing controller)를 도입했습니다. 타이밍 조절기는 현재 남은 시간과 거리를 바탕으로 DS 출력을 스케일링하여, 로봇이 정확히 물체 도착 시간에 맞춰 도달하도록 속도를 조절합니다. 예를 들면, 처음 예측으로는 잡기 시점이 0.5초 뒤로 계획되었다가 이후 예측이 0.6초로 수정되면, DS의 진행 속도를 20% 늦추어 팔이 약간 천천히 움직이도록 만드는 식입니다. 반대로 시간이 촉박해지면 DS의 스케일을 키워 최대 속도 한계까지 더 빨리 움직이도록 합니다. 이 타이밍 조절은 상시로 작동하여, 예측 시간이 바뀔 때마다 로봇이 그 새로운 시간에 맞춰 도달하도록 해줍니다. 단, 로봇의 물리적 한계를 넘어서까지 보상할 수는 없기 때문에, 만약 목표 잡기 자세에 도달하려면 처음 자세에서 너무 크게 움직여야 하는 경우(예: 반대편에 던져졌는데 로봇이 돌아가야 한다거나)에는 최대 속도로도 시간 내 도달이 불가능할 수 있습니다. 그런 상황은 실패 시도로 간주되며, 논문에서도 실패 사례의 약 12건이 이러한 동역학적으로 불가능한 목표 때문이었다고 보고하고 있습니다.\n이제 손가락 제어를 살펴보겠습니다. 앞서 강조했듯, 본 연구는 팔(손목)과 손가락의 움직임을 밀접히 연동시켰습니다. 이를 위해 팔 동작을 담당하는 DS를 마스터(master) DS, 손가락 쪽을 담당하는 DS를 슬레이브(slave) DS로 설정한 연합 Dynamical System (CDS) 구조를 사용했습니다. 팔의 end-effector 상태 (위치 \\xi_h \\in \\mathbb{R}^3, 손목 자세는 별도 \\xi_o \\in \\mathbb{R}^3 축각(axis-angle) 표현)와 손가락 벌어짐 정도 (\\xi_f \\in \\mathbb{R}, 이를 여러 손가락 조인트로 매핑) 등을 모두 상태로 포함하는 결합 시스템을 구성한 것입니다. 마스터 DS는 팔의 궤적을 생성하고, 슬레이브 DS는 손가락의 궤적(벌리고 쥐는 동작)을 생성하되, 슬레이브는 마스터의 진행 상황에 따라 결정되도록 설계되었습니다. 쉽게 말해, 팔이 목표에 가까워짐에 따라 손가락이 닫히는 속도가 빨라지거나, 팔의 속도가 느려지면 손가락 동작도 지연되는 식으로 상호 결합(coupling)된 것입니다. 이 모델을 학습시키기 위해, 연구진은 별도의 데모 실험도 수행했습니다. 사람에게 센서를 장착하여 직접 물체를 잡게 하고, 이 때 손의 궤적과 손가락 굽힘 각도 변화를 기록하여 팔-손 연동 동작의 예시로 삼았습니다. 구체적으로, 5DT 데이터 글러브로 사람 손가락 관절각을 측정하고, 모션캡쳐로 물체와 손의 위치를 추적하면서, 사람이 공중에 던져진 물체를 한 손으로 잡는 동작을 여러 차례 수행했습니다. 이 데이터를 통해 “팔이 이만큼 움직였을 때 손가락은 얼마나 닫혀야 하는가”의 관계를 학습하고, CDS의 마스터-슬레이브 DS 간 결합 함수를 구했습니다. 논문에서는 손가락 동작을 팔 동작의 특정 지표(distance)에 따라 트리거하는 대신, 이러한 데이터 기반 결합 모델을 사용함으로써 별도의 튜닝 없이도 손가락이 정확한 타이밍에 닫히도록 만들었다고 강조합니다. Fig. 15에는 Barrett Hand를 사용한 팔-손 CDS 제어 구성요소들이 제시되어 있고, Fig. 16에는 실제 잡기 동작 시 로봇 팔의 궤적이 예측 궤적을 계속 추종하면서 시간에 맞춰 도달하는 모습이 그려져 있습니다.\n최종적으로, 로봇 제어 시스템은 Thread 2 (Arm Controller)로 구현되어 500 Hz 주기로 동작합니다. 이 제어 스레드는 매 주기마다 현재 로봇 팔의 상태와 Thread 1이 업데이트한 최신 목표(잡기 위치, 도달 시각)를 받아서, CDS 컨트롤러를 통해 다음 순간의 팔-손속도 명령을 계산합니다. 계산된 로봇 손끝의 목표 변화(선속도, 각속도)는 다시 역기구학(IK) 모듈을 거쳐 7개 관절의 목표 회전각으로 변환되며, 임계 감쇠 필터(critically damped filter)로 신호를 부드럽게 한 뒤 로봇에 전송됩니다. KUKA LWR 4+ 로봇팔은 자체 내장 제어기로 500Hz의 위치 제어가 가능하므로, 호스트 컴퓨터에서 보낸 관절각 명령을 실시간으로 추종하게 됩니다. 이처럼 예측-계획 쓰레드와 제어기 쓰레드가 동시에 돌아가면서, 로봇은 물체를 향해 팔을 뻗고 손가락을 벌린 채 접근하다가, 적절한 순간에 손가락을 오므려 물체를 붙잡게 됩니다."
  },
  {
    "objectID": "posts/paper/2025-07-25-catching.html#실험-결과-및-분석",
    "href": "posts/paper/2025-07-25-catching.html#실험-결과-및-분석",
    "title": "📃Catching Objects in Flight 리뷰",
    "section": "2.6 실험 결과 및 분석",
    "text": "2.6 실험 결과 및 분석\n논문에서는 제안된 시스템을 시뮬레이터와 실제 로봇 양쪽에서 검증하였습니다. 시뮬레이션 실험은 iCub 휴머노이드 로봇의 상반신 모델을 사용했고, 실제 실험은 KUKA LWR 4+ (7-자유도 로봇팔)에 SimLab Allegro Hand (4지 로봇 손)를 장착한 구성으로 진행되었습니다. 물체 인식 및 추적을 위해 OptiTrack 다중 카메라 모션캡쳐 시스템이 이용되었으며, 물체마다 표면에 반사 마커 3개를 부착하여 240 Hz로 6DOF 자세를 측정했습니다. 주요 실험 시나리오는 사람이 물체를 로봇을 향해 던지면 로봇이 이를 받아잡는 형태로, 던지는 초기 조건(위치, 속도, 각속도)은 실험마다 무작위로 조금씩 달리하여 다양한 상황을 시험했습니다. 특히 물체로는 망치, 테니스 라켓, 빈 플라스틱 병, 절반가량 물이 든 병, 작은 박스 등을 사용하여, 물체의 질량 분포와 공기역학적 특성이 크게 다르도록 구성했습니다. 이들은 로봇에게 매우 다양한 도전 상황을 제공하는데, 예컨대 라켓이나 망치는 잡아야 할 부분(손잡이)이 무게중심과 떨어져 있고, 특히 부분 채운 물병은 비행 중에 내부 유체 이동으로 무게중심이 계속 변하는 등 난해한 움직임을 보입니다. 그럼에도 불구하고, 로봇은 이러한 물체들을 정확히 추적하여 대부분의 경우 공중에서 붙잡는 데 성공하였습니다.\n시뮬레이션 결과는 앞서 언급한 대로 iCub 로봇을 통해 확인되었으며, 50회의 무작위 던지기 중 reachable space에 들어온 약 47회의 상황에서 100% 잡기 성공률을 보였습니다. 시뮬레이터에서는 물체 움직임에 모델링 오차나 외란이 없으므로 예측이 완벽에 가깝게 맞아떨어졌고, DS 제어기의 이상적 동작으로 로봇팔이 신속히 목표에 도달하여 실패 사례가 전혀 없었습니다. 이에 반해 실제 로봇 실험에서는 소프트웨어적/기계적 한계와 센서 불확실성이 존재하기 때문에 일부 실패가 발생했지만, 그럼에도 불구하고 매우 높은 성공률을 달성했습니다. 논문에 따르면, 빈 병, 절반 채운 병, 라켓, 박스의 4가지 물체에 대해 각각 20회씩 (총 80회) 던져 실험한 결과, 로봇 작업공간에 들어오지 않은 9회를 제외한 71회의 시도 중 52회 성공하여 73.2%의 전체 성공률을 기록했습니다. 이때 한 번 던져진 물체의 평균 비행시간은 약 0.5초에 불과했으며, 3.5m 거리에서 던진 물체를 그 짧은 시간 안에 잡아낸 것입니다.\n성공률 73%라는 수치는 언뜻 완벽해 보이지 않을 수 있으나, 상대적으로 인간의 성능과 비교하면 이 로봇 시스템의 우수성이 두드러집니다. 저자들은 동일한 실험 조건에서 사람 10명에게 맨손으로 물체 잡기를 시도하도록 하여 결과를 비교했는데, 피험자들은 100회의 빈 병 던지기 중 평균 38회밖에 잡지 못했고, 잘 하는 사람도 성공률 70% 수준에 그쳤다고 합니다. 특히 경험이 적은 사람의 경우 성공률이 겨우 10%대에 불과하여, 공중의 변칙적인 물체를 한 손으로 잡는 일이 얼마나 어려운지 보여주었습니다. 이에 비하면, 로봇은 일관되고 높은 성공률(73%)을 보이며 숙련 인간 수준에 가까운 성능을 발휘한 것입니다. 이는 본 논문의 접근법이 실제 현실 상황에서도 효과적으로 작동한다는 강력한 증거라 할 수 있습니다.\n물론 실험에서 관찰된 실패 사례들은 향후 개선점을 시사합니다. 논문에서는 총 19회의 실패 원인을 분석하였는데, 12건(과반수)은 잡기 목표가 로봇의 초기자세로부터 너무 멀리 떨어져 관절속도 한계를 넘은 경우였습니다. 이때는 로봇이 최대로 팔을 뻗어도 시간 내 도달하지 못해 물체를 놓치게 됩니다. 4건은 잡는 순간에 손가락 일부가 물체를 잘못 쳐서 물체가 튕겨나간 경우였습니다. 이러한 경우는 드물게 발생했으며, 아마도 물체의 접근 각도가 예상보다 미묘하게 달라 손가락이 충돌한 것으로 보입니다. 남은 3건은 로봇 관절에 부하가 급격히 걸려 토크 제한에 걸리며 로봇이 안전정지한 경우였습니다. 전체적으로 보면 실패는 주로 하드웨어적 한계 상황에서 발생했으며, 제어 알고리즘 자체의 문제로 인한 실패는 매우 적었습니다. 특히 센서의 잡음이나 예측 오차 때문에 엉뚱한 곳으로 팔이 간 경우는 없었다는 점에서, 제안된 확률적 예측+계획 통합 알고리즘의 견고함을 확인할 수 있습니다.\n마지막으로, 논문의 실험 결과에는 제안된 방법의 학습 효율과 범용성에 대한 언급도 있습니다. 물체 동역학 모델 학습에 약 20개 정도의 던지기 궤적을 사용하고, 팔 DS 및 파지 모델 학습에도 10~20개 안팎의 시연을 사용한 정도로, 비교적 적은 데이터로 상당히 복잡한 과제를 달성한 것이 인상적입니다. 또한 이 프레임워크를 서로 다른 두 로봇 플랫폼(iCub과 KUKA)에서 모두 구현해보았고, 여러 형태의 물체에 대해서도 유사한 성능을 보였다는 점에서, 제안 기법의 일반화 가능성을 보여줍니다. 이는 핵심 아이디어들이 구체적인 로봇 파라미터나 물체 속성에 강하게 의존하지 않고, 학습으로부터 자동적으로 유도되기 때문으로 판단됩니다. 실제로 저자들은 “본 논문이 불확실성 하에서 초고속 제어의 한 예를 제시하였으며, 과거 연구들이 주로 단순 공 모양 물체나 천천히 회전하는 물체에 국한되었던 데 비해 우리의 연구는 그 범위를 크게 넓혔다”라고 강조하고 있습니다."
  },
  {
    "objectID": "posts/paper/2025-07-25-catching.html#결론",
    "href": "posts/paper/2025-07-25-catching.html#결론",
    "title": "📃Catching Objects in Flight 리뷰",
    "section": "2.7 결론",
    "text": "2.7 결론\n“Catching Objects in Flight” 논문은 로봇이 공중에서 복잡한 물체를 잡는 문제에 대해 학습 기반의 종합적 해결책을 제시한 뛰어난 연구입니다. 물체 궤적 예측부터 잡기 자세 결정, 팔-손 제어에 이르는 전 과정을 통합하였고, 각각의 단계에서 기존 방식의 한계를 극복하는 독창적인 접근을 보여주었습니다. 프로그래밍-바이-데몬스트레이션 기법으로 물체의 비행 동역학과 로봇의 동작 패턴을 획득하고, 이를 확률적 모델과 동적 시스템 제어기로 구현함으로써, 로봇이 밀리초 단위의 반응속도와 높은 적응성을 갖추게 했습니다. 특히 잡기 대상이 불규칙한 모양이고 비행 시간이 매우 짧아 사람조차 잡기 어려운 상황에서, 제안된 시스템이 인간에 필적하는 수준의 성능(성공률 ~73%)을 보인 점은 주목할 만합니다. 이는 로봇이 단순 반복 작업을 넘어 동적인 실제 환경에 대응할 수 있음을 보여준 사례로, 향후 우주 쓰레기 포집이나 고속 물체 조작 등 다양한 응용에 활용될 수 있을 것으로 기대됩니다. 물론 현재 프레임워크는 고정된 위치의 모션캡쳐 카메라에 크게 의존하고 있고, 로봇의 물리적 스펙 한계로 인한 제약도 존재합니다. 따라서 이동 로봇 플랫폼이나 온보드 센서로 확장하고, 보다 빠르고 강인한 로봇 하드웨어와 결합한다면, 공중에서 물체를 잡는 로봇의 활용 범위는 더욱 넓어질 것입니다. 전체적으로 이 논문은 로봇 학습과 실시간 제어를 접목하여 난제를 해결한 훌륭한 사례로서, 추후 동분야 연구자들에게도 많은 영감을 제공하고 있습니다.\n참고 자료: 본 리뷰에서는 논문의 내용과 실험 결과를 기반으로 핵심적인 사항들을 정리하였습니다. 자세한 기술 구현이나 추가 실험 정보는 원문을 참고하시기 바랍니다. 또한 관련 분야의 발전된 연구로, 물체 포획을 위한 궤적 최적화 기법이나 딥러닝 기반 접근법 등이 있으나, 해당 논문의 범위를 벗어나므로 본 리뷰에서는 다루지 않았습니다. 이 논문의 접근법은 로봇의 인지-계획-제어 전 과정을 아우르는 통합 설계를 보여주며, 향후 더욱 복잡한 동작 학습과 고속 제어 연구의 밑거름이 될 것으로 평가됩니다."
  },
  {
    "objectID": "posts/paper/2025-09-19-text2touch.html",
    "href": "posts/paper/2025-09-19-text2touch.html",
    "title": "📃Text2Touch 리뷰",
    "section": "",
    "text": "Paper Link\nHomepage"
  },
  {
    "objectID": "posts/paper/2025-09-19-text2touch.html#논문의-핵심-기여-요약",
    "href": "posts/paper/2025-09-19-text2touch.html#논문의-핵심-기여-요약",
    "title": "📃Text2Touch 리뷰",
    "section": "논문의 핵심 기여 요약",
    "text": "논문의 핵심 기여 요약\n이 논문의 저자들은 “Text2Touch” 프레임워크를 제시하며, 다음과 같은 핵심 기여를 합니다:\n\nLLM 기반 촉각 보상 설계의 최초 구현: 기존 LLM 활용 연구들이 시각이나 고유감각(Proprioception)에 한정되었던 데 반해, 이 논문은 촉각 센싱을 통합한 최초의 자동 보상 함수 생성 기법을 선보였습니다. 즉, 로봇이 물체를 “느끼는” 접촉 정보까지 활용하여 보상을 설계함으로써, 인간 수준의 섬세한 조작에 한 발 더 다가섰습니다.\n복잡한 환경에 대한 프롬프트 엔지니어링 기법: 저자들은 70개가 넘는 환경 변수를 LLM에 효과적으로 전달하기 위한 프롬프트 구조화 전략을 개발했습니다. 이를 통해 LLM이 복잡한 상태 공간에서도 오류 없이 동작하는 보상 코드를 생성하도록 유도하고, 시뮬레이션 학습 성능을 향상시켰습니다.\n시뮬레이션-현실 이식(Sim-to-Real) 검증: 시뮬레이터에서 LLM이 설계한 보상으로 학습된 정책을 교사-학생 모델 증류를 통해 실제 로봇 손에 성공적으로 이식하였습니다. 그 결과, 로봇 손바닥을 위로 향한 경우뿐 아니라 아래로 향한 어려운 조건에서도 물체를 3축 회전시키는 데 성공했고, 해당 분야 기존 최고 성능(인간 설계 보상)을 뛰어넘는 회전 속도와 안정성을 달성했습니다. 이는 현재까지 보고된 가장 어려운 손내 조작 과제에서 새로운 최고 기록을 세운 것입니다.\n\n이러한 기여를 통해, 이 논문은 LLM으로 보상 함수를 자동 설계하여 복잡한 촉각 멀티모달 로봇 학습을 가속할 수 있음을 증명했습니다."
  },
  {
    "objectID": "posts/paper/2025-09-19-text2touch.html#사용된-방법론-및-실험-구성-분석",
    "href": "posts/paper/2025-09-19-text2touch.html#사용된-방법론-및-실험-구성-분석",
    "title": "📃Text2Touch 리뷰",
    "section": "사용된 방법론 및 실험 구성 분석",
    "text": "사용된 방법론 및 실험 구성 분석\nText2Touch의 전체 학습 파이프라인 개요. 왼쪽은 LLM에 제공되는 프롬프트 구성 요소들로, 환경의 관측 변수 정의, 과제 목표에 대한 자연어 설명, 그리고 보상 설계 지침 등이 포함된다. 가운데는 LLM이 생성한 보상 함수를 이용해 시뮬레이터에서 강화학습을 수행하는 과정이다. 여러 후보 보상 함수를 시험하여 최적의 정책(노란색 교사 모델)을 얻은 후, 오른쪽의 보라색 학생 모델로 지식 증류를 거쳐 촉각 센서와 관절각 등 실제 센서 입력만으로 동작하는 정책을 확보한다. 마지막으로 이 학생 정책을 실제 로봇 손에 배치하여 물체 회전 과제를 수행한다.\n\n과제 정의 및 LLM 보상 함수 생성\n연구의 목표 과제는 손바닥 안팎의 어떤 방향에서도 물체를 공중에 든 채로 3축 회전시키는 것입니다. 이는 물체를 놓치지 않으면서 X, Y, Z축으로 연속 회전시키는 고난도 조작으로, 로봇 손에 장착된 시각 기반 촉각 센서(예: TacTip)에서 얻은 접촉힘/미끄럼 정보를 활용해야 달성할 수 있는 과제입니다. 이처럼 고차원 센서 정보(촉각 + 관절각 등 총 70여 개 변수)를 다루는 환경에서, 저자들은 체계적인 프롬프트 엔지니어링을 통해 LLM (주로 GPT-4)을 활용한 보상 설계 방법을 개발했습니다.\n프롬프트에는 환경 상태 변수들의 목록과 의미, 과제 목표에 대한 자연어 설명, 그리고 보상 함수에 필요한 보너스/패널티 구조 지침이 포함되었습니다. 예를 들어 “물체의 회전각 속도를 높일수록 보상을 높이고, 물체를 떨어뜨리면 큰 패널티를 부여하라”와 같은 식으로 보상 구조를 서술합니다. 이렇게 하면 LLM이 주어진 변수들을 활용하여 성공 보너스와 실패 패널티를 명시적으로 포함한 파이썬 보상 함수를 생성하도록 유도할 수 있습니다. 저자들은 프롬프트 설계를 반복 실험하며 다듬었는데, 그 결과 LLM이 짧고 실행 가능한 보상 코드를 높은 확률로 산출하게 되었습니다. (향상된 프롬프트 없이 LLM을 직접 사용할 경우, 엉뚱한 변수에 의존하거나 실행 오류가 나는 코드가 생성되기 쉽다는 것도 실험을 통해 확인되었습니다.)\n\n\n강화학습 및 보상 함수 탐색\nLLM이 출력한 보상 함수는 시뮬레이터 상의 로봇 손에 적용되어 강화학습(RL)으로 정책을 훈련하는 데 쓰입니다. 이때 교사-학생 2단계 학습이 활용됩니다. 먼저 교사 모델은 특권 정보(privileged info)를 활용하는 정책으로, 시뮬레이터에서 물체의 실제 자세나 접촉 여부 등 완전한 상태정보를 참고하여 학습됩니다. 이러한 교사 정책은 주어진 보상 함수가 이론적으로 낼 수 있는 최상의 성능을 대표합니다. 저자들은 최신 LLM 연구인 Eureka 방식을 따라, 여러 후보 보상 함수들을 자동 탐색하여 가장 성능이 좋은 보상 함수를 선택했습니다. 그 결과 GPT-4를 비롯한 몇 가지 LLM이 제시한 보상 함수들이 시뮬레이터에서 높은 회전 속도 성능을 보였고, 이 중 최고 성능의 보상 함수들을 추려냈습니다. 선택된 보상으로 학습된 교사 정책들은 에피소드당 평균 5회 이상의 360° 회전을 달성했는데, 이는 기존 인간 설계 보상의 약 4.7회보다 높은 수치입니다. 뿐만 아니라 LLM이 만든 보상들은 코드가 훨씬 간결함에도 불구하고, 학습 과정에서 안정적인 그립 유지(물체를 떨어뜨리지 않는 비율) 역시 개선되었습니다.\n\n\n교사 모델의 지식 증류 및 실험 설계\n다음으로, 시뮬레이터 상에서 특권 정보로 학습된 교사 정책을 지식 증류(distillation) 기법으로 학생 정책에 이식합니다. 학생 정책은 오직 실제 로봇에서 얻을 수 있는 관측(손가락 관절각, 각 촉각센서 이미지 등)만을 입력으로 받아 동작하도록 학습됩니다. 이를 위해 교사 정책이 생성한 행동 데이터를 모으고, 학생 정책이 이 행동을 모방하거나 추가적인 RL 튜닝을 통해 성능을 맞추도록 합니다. 이렇게 하면 시뮬레이터에서 잘 동작하던 정책을 실제 세계의 촉각 센서 기반 정책으로 변환할 수 있습니다. 저자들은 현실 적응을 돕기 위해 시뮬레이터에서 물체의 마찰 계수, 질량 등 물리 특성을 무작위로 변화시키는 도메인 랜덤화 기법도 적용했습니다. 최종적으로 얻어진 학생 정책들은 영국 브리스톨 로봇랩의 4손가락 로봇 손(Allegro Hand)에 구현되었으며, 이 손가락들에는 TacTip 촉각 센서가 장착되어 있어 실제 접촉 정보를 감지합니다.\n\n\n실험 구성 및 평가\n평가를 위해 다양한 형태와 질량의 실제 물체들을 사용했습니다. 예를 들어 플라스틱 사과와 오렌지(둥근 물체), 네모난 작은 상자, 고무 오리 인형(복잡한 형상) 등 약 8~10종의 물체를 로봇 손으로 집어들어 회전시키는 실험을 진행했습니다. 특히 손바닥을 위로 향한 경우(palm-up)와 아래로 향한 경우(palm-down) 모두 테스트하여, 중력 방향과 관계없이 물체를 놓치지 않고 돌릴 수 있는지를 검증했습니다. 성능 지표로는 에피소드당 연속 회전 횟수(한 번 잡아서 떨어뜨리기 전까지 몇 바퀴 돌리는가)와 에피소드 지속 시간(버티컬 안정성 지표로 해석 가능)을 측정했습니다. 또한 학습에 사용하지 않은 새로운 무게나 새로운 형태의 물체에 대해서도 일반화 성능을 시험했습니다 (논문에서는 이를 OOD (out-of-distribution) Mass/Shape 실험으로 보고합니다).\n그 결과, LLM-설계 보상으로 학습된 정책들은 기존 인간 보상 정책보다 뛰어난 성능을 보였습니다. 예를 들어, GPT-4 기반 보상 함수로 학습한 정책은 인간 보상 대비 회전 속도가 빠르고(에피소드당 평균 5.2회 vs 4.7회), 물체를 더 오래 떨어뜨리지 않고 유지했습니다. 여러 LLM 중에서도 Gemini-1.5 (구글 Gemini 모델) 기반 보상이 가장 많은 회전을, GPT-4 기반 보상이 가장 높은 성공 확률을, DeepSeek 기반 보상이 가장 긴 에피소드 지속시간을 보여주는 등 모델별 특성이 나타났으나, 모든 LLM 보상 정책이 일관되게 인간 보상 정책을 상회했습니다. 특히 Deepseek-R1 모델로 생성된 보상을 사용할 경우, 현실 실험에서 회전 횟수가 38% 증가하고 실패하기 전 유지 시간도 25% 늘어났다고 보고되었습니다. 한편, 인간이 공들여 설계한 기준 보상(Baseline)은 매우 보수적으로 동작하여 물체를 떨어뜨리는 경우는 적었지만 회전 속도가 느렸고, LLM 보상 기반 정책들은 다소 공격적이지만 빠르게 회전시키면서 미끄럼이 감지되면 즉각 대응하는 민첩한 조정 능력을 보였습니다. 이러한 차이는 손바닥을 위로 향한 상태에서 오리 인형을 돌리는 동작 등에서 두드러졌는데, LLM 보상 정책은 살짝 미끄러지면 빠르게 자세를 고쳐 잡고 계속 회전한 반면, 인간 보상 정책은 아예 미끄럼이 안 생기도록 천천히 신중히 돌리는 전략을 취해 효율이 낮았습니다.\n마지막으로, 저자들은 LLM 보상 함수의 복잡도를 정량적으로 비교했습니다. 흥미롭게도, LLM이 출력한 보상 코드는 매우 단순하고 짧았습니다. 예를 들어 GPT-4가 설계한 최적 보상 함수는 코드 본문이 약 30줄에 불과하고 사용한 상태 변수도 8개 정도뿐이었습니다. 반면, 인간 전문가가 만든 기존 보상 함수는 각종 세부 항목(물체의 각도 정렬 항목, 접촉 안정화 항목, 속도 부드러움 항목 등)을 모두 합쳐 수백~수천 줄의 코드와 100개 이상의 변수로 이루어진 매우 복잡한 형태였습니다. 그럼에도 불구하고 LLM 보상은 동일한 핵심 아이디어들을 겹치지 않고 깔끔하게 구현해냈으며, 결과적으로 짧은 코드로 더 나은 성능을 얻어낸 것입니다. 저자들은 이러한 간결성과 가독성이 향후 보상 함수 설계의 유지보수 측면에서도 큰 이점이 될 것으로 언급했습니다."
  },
  {
    "objectID": "posts/paper/2025-09-19-text2touch.html#기존-연구와의-차별점-및-관련-연구-비교",
    "href": "posts/paper/2025-09-19-text2touch.html#기존-연구와의-차별점-및-관련-연구-비교",
    "title": "📃Text2Touch 리뷰",
    "section": "기존 연구와의 차별점 및 관련 연구 비교",
    "text": "기존 연구와의 차별점 및 관련 연구 비교\n본 연구는 강화학습의 보상 설계 문제와 로봇의 섬세한 손동작 학습이라는 두 영역의 최첨단을 교차하는 지점에 위치합니다. 선행 연구들과 비교하여 Text2Touch가 가지는 차별점은 다음과 같습니다.\n\nLLM을 통한 보상 자동화 연구의 확장: 최근 몇 년간 LLM을 이용해 로봇의 보상 함수를 자동 생성하려는 시도가 등장했습니다. 예를 들어 Eureka라는 연구에서는 GPT-4에게 보상 함수를 작성하고 개선시키게 하여, 시뮬레이션 상의 여러 작업에서 사람보다 나은 보상을 설계한 바 있습니다. 또한 사용자가 서술한 과제 내용을 코드로 변환해 로봇을 제어하거나, LLM이 스스로 진행률 지표나 보상 구성 요소를 설계하도록 한 사례들도 있었습니다. 그러나 이러한 선행 연구들은 주로 시뮬레이터 내부의 단순 센서 정보(로봇의 관절 상태나 위치 정보 등)만 사용하거나, 현실 실험을 하더라도 시각 카메라나 관절 센서 정도에 한정된 환경이었습니다. Text2Touch는 촉각 센서라는 고차원 감각 정보를 보상 설계에 도입함으로써, LLM 보상 자동화 연구의 범위를 새로운 센서 모달리티로 확장했다는 의의가 있습니다. 이는 이전까지 LLM이 다루지 않던 풍부한 접촉 정보까지 포함하도록 프롬프트 구성과 모델 활용을 정교화했다는 점에서 고유한 기여입니다.\n최첨단 손내 조작 기술과의 결합: 로봇 손으로 물체를 자유롭게 회전시키는 문제는 최근 크게 주목받아 왔습니다. 기존 연구들에서는 단일 축으로 살짝 돌리기나 손바닥 위에 공 균형 잡기처럼 제한된 과제를 다루거나, 물체를 잡았다 놓았다 반복하며 목표 자세로 재배치하는 등 간소화된 문제 설정이 많았습니다. 2023~2024년경에 이르러서야 비로소 AnyRotate나 General In-Hand Rotation과 같은 연구에서 중력의 영향을 받지 않는(hand orientation 무관) 다축 회전 과제가 가능해졌습니다. 예를 들어 Yang 등의 AnyRotate 연구는 시뮬레이션과 실제 로봇 모두에서 첫 번째로 중력-무관 다축 회전을 시현하며, 이를 위해 풍부한 촉각 센싱이 필수적임을 보여주었습니다. Qi 등이 수행한 연구에서는 비전+촉각을 결합해 일반적인 물체 회전을 달성하기도 했습니다. 그러나 이들 선행 성과들은 모두 전문가가 신중히 설계한 보상 함수에 의존하고 있었습니다. 보상 함수를 사람이 일일이 튜닝하는 작업은 매우 느리고, 사람의 직관에 따른 편향이 개입되기 쉽다는 한계가 지적됩니다. 반면 Text2Touch는 기존 최첨단 손내 조작 결과들을 LLM 활용을 통해 한 단계 발전시켰습니다. 자동 설계된 보상으로도 인간 설계 보상 만큼이나 복잡한 목표를 달성할 수 있음을 보였을 뿐 아니라, 오히려 성능 면에서 더 우수함을 입증했습니다. 이는 인간 전문가의 개입을 줄이면서도 최첨단 결과를 경신했다는 점에서 큰 차별화 포인트입니다. 나아가, 본 연구는 시뮬레이션-현실 사이의 격차를 메우는 과정에서도 LLM 보상이 통용될 수 있음을 처음으로 확인하였습니다. 이전까지 자동 생성된 보상이 현실 로봇에서 성공적으로 쓰인 사례는 거의 없었는데, Text2Touch는 촉각 센서까지 포함된 복잡한 현실 상황에서도 LLM 보상이 유효함을 보여준 것입니다.\n멀티모달 로봇 학습에서의 활용 가능성: 일부 다른 연구들은 LLM을 로봇 제어의 고수준 플래너로 활용하거나, 거대한 비전-언어 모델로 로봇의 다중센서 피드백을 통합하려는 시도를 해왔습니다. 예를 들어, LLM이 주어진 목표를 달성하기 위한 단계별 로봇 행동 시퀀스를 생성한다든지, 사전 학습된 거대 모델을 통해 시각과 텍스트 정보를 결합한다든지 하는 접근입니다. 그러나 이런 방법들은 주로 이미 학습된 하위 제어기를 가정하거나, 대규모 비전 모델을 필요로 하여 계산 비용이 막대하다는 문제가 있었습니다. Text2Touch는 반대로 학습되지 않은 새로운 기술(skill)을 바닥부터 훈련하면서, LLM을 보상 설계라는 한정된 목적에 활용함으로써 보다 경량의 접근을 취했습니다. 또한 자동 커리큘럼 설계나 환경 생성 기법 등도 로봇 학습을 돕는 다른 방안으로 연구되고 있으나, 본 논문에서는 고정된 커리큘럼을 사용하여 보상 함수의 영향만을 엄밀히 비교했습니다. 이러한 실험 설계는 LLM 보상 설계의 효과를 정확히 검증하는데 주력한 것으로, 멀티모달 로봇 학습의 한 요소기술로 LLM을 자리매김시킨다는 의미가 있습니다.\n\n요약하면, Text2Touch는 “촉각까지 아우르는 LLM 기반 보상 자동화”라는 독자적인 아이디어를 통해, 기존 인간 전문가 기반의 방법론을 능가하는 새로운 로봇 학습 패러다임을 제시했다는 점에서 관련 연구들과 차별화됩니다."
  },
  {
    "objectID": "posts/paper/2025-09-19-text2touch.html#논문의-장점과-한계점",
    "href": "posts/paper/2025-09-19-text2touch.html#논문의-장점과-한계점",
    "title": "📃Text2Touch 리뷰",
    "section": "논문의 장점과 한계점",
    "text": "논문의 장점과 한계점\n\n장점\n이 논문의 가장 큰 강점은 높은 효율성과 성능을 둘 다 잡았다는 것입니다. 인간 전문가가 며칠씩 고민하며 만들 법한 복잡한 보상 함수를 LLM이 짧은 시간에 자동 생성했고, 그 결과로 나온 정책이 성능까지 더 좋았습니다. 이는 로봇 학습에서 개발 사이클을 획기적으로 단축시킬 잠재력을 보여줍니다. 저자에 따르면, LLM을 활용하면 새로운 과제에 대해 “개념에서 배포까지” 걸리는 시간을 크게 줄일 수 있다고 합니다. 둘째, LLM이 만들어낸 보상 함수는 코드가 단순명료하여 해석 가능성(interpretability)이 높습니다. 복잡한 보상은 종종 의도치 않은 부작용을 일으키지만, Text2Touch의 자동 보상들은 사람이 읽어봐도 이해하기 쉬운 형태였고, 이는 추후 보상 구조를 수정하거나 디버깅하기에도 용이합니다. 셋째, 실제 로봇 실험으로 검증했다는 점도 강점입니다. 시뮬레이션 상의 좋은 결과가 현실에서도 이어질 수 있다는 것을 촉각 센싱이라는 까다로운 조건 하에 증명한 것은 큰 성과입니다. 넷째, 다양한 LLM 모델(예: GPT-4, Gemini, Llama 등)을 시도하고 서로 비교分析함으로써, 어떤 모델이 왜 더 나은 보상을 설계하는지 통찰을 제시하려 한 점도 눈에 띕니다. 마지막으로, 강화학습 커리큘럼과 지식 증류 등 기존 로봇학습 기법들과 LLM 보상 설계를 잘 통합하여 안정적인 학습을 구현한 것도 장점입니다. 예컨대, LLM 보상만 바꾸고 나머지 조건은 동일하게 통제함으로써 공정한 비교를 했고, 결과적으로 LLM 보상의 이점이 명확히 드러났습니다.\n\n\n한계\n반면, 몇 가지 제한사항도 존재합니다. 첫째, 프롬프트 엔지니어링에 대한 의존성입니다. LLM이 유용한 보상 함수를 만들어내기까지 저자들은 프롬프트 설계를 여러 번 수정해야 했습니다. 단순한 설명으로는 LLM이 문제를 제대로 이해하지 못했고, 지나치게 상세한 프롬프트는 오히려 편향된 보상을 만들었다고 합니다 (예: 특정 성능 지표만 극대화하려는 편향). 결국 좋은 결과를 얻으려면 여전히 인간의 프롬프트 설계 노하우가 필요하며, 이는 완전한 자동화라고 보기 어렵습니다. 둘째, LLM 자체에 대한 의존성입니다. 실험 결과를 보면 거대 모델(GPT-4 등)은 우수한 보상을 찾았지만, 작은 모델(o3-mini 등)은 성능이 떨어졌습니다. 이는 결국 현 시점에서는 성능 좋은 대형 LLM (종종 유료 서비스)을 필요로 한다는 뜻이며, 모델 접근성이 제한될 수 있습니다. 셋째, 현 방법은 보상 함수 설계만 자동화한 것이지, 정책 학습 자체의 시간/비용은 그대로 듭니다. 시뮬레이터에서 여러 후보 보상을 테스트하려면 그만큼 다수의 RL 훈련을 병렬로 돌려봐야 하고, 최종 선발된 보상으로 다시 충분한 학습과 현실 도메인 적응 단계를 거쳐야 합니다. 논문에서도 5종 LLM * 4종 프롬프트 전략 = 20여 개 보상 함수를 실험한 것으로 나오는데, 이는 상당한 계산 자원을 요합니다. 넷째, 일반화 범위의 한계입니다. 본 연구는 “물체 회전”이라는 비교적 명확한 목표에 집중했습니다. 다른 유형의 과제(예: 조립 작업이나 도구 사용 등)는 보상 구조가 더욱 복잡할 수 있는데, LLM이 이런 복잡한 목표까지 제대로 보상으로 표현할 수 있을지는 미지수입니다. 또한 촉각 이외의 센서(예: 소리, 온도 등)를 다루거나, 로봇 팔과 손의 협조 제어처럼 범위가 넓어지면 현 기법을 그대로 적용하기 어려울 수 있습니다. 저자들도 미래 과제로 새로운 작업과 센서로의 일반화 문제를 언급하며, 복잡한 다단계 작업에 대해서는 추가 연구가 필요함을 인정했습니다. 다섯째, 안전성 및 현실 구현 이슈도 있습니다. LLM이 생성한 보상 함수가 로봇 하드웨어에 무리를 주는 행동(예를 들어 과도한 힘을 가하도록 유도)으로 이어질 가능성도 배제할 수 없습니다. 이를 막으려면 프롬프트나 환경에서 별도의 제약을 걸어야 하는데, 이런 부분은 논문에서 크게 다루지 않았습니다. 끝으로, 이 접근법이 진정 인간 전문가의 완전한 대체가 될 수 있을지는 아직 열린 질문입니다. 보상 설계 이외에도 하이퍼파라미터 튜닝, 알고리즘 선택 등 사람의 개입 지점이 남아 있고, LLM이 항상 최적의 보상을 만들어준다고 보장할 수도 없습니다.\n그럼에도 불구하고, Text2Touch는 현재 한계들에도 불구하고 LLM 활용의 실질적 유용성을 로봇 분야에 증명한 사례로 평가할 수 있습니다. 보상 함수 설계처럼 사람의 노력이 많이 드는 문제에 AI 조수를 투입함으로써 성과를 높인 선구적인 예시이며, 향후 이를 개선·확장하는 연구가 다수 뒤따를 것으로 기대됩니다."
  },
  {
    "objectID": "posts/paper/2025-09-19-text2touch.html#마무리",
    "href": "posts/paper/2025-09-19-text2touch.html#마무리",
    "title": "📃Text2Touch 리뷰",
    "section": "마무리",
    "text": "마무리\n사람이 눈을 감고도 손안에서 물체를 이리저리 굴릴 수 있는 건, 촉각으로 느끼는 정보 덕분입니다. 로봇도 카메라로 보는 것만으로는 한계가 있어서, 손가락에 작은 카메라와 젤리 같은 센서를 달아 물체의 미끄러짐이나 힘을 “촉감”으로 느낄 수 있게 만들었습니다. 이제 로봇에게 “이 물체를 손 안에서 돌려봐”라고 가르치려면, 로봇이 어떨 때 잘했고 어떨 때 못했는지 채점을 해줘야 합니다. 이 채점 기준을 강화학습의 보상 함수라고 부릅니다. 예컨대 “물체를 많이 회전하면 점수를 높게 주고, 떨어뜨리면 큰 벌점을 준다” 같은 규칙이 보상 함수에 해당합니다. 문제는, 이렇게 점수를 매기는 규칙을 사람이 일일이 만들어주기가 무척 어렵다는 점입니다. 너무 단순하게 주면 로봇이 엉뚱한 행동을 하게 되고, 세세하게 짜주자니 경우의 수가 너무 많아 복잡해집니다.\n여기서 대규모 언어 모델(LLM), 쉽게 말해 똑똑한 인공지능 비서에게 도움을 청해보자는 게 이 논문의 아이디어입니다. 사람 대신 “이러이러하게 점수를 매겨줘” 하고 LLM에게 시키는 것이죠. 연구자들은 로봇 손의 상태를 나타내는 여러 숫자들(센서 값들)을 LLM에게 설명해주고, “목표는 물체를 잘 돌리는 것이다”라고 알려준 다음 알맞은 채점 프로그램(보상 함수)을 코드로 써달라고 부탁했습니다. 그러자 놀랍게도, LLM이 사람 전문가가 짠 것보다 짧고 간단한 채점 프로그램을 만들어냈습니다. 예를 들어 사람은 수십 가지 조건을 고려해 1000줄이 넘는 복잡한 코드를 짰는데, LLM은 불과 30줄 남짓한 코드로 핵심을 표현해낸 것이죠. 물론 그냥 한 번에 성공한 것은 아니고, 연구자들이 “이 부분은 보너스로 줘”, “이 상황엔 패널티를 빼먹지 마” 등 몇 가지 힌트를 주면서 여러 번 시도하여 얻은 결과입니다.\n이렇게 AI가 만들어준 보상으로 로봇을 학습시켜 봤더니, 로봇이 물체를 더 빨리 그리고 안정적으로 돌리는 것을 확인했습니다. 원래 사람이 만든 보상으로 학습시킨 로봇은 너무 조심스럽게 움직여서 시간도 오래 걸리고 회전 횟수도 적었는데, AI 보상으로 학습한 로봇은 약간 과감하게 움직이면서도 물체를 떨어뜨릴 것 같으면 재빨리 자세를 고쳐 잡는 영리한 행동을 보였습니다. 그 결과 회전 속도가 이전보다 약 30~40% 빨라졌습니다. 쉽게 말해, AI가 채점 기준을 잘 만들어주니 로봇 학생이 더 훌륭하게 과제를 해낸 셈입니다.\n이 연구가 의미하는 바는, 앞으로 로봇공학 분야에서도 복잡한 설정값이나 규칙들을 사람이 일일이 손대기보다 AI의 도움을 받아 자동 생성하는 방향으로 나아갈 수 있다는 것입니다. 특히 다양한 감각(시각, 촉각 등)을 활용하는 어려운 문제에서도 효과가 입증되었기에, 추후 가사로봇이나 제조로봇이 스스로 배워나갈 때 인간의 부담을 덜어줄 기술로 발전할 수 있습니다. 물론 아직은 사람이 프롬프트를 잘 짜줘야 하고, 모든 과제에 다 적용되지는 않겠지만, Text2Touch는 로봇에게 “이렇게 하면 잘했어!”라고 알려주는 방법을 똑똑한 AI에게 맡겨본 흥미로운 시도이자 한 걸음 진보라 할 수 있습니다."
  },
  {
    "objectID": "posts/paper/2025-06-09-geort.html",
    "href": "posts/paper/2025-06-09-geort.html",
    "title": "📃GeoRT 리뷰",
    "section": "",
    "text": "Project Homepage\nPaper"
  },
  {
    "objectID": "posts/paper/2025-06-09-geort.html#소개-인간-손-동작을-로봇-손으로-직관적으로-전달하려는-노력",
    "href": "posts/paper/2025-06-09-geort.html#소개-인간-손-동작을-로봇-손으로-직관적으로-전달하려는-노력",
    "title": "📃GeoRT 리뷰",
    "section": "2.1 소개: 인간 손 동작을 로봇 손으로 직관적으로 전달하려는 노력",
    "text": "2.1 소개: 인간 손 동작을 로봇 손으로 직관적으로 전달하려는 노력\n로봇 원격 조작(teleoperation) 기술은 사람의 섬세한 손동작을 로봇 손으로 전달함으로써, 복잡한 조작 데이터를 수집하거나 위험한 작업을 대행하는 데 필수적인 요소입니다. 특히 손 기구학 리타게팅(kinematic retargeting)은 사람의 손 제스처를 로봇 손의 자세로 변환하는 핵심 과정으로, 사용자가 로봇을 자연스럽게 제어하도록 해줍니다. 그러나 사람 손과 로봇 손의 형태 및 관절 구조 차이로 인해 효과적인 리타게팅 함수를 정의하기가 매우 어렵습니다. 손가락 길이, 관절 가동범위 등이 다르기 때문에, 어떤 기준으로 사람 손동작을 로봇 손동작에 대응시킬지 명확한 해답이 없습니다. 실제로 수많은 매핑 방법이 가능하지만 그중 어느 것이 인간의 의도를 가장 잘 반영하면서도 로봇의 자연스러운 움직임을 유지하는지 합의된 해법은 없는 상태입니다.\n기존 접근법들은 주로 휴리스틱한 과제 벡터(task vector) 설정에 의존해 왔습니다. 예컨대 사람 손의 특정 키포인트(keypoint)를 로봇 손의 특정 지점에 1대1로 맞추고, 각 축마다 스케일이나 오프셋을 조정하는 선형 매핑 공식을 사용하는 식입니다. 하지만 이런 방식은 조정해야 할 하이퍼파라미터가 매우 많고, 개인별로 값이 달라 일일이 보정(calibration)해야 하는 번거로움이 있습니다. 또한 단순 선형 매핑으로는 사람 손과 로봇 손 공간의 비선형적 차이를 포착하기 어렵습니다. 실제 논문에서도 인간 손가락 끝의 동작 범위와 로봇 손가락 끝의 동작 범위를 비교해보면, 인간 손의 구성 공간이 곡률을 띠며 좁은 반면 로봇 손은 보다 넓고 규칙적인 형태를 보여 선형 관계로 겹치지 않는다고 지적합니다. 이러한 차이 때문에 기존 선형 매핑은 대응 관계를 정확히 재현하지 못하고, 결과적으로 로봇 손의 일부 동작 공간만 제한적으로 활용되는 문제가 있습니다. 요약하면, 사람의 의도를 잃지 않으면서 로봇의 가용 범위를 최대화할 수 있는 보다 원리적인(retargeting) 기준과 기법이 요구되어 왔습니다.\n이번에 소개하는 “Geometric Retargeting” (GeoRT) 알고리즘은 이러한 배경에서 제안된 최신 연구로, 초당 1000Hz 수준의 초고속 동작 변환과 원리에 기반한 명확한 목표 설정을 통해 이 문제를 해결하고자 합니다. 이 리뷰에서는 해당 논문의 주요 기여와 내용을 기술적으로 분석합니다. 먼저 논문의 핵심 기여를 정리한 뒤, GeoRT 알고리즘의 구조와 수학적 원리를 상세히 설명하고, 기존 작업들과의 비교를 통해 혁신성을 평가하겠습니다. 또한 실험 결과를 살펴보고 이 연구의 의의, 한계 및 향후 연구 방향에 대해 논의합니다."
  },
  {
    "objectID": "posts/paper/2025-06-09-geort.html#주요-기여",
    "href": "posts/paper/2025-06-09-geort.html#주요-기여",
    "title": "📃GeoRT 리뷰",
    "section": "2.2 주요 기여",
    "text": "2.2 주요 기여\n논문에서 저자들은 GeoRT를 통해 다음과 같은 두 가지 핵심 기여를 이루었다고 요약합니다:\n\n원칙적인 손 리타게팅 목표 함수 제시: 사람 손동작을 로봇 손으로 변환하는 데 필요한 근본 기준(criteria)들을 기하학적으로 정의하여, 이를 학습형 모델의 손실 함수로 활용할 수 있게 하였습니다. 이로써 인간-로봇 손동작 사이의 대응을 수치적으로 명확히 규정하고, 기존의 복잡한 휴리스틱 대신 체계적인 목표 하에 모델을 학습시킬 수 있습니다.\n초고속 신경망 리타게팅 시스템 구현: 상기한 기하학적 목표들을 기반으로 경량의 신경망 모델을 설계 및 훈련하여, 1kHz(초당 1000회) 수준의 실시간 성능과 최첨단 수준의 정확도를 달성했습니다. 제안된 시스템은 필요한 하이퍼파라미터 수를 크게 줄였으며, 실제 원격 조작 실험에서 기존 방식 대비 향상된 작업 성공률과 속도를 보여주었습니다. 또한 이 접근법은 테스트 시 별도의 복잡한 최적화 절차가 필요 없으므로, 확장성과 실시간 운용성 면에서 뛰어납니다."
  },
  {
    "objectID": "posts/paper/2025-06-09-geort.html#geometric-retargeting-알고리즘-기하학적-목표와-학습-구조",
    "href": "posts/paper/2025-06-09-geort.html#geometric-retargeting-알고리즘-기하학적-목표와-학습-구조",
    "title": "📃GeoRT 리뷰",
    "section": "2.3 Geometric Retargeting 알고리즘: 기하학적 목표와 학습 구조",
    "text": "2.3 Geometric Retargeting 알고리즘: 기하학적 목표와 학습 구조\n\n2.3.1 리타게팅을 위한 기하학적 설계 원칙\nGeoRT 알고리즘의 핵심은 “기하학적 목표 함수”들을 정의하여 사람이 직관적으로 기대하는 동작 대응 특성을 수식으로 표현하고, 이를 신경망 학습의 지도신호로 삼는 것입니다. 저자들은 이상적인 손동작 매핑이 갖추어야 할 다섯 가지 기준을 제시하는데, 이를 각 손가락별로 적용되는 다섯 가지 손실 함수로 구현했습니다. 다섯 가지 기준(criteria)과 그 직관적인 의미는 다음과 같습니다:\n\n운동 보존 (Motion Preservation): 사람 손가락 끝이 어떤 방향으로 움직일 때, 로봇 손가락 끝도 동일한 방향으로 움직여야 한다는 원칙입니다. 사용자가 손가락을 어느 방향으로 움직이면 로봇 손가락도 그 움직임 방향을 따라갈 것이라고 기대하기 때문에, 매핑 함수는 국소적인 운동 방향을 보존해야 합니다. 이를 위해 사람 손가락의 현재 자세에서 작은 변화 \\delta를 주었을 때 로봇 손 끝의 변화 방향이 \\delta와 평행하도록 유도하는 손실 함수를 정의합니다. 이 기준을 통해 미세 조작 시의 직관성을 보장합니다.\n구성 공간 커버리지 (C-space Coverage): 사람 손가락을 최소 위치부터 최대 가동 범위까지 움직였을 때, 로봇 손가락도 전 범위에 걸쳐 대응하도록 한다는 원칙입니다. 즉 사람의 입력 동작 범위 전체가 로봇 출력 공간의 모든 유효 범위에 매핑되어, 로봇 손의 가용 움직임 능력을 남김없이 활용하도록 합니다. 이상적으로 매핑 함수 f가 인간 손 공간 S^h에서 로봇 손 공간 S^r로의 전단사 함수(특히 전사, surjection)가 되길 요구하지만, 이를 직접 달성하기 어렵기 때문에 저자들은 챔퍼(Chamfer) 손실을 사용한 근사 방법을 제안했습니다. 즉, 매 미니배치마다 인간 손 공간에서 샘플링한 점들과 로봇 손 공간에서 샘플링한 점들을 비교하여 양 집합 간 거리를 최소화하는 챔퍼 손실 L_{\\text{cov}}를 계산함으로써, 로봇 공간에 인간 동작의 사상이 고르게 퍼지도록 유도합니다. 이로써 로봇 손가락 끝 구성 공간의 미커버 영역(uncovered space)을 줄이게 되며, 사람 동작이 로봇 손의 전체 범위를 빠짐없이 활용하도록 합니다.\n높은 평탄성 (High Flatness): 매핑 함수의 민감도가 입력 전역에서 일정해야 한다는 원칙입니다. 사람 손의 동일한 움직임 변화가 어느 범위에서든 로봇 손의 유사한 크기의 변화로 이어지도록, 균일한 응답 특성을 추구합니다. 예를 들어 어떤 구간에서는 입력을 조금만 바꿔도 로봇 손이 크게 움직이고, 다른 구간에서는 같은 입력 변화에 로봇이 미세하게 반응한다면 사용자는 어느 구간에서는 로봇이 둔감하고, 다른 구간에서는 과민하다고 느끼게 될 것입니다. 이를 방지하기 위해 GeoRT는 매핑 함수의 곡률(curvature)을 낮추는, 쉽게 말해 2차 미분이 0에 가깝도록 만드는 손실 함수를 도입했습니다. 구현상으로는 각 손가락 자세를 약간씩 변화시킨 두 가지 입력에 대해 유한 차분으로 로봇 출력 변화를 비교하고, 출력의 이차 변위가 0에 수렴하도록 하는 방식으로 평탄성 손실 L_{\\text{flat}}를 계산합니다. 이 지역적 선형성 제약을 통해 매핑이 전 구간에서 예측 가능하고 균일한 비율로 작동하게 됩니다.\n핀치 대응 (Pinch Correspondence): 사람 손가락들 사이에 집는 동작(pinch grasp)이 발생할 때 로봇 손에서도 동일한 핀치 동작이 일어나야 한다는 기준입니다. 예를 들어 사람의 엄지-검지가 집게처럼 모여 물체를 집는다면, 로봇 손도 같은 손가락 쌍으로 집게 동작을 취해야 합니다. 이는 사용자가 로봇을 자기 손처럼 느끼게 하는 에이전시(agency) 감각에 매우 중요하지만, 앞선 기준들만으로는 엄밀히 보장되지 않을 수 있습니다. 따라서 GeoRT는 엄지와 다른 손가락 간 거리를 모니터링하여, 사람이 일정 임계값 이하로 손가락을 모으면(예: 1cm 이하) 로봇에서도 해당 손가락 간 거리가 가까워지도록 강제하는 핀치 손실 L_{\\text{pinch}}를 추가했습니다. 이 제약으로 사람-로봇 손 간 집기 동작의 일치도를 높일 수 있습니다. 핀치 사례의 식별을 위해 사람에게 몇 가지 집기 동작을 미리 해보도록 하여 데이터를 모았으며, 약 5분 이내의 짧은 움직임 기록만으로도 충분했다고 합니다.\n비충돌성 (Collision-Free Retargeting): 사람이 손을 움직이는 동안 손가락들끼리 부딪치지 않는다면, 로봇 손 역시 자체 충돌(self-collision)이 없어야 한다는 기준입니다. 로봇 손가락끼리 엉키거나 충돌하면 작업에 지장을 줄 뿐 아니라 손상 위험도 있으므로, GeoRT는 최종적으로 충돌 억제 손실 L_{\\text{col}}을 포함시켰습니다. 구현상 물리 시뮬레이션을 통해 다양한 로봇 손 관절 구성과 그 충돌 여부를 미리 데이터로 모은 뒤, 신경망 충돌 판별기를 훈련하여 어떤 관절 상태가 충돌을 일으킬 확률인 C(q)를 예측하게 합니다. 학습 중에는 이 사전학습된 충돌 판별기를 통해 현재 로봇 자세 q의 충돌 확률에 비례하는 손실을 추가로 부여하여, 모델이 충돌 위험이 높은 출력을 피하도록 유도합니다. 다만 흥미롭게도, 저자들은 다른 손실들만으로도 어떤 로봇 손(Allegro 등)에서는 자체 충돌이 거의 발생하지 않는 결과가 나오기도 했다고 언급합니다. 그럼에도 완전성을 위해 충돌 회피 항목을 최종 포함했다고 합니다.\n\n以上의 다섯 가지 목표는 서로 독립적이며 리타게팅 문제를 정의하는 최소한의 제약이라고 저자들은 강조합니다. 실제로 일부 기준(I, II, III)을 만족한다고 해서 다른 기준(예: 운동 보존)이 자동 충족되지는 않으므로 각각의 항목이 필요합니다. 이처럼 간단하지만 원리에 충실한 다섯 가지 목표를 세움으로써, 더 이상 사람이 임의로 정한 복잡한 규칙 없이도 손 리타게팅의 품질을 수치적으로 명세화할 수 있게 되었습니다.\n\n\n2.3.2 신경망 구조와 학습 방법\n위 기준들을 실제 모델에 구현하기 위해, GeoRT는 입력으로 사람 손가락들의 위치(keypoint 좌표)를 받아 출력으로 로봇 손의 관절 각도를 내놓는 신경망 함수를 학습합니다. 보다 구체적으로, 각 손가락마다 개별적인 소형 신경망 f_i를 두어 손가락별 매핑을 수행하는 구조를 채택했습니다. 예를 들어 Allegro 로봇 손은 엄지 포함 4손가락으로 이루어져 있는데, 각 손가락마다 독립적인 다층 퍼셉트론(MLP) 네트워크를 할당하여 사람 손가락 끝 좌표를 해당 로봇 손가락의 관절 구동 위치로 변환하도록 합니다. 각 f_i의 출력층에는 Tanh 활성화 함수를 사용하고, 로봇 관절 범위에 맞춰 출력값을 -1~1로 정규화하여 표현합니다. 손가락별로 네트워크를 분리함으로써 학습이 단순해지고 병렬 처리가 가능해 속도 면에서 유리하며, 핀치 동작이나 충돌과 같은 상호작용은 앞서 정의한 제약 손실을 통해 조정합니다 (예: 핀치 손실은 엄지와 검지 네트워크 출력 간 거리를 연결). 이렇게 하면 모델 구조가 가벼워져, 추론시 연산량이 매우 적으므로 결과적으로 초당 1000회 이상의 갱신 주기를 쉽게 달성할 수 있었습니다.\n모델 학습은 완전 비지도학습(unsupervised)으로 이루어집니다. 즉, 사람이 직접 짝지은 입력-출력 데이터셋 없이, 앞서 정의한 다섯 가지 기하학적 손실 항목들의 합만을 최적화 기준으로 삼아 신경망의 가중치를 학습합니다. GeoRT의 전체 손실 함수 L_{\\text{total}}은 아래와 같은 형태로 구성됩니다:\n\nL_{\\text{total}} = \\lambda_1 L_{\\text{motion}} + \\lambda_2 L_{\\text{cov}} + \\lambda_3 L_{\\text{flat}} + \\lambda_4 L_{\\text{pinch}} + \\lambda_5 L_{\\text{col}}\n\n여기서 \\lambda_1 \\sim \\lambda_5는 각 손실의 가중치로, 논문에서는 경험적으로 4개의 하이퍼파라미터만 조정하면 충분하다고 설명합니다 (5개 중 일부는 단위 스케일에 따라 고정). 이는 이전 방식들이 사람 손가락마다 일일이 설정해야 했던 수많은 스케일, 오프셋 등의 조율 변수에 비하면 현저히 단순한 설정입니다. 저자들이 권장한 가중치 조합은 적당한 범위 내에서 결과에 큰 영향 없이 안정적으로 동작하였고, 이는 본 접근법의 매우 높은 실용성을 보여줍니다.\n학습 데이터 준비도 비교적 간단합니다. 로봇 손 공간 쪽은 시뮬레이터에서 로봇 손의 관절들을 무작위로 움직이며 얻은 손가락 끝 위치들의 포인트 클라우드로 샘플링하고, 인간 손 공간 쪽은 사용자에게 자유롭게 손가락을 움직이도록 (쭉 펴고 구부리기, 다양한 집기 동작 등) 5분간 요청하여 모션 캡처로 얻은 손가락 위치 데이터들을 사용합니다. 즉 수 분간 손을 이리저리 놀리며 손가락들의 전체 가용 범위를 탐색한 움직임 기록이 곧 학습에 필요한 인간 손 포인트 클라우드가 됩니다. 이렇게 수집된 인간/로봇 손 공간 표본들을 이용해 앞서 설명한 챔퍼 손실 등을 계산하고, 작은 무작위 자세 변화로 운동 보존 및 평탄성 손실을 계산하며, 일부 핀치 예로 핀치 손실을 적용하는 식으로 각 미니배치마다 손실을 산출합니다. 이때 로봇 손가락 끝 좌표를 계산하려면 출력 관절값에 대해 순방향 기구학(forward kinematics)을 수행해야 하는데, 이를 위해 로봇 손의 해석적 모델을 사용하거나 미리 학습된 미분가능한 신경망 forward 모델을 활용하였습니다. 또한 충돌 여부 판별을 위해 앞서 훈련된 충돌 판별기를 사용하지요. 이러한 부가 모델들(순방향 모델, 충돌 판별기 등)은 오직 학습 단계에서만 사용되고 추론시에는 필요 없으며, 손실의 그래디언트는 이들을 거쳐 신경망 f_i들까지 역전파됩니다. 최종적으로 경사하강법으로 신경망 파라미터를 갱신하면, 각 손실 항목들을 균형 있게 최소화하는 매핑 함수로 수렴하게 됩니다.\n흥미로운 점은, 이러한 학습이 아주 빠르게 완료된다는 것입니다. 저자는 지포스 RTX 3060 단일 GPU에서 3~5분 이내로 최적 학습이 끝났다고 보고합니다. 이는 비교적 간단한 MLP 구조와 소량의 데이터(수분간의 손동작)로 충분히 모델이 학습됨을 보여주며, 상황에 따라 새 사용자나 새 로봇 손에 대해 신속히 재학습하여 적용할 수 있음을 시사합니다. 결과적으로 학습이 완료된 GeoRT 모델은 사람 손의 키포인트 입력을 받아 즉각적으로 로봇 손의 목표 관절각을 출력하며, 별도의 복잡한 계산이나 최적화를 실시간 단계에서 수행하지 않으므로 지연 없이 초고속 응답이 가능합니다."
  },
  {
    "objectID": "posts/paper/2025-06-09-geort.html#기존-연구와의-비교-휴리스틱-매핑-vs.-원리-기반-학습",
    "href": "posts/paper/2025-06-09-geort.html#기존-연구와의-비교-휴리스틱-매핑-vs.-원리-기반-학습",
    "title": "📃GeoRT 리뷰",
    "section": "2.4 기존 연구와의 비교: 휴리스틱 매핑 vs. 원리 기반 학습",
    "text": "2.4 기존 연구와의 비교: 휴리스틱 매핑 vs. 원리 기반 학습\n사람 손에서 로봇 손으로의 동작 매핑은 오랫동안 다양한 방식으로 연구되어 왔습니다. 전통적인 접근법 중 하나는 조인트 공간 매핑(joint-space mapping)으로, 사람 손가락 관절 각도를 센서 장갑 등으로 읽어와 미리 정해둔 대응 관계에 따라 로봇 손 관절 각도로 직접 매핑하는 것입니다. 이러한 방법은 특정 경우(예: 로봇 손 구조가 인간 손과 거의 유사한 경우) 직관적이지만, 일반적으로는 사람 vs 로봇의 기구학 구조 차이 때문에 정밀한 제어가 어렵습니다. 관절 축 개수나 배치가 다르면 1:1 対응이 성립하지 않는 부분이 생겨 오차와 불안정성이 커지기 때문입니다.\n좀 더 보편적으로 쓰이는 방법은 직교 좌표계 매핑(cartesian mapping)으로, 사람 손의 손가락 끝 위치 같은 작업공간 좌표(task vector)를 로봇 손가락 끝이 따라가도록 하는 방식입니다. 예를 들어 사람 검지 끝 좌표를 로봇 검지 끝 좌표에 맞추고, 이 목표 위치에 해당하는 로봇 관절각은 역기구학(IK)으로 풀이하는 식입니다. 최근 많은 원격 조작 연구들이 이 키포인트 기반 매핑을 활용하여 어느 정도 성과를 내왔습니다. 그러나 이런 접근 역시 어떤 키포인트를 어떻게 매칭할지 정하는 일이 까다롭습니다. 단순히 선형 비례식 (Equation 1)으로 각 좌표축을 맞추는 방법이 흔하지만, 앞서 논의했듯이 이 경우 개별 축마다 원점 오프셋과 스케일 계수 등 수많은 파라미터를 보정해야 하고, 그럼에도 사람-로봇 손 공간의 비선형 차이를 커버하지 못해 부자연스러운 포즈가 유발될 수 있습니다. 실제 DexPilot이나 AnyTeleop과 같은 비전 기반 원격조작 시스템에서는 이러한 과제 벡터 매핑을 사용하였는데, 매 실험 전 긴 보정 과정이 필요하고도 세밀한 동작 재현에 한계를 보였습니다. 반면 GeoRT는 사람이 임의로 고안한 매핑 함수를 따르지 않고, 기계적으로 도출된 목표들을 통해 매핑 함수를 자동 학습합니다. 즉, 휴리스틱한 “이 손가락은 여기에 맞춘다”와 같은 규칙을 설계하지 않고도, 로컬 운동학 특성+글로벌 공간 매칭이라는 큰 틀에서 모델이 스스로 최적 매핑을 찾아내도록 한 것입니다. 그 결과 추가적인 튜닝 없이도 인간과 로봇 손의 자연스러운 対응 관계가 부상(emerge)한다는 점을 논문은 강조합니다.\n또 하나의 비교 축은 실시간 성능과 확장성입니다. 기존 많은 방법은 사람이 손을 움직일 때마다 실시간으로 역기구학 계산이나 최적화를 수행하여 로봇 관절각을 결정하므로, 응답 속도가 제한되고 연산 부하가 컸습니다. 일부 최신 연구는 학습 기반으로 매핑을 모색했지만 대개 사람-로봇 데이터 쌍이 필요하거나, 정책(Search) 최적화를 매 시간스텝에 수행하는 등 실용화에 장벽이 있었습니다. GeoRT의 경우 학습 단계에서 모든 계산을 끝마치고, 운영 단계에서는 신경망 순전파(forward)만 수행하므로 현격히 가볍습니다. 논문에서 비교한 DexPilot이나 AnyTeleop 시스템은 리타게팅 속도가 약 60–100Hz 수준인 반면 GeoRT는 1000Hz로 10배 이상 빠르며, Robotic Telekinesis와 같이 오프라인 학습을 거친 방법과 동등한 최고 속도를 유지합니다. 또한 GeoRT는 하드웨어 제약에 대한 의존성이 낮습니다. 센서 장갑, 비전 모션캡처, Leap Motion 등 어떤 손 추적 수단으로 사람 손 키포인트를 얻어도 동일하게 적용 가능하며, 로봇 손도 인간형 오형이라는 가정만 성립하면 모델 구조나 파라미터 수 변경 없이 적용할 수 있습니다. (물론 사람 손가락 수보다 로봇 손가락 수가 현저히 적거나 하면 핀치 対응 등을 새로 정의해야 하므로, GeoRT는 현재로서는 인간형 로봇 손에 초점을 맞춘 해법입니다.)\n마지막으로, GeoRT에서 제시한 기하학적 목표들은 특정 구현에 국한되지 않고 다른 맥락에도 활용 가능하다는 장점을 갖습니다. 예를 들어 본 논문에서는 물체를 직접 다루는 과업지향적(hand-object) 리타게팅은 다루지 않았지만, 저자들은 제안한 규제항(regularization)들을 기존 방법에 추가하여 손-물체 동시 매핑의 품질도 향상시킬 수 있을 것으로 언급합니다. 이는 GeoRT의 철학이 보편적인 형태의 손동작 対응 문제로 확장될 수 있음을 시사합니다. 실제 최근 연구 중에는 형상 대응(shape correspondence) 문제로 손 리타게팅을 바라보는 시도들도 있는데, GeoRT의 원리는 이러한 접근(예: 두 손의 표면을 사상하여 변형 에너지를 최소화하는 방법 등)과도 일맥상통하는 부분이 있습니다. 요컨대 GeoRT는 기존 방식들의 경험적 한계를 인식하고, 이를 체계적인 수학적 원칙으로 극복함으로써 한 단계 진일보한 손 리타게팅 해법이라 평가할 수 있습니다."
  },
  {
    "objectID": "posts/paper/2025-06-09-geort.html#실험-및-결과-분석",
    "href": "posts/paper/2025-06-09-geort.html#실험-및-결과-분석",
    "title": "📃GeoRT 리뷰",
    "section": "2.5 실험 및 결과 분석",
    "text": "2.5 실험 및 결과 분석\n\n2.5.1 시뮬레이션 평가: 부드러운 제어와 공간 활용도\n저자들은 먼저 가상 시뮬레이터에서 제안한 GeoRT의 리타게팅 품질을 정량 평가하였습니다. 이를 위해 두 가지 지표를 정의했는데, 하나는 운동 보존율이고 다른 하나는 공간 커버리지율입니다. 운동 보존율은 앞서 기준 I에 대응하는 지표로, 무작위로 다양한 손 자세와 그 주변의 작은 방향 변화를 샘플링하여 로봇 손끝 움직임이 사람 손끝 움직임과 얼마나 방향 정렬이 잘 되는지를 나타냅니다. 값은 0~1 사이이며 1에 가까울수록 모든 국소 움직임 방향이 완전히 일치함을 의미합니다. 공간 커버리지율은 기준 II에 대응하는 지표로, 충분히 많은 인간 손가락 포즈 표본들을 로봇 손가락 포즈로 변환했을 때 로봇 손 구성 공간 중 얼마나 넓은 영역을 덮었는지를 백분율로 나타낸 것입니다. 0%이면 인간 동작이 로봇 공간의 극히 일부만 사용함을, 100%이면 로봇 손의 전체 가동 범위를 빠짐없이 커버했음을 의미합니다.\nGeoRT와 기존 방식들을 이 두 지표로 비교한 결과, GeoRT는 운동 보존율 약 0.94로 기존 선형 매핑 기반 방법(약 0.73)보다 훨씬 높았으며, 로봇 공간 커버리지도 약 90%로 기존 방식(약 38%)보다 크게 향상되었습니다. 즉 훨씬滑らか(부드럽고 일관된) 제어 감각과 거의 전역에 걸친 로봇 공간 활용이 달성된 것입니다. 이러한 결과는 GeoRT가 명시적으로 최적화한 목표들과 정확히 부합하는 것이어서 놀랍지는 않지만, 제안된 기하학적 손실 설정이 제대로 효과를 발휘함을 입증합니다. 결국 GeoRT를 쓰면 사용자는 로봇 손의 최대 가용 범위를 활용하면서도 미세한 손동작까지 로봇에서 자연스럽게 재현할 수 있음을 시뮬레이션을 통해 확인한 것입니다.\n또한 흥미로운 질적 실험으로, 저자들은 특정 매핑 휴리스틱 없이도 GeoRT가 얼마나 그럴듯한 사람-로봇 対응을 학습하는지 관찰했습니다. 예를 들어 인간 손의 약지-검지 사이 핀치 동작 등은 기존 선형 매핑에서는 잘 구현되지 않았지만, GeoRT 모델은 이러한 세부적인 対응 관계도 목표 손실들만으로 스스로 발견해냈습니다. 그림 7의 사례들을 보면, GeoRT는 작업 벡터 간 일치 항을 전혀 쓰지 않고도 인간과 로봇 손가락 사이에 자연스러운 対응이 형성되는 것을 볼 수 있습니다. 이는 제안한 접근법이 사람의 손동작 의도를 충실히 살려낸다는 점을 보여주는 인상적인 결과입니다.\n\n\n2.5.2 실제 로봇 실험: 물체 잡기 성능 비교\nGeoRT의 성능은 실제 로봇 시스템 상에서도 검증되었습니다. 저자들은 Franka Panda 로봇 팔 끝에 Allegro 로봇 손을 장착하고, 사람은 한 손에 Manus VR 장갑(손가락 위치 트래킹)과 손목에 Vive 트래커(팔 동작 트래킹)를 착용하여 원격 조작을 수행하는 실험을 진행했습니다. 사람의 손가락 움직임은 Manus 장갑으로 읽어 GeoRT 모델의 입력으로 들어가고, 출력된 Allegro 손 관절 위치 명령은 PD 제어를 통해 로봇 손을 구동했습니다. 한편 사람 팔의 움직임은 Vive 트래커로 받아 로봇 팔의 손끝 위치를 따라가도록 제어함으로써, 사용자의 손 위치와 로봇 손 위치도 동기화시켰습니다. 이렇게 구성된 원격 조작 시스템으로 여러 가지 집기(grasping) 실험을 실시하여, GeoRT 방식과 기존 방식의 작업 성공률과 소요 시간을 비교했습니다.\n비교 대상으로는 앞서 언급된 선형 매핑 기반 방법을 두 가지 버전으로 사용했는데, 하나는 매 프레임 실시간으로 보정이 이뤄지는 온라인 버전이고 다른 하나는 고정된 보정값을 쓰는 오프라인 버전입니다. 평가 지표로는 한 번 시도로 물체 잡기에 성공하는 비율(One-trial success)과 성공적인 그립을 이루기까지 걸린 평균 시간을 측정했습니다. 그 결과 GeoRT를 사용한 경우 한 번에 잡기 성공할 확률이 87.5%로, 오프라인 선형 매핑(55.0%)이나 온라인 보정 매핑(42.5%)보다 훨씬 높았습니다. 특히 온라인 방식은 잦은 보정에도 불구하고 성공률이 오히려 떨어졌는데, 이는 프레임 간 가변적인 매핑으로 사용자가 적응하기 어려웠기 때문으로 보입니다. 반면 GeoRT는 항상 일관된 대응을 유지하므로 사용자가 빠르게 숙달되어 높은 성공률을 보인 것입니다. 또한 평균 작업 완료 시간도 GeoRT가 3.2초로, 기존 오프라인(9.0초)이나 온라인 방식(19.3초)에 비해 월등히 짧았습니다. 이는 GeoRT를 쓸 경우 사용자가 여러 번 잡으려고 시도하거나 미세 조정에 시간을 보낼 필요 없이, 한번에 신속하게 물체를 집어 옮길 수 있다는 의미입니다. 사람의 감각으로도 GeoRT 방식은 손끝 움직임이 매끄럽고 직관적이라 작은 물체를 집거나 섬세한 조작을 할 때도 어려움이 적었다고 합니다. 반대로 기존 선형 매핑 기반으로는 손가락 미세 제어가 어색해 작은 물체를 집기 상당히 힘들었다고 관찰되었습니다.\n 알레그로 로봇 손과 프랑카 팔로 구성된 실험 시스템을 이용해 다양한 섬세한 조작 작업을 원격 수행하는 장면. (위 왼쪽) 평면 위 물체를 집어드는 동작, (위 오른쪽) 드라이버로 나사를 조이는 동작, (아래 왼쪽) 작은 블록을 정밀하게 쌓는 동작, (아래 오른쪽) 주사기 형태의 도구를 잡고 제어하는 모습. 제안된 GeoRT 기반 원격 조작을 통해 사용자는 이와 같은 다양한 정밀 작업을 안정적으로 수행할 수 있었다.\n추가로, 저자들은 GeoRT 시스템으로 주어진 작업들을 얼마나 빠르게 연속 수행할 수 있는지도 데모를 보였습니다. 예컨대 여러 가지 물건 12개가 흩어진 테이블에서 이를 한 손으로 집어 모두 통에 담는 과제를 약 100초만에 완료하였는데, 이때 느린 로봇 팔 움직임이 병목이었을 뿐 손 자체의 동작은 대부분 한 번 시도로 성공했다고 합니다. 이는 GeoRT 기반 제어의 효율성을 방증하는 예로 볼 수 있습니다. 종합하면, 실제 로봇 실험에서 GeoRT는 기존 대비 월등히 높은 작업 성공률과 빠른 조작을 구현했으며, 특히 정밀한 그립 동작에서 사용자에게 향상된 제어감과 자신감을 제공함을 확인했습니다."
  },
  {
    "objectID": "posts/paper/2025-06-09-geort.html#연구-의의-한계-및-향후-전망",
    "href": "posts/paper/2025-06-09-geort.html#연구-의의-한계-및-향후-전망",
    "title": "📃GeoRT 리뷰",
    "section": "2.6 연구 의의, 한계 및 향후 전망",
    "text": "2.6 연구 의의, 한계 및 향후 전망\nGeometric Retargeting (GeoRT) 알고리즘은 사람 손동작을 로봇 손으로 전달하는 오랜 문제에 대해 명확한 원리와 실용적인 해법을 제시했다는 점에서 크게 주목받고 있습니다. 우선 이 연구의 의의를 짚어보면 다음과 같습니다:\n\n원리 기반의 정형화: 사람-로봇 손 매핑 문제를 다섯 가지 기하학적 기준으로 정량화함으로써, 감에 의존하던 휴리스틱 설계 대신 과학적이고 재현 가능한 방법론을 마련했습니다. 이는 향후 유사한 문제(예: 인간의 팔 동작을 로봇 팔에 매핑, 인간 걸음새를 로봇에 매핑 등)에도 응용될 수 있는 틀을 제공한 것입니다.\n실시간 성능과 범용성: 간결한 MLP 구조와 사전 학습된 보조 모듈들을 활용하여 1kHz급 실시간 동작 변환을 달성했고, 추가 최적화나 복잡한 연산 없이도 다양한 시나리오에 적용 가능한 경량 프레임워크를 구축했습니다. 이는 원격 조작 시스템을 대규모로 확장하거나, 로봇 제어의 내부 피드백 루프에 통합하는 등 응용 범위를 크게 넓혀줍니다. 실제 저자들은 GeoRT를 자사의 DexterityGen이라는 파운데이션 제어기와 결합하여, 사용자의 거친 원격 조작을 뒷단에서 미세 조정해주는 액션 보정 시스템을 구현하기도 했습니다. 초고속 매핑 덕분에 이러한 상위 제어와의 실시간 연동이 가능해진 사례입니다.\n휴먼-로봇 협업 경험 향상: 실험 결과에서 보았듯 GeoRT는 사용자로 하여금 자신의 손처럼 로봇 손을 직관적으로 조종할 수 있게 해줍니다. 이는 곧 원격 로봇 조작의 학습 부담을 줄이고 생산성을 높이는 효과로 이어집니다. 예컨대 비숙련자도 짧은 시간내에 섬세한 로봇 작업을 수행할 수 있고, 피로도도 낮아질 것으로 기대됩니다. 장기적으로 이런 기술이 발전하면 원격 의료 수술, 원격 제조 등에서 인간과 로봇의 상호작용 효율이 크게 향상될 것입니다.\n\n한편, GeoRT에도 한계와 도전 과제가 존재합니다. 첫째, 인간형 로봇 손에 최적화되어 있다는 점입니다. 논문에서도 전제 조건(A1, A2)으로 로봇 손이 인간 손과 구조적으로 유사하고 손가락 対应관계가 명확함을 가정하고 있습니다. 따라서 사람보다 손가락 수가 적거나 많은 로봇, 혹은 구조가 크게 다른 로봇(hand이 아닌 집게 형태 등)에는 그대로 적용하기 어렵습니다. 이러한 경우 対应관계를 정의하는 추가 연구나 다른 형태의 목표 함수가 필요할 수 있습니다. 둘째, GeoRT는 물체와 상호작용하지 않는 맨손의 매핑에 한정됩니다. 물체를 쥔 상태에서의 손가락 움직임이나, 도구 사용 등의 시나리오에서는 단순 손가락 끝 위치만으로 이상적인 매핑을 정의하기 어려울 수 있습니다. 향후에는 물체의 상태나 힘/촉각 정보까지 포함한 과업 지향적 리타게팅으로의 확장이 필요합니다. 다행히 제안된 기하학적 목표들은 이러한 세팅에서도 규제 항으로 응용될 수 있을 것으로 보입니다. 셋째, 현재 모델은 손가락별 독립적으로 구성되어 엄지-검지 핀치 외의 복잡한 다손가락 협응 동작에 대한 명시적 제약은 부족합니다. 예를 들어 세 손가락으로 동시에 물체를 파지하는 동작 등에서는 보다 전체 손의 통합적인 매핑 전략이 필요할 수 있습니다. 이를 위해 추후에는 손 전체를 입력으로 받아 전체 관절을 출력하는 통합 모델이나, 손가락 간 종속성까지 학습하는 구조로의 발전 가능성도 있습니다.\n또 다른 흥미로운 확장 방향은, GeoRT를 오프라인 모션 사본 생성이나 모방 학습 분야에 활용하는 것입니다. 예컨대 사람의 시연 동작(비디오 혹은 Mocap 데이터)을 로봇으로 재현하는 데에도 동일한 원리의 매핑이 쓰일 수 있습니다. 실제 Robotic Telekinesis 연구는 유튜브 영상의 인간 손동작을 로봇이 모방하도록 학습했는데, GeoRT의 손실 함수를 적용하면 이러한 크로스 도메인 모방 학습의 성능도 개선시킬 여지가 있을 것입니다. 마지막으로, GeoRT의 초고속 성질은 단순히 원격 조작뿐 아니라 휴머노이드 로봇의 실시간 모방 제스처, VR 아바타 손 움직임 자연화 등 다양한 실시간 인터랙티브 시스템에 기여할 수 있습니다.\n요약하면, Geometric Retargeting 알고리즘은 기존의 인간-로봇 손동작 매핑 문제에 이론과 구현 양면에서 혁신적인 솔루션을 제시했습니다. 사람의 감각적인 기대치를 수학적으로 풀어내고 이를 빠른 학습 모델로 실현함으로써, 원격 로봇 조작의 정확성, 속도, 직관성을 모두 끌어올렸습니다. 앞으로 남은 과제들은 이 접근을 보다 다양한 로봇 형태와 시나리오로 확장하고, 인간과 로봇 간 물리적 상호작용까지 포괄하는 방향으로 나아가는 것입니다. 이러한 후속 연구들이 이루어진다면, 한층 자연스러운 인간-로봇 협업 시대를 앞당길 수 있을 것으로 기대됩니다."
  },
  {
    "objectID": "posts/paper/2025-07-31-meta-wrist.html",
    "href": "posts/paper/2025-07-31-meta-wrist.html",
    "title": "📃sEMG-RD 리뷰",
    "section": "",
    "text": "Paper Link\nBlog Link"
  },
  {
    "objectID": "posts/paper/2025-07-31-meta-wrist.html#연구-배경-및-문제의식",
    "href": "posts/paper/2025-07-31-meta-wrist.html#연구-배경-및-문제의식",
    "title": "📃sEMG-RD 리뷰",
    "section": "1. 연구 배경 및 문제의식",
    "text": "1. 연구 배경 및 문제의식\n오늘날 컴퓨팅 기술의 발전과 함께 입력 장치의 한계가 꾸준히 제기되고 있습니다. 키보드와 마우스, 터치스크린 등 기존 인터페이스는 책상 위나 손에 쥐고 사용할 물리적 기기가 필요하며, 스마트폰이나 AR 글래스 같은 이동 환경에서는 사용에 제약이 있습니다. 예를 들어 증강현실(AR) 안경을 착용한 사용자가 길을 걸으며 정보를 얻거나 메시지를 보내고 싶어도, 주머니에서 스마트폰을 꺼내거나 공중에 손짓을 크게 해야 한다면 경험이 끊기고 불편함이 발생합니다. 카메라 기반 제스처 인식은 카메라 시야가 확보되고 조명이 충분할 때만 정확하고, 손동작이 미세하거나 가려지면 인식 성능이 떨어집니다. 음성 제어는 주변에 소리가 공개되어 프라이버시 문제가 있고 시끄러운 환경에서는 어려움이 있습니다. 한편, 뇌에 센서를 이식해 의도를 읽는 침습적 뇌-컴퓨터 인터페이스(BCI)는 최근 연구에서 놀라운 성과를 내기도 했지만, 고성능을 내려면 머리에 전극을 심는 수술이 필요하고 개인 맞춤 복잡한 보정 과정이 필수라는 현실적인 장벽이 있습니다.\n이런 배경에서 Meta Reality Labs(전 Facebook Reality Labs)는 차세대 입력 기술로 신경 신호를 이용한 손목밴드 인터페이스를 제시했습니다. 특히 2021년경부터 뇌에 전극을 삽입하는 방식 대신 손목의 근전도 신호를 사용하는 접근법에 집중해 왔습니다. 최근 2025년 7월에는 해당 연구의 핵심 성과를 담은 논문을 Nature 저널에 발표하여 학계의 동료 심사 검증까지 받았습니다. Meta는 이 연구에서 손목에 착용하는 표면 근전도(sEMG) 기반 신경모터 인터페이스가 “차세대 인간-컴퓨터 상호작용(HCI)의 열쇠”라고 강조합니다. 다시 말해 사용자의 손목 근육에 흐르는 전기 신호를 읽어 별도의 물리적 장치 없이도 컴퓨터나 AR 안경을 조작할 수 있다는 것입니다. 이는 사용자의 손짓이나 의도를 눈에 띄지 않게 파악해주므로, 키보드나 마우스를 대체하고도도 자연스럽고 은밀한 상호작용을 가능케 할 새로운 패러다임으로 기대되고 있습니다.\nMeta는 2024년 자체 행사(Connect)를 통해 Orion이라는 코드명의 자사 첫 AR 안경 프로토타입을 공개한 바 있는데, 여기에도 초기형 sEMG 손목밴드가 시연되었습니다. 이번 Nature 논문은 바로 그 손목밴드 기반 인터페이스 기술을 학술적으로 정리한 것으로, Orion AR 안경과 함께 일상적인 AR 상호작용을 구현하려는 노력의 연장선에 있습니다. 요약하면, 연구진의 문제의식은 “언제 어디서나 쓸 수 있고, 사용자 개개인 맞춤 보정이 필요 없으며, 생각이나 작은 손짓만으로도 입력이 가능한 범용 인터페이스”의 필요성이었습니다. 이러한 목표를 달성하기 위해 선택된 접근이 비침습적으로 손목 근육의 전기 신호를 읽어내는 sEMG와 머신러닝 기술의 결합이었다는 점이 본 연구의 출발점입니다."
  },
  {
    "objectID": "posts/paper/2025-07-31-meta-wrist.html#사용된-기술-개요-표면-emg-및-머신러닝-아키텍처-등",
    "href": "posts/paper/2025-07-31-meta-wrist.html#사용된-기술-개요-표면-emg-및-머신러닝-아키텍처-등",
    "title": "📃sEMG-RD 리뷰",
    "section": "2. 사용된 기술 개요 (표면 EMG 및 머신러닝 아키텍처 등)",
    "text": "2. 사용된 기술 개요 (표면 EMG 및 머신러닝 아키텍처 등)\n표면 근전도(surface EMG, sEMG)란 피부 표면에 전극을 부착하여 그 아래 근육에서 발생하는 미세한 전기 신호를 측정하는 기술입니다. 뇌가 손가락을 움직이라는 명령을 내릴 때, 해당 신경 신호가 근육을 수축시키며 전기적 활동을 일으키는데, sEMG는 이 근육 전기 신호의 패턴을 포착합니다. 손목에는 손가락과 손 움직임을 제어하는 여러 근육의 힘줄과 근육들이 지나가기 때문에, 손목 부위의 sEMG로 손 움직임 전반을 읽어낼 수 있습니다. 이는 마치 “손목에서 생각을 읽는” 것과 같으며, 손가락을 실제로 움직이지 않고 마음먹기만 해도 근육에 약한 신호가 흐르기 때문에 겉으로 티 나지 않는 입력도 잡아낼 수 있습니다. 이러한 sEMG 기반 인터페이스는 비침습적이어서 수술이 필요 없고, 시각적 제스처 인식에 비해 가려짐이나 어두운 환경의 영향을 받지 않는 장점이 있습니다.\n연구팀은 손목 시계 정도 크기의 무선 sEMG 손목밴드를 자체 개발했습니다. 이 기기는 48개의 전극 핀을 이용해 16채널 sEMG 신호를 수집하며, 2 kHz의 높은 샘플링 레이트와 2.46 μV\\_rms의 낮은 노이즈 수준을 자랑합니다. 전극 배열은 손목 둘레를 따라 근육 밀도가 낮은 부위를 피하면서 배치되어, 미세한 근육 전위(MUAP)까지 포착할 수 있도록 설계되었습니다. 기기는 착용자의 손목 굵기에 맞춰 4가지 크기로 제작되었고, 몇 초 만에 쉽게 착용(don)하거나 해제(doff)할 수 있을 정도로 편리성을 갖추었습니다. 또한 배터리로 4시간 이상 연속 동작하며 블루투스 무선 통신으로 컴퓨터와 데이터를 주고받아, 일상 생활 환경에서 사용 가능하도록 만들었습니다. 아래 사진은 연구에 사용된 sEMG 손목밴드 프로토타입의 모습입니다.\n\n\n\n\nMeta Reality Labs에서 개발한 sEMG 기반 손목밴드 프로토타입. 사용자의 팔목에 착용되어 있으며, 이 밴드의 내부 전극들이 손과 손가락을 움직일 때 발생하는 근육의 전기신호를 포착한다. 이렇게 수집된 신호를 무선으로 전송하고 머신러닝 알고리즘을 통해 실시간으로 해석함으로써, 사용자 의도를 컴퓨터 입력으로 변환할 수 있다.\n\n이 손목밴드에서 취득한 다차원 근전도 시계열 데이터를 실시간 해석하기 위해, 연구진은 딥러닝 기반의 머신러닝 아키텍처를 개발했습니다. 수천 명에 달하는 피험자들로부터 대량의 sEMG 데이터를 확보한 뒤, 이를 지도학습으로 신경망에 학습시켜 사용자 의도 → 컴퓨터 명령으로 변환하는 모델들을 만들었습니다. 모델 구조는 시계열 근육 신호의 특성을 잘 포착하도록 설계되었는데, 예를 들어 멀티채널 시계열 신호 처리에 특화된 1차원 컨볼루션 계층과 순환 신경망(LSTM) 계층을 쌓아 제스처를 분류하는 모델이 사용되었습니다. 또한 손목밴드의 위치가 조금 달라지거나 사용자마다 신호 패턴이 다른 문제를 해결하기 위해, 채널 회전 불변성 모듈 등 특수한 입력 전처리와 특징 추출 기법(MPF 특징)을 도입하여 모델이 잡음 요인에 영향을 덜 받도록 설계했습니다. 핸드라이팅(손글씨) 인식처럼 복잡한 연속적 동작을 해석하는 경우에는, 음성 인식 분야에서 쓰이는 CTC(Connectionist Temporal Classification) 학습 기법과 Conformer 기반 트랜스포머 신경망을 활용했습니다. 이는 정확한 타이밍 정렬 없이도 시계열 신호와 텍스트 결과를 학습시킬 수 있는 방법으로, 근전도 신호의 패턴을 문자 시퀀스로 변환하는 데 효과적이었습니다. 전체적으로 이러한 맞춤형 딥러닝 아키텍처 덕분에, 본 연구의 sEMG 해독 모델은 새로운 사람에게도 별도 보정 없이 동작할 수 있는 범용성과 다양한 손동작을 실시간 인식할 수 있는 표현력을 모두 갖추게 되었습니다."
  },
  {
    "objectID": "posts/paper/2025-07-31-meta-wrist.html#주요-실험-및-결과-요약",
    "href": "posts/paper/2025-07-31-meta-wrist.html#주요-실험-및-결과-요약",
    "title": "📃sEMG-RD 리뷰",
    "section": "3. 주요 실험 및 결과 요약",
    "text": "3. 주요 실험 및 결과 요약\n연구진은 개발한 sEMG 손목밴드와 해독 알고리즘의 성능을 평가하기 위해 세 가지 대표 과제를 설정했습니다. - 첫째는 손목 각도 제어 실험으로, 사용자가 손목을 위아래로 기울여 화면상의 커서를 1차원 이동시키는 연속 제어 과제입니다. - 둘째는 불연속 제스처 인식 실험으로, 화면에 제시되는 지시에 따라 손가락 모으기(pinching)나 손가락 튕기기(swipe) 등 9가지 손동작 제스처를 수행하면 이를 인식해내는 과제입니다. - 셋째는 손글씨 쓰기(handwriting) 실험으로, 화면에 단어가 주어지면 사용자가 실제 펜을 쥔 것처럼 손가락을 움직여 공중에 해당 단어를 쓰는 동작을 하고, 이를 근전도 신호로부터 해독하여 텍스트로 인식하는 과제입니다.\n이 실험들을 위해 연구팀은 총 수천 명 규모의 다양한 피험자를 모집하여 데이터를 수집했습니다. 예를 들어 손글씨 데이터의 경우 6,627명의 참가자가 한 손가락에 펜을 쥐었다고 가정하고 공중에 단어를 쓰는 동작을 하여 방대한 근전도 데이터셋을 구성했습니다. 제스처 인식의 경우 1인당 아홉 가지 손동작을 여러 번 반복하게 하여 각 제스처마다 수천 건의 시그널 샘플을 확보했습니다. 이렇게 대규모 데이터 수집이 가능했던 것은, 손목밴드가 비교적 착용이 간편하고 무선으로 동작하여 다양한 환경에서 많은 사람들에게 적용할 수 있었기 때문입니다. 또한 동일인이 손목밴드를 뗐다가 다시 착용하는 등 센서 위치 변화에 따른 데이터도 포함시켜, 모델이 이러한 변동성에 견딜 수 있도록 했습니다.\n수집된 데이터를 활용하여 오프라인(offline) 성능 평가와 온라인(online) 실시간 실험이 진행되었습니다. 먼저 오프라인 평가에서는 각 과제별로 훈련되지 않은 새로운 참가자의 데이터를 모델이 얼마나 정확히 해석하는지 측정했습니다. 그 결과 필기 입력과 제스처 인식에서 90% 이상의 분류 정확도를 달성했고, 연속 손목 각도 추정에서도 초당 13도 미만의 평균 오차로 손목 움직임 속도를 예측할 수 있었습니다. 이는 별도의 개인별 보정이나 추가 학습 없이도, 훈련에 포함되지 않은 새로운 사용자의 근전도 신호를 높은 정확도로 이해해냈다는 뜻입니다.\n다음으로 좀 더 실제 사용 상황에 가까운 실시간 인터랙션 실험(온라인 실험) 결과를 살펴보겠습니다. 연속 커서 제어 과제에서는 사용자가 목표 지점을 향해 커서를 움직여 맞추는 속도가 중앙값 기준 초당 0.66개의 타깃에 도달했습니다. 이는 사용자가 약 1.5초에 하나씩 목표를 정확히 선정할 수 있었음을 의미하며, 간단한 손목 움직임만으로 화면 상의 목표를 연속적으로 가리킬 수 있음을 보여줍니다. 불연속 제스처 과제에서는 초당 0.88개의 제스처를 제대로 인식해냈는데, 9가지 손동작 중 하나를 사용자가 수행하면 약 1.1초 내에 이를 판별해 입력 명령으로 삼을 수 있다는 뜻입니다. 가장 복잡한 손글씨 쓰기에서는 평균 분당 20.9단어(WPM)의 속도로 단어를 인식해냈습니다. 일반적인 키보드 타자가 분당 40~60단어 수준인 것과 비교하면 아직 절반 정도 속도이지만, 손을 공중에 움직여 쓴 글씨를 근전도로 해석하여 얻은 속도로서는 상당히 고무적인 결과입니다. 더욱이 사용자별 약간의 추가 훈련(퍼스널라이제이션)을 거치면 필기 인식 속도가 약 16% 개선되어 약 분당 24단어 이상으로 향상될 수 있음을 확인했습니다. 이러한 결과들은 이 손목밴드 기반 인터페이스가 단순 포인터 움직임부터 제스처 명령, 텍스트 입력까지 다양한 입력 작업을 꽤 높은 정확도와 실용적인 속도로 수행할 수 있음을 보여줍니다.\n흥미로운 점은, 위 성능이 사용자 간 교차 검증으로 얻어진 것이란 사실입니다. 즉, 어떤 사람이 처음 이 장치를 착용하더라도 미리 학습된 범용 모델로 즉시 80~90% 이상의 성능을 기대할 수 있다는 것입니다. 이는 기존의 많은 BCI나 근육 인터페이스 연구에서 개인별 보정 없이 높은 정확도를 내기 어려웠던 점을 크게 개선한 부분입니다. 연구진은 “우리가 아는 한, 이번에 달성한 수준은 사람 간 일반화에 성공한 신경모터 인터페이스로서 최고 수준의 대역폭 성능”이라고 자평하고 있습니다."
  },
  {
    "objectID": "posts/paper/2025-07-31-meta-wrist.html#기술적-의의-및-혁신성-분석",
    "href": "posts/paper/2025-07-31-meta-wrist.html#기술적-의의-및-혁신성-분석",
    "title": "📃sEMG-RD 리뷰",
    "section": "4. 기술적 의의 및 혁신성 분석",
    "text": "4. 기술적 의의 및 혁신성 분석\n이 연구는 비침습적 웨어러블 기기와 대규모 딥러닝을 결합함으로써, 오랫동안 난제로 여겨진 범용 뇌-기계 인터페이스에 새로운 해법을 제시했다는 점에서 큰 의의를 갖습니다. 우선 기술적 혁신의 핵심은 “고대역폭 + 범용성”이라는 두 마리 토끼를 잡았다는 점입니다. 고대역폭(high-bandwidth)이란 단위 시간당 많은 정보量을 전달할 수 있다는 뜻으로, 본 장치는 손목의 미세 신호를 통해 사람의 의도를 빠르게 읽어 다채로운 명령을 내릴 수 있게 해주었습니다. 예컨대 초당 0.88 제스처나 분당 20개 이상의 단어 입력은, 기존의 어떤 비침습 BCI 장치보다도 훨씬 높은 통신 속도입니다. 동시에 범용성(generality)이란 여러 사람에게 두루 통하는 보편성을 말하는데, 이 연구에서는 수천 명의 데이터를 학습한 모델을 통해 개인 차이를 극복한 보편 모델을 구현했습니다. 덕분에 사용자마다 별도로 캘리브레이션(calibration)하거나 다시 훈련시키지 않아도 “바로 착용하면 동작하는” 인터페이스를 시현한 것입니다. 이는 신경신호 기반 인터페이스 분야에서 첫 사례로 평가되며, 향후 관련 연구개발에 새로운 표준을 세운 성과로 볼 수 있습니다.\n또한 이 연구는 데이터 주도 접근법으로 혁신을 이끈 점도 주목해야 합니다. 기존에는 한정된 피험자(수십 명 이하)와 제한된 데이터로 모델을 만들어서 사람 간 변동성을 감당하지 못하는 경우가 많았습니다. 반면 이 논문은 수천 명 규모의 데이터셋을 구축하고, 거기에 특화된 모델 아키텍처를 고안함으로써 “데이터와 학습으로 해법을 찾은” 전례를 만들었습니다. 이는 마치 언어 AI가 방대한 코퍼스로 언어 이해를 향상시켰듯, 신경인터페이스도 빅데이터로 성능을 끌어올린 사례라 할 수 있습니다. 특히 제스처 인식 모델에 1차원 CNN+LSTM 구조를 쓰고 필기 인식에는 음성인식 모델 기법(Conformer+CTC)을 적용한 것은, 각 문제에 최적화된 딥러닝 모듈을 적재적소에 활용한 것으로 모델링 측면의 창의성을 보여줍니다. 이러한 맞춤 설계 덕분에 신호 노이즈, 사람마다 다른 근육 사용 습관, 밴드 위치 변화 등의 잡음 변수(nuisance variables)를 신경망이 내부적으로 점차 불변(feature invariance)으로 처리할 수 있게 되었음을 연구진은 확인했습니다. 쉽게 말해, 딥러닝이 다양한 상황 속에서도 손동작의 본질적 패턴을 학습해낸 것입니다.\n하드웨어 측면에서도 공학적 진보가 돋보입니다. 연구팀의 sEMG 손목밴드는 기존 연구용 근전도 장비들처럼 거추장스러운 전선이나 거대한 증폭기 없이, 손목시계 크기의 일체형 기기로 구현되었습니다. 건식 전극(pin 형태)을 사용해 겔을 바르거나 피부를 특별히 처리하지 않아도 되는 점, 극미한 신호까지 증폭해내는 자체 회로 기술, 블루투스를 통한 실시간 전송 등은 웨어러블 인터페이스 기기로서 완성도를 높여주는 요소입니다. 또한 사용자 편의성을 고려해 4종 사이즈를 제작하고 수 초 내 착탈이 가능하도록 한 부분에서도, 연구용 프로토타입을 넘어 실제 제품화를 염두에 둔 설계 의도를 엿볼 수 있습니다.\n더 나아가 이 연구는 AR 글래스와의 결합이라는 명확한 사용 시나리오를 바탕으로 한다는 점에서도 의미가 큽니다. Orion이라 불리는 Meta의 AR 안경 프로토타입은 외형상 일반 안경과 비슷하지만, 렌즈에 마이크로 디스플레이를 내장해 눈앞에 홀로그램 정보를 띄워주는 첨단 웨어러블 컴퓨터입니다. 그러나 이러한 AR 안경이 실생활에 보급되려면 사용자 명령을 입력하는 효율적이고 자연스러운 방법이 반드시 필요합니다. 손가락을 허공에서 튕기거나 음성으로 명령하는 방식은 눈에 띄고 어색하며 한계가 있습니다. 이번 연구의 손목밴드 인터페이스는 눈에 보이지 않는 작은 손 근육 신호만으로 AR 안경을 제어할 수 있게 함으로써, 마치 “생각만으로 기기를 조종”하는 듯한 경험을 선사합니다. 이는 향후 AR 글래스 보급의 장애였던 입력 문제를 해결하는 열쇠로서, Meta가 왜 이 기술을 차세대 HCI 패러다임의 핵심으로 칭하는지 알 수 있는 대목입니다.\n마지막으로, 본 연구는 업계 동향과 학술 연구의 접점을 보여준 사례로도 볼 수 있습니다. Meta Reality Labs는 2019년 신경인터페이스 스타트업 CTRL-labs를 인수하며 이 손목밴드 기술을 확보했고, 수년간의 비공개 개발 끝에 처음으로 과학 저널을 통해 그 성과를 공개했습니다. 논문 저자에도 CTRL-labs 출신 연구자들이 포함되어 있으며, 논문을 통해 연구 커뮤니티에 데이터와 모델을 공유(오픈 소스)하고 평가받는 과정을 거쳤습니다. 이는 첨단 기술 기업이 자사의 핵심 기술을 학술적으로 검증받고 투명하게 협력하려는 움직임으로, 추후 학계와 산업계의 협력을 가속화하는 긍정적 신호로 해석할 수 있습니다."
  },
  {
    "objectID": "posts/paper/2025-07-31-meta-wrist.html#한계-및-향후-과제",
    "href": "posts/paper/2025-07-31-meta-wrist.html#한계-및-향후-과제",
    "title": "📃sEMG-RD 리뷰",
    "section": "5. 한계 및 향후 과제",
    "text": "5. 한계 및 향후 과제\n혁신적인 성과에도 불구하고, 이 기술이 즉각 상용화되기까지는 여전히 몇 가지 과제와 제한점이 존재합니다. 우선 인식 속도와 정확도 측면에서, 비록 현재 수준이 놀랍도록 높지만 기존 입력 장치 대비 100% 대등한 수준은 아니다라는 점을 들 수 있습니다. 예를 들어 분당 20~25단어의 필기 입력 속도는 일반 키보드 타자 속도의 절반 이하이며, 손목 각도 제어의 정확도(평균 13°/s 오차)도 손으로 직접 포인팅하는 것만큼 정밀하진 않을 수 있습니다. 물론 이런 수치는 머신러닝 모델의 개선과 개인별 튜닝으로 계속 향상시킬 여지가 있습니다. 실제로 논문에서도 추가 개인 학습 데이터로 성능을 10% 이상 높일 수 있다고 보고한 만큼, 향후 하이브리드 접근법(초기 범용 모델 + 소량 개인 데이터 보정)이 도입될 가능성도 있습니다.\n하드웨어적인 한계도 고려해야 합니다. 현재 프로토타입 손목밴드의 배터리 지속 시간은 4시간 정도로 하루 종일 사용하기에는 다소 부족합니다. 전극이 피부에 밀착되어야 하므로 땀이나 움직임에 따른 접촉 불량, 피부 자극 등의 현실적인 문제도 해결해야 합니다. 디자인 면에서도 본 프로토타입은 기능 위주로 만들어져 부피가 있고 미적으로 세련되지 않는데, 소비자용으로 가려면 더 소형화되고 편안한 형태로 개선이 필요합니다. Meta의 시연 영상 등을 보면 손목 위에 두툼한 밴드 형태인데, 장시간 착용해도 부담 없도록 경량화 및 밴드 유연성을 높이는 연구가 요구됩니다. 또한 센서 위치 민감도도 과제입니다. 현재는 밴드를 정확한 위치에 차야 최상의 성능이 나오며 위치가 어긋나면 인식률이 떨어질 수 있습니다. 논문에서는 회전 불변 특징 등을 도입해 위치 변화에 강인하도록 했지만, 실제 일상에서 사용자가 매번 같은 위치에 차지 않을 수 있으므로 자동 보정 알고리즘이나 다중 전극 배열로 이 문제를 완화해야 합니다.\n맥락 인식과 오작동 방지 문제도 남아 있습니다. 사용자가 의도적으로 손가락을 움직이는 신호와 우연한 근육 긴장 신호를 어떻게 구분할지, 원하지 않는 허위 입력(false input)을 막는 안정성 장치도 중요합니다. 예를 들어 걸으면서 팔을 흔드는 동작이나 다른 물건을 잡는 동작 중에 밴드가 잘못된 입력으로 해석하지 않도록, 사용자 의도 컨텍스트를 파악하는 추가 센서 융합이나 소프트웨어 필터링이 필요할 것입니다. 현재 연구에서는 비교적 통제된 실험 상황에서 성능을 낸 것이므로, 복잡한 현실 환경에서의 성능 검증이 다음 단계 과제로 보입니다.\n마지막으로 상용화 전략 측면에서, Meta는 아직 이 기술의 정확한 출시 일정이나 제품화 계획을 공개하지 않고 있습니다. 경쟁사들의 동향이나 시장 성숙도에 따라 제품 형태가 결정되겠지만, 개인 프라이버시와 데이터 보안에 대한 사회적 논의도 선행되어야 할 것입니다. 손목의 신경신호를 읽는다는 것은 일종의 개인 생체 데이터를 수집하는 것이므로, 이 데이터가 어떻게 관리되고 보호될지, 오남용 우려는 없을지 투명한 정책이 요구됩니다. 또한 규제 측면에서 의료기기로 분류될 가능성, 혹은 새로운 인증 표준 마련 등도 장기적으로 해결해야 할 과제입니다."
  },
  {
    "objectID": "posts/paper/2025-07-31-meta-wrist.html#향후-응용-가능성-및-산업적사회적-영향",
    "href": "posts/paper/2025-07-31-meta-wrist.html#향후-응용-가능성-및-산업적사회적-영향",
    "title": "📃sEMG-RD 리뷰",
    "section": "6. 향후 응용 가능성 및 산업적·사회적 영향",
    "text": "6. 향후 응용 가능성 및 산업적·사회적 영향\n이번 연구가 보여준 손목형 신경모터 인터페이스 기술은 미래의 다양한 응용 가능성을 열어주고 있습니다. 가장 직접적인 응용 분야는 역시 증강현실(AR)과 가상현실(VR)입니다. 불과 몇 년 전까지만 해도 SF 영화에서나 보이던 생각으로 컴퓨터 제어하기에 가까운 경험이, AR 안경과 손목밴드의 결합으로 현실화될 수 있기 때문입니다. 사용자는 길을 걸으면서 눈앞의 AR 정보와 자연스럽게 상호작용할 수 있고, 주머니에서 휴대폰을 꺼내지 않고도 손목의 작은 움직임만으로 메시지 전송, 지도 조작, 웹 검색 등을 할 수 있을 것입니다. 예컨대 눈앞에 나타난 가상 키보드에 손을 대지 않고도 타이핑을 해서 문자 메시지를 보낸다거나, 메뉴를 생각하듯이 훑어보고 선택하는 일이 가능해집니다. 이런 시각-입력의 일체화 경험은 스마트폰 이후의 차세대 컴퓨팅 플랫폼에서 경험 혁신을 이끌 핵심 요소로 꼽힙니다. Meta를 비롯한 빅테크 기업들이 AR 글래스 개발에 막대한 투자를 하는 이유도 이러한 입력 혁신에 따른 새로운 시장을 선점하기 위해서입니다.\n이 기술은 사용자 접근성(accessibility) 측면에서도 큰 사회적 가치를 지닙니다. 손이나 팔의 자유로운 움직임이 어려운 장애인 또는 거동이 불편한 분들도, 손목의 남은 미세 신경 신호를 활용해 컴퓨터나 의사소통 보조기기를 제어할 수 있을 전망입니다. 연구에서도 겉으로 뚜렷한 손 움직임을 할 수 없는 참가자도 근육 신호는 발생시킬 수 있고, 이를 활용해 가상의 손을 움직이거나 인터페이스를 조작할 수 있었다고 보고했습니다. 즉, 신체 능력이 제한된 사람들의 디지털 접근성을 높여 삶의 질을 향상시킬 보조공학으로 발전할 가능성이 있습니다. 향후에는 언어 능력을 잃은 사람이 머릿속으로 단어를 생각하며 손 근육에 미세 신호만 줘도, 이것이 자동으로 문장으로 변환되어 소통할 수 있게 될지 모릅니다. 이러한 의료 재활 및 보조 분야의 파급 효과는 사회적으로 매우 의미있는 혁신입니다.\n산업적으로도 손목 EMG 인터페이스는 인간과 기계의 상호작용 범위를 크게 넓힐 기술로 주목받습니다. 예컨대, 손을 직접 쓰기 어려운 환경(우주나 해저 작업, 방호복을 입은 상태 등)에서도 손목 신호로 로봇을 조종하거나 기기를 제어할 수 있습니다. 또 복잡한 기계 조작을 제스처로 매핑하여 직관적으로 컨트롤하는 산업용 인터페이스로 활용될 수 있습니다. 한편 게임 및 엔터테인먼트 분야에서도 사용자 의도를 즉각 반영하는 새로운 형태의 컨트롤러로서 시장을 창출할 수 있습니다. VR 게임에서 손가락을 실제로 움직이지 않고 생각만으로 마법을 쓰는 컨트롤이 가능해진다면, 완전히 새로운 몰입형 경험을 제공할 수 있을 것입니다.\n사회적 영향 측면에서, 이 같은 비가시적 인터페이스의 등장은 디지털 기술과 일상의 경계를 한층 더 허물 것으로 예상됩니다. 긍정적으로는 컴퓨팅이 우리의 일에 더 깊이 스며들어 사용자 경험이 매끄럽고 자연스러워지며, 기술이 인간을 방해하는 대신 보조하는 방향으로 갈 것입니다. 사람들은 손을 주머니에 넣은 채로도 디지털 세계와 교감하고, 주변 사람들과의 물리적 교류를 해치지 않으면서 개인만의 비서 역할을 수행하는 AR 기기를 활용하게 될 것입니다. 한편 이러한 기술이 보편화되면 프라이버시나 윤리에 대한 새로운 논의도 필요합니다. 예를 들어, 손목밴드가 읽은 신호로 사용자의 의도를 지나치게 추적하거나, 해킹을 통해 민감한 신체 데이터 유출 가능성은 없는지 등의 문제입니다. 기술이 사회적으로 수용되려면 이러한 우려를 불식시키기 위한 보안 기술과 제도 정비도 뒤따라야 할 것입니다.\n종합하면, Meta Reality Labs의 표면 EMG 기반 신경모터 인터페이스 연구는 미래 HCI의 방향을 제시한 획기적인 사례입니다. Orion AR 안경과의 연계를 통해 한층 구체화된 이 기술은, 가까운 장래에 키보드와 마우스를 대체할 새로운 입력 방식을 산업계에 제시하고 있습니다. 전문가들은 이 기술이 모바일 컴퓨팅의 패러다임 시프트를 가져올 것으로 기대하며, 연구팀도 “이 기술이 모두의 컴퓨터 상호작용 방식을 더 나은 방향으로 바꿔놓을 것”이라고 강조하고 있습니다. 앞으로 추가 연구와 개발을 통해 한계를 보완해 나간다면, “손목으로 읽는 생각”이라는 멋진 아이디어가 현실이 되어 우리 삶 속에 스며들 날이 머지않아 보입니다.\n\nOptional\n\nAdvancing Neuromotor Interfaces by Open Sourcing Surface Electromyography (sEMG) Datasets for Pose Estimation and Surface Typing"
  },
  {
    "objectID": "posts/paper/2025-08-08-dexflow.html",
    "href": "posts/paper/2025-08-08-dexflow.html",
    "title": "📃DexFlow 리뷰",
    "section": "",
    "text": "IEEE/RSJ IROS 2025"
  },
  {
    "objectID": "posts/paper/2025-08-08-dexflow.html#소개-배경-및-문제점",
    "href": "posts/paper/2025-08-08-dexflow.html#소개-배경-및-문제점",
    "title": "📃DexFlow 리뷰",
    "section": "2.1 소개 (배경 및 문제점)",
    "text": "2.1 소개 (배경 및 문제점)\n로봇의 섬세한 손 동작 조작(dexterous manipulation)을 위해 인간 손의 동작을 로봇 손으로 포즈 리타게팅(pose retargeting)하는 기술은 오랫동안 도전적인 문제로 남아 있습니다. 오늘날 인간 손의 모션 캡처 기술(예: MANO 모델 등)로부터 정밀한 손 추적이 가능해졌지만, 이를 로봇 손으로 옮기는 과정에는 여전히 여러 난제가 존재합니다. 대표적으로 (1) 인간 손과 로봇 손의 형태적 차이(길이, 관절 범위 등)로 인한 불일치, (2) 손과 물체 사이 접촉 상호작용을 제대로 모델링하지 못해 발생하는 비현실적인 동작(예: 손가락이 물체를 뚫고 지나가는 관통 현상 등), 그리고 (3) 비효율적인 최적화 과정으로 인한 실시간성 부족 및 부정확성 문제가 지적되어 왔습니다. 기존의 단순 운동학적 매핑 기반 리타게팅 방법들은 사람 관절 각도를 로봇 관절에 직접 대응시키지만, 사람/로봇 손 구조 차이를 고려하지 않아 손가락이 물체를 관통하거나 접촉이 불안정해지는 문제가 컸습니다. 한편 에너지 최적화 기반 접근들은 관통 페널티나 접촉 거리 최소화 같은 인위적인 비용 함수를 설계하여 문제를 풀려고 시도했으나, 인간 손 동작의 고유한 제약(예: 자연스러운 그립 형태)을 활용하지 못해 한계가 있었습니다. 또한 학습 기반 방법(예: DexPilot, AnyTeleop 등 실시간 테레오퍼레이션 기법)은 데이터 기반 사전지식을 활용하여 속도는 높였지만, 정밀한 공간 정렬이나 시간적 일관성 측면에서는 여전히 부족함을 보였습니다. 요컨대, 이전까지의 방법들은 정확도 vs. 속도, 물리적 현실감 vs. 데이터 다양성 사이에서 균형 잡힌 해법을 제시하지 못했습니다.\nDexFlow는 이러한 문제들을 해결하기 위해 제안된 통합적 손 포즈 리타게팅 및 상호작용 모델로서, 사람 손의 동작을 로봇 손으로 옮기는 과정에서 정확성과 현실감, 그리고 데이터 확보 효율까지 모두 향상시키는 것을 목표로 합니다. 이 논문 리뷰에서는 DexFlow의 기술적 기여, 기존 연구와의 차별점, 그리고 실험 결과를 중점적으로 살펴보겠습니다."
  },
  {
    "objectID": "posts/paper/2025-08-08-dexflow.html#기술적-기여-핵심-아이디어-및-메커니즘",
    "href": "posts/paper/2025-08-08-dexflow.html#기술적-기여-핵심-아이디어-및-메커니즘",
    "title": "📃DexFlow 리뷰",
    "section": "2.2 기술적 기여 (핵심 아이디어 및 메커니즘)",
    "text": "2.2 기술적 기여 (핵심 아이디어 및 메커니즘)\nDexFlow가 제안하는 핵심 아이디어는 계층적 최적화와 접촉 인식을 결합한 파이프라인으로, 사람 손의 자연스러운 동작을 로봇 손에 이식하면서도 물리적으로 그립(grip) 상호작용이 사실적으로 유지되도록 하는 것입니다. 구체적으로, 전역 최적화 → 접촉 추출/필터링 → 국소 최적화의 3단계로 구성된 절차를 통해 문제를 단계별로 해결합니다. 아래에서는 DexFlow의 주요 기술적 기여를 세 가지 측면에서 정리합니다:\n\n① 계층적 전역-국소 최적화 접근: 우선 사람 손 포즈와 최대한 유사한 로봇 손 초기 자세를 얻기 위해 전역 최적화(global search)를 수행합니다. 이는 사람 손 관절 구성과 로봇 손 관절 사이의 차이를 줄이는 에너지 함수를 정식화하여, 로봇 손이 해부학적으로 정렬된 자세를 취하도록 하는 단계입니다. 논문에서는 GN_CRS2_LM이라는 글로벌 탐색 알고리즘을 사용해 로봇 손의 관절 각도를 최적화했다고 설명하는데, 이 과정에서 사람 손의 관절 제약과 로봇 손의 기구학을 모두 고려하여 초기 관절 구성을 찾아냅니다. 이렇게 얻은 초기 포즈를 바탕으로, 두 번째 단계에서는 지역적 탐색 및 접촉 조정을 수행합니다. 즉, 전역 단계 결과를 출발점으로 빠르게 현실성 있는 손가락 구성을 찾아낸 뒤, 실제 물체와의 접촉을 고려한 미세 조정(contact-aware refinement)을 적용합니다. 이러한 2단계 최적화 전략을 통해 먼저 인간 동작의 거시적 형태를 맞추고, 이후 미시적 접촉까지 정확히 반영함으로써 해부학적 정합성과 물리적 개연성을 동시에 달성합니다. 특히, 저자들은 새롭게 설계된 에너지 항들을 도입하여 정렬 오차 최소화와 물리적 그립 안정성 두 목표를 균형 있게 달성했다고 강조합니다.\n② 이중 임계값 접촉 감지 및 시간적 스무딩: DexFlow의 두 번째 기여는 손-물체 접촉 정보를 안정적으로 추출하는 모듈입니다. 전역 리타게팅 단계를 거친 로봇 손이 물체에 근접하고 난 후, 각 손가락이 물체에 접촉했는지 여부를 판정해야 합니다. 이를 위해 이중 임계값(double-threshold) 기반 접촉 검출 알고리즘을 도입합니다. 구체적으로, 손가락 끝과 물체 표면 사이 거리가 첫 번째 임계값 이내로 들어오면 잠정적으로 접촉으로 간주하고, 두 번째 더 엄격한 임계값을 적용해 노이즈나 오차로 인한 잘못된 접촉 판단을 걸러냅니다. 이렇게 프레임별 얻어진 접촉 정보는 바로 사용되지 않고, 인접 프레임들과 비교하여 스무딩됩니다. 즉, 접촉 상태가 한 프레임에서 발생했다 사라지는 일시적 플럭투에이션(출렁임)을 제거하기 위해 슬라이딩 윈도우 기반의 프레임-투-프레임 완화(smoothing) 처리를 합니다. 이러한 시간적 필터링을 거치면 잡음에 강인한 안정된 접촉 지도(contact map)를 얻을 수 있으며, 연속된 동작 시퀀스에서 접촉 여부가 일관성 있게 유지됩니다. 요약하면, 이중 기준으로 접촉을 검출하고 시간적으로 확정함으로써 기존 방법에서 흔했던 접촉 신호의 들쭉날쭉함을 효과적으로 해소했습니다.\n③ 대규모 데이터 변환 파이프라인 및 크로스-핸드(topology) 이식: DexFlow는 단일 알고리즘에 그치지 않고, 데이터 생성 측면에서도 큰 기여를 합니다. 저자들은 DexFlow를 활용해 다양한 데이터 소스로부터 인간 손 및 객체 상호작용 데이터를 통합하고, 이를 통해 대규모 로봇 그립 동작 데이터셋을 구축했습니다. 구체적으로, 인간 손 모션 캡처 데이터(MANO 기반)와 여러 3D 물체 모델(YCB 벤치마크 객체 등)을 결합하여, 로봇 손(ShadowHand 및 Allegro Hand)에 대한 292,000프레임에 달하는 그립 시퀀스 데이터를 생성했습니다. 이 데이터셋은 다양한 그립 동작 시나리오(안정적 파지, 동적 조정, 여러 손가락 협력 등)를 포괄하며, 특히 한 인간 손 동작을 서로 다른 로봇 손 형태에 매핑하는 크로스-손 토폴로지 이식까지 지원하는 것이 특징입니다. 예를 들어, 인간 손의 하나의 grasp 동작(예: 집게 잡기, 감싸잡기 등)을 ShadowHand와 Allegro 같이 손가락 개수와 형태가 다른 로봇 손에 각각 전달해도 본래의 의도된 파지 형태가 유지되도록 합니다. 이러한 데이터 파이프라인을 통해 얻은 통합 데이터셋은 기존 대비 학습 및 평가에 유리한 규모와 다양성을 가지며, DexFlow의 성능 개선을 정량적으로 뒷받침합니다. 논문에 따르면 이 데이터셋을 활용한 DexFlow는 기존 리타게팅 솔루션들 대비 수 배에 이르는 semantic 성공률 향상을 보여주었다고 보고됩니다.\n\n\n\n\n\n그림 1: DexFlow가 제안하는 손-물체 그립 리타게팅 파이프라인의 개략도. 사람 손 동작과 물체 상호작용 시퀀스(왼쪽)를 입력받아, (1) 객체 스케일 조정 및 초기 로봇 손 자세 리타게팅을 수행한다. 이후 (2) 이중 문턱 접촉 검출 알고리즘으로 로봇 손과 물체 간 초기 접촉 정보를 추출하고, 인접 프레임들에 걸쳐 시간적 스무딩을 적용하여 안정된 접촉 상태를 확보한다. 마지막으로 (3) 손가락별 세부 최적화 단계를 통해 접촉이 감지된 손가락(예: 엄지, 중지 등)을 순차적으로 미세 조정한다. 이때 접촉 정보가 없는 손가락(그림 예시의 검지)은 건너뛰어 불필요한 계산을 줄이고 효율을 높인다. 이런 단계적 최적화를 거치면 사람 손의 조작 의도가 로봇 손에 정확히 전달되는 동시에 물리적으로도 실행 가능한 파지 동작이 완성된다."
  },
  {
    "objectID": "posts/paper/2025-08-08-dexflow.html#기존-연구와의-비교-dexflow의-차별점",
    "href": "posts/paper/2025-08-08-dexflow.html#기존-연구와의-비교-dexflow의-차별점",
    "title": "📃DexFlow 리뷰",
    "section": "2.3 기존 연구와의 비교 (DexFlow의 차별점)",
    "text": "2.3 기존 연구와의 비교 (DexFlow의 차별점)\n손 포즈 리타게팅 및 상호작용 분야에서 DexFlow가 가지는 차별점을 이해하기 위해, 몇 가지 대표적인 기존 접근들과 기술적으로 비교해보겠습니다.\n\n직접 매핑 기반 리타게팅 vs. DexFlow: 초창기 리타게팅 기법들은 주로 운동학적 직접 매핑을 사용하여, 인간 손 각도를 로봇 손 관절에 단순 대응시켰습니다. 이러한 방법은 구현이 쉽고 실시간 적용에 유리하지만, 사람 손 vs. 로봇 손의 형태 차이(예: 손가락 길이, 관절 범위)가 반영되지 않아 손가락이 물체나 다른 손가락을 뚫고 지나가는 관통 문제가 심각했습니다. 예컨대 사람에겐 자연스러운 움켜잡는 동작이 로봇에 그대로 적용되면 로봇 손가락이 겹치거나 물체 내부로 들어가는 경우가 빈번했습니다. DexFlow는 이러한 문제를 전역 최적화 단계에서 로봇 손을 사람 손에 최대한 맞추고, 지역 접촉 최적화 단계에서 관통을 줄이는 방향으로 체계적으로 해결합니다. 즉, 단순히 관절각을 복사하는 대신 최적화 문제로 정식화하여 물리적 제약을 반영함으로써, 관통 아티팩트와 불안정한 접촉 패턴을 크게 개선했습니다. 결과적으로 DexFlow는 기존 직접 매핑 기법에 비해 현실적인 그립 재현을 달성합니다.\n최적화 기반/물리 시뮬레이션 기법 vs. DexFlow: 관절 각도 최적화를 통해 그립을 생성하는 접근은 DexFlow 이전에도 존재했으며, 물리 시뮬레이터나 에너지 함수 최적화를 활용한 예로 GraspIt!, DexGraspNet, FRoGGeR, SpringGrasp 등이 있습니다. 이들은 물체 파지를 제약 충족 문제로 보고 접촉 안정성, 힘 폐쇄(grasp wrench) 등 조건을 만족하는 그립을 찾았습니다. 그러나 전통적 최적화 기법들은 대체로 계산량이 많고, 무엇보다 인간 손의 자연스러운 모션에 대한 사전지식이 부족했습니다. 예를 들어, FRoGGeR나 SpringGrasp 같은 물리 기반 방법은 다양한 그립을 만들어내지만 그 과정에서 사람스러운 손모양을 보장하지는 못하고, 해답 탐색에 긴 시간이 소요되었습니다. DexFlow는 이러한 점을 인간 시演 데이터 활용과 계층적 접근으로 개선했습니다. 사람 손 모션 캡처 데이터(MANO)를 기반으로 출발하기 때문에 초기 해가 현실성 있고, 이를 토대로 빠른 전역 탐색 후 국소 접촉 미세조정을 함으로써 계산 효율을 높였습니다. 실제 논문 비교에 따르면 DexFlow는 DexGraspNet 대비 접촉 거리(contact distance)를 한 자리 수로 줄이고** (6.90 → 0.77), SpringGrasp 대비 관통 깊이(penetration depth)를 크게 낮추는 등 물리적 품질 면에서 한 단계 향상된 결과를 보입니다. 특히 접촉 품질 면에서 DexFlow의 접촉 간격은 기존 대비 10배 이상 개선되었고, 관통 현상은 기존 방법들에 비해 현저히 감소했습니다. 다만 특정 최적화 기법(BODex 등)이 관통을 거의 완전히 제거하도록 특화된 경우도 있는데, DexFlow도 이에 버금가는 수준에 근접하면서도 전반적인 균형 잡힌 성능을 달성한 것이 특징입니다. 요약하면, DexFlow는 이전 최적화/시뮬레이션 기반 접근들의 물리적 현실성을 계승하면서도 인간 동작의 자연스러움과 계산 효율을 동시에 확보한 발전된 기법입니다.\n학습 기반(영상·시演·강화학습) 기법 vs. DexFlow: 최근 들어 인간 동작 데이터를 활용한 학습 기반 접근도 다수 등장했습니다. DexMV는 비디오로부터 3D 손-물체 포즈 시퀀스를 추출하여 로봇으로 모방하는 시도를 했으나, 객체의 정확한 상태 정보를 가정해야 하는 등 현실 적용에 제약이 있었습니다. AnyTeleop, DexPilot 등의 텔레옵 제어 시스템은 카메라로 추적한 인간 손 동작을 로봇 손에 실시간 전송해 원격 조작을 구현했지만, 빠른 응답을 위해 정교함을 일부 포기하면서 정밀 작업에서 공간 정렬 오차가 발생하곤 했습니다. 또한 ViViDex와 같이 강화학습을 통해 인간 비디오 시演을 모방하는 접근도 제안되었는데, 물리적 그립 성공률을 높이기 위해 과거 궤적 보상 등을 사용하면서도 특정 작업별 대량의 학습 데이터가 필요하다는 단점이 있었습니다. 이와 달리 DexFlow는 명시적인 최적화와 접촉 검출 메커니즘으로 문제를 풀기 때문에, 새로운 작업이나 객체에 대해 범용적으로 적용하기 수월하고 특정 작업 데이터에 덜 의존적입니다. 또한 학습 기반 방법들이 간혹 놓치는 미세한 손가락 위치나 시간적 안정성을 DexFlow는 에너지 함수를 통한 미세조정과 스무딩으로 확보합니다. 결과적으로 DexFlow는 실시간성은 다소 양보하지만, 학습 기반 기법들이 달성하지 못했던 공간적 정확도와 일관성 있는 프레임간 동작을 구현하여 오프라인 데이터 생성 측면에서 뛰어난 성능을 보입니다. 생성된 데이터는 차후 학습 알고리즘의 학습용으로 활용될 수 있기 때문에, DexFlow는 기술 데모 뿐 아니라 데이터 기반 학습 파이프라인의 전처리로서도 의미가 큽니다."
  },
  {
    "objectID": "posts/paper/2025-08-08-dexflow.html#실험-결과-분석-성능-평가-및-시각화",
    "href": "posts/paper/2025-08-08-dexflow.html#실험-결과-분석-성능-평가-및-시각화",
    "title": "📃DexFlow 리뷰",
    "section": "2.4 실험 결과 분석 (성능 평가 및 시각화)",
    "text": "2.4 실험 결과 분석 (성능 평가 및 시각화)\nDexFlow의 유효성을 확인하기 위해 저자들은 다양한 벤치마크 실험을 수행했습니다. 실험은 주로 시뮬레이션 환경에서 이루어졌으며, 50개의 YCB 표준 물체에 대해 ShadowHand 로봇 손(5지)과 Allegro 로봇 손(4지)을 이용한 다수의 그립 시퀀스를 생성하고 평가했습니다. 앞서 언급한 바와 같이 약 292K (29만 2천) 프레임의 그립 데이터가 DexFlow로부터 생성되었고, 이 데이터를 기존의 공개 데이터셋 및 기법들과 비교 분석하였습니다.\n\n\n\nTable I은 DexFlow가 생성한 데이터셋과 기존 데이터셋들의 규모를 비교한 것입니다. 예를 들어, 기존 DexGraspNet은 약 132만 개의 그립을 시뮬레이션으로 생성한 반면, DexFlow는 50개 물체에 대해 29만여 프레임의 연속 동작 시퀀스를 제공함으로써 다양한 시나리오를 포괄하는 새로운 데이터 자원을 제시했습니다. 또한 DexFlow 데이터는 다양한 로봇 손 구조(Shadow, Allegro)에 모두 적용 가능하도록 생성된다는 점에서, 특정 손에 한정되지 않는 범용성을 입증했습니다.\n\n2.4.1 정량적 지표 비교 (Single-Frame 기준 성능)\n논문에서는 DexFlow의 성능을 기존 방법들과 정량적으로 비교하기 위해 여러 품질 지표를 측정하였습니다.\n\n\n\nTable II는 대표적인 생성 기법들의 성능을 비교한 표로, 주로 단일 프레임 기준의 그립 품질 통계를 담고 있습니다. 여기에는 Semantic Success Rate (SSR), 생성 속도(SPD), 관통 깊이(PD), 접촉 거리(CD), 그립 안정성 지표(FVR) 등이 포함됩니다. 각 지표는 BODex라는 선행 연구의 평가 프로토콜을 따르는데, 간략히 설명하면 다음과 같습니다:\n\nSemantic Success Rate (SSR): 생성된 그립이 성공적인 파지로 간주되는 비율입니다. 논문에서는 물리 시뮬레이션 상에서 어떤 방향으로 중력을 걸어도 100 스텝 동안 물체를 놓치지 않으면 성공으로 판정하였으며, 그 비율을 SSR로 보고합니다. 값이 높을수록 많은 그립이 실제로 물체를 들 수 있음을 의미합니다.\n생성 속도 (SPD): 초당 몇 개의 그립을 생성할 수 있는지를 나타내는 지표로 볼 수 있습니다. 수치가 클수록 데이터 생성 속도가 빠름을 의미하며, 실시간성에 가까움을 나타냅니다.\n관통 깊이 (PD): 손가락 메쉬가 물체를 얼마나 깊게 관통했는지를 측정한 값입니다. 값이 작을수록 관통이 적어 물리적으로 더 타당한 파지입니다.\n접촉 거리 (CD): 손가락과 물체 사이 접촉점 간격을 나타냅니다. 이 값이 작을수록 손가락이 물체를 빈틈없이 밀착하게 잡고 있음을 의미합니다. (일부 문맥에서 Chamfer Distance를 의미하기도 하나, 여기서는 접촉 품질 관련 지표로 활용되었습니다.)\nFVR: 논문에서 정의한 추가적인 품질 지표로, (force closure나 grasp 안정성과 연관된 비율로 추정됩니다. 값이 높을수록 안정적인 그립일 가능성이 높음)\n\n이러한 지표로 비교한 결과, DexFlow는 전반적으로 균형 잡힌 성능을 보여주었습니다. 우선 Semantic Success Rate(SSR)을 보면, DexFlow는 약 40.3%의 성공률을 달성하여, 기존 전통적 리타게팅 방법인 DexRetarget의 5.35%에 비해 큰 폭(약 7.5배)으로 향상된 것을 확인할 수 있습니다. (DexRetarget은 DexMV의 후속 오픈소스 기법으로, 접촉 고려가 없어 성공률이 매우 낮았습니다.) DexFlow의 SSR 40%대는 학습 기반 최적화 기법인 FRoGGeR의 41.97%와 거의 유사한 수준으로, 데이터 기반 접근이 아닌 방법론으로 이룬 성과치고는 매우 고무적입니다. 한편 BODex라는 최적화 기법은 SSR이 89.5%로 유달리 높았지만, 이는 특정 로봇 손에 특화된 접근으로 DexFlow와 직접 비교하기엔 성격 차이가 있습니다. 그 외의 기법들(DexGraspNet: 31.4%, SpringGrasp: 37.2%)과 비교하면 DexFlow가 가장 앞선 그룹에 속함을 알 수 있습니다.\n다른 물리적 지표들을 살펴보면, 관통 깊이(PD) 측면에서 DexFlow는 8.5로, 기존 리타게팅(예: DexRetarget의 84.4)에 비해 현격히 낮은 관통을 보였습니다. 이는 DexFlow의 접촉 최적화 단계가 손가락이 물체를 지나치게 파고드는 현상을 효과적으로 억제했음을 나타냅니다. 비록 FRoGGeR나 BODex가 관통 깊이를 각각 2.17, 0.37까지 줄여 DexFlow보다 더 우수하지만, 이들은 물리엔진 기반의 반복 최적화로 계산 비용이 큰 대가를 치른 결과입니다. 접촉 거리(CD)는 DexFlow가 0.77을 기록하여, FRoGGeR(0.88)보다 낮고 BODex(0.28) 다음으로 두 번째로 우수한 접촉 밀착도를 보였습니다. DexFlow의 CD는 DexGraspNet(6.90)이나 SpringGrasp(6.18)에 비해 10배 이상 작은 값으로, 사람이 잡듯이 빈틈없이 물체를 쥐는 자연스러운 그립을 얻었음을 알 수 있습니다. 마지막으로 생성 속도(SPD)를 보면 DexFlow는 0.37로, 1.0에 가까운 DexRetarget(0.96)보다는 느리지만 DexGraspNet(0.93)과 유사한 수준이고 SpringGrasp(0.48)보다는 약간 느린 정도였습니다. 특히 FRoGGeR의 SPD가 0.0002에 불과한 것과 비교하면, DexFlow가 현실적인 시간 안에 데이터 생성을 수행할 수 있음을 의미합니다 (FRoGGeR는 물리 기반 미분 가능 최적화를 사용하여 한 개 그립을 찾는데 매우 오래 걸림). 종합하면, DexFlow는 절대적인 성공률 면에서 일부 최적화 기법에 약간 뒤쳐질지 몰라도, 관통/접촉/속도 등 여러 지표에서 고르게 우수한 “균형형” 성능을 발휘한다는 것이 실험으로 입증되었습니다. 이는 곧 DexFlow가 현실적인 로봇 그립 데이터 생성에 전반적으로 적합한 접근임을 보여줍니다.\n\n\n\n\n그림 2: 크로스-도메인 손 모션 이식에 대한 DexFlow의 데모 장면. 왼쪽은 인간 손이 작은 상자를 검지와 엄지 손가락으로 집는 pinch grasp 동작이고, 오른쪽은 해당 동작을 Allegro 로봇 손(파란색, 4손가락)으로 리타게팅한 결과입니다. 사람 손의 엄지~약지 4개 손가락 움직임이 로봇 손의 4개 손가락에 자연스럽게 대응되어, 로봇 손도 동일한 물체를 성공적으로 집을 수 있습니다. DexFlow는 이처럼 서로 형태가 다른 로봇 손들 간에도 일관된 파지 동작 이식을 가능케 하며, 인간 손 동작의 의미론적 의도(어떤 방식으로 잡는가)를 유지한다는 점에서 큰 강점을 보입니다.\n\n\n\n2.4.2 시퀀스 모션 품질 및 동작 자연스러움\nDexFlow의 평가에서는 단일 프레임 성공률뿐 아니라, 연속적인 동작 시퀀스의 품질도 중요하게 다루어졌습니다. 이를 위해 논문에서는 시간에 따른 물체 위치 변화를 정밀 비교하는 Chamfer Distance (CD) 기반 지표와, 속도/가속도 프로파일의 차이를 분석하였습니다. 우선 시퀀스 Chamfer 거리는 각 시점에서 물체의 점군(point cloud)을 비교하여 로봇 손이 물체를 움직이는 궤적이 원본 인간 시연과 얼마나 일치하는가를 나타냅니다. DexFlow의 1단계 리타게팅 결과는 Chamfer Distance가 0.008로, 기존 DexRetarget의 0.016보다 절반으로 감소했습니다. 이는 로봇 손이 물체를 움직이는 궤적의 형상이 사람 손의 궤적과 매우 가깝게 맞아떨어진다는 것을 의미하며, DexFlow의 전역 최적화가 공간적 정합성을 크게 개선했음을 보여줍니다. 이어서 접촉 최적화 후에도 Chamfer 값이 0.009로 소폭 증가했을 뿐으로, 여전히 DexRetarget 대비 상당히 낮은 오차를 유지했습니다. 즉, DexFlow는 형태 추종 면에서 뛰어난 정확도를 유지하면서도 관통 문제를 해결하는 두 마리 토끼를 잡았다고 볼 수 있습니다.\n또한 DexFlow가 생성한 동작의 시간적 자연스러움을 측정하기 위해 속도 분포 차이와 가속도 변화를 비교했습니다. 인간 손 동작 대비 로봇 손 동작의 속도 분포 차이는 KL 발산으로 측정되었는데, DexRetarget의 값이 0.54인 반면 DexFlow 리타게팅 결과는 0.48로 더 낮아졌습니다. 이는 로봇 손 움직임의 속도 패턴이 인간의 원본 동작과 더 유사해졌음을 의미합니다. 접촉 최적화를 거치면서 속도 분포 차이는 약간 증가하여 0.57이 되었지만, 이는 접촉을 조정하는 과정에서 불가피한 미세 조정이 들어갔기 때문입니다. 그에 반해 가속도 RMS 값은 DexRetarget의 0.083에서 DexFlow 리타게팅 단계에서 0.073으로 감소하였다가, 최종 최적화 후 0.080으로 소폭 상승하였습니다. 가속도 RMS 증가는 손가락 관통을 없애는 마지막 단계 최적화에서 다소 급격한 조정이 추가된 영향이지만, 여전히 DexRetarget 수준과 비슷하게 유지되었습니다. 저자들은 이러한 변화를 두고 “리타게팅 단계에서는 기하학적 정합성을 극대화하여 Chamfer 오차를 줄이고, 이후 물체 중심의 세밀 조정 단계에서 약간의 가속도 증가(움직임 변화)를 받아들이는 균형 잡힌 최적화 전략”이라고 설명합니다. 즉, 1단계에서는 형상을 맞추고 2단계에서는 물리적 충돌을 해결하는 분리 최적화 덕분에, 전체적으로 자연스러운 움직임 흐름은 최대한 보존하면서 필요한 부분만 수정할 수 있었다는 것입니다.\n마지막으로, DexFlow가 생성한 다양한 그립 동작들은 시각적으로도 자연스럽고 다양한 것으로 나타났습니다. 논문에는 여러 물체에 대한 로봇 손의 파지 결과들을 나열한 그림이 포함되어 있는데, 이를 통해 DexFlow가 큰 물체부터 작은 도구, 원통형 물체, 박스형 물체 등에 이르기까지 다양한 형태의 그립을 구현하는 모습을 볼 수 있습니다. 특히 사람 손의 의도가 잘 반영되어, 예를 들어 긴 막대형 물체는 집게손가락과 엄지로 집는 동작, 큰 원통형 물체는 손바닥 전체로 감싸쥐는 동작 등 맥락에 맞는 파지 형태가 나오는 것이 인상적입니다. 이러한 정성적 결과는 DexFlow의 데이터가 자연스러운 인간 그립 동작을 닮았기 때문으로, 기존 생성 기법에서 지적된 부자연스러운 손모양 문제를 크게 완화시켰습니다."
  },
  {
    "objectID": "posts/paper/2025-08-08-dexflow.html#결론-및-시사점",
    "href": "posts/paper/2025-08-08-dexflow.html#결론-및-시사점",
    "title": "📃DexFlow 리뷰",
    "section": "2.5 결론 및 시사점",
    "text": "2.5 결론 및 시사점\nDexFlow는 인간 손 모션을 로봇 손으로 옮기는 손 포즈 리타게팅 문제와, 로봇 손의 물체 파지 상호작용 문제를 하나의 프레임워크 안에서 효과적으로 해결한 통합 접근법입니다. 기술적으로 전역-국소 이중 단계 최적화, 접촉 상태 인식 및 시간적 안정화, 대규모 데이터 통합 등의 기여를 통해, 기존 방법들이 개별적으로 다뤘던 문제들을 한꺼번에 addressed하였습니다. 실험 결과 DexFlow는 정량적 지표에서 기존 대비 뛰어난 성능(특히 성공률 약 7~8배 향상, 관통/접촉 오류 감소 등)을 보였고, 정성적으로도 인간스러운 그립 동작을 다양하게 구현해냈습니다. 비록 일부 최고 성능 기법들과 비교해 단일 프레임 성공률만 놓고 보면 절대값에서 약간 모자랄 수 있으나, DexFlow는 종합적인 균형과 데이터 활용성 면에서 새로운 패러다임을 제시했다고 평가할 만합니다. 특히 본 논문이 제공하는 대규모 로봇 손 조작 데이터셋과 접촉 처리 기법은 향후 이 분야 연구자들에게 소중한 자원이자 아이디어의 기반이 될 것입니다. 저자들도 논문에서 현재 한계로 입력 데이터(인간 시연)의 정밀도 문제와 메타데이터 오차에 따른 제한사항을 언급하며, 앞으로 비디오로부터 직접 신뢰도 높은 접촉 정보를 얻는 방향 등 추가 연구 과제를 남겼습니다. 그럼에도 불구하고 DexFlow는 로봇 손의 섬세한 조작을 위한 데이터 생성과 모델링에 있어서 새로운 지평을 열었으며, 향후 로봇 학습, 텔레로보틱스, 인간-로봇 상호작용 분야에서 다양하게 응용될 것으로 기대됩니다. 전체적으로 DexFlow는 손 기반 조작 연구 커뮤니티에 정확성, 자연스러움, 다양성을 모두 충족시키는 솔루션의 가능성을 보여준 의미있는 성과입니다.\n참고 문헌: DexFlow 논문 원문 및 관련된 선행 연구들을 참조하였습니다."
  },
  {
    "objectID": "posts/paper/2025-09-06-fungrasp.html",
    "href": "posts/paper/2025-09-06-fungrasp.html",
    "title": "📃FunGrasp 리뷰",
    "section": "",
    "text": "Paper Link\nHomepage"
  },
  {
    "objectID": "posts/paper/2025-09-06-fungrasp.html#논문의-핵심-기여점-요약",
    "href": "posts/paper/2025-09-06-fungrasp.html#논문의-핵심-기여점-요약",
    "title": "📃FunGrasp 리뷰",
    "section": "2.1 1. 논문의 핵심 기여점 요약",
    "text": "2.1 1. 논문의 핵심 기여점 요약\nFunGrasp는 다양한 로봇 손(dexterous hands)에 기능적 파지(functional grasping)를 구현하기 위한 새로운 시스템을 제안한 연구입니다. 기존의 다지 로봇 손 연구들은 주로 물체를 강하게 쥐는 힘 파지(power grasp)에 집중하여 작업 특유의 그립 자세는 간과되어 왔습니다. 이에 반해 FunGrasp는 사람의 작업 맥락에 맞춘 그립(pose)을 로봇 손에 모사함으로써, 예를 들어 가위를 자르기 위해 손잡이를 쥐거나, 안전하게 건네주기 위해 날부분을 잡는 등 작업에 최적인 자세를 로봇이 취하도록 합니다. 이 논문의 주요 기여는 다음과 같습니다:\n\nFunGrasp 시스템 구현: 단 하나의 RGB-D 이미지에 포착된 인간의 작업별 그립 자세로부터, 미지의 새로운 물체에 대해서도 원샷(one-shot) 일반화를 달성하는 기능적 파지 로봇 시스템을 제시하였습니다. 이 시스템은 시뮬레이션을 넘어 실제 환경에서도 동작하며, 인간 그립 영상 하나만으로 다양한 새로운 물체를 잡을 수 있습니다.\nH2R 그립 재타게팅 모듈: Human-to-Robot (H2R) 그립 재타게팅 모듈을 개발하여, 인간의 작업 특화 그립 자세를 다양한 형태의 로봇 손 모델로 효과적으로 전이합니다. 이때 인간과 로봇 손의 형태적 차이(손가락 개수, 관절 구성 등)에도 불구하고 인간 유사한 손가락 자세와 정확한 접촉 지점을 최대한 보존하는 것이 특징입니다.\n시스템 식별 기반의 동역학 모델 보정: 로봇 손의 정확한 관절 동역학 모델을 확보하기 위해 시스템 식별(module) 기법을 도입하고, 이를 통해 시뮬레이션에서 학습한 정책을 현실 로봇에 강건하게 이식(sim-to-real)할 수 있도록 하였습니다. 구체적으로, 로봇 손 관절의 강성(stiffness) 및 감쇠(damping) 계수를 실제 하드웨어에 맞게 최적화하여 시뮬레이션과 현실의 차이를 줄였습니다.\n다양한 로봇 손 및 환경에 대한 일반화 실험: 서로 다른 구조를 가진 여러 로봇 손에 단일 시스템을 적용하여도 성능이 유지됨을 시뮬레이션 및 실제 로봇 실험을 통해 입증했습니다. 또한 다양한 새로운 물체에 대한 기능적 파지 성공률을 측정하고, 각 모듈의 효과를 종합적인 소절(ablation) 실험으로 검증함으로써 시스템의 구성 요소별 기여도를 밝혔다고 보고하였습니다.\n\n요약하면, FunGrasp는 기능적이고 다양한 작업별 그립 자세를 실제 다지 로봇 손에 구현한 첫 사례로서, 단일 인간 시연만으로 새로운 물체와 다양한 로봇 손에 빠르게 적응하는 점에서 큰 의의가 있습니다. 이러한 접근은 가정, 의료 등 사람의 일상 활동을 보조하는 로봇 손에 직접적으로 응용될 수 있는 잠재력을 보여줍니다."
  },
  {
    "objectID": "posts/paper/2025-09-06-fungrasp.html#주요-구성-요소-및-기술적-접근-방식",
    "href": "posts/paper/2025-09-06-fungrasp.html#주요-구성-요소-및-기술적-접근-방식",
    "title": "📃FunGrasp 리뷰",
    "section": "2.2 2. 주요 구성 요소 및 기술적 접근 방식",
    "text": "2.2 2. 주요 구성 요소 및 기술적 접근 방식\nFunGrasp 시스템은 세 단계 모듈로 구성되며, (A) 인간 그립 자세의 로봇 손 재타게팅, (B) 재타게팅된 자세를 활용한 동적 그립 제어(강화학습), (C) 학습 결과를 실제 로봇에 이전하는 시뮬레이션-실환경 전이로 나뉩니다. 그림으로 나타내면, 사람 손의 입력 자세를 받아 로봇 손 목표 자세로 변환하고, 이를 기준으로 로봇 손가락을 움직이는 제어 정책을 학습한 뒤, 여러 현실적인 보정 기법을 통해 실제 로봇에서 동작시키는 파이프라인입니다. 각 구성 요소의 기술적 접근은 다음과 같습니다. \n\n2.2.1 H2R 그립 자세 재타게팅\nFunGrasp는 단일 RGB-D 이미지에서 인간 손과 물체의 자세를 추정하여 입력으로 사용합니다. 연구진은 오프라인 학습된 손-물체 포즈 추정 모델(예: FoundationPose 등)을 활용하여, 컬러/깊이 이미지로부터 사람의 손 관절 각도, 물체의 6-자유도 자세(3D 위치 및 방향) 및 손-물체 접촉 지점을 인식합니다. 이렇게 얻은 인간의 기능적 파지 참고 값(hand grasp reference)을 로봇 손으로 재타게팅(retargeting)하는 것이 1단계의 목표입니다.\n재타게팅 모듈에서는 우선 사람 손과 로봇 손의 관절 대응 관계를 정의하지 않고도, 물체 좌표계에서 각 손가락 마디의 방향을 정렬시키는 방식으로 로봇 손의 초기자세를 결정합니다. 이후 최적화 알고리즘을 통해 이 초기 로봇 손 자세를 세밀하게 조정하여, 다음의 조건들을 만족하도록 합니다:\n\n정밀한 접촉 위치 보존: 인간 손가락이 물체를 접촉한 위치에 로봇 손가락도 최대한 근접하도록 손가락 위치를 미세 조정합니다. 이를 위해 접촉점 위치 오차를 줄이는 항목(L_{pos})의 손실 함수를 사용합니다.\n관통 및 충돌 최소화: 로봇 손가락 모델이 물체 모델을 관통(penetration)하거나 책상/자기 자신과 충돌(collision)하지 않도록 제약을 겁니다. 이를 위해 물체와의 겹침을 벌주는 손실(L_{pen}) 및 자기충돌/환경충돌 손실(L_{col})을 포함시켜 최적화합니다.\n힘닫힘(Force Closure) 확보: 로봇 손가락들이 물체를 놓치지 않고 안정적으로 잡도록 힘닫힘 조건을 평가하는 손실(L_{fc})을 도입하여, 접촉된 여러 면에서 물체를 고정할 수 있는 안정적 그립을 찾습니다.\n관절 가동범위 및 자세 유지: 각 로봇 손가락 관절은 물리적 가동 한계 내에서 움직이게 하고, 사람 손의 형태에 가까운 손가락 자세를 유지하도록 유도합니다. 이를 위해 인간 유사 자세에 대한 보상(term)을 추가하여 최적화 중 불필요하게 어색한 손 모양이 되지 않도록 합니다.\n\n이러한 다목적 최적화를 통해 FunGrasp의 재타게팅 모듈은 사람의 그립 의도를 로봇 손에 최대한 충실히 반영합니다. 기존의 단순한 관절각 매핑이나 손가락 끝점 맞춤 기법들은 각각 접촉 정확도 부족, 자세 불안정의 문제를 가졌는데, FunGrasp는 접촉 지점과 자세를 동시에 고려함으로써 보다 정밀하고 인간다운 그립姿勢를 얻어냅니다. 이 점은 이후 단계에서 로봇이 얇거나 작은 물체를 다룰 때 책상과의 충돌을 피하며 정확히 집는 능력으로 이어집니다. \n\n\n2.2.2 강화학습 기반 동적 파지 제어\n2단계에서는, 앞서 결정된 정적(static) 로봇 손 목표 자세를 실제로 구현하기 위한 동적인 폐루프 제어 정책을 학습합니다. 이는 단순히 그 자세로 손가락을 쳐들어 고정하는 것이 아니라, 팔과 손가락을 움직여 물체를 잡아 들어올리는 동작 전체를 수행하는 문제입니다. 연구진은 이를 강화학습(RL) 문제로 구성하여, 시뮬레이션 환경에서 로봇 손이 물체를 잡고 들어올리는 제스처를 배우도록 했습니다.\n강화학습 상태(state) 관측값에는 로봇 손의 손목 위치/자세와 속도, 물체의 6D 자세와 속도, 로봇 손의 모든 관절 각도가 포함됩니다. 여기에 더해, 핵심적으로 목표로 하는 그립 자세에 관한 정보가 제공되는데, 이는 이전 단계의 재타게팅 모듈이 출력한 로봇 손 목표 관절각도 설정과 목표 접촉 지점들입니다. 이러한 목표 그립 정보는 로봇에게 물체의 국소 형상에 대한 사전지식을 제공하는 역할을 합니다. 즉, 어떤 손가락이 어디를 잡아야 하는지에 대한 암시적 힌트로 작용하여, 한 가지 정책으로도 다양한 모양의 물체들을 다룰 수 있도록 돕습니다.\nFunGrasp의 정책은 실시간 센서 정보를 활용하는 폐루프(closed-loop) 제어입니다. 시뮬레이션에서는 접촉 여부나 힘 등의 접촉 상태 정보(c, f)도 알고리즘이 직접 관측할 수 있지만, 실제 환경에서는 이러한 특권 정보를 얻기 어렵습니다. 이를 해결하기 위해 FunGrasp는 교사-학생(teacher-student) 강화학습 전략을 채택하였습니다. 먼저 접촉 상태를 모두 알고 있는 시뮬레이션 환경에서 교사 정책(Teacher)을 충분히 학습시킨 후, 이 정책의 행동을 모방하면서 접촉 정보를 추론하도록 설계된 학생 정책(Student)을 추가 학습시킵니다. 학생 정책은 과거 몇 시각의 관절각도 변화, 목표 대비 손목 위치 오차, 그리고 이전 행동 값 등을 입력으로 받아 LSTM 기반 인코더를 통해 접촉 여부를 예측하고, 이를 이용해 제어를 수행합니다. 이 과정에서 교사 정책의 행동과의 차이를 최소화하는 행동 모방 손실과, 접촉 예측 정확도를 높이는 접촉 재구성 손실을 함께 최적화하여, 결과적으로 실제 센서로 얻을 수 있는 정보만으로도 교사와 유사한 성능을 내는 폐루프 제어기가 완성됩니다. 그 결과, 로봇 손은 자신의 관절 움직임과 물체 움직임을 감지하여 접촉을 추론하고, 물체가 미끄러지거나 외부 방해가 발생해도 실시간으로 손가락 힘과 자세를 조정하는 적응적 행동을 학습하게 됩니다. \n강화학습 보상 함수는 로봇이 목표 자세로 정확히 파지하고 안정적으로 물체를 다루도록 설계되었습니다. 구체적으로, 보상 R는 (1) 목표 잡기 자세에 대한 관절 위치 오차 보상 R_{jp}, (2) 지정된 접촉 점들을 달성하는 접촉 보상 R_{c}, (3) 불필요한 충돌이나 과도한 힘을 피하는 안전 보상 R_{safety}, (4) 인간과 유사한 손가락 자세 유지를 장려하는 보상 R_{pose}의 합으로 구성됩니다. 이 중 R_{c}에는 동적 가중치를 도입하여 접촉 지점 부근에서 정밀하게 손가락을 조정하도록 하였고, R_{safety}는 로봇 손가락이 책상이나 자기 손에 강하게 부딪히는 힘이 감지되면 패널티를 주어 충돌을 피하도록 유도합니다. R_{pose}는 각 손가락 마디의 공간적 방향이 인간 손의 해당 마디 방향과 얼마나 일치하는지를 비교하여 계산되며, 이를 통해 로봇 손이 인간다운 자세를 유지하면서 파지하도록 유도됩니다. 이러한 다항목 보상 설정 하에서, 강화학습 에이전트는 목표 접촉을 이루고 안정적으로 물체를 잡는 동작을 배우게 되며, 동시에 테이블과의 충돌을 회피하고 사람과 유사한 손 모양을 유지하는 해결책을 찾게 됩니다. \n정책 학습은 물리 시뮬레이터 RaiSim 상에서 진행되었으며, Proximal Policy Optimization (PPO) 알고리즘을 사용하여 다중 병렬 환경에서 훈련이 이루어졌습니다. 훈련에는 NVIDIA RTX 3090 GPU 1장과 128개의 CPU 코어가 동원되어 약 2일간 실행되었으며, 시뮬레이션 초기 단계에서 로봇 손의 손가락 관절은 인간 목표 각도의 일부까지만 구부린 상태로 세팅하여 학습을 용이하게 하였습니다. 이처럼 초기 자세를 인간 시연과 유사하게 설정함으로써 학습 초기에 무작위 탐색으로 인한 불필요한 실패를 줄이고 빠른 수렴을 유도하였습니다. 또한 에피소드 시작 시 로봇 팔(UR5 로봇팔)의 위치도 적절히 조정하여, 손이 물체 중심에서 일정 거리 떨어진 곳에서 물체를 향하도록 IK(역기구학) 기반으로 배치하였고, 속도/가속도 제한 등을 걸어 안전한 탐색을 보장했습니다. \n\n\n2.2.3 시뮬레이션-실환경 전이 기법\n시뮬레이션으로 학습된 정책을 실제 로봇 손에 이식할 때 발생하는 모델 차이, 센서 잡음 등의 문제를 극복하기 위해, FunGrasp는 여러 가지 Sim-to-Real 전이 기술을 병합하여 활용하였습니다. (1) 특권 정보 학습(Privileged Learning), (2) 시스템 식별(System Identification), (3) 도메인 랜덤화(Domain Randomization), (4) 중력 보상(Gravity Compensation)의 네 가지가 주요 기법입니다. 앞서 설명한 교사-학생 정책 지식증류 과정은 특권 정보 학습의 일환이며, 이로써 시뮬레이션 전용 정보에 의존하지 않는 정책을 확보했습니다.\n시스템 식별은 실제 로봇의 동작 데이터를 활용해 시뮬레이터의 모델 파라미터를 보정하는 과정입니다. FunGrasp에서는 우선 시뮬레이션 상에서 근사 파라미터로 학습한 정책을 실제 로봇(알레그로 손)에 오픈루프로 실행하여, 로봇의 관절 각도 변화 등의 실측 궤적 데이터를 수집했습니다. 그런 다음 동일한 입력 행동 sequence를 시뮬레이터에 적용하여 시뮬레이션의 관절 궤적과 실제 로봇의 관절 궤적 간의 차이를 측정하고, 이 차이가 최소화되도록 시뮬레이터의 관절 강성, 감쇠 계수 등을 CMA-ES 최적화 방법으로 조정하였습니다. 즉, 실제 로봇의 움직임을 가장 잘 모사하는 시뮬레이터 파라미터를 식별한 후, 그 값으로 시뮬레이터를 업데이트하고 정책을 미세 재학습(fine-tuning)함으로써 현실 오차를 줄였습니다. 이러한 시스템 식별 과정을 거친 정책은 관절 움직임의 관성, 마찰 등 현실적인 특성을 반영하므로, 보정 전보다 현실에서의 성공률을 크게 향상시켰습니다.\n도메인 랜덤화는 학습 시 시뮬레이션의 다양한 물리 속성을 무작위로 변화시키는 기법으로, FunGrasp에서는 관절 마찰계수, PID 제어 게인, 물체의 질량, 테이블 높이, 센서 잡음 등을 무작위로 변동시키며 정책을 훈련했습니다. 이렇게 함으로써 정책이 환경 변화나 모델 불확실성에 둔감하게 되고, 현실에서도 강인하게 동작할 확률이 높아집니다.\n마지막으로 중력 보상은 로봇 손가락의 무게로 인한 처짐 현상을 상쇄하기 위한 것입니다. 각 로봇 손 링크의 질량과 질량중심을 고려하여 관절별 중력에 의한 토크를 실시간 계산하고, 제어 명령에 이 중력 보정 토크를 추가로 더해주는 피드포워드 방식을 적용하였습니다. 이를 통해 로봇 손이 어떤 자세를 취하든 자기 무게에 의해 손가락이 벌어지거나 힘을 못 주는 문제를 완화하여, 의도한 접촉력과 자세를 정확히 유지하도록 하였습니다."
  },
  {
    "objectID": "posts/paper/2025-09-06-fungrasp.html#사용된-모델-및-학습-기법-실험-설정-분석",
    "href": "posts/paper/2025-09-06-fungrasp.html#사용된-모델-및-학습-기법-실험-설정-분석",
    "title": "📃FunGrasp 리뷰",
    "section": "2.3 3. 사용된 모델 및 학습 기법, 실험 설정 분석",
    "text": "2.3 3. 사용된 모델 및 학습 기법, 실험 설정 분석\nFunGrasp의 핵심 모델은 강화학습 정책 신경망입니다. 초기 교사 정책은 비교적 단순한 MLP(Multi-Layer Perceptron) 구조로, 관찰 상태를 입력받아 각 손가락 관절 및 팔 관절에 대한 목표 움직임(속도/토크)을 출력합니다. 이 교사 정책은 시뮬레이션 상에서 접촉 여부(c)와 접촉 힘(f)까지 모두 포함된 완전한 상태정보로 훈련되므로 이상적인 조건에서 성능을 극대화할 수 있습니다. 이후 학생 정책에는 LSTM(Long Short-Term Memory) 기반 인코더가 추가되어, 실제 환경에서 얻을 수 있는 고유감각 정보(관절 각도, 속도 등)와 과거 몇 스텝의 상태-액션 기록만으로 접촉 여부와 힘을 추정하도록 설계되었습니다. 학생 정책의 MLP 부분은 교사 정책의 가중치를 초기화 값으로 사용하고, LSTM 인코더는 접촉 재구성 손실과 행동 모방 손실에 의해 학습됩니다. 이 지식 증류(knowledge distillation) 방식의 학습 기법은 강화학습과 지도학습(모방학습)을 결합한 형태로, 시뮬레이터에서만 가능한 부가 정보를 교사로부터 간접적으로 전수받아 현실 적용성을 확보합니다. \n시뮬레이션 환경으로는 RaiSim 물리 엔진이 사용되었으며, 시간당 1000Hz 이상의 고속 시뮬레이션을 통해 미세한 접촉 동작까지 모델링하였습니다. 학습 알고리즘은 PPO로, 약 2일간의 훈련을 통해 정책이 수렴하였습니다. 학습 초기에는 에이전트가 물체를 제대로 잡지 못하고 자주 실패하지만, 설정된 복합 보상 함수의 지표들(접촉, 자세, 안전 등)을 점차 향상시키면서 에피소드 당 성공률이 높아집니다. 최종적으로 시뮬레이터 상에서 성공 에피소드 비율이 충분히 높아지면(예: 80% 이상), 해당 정책을 기반으로 앞서 기술한 학생 정책으로의 이식 및 추가 훈련을 진행합니다. \n실험 설정 측면에서, 저자들은 DexYCB 데이터셋의 우손(right-handed) 그립 시퀀스를 활용했다고 명시하고 있습니다. YCB Object Set은 다양한 가정용 물체들의 3D 모델과 실제 물체를 제공하는 벤치마크로, FunGrasp에서는 이 YCB 물체들 중 다수를 선택하여 실험에 사용했습니다. 학습 시에는 DexYCB의 전체 그립 레퍼런스 중 75%를 훈련용으로, 25%를 테스트용으로 분할하여 사용하였고, 훈련에 사용되지 않은 새로운 물체에 대해서 평가하였습니다. 특히 한 번도 본 적 없는 범주의 물체라도, 해당 물체를 사람이 잡고 있는 RGB-D 사진 한 장만 제공하면 그에 맞게 로봇 손이 파지를 시도하도록 설정하였습니다. 예를 들어 학습 데이터에 없던 망치나 인형도, 사람이 쥐고 있는 모습을 한 번 관측하면 그 기능적 잡기 방식을 로봇이 모방하도록 한 것입니다. \n하드웨어 구성으로는, 기본적으로 6자유도 UR5 로봇 팔 끝에 알레그로 로봇 손(Allegro Hand)을 장착한 형태를 사용했습니다. 알레그로 손은 사람 손과 유사한 4개의 손가락(검지~소지 3개 + 엄지)과 총 16개의 관절 자유도를 지닌 다지 로봇 손입니다. 추가로, 형태적 일반화 실험을 위해 인스파이어(Inspire) 손이라는 상이한 구조의 로봇 손도 사용되었는데, 이 손은 엄지에 4개 관절, 나머지 손가락들에 2개 관절씩을 가진 보다 간소화된 형태입니다. 이러한 서로 다른 로봇 손들에 대해, FunGrasp 시스템이 별도 구조 변경 없이 적용될 수 있음을 확인하고자 했습니다. 카메라는 Intel RealSense D435i 깊이 카메라를 고정 배치하여 사용하였으며, 물체의 6D 자세 추적에는 사전에 알려진 물체 CAD 모델을 이용하는 퍼스펙티브-n-포인트 기반 알고리즘(FoundationPose 등)을 활용했습니다. 실시간 제어는 10Hz로 수행되었는데, 매 시각 정책이 출력한 손가락 목표와 IK로 계산된 팔 움직임이 PD 제어기를 통해 로봇에 명령되는 방식입니다. 실험 중 안전을 위해 손끝 속도 0.25m/s, 가속도 0.3m/s^2 이하로 제한을 두었으며, 충돌 감지 시 즉시 종료하도록 하였습니다. \nFunGrasp의 성능 측정 지표로는 그립 성공률(Success Rate), 시뮬레이션 상에서 물체 흔들림 정도(Simulated Displacement, SimD), 목표 접촉 달성률(Contact Ratio) 등이 사용되었습니다. 그립 성공은 로봇이 물체를 들어올려 일정 시간 이상 떨어뜨리지 않고 유지하면 1회 성공으로 간주하였고, SimD는 잡은 후 물체의 미세한 움직임(흔들림 속도)을 나타내며 값이 작을수록 안정적 파지를 뜻합니다. Contact Ratio는 로봇 손이 달성한 실제 접촉 지점 수를 인간 시연시 목표로 한 접촉 지점 수로 나눈 비율로 정의되어, 1.0이면 모든 의도된 접촉을 이뤘음을 의미합니다. 추가로, 다양한 로봇 손 사이 일반화 성능 평가, 앞서 언급한 어블레이션(ablation) 실험 등도 실시하여 세부 모듈의 효과를 정량화했습니다. 전반적인 실험 구성은 시뮬레이션 결과를 우선 확인한 뒤 실제 로봇에서 최종 검증하는 순서로 이루어졌으며, 모든 실험은 다섯 번 이상의 반복 수행으로 신뢰도를 높였습니다."
  },
  {
    "objectID": "posts/paper/2025-09-06-fungrasp.html#실험-결과-및-성능-비교-분석",
    "href": "posts/paper/2025-09-06-fungrasp.html#실험-결과-및-성능-비교-분석",
    "title": "📃FunGrasp 리뷰",
    "section": "2.4 4. 실험 결과 및 성능 비교 분석",
    "text": "2.4 4. 실험 결과 및 성능 비교 분석\nFunGrasp의 실험 결과, 다양한 측면에서 시스템의 우수성과 한계를 보여줍니다. 먼저, 새로운 물체에 대한 원샷 기능적 파지 실험에서, 사람 시연 이미지만 주어졌을 때 로봇이 해당 물체를 제대로 잡아드는지를 평가하였습니다. 18가지의 가정용 물체들에 대해 시도한 결과, 평균 74%의 높은 성공률을 달성하였습니다. 여기에는 무겁고 기다란 망치부터 크지만 가벼운 바구니, 심지어 휘어지는 재질의 봉제 인형(loopy doll)까지 다양한 형태와 질량의 물체들이 포함되었습니다. FunGrasp는 훈련 때 전혀 접하지 못했던 이러한 물체들도 대부분 성공적으로 파지하였으며, 특히 인형처럼 훈련 물체들과 물리적 성질이 완전히 다른 경우에도 적절히 대응하는 모습을 보였습니다. 이는 본 시스템의 범용적인 일반화 능력을 잘 입증합니다. 성공 사례들을 보면, 예를 들어 양동이는 손잡이를 걸어 쥐고, 스프레이 병은 손잡이 목부분을 움켜쥐며, 장난감 자동차는 위에서 집어 들어올리는 등, 인간이 실제 사용하려고 의도한 방식대로 로봇이 물체를 파지하고 있음을 알 수 있습니다. 이러한 사람과 유사한 그립 형태들은 FunGrasp가 단순한 힘 파지가 아닌 과제 지향적 파지를 구현했음을 보여주는 정성적인 증거입니다. \n한편, 일부 한계 사례도 관찰되었습니다. 예를 들어 글루 건(공업용 풀건)의 경우, 모양이 특이하고 한쪽으로 치우쳐져 있어 로봇 손이 잡는 데 어려움을 겪어 성공률이 낮았습니다(2/6회 성공). 저자들은 글루 건의 비정형적 형상 때문에 이상적인 접촉 지점을 찾기 어려웠던 점을 한계로 지적하였습니다. 이 외에도 노트북 충전기 케이블과 같이 잡기가 애매한 형태의 일부 물체에서도 간혹 실패가 발생하였으나, 전반적으로 대부분의 물체에 대해 4회 이상/6회 시도 중 성공이라는 준수한 성적을 거두었습니다. 더욱이 FunGrasp의 폐루프 제어는 외부에서 일부러 물체를 잡아당기는 방해를 주었을 때도 즉각적인 적응 동작으로 그립을 유지하는 견고함을 보였다고 보고되었는데, 이는 공개된 보조 영상을 통해 확인할 수 있습니다. 이러한 적응 능력은 강화학습 기반 폐루프 제어의 이점으로서, 사전에 정해둔 정적 자세만 실행하는 개방형 제어보다 훨씬 유연하고 안정적인 파지 동작을 가능케 함을 의미합니다. \n다양한 로봇 손에 대한 일반화 실험에서는, FunGrasp가 로봇 손의 형태가 달라져도 일관된 성능을 내는지 평가했습니다. 시뮬레이션에서 인간 손과 형태가 상이한 Shadow Hand(쉐도우 핸드), Faive Hand, Allegro Hand 세 종류에 대해 각각 정책을 학습 및 테스트한 결과, 모든 손에서 성공률 75% 이상을 달성하였습니다. 구체적으로 Shadow Hand 75%, Faive Hand 81%, Allegro Hand 85%의 성공률을 보였는데, Allegro Hand의 성능이 가장 높았다고 보고되었습니다. 이는 알레그로 손이 상대적으로 손가락 마디가 굵어 잡기가 쉬운 구조인 덕분으로 분석되었습니다. 즉, 손가락 관절의 물리적 형태에 따라 약간의 난이도 차는 있지만, FunGrasp의 알고리즘 자체는 손가락 개수(Shadow는 5지, Allegro는 4지 등)나 관절 구성의 차이에 견고하게 대응함을 알 수 있습니다. 실제 로봇으로도 두 종류(Allegro, Inspire)의 손을 테스트한 결과, 두 로봇 손 모두 동일한 인간 시연 입력에 대해 얇고 작은 물체를 테이블 위에서 집어올리는 동작까지 성공적으로 수행하여 정량적 결과와 일치하는 일반화 능력을 확인했습니다. 특히 이는 책상과 손가락 간 잠재적 충돌 위험이 있음에도 불구하고 사람 시연의 정밀한 접촉 자세를 보존한 덕분에 가능했던 것으로 분석됩니다. \n성능 비교를 위해 저자들은 관련 선행 연구들과 FunGrasp를 정성적으로 대비하는 표를 제시하였습니다. 예컨대, D-Grasp(CVPR 2022)나 DexTransfer(2022) 등의 방법은 실제 로봇 적용과 다양한 객체 범주 일반화 측면에서는 성과를 보였으나, 기능적 그립에는 초점을 두지 않았습니다. 반면 Agarwal 등(CoRL 2023)의 연구는 기능적 파지를 다루었지만 단일 로봇 손에 국한되었고, UniDexGrasp 계열 연구들은 여러 손 모형을 고려했으나 동적 폐루프 제어가 아니었습니다. FunGrasp (본 연구)만이 실제 하드웨어에서 다양한 자세의 기능적 파지를 다종의 로봇 손에 걸쳐 구현함으로써, 이 모든 측면을 아우르는 포괄적인 해법을 제시한 것으로 평가됩니다. 요컨대 FunGrasp는 이전 기법들의 장점을 모으면서도 각자의 한계를 넘어서, 광범위한 범용성을 달성했다는 의의가 있습니다. (OpenReview, Proceedings of Machine Learning Research)\n마지막으로, FunGrasp의 각 모듈이 성능에 얼마나 기여하는지 확인하기 위한 어블레이션 실험 결과를 살펴보겠습니다. 첫째, H2R 재타게팅 모듈의 효과를 검증하기 위해, 이를 대체하는 두 가지 방안을 비교했습니다: (i) 기존의 DexGraspNet 모델을 이용해 로봇 그립 자세를 생성하는 경우와, (ii) Angle Reset이라 하여 인간 손가락 관절 각도를 로봇 손에 그대로 복사만 하는 경우입니다. 이 둘은 각각 “접촉점은 고려하나 자세 제약이 부족한 방법”과 “자세는 흉내내나 접촉 조정이 전혀 없는 방법”이라 할 수 있습니다. 시뮬레이션으로 동일한 강화학습을 수행한 결과, DexGraspNet 대안은 65% 성공률, Angle Reset은 62% 성공률을 보인 반면, FunGrasp의 재타게팅을 사용한 경우 85% 성공률로 월등히 높은 성능을 냈습니다. 또한 접촉 정확도를 나타내는 Contact Ratio도 각각 0.68, 0.65에 그친 대안들에 비해 FunGrasp는 0.79로 크게 향상되었습니다. 실패 양상을 분석하면, DexGraspNet 기반은 파지의 안정성(힘닫힘)을 높이기 위해 물체를 감싸쥐는 파워그립 자세만 생성하려는 경향이 있어 얇은 물체를 책상 위에서 집을 때 테이블과 충돌하는 사례가 많았습니다. Angle Reset 방식은 접촉점 정밀 조정이 전혀 없다보니 작은 물체에서 손가락이 빗나가거나 헛집는 경우가 빈번했습니다. 반면 FunGrasp의 재타게팅은 인간의 세밀한 접촉 위치와 손 모양을 유지해주므로, 이러한 상황에서도 보다 견고한 파지가 가능했고 결과적으로 성능 우위로 나타났습니다. \n둘째, Privileged Learning(교사-학생 학습)의 효과를 확인한 실험에서, 접촉 정보 사용 유무와 학습 방식에 따른 네 가지 조건을 비교했습니다. 접촉 정보를 아예 제공하지 않고 RL만 수행한 경우(w/o Priv. Info.) 성공률은 61%에 머물렀고, 접촉 정보가 없는 상태에서 LSTM 인코더를 바로 RL로 학습시킨 경우(w/o Priv. Learn.)는 40%로 성능이 크게 저하되었습니다. 후자의 경우 학생 정책을 한 단계로 학습시킨 것인데, 접촉 단서를 전혀 모르는 상태에서 랜덤하게 탐색해야 하므로 학습 난도가 높아진 것입니다. 반면 교사 정책(시뮬레이션 접촉 GT 사용)은 85%의 성공률을 보였고, FunGrasp의 학생 정책도 85%로 교사와 거의 동등한 성능을 달성하였습니다. 이는 LSTM 인코더를 활용한 2단계 학습 프레임워크가 효과적으로 작동하여, 시뮬레이터 상의 접촉 정보가 없어도 유사한 정보를 추론해냈음을 의미합니다. 특히 학생 정책의 성능이 교사 대비 손색이 없다는 것은, FunGrasp 접근법이 시뮬레이션-현실 간 정보 불일치 문제를 성공적으로 풀어낸 중요한 성과로 볼 수 있습니다. \n마지막으로, 시스템 식별과 중력 보상 기법의 중요성을 검증한 실험에서는, 이 두 가지를 제거한 경우의 실환경 성공률을 측정했습니다. 그 결과, 시스템 식별 없이 초기 추정 파라미터로만 정책을 쓴 경우 성공률이 가장 크게 떨어져 평균 38%에 불과했고, 중력 보상 없이인 경우도 59%로 성능 저하가 뚜렷했습니다. 반면 두 기법을 모두 적용한 본래 설정에서는 75%로 최고 성능을 보였습니다. 이는 정확한 관절 모델링의 중요성을 방증하는데, 특히 시스템 식별이 부족하면 거의 모든 테스트 물체에서 실패 확률이 크게 늘어남을 표에서 확인할 수 있습니다. 중력 보상 역시 무거운 손가락을 가진 로봇 손의 경우 파지 유지에 결정적인 역할을 함을 알 수 있습니다. 종합하면, FunGrasp에 도입된 여러 모듈들은 각각 의미있는 성능 향상에 기여하고 있으며, 이들의 조합이 최종 시스템의 강건함을 만들어냈음을 알 수 있습니다."
  },
  {
    "objectID": "posts/paper/2025-09-06-fungrasp.html#장점-및-제한점-평가",
    "href": "posts/paper/2025-09-06-fungrasp.html#장점-및-제한점-평가",
    "title": "📃FunGrasp 리뷰",
    "section": "2.5 5. 장점 및 제한점 평가",
    "text": "2.5 5. 장점 및 제한점 평가\nFunGrasp는 다관절 로봇 손 조작 분야에 여러 가지 혁신적 장점을 제공합니다. 가장 큰 강점은 범용성과 유연성입니다. 하나의 통합된 시스템으로 새로운 객체와 새로운 로봇 손 구조 모두에 빠르게 적응할 수 있다는 점은 매우 고무적입니다. 예를 들어, 이전의 많은 연구들은 특정 범주의 물체나 단일 로봇 손에 특화되어 일반화 범위가 제한적이었는데, FunGrasp는 범주 불문 다양한 물체에 대해, 그것도 작업에 맞는 방식으로 잡는 고차원적인 목표까지 달성하였습니다. 또한 새로운 물체마다 인간의 데모를 한 번씩만 제공하면 되므로, 데이터 효율성 측면에서도 현실적인 장점을 갖습니다. 이는 복잡한 텔레조작 데이터 수집이나 대규모 3D 스캔 데이터 없이도, 사람의 시연 한 번으로 로봇이 기능적 동작을 학습한다는 뜻이어서 실제 현장에서 로봇을 가르치는 비용을 크게 줄일 수 있습니다.\n두 번째로, FunGrasp는 인간의 지식을 효과적으로 활용했다는 점에서 의의가 있습니다. 인간은 물체를 어떻게 잡으면 되는지 직관적으로 알지만, 로봇에게 이를 학습시키는 것은 어려운 문제입니다. 본 연구는 인간→로봇 그립 모방이라는 직관적인 접근을 취하면서도, 단순 모방의 한계를 넘어 정밀 접촉과 안정성까지 고려하였습니다. 그 결과 로봇의 그립이 단순히 물체를 놓치지 않는 것을 넘어, 사람처럼 물체를 유용하게 활용할 수 있는 형태가 되었습니다. 이는 향후 로봇이 사람과 같은 환경에서 협업하고 도구를 사용하는 데 필수적인 능력이며, FunGrasp는 그 실마리를 제시했다고 볼 수 있습니다.\n세 번째 강점은 실시간 동적 제어의 견고함입니다. 기존의 많은 방법들이 정적 파지 자세를 계획한 후 그대로 실행(Open-loop)하는 방식이어서, 실행 중 예기치 않은 물체 미끄러짐이나 외력에 취약했습니다. 반면 FunGrasp는 강화학습으로 폐루프 제어 정책을 학습하여, 센서 피드백에 따라 그립을 지속적으로 보정할 수 있습니다. 실험에서 보인대로, 물체를 잡은 뒤 일부 방해를 줘도 로봇이 자동으로 움켜쥐는 힘과 각도를 조절하여 떨어뜨리지 않으려는 적응 행위를 보여주었습니다. 이러한 능력은 로봇 손을 실제 세계의 불확실성 속에서 신뢰성 있게 운영하는 데 핵심적인 요소입니다.\n네 번째로, FunGrasp는 실제 로봇 구현까지 검증되었다는 점에서 중요합니다. 유사한 이전 연구들 중에는 시뮬레이션상 수십만 가지 물체에 대한 놀라운 성능을 보이지만 정작 현실 로봇에는 검증되지 않은 경우도 있었습니다. FunGrasp는 설계 단계부터 Sim-to-Real 전이를 고려하여, 비교적 간단한 하드웨어(UR5 + 로봇 손, RGB-D 카메라) 조합으로도 실험을 성공시켰습니다. 이는 해당 알고리즘이 현실적 한계까지 감안하여 개발되었음을 의미하며, 결과적으로 논문 발표와 동시에 즉시 실용적 응용이 가능한 수준의 성능을 입증했다는 점에서 높이 평가할 만합니다.\n그럼에도 불구하고, 이 연구에는 몇 가지 제한점도 존재합니다. 우선, 물체의 3D 모델 정보를 사전에 알고 있어야 한다는 점입니다. FunGrasp 시스템은 이미 알려진 물체의 경우에 그 물체의 CAD 모델을 이용해 정확한 자세 추정을 하고 접촉 정보를 얻을 수 있었지만, 완전히 처음 보는 물체에 대해선 동일한 방식으로 접근하기 어렵습니다. 실제로 실험에서도 YCB 벤치마크처럼 표준 객체 세트 내에서 평가를 진행한 것으로 보이며, 임의의 일상 물체에 대해 모델 없이 동작할 수 있는지는 추가 연구가 필요한 부분입니다. 이와 관련하여 저자들도 현재는 물체 메쉬가 알고 있다는 가정이 한계이며, 장차 이미지 입력만으로 이를 대체할 통합 모델 개발이 필요하다고 언급하였습니다.\n또 다른 제한으로는, 각 새로운 물체마다 사람의 데모가 필요하다는 점을 들 수 있습니다. 본 연구는 one-shot 상황에 초점을 맞추었지만, zero-shot 일반화 즉 시연 없이도 로봇이 스스로 알맞은 기능적 파지를 찾는 것은 지원하지 않습니다. 물론 한 번의 시연이면 충분하다는 것은 큰 장점이지만, 만약 사람의 시연을 구하기 어려운 상황(예: 위험한 환경의 물체)에서는 이 접근을 바로 적용하기 어렵습니다. 궁극적으로는 사람 시연조차 필요 없이, 로봇이 학습된 경험과 사전 지식만으로 물체를 어떻게 잡을지 판단하게 하는 방향으로 나아가야 할 것입니다.\n이외에도 기술적인 한계로, 시각 모듈과 제어 모듈 간의 분리로 인한 잠재적 오류 전파 문제가 있습니다. 예를 들어 카메라로 측정된 손-물체 자세에 오차가 있을 경우 재타게팅 결과도 부정확해지고, 이는 강화학습 정책에도 영향을 미칠 수 있습니다. FunGrasp에서는 pose estimator의 잡음을 저역통과 필터로 안정화하는 등 대처를 했지만, 완전히 근본적인 해결은 아닌 것으로 보입니다. 또한 현재 시스템은 단일 손의 파지에 초점을 맞추고 있어, 두 손으로 물체를 협동으로 잡는 양손 파지나 파지 이후의 도구 사용 동작 등까지는 다루지 못합니다. 마지막으로, 성공률 74%라는 수치도 아직 상용화 관점에서 절대적인 신뢰성을 확보했다고 보긴 어렵습니다. 특히 일부 복잡한 형상의 물체에서는 실패가 누적될 가능성이 있으므로, 이러한 케이스를 더 줄이는 보완이 필요합니다.\n요약하면 FunGrasp는 혁신적인 기능성과 범용성을 지닌 시스템이지만, 사전 물체 모델 가정과 인간 시연 필요성 등의 한계가 존재하며, 이 부분은 향후 연구를 통해 개선될 여지가 있습니다."
  },
  {
    "objectID": "posts/paper/2025-09-06-fungrasp.html#향후-연구-방향-및-응용-가능성",
    "href": "posts/paper/2025-09-06-fungrasp.html#향후-연구-방향-및-응용-가능성",
    "title": "📃FunGrasp 리뷰",
    "section": "2.6 6. 향후 연구 방향 및 응용 가능성",
    "text": "2.6 6. 향후 연구 방향 및 응용 가능성\nFunGrasp 연구는 기능적 로봇 파지에 대한 새로운 장을 열었으며, 이를 바탕으로 여러 흥미로운 향후 연구 방향과 폭넓은 응용 가능성이 예상됩니다. 우선 학계적으로는, 보다 통합된 인지-제어 모델로의 발전이 자연스레 제기됩니다. 앞서 한계에서 지적한 바와 같이, 물체 모델에 의존하지 않고도 동작하기 위해서는 로봇이 카메라 이미지 자체에서 물체의 잡기 포인트와 용도를 이해할 수 있어야 합니다. 이를 위해 딥러닝 기반의 엔드투엔드(visuo-motor) 정책 학습이나, 대규모 시각-언어 사전지식을 활용한 작업 의미 추론 등을 접목하는 방향이考案될 수 있습니다. 예를 들어, 미래에는 로봇이 카메라로 처음 본 가위를 보고도 “이것은 자르는 도구이니 손잡이를 잡아야겠다”고 추론하여 파지하는, 더 높은 수준의 인지 능력까지 포함한 시스템으로 발전할 수 있을 것입니다.\n또한 진정한 의미의 원샷 학습으로 나아가기 위해, 단순히 한 번 시연을 본 물체만이 아니라 유사한 물체 전체에 일반화하는 연구도 필요합니다. FunGrasp가 one-shot generalization을 달성하였다고는 하지만, 이는 특정 개체의 시연을 바로 그 같은 개체에 적용한 형태입니다. 향후에는 예컨대 한 종류의 가위 시연 한 번으로 모든 종류의 가위를 쥘 수 있다거나, 사람 시연 없이 VR 등을 통한 간접 시연으로도 학습하는 형태로 확장될 수 있습니다. 이를 위해서는 메타 러닝이나 도메인 적응 기법을 접목하여 시연 경험의 범용화를 도모하는 연구가 이어질 것으로 보입니다.\nFunGrasp의 접근은 다지 로봇 손의 활용 범위를 크게 넓혔기 때문에, 응용 측면에서도 많은 기회가 있습니다. 예를 들어 서비스 로봇이나 케어 로봇 분야에서, 사람이 일일이 로봇에게 동작을 프로그래밍하지 않고 직접 시범을 보이며 가르칠 수 있는 직관적 인터페이스로 활용될 수 있습니다. 가령 간병인이 로봇에게 환자에게 컵을 쥐여주는 방법을 한 번 보여주면, 로봇은 그 컵을 같은 방식으로 잡아 환자에게 전달할 수 있을 것입니다. 산업 현장에서도 새로운 공구나 부품을 로봇에게 가르칠 때 유용할 것입니다. 작업자는 보호장갑을 낀 자신의 손으로 공구를 어떻게 잡는지 한번 보여주고, 로봇은 이를 즉각 학습하여 동일한 자세로 공구를 잡은 후 필요한 작업을 수행하게 할 수 있습니다. 이러한 라이브 데모 기반 학습은 기존의 프로그래밍이나 오프라인 학습보다 훨씬 빠르고 유연하게 로봇 투입을 가능하게 만들어줄 것으로 기대됩니다.\n더 나아가, FunGrasp의 기술을 발전시키면 로봇이 단순 파지를 넘어 물체를 능숙하게 조작(manipulate)하는 단계로도 나아갈 수 있습니다. 예컨대 가위를 손잡이에 쥐는 것에서 끝나지 않고, 실제로 가위질을 해서 물체를 자르는 동작까지 연계한다면 기능적 파지→기능 실행으로 임무를 확장할 수 있을 것입니다. 이를 위해서는 파지 이후의 세부 동작 계획과 력 제어 등이 추가로 필요하지만, 기능적 파지가 선행되어야 가능한 작업이므로 FunGrasp의 성과는 이러한 후속 연구의 토대가 됩니다. 실제로 FunGrasp의 저자들도 이후 연구로 양손을 이용한 물체 조작(예: 문고리를 한 손으로 잡고 다른 손으로 돌리기)이나 물체의 상태 변화(관절 움직임 등)까지 고려한 파지 등의 문제를 다루고자 하고 있으며, 관련된 선행 연구(ArtiGrasp 등)와의 접목도 충분히 가능해 보입니다.\n마지막으로, 센서 기술의 발전과의 통합도 기대되는 방향입니다. 현재 FunGrasp는 시뮬레이션 접촉 정보를 LSTM으로 추론하는 방식을 썼지만, 향후 로봇 손가락에 실제 고해상도 촉각 센서가 내장된다면 훨씬 풍부한 피드백으로 제어를 정교화할 수 있습니다. 예컨대 인간은 손끝의 촉각으로 미끄러짐을 바로 감지해 힘을 조절하는데, 로봇도 촉각 센서와 FunGrasp의 학습 정책을 결합하면 더욱 인간에 가까운 세밀 조작이 가능해질 것입니다. 요컨대, FunGrasp는 기능적 파지의 중요성을 환기시키고 실현 가능한 솔루션을 제시했다는 점에서 이후 많은 연구의 출발점이 될 것이며, 사람과 로봇이 함께 생활하고 작업하는 미래를 앞당기는 데 기여할 것으로 전망됩니다. \n\n주요 출처: FunGrasp 원문(초록·방법·결론·표/그림)과 구현·실험 상세(하드웨어, 학습 설정, 74% 성공률, 소절 결과 등)은 다음을 참고했습니다."
  },
  {
    "objectID": "posts/paper/2025-07-09-tacEx.html",
    "href": "posts/paper/2025-07-09-tacEx.html",
    "title": "📃TacEx 리뷰",
    "section": "",
    "text": "Paper Link\nProject Link"
  },
  {
    "objectID": "posts/paper/2025-07-09-tacEx.html#introduction",
    "href": "posts/paper/2025-07-09-tacEx.html#introduction",
    "title": "📃TacEx 리뷰",
    "section": "2.1 Introduction",
    "text": "2.1 Introduction\n로봇 제어 정책을 시뮬레이션으로 학습시키는 접근법이 최근 널리 활용되고 있지만, 접촉이 많은 정교한 조작 작업에 사용할 정밀하고 신뢰성 높은 촉각 시뮬레이터는 여전히 부족한 상태이다. 촉각 센싱은 인간의 촉감 인지와 로봇의 복잡한 조작 과제를 수행하는 데 핵심적인 역할을 하며, 접촉 지점을 통한 물체의 기하 형태, 재질 강도, 표면 질감 등의 정보를 얻거나 미끄럼 감지, 경도 추정, 부드러운 물체 파지와 같은 작업에 활용된다. 이러한 촉각 피드백을 로봇 제어에 적용하려면 고차원 센서 관측을 다루는 복잡한 제어가 필요하므로, 최근 심층 강화학습(Deep RL) 기법들이 탐구되어 왔다. 그러나 현재 심감 풍부한 조작 작업에 RL을 적용하는 데 가장 큰 장애 요인은, 연질 물체 간 상호작용과 촉각 센싱을 모두 포함하는 안정적이고 신뢰할 수 있는 접촉 시뮬레이션의 부재이다. 최근 들어 이를 극복하기 위한 촉각 시뮬레이터들이 여럿 등장했지만, 각자 물리 엔진, 촉각 센서 모델, 로봇 플랫폼이 제각각이라 결과를 비교하거나 상호운용하기 어려운 실정이다.\n이러한 격차를 해소하고자 본 논문의 저자들은 TacEx라 명명된 새로운 촉각 시뮬레이션 프레임워크를 제안하였다. TacEx는 NVIDIA Isaac Sim 환경과 그 상위 RL 프레임워크인 Isaac Lab에 모듈식(modular)으로 내장되어 작동하며, 최신 촉각 시뮬레이션 기법들을 통합한 확장 가능한 플랫폼을 지향한다. 특히 GPU 가속 연질 물체 시뮬레이터인 GIPC를 통합하여 기존에 부족했던 연질-연질 접촉의 물리적 변형까지 실시간으로 모사하고, 시각 기반 촉각 시뮬레이터(Taxim, FOTS)를 함께 결합함으로써 GelSight와 같은 비쥬얼태크타일(visuotactile) 센서의 작동을 포괄적으로 시뮬레이션한다. 아래에서는 TacEx의 주요 기여와 기술 구현, 실험 평가 결과를 순차적으로 살펴보고, 이 프레임워크의 장단점과 향후 연구에 대한 시사점을 논의한다."
  },
  {
    "objectID": "posts/paper/2025-07-09-tacEx.html#main-contributions",
    "href": "posts/paper/2025-07-09-tacEx.html#main-contributions",
    "title": "📃TacEx 리뷰",
    "section": "2.2 Main Contributions",
    "text": "2.2 Main Contributions\nTacEx 프레임워크가 기존 대비 제공하는 핵심적인 기여는 다음과 같다:\n\n통합된 촉각 시뮬레이션 플랫폼 – 다양한 기존 촉각 시뮬레이터들이 서로 다른 엔진과 환경에서 동작했던 한계를 극복하고, NVIDIA Isaac Sim 안에 일원화된 모듈형 프레임워크로 구축되었다. 이를 통해 각기 다른 물리 엔진·센서·로봇으로 인해 비교가 어려웠던 상황을 개선하고 상호 운용성을 높였다. 한 환경에서 다양한 촉각 센서 시뮬레이션을 수행할 수 있어 비교 평가 및 공동 활용이 용이하다.\n첨단 연질 물리 시뮬레이션 통합 – TacEx는 GIPC라 불리는 최신 GPU 가속 FEM 기반 연질 물체(contact) 시뮬레이션을 통합하였다. GIPC는 기존의 단순 강체(contact) 모델이나 간소화된 연질 모델과 달리, 비선형 연질-연질 접촉을 정확하고 안정적으로 모사할 수 있으며(static/dynamic 마찰까지 포함), 완전 GPU 기반 구현으로 연산 속도 또한 대폭 향상되었다. 이는 동시대 연구인 TacSL 등의 접근과 비교했을 때 보다 정밀한 젤패드 변형 모델링을 제공하는 차별점이다.\n시각-촉각 결합 센서 렌더링 – 물리 시뮬레이션뿐 아니라 GelSight 촉각 센서의 출력 자체를 사실적으로 재현하기 위해 비쥬얼태크타일 시뮬레이션을 함께 구현하였다. TacEx는 기존 GelSight 시뮬레이터인 Taxim(광학적 촉각 영상 생성)과 FOTS(젤패드 표면의 마커 움직임 필드 생성)를 Isaac Sim에 내장함으로써, 물체와 접촉했을 때 얻어지는 RGB 촉각 영상과 마커 변위 데이터를 동시에 만들어낸다. 이 소프트 바디 + 시각 센서 하이브리드 접근을 통해 실제 GelSight 센서의 동작을 종합적으로 모사하는 새로운 방식을 제시하였다.\n모듈식 설계와 구성 유연성 – TacEx는 물리 시뮬레이션 방식과 센서 렌더링 구성을 필요에 따라 선택적으로 켜고 끌 수 있도록 설계되었다. 사용자는 과제의 요구에 맞춰 젤패드 물리 모델을 간단한 강체 접촉 모드부터 PhysX 기반 연질 모드, GIPC 기반 연질 모드까지 선택할 수 있으며, 광학 시뮬레이션이나 마커 시뮬레이션 기능도 모듈 단위로 활용할 수 있다. 이러한 유연성은 정확도와 속도 사이의 트레이드오프를 상황에 맞게 조율할 수 있게 해주며, 프레임워크의 확장성을 높이는 장점이 된다.\n강화학습 환경과의 통합 – 본 시뮬레이터는 Isaac Lab 기반으로 구현되어 별도의 복잡한 설정 없이도 강화학습(RL) 실험에 바로 활용될 수 있다. 저자들은 TacEx를 이용해 물체 밀기, 물체 들어올리기, 막대 균형잡기와 같은 접촉 풍부한 작업의 RL 환경을 구성하고, PPO 알고리즘 등으로 정책을 학습시켰다. 이를 통해 TacEx가 병렬 시뮬레이션 및 텔레오퍼레이션 지원, 그리고 기존 RL 라이브러리와의 연계 등 학습 워크플로우에의 적합성을 입증한 것도 중요한 기여이다."
  },
  {
    "objectID": "posts/paper/2025-07-09-tacEx.html#technical-implementation",
    "href": "posts/paper/2025-07-09-tacEx.html#technical-implementation",
    "title": "📃TacEx 리뷰",
    "section": "2.3 Technical Implementation",
    "text": "2.3 Technical Implementation\nTacEx는 NVIDIA Isaac Sim 상에서 물리 시뮬레이션 계층과 시각-촉각 렌더링 계층을 결합하여 구현되었다. 구체적으로, GelSight 센서의 젤패드 변형을 시뮬레이트하기 위해 세 가지 물리 모델이 제공된다:\n\n강체 탄성(Contact) 모델 – Isaac Sim의 기본 PhysX 엔진에서 젤패드를 탄성 계수를 가진 강체(compliant rigid body)로 간주하고 접촉 힘을 계산하는 간단한 모델.\nPhysX FEM 연질 모델 – PhysX 엔진의 유한 요소법(FEM) 기반 연질 물체 시뮬레이션 기능을 이용하여 젤패드 변형을 표현하는 모드.\nGIPC 연질 모델 – 저자들이 새롭게 통합한 GIPC 엔진을 통해 젤패드를 FEM 기반 완전 연질 물체로 시뮬레이션하는 모드.\n\nPhysX 기반의 모드(i, ii)는 Isaac Sim에서 자산 속성만 설정하면 비교적 쉽게 구현 가능하지만, GIPC 연동(iii)을 위해서는 GIPC 라이브러리의 코드를 수정하고 파이썬 바인딩을 작성하는 추가 개발 작업이 필요하였다. GIPC는 IPC(Incremental Potential Contact) 기법을 GPU 상에서 구현한 것으로, 기존 CPU 기반 FEM보다 계산 안정성과 속도 면에서 우수한 성능을 보인다.\nTacEx–Isaac Sim 통합: TacEx의 물리 시뮬레이션은 Isaac Sim의 PhysX 엔진과 GIPC 엔진이 결합된 형태로 동작한다. 예를 들어 로봇 팔 끝에 GelSight 센서를 장착하여 어떤 물체를 누르는 상황을 생각해보면, 젤패드(센서의 젤 부분)는 센서 케이스에 부착되어 로봇의 움직임에 따라 함께 이동한다. 이러한 센서의 전체 운동(센서 케이스의 위치/자세 변화)은 PhysX가 처리하며, 그 결과 얻은 젤패드 부착 지점의 변위를 GIPC에 전달한다. GIPC 엔진은 젤패드 메시(mesh)의 나머지 비부착 자유 격자점들에 대해 물체와의 접촉 변형을 계산하여 새 위치를 결정하고, 그 변형 결과를 Isaac Sim 내의 해당 젤패드 메시에 실시간 업데이트해준다. 이로써 로봇의 강체 운동과 젤패드의 연질 변형이 동시에 시뮬레이션되어, GelSight 센서의 물리적 접촉 반응을 전체적으로 재현하게 된다.\n촉각 센서 출력 렌더링: 물리 시뮬레이션으로 결정된 젤패드 변형 상태를 기반으로, TacEx는 GelSight 시각 출력을 생성한다. 구체적으로, Isaac Sim 내 젤패드에 부착된 가상 카메라를 통해 접촉 면의 높이 맵(height map) 이미지를 획득한 후 가우시안 필터로 평활화한다. 이어서 이 높이 맵의 노멀 벡터를 다항식 LUT(룩업 테이블)에 대입하여 RGB 이미지로 변환하고, 그림자 효과를 추가함으로써 사람이 보는 GelSight 촉각 영상과 유사한 렌더링 이미지를 합성한다. 아울러, 생성된 높이 맵을 입력으로 별도의 FOTS 알고리즘을 적용하여 젤패드 표면의 마커(marker) 점들의 변위 벡터장도 계산한다. 이 마커 변위장은 젤패드의 미세한 변형 분포를 반영하는데, TacEx는 이를 최종 촉각 관측 정보로서 제공한다. 이러한 방식은 Taxim이 제공하는 데이터드리븐 광학 모델과 FOTS의 마커 모션 모델을 재사용하는 것으로, 실제 젤패드의 물리 모델링 정확도에 크게 의존하지 않으면서도 현실감 있는 촉각 출력을 산출할 수 있다는 장점이 있다.\n강화학습 연계: TacEx는 Isaac Lab에 통합되어 있어 강화학습 실험 워크플로우에 자연스럽게 녹아들도록 구현되었다. 예를 들어 병렬로 다수의 환경을 실행하여 데이터를 수집하거나, ROS 통신 지원을 통해 외부 제어기와 연동하고, VR 장비로 원격 조작(teleoperation)을 하는 등의 고급 기능을 Isaac Sim의 인프라를 그대로 활용하여 구현할 수 있다. TacEx에서 출력하는 고차원 촉각 관측 (젤패드 변형, RGB 이미지, 마커장 등)은 곧바로 RL 알고리즘의 입력으로 사용될 수 있으며, 저자들이 제공한 예시 환경과 코드베이스를 통해 PPO 등 표준 RL 알고리즘과도 손쉽게 접목시킬 수 있다."
  },
  {
    "objectID": "posts/paper/2025-07-09-tacEx.html#evaluation",
    "href": "posts/paper/2025-07-09-tacEx.html#evaluation",
    "title": "📃TacEx 리뷰",
    "section": "2.4 Evaluation",
    "text": "2.4 Evaluation\n저자들은 TacEx의 성능과 가능성을 검증하기 위해 여러 시나리오에서의 실험을 수행하였다. 주요 실험 사례와 결과는 다음과 같다:\n\n공 굴리기 (Ball Rolling): 단일 GelSight 센서를 장착한 로봇이 젤패드를 이용해 바닥의 공을 굴리는 동적 접촉 상황을 시뮬레이션하였다. 이 실험을 통해 시뮬레이터의 안정성과 병렬 확장성을 시험했는데, PhysX 강체 모드에서는 다수의 로봇을 동시에 시뮬레이션해도 안정적으로 동작했으나, 카메라 렌더링 메모리 제한 때문에 일정 수 이상의 환경에서는 VRAM이 한계에 도달했다. 한편 GIPC 기반 연질 모드의 경우 연산 부하가 크기 때문에 VRAM 제약으로 동시에 한 대의 로봇만 정상적으로 시뮬레이션할 수 있었다. 이는 TacEx가 동적 시나리오에서 현실감 있는 접촉을 모사하면서도, 연산 자원 측면의 제약을 받는다는 점을 보여준다.\n물체 들어올리기 (Object Lifting): 두 개의 GelSight 센서를 로봇 그리퍼에 장착하여 물체를 집어서 들어올리는 작업을 실험하였다. 이 예제를 통해 촉각 피드백을 활용한 그립(grasp) 안정성을 검증하고자 했는데, PhysX 연질 물체 모드만으로는 물체를 끝까지 붙들고 들어올리는 데 실패하였다. 젤패드의 강성, 감쇠 등의 여러 매개변수를 바꿔도 물체가 미끄러져 떨어졌는데, 이는 현재 PhysX의 연질 바디 시뮬레이션에 정적 마찰(static friction) 모델이 구현되어 있지 않기 때문이라고 보고되었다. 심지어 젤패드 하나만 연질로 하고 다른 하나는 강체로 한 혼합 설정에서도 그립이 불안정했으며, PhysX만으로는 신뢰성 있는 파지 동작을 재현하기 어려움을 확인하였다. 반면 GIPC 기반의 연질 젤패드 두 개를 사용한 경우 물체를 비교적 안정적으로 집어올릴 수 있었는데, 이는 GIPC의 접촉 모델이 정적 마찰과 연질-연질 상호작용을 표현할 수 있어 보다 실제에 가까운 파지 동작을 가능케 함을 보여준다.\n빔 비틀기 (Beam Twisting): 원단에 고정된 긴 연질 기둥(beam)을 로봇 팔에 부착된 두 젤패드로 붙잡고 비틀어 꼬는 극한 변형 시나리오를 실험하였다. 이 사례는 GIPC 연질 시뮬레이션의 한계를 시험하기 위한 것으로, 상당한 비틀림과 늘어남 변형에도 시뮬레이션이 발산하거나 불안정해지지 않고 유지되었다. 로봇이 비틀던 빔을 놓았을 때 원래 형태로 튕겨 돌아오는 과정에서 마찰도 현실성 있게 모사되어, TacEx의 GIPC 통합이 극한 변형 및 마찰 거동까지 견실하게 재현함을 확인하였다.\n강화학습 테스트: 앞서 구현된 TacEx 기반 3가지 RL 환경 (물체 밀기, 물체 들어올리기, 장대 세우기)을 활용하여, 촉각 센싱 정보를 포함한 RL이 가능한지 검증하는 실험도 수행되었다. 각 환경에서 젤패드 마커 변위장 등을 관측으로 사용하고, PPO 알고리즘으로 정책을 학습시킨 결과, 학습 파이프라인이 큰 문제 없이 작동함을 보였다. 다만 논문 시점에서는 아직 촉각 피드백을 활용한 성공적인 정책이 완성되지는 않았고, 저자들은 현재 해당 과제를 달성하기 위해 학습을 진행 중이라고 밝히고 있다. 그럼에도 불구하고, TacEx 환경에서 고차원 촉각 관측을 포함한 RL訓練이 안정적으로 가능함을 실증한 것은 의미가 크다.\n성능 벤치마크: TacEx의 시뮬레이션 속도 및 확장성에 대한 평가도 이루어졌다. 논문에서는 다양한 병렬 환경 개수에 대하여 프레임 당 시뮬레이션 소요 시간을 측정하여 표로 보고하고 있다. 광학+마커 시뮬레이션(Taxim+FOTS)의 경우 GPU 메모리 사용량이 커서 환경 수 증가에 따른 성능 저하가 뚜렷했으며, PhysX 물리 시뮬레이션도 연질 모델에서는 약 256개 환경에서 메모리 한계로 시뮬레이션이 불가해지는 등 확장에 제약이 있었다. 한편 GIPC 물리 시뮬레이션은 젤패드/물체의 메시 복잡도(격자 점/사면체 개수)에 따라 프레임 시간이 증가하는 양상을 보였지만, IPC 기반 기법을 완전히 GPU화한 덕분에 여전히 CPU FEM보다 대폭 향상된 속도를 나타냈다. 요약하면, TacEx는 단일/소수 환경 수준에서는 실시간 시뮬레이션에 근접하는 성능을 보여주나, 대규모 병렬 환경 실행 시에는 연산 자원이 크게 요구됨을 알 수 있다."
  },
  {
    "objectID": "posts/paper/2025-07-09-tacEx.html#strengths-and-limitations",
    "href": "posts/paper/2025-07-09-tacEx.html#strengths-and-limitations",
    "title": "📃TacEx 리뷰",
    "section": "2.5 Strengths and Limitations",
    "text": "2.5 Strengths and Limitations\n장점: TacEx가 보여준 주요 강점은 다음과 같다:\n\n현실감 있고 정밀한 촉각 모델링 – 연질 젤패드의 FEM 기반 변형을 모사함으로써, 기존 강체 근사 방식보다 정확한 접촉 표현이 가능하다. 특히 GIPC 통합으로 정적 마찰과 연질-연질 접촉까지 시뮬레이션되어 실제 센서의 물리 반응을 충실히 재현한다. 빔 비틀기 등의 극한 상황에서도 시뮬레이션이 안정적으로 유지된 것은 이러한 물리 모델의 신뢰성을 보여준다.\n모듈식 플랫폼의 유연성 – 사용자 필요에 따라 물리 엔진과 센서 시뮬레이션 구성요소를 선택적으로 활용할 수 있는 설계는 다목적 활용을 가능케 한다. 예를 들어, 빠른 계산이 필요하면 간략한 강체 모델만 사용하고, 정확성이 필요하면 GIPC 연질 모델을 켜는 식으로 정밀도↔︎속도 트레이드오프를 조절할 수 있다. 또한 Taxim/FOTS 기반의 광학 시뮬레이션은 젤패드 물리 정확도에 덜 의존하면서도 촉각 이미지를 만들 수 있어, 극단적인 경우 강체 젤패드로도 마커 변위장을 생성하는 등 유연한 센서 시뮬레이션이 가능하다.\nIsaac Sim 생태계 활용 – TacEx는 NVIDIA Isaac Sim에 네이티브하게 포함되므로, 동 시뮬레이터의 강력한 기능들을 모두 활용할 수 있다는 장점이 있다. 예를 들어 물리 기반 렌더링으로 고해상도 촉각 이미지를 얻거나, ROS 지원을 바로 활용하고, GPU 가속 PhysX를 통한 빠른 로봇 시뮬레이션과 병렬 환경 실행 등을 그대로 사용할 수 있다. 이는 별도 맞춤 시뮬레이터를 개발하는 것보다 활용도와 편의성이 높아, 연구 생산성 측면에서도 큰 이점을 제공한다.\n강화학습 및 연구 확장성 – 제공된 TacEx 환경들은 곧바로 RL 실험에 투입될 수 있을 만큼 잘 구성되어 있으며, 다양한 RL 알고리즘과 쉽게 연계된다. 본 프레임워크는 추가적인 촉각 센서 모듈이나 새로운 시나리오를 손쉽게 플러그인하는 형태로 확장할 수 있어, 향후 다른 유형의 촉각 센서나 멀티모달 센싱 시뮬레이션에도 응용할 수 있는 연구 플랫폼으로서의 잠재력이 있다.\n\n한계: 반면 TacEx에는 다음과 같은 한계와 향후 보완 과제가 존재한다:\n\n실세계 검증 부족 – 현재 결과들은 시뮬레이션 상의 질적 실험에 국한되어 있으며, TacEx에서 얻은 촉각 데이터로 학습된 정책이나 시뮬레이터 자체를 실제 로봇 및 센서에 적용해본 Sim2Real 검증이 이루어지지 않았다. 따라서 시뮬레이터의 실제 유용성을 평가하려면 추가적인 물리 실험과 비교가 필요하다.\n정량적 평가 미비 – 논문에서는 다양한 사례를 보여주었지만, 각 구성 요소에 대한 정량적 성능 평가나 시뮬레이션 정확도 분석(예: 실제 GelSight 센서 데이터와의 비교)이 충분히 이루어지지 않았다. 예를 들어 GIPC vs PhysX의 모델 오차나 촉각 영상의 픽셀 단위 정확도 등에 대한 계량 평가가 없어, 어느 정도의 물리 현실성을 확보했는지 정량적으로 판단하기 어렵다. 저자들은 향후 이러한 부분을 보완할 계획임을 언급하였다.\n높은 계산 자원 요구 – 사실적인 촉각 시뮬레이션의 대가로 연산 부하가 상당히 큰 편이다. 특히 GIPC 기반 연질 시뮬레이션과 고해상도 Taxim 렌더링은 GPU 메모리와 연산 시간을 많이 소모하여, 병렬 환경을 늘릴 경우 성능 저하나 메모리 부족에 직면한다. 이는 TacEx를 대규모 학습에 바로 활용하는 데 제약으로 작용할 수 있으며, 최적화나 경량화가 추후 과제로 남는다.\n엔진 종속적 제약 – TacEx가 의존하는 Isaac Sim/PhysX 엔진의 한계도 존재한다. 예를 들어 PhysX의 기본 연질 물리에서는 정적 마찰 모델 부재로 인해 실제와 다른 거동을 보였으며, 광학 시뮬레이션의 경우 Isaac Sim 카메라 성능에 일부 의존하기 때문에 매우 높은 프레임레이트로 진행하기 어렵다. 이러한 엔진 차원의 제한은 현재 GIPC 통합 등으로 일부 보완되었으나, 완전히 해결된 것은 아니다.\n초기 단계 및 공개 여부 – TacEx는 현재 워크숍에 게재된 비교적 초기 단계 연구로, 코드와 영상 등이 공개 예정이라고만 언급되어 있다. 커뮤니티에 공개되어 광범위한 테스트와 활용이 이뤄지기 전까지는, 예측하지 못한 문제나 사용성 측면의 개선점이 나타날 가능성이 있다."
  },
  {
    "objectID": "posts/paper/2025-07-09-tacEx.html#future-work-and-impact",
    "href": "posts/paper/2025-07-09-tacEx.html#future-work-and-impact",
    "title": "📃TacEx 리뷰",
    "section": "2.6 Future Work and Impact",
    "text": "2.6 Future Work and Impact\n논문의 결론에서 저자들은 TacEx의 다음 단계 연구 방향을 제시하고 있다. 우선, 현 시뮬레이터에 구현된 여러 물리 시뮬레이션 기법들의 성능을 정량적으로 비교하고, TacEx를 활용한 Sim2Real 실험을 통해 가상 환경에서 학습된 정책이 실제 로봇 촉각 센싱에도 통용될 수 있는지 검증할 계획이다. 또한 TacEx 프레임워크에 더 다양한 RL 환경과 촉각 시뮬레이션 방법을 계속 추가함으로써, 학계에서 공통으로 활용할 수 있는 촉각 시뮬레이션 벤치마크 플랫폼으로 발전시키고자 한다. 예를 들어, 다른 종류의 촉각 센서 모델이나 새로운 접촉 물리 엔진을 모듈로 통합하여 여러 방법론을 한 곳에서 시험·비교할 수 있도록 할 전망이다.\n이러한 노력은 촉각 로보틱스 연구 전반에 걸쳐 중요한 파급효과를 가져올 것으로 기대된다. TacEx와 같은 통합 시뮬레이션 플랫폼이 갖춰짐으로써, 연구자들은 촉각 데이터를 수반하는 복잡한 조작 과제를 보다 손쉽게 가상 실험으로 테스트하고, 효과적인 알고리즘을 빠르게 검증한 뒤 실제 로봇에 이전(Sim2Real)하는 사이클을 가속할 수 있다. 특히 시각 정보와 촉각 정보를 모두 포토리얼리스틱하게 생성해주는 TacEx의 접근은, 로봇이 사람처럼 시각–촉각 복합 인지를 할 수 있는 정교한 조작 기술을 연마하는 데 유용한 도구가 될 것이다. 앞으로 TacEx가 지속적으로 확장·보완되어 나간다면, 촉각 센싱을 활용한 로봇 학습의 표준 플랫폼이자 평가 벤치마크로서 자리매김하며 해당 분야의 발전을 크게 견인할 수 있을 것으로 보인다."
  },
  {
    "objectID": "posts/paper/2025-07-03-physics-rl.html",
    "href": "posts/paper/2025-07-03-physics-rl.html",
    "title": "📃Physics Informed RL Survey 리뷰",
    "section": "",
    "text": "Paper Link"
  },
  {
    "objectID": "posts/paper/2025-07-03-physics-rl.html#introduction",
    "href": "posts/paper/2025-07-03-physics-rl.html#introduction",
    "title": "📃Physics Informed RL Survey 리뷰",
    "section": "Introduction",
    "text": "Introduction\nRL은 시행착오를 통해 의사 결정 및 최적화 문제를 해결하는 유망한 접근 방식입니다. 자율 주행, 로봇 공학, 연속 제어 등 다양한 분야에서 성공을 거두었지만, 실제 데이터의 샘플 효율성 부족, 고차원 연속 상태/액션 공간 처리의 어려움, 안전한 탐색, 적절한 보상 함수 정의, 시뮬레이터-실제 환경 간의 차이 등의 문제에 직면해 있습니다. 물리 정보를 ML 모델에 통합하는 PIML(Physics-Informed Machine Learning)은 불완전한 물리 정보와 데이터로부터 더 효율적으로 학습하고, 더 나은 일반화 성능을 보이며, 물리적으로 타당한 솔루션을 제공하는 장점이 있습니다. RL은 대부분 실제 세계 문제와 관련이 있으며 설명 가능한 물리적 구조를 가지고 있기 때문에 물리 정보 통합에 적합한 분야입니다. 최근 연구들은 물리 정보를 RL 파이프라인에 통합하여 이러한 과제를 해결하고 있습니다. 예를 들어, 물리 정보를 사용하여 고차원 연속 상태를 직관적인 표현으로 줄이거나 더 나은 시뮬레이션을 구축하며, 안전한 학습을 위한 물리적 제약 조건을 보상 함수에 통합하는 등의 시도가 이루어지고 있습니다. PIRL 연구는 지난 6년간 증가하는 추세를 보이며 주목받고 있습니다.\n\nTaxonomy: 어떤 물리 지식/프로세스가 모델링되고, 어떻게 표현되며, RL 접근 방식에 어떻게 통합되는지에 대한 통합 분류 체계를 제시합니다.\nAlgorithmic Review: 물리 정보 기반 RL 방법론에 대한 최신 접근 방식을 통일된 표기법과 기능 다이어그램을 사용하여 검토합니다.\nTraining and evaluation benchmark Review: 검토된 문헌에서 사용된 평가 벤치마크를 분석하여 인기 있는 플랫폼/도구를 제시합니다.\nAnalysis: 다양한 도메인에 걸친 model-based 및 model-free RL 애플리케이션에서 물리 정보가 특정 RL 접근 방식에 어떻게 통합되는지, 어떤 물리 프로세스가 모델링/통합되는지, 어떤 네트워크 아키텍처 또는 증강이 사용되는지 상세히 분석합니다.\nOpen Problems: 현재 직면한 과제, 미해결 연구 질문 및 향후 연구 방향에 대한 관점을 제시합니다."
  },
  {
    "objectID": "posts/paper/2025-07-03-physics-rl.html#piml-an-overview",
    "href": "posts/paper/2025-07-03-physics-rl.html#piml-an-overview",
    "title": "📃Physics Informed RL Survey 리뷰",
    "section": "PIML: An Overview",
    "text": "PIML: An Overview\n\n물리 정보를 활용한 기계 학습 개요\n\nPIML은 수학적 물리 모델과 관측 데이터를 학습 과정에 통합하여, 불완전하고 불확실하며 고차원적인 복잡한 시나리오에서도 물리적으로 일관된 솔루션을 찾는 것을 목표로 합니다. 물리 지식을 ML 모델에 추가하는 것은 물리/과학적 일관성 보장, 데이터 효율성 증가, 학습 과정 가속화, 일반화 능력 향상, 투명성/해석 가능성 증진과 같은 이점을 제공합니다. 물리 지식을 통합하는 세 가지 주요 전략은 다음과 같습니다.\n\nObservational bias: 물리적 원리를 반영하는 multi-modal 데이터를 사용하여 DNN을 학습시킵니다. 관측, 시뮬레이션, 물리 방정식 생성 데이터, 지도, 추출된 물리 데이터 등 다양한 소스의 데이터를 활용합니다.\nLearning bias: 손실 함수에 물리 기반의 페널티 항을 추가하여 사전 지식을 강화하는 방식입니다. PINN(Physics-Informed Neural Networks)은 PDE를 신경망의 손실 함수에 포함시키는 대표적인 예입니다.\nInductive biases: custom neural network 구조를 통해 물리 원리를 ‘하드’ 제약 조건으로 통합하는 방식입니다. Hamiltonian NN, Lagrangian Neural Networks (LNNs) 등이 있습니다."
  },
  {
    "objectID": "posts/paper/2025-07-03-physics-rl.html#pirl-fundamentals-taxonomy-and-examples",
    "href": "posts/paper/2025-07-03-physics-rl.html#pirl-fundamentals-taxonomy-and-examples",
    "title": "📃Physics Informed RL Survey 리뷰",
    "section": "PIRL: Fundamentals, Taxonomy and Examples",
    "text": "PIRL: Fundamentals, Taxonomy and Examples\n\n물리 정보를 활용한 강화 학습: 기본, 분류 및 예시\n\nRL 기본 (RL fundamentals)\nRL은 MDP (Markov Decision Process) 프레임워크를 따르는 순차적 의사 결정 문제를 해결합니다. 에이전트(agent)와 환경(environment)이 상호 작용하며, 에이전트는 상태(s_t)를 관찰하고 행동(a_t)을 선택하며, 환경은 다음 상태(s_{t+1})와 보상(r_t)을 제공합니다. 목표는 누적 보상을 최대화하는 정책 \\pi_\\phi(a_t|s_t)의 매개변수 \\phi를 찾는 것입니다. MDP는 튜플 (S, A, R, P, \\gamma)로 표현되며, S는 상태 공간, A는 액션 공간, R은 보상 함수, P(s_{t+1}|s_t, a_t)는 환경 모델(전이 확률), \\gamma \\in [0, 1]는 할인 계수입니다. 목표 함수는 다음과 같습니다. J(\\phi) = \\mathbb{E}_{\\tau \\sim p_\\phi(\\tau)} \\left[ \\sum_{t=1}^T \\gamma^{t-1} R(a_t, s_{t+1}) \\right] 여기서 \\tau는 에피소드의 상태-액션 시퀀스입니다. RL 알고리즘은 model-free (환경 모델 없이 학습)와 model-based (환경 모델을 사용하여 계획/학습)로 나눌 수 있습니다. 또한, online (최신 정책으로 수집한 데이터 사용), off-policy (경험 리플레이 버퍼의 데이터 사용), offline (고정된 데이터셋 사용)으로 분류됩니다.\nPIRL 소개 (PIRL: Introduction)\nPIRL은 물리 구조, 사전 지식(priors), 실제 물리 변수를 정책 학습 또는 최적화 과정에 통합하는 개념입니다. 이는 RL 알고리즘의 효율성, 샘플 효율성, 훈련 가속화에 기여합니다.\nPIRL 분류 체계 (PIRL Taxonomy)\n이 논문은 물리 정보 유형, 물리 정보를 통합하는 PIRL 방법, 그리고 RL 파이프라인의 세 가지 축을 중심으로 PIRL 분류 체계를 제시합니다.\n\nPhysics information (types): representation of physics priors\n\nDifferential and algebraic equations (DAE): PDE/ODE, 경계 조건(BC) 등 시스템 동역학 표현 (예: PINN).\nBarrier certificate and physical constraints (BPC): CLF, BF, CBF/CBC 등 안전 제약 조건 (예: 안전 중요 애플리케이션의 탐색 규제).\nPhysics parameters, primitives and physical variables (PPV): 환경/시스템에서 추출/도출된 물리 값 (예: jam-avoiding distance, dynamic movement primitives).\nOffline data and representation (ODR): 시뮬레이터 기반 학습 개선을 위한 오프라인 데이터 또는 물리적으로 관련된 저차원 표현 학습.\nPhysics simulator and model (PS): RL 알고리즘의 테스트베드 또는 물리적 정확성을 부여하기 위한 시뮬레이터 활용 (예: MBRL에서 시스템 모델 학습).\nPhysical properties (PPR): 시스템 형태, 대칭 등 기본적인 물리 구조/속성 지식.\n\nPIRL methods: physics prior augmentations to RL\n\nState design: 관찰된 상태 공간 수정/확장 (예: 상태 융합, 특징 추출).\nAction regulation: 액션 값에 제약 조건 부과 (예: 안전 필터).\nReward design: 효과적인 보상 설계 또는 보상 함수 증강.\nAugment policy or value N/W: 정책 또는 가치 함수의 업데이트 규칙, 손실, 구조 변경.\nAugment simulator or model: 기초 물리 지식 통합을 통한 시뮬레이터/모델 개선.\n\nRL Pipeline\n\nProblem Representation: 실제 문제를 MDP로 모델링 (상태, 액션, 보상 정의).\nLearning strategy: 에이전트-환경 상호 작용 방식, 학습 아키텍처, 알고리즘 선택 결정.\nNetwork design: 정책/가치 네트워크의 세부 구조 설계.\nTraining: 네트워크 학습 (Sim-to-real 등 훈련 증강 포함).\nTrained policy deployment: 훈련된 정책 배포.\n\n\n추가 분류 (Further categorization)\n이 논문은 추가적으로 두 가지 범주를 사용하여 PIRL 구현을 설명합니다.\n\nBias: PIML에서 사용되는 bias 개념(Observational, Learning, Inductive)과 PIRL 접근 방식의 관계를 분석합니다.\nLearning architecture: 물리 정보 통합을 위해 전통적인 RL 학습 아키텍처에 도입된 변경 사항에 따라 분류합니다.\n\nSafety filter: 안전 제약 조건을 보장하기 위해 에이전트의 액션을 조절하는 모듈 포함.\nPI reward: 보상 함수를 물리 정보로 수정.\nResidual learning: 물리 정보 기반 제어기와 데이터 기반 정책을 결합.\nPhysics embedded network: 정책 또는 가치 함수 네트워크에 시스템 동역학 등 물리 정보 직접 통합.\nDifferentiable simulator: 손실 기울기를 제어 액션에 대해 직접 계산할 수 있는 미분 가능한 물리 시뮬레이터 사용.\nSim-to-Real: 시뮬레이터에서 학습 후 실제 환경으로 전이.\nPhysics variable: 물리 매개변수, 변수, 프리미티브를 상태/보상 등에 추가.\nHierarchical RL: 계층적 또는 커리큘럼 학습 설정에서 물리 정보를 통합.\nData augmentation: 입력 상태를 저차원 표현 등으로 대체/증강하여 물리적으로 관련된 특징 도출.\nPI model identification: MBRL 설정에서 물리 정보를 모델 식별 과정에 통합."
  },
  {
    "objectID": "posts/paper/2025-07-03-physics-rl.html#pirl-review-and-analysis",
    "href": "posts/paper/2025-07-03-physics-rl.html#pirl-review-and-analysis",
    "title": "📃Physics Informed RL Survey 리뷰",
    "section": "PIRL: Review and Analysis",
    "text": "PIRL: Review and Analysis\n\nAlgorithmic review: 위에 제시된 PIRL 방법 및 학습 아키텍처 범주를 기반으로 연구들을 그룹화하여 논의합니다. 예를 들어, State design에서는 CAV 제어에서의 물리 기반 상태 융합, Adaptive cruise control에서의 jam-avoiding distance 활용 등이 논의됩니다. Action regulation에서는 안전 중요 시스템의 CBF/CBC를 활용한 액션 제약이 강조되며, B_\\epsilon(x)와 Lie derivative \\mathcal{L}_f(x, u_{RL}) B_\\epsilon(x)를 이용한 안전 조건이 언급됩니다. Reward design에서는 로봇 보행, 에너지 관리, 유체역학 등 다양한 분야에서 물리 기반 보상 함수 설계 사례가 제시됩니다. Augment simulator or model에서는 LNN을 사용한 시스템 모델 학습, sim-to-real 전이 개선을 위한 시뮬레이터 증강, 미분 가능한 시뮬레이터 사용 등이 포함됩니다. Augment policy and/or value N/W에서는 신경망 정책에 동적 시스템을 미분 가능한 레이어로 통합하는 Neural Dynamic Policies (NDP), 가치 함수를 HJB PDE를 푸는 PINN으로 취급하는 접근 방식 등이 소개됩니다.\nSimulation/ evaluation benchmarks: 연구에서 사용된 다양한 시뮬레이터 및 평가 환경을 OpenAI Gym, MuJoCo, Pybullet, Deep mind control suite와 같은 표준 벤치마크와 SUMO, CARLA, IEEE distribution system benchmarks 같은 도메인별 플랫폼, 그리고 다수의 맞춤형 환경으로 분류하여 제시합니다.\nAnalysis:\n\n연구 동향 및 통계: 가장 많이 사용되는 RL 알고리즘은 PPO이며, 그 뒤를 DDPG, SAC 등이 잇습니다. 물리 정보 유형으로는 물리 시뮬레이터, 시스템 모델, 배리어 인증서/물리 제약이 가장 흔하게 사용됩니다. 학습 아키텍처 중 PI reward와 safety filter는 주로 learning bias를 통해, physics embedded network는 inductive bias를 통해 물리를 통합합니다. 애플리케이션 도메인의 85% 가량이 제어 또는 정책 설계와 관련 있으며, 그 중 Miscellaneous control, Safe control and exploration, Dynamic control이 주를 이룹니다.\nRL 해결 과제: PIRL은 다음과 같은 RL 과제 해결에 기여합니다. Sample efficiency (시뮬레이터/모델 증강), Curse of dimensionality (물리 관련 저차원 표현 학습), Safety exploration (CBF/CLF 등 제어 이론 활용), Partial observability (상태 증강/융합), Under-defined reward function (물리 기반 보상 설계/증강).\n\n\n미해결 과제 및 연구 방향 (Open Challenges and Research Directions)\n\nHigh Dimensional Spaces: 고차원 공간에서 물리적으로 관련된 정보성이 풍부한 저차원 표현을 학습하는 것이 여전히 과제입니다.\nSafety in Complex and Uncertain Environments: 복잡하고 불확실한 환경에서 model-agnostic하며 일반화 가능한 안전한 탐색 및 제어 접근 방식 개발이 필요합니다. 데이터 기반 모델 학습에 물리를 통합하는 일반화된 접근 방식도 중요합니다.\nChoice of physics prior: 문제에 적합한 물리 사전 지식을 선택하는 것은 어렵고 도메인별 전문 지식이 필요합니다. 새로운 물리적 태스크를 다룰 수 있는 포괄적인 프레임워크 구축이 필요합니다.\nEvaluation and bench-marking platform: PIRL 연구를 위한 포괄적인 벤치마킹 및 평가 환경이 부족하여 새로운 방법론의 비교 및 평가가 어렵습니다. 도메인별로 맞춤화된 환경에 의존하는 경향이 있습니다.\n\n결론 (Conclusions): 본 논문은 PIRL 패러다임을 소개하고, 물리 사전 지식 유형 및 물리 정보 통합 방식(RL 방법)에 기반한 분류 체계를 제시합니다. 또한, 학습 아키텍처 및 bias에 따른 추가 분류를 통해 PIRL 구현을 더 잘 이해할 수 있도록 돕습니다. 최신 문헌을 검토하고, 물리 정보가 RL 파이프라인의 다양한 단계에 어떻게 통합되는지 분석하며, 사용된 벤치마크를 요약합니다. 마지막으로, 현재 PIRL 연구의 한계점과 미해결 과제를 논의하며 향후 연구 방향을 제시합니다. PIRL은 물리적 타당성, 정밀도, 데이터 효율성, 실제 환경 적용 가능성을 높여 RL 알고리즘을 향상시킬 잠재력이 있습니다."
  },
  {
    "objectID": "posts/paper/2025-06-03-digit360.html",
    "href": "posts/paper/2025-06-03-digit360.html",
    "title": "📃Digit360 리뷰",
    "section": "",
    "text": "✨ 이 논문은 시각, 오디오, 진동, 힘, 열, 냄새 등 다양한 감각을 동시에 측정하는 고해상도 인공 촉각 센서인 ’Digit 360’을 제시합니다.\n🔬 이 손끝 센서는 7 마이크로미터의 공간 해상도와 1 밀리뉴턴 수준의 힘 감지 능력을 포함하여 인간의 촉각 성능을 능가하는 결과를 보였습니다.\n🤖 장치 내 AI 프로세싱을 통해 빠른 반응 속도를 구현했으며, 로봇 공학, 가상 현실, 원격 조작 등 광범위한 응용 분야를 위한 유망한 플랫폼입니다."
  },
  {
    "objectID": "posts/paper/2025-06-03-digit360.html#기술-분석-인공-멀티모달-손끝의-구성과-작동-원리",
    "href": "posts/paper/2025-06-03-digit360.html#기술-분석-인공-멀티모달-손끝의-구성과-작동-원리",
    "title": "📃Digit360 리뷰",
    "section": "2.1 기술 분석: 인공 멀티모달 손끝의 구성과 작동 원리",
    "text": "2.1 기술 분석: 인공 멀티모달 손끝의 구성과 작동 원리\n\n2.1.1 센서 설계 및 주요 구성 요소\n이 인공 손끝(Digit 360이라 명명)은 인간 손가락과 유사한 반구형의 유연한 촉각 표면을 갖추고 있으며, 그 내부에 복합 센서들이 층상 구조로 통합되어 있습니다. 가장 핵심이 되는 것은 시각 촉각 센서(vision-based tactile sensor)로, 표면의 미세한 변형을 고해상도로 포착하는 역할을 합니다. 반구형 엘라스토머 재질의 젤이 손가락 표면을 이루고, 그 표면 아래에는 매우 얇은 은(Ag) 반사막 코팅층이 입혀져 있어 접촉 시 움푹 패이는 변형을 시각적으로 표현합니다. 센서 내부에는 초광각 어안 렌즈(hyper-fisheye lens)를 장착한 고해상도 카메라가 배치되어 있는데, 이 특수 렌즈 덕분에 하나의 카메라로 손가락 끝 부분부터 옆면까지 전방위(360°) 촉각 이미지를 수집할 수 있습니다. 이는 기존에 다수의 카메라를 사용하거나 평면 부위만 감지하던 접근과 달리, 하나의 카메라로 전방위 접촉을 감지하는 혁신으로 센서 통합과 데이터 처리가 한층 단순화됩니다.\n이 시각 촉각 모듈은 동작 시 구조화 조명(structured light) 기법을 활용하여 표면 변형 정보를 획득합니다. 구체적으로, 내부에 배치된 다색 LED 조명(예: 360도 방향에 고르게 배열된 RGB LED들)을 통해 젤 내부를 조명하고, 반사막 표면의 압흔(depression)이 만들어내는 빛 반사 변화를 카메라가 포착합니다. 연구진은 조명의 세기와 색상 스펙트럼을 동적으로 제어함으로써, 조명 불균일로 인한 영상 왜곡(glare, hotspot)을 최소화하고 표면 눌린 자국과 배경을 높은 대비로 분리해내는 최적의 환경을 구축했습니다. 또한 내부 구조물을 두지 않고 고체형 실리콘 볼륨으로 일체화된 젤을採用하여, 빈 공간에서 발생하는 빛 산란 노이즈를 줄이고 깨끗한 촉각 영상을 얻었습니다. 이러한 설계 덕분에 시각 촉각 센서는 약 8백만 개 이상의 촉각 화소(taxel)로 이루어진 고해상도 영상을 제공하며, 7μm에 불과한 미세 공간 구조까지 구분할 수 있는 초정밀 공간 해상도를 달성했습니다. 이는 인간 손끝이 인지하는 수십 μm 수준의 촉감 해상도를 뛰어넘는, 말 그대로 “초인적(superhuman) 성능”입니다. 더불어, 촉각 영상의 프레임률을 240Hz까지 높여 빠르게 변하는 접촉 사건도 포착할 수 있도록 하였는데, 이는 일반 카메라 기반 촉각센서(보통 30~60Hz)의 한계를 크게 확장한 것입니다. 시각 정보만으로도 접촉 지형(contact geometry)과 힘의 방향/세기 추정이 가능하지만, 이 논문의 센서는 여기서 그치지 않고 다양한 물리 신호를 추가로 감지하도록 설계되었습니다.\n멀티모달 감각 통합을 위해 손끝 내부에는 카메라 외에도 여러 센서들이 융합되어 있습니다. 우선, 소형 마이크로폰 및 압력 기반 진동 센서가 내장되어 손가락이 표면을 스칠 때 나는 마찰음이나 미세 진동 정보를 포착합니다. 덕분에 이 손끝은 240Hz 카메라로는 포착하기 어려운 수 kHz 대역의 빠른 진동까지도 감지할 수 있으며, 실제 10kHz에 이르는 고주파 촉각 진동까지 인지할 수 있음이 확인되었습니다. 이는 인간의 진동 감각 수용기(Pacinian corpuscle)가 약 수백 Hz 정도까지 반응하는 것과 견줘볼 때 월등히 높은 수치입니다. 높은 대역의 진동 및 음향 정보는 재질 특성 파악에 유용한데, 예를 들어 이 센서는 손가락으로 가볍게 두드리거나 문지르는 동작만으로도 나무와 플라스틱, 직물과 거친 면과 같은 다양한 표면의 질감 차이를 음향/진동 스펙트럼으로 구별해낼 수 있었습니다. 또한 연구진은 이 진동 센서를 활용하여 불투명 용기의 내용물량을 파악하는 데 응용했는데, 손끝으로 병을 톡톡 두드릴 때 발생하는 공명 소리의 특징으로부터 병에 든 액체의 높이를 가늠하는 데 성공하였습니다. 이러한 사례들은 시각+청각 촉각의 융합이 제공하는 정보를 잘 보여줍니다.\n다음으로, 이 손끝에는 온도 센서가 포함되어 접촉 물체의 온도 변화를 감지할 수 있습니다. 이를 통해 단순한 촉감 이상으로, 물체가 차가운지 따뜻한지, 혹은 온도 상승/하강 추세에 있는지까지 파악이 가능하며, 뜨거운 표면이나 냉각 중인 물체 등 안전과 관련된 정보를 얻을 수 있습니다.\n나아가 화학 물질 센서(가스 센서)도 내장되어 있어, 접촉 중이거나 주변 공기 중에 휘발성 화합물이 있는지를 감지합니다. 일종의 “후각”에 해당하는 이 능력은 일반적인 촉각 센서에서는 찾아보기 힘든 독특한 모달리티로서, 예를 들어 손가락이 젖었을 때 비누 냄새나 기름기 냄새를 감지하여 표면에 비누나 윤활유가 묻어 미끄럽게 할 요인이 있는지 사전에 인지하는 활용이 가능합니다. 이처럼 힘/형상(시각), 진동/음향, 온도, 화학에 이르는 다채로운 센싱을 하나의 손가락 팁에 통합한 것은 해당 장치의 가장 큰 강점입니다. 인간 피부 역시 촉각 수용기가 여러 종류(기계적, 열, 통증 등)로 이루어져 다양한 자극을 느끼는데, 기존의 인공 촉각들은 그 중 일부 신호만 개별적으로 모방했을 뿐 이처럼 통합적인 멀티모달 센싱을 구현한 예는 전무했습니다. Digit 360 인공 손끝은 사람 손끝의 감각 스펙트럼을 거의 전부 (또는 그 이상으로) 디지털화했다는 점에서 의미가 큽니다.\n한편 이러한 방대한 센서 데이터를 효과적으로 활용하기 위해, 손끝 내부에는 온장치 인공지능 가속기(on-device neural network accelerator)를 내장하고 있습니다. 이 작은 AI 전용 프로세서는 RISC-V 기반 9코어 NPU로, 일종의 “로봇 말초신경계” 역할을 수행합니다. 즉, 손끝에서 취득한 멀티모달 데이터를 로컬에서 즉각적으로 신경망으로 처리하여 유의미한 고레벨 정보(접촉 여부, 힘 추정치, 미끄럼 발생 등)를 뽑아내고, 필요한 경우 로봇의 구동기로 바로 피드백 제어명령을 내리는 것입니다. 마치 인간이 뜨거운 물체를 손끝으로 짚었을 때 뇌까지 생각하지 않고 즉각 손을 떼는 척수 반사(reflex arc)처럼, 이 손끝은 중요한 촉각 이벤트를 자체적으로 판별해 로봇 팔에 즉각 신호를 줄 수 있습니다. 연구 결과에 따르면, 기존 방식처럼 모든 센서 데이터를 중앙 컴퓨터로 보내 처리하는 경우 약 5~6ms 소요되던 이벤트 감지부터 반응 출력까지의 지연이, 온보드 처리 시 약 1.2ms로 대폭 단축되었습니다. 이는 처리 지연을 5배 이상 줄인 것으로, 로봇 제어의 민첩성을 크게 높여줍니다. 요컨대 Digit 360 플랫폼은 센서 하드웨어부터 즉각적 AI 처리까지 한 몸체에 넣음으로써, 방대한 촉각 데이터를 다루는 복잡성을 줄이고 실시간성을 극대화한 설계라 할 수 있습니다.\n\n\n2.1.2 센서 데이터 융합 및 학습 모델 구조\n이 인공 손끝이 생성하는 멀티모달 데이터 스트림(초당 240장의 촉각 이미지와 여러 실시간 센서신호)을 어떻게 유의미한 정보로 바꿀 것인가도 중요한 기술 요소입니다. 연구팀은 전통적 공학 모델 대신 딥러닝 기반의 학습 모델을 활용하여, 복잡한 센서 패턴을 힘이나 물체 특성으로 변환했습니다. 먼저 촉각 이미지로부터 접촉 힘을 추정하기 위해 심층 합성곱 신경망(CNN)을 학습시켰습니다. 구체적으로, 시각 촉각 이미지(3채널 RGB)를 입력 받아 해당 접촉의 법선력(normal force)과 전단력(shear force) 값을 회귀 출력하는 CNN 모델을 만들었는데, 이는 ResNet-50 백본을 변형하여 다중 분류 대신 2개의 연속값(힘)을 내도록 구성했습니다. 이 모델을 수천 건의 교정 데이터를 통해 훈련한 결과, 손끝 이미지만 가지고도 법선 및 전단 힘을 각각 오차 약 1.0~1.3 mN 이내로 예측할 수 있었습니다. 이는 앞서 언급한 센서의 힘 분해능(≈1 mN) 수준에 근접하는 정확도로서, 광학식 촉각센서로 힘을 측정하는 정량적 캘리브레이션의 새로운 기록이라 할 수 있습니다. 특히 전단력 추정의 경우, 기존 시각 촉각센서들은 표면에 마커(marker)점을 찍어 변형 추적을 도와주지 않으면 어려움이 있었으나, 본 센서는 고해상도 영상 자체에 풍부한 표면 특징이 담겨있어 마커 없이도 정밀한 전단 추정이 가능했다고 보고됩니다. 그만큼 고해상도의 이점이 학습 모델에도 기여한 것입니다.\n또한 연구진은 이 손끝이 산출하는 다양한 모달 데이터를 종합적으로 해석하기 위해 멀티모달 딥러닝 모델을 구성하였습니다. 하나의 예로, 촉각 센서를 이용한 행동 분류(예: 밀기, 두드리기, 저어섞기)와 재질 분류(예: 물체가 나무인지 플라스틱인지) 문제를 학습시켰습니다. 여기에는 이미지, 진동(음향), 온도, 가스, 관성 등 여러 종류의 센서 값들이 입력으로 사용되었으며, 각 모달리티에 따라 다른 신경망 구조를 결합하는 방식을 취했습니다. 예를 들어 시각 및 음향 정보는 시계열 데이터를 주파수-시간 스펙트럼 이미지로 변환하여 ResNet-18 기반 CNN 인코더로 처리하고, 관성(IMU), 온도, 가스와 같은 스칼라 신호는 별도의 다층퍼셉트론(MLP) 인코더로 처리한 후, 최종적으로 이들 특징을 하나의 잠재 벡터로 융합하여 행동 및 재질을 예측하도록 했습니다. 이러한 멀티모달 신경망을 통해 각 모달리티의 기여도와 상호 작용도 분석할 수 있었는데, 연구 결과 여러 모달의 결합이 단일 모달 대비 분류 정확도를 향상시켰으며 특히 동작 종류에 따라 핵심 단서가 되는 센서가 다름을 관찰했습니다. 예를 들어 슬라이딩하는 동작에서는 진동/음향 신호가 중요하고, 물체의 재질 식별에는 시각적인 접촉 패턴이 주요하게 작용하는 식입니다. 이처럼 딥러닝 모델을 통해 센서 융합의 유효성을 정량적으로 확인하였지만, 동시에 복잡한 모달 조합에서 최적의 특징을 학습하는 것은 난이도가 높습니다. 여기서 자연히 “센서 융합이 학습에 어떤 영향을 주는가?” 라는 질문이 따라옵니다. 여러 이질적인 센서 데이터를 함께 사용할 때 발생하는 모델 복잡도 증가, 학습 데이터 요구량 증가, 그리고 모달 간 동기화와 상관관계 파악 등의 이슈가 존재합니다. 논문의 접근처럼 각 모달별 특징 추출 후 융합하는 구조는 한 해결책이지만, 어떤 모달이 얼마나 기여하도록 모델을 설계해야 최적인지는 향후 추가 연구가 필요한 영역입니다. 다행히 저자들은 멀티모달 모델의 cross-modal significance를 분석하여 촉각 행동 인식에 기여하는 센서들의 상관관계를 탐구하였고, 이는 향후 센서 선택 및 모델 경량화에 참고되는 통찰을 제공합니다.\n이 인공 손끝 플랫폼의 또 다른 기술적 특징은 모듈식 설계입니다. 센서와 프로세서 전체를 하나로 묶었음에도 불구하고, 내부 구성은 카메라 모듈, 마이크/IMU 모듈, AI 가속기 모듈 등 기능별로 분리된 소형 회로 블록들로 구성되어 있습니다. 이러한 모듈식 구조의 장점은 연구 개발 시 구성요소의 교체나 확장이 용이하다는 것입니다. 실제로 저자들은 필요에 따라 특정 센서를 빼서 비용을 낮추거나, 새로운 센서로 교체하여 성능을 높이는 식의 유연한 플랫폼을 지향하고 있습니다. 예를 들어 마커가 있는 손끝 젤로 교체하여 전단력 추정을 더욱 용이하게 하거나, 젤 재질의 경도나 두께를 변경해 감도를 조절하는 것도 쉽게 시도해볼 수 있습니다. 이러한 유연성은 학계와 산업계 연구자들이 자신들의 용도에 맞게 손끝 센서를 재구성(reconfigure)하여 실험해볼 수 있게 하며, 신기술 도입 시 전체를 새로 설계하지 않고 해당 모듈만 교체하면 되므로 개발 사이클을 단축시켜 줍니다. 실제 논문 저자들은 촉각 연구 활성화를 위해 이 손끝의 설계 도면과 소프트웨어를 오픈소스로 공개해 두었고, 이를 통해 더 많은 연구자들이 해당 플랫폼을 기반으로 촉각의 본질과 멀티모달 처리 기법을 탐구하기를 기대하고 있습니다."
  },
  {
    "objectID": "posts/paper/2025-06-03-digit360.html#응용-가능성-산업부터-vr까지의-활용-전망",
    "href": "posts/paper/2025-06-03-digit360.html#응용-가능성-산업부터-vr까지의-활용-전망",
    "title": "📃Digit360 리뷰",
    "section": "2.2 응용 가능성: 산업부터 VR까지의 활용 전망",
    "text": "2.2 응용 가능성: 산업부터 VR까지의 활용 전망\n이처럼 다중 센서융합 인공 손끝이 제공하는 풍부한 촉각 데이터와 정밀 제어 능력은 다양한 분야에서 혁신적인 응용을 가능케 합니다. 저자들은 논문에서 본 기술의 잠재적 적용 분야로 로보틱스(산업, 의료, 농업, 소비자 영역), 가상현실/텔레프레즌스, 의수(보철), 전자상거래 등을 직접 언급하고 있습니다. 이하에서는 주요 분야별로 기대되는 활용 사례와 기존 기술 대비 이점 및 한계를 살펴봅니다.\n\n2.2.1 로보틱스 및 자동화 분야\n산업용 로봇에서 정밀 촉각 센싱은 매우 큰 가치를 갖습니다. 예를 들어 제조 조립 공정에서 로봇팔이 작은 부품을 끼우거나 나사를 조이는 작업을 생각해보면, 육안으로는 보이지 않는 미세한 맞촉감(fit)이나 나사선의 걸림 여부 등을 사람이 손끝으로 느끼며 작업하는 경우가 많습니다. 인공 멀티모달 손끝을 장착한 로봇은 이러한 작업에서 사람 못지않은 (어떤 면에서는 사람 이상의) 섬세한 촉각 피드백을 얻을 수 있습니다. 7μm 수준의 공간 분해능이라면 머리카락 굵기의 흠집이나 표면 거칠기 변화도 감지할 수 있어, 부품 간 결합의 미세한 오차나 표면 결함 검사에도 활용될 수 있습니다. 또한 1mN 단위의 힘 제어가 가능하다는 것은, 로봇이 매우 부드럽게 힘을 조절하여 쉽게 깨지는 부품이나 연성 재료를 다루는 정밀 조작에 강점을 지닐 것임을 시사합니다. 기존 로봇은 주로 관절 토크센서나 간단한 그립센서로 힘을 제어해왔는데, 이러한 저해상도 정보로는 구현하기 어려웠던 미끄러짐 감지, 접촉면 마찰 측정, 물체 무게 파악 등을 이제는 하나의 손끝 센서로 모두 수행할 수 있습니다. 실제 연구 결과, 본 센서는 물체가 손에서 미끄러지는 초기 징후를 고속 진동 센싱으로 탐지하여 즉각적으로 그립을 조정할 수 있음을 보였는데, 이는 산업 로봇의 자동 그립 조절(reflex) 기능 구현에 응용될 수 있습니다. 또한 제조 라인에서 제품 표면을 이 센서로 스캔하면, 시각 검사로 놓치기 쉬운 미세 흠집이나 표면 텍스처 이상까지도 촉각으로 검출하여 품질 관리에 활용할 수 있습니다.\n물류 창고나 서비스 로봇에서도 응용 가능성은 큽니다. 예를 들어 소비자용 가정 로봇이 있다면, 섬세한 촉각이 있으면 물건을 집거나 정리할 때 물체의 종류와 상태를 파악하여 알맞게 다룰 수 있습니다. 옷가지나 식기, 전자제품 등 각각 다른 취급이 필요한 물건들을 손끝의 촉감으로 구분하고, 힘 조절을 다르게 할 수 있습니다. 특히 농업 로봇의 경우 과일 수확이나 식물 다루는 작업에서, 과일의 숙성도를 촉감으로 느끼거나 (단단함, 표면 질감), 식물 줄기의 두께와 강도를 파악하여 적절한 힘으로 잡는 등의 활용이 가능합니다. 토마토처럼 살짝 눌러봐야 익은 정도를 아는 작물도 이 센서로는 익은 정도에 따른 미세한 탄성 변화를 정량화할 수 있을 것으로 기대됩니다. 나아가 젖은 흙과 마른 흙의 감촉 차이나, 병해 충격 받은 잎의 질감 변화 등을 감지하는 등 농업 환경에서의 촉각 데이터 수집도 가능할 것으로 보입니다. 산업 및 서비스 로봇 분야에서 이 기술의 기술적 이점은 결국, 사람 수준의 촉각 피드백을 로봇이 얻음으로써 자율작업의 신뢰성과 정밀도가 높아진다는 점입니다. 기존에는 힘을 세게 가해 실험적으로 실패를 겪으며 수행하던 작업도, 섬세한 촉감으로 사전에 상황을 인지하고 적응함으로써 보다 안정적인 자동화가 가능해질 것입니다.\n물론 이러한 응용을 위해서는 센서의 견고성과 실시간 처리능력이 뒷받침되어야 합니다. 산업 현장은 온도, 먼지, 전자기 노이즈 등이 존재하므로, 광학식 촉각센서가 그런 환경에서도 유지보수가 용이할지 검증이 필요합니다. 이는 뒤의 한계점 부분에서 논의하겠지만, 간단히 말해 “이 방식이 실제 환경에서 견딜 수 있는가?”라는 실용적 질문이 남아 있습니다. 그럼에도 불구하고, Digit 360 센서가 보여준 성능 지표(공간/힘 해상도, 다중감각)는 현재까지 보고된 어떤 촉각 센서보다 뛰어나 인간 수준에 근접하거나 초과하는 것으로 평가되므로, 로보틱스 응용에 있어서 게임 체인저가 될 잠재력이 충분하다고 하겠습니다.\n한편 의료 로봇틱스 분야에서도 정밀 촉각은 매우 중요합니다. 예를 들어 외과 수술 로봇의 수술 도구 끝에 이러한 센서를 장착하면, 조직의 경도나 질감, 맥박 진동까지 느끼면서 수술을 진행할 수 있습니다. 종양과 정상 조직의 미세한 경도 차이나, 동맥의 박동, 미세 calcification 등을 촉각으로 구분하여 더 정교한 수술이 가능해질 수 있습니다. 또한 로봇 간호 및 재활 기기에서도 환자와 접촉하는 부분에 정밀 촉각 센서가 있다면, 환자의 피부상태, 근육 긴장도, 반응 등을 감지하여 안전하고 개인화된 케어를 제공할 수 있습니다. 예를 들어 재활 로봇 손이 환자의 관절을 굽힐 때, 환자가 과도한 통증으로 미세한 저항을 보이면 (근육 경직 등 촉각 피드백) 이를 감지해 동작을 완화한다든지 하는 피드백 제어가 가능해집니다. 이러한 의료/복지 로봇 응용에서도 중요한 것은 신뢰성과 안전성이며, 멀티모달 손끝 센서는 촉각을 통한 이중 안전장치 역할을 하여 인간에게 위해가 가는 상황을 미연에 방지하는 데 도움을 줄 것입니다.\n\n\n2.2.2 가상현실(VR) 및 텔레프레즌스 분야\n가상현실(VR)과 증강현실(AR)에서는 시각, 청각 기술은 크게 발전했지만 촉각 경험의 부재가 한계로 지적되어 왔습니다. 본 기술은 현실 세계의 촉감 데이터를 디지털 콘텐츠로 녹여낼 수 있는 수단을 제공함으로써, VR/AR의 리얼리티를 한층 높일 수 있습니다. 예를 들어 이 손끝 센서를 이용해 다양한 물체의 질감, 표면 구조, 단단함을 데이터화하여 가상 객체에 부여하면, 사용자가 VR 장비를 통해 인터랙션할 때 사실감 있는 촉각 피드백을 줄 수 있을 것입니다. 현재도 VR용 촉각장갑 등이 연구되고 있으나 주로 진동 정도의 피드백만 제공하는데, 본 센서로 측정된 정교한 촉각 프로파일을 기반으로 하면 사용자가 실제 나뭇결을 손으로 문지르는 듯한 미세한 질감까지도 재현하는 것이 궁극적으로 가능해집니다. 또한 AR 환경에서 로봇이나 장치가 현실 물체를 만졌을 때 그 촉감 정보를 실시간으로 사용자에게 전달한다면, 원격 현실감(remote presence)이 크게 향상될 것입니다. 이를테면, 사용자가 원격 로봇을 통해 박물관의 유물을 만져보는 AR 투어를 한다면 로봇 손끝의 촉각이 사용자의 장갑에 피드백되어 “만지는 경험”을 전달할 수 있을 것입니다.\n텔레프레즌스(원격현존)와 원격 조작(teleoperation) 분야는 이 기술과 특히 직접적인 관련이 있습니다. 원격조작 로봇 (예컨대 해저 로봇 팔, 우주 로봇 팔, 원격 의료수술 로봇 등)에서 조종자는 카메라 화면과 약간의 힘 피드백만으로 작업하는 경우가 많은데, 만약 로봇 손끝에 Digit 360 센서를 달아두면 원격지의 미세한 촉각 정보를 실시간 전송받아 느낄 수 있습니다. 예를 들어 원격 수술에서 조직의 질감이나 경계를 손끝 감각으로 느끼면서 수술할 수 있게 되며, 위험물 처리 로봇이 폭발물의 표면 상태나 흔들림을 손끝으로 감지하여 더 섬세하게 다룰 수 있습니다. 이러한 고품질 촉각 정보는 양방향(haptic feedback) 장치와 결합되어 비로소 인간 조종자가 느낄 수 있게 되므로, 향후 촉각 피드백 장치 기술과 함께 발전이 필요합니다. 하지만 일단 정보를 획득하는 측면에서의 돌파구로서, 본 센서가 원격 촉각의 눈 역할을 할 수 있다는 의의가 있습니다. 특히 인터넷을 통한 원격작업에는 지연 문제가 큰데, 앞서 언급했듯 온보드 AI가 1ms 수준으로 로컬 반사를 수행한다면, 원격 조작에서도 로봇 자체가 위험을 감지해 즉각 대처함으로써 네트워크 지연으로 인한 사고를 줄일 수 있습니다. 예컨대 원격 로봇 손이 미끄러운 물체를 잡았을 때, 사람이 느끼기도 전에 센서가 미끄러짐을 감지해 그립을 조정해준다면 원격 조작의 안정성이 크게 향상될 것입니다. 이처럼 VR/텔레프레즌스 분야에서 인공 멀티모달 손끝은 현실의 촉감을 디지털로 복제하고 전송하는 역할을 하며, 이는 궁극적으로 현실-가상의 경계를 허무는 몰입형 경험으로 이어질 것입니다.\n\n\n2.2.3 의료 및 보조공학 분야\n본 기술은 의수나 의족 등의 보조공학 분야에도 직접적인 파급 효과가 있을 것으로 기대됩니다. 현존하는 의수의 손가락에는 간단한 압력센서나 가속도센서 등이 달려 있을 뿐, 인간 고유의 풍부한 촉각을 제공하지 못합니다. 사용자(절단 장애인)는 물체를 잡아도 어느 정도 힘으로 잡는지, 미끄럽게 생겼는지 등의 정보를 거의 받지 못해 섬세한 작업이 어렵고, 심지어 뜨거운 물체를 만져도 느끼지 못해 위험하기도 합니다. 인공 멀티모달 손끝을 의수에 적용한다면 이러한 제약을 크게 줄일 수 있습니다. 우선 의수가 물체를 쥐는 힘을 1mN 단위로 제어할 수 있게 되어 달걀처럼 약한 물체도 깨뜨리지 않고 집을 수 있고, 반대로 무거운 물건도 미끄러지지 않을 적절한 힘을 자동 조절할 수 있습니다. 또, 잡은 물체의 질감과 온도, 맥동까지 감지하여 사용자에게 피드백해줄 수 있다면, 사용자는 거의 실제 손과 유사한 촉감 경험을 얻을 수 있을 것입니다. 예를 들어 따뜻한 커피잔을 쥐었을 때 온기가 느껴지고, 사과를 쥐었을 때 표면의 매끈함과 단단함이 느껴진다면 사용자는 훨씬 자연스럽게 의수를 자신의 일부처럼 여길 수 있을 것입니다. 실시간 촉각 피드백을 사용자에게 전달하는 기술(신경 인터페이스나 촉각 자극 디스플레이)은 별도의 연구과제지만, 본 센서 기술의 존재로 인해 이식형 촉각 피드백 시스템도 구체적으로 논의할 수 있게 됩니다. 최소한의 단기 응용으로는, 의수에 멀티모달 손끝을 달아 물체가 미끄러질 때 자동으로 꽉 쥐거나 너무 뜨거울 때 즉각 놓아버리는 자동 반사 동작을 구현해줄 수 있습니다. 이는 감각이 없는 의수를 사용하는 사람에게 안전망이 되어 줄 것입니다. 더 나아가 의수 사용자에게 손끝 센서 데이터를 진동이나 전기 자극으로 환원(haptic feedback)해 주면, 사용자 스스로 힘 조절을 학습하여 더욱 섬세한 작업 (예: 계란 깨지 않게 들기, 옷감의 두께 느끼기 등)을 수행할 수도 있습니다. 이러한 보조공학 응용에서 가장 큰 기술적 이점은, 사용자가 잃어버린 촉각 감각을 기계가 대폭 복원 또는 증강해줄 수 있다는 점입니다. 기존 의수 개발은 주로 모터 제어에 집중되어 촉각은 등한시되었는데, 이제는 센서 측면에서 인간 촉각에 필적하는 장치를 활용할 수 있게 된 것입니다.\n이 밖에도 전자상거래(e-commerce) 분야에서의 잠재적 활용도 흥미롭습니다. 온라인 쇼핑에서는 소비자가 물품을 직접 만져볼 수 없다는 한계가 있는데, 장차 촉각 센싱 데이터베이스를 구축하여 이를 부분적으로나마 해소할 수 있습니다. 예를 들어 원단이나 의류 상품의 경우, Digit 360 센서로 천의 질감, 두께, 뻣뻣함, 부드러운 정도 등을 측정한 데이터를 제공함으로써 소비자가 촉감 특성을 유추하도록 할 수 있습니다. 또 전자제품 버튼의 조작감이나, 가구 표면의 마감 촉감 등을 정량화해 보여주는 것이 가능합니다. 나아가 매장에 원격 로봇을 배치하고 소비자가 집에서 VR 장비를 통해 로봇을 조작하면서 상품을 만져볼 수 있게 하는 시나리오도 생각해볼 수 있습니다. 이때 본 손끝 센서가 부착된 로봇이 상품의 촉감을 실시간 전송해주면, 온라인과 오프라인의 체험 격차를 크게 좁힐 수 있을 것입니다. 또한 물류 분야에서 로봇이 상품을 집을 때 얻는 촉감 정보를 바탕으로 상품의 파손 여부나 상태 이상을 자동으로 판별하는 등의 응용도 가능합니다. 예컨대 택배 상자를 로봇팔이 쥐었을 때 살짝 눌러본 촉감으로 내부 내용물이 깨졌는지(딸그락 거리는 진동? 균일하지 않은 단단함?)를 감지한다면 비대면 자동 검사가 될 수 있습니다. 이러한 전자상거래 및 물류 응용은 아직은 개념적인 단계이지만, 촉각 데이터의 축적과 활용이라는 새로운 영역을 개척할 수 있다는 점에서 주목할 만합니다."
  },
  {
    "objectID": "posts/paper/2025-06-03-digit360.html#한계점-및-향후-과제-도전-과제와-비판적-고찰",
    "href": "posts/paper/2025-06-03-digit360.html#한계점-및-향후-과제-도전-과제와-비판적-고찰",
    "title": "📃Digit360 리뷰",
    "section": "2.3 한계점 및 향후 과제: 도전 과제와 비판적 고찰",
    "text": "2.3 한계점 및 향후 과제: 도전 과제와 비판적 고찰\n혁신적인 인공 멀티모달 손끝 기술이지만, 실제 활용을 위해서는 몇 가지 극복해야 할 한계와 고려사항이 존재합니다. 여기서는 실험적 제약, 내구성 및 환경적 요인, 비용/복잡성, 데이터 처리 및 확장성 측면에서 주요 한계를 지적하고, 동시에 연구 개발자들이 관심을 가질 만한 후속 연구 방향과 질문을 제시합니다.\n내구성 및 환경 적응성:\n현재 공개된 결과는 실험실 환경에서 얻어진 성능 수치입니다. “이 방식이 실제 환경에서 견딜 수 있는가?” 라는 의문은 여전히 남습니다. 예컨대 산업 현장의 먼지, 기름기, 온도 변화, 충격 등에 노출되었을 때 광학식 + 젤 구조의 센서가 얼마나 유지될지 알 수 없습니다. 젤 표면의 얇은 은 코팅은 반복 접촉으로 마모되거나 긁힐 우려가 있고, 젤 자체도 날카로운 모서리에 여러 번 눌리면 찢어질 가능성이 있습니다. 논문에서는 일정 힘 이상 가해지면 손끝이 본체에서 분리되는 한계(force threshold)도 실험으로 보였는데, 이는 최대 하중 이상에서는 센서가 물리적으로 파손될 수 있음을 의미합니다. 따라서 거친 산업 작업이나 돌발적인 큰 충격이 가해지는 상황에서 센서를 보호할 수 있는 외장 커버나 오버레이 등에 대한 추가 설계가 필요할 것입니다. 그러나 보호층을 추가하면 감도가 낮아질 수 있으므로, 내구성과 성능의 트레이드오프를 최적화해야 합니다. 또, 실외 환경 (예: 농업 로봇)에서는 온습도 변화로 젤의 탄성이 바뀌거나, 광학 부품에 습기가 차는 등의 변수가 있습니다. 이러한 환경 변수에 자동 보정(calibration)할 수 있는 기법이나 자가진단 기능도 후속 연구 과제입니다. 장시간 사용에 따른 노화도 고려해야 하는데, 센서 재료(실리콘 젤, 코팅 등)가 수백 시간의 사용 후에도 초기 성능을 유지하는지, 혹은 일정 주기마다 젤 팁 교체가 필요한 소모품으로 봐야 할지 평가가 필요합니다. 다행히 모듈식 설계로 손쉽게 팁을 교체 가능하므로 현장에서의 유지보수는 큰 어려움이 없을 수 있으나, 잦은 교체가 필요하다면 비용과 다운타임 문제가 발생할 것입니다.\n복잡성 및 비용 문제:\nDigit 360 센서는 최첨단 소자로 구성된 고급 연구 플랫폼입니다. 830만 화소의 카메라, 커스텀 광각 렌즈, 다종의 MEMS 센서, 전용 AI 칩 등 부품 구성이 복잡하며 원가도 높을 것으로 예상됩니다. 논문에서도 이전 세대인 Lambeta 등(2020)의 저가형 촉각센서와 대비되는 하이엔드 디자인임을 밝히고 있습니다. 일반 상용화를 위해서는 이러한 복잡성을 줄이고 비용을 낮추는 작업이 필수입니다. 예를 들어 활용 분야별로 정말 필요한 모달리티만 남기고 제거한다든지, 카메라 해상도를 약간 낮춰도 용인되는 작업에는 저해상도 이미지 센서를 써서 가격을 줄이는 식의 라인업 다양화가 고려될 수 있습니다. 실제로 모듈식 구조이기에 이러한 스케일 다운도 비교적 수월할 것으로 보입니다. 하지만 성능과 비용은 반비례하기 쉽기 때문에, 어느 정도 성능을 포기하고 상용화할지에 대한 기준점 설정이 필요합니다. 복잡성이 높다는 것은 조립과 제조의 어려움도 의미합니다. 예컨대 논문에서 사용된 맞춤형 하이퍼어안 렌즈는 일반 제품이 아니어서 제조사가 한정적이고 가격도 높을 것입니다. 은 코팅 같은 공정도 일반 전자제품 제조라인에서는 특수한 공정입니다. 따라서 대량 생산 단계에서 공정 표준화와 비용 절감을 위한 공학적 연구가 필요합니다. 또한 부품이 많아지면 고장 가능성도 높아지므로 신뢰성 확보도 과제입니다. 향후 연구에서는 센서 구조 단순화를 위해 한 가지 소자로 다중 물리량을 감지하는 방법 (예: 광센서만으로 온도도 추정하거나, 압전소자로 진동+힘 동시 측정 등)도 모색될 수 있습니다. 궁극적으로는 성능을 크게 해치지 않으면서도 부품수를 줄이고 가격을 낮추는 엔지니어링이 이루어져야 산업계에서 폭넓게採用될 것입니다. 이와 관련하여 “이 정도로 복잡한 센서가 상용 로봇에 현실적으로 부착될 수 있을까?” 하는 질문에 답하기 위해서는, 실제 현장에서 요구되는 적정 성능과 허용 가능한 비용의 균형점을 연구 커뮤니티와 산업계가 함께 찾아나가야 할 것입니다.\n데이터 처리 및 학습 측면 한계:\n멀티모달 손끝이 만들어내는 방대한 데이터 스트림을 실시간으로 처리하는 것은 또 다른 도전입니다. 비록 온보드 NPU로 1차 처리를 한다고 해도, 8백만 픽셀 이미지(240fps)와 다수 센서신호를 다루는 일은 여전히 연산 부담과 통신 부담이 큽니다. 현재 프로토타입에서는 간단한 힘 추정이나 분류 작업을 데모했지만, 향후 더 복잡한 작업(예: 동시에 여러 손가락 데이터를 통합해 물체 모양을 재구성하거나, 연속적인 촉각 탐색을 통한 물체 인식 등)을 수행하려면 더 고성능의 프로세싱이나 효율적인 알고리즘이 필요합니다. 딥러닝을 활용하는 경우, 학습 데이터셋 확보와 학습 시간도 고려해야 합니다. 논문에서는 비교적 제한된 환경(일정한 물체들, 정해진 동작들)에서 모델을 학습시켰는데, 이를 실제로 다양하고 예측 불가능한 상황에 대응하려면 훨씬 광범위한 데이터가 필요할 수 있습니다. 예컨대 수천 종류의 물체 재질을 구분하려면 그만큼 데이터 수집이 필요한데, 촉각 데이터 수집은 시각 데이터 대비 시간이 오래 걸리고 자동화하기 어려운 측면이 있습니다. 따라서 데이터 효율적인 학습 방법 (예: 자율적 탐색을 통한 자기지도 학습, 도메인랜덤화나 시뮬레이션 활용)이 중요한 연구 과제가 될 것입니다. 또한 여러 모달의 데이터는 시간 동기화(synchronization)와 샘플링 레이트 차이 문제도 존재합니다. 카메라는 240Hz이지만 마이크는 20kHz 수준으로 샘플링되므로, 이를 한 신경망에 넣을 때 시계열 정렬을 어떻게 할지 등이 난제입니다. 논문에서는 융합 전에 각각 적절한 창(window)으로 묶어 입력하였지만, 일반적인 임의 동작에서는 최적의 윈도우 설정이나 멀티모달 특징 추출 방법론 연구가 더 필요합니다. 센서 융합이 학습과 추론에 미치는 영향을 규명하기 위해서는, 각 모드별 신호의 상호보완적 정보량을 이론적으로 분석하거나 다양한 모델 아키텍처(예: transformer 기반 멀티모달 인식 등)를 시험해볼 필요가 있습니다. 요약하면, “방대한 촉각 데이터를 얼마나 효율적으로 처리하고 학습시킬 것인가?”가 중요한 질문이며, 이는 센서 하드웨어뿐만 아니라 소프트웨어 알고리즘 측면의 혁신을 지속적으로 요구하는 분야입니다. 향후 연구에서는 경량화된 학습 모델 설계, 중요 촉각 특징만 추출하는 특징 공학 또는 압축 기술, 그리고 로봇의 전략적인 능동적 탐색(active sensing)과의 연계 등으로 이 문제에 도전해야 할 것입니다.\n다수 센서의 확장성과 표준화:\n현재 프로토타입은 손가락 하나에 대한 것이지만, 사람 손은 다섯 손가락이 모두 촉각을 협업합니다. 로봇 핸드에 5개의 Digit 360 센서를 모두 장착하여 동시에 구동할 경우, 데이터량과 시스템 복잡성은 더욱 증가합니다. 손가락마다 온보드 처리를 하더라도, 최종적으로 로봇 컨트롤러는 여러 손끝에서 오는 정보를 통합해야 합니다. 이때 여러 손끝 간 데이터 동기화와 통신 지연 문제가 발생할 수 있고, 손가락들 사이의 상호 간섭(예: 한 손가락의 진동이 다른 손가락의 마이크에 들어가는 등)도 고려해야 합니다. 이러한 멀티-핑거 센서 네트워크를 효과적으로 구성하는 방법 역시 향후 과제입니다. 한 가지 방향은, 인간의 말초신경계를 모방하여 분산처리 + 중앙요약 구조를 취하는 것입니다. 이미 손끝 개별 처리는 구현되었으니, 향후에는 여러 손끝의 고레벨 정보를 다시 통합해 맥락을 해석하는 상위 레벨 AI가 필요합니다. 예를 들어 물체를 양 손가락으로 집었다면 양쪽 센서의 힘/변형 데이터를 결합해 물체의 물성이나 잡힘 상태를 추론하는 식입니다. 이러한 멀티-센서 융합은 아직 개척되지 않은 영역으로, 새로운 모델과 제어 프레임워크 연구가 필요합니다.\n또한 촉각센서 분야 전반의 재현성(reproducibility)과 표준화 이슈도 존재합니다. 논문은 센서 설계를 공개했지만, 이를 다른 연구자들이 쉽게 제작하여 실험할 수 있는지에는 의문이 있습니다. 앞서 언급한 특수 부품 수급이나 제작 공정 문제로 누구나 만들 수 있는 수준은 아닐 수 있습니다. 만약 이 센서가 상용 제품으로 출시되지 않는다면, 연구 커뮤니티에서 널리 활용되기까지는 시간이 걸릴 수 있습니다. 따라서 후속 연구로, 이와 유사한 성능을 내면서도 제작 용이성이 높은 대안 센서 개발도 고려될 수 있습니다. 또는 주요 부품 (예: 렌즈, 코팅된 젤)을 모듈 형태로 판매하여 조립을 쉽게 하는 방안도 있을 것입니다. 더불어 촉각 연구의 발전을 위해서는 평가 기준의 표준화도 중요합니다. 이 논문에서는 7μm, 1mN 등의 수치를 썼지만, 다른 연구에서는 각기 다른 조건으로 성능을 재기 때문에 직접 비교가 어렵습니다. 향후에는 촉각 센서의 공인된 벤치마크 테스트(예: 표준화된 패턴으로 공간해상도 측정, 규격화된 힘센서로 최소 감지력 측정 등)가 마련되어야, 여러 접근법 간 객관적 비교와 선택이 가능해질 것입니다. Digit 360의 등장은 이러한 논의를 촉발시킬 것으로 보이며, 이를 계기로 촉각 센서 분야의 협력적 발전이 가속되길 기대합니다.\n\n미래를 향한 질문들\n마지막으로, 본 리뷰를 통해 도출된 핵심 질문들을 정리하면 다음과 같습니다:\n\n\n이 인공 손끝 센서가 인간 수준을 뛰어넘는 성능을 달성했지만, 실제 복잡한 물리 환경과 장기간 사용에서도 그 정밀도와 신뢰성을 유지할 수 있을까요? 이는 센서의 내구성, 환경 적응성에 관한 질문으로, 향후 혹독한 조건에서의 장기 테스트와 보완 설계 연구가 필요합니다.\n다양한 모달리티를 통합한 촉각 데이터는 풍부하지만, 그 방대한 정보를 실시간으로 처리하여 로봇 행동에 활용하는 데에는 어떤 제약이 있을까요? 여러 센서의 데이터 융합이 가져오는 학습상의 이득과 비용을 정량화하고, 효율적인 처리 알고리즘을 개발하는 것이 중요합니다.\n복잡하고 값비싼 이 연구 시제품을 현실에서 경제적으로 구현하려면 무엇을 타협하고 개선해야 할까요? 예컨대 불필요한 기능을 줄이는 모듈 구성, 저가 대체 소재 활용 등의 엔지니어링 과제가 있습니다. 이는 기술적 성능과 상용화 가능성 사이의 균형점을 묻는 질문입니다.\n\n이러한 질문들은 아직 완전히 답변되지 않았지만, Digitizing Touch 논문은 촉각센서 연구의 지평을 크게 확장하며 동시에 새로운 도전들을 부각시켰습니다. 앞으로 소재공학, 광학설계, 신호처리, 인공지능 등 다양한 분야의 협력을 통해 이 질문들에 대한 해답이 모색될 것입니다. 결론적으로, 인공 멀티모달 손끝 기술은 인간 촉각의 풍부함을 기계에 불어넣음으로써 로봇과 인간의 상호작용 방식에 근본적 변화를 가져올 잠재력을 보여주었습니다."
  },
  {
    "objectID": "posts/paper/2024-12-22-rotating-without-seeing.html",
    "href": "posts/paper/2024-12-22-rotating-without-seeing.html",
    "title": "📃Rotating without Seeing 리뷰",
    "section": "",
    "text": "이번 포스팅에서 리뷰할 논문은 Rotating without Seeing: Towards In-hand Dexterity through Touch 입니다. RSS(Robotics: Science and Systems) 2023 학회에서 발표된 해당 논문은 사람이 시각 없이 촉각만으로 손안에서 물체를 정교하게 조작하는 능력을 로봇 핸드에 구현하고자, 손바닥, 손가락 관절, 손끝 전체에 넓게 분포된 저비용의 이진 촉각 센서를 활용하여, 시뮬레이션에서 강화학습으로 학습한 정책을 실제 로봇 손에 적용하고, 이를 통해 학습한 물체뿐만 아니라 학습하지 않은 새로운 물체까지 조작할 수 있는 시스템인 Touch Dexterity를 제안합니다."
  },
  {
    "objectID": "posts/paper/2024-12-22-rotating-without-seeing.html#problem-formulation",
    "href": "posts/paper/2024-12-22-rotating-without-seeing.html#problem-formulation",
    "title": "📃Rotating without Seeing 리뷰",
    "section": "2.1 Problem Formulation",
    "text": "2.1 Problem Formulation\nTouch Dexterity는 강화학습 방법으로 제어를 하기 때문에 강화학습의 문제 정의 방식인 MDP(Markov Decision Process)의 요소들, State, Action, Reward 순으로 확인해보겠습니다.\n\n2.1.1 State\nState는 Hand Robot Agent의 상태를 나타내는 요소들로 이루어집니다. Allegro hand 로봇의의 joint position(16 차원), sensor observation(16차원), 이전 position target(16차원), 그리고 rotation axis(2차원)로 구성되어 있습니다. 핸드 로봇의 관절(joint) 부분들이 작은 모터들 16개로 이루어져 있고, FSR(Force Sensing Resistor) 센서들도 총 16개가 아래 그림처럼 손가락과 손바닥에 분포되어 있어 State 벡터의 차원들이 다음과 같이 구성되게 됩니다. 이렇게 구성된 State가 학습 시에 Policy Network의 Input으로 들어가게 되는데 1 time step 정보만으로는 학습하기에 부족한 정보량이기 때문에 현재 시점 기준 이전 스텝 2 time step을 합쳐(concatenation), 총 3 time step 을 쌓아서 policy network에 input으로 넣어줍니다.\n\n\n\nState 구성요소\n\n\n\n\n2.1.2 Action\nHand Agent가 움직이는 Action은 로봇의 각 관절(joint) 모터들이 움직이는 것으로 생각할 수 있습니다. 따라서 Policy network에서는 16차원의 모터와 관련된 어떠한 command 정보가 나오게 됩니다. 하지만 Policy network의 output인 a_t를 바로 쓰는 것이 아닌 PD Controller에 적용하기 위한 값으로 변환하는 과정을 거치게 됩니다. 결과적으로 PD Controller에서 사용하는 값은 \\tilde{q}_{t+1} (현재 time step이 t 이므로 앞으로 제어할 position의 time step 첨자는 t+1)인 것 입니다.\n하지만 여기서 \\tilde{q}_{t+1}을 바로 적용할 경우 생기는 문제가 있습니다. policy network output 값들이 연속적인 시간 순으로 봤을때 갭이 큰 값들이 나타나게 되면 부드러운 움직임을 가질 수 없습니다. 따라서 해당연구에서는 Exponential moving average 방법을 사용하여 smoothing하는 과정을 거치게 됩니다.\n\n\n\nAction이 적용되는 과정\n\n\n아래 그래프는 논문에서 제시한 파라미터(\\eta, 2 consecutive steps)로 랜덤한 포인트들을 가지고 smoothing하는 모습을 보여줍니다.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Parameters\neta = 0.8  # Smoothing factor\nsteps = 2  # Step size for x-axis\nn_points = 200  # Number of points\n\n# Generate data\nx = np.arange(0, n_points, steps)\ndata = np.sin(x / 5) + np.random.normal(0, 0.3, len(x))  # Random data with noise\nema = []\n\n# Calculate EMA\nfor i, point in enumerate(data):\n    if i == 0:\n        ema.append(point)  # Initialize EMA with the first data point\n    else:\n        ema.append(eta * point + (1 - eta) * ema[-1])\n\n# Plot\nplt.figure(figsize=(8, 3))\nplt.plot(x, data, label=\"Data\", marker=\"o\", linestyle=\"--\", alpha=0.6)\nplt.plot(x, ema, label=\"Exponential Moving Average (EMA)\", linewidth=2)\nplt.xlabel(\"Step\")\nplt.ylabel(\"Value\")\nplt.title(f\"Exponential Moving Average (eta={eta}, step={steps}) \")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n이렇게 최종적으로 계산된 Action 값으로 Hand Agent의 모션이 만들어지게 됩니다.\n\n\n2.1.3 Reward\n보상함수는 아래와 같이 6개의 term들로 구성되어 있습니다. 각 6개의 reward term들은 linear weighted sum이 되어 해당 timestep에서의 최종 reward가 됩니다.\n\n\n\nReward Function\n\n\n\nReward of rotation r_{rot}\n\n회전 축 k의 법선 평면 \\Pi에서 샘플링된 단위 벡터의 회전 각도 \\Delta \\theta로 정의된 회전 보상입니다.\n\n\n\nReward Function\n\n\n계산하는 과정\n\n법선 평면 \\Pi에서 단위 벡터 v를 임의로 샘플링하며, 이 벡터는 object에 부착된 것으로 간주할 수 있습니다.\n다음 상태에서의 해당 벡터 v'를 가져와 \\Pi에 투영(projection)합니다: v'_p = \\text{Proj}(v', \\Pi)\n\\Delta \\theta \\in [-\\pi, \\pi)는 축 k에 대해 v'_p와 v 사이의 부호 있는 거리로 정의됩니다.\n\nobject의 움직임이 매우 복잡한 경우, 시뮬레이터가 제공하는 각속도가 매우 노이즈가 심하기 때문에 보상에 이 각속도를 보상함수에 사용할 경우 특정 자세에서 진동하는 등 바람직하지 않은 object 움직임 패턴이 나타날 수 있습니다.\n이 유한 차분(finite difference)을 보상으로 사용하는 것이 서로 다른 실행에서도 일관된 회전 동작을 생성할 수 있습니다.\n\nPenalty of object’s velocity r_{vel}\n\n손이 object를 안정적으로 회전시키도록 장려하며, 훈련된 정책의 transferability을 향상시킵니다.\n\nPenalty of falling r_{fall}\n\nobject가 손바닥에서 떨어질 때 적용되는 negative penalty입니다.\n\nPenalty of the work controller r_{work}\n\n컨트롤러의 일(work)의 양을 패널티로 부과합니다. 이 reward term의 torque \\tau는 t에서 PD 컨트롤러가 출력한 토크입니다. 이 페널티는 손가락 움직임의 부드러움을 향상시키는 데 도움을 줍니다.\n\nPenalty of torque r_{torque}\n\n큰 토크 출력값에 패널티를 부과합니다.\n\nReward of distance r_{dist}\n\n거리 보상으로, 손끝이 객체에 가까이 가서 상호작용하도록 장려합니다.\n\nd(x_{\\text{tip}}, x_{\\text{obj}})는 손끝 위치 x_{\\text{tip}}와 객체 위치 x_{\\text{obj}} 사이의 거리입니다.\n\\epsilon은 작은 양으로, 분모가 0이 되는 것을 방지합니다.\nc_2와 c_3는 보상의 클리핑 범위를 정의하는 하한과 상한입니다.\n\n\n\n\n\n2.1.4 Reset\n불필요한 exploration를 줄이고 학습 과정을 가속화하기 위해 object가 초기 위치(즉, 손바닥의 중심)에서 너무 많이 벗어날 경우 에피소드를 리셋합니다. 또한, object의 주요 축이 회전 축에서 너무 많이 벗어날 경우에도 에피소드를 리셋합니다."
  },
  {
    "objectID": "posts/paper/2024-12-22-rotating-without-seeing.html#domain-randomization",
    "href": "posts/paper/2024-12-22-rotating-without-seeing.html#domain-randomization",
    "title": "📃Rotating without Seeing 리뷰",
    "section": "2.2 Domain Randomization",
    "text": "2.2 Domain Randomization\n강화학습의 Sim2Real Gap을 줄이기 위해 학습 단계에서 Domain Randomization을 적용하는데 해당 연구에서는 2가지 Domain Randomization을 진행합니다.\n\n물리적 랜덤화:\n\nrotation하는 object의 초기 위치, 질량, 형태, 마찰을 랜덤화하여 학습된 정책이 다양한 종류의 객체를 처리할 수 있도록 합니다.\nPD 컨트롤러의 게인을 랜덤화하여 실제 환경에서 PD 컨트롤러의 불확실성을 모델링합니다.\n각 촉각 센서를 랜덤화하는 것도 고려합니다. 활성화된 접촉 센서(출력이 1인 경우)에 대해, 확률 p로 출력을 0으로 뒤집습니다.\n지수 지연 모델(exponential delay)을 통해 접촉 센서의 신호 지연을 모델링합니다.\n\n비물리적 랜덤화\n\npolicy의 observation과 출력된 action에 화이트 노이즈를 주입하여 작은 외란에도 강인하도록 만듭니다."
  },
  {
    "objectID": "posts/paper/2024-12-22-rotating-without-seeing.html#training-procedure",
    "href": "posts/paper/2024-12-22-rotating-without-seeing.html#training-procedure",
    "title": "📃Rotating without Seeing 리뷰",
    "section": "2.3 Training Procedure",
    "text": "2.3 Training Procedure\nProximal Policy Optimization (PPO) 알고리즘을 사용하며 policy 네트워크와 value 네트워크 모두에 다층 퍼셉트론(MLP)을 사용했습니다.\n\n훈련 설정:\n\n이점(advantage) 클립 임계값 ϵ=0.2= 0.2와 KL 발산 임계값 0.020.02를 사용\n네트워크에서 활성화 함수로 ELU를 사용\n정책 네트워크는 학습 가능한 상태 독립적인 표준편차를 가지는 가우시안 분포를 출력\n\n비대칭 관찰(asymmetric observation):\n\n정책 및 가치 네트워크의 학습 난이도를 줄이기 위해 asymmetric observation 을 사용\n\n가치 네트워크: 입력에 접촉력, object의 ground-truth pose, 물리적 파라미터와 같은 특권 정보를 추가\n정책 네트워크: 현재 상태와 함께 3개의 과거 상태를 입력으로 사용하며, 특권 정보는 접근할 수 없음\n\n\n시뮬레이션 설정:\n\nIsaacGym 시뮬레이션에서 시간 간격(dt)은 0.01667초로 설정하고, 2 sub step을 사용\n8192개의 병렬 환경에서 시뮬레이션을 실행\n정책 네트워크가 출력하는 행동(제어 목표)은 6단계 동안 실행되며, 이는 실제 환경에서 10Hz의 제어 주파수에 해당\n\n\n\n\n\nTraining Process"
  },
  {
    "objectID": "posts/paper/2024-12-22-rotating-without-seeing.html#real-world-system-setup",
    "href": "posts/paper/2024-12-22-rotating-without-seeing.html#real-world-system-setup",
    "title": "📃Rotating without Seeing 리뷰",
    "section": "3.1 Real-world System Setup",
    "text": "3.1 Real-world System Setup\n\n\n\nOverview\n\n\n하드웨어 구성은 XArm 로봇 팔과 16자유도(16-DOF)를 가진 Allegro Hand에 접촉 센서 배열을 장착한 형태로 이루어져 있습니다. 손바닥과 손가락 끝을 포함한 Allegro Hand의 여러 부위에 부착된 16개의 접촉 센서로 구성됩니다.\n사용된 접촉 센서는 외부 힘이 표면에 가해질 때 저항이 변하는 Force-Sensing Resistor(FSR) 기반입니다. STM32F 마이크로컨트롤러를 사용하여 각 센서의 아날로그 전압 신호를 수집하고, 이를 디지털 신호로 변환하여 호스트로 전달합니다. 이 접촉 센서는 연속적인 접촉력 측정을 출력할 수 있지만, 신호는 일반적으로 비선형적이고 노이즈가 많습니다. 따라서 이를 제어에 사용하기 전에 적절한 전처리가 필요합니다. 선택된 임계값 \\theta_{\\text{th}}에 따라 이 측정값을 이진화(binarize)하고 이 신호를 사용합니다.\n이진 신호를 사용하는 장점:\n\n시뮬레이션과 실제 로봇 간의 차이를 줄이고, Sim2Real 전이 절차를 단순화할 수 있습니다.\n이진화된 측정값은 임계값을 조정하여 쉽게 보정(calibrate)할 수 있습니다."
  },
  {
    "objectID": "posts/paper/2024-12-22-rotating-without-seeing.html#simulation-setup",
    "href": "posts/paper/2024-12-22-rotating-without-seeing.html#simulation-setup",
    "title": "📃Rotating without Seeing 리뷰",
    "section": "3.2 Simulation Setup",
    "text": "3.2 Simulation Setup\n이 논문에서는 IsaacGym 시뮬레이터 사용합니다. 각 접촉 센서는 손가락과 손바닥 링크의 고정된 링크로 시뮬레이션됩니다. 시뮬레이터는 매 시뮬레이션 단계에서 각 센서 링크에 대한 순 접촉력 F=[Fx,Fy,Fz]F = [F_x, F_y, F_z]를 제공합니다. \\|F\\|을 시뮬레이션된 접촉력 측정값으로 사용한 다음, 이 측정값을 다른 임계값 \\tilde{\\theta}_{\\text{th}}으로 이진화합니다.\n\n\n\n중요한 점은 센서의 부모 링크에서 제공되는 힘은 순 접촉력에 기여하지 않는다는 것입니다. 시뮬레이션에서 실제 환경과 유사한 동작을 보장하기 위해 이 센서들의 임계값 \\tilde{\\theta}_{\\text{th}}을 조정합니다. 시뮬레이션에서는 \\tilde{\\theta}_{\\text{th}} = 0.01N을 사용합니다."
  },
  {
    "objectID": "posts/paper/2024-12-22-rotating-without-seeing.html#benchmark-in-hand-rotation",
    "href": "posts/paper/2024-12-22-rotating-without-seeing.html#benchmark-in-hand-rotation",
    "title": "📃Rotating without Seeing 리뷰",
    "section": "3.3 Benchmark: In-hand Rotation",
    "text": "3.3 Benchmark: In-hand Rotation\n이 논문에서는 시스템의 손재주(dexterity)를 연구하기 위해, 시스템을 사용하여 손 안에서 회전하는 작업(in-hand rotation task)을 목표로 합니다. 이 task는 object가 손바닥에 초기화된 상태로 시작하며, 로봇 손은 주어진 회전 축을 따라 객체를 회전시켜야 합니다. 손 안에서 객체를 회전하는 동안, object의 움직임은 손끝 회전(finger-tip rotation)보다 훨씬 더 복잡하며 특히, 손 안에서 조작하는 동안 객체는 손바닥에서 미끄러지거나 구를 수 있습니다.\n이와 같은 복잡한 움직임 패턴 때문에, 성공적인 조작을 위해 촉각 센서나 비전(vision) 시스템의 명시적인 피드백이 필요합니다. 그렇지 않으면, 현재 객체의 상태를 추론할 수 없으며, 객체를 안전하게 밀거나 회전시키는 데 실패할 수 있습니다."
  },
  {
    "objectID": "posts/paper/2025-08-14-grasp-as-you-say.html",
    "href": "posts/paper/2025-08-14-grasp-as-you-say.html",
    "title": "📃Grasp as You Say 리뷰",
    "section": "",
    "text": "Paper Link\nProject Link\nCode Link"
  },
  {
    "objectID": "posts/paper/2025-08-14-grasp-as-you-say.html#핵심-아이디어-및-문제-설정",
    "href": "posts/paper/2025-08-14-grasp-as-you-say.html#핵심-아이디어-및-문제-설정",
    "title": "📃Grasp as You Say 리뷰",
    "section": "2.1 핵심 아이디어 및 문제 설정",
    "text": "2.1 핵심 아이디어 및 문제 설정\n다지 로봇 손을 이용한 섬세한 파지(dexterous grasping)를 사람의 자연어 지시로 수행하는 새로운 과제 “Dexterous Grasp as You Say (DexGYS)”를 제안한 논문입니다. 기존의 다지 손 파지 연구들은 주로 그립 안정성 확보에 집중했지만, 사람 의도에 맞는 섬세한 파지에는 미치지 못했습니다. 예를 들어, 이전 작업들은 로봇 손이 물체를 떨어뜨리지 않도록 잡는 데 초점을 맞추었으나, “손잡이를 검지로 눌러 잡아라”와 같이 특정 방식으로 물체를 잡는 인간의 의도를 반영하지는 못했습니다. 최근 과업 지향 또는 기능 지향 다지 파지 연구들이 등장했지만, 미리 정해진 한정적 작업들만 대응하여 유연성과 범용성이 부족했습니다.\nDexGYS 과제는 사람이 자연어로 설명하는 유연하고 세분화된 파지 의도를 로봇 손이 그대로 구현하는 것을 목표로 합니다. 이 과제 설정의 핵심 아이디어는, 로봇에게 언어로 의도를 전달하여 인간과 보다 자연스러운 상호작용을 가능케 하고, 로봇 다지 손의 잠재력을 의도 기반의 인간유사 파지로 활용하는 것입니다. 하지만 이러한 새로운 과제에는 두 가지 큰 도전이 존재합니다.\n첫째, 자연어 지시가 포함된 다지 파지 데이터가 부족합니다. 다지 손의 자세와 그에 대응되는 사람의 언어 지시를 동시에 고품질로 수집하는 것은 비용이 매우 높고 번거롭습니다. 기존에 언어와 결합된 파지 데이터셋이 없기 때문에, 학습을 위한 데이터 기반 자체가 부족한 상황이었습니다. 둘째, 의도 정합성, 파지 품질(안정성), 다양성을 모두 만족하는 로봇 손 자세를 동시에 생성하는 것이 어렵습니다. 특히, 다지 손이 물체를 관통하지 않도록 하는 penetration loss를 학습에 넣으면 파지 품질은 좋아지지만 오히려 의도에서 벗어나거나 다양성이 감소하는 현상이 발생합니다. 반대로 관통 페널티를 없애면 손가락이 물체를 뚫고 지나가는 물리적으로 불가능한 파지가 생길 수 있습니다. 이러한 트레이드오프 때문에 의도-품질-다양성을 한꺼번에 달성하기가 까다로운 문제가 제기됩니다."
  },
  {
    "objectID": "posts/paper/2025-08-14-grasp-as-you-say.html#제안한-방법의-기술적-기여-및-기존-연구와의-차별성",
    "href": "posts/paper/2025-08-14-grasp-as-you-say.html#제안한-방법의-기술적-기여-및-기존-연구와의-차별성",
    "title": "📃Grasp as You Say 리뷰",
    "section": "2.2 제안한 방법의 기술적 기여 및 기존 연구와의 차별성",
    "text": "2.2 제안한 방법의 기술적 기여 및 기존 연구와의 차별성\n이 논문의 기여는 데이터셋부터 모델에 이르는 총체적 해결책을 제시했다는 데 있습니다. 주요 기술적 기여와 기존 작업과의 차별성을 정리하면 다음과 같습니다.\n\n세계 최초의 언어 안내 다지 파지 데이터셋 구축: 저자들은 DexGYSNet이라는 대규모 언어-다지 파지 데이터셋을 새로 구축했습니다. 이 데이터셋은 1,800개 일상 물체에 대해 50,000쌍의 로봇 손 자세와 인간 자연어 지시로 이루어져 있습니다. 이전에는 이러한 형태의 데이터가 없어 학습에 어려움이 있었는데, DexGYSNet은 LLM(대규모 언어 모델)을 활용한 텍스트 주석 생성과 손-물체 상호작용 재타깃팅(HOIR) 기법으로 비용 효율적이면서도 고품질의 데이터를 확보했다는 점에서 큰 기여를 합니다. HOIR 기법을 통해 인간 손 모션 캡처 데이터를 로봇 손 모델로 전이하여 접촉 지점의 일치와 자연스러운 손 자세를 얻었고, LLM 기반 자동 주석으로 세밀하고 다양한 표현의 언어 지시를 생성했습니다. 이는 기존 작업들이 소수의 정형화된 지시나 제한된 기능만 다룬 것과 달리, 유연하고 풍부한 언어-행동 쌍 데이터를 처음 제공한 것입니다.\n의도 정합성과 품질을 모두 만족하는 새로운 파지 생성 프레임워크 제안:** 데이터셋을 바탕으로 저자들은 DexGYSGrasp라는 2단계 파지 생성 프레임워크를 제안했습니다. 이는 기존에 하나의 모델로 모든 목표를 달성하려다 관통 페널티로 인한 성능 저하를 겪었던 접근들과 달리, 학습 목표를 두 단계로 분리하여 문제를 해결합니다. 첫 번째 단계는 “의도 및 다양성 파지 생성 (IDGC: Intention & Diversity Grasp Component)”으로, 언어 의도에 부합하면서 다양한 파지를 생성하는 확산 생성 모델입니다. 이 단계에서는 관통에 대한 제약을 과감히 제외하여, 모델이 의도 정합한 다양한 손 자세 분포를 자유롭게 학습하도록 합니다. 두 번째 단계는 “품질 향상 파지 생성 (QGC: Quality Grasp Component)”으로, 1단계 출력인 거친 파지 결과(coarse pose)를 받아 미세 조정함으로써 안정적이고 물체를 관통하지 않는 고품질 파지로 개선합니다. 이 단계에서는 관통 손실과 품질 관련 손실을 적용하여 손가락들이 물체 표면에 밀착되도록 조정하되, 손바닥의 위치나 전체 의도는 유지하여 처음 의도에서 벗어나지 않도록 합니다. 이러한 점진적 학습 전략은 기존 연구에 없던 발상으로, 복잡한 최적화 문제를 둘로 쪼개어 각각 해결함으로써 의도-품질-다양성을 모두 달성한다는 점에서 차별화됩니다. 저자들은 특히 관통 손실을 1단계에서 배제하고 2단계에서만 적용하는 프로그레시브 학습 아이디어로, 기존 방법들이 직면했던 학습 상충 문제를 해결했다는 점을 강조합니다.\n기존 기법 대비 향상된 성능 입증: 제안한 DexGYSGrasp 프레임워크는 다양한 비교 대상(SOTA 기법)보다 우수한 성능을 보였습니다. 예를 들어, GraspCVAE(확률적 생성모델), GraspTTA(테스트시 최적화 기법), SceneDiffuser(확산 모델 기반), DGTR(Transformer 기반) 등과 비교했을 때, 의도 정합성 오차가 가장 낮고 파지 자세의 다양성은 월등히 높으면서도 파지 안정성도 우수했습니다. 또한 학습 전략의 유효성을 검증하기 위한 내부 실험(삭제 실험)에서도, 제안한 2단계 구성과 손실 설계가 없으면 성능이 급격히 악화됨을 보여주어, 해당 설계가 이 문제에 필수적임을 증명하였습니다. 요약하면, 이 논문은 새로운 문제 설정과 함께 이를 해결하기 위한 데이터셋, 모델, 학습법을 모두 제시하고, 그 결과로 사례 연구 분야에서의 새로운 SOTA를 달성했다는 점에서 큰 기술적 의미를 갖습니다."
  },
  {
    "objectID": "posts/paper/2025-08-14-grasp-as-you-say.html#모델-아키텍처-및-학습-방법의-구체적-분석",
    "href": "posts/paper/2025-08-14-grasp-as-you-say.html#모델-아키텍처-및-학습-방법의-구체적-분석",
    "title": "📃Grasp as You Say 리뷰",
    "section": "2.3 모델 아키텍처 및 학습 방법의 구체적 분석",
    "text": "2.3 모델 아키텍처 및 학습 방법의 구체적 분석\nDexGYSGrasp 프레임워크는 앞서 언급한 두 가지 컴포넌트(단계)로 구성됩니다. 첫 번째 컴포넌트(IDGC)는 조건부 확산 모델로서, 물체의 점 구름(point cloud)과 언어 임베딩을 입력 받아 로봇 손의 포즈(자세 파라미터)를 생성합니다. 이때 물체의 형상 정보는 PointNet++로 인코딩하고, 언어 지시는 사전 학습된 CLIP 모델로 임베딩하여 조건으로 활용합니다. 확산모델은 DDPM (Denoising Diffusion Probabilistic Model) 기법을 사용하여, 노이즈에서부터 점진적으로 손 자세를 생성해냅니다. 한마디로, IDGC는 “노이즈 → 손 파지 자세”로의 분포 생성을 학습하는 모듈입니다. 학습 시 손실 함수는 L2 회귀 손실(예측한 관절 각도 등이 실제 값과 가까워지도록)과 손 모양 챔퍼 손실(예측 손 모델과 정답 손 모델 간의 Chamfer 거리 최소화)로 구성됩니다. 중요한 점은, 이 단계에서는 물체와 손의 겹침(관통)에 대한 페널티를 넣지 않습니다. 관통 제약이 없어야 모델이 다양한 자세를 자유롭게 시도하며 언어 의도에 맞는 거친 파지 형태들을 폭넓게 익힐 수 있기 때문입니다. 이렇게 함으로써 IDGC는 의도 정합성과 다양성을 최우선으로 학습합니다.\n두 번째 컴포넌트(QGC)는 Transformer 기반의 후처리 모듈로, 1단계에서 생성된 거친 파지 결과를 세밀 조정하여 물리적으로 실행 가능한 고품질 파지로 변환합니다. QGC는 회귀적 접근(regressive manner)을 취하는데, 거친 손 자세와 해당 상황의 물체/손 점구름 정보를 입력으로 받아 미세 조정된 새로운 손 자세 출력을 예측합니다. 이때 학습 데이터는 IDGC로 생성한 거친 파지 결과에 대해, 유사한 의도의 인간 시연 정답(grasp)을 타겟으로 짝지어 구성합니다. 예를 들어 언어 지시가 “컵을 옆면에서 집게손가락과 엄지로 집어라”인 경우, IDGC 출력이 컵 옆면을 향하고 있다면, 그와 의도적으로 비슷한 데이터셋 내의 실제 그립 자세를 찾아 목표 값으로 삼아 QGC를 학습시키는 방식입니다. 이렇게 하면 QGC가 언어 의도에 부합하는 결과를 내도록 보장하면서, 동시에 타겟 파지를 모방하며 품질 향상을 배우게 됩니다. QGC의 네트워크는 트랜스포머 인코더-디코더 구조 등을 사용하여, 물체와 손의 복합 특징을 고려하면서 관절 각도 등을 조정하는 것으로 보입니다. 특히 학습 시 손바닥의 위치나 방향(6-자유도 루트 포즈)는 크게 변경하지 않고 고정하거나 작은 범위에서만 수정하도록 함으로써, 초기 거친 파지의 의도를 유지한 채 손가락 배치만 최적화하도록 설계되었습니다. QGC 단계의 손실 함수에는 물체 관통 페널티가 비로소 포함되며, 그 외에 목표 파지와의 회귀 손실 등이 사용됩니다. 관통 페널티는 손 모델(mesh)과 물체 점구름 간 최대 관통 깊이(P)를 줄이는 방향으로 작용하여, 출력 자세에서 손이 물체를 뚫고 들어가지 않도록 만듭니다. 이처럼 QGC는 품질만을 집중적으로 개선하기 때문에, 1단계와 대비하여 훨씬 좁은 탐색 공간(초기 파지 주변의 작은 조정)에서 효율적으로 학습될 수 있습니다.\n전체적으로 두 단계의 분리 덕분에, 1단계 IDGC는 의도와 다양성에 특화되어 학습하고 2단계 QGC는 물리적 타당성과 안정성 확보에 주력하게 됩니다. 프로그레시브 학습 전략으로 각 단계의 최적화 목표가 단순화되어, 단일 모델로 한 번에 학습할 때 발생하던 의도-품질 상충 문제를 해소할 수 있었습니다. 저자들의 설명에 따르면, 모든 손실을 한 단계에 동시에 최적화하려 하면 한 쪽을 충족하면 다른 쪽이 나빠지는 문제가 컸지만, 단계를 나누고 적절한 손실을 배치한 덕분에 의도 정합성, 그립 품질, 다양성 모두에서 뛰어난 성능을 얻을 수 있었다고 합니다.\n한편, DexGYSNet 데이터셋 구축 과정의 기술적 요소도 눈여겨볼 만합니다. HOIR(Human-to-robot Hand-Object Interaction Retargeting) 전략을 통해 사람 손 동작을 로봇 손으로 옮길 때 접촉 지점과 포즈의 일관성을 유지하도록 했습니다. 이로써 로봇 손이 사람이 잡은 형태를 최대한 그대로 모사하는 고품질 파지 데이터를 얻었습니다. 그리고 언어 주석을 달기 위해 GPT 등의 대규모 언어 모델을 활용, 각 파지에 대해 유연하고 상세한 자연어 설명을 자동 생성했습니다. 예를 들어 한 손 자세에 대해 “검지로 스프레이의 방아쇠를 누르듯이 잡는다”와 같은 문장이 주어지는 식입니다. 이러한 LLM 기반 기법은 사람을 일일이 참여시키지 않고도 다양한 표현의 지시 문장을 붙일 수 있게 해, 결과적으로 풍부한 학습 데이터를 저비용으로 확보했습니다.\n마지막으로 모델의 학습 파이프라인을 보면, 1단계 확산 모델(IDGC)은 약 100 epoch 동안 학습하고, 2단계 QGC는 20 epoch 남짓 학습했다고 합니다. 확산 모델의 샘플링 특성상 여러 샘플을 생성할 수도 있는데, 실험에서는 하나의 조건에 대해 8개의 샘플을 뽑아 다양성을 평가에 활용하기도 했습니다. 학습에는 SGD 등의 최적화 방법이 쓰였으며, 구체적인 하이퍼파라미터(예: 관통 손실 가중치는 2단계에서 얼마를 사용 등)는 논문에 기술되어 있습니다. 종합하면, DexGYSGrasp의 아키텍처는 확산 모델 + 트랜스포머의 2단 구조, 손실함수의 단계적 적용, 데이터셋 기반 지도학습의 조합으로 이루어져 있습니다."
  },
  {
    "objectID": "posts/paper/2025-08-14-grasp-as-you-say.html#실험-결과-분석-데이터셋-비교-대상-성능-지표-평가",
    "href": "posts/paper/2025-08-14-grasp-as-you-say.html#실험-결과-분석-데이터셋-비교-대상-성능-지표-평가",
    "title": "📃Grasp as You Say 리뷰",
    "section": "2.4 실험 결과 분석 – 데이터셋, 비교 대상, 성능 지표 평가",
    "text": "2.4 실험 결과 분석 – 데이터셋, 비교 대상, 성능 지표 평가\nDexGYSNet 데이터셋은 앞서 말한 대로 총 50,000개의 (언어 지시, 로봇 손 파지 자세) 페어로 이루어진 대규모 데이터셋입니다. 저자들은 이 데이터셋을 객체 인스턴스 수준에서 분리하여 실험했습니다. 즉, 같은 카테고리의 물체라도 일부는 학습용, 나머지 20%는 평가용으로 하여, 모델이 보지 못한 새로운 물체에 대해서도 파지를 생성하도록 설정했습니다. 이는 일반화 성능을 보기 위함으로, 특정 물체 모양만 외우지 않고 새 물체에도 언어 지시 기반 파지 생성이 가능한지를 평가한 것입니다.\n평가 지표는 의도 정합성, 파지 품질(안정성), 파지 다양성의 세 측면에서 설정되었습니다. 의도 정합성은 예측한 로봇 손의 형태가 목표 의도와 얼마나 일치하는가를 나타내며, Chamfer 거리와 Contact distance 두 가지를 사용했습니다. Chamfer 거리는 예측한 손 모델과 데이터셋 정답 손 모델 간 점구름 거리로, 값이 작을수록 손의 형태가 정답과 비슷함을 의미합니다. Contact distance는 예측 손과 정답 손이 물체를 접촉한 지점들의 분포 차이를 L2 거리로 측정한 것으로, 이것 역시 낮을수록 접촉 패턴이 유사함을 뜻합니다. 쉽게 말해 Chamfer와 Contact 지표가 작으면, “로봇 손이 잡은 모양과 위치가 사람이 의도한 그것에 가깝다”고 볼 수 있습니다. 파지 품질은 안정적으로 물체를 잡았는지를 평가하며, Q1 지표와 관통 깊이(P)를 사용했습니다. Q1은 DexGraspNet 논문【15†】에서 정의된 지표로서, 일정 기준(충분한 접촉면적, 허용 관통범위 등)을 만족하는 성공 그립의 비율을 의미합니다. 값이 높을수록 파지가 성공적이라는 뜻인데, 0~1 범위가 아닌 평균 접촉수 등의 형태로 산출되어 상대 비교에 사용됩니다. 관통 깊이(P)는 물체 표면과 손 모델 사이의 최대 겹침 깊이(cm)로, 값이 클수록 손이 물체를 많이 뚫고 들어갔음을 의미합니다. 이상적인 파지라면 P=0(관통 없음)이겠지만, 시뮬레이션/모델 한계상 약간의 겹침은 발생할 수 있으므로 작을수록 좋다고 봅니다. 파지 다양성은 동일 조건에서 생성된 여러 파지 결과의 변동 폭을 나타냅니다. 하나의 언어 지시와 물체에 대해 모델이 여러 번 파지를 생성해보면 매번 조금씩 다른 자세가 나올 수 있는데, 이 자세 파라미터들(손의 위치, 회전, 각 관절각)의 표준편차를 계산하여 수치화했습니다. 값이 크면 다양한 자세가 나온다는 뜻이고, 0에 가까우면 매번 비슷한 자세만 생성한다는 의미입니다.\n저자들은 여러 최신 기법(SOTA)들을 비교 대상으로 선정하여, 제안한 방법의 성능을 평가했습니다. 테이블 1에는 대표적인 비교 결과가 정리되어 있습니다. 비교 기법으로는 GraspCVAE【48†】(조건부 변분오토인코더 기반 생성), GraspTTA【41†】(기 학습된 모델을 테스트시 미세조정하는 기법), SceneDiffuser【4†】(3D 장면 확산모델 기반 파지 생성), DGTR【7†】(Dexterous Grasping Transformer, 트랜스포머 기반 생성) 등이 포함되었습니다. 공정한 비교를 위해, 이들 기존 기법에도 언어 조건을 입력으로 줄 수 있도록 약간의 구조 수정(예: 물체 점구름 특징과 언어 임베딩을 결합)하여 실험했다고 합니다.\n결과를 살펴보면, DexGYSGrasp(ours)가 전반적으로 가장 우수한 성능을 달성한 것을 알 수 있습니다. 의도 정합성 측면에서, Chamfer 거리와 Contact 거리 모두 우리 방법이 가장 낮았습니다. 구체적으로 Chamfer 거리의 경우 우리 방법은 1.198로, 두 번째로 낮은 SceneDiffuser의 1.679보다 훨씬 작고, 다른 방법들(대부분 2.03.1 이상)에 비해 크게 향상되었습니다. Contact 거리도 우리 방법은 0.036으로, 다른 방법들(0.045 이상) 대비 뚜렷하게 낮았습니다. 이는 예측한 손 자세가 정답 대비 매우 정확히 의도를 따라잡고 있음을 보여줍니다. 파지 품질(안정성) 측면에서는, 관통 깊이(P)의 경우 우리 방법은 0.223 cm 정도로, GraspTTA가 0.188 cm로 가장 작긴 했지만 그 외 다수 기법들은 0.250.55 cm 수준이어서, 우리 방법이 상당히 낮은 관통을 유지함을 알 수 있습니다. Q1 지표는 값이 높을수록 안정적인데, 우리 방법이 0.083으로 가장 높았고, 다른 기법들은 0.050.08 사이였습니다. 특히 GraspTTA는 관통이 적은 대신 Q1이 0.071로 우리보다 낮았고, SceneDiffuser 등은 관통이 약간 더 크면서 Q1은 비슷하거나 더 낮았습니다. 이를 종합하면 우리 방법은 품질 면에서도 타 기법들과 대등하거나 더 나은 안정성을 확보하고 있음을 의미합니다. 파지 다양성은 우리 방법의 두드러진 강점으로 나타났습니다. 우리 방법은 동일 조건 8회 생성 시 손바닥 위치의 표준편차 약 6.118, 회전 각도 표준편차 55.68, 관절 각도 표준편차 6.118 등을 기록했는데, 다른 방법들은 회전 각도 변동이 많아야 14 정도(DGTR)이고 대부분 18 범위에 그쳐 현저히 낮았습니다. 이는 우리 방법이 하나의 지시에도 아주 다양한 손 모양으로 잡을 수 있음을 뜻하며, 생성의 풍부함 측면에서 기존 방법들과 차별화되는 결과입니다. 결국 테이블 1의 결과는 DexGYSGrasp가 의도 일치도와 다양성에서 월등하며, 품질도 희생하지 않는 균형 잡힌 성능을 보여주는 것을 입증합니다.\n논문에서는 이러한 정량 평가 외에도 다양한 분석 실험을 수행하여 제안 기법의 동작을 검증하였습니다. 테이블 2에서는 프레임워크 구성요소와 학습 전략에 대한 ablation(요소 제거) 실험 결과를 제시합니다. 여기서는 한 단계 모델로 모든 것을 학습하거나(IDGC만 사용), 1단계 학습 중간에 관통 페널티를 서서히 높이는 등 다양한 변형을 시험했습니다. 그 결과, 두 단계로 나누지 않고 단일 모델로 학습하면 의도-품질-다양성의 균형을 이루지 못하고 한두 측면만 만족시키는 반쪽짜리 성능을 보였습니다. 또한 단계를 나누더라도 관통 페널티 사용 방식 등을 우리처럼 하지 않으면 성능이 개선되지 않았고, 2단계 없이 1단계로만 관통까지 모두 학습시키는 경우 역시 의도가 크게 어긋나는 문제가 생겼습니다. 반면 우리의 프로그레시브 설계(IDGC+QGC 둘 다 적용, 단계별 손실 할당)만이 의도 정합성, 품질, 다양성 세 마리 토끼를 동시에 잡는 결과를 냈습니다. 흥미롭게도, 테스트 타임 적응 기법(TTA)을 활용해 품질을 높이면 관통은 줄었지만 의도 일관성이 심각하게 떨어지는 현상도 관찰되었는데, 이는 품질만 후처리로 높이는 기존 접근의 한계를 보여주는 예라 할 수 있습니다. 종합하면 ablation 실험은 제안한 2단계 구조의 필요성과 설계 선택의 타당성을 강력하게 뒷받침합니다.\nHOIR 전략의 효과도 별도로 평가되었습니다. 저자들은 동일한 파지 데이터를 단순히 로봇 손으로 변경한 경우와 HOIR를 통해 접촉 일치시키며 변경한 경우를 비교하여 데이터 품질을 분석했습니다. 그 결과 HOIR를 사용했을 때 생성된 데이터의 손-물체 접촉 분포가 실제 사람 파지와 훨씬 유사해졌고, 모델 학습 시에도 더 안정적인 수렴과 성능 향상을 가져왔습니다 (세부 수치는 부록에 제시). 이는 HOIR가 없다면 데이터에 물체 표면을 스치지 못한 부실한 파지나 비현실적 손 모양이 생길 수 있지만, HOIR로 자연스러운 파지 예시들을 제공했기 때문에 모델이 현실성 높은 파지를 배울 수 있었음을 의미합니다.\n마지막으로, 저자들은 실물 로봇 실험을 통해 제안 기법의 현실 적용 가능성도 검증했습니다. 알레그로(Allegro) 로봇 핸드와 Flexiv Rizon 4 로봇 팔, 그리고 Intel RealSense D415 카메라로 구성된 실제 환경에서, 여러 가지 물체를 놓고 자연어 지시대로 집어보는 실험을 수행했습니다. 현실에서는 물체의 완전한 3D 형태를 알 수 없으므로, 카메라로 촬영한 물체 영상을 처리하여 부분 점구름을 얻은 뒤, SAM(Segment Anything Model)을 활용한 시각적 그라운딩으로 물체만 분리하고 포인트 클라우드 보완 네트워크로 완전한 물체 점구름을 복원하는 파이프라인을 사용했습니다. 이렇게 얻은 추정 물체 형상을 이용해 우리 모델이 파지 자세를 예측하면, 로봇 팔을 해당 위치로 이동시키고 로봇 손의 관절 각도를 예측 값으로 설정하여 파지를 실행했습니다. 그 결과 여러 가지 다양한 모양의 물체에 대해 지시한 방식으로 로봇 손이 물체를 성공적으로 움켜쥐는 모습을 보여주었고 (예: 스프레이 병의 방아쇠 누르기, 머그컵 손잡이 잡기 등), 이는 본 논문의 기법이 시뮬레이션을 넘어 현실 로봇에서도 효과적임을 입증했습니다. 실제 동영상 예시는 논문 사이트를 통해 공개되었으며, 전반적으로 사람의 언어 지시에 따라 로봇이 물체를 잡는 데 성공하는 장면들을 확인할 수 있습니다. 다만 센서 오차나 점구름 보완의 한계로 인해 일부 파지에서 아주 미세한 관통이나 불완전 접촉이 발생하기도 했지만, 이는 추가적인 제어 보정으로 개선 가능할 것으로 보입니다. 저자들은 전반적인 실험을 통해 제안 기법이 현재까지 보고된 방법들보다 우수하며, 실제 환경에서도 유용하다는 것을 강조하고 있습니다."
  },
  {
    "objectID": "posts/paper/2025-08-14-grasp-as-you-say.html#강점과-한계-향후-발전-가능성",
    "href": "posts/paper/2025-08-14-grasp-as-you-say.html#강점과-한계-향후-발전-가능성",
    "title": "📃Grasp as You Say 리뷰",
    "section": "2.5 강점과 한계, 향후 발전 가능성",
    "text": "2.5 강점과 한계, 향후 발전 가능성\n“Grasp as You Say” 논문의 강점은 명확합니다. 첫째, 문제 정의의 참신성입니다. 로봇 파지에 일반적인 자연어를 적용함으로써 인간-로봇 상호작용의 새로운 지평을 열었고, 기존 연구들이 다루지 못한 유연한 의도 반영 파지를 가능케 했습니다. 둘째, 이를 뒷받침하는 데이터셋과 기법의 완성도입니다. DexGYSNet 데이터셋은 학계 최초로 자연어 설명이 포함된 대규모 다지 파지 데이터를 제공하여 향후 관련 연구의 기반이 될 수 있습니다. HOIR+LLM을 통한 데이터 구축은 효율성과 다양성을 모두 확보한 뛰어난 방법으로, 이후 다른 로봇 행동 데이터셋 구성에도 응용될 수 있을 것입니다. 셋째, DexGYSGrasp 프레임워크의 독창적 설계와 효과성입니다. 관통 페널티에 기인한 학습 어려움을 2단계 프로그레시브 학습으로 풀어낸 것은 로봇 학습 분야에서 복잡한 다목적 최적화 문제를 해결하는 새로운 방법론으로 평가할 만합니다. 실험으로 입증되었듯, 이 접근법은 의도, 품질, 다양성이라는 상충하는 요소들을 모두 달성하여 이전까지 어려웠던 영역에서 성과를 냈습니다. 넷째, 시뮬레이션과 실제 실험을 아우르는 검증을 수행한 점도 강점입니다. 논문은 알고리즘 제안에 그치지 않고, 실제 로봇 팔과 손으로 동작시켜 봄으로써 현실성을 검증했습니다. 이는 해당 기법이 이론적 성능뿐 아니라 실용적 가치도 있음을 보여주며, 연구의 완성도를 높입니다.\n그럼에도 불구하고 몇 가지 한계와 향후 과제도 존재합니다. 우선, 데이터셋의 범위에 관한 한계입니다. DexGYSNet은 1,800개의 일상 물체를 포괄하지만, 산업용 복잡한 부품이나 비정형 물체 등은 포함되지 않았을 수 있습니다. 또한 언어 지시도 파지 동작에 초점을 맞춘 문장들로 구성되어 있어, 복합적 작업 시나리오(예: “잡아서 옮겨 놓아라”와 같이 파지 후 다른 행동을 수반하는 지시)에는 대응하지 못합니다. 향후에는 보다 다양한 객체 및 작업에 대해 데이터셋을 확장하고, 연속적 조작까지 포함하는 방향으로 발전시킬 수 있을 것입니다. 둘째, 모델의 복잡도와 실행 시간도 고려해야 합니다. 확산 모델을 사용한 1단계 생성은 본질적으로 샘플링에 다수의 확률적 단계를 거치기 때문에, 실시간 응용에는 속도 제약이 있습니다. 실제 로봇에 적용하려면 생성 시간 단축이나 경량화가 필요할 수 있습니다. 이를 위해 디퓨전 모델의 가속화 기법이나 단계 축소(예: DDIM), 또는 학습된 픽스드 모션 라이브러리 활용 등이 연구될 수 있습니다. 셋째, 일반화 능력에 대한 추가적인 검증이 필요합니다. 본 논문에서는 학습에 사용되지 않은 새 물체에 대해서도 실험했지만, 전혀 보지 않은 새로운 유형의 물체나 아주 다른 문장 표현에 대해서는 성능이 어떻게 되는지 더 살펴봐야 합니다. 예컨대, “이 물체를 아주 느슨하게 쥐어봐”와 같은 미묘한 힘 조절이나 추상적인 지시도 처리하려면, 모델을 보완하거나 추가 학습이 필요할 것입니다. 넷째, 물리기반 제한의 부족입니다. 2단계에서 관통을 줄였다고는 하나, 접촉 마찰이나 동적 안정성 등 정량화하기 어려운 물리적 요소는 고려되지 않았습니다. 향후에는 강화학습(RL)이나 물리 시뮬레이터 상의 fine-tuning으로 진짜 떨어뜨리지 않고 잡는 안정성까지 확보하면 더 완벽한 솔루션이 될 것입니다.\n그럼에도, 이러한 한계들은 현재 연구의 범위 밖의 것들이고, 본 논문의 기여를 폄하하지는 않습니다. 오히려 이 한계들은 향후 연구 기회를 제시합니다. 향후 발전 가능성으로는, 언어-로봇 상호작용을 더욱 확장하여 다단계 작업계획에 언어 지시를 연결하거나, 시각 인지와 언어, 행위를 통합하는 종합적인 프레임워크로의 발전이 기대됩니다. 예를 들어, “컵을 집어 식탁 오른쪽 구석에 놓아둬” 같은 복합 지시를 수행하려면, 파지뿐 아니라 이동, 놓기까지 통합된 계획이 필요하며, 본 연구의 성과는 이러한 방향으로 나아가는 시발점이 될 수 있습니다. 또한 휴먼 피드백 강화학습(RLHF) 등을 통해 사용자로부터 파지에 대한 피드백을 받아 더 미세한 조정을 학습하는 방법도 고려해볼 수 있습니다.\n요약하면, Grasp as You Say 논문은 자연어로 로봇 손 파지를 제어하는 혁신적 아이디어를 제시하고, 이를 구현하기 위한 데이터셋, 모델, 학습기법의 정교한 조합을 통해 새로운 수준의 성능을 달성한 연구입니다. 로보틱스 전문가에게 본 논문의 접근법은 다지 로봇 핸드 활용과 인간-로봇 인터랙션 영역에서 많은 영감과 시사점을 줄 것으로 보입니다. 앞으로 이 개념을 바탕으로 한 다양한 응용과 연구의 전개가 기대됩니다."
  },
  {
    "objectID": "posts/paper/2025-06-02-sparsh.html",
    "href": "posts/paper/2025-06-02-sparsh.html",
    "title": "📃Sparsh 리뷰",
    "section": "",
    "text": "Paper Link\nProject Link"
  },
  {
    "objectID": "posts/paper/2025-06-02-sparsh.html#연구-목표-및-주요-기여",
    "href": "posts/paper/2025-06-02-sparsh.html#연구-목표-및-주요-기여",
    "title": "📃Sparsh 리뷰",
    "section": "2.1 연구 목표 및 주요 기여",
    "text": "2.1 연구 목표 및 주요 기여\n이 논문의 핵심 목표는 여러 종류의 촉각 센서에 공통으로 적용 가능한 일반적인 촉각 표현을 학습하는 것입니다. 즉, 한 번 거대한 촉각 이미지 데이터로 사전학습한 백본 신경망을 갖추면, 개별 작업별로 일일이 센서 특성에 맞춘 모델을 새로 설계하거나 대량의 레이블을 모으지 않아도, 다양한 로봇 조작 과제에 촉각 정보를 활용할 수 있게 하자는 것입니다. 저자들은 이 목표를 달성하기 위해 자기 지도학습 기법을 촉각 데이터에 적용하고, 학습된 표현의 효과를 체계적으로 검증할 벤치마크를 구축하였습니다.\n논문에서 밝힌 주요 기여는 다음 세 가지로 요약됩니다:\n\n범용 촉각 표현 모델 (Sparsh)을 제시하였습니다. 다양한 비전 기반 촉각 센서 (예: DIGIT, GelSight 등)가 생성하는 이미지를 한데 모은 460,000장 이상의 촉각 이미지 데이터셋으로 SSL 사전학습한 공통 인코더를 개발하였습니다. 별도의 작업별 레이블 없이 마스킹 복원과 자기 증류 등의 자기 지도 학습 기법으로 학습된 이 모델들은 여러 센서에 걸쳐 사용할 수 있는 일반적인 촉각 특징 표현을 제공합니다.\nTacBench 벤치마크를 구축하였습니다. 이는 다양한 센서로부터 수집된 6가지 촉각 관련 과제에 대한 데이터와 평가 프로토콜로 구성된 벤치마크로, 촉각 표현의 성능을 표준화된 방식으로 평가할 수 있습니다. TacBench의 과제들은 접촉 힘 추정, 미끄럼 감지 등의 촉각 물리량 이해 과제부터 물체 위치 추적, 그립 안정성 예측, 재질 인식 등의 물체 인지 과제, 그리고 비드 미로 조작 같은 로봇 조작 계획 과제까지 폭넓게 포함하고 있습니다.\n대규모 촉각 데이터셋의 수집 및 통합을 수행하였습니다. 저자들은 기존에 공개된 여러 촉각 센서 데이터셋(YCB-Slide, Touch-and-Go, ObjectFolder 등)에 더해, 자체적으로 Touch-Slide라는 새 데이터셋을 수집하여 총 66만장 이상의 촉각 이미지 데이터를 확보했습니다. 이 중 약 46만장(70%)을 사전학습에 사용하고 나머지는 검증에 사용하였으며, 별도로 TacBench의 각 과제에 맞는 레이블된 데이터셋도 구축하여 공개하였습니다. 이러한 노력은 촉각 표현 학습을 위한 데이터 규모를 한층 끌어올려, 이전의 어떤 연구보다 1자리수 이상 많은 이미지로 모델을 훈련할 수 있게 하였다는 의의를 갖습니다.\n\n요약하면 Sparsh 연구는 (1) 다양한 촉각 센서에 일반화되는 대규모 사전학습 촉각 표현 모델을 만들고, (2) 그 성능을 평가할 수 있는 표준 벤치마크와 데이터셋을 제시했으며, (3) 이를 통해 한정된 레이블 데이터로도 높은 성능을 달성할 수 있음을 보여준 것입니다. 실제로 TacBench 실험 결과, 사전학습한 Sparsh 모델이 작업별로 처음부터 끝까지 학습한 모델보다 평균 95.1% 높은 성능을 보였으며(레이블 데이터 33~50% 사용 시), 특히 잠재 공간 표현(latent representation)을 학습하는 Sparsh (DINO)와 Sparsh (IJEPA) 변형이 가장 우수했다고 합니다. 이는 촉각 이미지의 미묘한 변화와 노이즈를 직접 복원하는 픽셀 공간 학습보다, 잠재 특징 공간에서의 예측 학습이 효과적임을 시사합니다."
  },
  {
    "objectID": "posts/paper/2025-06-02-sparsh.html#방법론-자기-지도학습-기반-촉각-표현-학습",
    "href": "posts/paper/2025-06-02-sparsh.html#방법론-자기-지도학습-기반-촉각-표현-학습",
    "title": "📃Sparsh 리뷰",
    "section": "2.2 방법론: 자기 지도학습 기반 촉각 표현 학습",
    "text": "2.2 방법론: 자기 지도학습 기반 촉각 표현 학습\nSparsh의 핵심은 자기 지도학습(SSL)을 통해 레이블 없는 촉각 이미지 데이터에서 유용한 표현을 학습하는 것입니다. 이를 위해 저자들은 최신 컴퓨터 비전 SSL 기법들을 촉각 도메인에 맞게 변형하여 활용하였습니다. 전체 프레임워크는 크게 대규모 촉각 이미지 사전학습 단계와 다운스트림 과제 평가 단계로 나뉩니다. 사전학습 단계에서 하나의 비전 트랜스포머(ViT) 인코더가 여러 센서의 촉각 이미지를 입력으로 공통된 잠재 표현을 학습하고, 이후 다운스트림 단계에서는 이 인코더를 고정(freeze)시킨 채 각 과제별로 간단한 디코더나 프로브(probe)를 학습시켜 표현의 품질을 평가합니다.\n그림 2: Sparsh 모델의 자기 지도학습 기법들. (a) 이번 연구에서 통합한 대규모 촉각 이미지 데이터셋의 구성: 새로운 Touch-Slide(인간이 장난감 부품을 문지르는 데이터)와 기존 YCB-Slide, Touch-and-Go, ObjectFolder 데이터셋 등으로 총 약 66만 장의 촉각 이미지가 사용되었다. 이 중 70%에 해당하는 약 46만 장을 자기 지도 사전학습에 활용하였다. (b) Sparsh (MAE) – 마스킹 자동인코더(Masked Autoencoder) 방식: 입력 이미지의 상당 부분(예: 75%)을 무작위로 가린 후, 남은 일부분만을 보고 전체 이미지를 복원하도록 인코더-디코더를 학습시킨다. 인코더는 ViT 기반이며, 마스크된 영역 복원은 간단한 디코더를 통해 수행되고 L2 픽셀 손실로 학습된다. (c) Sparsh (DINO) – 자기 증류(self-distillation) 방식: 동일한 구조의 학생 네트워크와 EMA로 업데이트되는 교사 네트워크 두 개를 두고, 하나의 촉각 이미지에 서로 다른 변환(crop 등)을 가한 두 입력을 통해 각각 특징을 추출한다. 각 특징을 별도의 MLP 헤드에 통과시켜 임의의 범주 분포(클러스터 확률)를 예측하고, 교사 출력에 학생 출력이 맞춰지도록 크로스엔트로피 손실로 학습한다. 이렇게 하면 레이블 없이도 학생 네트워크가 교사의 군집화된 표현을 모방하며 의미 있는 잠재 표현을 얻게 된다. (d) Sparsh (I-JEPA) – Joint Embedding Predictive Architecture 방식: 학생(컨텍스트 네트워크)과 교사(타겟 네트워크)로 구성되며, 교사 네트워크는 EMA로 갱신된다. 학생 네트워크는 이미지의 일부 영역만 관찰하여 특징을 만들고 작은 예측기(predictor)를 통해 교사 네트워크의 특징을 맞춰보는 작업을 한다. 구체적으로, 학생은 이미지의 상당 부분을 마스킹한 글로벌 시야를 가지고, 교사는 이미지의 국소 패치들만을 본다. 학생의 컨텍스트 특징을 통해 교사의 지역 특징을 예측하도록 L1 손실로 학습함으로써, 부분 관찰을 기반으로 전체 정보를 추론하는 능력이 향상된다. Sparsh에는 정적인 이미지에 적용한 I-JEPA뿐 아니라, 연속적인 비디오 프레임에 적용한 V-JEPA도 포함되어 있다.\n사전학습에 사용된 모델 아키텍처는 기본적으로 ViT-Base (패치 크기 14) 구성입니다. 입력으로 들어오는 촉각 이미지는 배경 차감(background subtraction)을 거치는데, 이는 각 센서에서 접촉이 없을 때의 기본 영상을 빼줌으로써 조명이나 마커 패턴 차이 등 센서별 잡음을 줄이기 위함입니다. 이렇게 하면 같은 종류의 센서 내에서는 조금 다른 제조 공정으로 인한 배경 차이도 보정되어, 모델이 진짜 접촉에 의한 변화에 집중할 수 있게 됩니다. 또한 촉각 센싱은 시간적인 맥락이 중요한 경우가 많기 때문에, 저자들은 시계열 정보를 입력에 반영하였습니다. 구체적으로, 정적인 이미지 기반 SSL 기법들(MAE, DINO 등)에는 한 센서 프레임에서 약 80ms 간격으로 떨어진 두 장의 이미지를 채널 차원으로 붙여서 한 입력으로 사용하였습니다. 보통 촉각 센서가 60 FPS로 동작함을 감안하여 5프레임 차이를 둔 것으로, 이는 인간이 미끄러짐을 감지하고 힘을 조절하는 반응 시간과 비슷한 수준입니다. 한편 비디오 기반 SSL 기법(V-JEPA)에는 4프레임 길이의 짧은 클립(동영상 조각)을 입력으로 사용하여 시간에 따른 변화까지 학습하도록 하였습니다. 이렇게 두 장 또는 여러 장의 연속 이미지를 활용함으로써, 촉각 변화의 동적 패턴(예: 미끄러지기 전의 미세한 움직임)을 포착할 수 있도록 한 것이 특징입니다.\nSSL 학습은 무엇을 예측하느냐에 따라 픽셀 공간 또는 잠재 공간에서 이루어지는데, 앞서 언급했듯이 저자들은 잠재 공간에서의 학습이 더 효율적일 것으로 가설을 세웠습니다. 촉각 이미지는 센서의 국소적 접촉만 담고 있어 한 장면만으로는 모호성이 존재할 수 있고, 촬영 조건에 따라 마커나 조명 변화 같은 방해 요소도 큽니다. 따라서 모든 작은 디테일까지 복원하려는 픽셀 재구성보다는, 더 추상화된 특징에 집중하는 방법(예: DINO, JEPA)이 이런 불확실성과 잡음을 무시하고 유용한 패턴을 학습하는 데 유리할 것이라 본 것입니다. 실제 실험에서도 나중에 보겠지만, DINO나 I-JEPA처럼 잠재 표현 학습을 한 모델이 MAE같이 픽셀 복원을 한 모델보다 전반적으로 우수한 다운스트림 성능을 보였습니다.\nSparsh 사전학습은 150 에포크 동안 진행되었으며, AdamW 옵티마이저와 코사인 학습률 스케줄 등이 사용되었습니다. 모든 모델은 학습 시 [CLS] 토큰을 사용하지 않고, 패치 임베딩들의 풀링이나 별도 헤드를 통해 학습되었습니다. 특히 DINO의 경우 [CLS] 토큰 대신 ViT의 특정 레지스터를 활용하여 클러스터 확률 출력을 얻는 등, 약간의 구조 변형이 있었습니다. 하이퍼파라미터로는 DINO와 JEPA 방식에서 EMA 모멘텀 계수(예: DINO 0.998, IJEPA 0.996)와 학습률 등이 조정되었고, 배치 크기는 MAE 100, 나머지 150으로 세팅되었습니다. 최종적으로 학습된 Sparsh 인코더는 약 0.86억 개 파라미터(ViT-Base 수준)이며, GPU 기준으로 100 FPS 이상의 추론 속도를 보여 실시간 활용에도 무리가 없음을 확인하였습니다 (예: Sparsh (DINO) ~112FPS)."
  },
  {
    "objectID": "posts/paper/2025-06-02-sparsh.html#실험-tacbench-벤치마크-평가",
    "href": "posts/paper/2025-06-02-sparsh.html#실험-tacbench-벤치마크-평가",
    "title": "📃Sparsh 리뷰",
    "section": "2.3 실험: TacBench 벤치마크 평가",
    "text": "2.3 실험: TacBench 벤치마크 평가\nSparsh의 효과를 검증하기 위해 저자들은 TacBench라 불리는 벤치마크를 구축하여 일련의 다운스트림 과제 실험을 수행했습니다. TacBench에는 앞서 소개한 대로 6개의 대표적인 촉각 과제(T1–T6)가 포함되어 있습니다. 각 과제는 서로 다른 센서와 데이터셋으로 구성되며, 구체적인 내용은 다음과 같습니다:\n\n[T1] 힘 추정 (Force Estimation): 촉각 센서의 젤에 가해지는 3축 힘(수직+두 축의 전단력)을 추정하는 회귀 문제입니다. 저자들이 구축한 Shear Load 데이터셋을 사용하며, 로봇 팔에 장착된 촉각 센서(DIGIT 또는 GelSight Mini)로 반구형, 뾰족한, 평평한 모양의 인덴터(indenter)를 눌러가며 동시에 힘 센서 값과 촉각 영상을 기록하였습니다. DIGIT은 60fps, GelSight Mini는 25fps로 촬영되어 총 각 7.5만 샘플의 정렬된 영상-힘 데이터가 얻어졌습니다. 평가 지표는 3축 힘의 RMSE(평균 제곱근 오차)이며, 학습 시 실제 힘값을 정규화하여 L1 손실로 예측하도록 디코더(작은 MLP)를 학습시켰습니다.\n[T1A] 힘장 시각화 (Force Field Visualization): T1의 부가 실험으로, 촉각 이미지로부터 젤 표면 전체의 접촉 힘 분포 (정규력 분포 및 전단력 벡터장)를 추정하는 과제입니다. 마커가 있는 센서의 경우 마커 움직임 추적으로 전단 변형장을 얻는 것이 가능하지만, 마커가 없는 센서에서는 전체 장(field)의 ground truth를 얻기 어려워 이 작업이 잘 수행되어오지 않았습니다. 저자들은 Sparsh 표현을 이용해 마커 없는 센서의 힘장도 추정이 qualitatively 가능함을 보였는데, 정규력장은 깊이지도(depth map)로, 전단력장은 옵틱 플로우 문제로 프레임 간 변위를 예측하도록 설정하여 감독없이 디코더(CNN+DPT 구조)를 학습시켰습니다. 그 결과 Sparsh (DINO) 모델을 활용하면, 접촉 패치의 움직임 방향, 예를 들어 미끄럼 방향이나 비틀림(torsion), 접촉 시의 퍼져나가는 형태 등을 유의미하게 보여주는 전단력 벡터장을 그려낼 수 있었습니다. 이는 그림 4 (vi)에 시각화 예시가 제시되어 있습니다.\n[T2] 미끄럼 감지 (Slip Detection): 물체가 손가락에서 미끄러지는지 여부를 이진 분류하는 과제입니다. 실험을 위해 T1과 동일한 장비로 반구형 인덴터를 사용하되, 일부 구간에서는 센서 표면이 미끄러지도록 힘의 방향을 조절하여 stick-slip 동작을 만들어냈습니다. 마찰 원뿔 모델에 따라, 정적 마찰 한계를 넘은 경우를 미끄럼 발생으로 레이블링 하였고 (정적 마찰계수는 실험적으로 추정), 총 12.5만 샘플 중 약 13%가 미끄럼으로 레이블되었습니다. 데이터 불균형이 있기 때문에 평가에는 F1-score를 주로 사용했습니다. 학습 시에는 미끄럼 유무 분류 MLP와 힘 변화량(regression) MLP를 멀티태스크로 동시에 학습시켰는데, 힘 변화 (특히 전단력의 변화)가 미끄럼과 밀접한 관련이 있어 두 값을 함께 예측하면 성능이 좋아짐을 관찰했기 때문입니다.\n[T3] 물체 자세 추적 (Pose Estimation): 촉각 이미지를 통해 센서에 접촉한 물체의 상대 자세(SE(2) 변환) 변화를 추정하는 과제입니다. 이는 손가락과 물체 사이의 미끄러짐 정도(평면 이동 및 회전)를 추적하는 것으로 볼 수 있습니다. 데이터셋은 DIGIT 센서로 사람이 물체(예: YCB 물체)를 살짝 미끄러뜨리는 장면을 촬영하고, 동기화된 모션 캡처 시스템으로 물체의 실제 자세 변화를 기록하여 만들었습니다. 총 4.9만 쌍의 (이전 이미지, 현재 이미지)와 그 사이의 물체 변환 레이블이 있으며, 학습 시 분류를 통한 회귀(regression-by-classification) 방식을 채택했습니다. 즉, 평면 상의 x, y 이동 및 회전 변화를 연속값 대신 여러 구간으로 양자화(binning)하여 다중 클래스 분류 문제로 변환한 것입니다. 이동은 수백 μm 정도의 해상도로, 회전은 수 도(degree) 단위로 구간화하여, 각 자유도에 대해 별도의 Softmax 출력을 내도록 하였습니다. 평가 지표로는 분류 정확도(정답 구간 맞춘 비율)를 사용했습니다.\n[T4] 그립 안정성 예측 (Grasp Stability): 로봇 손가락으로 물체를 잡았을 때 성공적으로 들었는지(안정적으로 파지되었는지) 여부를 맞추는 이진 분류 과제입니다. 이는 촉각 연구에서 오랫동안 다뤄진 주제인데, 본 논문에서는 기존 Feeling of Success 데이터셋을 활용했습니다. 이 데이터셋은 병 따기용 로봇 손가락에 GelSight 센서를 부착하여 100여 개 물체를 잡은 후 성공/실패를 기록한 것으로, 하나의 시도마다 잡기 전, 잡는 중, 잡은 후의 세 장의 촉각 이미지가 제공됩니다. 총 약 9300회 시도 중 성공과 실패 사례가 모두 포함되어 있습니다. 본 연구에서는 한 손가락의 촉각 정보만 사용하므로, 세 개 이미지 중 잡기 전과 잡는 중 두 이미지를 히스토리로 입력으로 주었고, 레이블은 그립 성공 여부(예/아니오)입니다. 데이터셋에 공식 분할이 없어서 임의로 8000여 개 시도를 학습에, 1300여 개를 테스트에 사용하였습니다. 성능 지표는 분류 정확도입니다.\n[T5] 직물 재질 인식 (Textile Recognition): 촉각만으로 다양한 직물의 재질을 분류하는 과제입니다. 기존 연구에서 정의한 Clothing Dataset을 사용하였는데, 이는 GelSight 2017 센서(마커 패턴 있음)로 가죽, 면, 폴리에스터 등 20가지 직물을 잡았을 때의 촉각 영상이 담긴 짧은 비디오 클립 4467개로 이루어져 있습니다. 각 클립은 10~25프레임 길이이며, 로봇이 옷감을 살짝 쥐었다 놓는 동작 등을 포함합니다. 우리는 제공된 메타데이터의 학습/시험 분할을 따랐습니다. 이 문제는 20-class 분류이며 평가 지표는 정확도입니다.\n[T6] 비드 미로 (Bead Maze) 조작: 장난감인 비드 미로(고리 모양의 구슬을 막대 기둥을 따라 움직이는 퍼즐)를 로봇이 촉각 피드백으로 풀도록 하는 강화 학습적 과제입니다. 한쪽 끝에 구슬이 걸려 있는 구불구불한 막대 경로를 따라 구슬을 다른 끝까지 움직이는 것이 목표로, 로봇은 구슬을 집은 손가락에 달린 촉각 센서로부터 저항을 느끼며 힘을 조절해야 합니다. 시각적으로는 손가락에 가려 잘 안 보이고, 마찰로 인한 미세한 막힘 등을 감지해야 하기 때문에 촉각에 의존하는 문제 설정입니다. 저자들은 프랑카(Franka) 로봇팔과 DIGIT 센서를 이용하여 다양한 모양의 미로에서 50회 데모 시연을 모았습니다. 절반은 VR 장치를 활용한 원격 조작으로, 절반은 사람의 수동 조작으로 수행되었습니다. 총 3.4만 개의 (촉각 이미지 시퀀스, 로봇 관절 명령) 쌍이 데이터로 확보되었고, 이를 이용해 Diffusion Policy 라는 최신 Behavior Cloning 알고리즘으로 정책(policy) 학습을 합니다. 이때 시각용으로 설계된 원래 Diffusion Policy의 인코더를 Sparsh 사전학습 인코더로 교체하여, 촉각 관측을 처리하도록 만들었습니다. 학습된 정책은 현재까지의 촉각 이미지 히스토리(예: 2~3장)와 현재 로봇 관절각을 입력으로 받아, 다음 관절 각 변화량을 출력합니다. 평가에서는 데모와 로봇의 실행 결과 간의 거리 차이(trajectory error)를 짧은 구간별(3cm 이동당)로 측정하여 누적 오차를 계산하였으며, 최종적으로 실제 로봇에 정책을 실행하여 얼마나 먼 거리까지 구슬을 운반하는지도 측정했습니다.\n\n각 과제마다, Sparsh로 사전학습된 인코더는 동결하고, 그 위에 작은 태스크별 디코더/프로브만 학습시켰습니다. 구체적으로, [T1]~[T5] 대부분의 과제는 어텐션 풀링 기반 디코더를 사용했습니다. 이는 Sparsh 인코더의 패치별 출력에 교차 어텐션(cross-attention)을 적용해 전역적인 특징을 모은 후, 2계층 MLP를 통해 최종 예측을 산출하는 소규모 네트워크입니다. 이렇게 하면 사전학습 표현이 얼마나 해당 과제의 정보를 담고 있는지 선형 분류기 수준에서 평가할 수 있습니다. 한편 힘 분포 필드 재구성(T1A) 같은 Dense 예측이 필요한 경우에는, Sparsh 인코더의 중간 특징들을 받아 DPT(Dense Prediction Transformer) 디코더로 픽셀 단위 출력을 복원하도록 설계했습니다. [T6] 비드 미로의 경우는 강화학습 정책 특성상 end-to-end로 학습이 이뤄지지만, 비교를 위해 Sparsh 인코더를 고정한 버전과, 처음부터 정책과 인코더를 함께 학습한 버전을 모두 시험했습니다.\n이제 TacBench 실험 결과를 살펴보겠습니다. 전체적인 결론부터 말하면, Sparsh 사전학습 표현은 다양한 촉각 과제에서 매우 뛰어난 성능 향상을 보여주었습니다. 우선 공통적으로, 레이블된 데이터가 적을수록 Sparsh의 효과는 더욱 두드러졌습니다. 그림 1 가운데 그래프에서 볼 수 있듯이, 사전학습 없이 개별 과제 전용으로 학습한 모델(E2E)들은 학습 데이터가 충분할 때는 일정 수준 성능을 내지만, 데이터가 줄어들수록 성능이 급격히 떨어졌습니다. 반대로 Sparsh 인코더를 사용한 모델들은 레이블 1/3만으로도 준수한 정확도를 유지했고, 10%나 1% 수준의 극소량 레이블로도 어느 정도 유의미한 결과를 냈습니다. 예를 들어 힘 추정의 경우, Sparsh (DINO) 모델은 레이블의 10%만 써도 0.1N 이하의 오차를 유지했으며, 미끄럼 감지도 1% 데이터로 학습해도 F1-score가 꽤 유의미하게 나왔습니다. 반면 E2E 모델은 33% 이하에서는 아예 출력이 한쪽으로 치우치는 등 제대로 학습되지 않는 현상이 관찰되었습니다. 이러한 경향은 정량적인 평균 지표로도 확인되는데, 저자들이 명시하였듯이 Sparsh 모델들은 레이블 33% 조건에서 E2E 대비 평균 95.1% 성능 향상을 이루었습니다. 다시 말해 레이블이 부족할 때 두 배 가까운 성능 격차를 낸 것입니다. 심지어 모든 과제가 레이블 충분 조건(Full data)에서도 Sparsh가 E2E보다 대체로 우수했는데, 이는 사전학습 표현이 학습 효율뿐 아니라 일반화 성능 측면에서도 이점이 있음을 시사합니다.\n각 과제별 상세 결과를 요약하면 다음과 같습니다:\n\n힘 추정 [T1]: Sparsh 표현을 이용하면 정규화된 힘 추정 오차를 매우 낮게 유지할 수 있었습니다. 특히 GelSight Mini 센서의 경우 해상도가 높고 배경 대비 접촉 영역이 작아서 E2E 모델은 충분한 데이터가 있어도 과적합 등의 문제로 정확도가 낮았지만, Sparsh 인코더를 쓴 모델은 안정적으로 작은 RMSE를 기록했습니다. DIGIT의 경우는 데이터가 충분하면 E2E도 어느 정도 성능을 냈으나, 레이블이 적을 때는 Sparsh (특히 DINO 변형)이 훨씬 견고하게 낮은 오류를 보였습니다. Sparsh (DINO)는 소량의 학습데이터로도 강건하게 힘을 예측하여, 레이블 부족 상황에서 두드러진 성능을 발휘했습니다.\n힘장 시각화 [T1A]: 정량적 평가가 어려운 과제이지만, Sparsh 표현을 사용해 추정한 전단력 벡터장은 마커 없는 GelSight 센서에서도 접촉 패치의 움직임 방향을 잘 나타내주었습니다. 예컨대 미끄러지는 방향으로 화살표가 일정하게 그려진다든지, 비틀리는 경우 회전형 패턴이 나타난다든지, 새로운 접촉이 일어날 때 퍼져나가는 형태가 보이는 등, 사람의 촉각적 직관과 맞아떨어지는 시각화를 얻었습니다. 이는 기존에 마커가 없으면 얻기 힘들었던 정보를 사전학습 표현으로 보완한 흥미로운 결과입니다.\n미끄럼 감지 [T2]: 불균형 데이터(미끄럼 적음)와 센서 노이즈로 인해 DIGIT 센서에서는 특히 어려운 문제였지만, Sparsh의 V-JEPA 변형이 가장 뛰어난 성능(F1-score)을 달성했습니다. Sparsh (VJEPA)는 비디오 클립 입력을 활용해 시간에 따른 변화를 직접 학습한 덕분에, 정적인 방법들보다 미끄럼 징후 포착에 유리했습니다. 그 결과 DIGIT처럼 표면에 마커 패턴이 없어 전단력 방향 파악이 어려운 센서에서도, 미끄럼 여부를 훨씬 정확히 분류해냈습니다. 특히 학습데이터의 50% 이하만 사용할 경우 Sparsh 모델과 E2E 사이 성능 격차가 크게 벌어졌는데, Sparsh (VJEPA)는 50% 데이터로 학습해도 E2E의 100% 데이터 성능을 능가할 정도였습니다.\n자세 추정 [T3]: 이 과제에서는 클래스가 촘촘하게 양자화되어 있어, 적은 데이터로 학습하면 인접한 각도나 이동 구간을 구별하기 어려워집니다. 실제로 E2E 모델은 레이블이 줄어들수록 혼동이 심해져, 예컨대 0도와 5도의 회전 차이도 맞히지 못하고 엉뚱한 큰 값만 예측하는 경향(극단값으로 수렴)이 나타났습니다. 반면 Sparsh 인코더 기반 모델은 1/3 수준의 데이터로도 꽤 높은 정확도를 유지했고, 적은 데이터에서 극단값으로 치우치는 현상도 덜했습니다. 충분한 데이터가 있을 때는 E2E와 큰 차이가 없었지만, 저데이터 시나리오에서 Sparsh의 이점이 두드러져, 미세한 각도 차이까지 비교적 잘 예측하였습니다.\n그립 안정성 [T4]: 학습 데이터가 충분한 (8000개 이상) 경우에는 Sparsh나 E2E나 모두 유사한 높은 정확도를 보였습니다. Sparsh (IJEPA)와 Sparsh (VJEPA)는 약 90%에 가까운 정확도를 달성하여, 시각+촉각 멀티모달 정보를 함께 쓴 이전 연구보다도 오히려 높은 성능을 나타냈습니다. 주목할 점은 여기서 Sparsh는 한 손가락의 촉각 정보만 사용하고도, 과거에 시각까지 동원한 모델을 앞질렀다는 것입니다. 또한 Sparsh 기반 모델은 데이터 33%로 학습해도 큰 성능 저하 없이 80% 이상 정확도를 유지했고, 10% (800개 미만)만으로도 70%대 수준을 보여 실용적인 성능을 발휘했습니다. 그러나 80개 (1%) 정도로 극단적으로 줄이면 확연히 떨어져서, 저자들은 이 경우엔 무작위 추측 수준으로 전락함을 관찰했습니다. 그럼에도 불구하고 전반적으로 Sparsh 표현은 촉각만으로 그립 성공 여부를 신뢰성 있게 예측할 수 있음을 검증한 셈입니다.\n직물 인식 [T5]: 이 과제는 20-way 분류로 난이도가 높고, 기존 연구에서도 E2E 학습이 어렵다고 보고된 바 있습니다. 본 연구 결과 역시 E2E로는 좋은 성능을 내기 힘들었지만, 사전학습 표현을 사용하자 성능이 크게 향상되었습니다. 특히 Sparsh (MAE) 모델이 두드러졌는데, 픽셀 수준의 질감 특징을 복원하도록 학습된 덕분에 촉감으로 재질을 구분하는 데 유리했던 것으로 보입니다. Sparsh (MAE)를 사용하면 전체 데이터로 학습할 때 정확도가 크게 높아졌을 뿐 아니라, 10% 이하의 데이터로도 E2E의 풀데이터 성능에 맞먹는 결과를 얻었습니다. 흥미로운 추가 실험으로, 저자들은 센서 간 전이 학습을 시도했는데, 예컨대 GelSight로 사전학습한 Sparsh를 이용해 DIGIT 센서의 직물 인식을 몇 개 샘플로 파인튜닝하면 금방 적응한다는 것을 보였습니다. 단 10-shot(직물별 10장) 정도의 소량 학습으로도 새로운 센서에 맞게 분류기가 조정되어, 사전학습 표현의 범용성을 확인시켜주었습니다.\n비드 미로 [T6]: 강화학습(행동 클로닝) 문제에서도 Sparsh 표현이 도움을 주는지 검증했습니다. 우선 시뮬레이션 환경(데모 경로 추종)에서의 결과를 보면, Sparsh (DINO)와 Sparsh (IJEPA) 기반 정책이 E2E 정책보다 궤적 오차가 약 16% 적게나타났습니다. 같은 데이터로 학습해도 사전학습 인코더를 쓴 쪽이 훨씬 데모를 정확히 재현한다는 의미입니다. 또 학습에 쓰인 데모 개수를 줄여보는 실험에서도 Sparsh 모델은 적은 시연으로도 괜찮은 정책을 학습했지만, E2E는 데모가 줄면 성능 급하락을 보여 표현 학습의 데이터 효율성을 재확인했습니다. 마지막으로 실제 로봇에 정책을 실행하여 테스트한 결과, 모든 모델이 미로 완주는 실패했지만 Sparsh 기반 정책들이 E2E보다 20~53% 더 먼 거리까지 구슬을 움직이는 데 성공했습니다. 즉, 완전한 성공률 측면에서는 한계가 있었으나, 사전학습 표현이 부분적인 성능 향상과 안정적인 조작에 기여함을 알 수 있습니다. 다만 실제 로봇 실험에서는, 시뮬레이션과 달리 로봇의 미세한 제어 한계, 한번 미끄러지면 재잡기 어려운 물리적 상황 등이 작용하여 사전학습 효과가 완전히 발휘되지 못했습니다. 그럼에도 Sparsh를 사용한 정책이 전반적으로 더 나은 결과를 보였다는 점에서, 촉각 표현 학습이 로봇 정책 학습에도 유용할 수 있다는 가능성을 보여주었습니다.\n\n以上의 실험들을 종합하면, Sparsh로 학습된 촉각 표현은 여러 센서와 여러 과제를 아우르며 성능을 향상시킵니다. 특히 레이블이 제한적인 현실에서, 사전학습 모델을 쓰면 훨씬 안정적이고 높은 성능을 얻을 수 있음을 확인했습니다. 또한 Sparsh의 다양한 변형들 중 DINO 기반 모델은 힘/자세 추정 같은 물리량 회귀 과제에 강했고, I-JEPA 기반 모델은 미끄럼, 안정성, 재질 분류 같은 의미론적 분류 과제에 좀 더 성능이 우수한 경향을 보였습니다. 이는 픽셀 복원 vs 특징 예측의 차이뿐 아니라, 학습 목표에 따른 특화 효과도 일부 있음을 시사합니다. 재미있게도, 두 모델은 비드 미로 정책처럼 복합적인 과제에서는 비슷한 성능을 냈는데, 이는 그 과제가 힘과 미끄럼에 대한 종합적 이해를 요구하기 때문일 것입니다."
  },
  {
    "objectID": "posts/paper/2025-06-02-sparsh.html#기존-연구와의-비교",
    "href": "posts/paper/2025-06-02-sparsh.html#기존-연구와의-비교",
    "title": "📃Sparsh 리뷰",
    "section": "2.4 기존 연구와의 비교",
    "text": "2.4 기존 연구와의 비교\nSparsh 이전의 비전 기반 촉각 인지 연구들은 주로 특정 작업에 초점을 맞춰 개별적인 모델을 개발하곤 했습니다. 초창기 접근법으로는, 촉각 이미지를 해석하기 위해 마커 추적이나 물리 시뮬레이션(FEM) 등을 활용하여 접촉 지형과 힘을 계산하는 방식이 있었습니다. 그러나 이러한 방식은 센서마다 마커 패턴 보정이 필요하거나 계산 비용이 크다는 한계가 있었습니다. 이후 학습 기반 기법들이 등장하면서, 개인별로 커스텀 디자인한 신경망을 사용하여 촉각 정보를 처리하는 시도가 많았습니다. 예를 들어, 어떤 연구에서는 특정 촉각 센서에 맞춘 CNN 인코더를 설계해 종이 질감 분류를 학습하거나, BioTac이나 GelSight 등의 센서 데이터를 전이학습하여 패브릭의 여러 성분(실, 질감 등)을 알아내는 작업을 하기도 했습니다. 심지어 최근에는 XELA라는 센서의 출력을 최근접 이웃 비교만으로 조작에 활용한 사례도 있었는데, 이는 촉각 표현을 명시적으로 학습하지 않고 임베딩 공간에서 비슷한 촉각을 찾아내는 방식이었습니다. 하지만 이렇듯 파편화된 접근들로 인해, 서로 다른 연구 간에 결과를 비교하거나 표준 모델을 공유하기가 어려웠습니다. 표준화된 사전학습 모델이 없었기 때문에, 매번 새로운 문제나 센서가 나오면 다시 데이터 수집에서 모델 디자인까지 처음부터 해야 하는 비효율이 존재했습니다.\n자기 지도 표현 학습에 대한 관심도 점차 높아져, Sparsh 이전에도 몇 가지 관련 연구가 등장했습니다. 한 예로, MAE 기법을 촉각에 적용하여 재질 분류 등에 효과적임을 보인 초기 실험이 있었고, 다양한 촉각 센서(BioTac 등)에 대해 사전학습 후 미세조정(fine-tuning)을 하여 성능을 높인 시도도 있었습니다. 그러나 이들 역시 단일 센서 혹은 한정된 범위의 작업에 초점을 맞추고 있어 일반성이 부족했습니다.\n한편, 멀티모달 학습 측면에서는 시각-촉각 간의 연관성을 학습하려는 노력들도 있어 왔습니다. 예를 들어, 동일한 물체를 만졌을 때의 촉각 이미지와 시각 이미지 쌍을 모아 대조 학습(contrastive learning)으로 공통 임베딩 공간에 매핑하거나, 촉각 정보로 시각적 스타일을 변환하는 등의 실험이 이루어졌습니다. 이러한 연구들은 재질 분류나 그립 성공 예측 등에 있어서 촉각+시각 결합의 가능성을 보여주었지만, 주로 표면의 질감이나 물체의 시각적 특징에 집중되었습니다. 즉, 힘, 미끄럼, 자세 변화와 같은 물리적 접촉 특성까지 다루지는 못하였습니다. 이러한 부분은 여전히 멀티모달 접근법의 도전 과제로 남아 있습니다.\nSparsh와 가장 유사하거나 병행된 연구로 논문에서 언급한 것은 T3와 UniT라는 두 가지입니다. T3는 여러 촉각 센서에 대해 공유 인코더(shared trunk)를 학습하되, 센서별로 개별 가지(branch)를 둔 구조입니다. MAE 방식의 복원 과제와 함께 몇 가지 레이블된 작업을 혼합해 학습한 점이 특징이지만, 결국 센서마다 별도 인코더 파라미터를 유지해야 하므로 완전히 하나의 통합 모델이라고 보긴 어렵습니다. 또한 T3는 제한된 데이터로 학습되어 Sparsh만큼 광범위한 검증은 이루어지지 않은 것으로 보입니다. UniT는 GelSight Mini (마커 있음) 센서를 대상으로 한 연구로, VQ-GAN을 활용한 생성모델 기반 표현 학습을 시도했습니다. 이는 패치 단위로 이미지를 압축하며 디스크리미네이터로 훈련하는 방식인데, 특정 센서 한 종류에만 국한되어 있고 주로 정적 특성에 초점을 맞추었습니다.\n이들에 비해 Sparsh의 차별점은 분명합니다. 첫째, Sparsh는 여러 대표적인 촉각 센서(DIGIT 계열, GelSight 계열 등)의 데이터를 모두 활용하여 하나의 공용 모델로 학습했다는 점입니다. 이로써 센서 교체나 추가에 대한 일반화 가능성을 처음으로 본격 탐구했다는 의의가 있습니다. 둘째, 최신 SSL 알고리즘들을 적극 도입하여, 픽셀 복원(MAE), 지식 증류(DINO), 잠재 예측(JEPA) 등 다양한 패러다임을 비교·분석했습니다. 이는 이전 연구들이 주로 하나의 SSL 기법만 시험한 데 반해, 어떤 자기 지도 방식이 촉각에 가장 적합한가라는 중요한 질문에 답을 제시하려 한 것입니다. 셋째, Sparsh는 단순 모델 제시를 넘어 TacBench라는 벤치마크를 구축함으로써, 촉각 표현 학습의 평가 표준을 마련했습니다. 이 벤치마크에는 물리적, 인지적, 조작 측면을 망라한 과제들이 포함되어, 앞으로 다른 연구들과 직접 성능 비교를 할 수 있는 장을 열었다는 점에서 큰 의미가 있습니다. 넷째, 데이터 규모 측면에서도 Sparsh는 이전보다 압도적으로 큰 데이터로 학습되었고, 이를 통해 모델 용량을 키우고도 과적합 없이 학습이 가능함을 보여주었습니다. 요컨대 Sparsh는 “촉각계의 Foundation Model”을 지향한 최초의 본격 시도로서, 이전 개별 연구들의 한계를 한 단계 뛰어넘은 통합적 접근이라 평가할 수 있습니다."
  },
  {
    "objectID": "posts/paper/2025-06-02-sparsh.html#장점과-한계",
    "href": "posts/paper/2025-06-02-sparsh.html#장점과-한계",
    "title": "📃Sparsh 리뷰",
    "section": "2.5 장점과 한계",
    "text": "2.5 장점과 한계\nSparsh의 성공 요인과 강점을 정리하면 다음과 같습니다:\n\n레이블 프리(self-supervised) 학습의 이점: 거대한 비지도 데이터로 학습했기에, 기존에 힘들었던 접촉 현상의 희귀 패턴까지 모델이 학습할 수 있었습니다. 예컨대 미끄럼이나 비틀림 같은 이벤트는 레이블링하기 어렵고 드물지만, SSL로 방대한 데이터를 학습하면서 이런 특징까지 잠재적으로 포착하게 되었습니다. 그 결과 일반적인 E2E 모델이 놓치기 쉬운 부분에서도 Sparsh 표현은 의미 있는 신호를 내재화하고 있어, 적은 레이블로도 높은 성능을 내는 밑바탕이 되었습니다.\n범용성 (Generalization): Sparsh 인코더 하나로 여러 센서와 여러 과제를 커버할 수 있다는 점이 입증되었습니다. 실제 TacBench 과제들을 보면, 센서 종류(DIGIT vs GelSight 등)나 출력 형태(회귀 vs 분류 vs 강화학습)가 제각각인데, 동일한 Sparsh 표현이 모두에 적용되어 좋은 결과를 냈습니다. 이는 로봇 시스템에서 센서를 교체하거나 새로운 작업에 투입할 때 기존 모델을 재사용할 수 있는 길을 열어줍니다. 예를 들어, Sparsh로 학습된 표현을 쓰면 새로운 GelSight 센서를 장착하거나 새로운 물체 잡기 과제가 주어져도, 일부터 학습하지 않고 10-shot 정도의 경미한 추가 학습만으로 적응이 가능함을 보였습니다.\n잠재 표현 학습의 효과: Sparsh (DINO)와 (IJEPA)가 MAE보다 좋았듯이, 노이즈와 센서 특수성을 걸러낸 잠재 공간 표현이 촉각 문제에 유리함이 확인되었습니다. 이는 촉각 센싱의 현실적인 문제, 즉 조명 변화, 카메라 잡음, 마커 패턴 등의 요소를 추상화하여, 핵심 접촉 정보에 집중할 수 있게 해줍니다. 그 결과 DIGIT이나 GelSight Mini같이 마커가 없는 센서에서도 전에는 풀기 어려웠던 전단력 분포 추정이나 미끄럼 감지 등이 Sparsh 표현으로는 가능해졌습니다.\n실시간성과 활용 가능성: ViT-Base 수준의 Sparsh 모델은 고성능 GPU에서 100 FPS 이상의 추론 속도를 보였습니다. 이는 로봇 제어에 충분히 투입할 만한 속도이며, 실제로 비드 미로 과제에서도 실시간 촉각 피드백으로 정책을 실행할 수 있음을 시연했습니다. 따라서 Sparsh는 실험실 프로토타입을 넘어 실시간 로봇 제어에 투입 가능한 준비된 모델이라고 할 수 있습니다.\n표준 벤치마크 제공: TacBench의 등장은 촉각 연구 커뮤니티에 크게 기여하는 점입니다. 이제 연구자들은 새로운 촉각 표현 모델이 나오면 이 6개 과제에서의 성능 지표로 비교 평가할 수 있습니다. 논문 저자들도 TacBench를 “초기 벤치마크”로 칭하며, 향후 과제들을 더 추가하거나 데이터셋을 확장함으로써 지속적으로 발전시킬 수 있다고 언급했습니다. 이런 표준화된 플랫폼은 공동 발전을 가속할 것입니다.\n\n물론 한계나 개선점도 존재합니다:\n\n실제 로봇 응용에서의 과제: Sparsh 표현을 사용한 정책도 복잡한 실제 조작 문제(비드 미로 완주 등)에서는 아직 완벽한 성과를 내지 못했습니다. 이는 꼭 Sparsh의 잘못이라기보다, 행동 복제 방식의 한계와 환경적 요인 때문입니다. 실제 로봇에서는 미세한 힘 제어의 어려움, 오차가 누적되면 복구 불가 등의 문제가 있어, 사전학습 표현이 있어도 추가적인 대책이 필요함이 드러났습니다. 이는 향후 강화학습과의 연계나 실시간 보정 기법 연구가 필요함을 시사합니다.\n데이터의 편중: 이번 연구에 사용된 공개 촉각 데이터들은 주로 불연속적인 접촉(물체를 뗐다 붙였다 하는) 위주입니다. 지속적으로 미끄러지는 상호작용이나 충격/진동 같은 역동적 데이터는 부족한 편입니다. 저자들도 전단이 풍부한 데이터를 추가로 모은다면 표현 학습이 더 좋아질 것이라고 예상했습니다. 따라서 현 단계 Sparsh는 저속, 준정적 접촉 상황에 최적화되어 있을 수 있으며, 고속 충돌이나 진동 감지 등에는 성능이 검증되지 않았습니다.\n히스토리 길드에 대한 미실험: Sparsh는 입력으로 두 프레임(또는 4프레임)을 사용하는 것으로 고정하였는데, 이 이력 길이를 늘리거나 줄이는 것에 대한 실험이 없습니다. 더 긴 시간 창의 입력이 잠재표현에 유리할지, 혹은 불필요한 잡음을 줄지에 대한 분석이 없어서, 현 설정이 최적인지는 추가 연구가 필요합니다.\n타 센서 유형에의 일반화: Sparsh는 비전 기반 촉각 센서들만 다룹니다. 압력 분포를 이미지로 변환하지 않는 다른 종류의 촉각 센서(예: 바이오태크의 전도도 변화, 힘 토크 센서 등)에 대해서는 이 접근법이 바로 통하지 않을 수 있습니다. 물론 비전 기반이라는 동일 원리에 속하는 센서들 사이에서는 일반화가 잘 되었지만, 그 외 방식까지 포괄하는 멀티모달 촉각 표현으로 확장하려면 추가 연구가 필요합니다.\n대규모 자원 필요: 장점으로 꼽았던 사전학습이란 과정은, 거꾸로 보면 상당한 계산 자원과 데이터 수집 노력이 필요한 작업입니다. 본 연구는 Meta 및 대학 협력을 통해 데이터를 모으고 수 주간 대용량 모델을 학습시켰는데, 모든 연구팀이 이를 수행하기는 어렵습니다. 다행히 학습된 가중치 공개(프로젝트 깃허브 등)가 이루어진다면 이를 활용하면 되겠지만, 향후 더 큰 모델들은 더 많은 데이터/연산이 필요할 것이기에 규모의 경제를 누릴 수 있는 기관 중심으로 연구가 진행될 우려도 있습니다.\n부분 파인튜닝 효과 미미: 본문에서는 다루지 않았지만, 부록에서 Sparsh 인코더를 부분적으로 미세조정(fine-tuning)해도 성능 향상은 크지 않고 거의 고정된 상태와 비슷했다고 합니다. 전체 파인튜닝하면 더 성능이 좋아졌지만 특히 latent기반 모델이 그렇다는 것이고, 부분(예: 마지막 몇 계층만)만 훈련하는 건 큰 의미가 없었는데요. 이는 Sparsh 표현이 이미 충분히 일반적 특성을 뽑아내 주기 때문일 수도 있고, 반대로 보면 표현에 잠재된 한계가 있어서 더 튜닝해도 극복하기 어렵다는 의미일 수도 있습니다. 이 부분은 추후 연구로 사전학습 표현을 적절히 보완/미세조정하는 기법이 개발될 여지가 있습니다."
  },
  {
    "objectID": "posts/paper/2025-06-02-sparsh.html#향후-전망-및-응용",
    "href": "posts/paper/2025-06-02-sparsh.html#향후-전망-및-응용",
    "title": "📃Sparsh 리뷰",
    "section": "2.6 향후 전망 및 응용",
    "text": "2.6 향후 전망 및 응용\nSparsh 연구는 촉각 센싱 분야의 새로운 장을 여는 출발점으로 볼 수 있습니다. 저자들도 “시작에 불과”하다는 투으로 앞으로의 발전 방향을 제시하고 있습니다. 우선, 더 큰 규모의 촉각 데이터셋 구축이 주요 과제로 언급됩니다. 현재까지는 연구팀이 자체 수집하거나 공개된 데이터를 모았지만, 전세계적으로 촉각 데이터를 모으는 노력이 결집된다면 컴퓨터 비전이나 NLP처럼 거대하고 다양한 데이터로 모델을 학습시킬 수 있을 것입니다. 특히 다양한 센서(예: 새로운 저가 촉각센서, 질감 센서 등)와 다양한 환경(산업용, 의료용 등)의 데이터를 포함시키면 Sparsh의 후속 모델은 더욱 범용적으로 진화할 것입니다.\n모델의 스케일 업(scale-up)도 기대해볼 수 있습니다. Vision Transformer 기반의 Sparsh는 Base 수준이지만, 향후 ViT-Large나 Hybrid 모델로 키운다면 더 정교한 표현을 얻을 수 있을 것입니다. 물론 데이터도 비례해서 늘어나야 하겠지만, 컴퓨터 비전 분야의 경험상 모델이 커질수록 성능이 올라가는 추세가 촉각에도 적용될 가능성이 높습니다. 이러한 대형 사전학습 촉각 모델은 진정한 의미의 촉각 Foundation Model로 자리매김하여, 사람 손의 촉각처럼 범용적인 지능을 보여줄지도 모릅니다.\n또 하나의 방향은 다중 모달 통합입니다. 앞서 비교한대로 Sparsh는 오직 촉각 모달리티만 사용했는데, 이후에는 시각+촉각 공동 학습이나 더 나아가 언어까지 결합한 멀티모달 학습으로 확장할 수 있습니다. 예를 들어, 동일한 상황에서 촉각과 시각 정보를 모두 사전학습하여 공용 임베딩을 만들면, 로봇이 보거나 만지는 것에 대해 일관된 표현 공간에서 이해할 수 있게 될 것입니다. 이는 인간이 보고 느끼는 감각을 결합하여 물체를 인지하는 방식과도 유사합니다. 실제로 이전 연구들에서 멀티모달 시도가 있던 만큼, Sparsh를 기반으로 그런 방향을 모색한다면 촉각-시각 동시 활용 작업(예: 물체 식별, 재질 탐색)에 혁신을 가져올 수 있습니다.\n강화학습 및 로봇 제어 측면에서는, 표현 학습과 정책 학습의 접목이 중요할 것입니다. Sparsh는 Behavior Cloning 예제로 비드 미로를 시도했지만, 앞으로는 모델 기반 강화학습이나 온-정책(on-policy) 방법에서 사전학습 표현을 활용하는 연구가 필요합니다. 특히 실시간 상호작용 중에 표현을 계속 적응시켜나가는 표현 강강학습 같은 개념도 생각해볼 수 있습니다. 이는 Sparsh 표현을 동적으로 업데이트하거나, 혹은 프로브 네트워크에 피드백을 주는 방식으로 이루어질 수 있을 것입니다. 궁극적으로는, 사전학습된 표현을 써서 학습 효율을 높이면서도, 실제 환경에서의 피드백으로 지속 개선하는 방향이 바람직할 것입니다.\n응용 분야를 살펴보면, Sparsh는 다양한 산업 및 연구 도메인에 파급효과를 가져올 수 있습니다. 예를 들어:\n\n정밀 조립 및 삽입 작업: 촉각 센서를 이용한 미세 조립(기계 부품 끼우기 등)에서는 힘과 위치를 섬세히 감지해야 합니다. Sparsh 표현이 있으면 작은 접촉 변화를 놓치지 않고도, 다양한 부품에 대한 범용 조립 모듈을 구축할 수 있을 것입니다.\n의료 로봇 및 의수(의족): 사람의 촉각을 대체하거나 보조하는 의수/의족에 Sparsh 같은 모델이 들어가면, 물건을 쥘 때 미끄러짐을 자동으로 감지해 힘을 조절하거나, 촉각으로 물체의 재질을 식별하여 사용자에게 피드백을 줄 수도 있습니다. 이는 촉각이 결여된 로봇이나 보조장치에 사람 비슷한 촉감 능력을 부여하는 방향입니다.\n재료 및 품질 검사: 촉각 센싱은 표면의 거칠기나 소재 특성을 파악하는 데 쓰일 수 있습니다. Sparsh 모델로 학습된 임베딩 공간은 질감이나 마찰계수 등의 정보를 내포하고 있으므로, 산업 공정에서 제품의 표면 품질 검사, 직물이나 종이의 분류 작업 등에 바로 활용될 수 있을 것입니다.\n연구 플랫폼으로서의 활용: TacBench의 공개로, 이제 연구자들은 새로운 촉각 센서만 개발되면 Sparsh에 추가 학습시키고 동일한 TacBench에서 시험해볼 수 있습니다. 이는 새로운 센서 아키텍처 평가에도 표준을 제공하여, 어떤 센서 설계가 더 다양한 촉각 정보를 잘 담아내는지 비교할 수도 있게 됩니다. 즉, Sparsh는 촉각 센서 하드웨어 연구에도 정량적 평가 도구로 기여할 수 있습니다.\n\n마지막으로, Sparsh 연구진은 논문을 통해 *“Sparsh는 범용 촉각 백본을 향한 중요한 발걸음이다”*라고 강조합니다. 그리고 *“우리의 목표는 더 큰 촉각 데이터셋과 더 큰 SSL 백본을 활용하는 것”*이라고 밝히고 있습니다. TacBench는 시작일 뿐이며, 부족한 부분(예: 데이터 다양성, 시간 창 길이 등)은 향후 보완하고, 예컨대 액체를 따를 때 질량 변화 추적 같은 더 흥미로운 촉각 과제도 추가할 수 있다고 제안합니다. 이러한 청사진을 볼 때, 앞으로 촉각 연구 커뮤니티의 협업 하에 Sparsh를 잇는 더 발전된 모델들이 나올 것으로 기대됩니다. 시각과 언어에 이은 “촉각의 GPT”가 등장하는 날도 머지않을지 모릅니다. Sparsh는 그 첫걸음을 내딛었으며, 향후 로봇에게 인간 수준의 촉각 지능을 심어주는 여정에 중요한 이정표가 될 것입니다."
  },
  {
    "objectID": "posts/paper/2023-05-07-accessibility.html",
    "href": "posts/paper/2023-05-07-accessibility.html",
    "title": "📃K-Accessibility 리뷰",
    "section": "",
    "text": "이번 포스팅은 최근 ICRA(International Conference on Robotics and Automation) 2022에서도 발표된 Accessibility-Based Clustering for Efficient Learning of Locomotion Skills 논문을 읽고 정리한 내용입니다. 강화학습으로 로봇 제어를 학습할 때 어떻게 효율적으로 initial state distribution을 탐색하도록 만들어 줄 수 있을까?라는 질문을 K-means++ 알고리즘과 유사한 K-Access라는 알고리즘을 고안하여 해결한 논문입니다. 해당 논문에서는 quadruped robot의 Recovery와 Backflip 모션 학습을 보여주었습니다."
  },
  {
    "objectID": "posts/paper/2023-05-07-accessibility.html#initial-state-distrubutions",
    "href": "posts/paper/2023-05-07-accessibility.html#initial-state-distrubutions",
    "title": "📃K-Accessibility 리뷰",
    "section": "1.1 Initial state distrubutions",
    "text": "1.1 Initial state distrubutions\n앞서 Locomotion과 Recovery를 비교하며 살펴보았는데 강화학습으로 하는 로봇 제어의 관점에서 매우 큰 차이점 하나가 더 있습니다. 바로 Initial State Distribution, 강화학습의 Robot Agent가 학습 Episode를 시작하는 맨 처음의 State들의 분포입니다. Locomotion에서는 command(컨트롤러로 조작하는 로봇의 desired velocity 혹은 간단하게 방향키 조작으로 생각할 수 있음)를 따라 움직이는 것이기 때문에 Initial State로 로봇의 standing 자세를 가지고 학습 Episode를 시작하게 됩니다. 반면 Recovery는 로봇이 넘어진 상황(자세)가 각 Episode의 Initial State가 됩니다. 넘어진 자세는 매우 다양하기 때문에 어떤 넘어진 자세는 정상상태로 회복하기가 상대적으로 쉬운 반면, 어떤 자세는 정상상태로 회복하기가 어렵기 때문에 Recovery task에서는 RL(Reinforcement Learning) agent가 Initial State Distribution을 잘 탐색하고 학습할 수 있도록 만들어주는 것이 매우 중요합니다.\n\n위 사진에서 처럼 RL Agent가 탐색해야하는 전체 State Space와 어떤 한 Initial state(혹은 Initial pose, orange dot)와 유사한 state들의 집합 영역 Effective Exploration Region(EER)을 주황색 원 영역으로 표시할 수 있습니다. 여기서 주황색 원 안의 영역의 State들은 원 중심의 하나의 Initial State를 탐색하고 학습하고 나면 어렵지 않게 강화학습 Policy가 잘 학습할 수 있는 State들이라고 볼 수 있습니다. Case 1은 전체 탐색해야 하는 State Space를 빈틈의 최소화하도록 많은 Initial state를 학습하지만 각 EER들이 많이 중복되어 학습되기 때문에 학습 효율이 매우 떨어지는 것을 알 수 있습니다. Case 2에서는 적은 Initial state로 학습해서 State Space가 잘 커버되지 않았을 뿐만 아니라 목표로하는 Target State도 잘 학습되지 않아 학습 Policy의 성능이 매우 떨어지는 것을 알 수 있습니다. Case 3는 목표로하는 Target State는 EER에 들어가서 Policy가 학습한 state라고 할 수 있지만 전체 State Space에서 커버되지 못한 state들이 있기 때문에 Corner case들(Policy가 잘 작동되지 않는 경우)이 있어 Policy의 robustness가 떨어진다고 볼 수 있습니다. 따라서 가장 이상적인 상황은 Case 4에서처럼 Target State도 EER의 범주에 들어가 있고 전체 State Space도 적절한 수의 Initial State들로 탐색되어 Policy의 Robust한 상황이라고 할 수 있습니다."
  },
  {
    "objectID": "posts/paper/2023-05-07-accessibility.html#pose-of-quadruped-robots",
    "href": "posts/paper/2023-05-07-accessibility.html#pose-of-quadruped-robots",
    "title": "📃K-Accessibility 리뷰",
    "section": "1.2 Pose of Quadruped Robots",
    "text": "1.2 Pose of Quadruped Robots\n그렇다면 Initial State, 즉 4족 보행 로봇의 자세(pose)는 어떻게 표현할 수 있을까요? 전복된 상황은 넘어져 있는 로봇의 자세로 표현할 수 있을 것 입니다.\n\n전복된 상황은 움직임이 없는 넘어진 정적(Static) 상황이라 가정하고 로봇의 상황을 다음과 같이 2가지 정보로 표현할 수 있습니다. 첫번째로는 몸체의 기울어짐을 표현하는 Projected gravity vector로 지구 중력 방향의 벡터를 (0, 0, -1)이라고 했을 때, 로봇 몸체의 프레임에 gravity vector를 projection하고 normalized한 3차원의 벡터 정보는 몸체의 기울어짐을 표현할 수 있습니다. 두번째 요소는 로봇의 각 다리에 3개씩 배치되어 관절이 되는 12개의 revolute joint(motor) angle 입니다."
  },
  {
    "objectID": "posts/paper/2023-05-07-accessibility.html#distance-between-poses",
    "href": "posts/paper/2023-05-07-accessibility.html#distance-between-poses",
    "title": "📃K-Accessibility 리뷰",
    "section": "1.3 Distance between poses",
    "text": "1.3 Distance between poses\nPose를 정의한 다음으로 살펴볼 부분은 여러 pose들 간의 관계를 어떻게 정의할 수 있을까에 대한 부분을 고민해볼 수 있습니다. 여러 pose들 간에 가깝다(비슷하다), 멀다를 파악하기 위해서는 거리(Distance)를 정의할 수 있어야 합니다. 가장 직관적으로 pose를 이루고 있는 요소들 간의 유클리디안 거리를 생각해볼 수 있습니다. 앞서 정적인 자세를 구성하는 Projected gravity vector 와 Joint angles의 유클리디안 거리를 계산해서 나온 수치를 기반으로 pose가 서로 비슷하다, 많이 다르다를 판단할 수 있을 것 입니다.\n\n하지만 그림에서의 예시를 통해 유클리디안 거리가 Non-sense하다는 것을 볼 수 있습니다. 3가지 자세, Backward Leaning(B), Forward Leaning(F), Lying(L)를 가지고 유클리디안 거리를 계산해보면 B-F의 거리가 F-L의 거리보다 큰 수치인 것을 확인해볼 수 있습니다. 하지만 로봇을 직접 제어해서 자세를 transition한다고 생각했을 때, F에서 B로의 transition이 F에서 L로의 transition이 훨씬 어렵기 때문에 단순하게 구성 요소들의 유클리디안 거리로 pose들 간의 거리를 정의하는 것은 제어적인 측면에서 말이 된다고 볼 수 없습니다."
  },
  {
    "objectID": "posts/paper/2023-05-07-accessibility.html#sampling-static-poses",
    "href": "posts/paper/2023-05-07-accessibility.html#sampling-static-poses",
    "title": "📃K-Accessibility 리뷰",
    "section": "3.1 Sampling Static Poses",
    "text": "3.1 Sampling Static Poses\n전복된 다양한 자세들을 샘플링하기 위해서 로봇의 base frame의 roll, pitch 각도를 일정 범위에서 랜덤하게 샘플링하고 12개의 joint position도 로봇의 configuration을 고려하여 upper/lower limit range에 있는 각도로 자세를 set해서 전복된 자세를 만듭니다. (이때 yaw 방향은 flat terrain에선 의미가 없기 때문에 0으로 셋팅합니다.) 샘플링된 자세로 pose를 set 했을 때 self-collision을 확인한 뒤 self-collision이 되지 않은 자세 2.4k개를 sampling 합니다.\n\n\n\n예시 사진은 해당 논문의 코드를 연구실에서 개발된 AiDIN-VIII 로봇에 적용한 모습입니다"
  },
  {
    "objectID": "posts/paper/2023-05-07-accessibility.html#estimating-accessibility-values",
    "href": "posts/paper/2023-05-07-accessibility.html#estimating-accessibility-values",
    "title": "📃K-Accessibility 리뷰",
    "section": "3.2 Estimating Accessibility Values",
    "text": "3.2 Estimating Accessibility Values\n앞 단계에서 샘플링한 2.4k개의 pose들 중 1000개만 가지고 Accessibility를 측정하게 됩니다. 우선 2.4k개 중 1000개만 가지고 진행하는 이유는 학습 이후 Policy를 테스트하기 위한 Initial state로 사용하기 위해서 1.4개의 pose는 남겨놓는 것 입니다. 앞서 유클리디안 거리가 metric으로써 좋지 않은 점을 예시를 보며 확인할 수 있었기에 논문에서는 이를 대체할 metric으로 Accessibility라는 개념을 제안합니다.\n\n위의 예시는 어떤 pose A에서 pose B로의 Accessibility를 계산하는 과정을 보여줍니다. 특정 pose에서 다른 특정 pose로 transition하는 과정을 progress라는 작은 timestep들로 쪼개고 각 timestep에 해당하는 transition angle을 PD controller로 제어하게 됩니다. pose를 구성하는 12개의 joint position(angle)은 continuous value이기 때문에 처음과 끝 pose의 angle을 안다면 linear interpolation을 할 수 있습니다. progress를 scaled timeline(0~1로 normalized)이라고 하고 쪼갠 timestep 하나를 변수 t로 본다면 매 순간의 desired transition angle 은 t \\cdot \\text{[joint angle of B]} + (1-t) \\cdot \\text{[joint angle of A]}으로 계산될 수 있습니다. 이렇게 계산된 desired transition angle을 따라가도록 PD제어를 하면서 충분히 pose B에 가까워졌는가?를 판단하게 되는데 이때의 기준은 유클리디안 거리로 계산된 joint position distance, base의 height distance, gravity vector distance이 매우 작은 오차 범위내에 들어갔는지가 됩니다. pose A에서 pose B로 충분히 가까워진 해당 시간 t를 기록하게 되는데, 3초 내에 pose B에 가까워진 상태로 평형상태에 도달하는지 체크하게 됩니다. 저자가 공개한 코드에서 확인해봤을 때 20초를 상한선으로 설정하고 1000 pose \\times 1000 pose Time 매트릭스로 평형상태에 도달한 시간을 기록합니다.\n\n앞서 유클리디안 거리로 판단하는 것이 좋지 않다고 주장할 때는 pose들이 충분히 달랐을 때 pose들 간의 관계 정의로 사용하기에 부적절함을 들어 타당하지 않다고 주장한 것이었고, 현재 pose가 transition이 되었는가를 판단하기 위한 기준으로 유클리디안 거리가 매우 작은지로 판단하는 것은 similarness를 판단하는 것이기에 motivation을 해치지 않는다고 볼 수 있습니다.\n\n\n이렇게 측정한 transition time을 가지고 State Space를 해석해본다면 pose A(s_0)에서 pose B(s_1)으로의 시간 t(s_0, s_1)이 어떤 특정 시간 t_0이하라면 두 pose 사이 관계는 High Accessibility를 가지고 있다고 볼 수 있습니다. 반면, 만약 t(s_0, s_1)이 어떤 특정 시간 t_0 초과라면 Low Accessibility 라고 할 수 있고 이때의 기준이 되는 특정 시간 t_0가 EER R의 경계를 결정합니다. 따라서 이러한 Radial Boundary를 만들기 위해 앞서 계산한 Time 매트릭스(t(s_i, s_j))를 가지고 e^{-t(s_i, s_j)}을 계산한 것을 바로 Accessibility라고 정의하게 됩니다."
  },
  {
    "objectID": "posts/paper/2023-05-07-accessibility.html#clustering",
    "href": "posts/paper/2023-05-07-accessibility.html#clustering",
    "title": "📃K-Accessibility 리뷰",
    "section": "3.3 Clustering",
    "text": "3.3 Clustering\nK-Access Algorithm\n이제 State Space 상의 pose들간의 거리를 정의하는 Accessibility 값을 구한 다음에 어떻게 하면 클러스터링을 잘할 수 있을 것인가?에 대한 고민으로 넘어가게 됩니다. 각 cluster의 centroid가 되는 pose를 정할 수 있어야 하고 몇개의 cluster 갯수가 적절할 지 판단하는 알고리즘으로 K-Access알고리즘을 제안합니다.\n\n우선 결론적으로 cluster의 갯수의 적절성을 Index 지수가 최대가 되는 값로 판단하게 되는데, 이 Index 지수는 Intra-cluster Accessibility와 Inter-cluster Accessibility, 마지막으로 Regularization Term까지 합산하여 결정하게 됩니다.\n\nIntra-cluster Accessibility: 이름에서도 볼 수 있듯이 특정 클러스터에 속해있는(=내부에 있는) sample들(각 pose를 지칭)과 centroid sample간의 Accessibility 값들 중 최소값입니다. 이 값은 Index 지수에 positive sum이 되기 때문에 의미를 해석해본다면 한 클러스터에 속해있는 sample들의 centroid로 향하는 응집력이라고 볼 수 있습니다. Intra-cluster accessibility의 차원은 1000개 샘플이 자신이 속한 클러스터 centroid와의 값을 계산하므로 1000 dimension을 가지고 있습니다.\nInter-cluster Accessibility: 클러스터들 간에 overlapping이 되지 않고 적절히 거리를 유지하며 각 EER이 전체 State Space를 커버할 수 있도록 하기 위해서 centroid sample 간의 Accessibility의 평균을 구한 값\nRegularization Term: 클러스터의 개수가 너무 커지지 않도록 하는 부분으로 Index에 negative sum이 되는 부분입니다. \\alpha 값으로 Regularization의 비중을 높일 수 있는데 논문에서는 1을 사용했습니다.\n\n\nK-means++ VS. K-Access\nK-Access 알고리즘은 기존에 ML에서 자주 사용되는 클러스터링 알고리즘인 K-means++ 알고리즘을 기반으로 만들어진 알고리즘입니다. K-means++ 알고리즘처럼 (1) Initialize the centroids (2) Assignment step (3) Update step 단계를 거치는 것은 비슷하지만 K-means++ 알고리즘에서는 (3)단계에서 평균값을 기반으로 클러스터링이 진행되는 반면 K-Access 알고리즘에서는 robustness를 보장하기 위해 Maximal neighborhood accessibility를 사용합니다.\n좀 더 자세한 알고리즘 과정을 알아보고 싶으신 분들은 아래 Pseudo Code를 확인해주세요.\n\n\nPseudo Code of K-Access\n\n\n\n\nClustering Analysis\n논문에서 사용한 Bittle 로봇 플랫폼으로 clustering을 진행했을 때 43개의 cluster가 최적의 갯수로 정해집니다. 각 클러스터에 속하는 샘플 수를 히스토그램으로 확인해보면 아래 왼쪽 그래프같이 그려지며 이중 해당 클러스터에 속한 샘플 수가 많은 순서대로 top 20개의 클러스터들 간의 inter-cluster accessibility를 Chord graph를 가지고 시각화를 해보면 오른쪽 그래프와 같이 그려집니다. Chord graph에서 강조된 부분들은 0.15 이상의 Accessibility(약 1.9초 이내의 transition time)를 가진 부분들이며 옅게 표시된 부분들은 0.05 이하의 Accessibility(약 3초 이상 transition time)를 가지는 부분들입니다.\n\n\nChord graph 시각화 방법에 대해서는 해당 논문을 기반으로 실제 제가 연구하고 있는 로봇 플랫폼을 이용하여 적용한 코드 실습은 다음 포스팅 에서 확인할 수 있습니다."
  },
  {
    "objectID": "posts/paper/2023-05-07-accessibility.html#reinforcement-learning-process",
    "href": "posts/paper/2023-05-07-accessibility.html#reinforcement-learning-process",
    "title": "📃K-Accessibility 리뷰",
    "section": "3.4 Reinforcement Learning Process",
    "text": "3.4 Reinforcement Learning Process\n마치 Machine Learning에서 Feature Engineering이 많은 주의를 요하는 작업이듯이 앞서 Initial State를 정하는 작업을 진행하고 드디어 강화학습 과정에 들어오게 되었습니다. 유명한 강화학습 알고리즘인 SAC(Soft-Actor-Critic)을 단순한 MLP 레이어로 만들어서 사용했고 Policy Network의 Input과 Ouput 설계도 관련 연구들의 convention과 크게 다르지 않기 때문에 자세한 설명은 생략하고 특징적인 부분을 살펴보겠습니다.\nReward Functions w/ RBF\n\n해당 논문에서 다른 논문들의 강화학습 MDP 설계와 다른 특징적인 부분은 보상함수 설계 부분이었습니다. 일반적으로 Reward function은 각 Reward Term들을 Linear Weigthed Sum형식을 가집니다. 하지만 해당 논문에서는 RBF(Radial Basis Function)를 사용하여 각 Reward를 weighted sum한 값으로 최종 reward를 계산한 것을 알 수 있습니다. 사실 Reward Function을 설계하는 부분은 강화학습 연구에서 Reward Engineering 이슈가 큰 것처럼, 다분히 설계자의 의도와 설명이 필요한 부분이지만 논문에서 자세히 설명이 되어 있지 않고 Main Contribution이 아니라고 생각해서 그런지 Linear sum과 비교한 실험값도 있지 않아서 RBF를 사용한 이유를 파악하기 어려웠습니다.\n\n \n\nLecture 16 - Radial Basis Functions Slides(Caltech)\n\n\n따라서 이 부분은 RBF 커널에 대해 공부하고 나서 제가 생각한 이유를 덧붙이겠습니다. RBF 커널은 기본적으로 Gaussian Distribution 모습으로 target value와 data 간의 radial한 거리 가중치를 주게 되는데, linear sum과 비교했을 때 무한 차원 영역에서 매우 멀리 떨어져 있는 data로부터 영향을 덜 받을 수 있는 장점을 가지고 있습니다. 따라서 Reward를 계산하는 데에 RBF 커널을 통해 계산한 의도는 Maximization해야 하는 Reward term들을 단순히 Linear sum하는 것보다 여러 카테고리의 Reward target 값들에 민감하게 반응할 수 있는 정도를 \\alpha값(Slide에서는 \\gamma로 표현)을 이용하여 학습의 좋은 지표가 될 수 있는 Reward space를 설계한 것으로 보입니다.\n\nReward Term에서 사용된 Symbol의 의미가 궁금하신 분들은 아래 table을 확인해주세요.\n\n\n\nSymbols of Reward Terms for DRL\n\n\n\n\nOther Tasks - Backflip\n해당 논문에서는 Recovery 뿐만 아니라 Locomotion 보다 더 다이나믹한 모션도 학습하는 것을 보여주기 위해 Backflip 학습도 K-Accessibility 알고리즘을 이용하여 학습을 진행하였습니다. (이전에 리뷰했던 WASABI 논문에서도 다이나믹한 모션 4가지 중 하나를 Backflip으로 학습 결과를 보여주었던 것과 같은 맥락으로 해당 모션 Task를 설정했다고 보시면 됩니다.)"
  },
  {
    "objectID": "posts/paper/2025-09-01-gemini-robotics.html",
    "href": "posts/paper/2025-09-01-gemini-robotics.html",
    "title": "📃Gemini Robotics 리뷰",
    "section": "",
    "text": "Paper Link\nHomepage"
  },
  {
    "objectID": "posts/paper/2025-09-01-gemini-robotics.html#서론",
    "href": "posts/paper/2025-09-01-gemini-robotics.html#서론",
    "title": "📃Gemini Robotics 리뷰",
    "section": "서론",
    "text": "서론\n최근 거대 멀티모달 모델의 발전으로 디지털 환경에서 뛰어난 범용 AI 능력이 나타났지만, 이를 물리적 로봇에 적용하는 데에는 아직 큰 도전이 있습니다. Google DeepMind의 최신 연구 “Gemini Robotics: Bringing AI into the Physical World”는 이 격차를 해소하기 위해 Gemini 2.0 기반의 새로운 로봇용 AI 모델 군을 제시합니다. 여기에는 직접 로봇을 제어하는 비전-언어-액션(VLA) 모델인 Gemini Robotics와, 공간 이해 능력을 강화한 Gemini Robotics-ER(Embodied Reasoning)이 포함됩니다. 본 리뷰에서는 해당 논문을 바탕으로 시스템 구조, 학습 방식, 멀티모달 통합, 실제 로봇 실험, 기존 시스템과의 비교, 그리고 한계점을 전문가적 관점에서 깊이 있게 분석합니다. 이는 범용 로봇을 개발하려는 최근 흐름 속에서 Gemini Robotics가 어떤 역할을 하는지 조망하는 데에 도움이 될 것입니다."
  },
  {
    "objectID": "posts/paper/2025-09-01-gemini-robotics.html#시스템-아키텍처-구성-요소와-설계-철학",
    "href": "posts/paper/2025-09-01-gemini-robotics.html#시스템-아키텍처-구성-요소와-설계-철학",
    "title": "📃Gemini Robotics 리뷰",
    "section": "시스템 아키텍처: 구성 요소와 설계 철학",
    "text": "시스템 아키텍처: 구성 요소와 설계 철학\nGemini Robotics의 시스템은 거대 트랜스포머 기반의 Vision-Language-Action 통합 아키텍처로 설계되었습니다. 이 아키텍처의 핵심 구성 요소는 다음과 같습니다:\n\n비전 인코더: 로봇의 카메라나 센서로부터 들어오는 시각 데이터를 처리하여 객체 인식과 위치 파악, 주변 환경의 맥락 정보를 추출합니다. 동적 환경에서도 물체의 상태와 위치를 인지하고 2D/3D 특징을 파악하는 역할을 합니다.\n언어 인코더: 사람의 자연어 명령을 해석하는 언어 모델입니다. 사용자의 일상적이고 모호할 수 있는 지시를 내부 표현으로 변환하여, 이후 로봇 동작으로 옮길 수 있게 합니다. Gemini 2.0에서 물려받은 강력한 언어 이해력 덕분에 일상어로 phrased 된 지시나 불완전한 정보도 맥락에 따라 이해할 수 있습니다.\n액션 디코더: 앞서 통합된 비전+언어 이해를 토대로, 로봇의 구체적인 행동 시퀀스를 출력합니다. 예를 들어 팔 이동, 그립 동작, 내비게이션 등의 명령을 생성하여 로봇이 실제 물체를 잡거나 이동시키는 등 행동을 수행하게 합니다. Gemini Robotics에서는 이 물리적 행동 자체가 하나의 출력 모달리티로 추가되어, 모델이 자연어 답변이나 이미지 생성 대신 곧바로 로봇 제어 명령을 생성하도록 설계되었습니다.\n\n이러한 모듈들은 모두 하나의 트랜스포머 안에서 멀티모달 통합이 이루어지도록 구성되어 있습니다. 즉, 카메라 영상과 텍스트 명령이 공통의 표현 공간에서 결합되고, 그에 따라 로봇 동작 토큰이 생성됩니다. 이 설계 철학은 “행동까지 이해하는 AI”를 지향합니다. 단순히 시각 장면을 인지하고 언어로 설명하는 데 그치지 않고, 상황을 이해한 다음 물리적으로 대응하도록 하는 것입니다. Google DeepMind 로보틱스팀은 로봇용 AI에 필요한 세 가지 핵심 자질로 범용성, 상황 대응성, 섬세한 조작 능력을 강조하는데, Gemini Robotics는 이 세 축 모두에서 이전보다 비약적으로 향상된 성능을 보이며 진정한 범용 로봇에 한 걸음 다가선 결과라고 합니다.\n특히 설계 철학 측면에서, Gemini Robotics는 범용성(generality)을 최우선 목표로 합니다. 하나의 거대한 모델이 여러 환경과 작업에 두루 통할 수 있도록, 특정 작업에 특화된 모듈들을 따로 두지 않고도 학습된 지식을 새로운 상황에 적용할 수 있게 만들었습니다. 또한 대화형 상호작용성(interactivity)을 추구하여, 사람의 지시에 실시간으로 대응하고 환경 변화에 재빠르게 적응합니다. 마지막으로 섬세한 조작성(dexterity)을 갖춰 사람 손과 비슷한 수준으로 정교한 물체 조작이 가능하도록 했습니다. 이러한 원칙 하에, Gemini Robotics는 듀얼 암 로봇 플랫폼 ALOHA 2를 주로 활용해 학습되었지만, 애초부터 다양한 로봇 형태로 손쉽게 적응할 수 있게 설계되었습니다. 실제로 모델의 출력 인터페이스나 입력 형태를 범용적으로 만들어, 다양한 매니퓰레이터(예: 실험실에서 널리 쓰이는 Franka 암, 또는 인간형 로봇 Apptronik Apollo의 팔 등)에 대해 추가 학습만으로 적용될 수 있었습니다. 이는 로봇의 역학 모델이나 관절 구성이 달라도, Gemini의 내부 표현만 잘 활용하면 동일한 고차원 정책을 이식할 수 있음을 보여줍니다.\n요약하면, Gemini Robotics의 아키텍처는 시각-언어 인지 능력과 로봇 제어 능력의 유기적 결합이 핵심입니다. 거대 멀티모달 기반모델(foundation model)인 Gemini 2.0의 세계 지식을 물려받아, 물리 세계의 다양한 작업을 하나의 모델이 “바로 실행”해내도록 설계된 점이 혁신적입니다. 다음으로 이 모델이 어떻게 학습되었는지 살펴보겠습니다."
  },
  {
    "objectID": "posts/paper/2025-09-01-gemini-robotics.html#학습-방식-단계별-훈련과-데이터-전략",
    "href": "posts/paper/2025-09-01-gemini-robotics.html#학습-방식-단계별-훈련과-데이터-전략",
    "title": "📃Gemini Robotics 리뷰",
    "section": "학습 방식: 단계별 훈련과 데이터 전략",
    "text": "학습 방식: 단계별 훈련과 데이터 전략\nGemini Robotics의 학습 파이프라인은 크게 사전 학습(pre-training)과 로봇 특화 미세조정(fine-tuning)의 2단계로 구성되며, 각 단계에서 다종다양한 데이터 소스와 학습 기법이 활용되었습니다. 전체적인 목표는 모델이 일반적인 지식을 먼저 습득하고, 이후 로봇 제어 맥락에 특화되도록 하는 것입니다.\n\n데이터 수집 및 전처리: 첫 단계로, DeepMind 팀은 방대한 멀티모달 데이터를 수집했습니다. 시뮬레이션 환경과 실제 로봇 실험에서 얻은 다양한 영상, 깊이 센서, 로봇 상태 데이터와 그에 대응하는 자연어 설명 및 명령을 모았습니다. 예컨대, 로봇 팔로 물체를 집는 동작이 찍힌 비디오 클립에 “초록색 공을 집어서 상자에 넣어라” 같은 텍스트 설명을 붙이는 식입니다. 시뮬레이션으로부터 합성 데이터도 다량 생성했는데, 이는 현실에서는 수집 어려운 상황(조명 변화, 희귀 사물 배치 등)을 다양하게 커버하여 데이터 다양성을 확보하기 위함입니다. 이렇게 구축된 광범위한 멀티모달 코퍼스는 모델이 물체 인식, 경로 계획, 조작 동작 등에 대한 기본 개념을 배우는 토대가 됩니다.\n사전 학습 (Pre-training): 수집된 멀티모달 데이터를 활용하여 대규모 사전 학습이 이뤄졌습니다. 이 단계에서는 주로 자기지도학습 및 대규모 지도학습을 통해, 모델이 시각-언어 패턴과 행동 사이의 일반적인 상관관계를 학습합니다. 이를테면 다양한 이미지와 그 설명을 보면서 물체와 단어의 연결을 배우고, 간단한 시뮬레이션 작업들을 통해 “어떤 상황에서 어떤 행동이 유효한지” 감을 익히게 합니다. 이 과정은 기존 Gemini 2.0 모델의 파라미터를 초기값으로 활용하여 진행되었을 가능성이 높은데, 덕분에 모델은 기본적인 언어이해와 시각인지 능력을 이미 갖춘 상태에서 출발합니다. 사전 학습의 목표는 범용적 표현 학습으로, 새로운 환경이나 과제가 주어져도 일일이 처음부터 배우지 않아도 되도록 강인한 특성 표현을 모델 안에 심어주는 것입니다. 이는 일종의 모델의 세계지식 습득 단계로 볼 수 있으며, 별도의 로봇 제어 훈련 없이도 물체 종류나 물리 개념 등에 대한 이해도를 높이는 효과가 있습니다.\n미세조정 (Fine-tuning) 및 강화학습: 다음으로, 이렇게 사전훈련된 모델을 실제 로봇 제어 작업들에 맞게 미세조정합니다. 이 단계에서는 실제 로봇 팔이 테이블에서 물건을 집어 옮기는 등 현실 세계의 조작 데이터를 모델이 직접 모사하고 학습합니다. 수십~수백 가지에 이르는 다양한 작업(단순 물체 포착부터 도구 사용, 복잡한 다단계 조작까지)을 모델에 경험시켜, 특정 작업에 대한 성능과 안정성을 끌어올립니다. 미세조정에는 지도학습과 강화학습이 조합되어 사용되었습니다. 우선 인간이 시범을 보인 데이터에 대해 교사학습(모방학습) 방식으로 모델이 올바른 행동 시퀀스를 출력하도록 학습시키고, 동시에 환경 상호작용을 통한 강화학습(RL)을 도입하여 모델 스스로 행동을 실행하고 성공/실패 보상을 받으면서 정책을 개선하게 했습니다. 예를 들어, 모델이 문을 여는 행동을 연습할 때, 처음에는 열리는 확률이 낮더라도 시도하고 피드백을 받아 점진적으로 보상 극대화 정책을 배우는 식입니다. 강화학습을 접목함으로써, 단순 주입된 데이터에 의존하는 것을 넘어 스스로 시행착오를 겪으며 더욱 견고한 제어 능력을 얻습니다【10†L220-L227】. 이는 모델이 새로운 환경으로 일반화하는 능력도 향상시키는데, 미세조정 과정에서 다양한 변주 상황을 겪으며 로봇 행동의 민감도를 조절하는 법을 배우기 때문입니다【10†L220-L227】.\n체화된 추론(Embodied Reasoning) 확장: 논문에서는 기본 모델 외에 Gemini Robotics-ER이라는 별도의 확장 모델도 소개합니다. 이는 Gemini 2.0의 멀티모달 추론 능력에 공간적/물리적 추론 기능을 강화한 버전으로, 별도 학습 절차를 거쳤습니다. 구체적으로, Gemini-ER 모델은 물체 감지, 3차원 공간이해, 물체 간 관계 파악 등 로봇에게 필요한 시각지능 태스크들에 대해 추가 훈련되어, Gemini의 언어/코드 추론 능력을 물리세계에 연결해 주는 역할을 합니다. 이 모델은 단독으로 “어떻게 움직일까”를 생각해내는 두뇌처럼 동작할 수 있는데, 예를 들어 이미지로 컵을 보여주면 “손잡이가 옆에 있으니 저 부분을 잡아야겠다”는 식으로 적절한 그립 지점이나 움직임 경로를 자체적으로 계획할 수 있습니다. Gemini-ER은 포인팅, 2D/3D 물체 탐지, 경로 예측, 그립 지점 산출, 멀티뷰 정합 등의 능력을 기존 Gemini보다 크게 향상시켰으며, 이 결과를 Gemini Robotics 본 모델이 활용하여 실제 로봇 제어의 성공률을 높이도록 했습니다. 저자들에 따르면, 이러한 두 단계 모델 구성은 복잡한 장면에서 실시간 인식과 계획을 가능하게 하여 최종 행동 명령의 신뢰도를 높였다고 합니다.\n시뮬레이션에서 실제로 (Sim-to-Real Transfer): Gemini Robotics 학습에서 주목할 점은 시뮬레이션 데이터의 적극적 활용과 현실 도메인 적응입니다. 앞서 언급한 것처럼 합성 데이터로 사전 학습을 하고, 이후 현실 데이터를 섞어 미세조정함으로써, 모델이 시뮬레이터에서 학습한 지식을 실제 로봇에도 이식할 수 있게 했습니다. 또한 학습 과정에서 도메인 랜덤라이제이션(조명, 텍스처, 물리 파라미터의 다양화)을 적용해 현실 갭을 줄였고, 중요한 경우 실제 로봇으로 검증 및 재학습을 수행함으로써 Sim2Real 전이를 달성했습니다. 그 결과 최종 모델은 현실에서 보지 못한 환경이나 물체에도 강인한 일반화 성능을 보였다고 보고됩니다. 예컨대, 훈련 중 보지 못했던 새로운 가구 배치나 조명 조건에서도 로봇이 임무를 수행하는 데 성공하는 비율이 높았습니다.\n휴먼 피드백 및 상호작용 학습: 해당 논문에서 인간 피드백(RLHF)이 직접 언급되진 않았으나, Gemini Robotics는 인간의 데모를 통한 학습과 인컨텍스트 러닝 등의 방식으로 인간 지식을 흡수합니다. 모델이 기본적으로 거대 언어모델의 속성을 가지므로, 사람의 지시를 몇 개 예시로 보여주면 거기에 맞춰 작업 방법을 학습하는 능력이 보고되었습니다. 특히 Gemini-ER의 경우, 코드 생성 능력까지 활용하여 기존에 학습되지 않은 새로운 작업도 몇 차례의 시범만 보고 맥락적으로 파악해 해결책(예: 새로운 경로 계획 알고리즘 코드) 을 제시할 수 있다고 합니다. 이는 모델이 내재적으로 인간의 피드백 패턴을 반영하여 추가 학습 없이도 적응력을 보이는 흥미로운 현상입니다. 다만, 논문에서 별도의 보상 모델이나 선호도 학습 같은 RLHF 기법을 적용했다는 언급은 없으므로, 이러한 휴먼 피드백 활용은 주로 데모 시퀀스 제공이나 프롬프트 설계 수준에서 이루어진 것으로 보입니다.\n\n요약하면, Gemini Robotics의 학습은 “광범위한 멀티모달 사전학습 → 로봇 작업별 미세조정 → (필요시) 추가적 상호학습”의 단계로 진행되었습니다. 이러한 단계별 훈련 전략 덕분에, 모델은 방대한 지식을 흡수함과 동시에 로봇 제어에 특화된 세밀한 조정 능력을 갖추게 되었습니다. 또한 시뮬레이션과 현실 데이터를 혼합함으로써 가상-현실 간 갭을 극복하였고, 강화학습을 접목해 실제 환경 적응력을 높였습니다. 결과적으로 Gemini Robotics는 대량의 데이터에 의존해 하나의 작업만 익히는 기존 방식에서 벗어나, 적은 추가 데이터만으로도 새로운 작업이나 로봇에 빠르게 적응할 수 있는 범용 로봇 모델의 가능성을 보여주었습니다."
  },
  {
    "objectID": "posts/paper/2025-09-01-gemini-robotics.html#멀티모달-통합-비전언어제어의-융합",
    "href": "posts/paper/2025-09-01-gemini-robotics.html#멀티모달-통합-비전언어제어의-융합",
    "title": "📃Gemini Robotics 리뷰",
    "section": "멀티모달 통합: 비전·언어·제어의 융합",
    "text": "멀티모달 통합: 비전·언어·제어의 융합\nGemini Robotics의 큰 특징 중 하나는 멀티모달 AI(시각, 언어, 행동)의 긴밀한 통합입니다. 과거에는 로봇 시스템에서 컴퓨터비전 모듈이 환경을 인식하고 언어처리 모듈이 명령을 해석한 뒤, 이를 제어 알고리즘이 받아 실행하는 파이프라인 구조가 일반적이었습니다. 반면 Gemini Robotics는 이러한 단계를 단일 거대 모델 안에서 모두 다룹니다. 이 접근의 장점과 한계를 살펴보겠습니다.\n통합 방식과 장점: Gemini Robotics에서는 앞서 설명한 비전 인코더와 언어 인코더의 출력이 공동의 임베딩 공간에서 결합됩니다. 예를 들어, 사용자가 “오른쪽에 있는 빨간 공을 집어서 바구니에 넣어”라고 말하면, 언어 인코더는 이를 목표 동작으로 해석하고, 비전 인코더는 카메라 영상에서 “오른쪽에 있는 빨간 공”의 위치를 찾아 특성으로 추출합니다. 이 정보들은 트랜스포머의 어텐션 메커니즘을 통해 상호작용하며, 결국 액션 디코더는 적절한 로봇 팔 움직임 시퀀스를 토큰 형태로 산출합니다. 이러한 비전-언어-액션 결합은 모델이 풍부한 상황 이해력을 갖추게 합니다. 언어를 통해 추상적 개념이나 목표 의도를 파악하고, 시각을 통해 구체적 실시간 상황을 파악하여, 둘을 맞물린 채 행동을 결정하므로, 맥락적이고 유연한 행동 결정이 가능합니다. 실제 실험에서 Gemini Robotics는 훈련 중 보지 못한 새로운 지시도 대화 수준으로 이해하여 수행했는데, 이는 언어 통합 덕분입니다. 예를 들어 연구진이 “농구공을 슬램덩크 해봐”라고 장난 섞인 지시를 내렸을 때, 로봇은 농구대를 처음 보았음에도 불구하고 이 말을 이해하여 공을 쥐고 링에 넣는 동작을 첫 시도에 성공했습니다. 이전에 농구와 관련된 어떤 시연도 본 적 없음에도, “슬램덩크”라는 개념을 언어로 이해하고 시각적으로 상황을 판단해 실행한 것으로, 멀티모달 통합이 가져온 범용 추론+행동 능력을 보여주는 사례입니다.\n또한 멀티모달 통합은 로봇의 상호작용성을 높여줍니다. 사람과 대화하듯 명령을 주고받을 수 있기 때문에, 로봇에게 여러 단계를 연달아 설명하거나, 중간에 수정 지시를 내리는 것도 가능합니다. Gemini Robotics는 Gemini 2.0의 뛰어난 자연어 처리 능력을 이어받아 일상 언어, 여러 언어로 지시해도 이해하고 반응할 수 있고, 작업 도중 사람이 개입해 “잠깐 그건 내려놔”처럼 말을 걸면 바로 플랜을 재조정하여 새로운 상황에 적응합니다. 이러한 대화형 로봇의 모습은 멀티모달 통합 없이는 어려운 목표였습니다. 특히 Vision+Language를 함께 쓰면, 웹으로부터 학습한 거대한 지식을 로봇이 활용할 수 있다는 이점도 있습니다. 예컨대, RT-2와 같은 선행 연구에서 거대 VLM으로 학습된 로봇 모델이 “테이블에서 떨어지기 직전인 가방을 집어라” 같은 명령을 이해하고 수행했는데, 이처럼 시각적 상황(떨어질 것 같은 가방)과 언어적 개념(수학 문제 같은 추론까지)을 결합함으로써, 로봇이 훈련 데이터에 없던 새로운 개념의 작업도 해낼 수 있는 추론적 일반화가 나타납니다. Gemini Robotics 역시 거대 언어-시각 모델의 지식을 활용하여, 단순 반복 학습으로 얻은 능력이 아닌 “웹 지식+현실 감각”의 조합으로 새로운 상황을 풀어내는 에머전트 스킬(emergent skill)을 보여줍니다.\n멀티모달 통합은 또한 다중 센서 정보의 융합을 가능케 합니다. Gemini Robotics-ER의 구조를 보면, 카메라 영상 외에도 깊이 카메라, LiDAR 같은 다양한 센서 데이터를 함께 처리하도록 되어 있습니다. 로봇의 자기 자세나 관절 상태 같은 프로프리오셉션 정보도 토큰화하여 입력되는 것으로 추정됩니다(유사한 로봇 모델인 NVIDIA의 GR00T에서는 관절각 등 로봇 상태를 토큰으로 넣어 Transformer에 결합시켰다고 보고됨). 이를 통해 로봇은 시각+언어+자기 상태를 종합적으로 고려한 상황 인식을 합니다. 가령, 물체를 집으려 할 때 단순히 카메라에 보이는 이미지뿐 아니라, 손끝 힘 센서나 관절 각도 정보를 함께 활용해 미끄러짐을 감지하거나 충돌을 예방하는 의사결정을 내릴 수 있습니다. 이처럼 다양한 modality를 단일 모델에 통합하면, 개별 모달리티의 약점을 서로 보완하고 고차원적 판단을 할 수 있게 됩니다. 인간도 눈으로 보고 손의 감각을 느끼며 두뇌로 판단하듯, Gemini Robotics는 트랜스포머 내부에 이러한 멀티센서 융합 회로를 갖춘 셈입니다.\n한계와 도전: 물론 이러한 밀착된 멀티모달 통합에는 몇 가지 단점이나 한계도 존재합니다. 첫째, 모델의 복잡도와 자원 요구량이 매우 크다는 점입니다. 시각, 언어, 행동까지 하나로 합친 거대 모델을 학습하려면 막대한 데이터와 연산량이 필요하며, 실행 시에도 메모리와 연산 부담이 큽니다. 실제로 Gemini Robotics의 풀사이즈 모델은 로봇 자체에 탑재하기 힘들 정도로 크고 느렸던 것으로 보이며, 이를 해결하기 위해 DeepMind는 경량화된 Gemini Robotics On-Device 버전을 별도로 개발했습니다. On-Device 모델은 파라미터를 줄이고 최적화를 거쳐 로봇 내 장치에서도 실시간 동작할 수 있을 만큼 경량화한 것으로, 인터넷 연결 없이 로컬 추론이 가능하고 지연을 최소화한 것이 특징입니다. 이는 멀티모달 대형 모델을 실제 현장에 투입하려면 경량화 및 최적화가 필수임을 보여주는 사례입니다. 둘째, 디버깅과 해석의 어려움입니다. 비전/언어/제어 기능이 분리 모듈이 아니라 하나로 합쳐져 있으므로, 만약 로봇이 잘못된 행동을 했을 때 그 원인이 인지 오류인지, 명령 이해 오류인지, 제어 오류인지를 구분하기가 어렵습니다. 블랙박스 거대 모델 내부에서 모든 처리가 이루어지기 때문에, 로봇 공학자가 특정 오작동을 수정하려 해도 내부 가중치를 건드리는 수밖에 없고, 이는 곧 모델 신뢰성 문제와 연결됩니다. 이러한 이유로 DeepMind 팀도 Gemini 모델에 별도의 안전 장치와 품질 검증 루틴을 추가했습니다. 예를 들어 외부 안전 컨트롤러를 병렬로 두어 충돌이나 과도한 힘 작용을 즉시 차단하게 하고, Asimov의 로봇 3원칙 등에 영감을 얻은 규칙 기반 프레임워크로 모델의 행동 제약 조건을 설계했으며, 로봇공학 도메인 전문가들과 함께 면밀한 평가를 수행했다고 합니다. 이는 현재의 멀티모달 거대 모델이 완벽히 신뢰할 수준은 아니며, 추가적인 안전장치와 인간의 감시가 필요함을 의미합니다.\n또 다른 한계로는, 연속 제어의 안정성 문제가 있습니다. 일반적인 로봇 제어 알고리즘은 제어이론에 기반하여 안정도 보장을 하거나, 적어도 물리적으로 진동이나 발산이 없도록 설계됩니다. 그러나 거대 신경망 모델은 이러한 보장이 없고, 학습 데이터 분포를 벗어난 입력이 들어오면 예기치 못한 출력을 낼 수 있습니다. 예컨대, Gemini Robotics가 학습하지 않은 극단 상황(갑작스런 센서 오류나 비정형적 물체)에 직면하면 엉뚱한 동작을 산출할 위험이 있습니다. 논문에서도 이러한 안정성을 위해 Gemini-ER 모델이 자체적으로 현재 액션의 안전 여부를 판단하여 위험하면 다른 응답을 생성하도록 하는 메커니즘을 포함했다고 언급합니다. 그럼에도 불구하고 완전한 안전을 위해서는 향후 모델의 출력에 대한 검증 알고리즘이나, 모델이 신뢰도 판단을 할 수 있는 자체 평가 모듈 등이 추가로 필요할 것입니다.\n확장성과 범용성의 과제: Gemini Robotics는 두 팔을 가진 고정식 로봇(ALOHA 2)으로 주로 개발되었고, 이후 실험적으로 사람 형태의 Apollo 로봇까지 적용되었지만, 여전히 검증되지 않은 영역들이 남아 있습니다. 예를 들어, 다족보행 로봇이나 드론처럼 동적으로 균형을 잡아야 하는 시스템에 이 모델을 적용할 수 있을지, 적용한다면 별도 모듈(예: 보행 제어기)과 어떻게 통합할지 등은 향후 연구과제입니다. 다행히 현재 Gemini Robotics-ER 모델은 Boston Dynamics, Agility Robotics 등 여러 로봇 업체 파트너들과 시험 중이라고 하니, 추후 다양한 로봇 플랫폼에의 확장 가능성에 대한 결과가 나올 것으로 기대됩니다. 또한 작업 범위의 확장도 과제입니다. 논문과 데모에서 다룬 작업들은 주로 실내 조작 업무(요리 보조, 물건 정리, 장난감 게임 등)였는데, 이를 산업 현장(예: 제조 조립)이나 옥외 환경으로 넓힐 때 성능이 유지될지 미지수입니다. 대규모 물류창고나 복잡한 공장 환경에서는 여전히 전문 특화 로봇이 유리할 수 있고, Gemini와 같은 범용 모델은 섬세한 튜닝이 필요할 것입니다. 마지막으로, 데이터 종속성의 문제가 남습니다. Gemini Robotics는 기존 로봇보다 새로운 작업을 훨씬 적은 데이터로 배울 수 있다지만, 그 “기존에 학습된 방대한 능력” 자체를 얻기까지 들어간 데이터는 천문학적입니다. 일반 연구자나 중소 연구팀이 이와 같은 모델을 처음부터 학습시키기는 현실적으로 어려우므로, 향후에는 이러한 거대 모델을 어떻게 공개하고 활용할지 (예: API 형태로 사용, 또는 지식 증류를 통한 축약 모델 제공 등) 생태계 전략도 중요해 보입니다.\n요약하자면, Gemini Robotics의 멀티모달 통합은 로봇의 이해와 행동 능력을 비약적으로 향상시켰지만, 동시에 모델 크기와 복잡성, 안정성, 신뢰성 측면의 새로운 도전을 가져옵니다. 이러한 한계들을 인지하고 보완해나가는 것이 다음 단계 연구의 방향일 것입니다."
  },
  {
    "objectID": "posts/paper/2025-09-01-gemini-robotics.html#실제-로봇-응용-사례-시연-및-실험-결과-분석",
    "href": "posts/paper/2025-09-01-gemini-robotics.html#실제-로봇-응용-사례-시연-및-실험-결과-분석",
    "title": "📃Gemini Robotics 리뷰",
    "section": "실제 로봇 응용 사례: 시연 및 실험 결과 분석",
    "text": "실제 로봇 응용 사례: 시연 및 실험 결과 분석\n논문과 발표에서 공개된 Gemini Robotics의 데모 시연과 실험 결과는 이 모델의 능력을 잘 보여줍니다. 이 절에서는 몇 가지 대표적인 응용 사례와 그 의미를 살펴보겠습니다.\n1. 범용 조작 작업 데모: 연구팀은 Gemini Robotics의 범용성을 검증하기 위해, 훈련 시에 없던 새로운 작업들을 즉석에서 로봇에게 시켰습니다. 예를 들어, 앞서 언급한 농구공 슬램덩크 시연이나, 신발 속에 펜 넣기 같은 창의적인 지시가 그것입니다. ALOHA 2 로봇은 처음 대하는 사물(농구 세트, 연구원의 신발 등)임에도 불구하고, 사람의 명령어만 듣고 상황을 파악해 행동을 성공적으로 수행했습니다. 이는 Gemini Robotics의 내부 지식과 추론 능력이 얼마나 풍부한지 보여줍니다. 로봇은 “신발에 펜 넣기”라는 말을 듣고 신발 개념, 펜의 크기와 들어가는 방법 등을 추론했고, 이내 로봇팔로 신발을 집고 공간을 확보한 뒤 펜을 집어넣는 동작을 매우 부드럽게 실행했습니다. 이러한 한 번에 새로운 작업 해결 능력은 기존 특화 로봇들과 구별되는 놀라운 점입니다 (대부분의 로봇은 새로운 작업을 수행하려면 별도 프로그래밍이나 학습이 필요했습니다).\n2. 다양한 물체 조작 및 섬세한 작업: Gemini Robotics의 섬세한 조작 능력은 여러 시연을 통해 부각되었습니다. 예를 들어, 종이 접기(origami) 데모에서 로봇은 복잡한 종이접기 동작을 순서대로 따라 하여 여우 모양을 접어냈습니다. 사람 손처럼 정교하게 종이의 모서리를 잡고 접는 동작을 오류 없이 수행한 것은, 모델이 이러한 장기간의 섬세한 작업 순서까지 이해하고 실행했음을 의미합니다. 또 다른 데모로, 짐 싸기/정리 작업이 소개되었습니다: 주방에서 로봇이 도시락 통에 물건을 차곡차곡 채워 넣는 장면이나, 여러 가지 물품을 상자에 정렬해서 포장하는 장면 등입니다. 로봇은 각 물체의 크기와 무게를 고려해 어떤 순서로 넣어야 공간이 효율적으로 쓰일지 판단하고, 물건들을 부드럽고 정확하게 다뤘습니다. 심지어 깨지기 쉬운 물건도 안정적으로 옮기는 등 힘 조절까지 능숙했습니다. 이러한 결과는 모델이 단순히 “잡고 놓기” 수준을 넘어, 힘/경로 최적화까지 내재화했음을 보여줍니다. 연구진은 “정교한 종이 접기부터 물건 꾸려 담기까지, 세밀한 물리 조작을 해내는 능력이 Gemini Robotics의 혁신”이라고 강조합니다.\n3. 복잡한 다단계 작업: Gemini Robotics는 장기 계획(long-horizon)이 필요한 작업에도 도전했습니다. 예컨대 샐러드 준비 시연에서, 로봇은 냉장고에서 야채를 꺼내 씻고, 도마 위에서 썰고, 그릇에 담는 일련의 과정을 수행했습니다. 이 과정에는 여러 하위 작업(열기-잡기-이동-도구사용-담기 등)이 포함되며, 각 단계에서 상황에 따른 판단이 필요합니다. 로봇은 사람의 상위 지시 (“샐러드 좀 준비해줘”)만 받고도, 스스로 다음에 무엇을 해야 할지 결정하며 순서를 진행했습니다. 이때 만약 중간에 예상 밖 상황이 발생하면 (예: 도마 위 재료가 미끄러짐) 실시간으로 재계획하여 정상 진행했습니다. 이러한 반복없는 멀티스텝 작업 완수율은 Gemini Robotics의 큰 성과로, 기술 보고서에 따르면 긴 계획을 요하는 작업에서 Gemini Robotics는 기존 모델 대비 성공률을 크게 향상시켰습니다. 특히 Gemini-ER 모델과 결합된 경우, 환경에 대한 상태 추론을 병행하여, 각 단계마다 최적의 행동을 결정하는 능력이 뛰어났다고 합니다. 이는 과거의 “계획-실행” 이분화된 시스템과 달리, 계획과 실행을 한 모델이 연속적으로 해나가면서 가능한 적응형 전략으로 볼 수 있습니다.\n4. 로봇 간 범용성 실험: 앞서 언급했듯 Gemini Robotics는 한 종류 로봇(ALOHA 2)으로 주로 학습되었지만, 새로운 로봇으로의 전이가 시험되었습니다. 연구팀은 Franka Emika의 FR3 암(일반 연구용 7자유도 로봇팔)과 Apptronik의 Apollo 휴머노이드로 모델을 이식하여 테스트했습니다. 이때 추가로 사용된 데이터는 많지 않았는데, 불과 50~100개의 시연 혹은 몇 시간 분량의 추가 학습으로도 각 로봇에서 성능이 크게 향상됐습니다. 예를 들어, Franka 암으로는 처음 보는 새로운 물체들을 다루는 실험이 진행됐는데, ALOHA로 학습된 모델을 약간 미세조정하니 곧바로 이질적인 물체와 장면에서도 명령 수행을 해냈습니다. 옷을 개키거나 드릴로 나사를 조이는 등 산업용 조립 작업까지 성공적으로 수행하여, 정밀 작업 능력이 특정 하드웨어에 국한되지 않음을 증명했습니다. Apollo 휴머노이드에 대한 적용도 흥미로운데, 이 로봇은 이동형 플랫폼 위에 인간 팔 모양의 매니퓰레이터를 가진 형태입니다. Gemini Robotics 모델을 Apollo의 팔에 맞게 조금 튜닝한 결과, 사람과 유사한 높이와 범위에서 다양한 객체를 조작하고, 이동하면서 물체 운반 등의 과제를 수행했습니다. 특히 Apollo에게 이전에 없던 물건을 주고 “이걸 들어서 옆 테이블에 올려놔” 같은 지시를 했을 때, 모델이 당황하지 않고 주변 환경을 스캔하여 안전하게 임무를 완수한 사례가 보고되었습니다. 이는 Gemini Robotics의 모델 내 지식이 구체적인 로봇 구조에 상당히 중립적(embodiment-agnostic)임을 보여줍니다. 결국 이러한 실험들은 하나의 거대 모델이 여러 로봇의 뇌로 활용될 수 있다는 “로봇계의 GPT” 같은 비전을 뒷받침한다고 볼 수 있습니다.\n5. 성능 지표 및 비교: 논문에서는 다양한 벤치마크 평가 결과도 제시합니다. 그 중 눈에 띄는 것은 일반화 성능 종합 벤치마크에서 Gemini Robotics가 다른 최신 VLA(비전-언어-액션) 모델들 대비 2배 이상의 성공률 향상을 보였다는 점입니다. 이 벤치마크는 새로운 물체, 새로운 지시어, 새로운 환경 조합 등에 로봇이 얼마나 잘 대응하는지를 종합 측정한 것인데, Gemini가 탁월한 점수를 기록했습니다. 이는 PaLM-E, RT-2 등 이전 세대 모델들이 한계가 있었던 보지 못한 조합에 대한 대응에서 큰 진전을 이뤘음을 뜻합니다. 또한 Dexterity(섬세 조작) 부문 평가에서도, 작은 물체를 다루거나 정밀한 힘 조절이 필요한 작업에서 SOTA 대비 월등한 성공률을 보였습니다. 한편, Gemini Robotics-ER 모델 자체의 성능도 흥미로운데, 2D/3D 물체 탐지, 포인팅 정확도 등 순수 인지능력 관련 태스크들에서 기본 Gemini 2.0 대비 크게 향상된 정답률을 보고합니다. 예컨대, “이 장면에서 파란 머그잔의 손잡이를 가리켜라” 같은 질문에 Gemini-ER은 정확히 머그잔 손잡이 위치를 픽셀 단위로 지목해내는 식입니다. 이러한 인지 향상이 뒷받침되었기 때문에 실제 로봇 행동의 성공률도 높아진 것으로 해석됩니다. 마지막으로, 실험 결과 부분에서 강조되는 것은 안전성 평가입니다. 모델에게 일부러 위험한 지시(“사람을 칼로 찔러” 등)를 내리거나 충돌 위험이 있는 상황을 제시하여 모델의 반응을 본 결과, Gemini Robotics는 훈련된 안전 규칙에 따라 이를 거부하거나 우회하는 답변/행동을 생성했습니다. 물론 이러한 안전 테스트는 초기 단계지만, 최소한 명백한 위험 행동은 억제하도록 설계되었음을 확인시켜줍니다.\n전체적으로, 실제 응용 및 실험을 통해 드러난 Gemini Robotics의 능력은 다음과 같이 요약할 수 있습니다:\n\n새로운 과제에 대한 즉각 대응: 훈련되지 않은 임기응변 작업도 높은 성공률로 수행 (예: 슬램덩크, 새로운 물건 다루기 등).\n복잡하고 긴 조작 시퀀스 완수: 요리, 접기, 조립 등 여러 단계를 거치는 작업을 계획부터 실행까지 일관되게 성공.\n정밀한 조작과 힘 제어: 종이 접기, 섬세한 물건 포장, 옷 개기 등 인간 수준의 섬세함 요구 작업 가능.\n다양한 로봇에의 일반화: 하나의 모델로 여러 형태의 로봇팔/humanoid를 구동, 소량의 데이터로 신속 적응.\n대화형 상호작용 및 다언어: 자연스런 언어로 지시하고 피드백하며 작업 진행, 영어 외 다른 언어 명령도 이해 (보고서에 따르면 다국어 평가에서도 양호한 성능).\n안전하고 유연한 대응: 환경 변화나 돌발상황에 실시간 replanning, 위험한 명령은 거부 또는 수정.\n\n이러한 시연 결과는 로봇공학 전문가들에게 상당히 고무적인데, 이는 그동안 개별적으로 발전해온 시각 인지, 자연어 이해, 로봇 제어 기술이 하나로 융합될 때 얻을 수 있는 시너지 효과를 잘 보여주기 때문입니다. 물론 데모들은 최적의 시나리오를 부각한 면이 있으므로, 실제 평균적인 성능은 더 지켜봐야 하지만, “한층 범용적이고 똑똑한 로봇 비서”의 가능성을 엿볼 수 있었다는 점에서 의미가 큽니다."
  },
  {
    "objectID": "posts/paper/2025-09-01-gemini-robotics.html#기존-로봇-시스템과의-비교-palm-e-rt-2-robocat-등",
    "href": "posts/paper/2025-09-01-gemini-robotics.html#기존-로봇-시스템과의-비교-palm-e-rt-2-robocat-등",
    "title": "📃Gemini Robotics 리뷰",
    "section": "기존 로봇 시스템과의 비교: PaLM-E, RT-2, RoboCat 등",
    "text": "기존 로봇 시스템과의 비교: PaLM-E, RT-2, RoboCat 등\nGemini Robotics를 제대로 이해하기 위해서는, 최근 등장했던 유사한 개념의 로봇 AI 시스템들과 비교해보는 것이 유익합니다. 대표적으로 Google의 PaLM-E, DeepMind의 RT-2 (Robotics Transformer 2), 그리고 DeepMind의 또다른 연구인 RoboCat을 들 수 있습니다. 이들 각각은 로봇에 거대 모델을 적용하려는 선구적 시도였으며, Gemini Robotics는 이러한 흐름의 연장선이자 집대성이라 볼 수 있습니다. 각 시스템과 Gemini의 유사점과 차이점을 간략히 살펴보겠습니다.\n\nPaLM-E (Google, 2023): PaLM-E는 대형 언어모델 PaLM에 시각 입력을 추가하여, 로봇 환경의 정보를 언어모델에 직접 연결한 초기 시도였습니다. 예컨대 카메라 이미지가 들어오면 이를 묘사하는 문장이 LLM에 입력되고, LLM은 그 맥락에서 다음 행동을 텍스트 형태로 출력하는 방식입니다. PaLM-E의 특징은 멀티모달 거대 언어모델이 곧 로봇의 두뇌 역할을 한 것으로, 로봇 팔 제어를 포함해 이미지 캡셔닝, 질문답변, 심지어 시적 문구 생성까지 해낼 수 있는 올인원 모델이었습니다. 이는 로봇에게 인터넷 지식과 추론 능력을 부여했다는 의의가 있지만, 실제 로봇 제어는 LLM 출력 텍스트를 별도 정책으로 변환해야 했기에 간접적이었습니다. 즉, PaLM-E는 “생각을 잘하는 로봇 뇌”로서, 직접 모터 명령을 내리기보다는 고수준 플랜(예: “앞에 보이는 사과를 집어 컵에 넣어야 해”)을 말해주고, 저수준 제어는 다른 모듈이 맡는 구조였습니다. 이에 비해 Gemini Robotics는 애초에 물리 행동까지 직접 출력하도록 학습되었다는 큰 차이가 있습니다. Gemini는 PaLM-E와 동일하게 거대 언어/시각 지식을 활용하지만, 최종 출력이 연속적인 로봇 동작 명령이므로 엔드투엔드 제어가 가능합니다. 또한 성능 면에서도, Gemini는 PaLM-E 대비 훨씬 다양한 조작 임무에서 성공률이 높고 일반화 범위가 넓은 것으로 보고되었습니다. 다만 PaLM-E처럼 Gemini도 범용 언어능력을 갖추고 있어, 필요하면 로봇에게 관찰 결과를 설명하게 하거나 인간과 질의응답을 하게 할 수도 있습니다. 요컨대, PaLM-E가 “거대 언어모델을 로봇에 접목”한 첫 단계였다면, Gemini는 “거대 언어모델+비전 모델을 완전히 로봇 액션에 통합”한 진화된 형태로 볼 수 있습니다.\nRT-2 (Robotics Transformer 2, DeepMind, 2023): RT-2는 비전-언어 액션(VLA) 개념을 최초로 선보인 로봇 정책 모델입니다. 이 모델은 PaLM-E와 PaLI-X 등 거대 멀티모달 모델을 백본으로 활용하고, 그 출력 공간을 로봇 행동 토큰으로 재설계했습니다. 구체적으로, RT-2는 카메라 이미지를 입력으로 받아 텍스트 대신 미리 정의된 행동 시퀀스 토큰 문자열을 출력합니다. 예를 들어 “1 128 91 5 …”처럼 숫자열을 내보내면, 이를 해석하는 별도 tokenizer가 엔드 이펙터의 위치 이동과 그리퍼 여닫음을 실행하는 식입니다. 이러한 디스크리트 토큰 표현 덕에, 기존 VLM을 건드리지 않고도 행동 데이터로 파인튜닝이 가능했으며, 그 결과 RT-2는 웹으로 학습된 시각언어 지식을 로봇 행동에 상당 부분 이식하는 데 성공했습니다. RT-2는 특히 훈련에서 보지 못한 객체나 상황에 대한 일반화 능력이 향상되어, 예전 RT-1 기반 모델의 32% 성공률을 62%까지 높였다고 보고됩니다. 또한 사칙연산 개념 등 추론을 요구하는 명령(“바나나를 2+1의 합 위치로 옮겨”)도 웹에서 배운 지식을 활용해 수행하는 등, 에머전트 스킬을 보여주었습니다. Gemini Robotics vs. RT-2를 비교하면, 둘 다 VLA 모델이라는 점에서 개념상 유사하지만 스케일과 범용성 면에서 차이가 있습니다. RT-2는 주로 단일 팔 로봇(이터널들러)과 제한된 조작 세트에 집중했고, 파라미터 규모도 백본 12억~50억 수준이었습니다. 반면 Gemini는 듀얼 암, 인간형 등 다양한 로봇을 다루며, 기반 Gemini 2.0 자체가 거대(수백억~수천억)인 것으로 알려져 보다 고차원의 추론 및 계획까지 가능합니다. 또, RT-2는 행동을 토큰화하면서 일정한 프리미티브 집합 내에서만 동작할 수 있었지만, Gemini는 필요한 경우 코드 생성이나 언어 계획까지 활용하여 행동 표현의 유연성을 확보했습니다. 예를 들어 RT-2가 표현하지 못하는 새로운 행동이 필요하면, Gemini-ER은 파이썬 코드를 생성해 그 행동을 구현하거나, 몇 가지 데모를 참고해 즉석에서 해결책을 찾아냅니다. 이는 고정된 토큰 정책 vs. 가변적 정책의 차이로 볼 수 있습니다. 성능적으로도, Gemini Robotics는 자체 평가에서 RT-2 등 기존 VLA 대비 2배 이상의 성공률을 보였다고 하니, 한 세대 발전한 VLA 모델이라 할 수 있습니다. 다만 RT-2의 간결함(단일 Transformer로 실시간 제어)과 경량성은 Gemini보다 나은 점이었는데, DeepMind가 Gemini On-Device 버전을 내놓은 것도 아마 RT-2 수준으로 경량화하여 보급하려는 의도로 풀이됩니다.\nRoboCat (DeepMind, 2023): RoboCat은 성격이 조금 다르지만, 범용 로봇 조작을 목표로 한 또다른 접근입니다. RoboCat은 거대 언어모델이 아닌 멀티태스킹 비전-행동 모델로, 앞선 Gato 모델을 기반으로 여러 로봇 팔의 데이터를 모아 학습되었습니다. 특징은 “Self-Improving”, 즉 자기 스스로 새로운 시연 데이터를 생성하며 능력을 확장한다는 점입니다. RoboCat은 초기 학습 후 새로운 작업이나 새로운 로봇이 주어지면 100~1000개 수준의 인간 데모를 보고 해당 작업에 맞게 파인튜닝되어 스핀오프 에이전트를 만들고, 이 에이전트로 1만 회 가량 자율 연습을 해 데이터를 모은 뒤, 그 데이터를 본체 모델에 다시 합치는 사이클로 동작했습니다. 이러한 순환적 학습으로 RoboCat은 점점 데이터셋을 불려가며 수백만 건의 다중 경로 경험을 축적했고, 그 결과 4종 이상의 로봇에서 수백 가지 작업을 익히며, 100개의 데모로도 새로운 작업을 습득할 만큼 데이터 효율을 달성했습니다. RoboCat과 Gemini의 차이점은 우선 언어 활용 여부입니다. RoboCat은 언어 입력이 없고, 주어진 목표를 이미지나 좌표 등으로 명세하며, 모델이 이를 달성하는 행동 시퀀스를 내는 형태였습니다. 반면 Gemini는 앞서 본 대로 언어지시를 직접 이해하므로 사용 편의성이 높습니다. 둘 다 다중 로봇, 다중 작업 지향이지만, 접근 방식이 다릅니다: RoboCat은 모델+데이터 자체를 점진 확장하여 “스스로 학습하는 일반 에이전트” 느낌이라면, Gemini는 애초에 거대 지식을 장착하고 시작하여 필요시 조금의 파인튜닝으로 적응하는 “거대 기반모델” 접근입니다. RoboCat이 자기생성 데이터로 성능을 끌어올렸다면, Gemini는 인터넷 수준의 지식을 내장함으로써 별도의 self-play 없이도 높은 성능을 보이는 셈입니다. 결과적으로 Gemini가 보여준 다양한 새 작업에서의 성공은 RoboCat과 목표는 같지만 방법론이 Top-Down (지식장착)으로 달랐음을 알 수 있습니다. 또한 RoboCat은 주로 픽앤플레이스나 간단한 도구 사용 등의 짧은 호라이즌 작업에 집중했고, 장기간 계획이나 언어적 추론은 다루지 않았습니다. Gemini는 그 부분에서 훨씬 범용적이라, RoboCat 대비 적용 분야가 넓다고 할 수 있습니다. 다만 RoboCat의 self-improvement 사이클은 일종의 자동 데이터 증강으로, 향후 Gemini에도 접목 가능성이 있습니다. 예를 들어 Gemini도 자체 시뮬레이터에서 모의 실험들을 수행해 경험을 늘린다면 더욱 강력해질 것입니다.\n\n비교를 표로 정리하면 다음과 같습니다:\n\n\n\n\n\n\n\n\n시스템명\n접근 방식 및 특징\n한계점 및 비교\n\n\n\n\nPaLM-E (2023)\n- 거대 언어모델(LLM)에 비전 입력 추가- 로봇 환경을 텍스트로 기술, LLM이 추론/계획- 이미지 캡션, Q&A 등 멀티모달 임무도 수행 가능\n- LLM 출력이 텍스트라 최종 로봇제어엔 별도 모듈 필요 (간접 제어)- 직접 행동 출력 X; Gemini에 비해 엔드투엔드성 낮음- 범용 지식은 있으나 실시간 상호작용성 제한\n\n\nRT-2 (2023)\n- 사전학습 VLM을 로봇 데이터로 공동 미세조정- 행동을 토큰열로 표현하여 Transformer가 직접 예측- 웹 학습 지식을 활용, 본적 없는 상황 처리 향상\n- 행동 어휘가 한정됨 (정의된 토큰 조합만 가능)- 주로 단일 로봇/작업에 초점, 파라미터 규모 Gemini보다 작음- 장기 계획 위해 체인-of-thought 등 별도 기법 필요 (일부 적용함)\n\n\nRoboCat (2023)\n- 멀티태스크 비전-액션 모델 (Gato 기반)- 다종 로봇 다작업 데이터를 통합 학습- 자기훈련 사이클: 새로운 작업에 파인튜닝 → 자율시행 데이터 축적 → 재훈련- 적은 데모(100개)로 신속 적응, 자체 데이터 생성으로 성능 향상\n- 언어 이해 없음 (목표를 이미지/좌표로만 명세)- 긴 계획/추론 작업 미포함 (주로 짧은 조작)- self-play 사이클이 복잡하고 리소스 많이 요구- Gemini처럼 범용 지식 탑재 X (학습데이터 내 영역에 한정된 범용성)\n\n\nGemini Robotics (2025)\n- 거대 멀티모달(언어+비전) 기반 VLA 모델, 행동을 직접 연속 명령으로 출력- Gemini 2.0 지식 계승: 광범위 언어 이해+추론 겸비- 실세계 조작에 특화 미세조정, 범용성·상황대응·섬세조작 모두 향상- 듀얼암, 휴머노이드 등 다양한 로봇에 소량 데이터로 이식 성공- SOTA 대비 2배 이상 일반화 성능, 고난이도 작업 데모 (접기, 조립 등) 성공\n- 모델 규모 매우 큼: 기본 모델 온보드 구동 어려워 별도 On-Device 버전 필요- end-to-end 모델로 디버깅 어려움, 안전성 위한 외부 장치 필요- 훈련 데이터량 방대, 일반 연구자가 재현 어려움- 현재는 주로 팔 기반 조작에 한정; locomotion 등 확장은 추후 과제\n\n\n\n(주: 위 비교는 각 시스템의 1차 발표 기준 특징을 요약한 것이며, 이후 개선된 버전이나 추가 연구는 반영되지 않았습니다.)\n위 비교에서 볼 수 있듯, Gemini Robotics는 이전 세대들의 아이디어를 흡수·확장하여 언어+시각+행동의 완전 통합과 범용 로봇 제어라는 목표에 가장 근접한 사례로 평가됩니다. PaLM-E의 지식, RT-2의 엔드투엔드 제어, RoboCat의 다로봇 적응을 모두 한 시스템에 녹여낸 셈이며, 그 결과물은 곧잘 “로봇용 GPT”에 비유되곤 합니다. 특히 Gemini 2.0이라는 초거대 모델 기반이라는 점에서, 경쟁사인 OpenAI의 GPT-4 기반 로봇연구나, 다른 학계의 OpenVLA 연구들보다도 스케일과 완성도 측면에서 앞서 있다는 평을 받았습니다. 물론 현실은 여러 제약으로 완벽히 이상적이지 않지만, 일반 지능을 지닌 로봇이라는 오랜 꿈에 한 걸음 다가간 성취임은 분명합니다."
  },
  {
    "objectID": "posts/paper/2025-09-01-gemini-robotics.html#비판적-논의-한계와-향후-과제",
    "href": "posts/paper/2025-09-01-gemini-robotics.html#비판적-논의-한계와-향후-과제",
    "title": "📃Gemini Robotics 리뷰",
    "section": "비판적 논의: 한계와 향후 과제",
    "text": "비판적 논의: 한계와 향후 과제\n마지막으로, Gemini Robotics 시스템에 대한 한계점과 개선 필요 분야를 전문가 시각에서 논의해보겠습니다. 혁신적인 시스템일수록 냉철한 평가가 필요한 법이기에, 이 모델의 약점이나 리스크를 짚어보고 미래 방향을 생각해봅니다.\n1. 방대한 데이터와 모델 규모에 대한 의존성: Gemini Robotics의 성능은 결국 대규모 사전학습에 기댄 바가 큽니다. 인간의 개입 없이 새로운 작업도 해낼 수 있었던 비결은, 이미 모델 내부에 세상의 온갖 지식과 시나리오에 대한 통계가 학습되어 있었기 때문입니다. 이를 얻기 위해 투입된 데이터(시뮬레이터 생성 데이터+전이학습 코퍼스 등)는 일반 연구 단위에서는 감당하기 어려운 양일 것입니다. 이러한 데이터 의존성은 범용 로봇 모델 연구의 양날의 검인데, 데이터가 많을수록 강력한 모델을 얻지만, 동시에 데이터 편향이나 품질 문제도 내재할 수 있습니다. 예를 들어, 웹에서 수집한 언어 데이터에는 잘못된 상식이나 편견이 섞여 있을 수 있고, 시뮬레이터 데이터는 현실 물리의 복잡함을 완전히 담지 못할 수 있습니다. Gemini Robotics가 현재까지는 주로 탁상형 조작작업에 대한 학습을 했기 때문에, 자연이나 사람과의 물리적 상호작용 같은 영역은 데이터가 부족하여 약할 가능성이 있습니다. 그러므로 모델이 학습하지 못한 distribution에 놓이면 어떤 거동을 할지 미지수입니다. 이 문제를 풀기 위해서는 앞으로 데이터 다양성을 더욱 늘리고, 부족한 영역은 모델이 직접 실험하며 채우게(self-play) 하는 방법도 고려해야 합니다. 또한 거대 모델을 조금 더 작게 분해하거나 모듈화하여, 부분적으로 데이터를 추가 학습시킬 수 있게 하면 효율이 올라갈 것입니다. 예컨대, 시각 모듈은 지속 업그레이드하고 언어 모듈은 동결한다든지 하는 방식으로 데이터 의존성을 분산시키는 연구가 필요합니다.\n2. 제어 안정성과 안전성: 앞서도 다뤘듯, 이렇게 신경망이 모든 것을 결정하는 로봇은 전통적인 제어 시스템과 비교했을 때 신뢰성 면에서 걱정이 있습니다. 로봇공학에서 안정성(stability)이란 물리적으로 시스템이 예측 불가능하게 폭주하지 않고 안정된 궤적을 유지하는 것을 뜻하는데, 학습된 정책이 항상 그걸 보장하리란 법이 없습니다. 특히 산업 환경에서는 작은 오판도 큰 사고로 이어질 수 있으므로, Gemini Robotics 같은 시스템을 바로 적용하기는 어려울 수 있습니다. 이를 위해 논문 저자들도 안전장치를 병렬로 운용했지만, 이는 완전한 해결책이라기보다 임시방편입니다. 예컨대, 모델이 사람을 인지 못하고 충돌하려 하면 외부 센서가 멈추게 한다지만, 모델 자체가 사람을 잘 인지하도록 하는 편이 바람직할 겁니다. 또 하나, 신뢰도 추정의 부재도 문제입니다. 현재 모델은 모든 판단을 확률적으로 하지만, 자신이 얼마나 확신없는지를 출력하지는 않습니다. 이상적인 시스템이라면 “지금 상황을 잘 모르겠어”라고 스스로 인지하고 인간에게 도움을 청하거나 안전모드로 들어가야 할 텐데, 이런 메타인지 기능은 아직 구현되지 않았습니다. 미래에는 거대 모델에 불확실성 추정 모듈을 내장하거나, 외부에서 모델 판별기를 두어 출력의 신뢰도를 모니터링하는 것이 필요해 보입니다.\n3. 모델 해석 가능성과 디버깅 이슈: Gemini Robotics 같은 엔드투엔드 딥러닝 로봇은 그 내부 의사결정 과정을 사람이 따라가기 어렵습니다. 왜 이 행동을 했는지, 어디서 오류가 났는지를 알기 힘들기 때문에, 원인 분석과 개선이 난감합니다. 전통적 로봇 프로그램이라면 로그나 규칙을 보고 수정하면 되지만, 이 경우 학습 데이터나 가중치를 바꾸는 수밖에 없습니다. 이는 곧 개발 사이클이 느려지고 버그 수정이 불확실해짐을 의미합니다. 실제로 거대 모델이 잘못된 판단을 할 때, 그것이 언어 이해의 오류인지 비전 인식의 오류인지도 판단하기 어렵고, 결국 전부 다 재훈련해야 할 수도 있습니다. 이러한 문제를 완화하려면, 모델의 판단근거를 설명하는 기술(XAI)이나, 모듈별 책임 분담을 부분적으로라도 도입하는 것이 고려됩니다. 예컨대, Gemini-ER처럼 인지 전처리 모듈을 별도로 두는 것은 한 방안입니다. 또는 행동 출력 전에 내부 언어 추론 과정을 토큰으로 표출하게 하여, 인간이 개입할 여지를 만드는 연구도 가능합니다 (실제로 RT-2에서는 체인-of-thought을 활용해 중간 플랜을 언어로 생성하도록 하기도 했습니다). 궁극적으로, 인간 전문가와 모델이 공동으로 작업 계획을 수립하고 모델은 세부를 실행하는 반자동 방식으로 가는 것이 안전하고 해석가능성을 높이는 방향일 수 있습니다.\n4. 확장성(Scalability): 여기서 말하는 확장성이란, 과연 이 접근이 로봇 전반으로 확장될 수 있는가 하는 문제입니다. Gemini Robotics는 단일 연구기관(DeepMind)의 자원으로 개발되었고, 현재 Trusted Tester 프로그램을 통해 일부 파트너들에게만 제공되고 있습니다. 모든 연구자가 이 모델을 활용해 실험할 수 있는 건 아니며, 또 각자 새로운 데이터를 추가하여 개선판을 만들 수도 없는 상태입니다. 이는 연구 커뮤니티의 재현성과 협업 측면에서 한계입니다. 또한 산업 적용을 위해서는 맞춤 수정이 필요할 텐데, 폐쇄된 거대 모델을 수정하기는 어렵습니다. 이런 측면에서 오픈소스 로봇 foundation 모델의 필요성이 대두됩니다. 만약 Gemini Robotics 같은 모델이 공개되고 쉽게 fine-tune 가능해진다면, 다양한 특수 환경(예: 수술 로봇, 농업 로봇 등)에도 이 아이디어를 이식할 수 있을 것입니다. 그러나 현재로서는 상용화와 관련된 전략이 명확하지 않아 보입니다. 구글 내부에서는 아마 Gemini 로봇 플랫폼을 구축해 로봇 제조사들과 협력하려 할 것으로 추측되지만, 외부에서 볼 때에는 초거대 모델의 폐쇄성이 확장의 걸림돌일 수 있습니다. 이에 대한 해결은 기술보다는 정책과 전략의 문제일 수 있겠습니다.\n또 다른 측면의 확장성으로, 하드웨어 제약과 실시간성 이슈가 있습니다. 로봇은 실시간으로 움직여야 하며, 센서 피드백에 수십 Hz~100Hz 이상으로 반응해야 할 때가 많습니다. 하지만 거대 트랜스포머 모델이 그러한 고속 실시간 제어 loop에 직접 들어오는 것은 어려운 일입니다. 논문에서는 이 문제를 부분적으로 피하기 위해, 카메라 프레임당 한두 개 정도의 고수준 명령을 생성하고, 세부 모션은 내장된 로우레벨 컨트롤러(PID 등)가 수행하게 했을 가능성이 있습니다. 그러나 진정한 의미의 end-to-end라면 서브-millisecond 단위 토크제어까지 학습으로 대체해야 하는데, 이는 현재 기술로는 비현실적입니다. 결국 하이브리드 제어(학습된 고수준 계획 + 전통 저수준 제어)가 불가피하며, 이 경계를 어디까지 확장할지가 과제입니다. 추후 모델 경량화(예: 2B~10B 수준 파라미터로 양질의 성능을 내는)와 전용 가속 하드웨어 발전이 이루어지면, 더 고속의 피드백 루프에 딥러닝 정책을 넣을 수 있을지도 모릅니다. 실제 DeepMind도 On-Device 버전을 통해 이 방향을 모색하고 있습니다.\n5. 윤리 및 책임 문제: 마지막으로, AI의 물리 세계 진출에 따라 불거지는 윤리적 문제와 사회적 영향도 짚어야 합니다. Gemini Robotics 같은 범용 로봇이 상용화된다면, 인간 노동을 대체하거나, 인간과 직접 상호작용할 가능성이 있습니다. 이는 일자리나 안전, 프라이버시 이슈와 직결됩니다. 또한 거대 모델 특유의 할루시네이션이나 오류가 물리적으로 구현될 경우, 그 피해는 디지털 영역의 실수보다 훨씬 클 수 있습니다. 예를 들어, 모델이 잘못된 판단으로 위험한 약품을 엉뚱한 곳에 놓는다든지 하는 일이 생기면 심각한 사고로 이어질 수 있습니다. 따라서 기술 개발과 더불어, 법적 규제와 안전 표준 마련이 병행되어야 합니다. 논문에서도 Asimov의 로봇 3원칙을 언급하며 윤리적 프레임워크를 구축하려는 노력을 언급했지만, 이는 개념적인 수준이고 실제 현장에서 검증된 규범은 아닙니다. 결국 사람과 함께 일하는 로봇이라면 인증 체계와 책임 소재 규명이 중요한데, 현 단계의 AI 로봇은 그 경계가 모호합니다. 예컨대, 거대 모델의 잘못으로 사고가 났을 때, 그것을 개발한 기업의 책임인지, 사용하는 쪽의 책임인지 등이 불명확합니다. 이러한 문제를 선제적으로 해결하기 위해, 개발 단계부터 안전제어기+모델 식의 이중화 구조를 권고한다거나, 중요한 의사결정에는 인간 승인을 받도록 설계한다거나 하는 휴먼-인-더-룹 메커니즘이 필요할 수 있습니다. 결국 기술적 완성도뿐만 아니라 사회적 수용성까지 고려해야 Gemini Robotics와 같은 시스템이 실질적으로 쓰이고 발전할 수 있을 것입니다."
  },
  {
    "objectID": "posts/paper/2025-09-01-gemini-robotics.html#결론",
    "href": "posts/paper/2025-09-01-gemini-robotics.html#결론",
    "title": "📃Gemini Robotics 리뷰",
    "section": "결론",
    "text": "결론\n“Gemini Robotics: Bringing AI into the Physical World”는 로봇 공학 분야에서 하나의 중대한 이정표로 평가됩니다. 이 시스템은 거대 멀티모달 AI의 능력을 물리 세계에 접목함으로써, 로봇이 시각적으로 보고, 언어로 이해하고, 스스로 행동할 수 있는 새로운 경지를 선보였습니다. 시스템 아키텍처 측면에서는 비전-언어-액션을 하나로 융합한 혁신적인 구조를 채택하여 범용성과 상호작용성을 극대화했고, 학습 방법론 면에서는 막대한 사전학습 지식을 로봇 제어에 녹여내는 단계적 훈련 전략을 활용하였습니다. 이를 통해 다양한 실제 작업 데모에서 입증되었듯, Gemini Robotics는 이전 로봇들과 비교해 훨씬 폭넓은 능력을 발휘하며, 한계를 뛰어넘는 범용 로봇의 가능성을 엿보게 합니다.\n물론, 이러한 성과와 동시에 극복해야 할 한계들도 분명히 존재합니다. 모델의 거대함에 따른 실용성 문제, 블랙박스 모델의 안전성·신뢰성 이슈, 그리고 데이터 및 윤리적 측면의 도전들은 앞으로 풀어야 할 숙제입니다. 그러나 과거 좁은 범위의 작업에 갇혀있던 로봇 공학에 AI 혁명의 숨결을 불어넣었다는 점에서, Gemini Robotics의 등장은 매우 고무적입니다. 이 연구는 PaLM-E, RT-2, RoboCat 등으로 이어진 로봇을 위한 Foundation Model 연구 흐름의 정점을 찍은 것으로, “생각하고 행동하는 로봇”이라는 오랜 꿈이 이제 손에 잡힐 듯한 현실감으로 다가오고 있습니다.\n향후 몇 년간 우리는 Gemini Robotics의 발전형이나 유사한 시스템들이 더 다양한 로봇 플랫폼에 적용되어 나오는 것을 보게 될 것입니다. 예컨대 가정용 서비스 로봇, 의료 보조 로봇, 제조 현장 로봇 등이 이러한 범용 지능 모듈을 탑재함으로써, 복잡한 환경에서도 유연하게 작업을 수행하게 될 것입니다. 또한, 학계와 산업계에서 Gemini Robotics의 접근법을 개방하고 표준화하려는 움직임이 나타나, 더 많은 연구자들이 이 기술을 개선하고 변형해나갈 것으로 기대됩니다.\n결론적으로, Gemini Robotics는 “AI를 물리 세계로 가져온” 선구적 사례로서, 로봇 공학의 지평을 한층 넓혔습니다. 그 기술적 세부와 의의를 조망하여 요약하자면 다음과 같습니다:\n\nGemini Robotics는 Gemini 2.0 기반의 비전-언어-행동 통합 모델로, 로봇이 카메라로 보고(Look), 사람의 말을 이해하고(Listen), 곧바로 행동(Act)하는 엔드투엔드 지능을 구현했다.\n시스템 구조는 트랜스포머로 구현된 비전 인코더 + 언어 인코더 + 액션 디코더로 이루어지며, 일반적 상황 이해와 물체 조작까지 하나의 거대 모델이 처리한다. 추가적으로 공간추론 특화 Gemini-ER 모듈이 결합되어 3D이해를 돕는다.\n학습 면에서, 거대 멀티모달 사전학습으로 범용지식을 획득한 후, 실제 로봇 데이터로 미세조정과 강화학습을 시행하여 현실 적응력을 키웠다. 이를 통해 시뮬레이션-현실 간 갭을 줄이고, 100개 수준 데모로 새로운 작업도 습득 가능한 데이터 효율을 달성했다.\n멀티모달 통합의 힘으로, Gemini Robotics는 일상 언어 명령을 이해하고 맥락에 맞게 행동을 계획하며, 시각 피드백에 따라 행동을 즉흥 조정하는 능력을 보였다. 이는 로봇과의 자연스러운 상호작용과 에머전트한 문제 해결을 가능케 했다.\n실제 실험에서 로봇은 샐러드 만들기, 종이접기, 바구니에 물건 채우기, 지퍼 열고 닫기, 옷 개기, 장난감게임 등 다양한 과업을 성공적으로 수행했고, 새로운 물체/명령에 첫 시도 성공하는 등 범용성을 입증했다. 또한 Franka 팔, Apollo 휴머노이드 등에 모델을 이식하여 로봇 구조적 범용성도 확인했다.\nPaLM-E, RT-2, RoboCat 등과 비교할 때, Gemini Robotics는 이들의 장점을 통합한 상위 개념으로, LLM의 지식+VLM의 일반화+다로봇 지원을 모두 구현했다. 그 결과 기존 모델들보다 종합 성능에서 우월하며 더 광범위한 적용이 가능하다.\n한계로는 모델이 크고 데이터에 크게 의존하여 재현 비용이 높다는 점, 결정 과정을 해석하기 어려워 안전성과 디버깅에 새로운 문제가 생긴 점, 그리고 다양한 실세계 모든 분야로 가기까지 남은 영역(예: 이동 로봇, 인간 사회적 상호작용 등)이 있다는 점이 지적된다. 이를 위해 경량화, 모듈화, 안전장치, 표준화 등의 보완이 논의되고 있다.\n\n끝으로, Gemini Robotics는 AI와 로봇의 융합이 만들어낼 혁신을 선명히 보여주었습니다. 이 기술이 완전히 실용화되고 나면, 로봇은 더 이상 공장이나 실험실의 특정 임무 장치가 아니라, 우리 생활공간에서 사람과 협업하고 배우는 동반자로 발전할 가능성이 큽니다. 물론 거기에는 넘어야 할 과제들이 있지만, 이번 연구를 계기로 학계와 업계 모두가 “생각하고 행동하는 로봇”이라는 목표에 한층 집중하게 되었다는 점은 분명합니다. 앞으로 Gemini Robotics를 비롯한 차세대 로봇 AI가 어떻게 진화해갈지 지켜보며, 로봇 공학자가 가져야 할 역할—기술적인 통찰 뿐 아니라 윤리적 고찰과 사회적 준비—에 대해서도 깊이 고민해봐야 할 시점입니다. AI의 물리 세계 진출은 이제 막 시작되었으며, 그 가능성과 책임은 모두 우리의 몫으로 다가오고 있습니다."
  },
  {
    "objectID": "posts/paper/2023-03-12-wasabi.html",
    "href": "posts/paper/2023-03-12-wasabi.html",
    "title": "📃WASABI 리뷰",
    "section": "",
    "text": "이번 포스팅은 WASABI: Learning Agile Skills via Adversarial Imitation of Rough Partial Demonstrations 논문을 읽고 정리한 내용입니다. 4족 보행 로봇 연구에서 많은 연구 성과들을 발표하는 스위스의 ETH Robotic System Lab과 독일의 Max Plank Institude for Intelligent Systems에서 발표한 논문으로, 강화학습에서 중요한 부분들 중 하나인 reward design에 대한 고민을 generatvie adversarial method(WGAN, Wasserstein GAN)를 통해 해결할 수 있음을 보여주었습니다.\n보행 로봇의 모션 제어에서 기본적인 보행뿐만 아니라 다양한 다이나믹한 모션을 수행하도록 로봇의 퍼포먼스를 끌어올리는 방향으로 연구가 활발하게 진행되고 있습니다. 여기서 말하는 다이나믹한 모션들로는 로봇이 공중에서 한바퀴 돌아야 하는 backflip과 같은 기존의 전통적인 보행 제어 연구를 기반으로 rule-based로 제어하기에는 매우 어려운 모션들을 말합니다. 로봇이 이런 모션들을 수행하도록 수학적으로 자세히 명시하고 그리고 모든 물리적 환경요소들을 고려하여 제어하기 어려울 때, 강화학습이라는 인공지능 프레임 워크를 이용하여 reward라는 보상체계를 기준으로 trial-and-error를 통해 모션을 학습하도록 하는 것이 직관적으로 매우 좋은 해결책으로 보입니다.\n하지만 다이나믹한 모션을 각 task로 정의하고 우리가 원하는 방향대로 로봇이 모션들을 학습되기 위해서는 reward를 잘 정의해주어야 하는데 이 과정이 만만치 않게 까다롭고 어려우며, 오히려 수학적인 동역학 모델을 기반으로 제어할 때보다 분석적인 접근이 어렵기 때문에 reward design이라는 과제를 해결해야만 우리가 원했던 다이나믹 모션들을 강화학습을 이용하여 로봇이 수행할 수 있을 것 입니다. 바로 이 부분을 생성모델로 유명한 GAN 모델들 중 하나인 WGAN을 이용하여 해결하고자 했으며 해당 논문에서 가장 흥미로웠던 접근법은 강화학습의 policy를 GAN의 generator 관점으로 바라보고 reward를 추론하도록하는 프레임 워크를 만들었다는 점이었습니다. (이후 관련해서 더 논문들을 찾아보니 생성모델과 강화학습은 닮은 점이 많은 것 같습니다. 관련해서 흥미롭게 읽었던 다른 논문 Connecting Generative Adversarial Networks and Actor-Critic Methods도 관심이 있으시다면 가볍게 읽어보시는 것을 추천드립니다.)"
  },
  {
    "objectID": "posts/paper/2023-03-12-wasabi.html#gan",
    "href": "posts/paper/2023-03-12-wasabi.html#gan",
    "title": "📃WASABI 리뷰",
    "section": "GAN",
    "text": "GAN\n적대적 신경망에 대해 기본적인 이론부터 시작해보겠습니다. GAN은 생성 모델을 학습하기 위한 방법론 중 하나로 Generative, 어떠한 새로운 데이터 생성을 하는, Adversarial 게임과 같이 Discriminator와 Generator라는 2개의 알고리즘 모듈이 경쟁을 하며 학습을 하는 방법론 입니다. 아래 사진에서 보이는 예시로 보면 진짜 모나리자 그림이라는 Real example을 보고 이를 모사한 작품을 파는 화가를 Generator라고 생각해볼 수 있습니다. 그러면 미술 작품 감별사인 Discriminator는 이 작품이 진짜 모나리자 그림인지 아니면 화가가 모사한 가짜 모나리자 인지 판단하게 됩니다. 당연히 Generator 입장에서는 Discriminator가 감별하기 어렵게 점점 더 진짜같은 모나리자를 그리게 되고(new data) Discriminiator 입장에서는 진짜와 가짜 사이에 더 자세하고 민감한 차이를 찾아내어 Generator의 모사품을 찾아내려고 할 것 입니다.\n\n이러한 GAN의 학습 과정에는 지도 학습과 비지도 학습이 모두 들어있습니다. 우선 Discriminator 입장에서는 진짜와 가짜 라벨을 가진, 인풋 데이터가 들어오면 2개의 카테고리들 중 하나를 선택하는 지도학습을 하게 됩니다. Generator는 비지도 학습으로 latent code라는 일종의 trigger 요소인 어떤 벡터를 인풋으로 받으면 진짜 data distribution과 가까운 데이터인 new data를 생성하게 됩니다.\n\n잠깐 data distribution이라는 개념이 GAN에서는 중요한 개념이므로 Probability Distribution(확률 분포)을 간단하게 짚고 넘어가겠습니다. 확률 분포란 어떤 사건을 대변하는 랜덤 변수들의 확률 분포라고 볼 수 있습니다. 주사위를 총 6번 던져서 1, 2, 3, 5가 각각 1번씩 그리고 6이 2번 나왔다면 아래와 같은 확률 분포 그래프를 그릴 수 있고, 이때의 Expectation(기댓값)을 구해보면 1 \\cdot \\frac{1}{6} + 2 \\cdot \\frac{1}{6} + 3 \\cdot \\frac{1}{6} + 4 \\cdot \\frac{0}{6} + 5 \\cdot \\frac{1}{6} + 6 \\cdot \\frac{2}{6} = \\frac{23}{6} \\eqsim 3.8 임을 알 수 있습니다.\n\n이미지를 데이터 포인트 x라고 하고 우리가 가지고 있는 사람 얼굴 이미지 데이터 셋의 분포가 왼쪽의 분포와 같다고 한다면, 여러개의 모드(mode)가 있는데 가장 높은 확률의 mode에서는 금발 여성의 얼굴이 있고 상대적으로 낮은 확률로 흑발의 안경 쓴 남자의 얼굴 이미지가 있음을 알 수 있습니다. 또한 mode가 아닌 매우 낮은 확률을 보이는 분포의 꼬리 부분을 보면 매우 이상한 얼굴 이미지들이 나오는 것을 알 수 있습니다.\n\n바로 우리가 가지고 있는 이미지 데이터 셋 분포(빨강색)과 유사한 데이터 분포(파란색)를 학습하는 것이 생성 모델의 목표이고 이를 Discriminator와 Generator를 가지고 학습하도록 하는 것이 GAN입니다.\n\nDiscriminator의 Objective Function(V)을 보면, 먼저 첫번째 term은 데이터 x는 true dataset distribution인 p_{data}에서 샘플링 되었을 때 Discriminator는 이를 진짜라고 판별해야 하고 이는 output 1(true label)을 출력해야하는 방향으로 학습되어야 합니다. 두번째 term은 fake dataset distribution인, 즉 generator가 만든 데이터일 경우에 가짜라고 판별해야 하고 output 0(fake label)을 출력해야 합니다. 따라서 2개의 term을 모두 maxmization하는 것이 Discriminator의 목표이기 때문에 \\text{max}_DV(\\cdot)이 됩니다.\n\nGenerator의 Objective Function을 보면, 첫번째 true dataset distribution에서 샘플링 되는 부분은 Generator와 상관이 없습니다. 두번째 term에서 Generator에서 나온 ouput new data를 Discriminator에게 넘겨주었을 때 1(true label)로 착각하도록 만들어야 하므로 \\text{min}_GV(\\cdot)이 됩니다."
  },
  {
    "objectID": "posts/paper/2023-03-12-wasabi.html#wgan",
    "href": "posts/paper/2023-03-12-wasabi.html#wgan",
    "title": "📃WASABI 리뷰",
    "section": "WGAN",
    "text": "WGAN\n위에서 설명한 기본적인 GAN을 잘 학습했을 때 확률분포를 그려보면 다음과 같이 Discriminator의 판별 분포가 빨간색 그래프처럼 그려지는 것을 알 수 있습니다. 완벽하게 true distribution인 p_{data}에 대해서는 1을, generated distribution p_G에 대해서는 0을 보여주고 있지만 이런 상황에서는 유의미한 학습이 일어나기 힘듭니다.\n\nOptimal한 Discriminator를 가정하고 Objective function을 다시보면 p_{data}와 p_G가 너무 멀리 떨어져 있어서 사실상 계산된 V(\\cdot)값이 0이기 때문입니다. 따라서 Generator가 두 분포가 가깝도록 만드는 방향으로 학습을 해야 하는데 Classic GAN의 Objective Function에는 이러한 정보를 알려줄 수 있는 부분이 수학적으로 모델링이 되어 있지 않습니다.\n\n따라서 분포들간의 먼 정도를 모델링할 수 있는 WGAN(Wasserstein GAN)이 제안되었고 이에 대해서는 수학적으로 매우 딥한 내용이 있지만 본 포스팅에서는 간단하게 개념적으로 공사장의 포크레인을 이용하여 이해하고 넘어가겠습니다. Wassertein Distance는 Earth mover’s distance라고도 불리는데 이름에서 직관적으로 이해할 수 있듯이, 두 분포를 어떤 흙더미라고 생각하고 우리가 Generated Distribution에 있는 흙들을 Real Distribution의 모양대로 흙들을 옮긴다고 했을 때 드는 cost가 distance로 정의된다고 볼 수 있습니다. (수학적으로 더 궁금하신 분들은 Implicit DGM 29 | Wasserstein Distance with GAN을 추천합니다.) 본 연구에서는 이 WGAN을 이용하여 reward 디자인을 했습니다."
  },
  {
    "objectID": "posts/paper/2023-03-12-wasabi.html#rl-with-gan",
    "href": "posts/paper/2023-03-12-wasabi.html#rl-with-gan",
    "title": "📃WASABI 리뷰",
    "section": "RL with GAN",
    "text": "RL with GAN\nGAN 내용을 설명할 때 이미지 생성 분야의 예시가 직관적이고 쉽기 때문에 이를 가지고 설명하다 보니 문득 그래서 강화학습에서 어떻게 GAN을 사용하는데? 라는 의문이 생길 수 있습니다. 다시 강화학습에서의 여러 어려움들 중 Task reward를 잘 정의해주기가 어렵다는 점을 상기시켜보면 Task reward를 Discriminator가 결정해줄 수 있지 않을까라는 아이디어를 떠올려볼 수 있습니다. 모션의 reference가 될 수 있는 demonstration의 일련의 state들이 true distribution이 되고, policy에서 나오는 일련의 state들이 generated distribution이 되어서, Discriminator가 두 분포를 못 구분할 정도를 task reward로 정의한다면 policy가 demonstration에서 나타난 다이나믹한 모션들을 따라하도록 학습할 수 있는 지표가 될 수 있을 것 입니다. 이전에 locomotion이나 backflip 등의 각각의 모션마다 task reward를 hand design 할 때는 각 모션에서 보행 로봇의 발이 어떻게 움직여야 하는지, 몸체의 속도가 어떠해야 하는지 일일이 reward로 고려하고 여러 reward term들을 weighted sum하는 방식이었지만 이 GAN 방식을 이용하면 각 모션에 대한 demonstration의 state들을 보고 어떤 모션을 어떻게 따라해야하는지 agent의 policy가 알아서 task reward를 높이는 방향으로 학습할 수 있는 것 입니다."
  },
  {
    "objectID": "posts/paper/2023-03-12-wasabi.html#problem-definition",
    "href": "posts/paper/2023-03-12-wasabi.html#problem-definition",
    "title": "📃WASABI 리뷰",
    "section": "Problem Definition",
    "text": "Problem Definition\n이전에 AMP 방식에서 모션의 자연스러움을 학습하기 위해 Motion data가 매우 well-defined 되어 있어야 한다고 했습니다. 하지만 이러한 Motion data(혹은 demonstration)을 얻기는 어렵고 특히나 보행과 같이 이미 많이 연구가 되어왔고 동물들의 모습에서도 많이 관찰될 수 있는 task와는 다르게 다이나믹한 backflip하는 모션 task들은 참고할 데이터들도 매우 적고 만들어내기도 어렵습니다. 이런 문제 상황을 본 연구에서는 Rough하고 Partial한 demonstration만 있는 문제로 파악하고 Rough한 모션 데이터라는 것은 실제 로봇이나 동물이 움직여서 얻은 데이터가 아닌 사람이 로봇을 단순히 들고 움직여서 얻은 데이터를 말하며 Partial하다는 것은 로봇의 모션 데이터라고 해서 로봇을 구성하고 있는 모든 joint들의 움직임에 대한 데이터가 아닌 로봇의 몸체에 대한 정보만 있는 모션데이터만 있는 것을 말합니다.\n\n말로만 들으면 잘 와닿지 않기 때문에 위에 사진에서 한 연구자가 backflip하는 demonstration 데이터를 얻기 위해 로봇을 들고 손으로 그냥 한번 뒤집어주는 모습을 보면서 다시한번 설명을 해보겠습니다. 앞서 설명했듯이 로봇이 backflip하는 작동을 해서 데이터를 얻지 않고 사람이 단순히 로봇을 들고 원하는 모션의 demonstration 데이터를 얻습니다. 여기서 Backflip demonstration 데이터는 로봇의 12개의 joint들에 대한 정보는 없이 몸체에 대한 정보(base linear, angular velocity, projected gravity, base height)만을 포함하게 됩니다. 여기서 demonstration 데이터에 대한 놀라운 점은 로봇이 직접 움직여서 얻은 데이터도 아니고 실제 동물의 모션 데이터도 아니기 때문에 물리적으로도 시간적으로도 로봇 플랫폼에서는 사실상 따라하기 어려운 데이터라는 것입니다. 이런 demo 데이터만 있다고 문제상황을 가정한 이유는 backflip과 같이 다이나믹하고 다양한 모션에 대해서는 reference가 될 만한 motion data를 well-defined하기 어렵기 때문입니다.\n이쯤에서 다시한번 AMP와 WASABI를 다시 비교해보면, 두가지 방법 모두 expert의 action이 없이도 reference가 될 수 있는 motion data(혹은 demonstration)를 가지고 reward engineering을 잘해서 모션 제어를 할 수 있었다는 점에서 공통점이 있습니다. 하지만 AMP는 well-defined한 모션 데이터가 있어야 가능한 방법론인 반면 WASABI는 로봇의 몸체에 대한 partial한 모션 데이터만 있으면 학습할 수 있었고 AMP는 모션의 주요 reward를 디자인한 것이 아니라 자연스러움을 위한 보조적인 style reward 디자인을 했고 WASABI는 각 모션에 대한 task reward를 디자인 한 것이 큰 차이점이라고 볼 수 있습니다."
  },
  {
    "objectID": "posts/paper/2023-03-12-wasabi.html#reward-design",
    "href": "posts/paper/2023-03-12-wasabi.html#reward-design",
    "title": "📃WASABI 리뷰",
    "section": "Reward Design",
    "text": "Reward Design\nPartial하고 Rough한 모션 demo들을 가지고 어떻게 하면 다이나믹한 모션에 대한 reward를 정의할 수 있을까요?\n\nWASABI에서 제안한 전체적인 알고리즘 구조는 아래와 같습니다. r^I, r^R, r^T 라는 각각의 reward가 합쳐지는 것을 볼 수 있는데요 이제부터 각각의 reward가 어떤 의미와 목적을 가지고 있는 것인지 살펴보겠습니다.\n\n\nImitation(Task) Reward\n우선, task reward는 다이나믹 모션의 demo를 잘 모방(imitate)할 수 있도록 해야할 것 입니다. 그래서 imitation reward 혹은 task reward로 불리며 여기서 WGAN 방법을 이용해서 정의하게 되는 부분입니다. 다시한번 이야기하지만 우리가 backflip을 하는 학습을 하기 위해서 로봇의 몸체를 공중에 올리고 pitch 방향으로의 회전을 360도 해야해!라고 말해주는 imitation reward function(hand-designed)을 사용하는 것이 아니라 demo(true) distribution을 보고 이를 따라가는 generated distribution을 policy가 학습할 수 있도록 하는 것이 이 방법의 핵심입니다.\n\n잠깐 앞에서 이야기 했듯이 우리가 사용하는 demo 데이터는 well-defined한 데이터가 아닌 사람이 로봇을 들고 모은 데이터이기 때문에 로봇의 base에 대한 데이터(O)로 한정적입니다. 하지만 policy에서 generated된 observation 데이터(S)는 로봇의 각 joint에 대한 정보 등 더 많은 정보가 있는 vector space이기 때문에 true distribution과 generated distribution을 비교가능한 상태로 만들어주기 위해 Mapping function \\phi를 사용하여 맞춰줍니다. 쉽게 생각하자면 정보량이 더 많은 S를 차원이 적은 O로 맞춰주기 위해 joint position, velocity, last action과 같은 부분을 가리고 data distribution을 Discriminator에게 넘겨주는 것으로 볼 수 있습니다.\n\nmapping function을 통해 차원을 맞춘 \\phi(s) 와 o는 GAN의 objective function에서 Discriminator의 인풋으로 들어가는 seq. of states(observations)이며 아래와 같이 일정 time horizon H동안 모아진 states 벡터들로 볼 수 있습니다. 이러한 seq. of states들을 가지고 Discriminator가 만든 reward distribution을 각각 LSGAN(Least Squares GAN)과 WGAN의 objective function으로 아래와 같이 나타내 볼 수 있습니다. 여기서 LSGAN은 WGAN의 비교군이 되는 또 다른 GAN의 알고리즘이며 LSGAN의 Objective function을 해석해보면, policy에서 나온 state history를 가지고 나온 reward distribution은 -1에 가깝도록 demo를 통해 나온 reward distribution은 +1에 가깝도록 하는 것으로 볼 수 있습니다. 반면, WGAN은 이 두 분포간의 wasserstein distance 줄이도록하는 방향으로 학습합니다. 두 가지 GAN 모두 policy에서 나온 seq. of states로 나온 task reward distribution을 demo의 seq. of states로 나온 task reward distribution을 맞춰가도록 학습하는 것은 공통적입니다.\n\n\n이렇게 Discriminator를 통해 나온 task reward는 바로 사용되는 것이 아니고 zero-mean unit-variance로 만들어주는 과정을 한번 거친 후 비로소 Task(Imitation) Reward로 만들어집니다.\n\n\n\nRegularization Reward\n이전에 AMP에서의 Style reward의 역할을 WASABI에서는 Regularization Reward가 대신한다고 볼 수 있습니다. 이 reward는 task-dependent하지 않은 task-agnostic한 term들로 이루어져 있어서 backflip 모션을 하든 locomotion 모션을 하든 로봇의 자연스럽고 에너지 효율적인 모션을 위해 부가적으로 더해지는 reward라고 볼 수 있습니다.\n\n\n\nTermination Reward\n마지막으로 agent가 모션을 충분히 학습하기도 전에 episode를 더 빨리 끝내는 것이 이득이라 판단하고 학습이 잘 이루어지지 않는 경우를 방지하고자 Termination Reward를 추가해주었습니다. T는 episode를 너무 빨리 끝내버린 경우에 대해서 0 또는 1로 판단하는 인디케이터 역할을 하게 되고, termination에 대한 고려는 Imitation reward의 분포에서 나온 \\sigma와 할인율 \\gamma를 고려하여 다음과 같이 정해주게 됩니다.\n\n\n\nTotal Reward\n앞서 설명한 Imitation reward r^I, Regularization reward r^R, Termination reward r^T를 모두 합산하여 Total reward가 계산되게 되고 이를 Agent에게 학습 피드백으로 보내주게 됩니다. 이때 r^I와 r^T는 모션 task 마다 다르게 정의될 수 있는 부분이므로 task-related한 부분이라고 볼 수 있으며 r^R는 어떤 모션 task인지 상관없이 항상 동일한 reward term이기 때문에 task-agnostic한 부분이라고 할 수 있습니다. 물론 여기서 해당 연구의 contribution이 두드러진 부분은 Imitation reward r^I이라고 할 수 있습니다."
  },
  {
    "objectID": "posts/paper/2023-03-12-wasabi.html#induced-imitation-reward-distributions",
    "href": "posts/paper/2023-03-12-wasabi.html#induced-imitation-reward-distributions",
    "title": "📃WASABI 리뷰",
    "section": "Induced Imitation Reward Distributions",
    "text": "Induced Imitation Reward Distributions\n우선 Imitation Reward Distribution이 정말 의미있게 학습을 했는가(Informative한 reward distribution을 만들어 냈는가)를 보기 위해 reward distribution을 시각화해보았습니다. 먼저 Informative한 분포라는 것은 어떤 분포를 말하는가를 짚어볼 필요가 있습니다. 아래 사진의 오른쪽 2개의 분포 그래프에서 평평한 분포(파란색)보다는 뾰족한 분포(초록색)가 여러 x값들에 대해 분별적인 y값(확률)을 가지고 있기 때문에 더 informative하다고 할 수 있습니다.(더 자세한 내용은 정보이론을 살펴보셔도 좋을 것 같습니다.)\n왼쪽의 2개의 그래프는 각각 LSGAN과 WGAN(WASABI)를 가지고 학습했을 때, O의 요소들 중 고정된 pitch rate(\\dot\\theta)와 height(z)를 가지고 Imitation reward 분포를 시각화한 그래프입니다. LSGAN보다 WGAN으로 학습한 분포가 reward range도 더 넓고 더 구분되는 분포를 가지고 있는 것을 볼 수 있습니다. 마지막으로 세번째 그래프는 학습 과정 중에 r^I의 분포를 그린 것으로 LSGAN은 -1과 1, 각각으로 reward targeting을 하게 되는 objective function을 가지고 있었기 때문에 넓고 다양한 reward distribution을 가지지 못한 모습을 볼 수 있고 그에 반해 WGAN은 약 -5~2 정도의 range를 가지는 넓은 reward distribution을 가지고 있는 것을 확인할 수 있었다고 합니다."
  },
  {
    "objectID": "posts/paper/2023-03-12-wasabi.html#learning-to-mimic-rough-demonstrations",
    "href": "posts/paper/2023-03-12-wasabi.html#learning-to-mimic-rough-demonstrations",
    "title": "📃WASABI 리뷰",
    "section": "Learning to Mimic Rough Demonstrations",
    "text": "Learning to Mimic Rough Demonstrations\n그럼 정말로 Demo 모션 데이터들을 얼만큼 잘 따라 학습할 수 있었을까요? 이에 대한 지표는 단순히 reward가 높다고 판단할 수 있는 것이 아니라 모션의 유사성을 판단할 수 있는 다른 metric이 필요합니다.\n\nDynamic Time Warping\nDynamic Time Warping이란 각 데이터의 시간의 길이도 다르고 데이터 포인트의 수도 다른 2개의 시계열 데이터를 비교할 때 사용하는 방법으로 기존의 Euclidean distance라면 측정할 수 없거나 정확한 비교가 어려운 점을 DTW를 이용하면 시간적인 밀림이나 소실된 데이터 포인트까지 고려하여 시계열 데이터 간의 유사도를 판단할 수 있습니다. 바로 이 방법을 이용해서 사람이 들고 만들었던 demo의 모션 데이터와 실제 학습 후 policy에서 만들어낸 모션 데이터 간의 차이를 측정해보았습니다.\n\n\\tau_\\pi는 policy에서 만들어진 trajectory를, \\tau_M은 demo에서 따온 trajectory를 말하며 아래의 실험 결과표는 각각 WASABI와 LSGAN에서의 4 task에 대한 DTW를 구한 값을 나타내고 있습니다. DTW가 낮을수록 demo 데이터와의 유사성이 높은 것이며 잘 모션을 따라 학습했다고 볼 수 있습니다.(아래 Stand Still은 단순히 가만히 서 있는 모션의 데이터와 demo 데이터 간의 DTW 값을 나타낸 것이며 비교를 위한 DTW의 최대 상한선을 나타낸 것으로 볼 수 있습니다.)\n\n\n\nHandcrafted Task Reward\n또 다른 지표로는, 해당 모션 task에 대한 Handcrafted task reward로 점수를 매겼을 때 그 점수가 더 높다면 해당 모션을 잘 학습했다고 판단하는 지표가 있습니다. 예를 들어 STANDUP은 몸체의 pitch angle이 90도에 가깝고 몸체의 높이가 높고 몸체의 z축이 중력방향에 수직이 되는 상태라면 해당 모션을 잘 수행하고 있다고 볼 수 있을 것 입니다. 이처럼 우리가 원하는 모션에 대한 Handcrafted task reward를 계산해서 학습 iteration 마다 그려보면 오른쪽 그림과 같이 WASABI를 가지고 학습한 reward 점수가 대체적으로 LSGAN에 비해 높은 것을 알 수 있습니다. 아래 표에서는 학습을 끝낸 후 각 task에 대한 handcrafted reward 점수이며 맨 아래 점수는 최고 상한 기준 점수로 볼 수 있습니다. 표에서 볼드체로 표시된 부분은 roll-out을 했을 때 모션을 눈으로 확인한 결과 잘 수행했다고 판단한 경우를 나타내면 WASABI로 학습한 4가지 task 모두에서 성공적인 학습 결과를 볼 수 있었다는 것을 볼 수 있습니다."
  },
  {
    "objectID": "posts/paper/2023-03-12-wasabi.html#evaluation-on-real-robot",
    "href": "posts/paper/2023-03-12-wasabi.html#evaluation-on-real-robot",
    "title": "📃WASABI 리뷰",
    "section": "Evaluation on Real Robot",
    "text": "Evaluation on Real Robot\n학습이 시뮬레이션에서만 멈춘다면 당연히 의미가 없는 것이므로 실제 로봇을 가지고 해당 policy의 학습 결과를 확인해봐야 합니다. 따라서 WASABI로 학습한 policy를 가지고 실제 로봇으로 작동을 해보고 이때 10개의 marker를 이용해서 모션 데이터를 얻어 DTW를 측정해보았습니다. 그 결과 표에서 볼 수 있듯이 Sim-to-Real의 퍼포먼스 차이가 거의 없었고 실제 로봇에서도 4가지 task 모두 다 잘 수행하는 것을 확인할 수 있었습니다. 이 부분은 실험영상에서 직접 확인할 수 있습니다.\n\nLeap\n\nWave\n\nStand up\n\nBackflip"
  },
  {
    "objectID": "posts/paper/2023-03-12-wasabi.html#cross-platform-imitation",
    "href": "posts/paper/2023-03-12-wasabi.html#cross-platform-imitation",
    "title": "📃WASABI 리뷰",
    "section": "Cross-platform Imitation",
    "text": "Cross-platform Imitation\n사실 강화학습은 특정 로봇 플랫폼에서 학습한 결과를 다른 configuration을 가진 로봇 플랫폼에 바로 적용하기 어렵습니다. 하지만 WASABI 알고리즘은 처음에 Rough하고 Partial한 demo 데이터를 가지고 학습했기 때문에 다른 로봇 플랫폼에 적용해보는 것이 가능했으며 기존에 Solo 8 로봇 플랫폼을 가지고 학습한 policy를 단순히 로봇 플랫폼의 크기 차이만을 고려하여 base height를 0.25m 조금 더 큰값으로 수정해서 Anymal-C 로봇 플랫폼에 적용했을 때 특별한 추가적인 학습 과정없이도 적용할 수 있었다고 합니다. 이때에도 DTW 값을 찍어서 확인한 결과, 낮은 DTW 값과 함께 시뮬레이션으로 roll-out을 했을 때에 로봇이 잘 작동되는 것을 확인할 수 있었다고 합니다."
  },
  {
    "objectID": "posts/paper/2025-07-23-dextrah-g.html",
    "href": "posts/paper/2025-07-23-dextrah-g.html",
    "title": "📃DextrAH-G 리뷰",
    "section": "",
    "text": "Paper Link\nProject Link"
  },
  {
    "objectID": "posts/paper/2025-07-23-dextrah-g.html#논문의-주요-기여점-요약",
    "href": "posts/paper/2025-07-23-dextrah-g.html#논문의-주요-기여점-요약",
    "title": "📃DextrAH-G 리뷰",
    "section": "2.1 논문의 주요 기여점 요약",
    "text": "2.1 논문의 주요 기여점 요약\n이 논문에서는 DextrAH-G라는 새로운 dexterous 로봇 파지(grasping) 방법을 제안하며, 이는 23자유도 로봇 팔-손이 depth 카메라 입력만으로 다양한 물체를 빠르고 안전하게 파지할 수 있는 픽셀-투-액션 정책이다. 주요 기여사항은 다음과 같다:\n\n벡터화된 기하 패브릭 제어기 도입: RL 정책의 행동 공간(action space)으로 기하 패브릭(geometric fabric) 기반 제어기를 설계하였다. 이를 통해 정책 학습에 강력한 inductive bias을 제공하고, 로봇의 충돌 회피 및 관절 한계 준수를 보장함으로써 로봇 움직임을 안정적이고 자연스럽게 형성할 수 있었다.\n시뮬레이션 기반 교사 정책 학습: 특권 정보(privileged information)를 활용하는 교사 FGP(fabric-guided policy)를 시뮬레이션에서만 강화학습으로 훈련하였다. 이 교사 정책은 앞서 언급한 패브릭 제어기 위에서 동작하여, 다양한 물체들에 대한 고성능 파지 동작을 학습하는 데 성공하였다.\n학생 정책 증류(distillation): 심도 카메라 등의 멀티모달 관찰에 기반한 학생 FGP를 훈련하여, 교사 정책의 행동을 모방함과 동시에 물체의 위치까지 추론(predict)하도록 하였다. 이 온라인 지식 증류 과정을 거친 학생 정책은 교사의 원래 성능을 재현하면서도 실세계 센서 입력으로 동작 가능하게 된다.\n제로샷 실환경 적용: 학습된 학생 정책을 현실 로봇에 별도의 추가 튜닝 없이 바로 적용함으로써, 다양한 새로운 물체들에 대해 최고 수준(state-of-the-art)의 파지 성공률을 달성하였다. 이는 복잡한 형태의 물체도 사람처럼 잡을 수 있는, 이른바 “잡을 수 있는 것은 무엇이든 잡는(grasp-anything)” 능력에 한 걸음 다가선 성과로 평가된다."
  },
  {
    "objectID": "posts/paper/2025-07-23-dextrah-g.html#사용된-기술모델-및-아키텍처-분석",
    "href": "posts/paper/2025-07-23-dextrah-g.html#사용된-기술모델-및-아키텍처-분석",
    "title": "📃DextrAH-G 리뷰",
    "section": "2.2 사용된 기술/모델 및 아키텍처 분석",
    "text": "2.2 사용된 기술/모델 및 아키텍처 분석\nDextrAH-G의 핵심은 세 단계로 구성된 교사-학생 학습 프레임워크와, 이를 뒷받침하는 기하 패브릭 기반 제어 아키텍처이다. 먼저, 기하 패브릭 제어기란 비선형 기하학에 기반한 2차 동역학 제어기로서, 로봇의 움직임에 안전성과 안정성을 내재화한 기술이다. 이 제어기는 로봇 관절의 한계를 자동으로 고려하고, 자가 충돌 및 환경 충돌을 회피하며, 전역적으로 안정적인 경로로 로봇을 움직이게 한다. 또한 고차원 관절계를 저차원 행동 공간으로 노출시켜 제어 문제를 단순화하고, 다중 관절의 여유 자유도(redundancy)도 효율적으로 해소한다. 이러한 속성 덕분에, DextrAH-G에서는 학습된 RL 정책의 출력을 곧바로 로봇 모터에 보내지 않고 패브릭 제어기를 중간 계층으로 사용한다. 정책은 패브릭 상의 목표 동작(예: 손가락 끝 위치나 힘)을 출력하고, 패브릭 제어기가 이를 받아 로봇에게 안전한 실제 관절 명령으로 변환한다. 이는 일반적인 강화학습 정책이 종종 과격한 속도 명령이나 충돌 유발 동작을 내는 문제를 완화하며, 하드웨어 제약을 만족시키도록 명령을 가로채 변환하는 제어 계층을 둔다는 로봇 RL의 모범적인 설계와 일치한다. 특히 NVIDIA 연구진은 이러한 기하 패브릭 제어기를 대규모 병렬 RL 훈련에 사용 가능하도록 벡터화(vectorization)하여, 시뮬레이션 훈련부터 실제 배치까지 동일한 제어 로직을 일관되게 적용할 수 있음을 보였다. 그 결과 RL 정책이 패브릭 제어기와 함께 훈련되므로, 학습 단계에서부터 안전한 동작만 탐색하게 되고 sim2real 차이도 최소화된다.\n\n\n\nDextrAH-G의 학습 과정은 다음의 3단계 파이프라인으로 이루어진다:\n\n교사 정책 학습 (Privileged Teacher FGP): 시뮬레이터 상에서 강화학습(RL)으로 교사 정책을 훈련한다. 이때 비대칭 Actor-Critic 기법을 사용하여, 크리틱(가치망)에는 물체의 정확한 위치 등 모든 상태 정보를 제공하고 액터(정책)에는 로봇의 관절 상태나 노이즈가 추가된 센서 정보 등 제한된 관찰 정보만을 주는 방식으로 학습시킨다. 이를 통해 교사 정책은 현실에서도 이용 가능한 입력만으로 동작하면서도, 시뮬레이션에서는 충분한 정보로 효율적으로 학습할 수 있다. 학습 알고리즘으로는 Proximal Policy Optimization(PPO) 기반의 대규모 병렬 학습을 활용하였으며, LSTM 기반의 순환신경망 정책을 구성하여 부분 관측 상황에서도 메모리를 활용해 의사결정을 할 수 있게 했다. 특히 LSTM 층에는 skip-connection을 추가하여 출력값을 잔차 형태로 처리함으로써 순환신경망 학습의 안정성을 높였다. 또한 시뮬레이션 환경에서는 현실 적응력을 높이기 위해 다양한 도메인 랜덤화가 적용되었다. 예를 들어, 매 학습 에피소드마다 물체에 임의의 힘과 토크를 가해 위치를 흩뜨려보고, 물체 상태와 센서 관측에 노이즈를 추가하여 부분 관측 하에서도 견실한 파지 동작을 학습하도록 했다. 로봇의 물리 파라미터(질량, 마찰계수 등) 역시 범위를 두고 무작위로 변화시켜서 시뮬레이터와 현실 간 차이를 줄였다. 이렇게 훈련된 교사 FGP 정책은 시뮬레이터 내에서 다양한 물체에 대해 높은 성공률로 파지 및 조작을 수행할 수 있게 된다.\n학생 정책 학습 (Depth Student FGP Distillation): 두 번째 단계에서는, 앞서 얻은 교사 정책을 시연자(expert)로 삼아 학생 정책을 학습시킨다. 교사 정책과 동일한 환경에서 온라인 증류(distillation) 방법인 DAgger를 활용하여, 교사가 실행한 행동을 학생이 모방하도록 학습을 진행한다. 학생 정책은 연속적인 심도 영상을 주요 입력으로 받도록 설계되었으며, 약 15Hz 주기로 이미지를 받아들여 그때그때 상황에 반응하는 폐루프 정책을 학습한다. 학생 정책의 관측에는 로봇의 관절 상태 등의 proprioception도 포함되어 있으며, 심도 영상으로부터 물체의 위치를 추정하는 보조 출력도 내도록 멀티태스크 학습시켰다. 즉 학생 정책은 매 시각 심도 카메라 영상과 자기센서 값으로 현재 상황을 파악하고, 다음 순간 로봇 팔과 손가락에 줄 명령을 출력함과 동시에 현재 물체의 예상 위치도 추론하여 보고한다. 이러한 설정은 실제 환경에서 물체가 보이지 않게 되는 가림(occlusion) 상황에서도 proprioception과 과거 정보를 활용해 물체 위치를 끝까지 추적할 수 있도록 하기 위함이다. 한편 학생 정책 학습시에도 시뮬레이션 심도 영상에 랜덤 노이즈, 잡음 객체 등을 추가하여 현실 카메라 화질과 최대한 유사하게 맞추는 기법을 사용하였다. 이 과정을 통해 결과적으로 학생 FGP는 교사 정책에 필적하는 성능을 가지면서도 픽셀 단위의 센서 입력만으로 동작하는 정책으로 거듭난다.\n실세계 배치 (Zero-Shot Deployment): 마지막으로, 이렇게 획득한 학생 정책을 현실 로봇에 이식한다. 시뮬레이션에서 학습된 모델을 별도 추가 학습이나 미세조정 없이 바로 제로샷(sim2real)으로 적용하는 것이 특징이며, 이는 앞서 사용된 기하 패브릭 제어기 덕분에 가능했다. 실제 로봇 하드웨어는 KUKA LBR iiwa 7-자유도 로봇팔에 Allegro 다관절 로봇 손(4 finger, 16 DOF)을 장착한 구성으로, 총 23개의 모터를 가진 플랫폼이다. 테이블 위에 Intel Realsense D415 깊이 카메라 한 대를 고정 설치하여 작업 공간을 내려다보게 했고, 이 단일 카메라 영상 스트림이 정책의 주된 시각 입력이다. 로봇 제어는 ROS 2 기반으로 구현되었으며, 팔과 손에 각각 1kHz와 333Hz 주기의 저수준 PD 제어기가 동작하고 있다. 학습된 FGP 정책 모듈은 별도 노드로 실행되어 15Hz 주기로 카메라 이미지와 로봇 상태를 받아 액션을 출력하며, 기하 패브릭 제어기는 또 다른 노드에서 60Hz 주기로 실행되어 정책이 보낸 액션 명령을 실제 관절 명령으로 변환한다. 이처럼 정책과 패브릭 제어를 모듈화하여 병렬 실행함으로써, 설령 정책 모듈이 일시적으로 지연되거나 비정상 동작하더라도 패브릭 제어기가 로봇 움직임의 안전성을 지속적으로 관리할 수 있게 설계되었다. 결과적으로 DextrAH-G는 현실 환경에서 교사 정책을 모사한 학생 정책 + 패브릭 제어기 조합으로 구동되며, 시뮬레이션에서 보여준 높은 성능을 현실에서도 이어갈 수 있게 된다."
  },
  {
    "objectID": "posts/paper/2025-07-23-dextrah-g.html#실험-설정-및-결과-분석",
    "href": "posts/paper/2025-07-23-dextrah-g.html#실험-설정-및-결과-분석",
    "title": "📃DextrAH-G 리뷰",
    "section": "2.3 실험 설정 및 결과 분석",
    "text": "2.3 실험 설정 및 결과 분석\nDextrAH-G의 성능은 시뮬레이션과 실환경 모두에서 면밀히 평가되었다. 먼저 시뮬레이터상에서 학습된 교사 정책은 140개의 훈련 물체들에 대해 에피소드 기준 99% 이상의 성공률을 보였으며, 개별 물체 기준으로도 평균 80% 수준의 성공률을 달성했다. 학생 정책으로 증류한 후에도 시뮬레이터에서 교사 대비 근소한 성능 저하만 나타났을 뿐 대체로 유사한 파지 성공률을 유지했으며, 이는 sim2real 이전 단계에서 이미 정책 성능이 충분히 확보되었음을 의미한다.\n실세계 평가는 두 가지 프로토콜로 진행되었다. 첫째, 단일 객체 파지 평가(single object grasping)로 표준 벤치마크에 준거한 실험이다. 여러 가지 대표적인 형태의 물체 11종을 선정한 후, 각 물체를 테이블 위에 다섯 번씩 임의의 자세로 배치하고 로봇이 이를 집어 들게 하여 성공 여부를 측정하였다. 만약 첫 시도에 실패해도 로봇이 연속적으로 재시도하도록 하여, 정책의 지속적인 적응 능력도 평가했다. 그 결과 DextrAH-G는 대부분의 물체에서 5회 시도 내 100%에 가까운 성공률을 기록하며, 새로운 최고 성능을 달성했다. 구체적으로, 물체별 평균 성공률이 80%~100% 사이였고, 예를 들어 머그컵이나 과자 상자(직육면체)의 경우 기존 방법(Matak 등)은 0% 성공에 그쳤던 반면 DextrAH-G는 100% 성공을 거두었다는 보고가 있다. 이러한 결과는 DextrAH-G의 견고한 일반화 능력을 보여주는 것으로, 한정된 훈련 세트로 학습했음에도 불구하고 처음 보는 형태의 물체까지 실시간 파지가 가능함을 입증한다. 다만 이 단일 객체 평가는 속도나 연속 작업 상황을 반영하지 못하기 때문에, 논문 저자들은 추가로 연속 작업 평가를 제안하였다.\n둘째 평가로 빈(pack) 채우기 연속 파지 테스트를 수행하였다. 이는 로봇이 다양한 물체들을 연달아 집어서 옆 상자에 옮기는 작업을 지속적으로 수행하도록 하는 시나리오로, 실제 산업용 피킹 작업을 방불케 하는 응용 맥락을 실험에 도입한 것이다. 로봇 앞 테이블에 30여 종류의 다양한 물체를 무작위로 놓고 한 번에 하나씩 집어 들게 한 다음, 잡은 물체를 옆의 빈(bin) 상자에 떨어뜨리면 다시 새로운 물체를 잡는 식으로 실시간 연속 작업을 진행하였다. 이러한 한 사이클(집기-이동-놓기)에 걸리는 평균 시간(cycle time)과 연속 성공 횟수, 그리고 종합 성공률이 성능 지표로 사용되었다. 총 8회에 걸친 연속 테스트 결과, DextrAH-G는 평균 6.56회 연속 성공(한 번 실패하기 전까지 연속 집어 옮긴 물체 수, 95% 신뢰구간 ±2.41)으로 여러 물체를 연속 처리할 수 있음을 보였다. 사이클 타임은 한 사이클당 평균 10.66초(표준편차 ±0.84초)로 측정되었는데, 이는 분당 약 5.63개의 물체를 옮길 수 있는 속도에 해당한다. 총 256번의 집기 시도 중 87%에서 최종적으로 물체를 성공적으로 옮기는 데에 성공하여, 종합 성공률 87%를 기록하였다. DextrAH-G는 이처럼 높은 신뢰도(87% 성공)와 빠른 동작 속도(5.6 PPM)를 동시에 달성함으로써, 현재까지 보고된 덱스터러스 로봇 손 파지 연구 중 가장 뛰어난 속도-정확도 트레이드오프를 달성했다는 평가를 받는다. 저자들은 이러한 신속성과 신뢰성의 조합이 실제 활용에 한 걸음 다가선 성능 지표라고 강조하며, DextrAH-G의 연속 작업 사이클 타임이 이미 실용적 기준에 근접했다고述하고 있다. 참고로 산업공정 분석에 따르면 인간 작업자의 이상적인 피킹 속도는 분당 약 16.5회 정도로 추산되는데, DextrAH-G는 현재 그보다 약간 느리지만(≈5.6회/분) 개선 여지를 충분히 가지고 있으며 가까운 미래에 속도를 더욱 높일 수 있을 것으로 기대된다. 실제로 논문에서는 연속 동작 중에도 로봇에 어떤 손상도 발생하지 않았으며, 수 시간 동안 수백 회에 걸친 테스트에도 하드웨어 고장이나 파손 없이 원활하게 작동했음을 보고하고 있어, 안전성 측면에서도 본 기법의 우수함을 보여주었다."
  },
  {
    "objectID": "posts/paper/2025-07-23-dextrah-g.html#기존-연구와의-비교-및-차별점",
    "href": "posts/paper/2025-07-23-dextrah-g.html#기존-연구와의-비교-및-차별점",
    "title": "📃DextrAH-G 리뷰",
    "section": "2.4 기존 연구와의 비교 및 차별점",
    "text": "2.4 기존 연구와의 비교 및 차별점\nDextrAH-G는 기존의 덱스터러스 그리퍼 연구들과 여러 면에서 차별화된다. 먼저 접근 방법의 차이가 두드러진다. 과거의 전통적인 로봇 파지 기법들은 주로 시각적 인식을 통한 그립 포즈 선정에 집중하고, 선정된 자세로 로봇을 움직이기 위해 모델 예측 제어(MPC)나 경로 계획 알고리즘을 사용하는 계층적 접근이 많았다. 이러한 방식은 개별 동작 단계(탐지-접근-파지)로 분리되어 있어 어느 정도 성과를 보였지만, 실시간으로 연속적으로 반응하지 못하고 일회성 실행에 그치는 경우가 대부분이었다. 예를 들어 팔의 전체 관절 자유도를 모두 활용하여 파지 지점을 향해 유연하게 접근하거나, 파지 후에 손가락과 팔을 조화롭게 재조정하는 등의 능력이 부족했다. 반면 DextrAH-G는 한 번에 끝까지 계획을 세우는 대신, 센서 데이터가 들어올 때마다 15Hz로 계속 재계획 및 제어한다는 점에서 진정한 실시간 폐루프 제어를 구현하였다. 카메라 영상과 로봇 관절 센서 등 모든 정보원을 통합(fuse)하여 고주파수 제어 명령을 생성함으로써, 부분 관측 환경에서 발생하는 불확실성에 대응하고 파지 동작의 성공률을 향상시킨 것이다. 이는 기존 방식과 달리 항상 주변을 감지하고 즉각 피드백을 반영하므로, 물체가 미끄러지거나 움직이더라도 신속히 보정할 수 있다는 강점이 있다.\n학습 기반 기법들의 입력과 모델 의존성 측면에서도 중요한 차이가 존재한다. 기존의 많은 딥러닝/강화학습 기반 덱스터러스 파지 연구들은 심층 센서 데이터의 한계 또는 사전 지식에 대한 의존을 가지고 있었다. 예를 들어, 3D 물체 모델로부터 수십만 건의 합성 파지 데이터셋을 만들고 학습하거나, 물체의 CAD 모델을 미리 알고 있어야만 사용할 수 있는 분석적 지표(예: form closure, force closure 등)를 활용하기도 했다. 일부 최신 연구들은 전체 물체의 포인트클라우드를 입력으로 받아 파지 후보를 생성하고 RL 정책으로 집는 시도를 했으나, 실제 적용 시에는 물체의 정확한 3D 모델을 알아야 하거나 센서 점구름과 그 모델을 정합(registration)해야 하는 어려움이 있어 범용성이 떨어지는 문제가 있었다. 예를 들어 Liu 등(2023)은 손-물체 상호작용을 표현하는 새로운 피처를 제안했지만, 실제로는 물체의 CAD 모델을 센서 데이터에 맞춰 정렬해야 했기 때문에 현실에서는 적용이 제한적이었다고 보고한다. 그 외에도 Agarwal 등(2023)은 이미 학습된 비전 트랜스포머(DINO-ViT) 특징으로 물체를 분류하여 사전 정의된 손 자세(eigengrasp)로 잡는 RL 정책을 제시했으며, Qin 등(2022)은 특정 범주(category)의 물체들에 대해서만 동작하는 포인트클라우드 기반 RL 정책을 선보였다. 이들은 모두 흥미로운 접근이지만, 특정 상황에 특화되어 있거나 시뮬레이션에 한정되는 경우가 많았다. DextrAH-G는 이러한 제약을 대폭 완화하여, 단 한 대의 심도 카메라로부터 얻은 깊이 영상과 로봇의 관절각 등의 proprioception만으로 팔과 손을 동시에 제어하는 일원화된 정책을 학습시켰다. 물체의 CAD 모델이나 사전 촬영 데이터베이스 없이도 동작하며, 한 번 학습되면 카테고리에 상관없이 새로운 물체에도 바로 적용 가능함을 실제로 시연한 것이 큰 차별점이다. 요약하면, DextrAH-G는 센서리얼(sim-to-real) 관점에서 훨씬 간결하고 실용적인 입력만으로 훈련되었음에도 기존 기법들보다 뛰어난 범용 파지 능력을 보여준다.\n제어 및 안전성 측면에서도 DextrAH-G는 기존 연구들과 다른 철학을 취하고 있다. 전통적인 RL 기반 로봇 정책은 주로 조인트 PD 제어기나 OSC(작업공간 제어) 같은 단순 제어기에 명령을 보내는 형태로 작동했다. 이렇게 낮은 수준의 단순 제어를 쓰면 구현은 쉽지만, 로봇 행동의 모든 세부를 학습된 정책이 담당해야 하므로 학습 난이도가 크게 증가하고, 자칫 충돌 회피나 관절 한계 준수같이 중요한 안전 요소들이 정책에 의해 제대로 학습되지 않을 위험이 있다. 실제로 고차원 행동 우주에서 복잡한 우선순위(목표 달성, 하드웨어 보호, 자연스러운 움직임 등)를 신경망 하나로 모두 발견해내는 것은 극히 어렵기 때문에, 정책이 최적화 과정에서 편협한 해법(local optima)에 빠지거나 예상치 못한 부자연스러운 동작을 만들어내기 쉽다. DextrAH-G는 이러한 문제를 해결하기 위해, 제어기 쪽에 많은 지능을 미리 넣어두는 방식을 채택했다. 앞서 설명한 기하 패브릭 제어기가 바로 그런 역할을 하며, 이 고도화된 제어 레이어가 알아서 충돌을 피하고 관절 제약을 지키며 손가락 말단의 움직임을 의미있는 방식으로 유도해준다. 실제 Van Wyk 등(2024)의 선행 연구에서는 이 패브릭 제어기를 활용해 손가락 끝이 물체를 향하도록 자연스럽게 끌어당기는 힘을 적용하고 관절각 제한을 자동 처리함으로써, 복잡한 다관절 손 내 물체 재배열 작업에서 새로운 SOTA 성능을 낸 바 있다. DextrAH-G 역시 이러한 패브릭 가이드 정책(FGP) 접근을 계승하여, RL 정책이 주요 목표 달성에만 집중해도 될 만큼 다른 부수적인 행동들은 제어기가 책임지도록 만들었다. 그 결과 보상 함수를 단순화할 수 있었고(예: 충돌 최소화 등의 보조 보상을 크게 신경쓰지 않아도 됨), 정책 최적화도 수월해졌다. 더 중요한 점은, 패브릭 제어기가 정책의 과격한 출력으로부터 로봇을 보호해주기 때문에 훈련된 정책을 실제 로봇에 적용할 때 안전성이 확보된다는 것이다. 논문에서도 이전 세대 RL 정책들은 실험 중 모터가 과열되고 연기가 나는 고장을 겪기도 했으나, DextrAH-G에서는 패브릭 제어층 덕분에 그런 사태 없이 자유롭게 실험을 반복할 수 있었다고 언급된다. 요컨대, DextrAH-G는 학습과 제어의 긴밀한 통합을 통해 기존 연구들이 직면했던 안전-성능 딜레마를 해결하고자 한 점에서 차별화되며, 이러한 모델 기반 + 학습 혼합 전략은 복잡한 로봇 기술 학습에 있어 한 방향성을 제示하고 있다.\n성능 비교 측면에서도 DextrAH-G의 우수성은 돋보인다. 단일 객체 파지 평가에서 제시된 성공률 수치만 보더라도, DextrAH-G는 기존의 거의 모든 방법들을 능가하는 결과를 얻었다. 예컨대, Dex-_diffuser 기반 생성모델 방법이나 ISA-Grasp, Matak 등의 기존 기법들이 각각 제한된 물체군에서 40~80% 정도의 성공률을 보고한 데 비해, DextrAH-G는 모든 평가 물체에 대해 80% 이상 (대부분 100%)의 성공률을 시현하였다. 연속 파지 시험에서도 87%의 종합 성공률과 5.6 PPM의 속도로 명확한 우위를 보였으며, 논문 저자들은 이러한 신뢰도와 속도의 조합이 현 시점 덱스터러스 파지의 새로운 SOTA임을 강조하고 있다. 요컨대, DextrAH-G는 학습 기반의 범용 파지 정책임에도 불구하고 기존의 특화된 방법들(예: 특정 물체군 전용 RL, 사전 모델 기반 계획 등)을 능가하는 성능과 범용성을 동시에 달성하여 두각을 나타낸다. 또한 sim-to-real 측면에서도, 다른 많은 연구들이 현실 적용을 위해 추가 튜닝이나 도메인 적응 단계를 필요로 한 반면 DextrAH-G는 한 번의 시뮬레이터 학습으로 곧장 현실 로봇에 투입하여 성과를 냈다는 점에서 실용적인 우월성이 있다. 이런 차별점들 덕분에 DextrAH-G는 “픽앤플레이(pick-and-play)”에 가까운 범용 로봇 파지 시스템의 가능성을 보여준 것으로 평가된다."
  },
  {
    "objectID": "posts/paper/2025-07-23-dextrah-g.html#장점과-한계점",
    "href": "posts/paper/2025-07-23-dextrah-g.html#장점과-한계점",
    "title": "📃DextrAH-G 리뷰",
    "section": "2.5 장점과 한계점",
    "text": "2.5 장점과 한계점\n\n2.5.1 장점\n\n탁월한 성능과 범용성: DextrAH-G는 다양한 형태·크기의 물체 30여 종에 대해 87%의 높은 파지 성공률을 달성하며, 현재까지 보고된 덱스터러스 파지 기술 중 최고 수준의 성능을 보여준다. 심도 카메라 한 대만으로 동작하면서도 이전 방법들이 다루기 어려웠던 불특정 다수의 새로운 물체들을 제약 없이 파지해 보임으로써, 향후 범용 로봇 파지(grasp-anything)에 한 발 다가선 성과를 이뤘다.\n빠른 작업 속도: 본 시스템은 한 사이클(집기-운반-복귀)에 평균 10.66초밖에 걸리지 않아 분당 약 5.6회의 연속 집게 작업을 수행할 수 있었다. 이러한 사이클 타임 단축은 다관절 로봇 손으로 이루어지는 파지 작업에서는 매우 고무적인 결과로, 산업적 활용에도 근접한 속도이다. (인간 작업자의 이상적 파지 속도가 약 16.5회/분으로 추정되는 것에 비하면 약 1/3 수준이지만, 현재 로봇 손으로 달성한 속도 중에서는 최고 수준이며 추가 개선의 여지가 있다.)\n안전한 연속 동작: DextrAH-G의 정책 출력은 항상 기하 패브릭 제어기를 거쳐 로봇을 구동하므로, 로봇을 무리한 동작으로부터 보호하는 장치가 내재되어 있다. 실제 여러 시간에 걸친 수백 회의 실험 동안 모터과열이나 기구 손상 없이 시스템을 운영할 수 있었으며, 실험자들이 로봇 파손에 대한 우려 없이 정책을 자유롭게 테스트할 수 있었던 점은 큰 장점이다. 이전 세대의 RL 정책들이 종종 급격한 관절 움직임으로 하드웨어에 부담을 주었던 것과 달리, DextrAH-G는 연속적인 반응 속도를 유지하면서도 충돌 회피와 관절 제한을 준수하여 안정적인 동작을 보장한다.\n학습 효율성과 현실 적용 용이성: DextrAH-G는 전체 학습을 시뮬레이션에서 완료함으로써, 비싼 현실 로봇을 사용한 데이터 수집 없이도 고성능 정책을 얻어냈다. 도메인 랜덤화, 교사-학생 증류 같은 기법을 통해 sim2real 격차를 성공적으로 극복한 덕분에, 추가 실환경 학습 없이도 바로 현실 투입이 가능했다는 점은 실용적인 매력이다. 이는 향후 유사한 로봇 과제들에서도 시뮬레이터 기반 대량 학습 → 현실 즉시 배치의 워크플로를 활용할 수 있다는 희망을 준다. 또한 패브릭 제어기 덕분에 복잡한 보상 설계나 제약 조건을 일일이 벌점으로 학습시키지 않아도 되어 RL 학습 난이도 자체도 완화된 측면이 있다.\n자연스럽고 조화로운 동작: 패브릭 제어기가 로봇의 운동을 글로벌하게 최적화된 경로로 유도해주기 때문에, DextrAH-G의 팔과 손 동작은 비교적 부드럽고 일관된 모션을 보여준다. 기존의 일부 RL 정책들은 목표 달성에는 성공해도 사람이 보기에는 부자연스러운 자세를 취하거나 불안정한 동작을 보이는 경우가 있었는데, DextrAH-G는 안정적이고 사람 손과 유사한 움직임 패턴으로 물체를 잡고 옮겨 로봇 동작의 품질 측면에서도 진전을 이루었다. 이는 패브릭 제어를 통해 자연스러운 손가락 접촉 경로를 형성하고 관절 움직임을 매끄럽게 보간해준 결과로 볼 수 있다.\n\n\n\n2.5.2 한계점\n\n손가락 운동 범위의 제한: 현재 DextrAH-G의 상위 FGP 정책은 손가락 동작을 PCA(taskmap) 좌표계의 주요 축 방향으로만 목표치를 내도록 제한되어 있다. 이는 파지 행동 학습에 집중하기 위한 의도적 설계였으나, 그 대가로 손가락의 세밀한 조작 능력(kinematic dexterity) 일부를 포기한 셈이 된다. 다시 말해 손가락이 만들어낼 수 있는 다양한 모션 중 주성분적인 일부만 사용하므로, 잠재적인 복잡한 조작(skill)의 표현력이 떨어질 수 있다. 향후에는 손가락 운동 자유도를 더 활용하면서도 학습 효율을 유지할 방법이 과제로 남아 있다.\n학습 기반 충돌 회피의 부재: DextrAH-G는 로봇-환경 간 충돌 회피를 주로 패브릭 제어기의 내장 기능에 의존하고 있다. 이상적으로는 정책 자체가 카메라 등 센서 정보를 통해 장애물을 인지하고 회피 행동을 학습하는 것이 바람직하지만, 현재 정책은 이러한 능력이 없다. 그 결과 시뮬레이션 학습 시에도 테이블 면과 가까운 영역 등 높은 비용(risk)의 상태를 적극적으로 탐색하지 못하고 피해가는 경향이 있었다. 실제로 패브릭 제어기가 테이블과의 큰 충돌은 예방해주지만, 너무 보수적으로 회피하다 보니 테이블에 납작하게 놓인 낮은 물체를 잡을 때 어려움을 겪는 현상이 나타났다. 이 부분은 강화학습의 탐색 전략 개선이나 커리큘럼 학습 등을 통해 정책이 어느 정도 충돌을 무릅쓴 탐색도 할 수 있게 유도하는 방향으로 보완이 필요하다.\n탐색 알고리즘 및 RL 안정성: 앞의 문제와 관련하여, 현 정책의 RL 알고리즘이 고비용 영역을 탐색하는 데 한계를 보인다는 지적이 있다. 패브릭 제어기가 없었다면 아마 로봇이 테이블에 부딪히는 등의 시도를 통해 낮은 물체 잡는 법을 배울 수도 있었겠지만, 제어기의 보호막 때문에 애초에 해당 영역을 경험하지 못한 것이다. 이를 해결하려면 더 똑똑한 탐색 기법이나 알고리즘적 개선(예: 안전 탐험, 보상 도메인 개선 등), 또는 일부러 충돌에 가까이 가보도록 유도하는 커리큘럼을 도입하는 등 연구가 필요하다.\n복잡한 환경에서의 활용 한계: 현재 DextrAH-G 시스템은 단일 물체 파지에 초점을 맞추고 있어, 한 장면(scene)에 여러 개의 물체가 섞여 있는 경우에는 대처하기 어렵다. 예를 들어 바닥에 흩어진 여러 물건 중 하나를 집으라고 하면, 어떤 물체를 목표로 할지 인식하는 능력이나 잡고자 하는 대상 이외의 다른 물건을 무시하는 처리가 필요하다. 논문에서도 “하나의 장면에 한 물체만 존재한다”는 가정 하에 정책을 개발했기 때문에, 이를 다수 객체 난잡한 환경(clutter)으로 확장하려면 시각적 세그멘테이션 모듈을 붙이거나 정책을 여러 단계로 구성하는 등의 추가 설계가 필요함을 언급하고 있다. 실제 응용을 위해서는 로봇이 장면 내 목표 물체를 스스로 식별하고 그 물체에만 초점을 맞춰 파지하는 기술과의 통합이 과제로 남는다.\n\n이런 한계에도 불구하고, DextrAH-G는 현재까지 보고된 결과만 놓고 보면 덱스터러스 로봇 파지 분야의 상당한 진일보를 이뤄냈다. 고속·고성공률의 파지, 안전한 실환경 동작, 그리고 범용적인 물체 대응력 등을 모두 갖춘 사례로서, 향후 이 분야 연구와 실제 산업적용에 귀중한 참고가 될 것으로 기대된다. 동시다발적인 여러 연구의 발전 속에서 DextrAH-G가 보여준 교사-학생 학습 + 기하패브릭 제어의 조합은, 향후 고성능 로봇 스킬 학습을 위한 하나의 유망한 방향으로 평가할 만하다. 이번 연구를 바탕으로 남은 한계점들 – 예컨대 복잡한 작업 공간, 더욱 높은 속도 향상, 연성 물체나 촉각 활용 등 – 에 대한 후속 연구가 지속된다면, ‘무엇이든 정확히 빨리 집어내는’ 범용 로봇 손의 실현도 머지않아 보인다."
  },
  {
    "objectID": "posts/paper/2025-07-16-dexctrl.html",
    "href": "posts/paper/2025-07-16-dexctrl.html",
    "title": "📃DexCtrl 리뷰",
    "section": "",
    "text": "Paper Link"
  },
  {
    "objectID": "posts/paper/2025-07-16-dexctrl.html#방법론",
    "href": "posts/paper/2025-07-16-dexctrl.html#방법론",
    "title": "📃DexCtrl 리뷰",
    "section": "3.1 방법론",
    "text": "3.1 방법론\n문제 정의 및 작업 – DexCtrl은 다자유도 로봇 손의 학습된 정책을 현실로 이전할 때 발생하는 제어기 갭(controller gap) 문제를 해결하고자 합니다. 검증을 위해 두 가지 대표 작업을 다룹니다. 첫째는 손가락 끝으로 손바닥 위의 물체를 떨어뜨리지 않고 회전시키는 작업이고, 둘째는 책상 위에 놓인 물체를 집어 들어 뒤집는 작업입니다. 모두 물체-손-환경 간의 복잡한 접촉을 포함하며, 시뮬레이터와 실제 간 제어 파라미터 차이가 성능에 큰 영향을 주는 과제들입니다. 로봇 손은 16자유도의 UC Berkeley LEAP Hand를 사용했고, 관절 토크 제어 방식으로 구동됩니다. 저수준 제어기는 PD 형태로, 원하는 관절 위치 q_d와 현재 관절 위치 q 간 오차에 대해 강성 행렬 K (스프링 상수에 해당)과 감쇠 행렬 D (댐핑 계수)에 비례하는 토크를 출력합니다. 즉, 각 관절에 \\tau = K (q_d - q) + D (\\dot{q}_d - \\dot{q})의 토크를 가합니다. 이때 제어 파라미터 c={K,D}의 선택이 로봇 거동을 크게 좌우하므로 세심한 튜닝이 필요합니다. 예를 들어 강성 K를 높이면 정상 상태 오차는 줄지만 진동이 유발될 수 있고, 감쇠 D를 높이면 오버슈트(overshoot)를 억제하나 고주파 진동을 증폭시킬 수 있습니다. 따라서 기존 정책들은 일반적으로 고정된 K,D로만 동작하고 (적응 제어가 없는 경우), 이 값 세트를 시뮬레이터와 실제에서 맞추기 위해 많은 노력이나 운에 맡긴 랜덤화가 필요했습니다. DexCtrl은 바로 이 부분을 돌파하기 위해 정책이 자체적으로 제어 파라미터를 조정하도록 합니다. 한마디로, “정책이 행동만이 아니라 제어기 설정까지 함께 결정하면 어떨까?”라는 물음에 대한 해결책입니다.\nDexCtrl의 학습 과정 – DexCtrl의 학습은 크게 교사(oracle) 정책 단계와 학생(student) 정책 단계의 두 부분으로 이루어집니다. 먼저 시뮬레이터 상에서 충분한 데이터를 수집하기 위해, 모델 자유 강화학습 (PPO) 알고리즘으로 교사 정책을 훈련합니다. 이 교사 정책은 매 시간-step마다 상태를 받아 관절 행동 a_t (예: 다음 목표 관절 위치)와 제어 파라미터 c_t (예: 해당 step에서 사용할 K,D 값)을 동시에 출력합니다. 곧바로 a_t는 c_t를 파라미터로 하는 PD 제어기에 입력되어 로봇 손가락들의 토크로 변환되고, 로봇을 움직이게 합니다. (이때 a_t는 목표 joint 각도나 상대 위치 등의 형식이며, DexCtrl과 기존 정책 모두 관절 목표 속도는 0으로 설정합니다.) 이렇게 하면 교사 정책은 시뮬레이션 내에서 동시에 최적의 동작궤적과 제어기 게인 조합을 찾아내도록 학습됩니다. 다만 교사 정책은 시뮬레이터의 이점을 활용해 물체의 정확한 상태와 물리 속성(질량, 마찰 등)까지 관측으로 사용합니다. 예컨대 상태 s_t에는 최근 3 스텝에 걸친 로봇 관절 정보 (현재 위치 q, 목표 위치 q_d, 사용한 K,D)와 물체의 포즈 및 물체 속성 벡터 (크기, 무게, 마찰 계수) 등이 포함됩니다. 이러한 특권 정보(privileged info)는 현실에서는 알기 어려우므로, 다음 단계에서 이를 제거하는 작업이 필요합니다.\n학생 정책 학습을 위해, 우선 앞서 훈련된 교사 정책으로 여러 물체에 대한 시뮬레이션 데이터를 충분히 수집합니다. 그런 다음 이 데이터를 활용해 학생 정책을 모사 학습(distillation) 방식으로 훈련하는데, 학생 정책은 두 개의 서브-모델로 구성됩니다. 하나는 행동 예측 모듈로서 로봇의 다음 목표 joint 움직임을 생성하고, 다른 하나는 제어 파라미터 예측 모듈로서 해당 step에 사용할 적절한 K,D 값을 출력합니다. 두 모듈 모두 입력으로 과거의 이력 정보를 사용합니다. 구체적으로, 최근 10 스텝분의 로봇 관절 상태 기록을 활용하는데, 각 스텝마다 로봇의 실제 joint 각도, 그때의 목표 joint 각도, 그리고 그때 사용된 제어 파라미터(K,D)를 묶어 시계열로 제공합니다. 물체의 속성이나 주변 환경 정보는 직접 주어지지 않지만, 이러한 프로프리오셉션 이력 속에 간접적으로 녹아들어 있다는 가정입니다. 예를 들어 무거운 물체를 들고 있었다면 과거 10 스텝 동안 모터 토크 사용량과 운동 패턴에 그 정보가 반영될 것이고, 미끄러운 물체라면 작은 마찰로 인해 나타난 미세한 움직임 차이가 이력에 남을 것입니다. DexCtrl 학생 정책은 이렇게 오로지 로봇 자체 센서 데이터의 시간 이력만으로 물체의 특성까지 추측하며 행동을 결정하도록 설계되었습니다. 이는 현실 환경으로 정책을 옮길 때 추가 센서 없이도 동작 가능하게 하는 중요한 설계입니다. 실제로 저자들은 이전 연구의 결과를 인용하여, 회전 작업 등의 경우 과거 관절 정보만으로도 회전된 물체의 방향 등을 추정할 수 있음을 언급합니다.\n두 모듈의 구조는 시계열 정보를 효과적으로 처리하기 위해 어텐션 메커니즘을 활용합니다. 행동 예측 모듈은 과거 10개의 상태 시퀀스를 입력으로 자기-어텐션(self-attention)을 수행하여, 관절 움직임의 시간적 추이를 파악하고 다음 목표 joint 위치를 출력합니다. 반면 제어 파라미터 예측 모듈은 교차-어텐션(cross-attention) 구조를 갖는데, 이때 현재 생성된 행동 a_t를 쿼리(query)로 삼고, 과거 이력(최근 10 스텝 로봇 상태)을 키/값 (key/value)으로 삼아, 현 행동에 적합한 K,D 게인을 산출합니다. 이렇게 함으로써, 현재 로봇이 수행하려는 동작과 과거의 움직임-제어 맥락을 연관지어 “이번 동작을 안정적으로 수행하려면 제어기를 얼마나 단단하게 혹은 부드럽게 설정해야 하는가?”를 학습하는 것입니다. 저자들은 두 모듈의 입력 이력은 같지만 의미가 다르다고 설명합니다: 행동 모듈은 이력으로부터 궤적의 추세를 읽어내고 (이전 연구들과 유사), 제어 모듈은 이력으로부터 이전 동작-제어기 간 관계를 학습하여 인간이 과거 운전 감으로 현재 차량 제어를 조절하듯이 현재 필요한 제어기 설정을 유추한다고 비유합니다. 또한 모듈을 분리함으로써 제어 파라미터 학습이 행동 생성에 간섭하는 것을 방지하여 학습을 안정화할 수 있었다고 합니다. (만약 하나의 거대한 네트워크가 두 출력을 모두 학습한다면, 제어 파라미터 출력이 제대로 예측되지 못하면 행동 출력까지 혼란을 줄 수 있는데, 이를 구조적으로 차단한 것이죠.)\n학생 정책의 훈련 과정은 시뮬레이터 데이터셋을 활용한 오프라인 지도학습(behavior cloning) 형태로 진행됩니다. 수집한 데이터의 각 시간-step에서 교사 정책이 산출한 a_t, c_t를 목표로, 학생 정책의 두 모듈을 학습시킵니다. 이때 입력으로 사용되는 과거 이력 값들은 교사 정책 데이터셋의 값들을 그대로 사용하므로, 훈련 자체는 오픈-루프(open-loop)로 수행됩니다. (즉, 학생이 내놓은 출력으로 다음 상태를 시뮬레이션하지 않고, 모두 기록된 데이터로만 학습). 대신 학습 후 실제 실행(추론) 시에는 매 시각 실제 로봇의 센서에서 받아온 현재 상태를 이력에 추가하여 폐루프(closed-loop)로 동작합니다. 시뮬레이터와 현실 로봇 사이의 제어 파라미터 단위 차이가 있을 경우를 대비해, 미리 실험적으로 제어기 파라미터 값의 상하한을 비교하여 선형 스케일링해주는 정도의 이식 작업만 거쳤다고 합니다. 별도로 현실에서 미세 튜닝을 하지 않고도 잘 동작하였는데, 이는 교사 정책 학습 시에 무리한 랜덤노이즈 주입 없이도, 학생 정책 훈련 시 현재 관절값에 약간의 가우시안 노이즈만 더하는 것으로 충분했기 때문이라고 저자는 설명합니다. 이러한 결과는, 과도한 도메인 랜덤화 없이도 sim-to-real 이전이 가능함을 보여주는 동시에, 제어기 파라미터 관측을 제공하는 DexCtrl 접근의 이점을 부각합니다. 정리하면, DexCtrl은 시뮬레이터에서 학습한 전문가 정책을 데이터로 활용하여 이력 기반 학생 정책을 만들고, 이 학생 정책이 실행 시 매 순간 행동과 제어기를 모두 조절하도록 함으로써, 시뮬레이션과 현실 환경 사이의 미묘한 차이를 자동으로 메워주는 것입니다.\nBaseline 접근법과의 비교 – DexCtrl과 대비되는 두 가지 주요 baseline은 다음과 같습니다. 첫째, 수동 튜닝(Manual Tuning) 방식입니다. 이는 사람이 많은 시행착오를 거쳐 시뮬레이터와 실제 로봇의 동작 궤적을 비교해가며 PD 제어 게인을 조정하고, 추가로 학습 시 제어 파라미터에 약간의 랜덤 변동을 줘서 얻은 정책입니다. 이렇게 교사 정책부터 현실에 맞춰 새로 학습한 후 학생 정책을 만들어 사용하지만, 정책 자체는 적응형이 아니어서 실행 중에는 고정된 제어 파라미터만을 사용합니다. 이 접근은 특정 작업에 대해 사람이 노하우로 게인을 맞추는 것이기에 일반화가 어렵고, 접촉 조건이 조금만 바뀌어도 성능 보장이 안 되는 문제가 있습니다. 둘째 baseline은 “Ours w/o PD”, 즉 비적응형 DexCtrl입니다. 이는 DexCtrl 학생 정책에서 제어 파라미터 예측 모듈을 제거하고, 제어 파라미터를 위 수동 튜닝과 동일한 값으로 고정시킨 버전입니다. 대신 행동 예측 모듈만을 DexCtrl 방식으로 학습하여, 강인한 행동 궤적은 얻되 제어 파라미터는 조정하지 않는 구조입니다. 이 baseline은 “적응형 PD” 기능이 없는 DexCtrl로 볼 수 있어, 제어기 자동 조정의 효과를 검증하는 용도로 사용됩니다.\nDexCtrl의 방법론은 위 baseline들과 근본적으로 접근이 다릅니다. 수동 튜닝의 경우 사람 전문가의 경험에 의존해 게인을 맞추다 보니 반복 작업과 시행착오가 많고, 한 번 튜닝한 값을 실행 중 동적으로 바꿀 순 없습니다. 학습 시 랜덤화 기법은 정책이 다양한 상황을 버티도록 해주지만, 정작 중요한 특정 힘 조건에서는 최적의 대응을 배우지 못하고 평균적인 (소극적인) 전략에 머물기 쉽습니다. 반면 DexCtrl은 정책이 스스로 필요한 순간에 제어기 특성을 변화시키도록 학습되었기 때문에, 처음 접하는 상황에서도 즉각적으로 대응할 수 있습니다. 또한 관측에 제어 파라미터를 포함시켜 정책이 현재 힘 전달 특성을 인지하도록 했다는 점도 차별점입니다. 한편, DexCtrl과 문제 접근방식은 다르지만 관련된 맥락으로 DexPilot을 언급할 수 있습니다. DexPilot은 사람의 손 동작을 비전 기반으로 추적하여 로봇 손을 원격 조작함으로써 여러 정교한 작업을 수행해낸 체계로, 인간 조종사의 직관과 적응력을 로봇 제어에 활용한 사례입니다. DexPilot 시스템을 통해 얻은 시연 데이터로 향후 자율 정책 학습에 활용하는 방안도 제시되었지만, 실제 조작 시에는 어디까지나 숙련된 사람의 판단과 시각 피드백에 의존합니다. DexCtrl은 이러한 인간의 역할을 정책이 대체하도록 함으로써, 별도의 촉각 센서 없이도 힘 조절을 배우게 했다는 점에서 자율성과 실용성을 높였습니다. 마지막으로, 적응형 힘 제어(adaptive force control) 자체는 로봇 제어 분야에서 익숙한 개념으로, 간단한 접촉 작업(예: 테이블 닦기, 물체 젓히기, 조립 작업 등)에는 기존에도 상황에 따라 제어 게인을 바꾸는 기법이 성과를 보인 바 있습니다. 다만 이런 기법이 다지 손(dexterous hand)의 고난도 조작에 적용된 적은 거의 없었고, DexCtrl은 복잡한 다지 조작에서 적응 제어의 효과를 처음으로 입증했다는 의의를 갖습니다."
  },
  {
    "objectID": "posts/paper/2025-07-16-dexctrl.html#실험-결과",
    "href": "posts/paper/2025-07-16-dexctrl.html#실험-결과",
    "title": "📃DexCtrl 리뷰",
    "section": "3.2 실험 결과",
    "text": "3.2 실험 결과\n실험 설정 – 저자들은 DexCtrl의 성능을 앞서 언급한 두 가지 작업(손바닥 위 물체 회전, 테이블 위 물체 뒤집기)에서 평가하였습니다. 평가 환경은 시뮬레이터와 실제 로봇 모두 사용되었는데, 시뮬레이터에서는 학습과 동일하게 Mujoco 기반 LEAP Hand 모델을 활용했고, 실제로는 동일한 형태의 로봇 손(실제 LEAP Hand)에 학습된 정책을 zero-shot으로 이식하여 시험했습니다. 특히 여러 가지 물체에 대한 일반화 성능을 보기 위해, 무게와 마찰 계수가 서로 다른 다양한 물체들을 사용했습니다. 또한 제어 파라미터 조정의 영향을 세밀히 분석하기 위해, 같은 모양이지만 속을 채워 무게만 바꾼 물체, 표면 재질만 바꿔 마찰계수만 다르게 한 물체 등을 실험에 포함시켰습니다.\n비교를 위해 앞서 설명한 두 baseline (Manual Tuning, Ours w/o PD)을 모두 시뮬레이터에서 훈련하고 현실에 이식하여 테스트했습니다. Manual Tuning의 경우, 시뮬레이터와 실제 로봇의 출력 궤적을 맞추기 위해 세밀히 튜닝된 제어기 파라미터를 사용하고 학습 시에도 소량의 파라미터 랜덤화를 추가한 교사 정책을 새로 훈련한 후, 그로부터 학생 정책을 얻었습니다. 이러한 정책들은 실행 시 적응형 제어기 출력을 내지 않으므로 DexCtrl과 구조만 다르고 일반적인 방식의 정책이라 할 수 있습니다. Ours w/o PD는 DexCtrl의 제어기 예측 부분만 제외한 것으로, 교사 정책은 본래 DexCtrl 교사와 동일하지만 학생 정책이 행동 생성 모듈만 학습된 케이스입니다. 따라서 PD 게인은 고정값이고 (Manual Tuning과 동일한 값 사용), adaptiveness가 제거된 DexCtrl로 볼 수 있습니다.\n평가지표(metrics)로는 네 가지를 사용했습니다. 회전 보상 (RotR)은 물체를 원하는 축으로 얼마나 빠르게 회전시키는지를 나타내며, 시뮬레이션에선 에피소드 동안의 평균 회전 속도 보상으로, 실제에선 에피소드 종료 시 물체가 회전한 라디안 각도로 측정했습니다. 실패까지 걸린 시간 (TTF, Time to Fail)은 물체를 손에서 떨어뜨리거나 (회전 작업), 처음 위치에서 너무 벗어나 버리는 (뒤집기 작업) 시점까지 지속된 시간 스텝 수의 평균입니다. 값이 클수록 물체를 오래 안정적으로 다루었음을 의미합니다. 이 외에 물체의 불규칙 속도 (ObjVel)나 토크 페널티 등도 측정했는데, ObjVel은 물체가 불필요하게 흔들리는 정도(행동 스텝당 평균 속도)로 안정성을 나타내며 시뮬레이터에서만 계산했고, 토크 페널티는 에너지 소모량 관련 지표입니다. 실험은 시뮬레이션 평가와 실제 로봇 평가로 나뉘며, 시뮬레이션에서는 추가로 물체에 무작위 외란 힘(disturbance)을 가하는 상황과 가하지 않는 상황을 비교하여, 예기치 않은 힘 교란에 대한 견고성도 확인했습니다.\n시뮬레이션 성능 – 먼저 시뮬레이터 상에서 DexCtrl과 baseline들을 비교한 결과, DexCtrl가 모든 지표에서 우수한 성능을 보였습니다. Table 1 (회전 작업)과 Table 2 (뒤집기 작업)은 시뮬레이션 환경에서의 정량적 성능을 보여주는데, DexCtrl (Ours) 정책은 수동 튜닝 baseline보다 일관되게 높은 회전 속도(RotR)와 긴 실패 시간(TTF)을 기록했습니다. 예를 들어 손바닥 위 물체 회전의 경우, 외란 힘이 주어졌을 때 DexCtrl의 평균 회전 보상은 baseline보다 약 24% 높았고(35.05→43.51), 물체를 떨어뜨리기까지 버틴 시간도 늘어났습니다(239.4→255.9). 외란이 없을 때는 차이가 더 커져, DexCtrl의 회전 속도가 baseline보다 약 39% 향상되었습니다(37.64→52.33). 이는 제어 파라미터가 동일한 조건에서도 행동+제어 동시 학습이 단순 행동 학습보다 효과적으로 작업을 가속화하고 안정화시켰음을 의미합니다. 흥미롭게도 DexCtrl에서 PD 적응만 뺀 버전(Ours w/o PD)도 수동 튜닝보다는 높은 성능을 보였는데, 특히 회전 작업에서 baseline보다 빠른 회전(52.33 대비 47.87)을 달성했습니다. 저자들은 DexCtrl 방식으로 생성된 궤적 자체가 견고하기 때문이라고 분석합니다. 다만 접촉이 격렬한 작업인 뒤집기의 경우, 비적응형 DexCtrl(Ours w/o PD)은 baseline보다 나은 성능을 내지 못했습니다. Table 2를 보면, 외란이 있을 때 뒤집기 작업의 RotR은 수동 튜닝이 91.07, Ours w/o PD가 82.23로 오히려 낮았습니다. 반면 DexCtrl은 172.5로 두 배 가까이 높았습니다. 외란이 없을 때도 비슷한 경향이어서, 결과적으로 뒤집기 같이 난이도 높은 접촉 작업에서는 실시간으로 제어기를 조정해주는 DexCtrl만이 확실한 성능 개선을 이루었다고 볼 수 있습니다. 이는 바닥-손의 복합 접촉 상황에서, 고정 제어기로는 한계가 있지만 adaptive 제어를 통해 힘 분배를 조절하면 훨씬 유리함을 보여줍니다. 추가로, DexCtrl은 학습 속도 측면에서도 우위를 보여주었는데, 동일 조건에서 학생 정책을 학습시킬 때 baseline보다 빠르게 수렴하여 효율적이었다고 합니다. 요컨대 시뮬레이터 결과만 놓고 봐도 DexCtrl은 기존 방법 대비 작업 성공률과 안정성을 크게 향상시켰습니다.\n현실 세계 성능 – 더 중요하게, 학습된 정책들을 실제 로봇 손에 이식하여 테스트한 결과, DexCtrl의 우수성은 현실에서 더욱 두드러졌습니다. 손바닥 위 물체 회전 작업의 경우, 질량과 마찰이 서로 다른 12종의 물체에 대해 각 정책을 한 번도 추가 훈련하지 않고 바로 적용해 비교했습니다. 그 결과 DexCtrl은 어떤 물체에 대해서도 일관되게 가장 높은 회전 성능을 보여주었고, baseline들과의 격차가 시뮬레이터에서보다 더 크게 벌어졌습니다. Table 3은 예시로 몇 가지 물체에 대한 수치를 보여주는데, 가벼운 물체(예: 94g 큐브)에서는 수동 튜닝의 RotR이 1.963인 반면 DexCtrl은 9.386으로 약 4.8배 높았습니다. 무거운 물체(221g 사과 장난감)에서는 baseline이 1.914인 데 반해 DexCtrl은 9.676으로 5배 이상 차이가 났습니다. 거의 모든 물체에서 DexCtrl이 몇 배씩 빠르게 회전시켰으며, 이는 zero-shot sim-to-real 설정임을 감안할 때 놀라운 일반화 성능입니다. 안정성 측면에서도 DexCtrl이 뛰어났는데, 실패까지의 시간(TTF)을 보면 DexCtrl은 여러 물체에서 평균적으로 에피소드 최대 시간(300 스텝)에 육박했습니다. 이는 DexCtrl이 대부분 물체를 끝까지 떨어뜨리지 않고 잘 잡고 있었다는 뜻입니다. 반면 baseline들의 TTF는 조금 낮아서, 예를 들어 무거운 물체의 경우 수동 튜닝은 평균 243 스텝 정도에 물체를 놓쳤지만 DexCtrl은 300 스텝 내내 성공적으로 잡고 회전시켰습니다. 전체적으로 현실 환경에서 DexCtrl과 다른 방법들의 격차는 시뮬레이터보다 컸는데, 이는 시뮬레이터와 실제 사이의 미세한 차이를 DexCtrl만이 적응적으로 보완한 결과입니다. 특히 DexCtrl과 비적응형 DexCtrl(Ours w/o PD)의 차이가 현실에서 크게 벌어졌는데, 이는 매 시각 제어 파라미터를 조정해주는 기능이 실제 로봇에서 매우 중요함을 강조해줍니다. 즉, 시뮬레이터에서는 둘 다 일정 성능을 냈어도, 실제에서는 Ours w/o PD가 감당 못 하는 힘 변동을 DexCtrl은 실시간 대응하며 극복한 것입니다. 한편, 테이블 위 물체 뒤집기 작업도 실제로 시험되었고, Figure 3에 그 결과가 시각적으로 제시되었습니다. DexCtrl 정책은 실제에서도 물체를 들어올려 뒤집는 동작을 성공적으로 수행했고, 반복 실험에서 높은 성공률을 보였습니다. 바닥과의 충돌이 있는 이 복잡한 상황에서도 adaptive 제어를 통해 손가락의 충격 흡수와 힘 분배가 잘 이루어져, baseline에 비해 부드럽고 안정적인 뒤집기 동작을 구현했습니다. 이로써 DexCtrl의 접근이 한 가지 작업에 특화된 게 아니라 다양한 조작 작업에 일반화 가능함도 확인되었습니다. Figure 4는 실제 로봇으로 여러 가지 물체를 회전시키는 장면들을 보여주는데, DexCtrl 정책이 물체마다 알맞은 힘으로 잡고 회전시키는 모습을 볼 수 있습니다. 반면 고정 제어기의 baseline은 어떤 물체는 힘이 부족해 떨어뜨리고, 어떤 경우는 지나치게 세게 조여 불안정한 움직임을 보이는 등 일관성이 떨어지는 모습이 관찰되었습니다.\n물체 물성 변화에 대한 적응력 분석 – DexCtrl의 뛰어난 성능이 어떤 원리로 나오는가를 알아보기 위해, 저자들은 특히 질량과 마찰이라는 물체 물성 변수에 대한 정책의 반응을 상세히 분석했습니다. 앞서 언급했듯, 동일한 모양의 빈 플라스틱 상자에 내부 내용물을 추가해 무게를 가볍게/무겁게 바꾼 경우와, 동일한 무게 상자에 표면 마찰 코팅만 달리한 마찰 계수 변화 경우를 실험했습니다.\n그림 – 물체의 질량(왼쪽) 및 마찰 계수(오른쪽) 변화에 따른 회전 성능 비교. 그래프의 세 곡선은 파란색 수동 튜닝 baseline, 녹색 비적응형(Ours w/o PD), 자주색 DexCtrl을 나타냅니다. 왼쪽 그림은 가벼운 물체부터 무거운 물체로 질량이 증가할 때 세 방법의 회전 속도(RotR)를 보여주고, 오른쪽은 마찰 계수가 낮은 물체부터 높은 물체로 바뀔 때의 성능 변화를 나타냅니다. 결과를 보면 DexCtrl (자주색)은 모든 조건에서 가장 높은 회전 속도를 유지하며, 물체가 무거워지거나 마찰이 변화해도 성능 저하 폭이 매우 작다는 것을 알 수 있습니다. 예를 들어 질량이 가벼울 때는 세 방법 모두 비교적 높은 속도를 내지만, 무거운 물체로 갈수록 baseline들의 성능은 급격히 떨어지는 반면 DexCtrl은 상당한 속도를 유지하고 있습니다. 가장 무거운 경우를 보면, 수동 튜닝(파란선)은 회전 속도가 거의 0에 수렴할 정도로 실패하지만 DexCtrl(자주선)은 여전히 높은 속도로 물체를 돌릴 수 있습니다. 마찰 계수 변화에 대해서는, DexCtrl은 마찰이 작든 크든 안정적으로 높은 성능을 보이는 반면, baseline들은 중간 마찰 영역 등에서 성능이 뚝 떨어지는 등 들쑥날쑥한 양상을 보입니다. 수동 튜닝(파란선)은 마찰계수가 중간일 때 최저 성능을 내고, 비적응형(녹색)은 마찰이 작을 때 약간 성능이 높았다가 큰 마찰에서 다시 조금 낮아지는 등, 한 가지 추세로 설명하기 어려운 변화를 보입니다. 이는 고정 제어기에서는 마찰 조건에 따라 상당히 다른 거동을 보여 정책이 일관성 있게 대응하지 못함을 뜻합니다. 반면 DexCtrl은 전체적으로 높은 성능을 유지하며 약간의 변화만 보일 뿐입니다. 종합하면 DexCtrl 정책은 물체의 물리적 특성이 바뀌어도 알아서 제어 전략을 조정하여 성능을 유지하는 반면, baseline들은 특정 조건에서 크게 무너지는 모습을 보였습니다. 실제 데이터에서도 DexCtrl이 무거운 물체일수록 실패 없이 오래 버티고 (TTF 상승) 가벼운 물체일수록 빠르게 돌리지만 불안정성은 조금 있는(회전 속도 높지만 ObjVel 약간 상승) 경향 등을 보여, 힘 조절을 통해 트레이드오프를 잘 관리하고 있음을 알 수 있었습니다. 이는 사람이 무거운 물체를 들 때 힘을 더 주고, 미끄러운 물체를 다룰 때 주의를 기울이는 것과 유사한 적응력이라 할 수 있습니다.\n학습된 제어 파라미터의 패턴 – 마지막으로, DexCtrl이 구체적으로 어떻게 제어기 파라미터를 조절하고 있는지를 분석한 결과를 소개합니다. 저자들은 특히 제어 파라미터 중 강성(stiffness, K) 값에 주목했는데, 이유는 강성이 감쇠보다 작업 성능에 더 큰 영향을 주었기 때문이라고 합니다. 강성을 높이는 것은 곧 각 관절에 더 큰 토크를 가하는 효과를 내어 접촉 force 프로파일을 바꾸므로, 물체의 무게나 마찰에 따라 강성 조절 패턴이 달라질 것으로 예측했습니다. 이를 검증하기 위해 시뮬레이터에서 다양한 물체에 DexCtrl 정책을 적용한 trajactory 데이터를 모아 강성 값 변화를 분석했습니다. Figure 6의 좌측 그래프는 물체 질량에 따른 평균 강성 변화를 보여주는데, 결과는 물체가 무거워질수록 강성을 높이는 방향으로 정책이 학습되었음을 명확히 보여줍니다. Light (가벼움) → Heavy (무거움)으로 갈수록 평균 강성이 단조 증가하였는데, 이는 무거운 물체를 들고 조작하려면 관절에 더 큰 힘(토크)이 필요하므로 강성을 키운다는 직관적인 원리에 부합합니다. 한편, Figure 6의 중간 및 우측 그래프는 마찰 계수에 따른 강성 변화를 나타냅니다. 여기서는 흥미롭게도 일부 관절은 마찰이 증가할수록 강성이 증가하지만, 다른 관절은 마찰이 증가할수록 오히려 강성이 감소하는 양상이 관찰되었습니다. 저자들은 이를 작업 및 관절별 역학(dynamic)의 차이로 해석했습니다. 예를 들어 회전 작업에서 마찰이 큰 경우 어떤 관절들(예: 물체를 직접 비틀어 돌리는 역할의 관절)에는 움직임에 저항이 커지므로 더 큰 토크, 즉 높은 강성이 필요할 수 있습니다. 반면 다른 관절들(예: 물체를 지지하는 역할의 관절)에서는 마찰이 클 때 오히려 물체가 잘 미끄러지지 않아 조금 힘을 빼도 제어가 가능하고, 너무 강하게 잡으면 불필요한 힘 전달로 불안정해질 수 있습니다. 따라서 관절마다 마찰이 미치는 역할이 달라서, 어떤 관절은 마찰↑ → 강성↑, 다른 관절은 마찰↑ → 강성↓ 패턴을 보인다는 것입니다. 이러한 비일관적인 강성 조정 양상은 처음엔 이상해 보일 수 있으나, DexCtrl 정책이 관절별로 최적의 힘 분배를 학습한 결과라고 이해할 수 있습니다.\n위와 같은 경향은 실제 로봇 실험 데이터를 통해서도 확인되었습니다. Figure 7에서는 서로 다른 물체들에 대해 시간에 따른 강성 변화를 시각화하였는데, 두 가지 뚜렷한 패턴이 발견됩니다. (1) 무거운 물체의 경우 몇몇 중요한 관절에서 초기에 강성을 확 올리거나 최대값으로 유지하는 구간이 길어졌다는 점, (2) 표면이 매끄러운(낮은 마찰) 물체의 경우 어떤 관절들은 강성을 크게 주었다가 곧 낮추는 반면, 다른 관절들은 반대로 천천히 높이는 등 엇갈린 패턴이 보였다는 점입니다. 이 역시 관절/작업별로 필요한 힘 조절 전략이 다름을 반영하며, 종합하면 DexCtrl의 정책이 물체의 무게와 마찰에 따라 필요 접촉력을 판단하고 그에 맞게 강성을 체계적으로 조절하고 있음을 증명합니다. 이러한 학습된 강성 조절은 결과적으로 각 상황에서 최적의 힘으로 물체를 다루게 해 주어 조작 성능 향상으로 이어진다고 저자들은 결론짓고 있습니다. 요컨대 DexCtrl의 내부를 들여다본 결과, “무거운 물체는 세게 잡아라, 마찰은 상황에 맞게 힘을 재분배하라”라는 인간 전문가의 암묵적 지식을 스스로 터득한 셈입니다."
  },
  {
    "objectID": "posts/paper/2025-07-16-dexctrl.html#결론",
    "href": "posts/paper/2025-07-16-dexctrl.html#결론",
    "title": "📃DexCtrl 리뷰",
    "section": "3.3 결론",
    "text": "3.3 결론\n시뮬레이션에서 학습한 로봇 손의 정교한 조작 기술을 실제 환경에 성공적으로 이식하기 위해서는 넘어야 할 장벽이 많습니다. DexCtrl은 그 중에서도 특히 로봇 제어기의 적절한 파라미터 선택이라는 문제에 주목하여, 이를 학습을 통해 자동화한 최초의 시도입니다. 정책이 매 순간 제어기 게인을 함께 출력하도록 설계함으로써, 시뮬레이터와 실제 로봇 사이의 미묘한 동작 차이를 정책 자체가 메꾸도록 했습니다. 이 방법은 사람이 일일이 파라미터를 맞추는 번거로움 없이도, 접촉력에 대한 추론과 적응적 힘 조절을 가능하게 했고, 그 결과 복잡한 물체 조작 작업에서 획기적인 성능 향상을 이루었습니다. 시뮬레이션과 현실 실험 모두에서 DexCtrl은 기존 방법들을 크게 능가하는 성공률과 안정성을 보여주었으며, 다양한 물체 물성 변화에도 높은 범용성(generalizability)을 입증했습니다. 특히 zero-shot 현실 적용 시에도 여러 물체를 문제없이 다루는 모습은, 향후 로봇이 학습된 기술을 실세계에 직접 활용하는 데 한 걸음 더 다가선 성과로 평가됩니다.\n물론 한계와 향후 과제도 존재합니다. 현재 DexCtrl은 로봇의 고유 감각(proprioception) 정보만 활용하고, 촉각/힘 센서는 사용하지 않았습니다. 하드웨어 한계로 실험에 포함하지 못했지만, 접촉이 많은 실제 시나리오에선 이러한 실시간 힘 센싱이 있다면 더욱 정교한 제어가 가능할 것입니다. 미래에는 고성능 촉각 센서를 통합하고, 나아가 하드웨어가 지원된다면 실시간으로 정책을 미세 조정하는 온라인 학습까지 도입하여 (예컨대 실제 수행 중 약간의 추가 PPO 파인튜닝 등) 적응 제어를 한층 고도화할 수 있을 것으로 기대됩니다. 또한 본 연구는 하나의 작업에 특화된 두 모듈로 정책을 훈련했는데, 이는 각각의 작업에 별도 학습이 필요함을 의미합니다. 이를 개선하기 위해 여러 가지 조작 작업에 대해 공용의 제어 파라미터 예측 모듈을 갖도록 정책을 확장하는 연구도 고려되고 있습니다. 궁극적으로는 다양한 로봇 손 플랫폼에도 본 방법을 적용해보는 것이 목표인데, Leap Hand 이외의 다른 손에 일반화되는지도 향후 검증할 계획입니다.\nDexCtrl이 보여준 적응형 제어 학습 개념은, 향후 인간 수준의 섬세한 로봇 조작을 실현하는 데 중요한 단서를 제공합니다. 사람은 물체를 다룰 때 힘 조절을 무의식적으로 잘 하는데, 이제 로봇도 학습을 통해 그러한 능력을 갖출 수 있음을 증명한 것입니다. 복잡한 접촉이 존재하는 환경에서도 안정적이면서 민첩한(dexterous yet robust) 로봇 제어가 가능함을 보인 DexCtrl의 성과는, 향후 sim-to-real 문제를 넘어 실제 산업 및 일상에서 다재다능한 로봇 손을 구현하는 데 기여할 것으로 기대됩니다. 이번 연구는 강화학습 정책과 고전 제어의 만남을 창의적으로 이루어낸 사례로서, 로보틱스 연구자들에게 시사하는 바가 크며, 추후 다양한 확장과 활용이 뒤따를 것으로 보입니다.\n참고 문헌\n\nShuqi Zhao et al., “DexCtrl: Towards Sim-to-Real Dexterity with Adaptive Controller Learning,” arXiv preprint arXiv:2505.00991, 2025.\nAnkur Handa et al., “DexPilot: Vision Based Teleoperation of Dexterous Robotic Hand-Arm System,” ICRA 2020, arXiv:1910.03135."
  },
  {
    "objectID": "posts/paper/2025-08-12-dexart.html",
    "href": "posts/paper/2025-08-12-dexart.html",
    "title": "📃DexArt 리뷰",
    "section": "",
    "text": "Paper Link"
  },
  {
    "objectID": "posts/paper/2025-08-12-dexart.html#배경-로봇-섬세-조작과-관절-객체의-도전",
    "href": "posts/paper/2025-08-12-dexart.html#배경-로봇-섬세-조작과-관절-객체의-도전",
    "title": "📃DexArt 리뷰",
    "section": "배경: 로봇 섬세 조작과 관절 객체의 도전",
    "text": "배경: 로봇 섬세 조작과 관절 객체의 도전\n가정에서 인간이 다루는 일상 물체들의 상당수는 관절형 객체(articulated objects)입니다. 문손잡이, 수납장 도어, 수도꼭지, 뚜껑 등은 모두 하나 이상의 회전 또는 슬라이딩 관절을 지니고 있어, 물체 일부를 움직여야 전체 기능을 사용할 수 있습니다. 이러한 물체를 사람처럼 능숙하게 다루는 능력은 범용 가정용 로봇에게 필수적입니다. 그러나 로봇에게 인간 수준의 섬세 조작(dexterous manipulation) 기술을 학습시키는 일은 쉽지 않습니다. 기존 로봇 팔은 주로 두 손가락 집게 형태의 패러럴 그리퍼(parallel gripper)를 사용해왔는데, 이는 구조가 단순한 대신 취급할 수 있는 물체 형태에 제약이 크다는 한계가 있습니다. 예를 들어, 집게형 그리퍼로는 양동이의 손잡이를 들어올리거나 불규칙한 모양의 뚜껑을 여는 등 복잡한 형상의 물체를 다루기가 어렵습니다. 반면 다섯 손가락을 갖춘 인간형 로봇 손을 사용하면 인간의 손 동작을 보다 가깝게 모방할 수 있어 훨씬 다양한 물체를 조작할 수 있을 것으로 기대됩니다. 물론 다관절 로봇 손 자체가 갖는 높은 자유도(DoF) 때문에 제어가 복잡해지며, 거기에 더해 관절로 연결된 물체의 동적인 움직임까지 다뤄야 하므로 상태공간과 행동공간이 폭발적으로 늘어나 학습이 매우 어려운 문제가 됩니다. 사실 최근까지도 강화학습 등을 통해 로봇 손의 섬세 조작을 다루는 연구들은 주로 공 하나를 쥐고 흔들기, 손바닥 위에서 물체 돌리기 등의 단일 강체 물체에 국한되는 경우가 많았습니다. 다양한 관절형 도구들을 다루는 범용 스킬로 확장하려면, 시각적인 인식과 복잡한 제어 사이의 통합적 접근이 필요하며 이에 따른 새로운 연구 난제가 제기됩니다."
  },
  {
    "objectID": "posts/paper/2025-08-12-dexart.html#기존-벤치마크의-한계",
    "href": "posts/paper/2025-08-12-dexart.html#기존-벤치마크의-한계",
    "title": "📃DexArt 리뷰",
    "section": "기존 벤치마크의 한계",
    "text": "기존 벤치마크의 한계\n강화학습 및 로봇 제어 연구의 발전을 위해 여러 로봇 조작 벤치마크가 제안되어 왔습니다. 예를 들어, MetaWorld 벤치마크는 50개가 넘는 다양한 작업(task) 환경을 제공하여 강화학습 알고리즘을 평가할 수 있도록 합니다. 그러나 MetaWorld의 각 작업은 하나의 특정 물체만을 대상으로 설계되어 있어, 동일 작업에서 새로운 물체로의 일반화(generalization)를 측정하기 어렵다는 한계가 있습니다. 이러한 일반화 능력을 다루기 위해 나온 ManiSkill 벤치마크는 여러 유형의 조작 과제와 함께 과제당 다수의 객체 모델을 포함시켜, 한 정책이 여러 객체 인스턴스에 걸쳐 작동하도록 평가합니다. 이는 로봇의 범용성을 높이는 방향으로 고무적이었지만, ManiSkill이 취한 패러럴 그리퍼 사용이라는 전제에는 근본적인 제약이 있습니다. 두 손가락 그리퍼만으로는 로봇이 할 수 있는 조작의 종류와 방식이 제한되기 때문에, 양동이 손잡이를 드는 등 멀티핑거 손이 요구되는 섬세 작업을 다루지 못합니다. 요컨대 기존 벤치마크들은 일반화 문제를 충분히 고려하지 않거나, 로봇 손의 능력이 제한되어 실제 인간 수준의 다양한 작업을 포괄하지 못했습니다. 이로 인해 관절형 객체를 사람처럼 자유롭게 다루는 범용적이고 섬세한 조작 기술을 평가하고 향상시키는 데에는 여전히 적절한 평가 환경이 부족했습니다."
  },
  {
    "objectID": "posts/paper/2025-08-12-dexart.html#dexart-벤치마크-구성과-특징",
    "href": "posts/paper/2025-08-12-dexart.html#dexart-벤치마크-구성과-특징",
    "title": "📃DexArt 리뷰",
    "section": "DexArt 벤치마크: 구성과 특징",
    "text": "DexArt 벤치마크: 구성과 특징\n이러한 한계를 해결하고자 2023년 CVPR에서 발표된 DexArt 벤치마크는, 다관절 로봇 손을 이용해 다양한 관절형 물체를 조작하는 새로운 평가 표준을 제시했습니다. DexArt는 물리 시뮬레이터 상에서 정의된 여러 개의 조작 과제(task)들로 이루어져 있으며, 각 과제마다 훈련용으로 미리 본 객체(seen)와 테스트용으로 처음 보는 객체(unseen)의 구분을 통해 정책의 일반화 성능을 엄밀히 평가할 수 있게 합니다. 이번 벤치마크에서는 대표적인 4가지 조작 과제가 선정되었는데, 구체적인 내용은 다음과 같습니다:\n\n수도꼭지 돌리기 (Faucet): 로봇 손으로 수도꼭지의 손잡이를 잡고 약 90도 정도 회전시켜 물을 트는 동작입니다. 이 작업에서는 로봇 팔의 움직임과 다관절 손가락의 협응이 모두 필요하며, 성공 여부는 수도꼭지 손잡이를 돌린 각도로 평가합니다. (이 과제에는 18개의 수도꼭지 모델 중 11개를 훈련에 사용하고 7개로 일반화 성능을 평가합니다.)\n양동이 들기 (Bucket): 로봇 손을 양동이의 좁은 손잡이 밑으로 집어넣어 손잡이를 아래에서 들어올려 드는 작업입니다. 로봇 손가락을 펼쳐 손잡이를 아래에서 받치고 쥐는 형상 폐쇄(form closure)를 구현해야 안정적으로 들 수 있으며, 충분한 마찰 없이는 이 작업이 어렵습니다. 평가 기준은 양동이를 일정 높이 이상 들어올렸는지로 정의됩니다. (이 작업에는 총 19개 양동이 모델 중 11개를 훈련에, 8개를 테스트에 사용합니다.)\n노트북 열기 (Laptop): 닫힌 노트북의 화면을 중간 부분에서 손가락들로 집어들어 노트북을 여는 작업입니다. 로봇 손가락으로 화면 판을 잡고 젖혀 올리는 동작으로, 평행 그리퍼로도 시도할 수는 있으나 매우 정확한 위치로 파지를 해야 하고 큰 작업 공간이 필요해 어렵습니다. 다관절 손을 활용하면 보다 유연하게 화면을 파지하고 젖힐 수 있으며, 열림 각도의 변화를 가지고 성공을 측정합니다. (노트북 과제에는 17개 모델 중 11개 훈련, 6개 테스트로 사용됩니다.)\n변기 뚜껑 열기 (Toilet): 노트북 열기와 유사하게, 닫힌 양변기 뚜껑을 들어올려 여는 작업입니다. 변기 뚜껑은 모양이 크고 불규칙하여 화면이 평평한 노트북보다 파지가 까다롭고 무거운 경우가 많아 작업 난이도가 높습니다. 로봇이 뚜껑을 일정 각도 이상 열면 성공으로 간주하며, 이 작업을 통해 보다 복잡한 형상의 관절물체 조작에 대한 성능을 평가합니다. (변기 뚜껑은 28개 모델 중 17개 훈련, 11개가 테스트에 사용됩니다.)\n\n각 과제마다 17~28개의 다양한 객체 모델이 포함되어 있으며, 이 중 약 60%는 훈련에 사용되고 40%는 한 번도 본 적 없는 새로운 객체로 분리되어 성능 평가에 활용됩니다. 이러한 구성으로, 예를 들어 로봇이 수도꼭지 과제에서 11개의 수도꼭지 모델을 보고 학습한 후 처음 보는 7가지 새로운 디자인의 수도꼭지를 얼마나 성공적으로 틀 수 있는지를 시험함으로써, 범주 수준(category-level)의 일반화 능력을 객관적으로 측정할 수 있습니다.\nDexArt의 모든 작업 환경은 SAPIEN 물리 시뮬레이터 상에 구현되었고, 로봇 플랫폼으로는 6자유도 산업용 로봇팔 XArm6 끝에 인간 손과 비슷한 4손가락 알레그로 로봇 손(Allegro Hand)(16자유도 손가락 관절)을 장착한 형태를 사용합니다. 로봇이 받는 관측(observation) 정보는 두 부분으로 구성되는데, 첫째는 로봇 자체의 관절 각도, 속도, 팔 끝단(손바닥)의 위치와 자세 등의 프리오프리셉션(proprioception)이고, 둘째는 장면을 보는 깊이 카메라로부터 획득한 부분적인 포인트클라우드입니다. 이 3차원 점군은 로봇 팔과 물체가 놓인 작업공간 주위로 한정하고 다운샘플링하여 사용하며, 거기에 로봇 자신의 모델에서 생성한 점군도 합쳐 입력으로 제공합니다. 이러한 3D 시각 정보는 PointNet 신경망으로 처리되어 중요한 물체의 형태와 부위 특성을 인코딩하고, 이를 기반으로 정책이 의사결정을 하게 됩니다. 한편 행동(action) 공간은 로봇 팔의 6-DoF 속도 제어 지령과 손가락 16개 관절의 목표 위치로 이루어진 22차원 연속 벡터로 정의됩니다. 구체적으로 팔은 Operational Space Control 방식으로 손바닥의 선속도 및 각속도를 명령하고, 손가락은 각 관절의 목표 각도(position control)를 PD 제어로 수행하는 형태입니다. 이러한 정의를 통해 로봇은 자유롭게 팔을 움직여 물체를 쥐고 힘을 줄 수 있으며, 다양한 손 모양을 만들어낼 수 있습니다.\n학습을 원활하게 하기 위해 보상 설계에도 신경을 썼습니다. DexArt의 모든 과제는 공통적으로 3단계의 세분화된 보상 구조를 갖는데, 이를 통해 에이전트가 접근 → 파지(grasp) → 조작 수행의 순차 단계를 밟도록 유도합니다. 첫 번째 단계에서는 로봇 손바닥이 해당 물체의 기능적 부위(예를 들어 수도꼭지 손잡이)에 충분히 가까이 접근하면 거리 기반의 보상을 줍니다. 둘째 단계에서는 손바닥과 여러 손가락이 물체와 접촉하여 안정적인 파지를 형성하면 추가 보상을 지급합니다. 논문에서는 손바닥(palm)과 최소 두 개 이상의 손가락이 물체와 접촉한 상태를 만족도 높은 파지로 간주하여 보상하도록 구현했습니다. 마지막 단계에서는 물체의 관절을 목표치까지 실제로 움직여 과제를 완수한 정도(진행율)에 따라 보상을 부여합니다. 예를 들어 수도꼭지라면 현재 밸브 손잡이를 얼마나 돌렸는지 각도 변화량을 계산하여, 90도에 가까워질수록 높은 보상을 주는 식입니다. 추가적으로, 불필요하게 과격하거나 불안정한 움직임을 피하도록 패널티 항도 포함되었는데, 이는 행동 벡터의 크기에 대한 L2 노름 제재와 과제별로 정의된 약간의 벌점 항으로 구성됩니다. 전체 보상은 이 세 단계 보상과 패널티를 가중합하여 계산되며, 각 작업에 거의 동일한 구조를 적용함으로써 학습 난이도와 보상의 일관성을 유지했습니다. 요약하면, DexArt는 다양한 관절형 물체 모델들을 활용한 네 가지 과제 세트와, 다관절 로봇 손을 활용한 표준화된 시뮬레이션 환경, 그리고 일반화 성능을 고려한 평가 프로토콜로 구성되어 있습니다. 객체 모델들은 PartNet-Mobility 데이터셋으로부터 선별되었으며, 일관된 물리 모델을 위해 부적절한 모델은 제외하고 크기나 초기 위치 등을 사람이 직접 조정했습니다. 훈련 중에도 에피소드마다 객체의 초기 배치 각도나 위치를 약간씩 랜덤하게 변화시켜, 정책이 보다 다양한 상황에 노출되도록 처리했습니다. 이러한 세심한 환경 구축으로, DexArt는 기존에 다루기 어려웠던 범용 섬세 조작의 학습을 체계적으로 연구할 수 있는 장을 제공합니다."
  },
  {
    "objectID": "posts/paper/2025-08-12-dexart.html#성능-평가와-실험-설정",
    "href": "posts/paper/2025-08-12-dexart.html#성능-평가와-실험-설정",
    "title": "📃DexArt 리뷰",
    "section": "성능 평가와 실험 설정",
    "text": "성능 평가와 실험 설정\nDexArt에서는 학습된 정책의 성능을 판단하기 위해 각 에피소드가 성공적으로 완료되었는지의 비율인 성공률(success rate)을 주된 지표로 사용합니다. 성공 기준은 과제에 따라 앞서 언급한 관절 이동이 일정 임계치 이상 달성되었는지로 정의되며, 예를 들어 수도꼭지의 경우 제한 시간 내에 밸브를 90도 가까이 돌렸다면 성공으로 기록합니다. 특히 중요한 것은 일반화 평가로서, 훈련에 사용된 물체들(Seen)에서의 성공률뿐 아니라 처음 접하는 새 물체(Unseen)들에서의 성공률을 별도로 측정한다는 점입니다. 이를 통해 정책이 훈련 때 본 적 없는 모양의 객체에도 얼마나 잘 대응하는지를 객관적으로 확인할 수 있습니다. 그 외 보조 지표로 에피소드당 누적 보상(return)의 평균도 보고하며, 학습 샘플 효율성을 비교하기 위해 학습이 진행됨에 따라 성공률이 상승하는 곡선의 기울기 등을 함께 분석합니다. 모든 실험은 동일한 PPO(Proximal Policy Optimization) 알고리즘과 파라미터 세팅 하에 과제별로 3개의 서로 다른 시드를 사용해 반복 학습하여 결과의 신뢰도를 높였습니다.\n본 논문에서는 여러 가지 학습 설정을 비교 평가하여 DexArt 과제에서의 성능 변화를 상세히 분석하였습니다. 먼저, 시각 인코더 신경망(PointNet)에 대해 서로 다른 사전 학습(pre-training) 방법들을 적용한 후 강화학습을 진행한 경우들을 비교했습니다. 구체적으로, PointNet을 지도학습 방식으로 객체 분류(46개 범주의 객체 분류) 또는 부분 분할(segmentation) 과제를 수행하도록 미리 학습시키거나, 자기지도학습 방식으로 포인트클라우드 복원(reconstruction)이나 SimSiam 방식으로 사전학습시킨 후, 이렇게 얻은 가중치를 초기화값으로 써서 RL을 진행하는 실험들이 수행되었습니다. 그리고 이들과 아무 사전학습 없이 처음부터 학습한 경우를 대비하여, 시각 표현 학습의 효과를 정량적으로 평가했습니다. 다음으로, 훈련에 사용된 객체의 수를 절반으로 줄였을 때와 모두 사용했을 때를 비교하여, 주어진 훈련 데이터의 다양성이 일반화 성능에 미치는 영향을 실험적으로 검증했습니다. 예를 들어 수도꼭지 작업에서 원래 11개의 훈련 객체 중 5~6개만 가지고 학습시킨 정책과 11개 모두 사용한 경우를 비교하는 식입니다. 세 번째로, PointNet 시각 인코더의 모델 크기를 작게/중간/크게 세 가지로 변경하여, 신경망 용량에 따른 학습 속도와 성능 차이를 살펴보았습니다. 마지막으로, 학습된 정책의 카메라 시점 변화에 대한 강인성을 평가했습니다. 이는 학습 당시 고정된 시점이 아니라, 임의의 새로운 각도에서 촬영된 깊이 이미지로부터 얻은 포인트클라우드를 입력했을 때도 정책이 잘 동작하는지를 보는 실험입니다. 특히 이 실험에서는 3D 포인트넷 기반 정책과, 대비군으로 2D 이미지 기반 정책을 함께 시험하였습니다. 2D 정책은 RGB 카메라 영상을 입력으로 하고 ResNet-18 신경망을 시각 백본으로 사용하는데, 사전학습으로 R3M (대규모 인간 동작 영상으로 학습된 표현) 모델 가중치를 초기화한 후 DexArt 과제를 학습시킨 것입니다. 두 정책을 모두 다양한 시점의 입력에 놓아보며, 3D 대 2D 시각 표현의 견고성 차이도 분석했습니다. 이러한 다양한 실험 설정은 DexArt 벤치마크의 포괄성을 잘 보여주며, 각각 정책의 표현 학습, 훈련 데이터 구성, 모델 설계, 일반화 및 강인성 측면에 대한 유의미한 정보를 제공합니다."
  },
  {
    "objectID": "posts/paper/2025-08-12-dexart.html#주요-결과와-통찰",
    "href": "posts/paper/2025-08-12-dexart.html#주요-결과와-통찰",
    "title": "📃DexArt 리뷰",
    "section": "주요 결과와 통찰",
    "text": "주요 결과와 통찰\n위와 같은 벤치마크 실험을 통해 여러 흥미로운 결과가 도출되었으며, 특히 다음과 같은 핵심 통찰을 얻을 수 있었습니다:\n\n훈련 데이터 다양성의 중요성: 한 작업에 대해 훈련에 사용한 객체 종류가 많을수록 새로운 객체에 대한 일반화 성능이 유의미하게 향상되었습니다. 각 과제에서 훈련에 투입된 객체 수를 절반으로 줄인 경우와 100% 모두 사용한 경우를 비교하면, 후자가 훈련 중 내내 테스트 객체에 대한 성공률이 더 높게 유지되는 양상을 보였습니다. 이는 다양한 형태의 물체들을 두루 경험하며 학습하는 것이 더 범용적인 시각 표현을 익히게 함을 의미합니다. 다만 객체 종류가 늘어나면 한꺼번에 학습해야 할 변수가 많아져 학습 속도는 오히려 느려질 수도 있다는 관찰도 있었는데, 실제 실험에서 훈련 초기 수렴 속도는 적은 객체로 학습한 쪽이 빠르지만 최종 성능은 다양한 객체를 사용한 쪽이 높았습니다. 결국 충분한 훈련 데이터 다양성이 정책의 일반화 능력을 높이는 열쇠임을 확인한 것입니다.\n큰 신경망이 항상 유리한 것은 아님: 시각 특성을 뽑는 PointNet의 모델 크기를 달리한 실험에서 의외의 결과가 나왔습니다. 복잡한 구조의 대형 모델보다 간결한 소형 모델이 일관되게 더 나은 학습 성능을 보인 것입니다. 가장 작은 PointNet이 모든 환경에서 에피소드 성공률과 반환값 측면 모두 최고 성과를 냈고, 학습도 가장 효율적이었습니다. 이는 일반적인 컴퓨터 비전 관점에서 생각하면 다소 놀라운 결과인데, 논문 저자들은 대규모 신경망은 강화학습에서 최적화가 어려워지는 경향이 있으며 이전 연구들에서도 유사한 보고가 있었다고 언급합니다. 이번 결과는 파라미터 수를 줄여 모델을 단순화하는 것이 오히려 탐색을 용이하게 하고 과적합을 줄여, 샘플 효율을 높이는 효과가 있을 수 있음을 시사합니다.\n객체 부위 인식의 중요성: 멀티핑거 로봇 손으로 물체의 일부분(기능적 부위)을 조작해야 하는 과제의 특성상, 정책이 그 물체의 부위를 식별하고 파악하는 능력이 성능에 큰 영향을 미칩니다. 실험에서 부위 분할(segmentation) 과제로 PointNet을 사전학습한 경우, 네 가지 모든 작업에서 가장 높은 최종 성공률을 기록했고 학습 초기의 향상 속도도 빨랐습니다. 다른 사전학습 방법들도 노트북 열기 등의 일부 과제에서는 성능 향상에 도움을 주었지만, 작은 손잡이와 같은 섬세한 부위가 중요한 작업들에서 분할 사전학습의 이점이 특히 두드러졌습니다. 연구진이 시각화한 바에 따르면, 분할 학습을 거친 모델은 수도꼭지의 작은 손잡이 부분을 포인트클라우드 상에서 정확히 구별해내지만, 재구성 위주로 학습한 모델은 물체의 전체 형태만 파악할 뿐 세밀한 부분은 놓치는 경향이 있었습니다. 결국 부위 인지 능력을 기르는 사전학습(예: 기능성 부위에 라벨을 준 3D 분할 데이터로의 학습)이 관절 객체 조작에 특화된 시각 표현을 형성하여 정책 학습을 크게 도와준다는 결론을 얻을 수 있습니다.\n3D 표현 학습의 강인성: 포인트클라우드 기반으로 학습한 정책은 카메라 시점이 달라져도 성능이 안정적으로 나타났습니다. 한 각도에서 학습한 정책을 전혀 다른 방향에서 본 입력으로 실행해도 성공률 저하가 미미했고, 상당한 시점 변화에도 정확도가 유지되었습니다. 반면 동일한 작업을 2D 카메라 영상으로 학습한 ResNet-18 기반 정책은, 훈련 때와 다른 뷰포인트의 입력에 대해서 성공률이 급격히 떨어지는 현상을 보였습니다. 이는 3차원 포인트클라우드로 학습된 표현이 시각적 관점 변화에 본질적으로 불변(invariant)한 성질을 가지고 있는 반면, 평면 영상 기반 표현은 카메라 각도의 변화에 민감함을 보여줍니다. DexArt 결과는 이러한 3D 표현 학습의 이점을 정량적으로 증명해주었고, 특히 시뮬레이터와 현실 로봇 간 카메라 위치 불일치 문제 등에서 3D 기반 접근이 얼마나 유용한지 강조하고 있습니다. 실제 로봇에 정책을 이식할 때 카메라 캘리브레이션이나 시점 변화로 인한 성능 저하를 최소화할 수 있다는 점에서, 기하학적 3D 표현의 활용은 큰 강점으로 작용할 것입니다.\n\n이 외에도 실험에서 2D 대 3D 입력의 절대 성능 비교를 보면, 동일한 강화학습 알고리즘과 비슷한 사전학습을 했을 때 PointNet(3D) 기반 정책이 ResNet-18(2D) 기반 정책보다 훈련이 잘 되고 최종 성공률도 높았습니다. 이는 관절 조작과 같이 정확한 형상 파악이 필요한 작업에는 3D 정보가 훨씬 유리함을 보여주는 결과입니다. 전반적으로 DexArt를 통해 얻은 통찰들은 향후 로봇 학습 연구에서 데이터 다양성, 표현 학습 기법, 모델 구조 선택, 입력 형태 결정 등에 중요한 지침을 제시합니다."
  },
  {
    "objectID": "posts/paper/2025-08-12-dexart.html#한계점-및-향후-과제",
    "href": "posts/paper/2025-08-12-dexart.html#한계점-및-향후-과제",
    "title": "📃DexArt 리뷰",
    "section": "한계점 및 향후 과제",
    "text": "한계점 및 향후 과제\nDexArt 벤치마크와 연구는 범용 로봇 섬세 조작을 향해 나아가는 의미있는 진전을 이루었지만, 동시에 몇 가지 제약과 향후 과제를 남겨두고 있습니다. 첫째, 본 연구 결과들은 모두 시뮬레이터 환경에서 얻어진 것이므로, 이를 실제 로봇에 적용하기 위해 넘어야 할 시뮬레이션-현실 간 격차(sim-to-real gap)가 존재합니다. 현실에서는 센서 잡음, 정확한 물리 파라미터 차이, 마찰 등 여러 불확실성이 추가되며, 시뮬레이터에서처럼 카메라 시점을 이상적으로 맞추기도 어렵습니다. 따라서 학습된 정책을 실제 로봇에 이식하여 실험하고, 필요한 도메인 적응이나 보정 기법을 연구하는 것이 다음 단계로 필요합니다. 둘째, DexArt에서 다룬 작업 유형이 아직은 제한적입니다. 주로 회전 관절(힌지 형태)의 움직임을 다루는 과제들로 구성되어 있는데, 서랍을 여는 슬라이딩 관절 조작이나 가위 사용처럼 복합적인 다단계 작업 등은 포함되지 않았습니다. 향후 벤치마크를 확장하면서 이러한 다양한 형태의 일상 조작 시나리오를 추가한다면, 로봇의 범용 능력을 더욱 폭넓게 평가하고 향상시킬 수 있을 것입니다. 마지막으로, 본 논문에서는 각 과제마다 별도의 정책을 학습시켰지만, 실제로는 한 로봇이 여러 작업을 수행할 수 있어야 합니다. 따라서 멀티태스크 학습으로 여러 종류의 조작 기술을 단일 정책에 통합하거나, 혹은 사람의 시범 없이 스스로 학습하도록 하는 자율학습 기술 등으로 발전시킬 필요가 있습니다. 이를 통해 보상 설계의 부담을 줄이고 더욱 일반적인 학습 프레임워크로 나아가는 연구가 이어질 것으로 보입니다. DexArt가 제시한 환경과 평가 체계는 이러한 향후 연구를 촉진하는 발판이 될 것이며, 궁극적으로 가정용 로봇이 사람처럼 다양한 물건을 능숙하게 다루는 날을 앞당기는 데 기여할 것으로 기대됩니다."
  },
  {
    "objectID": "posts/paper/2025-08-16-kinematic-retargeting.html",
    "href": "posts/paper/2025-08-16-kinematic-retargeting.html",
    "title": "📃Kinematic Motion Retargeting 리뷰",
    "section": "",
    "text": "Paper Link\nCode Link"
  },
  {
    "objectID": "posts/paper/2025-08-16-kinematic-retargeting.html#주요-기여-및-기술적-내용-요약",
    "href": "posts/paper/2025-08-16-kinematic-retargeting.html#주요-기여-및-기술적-내용-요약",
    "title": "📃Kinematic Motion Retargeting 리뷰",
    "section": "1. 주요 기여 및 기술적 내용 요약",
    "text": "1. 주요 기여 및 기술적 내용 요약\n문제 배경: 이 논문의 연구 대상은 손-물체 상호작용 모션을 다양한 형태의 손(캐릭터 손이나 로봇 손)으로 전달하는 키네마틱 모션 리타게팅 기법이다. 기존에는 사람의 손 동작 데이터를 다른 손 모델에 적용하려 하면 접촉 불일치나 모션 어긋남 등의 문제가 흔했고, 특히 손가락 수나 형태가 다른 경우 그 어려움이 극심했다. 이 논문은 이러한 과제를 해결하기 위해 접촉 영역(contact areas) 정보를 활용한 새로운 프레임워크를 제안한다. 요약하면, 원본 손 동작에서 객체와 손 사이의 접촉 분포(어떤 부위가 언제 물체를 잡고 있는지)를 비등거리(non-isometric) 형태 매칭 문제로 공식화하고, 이를 이용해 대상(hand)의 손으로 동일한 접촉 패턴을 옮긴 뒤, 해당 접촉을 만족하는 손 자세 시퀀스를 계산하는 방식이다.\n주요 기여\n논문에서 저자들은 두 가지 핵심 기술적 기여를 강조한다:\n\n지역적 형태 매칭 알고리즘\n원본 손과 대상 손 사이에 조밀한 접촉 대응(dense contact correspondences)을 얻기 위한 새로운 형태 매칭 기법을 도입하였다. 이 알고리즘은 손바닥과 손가락 표면에 대한 로컬 차트(local chart) 기반 매핑을 활용하며, 비교적 적은 수의 사용자 지정 마커와 곡선(annotation) 입력만으로 두 손의 대응 접촉 지점을 효율적으로 찾아낸다. 구체적으로, 아티스트나 사용자가 원본 손과 대상 손에 소수의 가상 마커(virtual markers)와 축 곡선(axial curves)을 지정하면, 알고리즘이 이를 경계 조건으로 활용하여 손가락/손바닥의 대응 위치를 정의한다. 각 손가락의 축 곡선은 손가락의 중심선을 따라 그려진 곡선으로서, 원본 손의 곡선을 대상 손의 대응 부분으로 옮겨와 지역 좌표계를 생성한다. 이렇게 생성된 좌표계를 통해 원본 손에서 물체에 닿은 접촉점들을 대상 손 표면의 적절한 위치로 절차적 변환(procedural transfer)함으로써, 시간에 따라 변화하는 접촉 분포를 한 손에서 다른 손으로 일관되게 옮길 수 있다. 이 과정은 단발성의 사전 매핑 작업으로 수행되며, 한 번 설정된 대응관계는 전체 모션 시퀀스에 걸쳐 사용된다. 이러한 방법을 통해 손가락 수나 길이가 달라도 접촉 지점들을 논리적으로 대응시킬 수 있고, 소수의 몇개 기준점만 지정하면 복잡한 접촉 분포 전체를 매핑할 수 있다는 점에서 효율적이다.\n\n\n다중 단계 최적화 파이프라인\n접촉 매핑 후에는, 다단계 역기구학 최적화를 통해 대상 손의 모션을 생성하는 파이프라인을 제시하였다. 이 키네마틱 리타게팅 파이프라인은 총 5단계로 구성되며, Figure 2에 그 개요가 제시되어 있다.\n\n\n\n\n원본 손과 물체 사이의 조밀한 접촉쌍 추출(원본 손의 매 프레임 접촉점과 물체 표면점의 1:1 쌍을 추출).\n앞서 언급한 형태 매핑을 이용해 모든 프레임의 접촉점을 대상 손으로 전달.\n전달된 접촉점들과 가상 마커를 활용하여 대상 손의 초기 자세 시퀀스 추정 (프레임별 초기 역기구학 해를 계산).\n연속적인 모션의 자연스러움을 높이기 위해 시간적 일관성 개선을 위한 세부 Refinement(연속 프레임 간 부드럽게 연결되도록 최적화 재조정).\n최종적으로 스플라인 곡선 피팅을 통해 모션을 매끄럽게 보간하여 연속적인 최종 궤적을 완성한다.\n\n이 파이프라인은 접촉 역학의 보존을 목표로 설계되었기 때문에, 리타게팅 후에도 원본 시연과 동일한 상호작용 기법(예컨대 손가락이 어떤 방식으로 물체를 감싸쥐는지)을 최대한 재현하려 한다. 이를 위해 “모든 손은 동일한 물체 접촉을 공유한다”는 가정을 두어, 다른 손으로 옮길 때도 물체상의 접촉 위치와 순서는 유지되도록 하였다. 기술적 내용 요약: 제안된 방법의 핵심은 접촉 정보를 중심으로 한 모션 재구성이다. 원본 데이터로부터 얻은 손-물체 접촉 분포(시계열 접촉 점들의 집합)를 대상 손으로 옮기기 위해, 저자들은 앞서 언급한 형태 매칭을 통해 원본 손의 각 접촉 지점에 대응하는 대상 손의 접촉 지점을 계산한다. 이렇게 하면 대상 손이 매 시점에 물체의 어느 부위를 접촉해야 하는지가 결정되는데, 이 “목표 접촉 위치”들이 곧 대상 손 모션의 필수 제약 조건으로 활용된다. 다음으로, 이러한 접촉 제약을 만족시키는 대상 손의 관절 자세(q)를 찾기 위해 최적화 문제를 설정한다. 이때 목적함수는 크게 네 가지 페널티 항으로 구성된다:\n\n마커 정합 오차 E_M: 원본 손과 대상 손의 가상 마커 대응점 사이 거리를 최소화. 마커 i (총 N_M개)에 대해 두 손의 해당 점 사이 거리 제곱 합으로 정의된다[23]. 이를 통해 비접촉 구간이나 손가락 자세가 애매한 경우에도 원본 손 형태를 모방하도록 유도한다.\n접촉 정합 오차 E_C: 대상 손의 접촉점이 물체의 대응 접촉 위치에 최대한 일치하도록 하는 항. 대응하는 접촉점 j (총 N_C개) 사이의 거리 제곱합으로 정의되며, 추가적으로 접촉점에서 법선 방향이 뒤집히지 않도록 (손바닥 표면 법선이 물체 표면 법선과 반대 방향을 향하게) 제약을 주는 항도 포함된다. 즉 손이 물체를 바르게 쥐도록 표면 방향까지 고려한 비용을 더한다.\n테이블 충돌 페널티 E_T: 손이 받침대나 테이블을 뚫고 지나가는 현상을 막기 위한 항. 손의 표면 샘플링 점들(또는 래핑 메시)이 테이블을 관통하면 해당 지점의 Signed Distance Function(SDF) 값이 음수가 되는데, 이때 그 음수 깊이에 비례한 벌점을 준다[26]. 이를 통해 손이 테이블 위에 놓인 물체를 잡을 때 불필요하게 책상 속으로 들어가지 않도록 한다.\n자세 사전(prior) 오차 E_P: 모션의 부드러움을 위한 정규화 항으로, 현재 프레임의 관절 각도 q가 사전에 정의된 기준 값과 크게 벗어나지 않도록 제어한다. 기준값으로는 초기 프레임에서는 손의 기본 휴식 자세(rest pose)를, 이후 프레임들에서는 직전 프레임의 해(solution)를 사용할 수 있다. 따라서 이 항은 프레임 간 급격한 변화나 노이즈를 억제하여 모션을 안정화시킨다. 전체 최적화 문제는 위 항들의 가중합을 최소화하는 비선형 제약 최적화로 표현된다. 수식으로는 다음과 같다: \\min_{q} \\; w_M E_M(q) + w_C E_C(q) + w_T E_T(q) + w_P E_P(q), 단, q (대상 손의 모든 자유도 벡터)에 대해 관절 가동 범위 제약 q_L \\le q \\le q_U를 만족시킨다는 조건이 붙는다. 여기서 w_M, w_C, w_T, w_P는 각 페널티 항의 가중치 하이퍼파라미터이고, E_M, E_C, E_T, E_P는 앞서 정의한 페널티 함수들이다. 이 최적화는 프레임별로 수행되며, 해를 구하기 위해 저자들은 NLopt 라이브러리의 MMA (Moving Maximum Asymptotes) 알고리즘이라는 gradient-based solver를 활용했다. 다만 단순히 각 프레임을 개별 최적화하면 시간적으로 일관된 움직임을 보장하기 어렵다. 그래서 저자들은 이 과정을 세 단계(stage)로 나누어 수행하였다. 1단계에서는 초기 추정치를 얻기 위해 각 프레임별로 위 최적화를 수행하되, 주로 마커/접촉 오차에 집중하면서도 rest pose에 대한 약한 prior를 적용하여 대략적인 모션을 얻는다. 2단계에서는 이렇게 얻은 초기 궤적을 부드럽게 다듬기 위해, 앞의 결과를 prior로 삼아 다시 한 번 최적화를 수행하거나 저속도/저가속도 조건을 적용하여 스무딩 처리를 한다[29]. 마지막 3단계에서는 스플라인 곡선을 이용해 개별 프레임을 연속 곡선으로 피팅함으로써 최종 모션을 완성한다. 이러한 다단계 접근 덕분에 세부 조정 없이도 결과 모션이 부드럽고 자연스럽게 연결된다.\n\n한 줄 요약\n요컨대 본 논문은 “손-물체 접촉”이라는 풍부한 정보를 매개로 소스(hand)의 조작 동작을 임의의 타겟 손에 옮기는 새로운 파이프라인을 제시하였다. 형상 대응(mapping) → 접촉 이전(transfer) → 역기구학 모션 생성으로 이어지는 이 기법은, 복잡한 동적 시뮬레이션 없이도 다양한 형태의 손에서 높은 품질의 모션 재현을 가능케 하는 점이 특징적이다. 저자들은 이러한 접근이 애니메이션, 로보틱스 등 여러 분야에서 범용적으로 활용될 수 있는 표준화된(hand agnostic) 솔루션이 될 수 있음을 강조하고 있다."
  },
  {
    "objectID": "posts/paper/2025-08-16-kinematic-retargeting.html#기존-연구와의-차별점-및-비교-분석",
    "href": "posts/paper/2025-08-16-kinematic-retargeting.html#기존-연구와의-차별점-및-비교-분석",
    "title": "📃Kinematic Motion Retargeting 리뷰",
    "section": "2. 기존 연구와의 차별점 및 비교 분석",
    "text": "2. 기존 연구와의 차별점 및 비교 분석\n이 연구는 모션 리타게팅, 형상 매칭, 접촉 기반 조작 생성 세 분야의 관련 선행 연구들과 교차하는 지점에 위치한다. 각 분야에서의 기존 접근법과 본 논문의 방법을 비교하면 다음과 같은 차별점이 드러난다.\n\n손 모션 리타게팅 분야\n전신(Full-body) 캐릭터의 모션 리타게팅은 비교적 오랜 연구 역사를 가지며, 관절 대응 기반 기법이 일반적이다 (예: 소스와 타겟의 관절을 일대일 대응시켜 각도를 복사하거나 스케일 조정). 그러나 이러한 전통적 기법을 손에 적용할 때 가장 큰 문제는 접촉 처리이다. 전신의 경우 발이나 물체와 몇 군데 점 접촉을 신경 쓰면 되지만, 손-물체 상호작용은 손가락 여러 개가 동시에 복잡하게 접촉하기 때문에 단순 관절 매핑만으로는 현실감 있는 결과를 얻기 어렵다. 실제로 소수의 접촉점만으로는 손-물체 상호작용의 복잡성을 표현하기에 부족하다는 보고가 있으며, 저자들 역시 “접촉 정보가 없다면 고품질 결과를 낼 수 없다”는 문제의식을 밝히고 있다. 기존 손 리타게팅 접근법 중에는 손가락 관절각을 1:1로 이식하는 직접 관절 매핑이나, 소스 손의 몇몇 특징 점(keypoint) 위치(손끝, 손목 등)를 대상 손 IK로 맞추는 방법 등이 있다. 또한 기능적 포즈 등가성(functional pose equivalence)이라고 하여, 손가락 길이 등이 다를 때는 손가락 위치 사이의 상대 벡터나 거리 분포를 맞추도록 하는 기법도 제안되어 왔다. 이러한 기존 기법들은 손 형태가 유사한 경우에는 쓸만하지만, 손 모양이 많이 다르거나 접촉이 많은 작업에서는 본 논문의 저자들이 지적하듯이 유의미한 아티팩트(부자연스런 동작, 접촉 누락 등)가 발생하는 한계가 있었다. 특히 로봇이나 판타지 캐릭터같이 사람과 형태가 다른 손의 경우, 단순 IK나 관절 매핑으로 생성한 모션은 물체를 헛잡거나 손가락이 어긋나는 등 실패 사례가 많았다.\n본 논문은 이 점에서 접촉 자체를 1급 정보로 활용한다는 점이 근본적인 차별화다. 기존의 많은 리타게팅 연구는 접촉을 사후에 보정하거나 물리 시뮬레이션으로 충돌을 처리하는 경우가 많았다. 예컨대, Won & Lee(2019)나 Ryu et al.(2021)은 물리기반 시뮬레이션을 섞어 충돌을 완화하고자 했고, Villegas et al.(2021)은 발접지 vs 자가충돌을 구분하여 최적화하는 등 기법을 제안했지만, 이는 전신 동작의 일부 문제를 다룬 것이지 손처럼 동시에 다중 접촉이 이루어지는 섬세한 조작에는 그대로 적용하기 어려웠다. 반면, 본 연구는 처음부터 접촉 분포를 입력 및 제약 조건으로 삼아 대상 손의 모션을 생성하므로, 애니메이터의 수작업 수정 없이도 접촉 일관성이 보존된 결과를 얻을 수 있다. 실제 실험 비교에서도, 접촉 정보를 빼고 오직 마커 대응만으로 리타게팅을 수행한 경우 문 손잡이를 제대로 잡지 못하고 엉뚱한 자세가 되는 등 초기 단계부터 모션 재구성이 실패함을 보여준다. 특히 인간과 유사성이 낮은 형태의 손일수록 이런 마커 기반 접근의 오류가 더 커지는 반면, 제안 기법은 접촉을 함께 고려함으로써 이런 문제를 극복하였다. 요약하면 “마커+IK” 기반의 전통 접근 vs “접촉+형상대응” 기반의 본 연구로 대비되며, 후자가 월등히 자연스러운 조작 모션을 생성함을 검증하였다.\n\n\n형상 매칭 분야\n손의 형상이 사람마다 또는 로봇마다 천차만별이라는 점 때문에, 모션 리타게팅에서 이종 메쉬 간 대응(correspondence) 문제는 중요한 도전과제다. 기존에는 동일 토폴로지(mesh connectivity)를 갖는 캐릭터 간에는 비교적 수월하게 좌표나 관절 변환을 할 수 있었으나, 본 논문처럼 삼지(hand) vs 오지(hand)처럼 기하구조가 다른 경우에는 일반적인 등거리 등각 맵핑 기법을 쓰기 어렵다. 저자들은 관련 분야의 기법을 두 가지로 분류하는데, 하나는 등거리(isometric) 매칭으로서 랩라시안 등 고유 모드가 유지되는 변형(주로 동일 캐릭터 포즈 간 비교)에 적용되는 반면, 다른 하나는 비등거리(non-isometric) 매칭으로 완전히 형태가 다른 두 물체 사이의 대응을 찾는 영역이다. 후자는 난이도가 높아 보통 사용자 입력(랜드마크 포인트나 커브 등)을 받아 부분적 대응을 수립하는 방식을 취한다. 본 논문의 손 형상 매핑 역시 이 비등거리 매칭의 한 사례로 볼 수 있다. 기존 연구들에서도 Takayama et al.(2022), Aigerman & Lipman(2016) 등은 사용자가 지정한 랜드마크나 곡선을 경계 조건으로 주어 맵핑을 푸는 방식을 제안했는데[48], 본 연구는 이러한 접근을 손 접촉 분포에 응용한 셈이다. 특히 축 곡선(axial curve)의 활용은 저자들이 이전 연구(2023)에서 소개한 개념으로, 손가락의 중심 경로를 따르는 곡선을 그려 넣으면 그 곡선에 수직인 방향을 따라서 접촉 점들을 자동 대응시키는 국소 chart를 만들 수 있다. 이를테면 사람 손의 검지와 중지가 물체의 한 면을 잡는 접촉들이 있을 때, 만약 대상 손이 세 개 손가락뿐인 외계인 손이라면, 그 중 두 손가락의 축 곡선을 하나의 대상 손가락 축에 매핑함으로써 “두 손가락 분의 접촉을 하나의 손가락으로 통합”하는 식의 대응도 가능하다. 이러한 유연성은 기존 일반적인 메쉬 매칭 기법들에서는 쉽게 다루지 못하는 부분으로, 본 연구의 형태 매칭 기법이 갖는 실용적 장점이다. 또한 일반적인 비등거리 매핑 연구들은 주로 표면의 텍스처나 기능 맵을 이식하는 용도로 초점을 맞추지만, 여기서는 시간에 따라 변하는 점 집합(접촉 시퀀스)을 일관되게 옮긴다는 새로운 목표에 적용되었다는 점에서도 차별화된다. 저자들은 자신들의 로컬 차트 기반 매칭이 연속된 접촉 정보를 예측 가능하고 견고하게 전달해주며, 소수의 입력으로도 충분히 성능을 낼 수 있음을 강조하고 있다. 이는 기존 방식 대비 아티스트의 작업량을 크게 줄여준다는 실용적 이점도 동반한다.\n\n\n접촉 기반 모션 생성 분야\n손 조작에서 접촉 정보를 활용하려는 시도는 이전에도 존재했다. 예를 들어, Brahmbhatt et al.(2019) 등의 연구는 접촉맵(어느 부분이 물체를 만졌는지)을 이용해 로봇 그립을 합성하거나 학습하는 방향을 탐색했고, Pollard 연구진의 초기 작업(Li et al., 2007)은 contact-invariant optimization이라는 개념을 통해 접촉이 유지되도록 하는 제스처 생성을 시도한 바 있다. 그러나 이러한 방법들은 주로 새로운 grasp를 합성하거나 물리 시뮬레이션 맥락에서 접촉력을 고려한 최적화를 한 것이지, 이미 존재하는 특정 모션을 다른 손으로 정밀하게 재현하는 데 초점을 두지는 않았다. 또한 최근 로보틱스 분야에서 인간 시演 동작을 로봇 손 정책으로 학습할 때, 우선 키네마틱 리타게팅으로 시演 궤적을 얻은 뒤 강화학습의 레퍼런스로 사용하는 경우가 많다. 하지만 앞서 언급했듯 표준화된 신뢰성 있는 손 모션 리타게팅 솔루션이 부재하여, 리타게팅이 잘못된 경우 학습까지 실패하거나 이상한 보상 함정에 빠질 수 있다는 문제가 보고되어 왔다. 본 연구는 이러한 흐름에서도 상류 단계의 오류를 최소화할 수 있는 방법으로서, 비록 동적 제어는 사용하지 않지만 접촉 정보를 잘 활용한 키네마틱 솔루션만으로도 상당히 물리적으로 그럴듯한 결과를 낼 수 있음을 보여준다. 실제 결과에서 손-물체 간, 손가락 간 관통 현상(intersection)이 거의 발생하지 않고, 발생해도 부피 기준 몇 퍼센트 미만으로 매우 작았다고 보고하는데, 이는 비록 물리엔진 없이도 접촉 제약을 걸었더니 손가락이 자연스레 물체 표면을 따라 움직이며 겹침을 최소화했기 때문이다. 다시 말해, 접촉 영역 자체가 물리적 구속을 일부 대체하여 모션의 현실감을 높여준다는 것으로 해석할 수 있다. 이러한 접근은 기존의 “사후 물리 시뮬레이션 보정”과 대비되며, 사전 접촉 활용의 효과를 잘 보여준다. 요약하면, 본 논문은 기존 기법들이 간과하거나 부분적으로만 다룬 접촉 정보를 중심에 놓고, 형상 매칭 기술과 결합하여 손 모션 리타게팅 문제를 해결한 점에서 차별성을 지닌다. 이러한 통합 접근으로 인해 얻는 이점은 실험적 비교를 통해서도 분명하게 나타나며 (예: 접촉 미고려 방식의 실패 사례 제시), 이는 곧 본 연구가 동 분야의 새로운 표준 방안을 제시하는 것으로 평가될 수 있다."
  },
  {
    "objectID": "posts/paper/2025-08-16-kinematic-retargeting.html#실험-설정-및-결과에-대한-평가",
    "href": "posts/paper/2025-08-16-kinematic-retargeting.html#실험-설정-및-결과에-대한-평가",
    "title": "📃Kinematic Motion Retargeting 리뷰",
    "section": "3. 실험 설정 및 결과에 대한 평가",
    "text": "3. 실험 설정 및 결과에 대한 평가\n\n실험 설정\n저자들은 자신들이 개발한 알고리즘을 검증하기 위해 다양한 형태의 손에 여러 조작 동작을 리타게팅하는 실험을 수행했다. 구체적으로는, GRAB 데이터셋에 포함된 인간 손의 조작 시퀀스 중 6가지 접촉-풍부한 동작(예: 물건 잡기, 문손잡이 돌리기, 망치질, 휴대전화 잡기 등 서로 다른 물체 대상 동작)을 선택하였다. 이 원본 시퀀스의 주체(hand)는 MANO 모델에 기반한 가상의 인간 손으로서 5손가락의 표준 형태다[56]. 그런 다음 타겟 손으로는 총 5가지를 준비했는데, 각각\n\n일반적인 다른 인간 손 (형상/비율만 상이),\n마녀 손 (가늘고 마디가 긴 판타지 캐릭터 손),\n외계인 손 (손가락 3개짜리 비인간형 손),\n커스텀 의수 디자인 (손가락 관절 수 일부 제한 또는 특수 형태),\nAllegro 로봇 손 (실제 로봇 플랫폼, 4손가락 및 고정된 관절 구조)이다.\n\n\n\n\nFigure 1에 이들 손 모습이 제시되어 있으며, 원본 손까지 포함한 6가지 손들이 대비된다. 각 타겟 손 모델에는 사전에 아티스트가 가상 마커와 축 곡선을 설정하여, 원본 손과의 형상 대응 정보를 입력해 두었다. 중요한 것은 이때 손의 형태나 위상 차이가 있더라도 (예: 삼지 vs 오지) 최대한 직관적으로 대응되도록 곡선을 그렸다는 점이며, 필요한 경우 한 손가락에 두 개 이상의 곡선을 대응시켜 여러 손가락의 접촉을 합치는 등의 조정을 했다. 이러한 조정은 부가적으로 가능하지만, 실험 전반에서는 모든 손에 동일한 알고리즘 파라미터와 동일한 설정을 사용하여 결과를 생성했다는 점을 저자들이 강조한다. 즉, 손마다 혹은 동작마다 특별한 미세 조정 없이도 잘 작동하는지를 보기 위해 가중치나 최적화 설정을 일률적으로 적용했고, 그럼에도 불구하고 전반적으로 양호한 결과가 얻어졌다고 보고하고 있다.\n\n\n성과 지표\n실험 결과는 정량 평가와 정성 평가로 나뉘어 제시되었다. 정량적으로는, 리타게팅된 모션에서 발생한 교차/충돌 정도를 측정하였다. 구체적으로 손-물체 간의 관통(손이 물체를 지나치게 파고든 경우), 손 자체의 자기 관통 (손가락 끼리 겹침), 그리고 손-테이블 간의 충돌(예: 물건을 집기 위해 테이블에 손이 박히는 경우) 세 가지를 계산하였다. 계산 방법은 각 결과 모션에서 손 메시에 대해 레이캐스팅을 수행하여 물체나 자기 자신과 겹친 부분의 볼륨을 클러스터 단위로 산출한 다음, 그것을 손 전체 부피 대비 백분율로 표현하는 방식이다. 이러한 방식으로 30개 모든 사례(5손 * 6동작)에 대해 평균적인 관통 비율을 구한 결과, 손-물체, 자가 충돌, 테이블 충돌 모두 대체로 극소수 퍼센트 이하로 측정되었다고 한다. Figure 10에서 각 사례별 수치를 그래프로 보여주는데, 전반적으로 아무런 후처리 없이도 충돌이 거의 발생하지 않는 안정적인 모션이 생성되었음을 알 수 있다. 이는 본 방법이 명시적으로 물리 충돌 회피 최적화를 하지 않았음에도 접촉 제약을 통해 암묵적으로 현실성을 확보했음을 시사한다. 저자들은 “접촉 영역 자체가 자연스러운 파지(grasp) 상태를 암묵적으로 인코딩하기 때문에, 물리 시뮬레이션 없이도 충돌 최소화에 효과를 보였다”라고 해석하고 있다.\n정성 평가 측면에서, 최종 생성된 모션들의 시각적 품질을 사람의 눈으로 평가하였다. 저자들은 모든 결과 모션의 동영상을 부록으로 제공하였고, 논문 내 Figures 11-13 등을 통해 몇 가지 흥미로운 사례를 캡쳐하여 보여준다.\n\n\n\n예를 들어, 손가락 길이 설계 변화 실험에서, 원본 디자인의 엄지 길이로는 손전등의 스위치에 도달하지 못하지만 엄지를 약간 길게 설계한 수정안은 충분히 스위치에 닿는 동작을 구현해낸 모습을 Figure 11에서 볼 수 있다. 이는 동일한 접촉 시퀀스를 사용했을 때 손가락 길이에 따라 과제 수행 가능 여부가 어떻게 달라지는지를 잘 보여주는 사례다. 또 다른 예로, 외계인 손(3손가락)에게 스마트폰을 쥐는 인간 손 모션을 리타게팅할 때, 기본적으로는 세 손가락에 인간의 다섯 손가락 접촉을 분배하도록 곡선을 그렸지만, 이를 다르게 설정해보는 실험도 했다.\n\n\n\nFigure 12에는 한 손가락에 두 개의 축 곡선을 할당하여 인간의 검지+중지 역할을 외계인의 하나의 손가락이 수행하도록 매핑한 경우를 보여주는데, 그 결과 다소 독특한 “스팍(Vulcan) 경례” 모양 비슷한 파지 형태가 나왔음을 볼 수 있다. 이러한 대안적 매핑도 가능하다는 것은 본 프레임워크의 융통성을 나타내며, 상황에 따라 사용자가 접촉 대응을 의도적으로 조정해볼 수 있음을 시사한다.\n또한 객체 치환(Object substitution) 실험으로서, 원본의 사과를 건네주는(hand-off) 동작을 감자 객체로 바꿔서 리타게팅하는 예를 시도했다. 이때는 물체 형태도 달라지기 때문에 원본 사과 표면의 접촉 분포를 감자 모델에 옮겨야 하는 추가 단계가 있었다. 저자들은 이를 위해 레이캐스트 기반의 접촉 전이 방식을 활용했다고 한다. 즉 원본 손-사과 접촉점에서 사과 표면 법선 방향으로 광선을 쏴 감자 표면과 교차하는 지점을 찾고, 그 지점을 대상 접촉점으로 삼아 전 프레임에 걸쳐 접촉 시퀀스를 변환한 것이다. 이러한 기법을 통해 새로운 객체에 대해서도 손의 파지가 적응하도록 만들 수 있었으며, Figure 13과 부록 영상에서 감자로 바뀐 상황에서도 손가락들이 미세하게 위치를 조정하며 사과를 쥘 때와 유사한 동작을 수행하는 것을 확인할 수 있다.\n\n\n\n\n\n결과 평가\n전체적으로 30개의 리타게팅 결과는 높은 품질을 보여주었으며, 저자들은 특히 놀라운 점으로 단일 파라미터 셋으로 모든 손과 동작에 대응했다는 점을 들고 있다. 이는 본 기법의 일반화 성능을 뒷받침하는 것으로, 특정 케이스별로 가중치를 재튜닝하거나 하지 않고도 다양한 형태에 잘 적용되었음을 의미한다. 물론 몇 가지 한계 사례도 관찰되었다고 한다. 예를 들어, 마녀 손처럼 손가락이 매우 길고 가느다란 손의 경우, 테이블 위에 놓인 물체(예: 스테이플러)를 집을 때 손가락이 과도하게 뒤틀리거나 어색한 포즈를 잠깐 취하는 현상이 있었다. 이는 테이블 관통을 피하려는 제약과 긴 손가락의 자유도가 맞물리며 발생한 현상으로, 저자들은 이럴 경우 특정 관절을 고정하거나 하는 추가 처리로 완화할 수 있다고 언급한다. 또한 Allegro 로봇 손처럼 크기가 매우 크고 자유도가 제한된 손의 경우, 사람 손의 접촉 분포를 그대로 맞추려 하면 자기 손가락끼리 부딪히는 경우가 상대적으로 늘어났다. 예컨대 Allegro 손은 사람보다 덩치가 크다 보니 동일한 물체를 쥘 때 손가락 간섭이 증가하고, 또 두 마디(knuckle)밖에 없는 손가락으로 다섯 손가락의 섬세한 움직임을 흉내내려니 어려움이 있었다는 것이다. 이러한 경우는 애초에 원본 인간의 접촉 패턴을 100% 고수하는 것이 적절하지 않을 수도 있다고 저자들은 지적한다. 즉, 타겟 손이 원본 손과 상호작용 방식 자체가 달라야 하는 상황(예: 사람은 물건을 주로 쥐지만, 특정 로봇 손은 집는 방식이 다를 때)이면, 접촉 분포도 그에 맞게 조정되어야 하는데 본 연구는 거기까지는 다루지 못했다는 것이다. 이런 부분은 향후 연구로 남아있지만, 그 밖의 범위에서는 전반적으로 일관되게 성공적인 결과를 보였다고 평가할 수 있다.\n한편, 비교 평가로서 기존 기법(베이스라인)과의 차이를 보여준 것도 주목할 만하다. 앞서 설명한 접촉 미고려 마커기반 리타게팅 실험에서, 문손잡이를 돌리는 시나리오의 경우 접촉 정보를 사용하지 않은 모션은 손가락이 손잡이를 정확히 파지하지 못하고 헛도는 장면이 연출되었다. 반면 제안 기법은 동일 프레임에서 손가락이 정확히 손잡이 표면을 따라 밀착되어 회전하는 모션을 보여준다. 이 비교는 정 kiepoint기반 접근의 한계를 극명히 보여주는 사례로, 논문의 Figure 19에 해당 장면이 제시되어 있다. 이러한 결과를 통해 저자들은 “접촉 정보 없이는 손-물체 상호작용 모션의 섬세함을 재현하기 어렵다”는 주장을 실험적으로 입증하였다.\n\n\n\n종합하면, 본 논문의 실험은 제안 기법의 범용성, 정확성, 품질을 다각도로 확인한 것으로 볼 수 있다. 다양한 손과 동작에의 적용 결과, 별다른 튜닝 없이도 물리적 개연성이 높고 접촉이 잘 유지된 모션을 얻었으며, 이는 현시점 공개된 손 리타게팅 기법 중 가장 철저한 검증 사례 중 하나로 평가할 만하다. 물론 제한적인 어려움도 관찰되었지만, 그에 대한 논의도 솔직히 제시되어 있어 연구의 신뢰도를 높여준다."
  },
  {
    "objectID": "posts/paper/2025-08-16-kinematic-retargeting.html#활용-가능성-및-한계점-분석",
    "href": "posts/paper/2025-08-16-kinematic-retargeting.html#활용-가능성-및-한계점-분석",
    "title": "📃Kinematic Motion Retargeting 리뷰",
    "section": "4. 활용 가능성 및 한계점 분석",
    "text": "4. 활용 가능성 및 한계점 분석\n활용 가능성\n이 연구의 성과는 여러 분야에 걸쳐 응용될 수 있다. 우선 컴퓨터 애니메이션/게임 제작 분야에서, 사람의 손 동작 캡처 데이터를 다양한 캐릭터에 재사용하는 데 큰 도움을 줄 수 있다. 예를 들어, 영화나 게임에서 인간 캐릭터가 물체를 다루는 모션을 판타지 생물이나 독특한 프로포션의 캐릭터에게 옮길 때, 지금까지는 애니메이터가 일일이 손가락 위치를 조정해야 했지만 본 기술을 활용하면 상당 부분 자동화하여 제작 비용과 시간을 절약할 수 있을 것으로 기대된다. 저자들도 실제 아티스트가 본 시스템을 활용하여 몇 개의 축 곡선만 그려넣음으로써 다수의 모션을 손쉽게 변환할 수 있음을 보였고, 이러한 인터페이스는 창작 파이프라인의 생산성을 높일 수 있다. 또한 VR/AR 및 텔레프레젠스 영역에서도 응용 가능성이 있는데, 예컨대 VR 챗이나 원격 로봇 조작에서 인간 사용자의 손 동작을 가상 아바타의 손이나 원격 로봇 손으로 실시간(또는 준실시간) 리타게팅하는 데 활용할 수 있다. 현재 제안 기법은 오프라인 배치 처리를 가정하고 있고 몇 시간의 계산을 필요로 하지만, 만약 최적화 알고리즘을 개선하거나 하드웨어 가속을 도입하면 속도를 높일 여지가 있다. 기본 원리는 그대로 두고 속도만 개선한다면, 풍부한 접촉 기반의 정확한 제스처 매핑이 실시간 상호작용에도 사용될 수 있을 것이다.\n로보틱스 및 휴먼-로봇 상호작용 측면에서도 본 연구는 큰 시사점을 준다. 로봇 손의 설계자나 연구자는 사람의 시연 동작을 로봇 손으로 따라해보며 그 로봇 손의 조작 잠재력을 평가하고 싶어 한다. 본 방법을 쓰면, 굳이 복잡한 강제 제어나 학습 없이도 인간 데이터셋(GRAB 등)의 풍부한 시나리오를 로봇 손으로 가상 시험(test)해볼 수 있다. 예컨대 어떤 물체를 잡는 데 로봇 손가락 길이가 충분한지, 관절 배치는 적절한지 등을 접촉 분포만으로도 알아볼 수 있다. 논문에서 보여준 엄지 길이 사례는 바로 손 설계 평가 시각화의 좋은 예이다 – 설계안 A와 B의 엄지 길이에 따라 특정 작업(스위치 누르기)의 성공 여부가 리타게팅 결과로 명확히 드러났기 때문이다. 이러한 기능은 의수나 로봇 손 개발자가 디자인 단계에서 피드백을 얻는 도구로 활용될 수 있다. 뿐만 아니라, 이미 언급했듯 강화학습이나 모션 프리미티브 학습 전에 전문 시演 궤적을 생성하는 용도로도 쓰일 수 있다. 인간 데이터로부터 바로 로봇 정책을 학습하기 어려운 경우, 본 기법으로 로봇 형태에 맞춘 전문가 궤적(expert trajectory)을 만들고 이를 학습 초기화나 보상 설계에 활용하면 학습의 안정성과 속도를 높일 잠재력이 있다. 저자들도 이러한 방향 (접촉정보가 정책 학습에 도움을 줄지)에 큰 관심을 보이며 향후 연구를 제안하고 있다. 또 다른 흥미로운 응용은 의료 재활 또는 인간공학 분야다. 서로 다른 손 구조를 가진 사람들이나 환자의 의수에 대해, 동일 작업을 수행할 때의 운동 패턴 차이를 분석하는 연구에 본 기술을 활용할 수 있을 것이다. 예를 들어 특정 동작에서 손가락이 어떤 식으로 적응하는지, 불편함이 생기는지 등을 가시화함으로써 인체공학적 디자인 개선에 아이디어를 줄 수 있다.\n한계점\n그럼에도 불구하고, 이 접근법에는 몇 가지 제약과 한계가 존재한다.\n우선, 순전히 키네마틱한 방법이라는 점이다. 동역학적인 힘이나 마찰 고려는 없고, 오직 기하학적 접촉과 위치 기반으로 모션을 만들기 때문에 실제 물체 무게를 지탱하는 힘 조절이나 미끄러짐 방지 같은 요소는 다루지 못한다. 이는 애니메이션에는 문제가 없지만, 실제 로봇에 이 궤적을 그대로 실행하면 물체를 놓칠 수 있다는 뜻이기도 하다. 향후에는 접촉력까지 고려한 동적 확장이 필요하며, 저자들도 이를 다음 과제로 언급하였다. 다만 현 단계에서는 신뢰성을 위해 동역학을 배제했다고 하며, 이는 “일관된 결과를 얻기 위한 선택”이었음을 강조한다.\n두 번째로, 데이터 의존성의 문제가 있다. 본 기법은 충분히 정확한 접촉 정보가 주어질 때를 가정한다. GRAB 같은 데이터셋은 모캡과 보정된 시뮬레이션으로 접촉을 추정했지만, 완벽하지 않을 수 있다. 실제 물리에서의 접촉은 마찰, 접촉면 압력 등 다양하게 정의되는데, 현재의 접촉 데이터는 주로 거리 기반으로 “0/1 접촉”만 표시하는 수준이다. 이 때문에 접촉 위치가 살짝 어긋나거나, 힘이 가해지는 면적과 달라질 수 있다. 저자들은 향후 촉각 센서 등을 활용하여 보다 정확한 접촉+힘 데이터를 얻고, 이를 통해 접촉 기반 리타게팅을 한층 개선할 수 있을 것으로 보고 있다. 현실 세계에서 센서를 통한 접촉 수집이 가능해진다면, 본 기법의 실용성은 더욱 높아질 것이다.\n세 번째 한계는 형상 매핑의 불연속성이다. 앞서 말한 축 곡선 기반 매핑은 손을 몇 개의 구획으로 나눠서 각각 매핑하는 아틀라스(atlas) 방식인데, 이 결과 얻어지는 대응 맵은 엄밀히 말해 연속적이지 않고 미분 불가능하다. 이는 현재 접근에서는 큰 문제가 아니지만, 만약 향후 이 매핑 자체를 최적화 변수로 삼아 뭔가 해보고 싶을 때(예: 접촉을 부드럽게 이동시키거나 텍스처를 옮기거나) 제약이 될 수 있다. 하나의 이상적인 전역 매핑 함수를 만들 수 있다면 접촉 이외의 부가 정보도 함께 전이하거나, 그라디언트를 활용한 최적화도 가능해질 것이므로, 저자들은 연속체 매핑으로의 개선 여지를 언급하고 있다.\n네번째로, 모든 경우에 접촉 유지가 최선은 아닐 수 있다는 점이다. 앞서 실험 결과 논의에서 언급했듯이, 타겟 손이 소스 손과 지나치게 형태나 동작 양식이 다르면 차라리 접촉 패턴을 조정하는 편이 나을 수 있다. 예를 들어 사람은 물건을 다섯 손가락으로 감싸쥐지만, 어떤 로봇은 두 손가락 집게로 집을 수 있다면 굳이 다섯 군데 접촉을 흉내낼 필요는 없을 수 있다. 그러나 본 방법은 일단 “원본과 동일한 접촉을 모두 재현”하는 방향으로 최적화를 하므로, 때론 부자연스럽거나 비효율적인 동작이 나올 위험이 있다. 이를 방지하려면 어떤 접촉은 포기하거나 대체 전략을 취할 수 있어야 하는데, 이러한 의사결정은 현재는 사람이 축 곡선을 그릴 때 수동으로 해줘야 한다 (예: 불필요한 손가락 곡선을 안 그리면 해당 손가락 접촉은 무시됨). 향후에는 AI가 자동으로 “이 손에는 이 접촉은 생략하자” 또는 “대상 손에 맞게 접촉 분포를 수정하자”를 결정해주면 더 지능적인 리타게팅이 될 것이다. 이는 한계임과 동시에 발전 가능성이라 볼 수 있다.\n마지막으로, 연산 비용과 실용성 측면의 한계도 짚고 넘어가야 한다. 논문에 따르면 본 모션 한 시퀀스(수백 프레임 내외)를 리타게팅하는 데 4시간에서 최대 22시간까지 걸렸다고 한다. 대부분의 시간은 수치 최적화에서 유래하는데, 특히 현재 구현은 유도된 그라디언트가 아니라 유한 차분으로 경사를 구해 느리다고 한다. 물론 이는 연구 프로토타입 수준이기에 최적화 여지가 있지만, 상용 툴로 쓰려면 속도 개선이 필수적이다. 병렬처리나 GPU 활용, 또는 분석적 그라디언트 도출 등을 통해 이 시간을 대폭 줄이는 것이 과제다. 또한 아티스트의 작업 측면에서도, 축 곡선과 가상 마커를 손마다 설정해야 하는 수고가 있다. 다행히 논문에서는 이 작업이 상대적으로 적은 편이고 한번 설정한 뒤 여러 모션에 재사용할 수 있다고 언급하지만, 그래도 완전 자동은 아니라는 점에서 초기 세팅 비용이 존재한다. 그러나 이 부분은 현재의 거의 모든 비등거리 매핑 기법들이 갖는 공통점이므로, 본 연구만의 단점이라기보다는 향후 사용자 부담을 줄여야 할 연구과제일 것이다."
  },
  {
    "objectID": "posts/paper/2025-08-16-kinematic-retargeting.html#결론",
    "href": "posts/paper/2025-08-16-kinematic-retargeting.html#결론",
    "title": "📃Kinematic Motion Retargeting 리뷰",
    "section": "결론",
    "text": "결론\nKinematic Motion Retargeting for Contact-Rich Anthropomorphic Manipulations 논문은 접촉을 매개로 한 손 모션 리타게팅의 새 지평을 연 연구로 평가된다. 주요 기여인 형태 매칭 알고리즘과 최적화 파이프라인을 통해, 다양한 손 모델 간에 복잡한 조작 동작을 높은 충실도로 전이할 수 있음을 보였다. 실험을 통해 그 효과와 한계를 균형 있게 제시하였으며, 특히 접촉 정보의 중요성을 부각시켜 관련 커뮤니티에 유의미한 통찰을 제공한다. 향후 이 기법에 물리적 동역학을 접목하거나 학습 분야에 응용한다면, 인간→로봇 복잡 조작 전이와 같은 난제도 해결하는 방향으로 발전할 가능성이 크다. 요약하면, 본 연구는 손 모션 리타게팅의 표준화된 신뢰성 있는 해법을 제시함과 동시에, 향후 확장될 수 있는 다양한 응용 시나리오를 제시했다는 점에서 의의가 크다."
  },
  {
    "objectID": "posts/paper/2025-08-27-eurekaverse.html",
    "href": "posts/paper/2025-08-27-eurekaverse.html",
    "title": "📃Eurekaverse 리뷰",
    "section": "",
    "text": "Paper Link\nProject Link\nCode Link"
  },
  {
    "objectID": "posts/paper/2025-08-27-eurekaverse.html#사용된-방법론과-기술적-접근",
    "href": "posts/paper/2025-08-27-eurekaverse.html#사용된-방법론과-기술적-접근",
    "title": "📃Eurekaverse 리뷰",
    "section": "사용된 방법론과 기술적 접근",
    "text": "사용된 방법론과 기술적 접근\nEurekaverse의 핵심은 LLM으로 환경(장애물 코스)의 프로그램 코드를 생성하고, 이를 강화학습과 결합해 커리큘럼을 만들어 나가는 것입니다. 4족 보행 로봇 파쿠르 과제를 예로 들면, 환경은 땅의 높이를 나타내는 2D 그리드(높이장(height field))와 로봇이 차례로 도달해야 할 목표(goal) 좌표들의 리스트로 구성됩니다. 이 둘은 파이썬 코드(NumPy 연산 포함)로 구현되며, LLM은 이러한 코드 형태로 코스를 생성합니다 . 예를 들어 강아지 공원이나 놀이터에서 영감을 얻어 장애물을 설계하도록 프로프트(prompts)를 구성합니다. 이때 생성된 코스는 최대 높이 제한, 경사 차이 제한 등의 간단한 검증 절차를 통해 물리적으로 불가능하거나 매우 위험한 코스는 걸러냅니다 .\n방법의 전체 흐름은 다음과 같이 단계별로 이루어집니다 :\n\n초기 환경 생성: 우선 LLM에 과제 설명과 간단한 예시 코스(인컨텍스트 예제)를 제공하고, 여러 개의 초기 환경 코드를 샘플링합니다 . (예: GPT-4o를 이용하여 8개의 환경을 무작위 온도로 생성)\n정책 훈련: 생성된 환경 집합에서 각 환경을 포함하는 라이브러리(train library)별로 병렬로 복수의 RL 에이전트를 훈련합니다. 각 에이전트는 PPO와 같은 알고리즘으로 학습되며, 모든 환경에서의 도달 목표 개수 등의 성과 지표를 기록합니다 .\n정책 평가 및 선택: 훈련이 끝나면 전체 환경(이전까지 생성된 모든 코스)에서 에이전트들의 성능을 평가하여 가장 잘 수행한 정책(π*)을 선택합니다 . 여러 개의 에이전트를 유지하는 이유는, 나쁜 환경 설계에 영향을 받지 않고 일부는 유용한 환경에서 학습할 가능성을 높이기 위함입니다 .\n환경 진화 (LLM 재생성): 선택된 최고의 정책이 훈련된 환경들(효과적인 것으로 판명된 환경)을 바탕으로, LLM에게 해당 환경을 좀 더 어렵게 변형하도록 요청합니다. 즉, 기존 환경 코드를 LLM에 다시 보내면서 로봇의 현재 성능 지표(예: 성공률, 평균 보상)와 환경 통계(최대 높이 등)를 함께 제시하여 다음 세대 환경을 생성합니다 . 이 과정을 충분히 반복하여 새로운 환경 라이브러리를 만듭니다.\n반복 학습: 이렇게 생성된 새 환경에서 다시 RL 정책을 학습시키고, 위 과정을 반복(에이전트-환경 공진화)하여 점진적으로 난이도를 높여 갑니다 .\n\n이 과정은 전통적인 커리큘럼 학습처럼 점진적으로 어려운 환경을 제공하지만, 커리큘럼 자체를 LLM이 자동으로 설계한다는 점이 다릅니다 . 저자들은 “소프트 선택(soft selection)” 방식을 도입하여 매 세대마다 성능 순위에 따라 정책을 무작위로 선택하기도 했으며, 최종적으로 가장 좋은 정책을 출력합니다. LLM으로는 GPT-4o를 사용하였으며, 전체 학습은 GPU 8대에서 약 24시간, OpenAI API 비용 약 15달러가 소요되었다고 합니다 ."
  },
  {
    "objectID": "posts/paper/2025-08-27-eurekaverse.html#실험-설계-및-결과-평가",
    "href": "posts/paper/2025-08-27-eurekaverse.html#실험-설계-및-결과-평가",
    "title": "📃Eurekaverse 리뷰",
    "section": "실험 설계 및 결과 평가",
    "text": "실험 설계 및 결과 평가\n저자들은 대규모 시뮬레이션과 실제 로봇 실험을 통해 Eurekaverse의 성능을 평가했습니다. 시뮬레이션 학습은 Cheng et al.[21]의 프레임워크를 기반으로, Unitree Go1 로봇(4족 보행 로봇)에 대해 높이장(height field)과 목표 좌표로 정의된 파쿠르 코스를 사용했습니다. 난이도에 따라 장애물의 크기나 간격이 조정되며, PPO로 교사 정책을 학습한 뒤 깊이 이미지 기반 학생 정책으로 증류(distillation)하여 실제 로봇에 적용합니다. 비교 대상으로는 사람이 설계한 코스(Human-Designed)와 무작위 배치 코스(Random) 등이 있습니다. 특히 사람 설계 코스는 일반적인 파쿠르 기술을 가르치도록 잘 만들어진 것이지만, 적응 학습이 빠르게 포화하는 한계가 있습니다.\n시뮬레이션 결과, 환경 설계가 필수적임이 확인되었습니다. 무작위 환경(Random)에서는 성능 향상이 거의 없었으며, 이는 장애물들이 무작위로 배치되어 로봇이 제대로 학습하지 못했기 때문입니다 . 반면 Eurekaverse는 학습 곡선이 꾸준히 상승하며 지속적인 개선을 보였습니다. 최종적으로 Eurekaverse가 만든 커리큘럼으로 학습한 정책은, 사람 설계 코스의 정책보다 평균적으로 거의 두 개의 추가 목표(goal)를 더 달성할 정도로 뛰어난 성능을 보였습니다 . 사람 설계 코스도 초반에는 빨리 학습되지만 곧 성능이 포화되는 반면, Eurekaverse는 초반 학습이 다소 느려도 환경이 정책 성능에 맞춰 지속적으로 진화하기 때문에 마지막에는 더 높은 성능을 얻었습니다 . 실제로 20개의 테스트 코스(시뮬레이션)에서 평가했을 때, Eurekaverse 학습 정책은 인간 설계 코스를 따라 학습한 정책보다 목표 달성 개수가 평균 2개가량 더 높았습니다 . 또한, “초기 환경(Initial Envs)”이나 “최종 환경(Final Envs)”처럼 환경 변화를 고려하지 않은 경우, 정책은 다양한 기술을 익히지 못하고 성능이 저하되는 것으로 나타났습니다 . 이는 단순히 어려운 환경을 늘려주는 것이 아니라, 현재 정책 수준에 맞게 조정된 적응적 커리큘럼이 필요함을 시사합니다 .\n실제 로봇 실험에서도 Eurekaverse의 우수성이 드러났습니다. 네 가지 대표적인 실제 과제(박스 오르기, 간격 뛰어넘기, 경사로 오르기, 계단 넘기)에서 Eurekaverse로 학습한 정책은 거의 모든 난이도에서 사람 설계 정책보다 성공률이 높았습니다 . 예를 들어 Go1 로봇에 대해 75cm 높이의 점프, 50cm 높이의 장애물 오르기, 30도 경사로 등 까다로운 조건에서도 성공했으며, 계단 과제에서는 전체 계단을 완주했습니다. 반면 사람 설계 코스로 학습한 정책은 모터 과부하로 넘어지는 등의 불안정한 동작이 잦았고, 실제 경사로 과제에서는 로봇이 손상될 정도여서 더 이상의 실험을 진행하지 못했습니다 .\n\n\n\n\n실제 4족 보행 로봇 실험에서 Eurekaverse로 학습한 정책(빨간 점선)이 사람 설계 정책(파란 점선)보다 전반적으로 높은 성공률을 보였다.\n\n더욱 인상적인 것은 학습 과정에서 한 번도 본 적 없는 완전히 새로운 코스에서도 Eurekaverse 학습 정책이 뛰어난 범용성을 보인 점입니다. 논문에서는 박스+점프+경사+요가볼, 좁은 런웨이+65cm갭, 연속 점프+기울어진 경사면, 장애물 투척+유동형 갭 등 4가지 새로운 코스를 구성했는데, 우리 정책은 이들 모든 장애물 코스를 성공적으로 돌파했습니다 . 이는 높은 일반화 능력을 보여주며, 시뮬레이션에서 얻은 정책이 실제 및 예상치 못한 환경에서도 강인한 적응력을 가진다는 것을 의미합니다 ."
  },
  {
    "objectID": "posts/paper/2025-08-27-eurekaverse.html#기존-연구와의-차별점-및-기여",
    "href": "posts/paper/2025-08-27-eurekaverse.html#기존-연구와의-차별점-및-기여",
    "title": "📃Eurekaverse 리뷰",
    "section": "기존 연구와의 차별점 및 기여",
    "text": "기존 연구와의 차별점 및 기여\n기존 연구들에서도 로봇 학습에 커리큘럼을 적용하려는 시도는 있었습니다. 다만 대부분은 환경 변형 범위를 사람이 직접 설계하거나, 물리 파라미터 범위를 무작위로 조절하는 수준에 머물렀습니다 . 예를 들어 이전 연구들은 2D 미로 배치 같은 단순 공간에서 강화학습 에이전트와 환경 생성자를 대결시키는 방식을 고안했지만, 이것도 설계자가 환경 생성 규칙을 만들어야 했고 파쿠르처럼 복잡한 지형 전체를 다루기엔 한계가 있었습니다 . LLM을 활용한 연구들도 있긴 하지만, 주로 상위 수준의 계획이나 픽앤플레이스 작업에서 환경 샘플링 정도로 사용되었으며 , 복잡한 3D 물리 환경 전체를 점진적으로 진화시키는 예는 없었습니다. 한편 파라미터 랜덤화(simulation randomization) 연구도 있었는데, 이는 중력이나 질량 같은 물리 수치를 무작위로 선정하는 단순한 방식이라 환경 변화의 폭이 제한적입니다 .\nEurekaverse는 두 가지 측면에서 독창적입니다. 첫째, 환경을 프로그램 코드로 취급하여 LLM이 직접 코스를 생성하고 수정할 수 있도록 했습니다 . 복잡한 장애물의 기하학적 구성을 명시적으로 코드로 표현함으로써 LLM이 유연하게 다양한 코스를 만들 수 있습니다. 둘째, LLM 생성과 정책 학습을 adaptive co-evolution으로 결합한 점입니다 . 기존 연구와 달리, 정책의 현재 성능을 반영하여 LLM에게 다음 세대 코스를 진화시켜 달라고 하는 피드백 루프가 추가되어 있습니다. 이를 통해 학습이 진행될수록 커리큘럼이 자연스럽게 어려워지고 다양해지며, 로봇의 기술이 점점 향상됩니다 . 이처럼 LLM을 이용한 자동 커리큘럼 생성은 이전에 없던 접근입니다. 특히 4족 보행 로봇 파쿠르와 같이 복잡한 실제 물리 환경에서도 성공적인 실험 결과를 보인 점이 기존 연구 대비 중요한 기여라고 할 수 있습니다 ."
  },
  {
    "objectID": "posts/paper/2025-08-27-eurekaverse.html#한계점-및-향후-연구-과제",
    "href": "posts/paper/2025-08-27-eurekaverse.html#한계점-및-향후-연구-과제",
    "title": "📃Eurekaverse 리뷰",
    "section": "한계점 및 향후 연구 과제",
    "text": "한계점 및 향후 연구 과제\nEurekaverse는 강력하지만, 몇 가지 한계와 과제도 지닙니다. 논문에서 저자들은 LLM 샘플 효율을 주요 문제로 지적합니다 . 매 반복마다 많은 환경을 GPT-4o로 생성해야 하므로, 보다 적은 호출로도 좋은 환경을 만들도록 LLM을 미세조정(fine-tuning)하는 연구가 필요합니다. 또한 현재 환경 변형 프롬프트는 텍스트 정보(보상, 성공률 등)만 활용하기 때문에, 앞으로는 환경의 3D 기하 구조나 실제 화면을 함께 제공하는 멀티모달 입력을 적용해 LLM의 공간적 추론 능력을 끌어올릴 여지도 있습니다 .\n그 밖에 실용적 고려사항도 있습니다. 예를 들어 본 실험에서는 GPT-4o API를 사용하였고 24시간 학습에 걸려 비용이 발생했습니다. 따라서 실시간 로봇 학습이나 더 적은 계산 자원으로 적용하기 위해서는 효율화가 필요합니다. 또 현재는 파쿠르에 집중했으나, 다른 로봇 과제(예: 조작, 비행 등)에도 적합한 환경 생성이 가능한지 실험을 통해 확인해야 합니다. 마지막으로, LLM이 종종 비현실적인 코드를 생성할 수 있기 때문에 이를 걸러내는 안전 장치나 필터링 기법도 중요합니다.\n요약하자면, Eurekaverse는 언어 모델의 코드 생성 능력을 로봇 환경 설계에 적용한 흥미로운 시도로서, 복잡한 장애물 코스 커리큘럼을 자동화했습니다. 이로써 로봇 학습에서의 환경 커리큘럼 제작 부담을 크게 덜어줄 수 있는 가능성을 보여주었지만, 동시에 LLM 활용의 효율성과 안정성 측면에서 후속 연구가 필요함도 드러냈습니다 . 향후 다양한 도메인으로의 확장과 시스템 최적화를 통해, 더 일반적이고 뛰어난 로봇 학습 환경을 만드는 연구로 이어질 것으로 기대됩니다."
  },
  {
    "objectID": "posts/paper/2022-10-02-vae.html",
    "href": "posts/paper/2022-10-02-vae.html",
    "title": "📃VAE 리뷰",
    "section": "",
    "text": "이번 포스트는 생성모델에서 유명한 Variational Auto-Encoder(VAE)를 다루고 있는 Auto-Encoding Variational Bayes라는 논문 리뷰입니다. 이번 포스트를 정리하면서 가장 많이 인용하고 도움을 받은 오토 인코더의 모든 것를 보시면 훨씬 더 자세하고 깊은 이해를 하실 수 있습니다. 포스트의 순서는 아래와 같이 진행됩니다."
  },
  {
    "objectID": "posts/paper/2022-10-02-vae.html#regularization-term",
    "href": "posts/paper/2022-10-02-vae.html#regularization-term",
    "title": "📃VAE 리뷰",
    "section": "2.1 Regularization term",
    "text": "2.1 Regularization term\nELBO term을 나누었을 때 나왔던 첫번째 Regularization term에 대해 보겠습니다. True posterior를 추정하기 위한 q_\\phi(\\mathrm{z} \\mid \\mathrm{x})은 KL 값을 계산하기 쉽도록 하기 위해 Multivariate gaussian distribution으로 설계합니다. 또한 앞서 이야기했던 것 처럼 controller 부분인 p(z)는 다루기 쉬운 분포이어야 하기 때문에 정규분포로 만들어 줍니다. 그러면 논문의 Appendix F.1에서 볼 수 있듯이 가우시안 분포들 사이의 KL 값은 mean과 std를 사용하여 다음과 같이 쉽게 계산될 수 있습니다.\n\n\n\nRegularization term6"
  },
  {
    "objectID": "posts/paper/2022-10-02-vae.html#reconstruction-error-term",
    "href": "posts/paper/2022-10-02-vae.html#reconstruction-error-term",
    "title": "📃VAE 리뷰",
    "section": "2.2 Reconstruction error term",
    "text": "2.2 Reconstruction error term\nELBO의 두번째 term인 Reconstruction error에 대해 살펴보겠습니다. Reconstruction error의 expectation 표현을 integral로 표현하면 다음과 같고 이는 몬테카를로 샘플링을 통해 L개의 z_{i, l}를 가지고 평균을 내서 구할 수 있습니다. 여기에서 index i는 데이터 x의 넘버링이고 index l은 generator의 distribution에서 샘플링하는 횟수에 대한 넘버링입니다. VAE는 한정된 몬테카를로 샘플링을 통해 효과적으로 optimization을 수행합니다.\n\n\n\nRecontruction error term6\n\n\n\n2.2.1 Reparametrization Trick\n위에서 Reconstruction error를 구하기 위해 샘플링하는 과정에서 backpropation을 하기 위해 Reparametrization trick을 사용하게 됩니다. 단순히 정규분포에서 샘플링 하면 random node인 z에 대해서 gradient를 계산할 수 없기 때문에 random성을 정규분포에서 샘플링 되는 ϵ으로 만들어주고 이를 reparametrization을 해주어서 deterministic node가 된 z를 backpropagation 할 수 있게 됩니다.\n\n\n\nReparametrization trick6\n\n\n# sampling by re-parameterization technique\nz = mu + sigma * tf.random_normal(tf.shape(mu), 0, 1, dtype=tf.float32)\nz를 샘플링하는 generator의 distribution은 Bernoulli로 디자인할 경우 NLL이 Cross Entropy가 되며 Gaussian 분포로 디자인할 경우 MSE가 되어서 보통 계산하기 용이한 2개의 분포 중 하나를 사용하게 됩니다. 모델 디자인의 조건은 데이터의 분포에 따라 결정되는데 데이터의 분포가 continuous 하다면 Gaussian 분포에 가깝기 때문에 Gaussian으로 디자인하고, 데이터의 분포가 discrete 하다면 Bernoulli분포에 가깝기 때문에 Bernoulli로 디자인합니다.\n\n\n\nTypes of generator distributions6"
  },
  {
    "objectID": "posts/paper/2025-08-05-micro-lie.html",
    "href": "posts/paper/2025-08-05-micro-lie.html",
    "title": "📃Micro Lie Theory 리뷰",
    "section": "",
    "text": "Paper Link"
  },
  {
    "objectID": "posts/paper/2025-08-05-micro-lie.html#lie-군과-lie-대수-기본-개념-복습",
    "href": "posts/paper/2025-08-05-micro-lie.html#lie-군과-lie-대수-기본-개념-복습",
    "title": "📃Micro Lie Theory 리뷰",
    "section": "2.1 Lie 군과 Lie 대수 기본 개념 복습",
    "text": "2.1 Lie 군과 Lie 대수 기본 개념 복습\nLie 군(Lie group)은 매끄러운(manifold) 곡면 위에 그룹 구조를 결합한 수학 객체입니다. 구체적으로, Lie 군 G는 미분 가능한 매니폴드(국소적으로 평탄한 공간)로서 그 원소들이 그룹의 네 가지 공리(폐쇄성, 항등원, 역원, 결합법칙)를 만족합니다. 다시 말해, Lie 군은 국소적으로는 선형 공간처럼 미분 연산이 가능하면서도 전역적으로는 비선형 결합(composition)이 허용되는 구조를 갖습니다. 대표적인 예로 3차원 회전의 공간 SO(3)나 로봇 자세(pose)를 나타내는 SE(3) 등이 있습니다.\n매니폴드란 국소적으로 유클리드 공간과 유사하지만 전역적으로는 곡률 등의 제약이 있는 공간입니다. 예를 들어, 단위 노름을 갖는 4차원 벡터들의 집합(유니터니언)은 4차원 구면 S^3을 이루며, 이는 단위 쿼터니언의 공간이 됩니다. 이처럼 로봇 상태가 충족해야 할 제약(예: 쿼터니언의 단위 노름)은 매니폴드를 정의하며, 우리의 상태 벡터는 이 매끄러운 곡면 위를 움직인다고 볼 수 있습니다. 중요한 점은, 매니폴드 위의 모든 점마다 고유한 접공간(tangent space)이 존재하고, 이 접공간은 그 점을 스쳐 지나가는 평면으로 이해할 수 있습니다. 접공간은 선형 벡터 공간이므로 우리가 그 위에서는 미분이나 선형대수 계산을 자유롭게 할 수 있습니다.\n한편, Lie 군에는 한 특별한 원소(항등원 E)와 그에 대응하는 접공간이 존재합니다. 이를 Lie 대수(Lie algebra)라고 부르며, 기호로 \\mathfrak{g}=T_E G로 나타냅니다. Lie 대수는 차원이 Lie 군의 자유도와 같고, 항등원에서의 접공간이기 때문에 벡터 공간의 구조를 가집니다. 특히 Lie 대수의 원소들은 종종 \\mathbb{R}^n의 벡터로 간주할 수 있는데, 이는 \\mathfrak{g} \\simeq \\mathbb{R}^n (벡터 공간으로 동형)이라는 의미입니다. 예를 들어 회전군 SO(3)의 Lie 대수인 so(3)은 3\\times3 반대칭 행렬들의 공간이지만, 이를 축각(axis-angle) 3-벡터로 대응시킬 수 있어서 \\mathbb{R}^3와 동형입니다. 이러한 벡터와 행렬 간 변환을 편리하게 하기 위해 해트 연산(^\\wedge)과 브이 연산(^\\vee)이 사용됩니다. 3-벡터 \\omega = [\\omega_x,\\omega_y,\\omega_z]^\\top에 대해 해트 연산은 so(3)의 원소인 [\\omega]_\\times (skew-symmetric matrix)을 만들고, 브이 연산은 그 반대로 행렬을 벡터로 돌려놓습니다. 이를 통해 Lie 대수 원소와 유클리드 벡터를 자유롭게 넘나들며 계산할 수 있습니다.\n이제 지수 맵(Exponential)과 로그 맵(Logarithm)을 소개합니다. 지수 맵 \\text{Exp}: \\mathfrak{g} \\to G는 Lie 대수의 원소(접공간의 벡터)를 Lie 군의 원소로 변환하는 함수이고, 로그 맵 \\text{Log}: G \\to \\mathfrak{g}는 그 역변환입니다. 이는 일반적인 행렬 지수함수(\\text{Exp})로 정의되며, 작은 변화량을 곡면 위의 유한한 움직임으로 바꿔주는 역할을 합니다. 예를 들어 so(3)에서 \\text{Exp}([\\omega]_\\times)는 \\omega만큼 회전하는 회전행렬 R을 생성하며, 이는 로드리게스 공식으로 주어집니다. 로드리게스 공식에 따르면 |\\omega|를 회전 크기로 할 때:\n\\text{Exp}_{SO(3)}(\\omega^\\wedge) = I + \\frac{\\sin\\|\\omega\\|}{\\|\\omega\\|}[\\omega]_\\times + \\frac{1-\\cos\\|\\omega\\|}{\\|\\omega\\|^2}[\\omega]_\\times^2,\n이는 \\omega가 충분히 작을 때 \\text{Exp}(\\omega^\\wedge) \\approx I + [\\omega]_\\times로 근사되며 익숙한 소(小)각도 근사와 일치합니다. 마찬가지로 \\text{Log}(R)는 주어진 군 원소를 다시 Lie 대수의 벡터(회전벡터)로 돌려놓습니다. 즉, \\text{Exp}와 \\text{Log} 덕분에 비선형 곡면인 Lie 군과 선형 공간인 Lie 대수를 서로 연결할 수 있으며, Lie 군상의 문제를 Lie 대수상의 문제로 변환해서 풀 수 있게 됩니다. 이러한 변환을 활용하면, 복잡한 제약을 직접 다루는 대신 간단한 선형 공간에서 계산을 수행한 뒤 결과를 다시 매니폴드로 옮기는 방법이 가능해집니다. Solà 등의 논문이 제시하는 “micro Lie theory” 역시 Lie 군의 깊은 이론 중 실용적인 핵심만 뽑아 쓴 것으로, Lie 군과 Lie 대수를 왕복하는 기본 도구들만으로도 로봇 상태 추정에 충분한 정밀도를 얻을 수 있음을 보여줍니다."
  },
  {
    "objectID": "posts/paper/2025-08-05-micro-lie.html#상태-추정에서의-manifold-상태-표현과-oplus-연산-retraction",
    "href": "posts/paper/2025-08-05-micro-lie.html#상태-추정에서의-manifold-상태-표현과-oplus-연산-retraction",
    "title": "📃Micro Lie Theory 리뷰",
    "section": "2.2 상태 추정에서의 Manifold 상태 표현과 \\oplus 연산 (Retraction)",
    "text": "2.2 상태 추정에서의 Manifold 상태 표현과 \\oplus 연산 (Retraction)\n로봇의 상태가 유클리드 공간 \\mathbb{R}^n에 국한되지 않고 곡면 위에 놓이는 경우, 이를 다루기 위해 특수한 연산이 필요합니다. 예를 들어 드론의 자세(orientation)를 나타내는 단위 쿼터니언은 4차원 벡터이지만 항상 단위 노름을 가져야 하므로 임의의 4차원 변화량을 더할 수 없습니다. 이 때 사용하는 개념이 바로 Lie 군상의 retraction, 즉 \\oplus 연산입니다. 유클리드 공간에서 상태 업데이트를 x_{\\text{new}} = x_{\\text{old}} + \\Delta x로 한다면, Lie 군에서는 이를 대체하는 연산으로 X_{\\text{new}} = X_{\\text{old}} \\oplus \\Delta를 정의하는 것입니다. Solà 논문에서는 이를 플러스(\\oplus) 연산자로 표기하며, 한 번의 지수 맵(Exp)과 군 합성(\\circ)으로 구현됩니다. 구체적으로 오른쪽 플러스 (right-⊕)를 기준으로 하면 다음과 같습니다:\nY = X \\oplus \\delta := X \\circ \\text{Exp}(\\delta), \\qquad \\delta = Y \\ominus X := \\text{Log}(X^{-1}\\circ Y).\n즉, Lie 군 원소 X에 접공간의 작은 벡터 \\delta를 지수맵으로 군 원소화한 \\text{Exp}(\\delta)를 오른쪽에서 곱해 새로운 원소 Y를 얻는 것이 X\\oplus\\delta입니다. 반대로 두 원소 Y, X \\in G 사이의 “차이”를 나타내는 마이너스(\\ominus) 연산 Y \\ominus X는 X^{-1}Y라는 군 연산의 결과를 로그맵으로 벡터화한 것입니다. 이러한 \\oplus, \\ominus 연산자를 도입함으로써, Lie 군상에서 일반 덧셈이 불가능한 경우에도 마치 벡터 더하기/빼기처럼 상태의 합성과 차이를 표현할 수 있습니다. 특히 \\oplus 연산은 retraction(리트랙션)으로 볼 수 있는데, 이는 최적화나 필터링 맥락에서 국소 좌표계로 이동했다가 다시 매니폴드로 복원하는 함수를 의미합니다. 여기서는 정확한 지수맵 \\text{Exp}를 사용하여 retraction을 구현하므로, 매우 엄밀한 방식으로 상태를 업데이트하게 됩니다.\n\n\n\n\n예를 들어, 위 그림은 매니폴드 곡면 (예: 단위 구면) 위의 한 상태 X에 대해 \\oplus 연산의 기하학적 의미를 보여줍니다. 점 X가 매니폴드 위에 주어졌을 때, 그 접평면(회색) 위의 한 벡터 \\tau (빨간 화살표)를 취해 지수맵을 적용하면 곡면 위의 한 점으로 사상됩니다. X \\oplus \\tau = X \\cdot \\text{Exp}(\\tau)의 결과로 얻어진 새로운 점 X'이 파란 화살표의 머리로 표시되어 있습니다. 이 과정은 곡면 위의 X에서 시작하여 접공간을 따라 \\tau만큼 움직였다가 다시 곡면으로 돌아오는 동작에 해당합니다. 즉, \\tau가 작다면 X'는 X에서 조금 이동한 위치가 되며, 항상 곡면 위에 머무르기 때문에 상태 제약이 자동으로 유지됩니다. 그림 상단의 I는 군의 항등원으로, 이때 \\tau가 항등원 근처(글로벌 기준)와 X 근처(로컬 기준)에서 동일하게 취급됨을 시사합니다. 이는 Lie 군에서는 모든 접공간이 본질적으로 동일한 구조를 가지기 때문이며, 곧 설명할 좌/우 플러스의 차이와 Adjoint와도 연결되는 개념입니다.\n\n\\oplus 연산에는 오른쪽-플러스와 왼쪽-플러스 두 가지 버전이 있습니다. 이는 Lie 군의 비가환성(non-commutativity) 때문에 발생하는 구분으로, 변화량을 왼쪽에서 곱하느냐 오른쪽에서 곱하느냐에 따라 달라집니다. 예를 들어 오른쪽-플러스에서는 X \\oplus \\delta = X\\text{Exp}(\\delta)인 반면, 왼쪽-플러스는 X \\oplus^L \\delta = \\text{Exp}(\\delta)X와 같이 정의될 수 있습니다. 두 방식 모두 허용되지만, 논문에서는 로컬 좌표계에서의 표현에 맞게 오른쪽-플러스를 기본으로 사용합니다. 쉽게 말해, 현재 추정값 X를 기준 좌표로 삼고 그 접공간에서 오차를 표현하는 방식을 택한 것입니다. 이렇게 하면 X 자체가 변할 때 접공간도 함께 움직이므로 (접평면이 항상 X에 붙어다님), 오차의 해석이 국소적으로 이루어집니다. 다른 문헌에서는 항등원에 대한 전역 좌표로 오차를 표현하기도 하지만, 그 경우에도 두 표현은 Adjoint 변환으로 상호 변환 가능함이 알려져 있습니다. 즉, 전역적인 오차 \\delta_E와 국소적인 오차 \\delta_X 사이에는 \\delta_X = \\text{Ad}_X^{-1},\\delta_E 관계가 있으며, 공분산 등의 변환에도 활용됩니다. Solà 등의 설명에 따르면 이 논문 및 여러 최신 방법들은 로컬 perturbation X \\oplus \\delta를 사용하며, 만약 다른 접근법에서 전역 오차(E \\oplus \\delta 형태)를 사용하더라도 최종 결과에서 차이는 Adjoint로 보정될 수 있습니다.\n정리하자면, \\oplus 연산은 현재 상태에 작은 Lie 대수 오차를 적용하여 상태를 업데이트하는 연산이며, 이를 통해 필터나 최적화에서 항상 유효한 상태 (Lie 군 원소)를 유지할 수 있습니다. 반대로 \\ominus 연산은 두 상태 간의 상대적인 오차를 Lie 대수 벡터로 산출하여 접공간상의 차로 표현해줍니다. 이 두 연산을 도입함으로써 우리는 마치 유클리드 공간에서 하듯 상태를 더하고 빼며 오차를 정의할 수 있게 되었고, 이러한 아이디어는 오차 상태(error-state) 칼만 필터로도 불리는 현대 로봇 상태 추정 필터들의 토대가 되었습니다."
  },
  {
    "objectID": "posts/paper/2025-08-05-micro-lie.html#lie-이론을-활용한-상태-추정-오차-표현과-필터-구성",
    "href": "posts/paper/2025-08-05-micro-lie.html#lie-이론을-활용한-상태-추정-오차-표현과-필터-구성",
    "title": "📃Micro Lie Theory 리뷰",
    "section": "2.3 Lie 이론을 활용한 상태 추정: 오차 표현과 필터 구성",
    "text": "2.3 Lie 이론을 활용한 상태 추정: 오차 표현과 필터 구성\n이제 위에서 정의한 \\oplus, \\ominus 개념을 실제 칼만 필터와 같은 상태 추정 문제에 어떻게 적용하는지 알아보겠습니다. 핵심 아이디어는 참된 상태(true state)와 추정 상태(estimate) 사이의 오차(perturbation)를 Lie 대수의 벡터로 표현하고 추적하는 것입니다. 예를 들어, 추정값을 X (Lie 군의 원소)라 하고 실제 값을 X^*라고 하면, 두 상태 간 오차를 다음과 같이 정의할 수 있습니다:\n\\tilde{\\xi} := X^* \\ominus X = \\text{Log}(X^{-1} X^*) \\in \\mathfrak{g},\n여기서 \\tilde{\\xi}는 Lie 대수상의 작은 벡터로서, 추정에서 실제로 가는 “오차 상태”를 나타냅니다. X^* = X \\oplus \\tilde{\\xi}로 풀어쓰면 X^* = X \\text{Exp}(\\tilde{\\xi})가 되며, 실제 상태는 추정 상태에 작은 지수 이동을 가한 것으로 표현됩니다. 이 오차 벡터를 상태로 삼아 칼만 필터를 수행하면, 필터의 추정은 항상 X 주변의 국소 선형 공간에서 이루어집니다. 다시 말해, 필터는 \\tilde{\\xi}라는 접공간의 가우시안 상태를 추적하며, 필요한 경우 이것을 \\text{Exp}를 통해 다시 군상의 X를 보정하는 방식으로 동작합니다. 이런 접근을 흔히 오차-상태 Kalman 필터라고 하며, Lie 군 이론을 적용한 필터에서는 자연스럽게 이러한 구조가 나타나게 됩니다.\nSolà 논문에서는 이러한 Lie 군 기반 필터의 절차를 유도하고, 그것이 전통적인 EKF와 거의 동일한 형태를 가짐을 보여줍니다. 우선 예측 단계를 생각해봅시다. 로봇의 상태가 시간에 따라 변화하는 모델이 주어졌다고 할 때, 만약 상태가 Lie 군 원소라면 그 상태 천이 역시 군 연산으로 표현됩니다. 예를 들어 임의의 작은 시간 \\delta t 동안 상태가 변화하는 미분방정식 \\dot{X}(t) = f(X(t), u(t))가 있다면, 이를 적분하여 이산화할 때 \\oplus 연산을 이용한 누적곱 형태로 나타낼 수 있습니다:\nX_{k+1} = X_k \\oplus \\tau_k = X_k \\circ \\text{Exp}(\\tau_k), \\qquad \\text{여기서 } \\tau_k \\approx f(X_k, u_k)\\,\\delta t_k\n즉 이전 상태 X_k에 각 시간구간의 작은 변화량 \\tau_k (Lie 대수 벡터)를 지수지도를 통해 적용함으로써 다음 상태를 얻습니다. 구체적인 예로, X가 3차원 회전행렬 R이고 제어입력으로 각속도 \\omega가 주어지는 경우, R_{k+1} = R_k \\text{Exp}([\\omega_k \\delta t]*×)와 같이 예측이 이루어집니다. 이 식은 R*{k+1} = R_k (I + [\\omega_k \\delta t]_\\times)로 1차 근사할 수 있으며, 소위 정확한 미분적분 방식을 제공하여, 유클리드 공간에서 오일러 각을 더하는 등의 근사보다 안정적이고 정확한 예측을 가능하게 합니다.\n예측 단계의 공분산 전파를 위해서는 선형화(Jacobian)가 필요합니다. Lie 군에서는 상태 천이가 비선형이지만, 오차 상태 \\tilde{\\xi}의 관점에서는 이를 선형화할 수 있습니다. X_{k+1} = X_k \\text{Exp}(\\tau_k)를 X_k와 \\tau_k에 대해 미소 변화시켜 Jacobian을 구하면, 이는 대략적으로 F_k = \\frac{\\partial (X_k \\circ \\text{Exp}(\\tau_k))}{\\partial \\tilde{\\xi}_k}와 같은 계산을 Lie 대수 공간에서 수행하는 것이 됩니다. 구체적인 Jacobian 계산은 다음 절에서 다루겠지만, 여기서는 결과적으로 Lie 군 기반 예측 공식이 기존 EKF의 형태와 거의 유사하게 나타난다는 점을 강조하겠습니다. 실제로 논문에서도 “이러한 Jacobian들을 사용하면 Lie 군에서의 불확실성 관리 공식이 벡터 공간에서와 매우 유사한 형태를 띤다”고 언급하고 있습니다. 이는 우리가 칼만 필터의 예측/갱신 공식을 거의 그대로 사용하되, 상태 더하기(+)를 \\oplus로 바꾸고, 필요한 Jacobian 행렬들만 새롭게 계산해주면 된다는 뜻입니다. 요컨대, 틀은 동일하고 내용물만 Lie 군에 맞게 조정되는 것입니다.\n다음으로 갱신 단계를 생각해보겠습니다. 로봇 센서로부터 관측된 값 z가 상태 X에 대한 어떤 함수 h(X)로 주어진다고 할 때, EKF에서는 잔차 y = z - h(\\hat{x}) 및 관측 Jacobian H = \\partial h/\\partial x 등을 구해서 칼만 이득을 계산하고 추정값을 보정합니다. Lie 군에서도 마찬가지로, 예측 관측값 h(X_{k|k-1})와 실제 관측 z_k의 차이를 \\ominus 연산으로 정의합니다. 예컨대 \\tilde{y} := z_k \\ominus h(X_{k|k-1}) = \\text{Log}!\\big(h(X_{k|k-1})^{-1} \\circ z_k\\big) 같은 형태로 잔차를 구할 수 있습니다. 이는 관측값이 만약 Lie 군 (예: 카메라로 본 로봇의 자세 측정이 또 하나의 Lie 군 값일 때)이라면 필요하고, 일반적인 스칼라나 벡터 관측의 경우에는 보통 \\ominus를 실수 뺄셈으로 대체하면 됩니다. 핵심은 상태와 관측을 동일한 국소 좌표계로 사상하여 비교한다는 점입니다. 관측 함수의 Jacobian H도 h: G \\to \\mathbb{R}^m의 미분을 계산하여 구하는데, 이 역시 \\frac{\\partial h}{\\partial X}(X) = \\lim_{\\tau\\to0} \\frac{h(X\\oplus \\tau) \\ominus h(X)}{\\tau}로 정의할 수 있습니다. 이렇게 얻은 관측 Jacobian H와 앞서의 예측 Jacobian F 등을 사용하면, 칼만 필터의 공분산 예측/갱신 공식은 기존과 동일하게 적용됩니다:\n\n예측: P_{k|k-1} = F_k,P_{k-1|k-1},F_k^\\top + Q_k\n갱신: K_k = P_{k|k-1} H_k^\\top (H_k P_{k|k-1} H_k^\\top + R_k)^{-1},\n상태 보정: \\hat{X}*{k|k} = \\hat{X}*{k|k-1} \\oplus (K_k \\tilde{y}_k),\n공분산 보정: P_{k|k} = (\\mathbb{I} - K_k H_k) P_{k|k-1},\n\n여기서 Q_k, R_k는 과정 및 관측 잡음 공분산이고, K_k는 칼만 이득입니다. 위에서 상태 보정 단계에 \\oplus 연산이 사용된 것에 주목하세요. 필터가 계산한 오차 상태 추정 K_k \\tilde{y}_k (접공간 벡터)를 \\oplus를 통해 실제 추정값에 반영함으로써, 새로운 \\hat{X}는 항상 유효한 Lie 군 원소로 유지됩니다. 이처럼 Lie 군을 사용한 칼만 필터는 구조적으로는 기존 EKF와 동일하지만, 내부 연산을 Lie 군에 맞게 조정하여 곡면 위의 상태도 일관되게 추정할 수 있게 해줍니다. Solà 등은 이를 두고 “우리의 미소 Lie 이론을 활용하면 결과적으로 얻어지는 필터 공식이 표준 EKF 공식과 거의 닮은 꼴”이라고 설명합니다.\n마지막으로, 이러한 방법론은 SLAM, 비주얼 오도메트리 등 다양한 모션 추정 분야에서 실용적인 가치를 입증하고 있습니다. 논문에는 몇 가지 응용 예제와 함께, 주요 Lie 군(SO(2), SO(3), SE(3), 쿼터니언 등)에 대한 수식 치트시트가 포함되어 있어, 실무자가 바로 활용할 수 있도록 했습니다. 또한 저자들은 이 이론을 구현한 C++ 라이브러리 manif를 공개하여, 개발자들이 보다 쉽게 Lie 이론 기반 필터를 적용할 수 있도록 지원하고 있습니다. 다음 절에서는 앞서 언급된 Jacobians(미분) 계산에 대해 조금 더 자세히 살펴보겠습니다."
  },
  {
    "objectID": "posts/paper/2025-08-05-micro-lie.html#lie-군-위의-미분-jacobian-계산-방법",
    "href": "posts/paper/2025-08-05-micro-lie.html#lie-군-위의-미분-jacobian-계산-방법",
    "title": "📃Micro Lie Theory 리뷰",
    "section": "2.4 Lie 군 위의 미분: Jacobian 계산 방법",
    "text": "2.4 Lie 군 위의 미분: Jacobian 계산 방법\n상태 추정에서 핵심은 선형화, 즉 Jacobian 행렬을 올바르게 구하는 것입니다. Lie 군에서는 입력과 출력이 모두 곡면 위에 있으므로, 그 미분 정의를 약간 변형해야 합니다. 일반적인 다변수 함수의 Jacobian은 J = \\frac{\\partial f(x)}{\\partial x} = \\lim_{h\\to0}\\frac{f(x+h)-f(x)}{h}로 정의되지요. Lie 군에서는 뺄셈 대신 \\ominus를 사용하면 유사한 정의를 내릴 수 있습니다. 함수 f: M \\to N가 Lie 군 M의 원소를 받아 N (또 다른 Lie 군 혹은 벡터 공간)의 원소를 반환한다고 할 때, X \\in M에서의 Jacobian \\frac{\\partial f}{\\partial X}(X)를 정의하는 한 가지 방법은 아래와 같습니다:\n\\frac{\\partial f}{\\partial X}(X) ~:=~ \\lim_{\\tau \\to 0} \\frac{\\,f(X \\oplus \\tau)\\; \\ominus\\; f(X)\\,}{\\tau}\\,\n여기서 \\tau \\in T_X M는 X의 접공간에서 임의의 방향으로의 작은 변화입니다. 분자의 f(X \\oplus \\tau)\\ominus f(X)는 f 출력 공간의 접공간에 놓인 벡터가 되며, 이를 \\tau로 나누어 \\tau \\to 0 극한을 취하면 두 접공간 사이의 선형 사상(matrix)으로 수렴합니다. 이때 분자에서 \\ominus를 취한 덕분에 f(X)의 변화량 역시 출력 공간의 국소적 선형 좌표로 표현된 점에 유의하세요. 이렇게 정의된 Jacobian은 입력 X의 국소 접공간에서 출력 f(X)의 국소 접공간으로 매핑되는 m\\times n 행렬이며, 우리가 익히 아는 Jacobian 개념을 Lie 군 상황에 맞게 일반화한 것에 해당합니다.\n이 정의는 개념적으로는 간단하지만, 실제 Jacobian을 계산할 때는 보통 함수 f를 이루는 기본 연산들에 대한 미분을 조합하는 방식이 더 효율적입니다. 마찬가지로 Solà 논문에서도 “역함수(Inversion), 합성(Composition), 지수(Exponentiation), 작용(Action) 등의 부분적 미분 블록을 이용하면 임의의 미분을 체인 룰로 쉽게 계산할 수 있다”고 설명합니다. 즉, Lie 군에서 자주 등장하는 기본 함수들의 Jacobian을 미리 유도해 두고, 이를 조합하면 임의의 복잡한 f의 Jacobian도 구할 수 있다는 것입니다. 예를 들어, 군의 곱 Y = X \\circ U에 대해 입력 X에 대한 미분이나 U에 대한 미분, 역원 X^{-1}에 대한 미분, 로그/지수 맵에 대한 미분 등이 그러한 기본 블록들입니다. 이런 미분들을 구하는 과정에서 등장하는 것이 바로 Adjoint(아조인트)와 Left Jacobian(왼쪽 야코비안) 같은 개념입니다.\n\nAdjoint 행렬 \\text{Ad}_X: 이는 Lie 군 G에서 특정 원소 X가 Lie 대수 공간에 갖는 작용을 나타냅니다. 직관적으로, \\text{Ad}_X: \\mathfrak{g} \\to \\mathfrak{g}는 항등원에서의 작은 움직임을 X에서의 작은 움직임으로 변환해주는 선형 사상입니다. 공식적으로는 \\text{Ad}_X(\\tau) = \\frac{d}{d\\epsilon}\\big|_{\\epsilon=0} X \\circ \\text{Exp}(\\epsilon\\tau)\\circ X^{-1}로 정의할 수 있습니다. 실무적으로 중요한 특성은, \\text{Ad}_X가 전역 접공간과 로컬 접공간 사이의 좌표 변환 행렬이라는 점입니다. 예를 들어 앞서 언급한 전역 vs 로컬 perturbation 전환에서 \\delta_X = \\text{Ad}_X^{-1}\\delta_E라는 식으로 나타났던 바로 그 \\text{Ad}가 여기에 등장합니다. \\text{Ad}_X의 구체적인 형태는 Lie 군마다 다른데, 예를 들어 SE(3) (3차원 강체 변환군)의 경우 다음과 같습니다:\n\\text{Ad}_{(R,t)} = \\begin{pmatrix} R & -[t]_\\times R \\\\ 0 & R \\end{pmatrix} \\in \\mathbb{R}^{6\\times6}\n여기서 X=(R,t)은 회전 R과 병진 t로 구성된 SE(3) 원소입니다. 이 행렬은 SE(3)의 Lie 대수 원소 (\\rho,\\theta) (병진 \\rho, 회전 \\theta)에 작용하여, X 좌표계에서 본 새로운 Lie 대수 값으로 변환하는 역할을 합니다. Adjoint 행렬은 군의 합성에 대한 미분을 다룰 때 유용합니다. 예를 들어 Y = X\\circ U에 대해 X 쪽의 변화 \\delta X가 출력 Y에 주는 영향은 U의 Adjoint를 통해 \\delta Y = \\delta X \\circ U = \\text{Ad}_U(\\delta X)로 표현할 수 있습니다. 반대로 U의 변화는 \\delta Y = X \\circ \\delta U = \\text{Ad}_X(\\delta U)로 나타나죠. 이처럼 Adjoint는 Lie 군의 곱셈 구조로 인한 미분적 상호작용을 선형 연산으로 옮겨주는 역할을 합니다.\n왼쪽 Jacobian J_l (및 오른쪽 Jacobian J_r): 이는 흔히 Lie 대수에서 Lie 군으로의 지수 맵 미분에 등장하는 행렬입니다. 예를 들어 R = \\text{Exp}(\\theta) (SO(3)에서)라고 할 때, \\theta의 작은 변화가 R에 주는 영향을 나타내는 미분이 왼쪽 Jacobian J_l(\\theta)입니다. Taylor 전개 관점에서 \\text{Exp}(\\theta+\\delta\\theta) \\approx \\text{Exp}(\\theta),J_l(\\theta),\\delta\\theta로 정의할 수 있습니다. SO(3)의 J_l(\\theta)에 대한 폐형식 해도 알려져 있으며, 다음과 같은 멱급수로 표현됩니다:\nJ_l(\\theta) = I - \\frac{1-\\cos\\|\\theta\\|}{\\|\\theta\\|^2}[\\theta]_\\times + \\frac{\\|\\theta\\| - \\sin\\|\\theta\\|}{\\|\\theta\\|^3}[\\theta]_\\times^2,\n이는 \\theta가 0에 가까울 때 J_l(\\theta) \\to I로 수렴하며, 회전 각도가 커질수록 J_l이 편차의 크기를 보정해주는 역할을 합니다. 오른쪽 Jacobian J_r(\\theta)는 이와 유사하지만, \\text{Exp}(\\theta)R 같은 오른쪽 곱 상황의 미분에 나타나는 행렬입니다. 사실 J_r(\\theta)와 J_l(\\theta)는 서로 전치관계(J_r(\\theta) = J_l(-\\theta))에 있습니다. 이러한 Jacobian 행렬은 불확실성 전파에 중요하게 활용되는데, 예를 들어 각도 공간의 공분산을 회전행렬 공간의 공분산으로 변환할 때 P_R = J_l(\\theta),P_\\theta,J_l(\\theta)^\\top와 같이 사용됩니다. 논문의 설명에 따르면 대부분의 파생된 Jacobian은 오른쪽 미분에 기반하고, 필요한 경우 왼쪽 Jacobian도 별도로 제공하고 있습니다.\n\n정리하면, Lie 군에서의 Jacobian 계산은 (1) 우선 각 개별 연산(Exp, Log, 곱, 역 등)의 미분 공식을 알고, (2) 체인 룰을 적용하여 복잡한 함수의 Jacobian을 구성하는 방식으로 이루어집니다. 이렇게 하면 자칫 직접 미분하면 실수하기 쉬운 부분들도 블록 조립하듯 안전하게 얻을 수 있습니다. Solà 논문 부록에는 주요 Lie 군들에 대한 거의 모든 필요한 미분 공식이 나열되어 있는데, 이는 실무자가 EKF나 최적화 알고리즘을 구현할 때 큰 도움이 됩니다. 예를 들어, h(X) = X \\cdot p (Pose X가 3D 점 p에 작용) 같은 단순한 경우부터 IMU의 복잡한 상태 천이에 이르기까지, 미리 유도된 Jacobian들을 모아두면 프로토타이핑 속도와 신뢰성이 크게 향상됩니다. 실제로 Manif 라이브러리에는 이러한 Jacobian 계산이 모두 구현되어 있어, 사용자가 일일이 미분 공식을 유도할 필요 없이 함수를 호출해 사용할 수 있습니다.\n참고로, Jacobian을 수기로 유도하는 작업은 매우 번거롭고 오류가 잦기 때문에, 최근에는 자동 미분이나 수치 미분을 활용하는 경우도 많습니다. 그러나 Lie 이론을 한 번 익혀 두면, 자동 미분 없이도 문제를 해석적으로 풀 수 있고 보다 깊은 이해를 얻을 수 있다는 장점이 있습니다. 논문의 저자들도 “Lie 이론을 쓰면 빠르게 최종 Jacobian을 얻을 수 있지만 수학이 많이 필요하며, 체인 룰을 직접 쓰는 편이 더 쉬울 수도 있다”는 취지의 언급을 하고 있습니다. 이는 결국 개발자가 선호하는 방식에 달렸지만, 원리를 알고 쓰는 것과 모르고 쓰는 것의 차이는 결과의 신뢰성과 확장성에서 드러날 것입니다."
  },
  {
    "objectID": "posts/paper/2025-08-05-micro-lie.html#유클리드-기반-필터와의-비교-무엇이-다르고-어떤-장점이-있나",
    "href": "posts/paper/2025-08-05-micro-lie.html#유클리드-기반-필터와의-비교-무엇이-다르고-어떤-장점이-있나",
    "title": "📃Micro Lie Theory 리뷰",
    "section": "2.5 유클리드 기반 필터와의 비교 – 무엇이 다르고 어떤 장점이 있나?",
    "text": "2.5 유클리드 기반 필터와의 비교 – 무엇이 다르고 어떤 장점이 있나?\n이제 Lie 이론 기반의 필터가 기존의 유클리드 공간 EKF 등과 어떻게 다른지, 또 어떤 장점을 갖는지 요약해보겠습니다. 핵심 차이는 당연히 상태를 다루는 방식입니다. 전통적인 필터에서는 상태를 하나의 벡터로 보고 그 위에서 + 연산을 수행합니다. 하지만 만약 상태가 본질적으로 \\mathbb{R}^n가 아닌 곡면(예: 회전)은, 기존 방식은 몇 가지 문제를 일으킵니다. 아래에 Lie 군 기반 접근의 차별점과 장점을 정리했습니다.\n\n상태 제약의 자연스러운 유지: Lie 군 필터에서는 상태 업데이트를 \\oplus와 \\text{Exp}로 수행하므로, 추정한 상태가 항상 유효한 군 원소로 유지됩니다. 예를 들어, 쿼터니언을 사용해 자세를 나타낼 때, 기존 EKF는 보정시 벡터에 \\Delta q를 더한 후 재규격화(normalize)해야 하지만 Lie EKF에서는 q \\leftarrow q \\oplus \\Delta q = q \\cdot \\text{Exp}(\\Delta q)로 업데이트함으로써 단위 노름 조건이 자동으로 보존됩니다. 이는 구현상 실수를 줄이고, 수학적으로도 일관성(consistency)을 확보해줍니다.\n큰 회전/변위에 대한 정확한 선형화: 유클리드 필터는 상태가 크게 변할 경우 선형화 오차가 커질 수 있습니다. 반면 Lie 이론을 사용하면, 예를 들어 90^\\circ 회전도 축-각 벡터 (\\pi/2)로 정확히 표현하고 지수맵으로 반영할 수 있습니다. 따라서 변화가 클 때도 오차를 적절히 반영할 수 있어 필터의 안정성이 향상됩니다. 이러한 특성 덕분에 SLAM/비주얼-관성 항법 등의 대규모 변환이 수반되는 문제에서 필터의 성능이 향상되었다는 보고가 다수 있습니다.\n통합적인 수학적 프레임워크: Lie 군은 다양한 유형의 상태(예: 2D/3D 회전, 위치, 확장된 포즈 등)를 하나의 이론으로 아우릅니다. 기존에는 각 경우마다 좌표 표현을 달리하며 EKF 공식을 유도해야 했지만, Lie 이론을 적용하면 하나의 통일된 틀 안에서 모든 경우를 다룰 수 있습니다. Solà 논문이 제공하는 것처럼, 주요 Lie 군에 대한 공통된 연산 표기와 공식이 있으므로 학습곡선도 완만해집니다. 예를 들어, SO(2), SO(3), SE(3), \\mathbb{R}^n (트리비얼 군) 등을 모두 동일한 \\oplus, \\ominus 표기로 다룰 수 있고, 필요한 Jacobian들도 형태는 유사한 구조를 가집니다.\n체계적인 불확실성 전파: Lie 군 기반 필터에서는 공분산이 항상 접공간 상에 정의됩니다. 이는 곡면의 곡률을 직접 다루지 않고도 불확실성을 다룰 수 있게 해주며, 필요할 경우 접공간 상의 가우시안을 다시 군상에 랩핑(wrapping)해서 해석할 수 있습니다. 예를 들어, 아래 그림에서 빨간 타원은 접공간에서의 공분산 등고선이고, 이를 지수맵으로 곡면에 투영하면 파란 리본 모양으로 나타납니다. 이러한 시각화는 곡면 위의 불확실성 영역을 직관적으로 보여주며, 유클리드 필터에서는 얻기 어려운 통찰을 제공합니다. 더 나아가, Adjoint와 left Jacobian 등을 이용해 좌표계 변환에 따른 공분산 이식도 엄밀하게 수행할 수 있습니다. 이는 복잡한 로봇 센서 융합 시스템에서 서로 다른 표현 사이의 불확실성 변환을 일관되게 해주는 장점입니다.\n기존 알고리듬과의 유사성 (쉬운 이식성): 앞서 설명했듯, Lie EKF의 수식은 기존 EKF와 거의 같은 형태를 띱니다. 따라서 이미 칼만 필터나 그래프 최적화 등에 익숙한 실무자라면 기존 코드를 약간 수정하는 것만으로 Lie 군 버전으로 옮길 수 있습니다. 예를 들어, Plus 연산을 지원하는 데이터 구조(쿼터니언, 변환행렬 등)를 만들고, 칼만 필터의 업데이트 부분에서 벡터 합 대신 \\oplus를 호출하게 바꾸는 정도의 수정이면 됩니다. Solà 등은 “Lie 이론을 적용해도 불확실성 관리 공식이 벡터 공간의 경우와 거의 닮아 있다”고 강조하며, 독자들이 거부감 없이 기존 알고리듬을 확장하도록 돕고 있습니다.\n이론적으로 검증된 정확성: Lie 군 필터는 근본적으로 미분 기하학에 기반하고 있어, 특이점(singularity)이나 좌표계 의존성 등의 문제가 최소화됩니다. 이는 특히 3차원 회전 같이 전통적으로 특이점 문제가 있던 경우 (예: 오일러 각의 짐벌락 문제)에도 강인합니다. 또한 시스템 모델이 좌표계 변화 아래 불변(invariant)인 성질을 이용하면, 필터의 구조적인 강건성을 높이는 Invariant-EKF와 같은 기법도 등장했는데, 이는 모두 Lie 군 이론의 산물입니다. 결과적으로 Lie 이론 기반 접근은 이론적으로 보다 철저히 검증되었고, 좌표 선택에 덜 민감한 추정을 가능케 합니다.\n\n이와 같은 장점에도 불구하고, 실무적으로 고려해야 할 점은 구현의 복잡성 증가입니다. Lie 군을 다루려면 수치적으로 Exp/Log를 계산해야 하고, Jacobian도 새롭게 구해야 하므로 초기 구현량은 다소 늘어날 수 있습니다. 하지만 앞서 언급한 manif 라이브러리나, 이미 잘 알려진 Sophus, g2o 등의 라이브러리를 사용하면 이러한 구현 부담은 크게 줄어듭니다. 무엇보다, 한 번 Lie 이론을 도입해두면 향후 새로운 상태 변수가 추가되어도 같은 원리로 확장할 수 있고, 시스템 전반의 일관성 유지와 디버깅 용이성이 좋아지는 효과가 있습니다. Solà 등은 “필요없는 이론까지 다 동원하지 않아도, 우리에게 유용한 핵심만으로도 충분히 정확한 알고리듬을 만들 수 있다”는 메시지를 전하고 있습니다. 이는 곧 실용성과 엄밀함의 균형을 의미하며, 로봇공학자가 Lie 이론을 학습할 충분한 이유가 될 것입니다."
  },
  {
    "objectID": "posts/paper/2025-08-05-micro-lie.html#맺으며",
    "href": "posts/paper/2025-08-05-micro-lie.html#맺으며",
    "title": "📃Micro Lie Theory 리뷰",
    "section": "2.6 맺으며",
    "text": "2.6 맺으며\n지금까지 “A micro Lie theory for state estimation in robotics” 논문을 따라가며 Lie 군/대수의 기본부터 상태 추정에의 응용까지 살펴보았습니다. 요약하면, Lie 군상의 칼만 필터는 상태를 매니폴드 위에서 표현하고, 오차는 접공간에서 가우시안으로 추정하며, 업데이트는 \\text{Exp}/\\text{Log} 연산을 통해 이루어집니다. 이는 항상 유효한 상태를 유지하고 선형화의 정확성을 높이며, 궁극적으로 필터의 성능과 안정성을 개선합니다. 논문 저자들이 강조하듯이, 로봇 상태 추정에서 Lie 이론의 일부분만 활용해도 얻을 수 있는 이득이 매우 크다는 것이 현대 사례들로 증명되고 있습니다. 독자께서도 오래 잊고 있던 이론 감각을 되살려, 실제 로봇 문제에 Lie 이론을 적용해보길 권합니다. 작은 Lie 이론이 모여 큰 발전을 이루듯, 엄밀한 수학적 도구의 현장 적용이 로봇공학의 견고한 발전으로 이어질 것입니다.\n참고문헌: Joan Solà, Jérémie Deray, Dinesh Atchuthan, A micro Lie theory for state estimation in robotics, arXiv:1812.01537v9, 2021."
  },
  {
    "objectID": "posts/paper/2025-07-11-mag-fs.html",
    "href": "posts/paper/2025-07-11-mag-fs.html",
    "title": "📃Gradiometric Magnetic FS 리뷰",
    "section": "",
    "text": "Paper Link\nPost Link"
  },
  {
    "objectID": "posts/paper/2025-07-11-mag-fs.html#서론",
    "href": "posts/paper/2025-07-11-mag-fs.html#서론",
    "title": "📃Gradiometric Magnetic FS 리뷰",
    "section": "2.1 서론",
    "text": "2.1 서론\n로봇이 인간 수준의 섬세함으로 물체를 다루려면 촉각 센서를 통한 정밀한 힘 제어가 필수적입니다. 시각 센싱만으로는 한계가 있기 때문에, 접촉 지점에서의 수직(normal) 및 수평 방향(tangential) 힘 정보를 얻어 미끄럼 감지와 정밀 힘 제어를 수행해야 합니다. 이를 통해 로봇은 깨지기 쉬운 물체도 안전하게 잡거나, 미지 물체의 무게를 추정하고, 사람과의 안전한 상호작용도 가능해집니다. 인간 촉각의 성능을 고려한 로봇용 촉각 센서의 목표 사양으로는, 약 10 mN(1 그램 정도 무게) 수준의 힘 분해능, 5 mm 간격의 공간 해상도, 3축(force 3D 벡터) 측정, 1000:1의 다이나믹 레인지, 1 ms 응답속도 등이 제시되어 있습니다. 또한 센서 표면은 피부처럼 부드러워야 하며 접촉면적을 넓히기 위해 일정 수준의 유연성(compliance)을 가져야 합니다. 관절에 장착되는 경성 6축 힘-토크 센서와 달리, 손가락 끝 피부 표면에 분산 배치될 부드러운 촉각 센서 기술이 요구됩니다.\n현재까지 다양한 촉각 센서 기술들이 연구되어 왔습니다. 예를 들어, 유체(microfluid) 압력 또는 저항 변화를 이용한 센서들은 손가락 전체를 둘러싼 분산형 촉각 피부를 구현할 수 있지만, 국소적인 3축 힘을 직접 측정하지는 못합니다. 카메라 기반의 광학 촉각 센서(예: GelSight, NeuroTac)는 높은 해상도로 접촉면 변형을 측정하여 물체의 패턴이나 질감을 인식할 수 있으나, 부피가 크고 데이터 처리량이 많으며 전력 소모도 상당합니다. 포토다이오드 4분할 검출기를 이용한 상업 센서(구 OptoForce 등)는 카메라보다 경량화했으나 여전히 수백 mW 이상의 전력을 소모하며, 현재는 단품 판매가 중단되었습니다. 자기 센서 기반 촉각 센서는 이러한 대안으로 주목받아 왔습니다. 자기장 방식은 일반적으로 작은 3축 자기 센서 칩과 자석, 그리고 탄성체(elastomer)로 구성되며, 힘이 가해지면 자석이 미세 변위하여 주변 자기장이 변하고 이를 측정해 3축 힘을 계산합니다. 이 방식은 구조가 단순하고 비용 효율적이며, 한 개의 칩만으로도 3축 힘을 감지하는 컴팩트한 패키지를 구현할 수 있어 다수의 촉각 픽셀(taxel) 배열에 적합합니다. 실제로 선행 연구들에서 단일 센서의 힘 분해능이 수 mN 수준(수직 1.42 mN, 전단 0.71 mN)에 도달하였고, 이를 2×2 배열로 확장한 사례나 4×4, 최대 24개의 센서를 손끝에 분산 배치한 자기 촉각 피부 구현까지 보고되었습니다. 이러한 자기 촉각 센서는 3차원 힘 벡터를 높은 감도로 측정하고 소형화가 가능하며, 표면을 부드러운 실리콘 등의 탄성 재질로 만들어 인체 피부와 유사한 촉감을 가지게 할 수 있다는 장점이 있습니다.\n그러나 기존의 단일 자기 센서 기반 촉각 센서는 치명적인 약점이 있었는데, 바로 외부 자기장 교란(stray magnetic field)에 매우 민감하다는 점입니다. 센서가 측정하는 자기장은 원래 탄성체 내부 자석의 위치 변화로 인한 것이지만, 주변에 존재하는 다른 자석이나 전류에 의한 자기장이 있으면 센서는 이를 구분하지 못하고 합쳐서 측정해버립니다. 예컨대 지구 자기장(약 50 μT)이나 로봇 구동 모터, 가까운 자석에서 발생하는 필드는 센서 출력에 그대로 더해져, 실제 접촉력과 무관한 오차를 발생시킵니다. 실험실이 아닌 실제 환경의 로봇에는 이런 자속 교란이 흔하므로, 기존 자기 촉각 센서를 로봇에 탑재할 경우 신뢰성 문제가 제기되어 왔습니다. 일부 연구에서는 코일을 이용한 유도식 보상 방법 등으로 저주파 외란을 줄이려 했으나, 센서 크기가 1 cm 이상 커지고 주변 금속에도 민감한 단점이 있었습니다. 이러한 배경에서, 2022년 RA-Letter 논문 “A Gradiometric Magnetic Force Sensor Immune to Stray Magnetic Fields for Robotic Hands and Grippers”는 외란 자기장에 영향을 받지 않는 새로운 자기 촉각 센서 설계를 제안하였습니다. 이 센서는 다중 픽셀 자기 센서 구조를 통해 자기장의 기울기(gradient)를 측정함으로써 균일한 외부 자기 신호를 제거하는 방법을 사용합니다. 이하에서는 해당 논문의 기술적 내용과 기여를 심층 분석하고, 기존 기술과의 비교, 한계점 및 향후 발전 방향을 논의합니다."
  },
  {
    "objectID": "posts/paper/2025-07-11-mag-fs.html#기술적-개요",
    "href": "posts/paper/2025-07-11-mag-fs.html#기술적-개요",
    "title": "📃Gradiometric Magnetic FS 리뷰",
    "section": "2.2 기술적 개요",
    "text": "2.2 기술적 개요\n\n\n\n1\n\n\n그림 1. 단일 픽셀 기반 자기 힘 센서(위)와 다중 픽셀 그래디오메트릭 자기 힘 센서(아래) 개념도 비교. 기존 방식은 하나의 3축 자기 센서로 자석 변위를 측정하므로 외부 균일 자기장 B_{\\text{stray}}에 취약하여 힘 출력이 교란된다(왼쪽). 반면 새로운 다중 픽셀 센서는 동일한 패키지 내 4개의 자기 센서 픽셀을 배치하고, 내부적으로 차동 측정(differential measurement)**을 수행하여 균일한 외란 자기장을 상쇄한다(오른쪽).*\n그래디오메트릭 자기 힘 센서(GMFS)는 하나의 센서 모듈 안에 여러 개의 미소 자기 센서 픽셀을 가까이 배치하여, 출력 신호 간 차분(differential)을 취하는 방식으로 동작합니다. 논문에서는 Melexis 사의 Triaxis® 기반 MLX90372 칩을 활용했는데, 이 칩은 5 mm × 4.3 mm × 0.9 mm 크기의 작은 패키지 안에 2개의 CMOS 다이(die)를 담고 있고, 다이마다 2개의 자기 픽셀이 배열되어 있어 총 4개의 픽셀을 제공합니다. 픽셀 간 거리는 약 2 mm에 불과하여, 실질적으로 수 mm 공간 크기에서의 자기장 구배(gradient)를 측정할 수 있습니다. 각 픽셀은 2축의 자기장 성분(칩 표면에 수직인 B_z 축 성분과 한 방향의 수평 성분 B_x)을 측정합니다. (※ MLX90372 칩은 두 번째 수평 축 B_y는 출력하지 않지만, 이는 설계상 큰 문제는 없었습니다.) 이러한 멀티픽셀 자기 센서 IC 위를 부드러운 실리콘 계열 탄성체(논문에서는 Smooth-On사의 Dragon Skin 20)를 덮고 그 내부에 소형 원형 자석(네오디뮴 자석, 축 방향 자기화)을 매립합니다. 이 구조는 기존 단일 자기 촉각 센서와 유사하나, 한 패키지 안에 복수의 센서 픽셀이 있다는 점이 다릅니다. 외부에서 탄성체 표면에 힘이 가해지면, 자석이 미세하게 움직이면서 주변 자기장 분포가 변합니다. 이 변화를 바로 아래 배치된 4개의 픽셀이 각기 다른 위치에서 감지하여, 자석과 픽셀들 간 위치 관계 변화를 읽어내는 것입니다.\n자석의 변위가 수직 방향일 때는 모든 픽셀에서 자장 세기가 거의 균일하게 증가하거나 감소하지만, 서로 인접한 픽셀의 출력 차이를 보면 특히 B_x 성분의 공간 구배(∂B_x/∂x)가 뚜렷하게 나타납니다. 반대로 자석이 측면 방향으로 이동(전단 힘 작용)하면 픽셀마다 자석과의 거리가 달라져서 B_z 성분 분포에 차등이 생기고, 주로 ∂B_z/∂x 성분이 변화합니다. 다시 말해, 4개의 픽셀이 위치별 자기장 변화를 동시에 측정함으로써 자석의 3차원 이동을 추정할 수 있습니다. 그림 1의 개념도에서 보이듯, 이러한 공간 구배 신호는 공간 전체에 고르게 미치는 외란 자기장에 대해서는 모두 동일하게 나타나므로, 픽셀 간 평균값을 빼주면 상쇄됩니다. 결과적으로 센서는 자석의 위치 변화로 인한 국소적인 자기장 불균일 신호만 추출하게 되어, 외부에서 유입되는 균일한 자기 간섭에는 거의 영향을 받지 않게 됩니다.\n한편, 기계적 구조 측면에서 이 센서는 원통형의 평평한 실리콘 쿠션을 사용하였습니다. 이전 연구들에서는 피라미드형 돌기나 돔 형태의 탄성체를 쓰기도 했지만, 평평하고 넓은 접촉면을 주면 접촉 시 자석이 기울어지는 것을 줄이고 물체를 감싸는 안정적인 접촉이 가능하다는 이점이 있습니다. (자석 주변 탄성체를 육면체로 만들고 그 위에 얇은 피부를 덮는 형태도 대안이 될 수 있다고 합니다.) 자석은 탄성체 내부에 매립되어 있기 때문에 외부와 직접 접촉하지 않으며, 센서가 철제 물체에 달라붙는 일도 방지할 수 있습니다.\n이 센서의 설계에서 중요한 트레이드오프로 최대 측정 힘 범위(full-scale force)와 민감도(signal-to-noise ratio, SNR)를 들 수 있습니다. 일반적으로 더 큰 자석이나 자속이 강한 자석을 쓰면 픽셀이 얻는 자기 신호가 커져 SNR은 향상되지만, 탄성체의 강성과 두께에 따라 센서가 견딜 수 있는 최대 힘에는 큰 변화가 없었습니다. 반면 자석과 센서 간 거리(airgap)를 줄이면 신호는 커지지만 자석이 움직일 수 있는 물리적 여유가 줄어들어 최대 측정 힘이 감소합니다. 탄성체의 경도(hardness)도 유사한 trade-off를 보이는데, 더 단단한 재료일수록 동일 힘에 자석 변위가 작아져 신호 변화량이 작아지므로 감도는 떨어지지만, 큰 힘에도 덜 변형되므로 최대 측정 범위는 커집니다. 탄성체의 지름(diameter) 역시 영향을 주는데, 지름을 키우면 동일 힘이 넓은 면적으로 분산되어 자석 아래 국부 압력이 낮아지므로 신호가 줄어들지만 대신 더 큰 힘까지 버틸 수 있습니다. 논문에서는 로봇 손가락용으로 약 2 N 정도의 풀스케일 힘을 목표로 설계를 최적화했고, 이는 실제 서비스 로봇 손끝이 깨지기 쉬운 물체(예: 달걀)를 다룰 때 사용하는 힘 범위와 유사한 수준입니다."
  },
  {
    "objectID": "posts/paper/2025-07-11-mag-fs.html#주요-기여",
    "href": "posts/paper/2025-07-11-mag-fs.html#주요-기여",
    "title": "📃Gradiometric Magnetic FS 리뷰",
    "section": "2.3 주요 기여",
    "text": "2.3 주요 기여\n이 논문이 제안한 그래디오메트릭 자기 힘 센서는 기존 기술 대비 다음과 같은 주요 혁신과 성능 개선을 달성했습니다.\n\n외란 자기장 억제를 위한 다중 픽셀 차동 설계: 단일 3축 홀 센서로 구성된 기존 자기 촉각 픽셀은 외부 자기 교란에 취약하여, 주변에 자석이나 전류가 있을 경우 측정 힘 값이 크게 왜곡되었습니다. 반면 본 연구의 센서는 4개의 센서 픽셀이 칩 내부에서 밀집 배열되어 있어, 모든 픽셀에 공통으로 인가되는 균일 자기장을 서로 소거할 수 있습니다. 즉, 각 픽셀의 출력에서 공통 성분(평균 값)을 빼면 자석 위치 변화로 인한 국부적인 변화만 남기 때문에, 외부 자기장의 영향 없이 오직 접촉력에 의한 신호만을 읽어낼 수 있게 됩니다. 이러한 원리는 Helmholtz 코일로 ±2 mT의 강한 자기장을 걸어 실험한 결과에서 잘 나타났습니다. 제안된 센서는 ±2 mT의 균일 외란 필드가 존재해도 출력 힘 오차가 전체 범위의 약 0.3%에 불과했지만, 동일 조건에서 기존 단일 픽셀 방식으로 동작하도록 설정하면 약 20%에 달하는 큰 오차가 발생했습니다. 이는 두 자릿수(약 100배) 이상의 외란 내성 향상으로, 로봇에 센서를 장착한 상황에서 모터나 주변 자석으로 인한 잘못된 힘 피드백 문제를 획기적으로 줄일 수 있음을 의미합니다.\n고차 모델 기반 신호처리 및 실험적 검증: 여러 픽셀의 자기장 데이터를 종합하여 3축 힘으로 변환하기 위해, 저자들은 다항 회귀 모델을 이용한 신호처리 체계를 구축했습니다. 4개 픽셀로부터 얻은 B_x, B_z 값들에 대해 우선 온도에 따른 센서 감도 변화를 2차 보정하고, 각 축 성분의 평균을 제거하여 자기장 구배 성분을 추출했습니다. 이어서 각 픽셀의 B 벡터 크기(norm)까지 포함하여 픽셀당 3개의 특징을 만들고(총 12차원), 이들을 2차까지 조합한 다항식 특징 91개를 생성하였습니다. 마지막으로 이 91차원 벡터에 대해 선형 가중치 행렬(91×5)를 곱하여 5개의 출력(3축 힘 F_x, F_y, F_z 및 수평면내 2축 토크 T_x, T_y)를 계산하도록 모델을 설계했습니다. (자석의 축 대칭성 때문에 수직 방향 토크 T_z는 감지할 수 없음도 언급하고 있습니다.) 이러한 모델의 가중치 파라미터는 실제 힘 데이터를 이용해 학습되었는데, ATI Nano17 6축 로드셀을 장착한 3축 스테이지로 센서를 눌러가며 약 13,000개의 다양한 3D 위치에서 센서 출력과 참조 힘을 측정한 대규모 캘리브레이션을 수행했습니다. 수집된 데이터에 대해 허버 손실(Huber loss)과 L1 정규화를 사용한 확률적 경사하강법으로 최적의 매핑 가중치를 학습한 결과, 모델이 예측한 힘과 로드셀이 측정한 힘 간 상관도가 수직 힘 F_z에 대해 R^2 = 0.996, 수평 힘 F_x, F_y에 대해서도 0.966 및 0.949로 매우 높게 나타났습니다. 이는 3축 전체에 걸쳐 정밀한 힘 복원이 가능함을 보여주며, 마찬가지로 2축 토크도 높은 상관도로 일치함을 확인했습니다. 또한 센서 IC 내부의 온도 센서를 활용한 1차 보정 알고리즘으로 온도 변화(0–50 ℃)에 따른 출력 드리프트를 최소화하여, 열 팽창으로 인한 오차도 효과적으로 보상했습니다.\n고해상도 및 고감도 힘 감지 성능: 그래디오메트릭 설계는 신호 간 차동 동작으로 인해 이론적으로 신호 대 잡음비(SNR)가 낮아질 우려가 있었습니다. 매우 인접한 픽셀들에서 측정하는 자기장 분포 차이는 절대값에 비해 작기 때문에, 미세 신호를 증폭하는 과정에서 잡음 영향이 커질 수 있기 때문입니다. 그러나 논문 결과에 따르면, 제안된 센서는 약 2.7 mN (0.0027 N)의 힘 분해능(RMS 기준)을 달성하여, 인간 촉각 수준으로 제시된 10 mN 목표치를 훨씬 뛰어넘고 기존 단일 픽셀 자기 센서들과도 대등한 수준의 해상도를 유지했습니다. 이는 센서 출력 힘 0.3 그램 미만의 변화도 구별할 수 있는 민감도에 해당합니다. 흥미로운 점은, 다중 픽셀의 신호 결합으로 오히려 SNR을 향상시켜 이러한 성능 손실을 보완했다는 것입니다. 즉, 픽셀들의 출력을 조합함으로써 잡음 평균화 효과를 얻었고, 결과적으로 외란 자기장에 대한 면역성을 확보하면서도 높은 힘 감도를 유지할 수 있었습니다. 전체 센서의 크기도 5×5×5 mm 정도로 매우 작아 로봇 손가락 내부에 쉽게 내장할 수 있으며, 한 패키지에 센서 소자와 신호처리 회로 일부가 집적되어 있어 소형 모듈로 대량 생산하기에 적합한 형태를 보여주었습니다."
  },
  {
    "objectID": "posts/paper/2025-07-11-mag-fs.html#기술적-분석",
    "href": "posts/paper/2025-07-11-mag-fs.html#기술적-분석",
    "title": "📃Gradiometric Magnetic FS 리뷰",
    "section": "2.4 기술적 분석",
    "text": "2.4 기술적 분석\n1) 기존 자기 촉각 센서와의 비교: 그래디오메트릭 자기장 센서는 기존의 단일 홀센서 기반 촉각 픽셀들과 비교해 근본적인 전자기적 이점을 제공합니다. 과거의 자기 촉각 센서들은 Melexis MLX90393과 같은 단일 픽셀 3축 홀IC를 사용하였고, 여러 장을 배열하여 촉각 피부를 구성하더라도 각 픽셀이 개별적으로 외란 필드에 취약하다는 한계가 있었습니다. 반면 본 연구는 동일 칩 내에 복수의 센서를 넣어 외란을 장비적(하드웨어적으로 상쇄)하는 개념을 적용하여 이 문제를 해결했습니다. 이 접근법은 추가적인 외부 보정장치 없이도 센서 자체가 환경 자기 노이즈를 견딜 수 있는 내성을 가지게 했다는 점에서 가치가 큽니다. 그 결과, 센서를 로봇에 통합할 때 모터, 스피커, 전류선 등의 영향으로 센서가 오작동하거나 재보정해야 하는 빈도를 크게 줄여줄 것으로 기대됩니다.\n또한 그래디오메트릭 방식은 기존 자기 센서의 장점은 그대로 유지합니다. 즉, 접촉면이 부드러운 탄성체로 이루어져 있어 사람 피부처럼 물체와 안정적으로 밀착할 수 있고, 센서 단가도 카메라나 광센서에 비해 낮으며, 소모 전력도 매우 적습니다. Optical 센서의 경우 외란 자기장 문제는 없지만 여러 개의 광학 부품과 카메라로 구성되어 가격이 센서 한 개당 수백 달러 수준으로 높았던 반면, 자기장 센서는 수 달러 수준으로 대량 생산이 가능합니다. 실제로 Meta(Facebook) AI에서 발표한 ReSkin과 같은 개방형 자기 촉각 센서는 개당 6달러 정도의 저가에 제작할 수 있으나, 외란 자기장에 취약하다는 한계가 지적되었습니다. 본 논문의 연구는 이러한 실용적 약점을 근본적으로 개선함으로써, 자기 촉각 센서 기술을 연구실 단계에서 산업 현장으로 가져가는 데 기여할 것으로 보입니다. 아울러 Honeywell의 FMA 시리즈와 같은 상용 소형 힘 센서는 압전저항 소자를 이용해 작은 힘을 정밀 측정하지만 수직 방향 힘만 감지할 수 있는 제약이 있는데, 이 그래디오메트릭 센서는 한 개 모듈로 3축 힘을 모두 측정할 수 있어 한층 풍부한 촉각 정보를 제공합니다. 종합하면, 제안된 센서는 기존 자기 촉각 센서들의 장점(3D 벡터 측정, 부드러운 표면, 저가격, 소형화)을 그대로 가지면서도 가장 큰 약점이던 외란 필드 민감도를 극복한 균형 잡힌 혁신이라고 평가할 수 있습니다.\n2) 그래디오메트릭 설계의 장단점: 다중 픽셀을 이용한 그래디오메트릭 구조는 하드웨어적으로 보다 복잡합니다. 단일 센서로 구현하던 것을 4배수의 센서로 구현하므로, 칩 설계나 데이터 처리 측면에서 신호 경로가 늘어나고 보정해야 할 요소가 많습니다. 예를 들어 4개 픽셀의 상대 정렬이나 감도 편차 등을 고려한 보정이 선행되어야 하고, 3축 힘을 얻기 위해 다차원 다항식 연산을 수행해야 합니다. 논문에서도 센서 출력을 실제 힘으로 변환하기 위해 13,000개나 되는 캘리브레이션 데이터를 모아 학습시킨 바 있는데, 이 정도의 데이터 수집 및 모델 트레이닝 과정은 센서 제작 후 매번 수행하기 어려우므로 양산 시에는 모델 일반화 또는 공장 초기보정 등의 현실적인 방안이 필요할 것입니다. 다만 저자들은 현재의 신호처리 알고리즘은 오프라인(PC)으로 구현되었지만, 충분히 경량화 가능하며 칩에 통합하거나 마이크로컨트롤러로 실시간 처리하는 것도 가능하다고 언급하고 있습니다. 또 하나의 제한점으로는, 이 센서가 자석의 수평 회전에 대해 대칭 구조이므로 회전(z축 토크) 감도는 거의 없다는 점입니다. 즉, 물체를 비틀거나 하는 토크까지 한 픽셀로 측정하는 것은 어렵고, 이러한 6축 정보까지 필요할 경우 센서를 여러 개 배치하거나 구조를 수정해야 할 것입니다. SNR 측면의 단점도 이론적으로 존재하는데, 균일한 자기장은 제거되나 그만큼 유용신호 성분도 차분으로 인해 감소하므로, 주어진 회로에서의 잡음 비중이 커질 수 있습니다. 논문에서는 멀티픽셀 신호 결합으로 이 문제를 어느 정도 만회했지만, 근본적으로 단일센서 대비 신호 여유(margin)가 줄어드는 것은 감수해야 합니다. 따라서 향후에는 홀 센서 대신 TMR이나 GMR 같은 고감도 자기 센서 소자를 픽셀로 활용하거나, 픽셀 수를 늘려 신호를 증폭하는 방식도 고려될 수 있습니다. 요약하면 그래디오메트릭 구조는 약간의 복잡성과 추가 보정 노력을 요구하지만, 그 대가로 실사용에 치명적이었던 외란 민감도를 해결하고 3축 벡터와 높은 해상도를 유지하게 해주는, 충분히 투자 가치가 있는 개선으로 볼 수 있습니다.\n3) 로보틱 플랫폼 통합 가능성: 본 센서는 이미 소형 패키지로 구현되어 있어 로봇 손가락이나 그리퍼에 손쉽게 장착할 수 있습니다. 저자들은 시연을 위해 이 센서를 이탈리아 qbrobotics의 상용 qb SoftHand 로봇 손에 부착하고, 간단한 힘 제어 알고리즘을 적용하여 풍선을 살짝 쥐는 실험을 수행했습니다. 그 결과, 외란 자석이 없는 상태에서는 이전의 단일 픽셀 센서와 신형 센서 모두 풍선을 적절한 힘으로 잡았지만, 손 가까이에 자석을 가져다 대자 두 접근법의 차이가 극명하게 드러났습니다. 단일 픽셀 센서로 동작한 경우 자석의 필드 때문에 센서 출력이 오동작하여 로봇 손이 풍선을 놓쳐 버리거나 지나치게 힘을 주어 터뜨리는 상황이 발생했지만, 다중 픽셀 그래디오메트릭 센서로 동작한 경우 자석이 매우 가까이 접근해 약 3 mT 수준의 강한 교란이 가해져도 센서 출력이 흔들리지 않아 손이 풍선을 계속 안정적으로 쥐고 있을 수 있었습니다. 이는 실제 응용에서 이 센서가 가지는 큰 강점을 보여줍니다. 로봇이 전자제품 주위나 자기 재료가 많은 환경에서 작업할 때도 안정적인 그립 제어를 유지할 수 있고, 추가 차폐 없이도 센서가 작동하기 때문입니다. 더욱이 이 센서는 완전한 반도체 공정 기반으로 제작되었기 때문에, 기존의 광학식 촉각 센서처럼 복잡한 정렬이나 보정 없이도 칩 단위로 공장 출하 시 교정된 형태의 모듈로 제공할 수 있습니다. Melexis 등 반도체 업체를 통해 대량생산이 이루어지면, 로봇 손가락마다 수십 개의 촉각 픽셀을 피부처럼 붙이는 것도 비용 면에서 현실화될 수 있습니다. 실제로 저자들은 본 연구를 “Tactaxis”라는 프로토타입 센서로 명명하고 있으며, 차후 이 기술을 발전시켜 대량 생산 가능 제품화를 추진할 계획임을 밝히고 있습니다.\n\n\n\n2\n\n\n그림 2. 그래디오메트릭 자기 촉각 센서를 로봇 손에 통합하여 풍선을 잡은 상태에서 외란 자기장의 영향을 시험한 실험 장면. (a) 제안된 다중 픽셀 센서는 옆의 강한 자석이 접근해도 출력이 교란되지 않아 로봇 손이 풍선을 계속해서 안정적으로 쥐고 있다. (b) 기존 단일 픽셀 센서는 자석 때문에 측정값이 변동하여 로봇 손이 풍선을 놓치거나 눌러 터뜨리는 문제가 발생한다. (c) 실험에 사용된 폐회로 힘 제어 알고리즘의 개략도."
  },
  {
    "objectID": "posts/paper/2025-07-11-mag-fs.html#한계점-및-향후-과제",
    "href": "posts/paper/2025-07-11-mag-fs.html#한계점-및-향후-과제",
    "title": "📃Gradiometric Magnetic FS 리뷰",
    "section": "2.5 한계점 및 향후 과제",
    "text": "2.5 한계점 및 향후 과제\n그래디오메트릭 자기 힘 센서 기술은 외란 환경에서의 탁월한 성능을 입증하였지만, 실용화를 위해 고려해야 할 한계점과 도전 과제도 존재합니다. 먼저, 설계의 확장성(scale-up) 측면에서 현재 프로토타입은 4픽셀 한 묶음의 센서 하나로 동작했습니다. 로봇 전체 손바닥이나 손가락에 다수의 촉각 픽셀 배열을 형성하려면, 이와 같은 센서를 여러 개 매트릭스 형태로 배치해야 하는데, 이 경우 인접한 센서들 간 자석 필드 간섭 또는 패키지 간 간격 등에 대한 추가 연구가 필요합니다. 예컨대 픽셀 사이 거리 2 mm 수준에서는 한 픽셀의 자석이 주변 픽셀의 센서에 영향을 줄 가능성도 있으므로, 여러 센서 모듈을 동시에 사용할 때 발생하는 크로스토크(crosstalk)를 평가하고 방지하는 설계가 필요합니다. 다행히, 이전 연구들에서 단일 픽셀 자기 센서를 2×2, 4×4, 5×5 등으로 격자 배열한 사례들이 있었고 특별한 문제 없이 동작했으므로, 그래디오메트릭 센서 역시 모듈 단위로 배열하는 것이 충분히 가능할 것으로 보입니다.\n다음으로 양산 및 보정 문제가 있습니다. 앞서 기술한 바와 같이 이 센서는 제조 후 비교적 복잡한 다차원 보정 절차를 거쳐야 정확한 힘 값을 출력합니다. 이는 연구 단계에서는 충분히 가능한 작업이지만, 수많은 센서를 생산하여 일일이 교정하기에는 현실적인 비용과 시간이 문제가 될 수 있습니다. 따라서 공장 출하시 표준 캘리브레이션 데이터를 활용해 개별 센서의 보정 파라미터를 빠르게 세팅하거나, 생산 공정의 정밀도를 높여 센서 간 편차를 최소화함으로써 보정 과정을 단순화하는 방안 등이 필요합니다. Melexis 보도자료에 따르면 이러한 공정 통합과 자동 교정에 대한 로드맵이 언급되고 있는데, 이는 그래디오메트릭 센서가 상용화되기 위해 반드시 넘어야 할 과제입니다. 또한 현재 신호처리는 외부 프로세서에서 이뤄지지만, 최종적으로는 센서 출력이 바로 힘 값으로 제공될 수 있도록 온칩 신호처리를 포함하는 방향으로 발전할 수 있습니다. 이것은 센서 데이터를 다루는 시스템 통합을 간소화하고, 통신 대역폭 요구를 줄이며, 실시간 제어에 유리할 것입니다.\n기계적 신뢰성 및 내구성도 고려해야 합니다. 탄성체로 된 센서는 반복된 하중에 따른 피로, 크립(creep) 특성, 온도 및 시간에 따른 경도 변화 등이 발생할 수 있습니다. 논문에서도 온도 변화 0–50 ℃ 구간에서 탄성체 열팽창과 연화로 인한 힘 측정 드리프트가 최대 20~30%까지 생기는 것을 확인하고 1차 보정으로 이를 보상하였지만, 장시간 사용 시의 열적/기계적 안정성은 추가 검증이 필요합니다. 예를 들어 50 ℃ 이상 고온 환경이나 0 ℃ 이하 저온 환경, 또는 수만 회 이상의 반복 힘 작용 후에도 초기의 캘리브레이션이 유지되는지 관찰해야 합니다. 자석의 영구자화 세기 감소(demagnetization)나 탄성체의 변형 영구잔류(hysteresis) 등이 발생하면, 힘-자기장 모델이 변동하여 오차가 늘어날 수 있기 때문입니다. 이러한 문제에 대해서는 자석 재질 및 코팅의 개선, 탄성체 소재의 선택, 온도 센서 피드백을 활용한 다차원 온도 보정 알고리즘 등으로 대응할 수 있을 것입니다. 끝으로, 이 센서는 수직 토크 성분에 대해 민감하지 않다는 태생적 한계가 있습니다. 로봇 손가락 촉각에는 일반적으로 접촉력 3축 정보만으로도 충분한 경우가 많지만, 만약 물체와의 마찰로 발생하는 비틀림을 감지하려면 센서 구조를 추가하거나 다른 보조 메커니즘을 마련해야 합니다. 예를 들어 십자 배열로 두 개의 자석을 배치하거나, 비대칭 형상의 자석을 사용해 토크에 대한 응답을 일부러 만들어내는 등의 연구가 향후 진행될 수 있습니다.\n요약하자면, 그래디오메트릭 자기 촉각 센서는 큰 가능성을 지닌 기술이지만 상용 로봇에 폭넓게 적용되기까지는 생산 공정 최적화, 다중 센서 어레이 구성, 장기 안정성 검증 등의 추가 과제가 남아 있습니다. 이러한 부분은 향후 연구 및 개발을 통해 충분히 개선할 수 있으며, 실제 제품화 시 엔지니어링 솔루션으로 보완될 것으로 기대됩니다."
  },
  {
    "objectID": "posts/paper/2025-07-11-mag-fs.html#분석-및-비판적-논평",
    "href": "posts/paper/2025-07-11-mag-fs.html#분석-및-비판적-논평",
    "title": "📃Gradiometric Magnetic FS 리뷰",
    "section": "2.6 분석 및 비판적 논평",
    "text": "2.6 분석 및 비판적 논평\n이 논문은 로봇용 촉각 센서 분야에서 오랫동안 지적되어 온 외란 자기장 문제를 해결하기 위한 창의적 해법을 제시하고, 이를 실험적으로 입증했다는 점에서 큰 의의가 있습니다. 기술적 기여도 매우 명확하여, 기존 자기 촉각센서의 현실 적용성을 가로막던 장애를 극복하고도 성능 타협이 거의 없음을 보여주었습니다. 실제로 2 mT 수준의 강한 외부 자기장 하에서도 센서 출력 오차가 0.3%에 불과하다는 결과는 놀라운 수준이며, 3축 2.7 mN 해상도 역시 현재 보고된 촉각 센서 중 최상위권에 속합니다. 특히 주목할 점은, 이러한 외란 면역 설계가 로봇 제어에 미치는 효과를 직접 시연한 부분입니다. 단순히 센서 스펙 향상에 그치지 않고, 실제 로봇 손으로 풍선을 잡는 데 적용하여 기존 센서였으면 발생했을 문제(자석으로 인한 오작동)를 새로운 센서로는 해결할 수 있음을 명확히 보여주었습니다. 이는 이 기술의 실용적 가치를 독자들에게 강하게 인식시키는 요소로, 응용 측면과 학술 기여의 균형을 잘 맞춘 사례라 할 수 있습니다.\n물론 한편으로는 이 연구에서 다루지 않은 잠재적 문제들도 존재합니다. 첫째, 온도 변화 및 장기 안정성 이슈는 완전히 해결된 것이 아닙니다. 논문에서는 0–50 ℃ 구간에서의 선형 보정으로 온도 드리프트를 줄였지만, 센서가 다양한 환경(예: -20 ℃ 냉동창고나 70 ℃ 온실 등)에서 사용할 경우나 수개월 이상의 시간 경과 후에도 동일한 성능을 유지하는지는 추가 검증이 필요합니다. 탄성체의 열적 특성과 자석의 온도 의존성, 그리고 기계적 피로는 시간이 지날수록 보정 모델과 어긋날 수 있기 때문에, 자동 재교정(auto-calibration)이나 추가 센서(예: 스트레인 게이지로 탄성체 변형 직접 측정 등) 도입 등의 보완이 고려될 수 있습니다. 둘째, 비균일한 외부 자기장에 대한 면역성은 여전히 한계가 있을 것입니다. 그래디오메트릭 센서는 공간적으로 거의 일정한 외란 필드를 제거하는 데 최적화되어 있지만, 만약 외부 자석이 센서 한쪽 가까이에 위치하여 국부적으로 강한 구배 필드를 유발한다면 이를 완전히 구분해내기는 어려울 수 있습니다. 예를 들어 로봇이 영구자석 자체를 집거나, 강한 자기체를 지닌 물체를 다루는 경우, 그 물체에서 나오는 필드 패턴이 센서의 예상 모델과 섞여 오차를 일으킬 가능성이 있습니다. 이런 상황은 일반적이지는 않지만, 그래도 센서의 적용 범위에 대한 한계로 인지해야 합니다. 셋째, 자기적 간섭 외에 다른 요인들—예를 들어 인접한 금속 구조에 의한 자기장 왜곡, 혹은 전자기적 잡음—에 대해서도 추가적인 실험이 필요합니다. 논문에서는 로봇 손가락의 알루미늄/플라스틱 구조에서 문제없이 작동했지만, 만약 센서 주변에 강자성체 부품이 있다면 자석의 필드가 그쪽으로 달라붙어 왜곡되거나 감도가 저하될 수 있습니다. 따라서 센서 주위 재질 선택과 설치 위치 등도 신중히 고려해야 할 것입니다.\n마지막으로, 실용화 관점에서의 경제성도 짚어볼 점입니다. 이 논문은 기업(Melexis) 주도로 수행되어 비교적 현실을 감안한 내용이지만, 아직은 연구 프로토타입 단계로 소량 제작되었습니다. 대량 생산 시 단가와 신뢰성, 수율 등이 중요할 텐데, 그래디오메트릭 센서는 구조가 다소 복잡하여 단일 센서 대비 제조 공정이 늘어날 수 있습니다. 하지만 반도체 공정 기술과 센서 보정 기법이 꾸준히 발전하고 있으므로 이러한 문제는 충분히 해결 가능해 보입니다. 특히 메틀룩시스(Melexis) 등에서 이미 stray-field immune 위치 센서 등을 양산한 경험이 있다는 점을 감안하면, 본 촉각 센서도 조만간 상용 로봇에 탑재될 수준으로 개선될 수 있을 것입니다.\n결론적으로, “그래디오메트릭 자기 힘 센서” 연구는 로봇 촉각 센싱 분야에서 학술적 독창성과 응용적 유용성을 고루 갖춘 성과로 평가됩니다. 다중 자기 픽셀을 활용한 차동 측정 개념은 향후 다른 센서 분야(예: 전류 센싱, 지자기 계측 등)에도 응용될 잠재성이 있습니다. 이 연구를 발판으로, 보다 견고하고 정교한 로봇 촉각 센서들이 실현되어 로봇이 인간처럼 섬세한 촉감을 갖추는 날이 앞당겨지길 기대합니다."
  },
  {
    "objectID": "posts/paper/2025-08-25-maniptrans.html",
    "href": "posts/paper/2025-08-25-maniptrans.html",
    "title": "📃ManipTrans 리뷰",
    "section": "",
    "text": "Paper Link\nProject Link\nCode Link"
  },
  {
    "objectID": "posts/paper/2025-08-25-maniptrans.html#배경과-문제-정의",
    "href": "posts/paper/2025-08-25-maniptrans.html#배경과-문제-정의",
    "title": "📃ManipTrans 리뷰",
    "section": "배경과 문제 정의",
    "text": "배경과 문제 정의\n현대 다관절 로봇 손(dexterous robotic hand)은 인간 손처럼 정교한 조작을 목표로 개발되고 있으며, 특히 양손(bimanual) 협동 작업을 인간 수준으로 구현하는 것은 큰 도전입니다. 인간의 양손은 펜 뚜껑을 열고 닫거나 병 뚜껑을 비트는 등 복잡한 협조 동작을 수행할 수 있지만, 로봇에게 이러한 능력을 학습시키는 일은 쉽지 않습니다. 기존에 제시된 방법들은 강화학습(RL)을 통해 로봇 손 행동을 스스로 탐색하게 하거나, 사람 조작을 원격조작(teleoperation)으로 데이터 수집하는 방식을 사용해 왔습니다. 그러나 전통적 RL은 과제별로 정교한 보상 함수를 설계해야 해 확장성이 떨어지고 복잡한 과제에는 적용이 어렵습니다. 텔레오퍼레이션은 사람 운영자가 가상현실(VR) 기기를 활용해 로봇 손을 직접 조종하며 데이터를 모으는 방식인데, 비용·노력 면에서 비효율적이고 한정된 환경에 특화된 데이터셋만 얻는 한계가 있습니다.\n이러한 문제를 해결하기 위해, 최근에는 인간 시연 데이터(예: 모션 캡처 MoCap으로 기록한 사람 손 동작)를 로봇 손에 모방 전이하는 연구가 주목받고 있습니다. 사람의 조작 궤적을 모방하면 인간과 유사한 자연스러운 물체-손 상호작용을 얻을 수 있고, 대규모 MoCap 데이터셋과 손 추적 기술의 발전으로 양질의 인간 조작 시퀀스를 쉽게 확보할 수 있기 때문입니다. 시뮬레이션 환경에서 이런 모방 학습을 하면 현실에서 바로 실험하지 않고도 효과를 검증할 수 있다는 장점도 있습니다.\n모션 전이(motion transfer) 문제란, 인간 두 손의 조작 시연을 주어진 로봇 양손 시스템에 옮겨와 동일한 작업을 수행하도록 만드는 과제를 말합니다. 보다 공식적으로, 이 논문에서는 왼손과 오른손 두 개의 다관절 로봇 손이 주어진 인간의 왼손, 오른손 움직임을 모방해 두 물체를 협력 조작하도록 하는 시나리오로 문제를 정의합니다. 예를 들어 한쪽 손이 펜 뚜껑을 잡고 다른 손이 펜 몸체를 쥐는 펜 마개 닫기 작업을 생각해볼 수 있습니다. 입력으로는 인간 손 시연의 참조 궤적(프레임 시퀀스로 표현된 손목의 6자유도 자세, 손가락 관절 각도 및 각속도 등)과 물체들의 움직임 궤적이 주어지며, 목표는 로봇 손들이 물리 시뮬레이션 상에서 이 참조 동작을 정확히 따라하면서도 작업의 물리적 제약을 만족하는 정책을 학습하는 것입니다. 그러나 이와 같은 양손 조작 모션 전이는 몇 가지 어려움이 있습니다. 우선 인간 손과 로봇 손의 형태(morphology)가 다르기 때문에 단순히 관절 각도를 매칭시키는 직접 리타게팅은 부자연스러운 자세를 만들기 쉽습니다. 또, 모캡 데이터 자체가 정확하다 하더라도 프레임 단위의 작은 오차들이 누적되면 물체를 다루는 고정밀 작업에서는 실패로 이어질 수 있습니다. 마지막으로 한 손이 아닌 두 손을 동시에 제어하려면 동작 공간의 차원이 매우 높아져 학습 난이도가 폭증합니다. 이러한 이유로 선행 연구들 대부분은 단일 손의 grasp(쥐기)이나 물체 들어올리기 정도에서 멈추고, 병 뚜껑 돌려 열기나 펜 뚜껑 씌우기 같은 복잡한 양손 동작은 거의 다루지 못했습니다.\n이러한 배경에서 이번에 소개할 ManipTrans (CVPR 2025 채택 논문)는 인간의 양손 조작 시연을 로봇의 두 손에 효과적으로 전이하는 새로운 방법을 제안합니다. 특히 “모션 전이” 문제를 두 단계로 분할하여 생각한 독창적 접근이 돋보입니다. 전통적 리타게팅 방법이 모캡 데이터를 그대로 로봇 관절로 매핑하려다 물리적으로 불안정한 동작을 만들어내는 데 반해, ManipTrans는 시연 모션을 물리적으로 수행 가능한 로봇 행동으로 변환하는데 성공적입니다. 그림은 기존 리타게팅의 실패 사례(로봇이 모캡 궤적을 그대로 따르다 물체를 놓치는 모습)와 ManipTrans로 학습한 결과(펜 뚜껑 씌우기, 병 마개 열기 등 다양한 작업을 성공적으로 재현)의 비교입니다."
  },
  {
    "objectID": "posts/paper/2025-08-25-maniptrans.html#주요-기여-및-혁신점",
    "href": "posts/paper/2025-08-25-maniptrans.html#주요-기여-및-혁신점",
    "title": "📃ManipTrans 리뷰",
    "section": "주요 기여 및 혁신점",
    "text": "주요 기여 및 혁신점\n논문의 핵심 기여는 다음과 같이 요약할 수 있습니다:\n\n이중 로봇 손 모션 전이를 위한 2단계 프레임워크 제안 – 인간의 양손 조작 기술을 로봇에 정밀하게 전이하기 위해, 먼저 손 움직임 자체를 모방하고 이후 물체 상호작용을 미세 조정하는 ManipTrans 프레임워크를 제시했습니다. 이로써 참조 손/물체 궤적을 둘 다 정확히 추적하며 과제 수행이 가능합니다.\n대규모 모사 데이터셋 DexManipNet 구축 – 제안된 방법을 활용하여 다양한 새로운 양손 조작 작업(펜 뚜껑 씌우기, 병뚜껑 돌려 열기, 실험용 플라스크 흔들기 등)까지 포함한 대규모 로봇 조작 데이터셋을 생성했습니다. DexManipNet은 3,300개 에피소드에서 약 134만 프레임의 로봇 손 조작 데이터를 담고 있으며, 61가지에 이르는 풍부한 작업들을 포괄합니다. 이는 이전에 공개된 유사 데이터셋들보다 규모나 다양성 면에서 훨씬 크며, 향후 로봇 정책 학습 연구에 귀중한 자원이 될 것입니다.\n탁월한 성능 및 일반화 – 제안 방법을 다양한 실험으로 검증한 결과, 기존 최신 기법 대비 동작 정밀도와 전이 성공률에서 크게 향상된 성능을 보였습니다. 특히 개인용 PC 환경에서조차 학습 효율이 우수하여 전이 속도가 빠르고, 여러 형태와 자유도를 가진 로봇 손(예: Shadow Hand, Allegro Hand 등)에도 최소한의 수정만으로 적용되어 일관된 성능을 발휘했습니다. 더 나아가 시뮬레이터에서 학습한 정책을 실제 로봇 장비로 재생하여, 기존 강화학습이나 텔레오퍼레이션으로는 달성하지 못했던 민첩하고 자연스러운 양손 조작을 현실에서도 구현해 보였습니다. 이처럼 ManipTrans는 단순하지만 효과적인 이중 로봇 손 모션 전이 프레임워크를 제시하고, 이를 통해 복잡한 인간 조작 시연을 로봇에서 정확히 재현하는 데 성공함으로써 학술적으로나 실용적으로 큰 의의를 지닙니다."
  },
  {
    "objectID": "posts/paper/2025-08-25-maniptrans.html#기술적-구성-maniptrans-두-단계-방법론",
    "href": "posts/paper/2025-08-25-maniptrans.html#기술적-구성-maniptrans-두-단계-방법론",
    "title": "📃ManipTrans 리뷰",
    "section": "기술적 구성: ManipTrans 두 단계 방법론",
    "text": "기술적 구성: ManipTrans 두 단계 방법론\nManipTrans의 핵심 아이디어는 모션 전이를 “둘로 나누어” 학습하는 것입니다.\n첫 번째 단계에서는 손의 움직임 자체에 집중하고, 두 번째 단계에서 그 움직임을 기반으로 물체를 다루는 세부 조작을 보정합니다. 이러한 분리는 인간-로봇 손 구조 차이로 인한 문제를 완화하고, 높은 차원의 양손 제어를 효율적으로 다루기 위한 전략입니다.\n\n1단계: 손 동작 모방 (Trajectory Imitation Pre-training)\n초기 단계에서는 물체와의 상호작용을 배제한 채 손의 고유한 움직임 궤적을 모방하는 정책을 학습합니다. 구체적으로, 인간 시연에서 얻은 양손의 손목 6자유도 자세 및 손가락 관절 각도 시퀀스를 목표로, 로봇 손이 이를 동일하게 따라가도록 학습합니다[19]. 이때 로봇 손은 물체를 잡거나 힘을 주는 등의 상호작용 없이 공중에서 손가락 모양과 움직임만 흉내 내도록 설정됩니다.\n학습에는 강화학습 기법(PPO 알고리즘)을 활용하며, 설계된 보상 함수는 다음과 같은 요소들로 구성됩니다:\n\n손목 위치/자세 추종 보상: 로봇 손목이 참조 궤적의 손목과 얼마나 일치하는지에 대한 보상입니다. 손목의 위치와 방향 오차를 줄이도록 유도합니다.\n손가락 자세 추종 보상: 로봇 손가락 관절 각도가 참조 인간 손가락 관절 값과 가까워지도록 합니다. 특히 손가락 끝 위치가 잘 맞는 것이 중요하므로, 사람 손 모델(MANO)과 대응되는 로봇 손의 핵심 키포인트(엄지, 검지, 중지 끝 마디 등)에 가중치를 두어 정밀 추종하도록 설계했습니다[22]. 이는 사람과 로봇 손의 형태 차이를 보정해주는 역할을 합니다.\n움직임 부드러움 보상: 갑작스럽게 튀는 동작을 피하기 위해, 로봇 손 관절의 각속도 변화나 토크 사용량이 과도하지 않도록 페널티를 줍니다. 이를 통해 시연과 유사한 자연스러운 움직임을 얻습니다.\n\n훈련 데이터로는 손 동작만 포함된 모캡 데이터셋들을 활용했습니다. 예를 들어 기존 공개 손 모션 컬렉션 및 합성 보간 데이터를 사용하고, 좌우 손 동작 빈도를 균형 맞추기 위해 좌우 대칭 변환을 적용했습니다[23]. 물체가 없는 손 단독 데이터로 학습함으로써, 복잡한 물리 상호작용 없이도 손가락 움직임을 정교하게 모방할 수 있었으며, 이는 인간-로봇 손 형태 차이로 인한 문제를 크게 줄여줍니다. 또한 학습 효율을 높이기 위해 초기 상태를 시연 궤적의 임의 지점에서 시작(reference state initialization)시키고, 궤적에서 크게 벗어나면 조기 종료하여 다시 시도하도록 하는 등의 커리큘럼 전략을 도입했습니다. 그 결과 1단계에서는 노이즈에 강인한(hand motion with resilience to noise) 범용 손 모션 모방 모델이 얻어졌습니다[25].\n\n\n2단계: 잔차 정책을 통한 상호작용 미세 조정 (Residual Learning Fine-tuning)\n1단계에서 손가락 움직임을 잘 따라하게 된 정책을 바탕으로, 이제 실제 물체를 조작하는 제약을 반영하도록 Residual Learning 기법을 적용한 잔차(residual) 정책을 학습합니다. Residual Learning이란, 기존 정책의 행동에 작은 보정량(잔차)을 더해주는 형태로 정책을 학습시키는 방법으로, 복잡한 문제를 기존 솔루션 + α 방식으로 해결할 수 있어 효율적이고 안정적인 것으로 알려져 있습니다[27]. ManipTrans에서는 1단계 모방 정책이 내는 기본 동작에 대해, 2단계 잔차 정책이 필요한 추가 조작을 계산하여 합성된 최종 행동을 로봇에 실행시킵니다.\n이때 2단계에서는 로봇 손이 실제로 물체를 잡고 다루므로, 상태 공간(state)에도 물체와의 상호작용 정보가 추가됩니다. 구체적으로, 1단계의 손 관절 상태 등에 더해 물체의 상태(손목 기준 상대 위치 및 속도, 질량 중심, 중력 방향 등)를 포함시키고, 물체의 모양은 BPS(Basis Point Set) 표현으로 임베딩하여 제공했습니다. 또한 각 로봇 손가락 키포인트와 물체 표면 사이의 거리를 계산해 손-물체 공간적 관계를 피처로 넣고, 시뮬레이션으로부터 얻는 손가락-물체 접촉력도 명시적으로 포함시켰습니다[30]. 이를 통해 정책이 양손과 물체 사이의 물리적 상호작용을 인지하고 안정적으로 물체를 쥐거나 조작하는 데 필요한 정보를 충분히 얻도록 하였습니다.\nResidual 정책의 동작은 다음과 같습니다: 우선 매 시뮬레이션 스텝마다 1단계 모방 정책으로부터 현재 상태에서의 예상 동작 a_{\\text{im}}을 샘플링합니다. 이어서 Residual 정책은 확장된 상태 표현을 보고 보정 행동 a_{\\text{res}}을 산출합니다. 최종 로봇 제어 명령은 이 둘을 합친 a = a_{\\text{im}} + a_{\\text{res}} 형태로 결정됩니다. (필요할 경우 로봇 관절 한계 등을 넘지 않도록 클리핑 처리함) 처음 학습을 시작할 때는 이미 1단계 동작만으로도 참조 궤적과 유사한 움직임이 나오기 때문에, Residual 출력은 0에 가깝게 시작하는 것이 바람직합니다. 실제로도 Residual 정책의 가중치는 처음 평균 0의 작은 값들로 초기화하고, 학습 초반에는 Residual의 기여를 서서히 늘려가는 워밍업 전략을 사용하여 기존 모방 동작을 해치지 않고 미세 조정만 학습하도록 유도했습니다.\n2단계의 보상 함수는, 1단계에서 쓰인 손동작 모방 보상에 더해 두 가지 요소를 추가합니다:\n\n물체 경로 추종 보상은 시뮬레이터 상의 물체가 인간 시연의 물체 궤적을 잘 따라가도록 위치 및 속도 오차를 줄이는 보상입니다.\n접촉 힘 보상은 인간 시연에서 두 손가락이 물체를 잡고 있는 구간에 해당하면 로봇 손가락도 일정 수준 이상의 접촉력을 발생시키도록 장려하는 것입니다. 예를 들어 모캡 데이터에서 손가락이 물체를 쥐고 있는 프레임에서는, 로봇이 충분한 힘으로 물체를 잡지 않고 있으면 보상이 감소합니다. 이를 통해 로봇이 물체를 확실히 쥐고 놓치지 않도록 학습하게 됩니다. 전반적으로 ManipTrans는 특정 작업에 맞춘 특수한 보상 설계 없이도(task-agnostic 보상) 이러한 일반적 보상 구성만으로 다양한 작업에서 잘 동작하도록 설계되었습니다.\n\n학습 과정의 최적화를 위해 몇 가지 트릭을 활용했습니다. 잔차 정책 학습 초반에 흔히 발생하는 문제는, 물리 상호작용 제약 때문에 국소최적해에 빠지거나 학습이 불안정해질 수 있다는 것입니다. 이를 완화하기 위해 물리 파라미터 이완(relaxation) 기법을 도입했는데, 훈련 초기에는 중력을 0으로 줄이고 마찰 계수를 높이는 식으로 환경을 일시적으로 쉽게 만들어줍니다. 중력이 없고 마찰이 큰 상황에서는 로봇 손이 물체를 가볍게 붙잡고 참조 궤적에 맞추기가 수월해지므로, 초기에 빠르게 성공 궤적들을 찾아낼 수 있습니다. 학습이 진행됨에 따라 점차 중력을 실제값으로 복원하고 마찰 계수를 정상 수준으로 감소시켜, 최종적으로는 실제 물리 환경에 가깝게 만듭니다. 이 과정은 인위적인 별도 시뮬레이터를 쓴 선행연구(예: QuasiSim)와 달리, 표준 시뮬레이터(Isaac Gym)의 설정만 동적으로 바꾸는 방식이라 구현이 간단하면서도 효과적이었습니다. 이 밖에도 1단계와 마찬가지로 초기 상태를 시연 궤적 근처에서 샘플링하고, 물체가 일정 범위 이상 떨어지면 에피소드를 조기 종료하는 등의 규칙을 적용했습니다[39]. 특히 인간 시연에서 두 손으로 물체를 꽉 잡는 시점에 로봇이 제대로 힘을 주지 못하면 바로 종료해버리는 접촉력 조건도 두어, 반드시 물체를 놓치지 않도록 학습시키는 세밀한 장치를 마련했습니다. 위의 1단계 모방 정책과 2단계 잔차 정책은 NVIDIA Isaac Gym 시뮬레이터 환경에서 구동되는 수천 개의 병렬 에피소드를 통해 효율적으로 학습되었습니다. 논문에서는 4096개의 병렬 환경을 사용하여 PPO 기반 정책을 학습했고, GPU 한 대로도 원활히 훈련이 가능했음을 보고합니다. 이는 제안 방법의 학습 효율성이 높아 실용적이라는 점을 강조하는 부분입니다."
  },
  {
    "objectID": "posts/paper/2025-08-25-maniptrans.html#실험-결과-성능-평가-및-분석",
    "href": "posts/paper/2025-08-25-maniptrans.html#실험-결과-성능-평가-및-분석",
    "title": "📃ManipTrans 리뷰",
    "section": "실험 결과: 성능 평가 및 분석",
    "text": "실험 결과: 성능 평가 및 분석\nManipTrans의 성능은 다양한 지표에서 평가되었고, 여러 비교 방법을 크게 상회하는 것으로 나타났습니다. 실험에는 대표적인 양손 조작 데이터셋인 OakInk-V2의 검증 세트를 활용하였으며(약 절반이 양손 작업), 그 외에 GRAB, FAOVR, ARCTIC 등의 데이터도 정성 평가에 사용되었습니다. 평가 지표로는 물체의 위치/자세 오류, 로봇 손 관절 위치 오류, 손가락 끝 위치 오류 등이 평균적으로 얼마나 나타나는지 계산했고, 특히 성공률(success rate)은 로봇 두 손 모두가 참조 궤적을 일정 오차 이하로 추적하면 성공으로 간주하는 방식으로 정의했습니다. (양손 작업의 경우 어느 한 손이라도 기준을 못 만족하면 실패로 처리하여 성공 조건을 엄격하게 했습니다.)\n비교 대상 방법으로는\n\nRetarget-Only: 아무 학습 없이 모캡 손동작을 로봇 관절로 단순 이식한 경우,\nRL-Only: 모방 보상만으로 처음부터 끝까지 강화학습한 경우,\nRetarget + Residual: 인간-로봇 손가락 대응을 맞춰 리타게팅한 궤적을 기본 동작으로 하고, 그 위에 Residual RL만 적용한 경우 등이 포함되었습니다.\n\n이들은 ManipTrans의 일부 구성요소만 사용하는 부분 결합 기법들이나 기존 문헌의 접근법을 재구현한 것으로, ManipTrans의 효과를 검증하기 위한 비교군입니다.\n결과를 살펴보면 ManipTrans가 모든 지표에서 가장 우수했습니다. 예를 들어 성공률의 경우, 단순 리타게팅은 양손 작업 성공률 0%에 불과했고 RL-Only도 약 12% 수준으로 매우 낮았지만, ManipTrans는 약 39.5%의 양손 작업 성공률을 달성하여 크게 앞섰습니다. 단일 손 작업에서도 다른 방법들이 30~47% 선에 머문 데 비해 ManipTrans는 58% 수준의 높은 성공률을 보였습니다. 또한 물체 자세 오차, 손가락 끝 위치 오차 등 정밀도 지표도 ManipTrans가 가장 낮아 참조 동작을 가장 정확하게 따라함을 증명했습니다. 이러한 향상은 두 단계 전이 프레임워크가 손가락 세부 움직임과 물체 상호작용을 모두 효과적으로 포착해내기 때문이라고 분석됩니다.\n흥미로운 점은, Retarget-Only 방식은 높은 자유도의 로봇 손 공간에서 오류 누적을 감당하지 못해 사실상 거의 실패한다는 것입니다. 한편 RL-Only는 처음부터 탐색하다 보니 학습 시간이 오래 걸리고 모션 정밀도가 떨어져 아쉬운 성능을 보였습니다. Retarget + Residual 기법도 ManipTrans보다는 낮은 성능을 나타냈는데, 이는 초기에 리타게팅한 동작 자체가 물체 접촉 상황에서 충돌을 일으키는 등 부자연스러워 Residual 학습을 방해했기 때문입니다. 반면 ManipTrans는 사전 학습된 손 모션 모방 모델을 활용함으로써 보다 안정적이고 정확한 기본 동작을 제공하고, Residual 단계에서도 추가 제약만 학습하면 되므로 제어 난이도가 감소하여 최종 성능이 높았습니다.\n정성적인 결과로도 ManipTrans의 우수성이 드러났습니다. 논문에서 제시된 시뮬레이션 동영상과 이미지에 따르면, 로봇 손이 가는 꽃줄기를 두 손가락으로 집어 꽃병에 꽂는다거나, 긴 스푼으로 병 안의 물체를 함께 긁어내는 작업, 얇은 펜으로 글씨 쓰기 등 매우 섬세한 동작들도 자연스럽게 수행하는 것을 확인할 수 있습니다.\n\n\n\n그림 3에 일부 예시가 나타나 있는데, 상단 두 행은 단일 손 조작(플라스크 흔들기, 펜으로 쓰기) 장면들이고 하단 행은 양손 조작(꽃꽂이, 물 따르기, 스푼으로 긁어내기) 장면들입니다. 사람의 섬세한 손놀림이 필요한 이 작업들을 로봇이 큰 어색함 없이 재현했다는 점은, 제안한 모션 전이 방법의 현실성을 보여줍니다.\n다음으로 일반화 성능을 검증하기 위해, 학습된 정책을 다양한 로봇 손 플랫폼에 적용한 실험이 진행되었습니다. Shadow Hand (모터 24개), MANO hand 모형 (가상 관절 22개), Inspire Hand (12개), Allegro Hand (16개)처럼 구조와 자유도가 다른 로봇 손들에 대해 ManipTrans를 동일하게 적용해본 결과, 별도의 파라미터 튜닝 없이도 모든 경우에 유사한 성능과 자연스러운 동작이 나옴을 확인했습니다.\n\n\n\n\n\n\n즉, ManipTrans는 사람 손의 손가락-관절 대응만 정해주면 특정 손 구현체에 종속되지 않고 동작을 전이할 수 있어 플랫폼에 불가지론적인 범용성을 지녔습니다. 이는 1단계 모방 모델이 손가락 키포인트 트래킹에만 집중하고, 2단계에서 물리 상호작용을 다루는 구조 덕분입니다. 심지어 손가락 4개짜리 Allegro Hand의 경우도 일부 손가락 대응만 설정하면 큰 문제 없이 동작했음을 보고하고 있습니다.\n마지막으로, 현실 세계 적용 가능성을 보여주기 위한 시도가 이뤄졌습니다. 연구진은 두 대의 7자유도 로봇 팔 끝에 실제 Inspire 로봇 손 두 개를 장착하고, 앞서 시뮬레이션에서 생성한 DexManipNet의 양손 조작 궤적들을 재생시키는 실험을 했습니다. 다만 실제 하드웨어인 로봇 손은 시뮬레이터 모델보다 관절 자유도가 적기 때문에, 관절 각도를 피팅(fitting)하는 알고리즘을 추가로 사용하여 시뮬레이션 상 12-DoF 동작을 실제 6-DoF 기계손 움직임으로 근사했습니다. 또한 로봇 팔은 역기구학(IK)을 풀어 로봇 손목이 시뮬레이션 손목 경로를 따라가도록 제어했습니다. 이렇게 해서 실험한 결과, 예를 들어 “치약 뚜껑 열기” 작업에서 한 손으로 튜브를 꽉 쥐고 다른 손의 엄지와 검지로 작은 뚜껑을 톡 눌러 여는 동작을 로봇이 수행해냈습니다. 사람도 세심한 힘 조절이 필요한 이 움직임을 원격조작으로는 구현하기 어려운데, 학습된 정책을 이용해 비교적 쉽게 실현한 사례라 할 수 있습니다. 논문은 이 밖에도 여러 실제 로봇 실험 영상을 웹사이트에 공개하며, 본 기법이 향후 현실 로봇 학습에 큰 잠재력을 지님을 강조했습니다."
  },
  {
    "objectID": "posts/paper/2025-08-25-maniptrans.html#논의-및-한계점",
    "href": "posts/paper/2025-08-25-maniptrans.html#논의-및-한계점",
    "title": "📃ManipTrans 리뷰",
    "section": "논의 및 한계점",
    "text": "논의 및 한계점\nManipTrans는 다양한 복잡한 인간 조작을 로봇 양손에 성공적으로 전이했지만, 저자들은 몇 가지 한계와 향후 과제도 논의합니다. 먼저, 입력 모캡 데이터의 품질에 따른 제약이 있습니다. 일부 인간 시연은 오차나 잡음이 많아서, 손과 물체의 상호작용이 정확히 기록되지 않은 경우가 있는데, 이런 노이즈가 큰 시연 데이터에서는 전이 성능이 떨어질 수 있음을 지적했습니다. 예를 들어 손가락이 물체를 살짝 관통하거나 불안정하게 잡은 채로 기록된 데이터라면 로봇이 그 움직임을 따라가다 실패할 수 있습니다. 두 번째로, 물체의 시뮬레이션 모델 정확도 문제가 있습니다. 현실의 물체는 모양이나 관절(예: 뚜껑의 나사산 등)이 정교하지만, 시뮬레이터에서 사용하는 물체 모델이 단순/부정확하면 로봇의 조작이 엉뚱하게 진행될 수 있습니다. 특히 복합 구조(articulated)를 가진 물체의 경우 시뮬레이션 모델링이 어렵기 때문에 전이가 잘 안 되는 사례가 있었다고 합니다. 이러한 한계들 때문에 일부 시연 시퀀스는 ManipTrans로도 완벽히 재현하지 못했다고 보고하고 있으며, 이를 해결하려면 더 강인한 학습 기법 개발, 물리적으로 타당한 데이터 전처리 및 증강, 정교한 물체 모델 확보 등이 필요하다고 제언합니다. 저자들은 앞으로 ManipTrans의 강건성 향상, 현실 물체 모델의 물리적 정합성 개선 등을 연구하여 남은 어려운 사례들도 풀어나가는 것이 의미있는 방향이라고 전망합니다.\n전체적으로, “ManipTrans: Efficient Dexterous Bimanual Manipulation Transfer via Residual Learning” 논문은 양손 로봇 손의 복잡한 조작 동작을 인간 시연을 통해 효과적으로 학습하는 혁신적 방법을 제시했습니다. 잔차 학습을 활용한 2단계 모션 전이 구조는 손가락 움직임 모방과 물체 조작 제약 적응을 분리함으로써, 기존 방법들이 넘지 못했던 정확도와 효율성의 한계를 극복했습니다. 그 결과 펜 뚜껑 씌우기, 병 뚜껑 돌리기 같은 새로운 난제 과제들까지 성공적으로 구현해 냈으며, 이를 대규모 데이터셋으로도 정리하여 공개함으로써 향후 연구에 기여하고 있습니다. ManipTrans를 통해 인간처럼 섬세한 양손 조작을 로봇이 수행할 수 있는 가능성이 한층 가까워졌으며, 추후 남은 과제들만 해결된다면 가정용 서비스 로봇이나 산업용 조작 작업 등에 폭넓게 활용될 수 있을 것으로 기대됩니다."
  },
  {
    "objectID": "posts/paper/2025-09-05-dapg.html",
    "href": "posts/paper/2025-09-05-dapg.html",
    "title": "📃DAPG 리뷰",
    "section": "",
    "text": "Paper Link\nHomepage"
  },
  {
    "objectID": "posts/paper/2025-09-05-dapg.html#소개-및-배경",
    "href": "posts/paper/2025-09-05-dapg.html#소개-및-배경",
    "title": "📃DAPG 리뷰",
    "section": "2.1 소개 및 배경",
    "text": "2.1 소개 및 배경\n다수의 관절을 가진 다지 손 로봇(multi-fingered dexterous hand)은 인간 환경에서 다양한 작업을 수행할 잠재력을 지니지만, 제어의 난이도가 매우 높습니다. 손가락이 여러 개인 로봇 손은 고차원 관절 공간과 복잡한 접촉 상호작용을 갖고 있으며, 물체를 쥐거나 조작하는 과정에서 접촉 지점이 수시로 바뀌고 동역학이 불연속적으로 변화합니다. 이런 이유로, 기존 연구에서는 손 로봇 제어를 쉽게 만들기 위해 구조적으로 단순한 손이나 제한된 동작에 집중하곤 했습니다.\n예를 들어, 손가락을 두세 개로 제한하거나 특수 기계적 구조를 설계하여 문제를 단순화하거나, 파지(grasping)나 물체를 손 안에서 간단히 회전시키는 정도의 비교적 단순한 작업을 주로 다루었습니다. 모델 기반 최적화 방법으로 이러한 기본 동작들을 성공시킨 사례도 있었지만, 현실 세계의 복잡한 접촉이 있는 상황에선 정확한 모델링이 어려워 한계를 보였습니다.\n강화학습(RL)은 동역학 모델 없이 시도-오차를 통해 정책을 학습하므로, 복잡한 로봇 제어 문제에 유연하게 적용할 수 있다는 장점이 있습니다. 하지만 딥 강화학습(DRL)을 다지 손 조작에 적용한 선행 연구들은 거의 없었고, 주로 7-DoF 로봇 팔 등 비교적 간단한 조작이나 보행 같은 다른 분야에 국한되어 있었습니다. 심지어 표준적인 RL 벤치마크 과제들은 차원이 낮아, 선형 정책으로도 쉽게 해결되는 경우가 있을 정도로 단순하여, 고차원 손 조작의 난제를 대변하지 못했습니다. 모델 프리 RL이 이렇게 복잡한 손 조작 작업에 직접 적용되어 성공한 사례는 논문 발표 시점까지 전례가 없었습니다.\n본 리뷰의 대상인 Rajeswaran et al.(2018)의 논문은 이러한 간극을 메우기 위해 고차원 로봇 손의 복잡한 조작 작업을 딥 RL로 해결한 최초의 연구 중 하나입니다. 특히, 소수의 인간 Demo(demonstrations) 데이터를 활용하여 강화학습의 탐색 문제와 표본 효율 문제를 크게 개선하였음을 보여주었습니다. 그 결과, 과거에는 시뮬레이션에서 수백 시간 걸리던 학습을 불과 몇 시간의 로봇 경험(몇 회의 에피소드)으로 단축하였고, 학습된 정책의 동작이 더 인간처럼 자연스럽고 환경 변화에 견고해지는 효과도 확인하였습니다. 이 리뷰에서는 해당 논문의 핵심 기여와 기술적 아이디어, 제안한 알고리즘 DAPG (Demo Augmented Policy Gradient)의 구체적인 동작 원리, 그리고 실험 설정 및 결과를 중점적으로 깊이 있게 분석합니다. 또한 기존 관련 연구들과 비교하여 본 논문의 차별점도 함께 짚어보겠습니다."
  },
  {
    "objectID": "posts/paper/2025-09-05-dapg.html#주요-기여-요약",
    "href": "posts/paper/2025-09-05-dapg.html#주요-기여-요약",
    "title": "📃DAPG 리뷰",
    "section": "2.2 주요 기여 요약",
    "text": "2.2 주요 기여 요약\n이 논문에서 저자들은 복잡한 다지 손 조작 학습 분야에 다음과 같은 주요 기여를 하였습니다:\n\n모델 프리 딥 RL로 고차원 손 조작 작업 성공: 인간 손과 유사한 5-손가락 24자유도 로봇 손을 이용해, 물체 옮기기, 손안에서 물체 재배치, 도구 사용, 문 열기와 같은 다양한 접촉이 있는 복잡한 작업들을 모델 기반 사전지식 없이 RL만으로 학습하여 성공적으로 시연하였습니다. 이는 이론적으로나 실험적으로 최초의 성과로서, 고차원 손 조작에도 모델 프리 강화학습을 적용할 수 있음을 보였습니다.\nDemo 데이터 활용을 통한 학습 효율 비약적 향상: 각 작업마다 *25개 정도의 인간 Demo을 가상현실(VR) 인터페이스를 통해 수집하고 활용함으로써, 탐색 문제를 완화하고 학습에 필요한 샘플(데이터) 양을 드라마틱하게 감소시켰습니다. 그 결과, 시뮬레이터 상에서 몇 시간에 불과한 로봇 경험(예: 5시간 가량)만으로도 학습이 가능해졌습니다. 이는 해당 작업들을 실제 로봇에도 적용할 수 있을 만큼 실용적인 시간 내에 학습시킬 수 있는 수준으로 샘플 효율을 개선한 것입니다.\nDemo으로 얻은 자연스러움 및 강인성 향상: Demo 데이터를 통해 학습한 정책들은 *보상함수 설계를 최소화해도 성공률이 높았을 뿐 아니라, 사람의 동작을 닮은 부드럽고 자연스러운 움직임을 보였습니다. 또한 동일한 작업이라도 환경 조건(예: 물체 질량이나 마찰 등)을 바꾸었을 때 성능 저하가 덜하고 견고하게 동작하여, Demo을 통해 인간 전략의 내재된 강인성이 정책에 스며들었음을 확인했습니다.\n새로운 표준 과제 세트 제안: 저자들은 향후 연구에 활용할 수 있도록, 앞서 언급한 4가지 *다양한 손 조작 작업 환경을 정식으로 구축하여 제시하였습니다. 이 작업들은 고차원, 풍부한 접촉 상호작용, 과업의 다양성 측면에서 실제 인간 환경의 과제를 잘 대표하며, 로봇 조작과 머신러닝 교차 분야 연구자들에게 도전적인 벤치마크로 활용될 수 있습니다.\n\n요약하면 이 논문은, “고차원 로봇 손도 딥 RL로 학습시킬 수 있다. 그리고 소량의 Demo을 더하면 학습 속도와 정책 품질 모두 극적으로 향상된다.”는 것을 최초로 증명하며, 그 방법으로 DAPG 알고리즘을 제시한 것입니다.\n실험 환경: 복잡한 다지 손 조작 과제들저자들은 인간형 5-손가락 로봇 손 (24 DoF)의 난이도를 충분히 체감할 수 있는 네 가지 대표 작업을 시뮬레이션 환경에 구현하였습니다. 이 작업들은 일상에서 사람이 손으로 하는 다양한 조작 기술을 반영하며, 가상 현실(VR) 장치를 통해 사람으로부터 Demo 데이터를 수집할 수 있게 설계되었습니다. 각 작업에는 환경의 무작위성이 도입되어 초기 상태나 대상의 속성이 에피소드마다 달라지며, 최종 성공 여부만으로 보상이 주어지는 이진 성공 기준(sparsereward)을 갖습니다.\n네 가지 과제는 다음과 같습니다:\n\n물체 옮기기 (Object Relocation): 테이블 위에 놓인 파란 공을 집어 들어서 초록색 목표 지점까지 옮기는 작업입니다. 에피소드마다 공과 목표 위치가 테이블 위 임의의 곳으로 설정되며, 공이 목표 지점의 반경 ε 이내에 놓이면 성공으로 간주됩니다. 이 과제는 기본적인 파지 및 이송 능력을 평가하며, 손-물체 간 다중 접촉이 필요한 대표적 작업입니다.\n손 안에서 물체 조작 (In-hand Manipulation): 손에 쥔 펜의 방향을 재조정하여 주어진 목표 방향(녹색 표시)에 맞추는 작업입니다. 손목은 고정된 채 손가락들만 이용하며, 매 에피소드마다 펜의 초기 자세와 목표 자세가 무작위로 주어집니다. 펜의 방향이 목표와 일정 허용 오차 이내로 일치하면 성공입니다. 이 과제는 손가락들의 협조적인 미세 조작 능력을 필요로 하며, 연속적 접촉 및 재그립(re-grasping) 등이 요구됩니다.\n문 열기 (Door Opening): 손으로 문 손잡이의 걸쇠(latch)를 해제하고 문을 밀어 열기까지 수행하는 작업입니다. 문의 초기 닫힌 각도와 걸쇠의 상태가 무작위로 설정되며, 걸쇠에는 마찰력과 문을 닫히도록 잡아당기는 토크(스프링)가 존재해 손잡이를 어느 정도 힘줘 돌려야 열립니다. 문이 완전히 열려 문턱(stopper)에 닿으면 성공으로 판정됩니다. 이 과제는 복잡한 도구-환경 상호작용과 다단계 행동 시퀀스(걸쇠 풀기 → 밀기)를 포함하여, 손 조작의 탐색 난이도를 크게 높인 경우입니다.\n도구 사용 (Tool Use – Hammering): 바닥에 고정된 못을 망치로 내리쳐 박는 작업입니다. 테이블 위에 놓인 망치를 손으로 집어 들어 목표 못의 머리를 여러 차례 두드려서, 결국 못 전체를 나무 판자에 박아 넣으면 성공입니다. 못은 마찰로 약 15N의 힘까지 버티도록 설계되어, 충분한 타격 힘을 가해야 합니다. 이 과제는 물체 파지 → 근력 동작으로 이어지는 복잡한 활용이며, 비연속적인 충격 접촉과 도구의 정확한 조작이 요구되는 난도 높은 환경입니다.\n\n\n\n\n\nDAPG 알고리즘으로 학습된 로봇 손의 도구 사용 (망치질) 과제 수행 장면. 로봇 손이 책상 위의 망치를 집어 들고 바닥의 못을 여러 차례 타격하여 끝까지 박는 복잡한 동작을 성공시켰다. 각 작업마다 약 25개의 인간 Demo으로 초기정책을 학습시킨 후, 강화학습을 통해 몇 시간 만에 이러한 행동이 가능해졌다. 해당 정책은 사람 Demo을 참고하여 학습되었기 때문에 동작이 비교적 자연스럽고, 환경 변화에도 강건한 특징을 보였다.\n\n각 작업은 서로 다른 기술적 도전요소를 갖고 있어서, 하나의 알고리즘이 다양한 측면에서 성능을 검증받도록 합니다. 예를 들어, 물체 옮기기는 접근 및 파지(grasp & lift) 능력, 손내 조작은 정교한 자세 제어, 문 열기는 다단계 상호작용 및 힘 조절, 망치질은 공구 활용 및 충격력 제어라는 식으로, 손재주(hand dexterity)의 폭넓은 범위를 아우르도록 구성되었습니다. 특히 망치질이나 문 열기 등의 작업은 이전 연구들에서 다루지 않았던 복합 과제로서, 실제 가정환경에서 로봇 손이 해야 할 유용한 작업들에 가깝습니다. 저자들은 이러한 표준 과제 세트를 구축함으로써, 향후 연구자들이 로봇 손+강화학습 분야에서 공통으로 도전할 수 있는 벤치마크를 제공하였습니다.\n\n2.2.1 NPG의 한계\nDemo Augmented Policy Gradient (DAPG) 알고리즘 핵심 아이디어는 강화학습(RL)과 모방학습(Demo)을 효과적으로 결합하여, 탐색 어려움과 샘플 비효율 문제를 동시에 해결하는 것입니다. 기본적으로 저자들은 온-폴리시(on-policy) 정책 그래디언트 기반의 강화학습 알고리즘을 사용하였는데, 이는 Natural Policy Gradient (NPG) 방법으로 구현되었습니다.\nNPG는 정책 파라미터 공간에서 Fisher Information Matrix로 그래디언트를 정규화하여 업데이트하는 기법으로, 고차원 연속 제어 문제에서 기존 방법들보다 안정적이고 성능이 좋은 것으로 알려져 있습니다. 저자들은 먼저 이 기본 강화학습 알고리즘을 사용하여 작업을 푸는 것을 시도해보았는데, 다음과 같은 한계를 확인했습니다:\n\n보상 설계의 어려움: 순수 RL로는 성공/실패에 대한 이진 보상(sparse reward)만 주었을 때 학습이 진행되지 않았습니다. 무작위 탐색으로는 성공 사례를 거의 못 찾아내기 때문입니다. 결국 작업별로 사람이 세밀한 shaping 보상(힌트성 중간 보상)을 설계해주어야 학습이 겨우 가능했습니다. 이러한 보상 설계는 많은 노력과 휴리스틱을 필요로 합니다.\n샘플 요구량 문제: 보상을 잘 설계해 주어 RL이 학습에 성공하더라도, 수백만 스텝에 달하는 경험이 필요하여 학습 시간이 매우 길었습니다. 논문 결과에 따르면 어떤 작업은 100 시간분에 해당하는 시뮬레이션 데이터가 필요했는데, 이는 실제 로봇에 적용하기엔 비현실적으로 많은 양입니다 (로봇을 100시간 연속 구동하며 학습시키는 것은 안전이나 비용 면에서 쉽지 않습니다).\n학습된 정책 품질 문제: 순수 RL로 겨우 얻어진 정책들조차 동작이 어색하고 비효율적이며, 환경 조건이 조금만 달라져도 실패할 정도로 취약성을 보였습니다. 예를 들어, 사람이라면 하지 않을 불필요한 손가락 움직임을 반복하거나 비정상적인 자세로 물체를 잡는 등 인간 공감대와 동떨어진 행동들이 나타났고, 약간 다른 물체 크기나 마찰조건에서는 성공률이 급격히 떨어졌습니다.\n\nDAPG (Demo Augmented Policy Gradient) 알고리즘은 이러한 문제들을 해결하기 위해 인간 Demo 데이터를 강화학습 과정에 체계적으로 통합합니다.\n핵심 구성은 두 가지 단계로 이루어집니다: (1) Behavior Cloning을 통한 정책 초기화와 (2) Demo 정보가 포함된 보조 손실로 강화학습 파인튜닝입니다. 아래에서는 이 두 단계를 상세히 설명합니다.\n\n\n2.2.2 Behavior Cloning(BC)으로 초기 정책 학습\n먼저 소량의 전문가 Demo 데이터셋 \\rho_D를 모아 Behavior Cloning으로 초기 정책을 학습시킵니다. Demo 데이터셋 \\rho_D={(s_t^{(i)}, a_t^{(i)}, \\dots)}는 여러 에피소드 i에서 시간 단계 t별로 상태 s, 전문가 행동 a (그리고 보상 r, 다음 상태 등)을 모아둔 것입니다. Behavior Cloning은 이를 지도학습 문제로 보아, 주어진 상태에서 전문가의 행동을 모방하도록 정책 파라미터를 학습합니다. 수식으로 표현하면 다음과 같습니다:  \\max_{\\theta} \\sum_{(s,a)\\in \\rho_D} \\ln \\pi_\\theta(a \\mid s)\\,. \\tag{1} \n위 식은 Demo 데이터에서 정책 \\pi_\\theta의 로그 확률을 최대화하는 문제입니다. 즉, Demo 상태 s에서 전문가가 취한 행동 a를 가장 그럴듯하게 선택하도록 정책의 파라미터 \\theta를 조정합니다. 이 과정을 통해 얻어진 모방 정책은 전문가 경로를 따라가는 초기 전략을 제공합니다. Behavior Cloning으로 초기화를 하면, 완전히 무작위 정책으로 시작하는 것에 비해 탐색을 훨씬 효율적으로 만들 수 있습니다. 일반적인 정책 그래디언트 방법은 정책의 확률적 탐색 노이즈에만 의존하여 새로운 동작을 시도하는데, 고차원 문제에서는 무작위 시도로는 의미 있는 상태에 도달하기 어렵습니다. 반면, Demo 경로를 따라하도록 초기 정책을 잡아주면 초반부터 유망한 상태-행동 영역을 탐색하게 되어, 흔히 필요했던 복잡한 보상 shaping 없이도 학습을 시작할 수 있습니다. 실제 논문 결과에서도, Demo을 활용하지 않은 경우에는 각 작업마다 사람이 일일이 추가 보상 신호를 설계해줘야 했지만, DAPG는 Demo 덕분에 이런 보상 없이도 학습이 가능했음을 보여줍니다.\n하지만, Behavior Cloning만으로 최종 정책을 얻기에는 한계가 있습니다. Demo 데이터의 양이 많지 않으면 모방 학습된 정책은 분포 이동(distributional shift) 문제로 새로운 상태에 대응하지 못해 실패하게 됩니다. 실제로 저자들의 실험에 서도 BC만으로 학습한 정책은 대부분 작업을 끝까지 성공적으로 수행하지 못했습니다. 이는 Demo 경로를 조금만 벗어나도 정책이 어떻게 행동해야 할지 모르기 때문입니다. 더욱이, 모방학습만으로는 전문가를 넘어서 성능을 향상시킬수도 없습니다 – 기본적으로 주어진 Demo을 따라하기만 하므로 과업 성공에 대한 피드백이 없기 때문입니다. 따라서 모방으로 초기화한 후에도 강화학습을 통해 보상 신호로 정책을 향상시키는 단계가 필요합니다.\n\n\n2.2.3 강화학습 파인튜닝 (Demo 보조 손실 포함 정책 그래디언트)\n초기 정책을 얻었다면, 이후에는 온-폴리시 강화학습으로 정책을 계속 파인튜닝합니다. 이때 일반적인 정책 그래디언트 방식과 차별화되는 점은, Demo 데이터로부터 추가로 얻는 그래디언트 항(term)을 손실 함수에 포함시킨다는 것입니다. 이를 통해 학습 내내 정책이 Demo의 유익한 행동들을 참고하도록 만들고, 중간 단계 행동들에 대한 가이던스를 제공합니다. 저자들은 이 접근을 “데모 증강 정책 그래디언트(Demo Augmented Policy Gradient, DAPG)”라 명명하였습니다.\n구체적으로, 정책 그래디언트를 계산할 때 사용되는 목적 함수를 확장합니다. 기본이 되는 RL 목표는 정책의 기대 총 보상 J(\\pi_\\theta) = \\mathbb{E}_\\pi \\left[\\sum_t \\gamma^t r_t\\right]를 최대화하는 것으로, 이에 대응하는 정책 그래디언트는 REINFORCE 공식을 통해 아래처럼 주어집니다:\n g_{\\text{RL}} \\;=\\; \\mathbb{E}{(s,a)\\sim \\rho\\pi}!\\Big[ \\nabla_\\theta \\ln \\pi_\\theta(a|s)\\; A^\\pi(s,a) \\Big]\n\\,, \n여기서 \\rho_\\pi는 현재 정책 \\pi로부터 수집한 온-폴리시 데이터 분포이고, A^\\pi(s,a)는 현재 정책에 대한 어드밴티지(advantage) 값입니다. A^\\pi(s,a)는 해당 상태-행동이 평균적인 상태 대비 얼마나 더 좋은 결과를 내는지를 나타내는 값으로, 보상 Q^\\pi(s,a)와 가치함수 V^\\pi(s)의 차이로 정의됩니다. DAPG에서는 이 정책 그래디언트에 추가로 Demo 데이터 방향의 그래디언트를 더해줍니다. 즉, 최종 증강 그래디언트 g_{\\text{aug}}는 다음과 같이 구성됩니다:\n g_{\\text{aug}} \\;=\\; \\sum_{(s,a)\\in \\rho_\\pi} \\nabla_\\theta \\ln \\pi_\\theta(a|s)\\; A^\\pi(s,a)\\;+\\; \\sum_{(s,a)\n\\in \\rho_D} \\nabla_\\theta \\ln \\pi_\\theta(a|s)\\; w(s,a)\\,. \\tag{2} \n첫 번째 항은 온-폴리시 RL 그래디언트(앞서 설명한 g_{\\text{RL}})이고, 두 번째 항이 Demo 데이터로부터 오는 추가 그래디언트입니다. 이 추가 항은 Demo 데이터 분포 \\rho_D에 대해, 그 상태에서 정책이 Demo 행동 a를 취하도록 확률을 높이는 방향으로 작용합니다. 단, 모든 Demo 데이터를 동일하게 사용하기보다는 각각에 가중치 w(s,a)를 부여하여 얼마나 신뢰할지 조절합니다. 이런 방식으로 Demo을 활용하면 모방 학습과 강화학습을 연속적으로 접목할 수 있습니다.\n몇 가지 극단적인 경우를 살펴보면, 만약 모든 (s,a)\\in\\rho_D에 대해 w(s,a)=0이라면 두 번째 항이 없어지고 순수한 RL 학습과 동일해집니다. 반대로 w(s,a)가 매우 큰 상수로 설정되어 Demo 항이 지배적이라면, 이는 사실상 Behavior Cloning만을 수행하는 것과 가까워집니다. DAPG의 목표는 w(s,a)를 적절히 조절하여 RL의 성능 향상 효과와 Demo의 가이드 효과를 동시에 얻는 것입니다.\n\n저자들은 이상적인 w(s,a) 설계에 대해 고찰하면서, \\rho_\\pi와 \\rho_D의 혼합 분포 관점에서 분석합니다.\n분석에 따르면, 이론적으로는 Demo 데이터에서도 현재 정책의 Advantage를 계산하여 w(s,a) = A^\\pi(s,a)로 두는 것이 합리적입니다. 즉, Demo에서의 행동이 현재 정책보다 얼마나 이득인지에 따라 가중치를 주는 것이 최선이라는 것이죠. 하지만 현실적으로 Demo 각 상태의 A^\\pi(s,a) 값을 얻으려면 추가적인 시뮬레이션이나 가정이 필요하여 곧바로 계산하기 어렵습니다. 그래서 저자들은 경험적인 휴리스틱으로서 다음과 같은 단순한 형태의 가중치를 제안합니다 :\n w(s,a) \\;=\\; \\lambda_0 \\,\\lambda_1^k \\; \\max_{(s',a')\\in \\rho_\\pi} A^\\pi(s',a') \\qquad \\forall (s,a) \\in \\rho_D\\,, \n여기서 \\lambda_0는 초기 가중치 스케일, \\lambda_1은 감쇄율(decay factor), k는 학습이 진행된 iteration 횟수를 나타냅니다. 즉, 현재 정책으로 모은 온-폴리시 데이터에서의 최대 advantage 값 (현 정책이 얻 은 가장 좋은 행동의 advantage)을 하나의 기준 상수로 삼아서, 모든 Demo 샘플에 동일한 가치로 할당합니다. 그리고 학습이 거듭될수록 \\lambda_1^k 항을 통해 이 가중치를 지수적으로 감소시킵니다.\n이 설계의 의도는 명확합니다: 초기 학습 단계에서는 Demo가 정책보다 훨씬 우수한 행동들을 담고 있으므로 Demo의 영향을 강하게 주어야 합니다. 반면 후기 단계에서는 정책이 이미 웬만한 성능에 도달했기 때문에 Demo과 정책의 수준이 비슷해집니다. 이때까지도 Demo에 끌려다니면 정책이 더 나아지기 어렵기 때문에, 후반으로 갈수록 Demo의 비중을 줄여 정책이 스스로 성능을 최대로 끌어올리도록 합니다. 이렇게 함으로써 초반엔 Demo 위주 학습 → 후반엔 RL 위주 학습으로 점진적으로 전환되어, 전체적으로 Demo+RL의 시너지를 얻는 것이 DAPG의 목표입니다. 논문에서는 \\lambda_0=0.1, \\lambda_1=0.95로 설정하여 모든 실험을 진행했으며, 적절한 범위 내에서는 결과가 크게 민감하지는 않았다고 보고합니다.\n이러한 개념을 종합하여 DAPG 알고리즘의 흐름을 단계별로 정리하면 다음과 같습니다:\n\nDemo 수집: 전문가(휴먼)가 VR 장치를 통해 각 작업을 수행하여 성공 trajectories를 N개 (논문 실험에서는 약 25개) 기록합니다. 이로부터 Demo 데이터셋 \\rho_D를 구성합니다.\nBehavior Cloning 초기화: 식 (1)의 최대우도 추정 문제를 풀어 Demo을 모방하는 초기 정책 \\pi_{\\theta_0}를 얻습니다. (실제로는 딥러닝 옵티마이저를 활용하여 손실 함수를 최소화하는 방식으로 학습합니다.)\n강화학습 반복: k=1,2,\\dots 에 대해 다음을 수행합니다.\n\n현재 정책 \\pi_{\\theta_{k-1}}을 사용하여 시뮬레이터에서 여러 에피소드의 on-policy 데이터 \\rho_\\pi를 샘플링합니다. (논문 구현은 한 iteration에 200 에피소드씩 수집하여 사용.)\n수집한 데이터로부터 각 state-action의 advantage \\hat{A}^\\pi(s,a) 값을 추정합니다 (가치함수 baseline 등을 이용). 정책 그래디언트 g_{\\text{RL}} (온-폴리시 부분)와 Demo 그래디언트 항을 포함한 증강 그래디언트 g_{\\text{aug}}를 식 (2)에 따라 계산합니다.\ng_{\\text{aug}}를 사용하여 정책 파라미터 업데이트를 수행합니다. 이때 Natural Policy Gradient 방법을 사용하므로, 그래디언트에 Fisher 정보 행렬의 역을 곱해 스텝 크기를 조절한 업데이트를 적용합니다. (즉, $ k = {k-1} + , F^{-1} g_{} $ 형태로, \\alpha는 적절한 스텝 크기입니다.)\nDemo 그래디언트의 가중치 계수 \\lambda_1^k를 다음 iteration을 위해 감소시킵니다.\n\n수렴 또는 충분한 성능 달성 시 종료: 정책을 평가하여 성공률 등이 기준을 넘으면 학습을 종료합니다.\n\n이 전체 알고리즘에서, Demo 데이터는 초기 학습에서는 탐색을 크게 가속하고, 학습 중반에는 정책이 놓치고 있는 행동들을 보강하는 가이드 역할을 하며, 학습 후반에는 비중을 줄여 최종 성능을 RL이 주도하게 합니다. 저자들이 든 예를 다시 상기해 보면, Behavior Cloning 단계에서는 망치를 드는 것까지 겨우 배우지만 못을 치지는 못했습니다. 이후 강화학습이 망치 드는 부분을 자체적으로 개선하고 나면, 여전히 어려운 못 치는 동작을 Demo 데이터가 뒷받침해주어 정책이 이를 습득하도록 만든 것입니다. 이렇게 Demo의 정보가 전 학습 과정에 걸쳐 활용되기 때문에, 초기 Demo에서 담지못한 복잡한 행동까지 최종 정책이 얻게 되는 것이 DAPG의 장점입니다.\n\n참고: DAPG와 대비되는 접근으로, off-policy 방법인 DDPGfD (DDPG from Demonstrations)가 있습니다. DDPGfD는 Q러닝 기반 연속제어 알고리즘인 DDPG에 리플레이 버퍼 초기화 형태로 Demo을 넣고, 우선순위 경험 재생(PER), n-step 보상, 네트워크 가중치 정규화 등 여러 기법을 조합하여 Demo을 활용하는 방법입니다. off-policy 방법은 동일한 데이터로 반복 학습하므로 표본 효율은 높을 수 있지만, 훈련이 불안정하고 특히 고차원 환경에서는 민감도가 높다는 단점이 있습니다. 반면 DAPG는 on-policy 업데이트로 안정성을 확보하고 Demo으로 효율 향상까지 얻은 방법으로, 논문 실험에서도 DDPGfD보다 우수한 성능을 보였습니다. (자세한 비교는 뒤의 실험 결과에서 다룹니다.)"
  },
  {
    "objectID": "posts/paper/2025-09-05-dapg.html#실험-결과-및-분석",
    "href": "posts/paper/2025-09-05-dapg.html#실험-결과-및-분석",
    "title": "📃DAPG 리뷰",
    "section": "2.3 실험 결과 및 분석",
    "text": "2.3 실험 결과 및 분석\n\n2.3.1 강화학습 단독으로는 한계\n우선 Demo을 사용하지 않고 순수 RL만으로 앞서 소개한 네 가지 작업을 학습시켜 본 결과, 여러 문제점이 나타났습니다. Sparse reward (최종 성공 여부만 보상) 설정에서는 대다수 작업에서 아무런 학습 진전이 없었는데, 이는 앞서 언급한대로 무작위 탐색으로는 성공 사례를 만나지 못해 정책이 옳은 방향으로 갱신되지 않기 때문이었습니다. 예외적으로 펜 돌리기 (in-hand) 작업은 매우 간단한 성공 조건 덕에 극히 드물게 성공 에피소드가 발생하여 조금이나마 학습이 진행되었으나, 다른 작업들은 전혀 성공을 경험하지 못한 채 보상이 0으로 머물렀습니다.\n이 때문에 저자들은 RL을 학습시키기 위해 부득이하게 각 작업별로 세밀한 shaped reward를 설계하여 투입했습니다. 예컨대 문 열기 작업의 경우 “걸쇠를 어느 정도 돌리면 +보상, 문을 살짝 열면 +보상” 등의 중간 보상을 단계별로 주는 식입니다. 이러한 휴리스틱 보상 덕분에 NPG 알고리즘은 모든 작업에서 정책을 어느 정도 학습할 수 있었습니다. 최종 성능을 100회 시도 중 성공률(%)로 평가했을 때, NPG는 각 과제에서 상당히 높은 성공률을 보여주었습니다.\n\n\n\nFigure 7에 제시된 결과에 따르면 NPG의 경우 네 작업 모두 약 80~100%에 수렴하는 성능을 보인 반면, DDPG 알고리즘은 광범위한 튜닝에도 불구하고 어떠한 작업도 성공적으로 학습하지 못했습니다. (DDPG는 오프폴리시의 장점으로 데이터 효율은 높지만, 고차원 연속제어에서는 파라미터 민감도와 불안정성으로 학습 실패할 때가 많다는 지적이 있습니다. 이 실험에서도 복잡한 손 환경에 적합하지 않음이 확인되었습니다.) 비록 NPG로 shaped 보상 하에 학습이 되긴 했지만, 학습 속도와 정책 품질 면에서 문제가 남았습니다. 우선 학습에 요구되는 표본 수가 매우 컸는데, 논문 부속자료의 Table I에 정리된 바에 따르면 Demo 없이 shaped 보상으로 학습한 경우 수백 회의 정책 업데이트(하나의 업데이트당 200 에피소드 샘플) 후에야 90% 성공률에 도달했습니다. 이를 실제 로봇 시간으로 환산하면 수십~수백 시간에 이르는 분량입니다. 아래 표는 각 작업에 대해 DAPG vs. 순수 RL의 학습 소요 시간을 비교한 것으로, DAPG가 얼마나 학습을 가속했는지 잘 보여줍니다:\n\n\n\n\n\n\n\n\n\n작업 (Task)\nDAPG (Demo + sparse 보상)\nRL (NPG) – shaped 보상\nRL (NPG) – sparse 보상\n\n\n\n\nRelocation (물체 옮기기)\n52회 업데이트  (~5.8 시간)\n880회  (~98 시간)\n실패 (학습 불가)\n\n\nHammer (망치질)\n55회 업데이트  (~6.1 시간)\n448회  (~50 시간)\n실패 (학습 불가)\n\n\nDoor (문 열기)\n42회 업데이트  (~4.7 시간)\n146회  (~16.2 시간)\n실패 (학습 불가)\n\n\nPen (펜 회전)\n30회 업데이트  (~3.3 시간)\n864회  (~96 시간)\n2900회  (~322 시간)\n\n\n\n\n표 1: Demo 활용 여부에 따른 학습 소요 비교 (논문 Table I 기반 재구성).\n\nDAPG는 Demo 덕분에 sparse 보상만으로도 각 작업을 수 시간 내에 학습을 완료한 반면, 순수 RL (NPG)는 shaped 보상이 있어도 수십~수백 시간의 경험을 필요로 했습니다. 특히 sparse 보상만 주는 경우, Pen 작업을 제외하면 RL은 아예 학습 진행이 안 되어 무한대(∞)로 표시되었고, Pen도 300시간 이상의 방대한 경험을 쌓아야 겨우 성공률 기준을 만족시켰습니다. 또한 정책의 행동 품질도 큰 차이가 나타났습니다. 순수 RL로 학습된 정책들은 정의된 보상만 극대화하려다 보니 종종 엉뚱한 방식으로 과제를 수행했습니다. 예를 들어, 물체를 옮기는 작업에서 사람이라면 편하게 쥘 공을 매우 이상한 손가락 꼬임 자세로 쥔다든지, 망치질 작업에서 망치를 비틀어서 잡는 등 비효율적이거나 부자연스러운 동작이 관찰되었습니다 (논문 Figure 8 참조).\n\n\n\n이러한 정책은 사소한 변화에도 쉽게 실패했는데, 저자들이 정책의 강인성(robustness)을 실험한 결과 순수 RL 정책은 환경 파라미터가 조금만 달라져도 성공 확률이 급격히 떨어진 반면, DAPG로 학습한 정책은 변화된 상황에서도 높은 성능을 유지했습니다.\n\n\n\nFigure 9에 제시된 그래프에서, 예를 들어 DAPG 정책은 공의 질량이나 마찰계수가 달라져도 성공률 곡선이 완만하게 유지되지만, 순수 RL 정책은 기준 환경에서 벗어나면 성능이 크게 저하되거나 아예 학습 자체가 안 되는 모습이 확인되었습니다.\n\n\n2.3.2 DAPG의 성능\n빠른 학습과 향상된 동작 품질 Demo을 도입한 DAPG 알고리즘은 위의 문제들을 극적으로 개선하였습니다.\n가장 큰 개선은 학습 효율로, Table 1에서 보듯이 모든 작업에서 RL 단독 대비 월등히 적은 시간 내에 정책을 학습했습니다. 특히 Pen (펜 돌리기)의 경우 shaped 보상으로 96시간 걸리던 것이 3.3시간으로 단축되어 약 30배의 가속이 달성되었습니다. 다른 작업들도 8배에서 20배 이상의 속도 향상을 보여, 전반적으로 “Demo + RL” 조합의 효과가 입증되었습니다. 저자들은 DAPG가 아니었다면 수 일(또는 수 주) 걸릴 학습을 몇 시간 수준으로 줄임으로써, 강화학습으로 복잡한 손 기술을 배우는 것이 현실적인 시간 스케일에서 가능함을 보였다고 강조합니다. 실제 논문에서 DAPG는 모든 작업을 5시간 이내에 학습시켰으며, 이는 곧 충분한 병렬화나 시뮬레이터 고속화를 통해 실제 로봇으로도 학습을 돌려볼 수 있는 수준입니다. (물론 아직 시뮬레이션에서만 검증되었지만, “few hours of robot experience”라는 표현에서 시사하듯이 저자들은 DAPG의 효율이라면 실제 로봇 학습도 도전해볼 만하다고 언급합니다.)\n다음으로 정책의 성공률과 견고성 측면에서도 DAPG는 뛰어난 성능을 보였습니다. Demo 덕분에 별도 보상 shaping 없이도 충분한 탐색이 이루어져, 저자들은 DAPG 실험에서는 최종 성공 여부에 대한 sparse 보상만 사용하여도 학습이 가능하도록 설정하였습니다. 그럼에도 불구하고 DAPG 정책은 최종 성공률 면에서 앞서 shaped 보상으로 학습한 NPG 정책에 버금가거나 더 나은 수준을 달성했습니다. 또한 흥미롭게도 정책의 강인성이 크게 향상되었는데, DAPG로 얻은 정책은 환경 변화(무게, 마찰, 초기 조건 등)에 훨씬 둔감하여 폭넓은 상황에서 성공을 거두었습니다. 논문에서는 이를 “인간 전략의 내재적 강인성(intrinsic robustness of human strategies)”이 Demo을 통해 학습되었기 때문이라고 해석하고 있습니다. 인간 Demo 제공자는 작업을 성공하기 위해 다양한 요인을 보정하며 행동하는데, 이런 휴리스틱 노하우가 정책 네트워크에 반영되어 특정 환경에 오버피팅되지 않는 일반적 해결책을 얻게 되었다는 것입니다.\n마지막으로 정책 동작의 자연스러움과 관련해, DAPG는 눈에 띄게 인간과 유사한 움직임을 보여주었습니다. 이는 정량적 지표로 측정하기는 어렵지만, 논문 저자들이 함께 공개한 비디오에서 DAPG 정책이 수행하는 동작은 Demo 제공 자(인간)가 했을 법한 방식과 유사한 부분이 많음을 알 수 있습니다. 예를 들어, 공을 집어 옮길 때 손가락을 모아서 쥐는 모양새나, 망치를 휘두르는 속도와 궤적 등이 비교적 자연스럽습니다. 반면 보상 함수를 잘못 설계한 RL 정책은 때로 관절 가동 범위를 이상하게 쓰거나 목적에 맞지 않는 손가락 움직임을 보였는데, DAPG 정책에는 그런 엉뚱한 행동이 현저히 줄어든 것을 확인할 수 있습니다. 저자들은 “정책이 학습 과정에서 명시적으로 주지 않았던 ’인간스러움’의 특성을 Demo을 통해 얻게 되었다”고 평합니다. 물론 이는 사이드 이펙트이긴 하지만, 향후 인간과 함께 작업하는 로봇손이라면 이러한 사람다운 움직임이 주는 신뢰성과 안전성 이점도 무시할 수 없을 것입니다.\n\n\n2.3.3 DAPG vs. DDPGfD 등 다른 방법과의 비교\nDAPG의 효과를 더 입증하기 위해, 저자들은 기존의 Demo 활용 RL 기법들과 정량 비교를 수행했습니다. 그 중 한 가지 대표 비교 대상은 앞서 언급한 DDPGfD 알고리즘입니다. 실험 조건을 맞추기 위해 DAPG와 DDPGfD 모두 동일한 Demo 데이터(25개)를 사용하고, sparse 보상만으로 각 작업을 학습시켰습니다.\n\n\n\n\n\n\n(참고) DDPGfD(Deep Deterministic Policy Gradient from Demonstrations)\n\n\n\n\n\n\n개요(무엇이고 왜 고안되었는가)\n\n\nDDPGfD는 Vecerík et al. (2017)에서 제안된 방법으로, 기본적으로는 오프폴리시 연속 제어용 알고리즘인 DDPG를 사용하되, 인간(또는 전문가) 데모를 학습에 직접 결합해 탐색과 샘플 효율을 개선한 기법입니다.\n핵심 아이디어는 성공적인 데모 전이(transitions)를 리플레이 버퍼에 넣고, 데모 전이의 재생 빈도를 높이는 등으로 학습 초기에 의미있는 상태-행동(즉, 유의미한 보상)을 자주 보게 하여 sparse reward 문제와 탐색 문제를 완화하는 것입니다.\n\n\nDDPG(기본) 핵심 수식(요약)\n\n\nDDPG는 deterministic 정책 \\mu_\\theta(s)를 학습하고, Q-함수 Q_\\phi(s,a)를 비평자(critic)으로 학습합니다.\ncritic의 손실은 다음의 MSE를 최소화하는 형태입니다. L(\\phi)=\\mathbb{E}_{(s,a,r,s')\\sim\\mathcal{D}}\\bigl[(Q_\\phi(s,a)-y)^2\\bigr]\n타깃 값 y는 다음과 같이 정의됩니다. y = r + \\gamma Q_{\\phi'}\\bigl(s',\\mu_{\\theta'}(s')\\bigr)\nactor(정책) 업데이트는 deterministic policy gradient를 사용합니다: \\nabla_\\theta J \\approx \\mathbb{E}_{s\\sim\\mathcal{D}}\\Big[\\nabla_\\theta \\mu_\\theta(s)\\,\\nabla_a Q_\\phi(s,a)\\big|_{a=\\mu_\\theta(s)}\\Big].\n\n\nDDPGfD의 주요 구성요소(구체적 기법)\n\n\n데모 전이 추가: 수집한 전문가(또는 그와 유사한) 데모 전이들을 리플레이 버퍼에 미리 채워 넣습니다. 데모 전이는 학습 중 영구적으로(special flag로) 유지하거나, 다른 전이보다 높은 우선순위를 주어 더 자주 샘플되도록 합니다.\nPrioritized Experience Replay (PER): 표준 PER을 사용해 샘플링 확률을 TD-에러에 비례하도록 하되, 데모 전이에 작은 상수 우선순위를 더해(또는 초기 큰 우선순위로) 데모가 충분히 자주 재생되도록 합니다. PER 샘플링 확률의 일반적 형태는: p_i \\propto (|\\delta_i|+\\varepsilon)^\\alpha 여기서 \\delta_i는 i번째 transition의 TD-오차이고, \\varepsilon,\\alpha는 하이퍼파라미터입니다.\nn-step returns: 1-step 대신 n-step 누적보상을 사용해 TD 타깃을 계산하면 더 긴 시간축의 신호가 critic에 전달됩니다. n-step 타깃의 전형적 형태: y_{t}^{(n)} = \\sum_{k=0}^{n-1}\\gamma^{k}r_{t+k} + \\gamma^n Q_{\\phi'}(s_{t+n},\\mu_{\\theta'}(s_{t+n})).\nRegularization / 안정화: actor/critic 네트워크에 대한 L2 정규화나 드롭아웃은 아니더라도 weight decay, 타깃 네트워크의 느린 업데이트(soft target update) 등을 포함합니다.\n기타: DDPG 특유의 탐색 노이즈(예: Ornstein-Uhlenbeck 또는 가우시안)를 사용하여 행동을 탐색합니다.\n\n\nDDPGfD의 동작 흐름(간단한 절차)\n\n\n\n전문가 데모를 수집해서 리플레이 버퍼에 삽입(데모 레이블 표시).\n\n\n초기부터 데모와 에이전트 자기 샘플이 섞여서 학습이 진행됨. 데모는 우선순위를 높게 유지해 자주 리플레이됨.\n\n\ncritic은 n-step 및 1-step 타깃으로 학습, actor는 deterministic gradient로 업데이트.\n\n\nPER의 importance-sampling 보정 등을 적용하여 업데이트 오프셋 조정.\n\n\n\nDDPGfD의 장점과 한계(이 논문 및 기존 결과 기반) 장점:\n\n\nSparse reward 환경에서 탐색 문제를 크게 개선.\n오프폴리시 특성 덕분에 데모와 에이전트 데이터를 동일 버퍼에서 효율적으로 재활용하여 샘플 효율이 좋아짐. 한계 / 주의점:\nDDPG(기저)의 불안정성, 하이퍼파라미터 민감성(특히 고차원·복잡 접촉 환경에서는 더 심함).\n데모가 있더라도 고차원 손 조작(hand)처럼 행동·상태 차원이 크고 접촉 불연속성 많은 문제에서는 학습이 불안정하거나 수렴하지 않을 수 있음(본 논문에서는 DDPGfD가 ADROIT 24-DoF 작업에서 DAPG보다 성능이 낮음).\n데모와 에이전트 경험의 혼합 비중(우선순위·비율)에 민감하여 잘못 설정하면 과도하게 데모에 의존하거나 반대로 데모의 이점이 사라짐.\n\n\nDAPG와의 비교(이 논문 맥락) — 핵심 차이점 요약\n\n\n알고리즘 계열: DDPGfD는 오프폴리시 Q-러닝/actor-critic 계열(DPG 계열)이고, DAPG는 온폴리시 natural policy gradient (NPG) 기반(정책 그래디언트)입니다.\n데모 통합 방식:\n\nDDPGfD는 데모를 리플레이에 넣어 샘플링 빈도를 높이는 방식이고,\nDAPG는 (i) behavior cloning으로 정책을 초기화한 뒤 (ii) RL fine-tuning 과정에서 시연(데모)에 대한 추가 손실(데모 log-likelihood 가중치)을 정책 그래디언트에 직접 더하는 방식으로 데모를 사용합니다. DAPG의 데모 보조 그래디언트는 아래와 같이 표현됩니다. g_{\\text{aug}} = \\sum_{(s,a)\\in\\rho_\\pi}\\nabla_\\theta \\ln\\pi_\\theta(a|s)\\,A^\\pi(s,a) + \\sum_{(s,a)\\in\\rho_D}\\nabla_\\theta \\ln\\pi_\\theta(a|s)\\,w(s,a).\n\n안정성 vs. 샘플 효율: 오프폴리시 방법(DDPGfD)은 이론적으로 샘플 효율이 우수하지만 불안정할 수 있고 하이퍼파라미터에 민감합니다. 온폴리시 방식(DAPG)은 더 안정적이며, 본 논문 결과에선 데모 기반 보조(BC 초기화 + augmented loss)가 결합되어 샘플 효율과 안정성 모두에서 우수한 성능을 보였습니다.\n\n\n구현·재현 시 유의사항(실전 팁)\n\n\n데모 전이의 처리: 데모 전이를 영구 보존할지(즉 제거되지 않게) 또는 일정 기간만 우선시할지 결정해야 합니다. DDPGfD 원본/파생 구현들은 종종 데모 전이에 높은 우선순위를 부여하고 오래 유지합니다.\nPER 파라미터 튜닝: \\alpha,\\varepsilon와 importance sampling 보정 계수는 안정성에 영향 큽니다.\n탐색 노이즈: 고차원 핸드에서는 OU noise보다는 단순 가우시안을 쓰거나 탐색 전략을 더 섬세히 설계해야 할 수 있습니다.\nn-step의 선택: 너무 큰 n은 분산을 줄이지만 편향을 유발할 수 있으므로 환경에 따라 실험적으로 결정합니다.\n정규화 및 타깃 업데이트 속도: 타깃 네트워크의 soft update 계수 \\tau를 작게(느리게) 하면 안정성이 올라갑니다.\n\n\n연구적 개선 방향(제안)\n\n\n데모 신뢰도 가중치 자동화: 데모가 항상 최적이 아닐 수 있으므로, 데모 전이마다 불확실성/우수성을 추정해 가중치를 부여하는 방법(Apex-style priority + uncertainty) 제안.\n온 · 오프폴리시 하이브리드: DAPG의 augmented-policy-gradient 아이디어를 오프폴리시 DDPGfD에 결합하여, 데모에 대한 직접적인 행동 복제 손실을 actor 업데이트에도 추가하는 하이브리드 방식 시도.\n데모와 RL 이득의 자동 균형: 학습 진행에 따라 데모 영향력을 자동으로 감쇠시키는 적응형 스케줄(논문은 하이퍼파라미터로 감쇠했음)을 불확실성 기반으로 제어.\nHER( hindsight replay )와의 결합: 목표-조건화 과제에 대해 DDPGfD+HER 조합이 탐색에 큰 이득을 줄 수 있음(동시기 연구에서 제안됨).\n시뮬→실 이전을 위한 도메인 랜덤화·모델 앙상블 병행.\n\n요약하자면, DDPGfD는 데모를 오프폴리시 DDPG의 리플레이 메커니즘에 섞어 샘플 효율을 끌어올리는 실용적 접근입니다. 그러나 DDPG 기반의 민감성과 고차원 접촉 문제에서의 불안정성 때문에, ADROIT 같은 24-DoF 복잡 작업에서는 본 논문에서 제시한 DAPG(BC 초기화 + on-policy augmented gradient)가 더 견고하고 빠르게 수렴하는 결과를 보였습니다.\n\nDDPGfD VS. DAPG\n\nDDPGfD: 데모를 리플레이 버퍼에 “데이터(transition)”로 넣어 오프폴리시 학습 과정에서 재사용한다 — 데모는 주로 critic 학습과 오프라인 재생(우선순위 샘플링)에 영향을 준다.\nDAPG: 데모로 먼저 정책을 “행동 복제(behavior cloning)”로 초기화한 뒤, 온폴리시 정책 그래디언트에 데모 기반의 추가적인 복제-정규화 항을 직접 더해 policy 업데이트를 유도한다 — 데모가 정책(매개변수) 업데이트를 직접 제어한다.\n\n구체적 차이 (구성 요소별)\n\n데모의 저장/재생 방식\n\nDDPGfD: 데모 전이들을 리플레이 버퍼에 보관하고, 우선순위 경험 재생(PER)·n-step 타깃 등으로 자주 샘플한다. 데모는 actor·critic 학습 샘플의 일부로 간주된다.\nDAPG: 데모는 먼저 행동 복제(감독 학습)용 데이터로 사용되어 정책 파라미터를 초기화하고, 이후 학습 중에는 정책 그래디언트에 명시적 데모 항을 더하는 형태로 사용된다.\n\n데모가 영향을 미치는 대상\n\nDDPGfD: 주로 critic(Q) 추정과 그로부터 유도되는 actor gradient에 간접적으로 영향. 데모가 Q 타깃/TD-에러를 형성하므로 critic의 추정이 변하고, actor는 그 추정의 그래디언트를 따른다.\nDAPG: 데모가 정책의 파라미터(행동 분포)에 직접적인 감독 신호(BC 초기화 + 데모 log-likelihood 항)를 준다. 즉 데모가 policy 업데이트 식에 명시적으로 들어간다.\n\n\n핵심 수식(간단 참고)\n\nBehavior cloning(BC) 목적(데모로 초기화): \\max_\\theta \\sum_{(s,a)\\in \\rho_D} \\ln \\pi_\\theta(a|s).\nDAPG에서 fine‑tuning 시 사용하는 보강(augmented) 그래디언트(정책 그래디언트 + 데모 항): \n\\begin{aligned}\ng_{\\text{aug}} &= \\sum_{(s,a)\\in \\rho_\\pi} \\nabla_\\theta \\ln\\pi_\\theta(a|s)\\,A^\\pi(s,a) \\\\\n&\\quad + \\sum_{(s,a)\\in \\rho_D} \\nabla_\\theta \\ln\\pi_\\theta(a|s)\\,w(s,a).\n\\end{aligned}\n\nDDPG(비교용) critic 타깃과 actor gradient: \n\\begin{aligned}\ny &= r + \\gamma Q_{\\phi'}\\bigl(s',\\mu_{\\theta'}(s')\\bigr),\\\\\n\\nabla_\\theta J &\\approx \\mathbb{E}_{s\\sim\\mathcal{D}}\\Big[\\nabla_\\theta \\mu_\\theta(s)\\,\\nabla_a Q_\\phi(s,a)\\big|_{a=\\mu_\\theta(s)}\\Big].\n\\end{aligned}\n\n\n왜 결과가 다르게 나타나는가 (직관적 이유)\n\n직접성 vs 간접성: DAPG는 데모가 정책 업데이트 항에 직접 들어가므로, “사고(behavior)”를 바로 보존·유도할 수 있다. 반면 DDPGfD는 데모가 Q 추정에 영향을 주고 그 영향이 다시 actor로 전파되는 간접 경로를 택한다. 이 간접 경로는 함수 근사·부트스트래핑 오류에 의해 왜곡될 가능성이 크다.\n분포 일치성(distribution match): DAPG의 온폴리시 fine‑tuning은 advantage 계산이 항상 최신 정책 분포에서 이뤄지므로 데모와 정책 간 분포 불일치 문제를 비교적 잘 제어한다. DDPGfD는 과거 데모+다양한 오프폴리시 데이터가 섞여 critic에 분포 불일치를 일으키기 쉽다.\n안정화 수단: DAPG(기반 NPG)는 Fisher/신뢰영역과 같은 규제(또는 제한적 스텝)를 이용해 업데이트 크기를 제어하는 반면, DDPGfD는 critic 오차·타깃의 부트스트래핑으로 인해 잘못된 Q 추정이 actor를 크게 흔들 수 있다.\n실무적 결과: 그래서 데모가 적지만 고차원·접촉이 많은 환경에서는 DAPG 방식(BC 초기화 + 데모 항 보강)이 더 안정적이고 샘플 효율이 좋은 반면, DDPGfD는 잘 튜닝되면 샘플 재사용 측면에서 이득이 크지만 불안정성·과대평가 위험에 취약하다.\n\nDDPGfD (의사코드)\n\n전제: deterministic actor \\mu_\\theta, critic Q_\\phi, 타깃 네트워크 \\mu_{\\theta'}, Q_{\\phi'}, replay buffer \\mathcal{D}에 데모 전이 \\rho_D를 미리 삽입, Prioritized Experience Replay(PER), optional n-step returns.\n하이레벨:\n\nInitialize \\theta,\\phi,\\theta',\\phi'; fill replay buffer \\mathcal{D} with demo transitions \\rho_D (mark them as demo).\n반복(환경 interaction 단계):\n\n행동 생성: 현재 상태 s_t에서 행동 a_t=\\mu_\\theta(s_t)+\\text{noise}로 실행하여 (s_t,a_t,r_t,s_{t+1})를 수집하고 \\mathcal{D}에 저장.\n(주기적으로) 업데이트 반복:\n\n미니배치 B를 \\mathcal{D}에서 샘플(데모 샘플이 충분히 포함되도록 PER/우선순위 설정).\n(n-step 사용 시) n-step 타깃 계산: \ny_t^{(n)}=\\sum_{k=0}^{n-1}\\gamma^k r_{t+k} + \\gamma^n Q_{\\phi'}\\bigl(s_{t+n},\\mu_{\\theta'}(s_{t+n})\\bigr)\n\n1-step 타깃(기본): \ny_t = r_t + \\gamma\\, Q_{\\phi'}\\bigl(s_{t+1},\\mu_{\\theta'}(s_{t+1})\\bigr)\n\ncritic 손실: \nL(\\phi)=\\mathbb{E}_{(s,a,r,s')\\in B}\\bigl[ \\bigl(Q_\\phi(s,a)-y\\bigr)^2 \\bigr]\n\ncritic 파라미터 갱신: \\phi \\leftarrow \\phi - \\alpha_c \\nabla_\\phi L(\\phi).\nactor 업데이트(지연/주기적): \n\\nabla_\\theta J \\approx \\mathbb{E}_{s\\in B}\\Big[ \\nabla_\\theta \\mu_\\theta(s)\\; \\nabla_a Q_\\phi(s,a)\\big|_{a=\\mu_\\theta(s)} \\Big]\n 그리고 \\theta \\leftarrow \\theta + \\alpha_a \\nabla_\\theta J.\nPER 우선순위 갱신: 각 샘플의 priority p_i \\leftarrow |\\delta_i| + \\varepsilon (여기서 \\delta_i = y_i - Q_\\phi(s_i,a_i)).\n타깃 네트워크 소프트 업데이트: \n\\phi' \\leftarrow \\tau \\phi + (1-\\tau)\\phi',\\qquad \\theta' \\leftarrow \\tau \\theta + (1-\\tau)\\theta'.\n\n\n\n반복 종료 조건 만족 시 종료.\n\n특기사항(구현 팁)\n\n데모 전이는 PER에서 높은 우선순위를 주거나, 영구적으로 buffer에 남겨 자주 샘플되게 함.\nexploration noise 세팅, n-step, PER 하이퍼파라미터가 안정성에 큰 영향.\n\n\nDAPG (의사코드)\n\n전제: stochastic policy \\pi_\\theta(a|s) (예: Gaussian), 데모 집합 \\rho_D 사용, on-policy rollouts, Natural Policy Gradient(NPG) 기반 업데이트, 초기에는 Behavior Cloning(BC)으로 pretrain.\n하이레벨:\n\nBehavior cloning(사전학습): 데모로 정책 초기화 \n\\max_\\theta \\sum_{(s,a)\\in\\rho_D} \\ln \\pi_\\theta(a|s)\n (즉, supervised learning으로 \\theta 초기화).\n반복(on-policy 학습 단계):\n\n온폴리시 샘플 수집: 현재 정책 \\pi_\\theta로 여러 에피소드(또는 배치) 실행하여 롤아웃 집합 \\rho_\\pi 수집.\nAdvantage 추정: 수집한 롤아웃으로 advantage \\hat A^\\pi(s,a) 계산(예: GAE 또는 TD-return).\npolicy gradient(데모 보강 포함) 계산: \n\\begin{aligned}\ng_{\\text{RL}} &= \\sum_{(s,a)\\in\\rho_\\pi} \\nabla_\\theta \\ln\\pi_\\theta(a|s)\\; \\hat A^\\pi(s,a) \\\\\ng_{\\text{BC}} &= \\sum_{(s,a)\\in\\rho_D} \\nabla_\\theta \\ln\\pi_\\theta(a|s)\\; w(s,a)\n\\end{aligned}\n 여기서 w(s,a)는 데모 항의 가중치(논문에서는 heuristics, 예: w(s,a)=\\lambda_0\\lambda_1^k\\max_{(s',a')\\in\\rho_\\pi}\\hat A^\\pi(s',a')).\n\n합성 보강 그래디언트: \ng_{\\text{aug}} = g_{\\text{RL}} + g_{\\text{BC}}\n\n\nNatural Policy Gradient 업데이트(정규화된 피셔 전치 사용):\n\nFisher 정보행렬 근사: \nF_\\theta = \\mathbb{E}_{(s,a)\\in\\rho_\\pi}\\big[ \\nabla_\\theta \\ln\\pi_\\theta(a|s)\\, \\nabla_\\theta \\ln\\pi_\\theta(a|s)^\\top \\big]\n\nNPG 업데이트(정규화된 스텝 크기 \\delta): \n\\theta \\leftarrow \\theta + \\sqrt{\\delta}\\; \\frac{F_\\theta^{-1}\\, g_{\\text{aug}}}{\\sqrt{g_{\\text{aug}}^\\top F_\\theta^{-1} g_{\\text{aug}}}}\n\n\n(선택) 데모 가중치 w 감쇠: iteration k에 따라 w \\leftarrow \\lambda_1^k w.\n\n반복 종료 조건 만족 시 종료.\n\n특기사항(구현 팁)\n\nBC로 초기화하면 초기 탐색이 안정적으로 이루어져 critic(혹은 advantage 추정)의 분포 문제를 줄임(온폴리시이므로 리플레이로 인한 분포 mismatch 없음).\n데모 항의 가중치 스케줄은 안정성·개선 여지에 큰 영향.\n\n\n비교 요약\n\n데모의 사용 방식:\n\nDDPGfD: 데모를 transition 데이터로 replay buffer에 넣어 오프폴리시 방식으로 간접 활용(critic 학습 → actor에 간접 영향).\nDAPG: 데모로 정책을 직접 초기화(BC)하고, 온폴리시 정책 업데이트에 데모 기반 log-likelihood 항을 보강(정책에 직접 영향).\n\n데이터 분포 관점:\n\nDDPGfD: 리플레이에 의해 과거/데모/에이전트 경험이 섞이며 distribution mismatch 유발 가능.\nDAPG: 항상 최신 정책 데이터로 advantage를 계산하므로 분포 mismatch가 적음.\n\n안정성·민감도:\n\nDDPGfD: off-policy + 부트스트래핑으로 extrapolation/과대추정 문제에 민감(특히 고차원·접촉 환경).\nDAPG: NPG의 신뢰영역/BC 초기화로 업데이트가 더 보수적·안정적.\n\n\n\n왜 오프폴리시 DDPGfD가 고차원·접촉이 많은 환경에서 특히 불안정하게 동작하는가\n\n핵심 수식(요약)\n\n\nDDPG/ DDGPfD에서 critic( Q ) 업데이트와 target 값: L(\\phi)=\\mathbb{E}_{(s,a,r,s')\\sim\\mathcal{D}}\\big[(Q_\\phi(s,a)-y)^2\\big] 그리고 타깃 값은 보통 다음과 같습니다. y = r + \\gamma Q_{\\phi'}\\bigl(s',\\mu_{\\theta'}(s')\\bigr).\nDDPG의 actor 업데이트(Deterministic Policy Gradient): \\nabla_\\theta J \\approx \\mathbb{E}_{s\\sim\\mathcal{D}}\\Big[\\nabla_\\theta \\mu_\\theta(s)\\,\\nabla_a Q_\\phi(s,a)\\big|_{a=\\mu_\\theta(s)}\\Big]. (DDPGfD는 데모를 리플레이에 넣고 PER 등으로 샘플링을 조정하는 등의 추가 메커니즘을 갖습니다.)\n반면 온폴리시 NPG/ DAPG 계열에서는 정책 그래디언트(샘플 기반)와 자연 보정(피셔 행렬)을 사용합니다. REINFORCE 계열의 샘플 그레이디언트는: g=\\frac{1}{N}\\sum_{i,t}\\nabla_\\theta \\ln\\pi_\\theta(a_t^i|s_t^i)\\,\\hat A^\\pi(s_t^i,a_t^i). 자연정책경사(NPG) 업데이트 형태(정규화·신뢰영역 효과): \\theta_{k+1}=\\theta_k + \\sqrt{\\delta}\\; \\frac{F^{-1} g}{\\sqrt{g^\\top F^{-1} g}}, 여기서 F는 Fisher 정보 행렬입니다.\n\n\n오프폴리시(DP P G f D)에서의 불안정성 — 구체적 메커니즘과 타깃/업데이트 관점의 연결 아래 항목들은 서로 연쇄적으로 악영향을 주며, 특히 접촉이 많고 상태/보상이 불연속적인 환경에서 그 정도가 커집니다.\n\n\ndeadly triad: function approximation + bootstrapping + off-policy 학습\n\n\nDDPG는 함수 근사(신경망), 부트스트래핑(y가 Q_{\\phi'}를 포함)과 오프폴리시 경험 재사용(리플레이)을 동시에 사용합니다. 이 세 가지가 결합되면 학습이 발산하거나 잘못된 값(예: 과대추정)을 만들기 쉽습니다.\n이유(타깃 관점): 타깃 y가 critic의 추정값 Q_{\\phi'}에 의존하므로, critic의 오차가 다시 타깃으로 들어가며 증폭될 수 있습니다. 접촉 동역학에서 작은 위치 변화가 보상/전이에 큰 비선형 변화를 만들면 이 현상은 더 심합니다.\n\n\n리플레이 버퍼에 의한 분포 불일치(Distribution mismatch)와 extrapolation error\n\n\n오프폴리시 리플레이는 옛 정책(또는 데모, 환경에서 수집된 다양한 데이터)에서 온 상태·행동을 섞어 학습합니다. 이로 인해 critic은 현재 정책이 자주 방문하지 않는 상태·행동 쌍까지 근사하려 합니다. 함수 근사기는 데이터가 희박한 영역에서 과도하게 일반화하거나(특히 고차원 액션공간에서) 과대평가(overestimation)를 만듭니다(이를 extrapolation error라 부릅니다).\n정책 업데이트 관점: actor는 \\nabla_a Q_\\phi(s,a)에 의해 행동을 밀어붙입니다. 만약 Q_\\phi가 데이터 희박 영역에서 크게 과대평가되어 있다면, actor는 실제로 실행하면 재현 불가능한(또는 위험한) 행동으로 policy를 몰아갑니다. 이때 deterministic actor는 탐색 스무스닝이 약해 잘못된 최적화 방향으로 빠르게 수렴합니다.\n\n\ntarget network(지연된 타깃)과 타깃 지연의 양면성\n\n\n타깃 네트워크(\\phi',\\theta')는 안정화를 위해 느리게(soft update) 갱신하지만, 이로 인해 타깃 y는 최신 정책·critic의 분포와 불일치(혹은 lag)를 가지게 됩니다.\n접촉-풍부한 환경에서 작은 정책 변화가 전이 확률과 보상을 크게 바꿀 수 있는데, 타깃이 늦게 따라올 때 critic은 과거 분포에 맞춰 학습하고, actor는 현재 분포에서 critic의 잘못된 기울기를 신뢰하게 됩니다 — 불안정성이 증가합니다.\n\n\n우선순위 재생(PER)·데모 혼합의 편향\n\n\nDDPGfD는 데모의 전이를 많이 재생시키려 데모에 높은 우선순위를 주거나 PER로 TD-오차가 큰 샘플을 더 자주 뽑습니다. 이는 학습 데이터를 비정상적으로 편향시켜 critic이 특정 (때로는 드문) 고 TD-오차 샘플에 과적합하게 만듭니다.\n정책업데이트 관점: 이 편향 때문에 \\nabla_a Q가 특정 샘플에 의해 왜곡될 수 있고, actor는 그 왜곡된 기울기로 큰 변화를 받습니다.\n\n\n접촉 역학의 비연속성과 보상 희소성 → TD 분산 증가\n\n\n접촉(충돌, 마찰, 끊김)은 전이·보상을 비연속적으로 만들고, 동일한 상태·행동에서 큰 보상 차이 또는 분포의 꼬리를 생성합니다. 부트스트래핑 기반 업데이트는 이러한 분산/비연속성에 민감해 타깃이 자주 크게 흔들리며 TD 오차 분포가 넓어져 학습 불안정이 커집니다.\n결과적으로 critic의 추정 분산이 커지고 액션 그래디언트의 방향성도 불안정해져 actor 업데이트가 급격히 흔들립니다.\n\n\ndeterministic 정책의 민감성(탐색·정책의 폭)\n\n\nDDPG 계열은 deterministic policy \\mu_\\theta(s)를 중심으로 동작하고, 탐색은 외부 노이즈로 추가합니다. 이 구성은 critic의 오차가 policy에 즉각적으로 반영되기 쉬워, 잘못된 Q 추정값이 곧 policy 변화를 초래합니다. 반면 확률적 정책(예: SAC)은 엔트로피로 탐색을 더 부드럽게 하고 Q의 과대평가가 policy에 미치는 즉각적 영향이 더 완화됩니다.\n\n\n온폴리시(DAPG/NPG)와 비교 — 왜 더 견고한가?\n\n\non-policy 방법은 advantage/value 추정이 “현재(policy) 분포에 대해” 계산됩니다. 즉 데이터가 항상 최신 정책에서 수집되므로 분포 불일치가 작습니다(리플레이로 인한 extrapolation error 감소).\nNPG/TRPO/PPO 계열은 보통 신뢰영역(KL constraint) 또는 Fisher 기반 정규화를 통해 한 스텝에 정책이 크게 변하지 않도록 제약합니다. NPG 업데이트 식에서 Fisher 전치가 정책 파라미터의 스케일 문제를 완화해 큰 불안정한 파라미터 변화를 억제합니다.\nDAPG는 데모로 behavior cloning으로 초기화하여 critic과 policy가 합리적 영역에서 시작하므로 off-policy critic이 겪는 OOD 문제를 근본적으로 줄입니다. 또한 RL fine-tuning시 데모-로스 항을 추가하여(초기에) 정책이 위험한 OOD 행동으로 벗어나지 않도록 가이드합니다.\n\n\n실험적 진단 지표(무엇을 모니터링할 것인가)\n\n\nTD-error 분포(평균·분산)와 그 시간적 추세. 갑작스런 분산 증가 → 부트스트랩 불안정 신호.\nQ값과 실제 에피소드 누적 리턴의 상관관계(과대/과소추정 확인).\nactor가 생성하는 행동이 replay buffer에서 얼마나 OOD인지(예: 상태-행동 쌍의 평균 거리).\n액션-그레이디언트 \\nabla_a Q의 노름과 방향 변동성(큰 진동은 위험 신호).\n다른 랜덤 시드들 간 성능 편차(민감도).\n\n\n완화 기법(현실적 대안/개선)\n\n\nDouble Q / Clipped Double Q (TD3): Q 과대추정을 줄이기 위해 두 critic을 두고 타깃에 더 보수적인 값을 사용.\nDelayed policy updates: critic을 충분히 업데이트한 뒤 actor를 업데이트(예: TD3).\nEntropy-regularized stochastic policy (SAC): 확률정책·엔트로피로 안정화.\nConservative Q-learning (CQL) 등 보수적 오프폴리시 방법: OOD 상태-행동에 대해 Q를 낮추는 정규화로 extrapolation error 완화.\nEnsemble / bootstrap critics: 여러 critic 평균으로 과대추정을 완화.\nBehavior cloning + regularization: DAPG 스타일로 데모로 초기화하고, actor 업데이트에 BC 항(또는 행동 보존 항)을 추가해 OOD drift 억제.\n더 자주/짧게 타깃 업데이트하거나 타깃 네트워크 구조·학습률을 조정해 lag를 조절.\n\n\n요약(핵심 메시지)\n\n\n요약하면, 오프폴리시 DDPGfD의 불안정성은 (i) 부트스트래핑된 타깃이 critic의 오차를 증폭시키는 점, (ii) 리플레이로 인한 분포 불일치로 함수 근사기의 extrapolation error가 커지는 점, (iii) deterministic actor가 잘못된 critic 기울기를 받아 즉시 크게 변화하는 점이 결합되어 발생합니다. 이 문제들은 특히 접촉이 많아 전이·보상이 비연속적이고 보상이 희소한 고차원 환경에서 증폭됩니다. 반면 온폴리시 NPG/DAPG는 최신 샘플 기반의 advantage 추정과 신뢰영역/피셔 기반의 안정화, 데모 기반 초기화(및 데모 손실 보정)를 통해 이러한 불안정성에 더 강합니다.\n\n\n\n\n\n\n\n결과는 Figure 10에 학습 곡선으로 제시되어 있는데, DAPG가 모든 작업에서 DDPGfD보다 현저히 빠르고 높은 성능을 보였음을 알 수 있습니다. 구체적으로, DDPGfD는 학습 초반 거의 진전이 없다가 나중에서야 겨우 성공률이 오르는 양상을 보인 반면, DAPG는 매우 초기부터 급격히 성능이 향상되어 일정 에피소드 후에는 두 방법 간 큰 격차가 벌어졌습니다. 예를 들어 물체 옮기기의 경우, DAPG는 수십 회의 업데이트 내에 성공률 곡선이 가파르게 상승하여 목표 성능에 도달했지만, DDPGfD는 같은 시간 내 거의 0% 근처에 머물렀다가 한참 후에야 상승하는 모습을 보였습니다. 최종적으로 DAPG는 앞서 언급한 대로 5시간 내외로 모든 작업을 끝냈지만, DDPGfD는 문 열기 등의 몇몇 작업은 그보다 훨씬 느리거나 끝내 충분한 성능에 도달하지 못한 것으로 나타났습니다.\n이러한 결과는 온-폴리시 정책 그래디언트 방식과 오프-폴리시 Q러닝 방식의 차이를 보여주는 것이기도 합니다. 저자들은 DDPGfD가 근본적으로 off-policy 수렴 불안정성과 고차원에서의 튜닝 어려움 때문에 초반 탐색에서 헤매는 경향을 보인다고 분석합니다. 반대로 DAPG는 on-policy의 안정된 개선에 Demo의 도움까지 더해져 초반 탐색 난관을 빠르게 탈출할 수 있었다는 것입니다. 또한 DDPGfD는 Demo을 활용하긴 해도 경험 재생 버퍼에 섞어주는 방식이기 때문에, 학습과정에서 Demo가 정책에 미치는 영향력이 점차 희석됩니다. 반면 DAPG는 학습 내내 명시적인 Demo 그래디언트를 줬다는 점도 성능 차이의 요인으로 볼 수 있습니다.\n관련하여, 논문에서는 그 밖에도 몇 가지 Demo+RL 방법들을 소개하고 차이점을 언급합니다. 예를 들어 Hester et al.(2018)의 DQfD (Deep Q-learning from Demonstrations) 는 값 함수 기반에서 Demo을 활용한 초기 연구이고, 최근에는 Demo을 보상 함수에 통합하거나 (IRL/보상 shaping 방식), Demo 데이터에 노이즈나 실패 사례가 포함되어 있을 때의 학습법 등도 연구되고 있습니다. Guided Policy Search (GPS) 기반으로 Demo을 활용한 사례도 있는데, 주로 저차원 손 작업(예: 막대기 돌리기 등)에 한정되어 있고 모형 기반 기법이라 실제 적용에 제약이 있습니다. DAPG는 이러한 선행 연구들과 달리, 고차원 정책 신경망을 끝까지 end-to-end로 학습하면서도 샘플 효율과 안정성을 확보했다는 점에서 차별화됩니다. 또한 Demo의 활용 방식에 있어서도 DAPG는 사전 학습 + 학습 중 보조신호라는 두 단계 결합을 명확히 제시하여, 복잡한 행동의 단계별 학습을 가능케 한 점이 특징입니다."
  },
  {
    "objectID": "posts/paper/2025-09-05-dapg.html#결론-및-향후-전망",
    "href": "posts/paper/2025-09-05-dapg.html#결론-및-향후-전망",
    "title": "📃DAPG 리뷰",
    "section": "2.4 결론 및 향후 전망",
    "text": "2.4 결론 및 향후 전망\nRajeswaran 등(2018)의 이 논문은 딥 강화학습과 인간 Demo 데이터의 결합이 복잡한 로봇 손 조작 학습에 매우 효과적임을 보여준 사례로서, 이후 많은 관련 연구에 영감을 주었습니다. 저자들은 “DAPG”로 명명된 이 알고리즘이 30배에 달하는 샘플 효율 향상과 정책 품질 개선을 이루었음을 실험으로 입증하였고, 특히 인간 수준의 복잡한 작업들도 RL로 풀 수 있다는 가능성을 열어주었습니다. 이는 로봇 공학 및 강화학습 분야 모두에 의미 있는 성취로, 과거에는 어려움 때문에 시도되지 않던 고차원 다관절 조작 문제를 학습 기반 접근으로 다룰 수 있음을 보여준 것입니다.\n물론 한계도 있습니다. 본 연구는 시뮬레이션에서 수행되었으며, 현실 로봇에 직접 적용하기까지는 여전히 넘어야 할 장애물이 있습니다. 예컨대 실제 로봇 손의 물리적 한계, 센서 노이즈, 충돌 처리, 그리고 무엇보다 실시간 학습에서의 안정성 등이 해결되어야 합니다. 그러나 논문 결과에 따르면 DAPG 알고리즘으로는 5시간 이내의 데이터로도 충분히 학습이 가능하므로, 이를 그대로 실제 로봇에 이식한다면 하루 작업으로 정책을 학습시킬 수도 있을 것입니다. 저자들도 결론에서, 시뮬레이션 결과와 샘플 효율 향상을 바탕으로 실제 복잡한 손 조작 학습에 한 발 다가섰다고 밝히고 있으며, 향후에는 실제 하드웨어 상에서 DAPG를 검증하는 것을 목표로 하고 있습니다.\n이 논문의 발표 이후, 도메인 랜덤화(domain randomization) 등을 통해 시뮬레이터에서 학습한 손 정책을 실제 로봇에 적용한 사례(예: OpenAI의 Rubik’s Cube 해법)나, 진화 전략과 모델 기반 강화학습으로 샘플 효율을 높이는 연구 등 다양한 후속 연구들이 진행되었습니다. 그 중심에는 “어떻게 하면 복잡한 로봇 행동을 현실적으로 학습시킬 것인가?”라는 큰 질문이 있습니다. DAPG는 그 질문에 대해 “우선 인간에게 배워라, 그리고 스스로 향상시켜라”라는 통찰을 준 방법이라 할 수 있습니다. 이는 인간과 로봇의 협력 학습이라는 관점에서도 흥미로운 방향입니다. 앞으로도 Demo학습과 강화학습의 조합은 로봇에게 새로운 능력을 가르치는 강력한 수단으로 계속 연구될 것이며, 본 논문은 그 효과를 극적으로 보여준 선구적인 예시로 오래 회자될 것입니다.\nReference\n\nNPG 논문\nNatural Policy Gradients In Reinforcement Learning Explained\nCMU Material 1 + Material 2"
  },
  {
    "objectID": "posts/paper/2022-10-16-recovery.html",
    "href": "posts/paper/2022-10-16-recovery.html",
    "title": "📃Robust Recovery Controller 리뷰",
    "section": "",
    "text": "이번 포스트는 4족보행 로봇이 전복되었을 때 다시 정상적으로 보행하기 위해 자세를 회복하는 모션 제어(이하 Recovery 혹은 Reset task라고 지칭)을 강화학습 방법으로 해결하고자한 Robust Recovery Controller for a Quadrupedal Robot using Deep Reinforcement Learning라는 논문에 대한 리뷰입니다."
  },
  {
    "objectID": "posts/paper/2022-10-16-recovery.html#mpc-vs-rl",
    "href": "posts/paper/2022-10-16-recovery.html#mpc-vs-rl",
    "title": "📃Robust Recovery Controller 리뷰",
    "section": "MPC vs RL",
    "text": "MPC vs RL\n제어(Control) 분야에서 현재 크게 양대 방법론으로 MPC(Model Predictive Control)과 RL(Reinforcement Learning) 방법론이 주목을 받고 있습니다. 이분법적으로 두 방법이 확연하게 나누어지거나 우열이 가려지는 것이 아니기 때문에 두 방법론 간의 공통점, 차이점등을 살펴보며 시작해보려고 합니다.\n아래 그림의 Optimal Control(최적제어), Operations Research(경영과학), Reinforcement Learning(강화학습)을 각각 하나의 집합으로 표시하고 각 관계를 살펴보면 기존의 제어 방식의 발전 흐름을 이어온 MPC는 Optimal Control에 포함되어 있다고 볼 수 있습니다. Operations Research은 좀 더 넓은 범위의 의사 결정 모델들을 연구하는 분야로 주로 산업공학에서 배우는 부분이며 의사결정에 도움을 받기위해 수학적 모델, 통계학, 알고리즘들을 이용하는 연구분야 입니다. 따라서 Optimal Control, Operations Research와 오늘의 주제 방법론인 Reinforcement Learning과의 관계를 살펴보자면 수학적 모델링을 기반으로 한 Model-based Methods가 Optimal Control과 Reinforcement Learning의 교집합 부분으로 볼 수 있고 최근 강화학습 연구들도 Model-based RL으로 연구들이 많이 이루어지고 있습니다. Operations Research와 Reinforcement Learning의 교집합으로는 Large action-space Method로 볼 수 있는데, 기존의 아타리 게임과 같이 간단한 방향 조작키와 같은 action의 선택지가 적은 경우에 비해 의사결정을 내려야하는 상황이 복잡할 수록 즉, 문제가 더 어렵고 복잡할 수록 action의 선택지가 많아지기 때문에 action space가 더 커지는 방향으로 연구가 많이 이루어지고 있다고 볼 수 있습니다.\n그럼 좀 더 자세히 MPC과 RL을 비교해보면, MPC는 그림과 같이 마치 어둠속에서 손전등을 키고 빛을 따라 하나의 길을 보듯이, 하나의 최적의 trajectory를 찾아 그 trajectory의 1 step을 실행시킵니다. 수학적으로 모델링한 최적식의 해를 구해서 계산과 실행을 반복하며 제어를 하는 것 입니다. 반면 RL은 search space를 MDP(Markov Decision Process)를 이용하여 정의하고 학습을 하며 시점 t에서의 최적의 action을 할 수 있도록 합니다. 두가지 방법 모두 Advanced 시킬 부분이 많이 남아있고 MPC는 non-linear 방향으로 나아가고 있으며 RL은 value-based나 policy-based 기반의 방법들과 함께 최근에는 인공신경망의 gradient기법을 이용하여 발전해나가고 있습니다. MPC와 RL의 비교는 최근 ICRA와 같은 로봇 제어 관련 학회들에서는 관심이 많은 주제이며 Michiel van de Panne (UBC) 교수님의 발표에서 좀 더 자세히 확인하실 수 있습니다."
  },
  {
    "objectID": "posts/paper/2022-10-16-recovery.html#locomotion-vs-recovery",
    "href": "posts/paper/2022-10-16-recovery.html#locomotion-vs-recovery",
    "title": "📃Robust Recovery Controller 리뷰",
    "section": "Locomotion VS Recovery",
    "text": "Locomotion VS Recovery\n이런 강화학습 방법론으로 4족보행의 Recovery Task를 풀기 위해서는 Recovery task에 대한 특성을 좀 더 살펴볼 필요가 있습니다. 기본적으로 족형(Legged) 로봇들은 안정적인 보행(Locomotion)을 기반으로 여러가지 task를 하는데에 초점이 맞춰져 있습니다. 예를 들면 보행로봇의 위에 Manipulator를 달아서 원하는 위치로 보행을 한 뒤 Manipulator로 컵을 집는 일과 같은 task를 수행하는 것입니다. 하지만 4족보행 로봇을 사용하기 위해 전제된 안정적인 보행이 실제로 로봇이 운용되는 환경에서는 여러가지 예측할 수 없는 변수들로 인해 만족될 수 없는 조건일 수 있으며 따라서 로봇이 넘어질 수 밖에 없다면 다시 보행을 할 수 있는 정상적인 상태로 돌아오기 위한 능력도 갖추고 있어야 합니다. 바로 이를 위한 로봇의 기능을 Recovery라고 할 수 있고 Recovery를 Locomotion task와 비교하여 특징 몇가지를 살펴보겠습니다.\n\n우선 Recovery는 앞서 이야기한 것과 같이 Locomotion이 메인이 되는 task인데 반해, 로봇의 운용 life cycle을 위한 support task라고 할 수 있습니다. 로봇이 적용되는 현장에서 메인 task들을 끊임없이 수행하기 위해 흐름을 유지시켜주는 장치라고 볼 수 있습니다. 다음으로 Locomotion은 보행을 할 때 로봇 다리의 움직임이 주기(period)를 가지는 Cyclic한 모션을 하는 것에 반해 Recovery는 넘어진(Fall) 자세로부터 다시 걸을 수 있는 서있는(Stand) 자세로 가기 위한 모션을 취해야 하기 때문에 Non-cyclic하고 case-by-case로 다양한 회복 모션들을 할 수 있어야 합니다. 따라서 Locomotion은 환경에 따라(지면의 마찰력이나 외력 등) 보행의 난이도가 달라지긴 하지만 로봇의 joint의 동작 범위, 즉 action의 search space로 생각했을 때 Recovery에 비해 좀 더 좁은 search space를 가지고 있다고 할 수 있습니다. 마지막으로 Locomotion을 수행할 때 참고할 수 있는 동물이나 사람의 모션 데이터들이 있지만 Recovery 같은 경우 참고할 수 있는 모션 데이터들이 많이 없고 일어나는 방법에 대해 메뉴얼 같이 각각의 단계를 명시하기 까다롭다는 점이 있습니다."
  },
  {
    "objectID": "posts/paper/2022-10-16-recovery.html#overview",
    "href": "posts/paper/2022-10-16-recovery.html#overview",
    "title": "📃Robust Recovery Controller 리뷰",
    "section": "Overview",
    "text": "Overview\n논문에서 제시한 전체적인 Control System의 모습은 아래 그림과 같습니다. 강화학습 알고리즘으로는 TRPO(Trust Region Policy Optimization algorithm)와 GAE(Generalized Advantage Estimator)를 사용했으며 제시된 방법에서는 크게 3가지 특징들이 있습니다.\n\nControl Task Decomposed : Recovery하는 task를 3개로 Behavior들로 나누어서 각각의 Behavior를 수행하는 Control Policy를 학습\nHierarchical Structure : 여러개의 Behavior policies를 조율할 수 있는 상위계층의 Behavior Selector를 만들어서 계층적인 구조를 사용\nHeight Estimator : 로봇을 실제 운용할 때 필요한 상태 정보인 Height 값의 부정확한 정보를 보완하기 위해 Neural Network 사용(Regression model)"
  },
  {
    "objectID": "posts/paper/2022-10-16-recovery.html#behavior-policies",
    "href": "posts/paper/2022-10-16-recovery.html#behavior-policies",
    "title": "📃Robust Recovery Controller 리뷰",
    "section": "Behavior Policies",
    "text": "Behavior Policies\nRecovery task를 여러개의 Behavior들로 나누어서 총 3개의 각각의 task, Self-Righting, Standing up, Locomotion가 있고 각 behavior을 담당하는 policy가 있습니다.\n\nSelf-Righting은 임의의 로봇의 넘어진 자세에서 로봇의 몸체(base)가 똑바로(뒤집어져 있거나 옆으로 돌아가 있지 않은 상태) 되어 있고 4개의 발이 지면에 닿아 있는 자세로 움직이는 것을 말합니다. Standing up은 Self-righting에서 만든 자세에서 4개의 다리들을 이용하여 일어선 자세로 만드는 것을 말합니다. 마지막으로 Locmotion은 Controller에서 주는 command를 기반으로 보행을 하는 것을 말하며 이때 command로는 forward velocity, lateral velocity, turing rate(yaw 방향)을 주어 움직이게 됩니다.\n\n(1)Self-righting behavior\nSelf-righting을 학습하기 위해 policy에 들어가는 data(state 정보)로는 아래의 표에서 볼 수 있듯이 총 6개의 data가 있습니다. 그 중 e_g는 몸체 base의 z 방향으로의 단위 벡터로 몸체의 upright를 판단하기 위한 정보로 볼 수 있습니다.\n\n보통 강화학습에서는 reward를 최대화 하는 방향으로 학습(최적화)이 일어나지만 reward의 반대 개념인 cost의 값이 최소화하는 방향으로 학습을 시켰습니다. 아래의 식처럼 여러개의 cost term들이 있지만 그 중 self-righting에서 특징적인 cost term은 orientation cost term 입니다. 이 cost term은 c_o=\\left\\|[0,0,-1]^T-e_g\\right\\|로 계산되는데 이는 위에서 설명한 대로 policy network가 로봇 몸체의 base의 방향을 uprighting 하도록 학습되게 하기 위한 term이라고 볼 수 있습니다.\n\n(2)Standing up behavior\nStanding up은 이전의 Self-righting의 state 정보를 포함하고 더하여 Base linear velocity 정보까지 포함하여 policy의 input으로 들어가게 됩니다. Self-righting과 Standing up policy 간의 state 정보 포함 관계 뿐만 아니라 이후 소개할 Locomotion policy, Behavor Selector의 state 정보의 집합관계를 보면 다른 policy에 들어가는 정보를 포함하고 추가적인 data를 더하여서 Self-righting → Standing up → Locomotion → Behavior Selector 순으로 state space가 점점 더 커져가는 것을 확인할 수 있습니다.\n\nStaning up behavior policy가 학습해야할 cost 최소화 식은 아래와 같이 여러 cost term들이 있지만 그중 height cost term이 특징적인 cost라고 볼 수 있으며 ANYmal 로봇이 서있을 때의 지면으로부터 몸체(base frame)까지의 거리, height이 0.35m보다 작을시에는 1, 아니면 0으로 계산합니다.\n\n(3)Locomotion behavior\nLocomotion task policy에는 input으로 command까지 들어가게 되면서 cost식에는 command를 잘 수행하는지 판단하도록 하는 angular velocity, linear velocity, foot clearance, foot slippage cost가 들어가는 것을 확인할 수 있습니다.\n\n지금까지 policy network들의 input와 objective(=cost 최소화)에 대해서 이야기 했지만 결과적으로 network가 어떤 값들을 output 하는지, 그리고 실제로 그 output으로 어떻게 로봇을 움직이는지에 대해서는 아직 설명하지 않았습니다. 로봇을 움직이기 위해서는 timestep마다 로봇의 각 모터가 움직여야 하는 desired joint position을 구해서 해당 position으로 모터를 돌려주면 됩니다.\n\n각 Behavior policy에서 나오는 output은 o_t이며 이는 로봇을 구동시키는 12개의 joint motor에 대응하는 실수 벡터입니다. 대응이라고 표현한 이유는 해당 실수값을 바로 joint의 desired position으로 사용하는 것이 아니라 좀 더 빠른 학습 수렴을 위해 desired joint position을 구하는 식을 거쳐 계산된 값을 사용하기 때문입니다. 우선 Self-righting과 Standing up task에서는 네트워크에서 나온 값 o_t를 현재 각 모터의 position인 \\phi_t에 더해주어서 최종 desired joint position인 \\phi_d 값으로 제어합니다. Locomotion에서는 현재의 joint position 대신, 로봇이 서있는 자세인 nominal joint configuration \\phi_n에 o_t을 더해주어 최종 desired joint position인 \\phi_d을 사용합니다.(o_t에 곱해지는 k는 하이퍼파라미터처럼 찾아야 하는 상수값입니다.)"
  },
  {
    "objectID": "posts/paper/2022-10-16-recovery.html#height-estimator",
    "href": "posts/paper/2022-10-16-recovery.html#height-estimator",
    "title": "📃Robust Recovery Controller 리뷰",
    "section": "Height Estimator",
    "text": "Height Estimator\ndata-driven RL에서 policy에 들어가는 data의 quality는 매우 중요합니다. 따라서 실제 작동하는 로봇의 state를 추정하는 State estimation 또한 로봇에 RL을 적용하기 위해서 뗄레야 뗄수 없는 중요한 부분이라고 할 수 있습니다. 로봇에 모션 캡쳐와 같은 센서를 부착하지 않는한 실제 로봇의 base height값을 잘 알 수 없고 로봇이 정상적으로 보행할 때는 TSIF(Two State Implicit Filter)와 같은 State Estimation 기법을 통해 어느정도 추정할 수 있지만 넘어져서 base가 거의 바닥과 가까울 경우 추정값이 매우 불안정하게 됩니다. 따라서 해당 논문에서는 Regression Neural Network를 통해 height를 넘어진 상태에서도 잘 추정할 수 있도록 했습니다.\n\nHeight Estimator Network가 input으로 받는 data는 아래의 표와 같습니다. Output으로는 body의 IMU 값과 12개 joint들의 position을 출력하여 해당 값들을 가지고 forward kinematics를 이용하여 구한 height 값을 네트워크에서 추정한 값으로 사용합니다. 이 신경망은 강화학습으로 학습을 하는 것이 아니라 지도학습 방법으로 true 값을 맞춰가는 과정을 통해 학습하게 되는데 이떄 true data는 시뮬레이션 상에서는 쉽게 구할 수 있고 Regression model의 loss 값은 \\sum_{j=0}^K\\left\\|h_j-h_\\psi\\left(s_j\\right)\\right\\|^2으로 계산됩니다. (j: joint index, s_j: joint state, \\psi: network parameter)"
  },
  {
    "objectID": "posts/paper/2022-10-16-recovery.html#behavior-selector",
    "href": "posts/paper/2022-10-16-recovery.html#behavior-selector",
    "title": "📃Robust Recovery Controller 리뷰",
    "section": "Behavior Selector",
    "text": "Behavior Selector\n여러개의 behavior policy들을 어떻게 조율할 것인지는 상위 계층에 Behavior Selector를 NN을 이용하여 만들어서 학습시킵니다.\n\n로봇의 상황에 맞추어서 적절한 behavoir를 하도록 behavior selector는 categorical distribution을 학습하게 됩니다. Behavior selector와 앞서 설명한 Height estimator는 3개의 behavior policy들이 다 학습이 된 후에 아래 그림의 오른쪽에 보이는 Algorithm1의 흐름에 따라 학습되게 됩니다. Behavior selelctor의 state 나cost는 locomotion과 매우 유사하고 해당 포스트의 appendix에 표로 정리되어 있으며 cost식 같은 경우에도 모터의 power efficiency를 위한 term 정도 추가된 것이기 때문에 해당 cost 식이 궁금하신분은 원문 논문에서 확인하실 수 있습니다."
  },
  {
    "objectID": "posts/paper/2022-10-16-recovery.html#simulating-anymal",
    "href": "posts/paper/2022-10-16-recovery.html#simulating-anymal",
    "title": "📃Robust Recovery Controller 리뷰",
    "section": "Simulating ANYmal",
    "text": "Simulating ANYmal\n로봇에 강화학습을 적용할 때 큰 이슈들 중 하나는 Sim-to-Real입니다. 시뮬레이션으로 실제 환경을 단순화하고 모사한 것이기 떄문에 시뮬레이션에서 잘 학습이되고 잘 동작하더라도 실제 로봇을 deploy했을때 잘 작동하지 않는 문제가 생깁니다. 따라서 실제 환경에서도 로봇이 경험하게 되는 noise들을 시뮬레이션에도 random하게 적용시켜 최대한 실제 상황과도 유사하게 만든 환경에서 학습을 하게 됩니다.\n해당 논문에서도 Link length, Intertial property, Link mass, CoM(Center of Mass), Collision geometry, Coefficient of friction 등과 같이 물리적인 값들을 아래 표에서 볼 수 있듯이 일정 범위에서 random하게 값을 넣어주었고 policy의 input data가 되는 observation 값들에도 noise 값을 추가하여 Sim-to-Real 문제를 해결하였습니다. 이외에도 ANYmal 로봇 플랫폼에서 사용하는 SEA motor에 스프링과 같은 기계적 요소들의 변수로 인한 제어 이슈도 해결하기 위해 Actuator Network를 학습시켜 이를 해결하였습니다.\n\n(위 요약에서 언급했던 대로 사실상 Actuator Network나 Height Estimator까지 NN이 추가되므로 4개가 아닌 총 6개의 NN이 사용되었음을 알 수 있습니다.)"
  },
  {
    "objectID": "posts/paper/2022-10-16-recovery.html#the-success-of-recovery",
    "href": "posts/paper/2022-10-16-recovery.html#the-success-of-recovery",
    "title": "📃Robust Recovery Controller 리뷰",
    "section": "The success of recovery",
    "text": "The success of recovery\nRecovery 실험은 총 2가지로 진행되었는데 첫번째는 지면에 로봇을 임의의 position으로 넘어진 상황을 연출한 후 5초 이내로 일어날 수 있는지를 확인했고 두번째로는 로봇이 걷고 있을 때 발로 쳐서 넘어뜨린후 로봇이 다시 자연스럽게(각 behavior들 간의 switching이 자연스럽게) 일어나는지를 확인했습니다. 각각의 실험 모두 50번 이상씩 진행했으며이때 약 100번중 97번을 성공하여 97% 성공률을 보였습니다. (실패한 케이스들의 경우에는 joint의 position이 2\\pi를 넘어가는 값으로 나올 때 잘 작동하지 않았다고 합니다.)"
  },
  {
    "objectID": "posts/paper/2022-10-16-recovery.html#the-effectiveness-of-height-estimator",
    "href": "posts/paper/2022-10-16-recovery.html#the-effectiveness-of-height-estimator",
    "title": "📃Robust Recovery Controller 리뷰",
    "section": "The effectiveness of Height Estimator",
    "text": "The effectiveness of Height Estimator\n기존의 State Estimator TSIF(Two State Implicit Filter)만을 사용했을 때는 로봇이 바닥에 넘어져 있을 경우 error 값이 매우컸지마나 Neural Network를 통해 보정했을 때 오차가 1cm 미만으로 떨어지는 것을 확인할 수 있었습니다. 오른쪽의 height 그래프는 맨 아래 캡쳐되어있는 로봇의 일련의 모션과정 중에 height를 그래프로 plotting한 것인데 simulation(초록색)이 true값이며, TSIF만을 사용했을 때(파란색)는 로봇이 넘어져있을 때 오차가 큰데 반해 neural network(주황색)은 simulation data와 거의 같음을 확인할 수 있습니다."
  },
  {
    "objectID": "posts/paper/2022-10-16-recovery.html#the-competitiveness-of-beahavior-selector",
    "href": "posts/paper/2022-10-16-recovery.html#the-competitiveness-of-beahavior-selector",
    "title": "📃Robust Recovery Controller 리뷰",
    "section": "The competitiveness of Beahavior Selector",
    "text": "The competitiveness of Beahavior Selector\n기존의 제어방법들에서는 여러가지 mode를 조율하기 위해서 FSM(Finite State Machine)을 많이 사용합니다. 특정 조건을 if에 넣어주어서 각 mode를 transition하는 기법인데 논문에서도 강화학습으로 학습한 Behavior Selector의 비교를 위해 기존의 FSM 방식을 활용하여 State Machine을 만들어서 비교했습니다. FSM 방식은 여전히 corner case들이 Behavior Selector에 비해 많이 존재했고 각 behavior들 간의 전환도 자연스럽지 않았습니다.\n\nReview\n\n아직 minor 한 주제인 Recovery task에 대해 집중적으로 잘 분석하고 성과도 확실히 보여준 논문이라고 생각합니다. Control system을 working하게 하기 위해 각 파트들을 어떻게 설계하고 학습해야할 지 많은 고민을 했다는 것을 느낄 수 있었습니다. 여전히 flat ground에서만 진행된 연구이기에 slope가 있는 환경이나 다른 object가 있는 좀 더 실제 상황과 비슷한 상황에 대한 recovery가 해결되기 위해서는 연구되어야 할 부분이 충분히 많은 것 같습니다.\n\n\n\nAppendix\n\n\n\nReference\n\norginal paper : https://arxiv.org/abs/1901.07517\nhttps://youtu.be/veXcohbFxKQ"
  },
  {
    "objectID": "posts/paper/2022-06-26-keep-learning.html",
    "href": "posts/paper/2022-06-26-keep-learning.html",
    "title": "📃Legged Robots that Keep on Learning 리뷰",
    "section": "",
    "text": "Legged robots are physically capable of traversing a wide range of challenging environments but designing controllers that are sufficiently robust to handle this diversity has been a long-standing challenge in robotics. Reinforcement learning presents an appealing approach for automating the controller design process and has been able to produce remarkably robust controllers when trained in a suitable range of environments. However, it is difficult to predict all likely conditions the robot will encounter during deployment and enumerate them at training-time. What if instead of training controllers that are robust enough to handle any eventuality, we enable the robot to continually learn in any setting it finds itself in? This kind of real-world reinforcement learning poses a number of challenges, including efficiency, safety, and autonomy. To address these challenges, we propose a practical robot reinforcement learning system for fine-tuning locomotion policies in the real world. We demonstrate that a modest amount of real-world training can substantially improve performance during deployment, and this enables a real A1 quadrupedal robot to autonomously fine-tune multiple locomotion skills in a range of environments, including an outdoor lawn and a variety of indoor terrains."
  },
  {
    "objectID": "posts/paper/2022-06-26-keep-learning.html#system-process",
    "href": "posts/paper/2022-06-26-keep-learning.html#system-process",
    "title": "📃Legged Robots that Keep on Learning 리뷰",
    "section": "System Process",
    "text": "System Process\n\n\n\n\n\n\n위의 사진에 보이는 공원과 같은 새로운 환경에서 먼저 로봇 agent가 첫번째 시도로 locomotion task를 진행한다.\n만약에 땅이 고르지 못해서 agent의 학습된 policy를 활용할 수 없는 상황이 되어서 넘어지게 되는 상황이 될 수 도 있다.\n이때 reset controller를 이용해서 빠르게 다시 일어난다.\n실제 task에서 좀 더 몇 번 시도를 하면서 1~3의 과정을 몇 번 반복하게 되고 이 과정에서 policy가 업데이트 되게 된다.\n업데이트가 되면서 policy는 새로운 test 환경에서 제대로 작동할 수 있게 된다."
  },
  {
    "objectID": "posts/paper/2022-06-26-keep-learning.html#how",
    "href": "posts/paper/2022-06-26-keep-learning.html#how",
    "title": "📃Legged Robots that Keep on Learning 리뷰",
    "section": "How",
    "text": "How\n\n강화학습의 reward 가 robot의 on-board 센서로 측정되는 값들로만 디자인 되어야 실제 Real-world에서 작동하면서 fine tuning을 할 수 있다.\nAgile한 behavior를 학습하기 위해서 Motion imitation 기법을 활용했다.\n로봇의 넘어지고 나서 빠르게 정상자세로 회복할 수 있도록 Recovery policy를 학습했다.\n강화학습 알고리즘들 중에서 REDQ(Randomized Ensembled Double Q-Learning) 라는 알고리즘을 사용했는데, 이 알고리즘은 여러개 Q-network들의 앙상블을 통해 randomization을 해서 Q-learning 계열의 알고리즘들의 sample-efficiency와 안정성을 향상시킨 알고리즘이다."
  },
  {
    "objectID": "posts/paper/2022-06-26-keep-learning.html#main-contribution",
    "href": "posts/paper/2022-06-26-keep-learning.html#main-contribution",
    "title": "📃Legged Robots that Keep on Learning 리뷰",
    "section": "Main Contribution",
    "text": "Main Contribution\n\n본 논문의 주요 contribution은 다음과 같다.\n\n\n4족 보행 로봇의 agile한 locomotion skill을 real-world에서 학습하기 위한 fine-tuning 자동화 시스템을 제안하였다.\n처음으로 자동화 reset과 on-board 상태 추정을 통해 real-world에서 fine-tuning이 될 수 있음으로 보였다.\nA1 로봇을 가지고 dynamic skill들을 학습해서 외부 잔디에서 앞으로, 뒤로 pacing을 하고 3가지 다른 지형 특징을 가진 환경에서 side-stepping을 할 수 있었다."
  },
  {
    "objectID": "posts/paper/2022-06-26-keep-learning.html#details-with-hash-tags",
    "href": "posts/paper/2022-06-26-keep-learning.html#details-with-hash-tags",
    "title": "📃Legged Robots that Keep on Learning 리뷰",
    "section": "Details with Hash tags",
    "text": "Details with Hash tags\n\n원 논문의 II. Related Work section 참고\n\n#Cumbersome controller designs\n\n이전의 로봇 controller들은 footstep planning, trajectory optimization, model-predictive control (MPC) 등의 조합으로 만들어지고 있었다. 그러나 이런 방법들은 로봇의 동역학과 각 로봇마다 다르고 각 skill마다 다른 많은 요소들을 고려해야 하기 때문에 정말 어려웠다.\n\n#Sim2Real\n\ntrial-and-error라는 데이터에 매우 의존성이 높은 강화학습 알고리즘의 특성과 하드웨어의 safety 이슈 때문에 보통 로봇 강화학습 agent는 시뮬레이션 기반으로 학습된다. 하지만 시뮬레이션에서 학습하면서 실제로 만나보지 않은 real-world의 모든 조건들을 예상하고 학습하기란 사실상 불가능하며 가장 robust한 policy라고 할지라도 모든 상황에 대해 generalization 되었다고 할 수 없다.\n\n#Real-world\n\n이전에 복잡한 motion들을 학습하게 하기 위해서 environment의 다양한 장치들로 다양한 상태 정보를 만들어서 사용했지만 본 연구에서는 real-world에서 작동하고 있는 로봇에서 fine-tuning을 해야 하기 때문에 로봇의 on-board에서 받을 수 있는 모든 state estimation 정보들을 가지고만 진행했으며 motion capture나 외부 장치들을 별도로 사용하지 않았다.\nscratch부터 실제 환경에서 단순한 구조의 로봇들로 walking gaits들을 학습하는게 아니라, A1 로봇으로 pacing, side stepping 등 매우 자연스럽고 조금은 불안정하고 세밀한 balancing이 요구되는 skill들을 학습할 수 있었다. (기존의 연구들은 balancing에 매우 신경쓴 나머지 느리고 부자연스러운 walking gaits 에 치중한 면이 있었다.) 본 논문의 연구에서 motion imitation과 실제 환경에서의 fine-tuning 이 이런 다이나믹한 task들을 성공시키는데 매우 중요한 역할을 했다. 또한 실제 환경에서 로봇이 작동하면서 넘어질 때, manual하게 로봇의 reset하거나 recovery시키지 않고 강화학습으로 자동적으로 reset 할 수 있는 controller를 만들어서 사용했다.\n\n#Few-shot adaptation\n\n기존의 Adaptation structure라는 구조를 만들어서 학습시켜서 latent 또는 explicit한 환경에 대한 descriptor로 adaptive한 policy를 만드는 연구들이 있었으나, 이 기법들 또한 결국 training에서 경험했던 것들을 기반으로 adaptive함을 보이는 것이므로 실제 test 환경이 이 허용 범위에서 많이 벗어날 경우 제대로 작동안되는 것은 똑같다. 따라서 강화학습으로 지속적인 적응적인 학습능력을 보장해서 어떤 test 환경에서든 잘 작동할 수 있도록 했다.\n\n#RL Algorithm\n\n강화학습 알고리즘으로는 기존의 vision 기반 매니퓰레이터들에서 grasping 작업을 하는 task들에서 많이 쓰인 off-policy model-free RL 기법들을 참고하여 fixed되어 있는 매니퓰레이터들보다 더 challenging한 floating-based 보행 로봇의 locomotion에 적용해서 성공시켰다."
  },
  {
    "objectID": "posts/paper/2022-06-26-keep-learning.html#overview",
    "href": "posts/paper/2022-06-26-keep-learning.html#overview",
    "title": "📃Legged Robots that Keep on Learning 리뷰",
    "section": "Overview",
    "text": "Overview\n아래 사진의 전체 시스템의 개략도에서 볼 수 있듯이 각각의 policy는 하나의 desired skill을 학습하게 된다. 즉 하나의 policy는 forward를, 다른 policy는 backward를, 마지막 다른 policy는 reset을 담당하여 학습하게 된다. 이렇게 다양한 task를 수행할 수 있도록 만든 프레임워크 이기 때문에 Multitask framework인 것이다.\n\n\n\n\n\n\nPseudo Algorithm\n시스템 개략도에서 봤듯이 논문에 나와있는 시스템 전체를 보여주는 Algorithm2 알고리즘은 크게 2개의 과정으로 진행된다.\n\n\n\n\n\n\nAgent의 policy는 시뮬레이션에서 pretrained 한다. (Algorithm 2 line 2~7)\n\n각 에피소드가 끝날 때마다 학습된 recovery policy가 로봇을 다음 rollout을 할 수 있도록 준비시켜준다.\n각 skill을 위한 policy들은 독립적으로 학습되고 recovery policy도 마찬가지로 독립적으로 학습된다.\n\nFine-tuning을 실제 물리적인 환경에서 진행하면서 training process를 계속 이어나갈 수 있다. (Algorithm 2 line 8~14)\n\n시뮬레이션과 실제 환경의 차이를 고려하여 각 policy들의 replay buffer는 초기화 시켜준다.(Algorithm 2 line 12)"
  },
  {
    "objectID": "posts/paper/2022-06-26-keep-learning.html#motion-imitation-off-policy-rl",
    "href": "posts/paper/2022-06-26-keep-learning.html#motion-imitation-off-policy-rl",
    "title": "📃Legged Robots that Keep on Learning 리뷰",
    "section": "Motion Imitation & Off-policy RL",
    "text": "Motion Imitation & Off-policy RL\n\n\n\n\n\nMotion Imitation\nMotion Imiation 방법을 이용하여 reference motion clip들의 skill들을 모방 학습하도록 했는데 이는 Learning Agile Robotic Locomotion Skills by Imitating Animals라는 논문에서 제시한 방법을 따라했다. (Algorithm 1 line1~4)\nReference motion M이 주어지면 agent의일련의 pose들과 비교하여 section III-B에서 소개될 reward function을 기반으로 학습한다. - 이 방법을 통해 reference motion data만 바꿔주면 바로 다른 여러 skill들을 배울 수 있다. - recovery policy를 학습하기 위해서 standing pose를 모방하도록 할 수 있다.(III-C 참고)\nOff-policy RL\noff-policy 알고리즘인 REDQ algorithm 사용했다.(Algorithm 1 line5~9) - SAC 알고리즘을 더 발전시킨 알고리즘 - time step에 대한 gradient step비율을 증가시켜서 강화학습 알고리즘의sample efficiency를 높였다. - 너무 많은 gradient step을 할 경우에 일어날 수 있는 overestimation issue를 앙상블 기법을 이용해서 완화할 수 있었다."
  },
  {
    "objectID": "posts/paper/2022-06-26-keep-learning.html#a.-state-action-spaces",
    "href": "posts/paper/2022-06-26-keep-learning.html#a.-state-action-spaces",
    "title": "📃Legged Robots that Keep on Learning 리뷰",
    "section": "A. State & Action Spaces",
    "text": "A. State & Action Spaces\n\nState space\n\nState는 연속적인 3 timesteps에서 얻은 아래 정보들로 정의했다.\n\nRoot orientation (read from the IMU)\nJoint angles\nPrevious actions\n\nPolicy는 위에서 말한 Proprioceptive input 뿐만 아니라 a goal g_t에 대한 정보도 input으로 받게 된다.\n\ng_t는 future timesteps에서의 reference motion에서 계산된 Target pose (root position, root rotation, joint angles)의 정보를 포함한다.\n4 future target poses 는 현재 timestep에서 약 1초 정도 이후의 pose들이다.\n\n\nAction space\n\nAction은 12 joints들에 대한 PD position targets 이다.\n33Hz의 주파수로 command가 적용된다.\n자연스러운 움직임을 위해 PD targets을 low-pass filter를 로봇에 적용하기 전에 통과시켜준다."
  },
  {
    "objectID": "posts/paper/2022-06-26-keep-learning.html#b.-reward-function",
    "href": "posts/paper/2022-06-26-keep-learning.html#b.-reward-function",
    "title": "📃Legged Robots that Keep on Learning 리뷰",
    "section": "B. Reward Function",
    "text": "B. Reward Function\n\n\\begin{gathered}r_{t}=w^{\\mathrm{p}} r_{t}^{\\mathrm{p}}+w^{\\mathrm{v}} r_{t}^{\\mathrm{v}}+w^{\\mathrm{e}} r_{t}^{\\mathrm{e}}+w^{\\mathrm{rp}} r_{t}^{\\mathrm{rp}}+w^{\\mathrm{rv}} r_{t}^{\\mathrm{rv}} \\\\w^{\\mathrm{p}}=0.5, w^{\\mathrm{v}}=0.05, w^{\\mathrm{e}}=0.2, w^{\\mathrm{rp}}=0.15, w^{\\mathrm{rv}}=0.1\\end{gathered}\n\n\n\nr_{t}^{\\mathrm{p}} : 로봇의 joint rotation 값들을 reference motion의 joint rotation과 맞추도록 하는 reward term\n\n  r_{t}^{\\mathrm{p}}=\\exp \\left[-5 \\sum_{j}\\left\\|\\hat{q}_{t}^{j}-q_{t}^{j}\\right\\|^{2}\\right]\n  \n\n\\hat{q}_{t}^{j} : 시점 t에 reference motion의 j번째 joint의 local rotation\nq_{t}^{j} : 로봇의 j번째 joint local rotation\n\nr_{t}^{\\mathrm{v}} : joint velocities\nr_{t}^{\\mathrm{e}} : end-effector positions\n로봇이 reference root motion을 잘 tracking 하게 하기 위한 reward term\n\nr_{t}^{\\mathrm{rp}} : root pose reward\nr_{t}^{\\mathrm{rv}} : root velocity reward\n\n\n\n이전부터 강조해왔듯이, 실제 환경에서 fine-tuning과정을 진행하기 위해서 on-board 센서들의 값을 이용해서 reward function을 디자인하였고 실제 물리적인 환경에서 구동할 때 이를 상태 추정 기법을 이용해서 reward를 구하게 된다. 따라서 아래의 상태 추정 방법(State Estimation)이 fine-tuning의 성능을 결정하는 중요한 부분이 된다.\n\nReal-world에서 로봇의 linear root velocity를 잘 추정하기 위해서 Kalman filter를 사용했다.\n\n칼만 필터는 IMU 센서에서 acceleration과 orientation 값들을 읽어서 foot contact sensors로 값들을 보정한다.\n처음에 발 끝의 속도를 0으로 생각해서 각 다리의 joint velocities를 고려하여 몸체의 속도를 계산하고 IMU으로부터 추정했던 값을 보정한다.\n\n이렇게 계산된 linear velocity를 로봇의 position 추정값에 통합시킨다.\n\n\n\n\n\n\n위의 그래프들에 볼 수 있듯이(아래에서 위 방향으로),\n\nangular velocity와 orientation 센서 값들은 매우 정확했다.\nlinear velocity는 매우 정확하진 않았지만 허용가능했다.(reasonable)\nposition drifts는 상당히 벗어나는 부분이 있었지만, 각 에피소드에서 reward function을 계산할 정도로의 적합한 값들을 보여주었다."
  },
  {
    "objectID": "posts/paper/2022-06-26-keep-learning.html#c.-reset-controller",
    "href": "posts/paper/2022-06-26-keep-learning.html#c.-reset-controller",
    "title": "📃Legged Robots that Keep on Learning 리뷰",
    "section": "C. Reset Controller",
    "text": "C. Reset Controller\n\nreset policy를 시뮬레이션에서 학습하기 위해 다양한 initial states에서 시작하도록 했다.\n\n→ 로봇을 random한 height & orientation에서 떨어뜨려서 아래 사진에서 볼 수 있듯이 다양한 initial states를 설정\n\n\n\n\n\n\nMotion imitation 목적함수를 수정해서 single, streamlined reset policy를 학습시켰다.\nReference motion을 가지고 로봇이 정확히 어떻게 일어나야 할지를 알려주는 것이 아니라, 아래와 같은 방법으로 reset policy를 학습시켰다.\n\n\npolicy가 rolling right side up을 위한 reward만을 가지고 학습한다.\n만약 로봇이 upright하는데 성공하면 이후에 motion imitation reward를 추가시켜서 학습니다.\n\n이때의 reference motion은 standing pose가 되고 로봇이 똑바로 설 수 있도록 학습시킨다.\n\n\n\n이런 방식으로 학습된 reset policy는 다양한 test 지형에서 fine-tuning 없이도 잘 동작했다.(tranfered well)"
  },
  {
    "objectID": "posts/paper/2022-06-26-keep-learning.html#a.-simulation-experiments",
    "href": "posts/paper/2022-06-26-keep-learning.html#a.-simulation-experiments",
    "title": "📃Legged Robots that Keep on Learning 리뷰",
    "section": "A. Simulation Experiments",
    "text": "A. Simulation Experiments\n\nagent의 policy를 먼저 특정 시뮬레이션 셋팅에서 학습시킨 후에 학습된 시뮬레이션과 또 다른 시뮬레이션 환경 셋팅에 “deployed”한 후 결과를 살펴보았다.\nLearned forward pacing gait가 테스트 환경들에서 얼마나 빨리 적용되는지 확인해보았다.\nStandard dynamics randomization (mass, inertia, motor strength, friction, latency 변동)으로 Pre-train을 flat ground에서 진행했다.\n\nThe test terrains\ntest 환경들로는 총 3가지로 실험하였으며 pre-training 과정의 시뮬레이션 셋팅과 유사한 test 환경 [1]과 pre-training 과정의 시뮬레이션 셋팅과 다소 다른 test 환경 [2], [3]에서 진행됐다.\n\na flat ground\nrandomized heightfield : 랜덤하게 지형의 높이를 설정한 울퉁불퉁한 지형\na low friction surface : 낮은 마찰계수를 가지는 지형, 빙판길과 같은 미끄러운 지형(Training 과정에서 경험한 마찰계수 분포와 한참 동떨어진 마찰계수를 가지고 있음)\n\n비교군\n\nlatent space : 호율적인 다양한 dynamics parameters에 대한 학습을 하기 위해 latent space에 표현된 behaviors을 학습\nRMA: dynamics randomization한 모델. 위에서 언급한 Adaptation Module을 가지고 학습\nVanilla SAC : Soft Actor-Critic 알고리즘으로 학습\nOurs(REDQ): 10개의 Q-functions을 가지고 randomly sample 2로 학습\n\n\n\n\n\n\n\n실험 결과를 살펴보면, RMA는 training 환경에서만 높은 성능을 보여주어 Adaptation Module의 한계점을 명확히 보여주었다. SAC에 비해서 REDQ(Ours)가 sample efficiency가 좋을 뿐만 아니라 수렴하는 Return 값도 높았다."
  },
  {
    "objectID": "posts/paper/2022-06-26-keep-learning.html#b.-real-world-experiments",
    "href": "posts/paper/2022-06-26-keep-learning.html#b.-real-world-experiments",
    "title": "📃Legged Robots that Keep on Learning 리뷰",
    "section": "B. Real-World Experiments",
    "text": "B. Real-World Experiments\n시뮬레이션에서 학습된 Agent를 4개의 real-world 환경(Outdoor 1개, Indoor 3개)에서 test 했다. 모든 (real-world) test 지형 실험은 시뮬레이션의 flat ground에서 pre-training된 agent로 실험한 것이었으며, 처음에 buffer를 5000 samples로 초기화 해주고 시작한 다음 test real world 환경에서 policy를 fine-tuning 해주었다.\n\nOutdoor grassy lawn:\n\nslippery surface를 가지고 있어서 발이 잔디에서 미끄러지거나 흙에 빠질 수 있다.\n앞 혹은 뒤로 움직이는 pacing gait를 fine-tuning 하도록 했다.(pacing gait: 좌나 우의 2개의 다리가 한번에 움직이는 걸음새)\nPre-trained forward pacing policy는 매우 조금만 앞으로 갈 수 있었고, pre-trained backward pacing policy는 잘 넘어지는 경향이 있었다.\n작동한 지 약 2시간 만에, 로봇은 (아주 조금의 넘어짐은 있었지만) 지속적이고 안정적으로 앞 혹은 뒤로 pacing gait를 할 수 있었다.\n\n\n\n\n\n\nIndoor\n\nCarpeted room: 높은 마찰계수를 가지는 지형으로 (카펫이 푹신하므로) 로봇의 고무로 마감되어 있는 발이 시뮬레이션에서 학습된 것과 다르게 안정적이지 않은 컨택을 하게 될 수 있다.\n\n\n\n\n\n\n\nDoormat with crevices: 매트 표면에 발이 빠질 수도 있는 환경이다.\n\n\n\n\n\n\n\nMemory foam: 4cm 정도의 두께의 메모리폼으로 발이 매트리스에 빠지고 평평하고 딱딱한 바닥과 비교했을 때 이 환경에서는 gait(걸음새)가 상당히 변화가 많이 일어날 수 있다.\n\n\n\n\n\n\n\nIndoors에서는, pre-trained side stepping policy가 움직일 때 매우 불안정했고 motion을 끝내기 전에 넘어졌다.\n그러나 각 지형 셋팅에서 2.5 시간 이내로 로봇이 비틀거림 없이 skill을 수행할 수 있었다."
  },
  {
    "objectID": "posts/paper/2022-06-26-keep-learning.html#c.-semi-autonomous-training",
    "href": "posts/paper/2022-06-26-keep-learning.html#c.-semi-autonomous-training",
    "title": "📃Legged Robots that Keep on Learning 리뷰",
    "section": "C. Semi-autonomous training",
    "text": "C. Semi-autonomous training\n\n\n\n\n\n\n전반적인 모든 실험들에서, the recovery policy는 100% 성공적이었다.\n본 논문에서 제시된 방법으로 학습된 reset controller와 Unitree에서 제공한 built-in rollover controller를 비교해보았다.\n\nOn hard surfaces : 두 가지 controllers 모두 효과적으로 잘 작동했지만 built-in 컨트롤러는 learned policy에 비해 상당히 느렸다.\nOn the memory foam : built-in 컨트롤러는 더 성능이 좋지 못했다."
  },
  {
    "objectID": "posts/paper/2025-09-12-crossdex.html",
    "href": "posts/paper/2025-09-12-crossdex.html",
    "title": "📃CrossDex 리뷰",
    "section": "",
    "text": "Paper Link\nHomepage\nCode Link"
  },
  {
    "objectID": "posts/paper/2025-09-12-crossdex.html#배경과-문제-정의",
    "href": "posts/paper/2025-09-12-crossdex.html#배경과-문제-정의",
    "title": "📃CrossDex 리뷰",
    "section": "1. 배경과 문제 정의",
    "text": "1. 배경과 문제 정의\n다지 로봇 손을 이용한 물체 파지(grasping)는 로봇이 현실 세계의 물체와 상호작용하기 위한 핵심 기술로 오래전부터 연구되어 왔습니다. 기존 연구들은 주로 특정 로봇 손에 한정된 파지 정책 학습에 집중해 왔지만, 서로 다른 형태의 로봇 손들에 공통적으로 적용될 수 있는 범용 정책에 대한 연구는 거의 이루어지지 않았습니다. 예를 들어, 5손가락에 22자유도를 가진 ShadowHand와 4손가락에 16자유도인 LEAP Hand를 생각해 보면, 두 손의 형태 차이로 인해 한 손의 동작 제어 명령을 다른 손에 직접 적용하기가 매우 어렵습니다. 손가락 수와 관절구조, 가동 범위가 다르기 때문에 행동 공간(제어 명령의 형태)을 통일하기가 어렵고, 물체와 접촉하는 방식도 달라 단일 정책으로 다양한 손을 제어하는 것은 큰 도전 과제입니다.\n이러한 문제의식 아래, Cross-Embodiment Dexterous Grasping(이하 CrossDex) 논문은 다양한 로봇 손 체형에 통용되는 단일 강화학습 정책을 개발하고자 합니다. 각 로봇 손마다 별도의 정책을 학습하는 대신, 공유되는 구조적 특징을 활용해 여러 손에 걸쳐 일반화할 수 있는 파지 기술을 학습시키는 것이 목표입니다. 이는 새로운 로봇 손이 투입될 때 일일이 초기부터 학습해야 하는 비용을 줄이고, 이미 학습된 정책을 빠르게 이전하거나 제로샷(Zero-shot)으로 적용하는 기반을 마련한다는 점에서 실용적인 의미가 큽니다. 요약하면, 이 논문은 “다양한 로봇 손에 대해 하나의 뇌(정책)로 물체 잡기를 잘 해보자”라는 문제를 정의하고 있습니다."
  },
  {
    "objectID": "posts/paper/2025-09-12-crossdex.html#제안-방법의-핵심-기술-내용",
    "href": "posts/paper/2025-09-12-crossdex.html#제안-방법의-핵심-기술-내용",
    "title": "📃CrossDex 리뷰",
    "section": "2. 제안 방법의 핵심 기술 내용",
    "text": "2. 제안 방법의 핵심 기술 내용\nCrossDex의 핵심 아이디어는 인간 손 동작 공간을 매개체로 로봇 손들의 행동과 관측 공간을 통일하는 것입니다. 저자들은 사람이 여러 로봇 손을 원격 조작(teleoperation)할 때, 자신의 손 동작을 통해 직관적으로 로봇 손을 제어할 수 있다는 점에 주목했습니다. 이를 본뜬 “휴먼-라이크 (human-like) 정책”을 도입하여, 정책의 행동 출력이 곧 인간 손의 자세로 표현되도록 만들었습니다. 구체적으로, 인간 손의 다양한 자세를 주성분분석(PCA)을 통해 얻은 eigengrasp(고유 그립 모드)를 통합 행동 공간으로 채택합니다. MANO라는 인간 손 모델의 45차원 관절 각도 표현을 저차원(e.g. 수 개에서 수십 개 차원) eigengrasp 벡터로 압축하여 사용하며, 하나의 정책이 출력하는 이 eigengrasp 벡터가 어떤 로봇 손을 제어할 때든 공통된 의미를 갖도록 합니다.\n다음 단계는 이렇게 나온 인간 손 형태의 동작 명령을 각 로봇 손의 개별 관절 명령으로 변환하는 것입니다. 이를 리타게팅(retargeting)이라고 하며, 본 연구에서는 주로 DexPilot 알고리즘을 기반으로 하되, 초기에는 최적화 기반의 리타게팅을 활용하고 이후엔 이를 대신할 신경망 매핑을 학습시켰습니다. 리타게팅은 인간 손 모드의 움직임을 개별 로봇 손의 관절값(각 손의 PD 제어기에 넘겨줄 목표 각도)으로 변환하는 역할을 합니다. 예를 들어, 정책이 “손을 쥐는” 방향의 eigengrasp 값을 출력하면, 5손가락 ShadowHand든 4손가락 Allegro Hand든 해당 손가락들이 모두 오므려져 물체를 잡는 방향으로 관절이 움직이도록 매핑됩니다. 저자들은 이 매핑을 빠르게 수행하기 위해 각 로봇 손별로 4층 MLP 신경망을 학습시켜 최적화 과정을 대체하였고, 이를 통해 대량의 병렬 강화학습 환경에서도 속도 병목 없이 동작 변환을 할 수 있게 했습니다.\n한편, 관측 공간의 통합도 중요한 요소입니다. 서로 다른 로봇 손의 자기 수용감각(proprioception) 정보, 즉 관절 각도들은 종류마다 차원이 다르고 의미도 달라 직접 비교하거나 하나의 정책 입력으로 합치는 데 무리가 있습니다. CrossDex에서는 손바닥과 손가락 끝 포인트의 3차원 위치 정보만 관측에 사용함으로써 이 문제를 풀었습니다. 로봇 손의 구체적인 관절각 대신, 각 손의 손바닥 중심과 다섯 손가락(또는 네 손가락) 끝점들의 위치를 취하면, 비록 손마다 구조가 달라도 “손가락 끝이 어디 있는가”라는 공통 표현으로 정규화할 수 있다는 것입니다. 이는 해당 위치들이 파지에서 물체와의 접촉 및 배치에 핵심적인 역할을 하기 때문이며, Handa 등(2020)의 연구에서도 인간 손가락 끝 위치의 중요성이 강조된 바 있습니다. 요약하면, 행동 공간은 “인간 손 고유 동작 모드”로, 관측 공간은 “손바닥과 손가락 끝의 위치”로 통합하여 로봇 손 종류에 구애받지 않는 정책 입력/출력 구조를 설계한 것이 CrossDex 방법의 핵심입니다.\n이렇게 통합된 관측·행동 표현을 가지고, 강화학습 정책 학습은 크게 두 단계로 진행되었습니다. (i) 우선 teacher-student 전략을 활용하여, 상태기반(state-based) 교사 정책들을 개별 물체 대상 파지에 대해 학습시킵니다. 여기서 상태기반이란 물체의 정확한 위치와 형상, 로봇 손의 상태 등을 완전 관측한 조건에서 학습한다는 뜻입니다. 각 물체마다 PPO 알고리즘으로 최적 정책을 찾고, 이를 각 물체의 파지 전문가(교사)로 간주합니다. (ii) 이후 이러한 다수의 교사들을 이용해 비전기반(vision-based) 학생 정책을 학습시키는데, 여기에는 DAgger(Dataset Aggregation) 알고리즘을 사용했습니다. 비전 정책은 물체의 3D 포인트클라우드(시뮬레이션 상의 depth sensor 데이터)를 입력으로 받아 동작을 결정하며, 초기에는 교사 정책들의 시연을 모방하고 점진적으로 자기 정책으로 데이터를 확장해가는 방식으로 학습됩니다. 이 비전 정책이 최종적으로 모든 물체와 모든 로봇 손을 한꺼번에 다루는 범용 파지 정책이 됩니다. 학습에는 NVIDIA Isaac Gym 시뮬레이터를 활용하여 총 8192개의 병렬 환경에서 대량의 경험을 모았고, 이후 시뮬레이션에 16,384개 환경을 활용하여 비전 정책으로 지식을 이행(distill)했습니다. 이러한 대규모 병렬 학습 세팅 덕분에 수십 가지 물체와 여러 손에 대한 복잡한 정책을 현실적인 시간 안에 얻을 수 있었습니다. (참고로 정책 신경망 구조는 상태기반의 경우 5개 레이어 MLP, 비전 정책의 경우 PointNet 기반의 간소화된 구조로 물체 점군을 처리한 후 MLP로 액터/크리틱을 구성하였습니다.)\n마지막으로, 저자들은 이렇게 학습된 정책을 새로운 손이나 새로운 객체에 빠르게 적응시키는 파인튜닝(fine-tuning) 기법도 제안합니다. 사전학습된 CrossDex 정책을 초기화 값으로 활용하여 PPO 재학습을 할 때, 기존 정책과의 KL 발산 페널티를 추가로 줘서 급격한 변화(포겟팅)를 막는 형태로 미세조정을 수행하였습니다. 이를 통해 학습되지 않은 새로운 손이나 새로운 물체 세트에 대한 추가 학습 효율을 높일 수 있음을 보였습니다."
  },
  {
    "objectID": "posts/paper/2025-09-12-crossdex.html#기존-연구-대비-주요-기여점",
    "href": "posts/paper/2025-09-12-crossdex.html#기존-연구-대비-주요-기여점",
    "title": "📃CrossDex 리뷰",
    "section": "3. 기존 연구 대비 주요 기여점",
    "text": "3. 기존 연구 대비 주요 기여점\nCrossDex는 앞서의 기술적 설계를 통해 기존 연구 대비 몇 가지 두드러진 기여를 제공합니다. 첫째, 범용 다지 손 파지 정책의 구현입니다. 기존에는 로봇 손마다 별도의 정책을 만들어야 했고, 심지어 유사한 형태의 손 사이에서도 정책을 재학습해야 했습니다. Patel & Song (2024)의 GET-Zero 연구가 동일한 LEAP Hand의 변형들 사이에서 정책 일반화를 시도했지만, 서로 다른 종류의 로봇 손 간 일반화에는 실패했다고 보고합니다. 반면 CrossDex는 ShadowHand, Allegro Hand, Schunk SVH, Ability Hand처럼 구조가 제각각인 손 네 종류를 한 번에 학습시키고, 학습에 포함되지 않은 LEAP Hand와 Inspire Hand까지 단일 정책으로 제어 가능함을 처음으로 보여주었습니다. 이는 로봇 손 분야 최초의 범용 정책 시도로서 의의가 있습니다.\n둘째, 인간 손 기반의 행동 공간 통합과 손가락 끝 관측을 통한 일반화 전략 자체가 기여점입니다. 앞서 설명한 인간 eigengrasp 공간을 활용한 행동 통합은 인간 조작의 공통 분모를 활용하는 창의적인 접근으로서, 서로 다른 손의 움직임을 일관된 방식으로 표현할 수 있게 해주었습니다. 이를 통해 저자들은 기존에 복잡한 그래프 신경망이나 트랜스포머 등을 사용해 각 로봇 형태를 인코딩해야 했던 접근들과 달리, 훨씬 명시적이고 해석적인 공용 인터페이스로 문제를 단순화했습니다. 특히 teleoperation (원격 조작) 분야의 지식을 정책 학습에 접목하여, 인간-로봇 사이의 제어 맵핑을 로봇-로봇 사이에도 적용한 점이 흥미로운 혁신입니다. 또한 손가락 끝 위치만으로 관측을 구성한 것은 단순히 차원을 줄이는 효과뿐만 아니라, 손 크기나 관절 구성 차이에도 불구하고 파지의 핵심인 손-물체 관계를 공통되게 파악할 수 있다는 장점이 있습니다.\n셋째, 실험적으로 높은 성능과 일반화 능력을 입증했습니다. CrossDex 정책은 YCB 벤치마크 객체들을 대상으로 4가지 훈련 손에서 평균 약 80%의 그립 성공률을 기록했으며, 학습에 전혀 포함되지 않았던 두 가지 새로운 손에 대해서도 제로샷으로 35% 이상의 성공률을 보였습니다. 이는 단순히 하나의 손에 특화된 정책을 새로운 손에 적용했을 때 기대되는 성능 (거의 0에 수렴)과 비교하면 크게 뛰어난 일반화 결과입니다. 특히 CrossDex는 동시 학습한 손들에 대해서도 각 손별 전용 정책과 비슷한 성능을 유지하면서, 새로운 손에 대해서는 월등히 우수한 성공률을 나타냈습니다. 이러한 성능 우위는 저자들이 고안한 통합 관측/동작 공간의 효과와, 교사-학생 학습 프레임워크의 효용을 함께 보여주는 증거입니다.\n마지막으로, 학습 효율 및 전이 학습 측면의 개선도 중요한 기여입니다. 다중 손을 한꺼번에 학습하는 크로스-임바디먼트 훈련은 개별 손별로 따로 학습할 때에 비해 약간 더 안정적이고 효율적인 학습곡선을 보였습니다. 즉, 공통 정책으로 묶어서 학습하면 오히려 훈련이 수렴하는 데 도움이 되고 각 손에 대한 성능 저하도 없었다는 것입니다. 뿐만 아니라, 학습된 정책을 활용한 파인튜닝 실험에서는, 사전 학습 없이 처음부터 새 손에 대해 학습하는 경우보다 훨씬 빠르게 성능을 끌어올릴 수 있음이 확인되었습니다. 예컨대 LEAP Hand에 대해 처음부터 학습한 정책의 다중 물체 성공률이 43.6%였다면, CrossDex 사전학습 정책을 미세조정한 경우 64.3%까지 달성하였습니다. 새로운 객체들에 대한 학습에서도 사전학습의 이점이 나타나, 범용 파지 정책이 프리트레인 모델로서 유용함을 보여주었습니다."
  },
  {
    "objectID": "posts/paper/2025-09-12-crossdex.html#실험-설계-및-구성-평가",
    "href": "posts/paper/2025-09-12-crossdex.html#실험-설계-및-구성-평가",
    "title": "📃CrossDex 리뷰",
    "section": "4. 실험 설계 및 구성 평가",
    "text": "4. 실험 설계 및 구성 평가\n논문의 실험은 주로 시뮬레이션 환경에서의 다수 객체 파지 평가로 이루어졌으며, 설계 면에서 비교적 철저하고 신중하게 구성되었습니다. 실험에 사용된 로봇 손 플랫폼은 앞서 언급한 4가지 훈련용 손(ShadowHand, Allegro Hand, Schunk SVH Hand, Ability Hand)과 2가지 테스트용 손(LEAP Hand, Inspire Hand)입니다. 이들은 손가락 개수(4~5개)와 자유도(10~22 DoF)가 상이하여, CrossDex의 일반화 성능을 검증하기에 충분히 다양한 사례를 포함합니다. 모든 손은 RealMan RM65 6자유도 로봇팔 끝단에 장착되어 있고, 로봇팔의 베이스는 책상 옆면에 고정되어 있습니다. 이는 실제 로봇 실험을 염두에 둔 구성으로, 시뮬레이션에서의 설정이 현실의 하드웨어 배치와 일치하도록 했습니다.\n학습 환경으로는 NVIDIA의 물리 시뮬레이터인 Isaac Gym을 사용하였고, 한 번에 수천 개의 병렬 환경을 돌려 데이터 효율을 극대화했습니다. 예를 들어 PPO 기반 상태정책 학습 시 8192개 환경을 병렬 실행하여 4만 iteration을 수행했고, 비전 정책 학습 시는 16,384개 환경까지 활용하여 대용량의 데이터로 학습을 진행했습니다. 이렇게 대규모 환경을 사용하면서도, 한 대의 GPU에서 학습이 가능하도록(8192 환경의 경우 RTX 4090 한 장으로 가능) 시뮬레이션을 최적화했다고 합니다. 이는 실험 결과의 신뢰성 측면에서 중요합니다. 강화학습 결과는 보통 무작위성에 민감한데, 병렬 환경을 많이 사용하면 운에 따른 편차를 줄이고 더 안정적인 성능 추정을 할 수 있습니다. 또한 논문에서는 표와 그래프에 성공률의 평균과 표준편차(또는 표준오차)를 함께 제시하여 성능 차이가 통계적으로 유의미함을 보여주고 있습니다.\n평가 방식은 YCB Object Set에 속한 45개의 다양한 물체에 대해 로봇 손이 물체를 들어올릴 수 있는지(성공/실패)를 측정하는 형태였습니다. 물체는 책상 위에 무작위로 놓여 있고, 로봇 손은 초기 자세로부터 물체를 파지해 들어올리는 에피소드를 반복하게 됩니다. 각 손-물체 조합에 대해 여러 시도를 수행해 성공률을 계산하며, 이를 모든 손에 대해 평균내어 종합 성능으로 사용했습니다.\nBaseline(비교 방법)으로는 다중작업(Multi-task) RL 방식들을 설정하여 CrossDex의 설계 요소들을 하나씩 제거해본 변형들을 사용했습니다. 예를 들어, MT-Raw-OA라는 베이스라인은 관측과 행동을 그대로(raw) 사용하는 대신 손 종류를 구분하는 원-핫 임베딩을 추가하여 여러 손을 함께 학습시키는 방법입니다. 이때 손별 관절 상태 차원을 맞추기 위해 남는 부분을 0으로 패딩하고, 관절 증가 방향도 통일시키는 등의 정규화를 적용했다고 합니다. 또한 MT-Raw-A는 관측은 CrossDex처럼 통합하되 행동은 원래 각 손의 관절 명령을 쓰는 경우, MT-Raw-O는 그 반대로 행동은 eigengrasp로 통일하되 관측은 각 손의 원래 관절각을 모두 포함한 경우로 설정하여 비교했습니다. 이렇게 baseline을 구성함으로써, 공용 관측/행동 공간이 성능에 미치는 영향을 정량적으로 분석하려는 의도가 엿보입니다.\n전체적으로 실험 설계는 적절한 대조군을 갖추고 있고, 훈련에 사용되지 않은 새로운 손과 새로운 물체로의 일반화 평가까지 포함하여 제안 방법의 효과를 다각도로 검증했습니다. 다만 아쉬운 부분을 꼽자면, 본 논문에서는 주된 결과가 시뮬레이션 상의 성공률에 집중되어 있어 실제 로봇에의 적용 검증이 제한적이었다는 점입니다. 저자들이 프로젝트 페이지를 통해 LEAP Hand 실물과 RealSense 카메라를 사용한 Sim-to-Real 실험 영상을 공개하긴 했지만, 논문 본문에서는 이 부분이 정량적인 평가로 다루어지진 않았습니다. 따라서 현실 세계에서 다양한 손으로 범용 정책을 적용할 때 발생할 수 있는 미지의 변수들(센서 노이즈, 모델링 오차 등)에 대한 검증은 향후 과제로 남아 있습니다. 그럼에도 불구하고 시뮬레이터 내에서 충분한 다양한 상황을 실험했고, 코드와 환경 설정을 공개(PKU-RL/CrossDex 깃허브)하여 재현성도 높인 점은 긍정적으로 평가할 수 있습니다."
  },
  {
    "objectID": "posts/paper/2025-09-12-crossdex.html#주요-결과-및-한계점-논의",
    "href": "posts/paper/2025-09-12-crossdex.html#주요-결과-및-한계점-논의",
    "title": "📃CrossDex 리뷰",
    "section": "5. 주요 결과 및 한계점 논의",
    "text": "5. 주요 결과 및 한계점 논의\nCrossDex의 실험 결과는 범용 정책의 가능성을 뒷받침하는 인상적인 수치를 보여줍니다. 우선, 하나의 비전 기반 정책이 훈련에 사용된 4가지 손에 대해 평균 약 80%의 파지 성공률을 달성했습니다. 개별 객체별로 보면 45개 YCB 물체 중 42개에서 성공률이 거의 100%에 가깝게 나왔으며, 이는 여러 손을 동시에 제어하도록 학습했음에도 각 손에 대한 성능 희생이 크지 않았다는 뜻입니다. 또한 동시학습의 이점으로, 저자들은 동일 조건에서 손별로 따로 학습시킨 정책들과 비교해볼 때 크게 뒤지지 않거나 오히려 학습 안정성이 높았다고 보고합니다. Figure 3의 학습 곡선 비교를 보면 CrossDex와 개별 학습의 수렴 속도 및 최종 성능이 유사하거나 약간 더 나은데, 이는 서로 다른 손 간에 경험이 일부 공유되면서 학습 신호의 다양성이 증가한 덕분으로 해석할 수 있습니다.\nBaseline 대비 성능을 살펴보면, CrossDex의 통합 관측/행동 공간 전략이 기존 방식들을 확연히 능가함을 알 수 있습니다. Table 1의 결과에 따르면, CrossDex 정책은 훈련에 사용된 손들은 물론 보지 못한 새로운 손들에 대해서도 모든 baseline보다 높은 성공률을 보였습니다. 흥미로운 점은, raw 관측/행동을 사용한 다중학습도 어느 정도 일반화 능력을 갖긴 했다는 것입니다. 예를 들어 MT-Raw-A (관측 통일/행동 원래값) 방법은 새로운 손에 대해 21% 정도의 성공률을 보였는데, 이는 개발자들이 URDF 정렬 등을 통해 손들 간 관절 정의를 신중히 맞춰준 덕분입니다. 즉, 손가락 순서나 관절 증감 방향 등을 수작업으로 통일한 결과 어느 정도 제로샷 일반화가 가능했지만, 인간 손 eigengrasp를 활용한 CrossDex의 성능(35% 이상 성공률)에는 크게 못 미쳤습니다. 더욱이 다른 baseline인 MT-Raw-O나 MT-Raw-OA의 경우, 상태기반 학습 시 로봇 종류 원-핫벡터를 활용한 탓에 비전 정책으로 지식을 증류할 때 관측 정보가 감소하여 성능 하락이 두드러졌습니다. 이러한 분석은 CrossDex의 설계 선택이 단순히 편의성뿐만 아니라 성능 면에서도 최적임을 뒷받침합니다. 한편, CrossDex가 새로운 손에 대해 달성한 제로샷 성공률 ~35% 수준은 절대적으로 보면 낮게 느껴질 수 있습니다. 하지만 앞서 언급했듯 전혀 보지 못한 손의 경우 일반적인 정책은 거의 실패하는 반면, 3번 중 1번 이상 성공하게 만든 것만으로도 고무적인 결과입니다. 이는 추가 학습(finetuning)을 통해 쉽게 끌어올릴 수 있는 출발점으로 볼 수 있습니다.\nCrossDex의 한계점도 짚고 넘어갈 필요가 있습니다. 우선, 현재의 범용 정책은 파지(grasp) 동작에 초점을 맞추고 있습니다. 즉, 테이블 위의 정적(static)인 물체를 집어 드는 과제에 특화되어 있으며, 물체를 쥔 후 섬세한 조작(in-hand manipulation)이나 연속적인 작업으로의 일반화는 다루지 않습니다. 이는 연구 범위를 넘는 부분이지만, 결국 궁극적인 로봇 손 활용을 위해서는 파지 이후의 단계(예: 물체 회전시키기, 도구 사용 등)도 범용 정책으로 다룰 수 있어야 할 것입니다. 또한 시뮬레이터 기반의 학습과 평가라는 한계 때문에, 현실 세계에서 발생하는 마찰 모델 차이나 센서 오차, 카메라 인식 문제 등이 고려되지 않았습니다. 프로젝트 페이지에서 시도된 Sim-to-Real 적용에서는 어느 정도 성공을 거두었지만, 실제 실험에서는 카메라에 손이 가려 물체가 보이지 않게 된다거나, 로봇팔이 테이블과 충돌하는 등의 실패 사례도 확인되었습니다. 이는 향후 센서 융합이나 모션 플래닝을 결합하여 개선할 부분입니다.\n또 다른 한계로, 행동 공간을 인간 손 모드로 제한한 것이 복잡한 손 동작을 표현하는 데 제약이 될 가능성이 있습니다. 저자들은 MANO 기반 eigengrasp를 6개, 12개 등 여러 개수로 시도한 결과 큰 성능 차이는 없었다고 보고하고 있어 현재 설정이 충분함을 시사하지만, 이 공간이 표현하지 못하는 특이한 손 자세가 필요한 작업이 있을 수 있습니다. 마찬가지로 손가락 끝 위치만으로 관측을 구성한 것도 파지에는 충분했으나, 손가락의 세부 굽힘 정도나 관절 토크 등의 정보가 배제되었다는 점에서 향후 더 복잡한 상호작용에는 보완이 필요할 수 있습니다.\n요약하면, CrossDex의 결과는 현재 수준에서 범용 다지 손 파지의 개념 증명(proof of concept)을 성공적으로 보여줬다고 평가할 수 있습니다. 동일한 정책으로 여러 형태의 로봇 손에 일관되게 물체 파지 동작을 수행하고, 새로운 손에도 일부 일반화가 가능함을 입증한 것은 큰 진전입니다. 동시에, 현실 적용까지 넘어서기 위해서는 몇 가지 난제들 – 예를 들어 더 복잡한 작업으로의 확장, 현실 세계의 불확실성 대응, 더욱 다양한 손 구조 (예: 두 손가락 집게 그리퍼 등 극단적으로 형태가 다른 손) – 이 남아 있음을 확인하게 해줍니다."
  },
  {
    "objectID": "posts/paper/2025-09-12-crossdex.html#향후-연구-방향-제언",
    "href": "posts/paper/2025-09-12-crossdex.html#향후-연구-방향-제언",
    "title": "📃CrossDex 리뷰",
    "section": "6. 향후 연구 방향 제언",
    "text": "6. 향후 연구 방향 제언\n이번 연구를 바탕으로 생각해볼 수 있는 향후 연구 방향은 여러 가지가 있습니다:\n\n실세계 적용 및 검증 확대: 시뮬레이션에서 입증된 CrossDex 정책을 실제 로봇 플랫폼에서 더욱 광범위하게 시험하는 것이 필요합니다. 다양한 센서 피드백(예: 촉각센서)과 카메라 환경에서 정책이 얼마나 강인한지 평가하고, Sim-to-Real 간 갭을 줄이기 위한 도구(도메인 랜덤화, 적대적 학습 등)를 결합하는 연구가 유망합니다.\n다양한 작업과 목표로의 일반화: 파지 외에 조작(manipulation) 작업에도 범용 정책을 확장하는 방향입니다. 예를 들어 하나의 정책이 여러 손으로 병뚜껑 열기, 공 돌리기, 도구 사용하기 등의 다양한 조작 과제까지 수행하도록 학습시킬 수 있다면 진정한 범용 손 사용 로봇에 가까워질 것입니다. 이를 위해 고차원 행동을 효율적으로 탐색할 수 있는 계층적 RL이나 모방학습 결합 기법 등을 고려해볼 수 있습니다.\n정책 아키텍처의 진화: 현재 CrossDex는 인간 손이라는 정적 인터페이스를 통해 손들을 묶었지만, 향후에는 모든 형태의 로봇 손을 포괄할 수 있는 학습 기반 표현을 탐색할 수 있습니다. 예컨대 그래프 신경망으로 로봇 손의 형태를 인코딩하고, 거기에 정책을 조건부 생성(condition)하는 방법이나, 트랜스포머를 이용해 손 구조를 토큰화하여 in-context로 제어 명령을 생성하는 방법 등이 대안이 될 수 있습니다. 이러한 접근은 새로운 손이 투입되었을 때 인간 손 공간에 맞추기 어려운 경우에도 적용할 수 있다는 장점이 있습니다.\n휴먼 데몬스트레이션과의 결합: CrossDex는 인간 손 공간을 활용했지만 정작 인간 시연 데이터는 강화학습 교사로 직접 쓰지 않았습니다. 향후에는 인간 시연 데이터를 활용한 오프라인 RL이나 IL(모방학습) 기법을 결합하여 학습 효율을 높이고, 정책이 더욱 인간스러운 동작을 하도록 유도할 수 있을 것입니다. 특히 teleoperation 시스템에서 얻은 인간-로봇 손 동작 짝(pair) 데이터를 활용하면 리타게팅 학습이나 초기 정책 구성에 도움이 될 것입니다.\n추가적인 안전 및 제약 요소 통합: 로봇 손이 다양한 환경에서 동작하려면, 단순 성공률뿐 아니라 안전성도 중요합니다. 향후 연구에서는 정책 학습 시 로봇 손의 충돌 회피나 힘 제어 안정성 등의 제약을 모델에 통합하는 방향도 고려될 수 있습니다. 예를 들어 손가락 끝에 과도한 힘이 가해지면 감지하여 놓아주도록 하거나, 테이블과 부딪히지 않도록 학습에 페널티를 주는 등 현실적인 제약을 넣으면, 실제 적용에 한층 가까워질 것입니다.\n\n결론적으로, Cross-Embodiment Dexterous Grasping with RL 논문은 다양한 로봇 손에 걸쳐 통합적으로 동작하는 강화학습 정책의 가능성을 보여준 선구적인 연구입니다. 배경의 난제부터 제안 기법, 실험 검증까지 논리적이고 명확하게 전개되었으며, 로봇공학 및 강화학습 연구자들에게 새로운 관점을 제공하고 있습니다. 앞으로 이 방향의 연구가 지속되어, 사람 손처럼 유연하고 범용적인 로봇 손 조작 정책이 실현되기를 기대해봅니다."
  },
  {
    "objectID": "posts/paper/2025-08-24-immimic.html",
    "href": "posts/paper/2025-08-24-immimic.html",
    "title": "📃ImMimic 리뷰",
    "section": "",
    "text": "Paper Link\nProject Link"
  },
  {
    "objectID": "posts/paper/2025-08-24-immimic.html#summary",
    "href": "posts/paper/2025-08-24-immimic.html#summary",
    "title": "📃ImMimic 리뷰",
    "section": "2.1 Summary",
    "text": "2.1 Summary\nImMimic은 인간 시연 비디오와 소량의 로봇 시연 데이터를 함께 활용하여 로봇 조작 학습의 효율을 높이는 새로운 모방학습 프레임워크입니다. 인간과 로봇 사이에는 시각적, 형태적(모르포LOGY), 물리적 도메인 차이(domain gap)가 존재하여, 인간 비디오만으로 로봇이 직접 행동을 모방하기 어려웠습니다. 이 논문에서는 이러한 갭을 효과적으로 메꾸기 위해 Embodiment(embodiment)에 구애받지 않는 공동 학습(co-training) 방법을 제안합니다. 핵심 아이디어는 인간 손의 시연 경로를 로봇 관절 공간으로 retargeting(retargeting)하고, 동적 시간 왜곡 알고리즘(Dynamic Time Warping, DTW)을 이용해 인간과 로봇 시연의 시계열 정렬을 수행한 뒤, 정렬된 쌍을 MixUp 보간 기법으로 섞어 중간 도메인 데이터를 생성하는 것입니다. 이렇게 생성된 중간 분포의 시연 데이터들을 원래 로봇 데이터와 함께 동시에 학습함으로써, 인간 비디오로부터 획득한 지식을 로봇 정책에 원활히 이전합니다. 실제 4가지 조작 과제(집어서 놓기, 밀기, 망치질, 뒤집기)에 대해 4종의 로봇 손/그리퍼(Robotiq 그리퍼, Fin Ray 그리퍼, Allegro 다지 손, Ability 다지 손)에서 실험한 결과, ImMimic을 적용하면 작업 성공률이 향상되고 로봇 동작이 한층 매끄럽게 실행됨이 확인되었습니다. 이는 소량의 로봇 데이터(예: 5개)와 대량의 인간 비디오(예: 100개)만으로도, 기존 방법보다 도메인 차이를 효과적으로 극복할 수 있음을 보여줍니다[4]."
  },
  {
    "objectID": "posts/paper/2025-08-24-immimic.html#contributions",
    "href": "posts/paper/2025-08-24-immimic.html#contributions",
    "title": "📃ImMimic 리뷰",
    "section": "2.2 Contributions",
    "text": "2.2 Contributions\n\n인간-로봇 시演 정렬을 통한 교차 도메인 모방학습: 인간 비디오로부터 얻은 시연을 로봇 시연과 정렬(mapping)하여 함께 학습할 수 있는 공동 학습 프레임워크를 제시하였습니다. 동적 시간 왜곡(DTW)을 활용해 인간과 로봇의 시계열 데이터를 정밀하게 맞춤으로써, 인간 시연을 로봇 정책 학습에 직접 활용할 수 있는 공통 표현 공간으로 가져옵니다. 특히 행동 기반과 시각 기반 두 가지 매핑 전략(DTW-A, DTW-V)을 제시하여, 행동 시퀀스 또는 시각 피처 유사도를 기준으로 정렬하는 방법을 탐구했습니다.\nMixUp 보간을 이용한 중간 도메인 데이터 생성: 정렬된 인간-로봇 시연 쌍을 MixUp 기법으로 보간하여, 인간 도메인과 로봇 도메인 사이의 중간 형태의 데이터를 다수 생성했습니다. 구체적으로, 잠재 공간(latent space)에서의 관찰 표현과 행동 공간(action space)에서의 제어 신호를 각각 선형 혼합함으로써, 인간 시연이 점진적으로 로봇 시연의 특성을 띠는 연속적인 도메인 스펙트럼을 구축했습니다. 이러한 중간 도메인 데이터들은 공동 학습 시 인간 도메인 데이터가 로봇 도메인으로 부드럽게 적응하도록 도와주며, 결과적으로 도메인 갭을 완화**하는 데 핵심적인 역할을 합니다.\nEmbodiment에 불문한 정책 공동학습 및 확산 모델 활용: 인간과 로봇의 데이터를 단일 정책 모델에 통합하여 학습하는 embodiment-agnostic co-training을 구현했습니다. 이를 위해 비전 관찰(카메라 영상)과 로봇 고유감각(proprioception) 정보를 조건으로 받아 미래 행동을 예측하는 정책 신경망을 설계하였고, 특히 확산 모델 기반 정책(diffusion policy) 구조를 활용하여 시퀀스 형태의 행동 생성에 안정성을 더했습니다. 인간 비디오의 경우, 손 포즈 추적 모듈로부터 얻은 로봇 관절형태로 retargeting된 손동작을 행동 라벨로 사용하여, 마치 로봇이 해당 상황에서 취했을 행동인 것처럼 학습시켰습니다. 로봇 시연 데이터와 인간 비디오로부터 얻은 (실제+보간) 데이터에 대해 동일한 모델을 공동 최적화하며, 두 도메인의 학습 손실을 합산한 목표 함수를 최소화하는 방식으로 훈련을 진행했습니다. 이러한 접근으로 별도 도메인 구분 없이 하나의 통합 정책이 학습되며, 다양한 로봇 플랫폼에도 동일한 알고리즘을 적용할 수 있는 일반성을 확보했습니다.\n실험을 통한 성능 향상 및 특성 분석: 실제 로봇을 사용한 다양한 실험을 통해 제안 기법의 효과를 입증했습니다. 4개의 과제와 4개의 서로 다른 로봇 매니퓰레이터 조합에서, ImMimic은 기존 대비 성공률 향상과 동작 원활성 개선을 일관되게 보여주었습니다. 또한 대조 실험을 통해, 아무 보정 없이 인간+로봇 데이터를 함께 학습한 경우(naive co-training)나 인간-로봇 시연을 무작위로 짝지어 보간한 경우(random mapping) 대비 제안 방법의 우수성을 정량적으로 확인했습니다. 특히 시계열 매핑의 중요성(random mapping 대비)과 중간 도메인 보간의 기여(vanilla co-training 대비)를 분리하여 검증하였고, 동작 공간 기반 매핑(ImMimic-A)이 시각 피처 기반 매핑(ImMimic-V)보다 좋은 성능을 내는 것도 실험적으로 밝혔다. 마지막으로, t-SNE 시각화를 통해 ImMimic 적용 시 인간 데이터의 표현 공간이 로봇 데이터 공간과 연속적으로 겹쳐짐을 보였는데, 이는 특별한 보정이 없었던 경우 두 도메인 데이터가 분리된 클러스터로 남는 것과 대조적입니다. 이로써 ImMimic의 도메인 적응 효과를 직관적으로 확인할 수 있습니다."
  },
  {
    "objectID": "posts/paper/2025-08-24-immimic.html#methodology-analysis",
    "href": "posts/paper/2025-08-24-immimic.html#methodology-analysis",
    "title": "📃ImMimic 리뷰",
    "section": "2.3 Methodology Analysis",
    "text": "2.3 Methodology Analysis\nImMimic이 제안하는 교차 도메인 모방학습 방법은 크게 다섯 단계로 구성됩니다:\n\n로봇 시연 데이터 수집: 우선 각 작업(task)에 대해 소수의 로봇 시연을 수집합니다. 여기서는 원격 조작(teleoperation)을 사용하여 사람 운영자가 로봇을 직접 조작, 시각 관찰과 로봇 상태-행동 데이터를 기록하였습니다. 예를 들어 카메라 영상(외부 시점 + 손목부 카메라)과 로봇 관절각/그리퍼 상태 등의 프로프리오셉션이 시간에 따라 기록됩니다.\n인간 시연 데이터 수집 및 retargeting: 동시에 각 작업에 대해 다수의 인간 시연 비디오를 녹화하여 확보합니다. 인간 시연은 로봇과 유사한 환경(예: 동일한 작업대와 물체 배치)에서 에이전트 시점 카메라(agent-view)로 촬영하여, 로봇이 보게 될 시각 정보와 최대한 유사하게 하였습니다. 이렇게 얻은 인간 비디오로부터는 손 움직임을 추적 및 추출하여, 로봇의 제어 신호로 변환(retargeting)합니다. 이는 인간 손가락/팔의 움직임을 로봇의 관절각 혹은 그리퍼 움직임 등에 매핑하는 과정으로, 예컨대 인간 손가락으로 공을 잡는 동작을 로봇 그리퍼의 폐쇄 동작에 대응시키는 식입니다. 이 손 포즈 retargeting 모듈의 출력을 통해, 인간 비디오 각 시점마다 해당 시점에 로봇이 취했을 법한 행동(action label)을 라벨로 할당할 수 있게 됩니다. 이렇게 함으로써 인간 시연에는 원래 존재하지 않던 로봇 행동 라벨이 부여되어, 지도학습 형태의 모방학습이 가능해집니다.\n동적 시간 왜곡(DTW)을 통한 시퀀스 정렬: 다음으로 각 인간 시연 시퀀스와 유사한 로봇 시연 시퀀스를 짝지어, 동적 시간 왜곡 알고리즘을 적용합니다. DTW는 두 시계열 X=(x_1,…,x_m) 와 Y=(y_1,…,y_n) 사이의 유사도에 기반한 정렬을 찾는 알고리즘입니다. 예를 들어 여기서는 인간 시연의 특징과 로봇 시연의 특징 간 거리를 정의하고, DTW를 사용해 시간 축을 비선형적으로 조정함으로써 두 시퀀스 간 최적 매칭을 구합니다. 이때 정렬 기준이 되는 특징에 따라 두 가지 방식을 제안하였습니다:\n행동 기반 매핑(ImMimic-A): retargeting된 로봇 관절 행동 시퀀스를 비교 기준으로 사용합니다. 즉 인간 시연(로봇 공간으로 투영된)의 관절 움직임 벡터와 실제 로봇 시연의 관절 움직임 벡터 간의 차이를 거리 함수로 정의하여 DTW 정렬을 수행합니다. 이를 통해 동일하거나 유사한 동작 순간들이 시간 축을 넘어 매칭됩니다.\n시각 기반 매핑(ImMimic-V): 시각 피처를 정렬 기준으로 사용합니다. 인간 비디오 프레임과 로봇 시연 영상 프레임을 각각 인코더(예: ResNet)로 변환해 얻은 잠재 표현(latent feature) 간 거리를 계산하여 DTW를 수행합니다. 이 방법은 두 시퀀스가 시각적으로 비슷한 상태(예: 물체와 손의 상대 위치 등)에 있을 때를 정렬시켜 줍니다.\n\nDTW 정렬 결과, 인간 시연의 각 시간 단계 t_h 가 로봇 시연의 한 시간 단계 t_r 와 연결되어 시점 매칭 쌍들의 집합 {(t_h,t_r)} 이 얻어집니다. 이 연결을 통해 인간 시연 관찰 o_h (t_h) 와 해당 시점의 행동 라벨 a_h (t_h) (retargeting된 것) 그리고 매칭된 로봇 관찰 o_r (t_r) 와 로봇 행동 a_r (t_r) 가 하나의 정렬된 시퀀스 쌍으로 묶입니다.\n\nMixUp 보간을 통한 중간 도메인 데이터 생성: 이렇게 정렬된 인간-로봇 시연 쌍에 대해, MixUp 데이터 보강 기법을 적용하여 다수의 새로운 가상 시연 데이터를 만들어냅니다. MixUp이란 두 샘플을 선형 결합하여 새로운 샘플을 만드는 기법으로, 여기서는 인간 시연과 로봇 시연의 특성을 부분적으로 섞은 중간 시연을 생성하는 데 활용되었습니다. 구체적으로, 정렬된 쌍에서 동일한 상대 시점에 있는 인간 관찰의 잠재표현 z_h 와 로봇 관찰의 잠재표현 z_r 을 섞고, 인간 행동 a_h 와 로봇 행동 a_r 도 섞습니다. 예를 들어 한 정렬된 시점 쌍에 대해 임의의 보간 계수 λ∈[0,1] 를 선택하고 다음과 같이 생성합니다:\n\n\\begin{align*}\nz_{mixt} = \\alpha \\cdot z_{h t} + (1−\\alpha) \\cdot z_{r t′} ,     a_{mixt:t+k} = \\alpha \\cdot a^{h→r}_{t:t+k} + (1−\\alpha) \\cdot a_{r t′:t′+k}\n\\end{align*}\n이 z_“mix” 와 a_“mix” 는 인간-로봇 중간 특성을 갖는 하나의 새로운 학습 데이터 포인트를 나타냅니다. 전체 시퀀스에 대해 이런 식의 보간을 수행하면, 인간 시연의 연속된 프레임들이 점차 로봇 시연의 특성으로 변모하는 가상 시퀀스가 생성됩니다. 논문에서는 관찰의 잠재공간과 행동공간 모두에서 보간을 수행하여 일관성 있는 새로운 시퀀스를 얻었다고 보고합니다. 이 중간 도메인 데이터들은 겉보기에는 완전한 로봇 시연도, 완전한 인간 시연도 아니지만 두 도메인의 특징을 모두 조금씩 갖고 있어, 학습 시에 인간 도메인 데이터가 자연스럽게 로봇 도메인으로 이어지도록 만들어주는 다리 역할을 합니다. 이 과정은 여러 정렬된 시연 쌍들에 대해 반복하여 적용되며, 결과적으로 인간 비디오로부터 추출된 다량의 보간 시퀀스 데이터가 추가로 확보됩니다.\n\n공동 학습(co-training)으로 정책 훈련: 마지막으로, 위에서 얻어진 보간된 인간 데이터 + 원본 인간 데이터 + 소량의 로봇 데이터를 모두 함께 사용하여 하나의 정책 모델을 학습시킵니다[20]. 정책 모델은 종단간 비전-모터 정책으로서, 주어진 현재 관찰에 대해 다음 시간 스텝의 로봇 행동을 예측하도록 훈련됩니다. 로봇 관찰의 경우 로봇의 카메라 영상(Agent-view 및 Wrist-view 두 시점)과 로봇 관절 상태(프로프리오셉션)를 함께 신경망 인코더를 통해 잠재 상태로 변환하고, 이를 기반으로 출력 분포에서 다음 행동을 샘플링하거나 추론합니다. 인간 관찰의 경우 인간 비디오 프레임을 동일한 정책 네트워크에 통과시키되, 이때 현재 로봇 상태에 해당하는 입력이 필요합니다. 이를 위해 앞서 retargeting된 인간 손동작을 해당 시점의 로봇 프레임에서의 관절 상태(가상의 프로프리오셉션)로 간주하여 입력에 포함시킵니다. 다시 말해, 인간 비디오에 대응하는 정책 입력에는 “만약 이 시점에 로봇이 이 동작을 수행하고 있다면”이라는 가정 하에 로봇의 상태로 치환된 정보가 제공됩니다. 정책 출력으로는 로봇의 다음 행동(관절 명령)이 예측되는데, 로봇 데이터의 경우는 실제 로봇 시연의 다음 행동과 비교하고, 인간 데이터의 경우는 retargeting된 다음 행동(인간→로봇 변환된)과 비교하여 손실을 계산합니다. 모든 데이터에 대해 동일한 정책 네트워크의 파라미터를 공유하며, 인간 데이터와 로봇 데이터에서 오는 재구성 손실(모방 학습 오차)을 합산하여 모델을 최적화합니다. 이러한 공동 학습을 통해, 모델은 로봇 시연의 정확한 패턴을 학습함과 동시에 인간 시연으로부터 일반화에 유용한 다양성을 흡수합니다. 특히 보간된 중간 데이터 덕분에, 학습 과정에서 인간 도메인 분포 → 중간 분포 → 로봇 분포로 점진적인 도메인 이동이 유도되어, 학습 안정성과 도메인 적응 능력이 향상됩니다. 실제 저자들은 t-SNE 시각화를 통해, ImMimic으로 학습할 경우 훈련 중 인간 데이터의 잠재 표현들이 로봇 데이터 쪽으로 연속적으로 분포함을 보여주었습니다 (Vanilla 공동학습의 경우 인간/로봇 데이터가 분리된 군집을 형성함). 이는 중간 도메인 보간이 표현 공간 상에서 두 도메인의 간극을 메우는 역할을 수행함을 뒷받침합니다.\n\n정책 모델의 학습은 Diffusion Policy 방식을 차용하였다고 언급되는데, 이는 확산 확률모델(denoising diffusion)을 이용해 미래 행동 시퀀스를 생성하도록 한 것으로 해석됩니다. 확산 정책의 장점은 다중 모달 행동 분포를 잘 표현하고 안정적으로 시퀀스를 예측할 수 있다는 점인데, 논문에서는 이러한 기법을 도입하여 행동 예측을 시계열 생성 문제로 다룬 것으로 보입니다. 훈련 목표는 각 시점에서 모델이 데모 행동을 재구성하도록 하는 것으로, 인간 및 로봇 데이터 모두에 대해 행동 예측 오류(예: MSE 혹은 음의 로그우도)를 최소화합니다. 이렇게 학습된 정책은 추후 로봇에 주어졌을 때, 새로운 관찰(카메라 영상)을 입력받아 인간 시연에서 학습한 풍부한 동작을 바탕으로도, 실제 로봇에 유효한 제어 신호를 출력할 수 있게 됩니다.\n요약하면, ImMimic의 방법론적 참신성은 “인간→로봇 데이터의 사상(mapping)과 분포 보간(interpolation)”이라는 두 가지 기술로 도메인 차이를 극복한 점입니다. 복잡한 도메인 적응 알고리즘 대신, 시연 데이터를 직접 조작하여 도메인 간 격차를 줄이는 접근을 취했기에 구현이 비교적 간단하면서도 효과적입니다. 또한 여러 로봇 형태에 동일하게 적용 가능한 일반 프레임워크로 설계되어 다양한 플랫폼에 확장 가능하다는 장점이 있습니다."
  },
  {
    "objectID": "posts/paper/2025-08-24-immimic.html#experimental-results-and-analysis",
    "href": "posts/paper/2025-08-24-immimic.html#experimental-results-and-analysis",
    "title": "📃ImMimic 리뷰",
    "section": "2.4 Experimental Results and Analysis",
    "text": "2.4 Experimental Results and Analysis\n논문에서는 네 가지 실제 조작 작업에 대해 제안한 ImMimic의 성능을 평가하였습니다:\n\n집어서 놓기 (Pick and Place): 테이블 위의 작은 물체를 집어서 다른 위치에 정확히 내려놓는 작업.\n밀기 (Push): 물체를 원하는 방향으로 밀어서 이동시키는 작업.\n망치질 (Hammer): 망치나 망치형 도구를 사용해 목표 지점을 내리치는 작업 (예: 못 박기 등 유사 동작).\n뒤집기 (Flip): 물체를 들어올려서 뒤집거나, 또는 지렛대를 젖혀서 방향을 바꾸는 작업.\n\n이들 작업은 정밀한 그립부터 동적 타격 동작까지 다양한 조작 스펙트럼을 포함하여, 알고리즘의 범용성을 시험합니다. 각 작업에 대해 서로 다른 4종의 로봇 손/그리퍼를 사용했는데, 두 종류는 평행 그리퍼(Robotiq 2F-85 그리퍼, 그리고 Fin Ray 소프트 그리퍼)이고, 두 종류는 다지(多指) 로봇 핸드(Shadow Allegro Hand V4, PSYONIC Ability Hand)입니다. 이는 단순 집게형부터 인간형 손까지 다양한 형태적 Embodiment 차이를 포괄함으로써, ImMimic이 형태에 구애받지 않고 동작학습에 도움이 되는지 검증하기 위함입니다. 각 로봇에는 공통적으로 7자유도 로봇 팔(예: Franka Emika Panda)이 붙어 있어 작업을 수행하며, 그 끝단에 위의 그리퍼/핸드가 장착된 형태로 실험이 이루어진 것으로 추정됩니다.\n데이터 구성: 실험에서 사용된 데모 데이터의 규모는 인간 시연이 작업당 약 100개, 로봇 시연이 작업당 5개 수준으로 명시되어 있습니다[4]. 이 20:1 정도의 비율은 인간 비디오가 매우 풍부하지만 로봇 데모는 극도로 제한적인 상황을 가정한 것으로, ImMimic의 목표인 “대규모 인간 시연 + 소규모 로봇 시연” 상황을 잘 반영합니다. 각 작업마다 5개의 로봇 시연은 다양한 초기조건과 전략을 담도록 어느 정도 다양성을 가지게 수집되었고, 인간 시연 100개 역시 가능한 다양한 사람의 동작으로 구성되어 데이터 폭을 넓게 가져갔을 것입니다.\n학습 및 평가: ImMimic 모델은 앞서 기술된 방식대로 모든 데이터(원본 로봇 5개, 원본 인간 100개, 그리고 보간 생성된 가상 시연 다수)를 함께 사용하여 훈련됩니다. 학습 후 각 작업-로봇 조합에 대해 10회 이상의 반복 실험을 통해 성공률(task success rate)을 측정하였을 것으로 보입니다. 또한 학습된 정책으로 작업을 수행할 때 로봇 동작의 매끄러움(smoothness)을 정성적/정량적으로 분석하였습니다. 매끄러움은 예컨대 경로의 연속성, 속도 프로파일의 부드러움(가속도의 변화), 불필요한 멈춤/진동의 감소 등을 통해 평가되었을 가능성이 높습니다. 이외에도 도메인 적응 효과를 확인하기 위해 학습 과정 중 임베딩 분포(t-SNE)나 샘플 효율성 분석(데모 개수에 따른 성능 곡선) 등이 수행되었습니다.\n비교 기법 (Baselines): 성능 평가를 위해 몇 가지 비교 대상이 설정되었습니다:\n\nRobot-Only: 인간 비디오 데이터를 전혀 사용하지 않고 소량의 로봇 데모(5개)만으로 정책을 학습한 경우입니다. 이는 ImMimic을 사용하지 않았을 때 데이터 부족 상황에서의 기본 성능을 나타냅니다. 일반적으로 이런 극소량 데이터로 학습한 정책은 성공률이 낮을 것으로 예상되며, 이를 기준선으로 삼습니다.\nVanilla Co-Training: ImMimic의 핵심 기법(DTW 정렬 및 MixUp 보간)을 적용하지 않고, 인간 비디오에서 추출한 행동 라벨과 로봇 데모를 그냥 통합하여 한꺼번에 학습한 경우입니다. 즉, 별다른 도메인 적응 처리 없이 공동학습만 수행한 방법으로 볼 수 있습니다. 이 방법은 인간 데이터가 추가되긴 하지만 도메인 차이를 제대로 다루지 않아, 성능이 Robot-Only보다 나아지지 않거나 오히려 교란 효과로 악화될 수 있습니다. 이를 통해 도메인 갭을 다루는 기법의 중요성을 평가합니다.\nRandom Mapping: ImMimic과 동일하게 인간+로봇 데이터를 함께 학습하되, 인간-로봇 시퀀스 간 의미 있는 정렬 없이 임의로 짝지어 MixUp 보간을 수행한 경우입니다. 이는 DTW 기반 정렬의 효과를 검증하기 위한 실험으로, 정렬 없이 보간하면 비합리적 데이터(예: 전혀 다른 맥락의 인간/로봇 동작을 섞은 데이터)가 생성되어 학습에 악영향을 줄 것으로 예상됩니다. 실제 이 baseline과의 비교는 “올바른 매핑”의 중요성을 부각합니다.\nImMimic-V: 제안 기법 중 시각 기반 매핑만을 사용한 변형입니다. 즉 DTW 정렬 시 로봇/인간 시각피처 유사도로 정렬하고 MixUp 보간하는 방식으로, 행동 기반 매핑(ImMimic-A)과의 성능 차이를 비교합니다. 이를 통해 어떤 매핑 기준이 더 유효한지 확인하였습니다.\n\n성과 (Results): 전반적으로 ImMimic은 모든 작업과 로봇에 걸쳐 기존 방법들을 능가하는 성공률을 달성했습니다. 논문 프로젝트 페이지에 공개된 결과에 따르면, Robot-Only 대비 ImMimic 적용시 성공률이 현저히 향상되었으며, Vanilla 공동학습 대비로도 큰 개선이 있었습니다. 예를 들어, Pick and Place 작업에서 5개 로봇 데모만으로 학습한 경우 성공률이 매우 낮았으나(ImMimic 미사용), ImMimic을 통해 100개의 인간 비디오를 활용하면 성공률이 의미 있게 상승하는 것으로 보고되었습니다. Push, Hammer, Flip 등 다른 작업들에서도 일관되게 향상된 성능을 보였는데, 특히 복잡한 조작일수록 인간 데이터 활용의 이득이 컸다고 유추할 수 있습니다. Hammer나 Flip은 난이도가 높아 로봇 데모 5개만으로 학습하기 어려운 반면, 인간 비디오로 다양한 사례를 학습한 ImMimic 정책은 이러한 작업에서도 상당한 성공률 개선을 이뤘을 가능성이 높습니다.\n비교 기법 간 결과를 살펴보면:\n\nVanilla Co-Training: 인간 데이터 추가에도 불구하고 도메인 차이로 인해 Robot-Only 대비 뚜렷한 개선을 못 내는 경우가 있었습니다. 일부 작업에서는 약간의 향상이 있었지만, 다른 작업에서는 인간 데이터가 제대로 활용되지 못해 성공률 정체 혹은 불안정한 학습을 보였습니다. 이는 도메인 갭을 해소하지 않고는 인간 비디오의 잠재력이 발휘되지 않음을 보여줍니다.\nRandom Mapping: 이 방법은 대체로 Vanilla 공동학습보다도 성능이 낮게 나왔습니다. 인간-로봇 대응이 엉뚱하게 이루어져 의미 없는 보간 데이터가 다수 생성되었고, 이로 인해 정책 학습이 혼란을 겪었을 것입니다. 결과적으로 Robot-Only보다도 못한 성공률을 보인 경우도 있을 것으로 예상됩니다. 이 비교를 통해, 인간-로봇 시연 간 올바른 시계열 정렬(DTW)의 중요성이 실증되었습니다 – 잘못 연결된 데이터는 오히려 독이 됨을 확인한 것입니다.\nImMimic-V vs ImMimic-A: 두 매핑 전략을 비교한 결과, 행동 기반 매핑(ImMimic-A)이 시각 기반 매핑(ImMimic-V)보다 일관되게 우수한 성능을 보였습니다. 논문에 따르면 ImMimic-A가 ImMimic-V보다 나은 성능을 발휘했는데, 이는 로봇 손/그리퍼의 구체적인 관절 행동 정보가 도메인 정렬에 더 효과적이었다는 뜻입니다. 시각 피처 기반 정렬도 어느 정도 효과는 있었지만, 완전히 다른 형태의 손/그리퍼 사이에서는 시각적 유사성이 곧 행동의 유사성으로 이어지지 않을 수 있습니다. 반면, retargeting을 통해 인간 손동작을 로봇 관절 공간으로 변환한 뒤 이를 직접 비교하면 보다 물리적으로 타당한 매칭을 얻을 수 있어 보입니다. 이 결과는 정밀한 행동 레벨의 대응이 도메인 갭 해소에 중요함을 시사합니다. (시각 기반 정렬은 환경 배경 등이 동일한 통제된 상황에서는 그럭저럭 동작했지만, 일반적으로는 행동 기반 정렬이 바람직하다는 결론입니다.)\n\n성공률 이외의 지표: 저자들은 정성적인 결과로서 ImMimic으로 학습한 로봇이 보다 인간과 유사한 동작 패턴을 보이며, 실행이 매끄럽다(smoother)고 언급합니다. 예를 들어, Robot-Only 정책의 경우 동작이 불안정하여 물체를 놓칠 수 있었던 반면, ImMimic 정책은 연속적이고 안정적인 제스처로 작업을 수행함을 확인했습니다. 이는 아마 모델의 행동 출력에 가해진 인간 시演의 영향으로, 세밀한 조절이나 힘 조절 면에서 향상된 결과일 것입니다. 실행의 매끄러움은 또 다른 관점에서는 정량화된 지표로도 측정했을 수 있는데, 예컨대 모션의 가속도 변화율(jerk)의 분산, 엔드-이펙터 궤적의 평탄함, 충격 없이 임무 완수 등을 평가했을 가능성이 있습니다. 이 부분에 대한 수치는 논문에 명확히 제시되진 않았지만, 전반적인 과제 성공 과정에서의 품질 개선으로 해석할 수 있습니다.\n또 다른 흥미로운 분석으로, 데모 데이터의 양과 다양성에 따른 성능 변화를 평가한 결과가 있습니다. 인간 시연의 수를 0, 50, 100, 200개로 달리해가며 ImMimic-A의 성능을 측정한 결과, 인간 데이터가 많을수록 성능이 향상되는 상향 곡선을 보였습니다. 이는 대규모 인간 데이터의 가치를 실증한 것으로, 충분한 다양한 시연을 확보하면 로봇 데모 몇 개만으로도 정책 성능을 크게 끌어올릴 수 있음을 의미합니다. 반대로 로봇 데모의 수를 1, 5, 20개로 변화시킨 실험에서도, 로봇 데모가 늘수록 성능 향상은 있지만 ImMimic을 적용한 경우 적은 로봇 데모로도 동일 수준을 달성하거나 더 높은 성능을 발휘함을 보였습니다. 예를 들어, ImMimic-A를 사용하면 로봇 데모 5개로 달성한 성능을 Robot-Only는 20개를 써야 겨우 달성하는 식의 결과가 나타난 것으로 추측됩니다. 이러한 샘플 효율성 개선은 ImMimic의 주된 목표 중 하나로서, 실제로 적은 로봇 데이터로도 높은 성능을 얻도록 해준다는 점이 확인되었습니다.\n요약하면, 실험 결과는 다음을 보여줍니다:\n\nImMimic이 도메인 갭을 효과적으로 완화하여 성공률 향상과 동작 품질 개선을 달성했다.\n정렬(DTW)과 보간(MixUp)이라는 두 구성 요소가 모두 중요하며, 하나라도 결여되면 성능이 크게 감소함을 밝혔다 (Random Mapping이나 Vanilla와 비교).\n행동 수준의 매핑이 시각적 매핑보다 현재 시나리오에서는 더 효과적이었다.\n인간 데이터는 많을수록 좋고, 로봇 데이터 의존도는 줄일 수 있다는 것을 증명하여, 향후 대규모 인간 시연 활용의 가능성을 보여주었다.\n\n이러한 결과들은, 인간 비디오로부터 로봇 학습을 끌어올리는 데 있어 ImMimic 접근법의 유용성을 뒷받침합니다. 특히 물리적으로 복잡한 조작이나 데이터 수집이 어려운 시나리오에서, 사람이 맨손으로 시연한 영상 몇 백 개와 로봇 데모 몇 개만 있으면 충분한 학습이 가능하다는 것은 매우 고무적입니다. 이는 실제 산업 또는 가정용 로봇 학습에 큰 잠재적 의미를 가집니다."
  },
  {
    "objectID": "posts/paper/2025-08-24-immimic.html#limitations-and-discussion",
    "href": "posts/paper/2025-08-24-immimic.html#limitations-and-discussion",
    "title": "📃ImMimic 리뷰",
    "section": "2.5 Limitations and Discussion",
    "text": "2.5 Limitations and Discussion\nImMimic은 혁신적인 접근법을 보여주지만, 한편으로 몇 가지 한계와 향후 보완점도 존재합니다:\n\n소량이나마 로봇 시연이 필요함: 본 방법은 완전한 zero-shot 학습은 아닙니다. 여전히 몇 개의 로봇 시연 데이터가 필요하며, 이는 실제 로봇을 조작해 수집해야 하는 부담이 있습니다. 논문에서는 5개의 로봇 데모로 충분하다고 주장하지만, 작업 난이도나 다양성에 따라 더 많은 로봇 데모가 필요할 수 있습니다. 만약 로봇 데모가 아예 0이라면(ImMimic에서 인간 데이터만으로 학습), retargeting된 인간 행동만으로 과연 로봇에 제대로 동작할 정책을 배울 수 있을지 미지수입니다. (일부 선행 연구에서는 로봇 데이터 없이 인간 비디오만으로도 학습하려는 시도가 있으나, 안정적 성능을 보장하기는 어려웠습니다.) 따라서 ImMimic은 few-shot 환경에서는 뛰어나지만, true one-shot/zero-shot 환경은 추가 연구가 필요합니다.\nretargeting 품질과 휴먼 demonstration의 한계: ImMimic의 전제는 인간 손 동작을 정확히 로봇 행동으로 변환할 수 있다는 것입니다. 그러나 영상에서 인간 손의 3D 자세를 추정하거나 이를 로봇 관절 움직임으로 옮기는 것은 오차 가능성이 있는 복잡한 과정입니다. 추적 오류나 매핑 오류가 있다면, 잘못된 행동 라벨이 인간 시연에 달리게 되고 이는 학습에 노이즈로 작용합니다. 특히 다섯 손가락을 가진 인간 손 → 두 손가락 그리퍼로 매핑할 때 정보 손실이 발생합니다. 인간의 섬세한 손놀림을 투핑거 그리퍼의 열고 닫는 값 하나로 축소해야 하는데, 이때 어떤 세부 동작은 표현되지 못하고 버려집니다. 이러한 경우 ImMimic이 인간 데이터로부터 충분한 정보를 얻지 못할 수 있습니다. 마찬가지로 다관절 로봇 손의 경우도 인간 손과 형태 차이가 있어, 관절 각도 매핑에 보정이 필요합니다. 논문에서 이 부분을 어떻게 처리했는지 구체적 언급은 없지만, 손가락 길이/비율 차이 등을 고려한 보정이 필요했을 것입니다. retargeting 모듈의 신뢰성이 전체 성능에 직접 영향을 주므로, 이 부분은 향후 더 개선된 휴먼-로봇 매핑 알고리즘(예: 최적 제어 방식으로 인간 동작을 로봇 모션으로 계산)이나 학습 기반 매핑으로 대체될 여지가 있습니다.\n시각적 도메인 차이: 본 실험은 인간 비디오와 로봇 시연이 동일한 환경(배경, 물체 등)에서 촬영되었기 때문에, 시각적 도메인 갭이 비교적 작았습니다. 그러나 일반적으로 인터넷에서 수집한 인간 시연 영상이나 다른 장소에서 찍은 영상은 배경, 조명, 물체 형태 등이 로봇 환경과 크게 다를 수 있습니다. ImMimic은 이러한 시각적 차이 자체를 적극적으로 다루지는 않았습니다 (예: 이미지 스타일 전환이나 도메인 랜덤화 등의 기법을 사용하지 않음). 대신 latent MixUp으로 어느 정도 중간 표현을 얻었지만, 시각 차이가 극심하면 인코더가 충분히 공통 특징을 뽑아내기 어려울 수 있습니다. 따라서 환경이 다른 인간 영상에도 본 기법이 효과적인지는 추가 검증이 필요합니다. 향후에는 시각 도메인 적응(visual domain adaptation)을 위한 모듈을 결합하거나, 합성 데이터로 로봇 시점으로 변환하는 기법과의 접목도 고려해볼 수 있습니다.\n정렬 가정의 한계: Dynamic Time Warping을 통해 인간과 로봇 시퀀스를 정렬하려면, 양쪽 시퀀스가 유사한 단계들의 연속으로 이루어져 있음을 전제로 합니다. 즉, 인간과 로봇이 같은 작업을 수행하며 시작과 끝 상태도 비슷해야 효과적으로 정렬될 수 있습니다. 논문에서도 인간/로봇 데모가 페어로 수집된 것은 아니지만, “유사한 상태에서는 유사한 동작을 할 것”이라는 가정을 두고 있습니다. 이 때문에 한 작업 내에서는 인간이든 로봇이든 비슷한 해결 전략을 따를 것을 암묵적으로 요구합니다. 만약 인간 시연 중 어떤 것은 로봇 데모와는 전혀 다른 순서나 방식으로 작업을 수행했다면, DTW가 엉뚱한 매칭을 만들거나 해당 데이터는 활용하기 어려웠을 것입니다. 그러므로 ImMimic은 현재 단일 작업 내에서 비교적 동질적인 시연들을 전제로 동작하며, 다양한 전략이 존재하는 작업이나 여러 작업이 섞인 시연에는 적용하기 힘든 한계가 있습니다. 또한 DTW 알고리즘은 쌍(pair) 단위 정렬이므로, 다수의 인간 시연과 다수의 로봇 시연을 사용할 때 어느 것을 어느 것과 정렬할지 짝짓기 문제가 발생합니다. 논문에서는 아마도 각 로봇 데모마다 몇 개의 인간 데모를 선택하여 정렬한 듯하며, 5개의 로봇 데모에 대해 100개의 인간 데모를 분배해 사용하는 식으로 처리했을 것입니다. 이러한 데모 매칭은 현재는 수작업 혹은 heuristic에 의존하지만, 규모가 더 커지면 자동으로 유사한 시연을 군집화/매칭하는 기법이 필요할 수 있습니다.\n정책의 일반화 범위: ImMimic으로 학습한 정책은 훈련된 작업들 내에서는 좋은 성능을 보이지만, 훈련되지 않은 새로운 작업에 바로 적용할 수는 없습니다. 즉 작업 간 일반화는 고려 대상이 아니었습니다. 만약 미래에 여러 작업의 인간 비디오와 로봇 데모를 모두 모아 한꺼번에 학습한다면, 이는 멀티태스크 학습 문제가 되어 새로운 도전이 필요합니다. 또한 ImMimic 정책은 오프라인 시연 데이터를 모방하는 것이므로, 만약 새로운 상황이 주어지거나 작업 도중 예기치 않은 변화(물체 미끄러짐 등)가 발생하면 대처가 어려울 수 있습니다. 이는 모방학습 전반의 한계로, 필요하면 추가적 강화학습 파인튜닝이나 휴먼 피드백 등을 결합해 극복해야 할 것입니다.\n실시간성 및 계산 비용: ImMimic은 학습 시 모든 인간-로봇 데이터 쌍에 대한 DTW 계산과 대량의 MixUp 샘플 생성이 필요하므로, 전처리 비용이 다소 큽니다. 그러나 이는 오프라인 단계이므로 큰 문제는 아니지만, 향후 데이터 양이 매우 늘어나면 DTW의 계산 복잡도가 병목이 될 수 있습니다. 또한 정책 자체가 Diffusion 모델을 활용했다면, 추론에 시간이 오래 걸릴 가능성이 있습니다. Diffusion 기반 정책은 보통 다수의 샘플링 스텝을 거쳐 행동을 생성하므로, 실시간 로봇 제어에 사용하려면 속도 최적화나 모델 경량화가 필요할 수 있습니다. (물론 짧은 horizon의 행동만 예측하거나 네트워크 최적화로 일정 수준 속도를 확보할 수는 있지만, 일반적인 피드포워드 네트워크보다는 무거울 수 있습니다.) 따라서 실시간 로봇제어 적용 측면에서의 검토도 추후 보완점입니다.\n\n요약하면, ImMimic은 현재 단일 작업, 제한된 환경에서 탁월한 성능을 보이지만, retargeting 정확도, 다양한 전략 존재 시 정렬, 시각 도메인 큰 차이, 새 작업 일반화, 실시간성 등의 면에서 추가 연구의 여지가 있습니다. 이러한 한계들은 이 방법의 적용 범위를 결정짓는 요소이며, 향후 연구 커뮤니티가 해결해야 할 도전과제로 남아 있습니다."
  },
  {
    "objectID": "posts/paper/2025-08-24-immimic.html#conclusion-and-future-work",
    "href": "posts/paper/2025-08-24-immimic.html#conclusion-and-future-work",
    "title": "📃ImMimic 리뷰",
    "section": "2.6 Conclusion and Future Work",
    "text": "2.6 Conclusion and Future Work\n이 논문은 “인간 영상으로부터 배우는 로봇”이라는 오랜 과제에 대해 새로운 관점을 제시했습니다. 핵심은 인간과 로봇의 간극을 데이터 차원에서 메꾸는 것으로, Dynamic Time Warping 기반 정렬과 MixUp 보간이라는 비교적 간단하지만 효과적인 도구를 조합해 부드러운 도메인 전이를 구현한 점이 돋보입니다. ImMimic 프레임워크를 통해, 소수의 로봇 데모만으로도 대량의 인간 시연에서 배운 풍부한 정보를 로봇 정책에 이식할 수 있었고, 이를 실제 로봇 실험으로 입증하였습니다. 특히 다양한 로봇 Embodiment(그리퍼부터 다지 손까지)에 적용하여 모두 성능 개선을 얻음으로써, 이 방법의 일반성과 실용적 가치를 보여주었습니다.\n이 연구의 이론적/기술적 기여는 인간-로봇 시연 사이의 공통 표현(action label)을 찾고자 한 점과, 두 도메인을 잇는 연속적 데이터 스펙트럼을 형성했다는 점입니다. 이는 기존에 주로 시도되던 표현 학습 + 도메인 적응 모델(예: 도메인 분류자나 adversarial training)과는 다른 방향으로, 데이터 보강을 통해 문제를 푼 접근이라 할 수 있습니다. 이러한 관점의 전환은 향후 다른 모방학습 문제, 예컨대 시뮬레이션→실세계 도메인 전이나 로봇 간 교차학습 등에도 응용될 수 있을 것입니다.\n앞으로의 연구 방향으로는 다음과 같은 확장이 가능해 보입니다:\n\n완전한 인간 데이터 학습으로의 확장: 궁극적으로는 로봇 시연이 전혀 없어도 인간 영상만으로 로봇이 학습하는 것을 목표로 할 수 있습니다. ImMimic은 소량이나마 로봇 데이터를 필요로 했는데, 이를 없애기 위해서는 시뮬레이터 활용이나 자가기반 학습 등이 필요할 수 있습니다. 예컨대, 초기에는 인간 비디오로 학습하고 시뮬레이터에서 검증/보정하거나, 현실에서 안전한 한도 내에서 로봇이 자체 시행착오를 통해 인간 동작을 보정하는 방식을 생각해볼 수 있습니다. 이러한 방향은 추가적인 강화학습이나 도메인 랜덤화 기법과의 융합으로 이어질 수 있습니다.\n자동화된 시퀀스 매핑 기법: 현재 DTW를 사용한 정렬은 두 시퀀스 간 국소적 피처 거리 합을 최소화하는 방식입니다. 향후에는 배우-비주얼 트랜스포머나 시퀀스-to-시퀀스 매핑 신경망을 훈련시켜, 인간 시연을 입력하면 해당 로봇 시연(또는 그 경로)을 직접 예측하도록 하는 방법도 고려할 수 있습니다. 즉, 지도 학습으로 인간→로봇 시퀀스 매핑을 학습시켜 DTW를 대체하는 것입니다. 이렇게 하면 다수의 시연을 동시에 정렬하거나, 부분적으로 겹치는 시연도 처리할 수 있을 것입니다. 다만 이를 위해서는 일정량의 매칭된 데이터가 필요하므로, 초기에는 ImMimic처럼 DTW로 생성한 쌍을 학습시키고 점차 정교화하는 식의 접근이 생각됩니다.\n다중작업 및 일반화: 본 연구를 여러 작업 및 환경으로 확대하면, 진정한 범용 학습 프레임워크로 발전시킬 수 있습니다. 예를 들어 가정 내 여러 가지 작업(요리, 청소, 정리 등)에 대한 인간 영상과 몇 가지 로봇 데모를 모아 통합 학습한다면, 로봇이 다양한 작업을 인간처럼 배울 수 있을 것입니다. 이를 위해서는 작업 구분 없이 학습할 수 있는 거대 모델이나, 맥락에 따른 행동 생성을 위한 추가 입력(예: 작업 명령이나 목표 정보) 등이 필요할 것입니다. 또한 멀티태스크 환경에서는 작업 간 간섭 문제가 생길 수 있으므로, 모델 아키텍처의 개선(예: 모듈식 정책)도 연구해야 합니다.\n시각적 도메인 적응 통합: ImMimic이 latent MixUp으로 간접적으로 시각 도메인 차이를 완화했지만, 보다 직접적으로 영상 간 변환을 하는 방안도 고안할 수 있습니다. 예컨대 영상-to-영상 변환 모델을 사용해 인간 영상 속 장면을 로봇 시점의 장면으로 스타일 변환하거나, 생성 모델을 활용해 인간 시연 영상을 입력하면 로봇이 등장하는 모사 영상으로 변환하는 것도 한 방향입니다. 최근에는 영상 조건 생성이나 NeRF 기반 시각변환 기술도 발전했으므로, 이러한 것을 ImMimic과 결합하면 시각+행동 양측의 도메인 갭을 모두 줄일 수 있을 것입니다.\nretargeting 및 센서 융합 고도화: 손동작 retargeting을 더 정확히 하기 위해, 웨어러블 센서나 모션 캡처 데이터를 영상과 함께 사용하는 것도 고려됩니다. 예를 들어 인간이 장갑형 센서를 끼고 시연하여 손가락 관절각을 직접 측정하면, 영상 추정보다 훨씬 정확한 retargeting이 가능합니다. 물론 데이터 수집 비용은 올라가지만, 만약 양질의 매핑이 가능하다면 소량의 데이터로도 큰 효과를 볼 수 있습니다. 또한 힘/촉각 정보 등도 로봇 데모에서는 얻을 수 있으므로, 인간 시연에서는 힘 동작을 추정하여 로봇의 힘 제어 라벨로 활용하는 등 다중모달 확장이 가능할 것입니다.\n실제 응용 및 검증: 끝으로, 이 알고리즘을 현실의 새로운 작업 시나리오에 적용해보는 연구가 필요합니다. 예컨대 산업현장에서 인간 작업자들의 영상으로부터 조립 작업을 학습하거나, 재활 치료 로봇이 치료사의 시연으로부터 동작을 배우는 식의 응용을 상정해볼 수 있습니다. 이러한 도메인에서는 환경 변화나 안전 제약 등이 있을 수 있으므로, ImMimic에 안전장치(safety layer)나 적응 제어를 부가하는 연구도 뒤따라야 할 것입니다.\n\n결론적으로, ImMimic: Cross-Domain Imitation from Human Videos via Mapping and Interpolation은 인간 비디오를 로봇 학습에 활용하는 분야에서 의미 있는 성능 향상과 새로운 방향성을 제시한 작품입니다. 간단한 아이디어 조합으로도 큰 효과를 거둘 수 있음을 보여주었고, 향후 이를 토대로 다양한 발전형 연구가 이루어질 것으로 기대됩니다. 인간과 로봇의 격차를 좁혀 “로봇이 인간처럼 배운다”는 목표에 한 걸음 다가서게 한 본 논문의 기여는, 로봇 학습 커뮤니티에서 주목할 만한 이정표로 평가될 만합니다."
  },
  {
    "objectID": "posts/paper/2025-06-12-batch-online-rl.html",
    "href": "posts/paper/2025-06-12-batch-online-rl.html",
    "title": "📃Batch Online RL 리뷰",
    "section": "",
    "text": "Paper Link"
  },
  {
    "objectID": "posts/paper/2025-06-12-batch-online-rl.html#연구-동기-및-문제-설정",
    "href": "posts/paper/2025-06-12-batch-online-rl.html#연구-동기-및-문제-설정",
    "title": "📃Batch Online RL 리뷰",
    "section": "연구 동기 및 문제 설정",
    "text": "연구 동기 및 문제 설정\n현대 딥러닝은 대규모 데이터 활용을 통해 발전해왔지만, 로봇 공학 분야는 현실적으로 사용할 수 있는 데이터의 양이 현저히 부족합니다. 로봇에게 일일이 사람이 시연하여 데이터를 모으는 것은 큰 노력과 시간이 들기 때문에, 사람의 개입을 최소화하면서 로봇이 스스로 학습 데이터를 모으고 성능을 향상시키는 방법에 대한 관심이 높습니다. 이를 위해 과거에는 사람이 제공한 시연 데이터를 모방하는 모방학습(Imitation Learning, IL)을 주로 활용했지만, 이 방식은 주어진 데이터 범위 이상으로 성능을 끌어올리는 데 한계가 있습니다.\n강화학습(RL)을 적용하면 로봇이 스스로 시행착오를 겪으며 향상될 수 있다는 점에서 매력적이지만, 온라인 RL(online RL), 즉 로봇이 실시간 상호작용하며 동시에 학습을 진행하는 방법은 현실 로봇에게 적용하기에 어려움이 많습니다. 예를 들어, 학습 도중 로봇이 계속 움직이며 학습해야 하므로 물리적 피로도나 안전 문제, 그리고 학습 과정에서 초기 정책이 망가지는 분포 이동(distribution shift) 문제가 나타날 수 있습니다.\n이에 대한 중간 해결책(middle ground)으로 제시된 개념이 “배치 온라인 RL”(batch online RL)입니다. 이 방법에서는 초기에는 오프라인 데이터로 정책을 학습시키고 (예: 시연 데이터로 초기 정책 \\pi\\_0 학습), 이 정책을 로봇에 배치하여 일정량의 경험을 한꺼번에 수집한 뒤, 로봇을 멈추고 모은 데이터를 오프라인으로 학습하여 정책을 갱신합니다. 이렇게 새 정책 \\pi\\_1을 얻으면 다시 로봇을 사용해 자율적으로 데이터를 모으고(rollout), 축적된 데이터셋으로 다시 학습을 진행하는 과정을 반복합니다. 요컨대, 실험 환경에서 정책의 실행(데이터 수집)과 정책의 학습(업데이트) 단계를 교대로 배치 단위로 수행함으로써, 온라인 RL의 자기 향상 능력을 유지하면서도 실제 배치 시에는 학습을 하지 않으므로 안정성과 효율성을 높인 접근입니다. 이러한 Batch online RL은 스스로 모은 대량의 데이터로 정책을 향상시킬 수 있어 잠재적으로 대규모 로봇 학습을 가능하게 할 것으로 기대됩니다.\n하지만 배치 온라인 RL을 실제로 성공적으로 구현하는 데에는 여전히 도전과제가 있습니다. 과거 일부 연구들은 배치 온라인 RL 환경에 모방학습이나 변형된 모방학습(filtered IL) 알고리즘을 적용해 보았으나, 자율 수집한 데이터로부터 효과적으로 성능을 높이지 못하거나 금세 어느 수준에서 성능이 정체되는 문제가 보고되었습니다. 이에 따라 이 논문에서는 “로봇 배치 온라인 강화학습에서 무엇이 성능 향상을 결정짓는 핵심 요소인가?”라는 질문을 제기하고, 체계적인 실험을 통해 답을 찾고자 합니다."
  },
  {
    "objectID": "posts/paper/2025-06-12-batch-online-rl.html#기존-강화학습-방식과의-차별점",
    "href": "posts/paper/2025-06-12-batch-online-rl.html#기존-강화학습-방식과의-차별점",
    "title": "📃Batch Online RL 리뷰",
    "section": "기존 강화학습 방식과의 차별점",
    "text": "기존 강화학습 방식과의 차별점\n배치 온라인 RL은 기존의 오프라인 RL 및 온라인 RL과 비교해 몇 가지 측면에서 차별화됩니다.\n\n온라인 RL 대비: 전통적인 온라인 RL에서는 로봇이 환경과 상호작용하며 즉시 학습을 업데이트하지만, 배치 온라인 RL에서는 로봇이 데이터만 수집하고 학습은 나중에 오프라인으로 진행됩니다. 이로 인해 온라인 RL에서 문제가 되었던, 학습 중 실제 로봇을 지속 운용해야 하는 부담과 훈련 과정에서의 불안정성을 줄일 수 있습니다. 논문에서도 “배치 온라인 RL은 정책 훈련과 데이터 수집을 분리함으로써 온라인 RL의 복잡한 문제 없이도 자체 롤아웃 데이터를 활용한 향상을 가능케 한다”고 설명합니다. 또한 온라인으로 바로 미세조정(fine-tuning)하는 경우 나타날 수 있는 분포 차이로 인한 성능 붕괴나 초기 정책의 망각 현상을 배치 방식은 완화해 줍니다.\n오프라인 RL 대비: 오프라인 RL은 고정된 정적 데이터셋을 가지고 한 번에 학습을 끝내는 반면, 배치 온라인 RL에서는 새로운 데이터를 지속적으로 추가하며 여러 단계에 걸쳐 정책을 개선합니다. 즉, 오프라인 RL이 초기 주어진 데이터 품질에 전적으로 의존하는 데 반해, 배치 온라인 RL은 정책이 개선됨에 따라 새로운 (잠재적으로 더 나은) 데이터를 모아 학습함으로써 성능을 점진적으로 높일 수 있습니다. 한편 배치 온라인 RL도 오프라인 RL처럼 훈련과 데이터 수집이 분리된 상태에서 학습하므로, 순수 온라인 RL보다 안정적으로 정책을 향상시킬 수 있다는 장점이 있습니다.\n\n정리하면, 배치 온라인 RL은 오프라인과 온라인의 절충 방식으로, 자율주행 데이터 수집의 이점과 오프라인 학습의 안정성을 결합한 프레임워크입니다. 이 접근은 로봇이 스스로 데이터를 모으면서도, 모은 데이터로 학습을 별도로 수행함으로써 실시간 학습의 부담 없이도 자기 개선을 달성하려는 것입니다."
  },
  {
    "objectID": "posts/paper/2025-06-12-batch-online-rl.html#제안된-접근-방식-및-프레임워크-설명",
    "href": "posts/paper/2025-06-12-batch-online-rl.html#제안된-접근-방식-및-프레임워크-설명",
    "title": "📃Batch Online RL 리뷰",
    "section": "제안된 접근 방식 및 프레임워크 설명",
    "text": "제안된 접근 방식 및 프레임워크 설명\n이 논문에서는 배치 온라인 RL 문제에서 어떤 접근이 가장 효과적인지를 실험적으로 규명한 뒤, 그 결과를 토대로 일반적인 해법(recipe)을 제안합니다. 연구진은 배치 온라인 RL에 적용될 수 있는 방법들을 세 가지 축으로 분류하여 실험하였는데, 그 축은 (i) 알고리즘 유형, (ii) 정책 추출 방법, (iii) 정책의 표현력입니다. 각 축에서 어떤 선택을 하느냐가 성능에 큰 영향을 미친다는 것을 확인한 후, 최적의 조합을 일반 원칙으로 정리한 것이 이 논문의 핵심 제안입니다.\n① 알고리즘 클래스: 먼저 정책을 개선하는 알고리즘으로 세 가지를 비교했습니다. 하나는 모방학습(IL) 기반으로, 수집된 데이터를 행동클로닝(Behavior Cloning) 방식으로 학습하는 가장 단순한 방법입니다. 다른 하나는 필터링된 모방학습(filtered IL)으로, 자율주행으로 모은 데이터 중 성공적이거나 양질의 부분만 선별하여 모방학습에 사용하는 기법입니다. 마지막은 가치 기반 강화학습(value-based RL)으로, 수집 데이터에서 보상신호를 활용하여 Q-함수 등 가치함수를 학습하고 이를 통해 정책을 업데이트하는 방법입니다. 이 때 가치 기반 방법은 실패 사례 등도 학습에 활용하고 Bellman 업데이트를 수행하기 때문에, 오직 성공 사례에 의존하는 IL보다 잠재적으로 더 풍부한 학습이 가능합니다.\n② 정책 추출 방법: 다음으로, 가치 기반 RL을 쓸 경우 정책을 얻는 방식을 두 가지 고찰했습니다. 명시적 정책 추출(explicit policy extraction)은 오프라인 RL에서 흔히 쓰이는 접근으로, Q함수를 최대화하면서도 행동 분포가 기존 데이터에서 크게 벗어나지 않도록 정책을 학습하는 기법입니다. 예를 들면 Advantage-Weighted Regression (AWR) 같은 알고리즘이 이에 해당하며, Q함수의 신호를 정책 학습에 직접 반영하지만 동시에 행동클로닝 제약을 줘서 기존 데이터 분포와의 거리를 유지하게 합니다. 반면 암시적 정책 추출(implicit policy extraction)은 정책을 별도로 최적화하지 않고, 실행 시에 Q함수를 이용해 최선의 행동을 고르는 방식입니다. 구체적으로는 매 스텝 상태에서 현재 정책으로 여러 후보 행동을 샘플링한 후, Q값이 가장 높은 행동을 실제 실행에 선택하는 방식입니다. 암시적 방법은 Q함수의 정보를 정책 파라미터 학습에 직접 반영하지 않기 때문에 학습이 더 안정적일 수 있지만, 그 대신 Q함수의 유용한 신호를 정책 업데이트에 활용하지 못한다는 trade-off가 있습니다.\n③ 정책의 표현력(Expressivity): 마지막으로 정책 모델의 클래스에 따른 표현력 차이를 실험했습니다. 일반적으로 강화학습에서 많이 쓰이는 정책 모델은 가우시안 정책으로, 상태를 입력받아 행동의 평균과 분산을 출력하고 정규분포로 행동을 샘플링하는 비교적 단순한 형태입니다. 이런 가우시안 정책은 경험이 풍부하지 않은 초기 단계에서는 안정적이고 추론 속도가 빠른 장점이 있지만, 복잡한 여러 모달리티의 행동분포를 표현하기에는 한계가 있습니다. 이에 비해 논문에서 표현력이 높다고 지칭하는 정책은 디퓨전 모델 기반 정책(diffusion-based policy)으로, 마르코프 확률과 노이징/디노이징 과정을 활용하여 주어진 데이터를 생성 모델 형태로 학습하는 방식입니다. 디퓨전 정책은 다중 모달의 행동 분포를 더 잘 모델링할 수 있고, 특히 암시적 정책 추출 방식과도 잘 맞는 특성이 있습니다 (정책 자체가 다양한 후보 행동을 샘플링해낼 수 있으므로, 거기서 Q값이 높은 것을 고르는 전략과 시너지가 좋습니다).\n이상 세 가지 축에서 찾은 시사점을 바탕으로, 저자들은 배치 온라인 RL을 효과적으로 수행하기 위한 일반 해법(recipe)을 제안합니다:\n\n가장 첫 단계로 표현력이 높은 정책 모델을 IL(모방학습)로 초기 학습하여 배치 온라인 RL의 배우(행동자)로 사용합니다. (논문에서는 Diffusion 기반 정책네트워크를 시演 데이터로 학습하여 사용했습니다.)\n병행해서, 동일한 자율수집 데이터에 대해 Q-함수를 학습합니다 (논문 구현에서는 IQL 알고리즘, 즉 오프라인 RL 방식으로 Q를 업데이트).\n정책을 업데이트할 때는 암시적 정책 추출 방식을 활용합니다. 즉, 새로운 데이터를 모을 때 방금 학습한 Q함수를 이용해 현재 정책에서 샘플한 여러 행동 중 Q값이 가장 높은 행동을 실행하도록 합니다. 이렇게 하면 명시적으로 정책을 업데이트하지 않고도 Q함수가 높은 방향으로 행동이 유도되어 정책 향상이 이뤄집니다.\n위 과정에서 필요한 경우 정책의 탐색을 돕기 위해 시간적으로 상관된 노이즈(Ornstein–Uhlenbeck 과정 등을 통한 노이즈)를 행동에 추가로 주입할 수 있습니다. 이는 로봇이 좀 더 다양한 시도를 해보도록 유도하여 데이터 다양성을 높이기 위함입니다.\n\n이러한 레시피를 요약하면, “표현력 높은 정책 + Q함수 기반 가치학습 + 암시적 정책 추출”의 조합으로 정책을 반복 개선하는 것입니다. 논문에서는 이 조합을 IDQL (Implicit Diffusion Q-Learning) 알고리즘으로 구현했으며, 이는 배치 온라인 RL 1회 반복에 해당하는 알고리즘으로 볼 수 있습니다. 전체 배치 온라인 RL 프로세스에서는 이 과정을 여러 iteration 수행하면서 정책을 향상시킵니다. 마지막으로, 저자들은 레시피의 실용적 보완책으로 행동 노이즈 주입을 제안하였는데, 실험 결과 약간의 노이즈 추가는 적은 데이터 환경에서 성능 향상을 가져오지만 데이터가 충분히 많아지면 자연스럽게 탐색 다양성이 확보되기 때문에 노이즈의 효과는 제한적임을 확인했습니다. 즉, 노이즈 추가는 선택 사항이지 필수 요소는 아니며, 다만 데이터가 부족한 초기 단계에서는 소량의 노이즈로도 성능을 높이는 보탬이 될 수 있습니다."
  },
  {
    "objectID": "posts/paper/2025-06-12-batch-online-rl.html#실험-환경-및-평가-지표",
    "href": "posts/paper/2025-06-12-batch-online-rl.html#실험-환경-및-평가-지표",
    "title": "📃Batch Online RL 리뷰",
    "section": "실험 환경 및 평가 지표",
    "text": "실험 환경 및 평가 지표\n저자들은 제안한 접근과 다른 대안들을 여러 시뮬레이션 환경과 실제 로봇 작업에 걸쳐 평가했습니다. 시뮬레이션 실험으로는 총 6개의 복잡한 로봇 조작 과제를 선택했는데, 구체적으로 RoboMimic 데이터셋의 Lift, Can, Square 작업, MimicGen의 Stack 및 Threading 작업, 그리고 Adroit의 Pen 작업입니다. 이들 과제는 연속적 제어가 필요한 까다로운 작업들로, 난이도에 따라 초기 시 데이터(D₀)의 크기를 5개에서 최대 100개까지 제공하여 초기 정책 \\pi_0의 성공률이 약 30~65% 수준이 되도록 설정했습니다. 이렇게 비교적 미완성된 초기 성능에서 시작함으로써, 향후 스스로 개선할 여지가 충분한 현실적인 시나리오를 만들었습니다. 각 과제마다 배치 온라인 RL의 반복(iteration)을 10~20회 수행하며, 매 iteration마다 200개의 에피소드(rollouts)를 수집하도록 설정했습니다. 학습 중 성능 평가를 위해 각 반복 단계마다 정책을 고정시키고 여러 에피소드를 실행하여 얻은 누적 보상(returns)이나 성공률을 측정하였으며, 이러한 지표를 초기 성능 대비 정규화(normalize)하여 비교하였습니다. (그래프 상에서 100%는 과제 성공률 100% 또는 주어진 목표 대비 최고의 성능으로 정규화되어 있습니다.)\n한편 실제 로봇 실험으로는 비전 기반의 로봇 조작 작업을 하나 선정했습니다. 7자유도 Franka 암 로봇으로 테이프 롤을 집어서 고리에 거는 작업을 수행했으며, 로봇 손목 카메라와 외부 카메라에서 얻은 RGB 영상 및 로봇의 관절각/엔드이펙터 상태 등의 proprioceptive 정보를 입력으로 사용했습니다. 이 작업은 초기 정책으로 성공하기 어렵도록 설계된 어려운 과제로, 배치 온라인 RL을 통한 성능 개선 여부를 확인하기에 적합합니다."
  },
  {
    "objectID": "posts/paper/2025-06-12-batch-online-rl.html#주요-실험-결과와-분석",
    "href": "posts/paper/2025-06-12-batch-online-rl.html#주요-실험-결과와-분석",
    "title": "📃Batch Online RL 리뷰",
    "section": "주요 실험 결과와 분석",
    "text": "주요 실험 결과와 분석\n \n\n\n\nFigure: 배치 온라인 RL에서 서로 다른 알고리즘 클래스의 성능 비교(여러 작업에서 반복을 거듭하며 얻은 평균 누적 성과)\n\n\n\n주황색 곡선은 가치 기반 RL 방법, 회색은 필터링된 IL, 연한 회색은 기본 IL을 나타냅니다. 가치 기반 RL 방법이 대부분의 작업에서 반복이 진행될수록 가장 높은 성능 향상을 보이고 있으며, IL 기반 방법들은 초기 성능에서 크게 개선되지 못하거나 일부 향상 후 정체되는 양상을 보입니다.\n\n1) 알고리즘 클래스 비교: 가치 기반 RL 방법이 두드러지게 우수한 성능을 보였습니다. 위 그래프에서 보이듯 IL(모방학습)만으로 학습하는 경우 자율적으로 모은 실패 데이터까지 그대로 모방해버리기 때문에 오히려 성능이 떨어지거나 거의 향상되지 못했습니다. 필터링된 IL의 경우 초기에는 약간의 개선을 보였지만, 곧 아직 최적이 아닌 성능 수준에서 정체되어 더 이상 나아지지 못하는 경향이 있습니다. 반면 가치 기반 RL은 보상 신호를 통해 성공/실패 사례를 모두 학습에 활용함으로써, iteration을 거듭할수록 지속적인 성능 향상을 이루었고 최종적으로 가장 높은 성과를 달성했습니다. 이처럼 RL 방법이 우수한 이유는, 자율 수집한 데이터에 내재한 다양한 상황과 실패 사례로부터도 교훈을 얻어 정책을 수정할 수 있기 때문입니다. 실제로 논문에서는 RL 기법이 배치 데이터를 더 다양하게 활용하여 IL에 비해 정책 탐색 범위를 넓힌다고 분석합니다. 예를 들어, 성공한 경로들의 상태 방문 분포를 시각화한 결과 RL 알고리즘은 IL에 비해 훨씬 폭넓은 상태 공간을 커버하고 있었는데, 이는 RL이 다양한 시도를 통해 새로운 성공 경로를 개척한 반면 IL은 주어진 시演 궤적 주변만 반복했음을 보여줍니다. 또한 데이터 규모를 확장했을 때도 RL의 강점이 확인되었습니다. 배치당 수집하는 에피소드 수(M)를 소/중/대로 늘려가며 성능을 비교한 결과, 가치 기반 RL은 데이터가 많아질수록 성능이 꾸준히 향상된 반면 IL 기반 방법들은 일정 수준 이상에서 추가 데이터가 효과를 발휘하지 못하고 정체되었습니다. 즉 배치 온라인 RL에서 더 큰 데이터로 스케일하려면, IL로는 한계가 있고 RL 알고리즘이 필수적임을 알 수 있습니다. 다만 저자들은 “가치 기반 RL만으로 충분하지 않다”고 강조하는데, 이는 아래의 다른 요소들(정책 추출 방식, 정책 표현력)도 적절히 선택해야 진정한 효과를 볼 수 있다는 의미입니다.\n2) 정책 추출 방법 비교: 명시적 vs. 암시적 두 방식의 성능 차이도 뚜렷했습니다. 실험에 따르면 암시적 정책 추출이 반복 학습 후의 최종 성능에서 항상 우월했습니다. 흥미로운 점은, 학습 초반에는 명시적 방법(AWR 등으로 정책을 함께 최적화한 경우)이 다소 높은 성능으로 시작하는 경향이 있었으나, 여러 iteration을 거친 후에는 암시적 방법이 훨씬 더 크게 성능을 끌어올렸다는 것입니다. 명시적으로 정책을 업데이트하는 방법은 Q함수의 신호를 직접 활용한다는 장점 때문에 초기에 빠르게 성능을 올릴 수 있지만, 새로운 자율 데이터가 누적되면서 기존 행동 분포와 달라지는 변화에 적응하지 못하고 발목을 잡히는 것으로 해석됩니다. 실제로 논문에서는, 명시적 방법에서는 매 iteration마다 정책 학습 시 이전 데이터 분포에 묶이는 제약이 존재하여 새로운 데이터 분포로의 이동이 어렵다고 지적합니다. 그 결과 배치 온라인 RL 과정에서 데이터 분포가 다양해질수록 명시적 방법의 정책은 이를 제대로 따라가지 못해 성능이 저하되는 반면, 암시적 방법은 Q함수와 정책 학습을 분리하여 이러한 분포 변화에 유연하게 대응할 수 있기 때문에 안정적으로 성능 개선을 이루는 것으로 나타났습니다. 결론적으로 배치 단위 자기학습 시에는 암시적 정책 추출이 일관되게 효과적이며, 이는 오프라인 RL 방식의 정책 학습이 항상 최선은 아닐 수 있음을 시사합니다.\n3) 정책 표현력(모델) 비교: 정책 모델의 표현력도 결과에 큰 영향을 미쳤습니다. 비교 실험에서는 표현력이 낮은 가우시안 정책과 표현력이 높은 디퓨전 정책을 각각 사용해 보았습니다. 먼저 두 정책을 각각 최적의 방식으로 조합했을 때 (가우시안+명시적, 디퓨전+암시적) 결과는 명확했습니다: 디퓨전 기반의 고표현력 정책이 모든 작업에서 일관되게 가우시안 정책보다 뛰어난 최종 성능을 보였습니다. 특히 배치 RL 반복 전후의 성능 향상폭을 보면, 가우시안 정책은 개선 폭이 제한적인 반면 디퓨전 정책은 크게 향상하여 최종 성능 격차가 벌어졌습니다. 이는 고차원 복잡한 행동 분포를 모델링할 수 있는 능력이 있을 때 배치 RL 과정에서 더 다양한 시도를 하고 학습할 수 있음을 보여줍니다. 흥미로운 점으로, 저자들은 앞의 정책 추출 실험과 연관지어 “표현력 높은 정책은 암시적 추출과 특히 궁합이 좋다”고 분석합니다. 추가로 가우시안 정책에도 암시적 추출을 적용해보는 대조 실험을 했는데, 이 경우 성능이 어느 정도 개선되었지만 여전히 디퓨전 정책 조합에는 못 미쳤습니다. 결국 표현력 그 자체의 차이가 성능 격차의 중요한 원인이라는 것입니다. 왜 온라인 RL에서는 가우시안 정책으로도 잘 되는데 배치 RL에서는 부족한가? 라는 질문에 대해, 논문에서는 “온라인 RL에서는 정책이 매 스텝 업데이트되므로 초기 분포를 잘 모델링하지 않아도 새로운 행동을 계속 발굴하지만, 배치 RL에서는 각 iteration 동안 정책이 고정되므로 초기 정책이 충분한 다양성을 가지지 못하면 새 데이터를 모으기 어렵다”고 설명합니다. 즉 배치 온라인 RL에서는 초기에 정책의 표현력이 높아야만 충분히 탐색적인 데이터 수집이 이루어지고, 이는 곧 향상으로 이어진다는 통찰입니다.\n이상의 결과들을 종합하면, 배치 온라인 RL에서 성능을 극대화하기 위한 핵심 요소는\n(a) 가치 기반 RL 알고리즘의 활용,\n(b) 암시적 정책 추출 방식,\n(c) 표현력 높은 정책 모델 사용으로 요약됩니다.\n저자들은 이 세 가지를 모두 만족하는 조합(앞서 설명한 레시피)을 사용했을 때 기존 방법들 대비 크게 향상된 성능과 데이터 스케일링 효과를 거두었음을 보고합니다. 또한 추가로 제안된 행동 노이즈 주입은 초기 학습 단계에서 성과를 약간 높여주는 실용적 트릭으로 작용했지만, 핵심은 아니었다고 언급됩니다. 결국 이 연구는 어떤 방법으로 로봇이 스스로 모은 데이터로 학습해야 효율적인가에 대한 경험적 해답을 제시한 것입니다."
  },
  {
    "objectID": "posts/paper/2025-06-12-batch-online-rl.html#논문에서-제안하는-실질적인-로봇-적용-시-고려사항",
    "href": "posts/paper/2025-06-12-batch-online-rl.html#논문에서-제안하는-실질적인-로봇-적용-시-고려사항",
    "title": "📃Batch Online RL 리뷰",
    "section": "논문에서 제안하는 실질적인 로봇 적용 시 고려사항",
    "text": "논문에서 제안하는 실질적인 로봇 적용 시 고려사항\n배치 온라인 RL 레시피의 실용성은 실제 로봇 실험을 통해 검증되었습니다. 저자들은 프랑카 암(arm) 로봇에게 테이프 걸기 작업을 학습시키는 실험을 3회 반복하여, 앞서 도출한 방법이 현실 로봇에도 통하는지 확인했습니다. 이때 고려해야 할 실질적 요소 몇 가지가 있습니다:\n\n\n\nFigure: 실험에 사용된 실제 로봇 및 작업 예시.\n\n\n\n7자유도 로봇 암이 초기에는 집게에 테이프 롤을 쥐고 목표물 앞에 위치합니다 (좌측). 이후 정책이 개선되면 로봇이 테이프를 정확히 갈고리 모양의 걸이(hook)에 거는 최종 동작(우측)을 성공적으로 수행합니다. 이 과제는 컴퓨터 비전(RGB 이미지 입력)과 정교한 동작 제어가 모두 필요한 어려운 작업으로, 배치 온라인 RL의 효과를 시험하기 위한 현실적인 시나리오입니다.\n\n로봇 실험에서는 초기 시연을 단 5개만 제공하여 시작한 후, N=3회의 batch 자율학습 반복, 매회 30개의 롤아웃을 수집하였습니다. 실험 결과는 고무적이었는데, 제안된 레시피를 따른 경우 단 3번의 반복만에 초기 정책 대비 성공률 30%p 상승이라는 유의미한 향상을 달성했습니다. 아래 그래프의 녹색 선(Recipe)이 보여주듯, 약 45% 정도이던 초기 성공률이 세 번의 학습을 거쳐 약 70%까지 상승했습니다. 반면 회색 선(Filtered-IL)으로 표시된 필터링 모방학습 방법은 초기에 60% 수준에서 시작했으나 추가 향상이 거의 없었으며, 자주색 점선(Steering)으로 표시된 비교 기준 방법(기존 연구 [11]의 접근을 응용한, 새로운 데이터로 Q함수를 학습하되 정책은 갱신하지 않는 일종의 “조종” 방법)은 오히려 성능이 정체되어 약 50% 언저리에 머물렀습니다. 이는 새로운 롤아웃 데이터로 정책 자체를 계속 재훈련하는 것이 중요하며, 단순히 Q함수로 기존 정책의 행동을 보정해주는 것만으로는 충분치 않음을 의미합니다. 또한 필터링된 IL 방식이 초기에 높은 성능으로 시작하고도 추가 향상에 실패한 것은, 초기 정책이 이미 시演 데이터 분포를 거의 표현하고 있었기 때문으로 분석됩니다. 이 경우 새로운 데이터가 추가되어도 배울 새로운 내용이 거의 없어 개선이 이루어지지 않은 것입니다.\n\n\n\nFigure: 실제 로봇 실험 “테이프 걸기”의 학습 성과 비교 (성공률)\n\n\n\n녹색 선(Recipe)은 본 논문의 레시피 적용 결과, 회색 선(Filtered IL)은 필터링된 모방학습, 자주색 점선(Steering)은 정책을 업데이트하지 않고 Q함수로만 보조하는 기준 방법을 나타냅니다. 배치 온라인 RL 레시피를 적용한 경우(iteration 3) 성공률이 약 70%로 크게 향상되었으며, 다른 방법들은 초기 수준을 크게 넘지 못한 것을 볼 수 있습니다.\n\n실제 로봇에 배치 온라인 RL을 적용할 때 고려할 현실적 사항으로, 안전과 탐색의 균형이 있습니다. 예를 들어 성능 향상을 위해 탐색 노이즈를 추가하는 것이 유용하지만, 자칫하면 로봇에게 물리적으로 위험한 행동을 유발할 수 있습니다. 다행히 이 실험에서는 소량의 Ornstein-Uhlenbeck 노이즈를 추가해도 테이프를 놓친다거나 로봇이 이상 동작을 하는 등의 문제 없이 안정적으로 다양한 시도가 이루어졌습니다. 하지만 일부 실제 환경에서는 임의의 노이즈 주입이 어려울 수 있으므로, 다른 안전 탐색 기법이나 시뮬레이션을 병행한 사전 학습 등이 필요할 수 있습니다. 또한 현실 로봇 실험에서는 센서 노이즈, 시스템 지연 등 시뮬레이션에 없던 변수가 존재하므로, 배치 온라인 RL의 알고리즘이 이러한 비이상적 상황에서도 강인하게 작동하도록 설계되어야 합니다. 논문의 실험은 비교적 안정적인 실내 환경에서 이루어졌지만, 향후에는 더 복잡한 실제 시나리오(예: 이동 로봇, 협동 로봇 등)에서 이 접근법의 성능과 한계를 추가 검증해볼 필요가 있습니다."
  },
  {
    "objectID": "posts/paper/2025-06-12-batch-online-rl.html#본-논문의-한계점-및-향후-연구-방향",
    "href": "posts/paper/2025-06-12-batch-online-rl.html#본-논문의-한계점-및-향후-연구-방향",
    "title": "📃Batch Online RL 리뷰",
    "section": "본 논문의 한계점 및 향후 연구 방향",
    "text": "본 논문의 한계점 및 향후 연구 방향\n이 연구는 배치 온라인 RL의 효과를 좌우하는 요소들을 밝혀내고 일반적 지침을 제시하였지만, 저자들이 언급한 한계점도 몇 가지 존재합니다:\n\n적용 범위의 한계 (연속 vs. 이산): 본 논문의 모든 실험은 연속적 행동 공간을 갖는 로봇 제어 작업에 대해 이루어졌습니다. 만약 이산(discrete) 행동 공간을 가지는 문제 (예: 강화학습의 Atari 게임이나 로봇의 관절 제어가 아닌 개별 명령 선택 문제 등)에 이 방법을 적용하면 그대로 성립하지 않을 수 있습니다. 이산 환경에서는 Q함수를 직접 정책으로 사용할 수도 있고, 또 “표현력 높은/낮은 정책 클래스”라는 구분도 모호해지기 때문에, 배치 온라인 RL의 핵심 요소가 다르게 나타날 가능성이 있습니다. 향후에는 이산 행동을 포함한 다양한 영역에서 본 레시피의 유효성을 연구할 필요가 있습니다.\n노이즈 주입에 대한 고려: 본 논문에서 제안한 OU 노이즈 등 탐색 전략은 시뮬레이션과 제한된 실제 실험에서 일정 수준 효과를 보였지만, 모든 환경에서 통용될 수 있는지는 미지수입니다. 경우에 따라서는 노이즈로 인한 탐색보다 안전이 우선인 상황도 있고, 노이즈가 성능에 거의 영향을 주지 못할 수도 있습니다. 저자들도 실험에서 소량의 노이즈 추가가 정책 성공률을 극적으로 높이지는 못했고 약간의 향상만 주었다고 밝혔습니다. 따라서 미래 연구에서는 노이즈를 쓰지 않고도 탐색 다양성을 확보할 방법이나, 더 효과적인 탐색 노이즈 전략을 모색할 필요가 있습니다.\n초기 정책 성능에 대한 가정: 본 연구는 초기 정책이 어느 정도의 성공률은 갖춘 상태(약 30~60%)에서 시작하는 시나리오에 초점을 맞추었습니다. 그렇다면 만약 초기 정책이 거의 실패만 하는 수준(성공률 ~0%)이라면, 배치 온라인 RL로 개선이 가능할까요? 논문에서는 이 부분이 열린 문제로 남아 있다고 지적합니다. 초기 정책이 전혀 성능이 없다면 자율적으로 수집하는 데이터도 실패 투성이일 것이고, 아무리 RL이라도 완전한 탐색부터 시작해야 하는 어려움이 있습니다. 향후에는 초기 데모 없이도 학습을 시작하거나, 완전 무작위 정책으로부터 배치 RL을 안정적으로 수행하는 기법에 대한 연구가 필요합니다.\n\n이상의 한계들을 바탕으로, 향후 연구 방향으로는: (1) 다른 형태의 문제들(특히 이산적 결정 문제)에 대한 배치 온라인 RL 전략 확장, (2) 탐험-탐색(exploration-exploitation) 전략 개선 – 예를 들어 안전한 범위 내에서의 노이즈 주입, 혹은 모델 기반 탐색 기법과의 결합 등, (3) 초기 모델 없이 자율학습을 시작하는 방법 – 예를 들어 프리트레이닝된 세계 모델 활용이나 보상 신호 설계를 통해 완전 새로운 정책도 스스로 발전하게 하는 연구 등이 제시될 수 있습니다. 더 나아가, 본 논문의 레시피 자체도 각각의 구성요소(가치 학습 알고리즘, 정책 모델 구조, 암시적 추출 기법 등)를 최적화하거나 대체하여 성능을 더욱 높일 여지가 있습니다. 저자들은 이러한 후속 연구가 이루어진다면 스스로 개선하는 로봇의 성능 한계를 훨씬 끌어올릴 것으로 기대하고 있습니다."
  },
  {
    "objectID": "posts/paper/2025-06-12-batch-online-rl.html#전체적인-기여도와-의의",
    "href": "posts/paper/2025-06-12-batch-online-rl.html#전체적인-기여도와-의의",
    "title": "📃Batch Online RL 리뷰",
    "section": "전체적인 기여도와 의의",
    "text": "전체적인 기여도와 의의\n본 논문은 로봇 강화학습 분야에서 “스스로 모은 데이터로 점진적 학습”이라는 중요한 문제에 답하기 위해 체계적인 실증 연구를 수행했다는 점에서 큰 의미가 있습니다. 이전까지 여러 연구들이 시도했지만 명확한 결론을 얻지 못했던 배치 온라인 RL의 성공 요인을 세 가지 측면에서 분석해주었고, 이를 종합하여 실제로 효과가 입증된 학습 레시피를 제안하였습니다. 이 레시피는 복잡한 알고리즘이 아니라도 기존 기법들의 조합과 운영 방법을 조금 바꾸는 것만으로도 로봇의 자기학습 성능을 크게 높일 수 있다는 실용적 지침을 제공합니다. 특히 모방학습만으로 부족했던 이유와 강화학습을 도입할 때 주의할 점(정책 추출 방법, 모델 표현력)을 밝혀줌으로써, 향후 로봇 학습 실험을 설계하는 연구자들과 현장에서 실제 로봇을 학습시켜야 하는 실무 엔지니어 모두에게 유용한 인사이트를 준다고 볼 수 있습니다.\n또한 이 연구는 로봇 학습에서 데이터 효율성과 자동화에 대한 희망을 보여주었습니다. 사람이 일일이 데이터를 모으지 않아도, 로봇이 처음 배운 정책으로 스스로 실행하고 데이터를 모아 더 똑똑해질 수 있음을 실제 실험으로 증명하였습니다. 이는 마치 자율주행차가 주행하며 경험으로 학습하는 그림을 떠올리게 하며, 향후 로봇공학이 나아갈 방향이 인간의 지도 최소화와 자율성 극대화에 있음을 시사합니다. 물론 완전한 자율학습으로 가기까지 해결해야 할 과제들(안전, 초기 성능 등)은 남아 있지만, 본 논문은 그 실마리를 제공하고 있다고 평가할 수 있습니다.\n마지막으로, 학술적 기여도 측면에서 이 논문은 배치(off-policy) 환경에서의 강화학습 연구에 새로운 데이터를 제시했습니다. 과거에는 온라인 상에서 바로 학습하는 RL과, 고정 데이터로 학습하는 offline RL이 별개로 연구되는 경향이 있었는데, 본 연구는 오프라인-온라인의 경계를 넘나드는 배치 학습에 초점을 맞추어 중요한 통찰을 얻었습니다. 이로써 오프라인 RL 알고리즘을 반복 적용하면 어떨까?라는 물음에 답을 주고, 나아가 더 나은 오프라인 RL 기법 개발이나 효과적인 탐색 데이터 수집 전략에 대한 후속 연구를 촉발할 수 있습니다. 저자들이 언급했듯, 이 레시피의 각 요소를 더 발전시키는 연구를 통해 자기 개선형 로봇 모델을 한층 더 향상시킬 여지가 큽니다. 또한 저자들은 이번 연구의 알고리즘 구현 코드를 공개할 예정으로, 이를 통해 다른 연구자들이 쉽게 접근하여 배치 온라인 RL 분야의 발전을 가속할 수 있을 것으로 기대됩니다.\n요약하면, “What Matters for Batch Online RL in Robotics?” 논문은 로봇 강화학습에서 데이터 효율적 자기 학습을 실현하는 데 필요한 조건을 밝혀내고 검증함으로써, 로봇공학 및 강화학습 커뮤니티에 중요한 지식과 실용 해법을 제공한 작품이라 할 수 있습니다. 이는 궁극적으로 더 적은 인간 개입으로도 점점 똑똑해지는 로봇을 만드는 길을 여는 의미있는 걸음입니다."
  },
  {
    "objectID": "posts/paper/2025-08-21-tactile-beyond-pixel.html",
    "href": "posts/paper/2025-08-21-tactile-beyond-pixel.html",
    "title": "📃Tactile Beyond Pixel 리뷰",
    "section": "",
    "text": "Paper Link"
  },
  {
    "objectID": "posts/paper/2025-08-21-tactile-beyond-pixel.html#논문의-주요-내용-요약",
    "href": "posts/paper/2025-08-21-tactile-beyond-pixel.html#논문의-주요-내용-요약",
    "title": "📃Tactile Beyond Pixel 리뷰",
    "section": "1. 논문의 주요 내용 요약",
    "text": "1. 논문의 주요 내용 요약\n\n\n\n\nSparsh-X의 멀티센서 촉각 표현 융합 구조 예시. 이 구조는 Digit 360 센서로부터 얻은 촉각 이미지, 진동(오디오), 관성모션, 압력 신호를 입력으로 받아, 트랜스포머 기반 백본에서 이들을 융합하여 물체의 물리적 특성 및 접촉 상태를 나타내는 통합 표현을 학습한다.\n\n본 논문에서는 Meta/FAIR가 개발한 초고해상도 촉각 센서 Digit 360을 활용하여 4가지 촉각 모달리티(이미지, 오디오, 관성, 압력)를 통합하는 Sparsh-X라는 표현 학습 백본을 제안한다. 저자들은 약 100만 건의 접촉-조작 데이터(삽입, 미끄럼, 두드림, 회전 등 다양한 조작 행동)를 수집하여 자기 지도 학습으로 Sparsh-X를 사전학습시켰다. 이렇게 학습된 표현은 물체의 질량, 마찰, 힘 등의 물리적 특성을 추론할 수 있는 정보를 포함한다.\n\n제안 방식: Sparsh-X는 각 모달리티를 독립적인 트랜스포머 블록으로 인코딩한 후, 병목(Bottleneck) 토큰을 매개로 크로스-모달 퓨전을 수행하는 구조를 갖는다. 이를 통해 고해상도 촉각 이미지뿐 아니라 초음속 진동음, 가속도/자이로 센서 정보, 압력 신호 등 이질적인 데이터를 하나의 잠재 공간에 효과적으로 융합한다.\n데이터: 양팔 로봇 손가락과 수동 집게에 장착한 Digit 360으로 다양한 환경에서 수집한 약 1M의 비라벨 접촉 데이터를 사용했다. 이 대규모 무라벨 데이터셋은 향후 유사한 연구를 위한 벤치마크로도 활용될 수 있다.\n실험 과제: 학습된 표현은 두 가지 주요 실험에 적용되었다.\n\n흉내 학습(imitation learning)을 통한 플러그 삽입 과제(Allegro 로봇 손에 플러그를 꽂는 작업)와,\n시뮬레이션-현실 간 촉각 적응을 통한 손안 회전 과제(컵 모양 물체를 손 안에서 회전시키는 작업)이다.\n\n\n또한 물리적 속성 추론 과제(물체-행동 분류, 재질·질량·양 추정, 가해진 힘 추정 등)도 병행하여 평가했다.\n\n주요 결과:\n\nSparsh-X를 활용한 정책은 (a) 외부 카메라 정보만 사용한 정책 대비 성공률이 약 500%(즉 5배) 높고, (b) 촉각 이미지만 단독으로 사용한 종단간(end-to-end) 학습 대비 63% 높은 성공률을 보였다.\n특히, 플러그 삽입 실험에서는 Sparsh-X 기반 정책이 20회 시도 중 90%의 성공률을 기록했으며, 기존 비전·촉각 방식 대비 성능이 크게 향상되었다.\n손안 회전 과제에서는 Sparsh-X를 이용해 시뮬레이터에서 훈련된 정책을 적응시킨 결과, 물체의 수직 이동이 약 90% 감소하는 등 안정성이 크게 개선되었다.\n물리 속성 추론에서는 Sparsh-X 표현을 사용해 48% 높은 정확도를 기록해, 기존 종단 학습 방식 대비 물리적 특성 이해 능력이 크게 향상됨을 보였다. 예를 들어, 모든 모달리티를 결합하면 힘 추정 오차가 평균 35mN로 촉각 영상만 사용했을 때보다 17% 감소했다."
  },
  {
    "objectID": "posts/paper/2025-08-21-tactile-beyond-pixel.html#기술적-기여-및-한계-분석",
    "href": "posts/paper/2025-08-21-tactile-beyond-pixel.html#기술적-기여-및-한계-분석",
    "title": "📃Tactile Beyond Pixel 리뷰",
    "section": "2. 기술적 기여 및 한계 분석",
    "text": "2. 기술적 기여 및 한계 분석\n\n통합 멀티모달 백본: 본 논문의 핵심 기여는 Sparsh-X라는 최초의 멀티센서 촉각 표현 백본이다. 이전까지 촉각 표현 학습은 대부분 단일 모달리티(예: GelSight류 촉각 영상)나 모달리티별 개별 학습에 그쳤다. Sparsh-X는 네 가지 모달리티를 병목 기반 트랜스포머로 융합함으로써, 다양한 촉각 신호 간의 상호 보완적 정보를 포착한다. 특히, 단순 토큰 병합 방식을 사용한 기존 멀티모달 트랜스포머(MULSA)보다 계산 복잡도를 크게 낮추고, 네 가지 입력을 하나의 잠재 공간으로 압축할 수 있다.\n대규모 자기 지도 학습: 약 100만 건의 실제 접촉 데이터를 활용한 SSL(자기 지도 학습)을 수행했다. 이를 통해 데이터 라벨링 비용 없이 일반화 가능한 촉각 표현을 학습하였다. 사전학습된 Sparsh-X 표현은 downstream 학습 시 데이터 효율을 크게 높여, 적은 레이블 샘플로도 안정적인 정책 학습이 가능하다.\n정밀한 물리특성 학습: Sparsh-X는 물체의 질량, 마찰계수, 적용 힘 등 다양한 물리 속성을 포착하며, 이러한 속성 예측 성능이 크게 향상되었다. 실제로 본 논문에서는 물체-행동-면 분류, 재질·양 예측, 가해진 힘 추정 실험을 통해 Sparsh-X가 48% 더 높은 분류 정확도를 보임을 확인했다. 또한 정상(normal) 힘 추정 실험에서 모든 모달리티를 결합할 때 평균 오차가 17% 감소하여 힘 추정 정확도가 향상되었다.\n정책 학습과 시뮬레이터 적응: Sparsh-X 표현은 실제 로봇 조작 정책 학습에도 적용되었다. 예를 들어, 플러그 삽입 과제에서 이미지+촉각의 조합으로 행동을 예측하는 종단간 모델에 Sparsh-X를 추가하자 성공률이 63% 증가했다. 또한, 시뮬레이터에서 훈련된 손안 물체 회전 정책에 Sparsh-X 기반의 촉각 적응 모듈(ControlNet)을 적용하자 물체의 미끄러짐이 현저히 줄어들었으며, 기존 방법 대비 수직 이동량 90% 감소 효과를 보였다. 이처럼 멀티센서 촉각 표현은 시뮬-실전 전이(sim-to-real) 문제 해결에도 기여함을 보였다.\n한계점: 이 논문은 멀티센서 촉각의 잠재력을 보여주었으나 다음과 같은 한계도 지적한다. 첫째, 현재 사용된 데이터셋은 Digit 360 센서가 포함된 특정 플랫폼(예: Allegro 손, 수동 집게)에서 수집되었기 때문에 촉각 영상 모달리티의 다양성이 제한적일 수 있다. 센서별 광학적 특성 차이로 인해 일반화 성능이 제한될 우려가 있다. 둘째, 모든 실험에서 Sparsh-X 표현을 고정(frozen) 상태로 사용했으며, 다운스트림 과제별로 파인튜닝을 하지 않았다. 실제 적용 시 파인튜닝을 허용하면 개별 모달리티의 데이터 부족 문제를 보완하고 성능을 더욱 높일 수 있다. 셋째, 힘 추정 실험은 정상 방향 힘에 한정되었고, 다양한 접촉 기하나 전단력 추정은 다루지 않았다. 전단력은 Digit 360 구조(탄성돔) 때문에 모델링이 복잡하며, 본 논문에서는 별도 고려되지 않았다. 마지막으로, 대용량 트랜스포머를 학습하는 데 필요한 계산 자원과 데이터 수집 노력이 커서, 실제로 적용하는 데 비용 부담이 있다."
  },
  {
    "objectID": "posts/paper/2025-08-21-tactile-beyond-pixel.html#관련-연구와의-비교",
    "href": "posts/paper/2025-08-21-tactile-beyond-pixel.html#관련-연구와의-비교",
    "title": "📃Tactile Beyond Pixel 리뷰",
    "section": "3. 관련 연구와의 비교",
    "text": "3. 관련 연구와의 비교\n\n기존 촉각 기반 조작 연구: 전통적으로 로봇 촉각 연구에서는 GelSight, DIGIT 등 비전 기반 촉각 센서가 주로 사용되었다. 이런 센서는 고해상도 촉각 이미지를 제공해 물체 형상, 힘, 마찰 등을 추정할 수 있다. 그러나 대부분 작업은 단일 촉각 이미지에 의존하거나, 외부 카메라와 연계하는 방식이었다. 예를 들어, 다양한 조립, 표면 식별, 경로 추적 과제에서 GelSight류 센서가 활용되었지만, 영상 촉각만으로는 연속적인 접촉 동작의 미세한 변화를 완전히 포착하기 어렵다는 한계가 있다.\n오디오 및 기타 모달리티 활용: 일부 연구에서는 접촉 시 발생하는 진동음(오디오)이나 외부 카메라 영상을 함께 이용하여 물체 특성을 추정하려 했다. 예컨대, 접촉 마이크를 이용해 재질을 식별하거나, 영상-오디오 합성 학습을 시도한 바 있다. 하지만 오디오 단일 모달로는 접촉과정의 복잡한 힘·변형 정보를 온전히 얻기 어렵고, 멀티모달 학습을 하더라도 주로 시각과 청각에만 국한되었다.\n기존 멀티모달 접근: MULSA 등 최근 연구는 비전, 촉각 영상, 오디오를 함께 Transformer로 학습하는 방식으로 멀티모달 촉각 표현을 시도했다. 그러나 MULSA는 단순히 모든 토큰을 이어붙여(attention concatenation) 처리하기 때문에 계산 복잡도가 매우 커지는 문제가 있다. 또한 MimicTouch (Yu et al., 2024)와 같은 연구는 영상 촉각과 오디오를 개별적으로 SSL로 학습했지만, 모달리티 간 융합을 수행하지 않아 촉각 간 상호작용을 충분히 활용하지 못했다.\n본 논문의 차별점: Sparsh-X는 네 가지 모달리티(이미지, 오디오, 관성, 압력)를 통합하는 점에서 기존 연구와 뚜렷한 차별성을 가진다. 특히, 병목 토큰 기반의 멀티모달 트랜스포머를 도입해 모달리티 간 정보를 효율적으로 융합하며, 종래 방식보다 계산 효율성과 표현력 모두 개선했다. 이는 기존 음성 기반 접근이나 단일센서 기반 모델이 다루지 못한 다양한 촉각 신호를 통합하여, 더 풍부한 촉각 표현을 학습할 수 있게 한다. 즉, 종전 연구가 해결하지 못했던 멀티센서 융합 방식을 제안함으로써, 로봇의 촉각 인지가 한층 발전되었다."
  },
  {
    "objectID": "posts/paper/2025-08-21-tactile-beyond-pixel.html#실제-응용-가능성-평가",
    "href": "posts/paper/2025-08-21-tactile-beyond-pixel.html#실제-응용-가능성-평가",
    "title": "📃Tactile Beyond Pixel 리뷰",
    "section": "4. 실제 응용 가능성 평가",
    "text": "4. 실제 응용 가능성 평가\n\n센서 및 하드웨어 측면: 제안된 방법은 Digit 360 센서를 전제로 한다. Digit 360은 Meta FAIR와 GelSight가 공개한 최첨단 촉각 센서로, 지름 14mm의 인조 손가락 모양이며 18개 이상의 센싱 기능을 통합한다[16]. 물리적 변형과 압력, 진동을 초고해상도로 감지하여 인간 수준의 정밀도로 터치를 디지털화할 수 있다[16]. GelSight 측은 이 센서를 내년부터 본격 공급할 예정이므로, 향후 산업용 로봇에도 장착이 가능해질 것으로 보인다. 그러나 현재는 비교적 실험실용 프로토타입 수준이므로, 실제 공장이나 서비스 환경에 배치하려면 추가적인 내구성 검증과 비용 고려가 필요하다.\n시스템 요구 사항: Sparsh-X는 대규모 사전학습이 전제되므로 상당한 계산 자원과 데이터 수집이 필요하다. 산업 현장에서 도입하려면 개별 작업에 맞춰 추가 학습 또는 미세 조정(fine-tuning)이 필수적이다. 또한, Digit 360을 로봇 손가락에 부착하고 실시간으로 데이터를 처리하려면 고속 데이터 처리와 연산 하드웨어가 요구된다. 예를 들어 삽입 조립 작업에서는 센서-행동 반응 지연(latency)을 줄여야 하며, 산업용 로봇 암에 정확히 맞도록 센서 장착 방식을 고려해야 한다.\n적용 가능성: 그럼에도 불구하고, Sparsh-X의 촉각 표현은 고정밀 작업에 유리하다. 플러그 삽입, 나사 체결 등 카메라로는 어려운 정밀 조립 작업이나, 조리 로봇의 섬세한 조작, 복잡 형상의 부품 검사 등에 활용할 수 있다. 또한, 센서 데이터와 학습을 병행하면 비전 정보가 불충분한 어둡거나 부분 가려진 환경에서도 안정적인 조작이 가능하다. 예컨대, 복잡한 회로기판 위 작은 부품을 집거나, 의료용 로봇이 미세한 조직을 다루는 작업 등에 응용될 수 있다. 실제로 본 논문에서도 플러그 삽입과 같은 산업적 의미가 있는 조작에서 큰 성능 향상이 관찰되었다.\n제약 및 전망: 현재 연구 단계에서는 다양한 접촉 형태(경사진 표면, 전단력 등)에 대한 검증이 부족하며, 대량의 촉각 데이터 구축도 필요한 상태이다. 또한 실제 산업 환경에서는 센서의 내구성, 잡음·오염 문제, 모델의 추론 속도 등이 추가 과제가 될 수 있다. 그럼에도 불구하고 Sparsh-X는 촉각에 기반한 ‘기초 모델(Foundation Model)’ 접근의 가능성을 보여준다. 즉, 다양한 로봇 작업에 재사용 가능한 촉각 표현을 제공함으로써, 추후 도메인별 미세 조정으로 적용 범위를 확장할 수 있는 잠재력이 크다. 실제로 GelSight 측은 Digit 360을 로봇 촉각 연구의 다음 단계로 평가하며, 의료·가상현실·휴머노이드 등 다양한 분야에 응용할 수 있을 것으로 전망한다. 따라서 충분한 데이터와 연산 자원이 확보된다면, Sparsh-X 방식은 산업용·서비스용 로봇에서 섬세한 조작을 필요로 하는 다수 과제에 적용 가능할 것이다."
  },
  {
    "objectID": "posts/paper/2025-07-02-offline-rl-review.html",
    "href": "posts/paper/2025-07-02-offline-rl-review.html",
    "title": "📃Offline RL Survey 리뷰",
    "section": "",
    "text": "Paper Link"
  },
  {
    "objectID": "posts/paper/2025-07-02-offline-rl-review.html#종류",
    "href": "posts/paper/2025-07-02-offline-rl-review.html#종류",
    "title": "📃Offline RL Survey 리뷰",
    "section": "종류",
    "text": "종류\n논문은 오프라인 RL 방법론을 분류하기 위한 새로운 Taxonomy (분류체계)를 제안합니다. 상위 수준에서는 학습 대상을 기준으로 Model-Based (모델 기반), One-step (원스텝), Imitation Learning (모방 학습) 방법으로 나뉩니다. 또한, 손실 함수나 훈련 절차에 대한 변형인 Policy Constraints (정책 제약), Regularization (정규화), Uncertainty Estimation (불확실성 추정)을 부가적인 특성으로 설명합니다.\n\nPolicy Constraints: 학습된 정책\\pi_{\\theta}를 행동 정책\\pi_{\\beta}에 가깝게 제약합니다.\n\nDirect (직접):\\pi_{\\beta}를 명시적으로 추정하고\\mathcal{D}(\\pi_{\\theta}(\\cdot|s), \\hat{\\pi}_{\\beta}(\\cdot|s)) \\le \\epsilon와 같은 제약 조건(e.g.,f-divergence 사용)을 부여합니다 (BCQ, BRAC). 추정 오류에 민감합니다.\nImplicit (암묵적):\\pi_{\\beta} 추정 없이 수정된 목적 함수를 통해 암묵적으로 제약합니다. 아래와 같은 Advantage-weighted regression 형태가 대표적입니다 (BEAR, AWR, AWAC, TD3+BC). J(\\theta) = \\mathbb{E}_{s,a \\sim \\mathcal{D}}[\\log \\pi_{\\theta}(a|s) \\exp(\\frac{1}{\\lambda} \\hat{A}^{\\pi}(s, a))]\n\nImportance Sampling (IS): Off-policy 정책 평가를 위해 사용됩니다. 트라젝토리 확률 비율의 곱(w_{i:j})으로 인해 분산이 매우 높습니다. Variance Reduction (분산 감소) 기법(Per-decision IS, Doubly Robust Estimator, Marginalized IS)이 제안되었습니다. Marginalized IS는 상태 한계 분포 비율(\\rho_{\\pi}(s)) 또는 상태-행동 한계 분포 비율(\\rho_{\\pi}(s, a))의 벨만 방정식d^{\\pi_\\beta}(s')\\rho_\\pi(s') = (1-\\gamma)d_0(s') + \\gamma \\sum_{s,a} d^{\\pi_\\beta}(s)\\rho_\\pi(s)\\pi(a|s)T(s'|s,a)을 활용하여 분산 문제를 완화합니다 (GenDICE).\nRegularization: 정책 또는 가치 함수에 페널티 항을 추가하여 바람직한 속성을 부여합니다.\n\nPolicy Regularization: 정책의 엔트로피(entropy)를 최대화하여 확률성(stochasticity)을 높입니다 (SAC).\nValue Regularization: OOD 행동에 대한 Q-값 추정을 낮게 강제하여 보수적인 가치 추정을 수행합니다. CQL은 \\max_{\\mu} \\mathbb{E}_{s \\sim \\mathcal{D}, a \\sim \\mu(\\cdot|s)}[Q^{\\pi}_{\\phi}(s, a)] - \\mathbb{E}_{s \\sim \\mathcal{D}, a \\sim \\hat{\\pi}_{\\beta}(\\cdot|s)}[Q^{\\pi}_{\\phi}(s, a)] + \\mathcal{R}(\\mu) 와 같은 정규화 항을 통해 데이터셋의 가치 함수가 참 값의 하한(lower bound)이 되도록 학습합니다.\n\nUncertainty Estimation: 학습된 정책, 가치 함수 또는 모델의 불확실성을 추정하여 보수성의 정도를 동적으로 조절합니다. 보통 앙상블(ensemble)을 사용하여 예측 분산 등으로 불확실성을 측정합니다 (REM).\nModel-Based Methods: 데이터셋\\mathcal{D}로 전이 동역학(T)과 보상 함수(r)를 학습합니다. 학습된 모델은 계획(planning)에 사용되거나 모델 롤아웃(model rollout)을 통해 합성 데이터 생성에 사용됩니다. 모델 분포 변화 문제를 피하기 위해 불확실성을 기반으로 보상에 페널티를 주는 보수적인 모델(\\tilde{r}_{\\psi_r}(s, a) = r_{\\psi_r}(s, a) - \\lambda U_r(s, a))을 학습하는 접근 방식이 있습니다 (MOReL, MOPO, COMBO). COMBO는 모델 기반 환경에서의 가치 정규화(value regularization)를 통해 불확실성 정량화 없이도 보수성을 확보합니다.\nOne-Step Methods: 정책 평가 및 정책 개선 단계를 반복하지 않고, 행동 정책(\\pi_{\\beta})의 가치 함수(Q^{\\pi_{\\beta}})를 정확하게 학습한 후 단일 정책 개선 단계만 수행합니다. 이를 통해 OOD 행동에 대한 가치 평가를 피합니다. IQL(Implicit Q-Learning)은 가치 함수(V^{\\pi}) 학습에 Expectile Regression (분위 회귀) 손실 함수를 사용하여 데이터 분포 내의 ‘좋은’ 행동들에 대한 Q값의 상한에 근사합니다.\nImitation Learning: 행동 정책을 모방(mimic)합니다. 단순 Behavior Cloning (행동 복제, BC)은 전체 데이터를 복제합니다. 고급 기법은 가치 함수 등을 사용하여 차선 행동을 필터링하거나(BAIL, CRR) 원하는 결과(목표, 보상 등)에 조건화된 정책을 학습합니다(RvS).\nTrajectory Optimization (트라젝토리 최적화): 전체 트라젝토리(\\tau = (s_0, a_0, \\dots, s_H))에 대한 결합 상태-행동 분포(p_{\\pi_{\\beta}}(\\tau))를 시퀀스 모델(Sequence Model, 예: Transformer)로 학습합니다. 학습된 분포를 기반으로 원하는 수익(Return-to-Go) 등에 조건화하여 계획을 수행합니다(TT, DT). 희소 보상 문제에 강점을 보입니다."
  },
  {
    "objectID": "posts/paper/2025-07-02-offline-rl-review.html#평가",
    "href": "posts/paper/2025-07-02-offline-rl-review.html#평가",
    "title": "📃Offline RL Survey 리뷰",
    "section": "평가",
    "text": "평가\nOff-policy Evaluation (OPE, 오프-폴리시 평가)는 오프라인 RL의 중요한 Open Problem 중 하나입니다. 환경과의 상호작용 없이 오프라인으로 정책의 성능을 정확히 추정하고 하이퍼파라미터를 튜닝하는 것은 실용적인 오프라인 RL에 필수적입니다. 주요 OPE 방법에는 Model-Based 접근법, Importance Sampling, Fit Q Evaluation (FQE)가 있습니다. 경험적 연구들에 따르면 FQE가 종종 좋은 성능을 보이지만, 모든 설정에서 일관적으로 우수한 방법은 아직 없습니다 (DOPE 벤치마크).\n\n오프라인 RL Benchmark로는 D4RL과 RL Unplugged가 널리 사용됩니다.\n\n\n이들은 Narrow and Biased Data Distributions (좁고 편향된 데이터 분포), Undirected and Multitask Data (지향되지 않은 다중 작업 데이터), Sparse Rewards (희소 보상), Suboptimal Data (차선 데이터), Nonrepresentable Behavior Policies (표현 불가능한 행동 정책), Non-Markovian Behavior Policies (비 마르코프 행동 정책), Realistic Domains (현실적인 도메인) 등 실제 응용에 중요한 Dataset Design Factors (데이터셋 설계 요소)를 포함하는 다양한 환경과 데이터셋을 제공합니다.\n하지만 Stochastic Dynamics (확률적 동역학), Nonstationarity (비정상성), Risky Biases (위험한 편향), Multiagent 환경 등은 여전히 부족한 실정입니다. D4RL 벤치마크 성능 분석에 따르면 최근 방법(TT, IQL)과 트라젝토리 최적화 및 원스텝 방법이 희소 보상이나 다중 작업 데이터에서 강점을 보이며 유망한 분류로 나타납니다.\n미래 연구 방향으로는 OPE의 신뢰성 향상, Unsupervised RL 기법을 활용한 레이블 없는 데이터 활용, Incremental RL을 통한 온라인 Fine-tuning 전략 개발, Safety-critical RL (안전 필수 강화학습, 예: CVaR) 분야 연구 등이 제안됩니다. 효과적인 데이터 수집 및 curation 또한 알고리즘 개발만큼 중요합니다."
  },
  {
    "objectID": "posts/paper/2024-03-17-vcgs.html",
    "href": "posts/paper/2024-03-17-vcgs.html",
    "title": "📃VCGS 리뷰",
    "section": "",
    "text": "이번 포스팅은 Variational Constrained Grasp Sample 논문을 읽고 정리한 내용입니다. 해당 논문은 IROS 2023 학회에 Accept된 논문으로, 특정 대상 영역에 대한 제약을 가진 6자유도(DoF) Grasp을 샘플링하기 위한 새로운 생성적 그리핑 샘플링 네트워크, VCGS를 소개합니다. 뿐만 아니라 1,400만 개 이상의 훈련 샘플을 포함하는 새로운 데이터셋 CONG를 구축한 내용을 발표했습니다. 제안된 VCGS가 시뮬레이션 및 실제 테스트에서 비교 모델인 GraspNet보다 10-15% 높은 그리핑 성공률을 보이며, 2-3배 더 효율적인 것을 보여준 논문입니다."
  },
  {
    "objectID": "posts/paper/2024-03-17-vcgs.html#grasp-sampler",
    "href": "posts/paper/2024-03-17-vcgs.html#grasp-sampler",
    "title": "📃VCGS 리뷰",
    "section": "3.1 Grasp Sampler",
    "text": "3.1 Grasp Sampler\nVCGS의 Grasp Sampler의 기본적인 틀은 Conditional Variational Autoencoder (CVAE) 구조를 차용해서 아래와 같이 만들었습니다. Grasp pose를 다양하게 Sampling하기 위해 Encoder와 Decoder를 VAE 구조를 차용하여 Gaussian Prior Distribution을 이용해서 가능한 다양한 Grasp pose를 생성할 수 있도록 설계했습니다.\n\n\n\nC-VAE 구조\n\n\n\n\n\nLoss Function"
  },
  {
    "objectID": "posts/paper/2024-03-17-vcgs.html#grasp-evaluator",
    "href": "posts/paper/2024-03-17-vcgs.html#grasp-evaluator",
    "title": "📃VCGS 리뷰",
    "section": "3.2 Grasp Evaluator",
    "text": "3.2 Grasp Evaluator\n학습동안에 좋은 Grasp data만 학습하는 Encoder가 더 다양한 Grasp data를 경험할 수 있도록 Evaluator Network를 추가하여 Bad Grasp에 대한 경험도 할 수 있도록 만들었습니다.\n\n\n\nInput data 형태와 Evaluator Network"
  },
  {
    "objectID": "posts/paper/2024-03-17-vcgs.html#cong-dataset",
    "href": "posts/paper/2024-03-17-vcgs.html#cong-dataset",
    "title": "📃VCGS 리뷰",
    "section": "3.3 CONG Dataset",
    "text": "3.3 CONG Dataset\n\n\n\nCONG Dataset 구축과정\n\n\n구성 요소\n\nO: object point cloud\nG*: target area A에서 랜덤하게 샘플링된 successful grasp\n\n데이터셋 구축 과정\n\nobject를 원점에 랜덤한 orientation으로 놓고 O[N x 3] rendering\nO에서 query point I[K x 3]를 샘플링(K &lt;&lt; N) - Farthest Point Sampling 사용\n각 query point xi(∈I)에 대해서 반경 ri(~U[0, R]) 이웃한 point Ai들을 모두 찾음\n\n이때 R은 mesh bounding box의 대각선 길이\n\n[grasp center point]와 [Ai의 어떤 점]이라도 최대 d인 모든 G를 찾아냄\n\n\n\n\nmesh 데이터에서 grasp data를 추출하는 과정"
  },
  {
    "objectID": "posts/paper/2024-03-17-vcgs.html#a.-simulated-robotic-grasping",
    "href": "posts/paper/2024-03-17-vcgs.html#a.-simulated-robotic-grasping",
    "title": "📃VCGS 리뷰",
    "section": "4.1 A. Simulated Robotic Grasping",
    "text": "4.1 A. Simulated Robotic Grasping\n\nbest grasp, NOT the best reachable\ngripper와 object 둘 다 free-floating 상황\nIsaacGym simulator 사용\n\nAcronym dataset에서 123개의 random object\n물체의 observation data로는 depth sensor 사용\n\n시뮬레이터에서 2개 실험 진행\n\nUnconstrained sampling: target area 없이 그냥 grasp을 샘플링. A=O\nConstrained sampling: target area에서만 grasp 생성\n\n비교군\n\nGraspNet: SOTA\nGraspNetTaI: Target as Input. target area만 grasp sampling network에 넣어준 모델\n\n\n\n\n\n13\n\n\n\nVCGS는 GraspNet보다 3배 이상의 Ratio of grasps kept %를 보여줌\n\n네트워크 입력으로 Constrained grasp sampling을 넣어주는 것의 이점에 대한 증거\n\nGraspNetTaI는 GraspNet보다 Success Rate가 낮음\n\n물체의 전체 정보(global)를 사용하는 것이 특정한 target area에 대한 정보(local)를 사용하는 것보다 좋음을 알 수 있음\n\nGraspNet은 Success Rate가 # of grasps sampled에 영향을 받음\n\n만약 Unconstrained 경우라면 더 많은 sampling이 필요하다고 볼 수 있음\nRGK가 #GS에 영향을 받지 않은 결과를 보고도 확인할 수 있는 가설임\n\n\n\n\n\n14\n\n\n\n\n\n15\n\n\n마지막으로 해당 논문의 발표영상을 마지막으로 이번 포스팅을 마무리하도록 하겠습니다."
  },
  {
    "objectID": "posts/paper/2025-07-19-mambavision.html",
    "href": "posts/paper/2025-07-19-mambavision.html",
    "title": "📃MambaVision 리뷰",
    "section": "",
    "text": "Paper Link\nGithub Link"
  },
  {
    "objectID": "posts/paper/2025-07-19-mambavision.html#section",
    "href": "posts/paper/2025-07-19-mambavision.html#section",
    "title": "📃MambaVision 리뷰",
    "section": "1.1 +",
    "text": "1.1 +\nMamba의 장점\n\n선형 시간 복잡도 (Linear time complexity): Mamba는 State Space Model(SSM) 기반으로, 입력 시퀀스를 효율적으로 처리하도록 설계되어 Transformer의 주된 병목인 self-attention의 쿼드러틱(제곱) 연산 복잡도를 선형으로 줄였습니다. 이는 긴 시퀀스나 고해상도 이미지 패치 처리 시 매우 효율적입니다.\n하드웨어 친화적 설계와 선택적 처리 (Selective processing): 입력 의존적인 동적 파라미터 조정을 통해 중요한 정보에 집중하면서 불필요한 정보를 걸러내, 효율성과 표현력을 동시에 높입니다.\n우수한 시계열 및 연속 데이터 모델링 능력: SSM 기반 특성상 연속적이고 순차적인 데이터 패턴을 잘 포착할 수 있습니다.\n\nTransformer의 장점\n\n전역 문맥 인식 (Global context modeling): Self-attention 메커니즘 덕분에 이미지 내 모든 위치 간 상호작용을 동시다발적으로 모델링 가능하여, 장거리 공간적 의존성을 효과적으로 학습할 수 있습니다.\n유연성: 다양한 데이터 형식과 크기에 맞추어 손쉽게 조정 가능하며, 특히 비순차적이고 공간적인 분포가 중요한 비전 작업에 적합합니다.\n\nMambaVision에서 결합된 시너지\n\nMamba의 효율적인 지역 및 연속 시퀀스 모델링 능력과 Transformer의 전역적이며 장거리 의존성 포착 능력을 결합해, 이미지 내 로컬 특성과 글로벌 컨텍스트를 모두 효과적으로 학습합니다.\n특히, MambaVision은 초반 고해상도 이미지 처리에는 CNN 기반 컨볼루셔널 레이어를 사용하고, 중간과 후반 단계에서 Mamba 기반 믹서와 Transformer의 self-attention 블록을 하이브리드로 조합하여, 효율성과 정확도의 트레이드오프를 훌륭히 달성합니다.\nTransformer 블록은 최종 단계에 집중적으로 배치되어 이미지의 전역 문맥을 보완, Mamba 블록으로 인해 제한된 전역 수용 영역 문제를 보완해줍니다.\n\n\n결론적으로, Mamba는 효율적이고 연속적 시퀀스 처리에 뛰어나고, Transformer는 강력한 전역 공간 의존성 학습능력이 있어, 두 아키텍처가 만나면 각각의 단점은 완화되고 강점만 모아서, 이미지 인식 등 컴퓨터 비전 작업에서 더 탁월한 성능과 속도 조화를 이룰 수 있는 것입니다.\n\n\n1. MambaVision 아키텍처 개요\n\n4단계(Stages)로 구성된 계층형 구조\n\nStage 1, 2: 고해상도 이미지에서 빠르게 특징을 추출하기 위해 Residual CNN 블록을 사용합니다.\nStage 3, 4: 저해상도 공간에서 MambaVision Mixer(개선된 Mamba 블록)와 Transformer의 self-attention 블록을 결합해 사용합니다.\n\n이 구조는 높은 해상도에서는 효율적 지역 특성 추출에 집중하고, 점점 해상도가 낮아질수록 전역 컨텍스트 이해에 집중하도록 설계되어 있습니다.\n\n2. MambaVision Mixer (개선된 Mamba 블록)\n기존 Mamba (SSM 기반) 설계가 가지고 있던 순차적이며 인과적인 causal convolution의 한계를 보완해 다음과 같이 설계했습니다:\n\n인과적(convolution) causal conv 대신 일반 conv 사용:\n\n시계열 데이터처럼 좌-&gt;우 한 방향으로만 정보를 처리하지 않고, 시각 데이터를 위해 좌우 방향을 모두 고려하여 공간적 흐름에 대해 더 유연합니다.\n\n두 개의 분기(branch) 도입:\n\nSSM branch: Mamba 고유의 상태공간 모델(Selective scan)을 통해 시퀀스적이고 구조적인 정보를 입력으로 처리합니다. 이는 시간적·순차적 특징 포착에 강점.\n비SSM branch (Symmetric branch): 일반 1D convolution + SiLU (Sigmoid Linear Unit) 활성화 조합으로, SSM이 지나치게 한 방향성에 갇히는 문제를 보완하고, 컨텐츠 기반 전역 및 지역 특징을 보충적으로 학습합니다.\n\n출력 합치기:\n\n두 분기 출력은 절반 크기로 임베딩 후 연결(concatenation)하고, 마지막에 다시 선형 변환하여 원래 임베딩 크기로 되돌립니다.\n\n이러한 구조는 순차(SSM) + 공간(비SSM) 정보가 결합되어 풍부한 시각 특성 표현을 가능하게 하고, Mamba의 효율성은 유지하면서 Vision에 더 적합하도록 개선된 것입니다.\n\n3. Transformer 블록과의 하이브리드 통합\n\nMambaVision은 전체 Stage 3, 4 블록 중 뒷부분(N/2 층)을 Transformer의 multi-head self-attention 블록으로 구성합니다.\n이유는 Transformer의 self-attention이 전역적 문맥 모델링에 특화되어 있어, MambaVision Mixer가 주로 지역단위 및 구조적인 특징을 뽑아낸 뒤 후반부에서 전역 정보를 보강하도록 역할을 분담하기 위함입니다.\n성능 실험에서 Transformer 블록을 마지막 층(N/2, 혹은 N/4)에 두는 것이 임의적 혹은 초반부에 배치하는 것보다 훨씬 효과적임이 발견되었습니다.\n\n4. 모델 내부 처리 흐름 (수식 및 실험 코드 기준)\n\n입력: X \\in \\mathbb{R}^{T \\times C} (시퀀스 길이 T, 임베딩 차원 C)\nMambaVision Mixer 블록 동작:\n\n\\begin{aligned}\nX_1 &= \\text{Scan} \\big( \\sigma(\\text{Conv}(\\text{Linear}(C \\to C/2)(X))) \\big) \\\\\nX_2 &= \\sigma \\big(\\text{Conv}(\\text{Linear}(C \\to C/2)(X)) \\big) \\\\\nX_{out} &= \\text{Linear}(C, C)( \\text{Concat}(X_1, X_2) )\n\\end{aligned}\n\n여기서 Scan 함수는 Mamba 모델의 input-dependent selective state-space convolution이며, \\sigma는 SiLU 활성화입니다.\nX_1에서 SSM branch가 입력 시퀀스를 Mamba 방식으로 처리하며, X_2는 일반 conv branch가 spatial 정보를 보완합니다.\n이렇게 조합된 출력을 다음 MLP나 Self-Attention 블록에 넘겨 내부 표현을 진화시킵니다.\n\n5. 추가적으로 MambaVision의 설계 특징\n\nDownsampling CNN 블록: 이미지 규모가 크기 때문에 전형적인 풀링 대신 3×3 stride=2 Conv를 사용해 공간 해상도를 조절하며, 이는 전형적 CNN의 특징 추출 방식을 계승합니다.\nLayer Normalization 사용: Mamba와 Transformer 블록 모두 안정적인 학습 및 표현을 위해 LayerNorm 기법을 사용합니다.\n윈도우 기반 self-attention: Transformer 단계에서 연산 비용을 줄이기 위해 지역 윈도우 내에서 self-attention을 수행하며, Stage 3에서는 window size 14, Stage 4에서는 7을 기본값으로 해 최적의 성능과 효율을 추구합니다.\n이 윈도우 크기는 실험적으로도 최적값으로 확인되었으며, 윈도우를 크게 하면 전역성을 높일 수 있지만 속도가 약간 느려집니다.\n\n요약\n\nMambaVision은 지역적인 효율적 시퀀스 모델링(Scan 된 SSM)과 공간적 비SSM 토큰 믹서를 두 개의 병렬 분기로 결합.\n모든 입력을 연산하는데 있어 MambaVision Mixer가 먼저 지역/순차적 패턴을 효과적으로 파악.\n후반부 Transformer self-attention 블록들이 멀리 떨어진 공간 정보와 전역 문맥을 포착.\n전체적으로 효율적이면서도 장거리 및 다양한 공간적 의존성을 함께 포착하려는 복합적 설계를 통해 높은 정확도와 빠른 처리 속도를 달성."
  },
  {
    "objectID": "posts/paper/2025-07-19-mambavision.html#소개-introduction",
    "href": "posts/paper/2025-07-19-mambavision.html#소개-introduction",
    "title": "📃MambaVision 리뷰",
    "section": "2.1 소개 (Introduction)",
    "text": "2.1 소개 (Introduction)\n딥러닝 비전 분야에서는 이미지 인식 성능을 높이면서도 효율을 유지하기 위한 백본 모델 설계가 중요합니다. 과거에는 합성곱 신경망(CNN)이 주도했지만, 최근 Transformer 구조가 Vision Transformer (ViT) 등의 형태로 등장하여 뛰어난 성능을 보였습니다. Transformer는 자기어텐션(self-attention)을 통해 이미지 전역의 문맥 정보를 효과적으로 포착할 수 있다는 강점이 있지만, 입력 패치 수가 늘어날수록 연산량이 $O(N^2)$ 수준으로 기하급수적으로 증가하는 단점이 있습니다. 한편, 2023년에 제안된 Mamba는 상태 공간 모델(State Space Model, SSM) 기반의 새로운 시퀀스 모델로, 입력 길이에 선형 시간 복잡도를 달성하면서도 자연어 등의 시퀀스 작업에서 Transformer에 필적하는 성능을 보였습니다. Mamba의 핵심 아이디어는 입력 의존 동적 파라미터 선택 메커니즘을 통해 불필요한 정보는 거르면서 긴 시퀀스를 효율적으로 처리하는 것이었죠.\n이러한 장점에도 불구하고, Mamba와 같은 SSM 모델을 컴퓨터 비전에 직접 적용하는 데에는 한계가 존재했습니다. 이미지에서는 모든 픽셀이 순차적으로 의존하지 않고 공간상의 국소적 관계가 주로 중요하며, 글로벌 맥락도 한 번에 고려되어야 정확한 판단을 내릴 수 있습니다. 그러나 기본 Mamba는 자동회귀적 순차 처리 특성 때문에 한 번에 한 방향으로만 정보를 흘려보내 전역 정보를 충분히 활용하지 못했고, 이를 극복하려 양방향 처리 등을 도입하면 지연 시간이 커지는 문제가 있었습니다. 결국 최신 Vision Transformer나 CNN 기반 모델들이 여전히 Mamba 기반 비전 모델보다 우수한 정확도를 보여주는 상황이었습니다.\n“MambaVision: A Hybrid Mamba-Transformer Vision Backbone”은 이러한 배경에서 제안된 혁신적인 하이브리드 백본입니다. NVIDIA의 Ali Hatamizadeh와 Jan Kautz 연구진은 Mamba 구조를 비전에 맞게 개선하고, 그것을 Transformer의 자기어텐션과 결합함으로써 양쪽의 강점을 모두 살린 새로운 모델 MambaVision을 설계했습니다. 이 글에서는 해당 논문의 핵심 내용을 깊이 있게 분석하고자 합니다. Mamba와 Transformer를 어떻게 결합하여 더 나은 비전 백본을 만들었는지, 그리고 이러한 하이브리드 접근이 왜 중요한지를 알기 쉽게 풀어보겠습니다. 또한 모델 아키텍처의 세부 구성, 주요 실험 결과, 장단점과 시사점을 살펴봄으로써 MambaVision이 가져올 비전 모델 설계의 미래에 대해 논의하겠습니다."
  },
  {
    "objectID": "posts/paper/2025-07-19-mambavision.html#배경-background",
    "href": "posts/paper/2025-07-19-mambavision.html#배경-background",
    "title": "📃MambaVision 리뷰",
    "section": "2.2 배경 (Background)",
    "text": "2.2 배경 (Background)\n\n2.2.1 Transformer와 비전 모델\nTransformer는 원래 자연어 처리에서 등장한 혁신적인 모델이지만, ViT(Vision Transformer)를 필두로 이미지 분야에도 빠르게 도입되었습니다. Transformer의 자기어텐션 메커니즘은 입력 토큰(이미지 패치)들 간 모든 쌍을 비교하여 전역적인 상호 관계를 학습할 수 있기 때문에, 복잡한 이미지에서도 장거리 의존 관계와 전체 맥락을 포착하는 데 유리합니다. 예를 들어 Swin Transformer는 윈도우 단위로 자기어텐션을 수행하고 계층적 구조를 도입하여 지역과 전역 패턴을 모두 잡아내려 했습니다. 이처럼 Transformer 기반 비전 모델들은 높은 표현력을 보여주었지만, 막대한 연산량과 대용량 데이터 학습 필요성이라는 허들이 존재합니다. $N N$ 패치들 사이 모든 쌍을 처리하는 자기어텐션은 해상도가 커질수록 연산 비용이 급증하고, 대규모 데이터셋에서의 사전훈련 등이 요구되어 현실 적용에 부담이 될 수 있습니다. 이를 완화하기 위해 Swin Transformer는 국소 윈도우로 어텐션 범위를 제한하거나, ConvNeXt같은 현대적 CNN은 레이어 정규화 도입과 커널 크기 확대 등으로 Transformer의 일부 이점을 흡수하기도 했습니다. 그럼에도 전역 receptive field(수용 영역)를 효율적으로 확보하는 것은 여전히 순수 CNN에겐 어려운 과제이고, Transformer 기반 모델들에겐 계산 효율 측면의 도전 과제로 남아 있었습니다.\n\n\n2.2.2 Mamba와 상태 공간 모델(SSM)\n한편, 상태 공간 모델 (State Space Model)은 순차 데이터를 다루는 또 다른 패러다임으로, 연속 시간 역학 시스템으로 입력 시퀀스를 처리하는 기법입니다. 2022년 등장한 S4 모델 등이 대표적이며, 매우 긴 시퀀스도 효율적으로 처리할 수 있다는 점 때문에 주목받았습니다. Mamba는 이러한 SSM 접근을 발전시킨 최신 모델로, 2023년 보고되었으며 Transformer 대비 선형 시간에 복잡도를 억제하면서 유사하거나 더 나은 성능을 달성해 화제가 되었습니다. Mamba의 작동 원리를 간략히 풀어보면, 입력 신호를 연속 미분 방정식 형태의 은닉 상태 $h(t)$로 변환하여 처리한 뒤, 이를 다시 출력으로 변환하는 형태입니다. 이 과정에서 A, B, C 등의 행렬 파라미터가 시퀀스의 동특성을 결정하는데, Mamba는 선택적 스캔(selective scan) 메커니즘을 도입하여 입력에 따라 이 파라미터를 동적으로 조정함으로써 불필요한 부분은 거르고 핵심 정보만 효율적으로 전달하도록 했습니다. 또한 연속 모델을 이산화(discretization)하여 효율을 높이는 등 공학적 최적화도 포함하고 있습니다. 쉽게 비유하면, Transformer가 모든 토큰 쌍 사이를 비교하며 “모든 정보를 한꺼번에 보는” 방식이라면, Mamba는 시간 흐름에 따라 정보를 전달하며 “중요한 정보만 골라 기억하는” 방식이라 할 수 있습니다.\n그러나 이미지 처리를 위한 Mamba에는 넘어야 할 산이 몇 가지 있었습니다. 우선, 이미지 픽셀들은 문장처럼 선형 순서가 있는 것이 아니기 때문에, Mamba가 텍스트에서 하던 것처럼 한 방향으로 순차적으로 픽셀 시퀀스를 처리하는 것은 비효율적입니다. 예를 들어 Mamba가 이미지를 왼쪽 위에서 오른쪽 아래로 한 줄씩 스캔한다고 상상해보면, 상하좌우로 인접한 픽셀들의 지역적 패턴을 놓칠 위험이 있고 전역적인 윤곽 파악도 어렵습니다. 실제로 Vision Mamba (Vim)라는 후속 연구에서는 이러한 문제를 보완하고자 양방향 SSM으로 이미지를 위-왼쪽에서 아래-오른쪽 등 네 방향으로 반복 스캔하는 기법을 도입하기도 했습니다. 하지만 이렇게 모든 방향으로 순차 처리를 하면 글로벌 문맥은 얻을 수 있어도, 모든 토큰을 다 처리할 때까지 출력이 지연되어 효율이 떨어지고 학습도 어려워지는 문제가 있습니다. 또 다른 시도로 EfficientVMamba는 고해상도 구간에는 SSM, 저해상도 구간에는 CNN을 사용하는 혼합 전략을 썼지만, 여전히 전역 컨텍스트를 온전히 반영하지 못해 정확도 면에서 한계가 있었습니다. 종합하면, Mamba류 SSM 기반 모델들은 순차적 제약으로 인한 공간 이해 부족과 글로벌 컨텍스트 활용 미흡이라는 약점을 보이고 있었던 것입니다.\n이러한 배경에서 하이브리드 접근의 필요성이 대두되었습니다. CNN처럼 초기 국소 특징 추출은 빠르게 하고, Mamba의 효율적 시퀀스 처리 능력은 유지하되 부족한 전역 문맥 이해는 Transformer의 자기어텐션으로 보완하면 어떨까 하는 아이디어입니다. 즉, “최고의 정확도를 가장 효율적으로” 얻기 위해 한 가지 기법만 고집하기보다 서로 다른 장점을 가진 구조들을 조합하는 방향으로 연구가 발전한 것입니다. 이전에도 CoAtNet이나 ConvNext 등 CNN+Transformer 혼성 모델들이 있었지만, Mamba와 Transformer를 결합한 시도는 없었습니다. MambaVision은 바로 이러한 첫 도전으로서, Mamba 기반 토큰 믹서(token mixer)에 Transformer 블록을 결합한 새로운 비전 백본을 제안합니다."
  },
  {
    "objectID": "posts/paper/2025-07-19-mambavision.html#mambavision이란-what-is-mambavision",
    "href": "posts/paper/2025-07-19-mambavision.html#mambavision이란-what-is-mambavision",
    "title": "📃MambaVision 리뷰",
    "section": "2.3 MambaVision이란? (What Is MambaVision?)",
    "text": "2.3 MambaVision이란? (What Is MambaVision?)\nMambaVision은 Mamba와 Transformer를 단일 아키텍처 안에서 융합한 계층형(hierarchical) 비전 백본 모델입니다. 한 마디로 요약하면, Mamba의 효율성과 Transformer의 전역 표현력을 모두 잡은 모델이라 할 수 있습니다. 이 모델의 핵심 기여는 크게 두 부분으로 나뉩니다. 첫째, 기존 Mamba 블록을 이미지 처리에 적합하도록 재설계(re-design)하여 “Vision-Friendly” Mamba 토큰 믹서를 만들었다는 것입니다. 자동회귀 특성을 완화하고 추가 경로를 도입하는 등의 개선을 통해, 원본 Mamba 대비 정확도와 처리 효율을 모두 향상시킨 새로운 토큰 믹서가 탄생했습니다. 둘째, 이렇게 개선된 Mamba 토큰 믹서를 Transformer의 자기어텐션 블록과 혼합하여 하이브리드 계층 구조를 구현했다는 점입니다. 여러 가지 통합 방법을 실험한 끝에, 모델의 후반부 층들에 자기어텐션을 적용하는 전략이 최적임을 발견했고, 이를 토대로 MambaVision이라는 다중 해상도 단계형 모델을 완성했습니다.\nMambaVision은 전체적으로 4개의 Stage(단계)로 구성되며, 각 단계에서 입력 해상도를 점진적으로 줄여나가며 특징을 추출합니다. 앞단(Stage 1-2)에서는 CNN 기반 레지듀얼 블록들을 사용하여 고해상도의 입력을 신속히 처리하고 다운샘플링합니다. 후단(Stage 3-4)에서는 MambaVision Mixer 블록과 Transformer 자기어텐션 블록이 결합되어 동작합니다. 구체적으로, 각 Stage의 레이어들 중 첫 절반은 MambaVision Mixer + MLP로 구성되고, 나머지 절반은 Transformer Self-Attention + MLP로 이루어져 있습니다. 이렇게 함으로써 초반에는 Mamba 기반의 효율적 토큰 혼합으로 특징을 추출하고, 후반에는 자기어텐션으로 전역 패턴을 파악하도록 설계되었습니다. 논문에서는 “최종 몇 개 층의 Transformer 블록이 잃어버린 전역 문맥을 회복하고 장거리 의존성을 캡처해준다”고 설명합니다. 다시 말해, MambaVision은 로컬-글로벌 처리의 균형을 계층 구조 내에서 달성한 모델인 것입니다.\n결과적으로 MambaVision 모델군(MambaVision-T, S, B, L 등 크기별 모델)은 ImageNet-1K 이미지 분류 기준으로 이전까지 보고된 어떤 모델보다도 높은 정확도-속도 균형 성능을 달성했습니다. 논문에서는 여러 경쟁 백본들과 Top-1 정확도 대 추론 처리속도(throughput)를 비교한 결과를 제시하며, MambaVision이 새로운 Pareto 최적 선을 그린다고 강조합니다. 특히 동일한 수준의 정확도를 내는 다른 모델들보다 월등히 빠르며, 비슷한 속도에서는 훨씬 높은 정확도를 보이는데, 이는 하드웨어 효율성까지 고려한 Mamba의 장점과 Transformer의 표현력을 모두 활용한 덕분입니다. 또한 MS COCO 객체 검출이나 ADE20K 이미지 분할 등의 다운스트림 과제에서도, MambaVision을 백본으로 사용한 모델이 기존 동급 백본을 쓴 모델보다 일관되게 우수한 성능을 보였다고 보고됩니다. 요약하면, MambaVision은 비전 백본 설계의 새로운 가능성을 보여준 첫 사례로서, Mamba와 Transformer의 만남이 실용적 가치가 있음을 입증한 것입니다."
  },
  {
    "objectID": "posts/paper/2025-07-19-mambavision.html#아키텍처-상세-architecture-details",
    "href": "posts/paper/2025-07-19-mambavision.html#아키텍처-상세-architecture-details",
    "title": "📃MambaVision 리뷰",
    "section": "2.4 아키텍처 상세 (Architecture Details)",
    "text": "2.4 아키텍처 상세 (Architecture Details)\n\n\n\nMambaVision 백본의 계층적 아키텍처 개요. 입력 이미지는 합성곱 기반의 Stem과 Conv Block 단계를 거쳐 다운샘플링되고, Stage 3와 4에서 MambaVision Mixer 블록들과 Self-Attention 블록들이 조합되어 특징을 추출한다.\nMambaVision의 내부 구조를 단계별로 자세히 살펴보겠습니다. 그림 2(위 다이어그램)은 MambaVision의 4-Stage 계층 구조를 보여줍니다. 우선 Stem이라 불리는 입력 처리 단계에서, 이미지가 작은 패치들로 분할되어 몇 차례의 $3$ 합성곱과 스트라이드 2 다운샘플링을 거치며 C 차원 임베딩으로 변환됩니다. 이 Stem은 일종의 간단한 CNN 인코더 역할을 하여, 거대한 해상도의 이미지를 신경망이 다룰 수 있는 수준으로 줄여줍니다. 이어서 Stage 1과 Stage 2에서는 잔차 연결(residual connection)을 갖는 CNN 블록들이 사용됩니다. 각 Stage 사이에는 스트라이드 2의 다운샘플 합성곱이 있어 해상도를 단계적으로 낮추며, 채널 수는 늘려갑니다 (예: Stage 1 출력 채널 $C$에서 Stage 2 출력 $2C$로 증가). 이러한 디자인은 ConvNeXt 등 최신 CNN과 유사한 계층형 피라미드 구조로, 고해상도에서는 합성곱으로 국소 특징을 빠르게 추출하여 효율을 높이기 위함입니다.\nStage 3와 Stage 4가 MambaVision의 핵심 혁신이 들어있는 부분입니다. 각 Stage에는 다수의 레이어가 있는데, 절반은 MambaVision Mixer 블록, 절반은 Transformer Self-Attention 블록으로 구성됩니다. 예를 들어 Stage 3에 8개의 레이어가 있다면 처음 4개는 MambaVision 토큰 믹서를, 나머지 4개는 멀티헤드 자기어텐션을 사용합니다. 각 레이어는 Layer Normalization -&gt; 토큰 믹싱(Mamba 또는 Self-Attention) -&gt; 잔차합 그리고 Layer Normalization -&gt; MLP (두 개의 선형층으로 이루어진 피드포워드) -&gt; 잔차합의 표준 Transformer 스타일 구조를 따릅니다. Stage 3 끝에서 다시 다운샘플링이 한 번 이루어지고, Stage 4에서도 동일한 패턴으로 진행됩니다. 최종적으로 Stage 4 출력 특성맵을 2D 평균 풀링으로 공간 차원을 줄이고, 최종 선형 분류기를 통과시켜 예측을 얻습니다. 이처럼 MambaVision은 CNN+SSM+Transformer가 유기적으로 통합된 형태로, 각 구성 요소가 적재적소에 배치되어 있습니다. CNN은 고해상도 특징 추출을 가속하고, Mamba 기반 토큰 믹서는 중간 단계에서 효율적으로 특징을 융합하며, Transformer 블록은 후반부에서 전역 상호작용을 모델링합니다. 특히 Transformer 블록을 최종 단계에 배치한 것은, 앞 단계까지의 처리에서 놓쳤을 수 있는 글로벌 문맥 정보를 복원하고 장거리 의존성을 포착하기 위한 설계상의 선택입니다.\n\n2.4.1 MambaVision Mixer 블록 설계\nMambaVision 아키텍처의 백미는 MambaVision Mixer라 명명된 토큰 믹싱 블록입니다. 이는 기존 Mamba 블록을 Vision 용도로 개조한 것으로, 간단히 말해 SSM 기반 분지(branch)와 비-SSM 분지 두 갈래로 입력을 처리한 뒤 통합하는 구조입니다. 원본 Mamba Mixer는 한 방향으로만 정보를 흘려보내는 인과적(convolution causal) 1D 컨볼루션과 SSM으로 구성되어 있었는데, 저자들은 이를 개선하기 위해 두 가지 변경을 가했습니다:\n\n1) 인과적 1D 컨볼루션을 일반 1D 컨볼루션으로 대체하였습니다. 인과적 컨볼루션은 시퀀스 처리에서 현재 위치 이후의 정보를 차단하여 순방향 의존만 남기는 역할을 하는데, 이미지에서는 굳이 한쪽 방향으로 제한할 필요가 없기 때문입니다. 오히려 이러한 제약이 한 방향으로만 국소 패턴을 전달하도록 만들어 비전에는 불필요하고 비효율적입니다. 일반 컨볼루션으로 바꿈으로써 양방향 문맥을 모두 활용할 수 있게 됩니다.\n2) SSM이 없는 병렬 분지(branch)를 추가하였습니다. 구체적으로, 입력을 둘로 나누어 하나는 기존처럼 SSM 경로를 따르고 (Conv1D + SSM 연산), 다른 하나는 추가된 1D 컨볼루션 연산 + SiLU 활성화를 통과하도록 하였습니다. 이 두 번째 경로는 순차적 상태 업데이트 없이 즉각적인 공간 처리를 수행하므로, SSM 경로가 잡아내지 못할 수 있는 정적인 정보나 글로벌 패턴을 보완해주는 역할을 합니다. 쉽게 말해, SSM 분기가 “시간적(순차적) 통합”에 초점을 둔다면, 추가된 분기는 “공간적 필터링”을 수행하는 셈입니다.\n\n이 두 경로의 출력은 Concatenation(채널 축 연결)으로 합쳐지고, 다시 선형 결합을 통해 원래 차원으로 투사(projection)됩니다. 이렇게 출력 피처는 두 가지 분기의 정보를 모두 포함하게 되며, 최종적으로 잔차 연결을 통해 입력과 더해집니다. 저자들은 이러한 대칭 분기 설계를 통해 SSM의 순차 제약으로 인해 손실될 수 있는 콘텐츠를 보완하고, 두 분기의 장점을 모두 살린 표현을 얻을 수 있다고 설명합니다. 중요하게도, 새로 추가된 분기로 인해 파라미터 수가 크게 늘지 않도록 각 분기의 출력 채널을 절반으로 줄이는 등 크기 균형도 맞추었습니다.\n이 MambaVision Mixer 블록의 구조를 그림으로 나타낸 것이 논문 Figure 3입니다. 해당 그림에는 SSM 경로(좌측, SSM 블록 및 관련 Conv1D)와 신규 경로(우측, Conv1D 및 SiLU)가 병렬로 그려져 있고, 최종 Linear로 합쳐지는 모습이 묘사되어 있습니다. 결과적으로 MambaVision Mixer는 순차적 처리의 장점과 병렬 공간 처리의 장점을 모두 활용하는 커스텀 토큰 믹서로 볼 수 있습니다. 이 Mixer 블록 하나만 놓고 봐도 원본 Mamba 대비 큰 성능 향상이 있었는데, 논문 부록에 공개된 실험을 보면:\n\n아무 수정 없는 원래 Mamba 토큰 믹서는 ImageNet-1K Top-1 정확도 약 80.9%, ADE20K 분할 mIoU 44.2 등에 그쳤지만,\n인과 컨볼루션을 일반 컨볼루션으로 교체하자 모든 지표가 소폭 상승했고,\n대칭 Conv 분기(Conv2) 추가까지 했더니 Top-1 81.3%, mIoU 45.7%로 향상되었습니다,\n최종적으로 출력 결합 방식을 gating 대신 concat으로 바꾸는 최종 설계에서 Top-1 82.3%, mIoU 46.0%까지 성능이 도약했습니다.\n\n이는 MambaVision Mixer 설계가 비전 작업에 매우 효과적임을 보여줍니다. 정리하면, MambaVision의 성공은 단순히 Mamba와 Transformer를 결합했다는 데 그치는 것이 아니라, Mamba 자체를 비전에 최적화하여 업그레이드한 덕분이라고 할 수 있습니다. 이 Mixer 블록을 통해 모델은 순차적 장기 의존성과 공간적 글로벌 문맥을 모두 포착하는 강력한 토큰 혼합을 수행하며, 이후 이어지는 Transformer 자기어텐션 층들과 시너지를 냅니다. Transformer 블록은 표준 멀티헤드 self-attention을 사용하되, 윈도우 크기 등을 적절히 조절하여 높은 해상도에서도 효율적으로 동작하도록 설계되었습니다. 최종 Stage의 self-attention은 resolution이 많이 내려간 ($7$ 등) 상태이기에 부담이 크지 않고, 대신 이미지 전역의 정보를 통합해줍니다.\n요컨대 MambaVision 아키텍처는 “CNN + 개선된 Mamba Mixer + Transformer”가 단계별로 배치된 형태로, 각 구성의 장점을 최대한 발휘하도록 정교하게 조합되어 있습니다. 초기 Stage에서는 CNN이 로컬 패턴을 포착하고 다운샘플링하여 효율성 극대화, 중간 Stage에서는 MambaVision Mixer가 중/장기 의존성을 빠르게 통합, 후반 Stage에서는 Transformer 어텐션이 전역 관계를 모델링함으로써, 최종적으로 고속추론에도 SOTA급 정확도를 내는 백본을 완성한 것입니다."
  },
  {
    "objectID": "posts/paper/2025-07-19-mambavision.html#실험-결과-experimental-results",
    "href": "posts/paper/2025-07-19-mambavision.html#실험-결과-experimental-results",
    "title": "📃MambaVision 리뷰",
    "section": "2.5 실험 결과 (Experimental Results)",
    "text": "2.5 실험 결과 (Experimental Results)\n논문에서는 MambaVision의 우수성을 입증하기 위해 다양한 비전 벤치마크에서의 성능을 측정했습니다. 이미지 분류(ImageNet-1K)부터 객체 검출/분할(MS COCO), 장면 분할(ADE20K)에 이르기까지 폭넓은 평가에서 동일 세대 최고 모델들과 비교가 이루어졌습니다. 주요 결과를 요약하면 다음과 같습니다.\n\n2.5.1 ImageNet-1K 분류 성능\nImageNet-1K 데이터셋에 대한 Top-1 분류 정확도와 추론 속도 비교에서, MambaVision 모델들은 현 시점 최고의 균형 성능을 보여주었습니다. 예컨대 MambaVision-B 모델(약 98M 파라미터)은 Top-1 정확도 84.2%를 기록하며, 비슷한 크기의 ConvNeXt-B(88M param, 83.8%)나 SwinV2-S/B(50-88M param, 83.8~84.6%) 등을 앞질렀습니다. 놀라운 점은 추론 속도(이미지/초 기준)에서 MambaVision-B가 3670 Img/sec로, ConvNeXt-B의 1485 Img/sec 대비 2배 이상 빠르고 Swin과 비교하면 수배에 달하는 속도를 냈다는 것입니다. 즉, MambaVision-B는 더 정확하면서도 훨씬 빠른 모델인 셈입니다. 작은 모델에서도 유사한 양상이 나타나, MambaVision-T(Tiny급 32M param)은 82.3% 정확도로 ConvNeXt-T(29M, 82.0%)와 거의 동등하지만 속도는 6298 Img/sec로 ConvNeXt-T(3196 Img/sec)의 두 배에 달합니다. 이는 EfficientFormer나 NextViT 같은 기존 효율 모델들을 크게 앞지르는 수치입니다. 요약하면, MambaVision 시리즈는 모델 크기 전 범위에 걸쳐 최신 ConvNeXt, Swin 등의 정확도를 능가하거나 동등하면서도, 추론 처리량은 월등히 높아 새로운 Pareto 프론티어를 구축했습니다.\n이러한 경향을 한눈에 보여주는 것이 논문 Figure 1의 정확도-처리량 분포 그래프입니다. 해당 그래프에서 우측상단 방향으로 볼록하게 형성된 최선의 경계가 MambaVision 모델들로, 기존 EfficientNet, Swin, ViT 계열 및 다른 Mamba 기반 모델(Vim, VMamba 등)들이 그보다 아래쪽에 위치합니다. 특히 이전의 Mamba 기반 비전 모델들이 최고 83%대 정확도에 그쳤던 것에 비해, MambaVision은 이를 최대 약 85% 수준까지 끌어올렸습니다. 동시에 동일 정확도에서의 속도는 수배 향상되어, 예를 들어 VMamba-B(89M param, 83.9%)가 645 Img/sec에 불과했던 것과 대조적으로 MambaVision-B는 3670 Img/sec에 달합니다. 한 마디로 더 적은 연산으로 더 높은 성능을 얻는 데 성공한 것입니다.\n또한 저자들은 Mamba 기반 모델로서는 최초로 ImageNet-21K 거대 데이터셋으로 MambaVision을 사전훈련해 보는 실험도 했습니다. 그 결과 사전훈련을 거친 MambaVision은 더욱 향상된 성능을 보였는데, 예를 들어 MambaVision-B의 Top-1 정확도가 84.2% -&gt; 84.9%로 올라갔습니다. 대규모 데이터에서도 안정적으로 학습이 가능하며, 모델 스케일을 키워도 성능이 꾸준히 향상함을 확인한 것입니다 (논문의 Figure 4 그래프 참고). 이는 MambaVision이 확장성(scale-up) 측면에서도 잠재력이 있음을 시사합니다.\n\n\n2.5.2 객체 검출 및 분할 성능 (COCO, ADE20K)\n다음으로 다운스트림 비전 과제에 대한 백본으로서의 성능입니다. 논문에서는 MS COCO 데이터셋의 객체 검출 및 인스턴스 분할에 Cascade Mask R-CNN 프레임워크를 사용하여 실험하였고, ADE20K 데이터셋의 장면 분할에는 UPerNet 모델로 실험하였습니다. 모든 경우에서 동일한 조건(동일 헤드와 학습 스케줄) 하에 백본만 교체하여 비교했는데, MambaVision 백본이 들어간 모델이 일관되게 더 나은 결과를 보였습니다.\n\nCOCO 검출/분할: MambaVision-T/S/B 각각을 백본으로 썼을 때 박스 mAP 및 마스크 mAP 지표가, ConvNeXt-T/S/B 대비 +0.1~+0.7 정도씩 높게 나타났습니다. 예를 들어, MambaVision-B 백본은 박스 AP 52.8 / 마스크 AP 45.7을 기록하여 ConvNeXt-B 백본(52.7 / 45.6)보다 약간 높고, Swin-B 백본(51.9 / 45.0)보다는 박스 AP +0.9, 마스크 +0.7 정도 개선되었습니다. 향상 폭이 아주 크지는 않지만 일관되게 우세하며, 파라미터나 FLOPs 등이 비슷한 조건에서 얻은 개선이라 의미가 있습니다. 무엇보다, 처리 속도를 높이면서도 정확도 손실이 없다는 것이 중요한데, MambaVision 백본을 사용하면 동일 프레임워크에서 추론 FPS도 올라갈 것으로 기대되므로 실용적입니다.\nADE20K 분할: Semantic segmentation 작업에서도 mIoU 지표 상의 개선이 확인되었습니다. MambaVision-T, S, B 백본은 각각 46.0, 48.2, 49.1% mIoU를 기록했는데, 이는 대응되는 Swin-T/S/B 백본 사용 대비 +1.5, +0.6, +1.0% 향상된 수치입니다. ConvNeXt 계열과 비교해서도 비슷하거나 더 나은 성능을 보였습니다. 특히 MambaVision-B의 49.1% mIoU는 동급 백본 중 최고 수준으로, 기존 Focal Transformer-B(49.0%)나 Twins-L(48.8%) 등을 근소하게 앞질렀습니다. 이러한 이득들은 특별한 하이퍼파라미터 튜닝 없이 기본 설정으로 얻은 것이어서 더욱 고무적입니다. 즉, MambaVision은 분류뿐 아니라 검출/분할 같은 다운스트림 과제에도 범용적으로 강한 백본임을 보여줍니다.\n\n전반적으로 실험 결과들은 MambaVision의 효율적 설계가 다양한 비전 작업의 성능을 향상시킴을 입증합니다. 특히 추론 속도 향상과 정확도 향상이 동시에 이루어졌다는 점에서, 단순한 정확도 승부를 넘어 실제 적용 효율까지 고려한 큰 진전이라 평가할 만합니다."
  },
  {
    "objectID": "posts/paper/2025-07-19-mambavision.html#분석-및-논의-analysis-and-discussion",
    "href": "posts/paper/2025-07-19-mambavision.html#분석-및-논의-analysis-and-discussion",
    "title": "📃MambaVision 리뷰",
    "section": "2.6 분석 및 논의 (Analysis and Discussion)",
    "text": "2.6 분석 및 논의 (Analysis and Discussion)\nMambaVision의 성공 요인을 조금 더 깊이 들여다보고, 그 의미를 분석해보겠습니다. 이 모델이 보여주는 바는 “하이브리드 백본” 접근이 얼마나 강력할 수 있는지입니다. MambaVision은 Transformer류 모델의 세계와 SSM(Recurrent) 모델의 세계를 절묘하게 결합함으로써, 양쪽의 장점을 취하고 단점을 보완했습니다. 그 결과 기존 Conv-Transformer 하이브리드(예: CoAtNet 등) 수준을 뛰어넘는 새로운 SOTA 달성이 가능했습니다.\n먼저, MambaVision Mixer 블록의 도입은 Mamba 구조의 한계였던 공간적 맥락 파악 문제를 효과적으로 해결했습니다. SSM만으로 이미지를 처리할 때 문제가 되었던 한 방향 제약을 없애고, 별도 분기를 통해 병렬적인 공간 특징 추출을 함으로써 글로벌 컨텍스트를 확보한 것이 주효했습니다. 실제 ablation 연구에서도, 기존 Mamba 대비 제안한 변화(인과 컨브 제거, 대칭 분기 추가, concat 통합)를 하나씩 적용할 때마다 ImageNet, COCO, ADE20K 지표가 모두 상승했음을 보여주었습니다. 최종적으로 concat 방식 통합이 가장 성능이 좋았는데, 이는 두 분기 출력을 선형 결합하여 융합하는 것이 gating 등보다 효과적임을 의미합니다. 이 덕분에 모델은 순차적 특징과 공간적 특징의 “풍부한 표현”을 얻게 되었고, 결과적으로 다양한 다운스트림 태스크에서 경쟁력 있는 표현 학습이 가능해졌습니다.\n둘째, Transformer 자기어텐션의 통합 위치에 대한 저자들의 전략이 옳았다는 점입니다. 초기에는 “과연 Mamba와 Transformer 블록을 어떻게 섞는 것이 최선일까?” 하는 의문이 있었는데, 논문에서는 다양한 배치 패턴을 실험했습니다. 일부 레이어에만 랜덤하게 Transformer를 넣어보기도 하고, Stage 앞부분에 집중 배치하거나 교차 배치하는 등 시도를 했지만 대부분 최적이 아니었습니다. Transformer 블록을 너무 이른 단계(해상도가 높을 때)에 넣으면 연산량이 커져 비효율적일 뿐 아니라 성능 이득도 크지 않았습니다. 교차로 번갈아 배치하는 패턴은 서로 다른 토큰 믹서간 상호 간섭이 생겨 성능이 떨어졌습니다. 반면, 마지막 몇 개 레이어에 연속 배치하는 패턴이 가장 효과적이었는데, 특히 각 Stage의 최후반 절반을 Transformer로 할애했을 때 성능이 최고로 나타났습니다. 구체적으로, Self-Attention을 Stage 후반 N/2 레이어에만 적용한 최종 설계는 Top-1 정확도 82.3%로, 다른 패턴보다 0.5~1% 가량 높았습니다. 이는 “자기어텐션은 마지막에 몰아서 쓰는 것이 좋다”는 논문의 가설과 일치하는 결과로, 결국 현재 MambaVision의 구조로 채택되었습니다. 이로써 모델은 SSM의 효율과 어텐션의 전역성을 균형 있게 획득한 것입니다.\n한 가지 흥미로운 분석은 MambaVision의 Self-Attention 레이어들이 실제로 무엇을 학습하였는가입니다. 논문에서는 최종 Transformer 블록들의 어텐션 맵을 시각화하여, 이들이 의미론적으로 중요한 영역에 집중하고 있음을 보였습니다. 예를 들어, 비행기 이미지의 경우 어텐션 헤드가 비행기 전체 윤곽에 폭넓게 활성화되어 대상 전체 형태를 포착했고, 새(bird) 이미지에서는 한 헤드가 새의 머리와 꼬리 등 독특한 부분을 집중적으로 바라보는 등 세밀한 부분까지 구분했습니다. 사람 이미지에서는 손에 든 물체와 얼굴 등 상호 작용하는 두 요소를 모두 강조하여, 장면 내 요소들 간 관계를 이해하고 있음을 보여줬습니다. 이러한 시각적 해석은, MambaVision의 자기어텐션이 단순히 형식적으로 추가된 것이 아니라 실제로 글로벌 의존성과 의미 있는 특징들을 학습하고 있음을 뒷받침합니다. 달리 말해, 하이브리드 구조의 시너지가 내부 표현에서도 확인된 것입니다.\n물론 MambaVision에도 몇 가지 고려할 점이 있습니다. 첫째, Transformer 블록의 추가로 순수 Mamba 모델에 비해서는 구조가 다소 복잡해졌고, SSM과 어텐션 모두를 구현해야 하므로 구현상의 부담이 있습니다. 하지만 논문에서 공개한 코드와 PyTorch 구현이 이를 잘 추상화하고 있어 실용적 사용에는 큰 지장이 없을 것으로 보입니다. 또한 MambaVision이 뛰어난 처리량을 보이긴 하지만, 여기에는 NVIDIA A100 GPU에서의 최적화가 작용한 면이 있습니다. 실제 임베디드 환경 등에서 SSM 연산이 최적화되지 않으면 이론적인 이점이 모두 나타나지 않을 수 있으므로, 플랫폼에 따른 성능 편차는 추후 검증이 필요합니다. 그럼에도 불구하고, 동일한 하드웨어 조건에서 Transformer나 CNN 기반 모델보다 빠르다는 점은 매우 고무적입니다.\n또 하나, MambaVision의 정확도 향상 폭은 EfficientNet에서 ViT로 갈 때처럼 파격적이진 않지만, 효율 향상과 동반되었다는 점을 기억해야 합니다. 예컨대 ConvNeXt-B 대비 +0.4%p 정확도 상승은 얼핏 작아 보일 수 있으나, 속도를 2배 이상 내면서 이뤄낸 성과입니다. 실제 대규모 서비스나 응용에서는 처리 비용 절감이 곧 성능만큼 중요하기 때문에, 이러한 trade-off 개선은 실질적인 가치가 큽니다. 더 나아가, MambaVision은 크기 확장을 통해 상한선에 도달하지 않고 더 개선될 여지도 있음을 보여주었습니다 (L, L2 모델에서 85%+ 정확도 달성). 따라서 추가 데이터나 모델 확장을 통해 향후 86~87%대까지도 노려볼 수 있을 것입니다.\n하이브리드 접근의 시사점: MambaVision의 등장은 향후 비전 모델 설계에 몇 가지 교훈을 줍니다. 하나는, 이질적인 모델 패러다임의 결합이 생각以上의 시너지를 낼 수 있다는 점입니다. 그동안 Conv와 Transformer의 결합은 많이 시도됐지만, 시퀀스 모델(SSM)과 Transformer의 결합은 새 영역입니다. 이 연구를 통해 SSM 계열도 충분히 vision-friendly하게 개조 가능하고, Transformer와 상호보완적으로 작동할 수 있음이 증명되었습니다. 이는 앞으로 다른 SSM 변종(예: S4, Linear RNN 등)과 Transformer를 결합하거나, 더 나아가 CNN+SSM+Transformer 삼원 혼합 구조를 탐색하는 등 새로운 하이브리드 모델 연구의 가능성을 엽니다. 또한 MambaVision은 고해상도 입력 처리에서 CNN 사용, 저해상도에서는 SSM/어텐션 사용이라는 설계를 취했는데, 이는 “어떤 해상도 레벨에서 어떤 토큰 믹서를 쓰는 게 최적인가”에 대한 하나의 해답입니다. 향후 모델들은 해상도 단계별로 Conv, SSM, Attention을 적절히 배치하는 네트워크 검색을 통해 더욱 발전할 수 있을 것입니다."
  },
  {
    "objectID": "posts/paper/2025-07-19-mambavision.html#결론-conclusion",
    "href": "posts/paper/2025-07-19-mambavision.html#결론-conclusion",
    "title": "📃MambaVision 리뷰",
    "section": "2.7 결론 (Conclusion)",
    "text": "2.7 결론 (Conclusion)\nMambaVision은 Mamba(SSM)와 Transformer를 결합한 최초의 하이브리드 비전 백본으로서, 효율과 성능의 새 기준을 세웠습니다. 이 모델은 Mamba의 공식을 비전 도메인에 맞게 재설계하고, 최적의 방식으로 Transformer 블록을 통합함으로써 전역 문맥 표현 학습 능력을 크게 향상시켰습니다. 그 결과 ImageNet-1K 분류에서 Top-1 정확도와 처리량 측면의 새로운 SOTA Pareto 프론티어를 달성했고, 다양한 비전 과제(분류, 검출, 분할)에서 동급 모델 대비 뛰어난 성능을 입증했습니다. 특히 최종 단계에 자기어텐션을 배치하는 하이브리드 구조를 통해 장거리 의존성을 효과적으로 포착하면서도 효율을 유지할 수 있음을 보였고, 대규모 데이터셋과 큰 모델에서도 경쟁력 있는 성능을 내며 대규모 비전 응용 가능성도 보여주었습니다.\nMambaVision의 성공은 기존의 Mamba 기반 아키텍처의 한계를 극복하면서 그 장점을 살린 훌륭한 사례로, 미래의 비전 백본 설계에 새로운 방향을 제시합니다. 이제 연구자들은 하이브리드 토대 위에서 더 다양한 조합과 변형을 시도해볼 수 있게 되었습니다. 예를 들어, MambaVision을 토대로 크로스모달(Vision+Language) 모델이나 비디오 처리로 확장하는 연구, 또는 SSM 대신 다른 효율 시퀀스 모듈과의 결합 등이 기대됩니다. 결국 중요한 것은 장점은 극대화하고 단점은 보완하는 균형 잡힌 설계이며, MambaVision이 그 성공 가능성을 보여준 만큼 앞으로 이종 모델 융합을 통한 새로운 클래스의 비전 모델들이 속속 등장할 것으로 보입니다. MambaVision 자체도 향후 실시간 서비스나 경량화 연구 등으로 발전하면서, 보다 넓은 범위의 비전 응용을 위한 든든한 기반이 될 것으로 전망됩니다."
  },
  {
    "objectID": "posts/paper/2025-07-28-reward-engineering.html",
    "href": "posts/paper/2025-07-28-reward-engineering.html",
    "title": "📃Reward Engineering 리뷰",
    "section": "",
    "text": "Paper Link"
  },
  {
    "objectID": "posts/paper/2025-07-28-reward-engineering.html#서론",
    "href": "posts/paper/2025-07-28-reward-engineering.html#서론",
    "title": "📃Reward Engineering 리뷰",
    "section": "서론",
    "text": "서론\n강화학습(Reinforcement Learning, RL)은 에이전트(Agent)가 환경(Environment)과 상호작용하며 시도를 통해 최적의 행동 전략을 학습하는 기계학습의 한 분야입니다. RL 에이전트는 누적 보상(cumulative reward)을 최대화하도록 학습하며, 보상(reward)은 에이전트가 받는 피드백 신호로서 행동의 성과를 나타냅니다. 따라서 보상 함수의 설계는 RL 알고리즘의 효율성과 성능에 지대한 영향을 미치는 핵심 요소입니다. 잘 설계된 보상 함수는 에이전트에게 명확한 행동 지침을 주어 원하는 목표에 보다 빠르게 도달하도록 만들지만, 부적절한 보상 설계는 의도치 않은 행동이나 학습 지연을 초래할 수 있습니다.\n보상 설계(reward design)는 원하는 에이전트 행동과 목표에 부합하도록 보상 함수를 정의하는 섬세하고도 복잡한 과정입니다. 보상 설계의 중요성은 아무리 강조해도 지나치지 않으며, 실제 환경이 복잡해질수록 그 중요성은 더욱 커집니다. 일반적으로 보상 설계의 접근법은 두 가지 범주로 나뉩니다. 첫째, Reward Engineering(Reward Engineering)은 말 그대로 보상 함수를 만들어내는 작업으로, 상태와 행동 (및 결과 상태)을 특정 수치 보상으로 매핑하는 함수를 처음부터 설계하는 것입니다. 잘 설계된 보상 함수는 에이전트의 행동에 대해 정보성 있는 피드백을 제공하여 바람직한 행동을 강화해야 합니다. 동시에 너무 빈번한 보상은 에이전트가 쉬운 편법을 찾아내는 (reward hacking) 부작용을 낳을 수 있으므로, 적절한 희소성도 필요합니다. 둘째, Reward Shaping(Reward Shaping)은 기본 보상 함수를 설정한 후 학습 과정을 개선하기 위해 보상 신호를 세밀 조정하는 기법입니다. Reward Shaping은 기존의 최적 정책(optimal policy)을 바꾸지 않으면서 학습을 가속하기 위해 추가적인 보상 피드백을 주는 것을 목표로 합니다. 예를 들어, 로봇 팔 제어 과제에서 목표물까지의 음의 거리를 잠재적 보상(potential reward)으로 설정하면, 에이전트는 최종 성공 여부와 관계없이 목표물에 가까워질 때마다 추가 보상을 받아 목표에 점진적으로 다가가도록 유도할 수 있습니다.\n이처럼 Reward Engineering과 Reward Shaping은 RL 에이전트가 효율적으로 원하는 행동을 학습하도록 하는 강력한 도구입니다. 그러나 현실 세계의 RL에는 해결해야 할 여러 어려움이 남아 있습니다. 희소하거나 지연된 보상은 대표적인 문제로, 보상이 드물게 주어지면 에이전트의 학습 진행이 매우 더뎌질 수 있습니다. 또한 실제 환경의 동역학을 정확히 모델링하기 어렵고, 고차원 상태·행동 공간에서 RL 알고리즘을 학습시키는 데 막대한 계산 비용이 든다는 한계도 지적됩니다. 다행히 근래 딥러닝의 발전으로 복잡한 상태공간을 다루는 능력이 향상되어 로보틱스, 자율주행, 게임 등 다양한 복잡한 과제에도 RL이 응용되고 있지만, 이러한 분야에서 RL을 성공적으로 적용하려면 여전히 효과적인 보상 설계/형성 기법이 필요합니다. 본 종설 논문은 이러한 배경하에, 강화학습의 성능을 극대화하기 위한 Reward Engineering 및 Reward Shaping 기법들의 현재까지 연구 동향을 포괄적으로 정리하고 각 접근법의 장단점을 분석합니다. 이를 통해 RL 분야 연구자들에게 향후 연구 방향과 실제 응용 시 고려할 통찰을 제공하는 것이 목적입니다."
  },
  {
    "objectID": "posts/paper/2025-07-28-reward-engineering.html#연구-방법-문헌-조사-및-선정-과정",
    "href": "posts/paper/2025-07-28-reward-engineering.html#연구-방법-문헌-조사-및-선정-과정",
    "title": "📃Reward Engineering 리뷰",
    "section": "연구 방법 (문헌 조사 및 선정 과정)",
    "text": "연구 방법 (문헌 조사 및 선정 과정)\n이 논문은 Reward Shaping 기법에 관한 기존 연구들을 체계적 문헌조사(systematic literature review) 방식으로 종합했습니다. 저자들은 주요 학술 데이터베이스에서 “reward shaping”, “reward engineering”, “reinforcement learning”, “reward design”, “control systems” 등의 키워드를 조합하여 1999년부터 2024년까지 출판된 관련 문헌을 검색했습니다. 검색된 논문들은 PRISMA 2020 지침에 따라 단계적으로 선별되었으며, 최종적으로 55편의 핵심 연구가 리뷰에 포함되었습니다. 이들 논문은 주로 인공지능 기반 제어 시스템에서의 Reward Shaping 기법을 다루며, 실험적 결과를 보고한 연구들로 제한하였습니다. 다만 이론적 프레임워크만 제시하거나 실증 데이터가 없는 논문들은 배제되었으며, 예외적으로 영향력이 크다고 판단된 몇몇 이론 논문은 검토 대상에 포함되었습니다. 또한 포함된 각 연구의 품질을 평가하기 위해 사전에 정의한 기준에 따라 품질 평가도 수행하였습니다. 이러한 체계적인 문헌 조사 과정을 통해, 본 리뷰는 강화학습에서의 보상 설계 및 형성에 관한 신뢰도 높은 최신 지식들을 종합적으로 정리하였습니다."
  },
  {
    "objectID": "posts/paper/2025-07-28-reward-engineering.html#배경-및-기본-개념",
    "href": "posts/paper/2025-07-28-reward-engineering.html#배경-및-기본-개념",
    "title": "📃Reward Engineering 리뷰",
    "section": "배경 및 기본 개념",
    "text": "배경 및 기본 개념\n강화학습의 구성 요소: RL을 이해하기 위해 기본 개념을 짚고 넘어가면 다음과 같습니다:\n\n에이전트(Agent): 환경과 상호작용하며 학습하는 주체입니다.\n환경(Environment): 에이전트가 행동을 수행하는 세계로, 정해진 규칙과 동적 특성을 가지고 있습니다.\n정책(Policy): 주어진 상태에서 에이전트가 취할 행동을 결정하는 전략 또는 함수입니다.\n보상(Reward): 에이전트의 행동 결과로 주어지는 신호로, 특정 상태나 행동의 바람직함을 나타냅니다.\n가치 함수(Value Function): 특정 상태(또는 상태-행동 쌍)에 대해 미래에 얻을 누적 보상의 기대값을 추정하는 함수입니다.\n탐험 vs 활용(Exploration vs Exploitation): 새로운 행동을 시도하여 정보를 얻는 탐험과, 현재 학습한 최적 행동을 사용하는 활용 간의 균형을 말합니다.\n\nReward Shaping과 Reward Engineering의 정의: Reward Shaping과 Reward Engineering은 앞서 서론에서 설명한 대로 보상 설계의 두 축입니다. Reward Shaping(reward shaping)은 동물 훈련에서 영감을 얻은 개념으로, 추가적인 보상을 제공하여 학습 문제를 쉽게 만드는 기법입니다. 즉, 에이전트의 학습을 가이드하기 위해 원래의 보상 함수에 인센티브나 패널티를 덧붙여 보상 신호를 조정하는 것을 의미합니다. 이에 반해 Reward Engineering(reward engineering)은 보다 폭넓은 개념으로, 기존 알고리즘이나 기법을 활용해 보상 함수를 설계하거나 아예 처음부터 보상 함수를 만들어내는 모든 접근을 포괄합니다.\n“보상으로 충분하다” vs “보상만으로는 부족하다” 논쟁: 인공지능 연구자들 사이에서는 단일 스칼라 보상 신호만으로도 충분히 지능적 행동을 끌어낼 수 있는지에 대한 논쟁이 있습니다. 첫 번째 관점인 “보상이 충분하다(Reward is Enough)” 가설에 따르면, 하나의 스칼라 보상 값만 최대화하도록 학습시켜도 복잡한 인지능력이 저절로 발현될 수 있다고 주장합니다. 이 입장에서는 단일 보상 목표를 향해 나아가는 과정에서, 환경의 복잡성이 자연스럽게 언어 이해나 사회성 같은 고차원 능력을 끌어낸다고 봅니다. 반면 “스칼라 보상만으로는 부족하다(Scalar Reward is Not Enough)”라는 반론은 인간 지능의 다면적 특성을 단일 수치로 표현하기 어렵다고 지적하며, 벡터 보상(vector-valued reward) 등 다중 목표를 도입해야 안전하고 인간과 조화된 AI를 만들 수 있다고 주장합니다. 특히 윤리적 판단이나 주관적 목표가 필요한 영역에서는 여러 지표를 함께 고려해야 올바른 의사결정을 할 수 있다는 것입니다. 양측 모두 보상의 중요성은 인정하지만, 단일 보상의 충분성에 대해서는 이처럼 견해 차이가 존재합니다. 결국 실제 적용에서는 문제 특성에 따라 단일 보상의 단순함과 다중 보상의 풍부함 사이에서 적절한 선택을 해야 하며, 복잡한 인간 지능을 모사하려면 더 섬세한 보상 표현이 필요할 수 있음을 시사합니다.\n보상 설계 시 일반적인 함정들: 효과적인 보상 함수를 만들기란 쉽지 않습니다. 잘못된 보상 설계는 에이전트의 엉뚱한 행동을 유발하거나 학습 실패로 이어질 수 있는데, 이런 위험요소들을 미리 파악하는 것이 중요합니다. 보상 설계의 대표적인 함정(pitfall)들은 다음과 같습니다:\n\n보상의 희소성(Sparsity): 보상 신호가 너무 드물거나 지연되어 제공되면 에이전트는 어떤 행동이 좋은지 학습하기 어려워져 학습 속도가 매우 느려집니다.\n기만적 보상(Deceptive Reward): 설정한 보상이 의도와 달리 에이전트에게 쉬운 편법을 장려할 수 있습니다. 즉, 표면적인 보상만 높이고 실제 목표는 달성하지 못하는 행동을 취하게 만들 수 있습니다.\n보상 해킹(Reward Hacking): 에이전트가 보상 함수를 악용하는 현상으로, 원하는 목표는 이루지 않은 채 보상만 최대화하는 치트 행동을 찾는 경우입니다. 예를 들어 청소 로봇에게 “깨끗한 방 유지”에 보상을 주었더니, 로봇이 실제 청소는 하지 않고 센서를 가려 더러운 것을 인식하지 못하게 만드는 식입니다.\n의도치 않은 부작용(Unintended Consequences): 복잡한 환경에서는 보상 설계로 인해 예상치 못한 부작용이 나타날 수 있습니다. 에이전트 행동과 환경의 상호작용이 예측 불가능하여, 사소한 보상 항목이 큰 문제를 일으키기도 합니다.\n목표와 보상의 불일치(Misalignment): 보상 함수가 개발자의 진짜 의도를 완벽히 대변하지 못하는 경우입니다. 이 경우 에이전트는 높은 보상을 받아도 정작 우리가 원한 목표는 달성하지 못할 수 있습니다.\n복잡한 보상 함수(Complexity): 보상에 너무 많은 요소를 넣으면 보상 함수를 설계하고 해석하기가 어려워집니다. 다목적 보상은 그 가중치를 정하기 힘들고, 잘못하면 어느 한 쪽도 만족시키지 못할 우려가 있습니다.\n평가의 어려움(Evaluation Difficulty): 복잡한 환경일수록 한 번 정한 보상 구조가 제대로 작동하는지 객관적으로 평가하기 힘듭니다. 결국 시행착오를 거쳐야 하는데, 이 과정 또한 비용이 큽니다.\n\n위와 같은 이유들 때문에, 기존의 단순 보상 설계만으로는 한계가 있을 때 Reward Engineering적 기법들이 대안으로 활용됩니다. 즉, 보다 체계적이고 발전된 방법으로 보상 함수를 개선함으로써 이러한 함정을 피하려는 것입니다. 또한 한 가지 기억해야 할 점은, 보상 함정은 완전히 없앨 수는 없더라도 반복적인 테스트와 검증(iterative testing & validation)을 통해 발견하고 수정해야 한다는 것입니다. 에이전트가 보상을 잘못 최대화하고 있지는 않은지 지속적으로 관찰하여, 의도대로 행동하도록 보상 구조를 다듬어나가야 합니다.\n스칼라 보상 vs 벡터 보상: 앞서 언급한 논쟁과 관련하여, 보상을 단일 스칼라 값으로 줄 것이냐 아니면 다차원 벡터로 구성할 것이냐도 중요한 설계 선택입니다. 스칼라 보상은 한 가지 수치로 목표 진척도를 나타내므로 단순하고 명확하며 계산도 용이합니다. 예를 들어 게임에서 이겼으면 +1, 졌으면 0과 같이 하나의 값만 주면 되니 해석도 쉽습니다. 그러나 복잡한 과제에서는 스칼라 보상 하나로는 과제의 여러 측면을 모두 반영하기 어렵습니다. 그 결과 에이전트가 중요한 세부 요소를 놓치거나 국소 최적해에 빠질 위험이 있습니다. 반면 벡터 보상은 여러 개의 값을 통해 과제의 다양한 측면(예: 속도, 정확도, 자원 소모 등)을 평가하므로 더 풍부한 피드백을 제공합니다. 에이전트는 어떤 측면에 더 주력해야 하는지 학습할 수 있어 다목적 문제에서 유용합니다. 다만 벡터 보상을 사용하면 여러 보상 요소 간 트레이드오프를 다뤄야 하고, 보상 공간이 다차원이므로 학습이 복잡해지며 계산비용도 증가합니다. 따라서 과제의 특성, 요구 성능 수준, 계산 자원 등을 고려하여 스칼라 vs 벡터 보상을 결정하게 됩니다. 일반적으로 단순하고 명확한 목표에는 스칼라 보상이 좋지만, 안전성 등 다양한 목표를 동시에 달성해야 하는 문제라면 벡터 보상이나 다중 목표 접근이 필요할 수 있습니다."
  },
  {
    "objectID": "posts/paper/2025-07-28-reward-engineering.html#reward-shapingengineering-기법의-분류",
    "href": "posts/paper/2025-07-28-reward-engineering.html#reward-shapingengineering-기법의-분류",
    "title": "📃Reward Engineering 리뷰",
    "section": "Reward Shaping/Engineering 기법의 분류",
    "text": "Reward Shaping/Engineering 기법의 분류\n이 섹션에서는 다양한 Reward Shaping 및 Reward Engineering 기법들을 그 원리에 따라 분류하고, 각 범주별 주요 알고리즘과 특징, 장단점을 상세히 살펴봅니다. 인간의 동기부여 이론에 빗대어 심층강화학습의 보상도 외재적 보상(extrinsic reward)과 내재적 보상(intrinsic reward)으로 구분할 수 있습니다. 외재적 보상은 환경으로부터 주어지는 명시적인 보상(예: 게임에서의 점수)이고, 내재적 보상은 에이전트 스스로 부여하는 보상(예: 새로운 상태를 발견했을 때의 만족감)으로 탐험 동기를 부여합니다. 실제로 RL 연구에서는 이러한 내재적 동기(intrinsic motivation)를 활용하여 에이전트가 희소한 외재적 보상 상황에서도 학습을 지속하도록 하는 기법들이 발전해왔습니다. 이처럼 보상에는 여러 종류와 활용법이 존재하며, 아래에서는 최신 연구들을 중심으로 다양한 Reward Eningeering/Shaping 전략을 기술합니다. 필요에 따라 각 접근법의 구체적 메커니즘과 사례, 그리고 장점과 한계를 함께 설명하겠습니다.\n\n정책 경사 기반 방법 (Policy Gradient 기반 보상 최적화)\n정책 경사 기법은 보상 구조를 동적으로 학습하거나 최적화하는 접근입니다. 기본적으로 정책 경사(policy gradient) 방법은 가치 함수를 추정하지 않고 에이전트의 정책을 직접 최적화하는 RL 알고리즘군을 말합니다. 이러한 정책 경사 방법을 보상 설계에 응용한 대표 연구로 PGRD (Policy Gradient for Reward Design)가 있습니다. PGRD는 에이전트의 학습 중에 보상 함수의 파라미터를 온라인으로 조정하기 위해 그래디언트 상승(gradient ascent)을 적용합니다. 즉, 초기의 보상 함수를 고정된 것으로 두지 않고, 설계자가 원하는 목표에 근접하도록 보상 파라미터를 계속 업데이트하는 것입니다. 이는 보상 자체를 최적화 문제로 취급하여, 에이전트가 환경에서 겪는 에피소드 결과에 따라 보상 함수를 조금씩 개선해나갑니다. 이 방법의 핵심 아이디어는 “최적의 보상 파라미터는 결국 최적의 에이전트 성능을 이끈다”는 것이며, 보상 설계 문제를 해결하기 위해 또 다른 학습(proximal gradient ascent)을 돌리는 형태입니다. PGRD는 이론적으로 수렴이 보장되며, 부분 관측 환경 등에서도 효과를 보였습니다. 실제 실험에서 PGRD는 기존에 고정된 보상으로 학습한 에이전트보다 더 우수한 성능을 발휘하는 것으로 나타났는데, 이는 에이전트의 능력과 환경 불확실성에 맞게 보상 구조가 적응적으로 변한 덕분입니다. 예를 들어 PGRD를 적용한 결과, 일부 환경에서는 에이전트가 더 높은 평균 보상을 얻으며 학습 속도도 향상되는 것이 확인되었습니다.\nPGRD의 연장선상에서, LIRPG (Learning Intrinsic Reward for Policy Gradient) 알고리즘도 주목할 만합니다. LIRPG는 외재적 보상 외에 내재적 보상을 병렬로 학습함으로써 에이전트가 스스로 보상 신호를 구성하도록 합니다. 구체적으로, 정책 파라미터를 업데이트할 때 내재적 보상의 파라미터도 함께 업데이트하여, 정책 학습과 보상 학습을 동시에 이룹니다. 이를 통해 에이전트는 환경이 주는 보상뿐만 아니라 자체적으로 정의한 보상을 기준으로도 학습 방향을 잡습니다. Atari 게임이나 MuJoCo 로보틱스 환경에서의 실험 결과, LIRPG는 표준 PPO 에이전트보다 일관되게 높은 성능을 보였습니다. 특히 내재적 보상만으로 정책을 학습시킨 경우에도 일정 수준 이상의 성능을 내는 등, 표본 효율(sample efficiency)과 학습 속도를 개선하는 데 효과적이었습니다. 다만 LIRPG를 적용하려면 내재적 보상 함수의 형태와 파라미터를 잘 설정해야 하며, 내재적 보상과 정책 업데이트 간의 상호작용에 따라 민감도가 있을 수 있습니다. 즉, 새로운 보상함수를 함께 학습하는 만큼 하이퍼파라미터 튜닝의 부담이 있고, 부적절한 내재 보상은 오히려 학습을 교란할 위험도 존재합니다.\n또 다른 사례로, DDPGfD (DDPG from Demonstrations) 알고리즘은 정책 경사 방법(DDPG)에 시범 데이터를 활용하여 보상 신호를 보강한 방법입니다. 이 접근은 특히 희소 보상 환경에서 효과적인데, 인간 시범(전문가 데모)을 리플레이 버퍼에 함께 저장하여 초기 학습을 돕습니다. DDPGfD는 오프폴리시(off-policy) Actor-Critic 알고리즘인 DDPG에 다음과 같은 변형을 가했습니다: (1) 인간 시범 데이터를 버퍼에 추가하고, (2) 샘플링 시 우선순위 샘플링을 사용하여 시범과 자가 플레이 데이터를 적절히 혼합하며, (3) 1-step 보상뿐만 아니라 n-step 리턴 보상으로 학습 신호를 강화하고, (4) 한 번 환경 상호작용마다 다중 업데이트를 수행하여 학습 효율을 높입니다. 또한 과적합을 막기 위해 가치망(크리틱)과 정책망(액터)에 L2 정규화를 적용했습니다. 이러한 변경을 통해, 예를 들어 7자유도 로봇팔로 플라스틱 부품 끼워 맞추기 작업을 학습하는 실험에서, DDPGfD는 시범 데이터 양에 비례하여 학습 곡선이 개선되고 보다 안정적으로 과제를 성공시켰습니다. 이 결과는 인간이 알려준 경로(시범)와 보상 구조를 활용하면, 순수 강화학습보다 훨씬 빠르게 복잡한 과제를 익힐 수 있음을 보여줍니다. DDPGfD의 강점은 탐험 문제를 완화하고 학습 초기의 시행착오를 줄여준다는 점이지만, 인간 시범 데이터의 질과 양에 성능이 좌우되며, 추가된 시범 처리 단계로 알고리즘의 구현 복잡도가 증가하는 단점이 있습니다.\n\n\n강건성 및 적응성 향상 방법\n강건성(Robustness)과 적응성(Adaptability)을 높이기 위한 보상 설계 기법들은, 환경의 불확실성이나 변화를 견디고 새로운 상황에 빠르게 적응할 수 있도록 보상 함수를 조정하는 데 초점을 둡니다. 강건성은 에이전트가 잡음(disturbance)이나 모델 불확실성이 있어도 안정적 성능을 내는 능력이고, 적응성은 환경이나 임무가 바뀌어도 정책을 유연하게 조정하는 능력입니다.\n한 가지 접근으로, 리더-팔로워 프레임워크를 활용한 방법이 있습니다. 리더-팔로워 구조에서는 리더 에이전트가 팔로워의 보상 함수를 수정함으로써 팔로워의 행동을 원하는 방향으로 유도합니다. 예컨대, 리더가 보상에 페널티나 보너스를 살짝 가미해 팔로워가 안전한 행동을 우선하게 만드는 식입니다. 이를 통해 시스템 전체의 강건성을 향상시킬 수 있습니다. 이러한 아이디어는 멀티에이전트 시스템이나 계층형 제어에서 활용될 수 있으며, 본질적으로 한 에이전트가 다른 에이전트의 보상 구조를 정책적으로 관리하는 형태의 Reward Engineering이라 할 수 있습니다.\n또 다른 중요한 문제는 센서 노이즈로 인한 보상 신호 왜곡입니다. 현실에서 에이전트의 보상은 센서 측정에 의존하는데, 센서에 노이즈가 끼면 잘못된 보상이 입력되어 RL 성능이 크게 떨어질 수 있습니다. 이를 해결하기 위해, 한 연구에서는 혼동 행렬(confusion matrix)을 이용해 노이즈 보상을 보정하는 강건 RL 프레임워크를 제안했습니다. 이 방법은 센서 보상 신호의 오류 분포를 가정하지 않고, 관찰된 결과로부터 혼동 행렬을 추정해 편향을 제거한 대리 보상(surrogate reward)을 계산합니다. 구체적으로, 혼동 행렬을 기반으로 한 비편향 보상 추정 알고리즘을 도입하여 진짜 환경 보상의 추정치를 얻습니다. 그런 다음 이 추정치로 Q러닝을 수행하는데, 이론적으로 추정 보상을 쓸 때도 학습이 최적 Q함수로 수렴함을 증명하였습니다. OpenAI Gym과 Atari 게임들에 이 기법을 적용한 결과, 노이즈가 있는 환경에서도 에이전트의 기대 보상이 크게 향상되었고, PPO 알고리즘과 결합하면 노이즈에 강인한 정책을 학습할 수 있음을 확인했습니다. 심지어 어느 정도의 노이즈는 탐험을 도와주는 역할까지 하여 누적 보상이 증가하는 흥미로운 현상도 관찰되었습니다. 다만 이 방법은 혼동 행렬 추정 및 추가 연산으로 계산량이 늘어나며, 보정 효과가 다양한 노이즈 패턴에서 항상 잘 들어맞는지는 여전히 정확한 노이즈 모델링과 파라미터 튜닝에 달려 있습니다. 그럼에도 불구하고, 센서 노이즈 하에서 보상 함수를 재설계함으로써 RL 학습의 신뢰성을 높인 좋은 사례라 할 수 있습니다.\n이밖에, 특정 제어 성능 요구사항을 만족시키도록 보상을 형성하는 연구도 있습니다. 예를 들어 어떤 로봇 제어 문제에서 정착 시간이나 정상상태 오차 같은 기준을 충족시키고 싶다면, 모델 없이도 보상을 통해 이러한 목표를 달성하도록 shaping할 수 있습니다. 한 연구에서는 시스템 동역학 모델을 몰라도 RL 정책이 제어 목표(예: 빠른 안정화)에 맞는 궤적을 따르도록 체계적인 Reward Shaping 기법을 제시했습니다. 이는 보상에 제어 성능 조항을 넣어주거나, 에이전트가 이러한 성능 지표를 만족할 때 추가 보상을 주는 방식으로 이뤄집니다. 이 접근의 효과는 해당 제어 목표를 직접 최적화하도록 유도함으로써, RL 정책이 전통적인 제어기처럼 작동하도록 할 수 있다는 점입니다. 다만 이 역시 사전에 만족시켜야 할 성능 기준과 그를 보상으로 어떻게 정량화할지 도메인 지식이 필요합니다.\n\n\n탐험 전략 기반 방법 (Exploration-Driven Shaping)\n강화학습의 난제 중 하나는 탐험-활용 문제로, 새로운 정보를 얻기 위한 탐험과 이미 알고 있는 좋은 행동을 취하는 활용 사이의 균형을 잡는 것입니다. Reward Shaping 기법 중에는 에이전트의 탐험을 장려하여 학습을 촉진하는 것들이 다수 있습니다. 특히 희소 보상 환경에서는 에이전트가 보상을 받을 만한 상태를 찾아가야 하므로, 탐험 보너스 형태의 내재적 보상을 주는 전략이 효과적입니다.\n한 가지 전통적인 접근은 카운트 기반(count-based) 탐험으로, 에이전트가 방문하지 않은 상태에 보너스를 주는 방식입니다. 하지만 연속적이거나 고차원 상태 공간에서는 똑같은 상태를 반복해서 만날 가능성이 낮아 count 기반 방법이 힘을 쓰기 어렵습니다. 이를 해결하기 위해, 해시 함수를 이용한 상태 인코딩 기법이 제안되었습니다. 예를 들어 상태를 저차원 이진 코드로 해싱(hash)하여 비슷한 상태들을 동일하게 취급함으로써, 사실상 유사한 상태 재방문을 카운팅할 수 있습니다. 한 연구에서는 지역민감 해시(Locality-Sensitive Hashing) 기법의 일종인 SimHash를 사용해 연속 상태를 이진코드로 변환한 후, 그 해시 코드를 기반으로 방문 횟수를 세어 탐험 보너스를 주었습니다. 또한 자동인코더(AE)를 활용한 학습된 해싱 방법도 함께 사용하여, 중요한 상태의 특징을 잡아내는 해시 함수를 학습했습니다. 이러한 해시 기반 탐험 보너스를 TRPO(Trust Region Policy Optimization) 알고리즘에 적용한 결과, MountainCar, HalfCheetah 등의 환경에서 기존 TRPO로는 불가능했던 목표 도달을 해시 보너스 덕분에 달성할 수 있었습니다. 특히 상태 공간이 큰 HalfCheetah 환경에서, 해시 보너스를 준 TRPO는 매우 희소한 보상도 포착해내어 일부 성과는 다른 최신 탐험 기법(VIME 등)과 견줄 만한 수준을 보였습니다. 이 방법의 강점은 고차원 공간에도 적용 가능하다는 점과, 비교적 일반적인 방법으로 다양한 RL 도메인에 활용될 수 있다는 것입니다. 단점으로는 어떤 해시 함수를 쓰느냐에 따라 성능 편차가 생기므로 해시 함수 설정 및 학습의 난이도가 있고, 상태 표현의 질에 민감하여 환경마다 튜닝이 필요하다는 것입니다.\n다른 접근으로, 탐험 유도 Reward Shaping(EXPLORS) 프레임워크가 제안되었습니다. EXPLORS는 내재적 보상 학습과 탐험 보너스를 완전 자율방식으로 결합하여, 에이전트가 스스로 탐험을 극대화하도록 설계되었습니다. 이 기법에서는 알고리즘 1과 같은 절차로 매 스텝 보상과 정책을 번갈아 업데이트합니다. 간략히 말해, 에이전트가 행동할 때 환경 보상(외재적)뿐만 아니라 추가적인 내재 보상을 병렬로 학습하여, 두 보상의 합으로 정책을 개선합니다. 이를 통해 전통적 Reward Shaping이 어려운 상황에서도 학습을 가속할 수 있음을 여러 환경에서 검증하였습니다. 예컨대, 매우 희소하거나 노이즈가 큰 보상 환경에서 EXPLORS를 쓴 에이전트는 일반 REINFORCE 알고리즘보다 더 빠르게 수렴하였고, 기존의 수작업 보상 shaping 방법들을 능가하는 성능을 보였습니다. 다만 이 연구에서도 언급하듯이, 보다 복잡한 환경에서 EXPLORS의 효과를 검증할 필요가 있고, 내재 보상과 탐험 보너스를 효과적으로 결합하는 최적의 방법론에 대한 추가 연구가 필요합니다. 즉, 내재적 탐험 신호를 너무 강하게 주면 본래 목표에서 벗어날 수 있고, 너무 약하면 의미가 없기에 그 균형을 찾는 것이 과제로 남습니다.\n또 하나 흥미로운 접근은 RUNE (Reward Uncertainty for Exploration)이라는 방법으로, 선호기반 RL에서 보상 함수의 불확실성을 탐험 보너스로 활용한 것입니다. 사람의 피드백을 통해 학습되는 보상 모델(ensemble 여러 개)을 만들고, 그 예측 값들의 표준편차(불확실성)를 내재적 보상으로 주어봅니다. 이렇게 하면 에이전트는 여러 보상 모델 간 예측이 불확실한 상태를 더 탐험하게 되고, 이는 곧 인간 선호에 대해 명확한 정보를 주는 상태를 찾아다니는 것과 같습니다. RUNE의 장점은 탐험과 활용을 체계적으로 조율해준다는 점입니다. 즉, 불확실성이 큰 행동을 우선 시도함으로써, 장기적으로 더 많은 보상을 얻을 가능성을 높입니다. 실제 로봇 조작 과제에서 RUNE을 기존 탐험 기법들과 비교한 결과, 학습 샘플 효율 및 최종 성공률을 향상시키는 것으로 나타났습니다. 특히 RUNE 적용 시 성공률 상승 폭이 다른 기법보다 크고 적은 피드백으로도 높은 성능을 달성했습니다. RUNE의 도전 과제는 보상 모델의 앙상블을 유지해야 하므로 계산 비용이 증가하고, 다양한 수준의 보상 불확실성을 다룰 수 있는 안정적인 기법이 더 연구되어야 한다는 점입니다. 그럼에도, 인간의 선호 학습 분야에서 보상 불확실성을 탐험에 통합한 첫 사례로서 RL 탐색 전략의 새로운 가능성을 열었다는 의의가 있습니다.\n\n\n정책 파라미터화 (단순 정책 모델의 활용)\n정책 파라미터화는 에이전트의 정책 구조를 어떻게 설계하느냐에 관한 기법으로, 간접적으로는 보상에 의존하는 학습 성능과 탐색 행태에도 영향을 미칩니다. 일반적으로 RL에서 정책은 뉴럴네트워크 등 복잡한 함수로 표현되지만, 최근 한 연구에서는 단순한 선형 또는 RBF(Radial Basis Function) 정책도 충분한 성능을 낼 수 있음을 보여주었습니다. 핵심은 입력 상태에 대해 랜덤 Fourier 피쳐를 사용하여 표현력을 높이고 선형 정책에 적용한 것입니다. 예를 들어, 상태 관측에 대해 랜덤 주파수로 사인/코사인 변환한 피쳐 벡터를 만들고, 그에 가중치를 곱한 선형 정책을 학습하는 식입니다. 이렇게 간소화된 정책은 OpenAI Gym 연속 제어 벤치마크에서 뉴럴네트워크 정책에 필적하는 성능을 내면서도 학습 속도는 더 빠른 것으로 나타났습니다. 특히 신경망 없이도 TRPO로 훈련한 선형/RBF 정책이 일부 과제에서는 높은 보상을 얻었으며, 학습 시간도 단축되었습니다. 하지만 단순 정책은 환경 변화나 교란에 취약할 수 있어, 연구에서는 초기 상태 분포를 다양화하는 기법(도메인 랜덤화와 유사)을 써서 정책의 강건성을 높였습니다. 이렇게 학습하면 앙상블 모델이나 도메인 랜덤화처럼 정책이 다양한 상황에 노출되므로, 훨씬 안정적인 정책이 나온다고 합니다.\n이 접근의 장점은 모델 단순화로 인한 학습 안정성과 속도 향상입니다. 복잡한 신경망 구조 대신 선형 또는 RBF 기반 정책을 쓰면 수렴이 빠르고 이해도 쉽습니다. 그러나 단점도 분명합니다. 우선 고차원 상태·행동 공간으로 확장하면 단순 정책으로는 충분한 표현력을 얻기 어려울 수 있습니다. 또한 초기 상태 분포에 민감하여, 학습 시 다양한 상황을 충분히 겪지 않으면 실전에서 성능 저하가 생길 수 있습니다. 복잡한 행동이 필요한 과제에서는 신경망 정책만큼 섬세한 대응이 어려워 성능 한계에 부딪힐 가능성도 있습니다. 결국 이 연구는 “가능한 한 정책을 단순화하되, 환경적 다양성을 주어 보완한다”는 철학으로 볼 수 있으며, 이는 보상 설계 측면에서 볼 때 단순 정책 + 특별한 보상 구성(랜덤화) 조합을 통해 학습을 안정화하는 하나의 전략이라 할 수 있습니다.\n\n\n역보상 설계 (Inverse Reward Design, IRD)\n역보상 설계(IRD)는 직접 보상 함수를 공학적으로 설계하는 대신, 에이전트의 시연이나 행동을 관찰하여 보상 함수(목적)를 역추정하는 접근입니다. 이는 강화학습의 역문제에 해당하는 개념으로, 일종의 역강화학습(IRL) 기법이라 볼 수 있습니다. 일반적인 RL에서는 에이전트가 상태 s에서 행동 a를 취하면 미리 정의된 보상 함수 R(s,a)에 따라 보상을 받습니다. IRL에서는 반대로 전문가의 상태-행동 궤적이 주어졌을 때, 어떤 보상 함수를 가정하면 그 행동이 최적화될지를 학습하는 것입니다. IRD는 이 IRL의 아이디어를 보상 설계에 응용한 것으로, 기존에 인간이 설계한 보상 함수(프락시 보상)를 관찰 가능한 신호로 보고 진짜 의도한 보상 함수를 추정하려 합니다.\nHadfield-Menell 등의 연구에서 제안된 IRD 방법은, 설계자가 정한 보상 함수를 전문가의 행동을 야기한 신호로 취급합니다. 그리고 에이전트에게는 “이 보상은 사실 완벽하지 않을 수 있으니, 위험한 행동을 피하도록 보상 함수를 재해석하라”는 식으로 학습시킵니다. 예를 들어, 로봇이 용암 지대가 있는 환경에서 이동한다고 할 때, 설계자는 “목적지에 가면 +100”이라는 보상만 주었을 수 있습니다. 이 경우 로봇은 용암을 가로질러 빠르게 가려다가 파괴될 위험이 있습니다. IRD를 적용하면 로봇은 주어진 보상(목적지 도달)이 진짜 목표의 완전한 표현이 아니라고 간주하고, 전문가(인간)가 그런 상황에서 어떻게 행동했는지를 바탕으로 암묵적인 위험 회피 보상을 추론합니다. 실제 IRD 실험에서, 로봇은 용암 지역을 회피하는 경로를 선택하여 목표를 달성함으로써, 주어진 보상을 그대로 최적화했을 때 발생하는 위험 행동을 피했습니다. 이는 IRD가 보상 설계자가 미처 고려하지 못한 잠재적 위험 요소를 에이전트 스스로 인지하게 만들 수 있음을 보여줍니다. 또 하나 IRD의 효과는 보상 조작(reward manipulation) 문제를 줄이는 것입니다. 에이전트가 보상 함수를 절대적인 목표로 받아들이지 않고, 그것을 발생시킨 의도를 추론하기 때문에, 보상만 높이고 실제 목표는 성취하지 못하는 행동(보상 해킹)을 억제할 수 있습니다.\nIRD의 한계로는 계산적 복잡성과 가정의 단순함이 있습니다. IRD는 결국 IRL 문제를 푸는 것이므로 추론 단계에서 복잡한 최적화가 필요하고, 환경에 대한 단순한 가정을 놓는 경우가 많습니다. 예컨대 보상 함수가 선형 조합으로 구성된다고 가정하거나, 환경 모델이 알려져 있다고 가정하는 등입니다. 현실의 복잡한 상황에서는 이러한 가정이 어긋나 IRD가 엉뚱한 보상을 추론할 위험도 있습니다. 따라서 IRD를 적용할 땐 환경과 보상에 대한 적절한 모델링이 필요하며, 잘못된 추론을 막기 위한 안전장치도 고려해야 합니다. 그럼에도 IRD는 보상 설계의 새로운 패러다임으로, 에이전트가 디자이너의 의도를 역으로 생각하게 함으로써 보상의 불완전성에 대응했다는 의의를 가집니다.\n\n\n보상 지평 단축 (Reward Horizon Shaping)\n보상 지평(horizon)이란 에이전트가 현재 행동의 결과로 보상을 받기까지 걸리는 단계 수를 의미합니다. 만약 최종 보상까지 거쳐야 할 단계가 너무 많으면, 에이전트는 어떤 행동이 기여했는지를 알기 어려워 학습이 어려워집니다. Reward Shaping은 이러한 보상 지평을 줄여주는 역할도 합니다. 즉, 중간 단계마다 적절한 보상을 주어 장기 목표를 여러 개의 단기 목표로 분해하는 것입니다.\n한 연구에서는 Reward Shaping을 통해 학습 시간에 대한 이론적 보장을 제공하기도 했습니다. 해당 연구의 알고리즘 (Horizon_Learn 알고리즘)은 보상 지평을 H로 제한한 환경에서 에이전트가 학습할 때, 학습 시간이 MDP의 전체 상태 수와 관계없이 임계 영역(critical region)의 크기에 다항식(poly)으로 비례함을 증명했습니다. 이는 Reward Shaping을 잘 하면, 에이전트는 커다란 상태공간을 전부 탐색하지 않고서도 중요한 부분만 배워도 된다는 의미입니다. 직관적으로, 중간 목표에 보상을 주면 에이전트는 방대한 상태 공간 중에서 필요한 경로만 집중하게 되고, 결과적으로 학습 효율이 비약적으로 증가합니다. 예를 들어 미로 찾기에서 최종 출구에만 보상 주는 대신, 중간 체크포인트 몇 군데에도 보상을 주면 에이전트는 쓸데없는 경로를 적게 탐색하게 됩니다.\n이런 개념을 공식화한 알고리즘에서는 에이전트가 아직 모르는 상태로 진입하면 임의 정책으로 탐험하되, 이미 안다고 판정된 구간은 최적 정책을 따르는 식으로 진행합니다. 그리고 새로운 상태를 알게 될 때마다 그 구간의 최적 정책을 잠재적 보상으로 강화해줍니다. 그 결과, 에이전트는 MDP 전체를 탐색할 필요 없이 중요한 구간만 탐험해도 충분히 최적 정책에 도달하고, 학습 시간은 전체 상태 수가 커져도 크게 악화되지 않습니다. 이러한 이론은 Reward Shaping이 얼마나 학습 문제를 단순화할 수 있는지 보여주는 한 예이며, 실제 문제에도 적절한 형상(reward shaping)을 통해 효과적인 학습을 기대할 수 있음을 시사합니다.\n보다 일반적으로, 보상 지평 단축은 우리가 직관적으로 흔히 사용하는 기법입니다. 먼 훗날 받을 보상보다는 가까운 시점의 성과에 보상을 주면, 에이전트는 목표까지 가는 여정에서 헤매는 일이 줄어듭니다. 다만 주의할 점은 최적 정책의 불변성입니다. Reward Shaping은 어디까지나 최종 최적해(optimal policy)를 바꾸지 않는 범위에서 이루어져야 합니다. 너무 강한 중간 보상을 주면 에이전트가 애초 목표를 망각하고 중간 목표에 집착해버릴 수 있습니다. 이를 방지하는 대표적인 이론적 도구가 잠재 기반 Reward Shaping(potential-based shaping)으로, 아래에서 자세히 설명합니다.\n\n\n잠재 기반 방법 (Potential-Based Reward Shaping)\n\n\n\n\n강화학습의 외재적 보상과 내재적 보상 개념. 좌: 에이전트는 환경으로부터 보상을 받는다. 우: 에이전트 내부에 “내부 환경” 또는 비유적으로 생물체와 같은 모듈이 있어, 여기서 내재적 보상이 발생하여 함께 활용된다.\n\n잠재 기반 Reward Shaping은 이론적으로 가장 잘 정립된 Reward Shaping 기법 중 하나로, Ng 등의 고전적인 연구에서 처음 제안되었습니다. 잠재 함수 \\Phi(s)란 상태 s에 대해 정의되는 실수 값 함수로, 현재 상태와 다음 상태의 잠재값 차이를 추가 보상으로 주는 방식입니다. 수식으로 표현하면 shaped 보상 R'(s,a,s') = R(s,a) + \\gamma \\Phi(s') - \\Phi(s) 로, 여기서 \\gamma는 할인율입니다. 이 방식으로 보상을 변경하면 잠재 함수의 그라디언트 부분이 일종의 가이드 역할을 하지만, 결과적으로 최적 정책은 변하지 않는다는 정책 불변성 이론이 증명되었습니다. 따라서 잠재 기반 shaping은 이론적으로 안전하게 보상을 추가할 수 있는 방법입니다.\n잠재 기반 방법의 직관적인 이해를 위해, \\Phi(s)를 휴리스틱 함수라고 볼 수 있습니다. 예를 들어 미로 문제에서 \\Phi(s)를 현재 상태에서 목표까지의 예상 거리의 음수로 설정하면, 에이전트는 한 걸음 움직여 목표에 가까워질 때마다 잠재값 차이만큼 보상을 얻게 됩니다. 이 보너스는 최종 목표 도달이라는 본래 보상과 방향은 일치하지만, 중간 단계에서 방향성을 제시해주므로 탐색을 유도합니다. 실제로 로봇 팔 작업에서도, 목표물과의 음수 거리 잠재함수를 쓰면 로봇이 목표물에 일단 가까이 가려고 노력하게 만들 수 있습니다.\n이 리뷰 논문에서는 다양한 잠재 기반 방법의 발전형들을 소개합니다. 우선 기본 개념으로, 잠재 기반 shaping은 에피소드 전체 보상을 분석해 누적 보상을 강화하거나 벌점을 주는 형태로도 쓰일 수 있습니다. 한 연구에서는 게임 환경에서 에피소드 당 받은 총 보상을 추적하여, 현재 에피소드 성과가 최저치보다 높으면 보상 증가, 최고치에 가까워지면 강화하는 동적 잠재 함수를 제안했습니다. 예컨대 식 (13)에서, F_{\\text{min}}, F_{\\text{max}}는 지금까지 달성한 최악/최고 에피소드 점수인데, 현 에피소드 보상이 과거 최고치에 가까워질수록 추가 보상을 줌으로써 긍정적 행동을 강화합니다. 이 방법은 Pong, Breakout 게임에서 학습 속도를 높이고 여러 과제를 동시에 학습할 때도 성능을 높여준 것으로 보고되었습니다.\n잠재 기반 방법은 멀티에이전트 시스템(MAS)에도 적용되어 이론적 성질이 연구되었습니다. 잠재 보상을 다중 에이전트 상황에 도입하면, 본래 MDP가 아니라 스토캐스틱 게임(stochastic game)이 되는데, 이때도 모든 에이전트에 같은 잠재 보상함수를 적용하면 결과적으로 Q함수 초기화를 조정한 것과 동일하며, 게임의 내시 균형(Nash equilibrium)을 바꾸지 않는다는 것이 증명되었습니다. 이는 멀티에이전트에서도 정책 불변성이 유지됨을 보장해주는 중요한 이론입니다. 다만 잠재 보상을 주면 탐색 경로가 달라질 수 있어, 동일한 균형점이라도 다른 경로로 도달하거나 새로운 joint policy에 수렴할 수 있음이 보고되었습니다. 흥미롭게도, 적절한 잠재 보상을 주었을 때 에이전트들이 더 높은 글로벌 효용을 내는 균형에 도달할 확률이 높아지고, 수렴 시간은 줄어드는 경향이 관찰되었습니다. 이는 멀티에이전트에서도 잠재 기반 shaping이 탐색을 건전하게 유도하여 보다 좋은 협력 결과를 끌어낼 수 있음을 시사합니다.\n잠재 기반 shaping은 계층형 강화학습(HRL)에도 활용되었습니다. 예를 들어 PBRS-MAXQ-0라는 알고리즘은, 기존 HRL 알고리즘인 MAXQ에 잠재 보상(PBRS)을 통합한 방법입니다. 계층 학습에서는 상위 목표와 하위 목표로 문제가 나뉘는데, 여기에 잠재 보상을 추가하여 서브태스크 간 연계를 부드럽게 만들었습니다. 이 알고리즘은 일부 이론 조건 하에서 추가 보상이 있어도 수렴 보장을 제공하며, 실험적으로도 적절한 휴리스틱(잠재함수)을 넣으면 표준 MAXQ-0보다 학습 속도가 빨라지고 성능이 향상됨을 보였습니다. 흥미로운 점은, 심지어 휴리스틱이 잘못되어 (misleading heuristic) 에이전트를 오도하더라도 최종적으로는 표준 알고리즘 수준의 성능까지 따라온다는 것입니다. 이는 PBRS-MAXQ-0이 어느 정도 잘못된 잠재 보상에도 강인하여, 결국 충분히 학습 시간을 주면 극복한다는 의미입니다. 물론 현실적으로는 잘못된 잠재 보상을 쓰면 중간 학습 과정에서 시간 낭비가 발생하므로, 휴리스틱의 품질을 높이는 것이 중요합니다. PBRS-MAXQ-0의 과제로는 휴리스틱 값을 어떻게 최적화할 것인지와, 다양한 환경에서 일관되게 성능을 내는지 등이 꼽힙니다.\nPotential-Based Advice라는 이론 프레임워크도 소개되었는데, 이는 임의의 복잡한 보상 함수도 잠재 함수의 형태로 에이전트에게 주입할 수 있다는 개념입니다. 즉, 환경이 주는 본래 보상을 바꾸지 않더라도, 추가로 설계자가 원하는 임의의 보상 함수를 잠재적 추가 보상으로 제공하여 정책에는 영향 주지 않으면서 학습을 가속할 수 있다는 것입니다. 이 방법은 보조 가치함수(auxiliary value function)를 학습하여, 추가 보상이 마치 잠재 보상처럼 기능하도록 합니다. 이를 통해 설계자가 가진 도메인 지식을 행동 강화를 위해 녹여낼 수 있고, 이론적으로 시간과 메모리 복잡도도 선형으로만 증가하여 큰 부담 없이 구현 가능함을 보였습니다. 다만 현재까지는 이 개념이 주로 이론적 정식화로 머물러 있고, 이를 실제 알고리즘으로 구현한 예는 많지 않습니다. 저자들은 여러 Reward Shaping 접근법의 어려움을 비교하는데, 잠재 기반 방법은 이론적 보장은 있지만 효과적 적용을 위해 도메인 지식이 많이 필요하고, 보조 과제(auxiliary task) 기반 방법은 유연하지만 설계 복잡성이 증가하며, 내재적 동기 부여 방법은 파라미터 튜닝에 민감하다고 지적합니다. 따라서 실용적인 관점에서는 이론과 현실의 균형을 맞춰, 문제에 가장 적합한 shaping 접근을 택하는 것이 중요하다고 결론짓습니다. 향후에는 이러한 Reward Shaping 기법들을 보다 자동화하고 강인하게 만드는 연구가 필요하다고 제언합니다.\n잠재 기반 shaping은 에피소드형 강화학습(episodic RL) 맥락에서도 재검토되었습니다. 한 연구는 에피소드 초·말의 잠재값이 균형에 미치는 영향 등을 분석하여, 잠재 보상이 스토캐스틱 게임의 균형을 변경하거나 새로운 균형을 도입할 수 있음을 이론적으로 보였습니다. 특히 비-제로(non-zero) 터미널 잠재값은 새로운 균형을 만들 수 있어 주의해야 함을 지적했습니다. 또한 PAC-MDP 관점에서 잠재 보상이 탐험을 촉진함을 증명하여, 에피소드 시작·종료 상태가 구분되는 경우에도 잠재 기반 shaping이 유용함을 보여주었습니다. 요약하면, 잠재 함수는 에피소드 기반 문제에서도 학습 효율 향상에 기여하지만, 이론적으로 설계할 때 각별한 신경을 써야 할 요소들이 있음을 시사합니다.\n또한 안전 제약이 있는 강화학습에도 잠재 보상 개념이 활용되었습니다. 예를 들어 논리 검증된 하이브리드 시스템 모델을 사용하여 안전 속성을 만족하는 보상 함수를 자동으로 생성하는 연구가 있습니다. 이 접근에서는 잠재 기반 보상 함수를 논리 제약과 함께 사용하여, 제약을 어기지 않으면서 에이전트가 학습하도록 했습니다. 실제 차량 종방향 제어 환경에서, 논리 제약 기반 잠재 보상(PBRF)을 사용하니 학습 초기 수렴 속도가 빨라졌고, 안전한 동작을 하도록 유도되었습니다. 완전히 사람이 만든 보상과 비교해도 안전성 측면에서 거의 동등한 정책을 얻을 수 있었으며, 보상 스케일링을 조정하여 어떤 안전 측면을 더 강조할지도 통제할 수 있었습니다. 이는 형식 기법(formal methods)과 RL을 결합한 사례로, 잠재 보상을 통해 안전 영역으로의 유도를 효과적으로 한 것입니다.\n마지막으로, 잠재 기반 방법의 파생으로 DRiP (Difference Reward incorporating Potential-based reward)과 CaP (Counterfactual as Potential)이라는 기법도 등장했습니다. 이는 뒤에서 설명할 차이 보상(Difference reward) 개념을 잠재 보상과 결합한 것으로, 멀티에이전트에서 각 에이전트가 시스템 성능에 기여한 바를 잠재 함수 형태로 보상해주는 방법입니다. DRiP는 도메인 지식이 담긴 잠재 함수를 설계해야 하지만, 학습 가속과 균형 유지 측면에서 효과적이고, CaP는 동적으로 잠재 함수를 생성하여 사람의 개입을 줄인 방법입니다. 두 방법 모두 유사한 지식을 활용하지만, 합쳐 쓸 때 시너지는 없고 오히려 복잡도만 증가하므로 별개로 쓰는 것이 좋다고 합니다. 이론 보장이 필요하면 CaP를, 최대한 빠른 성능 향상이 목표면 DRiP을 권장하고 있으며, DRiP은 특히 도메인 지식이 충분할 때 강력한 성능을 보였습니다. 다양한 실험 도메인에서 DRiP은 일관되게 기존 방법보다 빠른 학습과 높은 정책 성능을 달성해, 멀티에이전트 shaping 기법의 발전 가능성을 보여줍니다.\n\n\n동적 잠재 기반 Reward Shaping (DPBRS)\nDPBRS (Dynamic Potential-Based Reward Shaping)는 잠재 함수를 시간에 따라 변화시키는 발전된 기법입니다. 기본 잠재 기반 방법에서는 \\Phi(s)가 고정된 함수이지만, DPBRS는 현재까지의 학습 상황이나 에이전트의 상태에 따라 잠재 값을 동적으로 조정합니다. 논문에서는 Q-러닝의 업데이트 식에 시간을 나타내는 매개변수를 추가하여 개념을 정의했습니다. 아이디어는 식 (16)처럼, 잠재 함수에 상태 도달 시각 t를 반영하여 \\Phi(s, t)로 확장하는 것입니다. 예컨대, 어떤 상태를 빨리 도달하면 높은 잠재 보상을 주고 늦게 도달하면 줄이는 식으로, 시간 요소를 고려한 보상을 줄 수 있습니다. 이렇게 하면 에이전트가 더 신속히 목표에 도달하도록 유도하거나, 학습 진행에 따라 보상 전략을 조정할 수 있습니다.\nDPBRS의 장점은 기존 잠재 방법의 이론적 보장(정책 불변성 등)을 유지하면서도, 상황에 맞게 shaping을 조절할 수 있다는 점입니다. 연구에서는 DPBRS를 싱글 에이전트 2D 미로와 멀티에이전트 게임에 테스트하여, 더 빠른 수렴과 협동 향상을 확인했습니다. 예를 들어 다중 에이전트의 경우, 시간에 따라 잠재 보상을 변동시켜 초기에 협동을 유도하고 나중에는 스스로 학습하도록 하는 식의 적용도 가능할 것입니다.\n하지만 이 방법은 아직 널리 시험되지 못했습니다. 논문에서도 다른 알고리즘과의 직접 비교 부족, 로보틱스나 복잡한 비선형 시스템에의 응용 미비 등을 한계로 들었습니다. 제시된 예시는 2D 미로와 간단한 게임 정도였고, 실제 로봇 제어나 고차원 문제에서는 효과가 검증되지 않았습니다. 따라서 DPBRS는 개념 증명 단계로 볼 수 있으며, 향후 다양한 환경에서 성능을 확인하고 다른 shaping 방법과도 비교 연구가 필요합니다. 그럼에도, 시간 가변적 보상이라는 개념은 중요하여, 향후 적응형 보상 설계의 한 방향으로 발전할 것으로 기대됩니다.\n\n\n상한 신뢰 기반 가치 반복 (UCBVI 기반 보상 설계)\nUCBVI (Upper Confidence Bound Value Iteration)는 샘플 효율 향상을 위해 이론적으로 제안된 알고리즘으로, Reward Shaping의 효과를 분석적으로 연구한 사례입니다. UCBVI 자체는 모델기반 탐색 방법으로, 탐색 보너스를 통해 가치 함수를 상향 편향(upper bound)시켜가며 최적 정책을 찾습니다. 이는 MBIE-EB 등 기존 탐색 알고리즘과 유사하지만 이론 분석에 편리한 형태로 설계되었습니다. 원 논문에서는 이 UCBVI에 Reward Shaping 개념을 도입하여, 탐색 보너스와 가치 함수 투영 등을 수정한 UCBVI-shaped 버전을 만들었습니다.\nUCBVI-shaped의 분석에 따르면, Reward Shaping을 하면 에이전트가 탐색할 상태 공간을 효과적으로 줄일 수 있어 후회(regret) 및 샘플 복잡도 면에서 유리해집니다. 이는 Reward Shaping이 문제와 관련 없는 부분을 가지치기(pruning) 하고, 에이전트가 정책 수립에 중요한 부분에 집중하게 해준다는 의미입니다. 결과적으로, UCBVI-shaped는 동일한 환경에서 기본 UCBVI보다 낮은 후회 한계와 적은 샘플로 비슷한 성능을 달성할 수 있음을 보였습니다. 특히 에이전트가 쓸데없이 환경의 불필요한 영역을 돌아다니는 일이 줄어들어, 최적 정책 발견 시간이 단축됩니다.\n실험으로는 미로 환경에서 UCBVI-shaped와 일반 UCBVI를 비교했는데, irrelevant한 구역이 많은 환경일수록 shaping의 효과가 두드러졌다고 합니다. 이는 직관적이며, Reward Shaping이 없었다면 에이전트가 헤맸을 부분을 Reward Shaping으로 탐색하지 않도록 유도했기 때문입니다. 이러한 연구는 주로 이론 위주이지만, 중요한 점은 Reward Shaping을 공식적으로 샘플 복잡도 분석에 통합했다는 것입니다. 이는 이전까지 대부분 실험적으로 좋다는 Reward Shaping 기법들을, 수학적 성능 보장 측면에서 조명한 것으로, 향후 Reward Shaping이 RL의 샘플 효율을 높이는 정당한 도구로 더욱 인정받을 수 있게 합니다.\n요약하면, UCBVI 연구는 “Reward Shaping을 하면 이론적으로도 학습 효율이 좋아진다”는 것을 증명함으로써, 향후 RL 분석에 Reward Shaping을 적극 고려해야 함을 시사했습니다. 실제로 RL 연구에서는 종종 보상은 문제 정의의 일부로 고정하고 탐색 알고리즘만 분석하곤 했지만, 이 결과는 보상 설계 자체도 알고리즘의 일부로 취급해야 함을 알려줍니다.\n\n\n차이 보상 (Difference Rewards)\n차이 보상(Difference Reward)은 멀티에이전트 강화학습이나 집단 최적화 문제에서 고안된 기법으로, 에이전트의 기여도에 따른 보상을 주는 방법입니다. 기본 아이디어는, “개별 에이전트의 행동이 전체 시스템에 얼마나 이익을 가져왔는가”를 보상으로 삼는 것입니다. 수식으로, 에이전트 i의 차이 보상 D_i는 보통 전체 시스템 보상에서 i의 공헌을 뺀 나머지 부분을 뺀 값으로 정의됩니다. 예를 들어 멀티에이전트 시스템의 전역 보상 G(z)가 상태 z에서 주어진다면, i 에이전트를 제외한 경우의 보상 G_{-i}(z)를 가정하고 D_i = G(z) - G_{-i}(z)로 보상을 구성하는 식입니다. 이렇게 하면 에이전트 i는 자기 행동이 시스템 전체 성능에 미친 순 효과를 보상으로 받게 되어, 개인 최적화가 곧 전체 최적화로 이어지도록 유도됩니다.\n단일 에이전트의 경우에도 비슷한 개념을 적용할 수 있는데, 이때는 특정 기준 상태(또는 이상적 상태)와 현재 상태의 차이를 보상으로 삼습니다. 예컨대 r이 원래 보상이고 \\Phi(s)가 현재 상태와 기준 상태의 차이를 재는 함수라면, 수정된 보상을 r' = r + \\gamma (\\Phi(s_{\\text{ref}}) - \\Phi(s)) 형태로 줄 수 있습니다. 이렇게 하면 에이전트는 기준 상태(목표 상태)에 가까워질수록 추가 보상을 받으므로 그 방향으로 행동합니다. 이때의 차이 함수 \\Phi는 잘 선택해야 효과가 있으며, 너무 단순하면 충분한 정보를 주지 못하고 너무 복잡하면 보상 함정이 될 수 있습니다.\n차이 보상의 큰 장점은 개별 에이전트의 관점에서 “무엇을 하면 시스템에 이득이 되는가”를 명확히 제시해준다는 것입니다. 예를 들어 교통 경로 선택 문제에서 개별 운전자에게 전체 교통 흐름을 개선하는 방향으로 보상을 주면, 각 운전자는 이기적으로 행동하는 대신 교통 체증 완화에 기여하는 쪽을 선택하게 됩니다. 실제 연구에서 IQ-learning (개인 보상만 사용하는 Q-learning)과 DQ-learning (차이 보상 사용하는 Q-learning)을 비교한 결과, DQ-learning은 전체 여행시간을 크게 감소시켜 혼잡을 완화했고, 전통적인 교통할당 기법보다도 나은 결과를 얻었습니다. 통계적으로도 유의미한 개선을 보였으며, 이는 차이 보상이 사회적 최적해에 가까운 결과를 낳았음을 의미합니다. 물론 이 결과는 해당 교통 시뮬레이션 모델에 한정된 것이어서, 다른 도메인에도 일반화될지는 추가 연구가 필요합니다. 특히 운전자 같은 인간이 보상을 따른다고 가정하는 등 몇 가지 이상화된 조건이 있어 현실적 한계도 있지만, 차이 보상의 개념 자체는 여러 협력적 MAS 문제에 적용되고 있습니다.\n멀티에이전트 상황에서의 차이 보상 정의는 앞의 식처럼 counterfactual 개념이 핵심입니다. 즉, “너 없었으면 어땠을지”를 계산하여 그 차이를 주는 것이죠. 이를 구현하려면 모든 에이전트의 공동 행동에 대한 전역 보상을 계산할 수 있어야 하고, 에이전트 i를 제외한 가상의 상황 G_{-i}를 추정할 수 있어야 합니다. 보통 이 G_{-i}는 i의 행동만 다른 동일한 상황이거나, i가 기여하지 않은 상태로 가정한 모델 등을 사용합니다. 이러한 counterfactual 보상은 에이전트 간 신호 상호작용을 제거해주므로, 불필요한 행동 신호의 잡음을 줄이고 크고 작은 협력 문제 모두 잘 해결하도록 도와줍니다.\n차이 보상의 단점은 전역 시스템 보상에 대한 가정이 필요하다는 점입니다. 시스템 전체의 상태나 보상을 에이전트가 알 수 있어야 하므로, 완전 정보 또는 중앙집중식 평가가 가능한 경우에 적합합니다. 또한 시스템 효용 함수를 수식으로 알고 있어야 하고, 에이전트 제외 상황 G_{-i}도 명시적으로 계산하거나 근사해야 하는데, 이 수식이 복잡할 경우 적용이 어려워질 수 있습니다. 반면 잠재 기반 보상(PBRS)은 도메인 전문가가 설계해야 하지만 개별 에이전트 수준에서 적용하기는 쉽다는 장단점 비교가 있습니다.\n결국 차이 보상은 협력적 멀티에이전트 RL에서 유용한 도구이며, 시스템 전체 최적화를 촉진한다는 점에서 의미가 큽니다. 실제 연구에서는 차이 보상과 잠재 보상을 동시에 활용하거나 비교하여, 다중 목표 환경에서 Pareto 최적해로 유도하는 실험이 이루어졌습니다. 그 결과, 차이 보상과 잠재 보상 모두 Pareto해 달성에는 효과적이었지만 각각 요구조건이 달랐습니다. 차이 보상은 시스템 전역 정보를 알아야 하고, 잠재 보상은 수작업 설계가 필요하다는 제약이 있으므로, 문제 상황에 따라 적합한 기법을 선택해야 한다고 결론내렸습니다. 일반적으로 시스템 모델을 잘 알고 통신이 원활하면 차이 보상이 유리하고, 그렇지 않으면 잠재 보상을 쓰되 적절한 휴리스틱을 마련하는 쪽이 나을 수 있습니다.\n\n\n지식 기반 다목적 다중에이전트 RL (MOMARL)\nMOMARL (Knowledge-Based Multi-Objective Multi-Agent RL)은 말 그대로 다중 목표를 가진 다중 에이전트 환경에서의 Reward Shaping 기법들을 비교한 연구입니다. 이 연구에서는 하나의 벤치마크 문제(Multi-Objective Beach Problem Domain)와 전력망 경제 배치 문제에 차이 보상(D)과 잠재 보상(PBRS)을 적용해 보았습니다. 실험 결과, 두 방법 모두 에이전트를 Pareto 최적 영역으로 안내하는 데 도움을 주었지만, 각자의 한계도 드러났습니다.\n차이 보상(D)은 앞서 언급했듯이 전역 지식이 필요합니다. 시스템 전체의 효용 함수를 알아야 하고 각 에이전트의 영향력을 구분할 수 있어야 합니다. 또한 목표 함수(평가 함수)가 수학적 형태로 명확히 주어져야 유효합니다. 만약 시스템 지식이 제한적이거나 계산 자원이 부족하면 차이 보상을 쓰기 어렵습니다.\n잠재 보상(PBRS)은 잠재 함수 설계의 어려움이 있습니다. 사람의 전문 지식으로 잠재 함수를 만들어야 하는데, 다목적 문제에서는 어떤 잠재 함수를 써야 여러 목표를 균형 있게 달성할지 판단하기가 까다롭습니다. 또한 잘못 설계된 잠재 함수는 특정 목표만 치중하게 하거나 오히려 방해가 될 수 있습니다. 이처럼 PBRS는 시간과 노력이 많이 드는 수작업 설계가 필요하며, 모범적인 설계를 못 하면 효과가 제한적입니다.\n연구에서는 일반적으로 차이 보상 D가 잠재 보상 PBRS보다 성능이 우수한 경향을 보인다고 보고했습니다. 특히 시스템 전체를 아우르는 지식이 사용 가능하고 통신 대역폭 문제가 없는 경우, 차이 보상이 더 직접적으로 목표 달성을 이끌어주기 때문입니다. 반면 에이전트 간 통신이 제한되거나, 중앙 집중 평가가 어려운 경우에는 차이 보상을 적용하기 어려우므로, 이럴 땐 잠재 보상이 현실적인 대안이 됩니다.\n이 연구의 결론은 결국 “주어진 다목적 다중에이전트 문제에 가장 적합한 Reward Shaping 기법을 선택해야 한다”는 것입니다. 시스템에 대한 지식 수준, 에이전트 간 정보 공유 가능성, 그리고 디자이너의 전문성 등을 종합적으로 고려해, 차이 보상이나 잠재 보상 중 하나 또는 둘 다를 선택적으로 사용하는 것이 바람직합니다. 또한 이들이 잘 동작하는지 비교할 수 있는 공개 벤치마크(MOBPD 등)를 제시하여, 향후 연구들이 쉽게 따라 실험하고 개선점을 모색할 수 있도록 했습니다. 요컨대, 멀티에이전트에서의 보상 설계는 단일 에이전트보다 복잡하지만, 적절한 접근법을 활용하면 개인 이익과 전체 이익을 조화시키는 방향으로 학습을 유도할 수 있습니다.\n\n\n계획 기반 방법 (Plan-Based Reward Shaping)\n계획 기반 방법은 모델 기반 계획(planning) 기법과 강화학습을 결합하여 보상을 형성하는 접근입니다. 이 아이디어는 에이전트가 실제 행동을 하기 전에 “가상의 시나리오(what-if)”를 정신적으로 시뮬레이션해보고, 거기서 얻은 통찰을 보상에 반영하는 것과 유사합니다. 다시 말해, 계획 알고리즘이 제시하는 최적 경로 정보를 이용해 RL의 탐색을 돕는 것입니다.\n한 가지 방법은 STRIPS와 같은 전통적 AI 계획 표현을 이용한 Reward Shaping입니다. STRIPS는 상태 변화에 대한 전제조건과 효과를 정의하는 논리적 틀인데, 이를 사용해 목표 달성까지 필요한 행동 순서를 구합니다. 그런 다음 RL의 보상으로 이 계획 경로에 따른 행동에는 추가 보상을 주는 것입니다. 예를 들어, 일반 MDP 기반으로 학습할 때는 전체 상태공간을 탐색해야 하지만, STRIPS 기반 shaping을 도입하면 계획된 최적 경로 주변으로만 탐색을 집중하게 되어 학습이 효율화됩니다. 실제 연구에서, STRIPS 계획에서 얻은 경로의 상태들을 따라가면 보상을 주고, 그 외에는 주지 않는 shaping을 했더니 정책의 수렴 속도가 빨라지고 최종 성능도 향상되었습니다. 특히 계획 지식에 오류가 있어도, 에이전트는 RL을 통해 이를 보완하며 여전히 MDP 전체를 사용하는 shaping보다 나은 성능을 보였는데, 이는 계획 정보가 주는 가이드가 유용함을 나타냅니다.\n또 다른 연구에서는 계획 기반 shaping vs 추상 MDP 기반 shaping을 비교했습니다. 추상 MDP 방법은 문제를 상위 레벨 MDP (간소화된 모델)로 풀고, 그 가치함수를 실제 학습에 shaping으로 쓰는 방식입니다. 계획 기반은 위와 같이 명시적 계획의 결과를 쓰고요. 비교 결과, 대규모 복잡한 환경에서는 세부 단계까지 알려주는 계획 기반 보상이 더 효율적이어서 총 보상과 수렴 속도 모두 우수했습니다. 반면 다중 에이전트나 충돌하는 목표가 있는 상황에서는, 추상 MDP 접근이 협조 문제를 더 잘 다루어 성능이 좋았습니다. 예컨대 여러 에이전트가 각자 계획을 따르면 충돌할 때, 추상 MDP는 상위 수준에서 조율된 목표를 주므로 충돌을 피하게 하는 식입니다. 이 결과는, 환경 특성에 따라 어떤 shaping이 적합한지 다르다는 것을 보여줍니다. 계획 기반 보상은 각 단계에 세밀한 지도를 주기 때문에, 단일 에이전트의 복잡한 과제에 유리합니다. 그러나 다중 에이전트에서는 계획들이 상충할 수 있고, 이러한 조율은 추상적인 상위 목표를 통해 하는 편이 나을 수 있습니다.\n요약하면, 계획 기반 방법은 모델 기반 AI의 지식을 활용하여 RL을 가이드한다는 점에서 의미가 큽니다. 실제로도 로보틱스 등에서, 먼저 시뮬레이터나 플래너로 행동 시퀀스를 짠 후 RL이 미세 조정을 하는 방식이 많이 연구되고 있습니다. 이러한 방법은 보상 함수만으로 에이전트를 끌고 가기 힘든 복잡한 문제에서 인간 지식을 효과적으로 주입하는 수단이 됩니다. 물론 플래너가 필요하고, 계획 결과가 항상 최적이 아닐 수 있다는 한계는 있지만, RL과 planning의 하이브리드는 앞으로도 유망한 방향입니다.\n\n\n신념 기반 Reward Shaping (Belief Reward Shaping, BRS)\n신념 기반 Reward Shaping(BRS)은 환경 보상 구조에 대한 사전 지식(prior)을 확률적 신념(belief) 형태로 활용하여 보상을 형성하는 접근입니다. 전통적인 RL은 환경과의 상호작용으로만 보상 구조를 파악하지만, BRS는 애초에 에이전트 내부에 보상 분포에 대한 가설을 넣어둡니다. 그리고 학습 중에 이 가설(내부 비평가, internal critic)이 환경의 실제 관측과 비교되면서 업데이트되고, 그 결과를 토대로 에이전트에게 형성 보상(shaping reward)을 추가로 제공합니다.\n쉽게 말해, 에이전트는 “내가 생각한 보상 구조”를 하나 가지고 있고, 실제 받는 보상과의 차이를 보며 그 생각을 조정해나갑니다. 그리고 행동할 때는 외부 환경 보상 + 내 신념이 준 보상을 함께 고려합니다. 이 접근은 잠재 기반 보상과 달리 상태 뿐만 아니라 행동까지 신념 보상의 함수로 활용할 수 있습니다. 잠재 보상은 일반적으로 상태에 대해서만 정의되지만, BRS는 상태-행동 쌍에 대해 신념에 기반한 shaping 보상을 줍니다. 예를 들어, 어떤 행동은 위험할 것이라고 사전에 믿고 있다면, 그 행동에 페널티를 주는 식입니다 (이 페널티는 경험이 쌓이며 업데이트될 수 있음).\nBRS에서 신념은 베이지안 방법으로 표현됩니다. 보상 분포에 대한 사전 분포를 두고, 환경 경험이 쌓일수록 posterior를 업데이트합니다. 가령 초기엔 “이 행동은 아마 -5 보상이 될 것이다”라고 믿었다가, 여러 번 해보니 -1 정도만 받는다면 내부 모델을 그에 맞게 수정합니다. 이처럼 에이전트 내부의 비평가(critic)가 지속적으로 자신의 보상 예측 모델을 개선하고, 에이전트는 그 비평가의 출력을 추가 보상으로 활용합니다.\nBRS의 효과는 정확한 사전 지식이 있을수록 극대화됩니다. 논문에서는 복잡한 prior일수록 에이전트 성능이 향상됨을 보였습니다. 이는 당연히, 에이전트가 미리 많은 것을 알고 시작하면 시행착오를 줄이기 때문입니다. 또한 이론적으로 BRS를 Q-러닝과 결합했을 때 추론한 Q값이 일관성(consistency)을 갖는다는 보장도 제시되었습니다. 단, 이 보장은 진짜 환경의 보상 분포가 에이전트의 가정 모델 안에 있을 때만 성립합니다. 만약 현실이 에이전트의 모든 가정을 벗어난 형태라면, 잘못된 신념으로 인해 오히려 학습이 꼬일 수 있습니다.\nBRS는 “환경 외부의 지식”을 활용한다는 점에서 흥미롭습니다. 개발자가 보상 구조에 대해 알고 있는 통계나 추세를 priors로 녹여낼 수 있습니다. 예를 들어 게임 설계를 하는 경우, 어떤 행동은 평균적으로 나쁘다고 가정하고 시작할 수 있습니다. 이렇게 하면 에이전트가 그걸 직접 학습하는 시간을 줄일 수 있죠. BRS는 또 잠재 기반 방식의 한계를 극복하려 했는데, 잠재 \\Phi(s)만으로는 표현 못 하는 요소 (행동에 따른 보상 차이 등)를 다룰 수 있게 했습니다. 즉, state뿐 아니라 action에도 의존하는 shaping이 가능해진 것입니다.\nBRS의 한계는 복잡한 사전 분포를 다뤄야 하므로 계산량이 늘 수 있고, prior 설정이 잘못되면 오히려 편향을 심어줄 위험이 있습니다. 또한 BRS를 실제 큰 환경에 적용한 예시는 아직 많지 않습니다. 그러나 개념적으로, BRS는 내부 모델을 통한 보상 평가라는 측면에서, 인간의 가치 판단을 모방하려는 시도로 볼 수도 있습니다. 인간도 어떤 행동이 나쁠 것이라고 사회적/도덕적 신념을 가지고 피하듯이, 에이전트도 미리 배운 신념으로 탐색을 가이드받는 것입니다. 이 접근은 향후 인간 지식과 RL의 통합에 있어 중요한 아이디어가 될 수 있습니다.\n\n\n이층 최적화 기반 Reward Shaping (BiPaRS)\nBiPaRS (bi-level optimization of parameterized reward shaping)는 Reward Shaping의 가중치를 학습하는 새로운 프레임워크입니다. 지금까지 Reward Shaping은 사람이 추가 보상의 형태와 세기를 결정하는 경우가 많았습니다. 하지만 BiPaRS는 “어떤 shaping 보상이 이롭고 어떤 것은 해로운가”를 스스로 학습하여, 좋은 shaping 신호만 활용하도록 합니다.\n구체적으로, BiPaRS는 상위 단계와 하위 단계의 2계층 최적화로 구성됩니다. 하위(level 2)에서는 주어진 shaping 보상 함수를 포함한 환경에서 정책을 학습합니다. 상위(level 1)에서는 shaping 보상의 가중치 함수 \\omega_\\theta(s,a)를 학습합니다. 이 가중치 함수는 상태-행동별로 shaping 보상을 얼마나 반영할지 결정하며, \\theta라는 파라미터를 가집니다. 상위 단계는 목표가 에이전트의 진짜 성능(원래 환경에서의 성과)을 높이도록 \\theta를 조정하는 것입니다. 쉽게 말해, 상위 단계는 “어떤 shaping은 이득이니까 더 주고, 어떤 건 해로우니 빼자”를 결정합니다.\nBiPaRS의 작동 방식을 방정식으로 나타내면 다음과 같습니다: 원래 보상 R(s,a)에 shaping 보상 F(s,a)를 합친 R'(s,a) = R(s,a) + \\omega_\\theta(s,a) F(s,a)를 에이전트에게 줍니다. 여기서 \\omega_\\theta(s,a)는 [0,1] 범위의 가중치 함수로, \\omega가 0이면 해당 shaping은 무시되고 1이면 온전히 반영됩니다. 상위 최적화는 \\theta를 조정하여, 하위 단계에서 학습된 정책이 원래 환경에서 누적 보상이 최대화되도록 합니다. 즉 상위 단계의 손실은 “원래 보상에 대한 성과”이고, 하위 단계는 “shaped 보상으로 정책 학습”입니다.\n이렇게 함으로써 BiPaRS는 인간이 준 shaping 보상 중 정말 도움 되는 부분만 뽑아쓰는 능력을 갖게 됩니다. 실험 결과, Cartpole 등의 고전 제어와 MuJoCo 연속 제어에서 BiPaRS를 적용한 경우, 일반 shaping을 사용한 것보다 안정적이고 좋은 성능을 보였습니다. 특히 Cartpole에서는 금방 성공하고, HalfCheetah 등에서도 shaping 덕을 보되, 잘못된 shaping 신호는 억제하는 모습을 확인했습니다. 다만 MuJoCo 등의 복잡한 영역에서는 아직 최첨단 성능에 미치진 못해, BiPaRS도 개선 여지가 있다고 언급됩니다.\nBiPaRS의 의의는, Reward Shaping의 자동화된 튜닝이라는 점입니다. 기존에는 shaping 신호의 세기나 형태를 일일이 사람이 조정했지만, BiPaRS는 그것을 학습하는 또 다른 최적화 문제로 격상시켰습니다. 이를 통해 인간의 편향이나 시행착오를 줄이고, 에이전트 스스로 적합한 shaping 전략을 발견하도록 했습니다. 물론 이 접근은 계산 비용이 두 배로 들고(상위 최적화 포함), 안정적인 수렴을 위해 여러 기술적 조건이 필요하지만, 앞으로 AutoRL의 한 분야로서 주목할 가치가 있습니다. 궁극적으로 BiPaRS 같은 방법이 발전하면, 사용자는 shaping 보상 후보들만 정의해주고 에이전트가 최적 조합과 세기를 알아서 설정하는 시대가 올 수 있습니다.\n\n\n메타-보상 네트워크 (MRN)\nMRN (Meta-Reward-Net)은 인간의 피드백을 제한적으로 이용하여 보상 함수와 정책을 함께 학습하는 프레임워크로, BiPaRS와 유사하게 이층 최적화 개념을 활용하지만 인간 선호에 중점을 둔 방법입니다. MRN은 선호기반 강화학습(preference-based RL)에서 발전한 아이디어로, 인간이 에이전트의 행동쌍을 비교하여 어느 쪽이 더 나은지 알려주는 피드백을 조금씩 제공하면, 이를 토대로 보상 모델과 정책을 동시에 학습합니다. 이때 보상 모델 학습과 정책 학습이 이층 구조로 묶여 있어, 보상 모델이 개선되면 정책이 바뀌고, 정책 경험이 늘면 보상 모델도 정교해지는 상호 메커니즘을 가집니다.\nMRN의 독특한 점은, bi-level optimization을 통해 보상 함수 설계와 정책 최적화를 메타 학습처럼 취급했다는 것입니다. 이는 BiPaRS의 철학과 유사하지만, 인간 피드백이 추가된 것이 차이입니다. MRN에서는 에이전트가 받는 보상이 전적으로 환경으로부터 주어지는 것이 아니고, 인간이 선호하는 방향으로 보상 함수 자체를 보정해 나갑니다. 이렇게 하면 희소/지연/노이즈 보상 문제도 인간의 직관으로 일부 해소할 수 있고, 에이전트가 중요한 것에 집중하도록 유도할 수 있습니다.\n실험에서 MRN은 몇 가지 로봇 시뮬레이션 작업에 적용되어, 한정된 인간 입력만으로도 기존 방법보다 우수한 성능을 보였습니다. 예를 들면, 로봇 팔 동작 시나리오에서 사람이 “이 동작이 저 동작보다 좋다”라는 비교 피드백을 조금 주면, MRN이 그 정보를 반영한 보상모델을 형성하고 곧바로 정책에 반영하여 샘플 효율적으로 학습했습니다. 최종적으로 MRN 기반 에이전트는 이전의 수작업 보상이나 단순 선호학습 알고리즘보다 높은 성공률을 보였습니다.\nMRN의 장점을 요약하면: 적은 인간 개입으로도 학습 효율을 높였고, Reward Shaping의 방향성을 사람에게서 얻음으로써 유연성과 안정성을 확보했다는 점입니다. 특히 논문에서는 기존 보상 shaping 기법들이 강건성과 효율 면에서 한계가 있다고 지적하며, MRN처럼 인간 가이드형 Reward Engineering이 필요하다고 강조합니다. 물론 MRN에도 단점은 있는데, 가장 큰 것은 인간의 피드백 품질에 의존한다는 것입니다. 사람이 잘못 선호를 표시하면 엉뚱한 보상 모델을 배울 수 있고, 또한 사람의 시간과 노력을 요구하기 때문에 대규모 학습에는 한계가 있습니다. 그럼에도 불구하고 MRN은 인간-에이전트 협력 학습의 좋은 사례로, 희소하거나 명시적 보상을 설계하기 어려운 문제에서 유용한 프레임워크가 될 수 있습니다.\n추가로 MRN의 구현 코드도 공개되어 있어, 다른 연구자들이 이를 시도해볼 수 있게 했다고 합니다. 이는 앞으로 선호학습과 보상 설계 연구를 가속할 것으로 기대됩니다.\n\n\n기타 기법 및 동향\n이 밖에도 다양한 혁신적 Reward Shaping 프레임워크들이 등장하고 있습니다. 몇 가지 눈에 띄는 동향을 소개하면 다음과 같습니다:\n\n배리어 함수(Barrier Function)를 이용한 안전형 RL: 보상 함수에 배리어 함수를 포함하여, 에이전트가 금지 구역이나 위험 행동을 하지 않도록 하는 방법입니다. 실제 로봇 팔 제어에서 배리어 함수를 보상에 넣었더니 수렴이 빨라지고 작동 에너지 소모가 줄어드는 등 효과가 있었습니다. 배리어 함수는 상태가 안전 영역 밖으로 나가려 하면 보상이 급감하도록 설계된 함수입니다.\n자연어 지시를 통한 보상 생성: 인간이 자연어로 설명한 목표나 규칙을 파싱하여, 그에 따라 dense reward를 만들어내는 기법도 연구되고 있습니다. 예를 들어 “붉은 물체를 녹색 지점으로 옮겨라”라는 문장을 읽고, 그 의미를 해석해 중간단계 보상을 생성하는 것입니다. 이를 통해 sparse reward 문제를 완화했으나, 자연어 처리와 RL의 통합이라는 난제가 따릅니다.\n시간 논리(Temporal Logic)를 활용한 Reward Shaping: 형식 언어로 정의한 목표(예: LTL, CTL 등의 논리식)를 만족하면 보상을 주는 프레임워크가 시도되고 있습니다. 특히 평균-보상 RL(episodic이 아닌 지속환경)에서 시간 논리 제약을 보상으로 녹여 학습 속도를 높이는 연구가 있었습니다. 논리식을 만족하는 경로에는 보너스를 주어, 에이전트가 일정한 패턴의 행동을 익히도록 하는 방식입니다.\n과적합 방지형 보상 (Uncertainty-Penalized Reward): 에이전트의 정책이 특정 상황에 과도하게 특화되는 것을 막기 위해, 보상에 불확실성 페널티를 추가하는 접근도 있습니다. 모델 앙상블을 사용해 정책의 불확실성이 큰 상태에서는 보상을 깎음으로써, 너무 위험하거나 확신 없는 전략을 피하게 합니다. 이는 RL의 오버피팅 문제를 완화하는 효과가 있습니다.\n기대 보상 기반 안정화: 에이전트가 미래의 기대 보상 분포까지 고려해 학습하는 방법으로, 보상의 기댓값을 사용해 정책을 안정화하고 수렴을 빠르게 하는 시도도 있습니다. 이는 불확실한 환경에서 보상이 들쭉날쭉할 때, 평균적인 성과를 높이도록 shaping하는 것입니다.\n\n또한 보상 계획(Reward Planning)이라는 개념도 등장했는데, 상태공간을 쪼개어 서브태스크로 나눈 다음 탐욕적인 분할-정복 방식으로 보상을 설정하는 접근입니다. 한 연구에서는 Pendubot(이중 진자 로봇) 제어에서 이 방법을 썼는데, 진자의 스윙업과 밸런싱이라는 두 개의 서브태스크로 쪼개고, 각각에 대한 보상 함수를 체계적으로 설계했습니다. 그리고 전체 보상은 이 두 단계의 성취에 기반하여 주는 식으로 했더니, 파라미터 불확실성이 200~300%나 되는 상황에서도 에이전트가 빠르게 적응하고 약 95%의 높은 성공률로 임무를 달성했습니다. 이 방법의 장점은, 큰 문제를 쪼개서 각 부분에 최적 보상을 설계할 수 있으므로 불확실성이 크고 복잡한 시스템에도 적용 가능하다는 것입니다. 또한 특정 행동(예: 지나치게 에너지를 쓰는 행동)은 아예 피하도록 보상에서 제외하는 등 미세한 제어도 가능합니다. 단점으로는, 이렇게 보상 함수를 체계적으로 쪼개고 만드는 작업 자체가 시간이 많이 드는 수작업이라는 점입니다. 결국 설계자가 시스템에 대한 깊은 이해가 있어야 하고, 서브태스크로 나누는 과정도 문제에 특화되어 있어 일반화가 어렵습니다.\n요약하면, 최신 Reward Shaping 기법들은 안전, 자연어, 논리, 불확실성, 태스크 분할 등 다양한 키워드를 중심으로 발전하고 있습니다. 이는 보상 설계가 더 이상 단순히 수식 하나 정하는 문제가 아니라, 멀티모달 정보와 고차원 지식, 그리고 인간의 언어/논리적 지식까지 포괄하는 영역으로 확장되고 있음을 보여줍니다."
  },
  {
    "objectID": "posts/paper/2025-07-28-reward-engineering.html#로봇공학-및-기타-분야에서의-응용",
    "href": "posts/paper/2025-07-28-reward-engineering.html#로봇공학-및-기타-분야에서의-응용",
    "title": "📃Reward Engineering 리뷰",
    "section": "로봇공학 및 기타 분야에서의 응용",
    "text": "로봇공학 및 기타 분야에서의 응용\n강화학습의 Reward Engineering/형성 기법들은 특히 로봇공학(Robotics) 분야에서 활발히 응용되고 있습니다. 로봇은 물리적 환경에서 움직이므로 잘못 학습하면 안전 문제가 생길 수 있고, 센서 노이즈 등 현실적인 어려움도 많습니다. 이에 따라 로봇 분야 연구자들은 보상 구조를 창의적으로 설계하여 로봇이 안전하고 효율적으로 학습하도록 시도하고 있습니다.\n\n인간-로봇 협업을 위한 안전 효율형 보상 설계\n산업 현장에서 사람과 로봇이 같이 일하려면, 로봇이 사람을 다치게 하지 않으면서도 작업 효율은 높여야 합니다. 한 연구에서는 이러한 인간-로봇 협업(HRC) 시나리오에서, 로봇이 충돌을 회피하면서도 업무를 잘 수행하도록 보상 함수를 설계했습니다. 구체적으로 IRDDPG (Intrinsic Reward DDPG) 알고리즘을 제안했는데, DDPG라는 Actor-Critic 강화학습 방법에 내재적 보상을 추가한 것입니다. 로봇 팔이 사람과 공동 작업할 때, 외재적 보상은 작업 완료나 충돌 여부 등으로 주고, 내재적 보상으로는 로봇 스스로 설정한 안전 거리 유지나 부드러운 움직임 등을 넣었습니다. 이렇게 복합 보상 함수를 사용하니, 실험 결과 로봇은 사람과 충돌하지 않으면서도 작업을 마치는 정책을 효과적으로 학습했습니다. 기존에 사람이 손수 만든 보상 함수보다도 학습이 빨랐고, 동적 상황에서도 적응력이 높았습니다. 즉, 학습 중에 보상을 조정함으로써 로봇이 안전과 효율 두 마리 토끼를 잡을 수 있었던 것입니다. 이 접근의 성과는, 일반적인 보상으로는 달성하기 어려운 안전 같은 목표를 내재적 보상으로 잘 녹여낸 점입니다. 다만 복잡한 보상 구조 최적화와 높은 연산량이 단점으로 지적되는데, 이는 내재+외재 보상 모두를 다뤄야 하고 DDPG 자체도 자원 소모적이기 때문입니다.\n\n\n대화형 피드백을 활용한 가정용 로봇 학습\n다른 시나리오로, 가정 내 서비스 로봇이 물건 정리 같은 작업을 배울 때 사람의 피드백을 활용하는 연구가 있었습니다. 로봇이 완전히 자율적으로 학습하면 느리고 시행착오가 많을 수 있으므로, 사람 또는 보조 에이전트가 중간에 피드백을 주어 학습을 돕는 방법입니다. 이 연구에서는 세 가지 학습 모드를 비교했습니다:\n\n자율 RL (오로지 환경 보상으로 학습),\n에이전트-보조 대화형 RL (다른 AI 에이전트가 피드백 제공),\n인간-보조 대화형 RL (사람이 피드백 제공).\n\n실험적으로, 동일한 작업에 대해 대화형 피드백을 받은 경우가 학습 속도가 더 빠르고 오류가 적었으며, 누적 보상도 높았습니다. 특히 인간이 피드백을 준 경우 성능이 가장 좋았는데, 이는 사람의 전문 지식이나 직관이 유용했기 때문입니다. 예를 들어 로봇이 물건을 엉뚱한 자리에 둘 때 사람은 즉각 “그건 잘못됐다”라고 신호를 줄 수 있어, 로봇이 긴 탐색 끝에 실패하지 않고도 바로 교정할 수 있습니다. 이러한 인터랙티브 shaping의 장점은, 에이전트가 짧은 에피소드 안에 더 많은 학습을 한다는 것입니다. 즉, 탐험 공간을 사람의 도움으로 좁혀주거나, 보상 신호를 세밀하게 만들어주는 효과입니다. 연구에서는 인간 피드백이 약간 더 효과적이었지만, AI 에이전트의 피드백도 상당한 향상을 주어, 자동화된 코치(agent coach) 개념도 가능성을 보였습니다. 물론 이러한 접근의 과제는, 사람이나 보조 에이전트의 피드백 전략을 어떻게 최적화할 것인가 하는 점입니다. 잘못된 조언을 주면 학습이 망가질 수 있으므로, 언제 개입하고 어느 정도 신호를 줄지 등을 체계화해야 합니다.\n\n\n현실 도메인 차이를 극복하는 보상 및 도메인 적응\n로봇 분야에서 흔히 겪는 문제로, 시뮬레이션과 현실 환경의 차이(sim-to-real gap)가 있습니다. 시뮬레이터로 학습한 정책을 실제 로봇에 이식하면 성능이 급락하는 경우가 많죠. 이를 극복하기 위해 도메인 적응(domain adaptation), 도메인 랜덤화(domain randomization), 메타학습(meta-learning) 등의 기법이 사용되는데, 보상 설계도 이 과정에서 중요한 역할을 합니다.\n\n\n\n\n앞서 그림의 상단에서 묘사된 도메인 적응은, 시뮬레이터와 현실 간 차이를 줄이기 위해 공통 특성 공간을 학습하는 방법입니다. 예를 들어 시뮬레이터 영상과 실제 카메라 영상을 동일한 표현 공간으로 임베딩하여, 시뮬레이터에서 학습한 정책이 현실 입력에도 잘 적용되도록 합니다. 그림 하단 좌측의 도메인 랜덤화는, 시뮬레이터의 물리 파라미터나 렌더링을 무작위로 변화시켜 학습함으로써, 현실의 다양한 조건에 견디는 정책을 얻는 기법입니다. 하단 우측의 메타 RL은, 다양한 환경들에 빠르게 적응할 수 있는 메타 정책을 학습하는 것으로, 이후 새로운 현실 환경에도 짧은 재학습(fine-tuning)으로 적응하도록 하는 방법입니다.\n\n보상 설계는 이러한 sim-to-real 전략들과 함께 쓰여, 시뮬레이션 단계에서부터 현실 적용을 염두에 둔 보상을 형성하는 데 활용됩니다. 예를 들어, 한 연구에서는 CSAR (Consensus-based Sim-And-Real) 알고리즘을 제안하여 시뮬레이터와 현실에서 동일한 보상 구조로 병렬 학습을 진행했습니다. 여러 개의 시뮬레이터 속 가상 로봇과 하나의 실제 로봇을 동시에 학습시키면서, 보상에 대한 합의(consensus)를 이루는 정책을 찾는 것입니다. 이렇게 하면 시뮬레이터의 빠른 학습과 현실 환경의 검증이 동시에 일어나, 현실 전이 시 성능 저하가 적고 훈련 시간도 줄어듭니다. 실험에서 시뮬레이터 로봇의 수를 늘릴수록 실제 로봇의 작업 성공률이 높아졌는데, 이는 시뮬레이터로 다양한 상황을 커버하면서도 보상 구조가 일관되게 유지되었기 때문입니다.\n또 다른 연구에서는 반가상(semi-virtual) 환경을 중간 단계로 도입하여 2단계 sim-to-real 전이를 수행했습니다. 1단계에서 시뮬레이터+약간의 현실 요소 (예: 실제 로봇의 동역학, 그러나 센서는 가상)로 학습하고, 2단계에서 그 정책을 실제 환경(실제 센서+로봇)에서 fine-tuning 하는 방식입니다. 이때도 보상 함수는 두 단계에 걸쳐 일관되게 유지하여, 에이전트가 학습 목표를 바꾸지 않고 적응만 하도록 했습니다. 이러한 체계로 복잡성을 점진적으로 증가시키면, 한 번에 현실로 뛰어드는 것보다 안전하고 안정적으로 학습할 수 있습니다.\nsim-to-real 전이에서 중요한 것은, 보상 해킹이 현실에서도 통할 수 있다는 점입니다. 앞서 예로 든 청소 로봇이 센서를 가리는 경우처럼, 시뮬레이션에서 보상 해킹을 배웠다면 실제 로봇도 유사한 꼼수를 부릴 수 있습니다. 따라서 시뮬레이터 단계에서부터 올바른 보상 구조를 설계하여 해킹 가능성을 줄이고, 에이전트가 진짜 목표에 집중하도록 해야 합니다. 예를 들어 청소 로봇에게는 “방이 깨끗해져라” 보상만 주는 게 아니라, 청소 행동 자체에도 약간의 보상을 줘서 적어도 행동은 하도록 유도할 수 있습니다. 또는 사람의 피드백을 시뮬레이터 단계에서 활용하여, 에이전트가 편법을 하면 즉시 교정받도록 할 수도 있습니다.\n결국 Reward Shaping + 도메인 적응 + 메타러닝 + 인간피드백 등이 종합적으로 활용되어야, 시뮬레이션에서 현실로의 성공적인 지식 이전이 이루어집니다. 이들 접근은 각각 정책의 강건성을 높이고, 효율성을 개선하며, 안전성을 확보하는 역할을 합니다. 앞으로 시뮬레이터의 사실성이 높아지고, 자동화된 보상 설계 기법이 발전한다면, 현실에서 시행착오를 거의 겪지 않고도 시뮬레이션만으로 고성능 정책을 학습하는 것도 꿈이 아닙니다. 다만 그 과정에서 항상 올바른 보상 설정을 유지해야 하며, 에이전트가 잘못된 성공 기준을 추구하지 않도록 지속적인 모니터링이 필요합니다."
  },
  {
    "objectID": "posts/paper/2025-07-28-reward-engineering.html#reward-engineering의-장점과-한계-미래의-핵심인가",
    "href": "posts/paper/2025-07-28-reward-engineering.html#reward-engineering의-장점과-한계-미래의-핵심인가",
    "title": "📃Reward Engineering 리뷰",
    "section": "Reward Engineering의 장점과 한계: 미래의 핵심인가?",
    "text": "Reward Engineering의 장점과 한계: 미래의 핵심인가?\n지금까지 살펴본 사례들이 보여주듯, Reward Shaping/Engineering은 RL 에이전트의 학습을 크게 향상시킬 수 있는 강력한 도구입니다. 정리해보면, 제대로 된 Reward Shaping은 다음과 같은 장점을 제공합니다:\n\n학습 가속화: 추가 정보를 제공함으로써 에이전트가 최적 정책을 훨씬 빠르게 찾도록 도와줍니다. 복잡한 환경에서 수십만 단계가 걸리던 학습이 Reward Shaping으로 크게 단축될 수 있습니다. 예를 들어 STRIPS 기반 shaping이나 UCBVI-shaped, DRiP 등의 방법들은 기존보다 빠른 수렴을 보여주었습니다.\n탐험 향상: 특히 희소 보상 환경에서, shaping 보상은 에이전트에게 탐험 동기를 부여하여 더 활발하게 환경을 탐색하게 합니다. 이는 에이전트가 놓쳤을 가치 있는 상태들을 발견하게 해주며, 결과적으로 더 나은 정책을 찾게 됩니다. 여러 사례들(해시 탐색, RUNE 등)에서 shaping을 통해 baseline이 전혀 성공 못 하던 과제를 해결하기도 했습니다.\n최종 성능 개선: Reward Shaping을 적절히 하면 에이전트가 애초 shaping 없이는 도달 못했을 고성능 정책에 도달하기도 합니다. 최적 정책의 공간에 여러 후보가 있을 때, shaping은 그 중 더 우수한 정책을 선택하도록 유도할 수 있습니다. Section IV의 여러 방법들이 기존 방법 대비 성능 향상을 입증했습니다.\n정책 강건성 증가: 잘 설계된 shaping은 학습된 정책을 잡음, 환경 변화 등에 덜 취약하게 만들 수 있습니다. 예를 들어 혼동행렬로 노이즈를 보정한 보상이나, 도메인 랜덤화와 결합한 shaping 등은 에이전트가 다양한 상황에서도 무너지지 않도록 합니다. 또한 잠재 보상 등으로 특정 위험 행동을 피하도록 학습하면, 환경이 약간 달라져도 기본적인 안전은 지켜집니다.\n\n이러한 장점들 때문에, 많은 전문가들이 Reward Engineering을 미래 강화학습 성과의 열쇠 중 하나로 봅니다. 그러나 한편으로는 몇 가지 중대한 도전과 한계도 존재합니다:\n\n도메인 지식 의존성: 많은 Reward Shaping 기법들이 해당 문제에 대한 전문가 지식에 의존합니다. 잠재 함수나 휴리스틱 보상, 계획 기반 보상 등은 사람이 일일이 설계해야 하고, 이는 복잡한 도메인일수록 어렵습니다. 처음 문제를 접하거나 전문 지식이 부족한 경우엔 어떤 shaping을 줘야 할지조차 모를 수 있습니다.\n추가 설계 시간과 노력: 보상 함수를 기본 환경 보상 외에 또 설계하는 것은 추가 작업 부담을 뜻합니다. 보상 함정에 빠지지 않도록 여러 번 시험하며 조정해야 할 수도 있고, 최적의 shaping 파라미터를 찾기 위해 많은 실험을 해야 할 수도 있습니다. 이는 프로젝트 일정이나 비용 측면에서 부담입니다.\n계산 복잡도 증가: 일부 shaping 방법은 알고리즘 구조를 복잡하게 만들어 계산 비용을 늘립니다. 예컨대 잠재 기반 방법에서 잠재값을 계산·업데이트하는 비용, BiPaRS처럼 이층 최적화를 수행하는 비용 등이 추가됩니다. 실시간 제어나 제한된 자원 환경에서는 이러한 오버헤드가 문제될 수 있습니다. (물론 반대로, shaping으로 필요 단계 수가 줄어들어 결과적 총비용은 감소하는 경우도 있습니다).\n파라미터 튜닝 문제: 많은 shaping에는 새로운 하이퍼파라미터들이 생깁니다 (보상 스케일, 혼합 비율 등). 이것들을 적절히 조율해야 효과를 보는데, 잘못 설정하면 학습이 망가질 수 있습니다. 다행히 BiPaRS같이 파라미터를 자율 튜닝하려는 시도도 있지만, 여전히 일반적이지는 않습니다. 사용자는 새로운 shaping 기법을 도입할 때 또다른 튜닝 과제를 떠안게 됩니다.\n설계 오류 가능성: 잘못된 Reward Shaping은 없느니만 못할 수 있습니다. 의도와 다르게 최적 정책을 바꿔버리거나, 보상 해킹을 유발하거나, 학습을 느리게 만들 수도 있습니다. 따라서 신중한 검증과 모니터링이 필요하며, 이는 실험적 노력을 요합니다.\n\n이러한 한계 때문에, Reward Shaping을 도입할 때는 문제 상황에 대한 면밀한 고려와 검토가 필요합니다. 그럼에도 불구하고, 잘만 활용하면 얻는 이익이 매우 크므로, 많은 RL 연구와 응용 분야에서 Reward Engineering은 포기할 수 없는 도구가 되고 있습니다.\n결론적으로, Reward Shaping은 강화학습 성능을 한 단계 끌어올리는 귀중한 기법입니다. 각 방법마다 장단이 있으므로, 과제의 특성에 맞는 적절한 방법을 선택하는 것이 성공의 열쇠입니다. 또한 Reward Shaping 기법들을 서로 조합하거나 (예: 잠재 보상 + 차이 보상, 인간 피드백 + 도메인 랜덤화 등), 다른 AI 기술과 통합하여 (자연어, 논리 등) 활용하는 방향도 유망합니다. 앞으로 RL을 실제 복잡한 시스템에 적용하려면, 단순한 보상만으로 학습하는 것보다는 이런 정교한 보상 설계 전략들을 능숙하게 구사하는 능력이 요구될 것입니다."
  },
  {
    "objectID": "posts/paper/2025-07-28-reward-engineering.html#개방된-과제-및-향후-연구-방향",
    "href": "posts/paper/2025-07-28-reward-engineering.html#개방된-과제-및-향후-연구-방향",
    "title": "📃Reward Engineering 리뷰",
    "section": "개방된 과제 및 향후 연구 방향",
    "text": "개방된 과제 및 향후 연구 방향\nReward Engineering 및 Reward Shaping의 발전을 위해 앞으로 해결해야 할 열린 과제들도 많이 남아 있습니다. 우선, 샘플 효율 향상은 지속적인 목표입니다. 현실에서 로봇이나 자율주행차를 학습시킬 때, 수많은 시도를 해볼 수 없으므로 적은 데이터로 학습하는 기법이 필수입니다. Reward Shaping은 그 해결책 중 하나로 거론되지만, 이를 더욱 개선하여 필요 최소한의 시도로도 최적 정책에 근접할 수 있게 하는 연구가 필요합니다. 예를 들어 BiPaRS나 MRN처럼 Reward Shaping 자체를 학습하는 방법, 혹은 메타러닝과 결합한 shaping으로 few-shot 학습을 가능케 하는 방향이 유망합니다.\n또한 강건성(Robustness)은 여전히 중요한 이슈입니다. 앞에서 예를 든 노이즈 보상 처리나 안전형 보상 등이 그 방향이지만, 더욱 일반적인 의미에서, 환경 변화나 예측 불가능한 상황에서도 안정적 성능을 보장하는 알고리즘이 요구됩니다. 이를 위해서는 이론적 보장과 실험적 검증이 균형을 이뤄야 합니다. 예컨대 Osinenko 등의 연구처럼, 강화학습에 대한 안정성 보장을 제공하면서도 Reward Shaping으로 실제 성능을 높이는 접근이 필요합니다.\n인간-로봇 협업이나 인간 피드백 통합 역시 미래 핵심 주제입니다. Industry 4.0 시대에 인간과 AI/로봇이 함께 일하는 경우가 많아지고 있으므로, 인간의 의도를 RL에 반영하는 방법이 중요합니다. 보상 설계 측면에선, 휴먼 인 더 루프(human-in-the-loop) 학습을 자연스럽게 만들 기술이 요구됩니다. MRN이나 선호 학습 방식은 한 예지만, 더 나아가 실시간으로 사람과 상호작용하며 배우는 RL (예: 음성이나 제스처로 보상 신호를 주고받는 시스템)도 생각해볼 수 있습니다. 이는 기술적으로 난이도가 높지만, 성공한다면 RL의 활용 폭을 크게 넓힐 것입니다.\n고차원 감각 데이터(이미지, LiDAR 등)로 작동하는 RL에서의 보상 설계도 과제입니다. 이러한 경우 명시적 보상을 설계하기 매우 어려우며, 종종 end-to-end RL로 접근합니다. 예를 들어 Sergey Levine 등의 연구에서는 보상 설계 없이 로봇에 성공 데모만 몇 개 보여주고 SAC(Soft Actor-Critic) 알고리즘으로 학습시키는 방식을 취했습니다. 이를 통해 이미지 입력 기반 물체 조작에서 100%에 가까운 성공률을 달성했다고 보고합니다. 이러한 보상 없는(or 최소 보상) 학습은, 보상 설계의 수고를 덜어준다는 장점이 있습니다. 그러나 완전히 보상을 없앤 방식에도 한계가 있습니다. 예컨대 데모를 통해 학습한 정책이 항상 최적은 아니어서, 비효율적인 동작이 나올 수 있고, 환경 변화(물체 위치나 조명 변화 등)에 약하며, 사용자의 질의 또는 교정 과정이 필요하면 그 양이 많아지면 힘들다는 점이 지적됩니다. 다시 말해, 명시적 보상을 제거하면 다른 형태의 어려움이 생길 수 있다는 것입니다.\n따라서 이상적인 방향은, 명시적 보상 설계와 암시적 시범/피드백 학습을 적절히 통합하는 것입니다. 예컨데, 기본적인 보상 구조는 간단히 설계하되, 세부적인 튜닝이나 예외 상황 처리는 데모나 인간 피드백으로 보완하는 식입니다. 혹은 반대로, 일단 데모로 학습한 후에 추가 Reward Shaping으로 성능을 끌어올리는 방법도 가능할 것입니다.\n또 하나의 열린 과제는 자동화된 보상 설계(Automated Reward Design)입니다. 이는 앞서 다룬 BiPaRS나 meta reward learning 등이 포함되는 주제로서, 강화학습 자체가 최적의 보상 함수를 찾도록 하는 것입니다. 이상적으로는, 사용자가 목표만 명시하면 AI가 스스로 적합한 보상 구조를 구성하고 학습까지 해주는 것이죠. 이를 위해선 메타러닝, 진화 알고리즘, 혹은 고차원 최적화 기법들이 활용될 수 있습니다. 현재는 이러한 자동화 기법들이 초기 단계이지만, 점차 발전하여 보상 설계의 부담을 크게 줄여줄 것으로 기대됩니다.\n실세계 검증도 중요한 미래 방향입니다. Reward Shaping 기법들은 주로 시뮬레이션이나 제한된 실험에서 효용을 보였지만, 현실 세계의 복잡한 문제들(예: 자율주행 전체 시스템, 공장 자동화 라인, 의료 처치 전략 등)에 적용된 사례는 적습니다. 따라서 이러한 영역에 대한 파일럿 적용 연구를 통해, 실제로도 Reward Engineering이 성과를 내는지 평가해야 합니다. 현실 적용에서는 예기치 못한 문제(보상 해킹이 사회적으로 위험한 결과를 낳는다든지, 윤리적 문제 등)가 나올 수 있으므로, 다학제적 관점에서 보상 설계의 영향을 살피는 것도 필요합니다.\n마지막으로, 인간 가치와의 정렬(Alignment) 문제를 들 수 있습니다. 강화학습 에이전트가 높은 보상을 쫓다 보면 인간이 원하지 않는 방향으로 행동할 수 있다는 우려가 있습니다. 이걸 방지하려면 애초에 인간의 가치관을 반영한 보상 함수가 중요합니다. 향후 연구에서는 윤리적 AI, AI 안전 분야와 연계하여, 보상 설계를 통해 AI의 행동을 사회적, 윤리적 기준과 부합하게 만드는 방법을 모색할 것입니다.\n정리하면, Reward Engineering은 앞으로 더 똑똑한 RL을 위해 반드시 풀어야 할 숙제들과 가능성을 함께 지니고 있습니다. 샘플 효율, 강건성, 인간 통합, 자동화, 실전 적용, 가치 정렬 등이 키워드이며, 각각 많은 연구 여지가 있습니다. 현 시점까지의 연구들을 기반으로, 이러한 방향으로 혁신이 이어진다면, Reward Engineering은 강화학습의 미래 핵심 기술로 자리매김할 것입니다."
  },
  {
    "objectID": "posts/paper/2025-07-28-reward-engineering.html#결론",
    "href": "posts/paper/2025-07-28-reward-engineering.html#결론",
    "title": "📃Reward Engineering 리뷰",
    "section": "결론",
    "text": "결론\n본 리뷰에서는 강화학습에서의 Reward Engineering과 Reward Shaping 기법들을 종합적으로 고찰하고 세부 내용을 분석하였습니다. 문헌조사 결과를 바탕으로, 다양한 방법론들을 체계적으로 분류하고 각 접근법의 메커니즘, 예시, 장단점을 살펴보았습니다. 또한 실제 로봇 공학 등 분야별 응용 사례를 통해 이러한 기법들의 효과와 도전과제를 논의하였고, 향후 연구 방향과 남은 과제에 대해서도 통찰을 제시했습니다.\n중요한 결론 중 하나는, 적절한 Reward Shaping은 강화학습의 학습 속도와 결과를 비약적으로 향상시킬 수 있다는 것입니다. 보상 shaping을 통해 학습을 가이드하면 에이전트는 더 빠르게 목표를 배우고, 불확실성이나 잡음이 있어도 견디며, 궁극적으로 더 나은 성능을 발휘할 수 있음을 다양한 연구들이 보여주었습니다. 특히 복잡한 현대 RL 과제 (예: 고차원 로봇 제어, 인간과의 상호작용 등)일수록 이러한 보상 설계 전략이 성공의 열쇠임을 확인했습니다.\n동시에, Reward Engineering의 구현에는 신중함과 노력이 필요합니다. 잘못된 설계는 원치 않은 행동을 낳거나 학습을 망칠 수 있으므로, 도메인 지식과 실험적 튜닝을 적절히 활용해야 합니다. 최근에는 BiPaRS 같은 자동화 기법들이 나오고 있으나, 여전히 사람의 판단과 창의성이 중요한 부분입니다. 결국 문제에 가장 맞는 방법을 골라 쓰는 안목이 요구됩니다.\n본 리뷰의 내용을 통해 얻을 수 있는 실용적 시사점은, 강화학습 연구자나 현업 엔지니어들이 새로운 과제를 접할 때 다양한 Reward Shaping 기법을 고려 목록에 넣어야 한다는 것입니다. 어떤 문제에는 잠재 보상이, 어떤 문제에는 차이 보상이, 또 다른 문제에는 인간 피드백이 어울릴 수 있습니다. 이 리뷰가 제시한 분류와 사례들은 그러한 판단에 가이드가 될 것입니다.\n마지막으로, 강화학습의 실제 적용에서 보상 설계는 이제 선택이 아닌 필수에 가까워지고 있습니다. 단순한 목표 함수를 정해두고 방치하기보다는, 학습이 잘 되도록 보상을 정교히 다듬어주는 것이 성패를 좌우합니다. 따라서 향후 연구에서는 Reward Shaping을 더 자동화하고 체계화하여, 많은 문제들에 쉽게 적용할 수 있게 하는 것이 중요합니다. 또한 인간의 협력을 받아들이는 방향으로 RL을 발전시켜, 안전하고 신뢰할 수 있는 AI를 만드는 데 기여해야 할 것입니다.\n요약하면, Reward Engineering 및 Reward Shaping은 강화학습의 발전과 실제 응용에 있어 핵심적인 역할을 하고 있으며, 본 종합 검토를 통해 그 현황과 전망을 살펴보았습니다. 이 지식이 연구자와 실무자들에게 유용한 지침이 되어, 더욱 성공적인 RL 시스템 개발로 이어지길 기대합니다.\n+\n\nReinforcement learning with guarantees: a review"
  },
  {
    "objectID": "posts/paper/2025-07-24-inhand-trans.html",
    "href": "posts/paper/2025-07-24-inhand-trans.html",
    "title": "📃In-Hand Translation Learning 리뷰",
    "section": "",
    "text": "Paper Link\nProject Link"
  },
  {
    "objectID": "posts/paper/2025-07-24-inhand-trans.html#기술-개요-배경과-목적",
    "href": "posts/paper/2025-07-24-inhand-trans.html#기술-개요-배경과-목적",
    "title": "📃In-Hand Translation Learning 리뷰",
    "section": "2.1 기술 개요: 배경과 목적",
    "text": "2.1 기술 개요: 배경과 목적\n사람은 일상적인 물체 조작에서 촉각에 크게 의존하며, 이를 모방하여 로봇에 촉각 센서를 장착하려는 연구가 활발합니다. 그러나 정교한 촉각 센서를 만들고 이를 로봇 제어에 활용하는 데에는 어려움이 있습니다. 고해상도의 촉각 정보는 처리와 시뮬레이션에 시간이 많이 들고, 반대로 빠른 시뮬레이션은 촉각 정보를 지나치게 단순화하는 경향이 있어 실제와 차이가 큽니다. 이러한 문제로 현재 많은 강화학습 기반 조작 연구에서는 촉각 정보를 생략하거나 단순화된 접촉 신호만 사용하는 경우가 많았습니다.\n본 논문에서는 이러한 시뮬레이션-실세계 격차(sim-to-real gap)를 줄이기 위해, 로봇 손바닥의 자기 기반 탄성 촉각 피부(sensor)인 ReSkin을 모델링하여 이산화된 촉각 신호를 출력하는 시뮬레이터를 개발했습니다. 이 모델은 각 촉각 센서 소자(taxel)에서 법선 방향 힘은 2값(binary)으로, 전단력은 3값(ternary)으로 출력합니다. 즉, 물체가 센서를 누르고 있으면 법선 방향 접촉 신호가 1 (없으면 0)이며, 전단력은 미끄러지는 방향에 따라 +1 또는 -1 (미끄럼 없음은 0)으로 표시됩니다. 이렇게 단순화한 촉각 표현을 통해 시뮬레이터의 효율을 높였으며, 두 개의 GPU 상에서 5000Hz 이상의 모의 센싱 속도로 동작하여 촉각이 없는 경우 대비 약 70%의 속도로 학습을 진행할 수 있었습니다. 이 센서 모델의 가장 큰 장점은, 시뮬레이션에서 학습한 정책을 실세계로 별도 미세조정 없이 바로 옮길 수 있다는 점(zero-shot transfer)으로, 논문 저자들은 이 모델로 훈련한 정책을 190회의 실험을 통해 검증하며 실제 로봇에서 안정적으로 동작함을 보였습니다.\n문제 정의: 저자들은 이 센서 모델을 활용하여 손바닥 위에서 물체를 한쪽에서 반대쪽으로 이동시키는 손안 평행이동(in-hand translation) 과제를 수행했습니다. 이 과제는 물체를 손가락들 사이에서 미끄러뜨리며 이동시켜야 하기 때문에 지속적인 슬라이딩 접촉 제어가 필요하고, 따라서 전단력에 대한 촉각 피드백이 특히 중요하다고 가정했습니다. 기존 연구들은 주로 물체의 자세 변경(회전)에 초점을 맞추거나, 촉각을 사용하더라도 법선 힘(접촉 여부)만 활용한 경우가 많았습니다. 반면 본 연구는 회전에 국한되지 않은 평행이동이라는 새로운 조작 기술에 도전하고 있으며, 특히 법선+전단 3축의 촉각 정보를 활용해 강화학습 정책을 학습함으로써 복잡한 접촉 상황에서의 적응 능력을 높이고자 했습니다. 저자들의 목표는 이처럼 복잡한 촉각 정보를 다루는 최초의 RL-활용 촉각 피부 모델을 제시하고, 이를 통해 섬세한 물체 조작(dexterous manipulation) 기술을 한 단계 발전시키는 것이었습니다.\n요약하면, 이 논문의 핵심 기여는 (1) 시뮬레이션에서 사용 가능한 고속 촉각 피부 센서 모델을 개발하여 전단/법선 촉각 신호의 sim-to-real 전이를 가능케 한 것, (2) 이를 활용해 손안에서의 물체 평행이동이라는 난이도 높은 조작 작업을 강화학습 정책으로 성공적으로 학습한 것, 그리고 (3) 실제 로봇 실험을 통해 촉각 센싱(특히 3축 전단+법선)이 있을 때 없는 경우보다 월등히 높은 성능과 일반화 능력을 발휘함을 실증한 것입니다."
  },
  {
    "objectID": "posts/paper/2025-07-24-inhand-trans.html#실험-세팅-로봇-손-센서와-작업-환경",
    "href": "posts/paper/2025-07-24-inhand-trans.html#실험-세팅-로봇-손-센서와-작업-환경",
    "title": "📃In-Hand Translation Learning 리뷰",
    "section": "2.2 실험 세팅: 로봇 손, 센서와 작업 환경",
    "text": "2.2 실험 세팅: 로봇 손, 센서와 작업 환경\n하드웨어: 실험에는 SimLab Allegro Hand 로봇 손이 사용되었습니다. 이 손은 사람 손과 유사하게 4개의 손가락(엄지 포함)을 가지고 각 손가락에 4개의 관절이 있어 총 16자유도를 갖습니다. 각 관절에는 관절 위치 센서(proprioception)가 있으며, 강화학습 정책은 매 30Hz마다 관절별 목표 위치 명령을 내고, 이 명령은 내부 PID 제어를 통해 토크로 변환되어 구동됩니다. 촉각 센서는 Meta AI에서 개발한 ReSkin이라는 자성 기반의 얇은 촉각 피부를 사용했으며, 약 37×96 mm 크기의 패치를 손바닥 중앙에 부착했습니다. ReSkin은 탄성 재질 안에 자성을 띤 미세 입자를 포함하고 있어, 변형될 때 그 분포 변화로 자기장 센서(magnetometer)가 3축 힘의 변화를 감지하는 원리입니다. 실험에서 ReSkin은 약 120Hz 속도로 데이터를 출력했고, 시뮬레이션 학습 단계에서는 이 센서의 동작을 가상 모델로 구현했습니다.\n한 가지 실제 구현상의 문제는, ReSkin 표면과 물체 사이 마찰력이 매우 높다는 점이었습니다. 손바닥에 물체가 닿으면 쉽게 미끄러지지 않으므로 손가락에 과도한 힘이 필요하고, 이는 Allegro Hand의 모터 발열이나 토크 한계를 초래할 수 있습니다. 이를 완화하기 위해 연구진은 ReSkin 위에 얇은 PET 필름(0.25mm)을 덮어서 마찰력을 감소시켰습니다. 또한 손가락 일부에는 고무 테이프를 부착해 물체가 손가락에서 잘 미끄러지지 않도록 보조했습니다. 이러한 물리적 조치는 강화학습 정책의 성능에는 직접 영향하지 않지만, 하드웨어 한계로 인한 문제를 완화하여 실험을 안정적으로 수행하기 위한 것입니다.\n훈련과 환경: 정책 학습은 NVIDIA Isaac Gym 시뮬레이터에서 이루어졌습니다. 시뮬레이션 환경에는 Allegro Hand 모델과 손바닥의 촉각 센서 모델, 그리고 이동시킬 원기둥 물체가 포함됩니다. 학습시 물체는 일정한 크기의 원기둥이 아니라 무작위로 크기(scale)를 바꾼 원기둥을 사용하여, 정책이 다양한 크기에 일반화하도록 했습니다. 매 에피소드마다 물체의 질량, 마찰계수, 무게중심 위치 등 물리 속성도 랜덤으로 변화시켜 도메인 무작위화(domain randomization)를 적용했습니다. 물체는 항상 초기에 손바닥 쪽으로 단단히 쥐어진 상태(안정된 파지)에서 시작하며, 목표 위치는 손바닥을 가로질러 초기 위치로부터 x축 방향 일정 거리 떨어진 지점으로 주어졌습니다. 에이전트(로봇 손)의 목표는 물체를 손에서 떨어뜨리지 않고 제한 시간 동안 목표 지점까지 이동시키는 것입니다. 한 에피소드는 4초(시뮬레이션 400 스텝)에 걸쳐 진행되며, 도중에 물체를 떨어뜨리면 실패로 간주하고 에피소드를 종료합니다.\n테스트 환경: 학습이 완료된 정책은 실제 로봇 손에 이식되어 검증되었습니다. 테스트에 사용된 물체는 총 4가지로, 훈련과 같은 원기둥(직경 5cm, 길이 15cm)을 비롯하여, 망치(머리 부분에 무게가 치우친 비대칭 물체), 드라이버(손바닥에 여러 점으로 닿는 복잡한 형상), 물병(안의 물이 움직여 무게중심이 변하는 물체) 등을 선택했습니다. 이들 중 원기둥 하나만이 훈련(domain) 범위 내 물체이고, 나머지 셋은 훈련에 없던 새로운 물체(OOD)로 간주되어 정책의 일반화 성능을 평가합니다. 또한 물체 이동 난이도를 조절하기 위해 로봇 손 자체를 기울이는 실험을 했습니다. 손바닥을 수평이 아니라 중력에 대해 최대 20도까지 기울여서 물체를 이동시키면, 중력에 의해 물체가 아래로 미끄러지려는 힘이 생겨 이동이 더 어려워집니다. 연구진은 0도(수평)에서 5도, 10도, 15도, 20도까지 기울인 상황을 시험하여, 기울기 변화(중력 영향)에 대한 정책의 견실성(robustness)도 평가했습니다. 각 조건(물체 종류 + 손 기울기)에 대해 약 5회의 실험 롤아웃을 수행했고, 총 190회의 실세계 실험 데이터를 수집하여 성능을 분석했습니다. 실험 중에는 OptiTrack 모션 캡처로 물체의 실제 이동 거리(cm)와 이동 속도(cm/s)를 측정하여 정량적 성과 지표로 사용했습니다. 또한 성공 여부는 제한 시간(최대 120초) 내에 물체가 0cm 이상이라도 움직였는지로 정의하여, 아예 물체를 떨어뜨리거나 전혀 못 움직인 경우만 실패로 간주했습니다. (※ 임계값을 0cm로 둔 이유는, 경사 환경에서는 일부 정책이 물체를 전혀 이동시키지 못하는 경우가 있어 이를 분리하기 위함입니다.)\n비교 대상: 정책의 효과를 분석하기 위해 여러 가지 입력 조합에 따른 정책들을 학습시켜 비교했습니다. 모든 정책은 동일한 강화학습 알고리즘과 보상 함수를 사용하되, 관찰 입력에 따라 다음과 같이 나뉩니다:\n\nProprioception Only (고유감각만): 촉각 없이 관절각도 등 로봇의 상태만으로 학습한 정책 (기준 시나리오).\nNormal Only (법선 힘만): 촉각 중 법선 접촉 신호만 2값 입력으로 사용한 정책. 이는 기존 연구에서 흔히 사용하는 접촉 유무 센싱과 유사합니다.\nSigned Shear Only (전단만, 부호 포함): 전단력 신호만 3값(방향성 포함) 입력으로 사용한 정책. 물체가 어디로 미끄러지는지 방향만 감지하고 접촉 유무는 고려하지 않습니다.\nUnsigned 3-Axis (U3-Axis, 전단 절대값): 법선+전단 모두 사용하되 전단력의 부호는 무시하고 크기만 반영한 정책입니다. 즉 전단도 2값(binary) 형태로 간주하여, 촉각 센서가 미끄러짐을 감지하면 방향에 상관없이 1로 처리하는 방식입니다. 실제 ReSkin 센서의 이산화에도 유사하게 전단 변화의 절대값만 사용하기 때문에 도입된 변형입니다.\nSigned 3-Axis (S3-Axis): 본 논문의 제안 방법, 전단력 방향까지 살린 3축 촉각 (법선+전단) 입력을 모두 사용하는 정책입니다.\n\n이들 정책은 모두 동일한 환경에서 학습되어, 촉각 정보의 유무와 종류가 성능에 미치는 영향을 평가하게 됩니다. 학습을 위한 알고리즘으로는 Proximal Policy Optimization (PPO)이 사용되었으며, 반복 학습을 통해 각 정책이 충분한 성능에 수렴하도록 했습니다. 특히 S3-Axis와 U3-Axis 정책은 동일한 사전학습된 “오라클” 정책으로부터 파생되는데, 이 상세 내용은 다음 방법론에서 설명합니다."
  },
  {
    "objectID": "posts/paper/2025-07-24-inhand-trans.html#핵심-방법론-촉각-센서-모델링과-2단계-학습",
    "href": "posts/paper/2025-07-24-inhand-trans.html#핵심-방법론-촉각-센서-모델링과-2단계-학습",
    "title": "📃In-Hand Translation Learning 리뷰",
    "section": "2.3 핵심 방법론: 촉각 센서 모델링과 2단계 학습",
    "text": "2.3 핵심 방법론: 촉각 센서 모델링과 2단계 학습\n1. 촉각 피부 시뮬레이션 모델: 저자들은 시뮬레이터에서 ReSkin 촉각 피부의 동작을 근사하기 위해, 손바닥 면적 전체를 연속적인 접촉 면으로 모사하고 그 위에 16개의 가상 촉각 소자(taxel)를 균일하게 배치했습니다. 각 촉각 소자는 실제 하드웨어의 자력 센서에 대응되며, 해당 위치의 피부 변형을 감지한다고 가정합니다. 구체적인 모델링 절차는 다음과 같습니다:\n\n손바닥 면과 물체 표면 사이의 충돌을 감지하기 위해, 물체의 표면을 여러 개의 점(point)으로 표면 점군(point cloud)화 합니다. 이때 손바닥에 매우 가까이 있는 물체 점들을 찾기 위해, 각 촉각 소자 위치를 중심으로 작은 원통형 감지 영역(sensing range)을 정의합니다. 논문에서는 각 taxel마다 물체와 충돌을 계산하는 기본 지오메트리(원통)를 두고, 그 반경을 약간 확대하여 소프트 재질의 탄성(compliance) 효과를 모사했다고 설명합니다. 다시 말해, 실제 촉각 피부가 약간 눌려도 접촉을 느끼는 것처럼 시뮬레이션에서도 충돌 판정 거리를 늘려서 물체가 완전히 부딪히지 않아도 가까이 오면 힘으로 간주합니다. 이러한 범위 확장과 다중 점 샘플링 기법을 통해, 일반적인 물리엔진의 “한 접촉당 한 점” 모델 대신 넓은 면적의 접촉으로 인한 힘 분포를 근사할 수 있습니다.\n\n\n\n\n\n촉각 스킨의 시뮬레이션-실사 전달을 위한 모델링 접근 방식입니다. A) 손바닥을 16개의 개별 taxel이 있는 연속적인 표면으로 모델링하며, 각 taxel은 하부의 자력계에 해당합니다. B) 각 taxel에 대해 실린더를 사용하고 감지 범위 (R)을 충돌 지오메트리 너머로 확장합니다. C) 물체에서 점들을 샘플링하고 충돌 표면을 포인트 클라우드로 나타냅니다. 감지 범위 내의 점들은 i로 표시됩니다. D) 침투 거리 P_i = R - l_i를 합산하며, 여기서 l_i는 점 i에서 센서 원점까지의 거리입니다. 전단력 및 수직력에 대한 센서 신호는 \\sum_{i=1}^{n} P_i, 물체 속도 및 물체 점 밀도를 사용하여 계산합니다.\n\n\n법선 힘 (Normal force): 각 촉각 소자에 대해 감지 영역 내 들어온 물체 표면 점들의 침투 깊이(penetration distance)를 합산합니다. 이 합산 값이 일정 임계치 이상이면, 해당 소자에 물체가 눌리고 있다(접촉 있음)고 판단하여 법선 접촉 신호 = 1을 출력하고, 아니면 0을 출력합니다. 침투 깊이 합산값 자체는 물체가 누르는 힘의 근사치로 간주됩니다. 또한 물체 크기에 따라 이 값이 달라지는 것을 보정하기 위해, 물체의 표면 점 밀도(점 개수/부피)로 나눈 정규화를 수행합니다.\n전단력 (Shear force): 전단의 경우 물체와 손바닥 사이에 상대 운동이 있어야 발생합니다. 저자들은 콜롬 마찰 모델에 착안하여, 물체의 순간 속도(선형 및 각속도)와 위에서 계산된 법선 힘 근사치를 활용해 전단 신호를 계산했습니다. 쉽게 말해, 물체가 손바닥을 미끄러지는 방향으로 움직이고 있고 (속도 방향), 접촉력도 일정 수준으로 작용하면 그 방향으로 전단 접촉 신호 = 1 (또는 -1)을 출력하도록 했습니다. 구체적으로는 물체의 x, y축 방향 속도 성분을 가져와, 그 방향에 법선 힘 규모에 비례하는 전단 값을 산출한 뒤 이 역시 임계값 기반으로 이진화합니다. 전단의 부호(방향)는 속도 방향으로 정해지며, 예를 들어 물체가 +x 방향으로 미끄러지면 해당 taxel의 x축 전단 신호 = +1, -x로 미끄러지면 = -1, 거의 정지되어 있으면 0으로 표현됩니다. 이렇게 계산된 전단력 신호 두 축(x, y)과 법선 신호(z), 총 3축의 이산 촉각 출력이 최종적으로 강화학습 정책의 입력으로 제공됩니다.\n이산화 및 이력 처리: 현실의 ReSkin 센서는 연속적인 아날로그 신호를 내지만, 이는 히스테리시스 등의 문제로 곧바로 사용하기 어렵습니다. 따라서 시뮬레이션에서는 처음부터 임계값을 넘겨 0/1 (또는 ±1)로 이산화(discretization) 하여 RL에 사용했고, 실제 센서에서도 시간에 따른 변화량을 보고 임계값을 넘을 때에만 상태가 바뀌는 방식으로 신호를 이진 처리했습니다. 예를 들어, 실제 실험에서 센서 출력이 한 방향으로 지속적으로 힘을 받고 변화가 없으면, 일정 시간이 지나 출력 0(변화 없음)으로 간주합니다. 이렇게 해야 ReSkin의 특성상 발생하는 신호 잔류(hysteresis)를 억제할 수 있었기 때문입니다. 다행히도 손안에서 물체를 움직이는 동안 센서 신호는 자주 변화하기 때문에, 이러한 방법이 크게 문제되지 않았다고 합니다.\n\n2. 강화학습 정책 학습 (두 단계): 촉각 피부 모델이 준비되었지만, 곧바로 3축 촉각 정보를 넣어 강화학습을 시키면 학습 난이도가 매우 높을 수 있습니다. 특히 초기에는 랜덤한 정책으로 인해 물체를 쉽게 떨어뜨릴 것이고, Sparse한 보상 때문에 학습이 안 될 위험이 있습니다. 저자들은 이 문제를 해결하고자 “교사-학생 학습”과 유사한 2단계 학습(framework)을 도입했습니다. 이는 [Chen et al., 2022] 등이 사용한 접근을 따랐다고 명시되어 있습니다:\n\n1단계 – 오라클(Oracle) 정책 학습: 여기서 오라클이란 학습에 시뮬레이터의 “특권 정보(privileged information)”를 활용한 이상적인 정책을 말합니다. 구체적으로, 시뮬레이션 내부의 물체 상태(물체의 위치, 속도, 크기, 질량, 무게중심, 마찰계수 등)를 모두 알고 있다고 가정하고, 이 정보를 8차원 벡터로 압축하여 정책의 입력으로 제공합니다. 물론 로봇의 관절 상태(proprioception)도 함께 입력됩니다. 이렇게 하면 정책이 어떤 상황에서 어떤 동작이 필요한지 쉽게 학습할 수 있습니다. 보상 함수는 물체를 목표 지점까지 이동시키는 정도(목표에 가까이 갈수록 큰 보상)와 물체를 떨어뜨리지 않는 것, 그리고 손가락 움직임의 효율성 등을 종합한 형태입니다. 저자들은 PPO 알고리즘으로 이 정책을 약 1.68일간 학습하여 높은 성능의 오라클 정책을 얻었습니다. 오라클 정책은 물체가 어디 있는지 정확히 아는 상태에서 동작하므로, 목표 달성에 매우 효과적이며 물체를 잡는 손가락 gait 패턴도 비교적 안정적으로 발견했습니다.\n2단계 – 촉각 정책 학습: 두 번째 단계에서는 실제 우리가 원하는 촉각 기반 정책을 학습합니다. 이때 1단계의 오라클 정책을 고정된 교사로 삼고, 학생인 촉각 정책은 오라클의 행동을 모방하도록 훈련됩니다. 구체적으로, 학생 정책은 관절 상태 + 시뮬레이터의 촉각 센서 출력(3축 이산 신호)를 입력으로 받으며, Transformer 인코더 구조를 사용해 이 입력을 처리합니다. 인코더의 역할은, 촉각+관절 정보를 받아서 오라클 정책이 이용했던 8차원 잠재 상태 정보(extrinsics 벡터)를 최대한 재현하는 것입니다. 즉, 학생은 촉각을 통해 물체의 상태를 간접적으로 추정하도록 학습됩니다. 논문에 따르면, 이 단계에서는 두 가지 목표로 학습이 진행됩니다: (a) 학생 인코더의 출력이 오라클의 잠재 벡터와 가깝도록 하고, (b) 학생 정책의 행동이 오라클 정책의 행동과 유사하도록 합니다. 이를 통해 학생 정책은 촉각만 가지고도 마치 물체 상태를 알고 있는 것처럼 행동하게 됩니다. 이 두 번째 단계는 지도학습적 손실로 이루어지므로 비교적 빠르게 수렴하며, 약 6.27시간만에 완료되었다고 보고됩니다. 최종적으로 얻어진 정책은 오라클 없이도 촉각과 관절 정보만으로 동작하므로, 이를 그대로 실제 로봇에 이식하여 테스트를 진행합니다.\n\n\n\n\n이러한 2단계 학습 전략은 시뮬레이션 상의 풍부한 정보를 활용해 학습 신호를 강화하는 한편, 최종 정책은 실제 센서 정보만으로 구동되도록 보장하기 때문에 sim-to-real 시 유리합니다. 또한 오라클 정책과 동일한 보상 함수를 사용하므로, 학생 정책도 물체 이동과 손가락 gait 두 측면을 모두 고려한 동작을 하도록 유도됩니다. 요약하면, 오라클은 “이상적인 플레이”를 보여주는 역할, 학생은 그 플레이를 촉각 감으로 보고 배우는 역할을 수행한다고 이해할 수 있습니다."
  },
  {
    "objectID": "posts/paper/2025-07-24-inhand-trans.html#실험-결과-및-해석",
    "href": "posts/paper/2025-07-24-inhand-trans.html#실험-결과-및-해석",
    "title": "📃In-Hand Translation Learning 리뷰",
    "section": "2.4 실험 결과 및 해석",
    "text": "2.4 실험 결과 및 해석\n실험 성능 비교:\n\n\n\n\n왼쪽 그래프는 손바닥 기울기(경사) 각도에 따른 정책들의 평균 이동 거리(cm)와 이동 속도(cm/s)를 보여줍니다. 보라색 선은 3축 촉각 정책(S3-Axis), 주황색 선은 촉각 없는 정책(Proprio-Only)입니다. 경사가 증가할수록 두 정책 모두 성능이 떨어지지만, S3-Axis 정책이 모든 각도에서 Proprio-Only보다 일관되게 더 나은 성능을 보입니다. 0도 (평지) 환경에서는 S3-Axis가 Proprio-Only 대비 약 38% 더 긴 이동거리(+2cm)를 달성했고, 이동 속도도 94% 더 빠르게(+0.16cm/s) 물체를 움직였습니다. 또한 S3-Axis는 시행 간 성능 변동성(표준편차)도 더 작아서, 더 안정적인 조작을 함축했습니다. 경사 각도가 커지면(5도, 10도, 15도) 두 정책 모두 성능이 떨어졌지만, S3-Axis가 항상 Proprio-Only보다 높은 거리와 속도를 유지했습니다. 이는 촉각 피드백 덕분에 손이 기울어 물체가 미끄러지려는 상황에서도 신속히 대응하여 물체를 붙잡고 목표 방향으로 밀어내는 손가락 조정 능력이 향상되었음을 시사합니다.\n오른쪽의 막대 그래프들은 여러 정책들을 비교한 결과로, 평균 성공률, 평균 이동 거리, 평균 속도를 나타냅니다. 보라색 계열이 3축 촉각 정책들(S3-Axis 진한 보라, U3-Axis 옅은 보라), 분홍은 전단만(Signed Shear), 빨강은 법선만(Normal), 주황은 Proprio-Only입니다. 성공률에서 S3-Axis 정책은 약 93%로 가장 높았고, 이는 Proprio-Only의 성공률을 크게 상회했습니다. 이동 거리 역시 S3-Axis가 평균적으로 가장 길었으며(두 번째 좋은 정책보다 50% 이상 더 이동), 이동 속도는 U3-Axis 정책이 약간 더 빠르긴 했지만 S3-Axis도 상위권을 유지했습니다. 한눈에 보면, 3축 촉각을 모두 활용하는 두 정책(S3, U3)이 단일 촉각 종류만 쓰는 경우나 촉각 없는 경우보다 전반적으로 우수함을 알 수 있습니다. 이는 법선+전단 촉각정보의 결합이 시너지를 내어, 물체 이동 작업에서 향상된 피드백 제어를 가능케 한다는 연구의 주장을 뒷받침합니다.\n\n일반화 성능: 새로운 물체(OOD)들에 대한 성능을 자세히 보면, 상황별로 촉각 정책들의 흥미로운 특징이 관찰됩니다:\n\n드라이버 (복잡한 표면, 0도 경사): 드라이버는 표면에 홈과 돌기가 있어 손바닥에 불연속적인 접촉을 만드는 물체입니다. 이 경우 모든 정책이 어느 정도 작업을 수행할 수 있었지만, U3-Axis 정책이 가장 좋은 성능을 보였습니다. S3-Axis와 큰 차이는 아니지만, U3-Axis가 두 번째로 좋은 정책보다도 이동 거리 6.3% 길게, 속도 32% 빠르게 목표에 도달했습니다. 전단 방향 정보를 무시하고 크기 변화에만 반응하는 U3-Axis가 오히려 유리했던 것은, 드라이버처럼 접촉 지점이 여러 개이고 복잡할 때 전단 방향 신호가 잡음이 많아질 수 있기 때문으로 해석됩니다. 이때는 차라리 “미끄러짐 발생 여부”만 보는 편이 더 안정적으로 물체 움직임을 감지한 것으로 보입니다.\n망치 (무게중심 치우침, 15~20도 경사): 망치는 머리 부분에 무게가 집중되어 있고 손잡이는 직사각형 단면이라, 손바닥에 닿는 면적이 훈련때의 원기둥보다 넓습니다. 게다가 손을 기울이면 무거운 망치머리가 한쪽으로 쏠려 강한 회전 토크가 발생하기 때문에, 이 조합은 가장 어려운 시나리오 중 하나였습니다. 결과는 극적이어서, 15도 경사까지는 대부분의 정책이 어떻게든 임무를 수행했지만 20도 경사에서는 S3-Axis를 제외한 거의 모든 정책이 실패했습니다. 특히 U3-Axis 정책은 아예 물체를 전혀 옮기지 못하고 실패했는데, 이는 전단 방향 정보를 버림으로써 망치가 어느 쪽으로 기울어지는지 감지하지 못한 것으로 추측됩니다. 반면 S3-Axis 정책과 (참고로 비교군인) Signed Shear Only 정책만이 간신히 망치를 붙잡고 이동을 수행할 수 있었습니다. 그 중에서도 S3-Axis 정책은 가장 뛰어나서, 다른 방법 대비 약 149% 더 긴 이동 거리를 기록했고 성공률도 20%p 이상 높았습니다. 이 결과는 전단력의 방향성 정보가 물체의 난류한 동적 거동에 대응하는 데 필수적임을 보여줍니다. 무게중심이 틀어진 물체가 미끄러질 때, 그 방향을 알아야 손가락들로 적절히 버티거나 밀어줄 수 있기 때문입니다.\n물병 (유동 질량중심, 0~10도 경사): 물이 반쯤 든 물병은 이동 중에 내용물이 움직여 무게중심이 바뀌는 특성이 있습니다. 초기에는 물이 아래에 있다가, 손바닥을 가로질러 이동하는 동안 물이 흔들리며 점차 중앙으로, 끝으로 옮겨집니다. 이러한 상황에서도 3축 촉각 정책들은 비교적 잘 적응했는데, U3-Axis 정책이 가장 우수한 이동 속도를 보였습니다. U3-Axis는 다음으로 좋은 정책보다 속도 153% 빠르게 물체를 움직일 수 있었고, 이동 거리도 약간(12.5%) 길었습니다. S3-Axis도 실패하지는 않았지만 속도 면에서 U3보다 낮았는데, 이것은 내용물 이동에 따른 센서 신호 변화가 S3에 약간 혼선을 준 가능성이 있습니다. U3는 방향을 무시하고 변화의 크기만 보니, 물 sloshing에도 민감하게 반응하여 빠르게 조절한 것으로 보입니다. 비록 U3가 일부 시나리오에서 두각을 나타냈지만, 종합적으로 볼 때 S3-Axis 정책이 모든 실험을 통틀어 가장 안정적이고 높은 평균 성능을 냈습니다. 특히 S3-Axis는 어떤 환경에서도 완전히 실패하지 않았지만, U3-Axis는 앞서 본 망치 20도 등 특정 조건에서 치명적인 실패를 겪었습니다.\n\n내부 행동 분석: 흥미로운 것은 촉각 정책이 어떻게 다르게 동작하는가에 대한 분석입니다. 저자들은 정책 신경망 내부의 잠재 상태 표현(extrinsics vector)을 t-SNE로 저차원 시각화한 결과, 정책이 실패할 때 내부 표현들도 일관성을 잃고 흩어지는 경향을 발견했습니다. 예컨대 U3-Axis가 망치 20도에서 실패한 실험들의 잠재 표현은 군집을 이루지 못하고 널리 퍼져 있었는데, 이는 정책이 상황을 제대로 파악하지 못하고 불안정하게 행동했음을 시사합니다. 반대로 S3-Axis 정책은 대부분의 실험에서 잠재 표현 공간이 밀집된 하나의 군집을 이루었는데, 이는 다양한 조건에서도 정책이 일관된 전략을 가지고 대응했음을 보여줍니다.\n또한 손가락 gait 패턴에 대해서도 정량적 비교를 했습니다. 각 정책의 실험 로그에서 손가락 관절 궤적을 분석한 결과, 촉각을 사용하는 정책들이 촉각이 없는 정책보다 평균적으로 더 다양한 관절 상태를 탐색함이 드러났습니다. 이는 촉각 피드백이 있을 때 로봇 손이 더 적극적으로 손가락들을 재배치하고 움직이는 경향을 보인다는 의미입니다. 예를 들어 S3-Axis 정책은 Proprio-Only에 비해 손가락 움직임의 위상 공간에서 35% 더 넓은 범위를 탐색했다고 합니다. 쉽게 말해, 촉각이 있다 보니 손가락들을 이리저리 더 많이 써보면서 최적의 움직임을 찾아가는 것입니다. 이러한 탐색적 거동은 특히 새로운 물체나 경사 조건에서 유연한 대처를 가능하게 해 주며, 결과적으로 더 높은 성공률로 이어졌습니다. 반면 촉각이 없는 정책은 제한된 정보로 인해 손가락 움직임 패턴이 반복적이고 경직되어, OOD 상황에서 적응력이 떨어졌음을 알 수 있습니다."
  },
  {
    "objectID": "posts/paper/2025-07-24-inhand-trans.html#한계점-및-논의",
    "href": "posts/paper/2025-07-24-inhand-trans.html#한계점-및-논의",
    "title": "📃In-Hand Translation Learning 리뷰",
    "section": "2.5 한계점 및 논의",
    "text": "2.5 한계점 및 논의\n이 연구는 촉각 센싱을 강화학습에 통합하여 인핸드 조작의 새로운 가능성을 열었지만, 몇 가지 한계점도 존재합니다:\n\n시각 정보 부재: 본 정책은 오로지 촉각과 고유감각 정보만으로 동작합니다. 이는 목표 위치가 미리 손안의 좌표로 정의된 평행이동 작업이기에 가능했습니다. 그러나 일반적인 조작 작업에서는 시각(vision)이 필요할 수밖에 없습니다. 예를 들어 물체를 정확한 위치나 자세로 옮기는 목표가 주어질 때, 카메라로 목표를 인지하고 조작을 미세조정해야 합니다. 저자들도 해당 정책을 보다 정밀한 작업에 적용하려면 시각 정보를 통합하는 것이 필수적임을 한계이자 향후 과제로 언급했습니다.\n정책의 고정 및 온라인 학습 부재: 시뮬레이션에서 학습된 정책을 실세계에 그대로 고정하여 적용(zero-shot)한 점이 본 연구의 특징입니다. 하지만 이는 동시에 한계이기도 합니다. 실제 실험 중에 정책이 보완되어야 할 부분이 보여도, 온라인으로 추가 학습이나 파라미터 수정이 불가능했습니다. 현실 환경에서 추가 학습이 된다면 더 나은 성능을 낼 여지가 있지만, 본 연구에서는 안전성과 일관성 때문에 정책을 고정한 채 실험했습니다. 저자들은 실제 tactile 피드백을 이용한 파인튜닝이나 온-라인 학습으로의 확장을 미래의 연구방향으로 제시했습니다.\n이산 촉각 신호의 한계: 촉각 센서 모델은 효율을 위해 신호를 이진화(또는 3단계화)했습니다. 이로 인해 세밀한 힘의 크기 변화는 정보를 잃게 됩니다. 예컨데 전단력이 약하게 작용하는지 매우 강하게 작용하는지는 (임계값만 넘으면) 동일하게 취급됩니다. 이러한 단순화가 정책 학습에는 오히려 도움이 되었지만, 궁극적으로 연속적인 촉각 신호를 제대로 활용하는 것이 이상적입니다. 실제 ReSkin 센서도 아날로그 출력을 가지고 있으므로, 장기적으로는 연속 값 접촉 힘을 정확히 모사하는 시뮬레이터로 개선할 필요가 있습니다. 저자들 역시 연속 신호로의 시뮬레이션 개선이 성능 향상에 기여할 수 있다고 보고, 이를 향후 연구로 제안했습니다.\n테스트 범위: 본 연구는 손바닥 평행이동이라는 단일 유형의 작업에 집중했습니다. 따라서 회전이나 복합 조작 등 다른 인핸드 조작 스킬에 바로 적용될지는 미지수입니다. 또한 OOD 객체들도 망치, 드라이버, 물병 정도였으며 형상/크기 분포가 한정되어 있습니다. 물론 이미 다양한 요소를 실험하긴 했지만, 예를 들어 더 큰 물체나 표면 재질이 다른 물체, 혹은 손가락에 닿는 경우 등에서는 추가 검증이 필요합니다. 손바닥에만 촉각 센서가 있다는 제한도 있습니다. 물체가 손가락 끝쪽으로 치우치면 촉각 정보가 줄어들어 정책 성능이 저하될 수 있습니다. 실제로 저자들도 실험 시작 전 항상 물체가 손바닥에 닿도록 안정된 파지를 준비하여 초기화했는데, 이는 촉각 입력이 충분히 주어지는 조건에서만 정책이 작동함을 의미합니다. 향후에는 손가락 측면이나 끝부분에도 촉각 센서를 달거나, 초기 파지에 의존하지 않고도 물체를 다룰 수 있도록 해야 할 것입니다.\n하드웨어 제약: 앞서 언급한 대로, 실제 로봇 핸드의 마찰/토크 문제로 추가 장치(필름, 테이프)를 붙이는 등 환경을 인위적으로 조정해야 했습니다. 이는 연구의 핵심은 아니지만, 현재 로봇 손의 기계적 한계를 보여주는 대목입니다. 더욱 강력한 구동기나 정교한 마찰 제어가 가능했다면, 이러한 조치는 불필요하고 더 다양한 환경을 다뤘을지도 모릅니다. 따라서 로봇 플랫폼의 개선 역시 이러한 촉각 조작 연구의 진전을 위해 병행되어야 할 과제입니다.\n\n요약하면, 본 연구는 촉각 기반 강화학습의 가능성을 증명했으나, 시각과의 결합, 온라인 적응 학습, 연속센서 모델링, 다양한 작업 확장, 실제 로봇 성능 향상 등 앞으로 해결해야 할 문제들이 남아 있습니다. 이는 동시에 이 분야의 흥미로운 열린 문제들이기도 합니다."
  },
  {
    "objectID": "posts/paper/2025-07-24-inhand-trans.html#연구-의의-및-향후-방향",
    "href": "posts/paper/2025-07-24-inhand-trans.html#연구-의의-및-향후-방향",
    "title": "📃In-Hand Translation Learning 리뷰",
    "section": "2.6 연구 의의 및 향후 방향",
    "text": "2.6 연구 의의 및 향후 방향\n이 논문은 로보틱스에서 오랫동안 도전적이었던 손안 조작(in-hand manipulation) 분야에 촉각을 적극 활용한 성공 사례를 제시했습니다. 특히 전단력 센싱을 포함한 3축 촉각 피부를 시뮬레이션과 실제에 적용함으로써, 촉각 피드백이 다섯 손가락 로봇 손의 섬세한 동작을 향상시킬 수 있음을 명확히 보여주었습니다. 기존에는 촉각 정보를 단순 접촉 여부로만 쓰거나, 비전 센싱에 의존하는 경우가 많았지만, 이 연구는 “보이지 않아도 만져서 조작할 수 있다”는 것을 멋지게 증명한 것입니다. 특히 학습된 정책이 한번도 본 적 없는 물체나 상황에서도 높은 성공률로 적응한 점은, 향후 범용 로봇 손 개발에 중요한 통찰을 줍니다. 촉각이 있으면 로봇이 물체의 미끄러짐이나 무게 변화를 즉각 감지하고 대응하여, 비전으로는 한참 늦을 피드백을 빠르게 반영할 수 있다는 것을 입증했습니다.\n또한 시뮬레이션 분야에서는, 복잡한 촉각 센서를 효율적으로 모델링하여 강화학습 훈련에 사용 가능하게 한 공헌이 큽니다. 고속 시뮬레이션과 현실감의 균형을 맞춘 이 방법은 향후 다른 촉각 센서 (예: GelSight와 같은 비전 기반 촉각)에도 아이디어를 적용할 수 있을 것입니다. 논문 저자들이 밝힌 대로, 이 연구는 “일반적인 손안 조작을 위한 촉각 피드백 활용의 중요한 한 걸음”이며, 앞으로 시각-촉각 멀티모달 통합, 연속 신호 처리, 실시간 학습 등으로 확장해 갈 수 있습니다. 예를 들어, 추후 연구에서는 시각으로 물체의 대략적 정보를 얻고 촉각으로 미세 조정을 학습하는 체계를 만들 수도 있고, 현재 이진화된 촉각 출력을 연속값으로 세분화하여 더 부드러운 제어를 구현할 수도 있을 것입니다.\n끝으로, 본 연구는 모범적인 시뮬레이션-현실 연계 사례로서도 의미가 있습니다. 시뮬레이터에서만 학습한 정책을 전혀 수정 없이 실제 로봇에서 190회나 실행하며, 그 과정에서 얻은 통찰을 분석했습니다. 이는 향후 강화학습 기반 로봇기술 개발에 있어 시뮬레이션 활용의 가능성과 한계를 잘 보여주며, 데이터 효율적인 로봇 학습을 위한 방향성을 제시합니다. 전반적으로, 촉각 피부를 장착한 로봇 손이 어떻게 학습을 통해 환경과 상호작용할 수 있는지를 깊이 있게 탐구한 이 논문의 성과는, 로봇의 감각 지능을 한 단계 끌어올린 것으로 평가할 수 있습니다. 향후 이러한 접근들이 누적되면, 인간처럼 보고 느끼고 조작하는 다재다능한 로봇 손에 한층 가까워질 것으로 기대됩니다.\n참고: 해당 연구의 프로젝트 웹사이트에는 실제 로봇 실험 영상들이 공개되어 있어, 정책이 물체를 어떻게 손가락으로 “걸어 옮기는지” 생생히 확인할 수 있습니다. 촉각 정보를 활용한 로봇의 새로운 능력을 보고 싶다면 한 번 살펴볼 가치가 있을 것입니다."
  },
  {
    "objectID": "posts/storage/2025-08-26-corl-2025.html",
    "href": "posts/storage/2025-08-26-corl-2025.html",
    "title": "🧩CoRL 2025",
    "section": "",
    "text": "“손/촉각/러닝” 중심 코스에 맞춰 정리한 CoRL 2025 Schedule 정리"
  },
  {
    "objectID": "posts/storage/2025-08-26-corl-2025.html#oral-6-humanoid-hardware-핵심-키워드-손촉각real-world",
    "href": "posts/storage/2025-08-26-corl-2025.html#oral-6-humanoid-hardware-핵심-키워드-손촉각real-world",
    "title": "🧩CoRL 2025",
    "section": "Oral 6 — Humanoid & Hardware (핵심 키워드: 손·촉각·Real World)",
    "text": "Oral 6 — Humanoid & Hardware (핵심 키워드: 손·촉각·Real World)\n\nDexUMI: Using Human Hand as the Universal Manipulation Interface for Dexterous Manipulation — arXiv\n\n한줄 핵심: 손 착용 외골격+비전 인페인팅으로 인간 손 동작을 다양한 로봇 핸드로 전이, Real World 평균 86% 성공.\n발표가치 예측: 🔥 Must-see — 범용 핸드 전이/데이터 수집 파이프라인이 실전성 높음. (arXiv)\n\nDexSkin: High-Coverage Conformable Robotic Skin for Learning Contact-Rich Manipulation — Project page\n\n한줄 핵심: 유연한 고밀도 정전용량형 e-skin으로 손가락 전면/배면을 촉각으로 덮어 접촉 풍부 과제의 학습/전달 시연.\n발표가치 예측: 🔥 Must-see — 저가·대면적 촉각 하드웨어의 Real World 학습 적용이 매력적. (arXiv 예정) (DexSkin)"
  },
  {
    "objectID": "posts/storage/2025-08-26-corl-2025.html#oral-3-manipulation-ii-핵심-키워드-소프트핸드촉각표현tactile-생성",
    "href": "posts/storage/2025-08-26-corl-2025.html#oral-3-manipulation-ii-핵심-키워드-소프트핸드촉각표현tactile-생성",
    "title": "🧩CoRL 2025",
    "section": "Oral 3 — Manipulation II (핵심 키워드: 소프트핸드·촉각표현·Tactile 생성)",
    "text": "Oral 3 — Manipulation II (핵심 키워드: 소프트핸드·촉각표현·Tactile 생성)\n\nKineSoft: Learning Proprioceptive Manipulation Policies with Soft Robot Hands — arXiv\n\n한줄 핵심: 소프트핸드 내부 변형/스트레인 기반 고유감각으로 키네스틱 티칭+형상조건 제어 결합한 모사학습 프레임워크.\n발표가치 예측: 👍 High — 소프트핸드 실사용 데모/정확도 향상 근거 명확. (arXiv)\n\nTactile Beyond Pixels: Multisensory Touch Representations for Robot Manipulation — arXiv\n\n한줄 핵심: 이미지·오디오·모션·압력 4모달 촉각표현(Sparsh-X) 사전학습으로 정책 성공률 +63%, 강건성 +90%.\n발표가치 예측: 🔥 Must-see — 멀티모달 촉각표현의 스케일·일반화 근거 제시. (arXiv)\n\nCross-Sensor Touch Generation — (예정/정보 제한)\n\n한줄 핵심: 이기종 촉각센서 간 생성/번역으로 데이터 증강·표현 정합을 겨냥한 작업.\n발표가치 예측: 👀 Watch — 세부 메트릭 공개 여부에 따라 가치 상향 가능. (arXiv)"
  },
  {
    "objectID": "posts/storage/2025-08-26-corl-2025.html#spotlight-5-poster-3-촉각휴머노이드-dexterity-집중",
    "href": "posts/storage/2025-08-26-corl-2025.html#spotlight-5-poster-3-촉각휴머노이드-dexterity-집중",
    "title": "🧩CoRL 2025",
    "section": "Spotlight 5 & Poster 3 (촉각·휴머노이드 Dexterity 집중)",
    "text": "Spotlight 5 & Poster 3 (촉각·휴머노이드 Dexterity 집중)\n\nSelf-supervised perception for tactile skin covered dexterous hands — arXiv\n\n한줄 핵심: Self-supervised Sparsh-skin 인코더로 손 전체의 자기자석식 스킨 신호를 잠재표현화, 성능 +41%/표본효율 개선.\n발표가치 예측: 🔥 Must-see — 전면 스킨 활용 퍼셉션·정책 모두 개선. (arXiv)\n\nSim-to-Real Reinforcement Learning for Vision-Based Dexterous Manipulation on Humanoids — arXiv\n\n한줄 핵심: 휴머노이드 양손 Dexterity에서 실-시뮬 자동튜닝, Reward 설계 일반화, 분할증류로 시연·일반화 확보.\n발표가치 예측: 👍 High — 데모 의존↓, RL 단독으로 접촉풍부 과제 달성. (arXiv)\n\nCrossing the Human-Robot Embodiment Gap with Sim-to-Real RL using One Human Demonstration — arXiv\n\n한줄 핵심: 단 1개 인간 RGB-D 데모에서 객체궤적 Reward+프리-그랩 포즈 초기화로 휴먼-로봇 격차를 RL로 브리지.\n발표가치 예측: 🔥 Must-see — 데이터 비용 혁신·휴먼→로봇 전이 실효성 큼. (arXiv)"
  },
  {
    "objectID": "posts/storage/2025-08-26-corl-2025.html#spotlight-6-poster-3-tactilerewardteaching-파이프라인",
    "href": "posts/storage/2025-08-26-corl-2025.html#spotlight-6-poster-3-tactilerewardteaching-파이프라인",
    "title": "🧩CoRL 2025",
    "section": "Spotlight 6 & Poster 3 (Tactile·Reward·Teaching 파이프라인)",
    "text": "Spotlight 6 & Poster 3 (Tactile·Reward·Teaching 파이프라인)\n\nVT-Refine: Learning Bimanual Assembly with Visuo-Tactile Feedback via Simulation Fine-Tuning — OpenReview\n\n한줄 핵심: 실데모+고충실 Tactile 시뮬+RL 조합으로 정밀 양팔 조립 학습(시연/어블레이션 포함).\n발표가치 예측: 👍 High — 비주-촉각 통합의 정석 사례. (OpenReview)\n\nKineDex: Learning Tactile-Informed Visuomotor Policies via Kinesthetic Teaching for Dexterous Manipulation — arXiv\n\n한줄 핵심: 핸드-오버-핸드 키네스틱 티칭+비주-Tactile 정책+힘제어로 접촉풍부 과제 74.4% 달성.\n발표가치 예측: 👍 High — 포스/촉각 결합 실성능 수치 제시. (arXiv)\n\nText2Touch: Tactile In-Hand Manipulation with LLM-Designed Reward Functions — Project page\n\n한줄 핵심: LLM 기반 Reward 설계를 실제 비전-기반 Tactile 인-핸드 회전에 적용해 Reward공학 비용 절감.\n발표가치 예측: 👀 Watch — 실제 Tactile 하드웨어와 LLM Reward의 접점 확인 가치. (efi robotics)"
  },
  {
    "objectID": "posts/storage/2025-08-26-corl-2025.html#spotlight-4-poster-2",
    "href": "posts/storage/2025-08-26-corl-2025.html#spotlight-4-poster-2",
    "title": "🧩CoRL 2025",
    "section": "Spotlight 4 & Poster 2",
    "text": "Spotlight 4 & Poster 2\n\n이동 시간 여유 시 강추\n\n\nLocoTouch: Learning Dexterous Quadrupedal Transport with Tactile Sensing — arXiv\n\n한줄 핵심: 등면 Tactile 어레이+시뮬-정합으로 무고정 원통물 장거리 운반을 4족 로봇이 제로샷 실전 수행.\n발표가치 예측: 👍 High — 촉각을 이동체(로코모션)에 접목한 참신 응용. (arXiv)"
  },
  {
    "objectID": "posts/storage/2025-08-26-corl-2025.html#이동청취-팁",
    "href": "posts/storage/2025-08-26-corl-2025.html#이동청취-팁",
    "title": "🧩CoRL 2025",
    "section": "✅ 이동/청취 팁",
    "text": "✅ 이동/청취 팁\n\n우선순위 규칙: (1) Oral 6·3 &gt; (2) Spotlight 5·6 &gt; (3) Spotlight 4 (충돌 시).\n포스터 전략: 위 목록을 우선 라우팅하고, 같은 섹션대 인접 포스터(손/촉각 키워드)까지 빠르게 스윕.\n버퍼 확보: 오럴→포스터 이동 전후 최소 10–15분 이동·대화 버퍼 확보(질문/네트워킹 포함).\n자료 회수: arXiv/프로젝트 페이지를 미리 즐겨찾기(모바일)해 부스/데모에서 바로 레퍼런스 공유."
  },
  {
    "objectID": "posts/storage/2025-09-25-corl-workshop.html",
    "href": "posts/storage/2025-09-25-corl-workshop.html",
    "title": "🧩CoRL 2025 Workshop",
    "section": "",
    "text": "Hompage"
  },
  {
    "objectID": "posts/storage/2025-09-25-corl-workshop.html#vitacformer-learning-cross-modal-representation-for-visuo-tactile-dexterous-manipulation",
    "href": "posts/storage/2025-09-25-corl-workshop.html#vitacformer-learning-cross-modal-representation-for-visuo-tactile-dexterous-manipulation",
    "title": "🧩CoRL 2025 Workshop",
    "section": "2.1 ViTacFormer: Learning Cross-Modal Representation for Visuo-Tactile Dexterous Manipulation",
    "text": "2.1 ViTacFormer: Learning Cross-Modal Representation for Visuo-Tactile Dexterous Manipulation\nShort summary: Proposes a cross-modal transformer that fuses vision + tactile using cross-attention and an autoregressive tactile prediction head; training uses a curriculum from ground-truth to predicted tactile inputs to stabilize representation learning for contact-rich manipulation. (OpenReview)\nQuestions:\n\nHow sensitive is performance to the tactile sensor quality/noise distribution used at training time?\nWhich cross-attention design choices (layers, heads) mattered most in ablations?\nCan the model operate when tactile and vision are intermittently unavailable (e.g., occlusion / sensor dropout)? Any experiments?\nDo you freeze visual backbone or fine-tune it jointly — which worked better?\nHow does the learned representation transfer to new tasks/objects not seen in training?\nWhat’s the compute/latency at inference — suitable for real-time control?"
  },
  {
    "objectID": "posts/storage/2025-09-25-corl-workshop.html#way-tu-a-framework-for-tool-selection-and-manipulation-using-waypoint-representations",
    "href": "posts/storage/2025-09-25-corl-workshop.html#way-tu-a-framework-for-tool-selection-and-manipulation-using-waypoint-representations",
    "title": "🧩CoRL 2025 Workshop",
    "section": "2.2 Way-Tu: A Framework for Tool Selection and Manipulation Using Waypoint Representations",
    "text": "2.2 Way-Tu: A Framework for Tool Selection and Manipulation Using Waypoint Representations\nShort summary: Introduces a waypoint-based representation and pipeline for selecting and manipulating tools — combining learned waypoint predictors with motion optimization to perform tool use tasks robustly. (OpenReview)\nQuestions:\n\nHow are waypoints represented (Cartesian, relative frames, keyframes) and why that choice?\nHow robust is tool selection when the perceived affordance is noisy or partially occluded?\nDid you compare direct end-to-end policy vs waypoint + optimizer — tradeoffs in sample efficiency & robustness?\nHow do you handle tool dynamics (e.g., flexible tools) in planning?\nCan the same waypoint representation generalize across different robot embodiments?\nWhat failure cases are common — poor grasp, imprecise waypoint timing, optimizer convergence?"
  },
  {
    "objectID": "posts/storage/2025-09-25-corl-workshop.html#hermes-human-to-robot-embodied-learning-from-multi-source-motion-data-for-mobile-bimanual-dexterous-manipulation",
    "href": "posts/storage/2025-09-25-corl-workshop.html#hermes-human-to-robot-embodied-learning-from-multi-source-motion-data-for-mobile-bimanual-dexterous-manipulation",
    "title": "🧩CoRL 2025 Workshop",
    "section": "2.3 HERMES: Human-to-Robot Embodied Learning from Multi-Source Motion Data for Mobile Bimanual Dexterous Manipulation",
    "text": "2.3 HERMES: Human-to-Robot Embodied Learning from Multi-Source Motion Data for Mobile Bimanual Dexterous Manipulation\nShort summary: HERMES provides a unified RL + sim2real pipeline to convert heterogeneous human motion sources into physically plausible mobile bimanual robot behaviors — includes depth image based sim2real transfer and closed-loop localization for mobile dextrous tasks. (OpenReview)\nQuestions:\n\nHow do you align heterogeneous human motion data (different capture setups) before training?\nWhat components most reduce the sim2real gap (depth transfer, domain randomization, etc.)? Any ablations?\nHow do you integrate navigation and manipulation timing reliably in mobile setups?\nDoes the policy exploit human kinematic priors or learn purely from RL?\nHow sample-efficient is the approach and how much human data is needed?\nAny limits when transferring to different robot hand kinematics / DoF?"
  },
  {
    "objectID": "posts/storage/2025-09-25-corl-workshop.html#scaling-cross-embodiment-world-models-for-dexterous-manipulation",
    "href": "posts/storage/2025-09-25-corl-workshop.html#scaling-cross-embodiment-world-models-for-dexterous-manipulation",
    "title": "🧩CoRL 2025 Workshop",
    "section": "2.4 Scaling Cross-Embodiment World Models for Dexterous Manipulation",
    "text": "2.4 Scaling Cross-Embodiment World Models for Dexterous Manipulation\nShort summary: Proposes particle-based world models that represent both human and robot embodiments as particle sets and define actions as particle displacements — enabling unified world models that scale to multiple embodiments and support cross-embodiment control. (OpenReview)\nQuestions:\n\nHow do you choose particle resolution and object/hand particle assignment for efficiency vs fidelity?\nDoes the particle representation keep crucial contact details for high-precision tasks?\nHow well does policy transfer when the robot and human have very different actuation constraints?\nAny emergent failure modes when scaling to deformable objects?\nHow does the approach compare with kinematic retargeting + robot dynamics modeling?\nWhat are memory/computation requirements for inference on real robots?"
  },
  {
    "objectID": "posts/storage/2025-09-25-corl-workshop.html#dexumi-using-human-hand-as-the-universal-manipulation-interface-for-dexterous-manipulation",
    "href": "posts/storage/2025-09-25-corl-workshop.html#dexumi-using-human-hand-as-the-universal-manipulation-interface-for-dexterous-manipulation",
    "title": "🧩CoRL 2025 Workshop",
    "section": "2.5 DexUMI: Using Human Hand as the Universal Manipulation Interface for Dexterous Manipulation",
    "text": "2.5 DexUMI: Using Human Hand as the Universal Manipulation Interface for Dexterous Manipulation\nShort summary: DexUMI is a hardware+software pipeline using the human hand as an interface (via wearable exoskeleton + software retargeting/inpainting) to collect dexterous demonstrations and transfer them to different robot hands with good real-world success. (OpenReview)\nQuestions:\n\nWhat kinematic limits of the exoskeleton limit the range of demonstrable motions?\nHow do you handle embodiment gaps for very different robot hands (finger count, joint limits)?\nWhat are privacy / safety considerations for wearables during long teleop sessions?\nHow much post-processing (retargeting correction) is required before policy training?\nIs inpainting of the human hand in video robust to occlusions / lighting?\nHow does performance degrade when switching to an unseen robot hand type?"
  },
  {
    "objectID": "posts/storage/2025-09-25-corl-workshop.html#mimic-one-a-scalable-model-recipe-for-general-purpose-robot-dexterity",
    "href": "posts/storage/2025-09-25-corl-workshop.html#mimic-one-a-scalable-model-recipe-for-general-purpose-robot-dexterity",
    "title": "🧩CoRL 2025 Workshop",
    "section": "2.6 mimic-one: a Scalable Model Recipe for General-Purpose Robot Dexterity",
    "text": "2.6 mimic-one: a Scalable Model Recipe for General-Purpose Robot Dexterity\nShort summary: A practical recipe combining a new 16-DoF tendon-driven hand, curated teleoperation data (with self-correction), and a large generative policy (diffusion) to achieve robust, real-world dexterous control and emergent self-correction behaviors. (OpenReview)\nQuestions:\n\nWhich element of the recipe (hardware, data protocol, model) contributes most to out-of-distribution success?\nHow is “self-correction” measured and how do you encourage it in training?\nWhat are the tradeoffs in using diffusion models vs autoregressive controllers for high-frequency control?\nHow expensive is data collection and what teleop interfaces were most effective?\nAny examples where the model fails to self-correct or produces unsafe motions?\nHow reproducible is the hardware design and codebase for other labs?"
  },
  {
    "objectID": "posts/storage/2025-09-25-corl-workshop.html#latent-action-diffusion-for-cross-embodiment-manipulation",
    "href": "posts/storage/2025-09-25-corl-workshop.html#latent-action-diffusion-for-cross-embodiment-manipulation",
    "title": "🧩CoRL 2025 Workshop",
    "section": "2.7 Latent Action Diffusion for Cross-Embodiment Manipulation",
    "text": "2.7 Latent Action Diffusion for Cross-Embodiment Manipulation\nShort summary: Learns a contrastive latent action space and uses diffusion modeling in that latent space to produce cross-embodiment manipulation policies that can imitate and transfer between different hand embodiments. (OpenReview)\nQuestions:\n\nHow is the latent action space structured and what prevents mode collapse?\nHow much action retargeting is needed when moving between embodiments?\nHow sample-efficient is diffusion in latent action space compared to direct action diffusion?\nAre there latency constraints for diffusion sampling in closed-loop control?\nHow do you evaluate safety / constraint satisfaction when sampling actions in new embodiments?\nDid you compare with non-diffusion generative models (VAE, normalizing flows)?"
  },
  {
    "objectID": "posts/storage/2025-09-25-corl-workshop.html#vision-free-object-6d-pose-estimation-for-in-hand-manipulation-via-multi-modal-haptic-attention",
    "href": "posts/storage/2025-09-25-corl-workshop.html#vision-free-object-6d-pose-estimation-for-in-hand-manipulation-via-multi-modal-haptic-attention",
    "title": "🧩CoRL 2025 Workshop",
    "section": "2.8 Vision-Free Object 6D Pose Estimation for In-Hand Manipulation via Multi-Modal Haptic Attention",
    "text": "2.8 Vision-Free Object 6D Pose Estimation for In-Hand Manipulation via Multi-Modal Haptic Attention\nShort summary: Presents a vision-free haptic attention estimator that fuses kinesthetic, contact, and proprioceptive signals and their temporal dynamics to estimate in-hand object 6D pose — demonstrated to support reliable reorientation without vision. (OpenReview)\nQuestions:\n\nWhat temporal window / filtering is required for robust haptic pose estimates?\nHow sensitive is estimation to slippage and changing contact modes?\nWhat’s the runtime and can it be used in closed-loop control at manipulation frequencies?\nHow does accuracy compare to vision-based 6D pose estimators under occlusion?\nCan the haptic model generalize to new object shapes or materials?\nHow do you handle ambiguous haptic signals that map to multiple pose hypotheses?"
  },
  {
    "objectID": "posts/storage/2025-09-25-corl-workshop.html#dexskin-high-coverage-conformable-robotic-skin-for-learning-contact-rich-manipulation",
    "href": "posts/storage/2025-09-25-corl-workshop.html#dexskin-high-coverage-conformable-robotic-skin-for-learning-contact-rich-manipulation",
    "title": "🧩CoRL 2025 Workshop",
    "section": "2.9 DexSkin: High-Coverage Conformable Robotic Skin for Learning Contact-Rich Manipulation",
    "text": "2.9 DexSkin: High-Coverage Conformable Robotic Skin for Learning Contact-Rich Manipulation\nShort summary: Introduces DexSkin, a soft, conformable capacitive electronic skin that provides dense, localizable tactile sensing across complex finger geometries; demonstrates its use for learning contact-rich manipulation on gripper fingers. (OpenReview)\nQuestions:\n\nWhat is the spatial and force resolution of the DexSkin sensors and how are they calibrated?\nHow robust is DexSkin to wear, contamination, and repeated contact cycles?\nDoes the skin change fingertip geometry or add compliance that affects grasp dynamics?\nHow easy is integration across different robot hand designs (curved surfaces, joints)?\nWhich downstream learning tasks showed the largest improvement using DexSkin?\nIs latency / sampling rate sufficient for high-bandwidth tactile control?"
  },
  {
    "objectID": "posts/storage/2025-09-25-corl-workshop.html#zero-shot-sim2real-transfer-for-magnet-based-tactile-sensor-on-insertion-tasks",
    "href": "posts/storage/2025-09-25-corl-workshop.html#zero-shot-sim2real-transfer-for-magnet-based-tactile-sensor-on-insertion-tasks",
    "title": "🧩CoRL 2025 Workshop",
    "section": "2.10 Zero-shot Sim2Real Transfer for Magnet-Based Tactile Sensor on Insertion Tasks",
    "text": "2.10 Zero-shot Sim2Real Transfer for Magnet-Based Tactile Sensor on Insertion Tasks\nShort summary: Proposes a technique to sim2real transfer magnet-based tactile sensing for insertion tasks with zero real training — likely via physics-aware simulation, sensor modeling and domain randomization to generalize to real sensors. (OpenReview)\nQuestions:\n\nWhat aspects of the tactile sensor model were most critical for zero-shot transfer?\nHow is magnetic field noise and manufacturing variation handled in simulation?\nDo you observe any failure modes on unusual object geometries or adhesives?\nHow does the method generalize to non-insertion contact tasks?\nWhat metrics and baselines did you compare for zero-shot success?\nWould small amounts of real fine-tuning drastically improve performance?"
  },
  {
    "objectID": "posts/storage/2025-09-25-corl-workshop.html#equicontact-a-hierarchical-se3-vision-to-force-equivariant-policy-for-spatially-generalizable-contact-rich-tasks",
    "href": "posts/storage/2025-09-25-corl-workshop.html#equicontact-a-hierarchical-se3-vision-to-force-equivariant-policy-for-spatially-generalizable-contact-rich-tasks",
    "title": "🧩CoRL 2025 Workshop",
    "section": "2.11 EquiContact: A Hierarchical SE(3) Vision-to-Force Equivariant Policy for Spatially Generalizable Contact-Rich Tasks",
    "text": "2.11 EquiContact: A Hierarchical SE(3) Vision-to-Force Equivariant Policy for Spatially Generalizable Contact-Rich Tasks\nShort summary: Presents a hierarchical policy architecture that enforces SE(3) equivariance (spatial symmetries) and maps vision to force/interaction behaviors — designed to generalize spatially (e.g., peg-in-hole) from few demonstrations. (OpenReview)\nQuestions:\n\nWhich parts are enforced analytically equivariant, and which are learned?\nHow does equivariance affect sample efficiency and generalization empirically?\nDoes equivariance hurt expressivity for asymmetric tasks?\nHow do you integrate force control at the low level with vision-level equivariant policies?\nAny limits observed when task geometry or object frames change drastically?\nHow sensitive to calibration and coordinate frame misalignments?"
  },
  {
    "objectID": "posts/storage/2025-09-25-corl-workshop.html#tacdexgrasp-compliant-and-robust-dexterous-grasping-with-qp-and-tactile-feedback",
    "href": "posts/storage/2025-09-25-corl-workshop.html#tacdexgrasp-compliant-and-robust-dexterous-grasping-with-qp-and-tactile-feedback",
    "title": "🧩CoRL 2025 Workshop",
    "section": "2.12 TacDexGrasp: Compliant and Robust Dexterous Grasping with QP and Tactile Feedback",
    "text": "2.12 TacDexGrasp: Compliant and Robust Dexterous Grasping with QP and Tactile Feedback\nShort summary: Uses tactile feedback with a quadratic programming (QP) controller to distribute contact forces and prevent rotational/translational slip for multi-fingered hands — a compliant tactile control approach without explicit torque models. (OpenReview)\nQuestions:\n\nHow are tactile signals mapped into QP constraints/objective — linearization choices?\nHow fast is the QP solved and is it real-time at control loop rates?\nHow robust is the method to unexpected external disturbances (bumps, pushes)?\nHow do you estimate friction coefficients or do you avoid explicit friction estimates?\nHow do you switch between manipulation vs hold/grasp modes?\nAny stability guarantees under contact switching?"
  },
  {
    "objectID": "posts/storage/2025-09-25-corl-workshop.html#fungrasp-functional-grasping-for-diverse-dexterous-hands",
    "href": "posts/storage/2025-09-25-corl-workshop.html#fungrasp-functional-grasping-for-diverse-dexterous-hands",
    "title": "🧩CoRL 2025 Workshop",
    "section": "2.13 FunGrasp: Functional Grasping for Diverse Dexterous Hands",
    "text": "2.13 FunGrasp: Functional Grasping for Diverse Dexterous Hands\nShort summary: FunGrasp focuses on task-oriented / functional grasping (e.g., grasping scissors by holes) by retargeting single RGBD human functional grasp demonstrations to different robot hands and training RL policies with sim-to-real techniques and privileged information. (OpenReview)\nQuestions:\n\nHow do you define & evaluate “functional correctness” vs geometric/grasp metrics?\nHow robust is one-shot transfer from a single RGBD human image to unseen objects?\nWhat retargeting errors are typical and how are they corrected during policy training?\nWhich sim2real tricks mattered most for real deployment?\nDoes the method work for safety-critical tools (blades, needles)? Any constraints?\nHow is the dataset of human functional grasps curated / annotated?"
  },
  {
    "objectID": "posts/storage/2025-09-25-corl-workshop.html#suction-leap-hand-suction-cups-on-a-multi-fingered-hand-enables-embodied-dexterity-and-in-hand-teleoperation",
    "href": "posts/storage/2025-09-25-corl-workshop.html#suction-leap-hand-suction-cups-on-a-multi-fingered-hand-enables-embodied-dexterity-and-in-hand-teleoperation",
    "title": "🧩CoRL 2025 Workshop",
    "section": "2.14 Suction Leap-Hand: Suction Cups on a Multi-fingered Hand Enables Embodied Dexterity and In-Hand Teleoperation",
    "text": "2.14 Suction Leap-Hand: Suction Cups on a Multi-fingered Hand Enables Embodied Dexterity and In-Hand Teleoperation\nShort summary: Describes a practical hardware add-on: mounting suction cups on fingertips/palm of a three-fingered dexterous hand, enabling new manipulation capabilities (adhesive in-hand manipulations) and improved teleoperation for challenging in-hand tasks. (OpenReview)\nQuestions:\n\nHow do suction cups change the control strategy (grasp forces, rolling/sliding actions)?\nWhat materials/porosities of objects break suction assumptions?\nAny tradeoffs in using suction vs frictional finger pads (speed, robustness)?\nHow is suction controlled (binary vs continuous vacuum) and integrated with finger force control?\nHow safe is teleoperation when using suction for delicate tasks?\nWere there tasks humans couldn’t do but suction enabled for robots (or vice versa)?"
  },
  {
    "objectID": "posts/storage/2025-09-25-corl-workshop.html#tactile-memory-with-soft-robot-tactile-retrieval-based-contact-rich-manipulation-with-a-soft-wrist",
    "href": "posts/storage/2025-09-25-corl-workshop.html#tactile-memory-with-soft-robot-tactile-retrieval-based-contact-rich-manipulation-with-a-soft-wrist",
    "title": "🧩CoRL 2025 Workshop",
    "section": "2.15 Tactile Memory with Soft Robot: Tactile Retrieval-based Contact-rich Manipulation with a Soft Wrist",
    "text": "2.15 Tactile Memory with Soft Robot: Tactile Retrieval-based Contact-rich Manipulation with a Soft Wrist\nShort summary: Introduces a tactile retrieval/memory system for contact-rich manipulation leveraging a soft wrist; uses stored tactile patterns to retrieve similar contact episodes to guide control in new situations. (OpenReview)\nQuestions:\n\nHow are tactile episodes indexed and retrieved (embedding, similarity metric)?\nHow does the soft wrist affect contact patterns compared to rigid wrists?\nDoes retrieval generalize across different objects or only similar contacts?\nHow is timeliness handled — retrieving past episodes quickly enough for closed-loop correction?\nHow much memory/storage is required for the tactile database as it scales?\nWhat are failure modes when retrieval returns poor matches?"
  },
  {
    "objectID": "posts/storage/2025-09-25-corl-workshop.html#flash-flow-based-language-annotated-grasp-synthesis-for-dexterous-hands",
    "href": "posts/storage/2025-09-25-corl-workshop.html#flash-flow-based-language-annotated-grasp-synthesis-for-dexterous-hands",
    "title": "🧩CoRL 2025 Workshop",
    "section": "2.16 FLASH: Flow-Based Language-Annotated Grasp Synthesis for Dexterous Hands",
    "text": "2.16 FLASH: Flow-Based Language-Annotated Grasp Synthesis for Dexterous Hands\nShort summary: FLASH is a flow-matching model that generates language-conditioned, physically plausible dexterous grasps conditioned on hand & object point clouds and a text instruction; trained on a curated, language-annotated grasp dataset and shows generalization to novel prompts. (OpenReview)\nQuestions:\n\nHow do you ensure generated grasps are physically admissible (no interpenetration, stable contact forces)?\nHow is language embedded and aligned with geometric affordances? Any failure examples with ambiguous language?\nHow large / diverse is FLASH-drive dataset and what annotation quality controls exist?\nHow does flow-matching compare to diffusion for grasp generation here?\nCan the model propose alternative grasps ranked by task suitability?\nHow does this integrate with downstream control for closing the loop (grasp execution)?"
  },
  {
    "objectID": "posts/project/quad_recovery.html",
    "href": "posts/project/quad_recovery.html",
    "title": "Deep Reinforcement Learning for Fall-Recovery Control on Non-Flat Terrain of Quadruped Robots",
    "section": "",
    "text": "tbd"
  },
  {
    "objectID": "posts/project/kompanion.html",
    "href": "posts/project/kompanion.html",
    "title": "Smart Device for Dog Triaining",
    "section": "",
    "text": "https://github.com/curieuxjy/K-ompanion"
  },
  {
    "objectID": "posts/project/traffic_signal_od.html",
    "href": "posts/project/traffic_signal_od.html",
    "title": "Traffic Sign Detection and Recognition Task",
    "section": "",
    "text": "https://github.com/curieuxjy/traffic_sign_object_detection"
  },
  {
    "objectID": "posts/code/2025-06-09-ros2-joint-state-trajectory.html",
    "href": "posts/code/2025-06-09-ros2-joint-state-trajectory.html",
    "title": "👩‍💻ROS2 JointState vs. JointTrajectory",
    "section": "",
    "text": "ROS에서 sensor_msgs/JointState와 trajectory_msgs/JointTrajectory는 목적과 사용 시점이 뚜렷하게 다릅니다.\n📌 비교 요약"
  },
  {
    "objectID": "posts/code/2025-06-09-ros2-joint-state-trajectory.html#상태-피드백용-jointstate",
    "href": "posts/code/2025-06-09-ros2-joint-state-trajectory.html#상태-피드백용-jointstate",
    "title": "👩‍💻ROS2 JointState vs. JointTrajectory",
    "section": "1.1 1. 상태 피드백용 – JointState",
    "text": "1.1 1. 상태 피드백용 – JointState\n\n하드웨어나 Gazebo 플러그인이 조인트 센서 데이터를 실시간으로 발행\n퍼블리셔 예: joint_states 토픽\n소비처: robot_state_publisher/JointStatePublisher, TF 시스템, Rviz, 모니터링 툴 등"
  },
  {
    "objectID": "posts/code/2025-06-09-ros2-joint-state-trajectory.html#궤적-명령용-jointtrajectory",
    "href": "posts/code/2025-06-09-ros2-joint-state-trajectory.html#궤적-명령용-jointtrajectory",
    "title": "👩‍💻ROS2 JointState vs. JointTrajectory",
    "section": "1.2 2. 궤적 명령용 – JointTrajectory",
    "text": "1.2 2. 궤적 명령용 – JointTrajectory\n\nMoveIt! 또는 다른 경로 생성기가 계획된 조인트 궤적을 생성\n메시지 구성 예: joint_names = [“joint1”, “joint2”], points는 여러 스텝으로 구성\n컨트롤러에 전달: FollowJointTrajectory 액션 서버 호출"
  },
  {
    "objectID": "posts/code/2025-06-09-ros2-joint-state-trajectory.html#제어-사이클-내-캡처",
    "href": "posts/code/2025-06-09-ros2-joint-state-trajectory.html#제어-사이클-내-캡처",
    "title": "👩‍💻ROS2 JointState vs. JointTrajectory",
    "section": "1.3 3. 제어 사이클 내 캡처",
    "text": "1.3 3. 제어 사이클 내 캡처\n\n궤적 실행 중 컨트롤러는 JointTrajectory 명령을 받아들이고,\n실제 하드웨어는 현재 상태를 다시 JointState로 발행하여\n피드백 루프 및 모니터링 가능 (e.g. FollowJointTrajectoryFeedback)\n\n❗ 자주 묻는 실수\n\nJointState를 명령용으로 사용하는 것은 바람직하지 않습니다. 내부 상태 보고용 메시지입니다\n반대로, JointTrajectory는 단순 상태 보고용으로는 적합하지 않으며, 시간 기반 waypoint 전달용입니다.\nJointState는 시간 시리즈 데이터를 여러 번 발행할 수 있고,\nJointTrajectory는 단 한 번의 메시지로 전 궤적을 함께 전달합니다"
  },
  {
    "objectID": "posts/code/2024-01-07-quarto-blog.html",
    "href": "posts/code/2024-01-07-quarto-blog.html",
    "title": "👩‍💻Quarto Blog 기본 셋팅",
    "section": "",
    "text": "이번 포스팅은 Github Blog를 만들어 갔던 경험과 그 경험들을 통해 Quarto라는 좋은 툴을 소개하기 위한 글로 준비를 했습니다. 본격적인 Quarto로 속편한 Github Blog를 구축하는 방법을 소개하기 전에 어떻게 Quarto를 알게 되었고 적극적으로 소개하게 되었는지 이야기를 드릴려고 합니다.\n이전에 글또 7기를 시작하는 다짐글에서 잠깐 기술 블로그 방황기에 대해 언급을 한 적이 있었습니다. 하지만 기술 블로그를 작성해보고 싶은 계기부터 잠깐 이야기해보려고 합니다. 학부 전공은 기계공학과인지라 컴퓨터, IT 관련글들을 볼 기회가 많았던 것은 아니지만 어쩌다보니 AI, 인공지능의 매력에 빠져서 공부를 하다보니 책보다는 구글링과 유튜브로 많은 지식들을 배우게 되는 경험들이 자연스럽게 기술 블로그를 작성하는 문화에 빠져들게 되었습니다. 저와 같이 아무것도 모르는 사람들을 위해 친절한 설명들과 단계별 캡쳐 사진들을 따라가다보면 원하는 프로그램을 실행시킬 수 있었고, 때론 내가 막혀있던 점들을 푼 이전의 누군가의 간단히라도 적어둔 팁을 확인하고 도움을 얻을 수 있었기 때문에 잘 키워가는 기술 블로그 하나 열 책 안 부러울 정도로 기술 블로그에 대한 막연한 동경이 있었습니다.\n그러다가 어느 정도 공부를 하다보니 \"나도 한번 내 기술 블로그를 운영해볼까?\"라는 호기가 생겼습니다.\n시작도 하기 전에 (살짝은) 비장했던 마음도 있었고 설렜던 마음도 있었습니다. 아직 어떻게 블로그를 만들지 생각도 없었지만 나도 무언가를 아카이빙하고 그 글들이 다른 많은 사람들에게 도움이 되면 좋지 않을까라고 생각하며 유명해지면 어떨까..?라는 행복회로를 돌리며 다들 어떻게 기술 블로그를 만드는지 찾아보기 시작했습니다. 나름 AI 공부를 하며 코딩을 할 줄 아니까 Markdown까지는 문서를 작성하는 규칙이니 쉽게 적응을 했지만, 웹 개발은 정말 잘 모르는 분야에다가 웹 개발 공부에 시간을 투자하면서 까지 Github 블로그를 만들 수는 없었기에 눈 앞이 깜깜해지는 기분이었습니다.\n물론..! 이전에 Hugo, Notion 개인 도메인, Velog 등등 여러 블로그 구축 방법들도 있었지만 플랫폼마다 커스텀하기가 어려운 점도 있었고 새로운 언어를 새롭게 공부해야 하는 경우들도 있었기 때문에, 저는 최대한 블로그의 내가 쓸 내용 Contents에 집중할 수 있도록 내용이 Compile/Rendering 되는 건 알아서 해주면 좋겠다는 바람이 있었습니다. (왜 Github Blog를 선택했는지는 여러 조건과 개인적 선호를 반영한 내용이 있으니 이전글을 참고해주세요!) 그러던 중에 FastAI에서 딥러닝 기술 블로그를 가장 심플하고 빠르게 작성할 수 있는 FastPages를 만들었다는 걸 알게 되었고 이 FastPages를 활용해서 잘 작성하고 있었습니다. FastPages의 최대 장점은 AI에서 많이 사용하는 Jupyter Notebook 형식의 파일을 그대로 포스팅을 할 수 있는 기능 등을 통해 원래 제가 원했던 점인 정말 Contents에 집중할 수 있는 환경을 만들어주는 Build 프로그램이었기에 정말 마음에 쏙 들었고 그대로 기술 블로그를 만들었습니다.\n그렇게 이제는 더이상 블로그 이사를 안할 줄 알았지만..(블로그 이사는 정말 품이 많이 드는 것 같아요🥲) fastpages repository에 청천벽력과 같은 공지가 붙게 되었습니다.\n아예 Fastpages 서비스를 닫아서 블로그가 아예 안열리는 것은 아니었지만, 블로그 지속성을 위해 마지막 블로그 이사를 Quarto로 하게 되었고 결과적으로는 Fastpages보다 Quarto가 훨씬 사용하기도 편하고 적당한 자유도가 있는 플랫폼이라는 생각이 들어서 오늘 이 포스팅을 통해 많은 분들께 알리고 싶어서 글을 작성하게 되었습니다. 생각보다 많은 분들이 알고 계시지 않은 것 같아 앞으로 주변에 더더욱 열심히 홍보하려고 합니다. Quarto를 중심으로 VSCode(+Copliot), Typora를 사용하면 더욱 쾌적한 블로그를 만들 수 있기 때문에 Quarto를 메인으로 다른 프로그램들은 가볍게 서포트 프로그램이라고 생각해주시면 될 것 같습니다.\n그럼 프로그램들을 하나씩 살펴본 후에 Github repository를 하나 만들어서 블로그(Website)를 하나 만들어보는 실습까지 가보도록 하겠습니다!"
  },
  {
    "objectID": "posts/code/2024-01-07-quarto-blog.html#repository-하나-만들기",
    "href": "posts/code/2024-01-07-quarto-blog.html#repository-하나-만들기",
    "title": "👩‍💻Quarto Blog 기본 셋팅",
    "section": "1. Repository 하나 만들기",
    "text": "1. Repository 하나 만들기\n블로그를 만들 Repository를 Github에 하나 만들어 줍니다. Repository는 어떤 이름을 가지고 있어도 상관없지만 보통 Github에서 개인 블로그 주소를 본인의 nickname.github.io로 만들기 때문에 저는 아래와 같이 블로그를 위한 Repository를 만들었습니다.\n\n\n\n다음으로 해당 Repository의 Settings에 들어가서 블로그의 웹페이지들이 랜더링될 폴더를 /docs로 설정하는 과정을 아래와 같이 설정해줍니다.\n\n\n\n다음으로 해당 repository를 원하는 경로에 clone 해서 블로그를 작성할 폴더를 준비합니다."
  },
  {
    "objectID": "posts/code/2024-01-07-quarto-blog.html#quarto-project-시작",
    "href": "posts/code/2024-01-07-quarto-blog.html#quarto-project-시작",
    "title": "👩‍💻Quarto Blog 기본 셋팅",
    "section": "2. Quarto Project 시작",
    "text": "2. Quarto Project 시작\n이제 VSCode를 열어서 확장 프로그램에서 Quarto를 검색한 후 해당 확장 프로그램을 VSCode에 깔아줍니다.\n\n\n\nVSCode에서 Ctrl+Shift+P 단축키로 명령어 팔레트를 열어서 Qaurto:Create Project를 실행합니다.\n\n\n\n출처: Quarto 공식 홈페이지\n\n\nBlog Porject를 선택합니다. 이때 프로젝트 이름은 아무거나 입력헤도 됩니다. 프로젝트를 생성한 후, 프로젝트 하위에 있는 모든 파일들을 Github repository 이름과 동일한 폴더로 옮겨줍니다.\n\n\n\n출처: Quarto 공식 홈페이지\n\n\n그러면 Github Repository의 이름이 test_blog라고 했을 때 아래와 같이 폴더와 파일들의 구성이 되는 것을 확인할 수 있습니다.\n\n\n\nBlog project 시작"
  },
  {
    "objectID": "posts/code/2024-01-07-quarto-blog.html#default-blog-localhost로-확인",
    "href": "posts/code/2024-01-07-quarto-blog.html#default-blog-localhost로-확인",
    "title": "👩‍💻Quarto Blog 기본 셋팅",
    "section": "3. Default Blog localhost로 확인",
    "text": "3. Default Blog localhost로 확인\n이제 VSCode에서 Blog Repository(test_blog)가 열린 프로젝트 창에서 터미널을 열어서 아래의 명령어를 입력해봅니다.\nquarto preview\n\n\n\n그러면 Default로 생성된 블로그 페이지가 localhost로 열리는 것을 확인할 수 있습니다.\n\n\n\nDefault Blog"
  },
  {
    "objectID": "posts/code/2024-01-07-quarto-blog.html#blog-publish",
    "href": "posts/code/2024-01-07-quarto-blog.html#blog-publish",
    "title": "👩‍💻Quarto Blog 기본 셋팅",
    "section": "Blog Publish",
    "text": "Blog Publish\n이전 단계에서는 localhost로 페이지를 랜더링해서 확인한 것이기 때문에 publish가 되지 않았습니다. quarto에서 프로젝트를 rendering하기 위해서는 publish를 하는 명령어를 실행해야 합니다.\n하지만 그전에!\npage들을 publish할 directory를 /docs로 따로 설정을 해주었기 때문에 해당 폴더에 랜더링된 html 파일들을 만들 수 있도록 아래와 같이 ._quarto.yml 설정파일을 아래와 같이 작성해줍니다.\nproject:\n  type: website\n  output-dir: docs\n\n\n\n위와 같이 render이 잘 완료되었다는 메세지가 확인이 되면 아래의 명령어를 실행하여 publish 합니다.\nquarto render\n마지막으로 Github에 Push를 하게 되면 이제 website를 확인할 수 있습니다!(build되는데 약 2-3분 걸릴 수 있습니다.)\n이번에는 Quarto Engine을 이용해서 간단하게 Blog Website를 올리는 부분을 진행했습니다. 좀 더 본격적으로 조정할 수 있는 옵션들에 대해서는 다음 포스팅에서 살펴볼 예정입니다.\n\nReference\n\nQuarto Homepage\nTypora Homepage"
  },
  {
    "objectID": "posts/code/2020-07-20-import-custom-module.html",
    "href": "posts/code/2020-07-20-import-custom-module.html",
    "title": "👩‍💻Import custom module",
    "section": "",
    "text": "실제 폴더와 파일들은 아래와 같다.\n\n\n\ngym_foo folder에 반드시 __init__.py를 만들어야 한다.\n\n\n\n파일 구조"
  },
  {
    "objectID": "posts/code/2020-07-20-import-custom-module.html#structure",
    "href": "posts/code/2020-07-20-import-custom-module.html#structure",
    "title": "👩‍💻Import custom module",
    "section": "",
    "text": "실제 폴더와 파일들은 아래와 같다.\n\n\n\ngym_foo folder에 반드시 __init__.py를 만들어야 한다.\n\n\n\n파일 구조"
  },
  {
    "objectID": "posts/code/2020-07-20-import-custom-module.html#code",
    "href": "posts/code/2020-07-20-import-custom-module.html#code",
    "title": "👩‍💻Import custom module",
    "section": "Code",
    "text": "Code\n\nutils_foo.py\n\ndef utils_test():\n    print(\"utils_foo\")\n\nprint(\"HERE: utils_foo\")\n\nHERE: utils_foo\n\n\nenv_foo.py\nfrom gym_foo import utils_foo\n\nutils_foo.utils_test()\n\nprint(\"HERE: env_foo\")\n\ndef env_test():\n    print(\"env_foo\") \nmain_foo.py\nfrom gym_foo import utils_foo\nfrom gym_foo import env_foo\n\nutils_foo.utils_test()\nenv_foo.env_test()"
  },
  {
    "objectID": "posts/code/2020-07-20-import-custom-module.html#goal",
    "href": "posts/code/2020-07-20-import-custom-module.html#goal",
    "title": "👩‍💻Import custom module",
    "section": "Goal",
    "text": "Goal\n\n실행 파일: main_foo.py\nimport하는 파일: env_foo.py\nimport하는 파일 내에서(=env_foo.py) import하는 파일: utils_foo.py"
  },
  {
    "objectID": "posts/code/2020-07-20-import-custom-module.html#how",
    "href": "posts/code/2020-07-20-import-custom-module.html#how",
    "title": "👩‍💻Import custom module",
    "section": "How",
    "text": "How\n\nmain_foo.py에서 env_foo.py를 import한다.\nenv_foo.py에서 utils_foo.py를 import 한다.\n이때 env_foo.py에서 import utils_foo로 utils_foo를 불러오면,\n\npython env_foo.py 실행시 잘 작동되지만(같은 위치)\npython main_foo.py 실행에서는 from gym_foo import *코드를 읽을 때 env_foo.py내의 from gym_foo import utils_foo를 불러올 수 없다고 error가 난다.(상위 위치)\n\n$ python main_foo.py \nTraceback (most recent call last):\nFile \"main_foo.py\", line 4, in &lt;module&gt;\n    env_foo.env_test()\nNameError: name 'env_foo' is not defined\nenv_foo.py에서 utils_foo module을 불러올 때, from gym_foo import utils_foo로 불러온다. 상위 위치인 gym_foo를 거쳐서 import해야한다는 뜻이다. 그러면 python main_foo.py 실행시 잘 작동한다.\n$ python main_foo.py \nHERE: utils_foo\nutils_foo\nHERE: env_foo\nutils_foo\nenv_foo\n한 가지 더 주의해야 할 점이 있다. main_foo.py에서 utils_foo와 env_foo를 import 할 때이다.\nfrom gym_foo import * 코드로 utils_foo와 env_foo가 모두 불러와질 것이라고 생각했으나, main_foo.py를 실행했을 때 import 하지 못한다. 따라서 위에 main_foo.py에서 볼 수 있듯이 from gym_foo import utils_foo, from gym_foo import env_foo각각 따로 import 해줘야 한다."
  },
  {
    "objectID": "posts/code/2023-04-23-orbit-existing-scripts.html",
    "href": "posts/code/2023-04-23-orbit-existing-scripts.html",
    "title": "👩‍💻Orbit Existing Scripts",
    "section": "",
    "text": "Orbit은 IsaacLab으로 변경되었습니다.\nOrbit 시리즈의 두번째 포스팅으로 이번에는 Orbit에서 제공하는 scripts를 살펴보며 Orbit으로 어떤 프로그래밍을 할 수 있을지 살펴보겠습니다. 공식 Documents에서 Running existing scripts를 따라가보며 진행될 예정입니다."
  },
  {
    "objectID": "posts/code/2023-04-23-orbit-existing-scripts.html#modules",
    "href": "posts/code/2023-04-23-orbit-existing-scripts.html#modules",
    "title": "👩‍💻Orbit Existing Scripts",
    "section": "2.1 Modules",
    "text": "2.1 Modules\nOrbit이 기반하고 있는 Isaac Sim Simulator을 먼저 키기 위해 SimulationApp class를 이용하여 시뮬레이터 앱을 설정해줍니다. 이때 headless는 시뮬레이터의 GUI를 띄우지 않고 실행하는 옵션을 가리킵니다.\n\"\"\"Launch Isaac Sim Simulator first.\"\"\"\n\nfrom omni.isaac.kit import SimulationApp\n\nimport argparse\n# add argparse arguments\nparser = argparse.ArgumentParser(\"Welcome to Orbit: Omniverse Robotics Environments!\")\nparser.add_argument(\"--headless\", action=\"store_true\", default=False, help=\"Force display off at all times.\")\nargs_cli = parser.parse_args()\n\n# launch omniverse app\nconfig = {\"headless\": args_cli.headless}\nsimulation_app = SimulationApp(config)\n다음으로 import하는 orbit의 core모듈을 살펴보겠습니다.\n\nprim_utils : 현재 USD stage에 prim을 생성하기 위한 모듈. (자세한 prim에 대한 내용은 다음 포스팅에서 다루도록 하겠습니다. 우선 간단하게 시뮬레이터의 오브젝트들을 prim으로 본다고 생각하고 진행하겠습니다.)\nSimulationContext : 타임라인 관련 이벤트를 처리하고(simulator 일시 중지, 재생, 단계적 실행 또는 중지 등), stage를 구성하며(stage 단위나 상/하 방향과 같은 설정), physicsScene prim을 생성합니다(중력 방향 및 크기, 시뮬레이션 시간 간격 크기, 고급 솔버 알고리즘 설정과 같은 물리 시뮬레이션 매개 변수를 제공). 여기서 physicsScene prim은 물리 시뮬레이션을 위한 초기화 및 설정을 담당하는 객체를 말하며, SimulationContext는 이러한 physicsScene prim을 생성하고 물리 시뮬레이션 설정을 관리하며, 타임라인 관련 이벤트 처리와 스테이지 설정을 담당합니다.\nset_camera_view : stage에서 카메라 prim의 위치와 대상을 설정하고, 해당 prim의 경로를 지정합니다. 여기서 카메라 prim은 stage 카메라로 사용될 객체를 의미하며, 위치(location)와 대상(target)은 각각 카메라의 위치와 카메라가 바라보는 대상의 위치를 지정하는 것을 의미합니다. 경로(path)는 스테이지에서 해당 카메라 prim을 가리키는 이름이나 경로를 의미합니다.\n\n이외의 모듈중에서 kit_utils는 시뮬레이터에서 제공하는 ground를 불러오기 위한(create_ground_plane) 모듈이며 로봇의 발 위치 마커를 표시하기 위해 markers 모듈에서 PointMarker, StaticMarker를 불러옵니다.\n다음으로 robot 모듈에서 4족보행로봇들을 위한 configuration들을 불러올 수 있습니다.\n# from core\nimport omni.isaac.core.utils.prims as prim_utils\nfrom omni.isaac.core.simulation_context import SimulationContext\nfrom omni.isaac.core.utils.viewports import set_camera_view\n\nimport omni.isaac.orbit.utils.kit as kit_utils\nfrom omni.isaac.orbit.markers import PointMarker, StaticMarker\n\nfrom omni.isaac.orbit.robots.config.anymal import ANYMAL_B_CFG, ANYMAL_C_CFG\nfrom omni.isaac.orbit.robots.config.unitree import UNITREE_A1_CFG\nfrom omni.isaac.orbit.robots.legged_robot import LeggedRobot"
  },
  {
    "objectID": "posts/code/2023-04-23-orbit-existing-scripts.html#helpers",
    "href": "posts/code/2023-04-23-orbit-existing-scripts.html#helpers",
    "title": "👩‍💻Orbit Existing Scripts",
    "section": "2.2 Helpers",
    "text": "2.2 Helpers\nMain 코드에서 시뮬레이터의 Scene을 구성하기 위해서 Ground-plane, Lights들을 편하게 구성하기 위해서 helper 함수 design_scene()를 만들어줍니다. 앞서 import했던 kit_utils를 이용해서 ground를 불러오고 prim_utils를 이용하여 빛 설정을 해줍니다.\ndef design_scene():\n    \"\"\"Add prims to the scene.\"\"\"\n    # Ground-plane\n    kit_utils.create_ground_plane(\n        \"/World/defaultGroundPlane\",\n        static_friction=0.5,\n        dynamic_friction=0.5,\n        restitution=0.8,\n        improve_patch_friction=True,\n    )\n    # Lights-1\n    prim_utils.create_prim(\n        \"/World/Light/GreySphere\",\n        \"SphereLight\",\n        translation=(4.5, 3.5, 10.0),\n        attributes={\"radius\": 2.5, \"intensity\": 600.0, \"color\": (0.75, 0.75, 0.75)},\n    )\n    # Lights-2\n    prim_utils.create_prim(\n        \"/World/Light/WhiteSphere\",\n        \"SphereLight\",\n        translation=(-4.5, 3.5, 10.0),\n        attributes={\"radius\": 2.5, \"intensity\": 600.0, \"color\": (1.0, 1.0, 1.0)},\n    )"
  },
  {
    "objectID": "posts/code/2023-04-23-orbit-existing-scripts.html#main",
    "href": "posts/code/2023-04-23-orbit-existing-scripts.html#main",
    "title": "👩‍💻Orbit Existing Scripts",
    "section": "2.3 Main",
    "text": "2.3 Main\n메인 코드에서는 먼저 SimulationContext를 이용하여 시뮬레이터의 시간관련 설정 등을 진행합니다. dt가 0.005라는 것은 시간 단위가 second로 시간간격이 0.005초로 설정하는 것을 말합니다. 또한 backend는 이후 포스팅에서도 설명하겠지만 물리 시뮬레이터에서 반환되는 tensor들을 어떤 backend로 캐스팅할 것인지 설정하는 부분입니다. 현재 orbit에서는 torch만 지원하고 있습니다.\n시뮬레이터의 시점이 되는 카메라를 설정하고 각 로봇을 어디에 놓을지 정하는 translation 파라미터와 함께 spawning하고 마지막으로 helper 함수로 만들어주었던 design_scene()을 이용하여 ground와 light를 설정합니다.\n\"\"\"Imports all legged robots supported in Orbit and applies zero actions.\"\"\"\n\n# Load kit helper\nsim = SimulationContext(stage_units_in_meters=1.0, physics_dt=0.005, rendering_dt=0.005, backend=\"torch\")\n# Set main camera\nset_camera_view(eye=[3.5, 3.5, 3.5], target=[0.0, 0.0, 0.0])\n\n# Spawn things into stage\n# -- anymal-b\nrobot_b = LeggedRobot(cfg=ANYMAL_B_CFG)\nrobot_b.spawn(\"/World/Anymal_b/Robot_1\", translation=(0.0, -1.5, 0.65))\nrobot_b.spawn(\"/World/Anymal_b/Robot_2\", translation=(0.0, -0.5, 0.65))\n# -- anymal-c\nrobot_c = LeggedRobot(cfg=ANYMAL_C_CFG)\nrobot_c.spawn(\"/World/Anymal_c/Robot_1\", translation=(1.5, -1.5, 0.65))\nrobot_c.spawn(\"/World/Anymal_c/Robot_2\", translation=(1.5, -0.5, 0.65))\n# -- unitree a1\nrobot_a = LeggedRobot(cfg=UNITREE_A1_CFG)\nrobot_a.spawn(\"/World/Unitree_A1/Robot_1\", translation=(1.5, 0.5, 0.42))\nrobot_a.spawn(\"/World/Unitree_A1/Robot_2\", translation=(1.5, 1.5, 0.42))\n\n# design props\ndesign_scene()\n시뮬레이터를 초기화하는 reset을 먼저 진행해줍니다. 각 로봇의 handle 또한 초기화를 시켜줍니다. 로봇의 각 정보를 담기 위한 buffer도 reset을 해서 본격적인 실행을 준비합니다.\n# Play the simulator\nsim.reset()\n# Acquire handles\n# Initialize handles\nrobot_b.initialize(\"/World/Anymal_b/Robot.*\")\nrobot_c.initialize(\"/World/Anymal_c/Robot.*\")\nrobot_a.initialize(\"/World/Unitree_A1/Robot.*\")\n# Reset states\nrobot_b.reset_buffers()\nrobot_c.reset_buffers()\nrobot_a.reset_buffers()\n4족 보행 로봇은 제어를 할 때 발의 움직임이 매우 중요합니다. 발의 위치에 마커를 위치시켜서 발에 대한 정보를 얻기 위해 marker를 설정해줍니다. 발의 x, y, z 축을 시각화하기 위해서 StaticMarker를 이용하여 설정하고 발이 contact 포인트를 보기 위해 PointMarker를 설정합니다.\n# Debug visualization markers.\n# -- feet markers\nfeet_markers: List[StaticMarker] = list()\nfeet_contact_markers: List[PointMarker] = list()\n# iterate over robots\nfor robot_name in [\"Anymal_b\", \"Anymal_c\", \"Unitree_A1\"]:\n    # foot\n    marker = StaticMarker(f\"/World/Visuals/{robot_name}/feet\", 4 * robot_c.count, scale=(0.1, 0.1, 0.1))\n    feet_markers.append(marker)\n    # contact\n    marker = PointMarker(f\"/World/Visuals/{robot_name}/feet_contact\", 4 * robot_c.count, radius=0.035)\n    feet_contact_markers.append(marker)\n각 로봇은 action을 하게되고 이를 로봇을 제어한다고 볼 수 있습니다. get_default_dof_state()은 로봇의 각 객체에 있는 method로 각 로봇의 standing 자세에 대한 joint position(dof) 정보가 들어있습니다.\n# dummy action\nactions = torch.zeros(robot_a.count, robot_a.num_actions, device=robot_a.device)\n\n# Define simulation stepping\nsim_dt = sim.get_physics_dt()\nsim_time = 0.0\ncount = 0\n# Simulate physics\nwhile simulation_app.is_running():\n    # If simulation is stopped, then exit.\n    if sim.is_stopped():\n        break\n    # If simulation is paused, then skip.\n    if not sim.is_playing():\n        sim.step(render=not args_cli.headless)\n        continue\n    # reset\n    if count % 1000 == 0:\n        # reset counters\n        sim_time = 0.0\n        count = 0\n        # reset dof state\n        for robot in [robot_a, robot_b, robot_c]:\n            dof_pos, dof_vel = robot.get_default_dof_state()\n            robot.set_dof_state(dof_pos, dof_vel)\n            robot.reset_buffers()\n        # reset command\n        actions = torch.zeros(robot_a.count, robot_a.num_actions, device=robot_a.device)\n        print(\"&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Reset!\")\n    # apply actions\n    robot_b.apply_action(actions)\n    robot_c.apply_action(actions)\n    robot_a.apply_action(actions)\n    # perform step\n    sim.step()\n    # update sim-time\n    sim_time += sim_dt\n    count += 1\n시뮬레이터 초기화할 때 reset했던 buffer에 각 로봇의 정보를 담습니다. marker로 설정해두었던 foot_marker와 contact_marker도 각 로봇에서 불러와서 state update를 진행합니다.\n# note: to deal with timeline events such as stopping, we need to check if the simulation is playing\nif sim.is_playing():\n    # update buffers\n    robot_b.update_buffers(sim_dt)\n    robot_c.update_buffers(sim_dt)\n    robot_a.update_buffers(sim_dt)\n    # update marker positions\n    for foot_marker, contact_marker, robot in zip(\n        feet_markers, feet_contact_markers, [robot_b, robot_c, robot_a]\n    ):\n        # feet\n        foot_marker.set_world_poses(\n            robot.data.feet_state_w[..., 0:3].view(-1, 3), robot.data.feet_state_w[..., 3:7].view(-1, 4)\n        )\n        # contact sensors\n        contact_marker.set_world_poses(\n            robot.data.feet_state_w[..., 0:3].view(-1, 3), robot.data.feet_state_w[..., 3:7].view(-1, 4)\n        )\n        contact_marker.set_status(torch.where(robot.data.feet_air_time.view(-1) &gt; 0.0, 1, 2))"
  },
  {
    "objectID": "posts/code/2022-12-14-gpu-status.html",
    "href": "posts/code/2022-12-14-gpu-status.html",
    "title": "👩‍💻Linux GPU 상태 확인하기",
    "section": "",
    "text": "1. nvidia-smi\nwatch -d -n 0.5 nvidia-smi\n\nwatch : 명령어를 주기적으로 실행\n-d : 차이를 보여줌\n-n : 주기적으로 실행할 시간 간격\n\n\n\n2. gpustat\nsudo apt install gpustat\ngpustat -i\noptions\n--color : Force colored output (even when stdout is not a tty)\n--no-color : Suppress colored output\n-u, --show-user : Display username of the process owner\n-c, --show-cmd : Display the process name\n-p, --show-pid : Display PID of the process\n-P, --show-power : Display GPU power usage and/or limit (draw or draw,limit)\n-i, --interval : Run in watch mode (equivalent to watch gpustat) if given. Denotes interval between updates.\n--json : JSON Output (Experimental, #10)\n\n\n3. gpumonitor\n\nGithub mountassir/gmonitor에서 설치방법 확인\n\n$ cd gmonitor\n$ mkdir build\n$ cd build\n$ cmake ..\n$ make\n$ sudo make install\n\\# use default\ngmonitor\n\n\\# Monitor the most recent state only\ngmonitor -d 1\n\n\\# Monitor current and history states for 4 GPUs.\ngmonitor -d 0 -g 0123\n\n\\# Monitor both current and previous states for all GPUs, refresh every 3 seconds.\ngmonitor -d 0 -r 3\n\n\n4. glance\n\n설치 sudo apt-get install -y python-pip; sudo pip install glances[gpu]\n실행 sudo glances\n\n\n\n[+] Jupyter Lab에서 GPU 상태 확인하기\n\nhttps://developer.nvidia.com/blog/gpu-dashboards-in-jupyter-lab/\n\n$ pip install jupyterlab-nvdashboard\n\n# If you are using Jupyter Lab 2 you will also need to run\n$ jupyter labextension install jupyterlab-nvdashboard"
  },
  {
    "objectID": "posts/code/2025-07-11-poe.html",
    "href": "posts/code/2025-07-11-poe.html",
    "title": "👩‍💻스크루-이론 기반 Product-of-Exponentials(PoE) 기구학 Deep-Dive",
    "section": "",
    "text": "Executive Summary PoE 기구학은 관절 스크루 \\xi와 홈 포즈 M만으로 전·역기구학, Jacobian, 동역학을 유기적으로 서술한다. 좌표계 선택에 의존적인 DH 패러다임을 벗어나 C^1 연속·좌표계 불변 파라미터화를 제공하므로 모델 보정·딥러닝·고속 제어 환경에서 뛰어난 수렴 특성을 발휘한다."
  },
  {
    "objectID": "posts/code/2025-07-11-poe.html#왜-poe인가-dh-모델과의-구조적-비교",
    "href": "posts/code/2025-07-11-poe.html#왜-poe인가-dh-모델과의-구조적-비교",
    "title": "👩‍💻스크루-이론 기반 Product-of-Exponentials(PoE) 기구학 Deep-Dive",
    "section": "1  왜 PoE인가? — DH 모델과의 구조적 비교",
    "text": "1  왜 PoE인가? — DH 모델과의 구조적 비교\n\n\n\n\n\n\n\n\n 구분 \nClassic / Modified DH\nPoE (Lie 기반)\n\n\n\n\n파라미터 개수\n링크당 (α, a, d, θ₀) 4개\nRevolute당 6() + M(6) (전역 1개)\n\n\n필요 프레임 수\n링크마다 좌표계 지정\nBase + Tool 2개면 충분\n\n\n파라미터 연속성\n±π 경계에서 불연속\n스크루 (\\mathbb S^2!\\times!\\mathbb R^3), C^1 연속\n\n\n학습·보정 수렴\n비평활 → LM 불안정\nGD·LM 모두 우수 수렴\n\n\n수학적 기반\n유클리드 + ad‑hoc\nLie 군 SE(3) + 스크루 이론\n\n\n\n\n\n\n\n\n\n직관적 예시\n\n\n\n비틀림각 α = ±π 경계를 지나는 2‑링크 평면 로봇을 생각해 보자. DH 방식에서는 α 값이 +π→–π로 순간 점프하며 Jacobian이 불연속이 된다. PoE는 동일 축을 단일 스크루로 표현하므로 경계가 사라진다."
  },
  {
    "objectID": "posts/code/2025-07-11-poe.html#수학-기초-se3스크루지수맵",
    "href": "posts/code/2025-07-11-poe.html#수학-기초-se3스크루지수맵",
    "title": "👩‍💻스크루-이론 기반 Product-of-Exponentials(PoE) 기구학 Deep-Dive",
    "section": "2  수학 기초 — SE(3)·스크루·지수맵",
    "text": "2  수학 기초 — SE(3)·스크루·지수맵\n\n2.1  SE(3) 정의 및 기하적 의미\n\nT = \\begin{bmatrix} R & p \\\\ 0 & 1 \\end{bmatrix},\\qquad R \\in SO(3),\\; p \\in \\mathbb R^3\n\n→ 회전 R과 병진 p를 동시에 나타내는 4×4 변환 행렬.\n\n\n2.2  스크루 벡터 \\xi=[\\omega;v]\n\nRevolute 관절: |\\omega|=1,  v=-\\omega\\times q (축을 지나는 점 q에서 속도가 0이 되도록 v 결정)\nPrismatic 관절: \\omega=0,  v가 이동 축 단위벡터.\n\n스크루는 나사 운동을 일반화한 개념으로, \\omega가 “나사 축”, v가 “나사 피치”에 대응한다.\n\n\n2.3  Rodrigues 지수맵\n\n\\exp(\\widehat\\omega\\,\\theta)=I+\\sin\\theta\\,\\widehat\\omega+(1-\\cos\\theta)\\,\\widehat\\omega^2\n\n병진 항(해석적 적분)\n\nJ(\\theta)=I\\theta+(1-\\cos\\theta)\\widehat\\omega+(\\theta-\\sin\\theta)\\widehat\\omega^2,\\quad p=J(\\theta)\\,v.\n\n\n\\widehat\\omega는 \\omega를 스큐‑대칭 행렬로 삽입한 Lie 대수 \\mathfrak{so}(3) 元."
  },
  {
    "objectID": "posts/code/2025-07-11-poe.html#poe-전방기구학-pinocchio-실무-절차",
    "href": "posts/code/2025-07-11-poe.html#poe-전방기구학-pinocchio-실무-절차",
    "title": "👩‍💻스크루-이론 기반 Product-of-Exponentials(PoE) 기구학 Deep-Dive",
    "section": "3  PoE 전방기구학 — Pinocchio 실무 절차",
    "text": "3  PoE 전방기구학 — Pinocchio 실무 절차\n아래 예시는 Allegro Hand 하나의 손가락(4 DoF)를 대상으로 한다. URDF에서 스크루 행렬 S와 홈 포즈 M를 자동 추출한 뒤, NumPy로 PoE FK를 구현하고 Pinocchio 결과와 검증한다.\n\n3.1  스크루 행렬 S 자동 추출\nimport pinocchio as pin\nimport numpy as np\nfrom pinocchio.robot_wrapper import RobotWrapper\nfrom pathlib import Path\n\n# 1) URDF 로드 ─ 메시 로딩은 생략해 속도 향상\nurdf = Path(\"../allegro/allegro.urdf\").resolve()\nrobot = RobotWrapper.BuildFromURDF(\n    str(urdf), [str(urdf.parent)], geometry_types=()  # geometry 무시\n)\nmodel, data = robot.model, robot.data\n\nS_cols = []  # 스크루 벡터들을 저장할 리스트\nfor jid in range(1, model.njoints):  # 0 = universe(월드)\n    # 2) 로컬 관절축 (URDF에서 정의) → 베이스 좌표계로 변환\n    axis_local = model.joints[jid].axis            # 3‑vector\n    oMi = model.jointPlacements[jid]               # 부모→관절 변환 SE3\n    omega = oMi.rotation @ axis_local              # ω: world frame 단위벡터\n\n    # 3) v = −ω × q (q = 관절축을 지나는 임의 점, 여기서는 관절 위치)\n    q_pt = oMi.translation\n    v = -np.cross(omega, q_pt)\n\n    S_cols.append(np.r_[omega, v])                 # [ω, v] 연결\n\nS = np.stack(S_cols, axis=1)  # shape = (6, n)\nprint(\"Screw matrix shape:\", S.shape)  # (6, 4)\n\n설명: Pinocchio가 제공한 axis와 jointPlacements만으로 베이스 좌표계 기준 스크루를 계산한다. 직선 관절이라면 omega = 0, v는 URDF의 이동축 단위벡터가 된다.\n\n\n\n3.2  홈 포즈 M 추출\nfid = model.getFrameId(\"link_3.0_tip\")          # 손가락 끝 프레임 ID\nM = model.frames[fid].placement.homogeneous      # 4×4 numpy array\n\n주의: 홈 포즈는 *“q=0일 때 엔드 이펙터 자세”*이다. URDF에서 z-up / x-forward 등의 차이에 따라 달라질 수 있으므로, Jacobian 해석이 꼬이면 M 정의부터 의심하자.\n\n\n\n3.3  NumPy PoE FK 함수\nfrom scipy.linalg import expm\n\ndef hat(xi: np.ndarray) -&gt; np.ndarray:\n    \"\"\"6‑벡터 → 4×4 잠재행렬(스크루 대수 元)\"\"\"\n    omega, v = xi[:3], xi[3:]\n    omega_hat = np.array([\n        [0,       -omega[2],  omega[1]],\n        [omega[2], 0,        -omega[0]],\n        [-omega[1], omega[0], 0]\n    ])\n    return np.block([\n        [omega_hat, v.reshape(3, 1)],\n        [np.zeros((1, 3)), 0]\n    ])\n\ndef fk_poe(q: np.ndarray) -&gt; np.ndarray:\n    \"\"\"PoE Forward Kinematics (4×4)\"\"\"\n    T = np.eye(4)\n    for i in range(len(q)):\n        T @= expm(hat(S[:, i]) * q[i])  # 관절 i 변환 누적\n    return T @ M\n\n\n3.4  Pinocchio FK와 일치성 검증\nfor _ in range(50):\n    q_rand = pin.randomConfiguration(model)       # 무작위 관절벡터\n    pin.forwardKinematics(model, data, q_rand)\n    pin.updateFramePlacements(model, data)\n    T_pin = data.oMf[fid].homogeneous             # Pinocchio FK\n    assert np.allclose(fk_poe(q_rand), T_pin, atol=1e-10)\nprint(\"PoE FK == Pinocchio FK ✅\")"
  },
  {
    "objectID": "posts/code/2025-07-11-poe.html#jacobians-adjoint-기반-계산-pinocchio-비교",
    "href": "posts/code/2025-07-11-poe.html#jacobians-adjoint-기반-계산-pinocchio-비교",
    "title": "👩‍💻스크루-이론 기반 Product-of-Exponentials(PoE) 기구학 Deep-Dive",
    "section": "4  Jacobians — Adjoint 기반 계산 & Pinocchio 비교",
    "text": "4  Jacobians — Adjoint 기반 계산 & Pinocchio 비교\n\n4.1  Adjoint 연산의 기하적 의미\nAdjoint(Ad)는 한 변환 T가 주어졌을 때, 벡터 공간 \\mathfrak{se}(3) 상에서 좌표계를 변환하는 선형 연산자이다. 쉽게 말해, 공간 Jacobian을 누적할 때 “지금까지 적용된 변환”이 다음 관절 축을 베이스 좌표계에서 어떻게 보이게 하는지를 알려준다.\ndef Ad(T: np.ndarray) -&gt; np.ndarray:\n    \"\"\"4×4 변환행렬 → 6×6 Adjoint\"\"\"\n    R, p = T[:3, :3], T[:3, 3]\n    p_hat = np.array([\n        [0, -p[2], p[1]],\n        [p[2], 0, -p[0]],\n        [-p[1], p[0], 0]\n    ])\n    return np.block([\n        [R,            np.zeros((3, 3))],\n        [p_hat @ R,    R]\n    ])\n\n\n4.2  공간 Jacobian 구현\ndef jac_space(S: np.ndarray, q: np.ndarray) -&gt; np.ndarray:\n    \"\"\"6×n 공간 Jacobian (베이스 좌표계)\"\"\"\n    T_prev = np.eye(4)  # 첫 관절 전까지 변환 = I\n    cols = []\n    for i in range(S.shape[1]):\n        cols.append(Ad(T_prev) @ S[:, i])        # 열 i\n        T_prev @= expm(hat(S[:, i]) * q[i])      # 다음 관절로 누적\n    return np.column_stack(cols)\n\n\n4.3  Pinocchio Jacobian과 검증\nq = pin.randomConfiguration(model)\npin.computeJointJacobians(model, data, q)\npin.updateFramePlacements(model, data)\nJ_pin = pin.getFrameJacobian(\n    model, data, fid,\n    pin.ReferenceFrame.LOCAL_WORLD_ALIGNED\n)\nJ_poe = jac_space(S, q)\nnp.testing.assert_allclose(J_poe, J_pin, atol=1e-8)\nprint(\"PoE Jacobian == Pinocchio Jacobian ✅\")\n\n\n4.4  바디 Jacobian 변환\nT_ee = fk_poe(q)\nJ_body = Ad(np.linalg.inv(T_ee)) @ J_poe\n\n바디 Jacobian은 엔드이펙터 프레임에서 해석된 Jacobian이다. Inverse Kinematics나 컨트롤에서 엔드이펙터 기준 오차를 직접 사용하고 싶을 때 유용하다."
  },
  {
    "objectID": "posts/code/2025-07-11-poe.html#casadi-codegen-딥러닝-연동",
    "href": "posts/code/2025-07-11-poe.html#casadi-codegen-딥러닝-연동",
    "title": "👩‍💻스크루-이론 기반 Product-of-Exponentials(PoE) 기구학 Deep-Dive",
    "section": "5  CasADi CodeGen → 딥러닝 연동",
    "text": "5  CasADi CodeGen → 딥러닝 연동\nPoE 수식을 CasADi로 표현하면, C 코드 자동 생성을 통해 임베디드 MCU나 GPU 커스텀 연산자로 쉽게 연결할 수 있다.\n\n5.1  C 코드 자동 생성 (float/double 전환 지원)\nimport casadi as ca\nimport numpy as np\n\n# ── 1. 한 관절의 exp(\\hat\\xi θ) 구현 (SX 심볼릭) ─────────────\n\ndef exp6_cas(xi, theta):\n    omega, v = xi[:3], xi[3:]\n    omega_hat = ca.skew(omega)          # 3×3\n    I = ca.SX.eye(3)\n\n    R = I + ca.sin(theta) * omega_hat + \\\n        (1 - ca.cos(theta)) * (omega_hat @ omega_hat)\n\n    J = I * theta + (1 - ca.cos(theta)) * omega_hat + \\\n        (theta - ca.sin(theta)) * (omega_hat @ omega_hat)\n    P = J @ v\n\n    T = ca.SX.zeros(4, 4)\n    T[:3, :3], T[:3, 3], T[3, 3] = R, P, 1\n    return T\n\n# ── 2. 4‑DoF 손가락 FK & Jacobian 심볼릭 정의 ────────────\n\nq = ca.SX.sym(\"q\", 4)           # 관절 변수 심볼릭\nT = ca.SX.eye(4)\nfor i in range(4):\n    T @= exp6_cas(ca.DM(S[:, i]), q[i])\nT @= ca.DM(M)\n\nJp = ca.jacobian(T[:3, 3], q)                       # 선속도 Jacobian\nJw = ca.hcat([S[:3, i] for i in range(4)])          # 각속도 부분(상수)\nJ6 = ca.vertcat(Jp, Jw)\n\nf = ca.Function(\"allegro_finger_jac\", [q], [J6])\n\n# ── 3. 코드 생성 ─────────────────────────────——\ncg = ca.CodeGenerator(\"allegro_finger_jac.c\")\ncg.add(f)\ncg.generate()            # allegro_finger_jac.c / .h 생성\nprint(\"C code generated ✔\")\n\nCASADI_REAL 매크로로 float/double 전환 가능. MCU(예: STM32) 등에 포팅 시 -DCASADI_REAL=float로 컴파일하면 싱글 프리시전 사용.\n\n\n\n5.2  PyTorch Custom Autograd 연동 (GPU 호환)\nimport torch\nfrom ctypes import CDLL, c_float, POINTER\nimport numpy as np\n\n# 1) 로드한 .so는 gcc -shared -fPIC ... 로 빌드했다고 가정\nlib = CDLL(\"./allegro_finger_jac.so\")\nlib.allegro_finger_jac.argtypes = [POINTER(c_float), POINTER(c_float)]\nlib.allegro_finger_jac.restype  = None\n\nclass FingerJac(torch.autograd.Function):\n    \"\"\"CasADi 생성 코드를 사용하는 PyTorch 연산자\"\"\"\n\n    @staticmethod\n    def forward(ctx, q):\n        q_np = q.detach().cpu().numpy().astype(\"float32\")\n        J_np = np.empty((24,), dtype=\"float32\")\n\n        # C 함수 호출: J_np = f(q_np)\n        lib.allegro_finger_jac(q_np.ctypes.data_as(POINTER(c_float)),\n                               J_np.ctypes.data_as(POINTER(c_float)))\n\n        J = torch.from_numpy(J_np.reshape(6, 4)).to(q.device)\n        ctx.save_for_backward(q)  # backward 용 저장\n        return J\n\n    @staticmethod\n    def backward(ctx, dL_dJ):\n        q, = ctx.saved_tensors\n        # 심플: 실습용으로 0 그라드 반환. 실제 프로젝트에선\n        # CasADi로 Hessian 생성, 또는 finite diff 등 사용.\n        return torch.zeros_like(q)\n\n# ── 사용 예시 ─────────────────────────────\nq_t = torch.randn(4, requires_grad=True)\nJ_t = FingerJac.apply(q_t)\nloss = (J_t ** 2).sum()\nloss.backward()\nprint(\"dLoss/dq:\", q_t.grad)\n\nTip 💡 torch.utils.cpp_extension.load()를 사용하면 빌드 및 로딩을 PyTorch가 자동 관리해, CUDA 커널을 포함한 확장 연산자를 빠르게 실험할 수 있다."
  },
  {
    "objectID": "posts/code/2025-07-11-poe.html#현업-tips-잠재적-함정",
    "href": "posts/code/2025-07-11-poe.html#현업-tips-잠재적-함정",
    "title": "👩‍💻스크루-이론 기반 Product-of-Exponentials(PoE) 기구학 Deep-Dive",
    "section": "6  현업 Tips & 잠재적 함정",
    "text": "6  현업 Tips & 잠재적 함정\n\n\n\n\n\n\n\n 항목 \n체크포인트 \n\n\n\n\n축 방향 부호\nCAD/URDF 축이 ± 반대일 수 있다. ω 부호 확인 후 핸드 규격서와 대조.\n\n\n홈 포즈 M\n프레임 잘못 잡으면 Jacobian 해석 오류. 툴 프레임 기준을 명확히.\n\n\n수치 안정성\n관절 수가 많은 로봇은 \\exp(\\hat\\xi_i q_i) 누적 시 오차가 커질 수 있음. → 중간마다 Adjoint 재계산하여 드리프트 억제.\n\n\n모델 보정\n\\xi 파라미터는 매니폴드 \\mathbb S^2!\\times!\\mathbb R^3에 놓인다. → Sophus 또는 pin.bias를 이용한 Lie 샘플러로 최적화.\n\n\n딥러닝 통합\nCasADi C → torch.autograd.Function 래핑 시 GPU inference ≈ native PyTorch 속도.\n\n\n병렬·폐쇄체인\nPoE는 직렬 체인 가정. 폐쇄 루프는 가상 6‑DoF 조인트 추가 후 라그랑주 승수로 제약 처리."
  },
  {
    "objectID": "posts/code/2025-07-11-poe.html#맺음말",
    "href": "posts/code/2025-07-11-poe.html#맺음말",
    "title": "👩‍💻스크루-이론 기반 Product-of-Exponentials(PoE) 기구학 Deep-Dive",
    "section": "맺음말",
    "text": "맺음말\nPoE 표현은 DH 기구학의 좌표계 의존성과 파라미터 불연속 문제를 근본적으로 해결한다. 스크루 \\xi와 홈 포즈 M만 정해지면 Forward Kinematics, Jacobian, 동역학까지 일관되게 파생되므로, 모델 보정·강화학습·MPC 등 고급 기법에서 탁월한 수렴성과 견고성을 보여 준다.\nPinocchio + CasADi 조합을 활용하면 파이썬 100여 라인 정도로 PoE 기구학 전체 파이프라인과 임베디드용 C 코드를 즉시 생성할 수 있다.\n\n이제 스크루 행렬과 홈 포즈만 준비했다면, 본문 코드 조각을 복사해 당신의 로봇 프로젝트에 바로 적용해 보자!\n\nReference\n\nhttps://hades.mech.northwestern.edu/images/7/7f/MR.pdf#page=154.20\nhttps://en.wikipedia.org/wiki/Product_of_exponentials_formula\nhttps://ddangeun.tistory.com/26\nhttps://web.casadi.org/\nhttps://stack-of-tasks.github.io/pinocchio/\nhttps://github.com/stack-of-tasks/pinocchio"
  },
  {
    "objectID": "posts/code/2022-12-21-torch-tensor-inverted.html",
    "href": "posts/code/2022-12-21-torch-tensor-inverted.html",
    "title": "👩‍💻torch.Tensor vs torch.tensor",
    "section": "",
    "text": "둘다 data(input)에 대해서 Tensor 객체로 만들어 주는 기능은 동일\n\n\n\n\ntorch.tensor() : function\n\ndocs: https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor\ntorch.tensor(data)에서 data는 필수 argument. (data 없을 시 TypeError)\ntorch.tensor는 항상 data를 복사\nint 입력 시 int 그대로 입력\n입력받은 데이터를 새로운 메모리 공간에 복사해 Tensor 객체 생성 (call by value)\n\ntorch.Tensor() : class\n\ndocs: https://pytorch.org/docs/stable/tensors.html#torch.Tensor\n이미 생성된 객체를 tensor로 바꾸고 싶을 때 사용\n빈 Tensor 객체를 만들 때 사용\nint 입력 시 float로 변환\nTensor 객체로 데이터를 입력할 경우 입력 받은 메모리 공간 그대로 사용 (call by reference)\nlist나 numpy 등 다른 자료형으로 입력 받을 경우 값을 복사하여 Tensor 객체 생성(call by value)\n\n\n\n\n\n\nimport torch\n\n\noriginal_data = torch.tensor([1])\nnew_data = torch.tensor(original_data)\nprint(f\"original : {original_data} {original_data.dtype}\")\nprint(f\"new : {new_data} {new_data.dtype}\")\n\n \n# original data를 수정\noriginal_data[0] = 2\nprint(f\"original : {original_data} {original_data.dtype}\")\nprint(f\"new : {new_data} {new_data.dtype}\")\n\noriginal : tensor([1]) torch.int64\nnew : tensor([1]) torch.int64\noriginal : tensor([2]) torch.int64\nnew : tensor([1]) torch.int64\n\n\n/tmp/ipykernel_847781/1124094978.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  new_data = torch.tensor(original_data)\n\n\n\nint에서 int로\ncall by value, 새로운 Tensor 객체이므로 변화 x\n\n\n\n\n\noriginal_data = torch.Tensor([1])\nnew_data = torch.Tensor(original_data)\nprint(f\"original : {original_data} {original_data.dtype}\")\nprint(f\"new : {new_data} {new_data.dtype}\")\n\n \n# original data를 수정\noriginal_data[0] = 2\nprint(f\"original : {original_data} {original_data.dtype}\")\nprint(f\"new : {new_data} {new_data.dtype}\")\n\noriginal : tensor([1.]) torch.float32\nnew : tensor([1.]) torch.float32\noriginal : tensor([2.]) torch.float32\nnew : tensor([2.]) torch.float32\n\n\n\nint에서 float으로\ncall by reference, shallow copy\n\n\noriginal_data = [1]\nnew_data = torch.Tensor(original_data)\nprint(f\"original : {original_data}\")\nprint(f\"new : {new_data} {new_data.dtype}\")\n\n# original data를 수정\noriginal_data[0] = 2\nprint(f\"original : {original_data}\")\nprint(f\"new : {new_data} {new_data.dtype}\")\n\noriginal : [1]\nnew : tensor([1.]) torch.float32\noriginal : [2]\nnew : tensor([1.]) torch.float32\n\n\n\nint에서 float으로\ncall by value, deep copy"
  },
  {
    "objectID": "posts/code/2022-12-21-torch-tensor-inverted.html#공통점",
    "href": "posts/code/2022-12-21-torch-tensor-inverted.html#공통점",
    "title": "👩‍💻torch.Tensor vs torch.tensor",
    "section": "",
    "text": "둘다 data(input)에 대해서 Tensor 객체로 만들어 주는 기능은 동일"
  },
  {
    "objectID": "posts/code/2022-12-21-torch-tensor-inverted.html#차이점",
    "href": "posts/code/2022-12-21-torch-tensor-inverted.html#차이점",
    "title": "👩‍💻torch.Tensor vs torch.tensor",
    "section": "",
    "text": "torch.tensor() : function\n\ndocs: https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor\ntorch.tensor(data)에서 data는 필수 argument. (data 없을 시 TypeError)\ntorch.tensor는 항상 data를 복사\nint 입력 시 int 그대로 입력\n입력받은 데이터를 새로운 메모리 공간에 복사해 Tensor 객체 생성 (call by value)\n\ntorch.Tensor() : class\n\ndocs: https://pytorch.org/docs/stable/tensors.html#torch.Tensor\n이미 생성된 객체를 tensor로 바꾸고 싶을 때 사용\n빈 Tensor 객체를 만들 때 사용\nint 입력 시 float로 변환\nTensor 객체로 데이터를 입력할 경우 입력 받은 메모리 공간 그대로 사용 (call by reference)\nlist나 numpy 등 다른 자료형으로 입력 받을 경우 값을 복사하여 Tensor 객체 생성(call by value)"
  },
  {
    "objectID": "posts/code/2022-12-21-torch-tensor-inverted.html#torch.tensor",
    "href": "posts/code/2022-12-21-torch-tensor-inverted.html#torch.tensor",
    "title": "👩‍💻torch.Tensor vs torch.tensor",
    "section": "",
    "text": "import torch\n\n\noriginal_data = torch.tensor([1])\nnew_data = torch.tensor(original_data)\nprint(f\"original : {original_data} {original_data.dtype}\")\nprint(f\"new : {new_data} {new_data.dtype}\")\n\n \n# original data를 수정\noriginal_data[0] = 2\nprint(f\"original : {original_data} {original_data.dtype}\")\nprint(f\"new : {new_data} {new_data.dtype}\")\n\noriginal : tensor([1]) torch.int64\nnew : tensor([1]) torch.int64\noriginal : tensor([2]) torch.int64\nnew : tensor([1]) torch.int64\n\n\n/tmp/ipykernel_847781/1124094978.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  new_data = torch.tensor(original_data)\n\n\n\nint에서 int로\ncall by value, 새로운 Tensor 객체이므로 변화 x"
  },
  {
    "objectID": "posts/code/2022-12-21-torch-tensor-inverted.html#torch.tensor-1",
    "href": "posts/code/2022-12-21-torch-tensor-inverted.html#torch.tensor-1",
    "title": "👩‍💻torch.Tensor vs torch.tensor",
    "section": "",
    "text": "original_data = torch.Tensor([1])\nnew_data = torch.Tensor(original_data)\nprint(f\"original : {original_data} {original_data.dtype}\")\nprint(f\"new : {new_data} {new_data.dtype}\")\n\n \n# original data를 수정\noriginal_data[0] = 2\nprint(f\"original : {original_data} {original_data.dtype}\")\nprint(f\"new : {new_data} {new_data.dtype}\")\n\noriginal : tensor([1.]) torch.float32\nnew : tensor([1.]) torch.float32\noriginal : tensor([2.]) torch.float32\nnew : tensor([2.]) torch.float32\n\n\n\nint에서 float으로\ncall by reference, shallow copy\n\n\noriginal_data = [1]\nnew_data = torch.Tensor(original_data)\nprint(f\"original : {original_data}\")\nprint(f\"new : {new_data} {new_data.dtype}\")\n\n# original data를 수정\noriginal_data[0] = 2\nprint(f\"original : {original_data}\")\nprint(f\"new : {new_data} {new_data.dtype}\")\n\noriginal : [1]\nnew : tensor([1.]) torch.float32\noriginal : [2]\nnew : tensor([1.]) torch.float32\n\n\n\nint에서 float으로\ncall by value, deep copy"
  },
  {
    "objectID": "posts/code/2022-12-21-torch-tensor-inverted.html#torch.tensor-2",
    "href": "posts/code/2022-12-21-torch-tensor-inverted.html#torch.tensor-2",
    "title": "👩‍💻torch.Tensor vs torch.tensor",
    "section": "torch.tensor()",
    "text": "torch.tensor()\n\na = torch.tensor([0,1,10])\nprint(f\"Shape of tensor: {a.shape}\")\nprint(f\"Datatype of tensor: {a.dtype}\")\nprint(f\"Device tensor is stored on: {a.device}\")\n\nShape of tensor: torch.Size([3])\nDatatype of tensor: torch.int64\nDevice tensor is stored on: cpu\n\n\n\n~a\n\ntensor([ -1,  -2, -11])\n\n\n\nb = torch.tensor([True, False, True])\nprint(f\"Shape of tensor: {b.shape}\")\nprint(f\"Datatype of tensor: {b.dtype}\")\nprint(f\"Device tensor is stored on: {b.device}\")\n\nShape of tensor: torch.Size([3])\nDatatype of tensor: torch.bool\nDevice tensor is stored on: cpu\n\n\n\n~b\n\ntensor([False,  True, False])\n\n\n\nc = torch.tensor([0.13, 1.5, -.116])\nprint(f\"Shape of tensor: {c.shape}\")\nprint(f\"Datatype of tensor: {c.dtype}\")\nprint(f\"Device tensor is stored on: {c.device}\")\n\nShape of tensor: torch.Size([3])\nDatatype of tensor: torch.float32\nDevice tensor is stored on: cpu\n\n\n\n~c\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nInput In [10], in &lt;cell line: 1&gt;()\n----&gt; 1 ~c\n\nTypeError: ~ (operator.invert) is only implemented on integer and Boolean-type tensors"
  },
  {
    "objectID": "posts/code/2022-12-21-torch-tensor-inverted.html#torch.tensor-3",
    "href": "posts/code/2022-12-21-torch-tensor-inverted.html#torch.tensor-3",
    "title": "👩‍💻torch.Tensor vs torch.tensor",
    "section": "torch.Tensor()",
    "text": "torch.Tensor()\n\na = torch.Tensor([0,1,10])\nprint(f\"Shape of tensor: {a.shape}\")\nprint(f\"Datatype of tensor: {a.dtype}\")\nprint(f\"Device tensor is stored on: {a.device}\")\n\nShape of tensor: torch.Size([3])\nDatatype of tensor: torch.float32\nDevice tensor is stored on: cpu\n\n\n\na\n\ntensor([ 0.,  1., 10.])\n\n\n\n~a\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nInput In [13], in &lt;cell line: 1&gt;()\n----&gt; 1 ~a\n\nTypeError: ~ (operator.invert) is only implemented on integer and Boolean-type tensors\n\n\n\n\nb = torch.Tensor([True, False, True])\nprint(f\"Shape of tensor: {b.shape}\")\nprint(f\"Datatype of tensor: {b.dtype}\")\nprint(f\"Device tensor is stored on: {b.device}\")\n\nShape of tensor: torch.Size([3])\nDatatype of tensor: torch.float32\nDevice tensor is stored on: cpu\n\n\n\nb\n\ntensor([1., 0., 1.])\n\n\n\n~b\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nInput In [16], in &lt;cell line: 1&gt;()\n----&gt; 1 ~b\n\nTypeError: ~ (operator.invert) is only implemented on integer and Boolean-type tensors\n\n\n\n\nc = torch.Tensor([0.13, 1.5, -.116])\nprint(f\"Shape of tensor: {c.shape}\")\nprint(f\"Datatype of tensor: {c.dtype}\")\nprint(f\"Device tensor is stored on: {c.device}\")\n\nShape of tensor: torch.Size([3])\nDatatype of tensor: torch.float32\nDevice tensor is stored on: cpu\n\n\n\nc\n\ntensor([ 0.1300,  1.5000, -0.1160])\n\n\n\n~c\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nInput In [19], in &lt;cell line: 1&gt;()\n----&gt; 1 ~c\n\nTypeError: ~ (operator.invert) is only implemented on integer and Boolean-type tensors\n\n\n\n결론: torch.tensor()로만 ~연산 사용 가능\nReference\n\ntorch.Tensor와 torch.tensor의 차이"
  },
  {
    "objectID": "posts/code/2020-07-13-install-mujoco-win10.html",
    "href": "posts/code/2020-07-13-install-mujoco-win10.html",
    "title": "👩‍💻Install Mujoco in Windows10",
    "section": "",
    "text": "Mujoco\n\nMujoco에서 License tab으로 들어가서 Personal Student Software License를 받는다.\n이메일로 Account number를 받는다.(spam도 확인하자)\n다시 License로 가서 Account number와 Computer Id를 입력한다.\n\n\n\n\n\n다시 이메일로 MuJoCo Pro Personal Activation Key(mjkey.txt)를 받는다.\n.mujoco라는 파일을 C:\\Users\\사용자명\\에 만든다.\nProducts에서 mjpro150 win64을 다운받아 .mujoco에 압축을 풀어준다.(C:\\Users\\사용자명\\.mujoco\\mjpro150)\n\n\n\n\n\n다운받은 mjkey.txt 파일을 C:\\Users\\사용자명\\.mujoco와 C:\\Users\\사용자명\\.mujoco\\mjpro150\\bin 에 옮긴다.\ncmd창을 열어서 cd 명령어로 C:\\Users\\사용자명\\.mujoco\\mjpro150\\bin 경로로 이동한 다음 simulate ../model/humanoid.xml명령어를 입력한다.\nHumanoid Simulation 창이 나오면 성공이다.\n\n\n\n\n\n\n\n\nMujoco-py\n\nSET PATH=C:\\Users\\사용자명\\.mujoco\\mjpro150\\bin;%PATH%;으로 경로설정을 해준다.\nmujoco-py-1.50.1.0 파일을 다운받아 C:\\Users\\사용자명\\.mujoco에 압축을 풀어준다.(C:\\Users\\사용자명\\.mujoco\\mujoco-py-1.50.1.0)\ncmd 창에서 python setup.py install를 입력하여 설치한다.\nC:\\Users\\사용자명\\.mujoco\\mujoco-py-1.50.1.0&gt; python setup.py install\n혹시 여기서 error가 난다면 전에 mujoco-py를 설치해서 버젼이 안 맞아 나는 것일 수도 있다. pip list로 mujoco-py의 버젼을 확인해보고 다른 버젼이라면 pip uninstall mujoco-py를 해준후 다시 설치한다.\ncmd 창에서 python examples\\body_interaction.py를 입력하여 잘 실행되는지 확인한다.\nC:\\Users\\사용자명\\.mujoco\\mujoco-py-1.50.1.0&gt; python examples\\body_interaction.py\n처음에 실행화면이 뜨는 시간이 오래걸리지만 이후에는 실행창이 빨리 나왔다.\n\n\n\n\n\n\n\nReference\n\nMuJoCo 설치 (윈도우 10 version)"
  },
  {
    "objectID": "posts/note/2021-01-31-ML-tu-berlin.html",
    "href": "posts/note/2021-01-31-ML-tu-berlin.html",
    "title": "📘2021 TU Berlin Winter Course 수강 후기",
    "section": "",
    "text": "사실 ML/DL 카테고리에는 공부내용만 적을려고 했으나 관련 수업을 들은 후기도 여기에 정리하면 좋을 것 같아 여기에 분류했다. 후에 수업에서 배운 내용들도 차근차근히 정리해볼 예정이다.\n\n\n학교에서 지원해주는 교환학생이라던가 해외 협업 수업은 언제나 하고 싶었지만 항상 발목을 잡은 건 영어였던 것 같다. 영어..읽고 쓰는 것도 좋아하고 듣는 것도 다 알아듣진 못해도 어느정도는 되는데 항상 ’말하기’가 문제였다. 사실 천성 내향형 인간이라 한국어로도 말은 원래 잘 못하는데 영어는 더더욱 자신이 없었다. 게다가 학교에서 지원하는 프로그램들은 내가 영어를 잘하는지 증명을 해야하는데 토플 준비만했을 뿐 정작 시험은 안봤다. 계속 다른 일에 우선순위가 밀리고 시험비도 그냥 한번 시도해보기엔 만만치 않아서가 변명이라면 변명이었다. 어쨌든 자격증이 없으니 지원자격자체가 안됬었다.\n그런데 영어 자격증이 없어도 지원할 수 있는 프로그램이 하나 생겼고 코로나로 인해 온라인이긴 했지만 지원했다. 마지막 4학년만 남았기에 이번이 마지막일 수도 있다는 생각도 있었다. 들을 수 있는 수업이 4개가 있었는데 데이터 사이언스/시각화분야 수업과 머신러닝 이론/적용분야 수업 중에 고민을 했다. 결과적으로는 머신러닝 이론/적용을 선택했다. 이유는 아래와 같았다.\n\nSyllabus에 이미 알고 있는 내용들이 좀 더 많았기에 혹시 놓치더라도 뒤쳐지지 않을 것 같았음 + 영어로 소통하는 것에 좀 더 집중할 수 있음\nFeature Engineering 부분이나 Unsupervised Learning과 같은 실제 적용은 많이 해보지 않은 부분들을 다지는 기회\nLecturer 중에 전공분야가 Reinforcement Learning이신 분이 있어서 (혹시라도 친해지면) 이것저것 물어볼 수 있지 않을까라는 생각\n\n그래서 학교에 지원을 넣고 합격이 될까 안될까 걱정반 설렘반이었는데, 담당 교수님과 편하게 카톡영상통화로 면접을 보고 난후 흔쾌히 허락해주셨다. 다행이었다. 이제 독일 학교에 지원서를 작성하는데 문제가 하나 있었다. 지원서 영어실력을 증명하라는 칸이 있었는데 위에 써놓은 것과 같이 난 자격증이 없었기에 뭘 제출하라는 말인지 이해하지 못했다. 근데 컨택해주신 교수님이 분명 영어 자격증없어도 가능하다고 하셨던 것 같은데..뭘까…고민을 하다가 학교에서 이수한 과목들 목록을 뽑았다. 그중에는 영국인 교수님이 수업하신 (물론 영어로 진행된) 영국문화 알아가기라는 교양 수업도 있었고, 필수 이수과목인 대학영어, 전공과목이지만 영어로 진행한 수업들도 있었다. 그래서 학기별 내가 들은 ’영어’와 관련된 수업 목록들을 하나하나 설명하는 보고서(?)를 작성하여 “이런 영어로 진행된 수업들을 잘 이수하였으니 난 영어로 소통가능하다”라고 어필했다. 다행히도 그 보고서로 나의 영어 실력이 증명이 되어서 간신히 수업등록을 마칠 수 있었다. 이렇게 우여곡절 끝에 수업등록을 하니 영어자격증을 빨리 따야겠다는 생각만 더 절실해졌다.😂(이제 조금 여유가 생긴 2월달에 토플 시험 볼 예정이다.)\n\n\n\n\n수업이 시작되고 역시나 걱정되는 건 영어로하는 의사소통이었다. 급하게 영어 말하기 세포를 깨우기 위해 Facebook 지인분들께 방법도 여쭤보고 혼자서 엄청 중얼중얼 거렸다. 단기간에 실력적인 향상 효과는 미미했지만 마음의 준비는 할 수 있었다. 근데 확실히 수업이 막상 시작하니 난관은 말하기보다 듣기에 있었다. 다양한 나라의 다양한 억양은 듣기 힘들다는 이야기를 듣긴 했어도 직접 체험해보니 잘 안들리는 황당함이란.. 다양한 나라의 사람들과 소통하고 싶다는 포부로 시작했던 처음 마음과 달리, 수업하는 동안은 ’잘’듣는 것만으로도 나에게 도전이었던 것 같다. 그런데 한편으로는 완벽한 영어를 구성하지 않는, 나와같이 영어가 제2외국어이신 분들과 수업을 하니 말할때는 부담없이 나도 막(?)말했던 것 같다. 어쨌든 걱정했던 ’말하기’는 생각보다 난관이 아니었고, ’듣기’가 난관이었다는 이야기다. 같이 수업을 듣는 분들은 약 20명 정도 였는데 첫시간에 자기소개할 때 들어보니 다들 박사과정이거나 직장인이었다..?!😲 학사도 졸업 못한 나랑은 너무나 대비되는 분들이라 신기하기도 하고 걱정도 됐다. (학교는 왜 이런 프로그램인지 왜 안알려줬지) 그분들의 능력은 나중에 프로젝트 발표하실 때 확연히 느낄 수 있었는데 프로젝트 완성도나 주제가 넘사벽이었다.\n수업은 대부분 강의 시청 + exercise + 질의 응답으로 이루어졌기에 내가 적극적으로 이야기하고 질문할 부분이 많진 않았다. 그리고 앞서 적었던 것처럼 수업내용이 나한테는 완전히 새로운 분야는 아니어서 그런지 그럭저럭 따라갈 수 있었다. (벌써 수업 마쳤다고 기억이 미화된건지는 모르겠지만) 그리고 모르는게 있으면 구글링으로 잘 정리해놓은 훌륭하신 한국인 분들의 포스팅, 가지고 있는 책들로 메꿔갔기에 수업을 따라가는 건 많이 어렵진 않았던 것 같다. 진짜 다른분들이 잘 정리해놓으신 포스팅에 도움을 많이 받아서 항상 드는 생각이지만 언젠가는 나도 저분들처럼 도움되는 포스팅을 하고 싶다라는 생각이 든다.\n사실 등록 전에는 몰랐던 Group Work는 수업계획서에서 처음 봤을 때 당황스러웠다. 그러나 걱정됐던 부분과 달리 팀구성도 잘됐고, 내가 주도해서 그런지 주제도 내가 원하는 걸로 진행하게되었다. 워낙 짧은 시간(3일 정도)에 완성해야하는 프로젝트였기에 마음과 달리 여러가지 기법들을 사용하기 보다는 심플하고 충실하게 ‘배운것만’ 응용해서 프로젝트를 마무리했다. 그래도 팀원분들과 영어로 의견 조율하는게 잠깐이었지만 재밌었고, 이번 경험으로 영어가 더이상 공포의 대상이 아닌 세상 다른 사람들과 ’소통’할 수 있는 도구라는 생각이 확실히 각인된 것 같다. 정말 ’언어’라는 생각을 체험해보는 기회였달까. 그렇게 더듬더듬거리는 영어지만 의견과 생각을 나누는 경험은 소중했고 마지막 프로젝트까지 잘 마칠 수 있었다. 나중에 Lecturer 분들께서 코멘트 해주실 때도 프로젝트 지적을 당했다라는 생각보다, 진정으로 우리 프로젝트를 봐주시고 인정해주신다는 생각이 들어서 감사했다. 그리고 사실은 진짜로 우리팀이 프로젝트에서 중요한 포인트를 빼먹었었는데 짚어주시는 거 보고 많이 배웠다.(한편으로는 말도 안되는 영어로 발표하는 거 들으시고도 이해하셨다는게 신기했다🤣) 아쉽게도 발표할 때가 처음이자 마지막으로 Lecturer 분들과 이야기한 순간이었다.(아 첫시간에 자기소개 빼고!) 프로젝트 깃헙은 여기에 있다.\n수업 마지막에 봤던 시험은 생각보다 어렵지 않았기에 사실 후기에 적을 내용이 별로 없다. 한국 시험에서는 선택형이어도 꼬아서 내거나 복수 정답지들을 만들어서 난이도를 높이는데, 여기 수업은 깔끔하게 중복 답안지 없다라고 말해주고 선택지들도 기본중의 기본이었다. 그나마 분별력을 가질 수 있는 부분은 프로그래밍 파트였으나 이 부분마저 코드를 외워서 치는게 아니라 오픈북이어서 그렇게 어렵진 않았던 것 같다. 게다가 시험시간은 3시간이었기에 타임 리밋도 거의 없는거나 다름없었다. 물론 시험보기 전에는 시험이 어떻게 나올지 예측할 수 없었기에 전날밤을 새워가며 공부하긴 했었다.\n\n\n\n마지막 날에 발표를 마치고 Lecturer분들의 ML/DL 트렌드에 관한 짧은 발표와 같이 수업들은 분들과 약간의 담소를 나누며 훈훈하게 수업이 끝났다. 약 3주간동안 저녁 시간에 열심히 배웠던 수업이 막을 내렸다.\n항상 가장 괴로울 때 가장 많이 배우는 것 같다. 사실 지나고 보면 그 고통이 어느정도 미화되는 것도 있고 위에 ‘어렵지 않았다.’, ’잘 마무리됐다.’라고 써놓기도 했지만, 그걸 하고 있던 순간에는 힘들었다. 아무리 알고 있던 내용이들이라도 그 내용들을 다시 다지는 것도 어렵고, 직접 코드를 써보는 것도 어렵고, 마무리 하기까지 불확실성과 두려움 때문에 괴로웠던 건 팩트다. 근데 항상 그러면서 배우는 것 같다. 그래서 이번 코스를 통해 배운 걸 정리해보자면,\n\nML은 확실히 DL보다 수학적인 이론이 어려웠음\n영어가 시험이나 부담이 아닌 ’언어’라는 것. 나를 표현할 수 있는 툴이라는 것\n전에 조금이라도 공부했던 것들이 나중에 도움이 ’크게’될 수 있다는 것\n내가 알고있는 것들을 꾸준하게 포스팅하는 습관을 길러야 하겠다는 생각\n해보지 않고 추측만으로 하는 생각들로 두려워하지 말 것\n\n수업을 마치고 certificate를 받았다. 사실 certificate에는 옵션이 있었는데, grade를 받을 것인지/pass or fail을 받을 것인지 내가 선택할 수 있었다. 등록 당시엔 근자감으로 grade를 받겠다고 했다가 수업 듣는 동안 과거의 나를 반성했다. 사실 수업하는 동안 그렇게 적극적이지도 않았고, 프로젝트 발표를 하고 지적을 받으면서 든 생각은 ‘최고 grade까지는 못받고 중간이라도 했으면..’ 했다. 그런데 생각지 못했던 최고 grade를 받을 수 있었다. 학점이 다가 아니긴 하지만 그래도 최선을 다했다는 것을 어느정도 뒷받침해줄 수 있기 때문에 뿌듯했다. 그래서 위에 배운 점들에 하나 더 추가하자면, ’최선을 다해서 어찌저찌해보면 뭐라도 한다.’라는 것. 식상한 멘트와 교훈이지만 매번 도전하면서 느끼는 바이기도 하다. 이렇게 2021년도 1월은 TU Berlin의 Machine Learning using Python: Theory and Application 코스와 함께 시간을 보냈다."
  },
  {
    "objectID": "posts/note/2021-01-31-ML-tu-berlin.html#tu-berlin-winter-university-online-2021",
    "href": "posts/note/2021-01-31-ML-tu-berlin.html#tu-berlin-winter-university-online-2021",
    "title": "📘2021 TU Berlin Winter Course 수강 후기",
    "section": "",
    "text": "사실 ML/DL 카테고리에는 공부내용만 적을려고 했으나 관련 수업을 들은 후기도 여기에 정리하면 좋을 것 같아 여기에 분류했다. 후에 수업에서 배운 내용들도 차근차근히 정리해볼 예정이다.\n\n\n학교에서 지원해주는 교환학생이라던가 해외 협업 수업은 언제나 하고 싶었지만 항상 발목을 잡은 건 영어였던 것 같다. 영어..읽고 쓰는 것도 좋아하고 듣는 것도 다 알아듣진 못해도 어느정도는 되는데 항상 ’말하기’가 문제였다. 사실 천성 내향형 인간이라 한국어로도 말은 원래 잘 못하는데 영어는 더더욱 자신이 없었다. 게다가 학교에서 지원하는 프로그램들은 내가 영어를 잘하는지 증명을 해야하는데 토플 준비만했을 뿐 정작 시험은 안봤다. 계속 다른 일에 우선순위가 밀리고 시험비도 그냥 한번 시도해보기엔 만만치 않아서가 변명이라면 변명이었다. 어쨌든 자격증이 없으니 지원자격자체가 안됬었다.\n그런데 영어 자격증이 없어도 지원할 수 있는 프로그램이 하나 생겼고 코로나로 인해 온라인이긴 했지만 지원했다. 마지막 4학년만 남았기에 이번이 마지막일 수도 있다는 생각도 있었다. 들을 수 있는 수업이 4개가 있었는데 데이터 사이언스/시각화분야 수업과 머신러닝 이론/적용분야 수업 중에 고민을 했다. 결과적으로는 머신러닝 이론/적용을 선택했다. 이유는 아래와 같았다.\n\nSyllabus에 이미 알고 있는 내용들이 좀 더 많았기에 혹시 놓치더라도 뒤쳐지지 않을 것 같았음 + 영어로 소통하는 것에 좀 더 집중할 수 있음\nFeature Engineering 부분이나 Unsupervised Learning과 같은 실제 적용은 많이 해보지 않은 부분들을 다지는 기회\nLecturer 중에 전공분야가 Reinforcement Learning이신 분이 있어서 (혹시라도 친해지면) 이것저것 물어볼 수 있지 않을까라는 생각\n\n그래서 학교에 지원을 넣고 합격이 될까 안될까 걱정반 설렘반이었는데, 담당 교수님과 편하게 카톡영상통화로 면접을 보고 난후 흔쾌히 허락해주셨다. 다행이었다. 이제 독일 학교에 지원서를 작성하는데 문제가 하나 있었다. 지원서 영어실력을 증명하라는 칸이 있었는데 위에 써놓은 것과 같이 난 자격증이 없었기에 뭘 제출하라는 말인지 이해하지 못했다. 근데 컨택해주신 교수님이 분명 영어 자격증없어도 가능하다고 하셨던 것 같은데..뭘까…고민을 하다가 학교에서 이수한 과목들 목록을 뽑았다. 그중에는 영국인 교수님이 수업하신 (물론 영어로 진행된) 영국문화 알아가기라는 교양 수업도 있었고, 필수 이수과목인 대학영어, 전공과목이지만 영어로 진행한 수업들도 있었다. 그래서 학기별 내가 들은 ’영어’와 관련된 수업 목록들을 하나하나 설명하는 보고서(?)를 작성하여 “이런 영어로 진행된 수업들을 잘 이수하였으니 난 영어로 소통가능하다”라고 어필했다. 다행히도 그 보고서로 나의 영어 실력이 증명이 되어서 간신히 수업등록을 마칠 수 있었다. 이렇게 우여곡절 끝에 수업등록을 하니 영어자격증을 빨리 따야겠다는 생각만 더 절실해졌다.😂(이제 조금 여유가 생긴 2월달에 토플 시험 볼 예정이다.)\n\n\n\n\n수업이 시작되고 역시나 걱정되는 건 영어로하는 의사소통이었다. 급하게 영어 말하기 세포를 깨우기 위해 Facebook 지인분들께 방법도 여쭤보고 혼자서 엄청 중얼중얼 거렸다. 단기간에 실력적인 향상 효과는 미미했지만 마음의 준비는 할 수 있었다. 근데 확실히 수업이 막상 시작하니 난관은 말하기보다 듣기에 있었다. 다양한 나라의 다양한 억양은 듣기 힘들다는 이야기를 듣긴 했어도 직접 체험해보니 잘 안들리는 황당함이란.. 다양한 나라의 사람들과 소통하고 싶다는 포부로 시작했던 처음 마음과 달리, 수업하는 동안은 ’잘’듣는 것만으로도 나에게 도전이었던 것 같다. 그런데 한편으로는 완벽한 영어를 구성하지 않는, 나와같이 영어가 제2외국어이신 분들과 수업을 하니 말할때는 부담없이 나도 막(?)말했던 것 같다. 어쨌든 걱정했던 ’말하기’는 생각보다 난관이 아니었고, ’듣기’가 난관이었다는 이야기다. 같이 수업을 듣는 분들은 약 20명 정도 였는데 첫시간에 자기소개할 때 들어보니 다들 박사과정이거나 직장인이었다..?!😲 학사도 졸업 못한 나랑은 너무나 대비되는 분들이라 신기하기도 하고 걱정도 됐다. (학교는 왜 이런 프로그램인지 왜 안알려줬지) 그분들의 능력은 나중에 프로젝트 발표하실 때 확연히 느낄 수 있었는데 프로젝트 완성도나 주제가 넘사벽이었다.\n수업은 대부분 강의 시청 + exercise + 질의 응답으로 이루어졌기에 내가 적극적으로 이야기하고 질문할 부분이 많진 않았다. 그리고 앞서 적었던 것처럼 수업내용이 나한테는 완전히 새로운 분야는 아니어서 그런지 그럭저럭 따라갈 수 있었다. (벌써 수업 마쳤다고 기억이 미화된건지는 모르겠지만) 그리고 모르는게 있으면 구글링으로 잘 정리해놓은 훌륭하신 한국인 분들의 포스팅, 가지고 있는 책들로 메꿔갔기에 수업을 따라가는 건 많이 어렵진 않았던 것 같다. 진짜 다른분들이 잘 정리해놓으신 포스팅에 도움을 많이 받아서 항상 드는 생각이지만 언젠가는 나도 저분들처럼 도움되는 포스팅을 하고 싶다라는 생각이 든다.\n사실 등록 전에는 몰랐던 Group Work는 수업계획서에서 처음 봤을 때 당황스러웠다. 그러나 걱정됐던 부분과 달리 팀구성도 잘됐고, 내가 주도해서 그런지 주제도 내가 원하는 걸로 진행하게되었다. 워낙 짧은 시간(3일 정도)에 완성해야하는 프로젝트였기에 마음과 달리 여러가지 기법들을 사용하기 보다는 심플하고 충실하게 ‘배운것만’ 응용해서 프로젝트를 마무리했다. 그래도 팀원분들과 영어로 의견 조율하는게 잠깐이었지만 재밌었고, 이번 경험으로 영어가 더이상 공포의 대상이 아닌 세상 다른 사람들과 ’소통’할 수 있는 도구라는 생각이 확실히 각인된 것 같다. 정말 ’언어’라는 생각을 체험해보는 기회였달까. 그렇게 더듬더듬거리는 영어지만 의견과 생각을 나누는 경험은 소중했고 마지막 프로젝트까지 잘 마칠 수 있었다. 나중에 Lecturer 분들께서 코멘트 해주실 때도 프로젝트 지적을 당했다라는 생각보다, 진정으로 우리 프로젝트를 봐주시고 인정해주신다는 생각이 들어서 감사했다. 그리고 사실은 진짜로 우리팀이 프로젝트에서 중요한 포인트를 빼먹었었는데 짚어주시는 거 보고 많이 배웠다.(한편으로는 말도 안되는 영어로 발표하는 거 들으시고도 이해하셨다는게 신기했다🤣) 아쉽게도 발표할 때가 처음이자 마지막으로 Lecturer 분들과 이야기한 순간이었다.(아 첫시간에 자기소개 빼고!) 프로젝트 깃헙은 여기에 있다.\n수업 마지막에 봤던 시험은 생각보다 어렵지 않았기에 사실 후기에 적을 내용이 별로 없다. 한국 시험에서는 선택형이어도 꼬아서 내거나 복수 정답지들을 만들어서 난이도를 높이는데, 여기 수업은 깔끔하게 중복 답안지 없다라고 말해주고 선택지들도 기본중의 기본이었다. 그나마 분별력을 가질 수 있는 부분은 프로그래밍 파트였으나 이 부분마저 코드를 외워서 치는게 아니라 오픈북이어서 그렇게 어렵진 않았던 것 같다. 게다가 시험시간은 3시간이었기에 타임 리밋도 거의 없는거나 다름없었다. 물론 시험보기 전에는 시험이 어떻게 나올지 예측할 수 없었기에 전날밤을 새워가며 공부하긴 했었다.\n\n\n\n마지막 날에 발표를 마치고 Lecturer분들의 ML/DL 트렌드에 관한 짧은 발표와 같이 수업들은 분들과 약간의 담소를 나누며 훈훈하게 수업이 끝났다. 약 3주간동안 저녁 시간에 열심히 배웠던 수업이 막을 내렸다.\n항상 가장 괴로울 때 가장 많이 배우는 것 같다. 사실 지나고 보면 그 고통이 어느정도 미화되는 것도 있고 위에 ‘어렵지 않았다.’, ’잘 마무리됐다.’라고 써놓기도 했지만, 그걸 하고 있던 순간에는 힘들었다. 아무리 알고 있던 내용이들이라도 그 내용들을 다시 다지는 것도 어렵고, 직접 코드를 써보는 것도 어렵고, 마무리 하기까지 불확실성과 두려움 때문에 괴로웠던 건 팩트다. 근데 항상 그러면서 배우는 것 같다. 그래서 이번 코스를 통해 배운 걸 정리해보자면,\n\nML은 확실히 DL보다 수학적인 이론이 어려웠음\n영어가 시험이나 부담이 아닌 ’언어’라는 것. 나를 표현할 수 있는 툴이라는 것\n전에 조금이라도 공부했던 것들이 나중에 도움이 ’크게’될 수 있다는 것\n내가 알고있는 것들을 꾸준하게 포스팅하는 습관을 길러야 하겠다는 생각\n해보지 않고 추측만으로 하는 생각들로 두려워하지 말 것\n\n수업을 마치고 certificate를 받았다. 사실 certificate에는 옵션이 있었는데, grade를 받을 것인지/pass or fail을 받을 것인지 내가 선택할 수 있었다. 등록 당시엔 근자감으로 grade를 받겠다고 했다가 수업 듣는 동안 과거의 나를 반성했다. 사실 수업하는 동안 그렇게 적극적이지도 않았고, 프로젝트 발표를 하고 지적을 받으면서 든 생각은 ‘최고 grade까지는 못받고 중간이라도 했으면..’ 했다. 그런데 생각지 못했던 최고 grade를 받을 수 있었다. 학점이 다가 아니긴 하지만 그래도 최선을 다했다는 것을 어느정도 뒷받침해줄 수 있기 때문에 뿌듯했다. 그래서 위에 배운 점들에 하나 더 추가하자면, ’최선을 다해서 어찌저찌해보면 뭐라도 한다.’라는 것. 식상한 멘트와 교훈이지만 매번 도전하면서 느끼는 바이기도 하다. 이렇게 2021년도 1월은 TU Berlin의 Machine Learning using Python: Theory and Application 코스와 함께 시간을 보냈다."
  },
  {
    "objectID": "posts/note/2024-12-19-init-vs-call.html",
    "href": "posts/note/2024-12-19-init-vs-call.html",
    "title": "📝__init__ VS. __call__",
    "section": "",
    "text": "Monkey Path를 공부하다가 한번 정리하면 좋을 것 같아 Python의 __init__과 __call__에 대해서 정리하고자 합니다.\n\n__init__은 인스턴스 초기화 시 실행\n__call__은 인스턴스 호출 시 실행\n\n\nclass A:\n\n    def __init__(self):\n        print('init')\n\n    def __call__(self):\n        print('call')\n\n    def myfunc(self):\n        print('my')\n\nprint(\"==== OUTPUT ====\")\n\na = A()\n\na()\n\na.myfunc()\n\n==== OUTPUT ====\ninit\ncall\nmy\n\n\n\nDecorator\n\n데코레이터는 자신이 수식할 함수나 메소드 내부에 받아 놓아야 함. 그러기 위해서 __init__에 데이터 속성 저장.\n데코레이터가 하는 일은 함수를 대리 호출.\n\n\nclass MyDecorator:\n    def __init__(self, data):\n        self.storage = data\n\n    def __call__(self):\n        print('data entered :', self.storage.__name__)\n        self.storage()\n        print('data exited :', self.storage.__name__)\n\n@MyDecorator\ndef printer():\n    print('I print the empty space.')\n\nprint('==== start ====')\nprinter()\n\n==== start ====\ndata entered : printer\nI print the empty space.\ndata exited : printer"
  },
  {
    "objectID": "posts/note/2023-07-05-daily-english-009.html",
    "href": "posts/note/2023-07-05-daily-english-009.html",
    "title": "🌎Casual English Phrases 009",
    "section": "",
    "text": "지지하다/응원하다\nroot for sm/sth\n\nto support or encourage somebody in a sports competition or when they are in a difficult situation\n\n\nMost of the crowd were rooting for the home team. 대부분의 관중들이 홈팀을 응원하고 있었다.\nGood luck, I’m rooting for you! You’ll be amazing. 행운을 빌어, 난 널 믿어! 넌 최고야.\nHis whole hometown was rooting for him as he made his professional boxing debut on live television. 그의 고향 사람들은 그가 생방송으로 전문 복싱 데뷔를 하는 것을 응원했습니다.\nI’ve always rooted for the company to succeed, since they made some of my most cherished games growing up. 나는 어릴 적부터 그 회사가 성공하기를 항상 바래왔습니다. 그들은 내가 가장 소중히 여기는 게임들을 만들어왔기 때문이죠.\n\n\n\n너 T야?(2nd meaning)\ntone-deaf\n\n\nunable to perceive public attitudes or preferences\nlacking emotional insight; insensitive or unsympathetic to others\n\n\n\nThey’re tone-deaf to the opinions of local people; they’re telling them where the new hospital will be without listening to them. 그들은 현지 주민들의 의견을 무시하고 있습니다; 현지 주민들의 의견을 듣지 않고 새로운 병원이 어디에 위치할 것인지 말하고 있습니다.\nThe council’s politically tone-deaf plan would cost lower income residents an extra 100 dollars a year. They simply can’t afford it. 시의 여론을 반영하지 않는(정치적으로 귀담아 듣지 못하는) 계획은 저소득 주민들에게 매년 추가로 100달러의 비용이 들게 할 것입니다. 그들은 당연히 그 비용을 감당할 수 없습니다.\nShe is often tone-deaf to her daughter’s needs. 그녀는 가끔 딸의 도움(정서적 교류의 필요)에 귀기울이지 못한다.\n\n\n\n내 손바닥 안이야\nhave someone in one’s pocket\n\nto have complete control over someone\n\n\nDon’t worry about the mayor. She’ll cooperate. I’ve got her in my pocket.\nJohn will do just what I tell him. I’ve got him and his brother in my pocket.\nI hear that the boss has half the police force in his pocket."
  },
  {
    "objectID": "posts/note/2023-03-29-daily-english-005.html",
    "href": "posts/note/2023-03-29-daily-english-005.html",
    "title": "🌎Casual English Phrases 005",
    "section": "",
    "text": "그거 참 위로가 되네\nWell, that’s very comforting\n\nThat’s very comforting↘️. Thanks a lot. 위안이 되네. 진짜 고마워\nThat’s very comforting↗️. 하나도 위로가 되지 않네요.\n\n\n문맥과 강약을 어떻게 주느냐에 따라 같은 말도 비꼬는 말이 될 수 있다.\n\n\n\n자는 것도 점점 더 어려워지네.\nEven sleeping is getting too complicated.\n\nEven meeting people is getting too complicated. 심지어 사람들 만나는 것도 점점 어려워져\nEven expressing my own feeling literally is getting too difficult. 내 감정을 말 그대로 표현하는 것조차 어려워지고 있어\n\n\n\n실수로\nby mistake\n\nThis filter paper I bought by mistake has only two holes. 실수로 리필 용지는 2구짜리로 샀거든요\nI made a reservation for tomorrow by mistake. 실수로 예약을 내일로 잡았어\n\n\n\n복수하겠어\nI’ll get you for this\n\nAs soon as I find out what’s going on, I’ll get you for this. 무슨 일인지 알아내기만 하면 반드시 복수하겠어\nHow dare you! I’ll get you for this. 어떻게 니가 그럴 수 있어! 내가 갚아줄거야"
  },
  {
    "objectID": "posts/note/2022-09-04-mid-retrospective.html",
    "href": "posts/note/2022-09-04-mid-retrospective.html",
    "title": "📘2022 상반기 회고",
    "section": "",
    "text": "이번 post는 2022년도 상반기 회고록에 관한 내용을 담고 있습니다. 지난 8월 25일에 있었던 글또콘 후기와 글또를 처음 시작했을 때의 다짐을 기반으로 회고를 해보았습니다."
  },
  {
    "objectID": "posts/note/2022-09-04-mid-retrospective.html#section",
    "href": "posts/note/2022-09-04-mid-retrospective.html#section",
    "title": "📘2022 상반기 회고",
    "section": "1",
    "text": "1\n글또에서 활동하기 시작하면서 개인적으로 Github의 커밋도 되도록이면 매일하는 목표를 이루고 싶었는데 상반기에 열심히 노력했으나 벌써 몇몇 빈구석이 있습니다. 최근에 1001일 매일 커밋하신 분을 봤었는데 하반기에는 더욱 더 노력을 해보려고 합니다. 더욱더 부지런하게..!"
  },
  {
    "objectID": "posts/note/2022-09-04-mid-retrospective.html#section-1",
    "href": "posts/note/2022-09-04-mid-retrospective.html#section-1",
    "title": "📘2022 상반기 회고",
    "section": "2",
    "text": "2\n글의 퀄리티는 항상 신경쓰지만 쉽게 올리기 힘든 것 같습니다. 개인적으로 글의 퀄리티라는 것은 내가 타겟으로 하는 독자들의 만족감이라고 생각하는데 우선 스스로 독자가 되어 되돌아봐도 아직 모자른 내용들이기에 아직 많이 모자른 것 같습니다. 개인 기록용으로 처음에 목표를 낮게 잡긴했지만 그래도 좀 더 욕심을 내보아서 다른 누군가에게 도움이 되고 이후에는 입소문도 날 수 있는 블로그가 되기를 기대해봅니다. 또한 글의 콘텐츠는 사실 하고 싶은 것이 많지만 하반기에는 특별히 지금까지 작성해보지 않았던 코드와 함께하는 포스팅을 도전해보려고 합니다."
  },
  {
    "objectID": "posts/note/2022-09-04-mid-retrospective.html#section-2",
    "href": "posts/note/2022-09-04-mid-retrospective.html#section-2",
    "title": "📘2022 상반기 회고",
    "section": "3",
    "text": "3\n삶의 풍요로움과 멘탈관리를 위해 조만간 특별한 취미생활을 시작할 것 같은데 글또 커뮤니티에 용기내서 공유할 수 있었으면 좋겠습니다. 사실 지금 글을 작성하면서도 이 취미생활을 할 수 있을까 많이 고민되기 떄문에 구체적으로 적진 않고 Slack에서 인사드리겠습니다.\n\n\n\n\n\n\n이번 포스팅에서는 글의 서두에 적은 것처럼 일기처럼 2022년도의 상반기를 한가닥 묶어보았습니다. 연말에 이글을 또 다시 읽어보며 조금 더 성장해있기를 바랍니다. :)"
  },
  {
    "objectID": "posts/note/2025-03-02-presentation-review.html",
    "href": "posts/note/2025-03-02-presentation-review.html",
    "title": "📘글또 발표 후기",
    "section": "",
    "text": "2025 데이터/AI 글또 반상회에서 미완성 로보틱스 AI 엔지니어 이야기: Not Cool, Not Chic, Not Chill이라는 제목으로 발표하게 된 일련의 이야기를 이번 포스팅에 남겨보려고 합니다.\n\n동기\n올해를 시작하면서, 으레 모두가 새해의 목표를 정하듯이, 저도 2025년에 도전해볼 무언가를 찾고 있었습니다. 나름 인생의 모토가 Stop Wishing, Start Doing인데 최근에는 도전하는 일들이 없다고 생각이 들었습니다. 그래서 아주 가끔씩 생기는 근거 없는 자신감과 용기를 얻어 머릿속으로만 내가 바라는 걸 생각하지 말고 질러보자라는 생각과 함께 도전 목록들을 생각하고 있었습니다. 마침 글또에서 반상회 발표자 모집을 하고 있었고, 이 기회가 나에게는 좋은 레벨업 스테이지가 될 수 있겠다는 생각이 들어서 바로 지원했습니다. (아직까지는) 글또가 10기가 마지막이라는 아쉬움도 있었고, 이제는 학교에서 벗어나 딱히 어디에서 앞으로 나와 발표할 기회가 없었기에 좋은 기회라는 생각이 들었습니다. 오랜만에 발표 실력도 일깨울겸 나의 이야기도 정리해볼겸 겸사겸사 저에게 주어진 몇 안되는 기회중에 하나라는 생각에 들뜬 마음으로 지원했던 것 같습니다.\n조금 더 솔직하게 발표 지원 동기를 이야기하자면, 저는 학교를 졸업하고 사회에서 일을 시작하면서 정체성에 대한 혼란을 많이 겪었습니다. 연구자라고 하기엔 애매하고, 엔지니어라고 부르기에도 어딘가 모호한. 로보틱스를 한다고 하기엔 부족하고, AI를 한다고 하기에도 어중간한. 회사에서도 저의 역할을 명확하게 정의하기 어려웠고, 모든 사회초년생이 그렇듯 제 실력과 역할에 대한 의구심까지 겹치면서 혼란스러운 시기를 보냈습니다. (솔직히 말하면, 지금도 그 혼란이 완전히 사라졌다고는 자신 있게 말하기 어렵습니다.) 그때 스스로가 정리되지 않아 답답할 때, 어떻게 해결해왔더라? 과거를 돌아보니, 다른 사람들에게 제 이야기를 꺼낼 때 오히려 말하면서 스스로 답을 찾아갔던 순간들이 떠올랐습니다. 그래서 반상회 발표도 마찬가지였습니다. 저를 잘 알지 못하고, 쉽게 판단하지 않을 사람들 앞에서 솔직하게 제 이야기를 풀어놓는다면, 답을 찾을 기회가 되지 않을까 싶었습니다. 솔직히 고백하자면, 반상회 발표는 다른 사람들에게 도움을 주기 위한 목적보다는, 오히려 저 스스로를 끄집어내기 위한 하나의 도구였습니다.\n\n\n준비\n야심찬 동기와 발 빠른 지원 태도와는 달리, 막상 준비를 시작하면서부터 많이 삐걱거리기 시작했습니다. 애초에 발표의 목적은 내 이야기를 꺼내서 스스로 정체성을 확립해보자는 것이었는데, 초안을 작성하고 구성을 고민하다 보니 한 가지 의문이 들었습니다. 사람들이 과연 내 이야기를 듣고 싶어할까? 이 생각이 들자마자, 이야기를 꺼내는 것 자체가 망설여지기 시작했습니다. 이야기의 시작점을 어디로 잡아야 할지, 어디까지 이야기해야 할지, 이러다가 괜히 삼천포로 빠지는 건 아닌지, 스토리의 흐름이 이게 맞는 건지… 머릿속에서 콘텐츠들이 둥둥 떠다니며 정리가 되지 않았습니다. 마치 질서 없는 물 위에서 표류하는 기분이었습니다.\n\n\n\n1달 조금 넘게 남았고, 어차피 내 이야기를 하는 발표이니 별다른 자료조사가 필요 없을 거라 생각했습니다. 그래서 시간 문제는 없을 거라 판단했지만, 불행히도 머릿속에서만 방황하는 데 3주 이상을 써버렸습니다. 발표 리허설을 준비하기 직전까지도 발표의 골자를 계속 수정하며 고민했고, 정작 중요한 내용보다도 발표 제목이나 중간에 넣을 밈(meme) 같은 부수적인 요소에 신경 쓰느라 시간을 허비하기도 했습니다. 그 결과, 준비 시간을 제대로 활용하지 못한 채 시간을 흘려보내고 말았습니다.\n그러는 사이 어느덧 반상회 준비위 분들 앞에서 온라인 리허설을 할 시간이 다가왔고, 그제야 이제 정말 정신 차려야 한다는 생각이 들었습니다. 동시에, 아직 이렇게 준비도 안 됐는데, 대체 왜 발표 지원을 했을까… 하는 후회도 밀려왔습니다. 결국 마음에 들지 않는 발표 자료를 꾸역꾸역 완성해 가며, 그때의 조급함과 혼란스러움을 온몸으로 느꼈던 기억이 납니다. 지금까지 수도 없이 발표를 해왔는데 이렇게까지 자신감이 없었던 적이 처음이었습니다.\n\n\n\n그럼에도 불구하고, 항상 위기를 기회로 만들어주는 글또 분들의 도움 덕분이었을까요. 저는 정말 복이 많은 사람인 것 같습니다. 늘 주변에 대단하고 따뜻한 분들이 계셨고, 이번에도 그 손길 덕분에 무너져가던 마음을 다잡을 수 있었습니다. 발표가 정리되지 않았고, 시간 체크도 한 번도 해보지 않은 상태였으니 리허설이 잘 나올 리 없었습니다. 그런데도 다들 진지하게 제 발표를 들어주시고, 어떤 부분을 개선하면 좋을지 함께 고민해 주셨습니다. 심지어 체크할 수 있는 지표까지 제안해 주시며 혼란스러웠던 제 멘탈을 붙잡아 주셨죠.\n\n\n\n사실 발표를 준비하면서 가장 망설였던 부분이 과연 내 이야기를 듣고 싶어할까?라는 의문이었습니다. 생각해보니 지금까지 발표들은 논문/이론 설명이나 연구 발표 등 정보 전달성이 큰 발표들이라 자료조사와 이해해서 자료를 준비하는게 힘들었지 내용을 구상하는게 힘들지 않았었습니다. 하지만 이번에는 내 이야기를 한다는 측면에서 애초에 발표의 컨텐츠 자체가 망설여지고 갈피가 잡혀지지 않으니 그 고민에 너무 오래 머물러 있었는데, (비록 격려 차원에서 배려의 말씀일 수도 있었겠지만) 이야기를 더 풀어줬으면 좋겠다, 충분히 흥미롭다는 피드백을 들으며, 마치 멈춰 있던 머릿속에 다시 재생 버튼이 눌리는 기분이었습니다. 게다가 중간중간 따뜻한 격려도 해주셔서, 자신감을 잃어가던 저에게 정말 큰 힘이 되었습니다.\n리허설을 마치고 나니 발표의 갈피가 조금씩 잡히기 시작했습니다. 제목도 수정하고, 발표의 방향도 다시 고민해보면서 내 이야기를 편하게 풀어가되, 로보틱스 AI 엔지니어라는 아이덴티티를 내가 만들어가고 싶은 것이니, 이 부분을 좀 더 집중해서 소개해보자는 생각이 들었습니다. 그래서 발표 구성을 다시 정리했습니다. 내 개인적인 이야기는 후반부에 배치하되, 조금 더 사람 냄새가 나는 이야기들을 첨가하기로 했습니다. 그리고 발표 초반에는 로보틱스 AI 엔지니어라는 개념이 생소한 사람들에게 도움이 될 만한, 초심자를 위한 정보성 내용을 담기로 했습니다. 이렇게 발표의 큰 틀을 정리하고 나니, 그때부터는 발표 자료를 다듬는 시간이 더 이상 괴롭지 않았습니다. 오히려 열정을 불태우며 즐겁게 준비할 수 있었던, 정말 시간이 지나가는 지도 모르게 재밌는 시간이었습니다.\n(덧. 사실 회사에서 우연히 한 동료분께서 저에게 로보틱스 개념에 대한 질문을 던지셨는데, 그 질문에 대한 답을 정리하면서 발표 자료를 다듬는 데 큰 도움이 되었습니다. 실명을 거론할 순 없지만, 항상 우연하게도 도움을 주시는 분들이 주변에 있다는 게 감사할 따름입니다. 그 마음을 담아 이곳에 간략히 남겨봅니다.)\n\n\n반상회와 발표\n드디어 데이터/AI 엔지니어 반상회 당일이 되었습니다. 퇴근을 하며 설렘 반, 두려움 반의 마음으로 행사장으로 향했습니다. 혹시 미리 준비할 게 있을까 싶어 일찍 도착했고, 긴장을 풀 겸 스스로에게 응원의 의미로 블루보틀에서 비싼 따뜻한 라떼를 한 잔 시켰습니다. 머릿속으로 발표 내용을 되새기며 마음을 다잡은 채, 천천히 발표장으로 들어섰습니다. 그때, 글또 반상회 준비위 분들이 마련해주신 타로 초콜릿을 받았는데, 그 안에 시킨 대로 한다면이라는 메시지가 적혀 있었습니다. 그걸 보는 순간, “그래… 준비한 대로만 하면 된다.” 그렇게 되뇌며 마음을 다잡았던 것 같습니다.\n\n\n\n발표 시간은 생각보다 떨지 않고, 오히려 편하게 이야기할 수 있었습니다. 그리고 후에 알게된 사실이지만 시간도 딱 맞춰서 발표해서 정말 다행이다라고 생각했습니다. 아마도 청중이 글또 분들이었기 때문에 자연스럽게 편안함을 느꼈던 것 같습니다. 준비하는 동안 함께해주셨던 분들, 글또에서 만나뵈었던 분들, 익숙한 얼굴들이 발표장 곳곳에서 보여서 더욱 안심이 되었던 것 같습니다. 혹시 이 포스팅을 보시면서 제 발표자료가 궁금하신 분들은 여기에서 발표 자료들을 확인하실 수 있습니다. :)\n\n\n\n그렇게 발표를 마치고 나서는, 반상회의 다양한 이벤트들을 마음껏 즐길 수 있었습니다. 그룹 커피챗을 비롯한 여러 프로그램들이 정말 재미있었고, 저뿐만 아니라 멋진 사이드 프로젝트를 공유해주신 장회정님의 발표도 (제 발표가 끝났다는 안도감 덕분에 더욱 편하게) 흥미롭게 들을 수 있었습니다. 또한 8조 그룹에서 만난 모든 분들과 웃으며 편하게 이야기를 나누며, 정말 즐거운 시간을 보낼 수 있었습니다. 그렇게 글또 10기 데이터/AI 반상회는 저에게 또 하나의 잊지 못할 하루로 남게 되었습니다.\n\n\n\n\n\n후기\n본격적인 후기를 시작하기 전에, 반상회 후 설문지에 남겨주신 글또 분들의 따뜻한 발표 후기를 먼저 공유하려 합니다. 이 감사한 글들을 올려두고, 앞으로도 두고두고 꺼내 보며 힘을 얻으려 합니다.(참고로, 중간에 제가 저 스스로에게 쓴 코멘트가 이스터에그처럼 숨어 있으니, 한번 찾아보시는 것도…! 😉) 실제로 발표에서 이야기했던 것처럼, 로보틱스에서 활용하는 AI에 대해 잘 모르셨던 분들이 흥미와 관심을 가지게 되었다는 점에서 무척 뿌듯했습니다. 그리고 사실, 이번 발표의 또 다른 목표였던 제 스스로의 정체성을 좀 더 다지는 것도 이루었다는 생각이 들어 더욱 의미 있는 시간이었습니다.\n\n \n\n글또를 7기부터 시작하면서 지금까지 글또라는 커뮤니티에서 활동하며 저는 개인적으로 가장 많은 특혜를 받은 사람이라고 생각합니다. 글또 지속 기간으로 따져도 가장 오래한 것도 아니고, 가장 글을 많이 썼는가?의 기준으로 보아도 한번도 빠뜨리지 않고 작성한 사람도 아니지만, 저 스스로 글또에서 글을 쓰며 성장해온 자신이 가장 자랑스럽기 때문에 가장 많은 특혜를 받았다고 생각합니다. 어쩌면 가장 많은 이라는 수식어의 비교 대상은 글또의 다른 분들이 아닌, 글또가 아닌 다른 기회 등으로 성장한 것보다 가장 나를 다져준 도구가 글또라는 반증이라서, 비교 대상은 내가 나를 끌어올린 성장의 도구들 중 글또가 가장이라고 볼 수 있을 것 같아요.\n이렇게 또 하나의 도전을 실행했고, 다행히도 좋은 경험으로 남아 앞으로도 큰 자양분이 될 것 같아 뿌듯합니다. 혹시라도 (글또가 지속되고) 나중에 글또 반상회가 다시 열려, 발표를 망설이는 글또 분들이 계신다면, 이 글이 작은 용기가 되었으면 좋겠습니다. 아니면, 꼭 글또가 아니더라도 자신의 이야기를 하는 것을 망설이는 모든 분들에게도 전하고 싶은 응원입니다. “이렇게 준비가 부족했던 사람도, 충분히 성장할 기회로 삼아 발표를 지원했고, 결국 잘 마무리할 수 있었구나.” 그렇게 보면서, 저처럼 한 걸음 내딛는 계기가 되었으면 좋겠습니다.\n그리고, 정말 마지막으로… 수고한 나에게 박수를! 👏👏👏"
  },
  {
    "objectID": "posts/note/2023-07-07-geultto-8th-end.html",
    "href": "posts/note/2023-07-07-geultto-8th-end.html",
    "title": "📘Geultto 8th End",
    "section": "",
    "text": "2번째로 참여한 글또 8기에서 마지막 글을 쓰는 날이 되었습니다. 지금까지 있었던 일들을 되돌아보며 마무리 회고를 해보려고 합니다. 이전에 회고할 때는 활동을 시작하며 작성했던 다짐글을 보며 그때 목표했던 것들을 잘 이루었는지 점검하는 방식으로 했었지만, 이번에는 자유롭게 활동했던 이야기들 중심으로 회고해보려고 합니다. 글또라는 공동체에서 했던 활동들을 주로 돌아보며 작성하겠지만 2023 상반기에 있던 개인의 일상사와 생각도 자연스럽게 담기는 회고가 될 것 같습니다.\n매번 시작할 때 예치금을 넣어놓고 예치금을 절대 까먹지 않으리라 했던 목표는 이뤘습니다. 사실 다짐글에서는 pass권도 안쓰고 모든 제출을 하는 것이 목표였지만 2번의 pass도 다 쓰고, 추가적으로 한번은 제출을 하지 못해서 만원 차감이 되었었습니다. 다행히도 커피드백 환급금으로 만원 손실은 매꿀 수 있었고 결과론적으로는 예치금은 사수할 수 있었습니다. 간단한 활동 결산표이지만 이번 글또 8기에서의 제 활동을 함축적으로 잘 보여주는 것 같습니다."
  },
  {
    "objectID": "posts/note/2023-07-07-geultto-8th-end.html#good-stories",
    "href": "posts/note/2023-07-07-geultto-8th-end.html#good-stories",
    "title": "📘Geultto 8th End",
    "section": "Good Stories",
    "text": "Good Stories\n\nWriting\n역시 글또의 본질, 글쓰기 활동에 대한 점검은 가장 중요한 부분이라고 생각합니다. 블로그 글을 쓰면서 1차로는 당연히 나에게 도움 되는 글을 쓰는 게 목적이었고 2차로는 다른 사람들에게도 도움이 되고 잘 읽힐 수 있는 글이 되는 것이 목표였습니다. 사실 이 생각은 이전에 7기를 마무리하며 썼던 글에서 이야기했던 것처럼 연구자로써 더 좋은 글을 쓰고 싶은 마음이 처음 동기가 되어 지금까지도 변함없는 생각입니다. 이전에 썼던 글들과 다르게, 이번 글또 활동에서부터 정말 청중들에게 발표하듯이 작성해보자라는 생각을 했었기 때문에 개괄식/~이다 문체를 피하고 서술식/~합니다 문체로 작성하는 연습을 했었습니다. 글의 길이가 길어지고 핵심까지 도달하는 시간이 좀 걸리는 글이지만 전체적인 독자의 이해도를 높일 수 있기 때문에 결과적으로 문체를 바꿔서 작성한 글들이 더 좋다는 생각이 들었습니다. 개인적으로 바뀐 문체로 작성한 글들이 퇴고를 할 때도 더 좋았기 때문에 앞으로도 문체를 유지하면서 글을 작성하려고 합니다. 형식 뿐만 아니라 내용과 주제 측면에서도 현재 연구하고 있는 분야와 연관된 논문들과 코드들을 중점적으로 다루면서 원하는 주제들을 재밌게 작성할 수 있었습니다. 이번 8기에서 작성한 총 9개의 글들을 통해 확실히 글쓰는 능력이 한층 성장되었다는 것을 느낄 수 있었습니다.\n스스로 느끼는 성장 히스토리가 가장 중요하지만 다른 사람들에게 어떻게 제 글이 보이는지도 많이 궁금했었습니다. 다른 사람들이 내 글을 볼 때 어떨까?라는 궁금증을 이전 7기 때까지는 없었던 운영진 분들의 감사한 수고로 생긴 큐레이션이라는 파트를 통해 확인해볼 수 있었습니다. 큐레이션을 통해 다른 분야들의 좋은 글들도 볼 수 있었고 객관적으로 제가 쓴 글을 본 독자들의 생각도 알 수 있었습니다. 이번 8기 활동을 하면서 총 4개의 제 글들이 큐레이션이 되었다는 사실이 지금도 얼떨떨하긴 합니다. 큐레이션이 되었다는 사실 자체가 좋은 글이다 라는 절대적인 판단 기준이 될 순 없지만, 글또에 실력 좋고 대단하신 분들이 2주 마다 작성하시는 많은 좋은 글들 중에 조금이라도 눈에 띄었다는 사실에 제가 쓴 글들에 대해 좋은 인정을 받은 것 같아 기분이 좋았습니다.\n\nCurated 1: WASABI\nCurated 2: K-Accessibility for RL\nCurated 3: Github Starstruck 128\nCurated 4: Chord Graph\n\n\n\n\n4번의 큐레이션 선정\n\n\n\n\nNetworking\n커피와 함께한 feedback&chat\n글을 쓰는 습관과 능력을 기를 수 있다는 것이 글또의 주된 매력적인 포인트지만, 글또에서 만나는 멋진 분들이 글또의 또 다른 매력인 것 같습니다. 이번 기수에서는 총 3회의 커피드백에 참여를 했었고 함께하신 분들의 이야기들을 들으면서 배울 점들도 많았고 내 이야기도 나누면서 느끼는 점도 많아서 다 소중한 시간이었습니다. 매번 모임마다 2-3시간은 기본으로 각자의 삶과 진로를 고민하는 이야기들을 나누면서 때론 나만 힘든 게 아니라는 사실에 위로도 받고 힘도 얻었으며, 때론 머릿속에서만 뱅뱅 돌고 있던 고민들을 입 밖으로 꺼내 이야기하면서 새삼 내가 이런 생각들을 가지고 있었구나 놀랐을 때도 있었습니다.\n\n\n\n3번의 커피드백 모임\n\n\n1, 3회차 때는 캐쥬얼하게 각자의 삶의 이야기나 세상 이야기들을 나누었지만, 커피드백이라는 이름에 맞게 2회차 때는 이때까지 글을 쓴 것들을 살펴보며 서로 좋게 생각한 점, 보완하면 좋을 것 같은 점들을 나누었었습니다. 아마 제일 오랫동안 서로 피드백을 주고 받았던 것으로 기억하는데(한 4시간 정도) 감사하게도 같이 모인 모든 분들이 처음부터 끝까지 진지하게 하나하나 각자의 글들을 읽어보고 좋은 생산적인 의견들을 주셔서 어떻게 글의 질을 높일 수 있을 지에 대한 많은 인사이트를 얻을 수 있었습니다. 글또 커피드백 참여가 정말 소중하다는 생각을 했었습니다.\n글또에서의 또 다른 만남들\n사실 오프라인 커뮤니티 모임에서 동기부여를 얻는 것을 매우 좋아합니다. 내가 잘 모르는 분야라도 집중해서 듣다 보면 익숙해지면서 새로운 세상을 알게 되는 것 같아 재밌고, 해당 분야에 열정이 많은 멋진 연사님들이 하시는 발표를 들으면 나도 모르게 좋은 에너지를 받아서 자주 오픈 세미나에 참석하는 편입니다. 그런 저에게 글또에서 하는 채널별 반상회는 정말 기다리던 행사였고 기대했던 만큼 너무나도 좋은 데이터 통합 반상회를 운영진 분들과 반상회 준비위 분들이 만들어 주셔서 정말 감사했습니다. 각자의 분야와 자리에서 데이터, AI 직군/연구에 대한 고민을 치열하게 했던 이야기들을 들으면서 스스로의 한계점에 지쳐있던 참에 힘을 받을 수 있었고, 데이터로 보는 글또 발표에서는 재밌고 공감 되는 부분들도 많아 많이 웃으면서 발표를 들을 수 있었습니다.\n\n\n\n글또 반상회와 글또 안에서의 네트워킹\n\n\n커피드백과 반상회 같은 공식적인 일정 말고도 슬랙을 통해 많은 분들을 만날 수 있었는데, 소심이 I이지만 이번에는 저도 용기 내서 몇 번 활동을 참여하거나 호스트 역할을 했습니다. 해외 대학원/취업을 고민하는 분들을 만나 잠시 접어두었던 꿈과 도전에 대해 생각해볼 수 있었고, 막막했던 코딩 테스트 준비를 위해 스터디 활동도 하고, 혼자 나름 커스텀한 키보드 자랑도 해보고, 진로 고민하는 시기(현재 진행 중..)에 해외 취업도 미쳤다 생각하고 도전해보자는 마음에 공고를 내고 사람들을 모아 보기도 했습니다. 돌아보니 뭔가 제가 이것저것 많이 활동을 했다는 게 신기합니다. 물론 모든 모임에서 제가 생각한 대로 생산적인 결과나 마무리를 하지 못했거나 부족한 점들이 있지만 좋은 사람들을 만나는 용기가 저에겐 쉽지 않은 시도였기 때문에 이후에도 내가 유지해야 할 인생의 태도라고 생각이 들었습니다."
  },
  {
    "objectID": "posts/note/2023-07-07-geultto-8th-end.html#bad-stories",
    "href": "posts/note/2023-07-07-geultto-8th-end.html#bad-stories",
    "title": "📘Geultto 8th End",
    "section": "Bad Stories",
    "text": "Bad Stories\n좋아하는 노래 중에 AJR의 100 Bad Days가 있는데 가끔씩 너무 힘들고 내가 너무 초라해져 보이거나 후회가 되는 일들이 있을 때, 노래 가사를 새겨보며 마음을 다잡습니다. 노래에서 “나쁜 이야기들이 나중에 재밌는 파티에서의 썰을 풀 소재가 된다”라는 메세지가 너무 심각해져있는 마음의 답답함과 긴장감을 풀어주는 느낌이고 “나쁜” 이야기(경험)이 꼭 나쁘지만은 않다는 점이 위로가 정말 많이 됩니다. 그런 면에서 회고 파트에서 Bad Stories를 기록하는 부분도 훗날 재밌는 안줏거리 혹은 지금의 나를 만든 한 페이지의 증거로 소중하다 생각하며 적어봅니다.\n꾸준함에 대한 반성\n글또 다짐글을 다시보며 든 생각은.. 정말 하나도 지킨 다짐이 없다는 생각이 들면서 너무 부끄러웠습니다. 작성했던 다짐들을 살펴보면 미래의 나에 대해 높은 기대감을 가지고 높은 꾸준함을 요구하는 목표들을 나열했었더군요. 마치 당장의 내가 180도 달라질 것을 기대하는 것 같은 느낌이 들 정도로, 지금 생각해보면 목표를 너무 크게 잡은 것이 아닌가하는 생각이 들었습니다. “모든 글 제출”, “제출 기간 외에도 작성하는 꾸준한 습관 기르기”등 목표에서 나에게 바라는 꾸준함의 기준은 굉장히 높은데 이를 조금 낮추어서 하나만 지켰더라도 좋지 않았을까..라는 페이스 조절에 대한 반성을 하게 되었습니다. 누가 강제로 시킨 약속이 아니더라도 스스로에게 한 약속에 대해 좀 더 엄격해질 필요성을 느끼며 지금은 실패한 역사로 남지만 훗날 꾸준히 글을 쓰는 사람이 되어있길 바래봅니다. 글을 쓰는 꾸준함 뿐만 아니라 생활 속에 고치고자 했던 작은 습관들도 포기하지 않고 좋은 방향으로 꾸준히 변화해가는 힘을 기르고 싶네요.\n현재를 살아가기\n저는 사서 고생이 아닌, 걱정을 하는 타입입니다. 걱정도 많고 미래의 불확실성에 대한 두려움이 정말 많은 사람이기 때문에 현재에 집중하지 못해서 상반기에 하고자 했던 일들을 많이 하지 못했습니다. Stop wishing, start doing이라는 좌우명을 설정해서 행동하는 사람이 되자는 취지로 스스로 격려를 하지만, 여전히 필요 없는 생각들로 집중력이 흐트러지고 앞으로 나아가지 못하는 것 같습니다. 내가 오늘, 지금 이 시간에 해야 하는 것을 단순하게 정리하고 집중하는 사람이 될 수 있도록 하반기에 다시 생활 리듬을 돌아보고 해야 할 것과 하지 말아야 할 것 목록을 만들어서 지켜보겠습니다. 그리고 불안한 마음들을 적어보고 이 감정이 현재 내가 개선할 수 있는 action plan으로 바꿀 수 있는 것인지 점검하는 시간도 하루를 마무리하면서 점검을 해보겠습니다."
  },
  {
    "objectID": "posts/note/2025-01-05-Goodbye-2024.html",
    "href": "posts/note/2025-01-05-Goodbye-2024.html",
    "title": "📘Goodbye 2024",
    "section": "",
    "text": "2024 정말 잊지 못할 한해였습니다.\n사람이 살아가면서 많은 마일스톤들이 있고, 그 마일스톤을 한해에 하나 담기도 어려운 법인데 2024는 3가지 마일스톤들이 지나간 해였기에 가장 기억에 남지 않을까 싶습니다.\n하나 하나의 키워드가 굵직굵직해서 솔직히 지금도 이게 저한테 한해동안 일어난 일이었던가 어떨떨하긴 한데, 돌이켜 보면 정말 그렇습니다. 그렇기에 2024년도 회고야 말로 정말 쓸것도 많고 다시 되짚어 볼 것도 많은 것 같습니다. 가장 기억이 나지 않는 1월부터 달력을 짚어보며 회고를 시작해보겠습니다."
  },
  {
    "objectID": "posts/note/2025-01-05-Goodbye-2024.html#월",
    "href": "posts/note/2025-01-05-Goodbye-2024.html#월",
    "title": "📘Goodbye 2024",
    "section": "1월",
    "text": "1월\n12월 폭풍 전야와 같은 논문 심사를 마치고 마지막 논문 편집을 하며 시간들을 보내고 있었습니다. 2년 동안 다사다난했지만 결국 이 논문 한편을 위해 달려왔다는 사실을 다시 깨닫게 되는 시간이었고, 내 자신이 나름 대견하다고 생각하며 마지막 졸업 논문 편집을 달리고 있었습니다. 비록 졸업 논문이기에 많은 사람들이 읽고 피드백을 주는 연구적인 임팩트가 큰 결과물이라고 볼 수는 없지만, 석사 기간동안 내가 공부한 것들과 주장하고자 하는 것들을 정리하는 시간이 지금까지도 큰 거름이 되는 것 같습니다.\n졸업을 하고 한동안은 나에게 주는 휴식 선물로 취직을 바로 할 생각이 없없지만, 말할 수 없는 사정에 의해 바로 취준을 시작했습니다. 그래서 아직 공채 기간이 시작되지 않았기에 스타트업들 위주로 열심히 지원서들도 쓰기 시작했던 기간이었습니다. 물론 석사 연구 내용과 유관한 직무들을 찾긴했었지만 막상 사회생활 전선으로 나아가니 나를 어필할 부분이 많지 않아 당황스럽기도 하고 조금은 좌절스럽기도 했지만, 다른 측면으로 나의 성과들을 돌아보고 정리할 수 있어서 좋았던 것 같습니다. 취준을 하면서 가장 막막했던 점은 코딩 테스트와 같은 스킬적인 측면을 보는 것들이 스트레스가 심했었는데, 아무래도 지금까지 연구 코드만 작성해오던 사람이라 알고리즘 능력에서는 많이 부족하기도 하고 시간 제한이 있는 상황이 익숙하지 않았던 것 같습니다."
  },
  {
    "objectID": "posts/note/2025-01-05-Goodbye-2024.html#월-1",
    "href": "posts/note/2025-01-05-Goodbye-2024.html#월-1",
    "title": "📘Goodbye 2024",
    "section": "2월",
    "text": "2월\n2월은 졸업식이 있었습니다. 대학교 졸업때는 코로나 시기였기에 친구들도 초대하지 못하고 바로 입학할 대학원으로 복귀해야 하는 상황이어서 졸업식을 잘 즐기지 못했던 아쉬움이 있었습니다. 하지만 석사 졸업은 연구실에 단 한명 졸업에다가 친구들도 초대해서 진정한 주인공 놀이를 할 수 있었던 것 같습니다.\n\n\n\n1월부터 여기저기 열심히 알아보던 취준은 한 스타트업에 합격이 되면서 마무리가 되었습니다. 사실 대기업과 스타트업 사이에서 많은 고민들이 있었지만 조금 더 많은 권한과 다이나믹한 환경에서 일하고 싶다는 생각에 스타트업을 선택했었습니다. 직무와 회사 위치 등 나쁘지 않은 조건으로 3월달부터 일하게 되어서 급하게 동생과 함께 일본 여행을 떠났었습니다.\n\n\n\n급하게 준비하는 여행이라 여행을 준비하는 설레는 마음이 크진 않았지만 그동안 긴 시간 공부하느라 수고 많았다고 스스로에게 상을 주는 여행이었기에 행복했던 시간이었습니다. 생각해보니 대학생때 교환학생을 할 수 있는 기회도, 여행을 갈 수 있는 시기들도 코로나 때문에 다 놓쳐버리고 코로나가 조금 잠잠해졌어도 대학원생이라는 신분에 걸맞게(?) 자유롭지 못했기에 해외 여행은 꽤 오랜시간 동안 마음 한켠에 소원으로만 있었습니다. 가까운 일본 조차 한번도 가본적 없었기에 바로 비행기 티켓을 끊고 떠났던 마음은 정말 최고의 상이었던 것 같습니다."
  },
  {
    "objectID": "posts/note/2025-01-05-Goodbye-2024.html#월-2",
    "href": "posts/note/2025-01-05-Goodbye-2024.html#월-2",
    "title": "📘Goodbye 2024",
    "section": "3월",
    "text": "3월\n진짜 직장인이 되었습니다. 사실 초기에 회사에 적응하는 게 쉽지 않았지만 나이대가 비슷하고 에너제틱한 회사 분위기 때문에 즐거웠던 것 같습니다. 물론 대학원생일 때와 차원이 다른 월급에 신났던 것도 사실이지만요. 마치 신학기에 들뜬 설렘과 긴장감이 교차하는 듯한 느낌을 직딩으로써 느끼는 한달이었습니다."
  },
  {
    "objectID": "posts/note/2025-01-05-Goodbye-2024.html#월-3",
    "href": "posts/note/2025-01-05-Goodbye-2024.html#월-3",
    "title": "📘Goodbye 2024",
    "section": "4월",
    "text": "4월\n4월은 회사에서 워크샵을 다녀왔었고 생각보다 대학교 MT같은 분위기에 재밌게 즐길 수 있었던 것 같습니다. 사실 첫 회사가 대학원 같은 성격이었어서 마치 대학원을 다시 입학한 느낌이긴 했습니다. 그래도 회사이기에 내가 어떻게 기여할 수 있고 어떻게 평가받을 것인지 계속 긴장했던 것 같습니다.\n또한 4월에 20년 이상 알고지낸 친한 언니의 결혼식이 있었는데 정말 감회가 새로웠습니다. 4-5살때부터 알고지내온 언니의 결혼식을 마주하고 나니 저도 벌써 그런 나이가 되었구나 새삼 깨달았고, 지금까지 학교생활만 해온 나에게 이후 삶에 대한 진지한 고민도 하게된 날이었습니다. 졸업하고 직장까지 가지게 되었으니 마치 결혼이 다음 숙제인 것 같은 느낌.. 그리고 나는 그런 인생의 동반자를 만날 수 있을까.. 이런 저런 고민들이 머리속에 떠올랐던 것 같습니다. 각설하고, 언니의 결혼식에서는 진심으로 축하와 행복을 기도했습니다."
  },
  {
    "objectID": "posts/note/2025-01-05-Goodbye-2024.html#월-4",
    "href": "posts/note/2025-01-05-Goodbye-2024.html#월-4",
    "title": "📘Goodbye 2024",
    "section": "5월",
    "text": "5월\n5월은 가정의 달이기도 하고 많은 휴일들이 있어서 행복했습니다. 또한 직딩으로써 처음으로 어버이날에 부모님께 용돈을 드릴 수 있어서 뿌듯했습니다. 28살이 되어서야 드린다는 게 죄송하긴 했지만 그래도 지금이라도 드릴 수 있어서 다행이다 싶었습니다.\n5월에 갑자기 대학 동기를 만나게 된 날이 있었는데, 이 날 제 꿈에 대해 다시 복기해보게 되었었습니다. 연말 회고에 적을 정도로 인상이 깊은 날이었는데, 퇴근을 하고 그 친구를 만나는 날 예전의 빛나는 눈빛이 사라진 것 같다라는 말을 듣게 되었습니다. 사실 회사에서 어떠한 이유로 점점 지쳐가고 있었고 어떻게 타파해야할지 몰라 힘든 상황이었는데 그런 상황을 모르는 친구가 그 이야기를 하자 다른 사람을 통해 진짜 내가 지쳐있구나 확인할 수 있었습니다. 그래서 어떤 방향을 찾아봐야겠다는 능동적인 태도를 취하게 되었고 나를 위한 다른 자리를 찾아보고자 이직을 준비하기 시작했습니다. 회사에서 무슨 보람을 찾는 거냐, 낭만을 쫓는 거 아닌가 싶을 수도 있었지만 친구의 말을 통해서 다시 마음속에 열정을 찾고 싶다는 생각이 들었던 것 같습니다."
  },
  {
    "objectID": "posts/note/2025-01-05-Goodbye-2024.html#월-5",
    "href": "posts/note/2025-01-05-Goodbye-2024.html#월-5",
    "title": "📘Goodbye 2024",
    "section": "6월",
    "text": "6월\n첫 회사에서의 첫 결과물을 내는 시기였습니다. 자세히 말할 수는 없지만 처음으로 사회에서 내가 참여한 프로젝트의 결실을 만들어 낸다는 사실이 많이 부담되기도 하고 정말 잘하고 싶다는 욕심도 많이 들어갔던 것 같습니다. 그 과정에서 미숙한 마음과 태도로 결과의 성패와 상관없이 실망한 부분들이 있었고 프로젝트가 마무리가 되는대로 퇴사를 하겠다는 의사를 전달하게 되었습니다. 퇴사와 별개로 마지막까지 제 역할을 잘 마무리하고 나온 것 같아 아쉬움은 없었습니다. 이때 얻은 교훈들은 지금도 많이 도움이 되고 있습니다.\n사실 이직하는 곳을 어느정도 정해놓았었기 때문에 이후 돈을 벌 걱정은 크지 않았습니다. 다만 이직을 처음하다보니 당황스러웠던 점들도, 두려웠던 점들도 많았습니다. 내가 할 수 있는 역할들과 별개로 다른 사람들과 같이 일한다는 건 많이 다르구나를 느꼈던 것 같습니다. 처음에는 이런 점들에 능숙하지 못한 스스로가 실망스러웠지만 이렇게 배워가며 성장하는 거겠지..라고 지금은 생각하고 있습니다."
  },
  {
    "objectID": "posts/note/2025-01-05-Goodbye-2024.html#월-6",
    "href": "posts/note/2025-01-05-Goodbye-2024.html#월-6",
    "title": "📘Goodbye 2024",
    "section": "7월",
    "text": "7월\n7월 중순까지 다니고 나서 퇴직을 하고, 졸업하고도 많이 놀지 못했으니 한달은 놀아야지! 하고 국내에서 전시회등을 돌아다니며 휴식을 취했었습니다. 버킷리스트 중 하나가 혼자 여행하는 것이 있었는데 서울 종로 한옥 숙소에서 약 4일간 여행을 했습니다.\n\n\n\n서울 태생이지만 바빠서 서촌과 같은 한옥의 정취를 느낄 수 있는 곳들을 가보지 못했었는데 이번 기회에 한옥 숙소에서 맘껏 즐기며 나를 돌아보는 시간을 가졌었습니다. 퇴사와 이직을 하면서 스스로도 단단하지 못함을 느꼈었는데, 왜 그랬는지 어떤 상황에서 내가 어떤 모습이었는지 등등 많은 생각들을 할 수 있었습니다. 이 때 내린 결론은 내가 하는 직무에서 너무 이상적인 Hero를 찾을려고 하지 말자는 결론을 내렸습니다. 주니어고 신입이니까 라는 이유로 내가 하는 직무에서 나를 리드해줄 누군가를 계속 찾느라 내 에너지가 고갈되어가고 있는 것 같았습니다. 사실 로보틱스 + AI를 하는 직무 자체가 신기술이고 빛나보여서 멋져보일지는 몰라도 실제 현장과 산업에서 프로젝트를 성공시킨 사례, 성공시켜본 경험이 있는 사람은 많지 않은 것 같습니다. 그래서 내가 보고 배울 수 있는 표본을 찾기 보다는 결국 내가 처음 찾아서 만들어가야 하는구나를 깨달았던 것 같습니다. 그래서 갑자기 뜬금 없지만 최애 밴드인 The Score의 Don’t need a hero 라는 노래가 이 교훈을 잘 담고 있는 노래라는 생각이 드네요."
  },
  {
    "objectID": "posts/note/2025-01-05-Goodbye-2024.html#월-7",
    "href": "posts/note/2025-01-05-Goodbye-2024.html#월-7",
    "title": "📘Goodbye 2024",
    "section": "8월",
    "text": "8월\n두번째 회사에 출근을 시작했습니다. 분위기와 사람들이 첫회사와 정말 정반대였습니다. 우선 거리부터 판교로 거의 2시간 출근 시간을 확보해야 한다는 것도 이전 회사가 40분 거리였다는 점이라 정말 달랐습니다. 마치 밸런스 게임의 정반대 선택지만 다 고른 것처럼 분위기, 사수, 일하는 방식.. 등등 완전 새로웠던 것 같습니다. 두번째 회사는 지금까지 잘 다니고 있습니다 :)"
  },
  {
    "objectID": "posts/note/2025-01-05-Goodbye-2024.html#월-8",
    "href": "posts/note/2025-01-05-Goodbye-2024.html#월-8",
    "title": "📘Goodbye 2024",
    "section": "9월",
    "text": "9월\n9월에는 점점 다가오는 시연날과 함께 현장도 바쁘게 오가며 열심히 배우고 열심히 일했던 것 같습니다. 첫회사에서도 그랬지만 입사하고 거의 3개월 내에 항상 시연해야하는 일이 있었습니다. 그래도 팀원들과 사수의 좋은 팀워크로 일하는 동안 외롭지는 않았었습니다."
  },
  {
    "objectID": "posts/note/2025-01-05-Goodbye-2024.html#월-9",
    "href": "posts/note/2025-01-05-Goodbye-2024.html#월-9",
    "title": "📘Goodbye 2024",
    "section": "10월",
    "text": "10월\n10월.. 잊지 못할 시연이 있었습니다. 시연 전날 새벽까지도 제가 맡은 부분이 잘 작동되지 않아 완전히 멘탈이 나갔었지만 팀장님과 팀원들의 도움과 끝까지 놓을 수 없었던(?) 멘탈을 붙잡아 결국 성공을 시켰습니다. 지금도 그 시연날을 생각하면 아찔하긴 하지만 결과적으로 좋은 결과를 냈기에 지금은 웃으면서 회고를 적을 수 있습니다. 원래 10월은 제 생일있는 달이기에 가장 좋아하는 달인데 24년도 10월은 시연 때문에 순간 최악의 달이 될 뻔 했습니다😅\n\n\n\n생일 말고도 또 한가지 경사가 있는 10월이었습니다! 석사 과정을 시작하면서 시작했던 4족 보행 로봇관련 연구자료들을 모은 awesome list repository가 512개 star를 달성했습니다! 사실 512개까지 모을 수 있을지 모르고 이전에 포스팅으로 자축을 다 한 상태였지만 이후로도 스타가 많이 늘어서 너무 기뻤던 하루였습니다.(현재 회고를 작성하고 있는 순간에는 벌써 606개의 스타가..!)"
  },
  {
    "objectID": "posts/note/2025-01-05-Goodbye-2024.html#월-10",
    "href": "posts/note/2025-01-05-Goodbye-2024.html#월-10",
    "title": "📘Goodbye 2024",
    "section": "11월",
    "text": "11월\n11월은 조금 한숨 쉬어가는 한달이었는데 지금까지 일과 공부에만 전념했던 생활에 조금 tweek을 준 기간이었습니다. 인간관계에 공부를 하는 시간을 가졌었는데 공개되는 회고에 적기에는 조금 민감할 수 있어서 이 부분은 개인적인 다이어리에만 쓰려고 합니다.\n직무적으로는 조금씩 자기효용감이 생기기 시작했던 것 같습니다. 두번째 회사에서의 적응도 어느정도 된 상태에서 내가 어떤 역할을 할 수 있는지, 그리고 회사에서 어떤 점을 바라고 있는지 align을 하면서 내가 성장할 수 있겠다라는 확신이 생겼었습니다. 특히나 회사에서 얻기 쉽지 않은 기회인 Facebook Meta와 협업하는 프로젝트에 내가 involve 할 수 있다는 사실에 감사하며 내가 할 수 있는 부분이 무엇인지 더 능동적인 태도를 가질 수 있게 되어서 좋았던 것 같습니다."
  },
  {
    "objectID": "posts/note/2025-01-05-Goodbye-2024.html#월-11",
    "href": "posts/note/2025-01-05-Goodbye-2024.html#월-11",
    "title": "📘Goodbye 2024",
    "section": "12월",
    "text": "12월\n연말이 다가오는 12월에 크리스마스 분위기를 한껏 느끼며 시간을 보냈었습니다. 첫번째 회사도 5개월, 두번째 회사도 5개월째 되는 달이라서 스스로 각 회사에서 느낀점이 어떤 부분에서 다른지 짚어보았습니다. 그 과정에서 내가 어떤 부분에서 취약하다고 느끼는지 파악해볼 수 있었습니다.\n12월에 갑자기 친구들의 추진력 덕분에 필리핀 여행을 다녀왔습니다. 딱 24년도 말일까지 필리핀에 있는 여행일정이었습니다.\n\n\n\n처음으로 동남아로 가족들이 아닌 친구들끼리 가는 여행이라 긴장도, 걱정도 많이했었지만 엑티비티도 많이하고 추억들을 많이 쌓을 수 있어서 정말 재밌었던 여행이었습니다. 여행을 하면서 내가 막연히 가지고 있는 걱정이 생각보다 많고 경험했을때 깨달을 수 있는 부분들이 많다는 걸 다시한번 짚어볼 수 있었습니다."
  },
  {
    "objectID": "posts/note/2025-01-05-Goodbye-2024.html#마무리",
    "href": "posts/note/2025-01-05-Goodbye-2024.html#마무리",
    "title": "📘Goodbye 2024",
    "section": "마무리",
    "text": "마무리\n한때 회고를 남기는 것에 대해 회의적일 때가 있었습니다. 그런데 기억한다는 점은 다음 단계로 나아가는 첫걸음이라는 생각이 들고 나서 다시 회고를 하고 있습니다. 글의 서두에도 적었듯이 2024년도가 정말 많은 일들이 있던 한해였기에 전체 인생에서도 기억에 남을 한해가 될 것 같습니다. 다가오는 2025년도에 가끔씩 24년도 회고를 보러올 것 같은데 그때에는 조금이라도 성장해있는 또 다른 내가 되어있길 바라며 회고를 이만 맺겠습니다."
  },
  {
    "objectID": "posts/note/2022-10-24-daily-english-004.html",
    "href": "posts/note/2022-10-24-daily-english-004.html",
    "title": "🌎IT English Experssions 004",
    "section": "",
    "text": "영어 회화 표현이나 논문에서 쓸 수 있는 유용한 표현들을 조금씩 익히기 위해 시작한 1000일 시리즈 포스트 입니다. 공부 리소스는 유튜브, 리얼클래스, PN님의 블로그, cake 어플, 책, 논문, 기사 등 다양한 출처에서 보고 기록합니다."
  },
  {
    "objectID": "posts/note/2022-10-24-daily-english-004.html#git으로-세련되게-협업하는-방법",
    "href": "posts/note/2022-10-24-daily-english-004.html#git으로-세련되게-협업하는-방법",
    "title": "🌎IT English Experssions 004",
    "section": "Git으로 세련되게 협업하는 방법",
    "text": "Git으로 세련되게 협업하는 방법\n\nConventional Commits\n\n커밋 메세지는 Subject (Title), Body, Footer로 구분\n구조\n\n&lt;type&gt;[optional scope]:&lt;description&gt;\n[optional body]\n[optional footer(s)]\n\n예시(http://www.conventionalcommits.org/)\n\nfix: prevent racing of requests                           ---&gt; Subject or Title\nIntroduce a request id and a reference to latest request. ---&gt; Body\nReviewed-by : Z                                           ---&gt; Footer(s)\nRefs : #123\n\n\nSubject(Title) 작성법\n커밋 메세지의 제목은 변경사항을 대표하는 텍스트이므로 대표적인 타입들이 있다.\n\nfeat: 코드에 새로운 기능(=feature) 추가\nfix: 버그 수정\nBREAKING CHANGE: 이전 버전과 호환되지 않는 변경 내역. !으로 표시할 수 있음. e.g.) feat!:\ndocs: 개발 문서 변경\nstyle: 들여쓰기, 따옴표, 세미콜론 등 코드 형식 및 스타일 변경\nci: CI/CD(continueout integration and deployment) 관련 코드 변경\nrefactor: 중복된 코드 제거, 변수명 변경, 코드 단순화 등 리팩터링\ntest: 테스트 관련 코드 변경\nbuild: 빌드 시스템 관련 코드 변경\nperf: 성능 개선 관련 코드 변경\nchore: 기타 코드 변경\n\n예시\n\nfix: remove deprecated features 수정: 권장되지 않는 기능 삭제\nfeat: add parameters to getImage 기능: getImage에 매개변수 추가\ndocs(readme): update build instructions 문서(readme): 빌드 지침 업데이트\nchore: update np dependencies to latest version 기타: npm 의존성 최신버전으로 업테이트\n\n나만의 예시 연습\n\ndocs(readme): add new papers \"THE TITLE OF PAPER\"\nfeat: add getJointPosition function\nrefactor: delete overlapped constants and variables of robot model\n\n\n\n5가지 커밋 작성법\n\n동사 원형으로 시작\n\n\n제목은 명령적 어조(Imperative Mood)의 동사원형으로 시작\nBody, Footer는 명령문이 아니어도 됨\n상황에 따라 과거형(e.g. Added) 또는 3인칭 단수 현재형(e.g. Adds)를 사용하기도 함\n커밋 메세지에 자주 등장하는 동사\n\nFix: 수정하다\nImprove: 개선하다\nHandle: 처리하다\nOptimize: 최적화하다\nUpdate: 업데이트하다\nImplement: 구현하다, 적용하다\nRefactor: 리펙터링하다\nAdd: 추가하다\nRevert: 되돌리다\nChange: 변경하다\nReplace: 대체하다\nMerge: 병합하다\nDocument: 문서를 작성하다\nBump: 버전을 올리다\nSimplify: 단순화시키다\nEnable: 가능하게 하다\nRun: 실행하다\nClean: 제거하다, 정리하다\nWrap: 감싸다, 그룹화하다\nDeploy: 배포하다\nModify: 변경하다\nRemove: 제거하다\nRename: 이름을 바꾸다\nMove: 이동하다, 이동시키다\n\n\n\n모두 소문자로 또는 첫 글자만 대문자로\n\n\nConventional Commits 형식에서는 모든 문자를 소문자로 작성. type: description에 맞추어 메세지 작성\nConventional Commits 형식을 적용하지 않는 경우, 일반적으로 앞부분만 대문자 사용하고 type 생략\n예시\n\n[1] docs: update build.md with detailed instructions\n[2] Update build.md with detailed instructions\n\n관사(a, an, the와 같은 Article) 생략\n\n\n커밋 타이틀은 최대 50글자 제한\n핵심 키워드만 활용하여 메세지를 작성하기 위해 관사 생략\n예시\n\n[X] Fix a typo in the header\n[O] Fix typo in header\n\n마침표와 같은 구두점(Punctuation Mark) 생략\n\n\n반드시 필요한 경우가 아니면 쉼표, 하이픈 등 생략\n예시\n\n[X] feat: implement google analytics.\n[O] feat: implement google analytics\n\n변경한 이유, 상세한 설명은 본문(Body)에\n\n\n코드 변경 사유와 상세 설명은 커밋 본문에 씀\n\n\n\nGit 주요 실무 영어\n\nSquash the last 3 commits: 최근 3개 커밋을 합치다\nPush commits to a repository: 리포지터리(코드 저장소)로 커밋을 전달하다\nMerge a feature branch into the base branch: 기능 브랜치를 기본 브랜치에 병합하다\nRevert a pull request: 풀 리퀘스트를 되돌리다(이전 상태로 되돌리다)\nRequest a review: 검토를 요청하다\nComment on a pull request: 풀 리퀘스트에 댓글을 남기다\nResolve a merge conflict: 병합 충돌을 해결하다\nRebase onto another branch: 다른 브랜치로 리베이스(base를 재설정하여 커밋 재적용)하다\nClone a repository: 리포지터리를 복제하다\nClose a pull request without merging it onto the branch: 풀 리퀘스트를 병합하지 않고 종료하다"
  },
  {
    "objectID": "posts/note/2024-01-15-cs-study-001.html",
    "href": "posts/note/2024-01-15-cs-study-001.html",
    "title": "📝Operating System 001",
    "section": "",
    "text": "Originial Repository: https://github.com/gyoogle/tech-interview-for-developer를 공부하며 2차 편집한 내용입니다."
  },
  {
    "objectID": "posts/note/2024-01-15-cs-study-001.html#운영체제의-역할",
    "href": "posts/note/2024-01-15-cs-study-001.html#운영체제의-역할",
    "title": "📝Operating System 001",
    "section": "운영체제의 역할",
    "text": "운영체제의 역할\n\n\n\n\n\n\n\n\n\n\n1. 프로세스 관리\n2. 저장장치 관리\n3. 네트워킹\n4. 사용자 관리\n5. 디바이스 드라이버\n\n\n\n\n프로세스, 스레드 스케줄링 동기화 IPC 통신\n메모리 관리 가상 메모리 파일 시스템\nTCP/IP 기타 프로토콜\n계정 관리 접근권한 관리\n순차접근 장치 임의접근 장치 네트워크 장치\n\n\n\n\n\n각 역할에 대한 자세한 설명\n\n\n\n\n\n\n1. 프로세스 관리\n\n\n\n\n\n운영체제에서 작동하는 응용 프로그램을 관리하는 기능이다.\n어떤 의미에서는 프로세서(CPU) 관리하는 것이라고 볼 수도 있다. 현재 CPU를 점유해야 할 프로세스를 결정하고, 실제로 CPU를 프로세스에 할당하며, 이 프로세스 간 공유 자원 접근과 통신 등을 관리하게 된다.\n\n\n\n\n\n\n\n\n\n2. 저장장치 관리\n\n\n\n\n\n1차 저장장치에 해당하는 메인 메모리와 2차 저장장치에 해당하는 하드디스크, NAND 등을 관리하는 기능이다.\n\n1차 저장장치(Main Memory)\n\n프로세스에 할당하는 메모리 영역의 할당과 해제\n각 메모리 영역 간의 침범 방지\n메인 메모리의 효율적 활용을 위한 가상 메모리 기능\n\n2차 저장장치(HDD, NAND Flash Memory 등)\n\n파일 형식의 데이터 저장\n이런 파일 데이터 관리를 위한 파일 시스템을 OS에서 관리\nFAT, NTFS, EXT2, JFS, XFS 등 많은 파일 시스템들이 개발되어 사용 중\n\n\n\n\n\n\n\n\n\n\n\n3. 네트워킹\n\n\n\n\n\n네트워킹은 컴퓨터 활용의 핵심과도 같아졌다.\nTCP/IP 기반의 인터넷에 연결하거나, 응용 프로그램이 네트워크를 사용하려면 운영체제에서 네트워크 프로토콜을 지원해야 한다. 현재 상용 OS들은 다양하고 많은 네트워크 프로토콜을 지원한다.\n이처럼 운영체제는 사용자와 컴퓨터 하드웨어 사이에 위치해서, 하드웨어를 운영 및 관리하고 명령어를 제어하여 응용 프로그램 및 하드웨어를 소프트웨어적으로 제어 및 관리를 해야한다.\n\n\n\n\n\n\n\n\n\n4. 사용자 관리\n\n\n\n\n\n우리가 사용하는 PC는 오직 한 사람만의 것일까? 아니다.\n하나의 PC로도 여러 사람이 사용하는 경우가 많다. 그래서 운영체제는 한 컴퓨터를 여러 사람이 사용하는 환경도 지원해야 한다. 가족들이 각자의 계정을 만들어 PC를 사용한다면, 이는 하나의 컴퓨터를 여러 명이 사용한다고 말할 수 있다.\n따라서, 운영체제는 각 계정을 관리할 수 있는 기능이 필요하다. 사용자 별로 프라이버시와 보안을 위해 개인 파일에 대해선 다른 사용자가 접근할 수 없도록 해야 한다. 이 밖에도 파일이나 시스템 자원에 접근 권한을 지정할 수 있도록 지원하는 것이 사용자 관리 기능이다.\n\n\n\n\n\n\n\n\n\n5. 디바이스 드라이버\n\n\n\n\n\n운영체제는 시스템의 자원, 하드웨어를 관리한다. 시스템에는 여러 하드웨어가 붙어있는데, 이들을 운영체제에서 인식하고 관리하게 만들어 응용 프로그램이 하드웨어를 사용할 수 있게 만들어야 한다.\n따라서, 운영체제 안에 하드웨어를 추상화 해주는 계층이 필요하다. 이 계층이 바로 디바이스 드라이버라고 불린다. 하드웨어의 종류가 많은 만큼, 운영체제 내부의 디바이스 드라이버도 많이 존재한다.\n이러한 수많은 디바이스 드라이버들을 관리하는 기능 또한 운영체제가 맡고 있다."
  },
  {
    "objectID": "posts/note/2024-01-15-cs-study-001.html#멀티-프로세스",
    "href": "posts/note/2024-01-15-cs-study-001.html#멀티-프로세스",
    "title": "📝Operating System 001",
    "section": "멀티 프로세스",
    "text": "멀티 프로세스\n\n하나의 프로그램을 여러개의 프로세스로 구성하여 각 프로세스가 병렬적으로 작업을 수행하는 것\n\n장점 : 안전성 (메모리 침범 문제를 OS 차원에서 해결)\n단점 : 각각 독립된 메모리 영역을 갖고 있어, 작업량 많을 수록 오버헤드 발생. Context Switching으로 인한 성능 저하\nContext Switching이란?\n\n프로세스의 상태 정보를 저장하고 복원하는 일련의 과정\n즉, 동작 중인 프로세스가 대기하면서 해당 프로세스의 상태를 보관하고, 대기하고 있던 다음 순번의 프로세스가 동작하면서 이전에 보관했던 프로세스 상태를 복구하는 과정을 말함\n→ 프로세스는 각 독립된 메모리 영역을 할당받아 사용되므로, 캐시 메모리 초기화와 같은 무거운 작업이 진행되었을 때 오버헤드가 발생할 문제가 존재함"
  },
  {
    "objectID": "posts/note/2024-01-15-cs-study-001.html#멀티-스레드",
    "href": "posts/note/2024-01-15-cs-study-001.html#멀티-스레드",
    "title": "📝Operating System 001",
    "section": "멀티 스레드",
    "text": "멀티 스레드\n\n하나의 응용 프로그램에서 여러 스레드를 구성해 각 스레드가 하나의 작업을 처리하는 것\n\n스레드들이 공유 메모리를 통해 다수의 작업을 동시에 처리하도록 해줌\n장점 : 독립적인 프로세스에 비해 공유 메모리만큼의 시간, 자원 손실이 감소 전역 변수와 정적 변수에 대한 자료 공유 가능\n단점 : 안전성 문제. 하나의 스레드가 데이터 공간 망가뜨리면, 모든 스레드가 작동 불능 상태 (공유 메모리를 갖기 때문)\n\n멀티스레드의 안전성에 대한 단점은 Critical Section 기법을 통해 대비함\n\n하나의 스레드가 공유 데이터 값을 변경하는 시점에 다른 스레드가 그 값을 읽으려할 때 발생하는 문제를 해결하기 위한 동기화 과정\n상호 배제, 진행, 한정된 대기를 충족해야함"
  },
  {
    "objectID": "posts/note/2024-01-15-cs-study-001.html#외부-인터럽트",
    "href": "posts/note/2024-01-15-cs-study-001.html#외부-인터럽트",
    "title": "📝Operating System 001",
    "section": "외부 인터럽트",
    "text": "외부 인터럽트\n입출력 장치, 타이밍 장치, 전원 등 외부적인 요인으로 발생\n\n전원 이상, 기계 착오, 외부 신호, 입출력"
  },
  {
    "objectID": "posts/note/2024-01-15-cs-study-001.html#내부-인터럽트",
    "href": "posts/note/2024-01-15-cs-study-001.html#내부-인터럽트",
    "title": "📝Operating System 001",
    "section": "내부 인터럽트",
    "text": "내부 인터럽트\nTrap이라고 부르며, 잘못된 명령이나 데이터를 사용할 때 발생\n\n0으로 나누기가 발생, 오버플로우, 명령어를 잘못 사용한 경우 (Exception)"
  },
  {
    "objectID": "posts/note/2024-01-15-cs-study-001.html#소프트웨어-인터럽트",
    "href": "posts/note/2024-01-15-cs-study-001.html#소프트웨어-인터럽트",
    "title": "📝Operating System 001",
    "section": "소프트웨어 인터럽트",
    "text": "소프트웨어 인터럽트\n프로그램 처리 중 명령의 요청에 의해 발생한 것 (SVC 인터럽트)\n\n사용자가 프로그램을 실행시킬 때 발생\n소프트웨어 이용 중에 다른 프로세스를 실행시키면 시분할 처리를 위해 자원 할당 동작이 수행된다."
  },
  {
    "objectID": "posts/note/2024-01-15-cs-study-001.html#인터럽트-발생-처리-과정",
    "href": "posts/note/2024-01-15-cs-study-001.html#인터럽트-발생-처리-과정",
    "title": "📝Operating System 001",
    "section": "인터럽트 발생 처리 과정",
    "text": "인터럽트 발생 처리 과정\n주 프로그램이 실행되다가 인터럽트가 발생했다.\n\n\n\nHandling Interrupts\n\n\n현재 수행 중인 프로그램을 멈추고, 상태 레지스터와 PC 등을 스택에 잠시 저장한 뒤에 인터럽트 서비스 루틴으로 간다.\n(잠시 저장하는 이유는, 인터럽트 서비스 루틴이 끝난 뒤 다시 원래 작업으로 돌아와야 하기 때문)\n만약 인터럽트 기능이 없었다면, 컨트롤러는 특정한 어떤 일을 할 시기를 알기 위해 계속 체크를 해야 한다.\n(이를 폴링(Polling)이라고 한다)\n폴링을 하는 시간에는 원래 하던 일에 집중할 수가 없게 되어 많은 기능을 제대로 수행하지 못하는 단점이 있었다.\n즉, 컨트롤러가 입력을 받아들이는 방법(우선순위 판별방법)에는 두가지가 있다.\n\n폴링 방식\n\n사용자가 명령어를 사용해 입력 핀의 값을 계속 읽어 변화를 알아내는 방식\n인터럽트 요청 플래그를 차례로 비교하여 우선순위가 가장 높은 인터럽트 자원을 찾아 이에 맞는 인터럽트 서비스 루틴을 수행한다. (하드웨어에 비해 속도 느림)\n\n인터럽트 방식\n\nMCU 자체가 하드웨어적으로 변화를 체크하여 변화 시에만 일정한 동작을 하는 방식\nDaisy Chain\n병렬 우선순위 부여\n\n\n인터럽트 방식은 하드웨어로 지원을 받아야 하는 제약이 있지만, 폴링에 비해 신속하게 대응하는 것이 가능하다. 따라서 ‘실시간 대응’이 필요할 때는 필수적인 기능이다.\n즉, 인터럽트는 발생시기를 예측하기 힘든 경우에 컨트롤러가 가장 빠르게 대응할 수 있는 방법이다."
  },
  {
    "objectID": "posts/note/2024-01-15-cs-study-001.html#fork",
    "href": "posts/note/2024-01-15-cs-study-001.html#fork",
    "title": "📝Operating System 001",
    "section": "Fork",
    "text": "Fork\n\n새로운 Process를 생성할 때 사용.\n그러나, 이상한 방식임.\n\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;unistd.h&gt;\n\nint main(int argc, char *argv[]) {\n    printf(\"pid : %d\", (int) getpid()); // pid : 29146\n    \n    int rc = fork();                    // 주목\n    \n    if (rc &lt; 0) {\n        exit(1);\n    }                                   // (1) fork 실패\n    else if (rc == 0) {                 // (2) child 인 경우 (fork 값이 0)\n        printf(\"child (pid : %d)\", (int) getpid());\n    }\n    else {                              // (3) parent case\n        printf(\"parent of %d (pid : %d)\", rc, (int)getpid());\n    }\n}\n\npid : 29146\nparent of 29147 (pid : 29146)\nchild (pid : 29147)\n\n을 출력함 (parent와 child의 순서는 non-deterministic함. 즉, 확신할 수 없음. scheduler가 결정하는 일임.)\n[해석]\nPID : 프로세스 식별자. UNIX 시스템에서는 PID는 프로세스에게 명령을 할 때 사용함.\nFork()가 실행되는 순간. 프로세스가 하나 더 생기는데, 이 때 생긴 프로세스(Child)는 fork를 만든 프로세스(Parent)와 (almost) 동일한 복사본을 갖게 된다. 이 때 OS는 위와 똑같은 2개의 프로그램이 동작한다고 생각하고, fork()가 return될 차례라고 생각한다. 그 때문에 새로 생성된 Process (child)는 main에서 시작하지 않고, if 문부터 시작하게 된다.\n그러나, 차이점이 있었다. 바로 child와 parent의 fork() 값이 다르다는 점이다. 따라서, 완전히 동일한 복사본이라 할 수 없다.\n\nParent의 fork()값 =&gt; child의 pid 값\nChild의 fork()값 =&gt; 0\n\nParent와 child의 fork 값이 다르다는 점은 매우 유용한 방식이다.\n그러나! Scheduler가 부모를 먼저 수행할지 아닐지 확신할 수 없다. 따라서 아래와 같이 출력될 수 있다.\n\npid : 29146\nchild (pid : 29147)\nparent of 29147 (pid : 29146)"
  },
  {
    "objectID": "posts/note/2024-01-15-cs-study-001.html#wait",
    "href": "posts/note/2024-01-15-cs-study-001.html#wait",
    "title": "📝Operating System 001",
    "section": "wait",
    "text": "wait\n\nchild 프로세스가 종료될 때까지 기다리는 작업\n\n위의 예시에 int wc = wait(NULL)만 추가함.\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;unistd.h&gt;\n#include &lt;sys/wait.h&gt;\n\nint main(int argc, char *argv[]) {\n    printf(\"pid : %d\", (int) getpid()); // pid : 29146\n    \n    int rc = fork();                    // 주목\n    \n    if (rc &lt; 0) {\n        exit(1);\n    }                                   // (1) fork 실패\n    else if (rc == 0) {                 // (2) child 인 경우 (fork 값이 0)\n        printf(\"child (pid : %d)\", (int) getpid());\n    }\n    else {                              // (3) parent case\n        int wc = wait(NULL)             // 추가된 부분\n        printf(\"parent of %d (wc : %d / pid : %d)\", wc, rc, (int)getpid());\n    }\n}\n\npid : 29146\nchild (pid : 29147)\nparent of 29147 (wc : 29147 / pid : 29146)\n\nwait를 통해서, child의 실행이 끝날 때까지 기다려줌. parent가 먼저 실행되더라도, wait ()는 child가 끝나기 전에는 return하지 않으므로, 반드시 child가 먼저 실행됨."
  },
  {
    "objectID": "posts/note/2024-01-15-cs-study-001.html#exec",
    "href": "posts/note/2024-01-15-cs-study-001.html#exec",
    "title": "📝Operating System 001",
    "section": "exec",
    "text": "exec\n단순 fork는 동일한 프로세스의 내용을 여러 번 동작할 때 사용함.\nchild에서는 parent와 다른 동작을 하고 싶을 때는 exec를 사용할 수 있음.\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;unistd.h&gt;\n#include &lt;sys/wait.h&gt;\n\nint main(int argc, char *argv[]) {\n    printf(\"pid : %d\", (int) getpid()); // pid : 29146\n    \n    int rc = fork();                    // 주목\n    \n    if (rc &lt; 0) {\n        exit(1);\n    }                                   // (1) fork 실패\n    else if (rc == 0) {                 // (2) child 인 경우 (fork 값이 0)\n        printf(\"child (pid : %d)\", (int) getpid());\n        char *myargs[3];\n        myargs[0] = strdup(\"wc\");       // 내가 실행할 파일 이름\n        myargs[1] = strdup(\"p3.c\");     // 실행할 파일에 넘겨줄 argument\n        myargs[2] = NULL;               // end of array\n        execvp(myarges[0], myargs);     // wc 파일 실행.\n        printf(\"this shouldn't print out\") // 실행되지 않음.\n    }\n    else {                              // (3) parent case\n        int wc = wait(NULL)             // 추가된 부분\n        printf(\"parent of %d (wc : %d / pid : %d)\", wc, rc, (int)getpid());\n    }\n}\nexec가 실행되면,\nexecvp( 실행 파일, 전달 인자 ) 함수는, code segment 영역에 실행 파일의 코드를 읽어와서 덮어 씌운다.\n씌운 이후에는, heap, stack, 다른 메모리 영역이 초기화되고, OS는 그냥 실행한다. 즉, 새로운 Process를 생성하지 않고, 현재 프로그램에 wc라는 파일을 실행한다. 그로인해서, execvp() 이후의 부분은 실행되지 않는다."
  },
  {
    "objectID": "posts/note/2024-01-15-cs-study-001.html#process-management",
    "href": "posts/note/2024-01-15-cs-study-001.html#process-management",
    "title": "📝Operating System 001",
    "section": "Process Management",
    "text": "Process Management\n\nCPU가 프로세스가 여러개일 때, CPU 스케줄링을 통해 관리하는 것을 말함\n\n이때, CPU는 각 프로세스들이 누군지 알아야 관리가 가능함\n프로세스들의 특징을 갖고있는 것이 바로 Process Metadata\n\nProcess Metadata\n\nProcess ID\nProcess State\nProcess Priority\nCPU Registers\nOwner\nCPU Usage\nMemeory Usage\n\n\n이 메타데이터는 프로세스가 생성되면 PCB(Process Control Block)이라는 곳에 저장됨"
  },
  {
    "objectID": "posts/note/2024-01-15-cs-study-001.html#pcbprocess-control-block",
    "href": "posts/note/2024-01-15-cs-study-001.html#pcbprocess-control-block",
    "title": "📝Operating System 001",
    "section": "PCB(Process Control Block)",
    "text": "PCB(Process Control Block)\n\n프로세스 메타데이터들을 저장해 놓는 곳, 한 PCB 안에는 한 프로세스의 정보가 담김\n\n\n\n\nPCB\n\n\n다시 정리해보면?\n프로그램 실행 → 프로세스 생성 → 프로세스 주소 공간에 (코드, 데이터, 스택) 생성 \n→ 이 프로세스의 메타데이터들이 PCB에 저장"
  },
  {
    "objectID": "posts/note/2024-01-15-cs-study-001.html#pcb가-왜-필요한가요",
    "href": "posts/note/2024-01-15-cs-study-001.html#pcb가-왜-필요한가요",
    "title": "📝Operating System 001",
    "section": "PCB가 왜 필요한가요?",
    "text": "PCB가 왜 필요한가요?\n\nCPU에서는 프로세스의 상태에 따라 교체작업이 이루어진다. (interrupt가 발생해서 할당받은 프로세스가 waiting 상태가 되고 다른 프로세스를 running으로 바꿔 올릴 때)\n이때, 앞으로 다시 수행할 대기 중인 프로세스에 관한 저장 값을 PCB에 저장해두는 것이다."
  },
  {
    "objectID": "posts/note/2024-01-15-cs-study-001.html#pcb는-어떻게-관리되나요",
    "href": "posts/note/2024-01-15-cs-study-001.html#pcb는-어떻게-관리되나요",
    "title": "📝Operating System 001",
    "section": "PCB는 어떻게 관리되나요?",
    "text": "PCB는 어떻게 관리되나요?\n\nLinked List 방식으로 관리함\nPCB List Head에 PCB들이 생성될 때마다 붙게 된다. 주소값으로 연결이 이루어져 있는 연결리스트이기 때문에 삽입 삭제가 용이함.\n즉, 프로세스가 생성되면 해당 PCB가 생성되고 프로세스 완료시 제거됨\n\n이렇게 수행 중인 프로세스를 변경할 때, CPU의 레지스터 정보가 변경되는 것을 Context Switching이라고 한다."
  },
  {
    "objectID": "posts/note/2024-01-15-cs-study-001.html#context-switching",
    "href": "posts/note/2024-01-15-cs-study-001.html#context-switching",
    "title": "📝Operating System 001",
    "section": "Context Switching",
    "text": "Context Switching\n\nCPU가 이전의 프로세스 상태를 PCB에 보관하고, 또 다른 프로세스의 정보를 PCB에 읽어 레지스터에 적재하는 과정\n\n보통 인터럽트가 발생하거나, 실행 중인 CPU 사용 허가시간을 모두 소모하거나, 입출력을 위해 대기해야 하는 경우에 Context Switching이 발생\n즉, 프로세스가 Ready → Running, Running → Ready, Running → Waiting처럼 상태 변경 시 발생!"
  },
  {
    "objectID": "posts/note/2024-01-15-cs-study-001.html#context-switching의-overhead란",
    "href": "posts/note/2024-01-15-cs-study-001.html#context-switching의-overhead란",
    "title": "📝Operating System 001",
    "section": "Context Switching의 OverHead란?",
    "text": "Context Switching의 OverHead란?\noverhead는 과부하라는 뜻으로 보통 안좋은 말로 많이 쓰인다.\n하지만 프로세스 작업 중에는 OverHead를 감수해야 하는 상황이 있다.\n프로세스를 수행하다가 입출력 이벤트가 발생해서 대기 상태로 전환시킴\n이때, CPU를 그냥 놀게 놔두는 것보다 다른 프로세스를 수행시키는 것이 효율적\n즉, CPU에 계속 프로세스를 수행시키도록 하기 위해서 다른 프로세스를 실행시키고 Context Switching 하는 것\nCPU가 놀지 않도록 만들고, 사용자에게 빠르게 일처리를 제공해주기 위한 것이다."
  },
  {
    "objectID": "posts/note/2024-01-15-cs-study-001.html#ipc-종류",
    "href": "posts/note/2024-01-15-cs-study-001.html#ipc-종류",
    "title": "📝Operating System 001",
    "section": "IPC 종류",
    "text": "IPC 종류\n\n익명 PIPE\n\n파이프는 두 개의 프로세스를 연결하는데 하나의 프로세스는 데이터를 쓰기만 하고, 다른 하나는 데이터를 읽기만 할 수 있다.\n한쪽 방향으로만 통신이 가능한 반이중 통신이라고도 부른다.\n따라서 양쪽으로 모두 송/수신을 하고 싶으면 2개의 파이프를 만들어야 한다.\n매우 간단하게 사용할 수 있는 장점이 있고, 단순한 데이터 흐름을 가질 땐 파이프를 사용하는 것이 효율적이다. 단점으로는 전이중 통신을 위해 2개를 만들어야 할 때는 구현이 복잡해지게 된다.\n\nNamed PIPE(FIFO)\n\n익명 파이프는 통신할 프로세스를 명확히 알 수 있는 경우에 사용한다. (부모-자식 프로세스 간 통신처럼)\nNamed 파이프는 전혀 모르는 상태의 프로세스들 사이 통신에 사용한다.\n즉, 익명 파이프의 확장된 상태로 부모 프로세스와 무관한 다른 프로세스도 통신이 가능한 것 (통신을 위해 이름있는 파일을 사용)\n\n\n하지만, Named 파이프 역시 읽기/쓰기 동시에 불가능함. 따라서 전이중 통신을 위해서는 익명 파이프처럼 2개를 만들어야 가능\n\nMessage Queue\n\n입출력 방식은 Named 파이프와 동일함\n다른점은 메시지 큐는 파이프처럼 데이터의 흐름이 아니라 메모리 공간이다.\n사용할 데이터에 번호를 붙이면서 여러 프로세스가 동시에 데이터를 쉽게 다룰 수 있다.\n\n공유 메모리\n\n파이프, 메시지 큐가 통신을 이용한 설비라면, 공유 메모리는 데이터 자체를 공유하도록 지원하는 설비다.\n프로세스의 메모리 영역은 독립적으로 가지며 다른 프로세스가 접근하지 못하도록 반드시 보호돼야한다. 하지만 다른 프로세스가 데이터를 사용하도록 해야하는 상황도 필요할 것이다. 파이프를 이용해 통신을 통해 데이터 전달도 가능하지만, 스레드처럼 메모리를 공유하도록 해준다면 더욱 편할 것이다.\n공유 메모리는 프로세스간 메모리 영역을 공유해서 사용할 수 있도록 허용해준다.\n프로세스가 공유 메모리 할당을 커널에 요청하면, 커널은 해당 프로세스에 메모리 공간을 할당해주고 이후 모든 프로세스는 해당 메모리 영역에 접근할 수 있게 된다.\n\n중개자 없이 곧바로 메모리에 접근할 수 있어서 IPC 중에 가장 빠르게 작동함\n\n\n\n메모리 맵\n\n공유 메모리처럼 메모리를 공유해준다. 메모리 맵은 열린 파일을 메모리에 맵핑시켜서 공유하는 방식이다. (즉 공유 매개체가 파일+메모리)\n주로 파일로 대용량 데이터를 공유해야 할 때 사용한다.\n\n소켓\n\n네트워크 소켓 통신을 통해 데이터를 공유한다.\n클라이언트와 서버가 소켓을 통해서 통신하는 구조로, 원격에서 프로세스 간 데이터를 공유할 때 사용한다.\n서버(bind, listen, accept), 클라이언트(connect)\n\n\n이러한 IPC 통신에서 프로세스 간 데이터를 동기화하고 보호하기 위해 세마포어와 뮤텍스를 사용한다. (공유된 자원에 한번에 하나의 프로세스만 접근시킬 때)"
  },
  {
    "objectID": "posts/note/2025-09-25-mujoco-vs-isaaclab.html",
    "href": "posts/note/2025-09-25-mujoco-vs-isaaclab.html",
    "title": "📝Comparative Analysis of Robotic Simulation Environments",
    "section": "",
    "text": "Original Link\n\nWe wanted to dive a little deeper into the role of physics in simulation and the landscape of physics engines currently popular within simulation and game engines in general. We also wanted put this in relation to the big Newton announcement this week.\nRobotic simulators are crucial for developing and testing control algorithms in a virtual setting. We compare several prominent physics engines and simulation frameworks – MuJoCo, NVIDIA Isaac Lab (PhysX-based), and other relevant engines (like Bullet and Brax) – in terms of performance, ease of use, compatibility, and application scope. Each simulator has different strengths, and Nvidia Newton is designed to integrate with the best of these environments.\n\nMuJoCo (Multi-Joint dynamics with Contact)\n\nPerformance: MuJoCo is known for its efficient physics simulation, often running faster than real time even on a CPU\nEase of Use: MuJoCo provides a well-designed C API and supports model descriptions in MJCF or URDF, making it relatively straightforward to define robots and environments\nCompatibility: MuJoCo is cross-platform and now open-source, which broadens its compatibility. It integrates well with reinforcement learning frameworks – for instance, the OpenAI Gym/Gymnasium and DeepMind Control Suite provide standardized MuJoCo environments for benchmarking. The MuJoCo engine itself focuses purely on physics; for rendering, it offers basic OpenGL visualization, though not photorealistic. It supports a range of robotic components (joints, actuators, sensors) needed for robotics research. With DeepMind’s open-sourcing, MuJoCo’s code can be extended or paired with other tools (e.g. JAX-based accelerators or the Warp GPU back-end).\nApplication Scope: MuJoCo has been heavily used in academia and industry for robot learning, motor control, and biomechanics. Its strength lies in simulating complex contact and articulated structures (e.g. legged robots, manipulators) with good physical fidelity, which is why it became the go-to for many RL environments\n\n\n\nNVIDIA Isaac Lab (PhysX-Based Simulation)\n\nPerformance: Isaac Lab is built on NVIDIA’s PhysX engine and Omniverse simulation tools. PhysX is a mature, highly optimized physics engine (used in many games) that can handle large-scale simulations and has support for GPU acceleration in certain features. In Isaac Lab/Sim, PhysX is configured for robotics, including accurate joint articulation solvers and support for many agents. While PhysX on CPU is multi-threaded and quite fast, one of Isaac’s strengths is leveraging NVIDIA GPUs: for example, Isaac Sim can offload rigid body computations and use RTX GPU ray-tracing for rendering sensors, enabling high-fidelity yet still performant simulation\nEase of Use: Compared to lightweight engines like MuJoCo, Isaac Lab/Sim can be more complex to set up – it typically requires a robust NVIDIA GPU and the installation of Isaac Sim (which is a fairly large software stack). However, NVIDIA has provided convenient tools: Isaac Lab itself is an open-source framework with a Python API that abstracts much of the low-level detail\nCompatibility: Isaac Lab is fully compatible with the NVIDIA robotics ecosystem. It uses the USD (Universal Scene Description) format to represent environments and robots, which promotes interoperability – one can import models from CAD or other simulators that support USD\nApplication Scope: Isaac Lab/Sim is aimed at end-to-end robotics development. This includes robot learning (reinforcement learning and imitation learning), but also robot design, testing algorithms for navigation or manipulation, and generating synthetic data for training perception models. Its ability to simulate cameras (RGB, depth, LiDAR) with realistic visuals is a key advantage for tasks where the robot’s sensors are part of the loop (e.g., training a vision-based policy or testing an autonomous vehicle in a virtual world). Isaac has been used in applications from warehouse robot simulation to humanoid control and even digital twins of real environments. Newton’s role: Nvidia Newton will integrate with Isaac Lab as a next-gen physics option\n\n\n\nOther Relevant Physics Engines (Bullet, ODE, Brax, etc.)\nAside from MuJoCo and Isaac/PhysX, several other physics engines are commonly used in robotics simulation, each with their own pros and cons:\n\nBullet Physics (and PyBullet): Bullet is an open-source physics engine widely used in games and robotics. It features rigid body and soft body simulation, and has a convenient Python interface (PyBullet) that became popular for quick robot simulation prototypes. Bullet is appreciated for being free and fairly full-featured (collision, joints, etc.), and it has been integrated into tools like ROS/Gazebo (as an optional physics engine) and V-REP/CoppeliaSim\nODE (Open Dynamics Engine): ODE is one of the older open-source physics engines, used historically in many robotics simulators (Gazebo’s default engine for years was ODE). It provides basic rigid body dynamics and joint physics. ODE is quite light and simple, but it struggles with simulation stability when many contacts or complex joints are involved, often requiring parameter tuning (e.g., ERP/CFM settings) to get reasonable behavior. Its performance is generally slower and less optimized compared to newer engines\nBrax (and other differentiable engines): Brax is a recent physics simulation library from Google that runs on JAX (on GPU/TPU) and is fully differentiable. It’s designed for extremely fast training of reinforcement learning agents via hardware acceleration. Brax achieves impressive speed by simplifying physics – it uses primarily spherical or capsule collisions and a penalty-based contact model, which is less accurate than constraint-based solvers but very parallelizable. The ease of use is high (Brax can be used via Python notebooks and Google Colab), and being differentiable, it’s great for research into gradient-based control. However, Brax’s simplified physics means it’s limited in scope (not suitable for fine contact dynamics or complex geometries). Other differentiable simulators (like Dojo or Tiny Differentiable Simulator) are in development as well. Newton’s relation: Newton similarly emphasizes GPU performance but aims for more generality than Brax. Newton’s differentiable physics will allow computing gradients through realistic contact dynamics, combining the benefits of Brax (speed, differentiability) with the richer physics of engines like MuJoCo\nPhysX and Others: NVIDIA’s PhysX (which we discussed via Isaac) is also used outside Isaac – for example, the Unity and Unreal game engines use PhysX or their variants for physics. Another engine, Havok, is common in industry (but not available for custom robotics use easily). DART/Drake are robotics-focused frameworks that include physics (DART uses ODE/Bullet, Drake has its own solver) and emphasize precision and robot model integration; they are powerful for planning and control research but are slower and not aimed at massive parallelism. Newton’s introduction does not directly integrate those engines, but by establishing USD as a common scene format and being open source, it opens the door for various communities to adopt Newton as an alternative backend. For instance, a robotics lab currently using Bullet or DART could switch to Newton for better performance once it matures, using the USD pipeline to transfer their models.\n\n\n\nNewton’s Integration and Unique Advantages\nNvidia Newton is designed to seamlessly integrate with existing simulation environments like MuJoCo and Isaac Lab, effectively serving as a high-performance physics backend. By doing so, it brings a set of unique advantages to each:\n\nUnified Engine Across Frameworks: Newton provides a common physics engine that can run within different simulators. Developers can use MuJoCo Playground or Isaac Lab front-ends, yet rely on the same Newton physics core\nGPU-Accelerated Performance: A primary benefit of Newton is its GPU-first design. By building on NVIDIA Warp, Newton can simulate many robots or complex interactions in parallel, drastically reducing experiment time\nDifferentiable and Advanced Physics: Newton offers built-in differentiable simulation, unlike standard engines. It can compute gradients of the physics outcomes with respect to input parameters or actions\nImproved Realism and Fidelity: Through collaboration and specialization, Newton is poised to improve realism. Disney’s input ensures that Newton can faithfully simulate the complex mechanisms of animatronic characters (geared linkages, compliance in joints, etc.), which in turn benefits anyone simulating robots with similar complexity. Google DeepMind’s influence ensures Newton maintains scientific rigor (e.g., proper dynamic equations, no arbitrary hacks that learning agents could exploit\nScalability and Community-Driven Development: As an open-source project, Newton invites contributions from the robotics community, much like OpenAI’s Gym or DeepMind’s open-sourced MuJoCo have communities around them. This means the engine can evolve quickly with new features, bug fixes, and performance improvements contributed by users. Its open nature and modular design make it easier to maintain in the long run and adapt to new hardware (future GPUs, etc.) or new research needs. For organizations using different simulators today, Newton offers a path to consolidate efforts: instead of each lab writing their own physics tweaks or each simulator having a separate physics core, Newton can become a standardized backbone. This convergence can lead to better validated physics (since many will test and improve it) and a richer set of tools (common file formats, viewers, analysis scripts that work with Newton outputs, etc.). NVIDIA, DeepMind, and Disney are providing the initial momentum, but the intent is a sustainable, community-driven engine\n\nNvidia Newton represents a significant step forward in robotics simulation. Each partner organization has infused the project with unique strengths – NVIDIA with GPU prowess and integration, DeepMind with fast and accurate contact physics, Disney with demanding real-world robot applications – resulting in an engine poised to serve a wide range of needs. When comparing simulation environments, Newton doesn’t replace MuJoCo or Isaac; instead, it enhances and unifies them. MuJoCo gains unprecedented speed and gradients, Isaac Lab gains openness and advanced physics, and the broader field gains a platform that marries realism with performance. As Newton becomes available (the first version is expected sometime in 2025), we can expect the gap between simulation and reality to narrow, accelerating the development of robots from animated characters in theme parks to general-purpose humanoids in industry."
  },
  {
    "objectID": "posts/note/2021-01-03-Goodbye-2020.html",
    "href": "posts/note/2021-01-03-Goodbye-2020.html",
    "title": "📘Goodbye 2020",
    "section": "",
    "text": "구글 캘린더의 도움을 받아 조금은 늦은 2020 회고록을 적어봤다. 기억이 희미하고 상기하면서 왜곡된 추억을 회상할 수도 있겠지만, 2020을 보내고 2021을 맞이하기에 충분한 시간을 가지는 건 이 시간이 아니면 할 수 없기에 소중하게 생각하며 한 글자 한 글자 적어봤다. 월마다 1~2개의 이야기를 쓰게 될 것 같으니 한 해를 12개 정도의 이야기로 잘 풀어가봐야 겠다."
  },
  {
    "objectID": "posts/note/2021-01-03-Goodbye-2020.html#월---2월",
    "href": "posts/note/2021-01-03-Goodbye-2020.html#월---2월",
    "title": "📘Goodbye 2020",
    "section": "1월 - 2월",
    "text": "1월 - 2월\n독서실 알바를 했었다. 벌써 1년전 일이라서 잊고 있었던 일이었는데..2020년도 일이었다니 새삼 놀랍다. 아빠의 해외 출장에 마음이 헛헛해지기도 하고, 방학 때 쉽게 쳐질 수 있는 생활 패턴도 잡고, 돈도 벌고자 찾은 아르바이트였다. 그동안 대치동 학원 알바도 해보고 동네 수학학원 보조교사도 해봤었는데 이번에는 처음으로 대면(?)알바가 아니었다. 청소담당을 지원했었는데 당시 실장님 면접 때 여자인 내가 와서 고개를 갸우뚱하셨던게 기억에 남는다. 나름 아침형 인간인지라 사람이 오기전 6~8시 사이에 독서실 모든 공간을 청소기 돌리고 물걸레질을 부지런히 했었다.(부모님이 제일 반대하던 알바였는데.. 생각해보면 은근 말 안듣는 딸인 듯)\n그때 청소를 하면서 독서실 위에 놓여진 공시, 어학 책들을 보며, (삼수생이었던 못난 면모를 아직 벗어던지지 못해서 그런지 겨울만 되면 센치해지는 감성때문일수도) 열심히 공부하는 청춘들의 시간이 왜 안타까워보이기만 했다. 오지랖일순 있지만.. 취업시장이 좁아지고, 스펙과 자격증만 바라보게 되고, 좁은 독서실 자리로 향해야한다는 현실이 너무 답답했다. 각설하고 그때 공짜로 받은 자리에서 공부했던 걸 생각해보면 Udacity RL 코스를 열심히 했던 것 같다. TOEFL도 준비하겠다고 옆에 책을 쌓아두긴 했었지만 제대로 공부 안한건 2021에 그대로 업보로 받아 이어지고 있다.(으이구🤪)\n\n오랜만에 가족들과 속초여행을 갔었다. 당시에 코로나가 조금씩 심해지고 있었는데 그땐 “여름까지 코로나가 계속되면 안되는데..”라고 걱정하고 있었다. 왠걸..2020 한 해를 온전히 코로나랑 함께할 줄은 몰랐다.😥 친구들과도 갔었던 속초였지만 가족들과 함께했던 속초는 또 다른 모습으로 즐길 수 있어서 좋았다. 타임랩스로 바다위로 떠오르는 태양을 찍었던 기억이 생생하다. 물회는 맛있었고, 겨울바다의 소리는 정말 맑고 시원했다.\n\n사실 2019 겨울부터해서 2월까지 KPMG Ideation이라는 대회에 멋진 분들과 함께 준비하고 있었다. 다른 대회보다 달랐던 점은 정말 나에게는 도전 그 자체였기에 더 기억에 남는다. 그 동안 기술적인 성격의 대회들은 대부분 숫자로 표현된 성적으로 판단하는게 대부분이었는데 이 대회는 아이디어 제안 성격을 가지고 있었기에 엔지니어 마인드보다 기획자 마인드를 배우게 되었던 것 같다. 또한 잘 몰랐던 NLP 분야에 대해 공부할 수 있는 기회였고, 특허라는 분야, 변리사라는 직업에 대한 이야기 등 새로운 세상 이야기 좋아하는 나에게는 진짜 재밌었던 시간이었다. 잊지못할 해프닝이 있던 대회이기도 했는데, 본선 대회 당일 대회장에 도착하자 마자 집으로 돌아가라고 통보(?) 받았던 대회였기도 했다. 혹시라도 밝히고 싶지 않으실 수도 있으니까 최대한 말을 아끼고 내 마음속에 저장하겠지만, 정말 감사했던 점은 함께 해주셨던 팀원분들이 정말 다 멋진 분들이었다는 것이다. 밤을 새워가며 함께 나누었던 이야기들이나 생각들 모두 너무 좋았고 평생 남을만한 따뜻한 추억이 생겼다는게 행복했다."
  },
  {
    "objectID": "posts/note/2021-01-03-Goodbye-2020.html#월---6월",
    "href": "posts/note/2021-01-03-Goodbye-2020.html#월---6월",
    "title": "📘Goodbye 2020",
    "section": "3월 - 6월",
    "text": "3월 - 6월\n본격적으로 코로나로 인한 대혼란이 시작됐다. 상황이 좋아질 기미가 보이지 않고 개강이 미뤄졌다. 지금은 익숙하지만 처음접하게 된 온라인 수업은 정말 당황 그 자체였다. 등하교 3시간 통학러인 나에게는 시간을 세이브할 수 있는 장점도 있긴 했지만 집에서 하루종일 노트북만 보고 있으면서 “이게 뭐하는 건가..”싶은 생각이 든게 한 두번이 아니었다. 진짜 대학의 역할에 대해 진지하게 고민도 했었다. 그래도 바쁘게 1학기를 잘 보냈고 랩실 생활도 열심히 했던 것 같다. 아 생각해보니 이때부터 나그네처럼 랩실을 다녔던 생활을 마치고 정식으로 학부연구생으로 인정받아 돈을 받으면서 연구하게 되었다.\n\n종강을 하고 미래연구소 14기 서브튜터를 하게되었다. 내가 처음으로 딥러닝을 공부하게 된 곳에서 서브튜터로 일하게 되었다는 게 정말 신기하고 감사했다. 대학교 가자마자 우연히 보게된 글을 보고 (지금 생각해보면 겁도 없이 혼자 찾아간게 신기하지만) 미래연구소 1기로 딥러닝을 공부했다. 오랜만에 랩장님도 보고 몰라보게 커진 미래연구소 모습을 보면서 괜시리 뿌듯하기도 했다. 사실 메인튜터님이 많이 배려해주시기도 하시고 서브튜터 업무 자체는 크게 부담스럽지는 않았지만 돈을 받고 일하는 자리는 항상 긴장하게 되기 때문에 조금 스트레스를 받았던 것 같긴하다. 14기 분들 중 완전 입문자를 위한 파이썬 기초 스터디는 따로 혼자 운영해야 했기 때문에 좀 더 긴장했던 것 같기도 하다. 그래도 항상 DL 공부 관련해서 INPUT만 했던 입장에서 처음으로 OUTPUT을 하게되는 도전적인 경험이었고, 지식적인 면으로나 태도적인 면으로나 성장할 수 있었던 큰 도약점이 됬었다. 부족한 서브튜터를 만났지만 열심히 공부하셨던 14기 분들이 모두 성공하셔서 나중에 커뮤니티에서 만나뵈면 정말 행복할 것 같다."
  },
  {
    "objectID": "posts/note/2021-01-03-Goodbye-2020.html#월---8월",
    "href": "posts/note/2021-01-03-Goodbye-2020.html#월---8월",
    "title": "📘Goodbye 2020",
    "section": "7월 - 8월",
    "text": "7월 - 8월\n여름방학에 역시 많은 일들이 있었다. 우선 정말 다이나믹했던 2020 국제창작자동차대회 PostNomad팀으로 참가한 이야기.🦈 4학년 분들의 졸업프로젝트였지만 운좋게도 참여할 수 있는 기회가 주어져서 제어팀으로 합류할 수 있었다. 후에는 딥러닝 테스크 비중이 높은 비젼팀으로 옮겨가야만 했지만. 그동안은 학교내에서 팀을 꾸려서 대회에 나가기보다는 대외 스터디나 모임에서 만난 분들과 프로젝트들을 했었기 때문에 또 다른 느낌이었다. 대형학과이다 보니 사실상 같은 학과여도 서로 잘 모르기 때문에 졸업할 때까지 모르는 동기도 많다. 평소에 아싸생활을 하는 나로써는 더더욱 학과 사람들이랑 친해질 기회가 없었는데 처음으로 기계과 사람들과 함께할 수 있었던 시간이 아니었나 싶다. 지금 회고하는 이 시점에서는 대회 결과도 아쉽고😢 준비하는 과정에서 랩뷰만 써야하는 답답함, 협업하면서 느꼈던 어려움과 실망했던 점들도 많지만, 진짜 소중한 시간들이었다. 지하 작업실에서 회의하고, 함께 공구들을 나르고, 덥고 습한 K-City에서 다같이 고생하고 고민했던 모든 시간들이 감사하다. 이 회고록을 빌려 고백을 하자면.. 우리 팀에게 조금 더 잘하지 못한 게 죄송하다. 다들 열심히 하시고 항상 나를 배려해주셨던 것 같은데 그에 비해 나는 잘 따라가는 팀원은 아니었던 것 같다.\n\n랩실 연구를 비롯해서 TOEFL 공부까지 하느라 정말 열심히 살았는데, 사실 이때 제대로 공부하지 않아서 후에 고생하게 된 건 안비밀이다. 항상 느끼는 것이긴 한데 나는 한가지 일을 집중해서 끝내는 능력이 아직도 부족한 것 같다. 이에 더하여 2020 Korea Health Datathon에 참가하여 부비동 데이터셋으로 최종 4위를 했었다. NSML 플랫폼은 할말하않이긴 하지만 꾸준히 데이터톤 경험을 갖게 해주신 것만으로도 감사하다. 2019 대회에도 참여했었는데 그때보다 발전된 성적을 가질 수 있어서 좋았다.\n\n산티아고 순례길을 걷고 싶다는 버킷리스트가 생겼다. 그래서 하나씩 뭘 준비해야 하나 고민하는 중에 체력을 길러야 겠다는 생각을 했다. 이에 더해 코로나로 떨어진 활동성을 보충하고자 등하교를 따릉이로 하기 시작했다. 가는데에만 2시간 걸리는 여정이었지만 한강을 따라 가는 길이 나쁘지 않았기 때문에 충분히 좋은 도전이었다. 처음에는 다들 미친 짓(?)이라고 만류했었고 나도 반신반의 했었지만 막상 해보니 죽을 정도는 아니었고 자전거 타고 보는 한강은 다리아픈 것 따위 다 잊게 만들정도로 예뻤다. 아침은 아침대로, 저녁은 저녁대로, 맑으면 맑은대로, 흐리면 흐린대로. 때로는 상수나들목 공사때문에 당황하기도 했지만 길은 항상 찾으면 되는 거였다."
  },
  {
    "objectID": "posts/note/2021-01-03-Goodbye-2020.html#월---10월",
    "href": "posts/note/2021-01-03-Goodbye-2020.html#월---10월",
    "title": "📘Goodbye 2020",
    "section": "9월 - 10월",
    "text": "9월 - 10월\n2학기도 여전히 코로나로 인해 온라인으로 진행됐다. 학기중에는 거의 수업에 집중하다보니 딱히 회고록에 적을 내용이 없는 것 같아서 한빛미디어에서 “나는 리뷰어다”로 참여했던 경험을 적어볼까 한다. 리뷰어로 활동을 하면서 2개의 도전이 있었다. 첫째는 글쓰는 것 자체에 대한 도전이었는데, 예전에는(그러니까 고2때 까지만 하더라도) 글을 쓰는데 어려움이 없었던 것 같다. 그리고 나름 글을 잘 쓴다고 인정도 받았던 것 같은데 이후에 딱히 글을 쓸 기회가 없었고 쓰지 않다보니 리뷰를 쓴다는 것 자체가 어색했다. 그리고 “독후감”과는 다른 목적이 있는 “리뷰”라는 글에 대한 고민이 있었다. 출판사에서 책을 주면서 나에게 리뷰를 쓰기를 원하는 니즈는 분명 있는 것이고, 다른 사람들에게 책의 내용이 잘 어필이 되길 바라는 것일터였다. 그러면 단순히 책의 장단점을 나 혼자 판단하고 즐기고 끝나는 것이 아닌 다른 사람들에게 잘 전달될 수 있도록 표현해야 한다는 것이었다. 두번째로는 빠르게 기술서를 봐야한다는 도전이었다. 사실 IT전공도 아니고 원래는 1개의 책도 적어도 2~3개월을 봐야하는 거북이 속도인데 리뷰를 하려면 1달에 1권을 무조건 다 보고 리뷰까지 완성해야 했다. 몇개의 책들은 사실 다 보지도 못하고 리뷰 적기에 급급했던 것도 사실이다. 제대로 리뷰어로 활동하지 못해 관계자분들께 죄송하다. 그래도 몇몇 리뷰는 제대로 적었다는 것에 스스로 조금 위안을 삼아본다."
  },
  {
    "objectID": "posts/note/2021-01-03-Goodbye-2020.html#월",
    "href": "posts/note/2021-01-03-Goodbye-2020.html#월",
    "title": "📘Goodbye 2020",
    "section": "11월",
    "text": "11월\n지금까지 인생의 경험들 중에 최악과 최고를 다 뽑으라고 하면 2020.11월에 다 있다. 좋은 것부터 먼저. 우선 최고는 내 인생 처음으로 학회에서 내가 한 연구를 가지고 발표를 완성도 있게 마무리 할 수 있었고 인정도 받아서 우수논문상까지 받게 된 경험이다. 진짜 학회 발표 전 리허설하는 랩미팅에서 울면서 나가기도 했었고 발표 전날까지도 결과가 잘 나오지 않아 정말 힘들었다. “힘들었다”라는 4글자로 밖에 표현 못한다는 게 억울할 정도로 최고의 스트레스를 받은 시간이었다. 그래도 진짜 주변에 천사들을 심어 놓으신 것인지 기적적으로 도움도 받고 몇일 밤을 새서 어찌저찌 마무리 할 수 있었고 IPNT에서 구두발표도 잘 마무리하여 우수논문상도 받게되었다.(지금생각해도 기적이라고 말할 수 밖에 없다.) 진짜 힘들었던 만큼 최고의 성취감은 말로 할 수 없었다.\n다음으로 최악의 경험은 사실 최고의 경험과 관련이 깊다. 앞서 적은 “최고의 스트레스”가 복선이었다. 학회를 마치고 체력이 바닥으로 떨어질대로 떨어졌고 긴장은 완전히 풀어진 상태에서 몸이 엄청 아팠다. 감을 먹고 체한 탓도 있었지만 몸이 정말 아팠고 힘이 빠지면서 “이대로 죽을 수도 있겠구나..” 싶은 생각이 들었다. 인생 처음으로 자다가 새벽에 구급차를 불러 응급실에 갔던 게 최악의 경험이지 않나 싶다. 근데 유감스럽게도 구급대원분들이 오시면서 급속도로 괜찮아져서 정말 난처하고 민망했다. 나중에 새벽 4시쯤 엄마랑 응급실을 나와서 걸어서 집으로 돌아갔다. 이때의 일은 가족들에게도 큰 충격을 주어서 지금도 아빠는 잊을만 하면 이야기를 하시는데 하지말라고 장난스럽게 말을 하면서도 죄송한 마음이 들기도 한다. 웃프긴 하지만 이 일 이후로 내가 제일 좋아하는 “감”은 금지어가 되었다."
  },
  {
    "objectID": "posts/note/2021-01-03-Goodbye-2020.html#월-1",
    "href": "posts/note/2021-01-03-Goodbye-2020.html#월-1",
    "title": "📘Goodbye 2020",
    "section": "12월",
    "text": "12월\n사랑하는 양가의 조부모님들이 몇년전만 해도 다 살아계셨다. 하지만 근 2년 정도 매년 겨울에 사랑하는 분들을 떠나보내게 되었다. 외할아버지, 친할아버지, 그리고 외할머니까지. 올해 후반에 건강이 급속도로 나빠지신 외할머니가 결국 우리곁을 떠나셨다. 4분의 할머니 할아버지 가운데 가장 사랑의 표현도 아끼시지 않고 항상 전화도 먼저 걸어주셨던 멋진 할머니였다. 지금 이렇게 “할머니 사랑해요”라고 말하던게 이렇게 그리워할 것이었다면 살아계셨을때 왜 그렇게 어색해하고 표현하기 부끄러워 했는지. 코로나로 인해 좋았던 점 하나는 장례식에 손님들 없이 식구들끼리 할머니를 추억하면서 얼마나 멋진 분이셨는지 되새길수 있어서 좋았다. 진짜 멋진 분이셨다."
  },
  {
    "objectID": "posts/note/2021-01-03-Goodbye-2020.html#bnm2h",
    "href": "posts/note/2021-01-03-Goodbye-2020.html#bnm2h",
    "title": "📘Goodbye 2020",
    "section": "BNM2h",
    "text": "BNM2h\n2020년도에 감사했던 많은 일들이 있었지만, 지금 나름 뿌듯하고 보람찬 시간들을 회고할 수 있도록 도와준 많은 분들이 있었다. 스터디를 통해서 만나는 인연들이 대부분이다. 물론 많은 스터디에 참여해보고 도전 받을 수 있는 좋은 시간들이었다. 그래도 가장 애정가는 스터디는 아무래도 BNM2h가 아닐까 싶다. 그렇다. 사실 가장 애정가는 이유 중 하나는 아무래도 내가 만든 스터디였기에 가장 책임을 느꼈고 가장 스트레스도 많이 받고 가장 노력했음을 나 스스로도 느꼈기 때문일 것이다.\n처음에는 진짜 가벼운 마음으로 시작했다. 따지고 보면 2019년도에 Kaggle KR에서 스터디 리더를 뽑는다는 글을 보고 마침 캐글에서 Connect-X라는 강화학습 대회가 베타수준으로 시도하기 시작했던 때라 “내가 한번 캐글 강화학습 스터디 리더가 되어보자!”라는 생각과 패기로 시작했던 거였다. 패기는 패기였던 걸로.. 그렇게 시작했으나 함께할 팀원분들이 모집되지 않아 그냥 한 순간의 불꽃으로 끝날 뻔 했다. 다행이도 다른 리더분들 중에 나와 같은 처지였던 분이 계셨었고 감사하게도 같이 공부하자 먼저 손을 내밀어 주셔서 BNM2h라는 스터디가 생기게 되었다!🙌\n그때 그때 마다 인연이 되는대로 지금까지 이어져오고 있는 스터디에서 강화학습을 공부하고 있다. 최고의 스터디라고 자랑할 순 없지만 최애 스터디라고는 할 수 있다. 아직도 강화학습이라는 분야에 대해, 스터디에서 함께 공부하는 방식에 대해, 스터디 매니징하는 것에 대해, 감사함과 겸손함을 표현하는 것에 대해 한참 모자른 애송이이지만 매주 스터디에 나와주셔서 나에게 성장할 기회를 주고 그런 시간들을 함께 보내주시는 BNM2h 스터디원분들은 천사들이신 것 같다. 나도 그런 분들에게 조금이나마 도움이 되고 함께 성장하기 위해 조금 더 노력하는 사람이 되어야겠다고 느낀 한 해였다."
  },
  {
    "objectID": "posts/note/2021-01-03-Goodbye-2020.html#마무리",
    "href": "posts/note/2021-01-03-Goodbye-2020.html#마무리",
    "title": "📘Goodbye 2020",
    "section": "마무리",
    "text": "마무리\n물론 당연히도 여기에 적지 못한 많은 이야기들이 있다. 이야기들은 열정을 담기도 하고, 아쉬움을 담기도 하고, 기쁨을 담기도 하고, 슬픔을 담기도 한다. 그 이야기들은 지금의 나의 마음이나 기억 속 어딘가에 잘 살아있겠지.\n멋있는 개발자분들의 회고록 같은 것을 기대했으나 적고나서 읽어보니 아직은 어디에 내놓기 부끄러운 새벽감성의 회고록이니 그냥 조용히 블로그에 남기기로 생각했다.😂\n잘가요 2020"
  },
  {
    "objectID": "posts/note/2025-03-30-gueltto-10th-end.html",
    "href": "posts/note/2025-03-30-gueltto-10th-end.html",
    "title": "📘글또를 마치며",
    "section": "",
    "text": "글또 활동 마무리 회고글을 작성하는 날이 왔네요.\n2022년 5월, 석사 1학기를 마무리하던 즈음, “글을 쓰는 것이 나를 성장시킬 것이다”라는 믿음 하나로 글또를 시작했습니다. 그리고 어느덧 시간이 흘러 2025년 3월, 사회 초년생이 된 지금, 그 긴 여정의 끝자락에 서 있습니다.\n사실 7기를 시작할 때만 해도, 그 이후의 기수까지 계속 참여하게 될 거라고는 상상하지 못했습니다. 하지만 매 순간, 크고 작은 고민과 방황 속에서 나를 붙잡아줄 무언가가 필요했고, 그럴 때마다 글또가 제게 하나의 버팀목이 되어주었습니다. 그리고 그렇게 이어지다 보니, 어느덧 관성이 습관이 되었고, 습관은 지금의 제가 되었네요.\n세상이란 주관성과 객관성이 공존하는, 나를 둘러싼 환경이라고 생각합니다. 그리고 그 중에서도, 좀 더 주관적으로 해석할 수 있는 세상, 즉 나라는 세상의 무대를 글로써 담아내며, 저만의 방식으로 나의 세상을 바꿔왔습니다. 그런 의미에서 저는 분명히, 제 세상을 바꿔온 사람입니다.\n이제, 그 세상을 바꿔온 제 여정을 회고하며 되돌아보려 합니다."
  },
  {
    "objectID": "posts/note/2025-03-30-gueltto-10th-end.html#그리고-잇기",
    "href": "posts/note/2025-03-30-gueltto-10th-end.html#그리고-잇기",
    "title": "📘글또를 마치며",
    "section": "그리고, 잇기",
    "text": "그리고, 잇기\n마지막으로, 지금까지 글또에서 항상 시작할 때와 마무리할 때 느꼈던 마음들, 그리고 중간중간, 스스로를 되돌아봐야 했던 시기에 적어두었던 작은 일기들을 한데 모아 정리해보았습니다. 그 순간의 감정과 생각들이 고스란히 담긴 기록들입니다. 때로는 방황했고, 때로는 나아갔으며, 언제나 그 안엔 조금씩 성장해온 제가 있었습니다.\n\n\n\n\n📘Geultto 7th Start\n📘2022 상반기 회고\n📘Gueltto 7th End\n📘Geultto 8th Start\n📘Geultto 8th End\n📘Geultto 9th Start\n📘Geultto 10th Start\n📘글또 발표 후기"
  },
  {
    "objectID": "posts/note/2022-10-11-daily-english-002.html",
    "href": "posts/note/2022-10-11-daily-english-002.html",
    "title": "🌎Casual English Phrases 002",
    "section": "",
    "text": "영어 회화 표현이나 논문에서 쓸 수 있는 유용한 표현들을 조금씩 익히기 위해 시작한 1000일 시리즈 포스트 입니다. 공부 리소스는 유튜브, 리얼클래스, PN님의 블로그, cake 어플, 책, 논문, 기사 등 다양한 출처에서 보고 기록합니다.\n\n\n속마음을 보여주다\nwear your heart on your sleeve\n\nIf you wear your heart on your sleeve, you openly show your feelings or emotions rather than keeping them hidden. 속마음을 보여준다는 말은, 너의 감정이나 느낌들을 숨기기보다 보여준다는 거야.\nIt takes courage to wear your heart on your sleeve, which is why few people do it. 속마음을 털어놓는다는 건 용기가 필요한 일이기 때문에 많은 사람들이 하지 못하는 일이지.\nI need someone whom I can wear my heart on my sleeve. 내 속마음을 털어놓을 사람이 필요해.\n\n\n\n고지식하거나 융통성 없는 성격\n약간 모범생 이미지\nsquare\nIf you are a SQUARE: You are an organized, logical, and hardworking person who likes structure and rules.\n\nBe there, or be square. : The expression be there or be square means that if one declines to attend an event, one is considered “uncool.” It implies that the event will be exciting.\nDon’t be square. 고리타분하게 굴지마\nShe is such a square - I’ve never met anyone so boring. 그녀는 진짜 고지식한 사람이야 이때껏 그사람처럼 지루한 사람은 처음봤어.\n\n\n\n위선적인\ntwo-faced\n\nDon’t trust her - I suspect she’s a bit two-faced. 그녀를 믿지마. 그녀가 조금 위선적인 사람인 것 같아.\nHe’s a two-faced cheater. 그는 두얼굴을 사진 사기꾼이야.\nHe had been devious and two-faced. 그는 기만적인고 양면적이었다.\n\ndevious : 정직하지 못한, 기만적인"
  },
  {
    "objectID": "posts/note/2023-03-30-daily-english-006.html",
    "href": "posts/note/2023-03-30-daily-english-006.html",
    "title": "🌎Casual English Phrases 006",
    "section": "",
    "text": "불만족스러운, 불편한\nnot (too) happy\n\nStomach’s not too happy, is it? 배가 고프구나?(아프구나)\nMy stomach’s not happy with me. 배가 아파.(고파)\n\n\n가끔씩 대화를 하다가 맥락을 서로가 알고 있는 상황이면 a 나 your 등을 생략하는 경우가 있음.(Stomach)\n\n\n\n유감스럽게도\nI’m afraid ~\n\nI’m afraid I haven’t heard any cookies calling you, plain or otherwise. 유감이지만 그냥 쿠키든 아니든 널 부르는 쿠키는 없었어.\nI’m afraid your suggestion is not accepted. 유감이지만 네 제안이 받아들여지지 않았어.\nI’m afriad I can’t go to your party. 미안하지만 네 파티에 못 갈 것 같아.\n\n\n\n속이다\nput (someone) on\n\nHe must be wondering if I was just putting him on. 내가 자기를 속이고 있는 건지 궁금해하고 있을 게 뻔해.\nDon’t believe it! She’s putting you on! 믿지마! 그녀는 너를 속이고 있어.\n\n\n\n기다리다 / 견디다, 버티다\nhang on\n\nHang on a few more minutes, I’m looking. 조금만 더 기다려, 찾고 있으니까.\nHang on a moment. 네 잠시만 기다리세요.\nIf you can just hang on for a little longer, we’ll finish soon. 네가 조금만 더 버텨준다면, 우리는 곧 끝낼거야.\n\n\n\n~로 만들어지다\nbe made of\n\nWhat’s it made of, rotten zombie eyes or something? 대체 뭐로 만든거야, 썩은 좀비 눈 같은 건가?\nThis desk is made of baobab wood. 이 책상은 바오밥 나무로 만들어졌다.\n\n\nbe made from : 재료의 형태를 알아 볼수없는 경우. Cheese is made from milk.\n\n\n\n역겨운\nsick\n\nJust looking at it almost makes me sick. 그냥 보기만 해도 구역질 나와.\nHe makes me sick! 그가 하는 짓은 역겨워!\nJust thinking about it makes me sick to the stomach. 그거 생각하는 것만으로도 정말 역겨워."
  },
  {
    "objectID": "posts/note/2025-01-13-ros2-build-issue.html",
    "href": "posts/note/2025-01-13-ros2-build-issue.html",
    "title": "📝ROS2 Build Issue",
    "section": "",
    "text": "Issue\nError Message\nCMake Error at CMakeLists.txt:5 (find_package):\n  By not providing \"Findament_cmake.cmake\" in CMAKE_MODULE_PATH this project\n  has asked CMake to find a package configuration file provided by\n  \"ament_cmake\", but CMake did not find one.\n\n  Could not find a package configuration file provided by \"ament_cmake\" with\n  any of the following names:\n\n    ament_cmakeConfig.cmake\n    ament_cmake-config.cmake\n\n  Add the installation prefix of \"ament_cmake\" to CMAKE_PREFIX_PATH or set\n  \"ament_cmake_DIR\" to a directory containing one of the above files.  If\n  \"ament_cmake\" provides a separate development package or SDK, be sure it\n  has been installed.\n\n\nSolution\nThis error typically appears when CMake cannot locate ament_cmake. In ROS 2, ament_cmake is a core build system package that you must have installed (and sourced) before building any ament-based packages.\nEnsure the ament_cmake package is actually installed. On Ubuntu, for Humble, you’d expect something like:\nIf you have a minimal or custom ROS 2 installation, you might need to install additional packages to get ament_cmake.\nsudo apt-get install ros-humble-ament-cmake"
  },
  {
    "objectID": "posts/note/2025-07-31-linux-cheetsheet.html",
    "href": "posts/note/2025-07-31-linux-cheetsheet.html",
    "title": "📝Linux Cheet Sheet",
    "section": "",
    "text": "git branch 삭제\n# 로컬에서 브랜치 삭제하기 명령어\ngit branch -d localBranchName(로컬의 브랜치 이름)\n\n# 원격에서 브랜치 삭제하기 명령어\ngit push origin --delete remoteBranchName(원격 브랜치 이름)\n\n\ngit command로 commit\n# If the branch already exists:\ngit checkout [branch-name]\n\n# Otherwise, create it and switch:\ngit checkout -b [branch-name]\ngit add .\ngit commit -m \"commit message\"\ngit push\ngit push -u origin [branch-name]\n\n\n다중 GPU에서 사용할 특정 GPU 설정\n\nexport CUDA_VISIBLE_DEVICES=&lt;idx&gt;[,&lt;idx&gt;...] 로 사용할 GPU만 노출\n다중 GPU 환경에서 “어떤 GPU를 쓸지” 지정하는 가장 간단한 방법은 환경 변수 CUDA_VISIBLE_DEVICES 를 설정. 이 변수에 지정한 인덱스(들)만이 애플리케이션에서 “GPU 0,1,2…” 로 인식되기 때문에, 실제 머신에 여러 개 장착된 GPU 중에서 원하는 것만 골라 쓸 수 있음\n\n터미널에서 아래 명령으로 현재 시스템에 붙어 있는 GPU 목록과 각 ID를 확인:\nnvidia-smi # 여기서 좌측의 `GPU` 컬럼이 실제 시스템 인덱스\n\n단일 GPU 사용\nexport CUDA_VISIBLE_DEVICES=1\npython train.py\n이렇게 하면 실제 시스템의 GPU#1 이 “내부적으로 0번 GPU” 로 보이게 됩니다.\n여러 GPU 사용\nexport CUDA_VISIBLE_DEVICES=0,2\npython train.py\n이렇게 설정하면 시스템 GPU 0번과 2번만 보이게 되고, 내부적으로는 [ “가상 GPU0” ← 시스템0, “가상 GPU1” ← 시스템2 ] 로 취급됩니다.\n\n\nTip: 쉼표 없이 CUDA_VISIBLE_DEVICES=0 또는 1, 공백 없이 0,2,3 처럼 지정해야 합니다.\n\n\n\n소스코드 찾기 grep\n\n파일 내에서 또는 입력값으로부터 특정 패턴을 검색\n\n\noption\n\n-v : 일치되는 내용이 없는 라인을 표시\n-c : 일치되는 내용이 있는 행의 개수를 표시\n-l : 일치되는 내용이 있는 파일 이름만 표시\n-h : 일치되는 내용을 찾은 파일의 이름을 표시하지 않음\n-n : 일치되는 내용이 있는 행은 행 번호와 함께 표시\n\n특정 문자열로 찾기: grep -r \"TEXT\" [PATH]\n\n예: grep -r \"TEXT\" ./*\n색 옵션: grep --color=auto \"TEXT\" ./*\n\n파일명만 보기: grep -l \"TEXT\" [PATH]\n특정 경로 및 파일명을 명시하여 탐색: find [PATH] -name \"FILENAME\" | xargs grep \"TEXT\"\n\nAdvanced\n\nripgrep\nfzf\n\n\n\nAppImage icon\nAppImage icon 생성\n\n.AppImage 파일 및 icon 이미지(.png) 경로: /opt\ndesktop 바로가기 설정 경로: /usr/share/applications\n\n\nappimage 파일과 icon image를 /opt로 이동\n\ne.g. /opt/[app_name].AppImage & /opt/[app_name].png\n\n바로가기 설정 파일 [app_name.desktop]을 /usr/share/applications에 만들어줌\n\n[Desktop Entry]\nName=[app_name]\nComment=[AppImage entry]\nExec=/opt/[app_name].AppImage\nIcon=/opt/[app_name].png\nType=Application\nTerminal=false\nEncoding=UTF-8\nCategories=Utility;\n\n\nconda 자동 활성화 해제\nconda config --set auto_activate_base false\n\n\nconda lib error\nImportError: libpython2.x.so.1.0: cannot open shared object file: No such file or directory\n또는\nTraceback (most recent call last):\n  File \"/path/to/project/load_object.py\", line 3, in &lt;module&gt;\n    import torch\n  File \"/path/to/env/lib/python3.10/site-packages/torch/__init__.py\", line 367, in &lt;module&gt;\n    from torch._C import *  # noqa: F403\nImportError: /path/to/env/lib/python3.10/site-packages/torch/lib/…/nvidia/cusparse/lib/libcusparse.so.12:\n  undefined symbol: __nvJitLinkComplete_12_4, version libnvJitLink.so.12\n\nSolution: export LD_LIBRARY_PATH=/home/avery/anaconda3/envs/[가상환경이름]/lib\n\npath는 conda env list로 구체적인 해당 경로 확인 필요. mamba나 miniforge3을 사용할 경우 경로가 다를 수 있음.\n\n\n\n\nconda freezing\nconda env export &gt; environment.yaml\nconda env create --file environment.yaml\n\npip list --format=freeze &gt; ./requirements.txt\npip install -r requirements.txt\n\n\nsymbolic link 설정 및 관리\n\n\n심볼릭 링크는 파일 시스템 내에서 다른 파일이나 디렉토리의 경로를 가리키는 “바로 가기”와 같은 역할을 하므로, 실제 파일이 아닌 링크 파일임을 유의\n여러 버전의 라이브러리나 프로그램 관리 시, 심볼릭 링크가 어떤 타겟을 가리키고 있는지 확인하는 것이 중요\n여러 버전의 프로그램(예: python, cuda)을 설치했을 때 어떤 버전이 활성화되어 있는지 관리\n\n\n\n심볼릭 링크 삭제\nrm &lt;symbolic_link&gt;\n예시:\nrm /usr/local/bin/python\n심볼릭 링크 생성\nln -s &lt;target&gt; &lt;symbolic_link&gt;\n예시:\nln -s /opt/python3.9/bin/python /usr/local/bin/python\n심볼릭 링크 변경 (덮어쓰기) 기존 링크를 새 타겟으로 변경할 때 사용합니다.\nln -Tfs &lt;new_target&gt; &lt;symbolic_link&gt;\n예시:\nln -Tfs /opt/python3.10/bin/python /usr/local/bin/python\n심볼릭 링크 확인\nls -l &lt;symbolic_link&gt;\n예시:\nls -l /usr/local/bin/python\nlrwxrwxrwx 1 root root 24 2025-01-22 12:34 /usr/local/bin/python -&gt; /opt/python3.9/bin/python\n\n\n\nlibrary 경로 설정\n\nconda env lib 연결시\n\n$ export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/usr/work/mylib\n\nvi ~/.bashrc (파일명 앞의 .은 숨김파일을 의미)\nexport LD_LIBRARY_PATH=/home/user/work/mylib:${LD_LIBRARY_PATH}\n\n\nGPU Memory 해제\n\n프로세스가 죽지 않을때 강제 종료\n\nps aux | grep python # 딥러닝 학습을 실행시킨 python 파일의 실행 ID를 찾기\nsudo kill -9 [ID_NUMBER]\n\n\nDocker\n\ndocker: Error response from daemon: could not select device driver \"\" with capabilities: [[gpu]].\nnvidia-container-toolkit 설치\n\n$ distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \\\n   && curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - \\\n   && curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list\n\n$ sudo apt-get update && sudo apt-get install -y nvidia-container-toolkit\n\n$ sudo systemctl restart docker\n\nGPU 설정\n\ndocker run -it --gpus '\"device=0,1,2,3\"'\n\ncontainer를 image로 저장\n\ndocker commit [CONTAINER_ID] [IMAGE_NAME]\n\n\n\n\nterminal zsh/bash\nchsh -s bin/bash\nchsh -s bin/zsh\n\n(@terminal)\n/bin/bash\n/bin/zsh\n\n\n용량 확인\ndf -h\n\n해당 경로에 있는 모든 파일과 폴더들을 용량 단위로 표기\n폴더가 1 순위, 파일이 2 순위로 나열\n경로에 있는 모든 폴더들과 각 폴더내에 있는 파일들을 모두 표기\n\ndu -ch [path] | sort -h\n\n해당 경로의 폴더 기준 트리 구조의 깊이 1만큼의 수준까지 탐색\n용량이 큰 폴더가 아래로, 가장 용량이 큰 10개만 보임\n\ndu -ch [path] --max-depth=1 | sort -h | tail -10\n\n현재 경로의 파일 용량\nls -lh\n\n\n\n파일 찾기\nfind [옵션] [경로] [표현식]\n\n# 현재 위치에서 log가 들어가는 파일 모두 찾기\nfind . -name \"*log*\"\n\n# 현재 디렉토리에서 .txt 확장자 파일 검색 후 모두 삭제\nfind . -name \"*.txt\" -delete\n\noptions\n\nP : 심볼릭 링크를 따라가지 않고, 심볼릭 링크 자체 정보 사용.\nL : 심볼릭 링크에 연결된 파일 정보 사용.\nH : 심볼릭 링크를 따라가지 않으나, Command Line Argument를 처리할 땐 예외.\nD : 디버그 메시지 출력.\n\n\n\n\n폴더나 파일 갯수 찾기\n\n현재 위치에서의 폴더 개수 ls -l | grep ^d | wc -l\n현재 위치에서의 파일 개수 ls -l | grep ^- | wc -l\n현재 폴더의 하위 파일 개수 find . -type f | wc -l\n\n\n\nmount 다중 경로 설정\n\n원래 마운트 지점에서 VFS(가상 파일 시스템) 노드를 생성\n\nmount --bind original-dir original-dir\n\n원래 마운트 지점을 공유로 표시\n\nmount --make-shared original-dir\n선택한 마운트 지점과 그 아래의 모든 마운트 지점에 대한 마운트 유형을 변경하려면 --make-shared 대신 --make-r shared 옵션 사용\n\n중복 생성\n\nmount --bind original-dir dupulicate-dir\n\n지우기\n\nsudo umount duplicate-dir\n\n\n\n\n원격과 로컬 사이 파일 복사\n\nscp 사용. scp [options] [source] [target]\n\n\n로컬에서 원격으로\n\nscp [source_path] [User]@[IP]:[target_path]\nscp /media/avery/source/test.txt avery@xxx.xx.x.xxx:mnt/\n\n원격에서 로컬로\n\nscp [User]@[IP]:[source_path] [target_path]\nscp avery@xxx.xx.x.xxx:mnt/test.txt /home/avery/Documents\n\n원격(A User)에서 원격(B User)으로\n\nscp [User]@[IP]:[source_path] [User]@[IP]:[target_path]\nscp ai@xxx.xx.x.xxx:mnt/test.txt avery@xxx.xx.x.xxx:/home/avery/Documents\n\n복수의 파일 전송\n\n여러 경로를 \" \"을 이용하여 묶어줌\nscp [options] [User]@[IP]:\"[file_1] [file_2]\" [target_path]\n\nOptions\n\n-r: 폴더를 복사할 때 사용(전송 대상을 폴더로 지정). 모든 폴더들을 재귀적으로 복사\n-P: ssh 포트 지정\n-i: identity file을 지정해서 사용\n-v: 상세 내용을 보면서 디버깅할 때 사용\n-p: 전송 시 파일의 수정 시간과 권한을 유지\n\n\n\n\n.cache 용량 해결\n\npip와 conda의 .cache 삭제\n\n#pip\npip cache purge\n\n#conda\nconda clean -a\n\n.cache 위치 변경\n\n# .bashrc를 vim으로 열어서 아래 변수를 추가하기\nexport XDG_CACHE_HOME=\"/{disk_location}/{user_name}/.cache\"\n\n# 해당 경로 생성하기\nmkdir -p /{disk_location}/{user_name}/.cache\n\n\nvim 편집\n\n:%d: 전체 내용 삭제 (press Esc to ensure you’re in Normal mode first)\ni 입력 후, Ctrl + Shift + v: 새로운 내용 붙여넣기\n:wq: 저장\nNormal mode shortcuts:\n\ngg (go to the first line of the file)\ndG (delete from current line to the end of the file)\n\n\n\n\npython module 경로 파악\nimport module\nprint(module.__file__)"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Pick-GPT\n\n\n\nApr 12, 2024\n\n\n\n\n\n\n\n\n\n\nDeep Reinforcement Learning for Fall-Recovery Control on Non-Flat Terrain of Quadruped Robots\n\n\n\nDec 11, 2020\n\n\n\n\n\n\n\n\n\n\nSelf-driving Public Mobility Get-off Safety System\n\n\n\nNov 12, 2020\n\n\n\n\n\n\n\n\n\n\nDeep Reinforcement learning for DME Pulse Design\n\n\n\nNov 11, 2020\n\n\n\n\n\n\n\n\n\n\nActive Learning Algorithm for Object Detection and Segmentation\n\n\n\nNov 12, 2019\n\n\n\n\n\n\n\n\n\n\nSmart Device for Dog Triaining\n\n\n\nOct 12, 2019\n\n\n\n\n\n\n\n\n\n\nTraffic Sign Detection and Recognition Task\n\n\n\nOct 12, 2019\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Curieux.JY",
    "section": "",
    "text": "Hello👋 I’m Jung Yeon. Thanks for visiting my blog.\n\n\nThis is where I document everything I explore with curiosity.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n🧩CoRL 2025 Workshop\n\n\n\ncorl\n\n2025\n\nworkshop\n\n\n\n2nd Workshop on Dexterous Manipulation - Learning and Control with Diverse Modalities\n\n\n\n\n\nSep 25, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃From Imitation to Refinement 리뷰\n\n\n\nresidual\n\nmanipulation\n\n\n\nResidual RL for Precise Assembly\n\n\n\n\n\nSep 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃DEXOP 리뷰\n\n\n\nexoskeletons\n\nperioperation\n\n\n\nTactile In-Hand Manipulation with LLM-Designed Reward Functions\n\n\n\n\n\nSep 20, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃Text2Touch 리뷰\n\n\n\ntouch\n\nllm\n\nrl\n\n\n\nTactile In-Hand Manipulation with LLM-Designed Reward Functions\n\n\n\n\n\nSep 19, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃CrossDex 리뷰\n\n\n\nisaacgym\n\ngrasp\n\nil\n\n\n\nCross-Embodiment Dexterous Grasping with Reinforcement Learning\n\n\n\n\n\nSep 12, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃Human2Sim2Robot 리뷰\n\n\n\nreal2sim\n\nsim2real\n\nrl\n\n\n\nCrossing the Human-Robot Embodiment Gap with Sim-to-Real RL using One Human Demonstration\n\n\n\n\n\nSep 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃DextrAH-RGB 리뷰\n\n\n\nfabric-guided\n\nrgb\n\ndextrah\n\n\n\nVisuomotor Policies to Grasp Anything with Dexterous Hands\n\n\n\n\n\nSep 10, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃FunGrasp 리뷰\n\n\n\nrl\n\nraisim\n\nfunctional\n\n\n\nFunctional Grasping for Diverse Dexterous Hands\n\n\n\n\n\nSep 6, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃DAPG 리뷰\n\n\n\nil\n\nrl\n\ndapg\n\n\n\nLearning Complex Dexterous Manipulation withDeep Reinforcement Learning and Demonstrations\n\n\n\n\n\nSep 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃GraspMPC 리뷰\n\n\n\nmpc\n\ngrasp\n\n\n\nClosed-Loop Visual Grasping via Value-Guided Model Predictive Control\n\n\n\n\n\nSep 2, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃Gemini Robotics 리뷰\n\n\n\ngemini\n\nvla\n\ngoogle\n\n\n\nBringing AI into the Physical World\n\n\n\n\n\nSep 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃Eurekaverse 리뷰\n\n\n\neureka\n\nllm\n\ncurriculum\n\n\n\nEnvironment Curriculum Generation via Large Language Models\n\n\n\n\n\nAug 27, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃DrEureka 리뷰\n\n\n\neureka\n\nllm\n\ndomain randomization\n\n\n\nLanguage Model Guided Sim-To-Real Transfer\n\n\n\n\n\nAug 26, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n🧩CoRL 2025\n\n\n\ncorl\n\n2025\n\nconference\n\n\n\nPlan & Search for meaningful insights\n\n\n\n\n\nAug 26, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃ManipTrans 리뷰\n\n\n\nretargeting\n\nimitation\n\nresidual\n\n\n\nEfficient Dexterous Bimanual Manipulation Transfer via Residual Learning\n\n\n\n\n\nAug 25, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃ImMimic 리뷰\n\n\n\nco-training\n\ndtw\n\nmimic\n\n\n\nCross-Domain Imitation from Human Videos via Mapping and Interpolation\n\n\n\n\n\nAug 24, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃DexPilot 리뷰\n\n\n\nretargeting\n\nvision\n\n\n\nKinematic Motion Retargeting for Contact-Rich Anthropomorphic Manipulations\n\n\n\n\n\nAug 19, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃Tactile Beyond Pixel 리뷰\n\n\n\ntactile\n\ndigit360\n\nmultlimodal\n\n\n\nMultisensory Touch Representations for Robot Manipulation\n\n\n\n\n\nAug 19, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃Kinematic Motion Retargeting 리뷰\n\n\n\nretargeting\n\nkinematic\n\n\n\nKinematic Motion Retargeting for Contact-Rich Anthropomorphic Manipulations\n\n\n\n\n\nAug 16, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃AnyTeleop 리뷰\n\n\n\nteleoperation\n\nvision\n\n\n\nA General Vision-Based Dexterous Robot Arm-Hand Teleoperation System\n\n\n\n\n\nAug 15, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃Grasp as You Say 리뷰\n\n\n\nllm\n\nretargeting\n\n\n\nLanguage-guided Dexterous Grasp Generation\n\n\n\n\n\nAug 14, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃3D Motion Field 리뷰\n\n\n\ndiffusion\n\nmotion\n\n\n\nObject-centric 3D Motion Field for Robot Learning from Human Videos\n\n\n\n\n\nAug 13, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃DexArt 리뷰\n\n\n\nbenchmark\n\nrl\n\nsurvey\n\n\n\nBenchmarking Generalizable Dexterous Manipulation with Articulated Objects\n\n\n\n\n\nAug 12, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃From One Hand to Multiple Hands 리뷰\n\n\n\nil\n\nrl\n\nvision\n\n\n\nImitation Learning for Dexterous Manipulation from Single-Camera Teleoperation\n\n\n\n\n\nAug 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃DexTrack 리뷰\n\n\n\nil\n\nrl\n\nisaacgym\n\n\n\nTowards Generalizable Neural Tracking Control for Dexterous Manipulation from Human References\n\n\n\n\n\nAug 10, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃Retargeting Survey 리뷰\n\n\n\nretargeting\n\nsurvey\n\n\n\nHuman to Robot Hand Motion Mapping Methods - Review and Classification\n\n\n\n\n\nAug 9, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃DexFlow 리뷰\n\n\n\nretargeting\n\nsimulation\n\noptimization\n\n\n\nA Unified Approach for Dexterous Hand Pose Retargeting and Interaction\n\n\n\n\n\nAug 8, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃SeqMultiGrasp 리뷰\n\n\n\ngrasp\n\ndiffusion\n\nmulti-objects\n\n\n\nSequential Multi-Object Grasping with One Dexterous Hand\n\n\n\n\n\nAug 6, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃ManiSkill3 리뷰\n\n\n\nsapien\n\nsimulation\n\n\n\nDemonstrating GPU Parallelized Robot Simulation and Rendering for Generalizable Embodied AI with ManiSkill3\n\n\n\n\n\nAug 6, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃Micro Lie Theory 리뷰\n\n\n\nlie\n\nstate-estimation\n\nbasic\n\n\n\nA micro Lie theory for state estimation in robotics\n\n\n\n\n\nAug 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃GraspGen 리뷰\n\n\n\ndifussion\n\ngrasp\n\n\n\nA Diffusion-based Framework for 6-DOF Grasping with On-Generator Training\n\n\n\n\n\nAug 2, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃Spin pens 리뷰\n\n\n\nin-hand\n\nrl\n\nspin-task\n\n\n\nLessons from Learning to Spin “Pens”\n\n\n\n\n\nAug 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃sEMG-RD 리뷰\n\n\n\nwrist-band\n\nemg\n\nmeta\n\n\n\nA generic non-invasive neuromotor interface for human-computer interaction\n\n\n\n\n\nJul 31, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃DexMachina 리뷰\n\n\n\nretargeting\n\nhand\n\n\n\nFunctional Retargeting for Bimanual Dexterous Manipulation\n\n\n\n\n\nJul 30, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃SMTR 리뷰\n\n\n\nretargeting\n\nquadruped\n\n\n\nSpatio-Temporal Motion Retargeting for Quadruped Robots\n\n\n\n\n\nJul 29, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃Reward Engineering 리뷰\n\n\n\nrl\n\nreward\n\n\n\nComprehensive Overview of Reward Engineering and Shaping in Advancing Reinforcement Learning Applications\n\n\n\n\n\nJul 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃PP-Tac 리뷰\n\n\n\ngmm\n\ncatching\n\n\n\nPaper Picking Using Tactile Feedback in Dexterous Robotic Hands\n\n\n\n\n\nJul 26, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃Catching Objects in Flight 리뷰\n\n\n\ngmm\n\ncatching\n\n\n\nCatching, Gaussian mixture model\n\n\n\n\n\nJul 25, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃In-Hand Translation Learning 리뷰\n\n\n\nrl\n\ntactile\n\nshear-force\n\n\n\nLearning In-Hand Translation Using Tactile Skin With Shear and Normal Force Sensing\n\n\n\n\n\nJul 24, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃DextrAH-G 리뷰\n\n\n\nrl\n\nfabric-guided\n\nhand\n\n\n\nPixels-to-Action Dexterous Arm-Hand Grasping with Geometric Fabrics\n\n\n\n\n\nJul 23, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃Sequential Dexterity 리뷰\n\n\n\nrl\n\nassembly\n\nhand\n\n\n\nChaining Dexterous Policies for Long-Horizon Manipulation\n\n\n\n\n\nJul 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃Eureka 리뷰\n\n\n\nretargeting\n\nhand\n\n\n\nHuman-Level Reward Design via Coding Large Language Models\n\n\n\n\n\nJul 20, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃MambaVision 리뷰\n\n\n\nmamba\n\ntransformer\n\nvision\n\n\n\nA Hybrid Mamba-Transformer Vision Backbone\n\n\n\n\n\nJul 19, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃Touch in the Wild 리뷰\n\n\n\ntouch\n\nvisuo-tactile\n\ngripper\n\n\n\nLearning Fine-Grained Manipulation with a Portable Visuo-Tactile Gripper\n\n\n\n\n\nJul 18, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃CuRobo 리뷰\n\n\n\nsim2real\n\nadaptive\n\nrl\n\nhand\n\n\n\nParallelized Collision-Free Minimum-Jerk Robot Motion Generation\n\n\n\n\n\nJul 17, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃DexCtrl 리뷰\n\n\n\nsim2real\n\nadaptive\n\nrl\n\nhand\n\n\n\nTowards Sim-to-Real Dexterity with Adaptive Controller Learning\n\n\n\n\n\nJul 16, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n👩‍💻스크루-이론 기반 Product-of-Exponentials(PoE) 기구학 Deep-Dive\n\n\n\npoe\n\nfk\n\ncode\n\n\n\nDH 파라미터 벗어나서 PoE로 기구학 다뤄보기\n\n\n\n\n\nJul 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃Gradiometric Magnetic FS 리뷰\n\n\n\nmelexis\n\nmagnetic\n\nforce sensor\n\ntactile\n\n\n\nA Gradiometric Magnetic Force Sensor Immune to Stray Magnetic Fields for Robotic Hands and Grippers\n\n\n\n\n\nJul 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃TacEx 리뷰\n\n\n\ngelsight\n\nisaacsim\n\ntactile\n\n\n\nGelSight Tactile Simulation in Isaac Sim – Combining Soft-Body and Visuotactile Simulators\n\n\n\n\n\nJul 9, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃Sparsh-Skin 리뷰\n\n\n\nsparsh-skin\n\nssl\n\ntouch\n\ntactile\n\n\n\nSelf-supervised perception for tactile skin covered dexterous hands\n\n\n\n\n\nJul 4, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃Physics Informed RL Survey 리뷰\n\n\n\nrl\n\nphysics-informed\n\nsurvey\n\n\n\nA Survey on Physics Informed Reinforcement Learning - Review and Open Problems\n\n\n\n\n\nJul 3, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃Offline RL Survey 리뷰\n\n\n\nrl\n\noffline-rl\n\nsurvey\n\n\n\nA Survey on Offline Reinforcement Learning - Taxonomy, Review, and Open Problems\n\n\n\n\n\nJul 2, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃Batch Online RL 리뷰\n\n\n\nbatch-online-rl\n\nrl\n\n\n\nWhat Matters for Batch Online Reinforcement Learning in Robotics?\n\n\n\n\n\nJun 13, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃Contact Trust Region 리뷰(feat.Dextreme)\n\n\n\nmpc\n\nrl\n\ndexterous\n\ncontact\n\ntrust-region\n\n\n\nDexterous Contact-Rich Manipulation via the Contact Trust Region\n\n\n\n\n\nJun 13, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃Dex Imitation Learning 리뷰\n\n\n\npaper\n\nimitation\n\ndexterous\n\n\n\nDexterous Manipulation through Imitation Learning / A Survey\n\n\n\n\n\nJun 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n👩‍💻ROS2 JointState vs. JointTrajectory\n\n\n\nros2\n\njoint\n\ncode\n\n\n\nROS2 메세지 JointState와 JointTrajectory 비교\n\n\n\n\n\nJun 9, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n👩‍💻ROS2 joint_state_publiser 디폴트 동작 제어하기\n\n\n\nros2\n\njoint\n\ncode\n\n\n\nROS2 메세지 JointState와 JointTrajectory 비교\n\n\n\n\n\nJun 9, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃GeoRT 리뷰\n\n\n\npaper\n\nteleoperation\n\nretargeting\n\n\n\nGeometric Retargeting - A Principled, Ultrafast Neural Hand Retargeting Algorithm\n\n\n\n\n\nJun 9, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃Neural feels with neural fields 리뷰\n\n\n\npaper\n\ntactile\n\nsdf\n\n\n\nVisuo-tactile perception for in-hand manipulation\n\n\n\n\n\nJun 4, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃Digit360 리뷰\n\n\n\npaper\n\ntactile\n\n\n\nDigitizing Touch with an Artificial Multimodal Fingertip\n\n\n\n\n\nJun 3, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃Sparsh 리뷰\n\n\n\npaper\n\ntactile\n\nsdf\n\n\n\nSelf-supervised touch representations for vision-based tactile sensing\n\n\n\n\n\nJun 2, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n🧩uSkin vs ReSkin\n\n\n\ntactile\n\nsensor\n\nmagneto\n\n\n\nComparison of Tactile Sensors\n\n\n\n\n\nMay 29, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃Reskin-Anyskin 리뷰\n\n\n\npaper\n\ntactile\n\n\n\n교체가능한 촉각센서\n\n\n\n\n\nMay 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n👩‍💻IsaacSim과 IsaacLab 알아보기\n\n\n\nisaacsim\n\nisaaclab\n\ncode\n\n\n\nIsaacSim 4.5.0 & IsaacLab\n\n\n\n\n\nMar 14, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n👩‍💻Hydra로 실험관리 하기\n\n\n\nhydra\n\ncode\n\n\n\n여러 실험 파라미터들을 관리해주는 Hydra\n\n\n\n\n\nFeb 16, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n👩‍💻ROS2 RealSense C++ Node\n\n\n\nros2\n\nrealsense\n\ncode\n\n\n\nC++로 ROS2 RealSense 카메라 노드 만들기\n\n\n\n\n\nFeb 2, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃Rotating without Seeing 리뷰\n\n\n\npaper\n\ntactile\n\nrl\n\nhand\n\n\n\nTowards In-hand Dexterity through Touch\n\n\n\n\n\nDec 22, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n📃IPO 리뷰\n\n\n\npaper\n\nrl\n\ncmdp\n\n\n\nInterior-point Policy Optimization under Constraints\n\n\n\n\n\nNov 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n📃VCGS 리뷰\n\n\n\ngrasp\n\npointcloud\n\nvae\n\npaper\n\n\n\nVariational Constrained Grasp Sample\n\n\n\n\n\nMar 17, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n👩‍💻Quarto Blog + α\n\n\n\nblog\n\nquarto\n\ncode\n\n\n\nQuarto로 속편한 Github Blog 구축하기(2)\n\n\n\n\n\nJan 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n👩‍💻Quarto Blog 기본 셋팅\n\n\n\nblog\n\nquarto\n\ncode\n\n\n\nQuarto로 속편한 Github Blog 구축하기(1)\n\n\n\n\n\nJan 7, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n📃DreamWaQ 리뷰\n\n\n\ncontext\n\nrl\n\npaper\n\n\n\nLearning Robust Quadrupedal Locomotion With Implicit Terrain Imagination via Deep Reinforcement Learning\n\n\n\n\n\nJul 2, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n👩‍💻Chord Graph\n\n\n\nchord\n\nvisualization\n\ncode\n\n\n\nHoloViews를 이용하여 Chord Graph 그리기\n\n\n\n\n\nJun 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n📃K-Accessibility 리뷰\n\n\n\nrl\n\nclustering\n\nquadruped\n\nrecovery\n\nbackflip\n\npaper\n\n\n\nAccessibility-Based Clustering for Efficient Learning of Locomotion Skills\n\n\n\n\n\nMay 7, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n👩‍💻Orbit Existing Scripts\n\n\n\norbit\n\nisaacsim\n\ncode\n\n\n\nIsaac Orbit Series 002\n\n\n\n\n\nApr 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n👩‍💻Orbit 설치하기\n\n\n\norbit\n\nisaacsim\n\ncode\n\n\n\nIsaac Orbit Series 001\n\n\n\n\n\nApr 4, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n📃WASABI 리뷰\n\n\n\nrl\n\ngan\n\nquadruped\n\nbackflip\n\npaper\n\n\n\nLearning Agile Skills via Adversarial Imitation of Rough Partial Demonstrations\n\n\n\n\n\nMar 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n👩‍💻class ⟷ dict ⟷ yaml\n\n\n\nconfig\n\nyaml\n\ncode\n\n\n\n여러 파라미터들을 기록하기 위한 config 관리\n\n\n\n\n\nDec 26, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n👩‍💻torch.Tensor vs torch.tensor\n\n\n\ntorch\n\ntensor\n\ninverted\n\ncode\n\n\n\ntorch Tensor 객체 생성 방법 비교와 Inverted 연산 확인하기\n\n\n\n\n\nDec 21, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n👩‍💻Linux GPU 상태 확인하기\n\n\n\nlinux\n\ngpu\n\ncode\n\n\n\n리눅스에서 GPU 상태를 확인하는 여러가지 방법을 알아봅니다.\n\n\n\n\n\nDec 14, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n📃Robust Recovery Controller 리뷰\n\n\n\nrl\n\ntrpo\n\ngae\n\nrecovery\n\nquadruped\n\npaper\n\n\n\nRobust Recovery Controller for a Quadrupedal Robot using Deep Reinforcement Learning\n\n\n\n\n\nOct 16, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n📃VAE 리뷰\n\n\n\ngenerative\n\nvae\n\npaper\n\n\n\nAuto-Encoding Variational Bayes\n\n\n\n\n\nOct 2, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n📃WaveNet 리뷰\n\n\n\nautoregressive\n\ngenerative\n\npaper\n\n\n\nA Generative Model for Raw Audio\n\n\n\n\n\nSep 17, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n📃GN-Block 리뷰\n\n\n\ngnn\n\nsystem identification\n\nmpc\n\nrl\n\npaper\n\n\n\nGraph Networks as Learnable Physics Engines for Inference and Control\n\n\n\n\n\nAug 7, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n📃Legged Robots that Keep on Learning 리뷰\n\n\n\nquadruped\n\nrl\n\nredq\n\npaper\n\n\n\nFine-Tuning Locomotion Policies in the Real World\n\n\n\n\n\nJun 26, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n📃NerveNet 리뷰\n\n\n\ngnn\n\nrl\n\npaper\n\n\n\nLearning Structured Policy with Graph Neural Networks\n\n\n\n\n\nJun 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n🧩GNN Materials\n\n\n\ngnn\n\nstorage\n\n\n\nGNN 자료들 모음\n\n\n\n\n\nJan 2, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n👩‍💻Import custom module\n\n\n\npython\n\ncode\n\n\n\ncustom module을 불러오는 방법\n\n\n\n\n\nJul 20, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n🧩My Reinforcement Learning Roadmap\n\n\n\nrl\n\nstudy\n\nstorage\n\n\n\n내가 공부했던 강화학습 Roadmap\n\n\n\n\n\nJul 17, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n👩‍💻Install Mujoco in Windows10\n\n\n\nmujoco\n\ncode\n\n\n\nRL에서 많이 쓰이는 시뮬레이션 Mujoco Windows10에 설치하기\n\n\n\n\n\nJul 13, 2020\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "post.html",
    "href": "post.html",
    "title": "Post",
    "section": "",
    "text": "📃 Paper Review | 🧩 Storage | 👩‍💻 Code\n\n\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n    \n  \n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\n\n\n\n\n\nSep 25, 2025\n\n\n🧩CoRL 2025 Workshop\n\n\n\n\n\n\nSep 22, 2025\n\n\n📃From Imitation to Refinement 리뷰\n\n\n\n\n\n\nSep 20, 2025\n\n\n📃DEXOP 리뷰\n\n\n\n\n\n\nSep 19, 2025\n\n\n📃Text2Touch 리뷰\n\n\n\n\n\n\nSep 12, 2025\n\n\n📃CrossDex 리뷰\n\n\n\n\n\n\nSep 11, 2025\n\n\n📃Human2Sim2Robot 리뷰\n\n\n\n\n\n\nSep 10, 2025\n\n\n📃DextrAH-RGB 리뷰\n\n\n\n\n\n\nSep 6, 2025\n\n\n📃FunGrasp 리뷰\n\n\n\n\n\n\nSep 5, 2025\n\n\n📃DAPG 리뷰\n\n\n\n\n\n\nSep 2, 2025\n\n\n📃GraspMPC 리뷰\n\n\n\n\n\n\nSep 1, 2025\n\n\n📃Gemini Robotics 리뷰\n\n\n\n\n\n\nAug 27, 2025\n\n\n📃Eurekaverse 리뷰\n\n\n\n\n\n\nAug 26, 2025\n\n\n📃DrEureka 리뷰\n\n\n\n\n\n\nAug 26, 2025\n\n\n🧩CoRL 2025\n\n\n\n\n\n\nAug 25, 2025\n\n\n📃ManipTrans 리뷰\n\n\n\n\n\n\nAug 24, 2025\n\n\n📃ImMimic 리뷰\n\n\n\n\n\n\nAug 19, 2025\n\n\n📃DexPilot 리뷰\n\n\n\n\n\n\nAug 19, 2025\n\n\n📃Tactile Beyond Pixel 리뷰\n\n\n\n\n\n\nAug 16, 2025\n\n\n📃Kinematic Motion Retargeting 리뷰\n\n\n\n\n\n\nAug 15, 2025\n\n\n📃AnyTeleop 리뷰\n\n\n\n\n\n\nAug 14, 2025\n\n\n📃Grasp as You Say 리뷰\n\n\n\n\n\n\nAug 13, 2025\n\n\n📃3D Motion Field 리뷰\n\n\n\n\n\n\nAug 12, 2025\n\n\n📃DexArt 리뷰\n\n\n\n\n\n\nAug 11, 2025\n\n\n📃From One Hand to Multiple Hands 리뷰\n\n\n\n\n\n\nAug 10, 2025\n\n\n📃DexTrack 리뷰\n\n\n\n\n\n\nAug 9, 2025\n\n\n📃Retargeting Survey 리뷰\n\n\n\n\n\n\nAug 8, 2025\n\n\n📃DexFlow 리뷰\n\n\n\n\n\n\nAug 6, 2025\n\n\n📃SeqMultiGrasp 리뷰\n\n\n\n\n\n\nAug 6, 2025\n\n\n📃ManiSkill3 리뷰\n\n\n\n\n\n\nAug 5, 2025\n\n\n📃Micro Lie Theory 리뷰\n\n\n\n\n\n\nAug 2, 2025\n\n\n📃GraspGen 리뷰\n\n\n\n\n\n\nAug 1, 2025\n\n\n📃Spin pens 리뷰\n\n\n\n\n\n\nJul 31, 2025\n\n\n📃sEMG-RD 리뷰\n\n\n\n\n\n\nJul 30, 2025\n\n\n📃DexMachina 리뷰\n\n\n\n\n\n\nJul 29, 2025\n\n\n📃SMTR 리뷰\n\n\n\n\n\n\nJul 28, 2025\n\n\n📃Reward Engineering 리뷰\n\n\n\n\n\n\nJul 26, 2025\n\n\n📃PP-Tac 리뷰\n\n\n\n\n\n\nJul 25, 2025\n\n\n📃Catching Objects in Flight 리뷰\n\n\n\n\n\n\nJul 24, 2025\n\n\n📃In-Hand Translation Learning 리뷰\n\n\n\n\n\n\nJul 23, 2025\n\n\n📃DextrAH-G 리뷰\n\n\n\n\n\n\nJul 22, 2025\n\n\n📃Sequential Dexterity 리뷰\n\n\n\n\n\n\nJul 20, 2025\n\n\n📃Eureka 리뷰\n\n\n\n\n\n\nJul 19, 2025\n\n\n📃MambaVision 리뷰\n\n\n\n\n\n\nJul 18, 2025\n\n\n📃Touch in the Wild 리뷰\n\n\n\n\n\n\nJul 17, 2025\n\n\n📃CuRobo 리뷰\n\n\n\n\n\n\nJul 16, 2025\n\n\n📃DexCtrl 리뷰\n\n\n\n\n\n\nJul 11, 2025\n\n\n📃Gradiometric Magnetic FS 리뷰\n\n\n\n\n\n\nJul 11, 2025\n\n\n👩‍💻스크루-이론 기반 Product-of-Exponentials(PoE) 기구학 Deep-Dive\n\n\n\n\n\n\nJul 9, 2025\n\n\n📃TacEx 리뷰\n\n\n\n\n\n\nJul 4, 2025\n\n\n📃Sparsh-Skin 리뷰\n\n\n\n\n\n\nJul 3, 2025\n\n\n📃Physics Informed RL Survey 리뷰\n\n\n\n\n\n\nJul 2, 2025\n\n\n📃Offline RL Survey 리뷰\n\n\n\n\n\n\nJun 13, 2025\n\n\n📃Batch Online RL 리뷰\n\n\n\n\n\n\nJun 13, 2025\n\n\n📃Contact Trust Region 리뷰(feat.Dextreme)\n\n\n\n\n\n\nJun 11, 2025\n\n\n📃Dex Imitation Learning 리뷰\n\n\n\n\n\n\nJun 9, 2025\n\n\n📃GeoRT 리뷰\n\n\n\n\n\n\nJun 9, 2025\n\n\n👩‍💻ROS2 JointState vs. JointTrajectory\n\n\n\n\n\n\nJun 9, 2025\n\n\n👩‍💻ROS2 joint_state_publiser 디폴트 동작 제어하기\n\n\n\n\n\n\nJun 4, 2025\n\n\n📃Neural feels with neural fields 리뷰\n\n\n\n\n\n\nJun 3, 2025\n\n\n📃Digit360 리뷰\n\n\n\n\n\n\nJun 2, 2025\n\n\n📃Sparsh 리뷰\n\n\n\n\n\n\nMay 29, 2025\n\n\n🧩uSkin vs ReSkin\n\n\n\n\n\n\nMay 5, 2025\n\n\n📃Reskin-Anyskin 리뷰\n\n\n\n\n\n\nMar 14, 2025\n\n\n👩‍💻IsaacSim과 IsaacLab 알아보기\n\n\n\n\n\n\nFeb 16, 2025\n\n\n👩‍💻Hydra로 실험관리 하기\n\n\n\n\n\n\nFeb 2, 2025\n\n\n👩‍💻ROS2 RealSense C++ Node\n\n\n\n\n\n\nDec 22, 2024\n\n\n📃Rotating without Seeing 리뷰\n\n\n\n\n\n\nNov 10, 2024\n\n\n📃IPO 리뷰\n\n\n\n\n\n\nMar 17, 2024\n\n\n📃VCGS 리뷰\n\n\n\n\n\n\nJan 21, 2024\n\n\n👩‍💻Quarto Blog + α\n\n\n\n\n\n\nJan 7, 2024\n\n\n👩‍💻Quarto Blog 기본 셋팅\n\n\n\n\n\n\nJul 2, 2023\n\n\n📃DreamWaQ 리뷰\n\n\n\n\n\n\nJun 18, 2023\n\n\n👩‍💻Chord Graph\n\n\n\n\n\n\nMay 7, 2023\n\n\n📃K-Accessibility 리뷰\n\n\n\n\n\n\nApr 23, 2023\n\n\n👩‍💻Orbit Existing Scripts\n\n\n\n\n\n\nApr 4, 2023\n\n\n👩‍💻Orbit 설치하기\n\n\n\n\n\n\nMar 12, 2023\n\n\n📃WASABI 리뷰\n\n\n\n\n\n\nDec 26, 2022\n\n\n👩‍💻class ⟷ dict ⟷ yaml\n\n\n\n\n\n\nDec 21, 2022\n\n\n👩‍💻torch.Tensor vs torch.tensor\n\n\n\n\n\n\nDec 14, 2022\n\n\n👩‍💻Linux GPU 상태 확인하기\n\n\n\n\n\n\nOct 16, 2022\n\n\n📃Robust Recovery Controller 리뷰\n\n\n\n\n\n\nOct 2, 2022\n\n\n📃VAE 리뷰\n\n\n\n\n\n\nSep 17, 2022\n\n\n📃WaveNet 리뷰\n\n\n\n\n\n\nAug 7, 2022\n\n\n📃GN-Block 리뷰\n\n\n\n\n\n\nJun 26, 2022\n\n\n📃Legged Robots that Keep on Learning 리뷰\n\n\n\n\n\n\nJun 10, 2022\n\n\n📃NerveNet 리뷰\n\n\n\n\n\n\nJan 2, 2021\n\n\n🧩GNN Materials\n\n\n\n\n\n\nJul 20, 2020\n\n\n👩‍💻Import custom module\n\n\n\n\n\n\nJul 17, 2020\n\n\n🧩My Reinforcement Learning Roadmap\n\n\n\n\n\n\nJul 13, 2020\n\n\n👩‍💻Install Mujoco in Windows10\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "asset/2025-02-16-hydra-handson.html",
    "href": "asset/2025-02-16-hydra-handson.html",
    "title": "Hydra 소개",
    "section": "",
    "text": "ML/DL 실험에서는 다양한 실험 파라미터들을 관리해야 합니다. 이를 위해 여러가지 방법이 있지만, 이번 포스팅에서는 FacebookResearch에서 만든 Hydra를 사용해보자 합니다."
  },
  {
    "objectID": "asset/2025-02-16-hydra-handson.html#config-초기화",
    "href": "asset/2025-02-16-hydra-handson.html#config-초기화",
    "title": "Hydra 소개",
    "section": "Config 초기화",
    "text": "Config 초기화\nHydra에서 설정(Config) 파일을 초기화하는 방법에는 여러 가지가 있습니다. 공식 문서 Initialization methods에서 확인할 수 있듯이, 총 세 가지 방법이 있으며, 각각의 방법을 예제를 통해 살펴보겠습니다.\nHydra의 설정 초기화 방법\n\ninitialize(): 호출하는 코드의 상대 경로를 기준으로 설정 파일을 초기화합니다.\n\ninitialize_config_module(): 절대 경로를 사용하여 설정 모듈(config_module)을 기반으로 초기화합니다.\n\ninitialize_config_dir(): 파일 시스템의 절대 경로를 사용하여 설정 디렉터리(config_dir)를 기반으로 초기화합니다.\n\n이 세 가지 방법은 (1)함수 호출 방식과 (2)컨텍스트(context) 방식으로 사용할 수 있습니다.\n\n함수 호출 방식으로 사용하면 Hydra를 전역적(global)으로 초기화하며, 한 번만 호출해야 합니다.\n반면, 컨텍스트 방식으로 사용하면 특정 블록 내에서만 Hydra를 초기화할 수 있으며, 여러 번 사용할 수도 있습니다.\n\n\n방법1 initialize()\nHydra를 초기화하고 config_path를 설정 검색 경로에 추가합니다.\nconfig_path는 호출하는 코드의 parent 디렉터리를 기준으로 한 상대 경로이며, 이 경우에는 현재 노트북이 위치한 디렉터리를 기준으로 설정됩니다.\n\n%cd /content/\n\nwith initialize(version_base=None, config_path=\"configs\"):\n    # Compose the configuration by selecting the main configuration.\n    cfg_1 = compose(config_name=\"main\")\n\n# full configuration을 YAML 형식으로 출력하여 쉽게 검사할 수 있습니다.\nprint(OmegaConf.to_yaml(cfg_1))\n\n\n\n방법2 initialize_config_module()\nHydra를 초기화하고 config_module을 설정 검색 경로에 추가합니다.\nconfig_module은 반드시 import 가능한 형태여야 하며, 최상위 디렉터리에 __init__.py 파일이 존재해야 합니다. 이번 예제에서는 module이라는 폴더를 만들어서 import 가능한 디렉토리로 만들어 줍니다. 그리고 첫번째 만들었던 main.yaml파일을 복사하여 module/main_2.yaml 파일로 만들어 줍니다.\n\n%cd /content/configs\n%mkdir -p /content/configs/module\n!touch /content/configs/module/__init__.py\n%cd /content\n!cp ./configs/main.yaml configs/module/main_2.yaml\n\n잘 복사가 되었는지 확인해보겠습니다.\n\n!cat /content/configs/module/main_2.yaml\n\n이번에는 2번째 방법인 initialize_config_module()함수를 이용하여 복사했던 main_2.yaml 파일을 이용하여 compose 해보겠습니다.\n\n%cd /content\nwith initialize_config_module(version_base=None, config_module=\"configs.module\"):\n    cfg_2 = compose(config_name=\"main_2\")\n    print(cfg_2)\n\n아래와 같이 config의 키들을 확인해볼 수도 있습니다.\n\nlist(cfg_2.keys())\n\n\n\n방법3 initialize_config_dir()\nHydra를 초기화하고 config_path를 설정 검색 경로에 추가합니다. config_path는 파일 시스템 상의 절대 경로여야 합니다. 미리 만들어 놓았던 main.yaml 파일을 이용하여 config를 구성해보겠습니다.\n\nCONFIG_DIR = \"/content/configs\"\n\nwith initialize_config_dir(version_base=None, config_dir=CONFIG_DIR):\n    cfg_3 = compose(config_name=\"main\")\n\n\nprint(cfg_3)\n\n여기까지 Hydra의 세 가지 방법을 사용하여 설정(Config)을 초기화하는 방법을 살펴보았습니다. 이제 이렇게 생성된 설정이 어떻게 구성되어 있는지 확인해보겠습니다. 대표적인 설정 예제로 cfg_1을 살펴보겠습니다.\n객체의 타입을 확인해보면, 이는 Omegaconf에서 제공하는 DictConfig 객체임을 알 수 있습니다. DictConfig는 딕셔너리 형태의 설정을 계층적으로 관리할 수 있도록 해주는 데이터 구조로, YAML 설정 파일을 로드하거나 동적으로 구성 값을 변경할 때 유용하게 활용됩니다.\n\ntype(cfg_1)\n\nConfig 객체의 키들은 다음과 같이 확인할 수 있습니다.\n\nlist(cfg_1.keys())\n\nDictConfig의 키는 config_name.key_name 또는 config_name[\"key_name\"] 형태로 접근할 수 있습니다. 이를 통해 일반적인 딕셔너리처럼 키를 참조하거나 점 표기법(dot notation)을 사용하여 계층적인 설정 값을 쉽게 조회할 수 있습니다.\n\nprint(cfg_1.env == cfg_1[\"env\"])\n\n하위 키들도 동일한 방식으로 접근할 수 있으며, config_name.key_name.sub_key_name 또는 config_name[\"key_name\"][\"sub_key_name\"] 형태로 호출할 수 있습니다. 이를 활용하면 계층적으로 구성된 설정에서 원하는 값을 직관적으로 조회할 수 있습니다.\n\nprint(cfg_1.env.name == cfg_1[\"env\"][\"name\"])"
  },
  {
    "objectID": "asset/2025-02-16-hydra-handson.html#override",
    "href": "asset/2025-02-16-hydra-handson.html#override",
    "title": "Hydra 소개",
    "section": "Override",
    "text": "Override\n이번에는 초기화로 만든 config를 override하는 예제를 살펴보겠습니다. 강화학습에서 여러개의 environment를 병렬로 학습하기 위해 env 하위에 num_envs config를 1000개로 추가하는 override를 진행해보겠습니다.\n\n%cd /content/\n\nwith initialize(version_base=None, config_path=\"configs\"):\n    # 기존 설정을 유지하면서 새로운 설정을 추가\n    cfg = compose(config_name=\"main\", overrides=[\"+env.num_envs=1000\"])\n\n# 기존 설정과 오버라이드된 설정이 함께 출력됨\nprint(OmegaConf.to_yaml(cfg))\n\noverride는 기존의 main.yaml 파일의 내용을 변경하지 않고 항목을 추가할 수 있습니다. 다시한번 main.yaml 내용을 확인해보면 num_envs가 없음을 알 수 있습니다.\n\n!cat /content/configs/main.yaml\n\n하지만 override된 cfg의 키에는 env.num_envs가 있습니다.\n\ncfg.env.num_envs"
  },
  {
    "objectID": "asset/2025-02-16-hydra-handson.html#resolver",
    "href": "asset/2025-02-16-hydra-handson.html#resolver",
    "title": "Hydra 소개",
    "section": "Resolver",
    "text": "Resolver\nHydra는 Omegaconf의 Resolver 기능을 활용할 수 있습니다. OmegaConf.register_new_resolver()를 사용하여 커스텀 resolver를 등록하면, 새로운 interpolation 타입을 추가할 수 있으며, 설정(Config) 노드가 접근될 때 해당 resolver가 호출됩니다.\n\nResolver 등록 및 기능\n\neq: 두 문자열을 소문자로 변환한 후, 동일한지 비교합니다.\n\ncontains: 첫 번째 문자열이 두 번째 문자열에 포함되어 있는지 검사합니다.\n\nif: 주어진 조건에 따라 두 값 중 하나를 선택합니다.\n\nresolve_default: 인자가 빈 문자열이면 기본값을 사용하고, 그렇지 않으면 인자 값을 반환합니다.\n\n이러한 resolver를 활용하면 설정 파일 내에서 조건부 로직, 문자열 비교, 기본값 처리 등을 동적으로 적용할 수 있습니다.\n예제로 아래와 같이 Resolver를 등록해보겠습니다.\n\n# Hydra 설정에서 사용할 Resolver들을 등록합니다.\nOmegaConf.register_new_resolver(\"eq\", lambda x, y: x.lower() == y.lower())\nOmegaConf.register_new_resolver(\"contains\", lambda x, y: x.lower() in y.lower())\nOmegaConf.register_new_resolver(\"if\", lambda pred, a, b: a if pred else b)\nOmegaConf.register_new_resolver(\"resolve_default\", lambda default, arg: default if arg == \"\" else arg)\n\n이번 예제에서 사용할 Config는 위의 예제 Config에서 Resolver를 확인하기 위해 아래 내용을 더 추가하여 구성해보겠습니다.\n\nyaml_config = \"\"\"\nenv:\n  name: \"CartPole-v1\"\n  seed: 42\n\nagent:\n  type: \"DQN\"\n  hidden_layers: [64, 64]\n  activation: \"relu\"\n  learning_rate: 0.001 # experiment의 default_lr 기준값으로 설정\n  gamma: 0.99\n\ntraining:\n  episodes: 1000\n  batch_size: ${if:${experiment.is_test}, 128, 32} # 테스트 모드일 경우 128, 아니면 32\n  replay_buffer_size: 10000\n\n# Resolver를 활용한 동적 설정\nexperiment:\n  mode: \"test\"\n  is_test: ${eq:${experiment.mode}, \"test\"}\n  default_lr: ${resolve_default:0.001, ${agent.learning_rate}}\n  is_debug: ${contains:${experiment.mode}, \"debug\"}  # \"debug\" 포함 여부 확인\n\"\"\"\n\nconfig를 로드하고 Resolver가 적용된 값을 출력합니다.\n\ncfg = OmegaConf.create(yaml_config)\n\n# Resolver가 적용된 값 출력\nprint(\"실험 모드:\", cfg.experiment.mode)\nprint(\"is_test:\", cfg.experiment.is_test)\nprint(\"배치 크기:\", cfg.training.batch_size)\nprint(\"default_lr:\", cfg.experiment.default_lr)\nprint(\"디버그 모드 여부):\", cfg.experiment.is_debug)\n\n예제 출력을 하나씩 살펴보겠습니다.\n\n${eq:${experiment.mode}, \"test\"} → experiment.mode가 \"test\"이면 True, 아니면 False\n\neq(x, y) Resolver는 두 값을 비교하여 같으면 True, 다르면 False를 반환합니다. ${experiment.mode} 값이 \"test\"인지 확인하는 역할을 합니다.\n\nexperiment:\n    mode: \"test\"\n    is_test: ${eq:${experiment.mode}, \"test\"}\n\n위 설정에서 experiment.mode 값이 \"test\"로 설정되어 있기 때문에, ${eq:${experiment.mode}, \"test\"}는 \"test\"와 \"test\"를 비교하는 형태가 됩니다. eq 함수는 대소문자를 구분하지 않고 두 문자열이 같은지 확인하는 역할을 하므로, \"test\"는 \"test\"와 일치하여 True를 반환합니다. 따라서 experiment.is_test의 값은 True로 설정됩니다.\n반면, 만약 experiment.mode 값이 \"train\"이었다면, ${eq:${experiment.mode}, \"test\"}는 \"train\"과 \"test\"를 비교하게 됩니다. 이 두 값은 서로 다르므로 eq 함수는 False를 반환하게 되고, 결과적으로 experiment.is_test 값은 False로 설정됩니다. 이를 통해 설정 값에 따라 특정 변수를 자동으로 조정할 수 있으며, 이를 활용하면 실험 모드에 따라 설정을 다르게 적용할 수 있습니다.\n\n${if:${experiment.is_test}, 128, 32} → experiment.is_test가 True이면 128, 아니면 32\n\nif(condition, true_value, false_value) Resolver는 condition이 True일 때 true_value를, False일 때 false_value를 반환합니다. experiment.is_test 값이 True인지 확인하여, 이에 따라 다른 값을 할당합니다.\n\ntraining:\n    batch_size: ${if:${experiment.is_test}, 128, 32} # 테스트 모드일 경우 128, 아니면 32\n\n${if:${experiment.is_test}, 128, 32} 구문을 살펴보면, if 함수는 첫 번째 인자로 주어진 조건이 True일 경우 두 번째 인자인 128을 반환하고, False일 경우 세 번째 인자인 32를 반환하는 역할을 합니다. 현재 experiment.is_test가 True이므로 if(True, 128, 32)는 128을 반환하고, 결과적으로 training.batch_size 값이 128이 됩니다.\n반면, 만약 experiment.mode가 \"train\" 등 다른 값으로 설정되어 있다면, eq(\"train\", \"test\")의 결과는 False가 되어 experiment.is_test가 False로 설정됩니다. 이 경우, if(False, 128, 32)는 False에 해당하는 세 번째 값인 32를 반환하게 되며, training.batch_size 값이 32로 설정됩니다.\n이러한 방식은 training과 test에서 배치 크기를 다르게 설정할 때 유용합니다. 예를 들어, 테스트 환경에서는 더 큰 배치 크기를 사용하여 빠르게 결과를 확인하고, 훈련 환경에서는 적절한 배치 크기를 유지하여 안정적인 학습이 가능하도록 조정할 수 있습니다. 이를 통해 설정 파일을 동적으로 관리할 수 있으며, 실험 조건에 따라 유연하게 설정을 변경할 수 있습니다.\n\n${resolve_default:0.001, ${agent.learning_rate}} → agent.learning_rate가 설정되지 않았으면 기본값 0.001 사용\n\nresolve_default(default, arg) Resolver는 arg 값이 비어 있거나 설정되지 않았을 경우 default 값을 반환합니다.\nagent.learning_rate 값이 존재하면 그대로 사용하고, 없다면 기본값 0.001을 사용합니다.\n\nagent:\n    learning_rate: 0.001 # experiment의 default_lr 기준값으로 설정\nexperiment:\n    default_lr: ${resolve_default:0.001, ${agent.learning_rate}}\n\n이 방식은 설정 파일에서 특정 값이 누락되었을 때 기본값을 자동으로 적용하는 데 매우 유용합니다. 예를 들어, learning_rate 값을 실험마다 다르게 설정할 수 있도록 설정 파일에서 값을 명시적으로 지정할 수도 있지만, 실수로 빠뜨렸을 경우에도 resolve_default를 사용하면 안전하게 기본값을 사용할 수 있습니다. 이를 통해 설정을 더욱 견고하게 만들고, 코드의 예외 처리를 간결하게 할 수 있습니다.\n\n${contains:${experiment.mode}, \"debug\"} → experiment.mode에 “debug”라는 글자가 포함되어 있는지 여부 확인\nexperiment:\nmode: \"test\"\nis_debug: ${contains:${experiment.mode}, \"debug\"}  # \"debug\" 포함 여부 확인\n\nexperiment.mode 값이 \"test\"라면 \"debug\"가 포함되지 않았으므로 contains(\"test\", \"debug\")는 False를 반환합니다.\n만약 experiment.mode 값이 \"test_debug\"라면 \"debug\"라는 문자열이 포함되어 있으므로 contains(\"test_debug\", \"debug\")는 True를 반환합니다.\n이를 통해 실험 모드에 따라 자동으로 디버깅 기능을 활성화하거나 로그 출력을 조정할 수 있습니다.\n\n\n이러한 Resolver 기능을 활용하면 설정 파일을 더욱 동적으로 관리할 수 있습니다!"
  },
  {
    "objectID": "posts/note/2020-10-21-on-and-off-policy.html",
    "href": "posts/note/2020-10-21-on-and-off-policy.html",
    "title": "📝On policy VS. Off policy",
    "section": "",
    "text": "강화학습에서 On-Policy와 Off-Policy는 에이전트가 데이터를 수집하고 학습하는 방식에 따라 구분되는 두 가지 접근 방식입니다. 이 문서에서는 두 방식의 차이점, 특징, 장단점 및 주요 개념들을 정리합니다.\n\n\n\n\n\n현재 학습 중인 정책(behavior policy)에서 생성된 데이터를 사용하여 같은 정책을 개선합니다. 즉, 학습에 사용하는 데이터와 정책이 동일합니다.\n\n\n\n\n에이전트는 현재의 정책을 따라 환경과 상호작용하며 데이터를 수집합니다.\n점진적으로 정책이 개선되며, 이전 정책과 크게 다르지 않습니다.\n데이터 효율성은 낮지만, 안정적이고 수렴 성능이 뛰어난 경우가 많습니다.\n\n\n\n\n\nPPO (Proximal Policy Optimization)\nA3C (Asynchronous Advantage Actor-Critic)\nTRPO (Trust Region Policy Optimization)\n\n\n\n\n\n장점:\n\n학습 안정성이 높음\n수렴 속도가 빠를 수 있음\n\n단점:\n\n새로운 데이터를 매번 수집해야 하므로 데이터 효율성이 낮고 계산 비용이 높음\n\n\n\n\n\n\n\n\n\n현재의 정책(target policy)을 학습하지만, 다른 정책(behavior policy)에서 생성된 데이터를 활용합니다. 즉, 학습에 사용하는 데이터와 정책이 다를 수 있습니다.\n\n\n\n\n과거의 경험(리플레이 버퍼 등)에 저장된 데이터를 재활용할 수 있습니다.\n정책이 바뀌어도 이전의 데이터를 계속 활용할 수 있어 데이터 효율성이 높습니다.\n행동 정책(behavior policy)이 반드시 학습 중인 정책(target policy)와 같을 필요는 없습니다.\n\n\n\n\n\nDQN (Deep Q-Network)\nDDPG (Deep Deterministic Policy Gradient)\nSAC (Soft Actor-Critic)\nTD3 (Twin Delayed Deep Deterministic Policy Gradient)\n\n\n\n\n\n장점:\n\n데이터를 재사용할 수 있어 학습이 효율적\n다양한 정책으로 생성된 데이터를 활용 가능\n\n단점:\n\n학습이 불안정할 수 있음\n복잡한 알고리즘 설계가 필요할 수 있음\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n특징\nOn-Policy\nOff-Policy\n\n\n\n\n데이터 수집 방식\n현재 학습 중인 정책에서 데이터를 생성\n이전에 수집한 데이터도 재활용 가능\n\n\n정책과 데이터 관계\n동일한 정책으로 생성된 데이터만 사용\n서로 다른 정책으로 생성된 데이터도 사용\n\n\n데이터 효율성\n낮음\n높음\n\n\n학습 안정성\n상대적으로 안정적\n불안정할 수 있음\n\n\n대표 알고리즘\nPPO, A3C, TRPO\nDQN, SAC, DDPG, TD3\n\n\n\n\n\n\n\n\n\n\n현재 정책(예: 행동 정책)만을 기반으로 데이터를 수집합니다.\n데이터 수집 후 즉시 정책 업데이트에 활용되며, 과거 데이터를 재사용하지 않습니다.\n\n\n\n\n\n목표 정책(예: 학습 중인 정책)과 다른 행동 정책에서 생성된 데이터를 활용합니다.\n과거 데이터를 재활용하기 위해 경험 재생(experience replay)을 자주 사용합니다.\nOff-policy 학습자들은 부트스트래핑(현재 Q 값을 추정하기 위해 다음 상태/동작의 Q 값을 사용하는 방식)을 사용하기 때문에 불안정할 수 있으며, 경험 재생으로 이를 보완합니다.\n\n\n\n\n\n탐험 상수 ε이 0으로 설정되면 Off-policy 방식이 On-policy 방식처럼 작동할 수 있습니다. (예: SARSA vs Q-러닝)\n하지만 ε-탐욕적 정책이 반드시 모든 환경에서 효율적이지는 않습니다.\n\n\n\n\n\n\n\nOn-policy: 현재 정책으로 데이터를 수집하고 학습하며, 데이터 효율성이 낮지만 안정적입니다.\nOff-policy: 과거 데이터를 재활용하며 데이터 효율성이 높지만, 학습이 불안정할 수 있습니다.\n경험 재생: Off-policy 방식에서 더 흔히 사용되며, 특히 함수 근사기와 함께 안정성을 높이는 데 기여합니다.\n\n\n\n\n\nSARSA와 Q-러닝의 비교에서 On-policy와 Off-policy의 차이를 이해할 수 있습니다.\n\nSARSA: On-policy 방식으로 ε-탐욕적 정책을 사용합니다.\nQ-러닝: Off-policy 방식으로 최대화 정책(maximising policy)을 사용합니다."
  },
  {
    "objectID": "posts/note/2020-10-21-on-and-off-policy.html#on-policy",
    "href": "posts/note/2020-10-21-on-and-off-policy.html#on-policy",
    "title": "📝On policy VS. Off policy",
    "section": "",
    "text": "현재 학습 중인 정책(behavior policy)에서 생성된 데이터를 사용하여 같은 정책을 개선합니다. 즉, 학습에 사용하는 데이터와 정책이 동일합니다.\n\n\n\n\n에이전트는 현재의 정책을 따라 환경과 상호작용하며 데이터를 수집합니다.\n점진적으로 정책이 개선되며, 이전 정책과 크게 다르지 않습니다.\n데이터 효율성은 낮지만, 안정적이고 수렴 성능이 뛰어난 경우가 많습니다.\n\n\n\n\n\nPPO (Proximal Policy Optimization)\nA3C (Asynchronous Advantage Actor-Critic)\nTRPO (Trust Region Policy Optimization)\n\n\n\n\n\n장점:\n\n학습 안정성이 높음\n수렴 속도가 빠를 수 있음\n\n단점:\n\n새로운 데이터를 매번 수집해야 하므로 데이터 효율성이 낮고 계산 비용이 높음"
  },
  {
    "objectID": "posts/note/2020-10-21-on-and-off-policy.html#off-policy",
    "href": "posts/note/2020-10-21-on-and-off-policy.html#off-policy",
    "title": "📝On policy VS. Off policy",
    "section": "",
    "text": "현재의 정책(target policy)을 학습하지만, 다른 정책(behavior policy)에서 생성된 데이터를 활용합니다. 즉, 학습에 사용하는 데이터와 정책이 다를 수 있습니다.\n\n\n\n\n과거의 경험(리플레이 버퍼 등)에 저장된 데이터를 재활용할 수 있습니다.\n정책이 바뀌어도 이전의 데이터를 계속 활용할 수 있어 데이터 효율성이 높습니다.\n행동 정책(behavior policy)이 반드시 학습 중인 정책(target policy)와 같을 필요는 없습니다.\n\n\n\n\n\nDQN (Deep Q-Network)\nDDPG (Deep Deterministic Policy Gradient)\nSAC (Soft Actor-Critic)\nTD3 (Twin Delayed Deep Deterministic Policy Gradient)\n\n\n\n\n\n장점:\n\n데이터를 재사용할 수 있어 학습이 효율적\n다양한 정책으로 생성된 데이터를 활용 가능\n\n단점:\n\n학습이 불안정할 수 있음\n복잡한 알고리즘 설계가 필요할 수 있음"
  },
  {
    "objectID": "posts/note/2020-10-21-on-and-off-policy.html#주요-차이점",
    "href": "posts/note/2020-10-21-on-and-off-policy.html#주요-차이점",
    "title": "📝On policy VS. Off policy",
    "section": "",
    "text": "특징\nOn-Policy\nOff-Policy\n\n\n\n\n데이터 수집 방식\n현재 학습 중인 정책에서 데이터를 생성\n이전에 수집한 데이터도 재활용 가능\n\n\n정책과 데이터 관계\n동일한 정책으로 생성된 데이터만 사용\n서로 다른 정책으로 생성된 데이터도 사용\n\n\n데이터 효율성\n낮음\n높음\n\n\n학습 안정성\n상대적으로 안정적\n불안정할 수 있음\n\n\n대표 알고리즘\nPPO, A3C, TRPO\nDQN, SAC, DDPG, TD3"
  },
  {
    "objectID": "posts/note/2020-10-21-on-and-off-policy.html#추가-설명",
    "href": "posts/note/2020-10-21-on-and-off-policy.html#추가-설명",
    "title": "📝On policy VS. Off policy",
    "section": "",
    "text": "현재 정책(예: 행동 정책)만을 기반으로 데이터를 수집합니다.\n데이터 수집 후 즉시 정책 업데이트에 활용되며, 과거 데이터를 재사용하지 않습니다.\n\n\n\n\n\n목표 정책(예: 학습 중인 정책)과 다른 행동 정책에서 생성된 데이터를 활용합니다.\n과거 데이터를 재활용하기 위해 경험 재생(experience replay)을 자주 사용합니다.\nOff-policy 학습자들은 부트스트래핑(현재 Q 값을 추정하기 위해 다음 상태/동작의 Q 값을 사용하는 방식)을 사용하기 때문에 불안정할 수 있으며, 경험 재생으로 이를 보완합니다.\n\n\n\n\n\n탐험 상수 ε이 0으로 설정되면 Off-policy 방식이 On-policy 방식처럼 작동할 수 있습니다. (예: SARSA vs Q-러닝)\n하지만 ε-탐욕적 정책이 반드시 모든 환경에서 효율적이지는 않습니다."
  },
  {
    "objectID": "posts/note/2020-10-21-on-and-off-policy.html#요약",
    "href": "posts/note/2020-10-21-on-and-off-policy.html#요약",
    "title": "📝On policy VS. Off policy",
    "section": "",
    "text": "On-policy: 현재 정책으로 데이터를 수집하고 학습하며, 데이터 효율성이 낮지만 안정적입니다.\nOff-policy: 과거 데이터를 재활용하며 데이터 효율성이 높지만, 학습이 불안정할 수 있습니다.\n경험 재생: Off-policy 방식에서 더 흔히 사용되며, 특히 함수 근사기와 함께 안정성을 높이는 데 기여합니다.\n\n\n\n\n\nSARSA와 Q-러닝의 비교에서 On-policy와 Off-policy의 차이를 이해할 수 있습니다.\n\nSARSA: On-policy 방식으로 ε-탐욕적 정책을 사용합니다.\nQ-러닝: Off-policy 방식으로 최대화 정책(maximising policy)을 사용합니다."
  },
  {
    "objectID": "posts/note/2025-08-25-simulators.html",
    "href": "posts/note/2025-08-25-simulators.html",
    "title": "📝Simluator Comparison",
    "section": "",
    "text": "시뮬레이터\n물리엔진 / 가속\n렌더·센서\n포맷/자산\nROS(2) 연동\n강점\n유의점\n\n\n\n\nIsaac Sim\nPhysX 5, GPU 가속 지원(옵션) (isaac-sim.github.io)\nOmniverse 기반 PBR, 카메라/LiDAR/접촉 등 센서 풍부 (docs.omniverse.nvidia.com)\nUSD(OpenUSD) 중심, URDF/MJCF 가져오기(Importer 제공) (docs.isaacsim.omniverse.nvidia.com, isaac-sim.github.io)\n공식 ROS 2 Bridge 제공(내장 라이브러리 포함) (docs.isaacsim.omniverse.nvidia.com)\n대규모/고충실도, 합성데이터·검증 워크플로우 강력 (NVIDIA Developer)\nGPU/VRAM 요구 높음(특정 GPU 제약) (docs.omniverse.nvidia.com)\n\n\nIsaac Gym\nPhysX 기반 완전 GPU 파이프라인(Tensor API) (docs.robotsfan.com, NVIDIA Developer)\n(렌더 경량) 대규모 병렬 RL에 초점\nURDF/MJCF 지원 (NVIDIA Developer)\n(직접 ROS 연동은 제한적)\n초고속 대량 병렬 RL\n미리보기(Preview) 릴리스, Isaac Lab로 마이그레이션 권장 (isaac-sim.github.io)\n\n\nPyBullet\nBullet(CPU 중심) / 역·순동역학·IK 제공 (GitHub)\n경량 렌더러(egl/tiny)\nURDF/SDF/MJCF 로딩 (GitHub)\n커뮤니티 브리지/패키지로 ROS 연동 가능 (GitHub, ros-pybullet.github.io)\n설치 간단·스크립팅 쉬움·학습/RL 빠른 반복\n고충실도 센서·대규모 장면 표현력은 제한\n\n\nGazebo (현행 Gazebo Sim)\n기본 DART(런타임 교체 가능: 플러그인 아키텍처) (gazebosim.org)\n전통적 렌더+센서 생태계\nSDF/URDF\nROS 2 통합 튜토리얼/패키지 완비 (gazebosim.org, docs.ros.org)\nROS(2) 워크플로우 표준 격\n물리·그래픽 충실도는 Omniverse/PhysX 5 대비 보수적\n\n\nSAPIEN\nPhysX 백엔드(GPU 지원), RL 친화 API/핀오치오 동역학 사용 옵션 (sapien.ucsd.edu, GitHub)\nVulkan 렌더러, 스테레오 깊이 등 센서(일부 NVIDIA GPU 필요) (sapien.ucsd.edu)\nURDF 등 로딩, RL/Gym 스타일 인터페이스 제공(ManiSkill 계열) (sapien-sim.github.io)\nROS 지원·연구 커뮤니티 활발(PartNet/Mobility 계보) (CVF Open Access)\n로보틱스·조작 연구(특히 파트 수준) 최적\nGPU 메모리/드라이버와의 호환 고려 필요(Colab 이슈 등) (GitHub)\n\n\n\n\n참고: Gazebo Classic(구 버전)은 ODE/Bullet/DART/Simbody 등 다중 백엔드 인터페이스를 제공했습니다. 최신 Gazebo Sim은 기본 DART를 사용하고, 엔진 플러그인으로 확장 가능합니다. (osrf-distributions.s3.amazonaws.com, boschresearch.github.io, gazebosim.org)\n\n\n\n\n\n고충실도 디지털 트윈 / 대량 센서 합성 / 산업 검증\n\nIsaac Sim: PhysX 5 + Omniverse/USD 생태계, ROS 2 Bridge·URDF/MJCF Importer 등 제품급 워크플로우 강점. 다만 GPU·VRAM 요구 확인 필수. (isaac-sim.github.io, docs.isaacsim.omniverse.nvidia.com, docs.omniverse.nvidia.com)\n\n초대규모 병렬 RL\n\nIsaac Gym(Preview): 전체 GPU 파이프라인·Tensor API로 수만 환경 병렬화. 단, 유지보수·API 최신성은 Isaac Lab 전환 흐름을 고려. (docs.robotsfan.com, isaac-sim.github.io)\nSAPIEN/ManiSkill: PhysX GPU 병렬 시뮬 + RL 친화 인터페이스. 손 조작/파츠 상호작용 벤치마크에 강함. (maniskill.readthedocs.io, sapien-sim.github.io)\n\nROS(2) 기반 로보틱스 교육/프로토타이핑 표준\n\nGazebo Sim: ROS 2 튜토리얼·패키지 체계가 가장 완비. 센서/월드 자산 풍부. (gazebosim.org, docs.ros.org)\n\n가벼운 연구/수업/빠른 스크립팅\n\nPyBullet: 설치·사용 간단, URDF/SDF/MJCF·IK/동역학 제공. 고충실·대규모 렌더/센서가 덜 중요하면 최적. ROS 연동은 커뮤니티 패키지로 보완. (GitHub, GitHub)\n\n정교한 접촉·최적화 중심 모델링 대안\n\nMuJoCo(참고): 고속·정확 접촉모델, 오픈소스. 조작/최적화 연구에서 여전히 강자. (mujoco.org, mujoco.readthedocs.io, Google DeepMind)\n\n\n\n\n\n\n\nIsaac Sim\n\nGPU 가속 가능한 PhysX 5 및 GPU 버퍼 사전할당 주의점(Isaac Lab 문서) (isaac-sim.github.io)\n카메라·LiDAR 등 센서 시뮬/Omniverse 기반 설명 (docs.omniverse.nvidia.com)\nURDF/MJCF Importer 및 GUI/스크립트 경로 (docs.isaacsim.omniverse.nvidia.com, isaac-sim.github.io)\nROS 2 Bridge 설치/내장 라이브러리 안내 (docs.isaacsim.omniverse.nvidia.com)\n시스템 요구(GPU 제약) (docs.omniverse.nvidia.com)\n\nIsaac Gym\n\nTensor API로 GPU 상 상태 접근/제어, 완전 GPU 파이프라인 설명 (docs.robotsfan.com)\n피처 요약(URDF/MJCF, 도메인 랜덤화, Jacobian/IK 등) (NVIDIA Developer)\nIsaac Lab로의 마이그레이션 가이드(후속 경로) (isaac-sim.github.io)\n\nPyBullet\n\nURDF/SDF/MJCF 지원, 역/순동역학·IK 제공(Quickstart PDF) (GitHub)\nROS 연동 예: pybullet_ros/ros_pybullet_interface (GitHub, ros-pybullet.github.io)\n\nGazebo\n\n최신 Gazebo Sim의 기본 DART 및 엔진 플러그인 구조 (gazebosim.org)\nClassic의 다중 물리엔진 인터페이스(ODE/Bullet/DART/Simbody) (osrf-distributions.s3.amazonaws.com, boschresearch.github.io)\nROS 2 통합 공식 튜토리얼들 (gazebosim.org, docs.ros.org)\n\nSAPIEN\n\nPhysX 백엔드 명시, 한 프로세스당 엔진 1개 등 기본 구조 (sapien.ucsd.edu)\nVulkan 렌더 및 스테레오 깊이 센서(GPU 요구) (sapien.ucsd.edu)\nRL/Gym 스타일 인터페이스(사피엔 3.x 문서) (sapien-sim.github.io)\n논문/공식 출처에서 PhysX 사용·ROS 지원 계보 정리 (CVF Open Access)\n\n\n\n\n\n\n\n손(Allegro 등) 조작·RL 연구를 많이 하신다면\n\nSAPIEN/ManiSkill: PhysX 기반 GPU 병렬 시뮬 + 파트/접촉 상호작용 강점. 작은 커스텀 과제 반복이 빠릅니다. (maniskill.readthedocs.io)\nIsaac Sim: 합성데이터/고충실 센서·광학이 필요하거나, ROS 2 통합·검증 파이프라인까지 한 번에 가져가려면 좋습니다. (docs.isaacsim.omniverse.nvidia.com)\nIsaac Gym → Isaac Lab: 수천~수만 개 환경 대량 학습이 핵심이면 후속 스택(Isaac Lab)로의 이전을 고려하세요. (isaac-sim.github.io)\n\nROS 2 패키지를 바로 연동해 통합 테스트하려면 → Gazebo Sim이 가장 문서/패키지가 정돈되어 있고, 드론/모바일 등 예제가 풍부합니다. (gazebosim.org, docs.ros.org)\n스크립팅으로 빠르게 아이디어 검증 → PyBullet이 설치/코딩 부담이 최소입니다(URDF/MJCF/SDF·IK/동역학 내장). (GitHub)\n\n\n\n\n\n\nMuJoCo: 오픈소스화 이후 로보틱스·최적화 연구 표준 중 하나. 접촉 모델·속도/정확도 균형이 강점. (mujoco.org, mujoco.readthedocs.io)\nWebots: 교육·모바일 로봇 친화, ROS 2 패키지 제공. (docs.ros.org, GitHub)\nCoppeliaSim: 풍부한 내장 기능과 스크립팅, ROS 2 인터페이스 공식 제공. (CoppeliaSim 사용자 설명서)\nDrake: MultibodyPlant를 통한 정확한 동역학/기하 파이프라인, 연구용 도구로 우수. (Drake)"
  },
  {
    "objectID": "posts/note/2025-08-25-simulators.html#어떤-용도에-무엇을-쓰면-좋은가",
    "href": "posts/note/2025-08-25-simulators.html#어떤-용도에-무엇을-쓰면-좋은가",
    "title": "📝Simluator Comparison",
    "section": "",
    "text": "고충실도 디지털 트윈 / 대량 센서 합성 / 산업 검증\n\nIsaac Sim: PhysX 5 + Omniverse/USD 생태계, ROS 2 Bridge·URDF/MJCF Importer 등 제품급 워크플로우 강점. 다만 GPU·VRAM 요구 확인 필수. (isaac-sim.github.io, docs.isaacsim.omniverse.nvidia.com, docs.omniverse.nvidia.com)\n\n초대규모 병렬 RL\n\nIsaac Gym(Preview): 전체 GPU 파이프라인·Tensor API로 수만 환경 병렬화. 단, 유지보수·API 최신성은 Isaac Lab 전환 흐름을 고려. (docs.robotsfan.com, isaac-sim.github.io)\nSAPIEN/ManiSkill: PhysX GPU 병렬 시뮬 + RL 친화 인터페이스. 손 조작/파츠 상호작용 벤치마크에 강함. (maniskill.readthedocs.io, sapien-sim.github.io)\n\nROS(2) 기반 로보틱스 교육/프로토타이핑 표준\n\nGazebo Sim: ROS 2 튜토리얼·패키지 체계가 가장 완비. 센서/월드 자산 풍부. (gazebosim.org, docs.ros.org)\n\n가벼운 연구/수업/빠른 스크립팅\n\nPyBullet: 설치·사용 간단, URDF/SDF/MJCF·IK/동역학 제공. 고충실·대규모 렌더/센서가 덜 중요하면 최적. ROS 연동은 커뮤니티 패키지로 보완. (GitHub, GitHub)\n\n정교한 접촉·최적화 중심 모델링 대안\n\nMuJoCo(참고): 고속·정확 접촉모델, 오픈소스. 조작/최적화 연구에서 여전히 강자. (mujoco.org, mujoco.readthedocs.io, Google DeepMind)"
  },
  {
    "objectID": "posts/note/2025-08-25-simulators.html#각-시뮬레이터-핵심-근거요약",
    "href": "posts/note/2025-08-25-simulators.html#각-시뮬레이터-핵심-근거요약",
    "title": "📝Simluator Comparison",
    "section": "",
    "text": "Isaac Sim\n\nGPU 가속 가능한 PhysX 5 및 GPU 버퍼 사전할당 주의점(Isaac Lab 문서) (isaac-sim.github.io)\n카메라·LiDAR 등 센서 시뮬/Omniverse 기반 설명 (docs.omniverse.nvidia.com)\nURDF/MJCF Importer 및 GUI/스크립트 경로 (docs.isaacsim.omniverse.nvidia.com, isaac-sim.github.io)\nROS 2 Bridge 설치/내장 라이브러리 안내 (docs.isaacsim.omniverse.nvidia.com)\n시스템 요구(GPU 제약) (docs.omniverse.nvidia.com)\n\nIsaac Gym\n\nTensor API로 GPU 상 상태 접근/제어, 완전 GPU 파이프라인 설명 (docs.robotsfan.com)\n피처 요약(URDF/MJCF, 도메인 랜덤화, Jacobian/IK 등) (NVIDIA Developer)\nIsaac Lab로의 마이그레이션 가이드(후속 경로) (isaac-sim.github.io)\n\nPyBullet\n\nURDF/SDF/MJCF 지원, 역/순동역학·IK 제공(Quickstart PDF) (GitHub)\nROS 연동 예: pybullet_ros/ros_pybullet_interface (GitHub, ros-pybullet.github.io)\n\nGazebo\n\n최신 Gazebo Sim의 기본 DART 및 엔진 플러그인 구조 (gazebosim.org)\nClassic의 다중 물리엔진 인터페이스(ODE/Bullet/DART/Simbody) (osrf-distributions.s3.amazonaws.com, boschresearch.github.io)\nROS 2 통합 공식 튜토리얼들 (gazebosim.org, docs.ros.org)\n\nSAPIEN\n\nPhysX 백엔드 명시, 한 프로세스당 엔진 1개 등 기본 구조 (sapien.ucsd.edu)\nVulkan 렌더 및 스테레오 깊이 센서(GPU 요구) (sapien.ucsd.edu)\nRL/Gym 스타일 인터페이스(사피엔 3.x 문서) (sapien-sim.github.io)\n논문/공식 출처에서 PhysX 사용·ROS 지원 계보 정리 (CVF Open Access)"
  },
  {
    "objectID": "posts/note/2025-08-25-simulators.html#상황별-추천사용자-환경을-고려한-tip",
    "href": "posts/note/2025-08-25-simulators.html#상황별-추천사용자-환경을-고려한-tip",
    "title": "📝Simluator Comparison",
    "section": "",
    "text": "손(Allegro 등) 조작·RL 연구를 많이 하신다면\n\nSAPIEN/ManiSkill: PhysX 기반 GPU 병렬 시뮬 + 파트/접촉 상호작용 강점. 작은 커스텀 과제 반복이 빠릅니다. (maniskill.readthedocs.io)\nIsaac Sim: 합성데이터/고충실 센서·광학이 필요하거나, ROS 2 통합·검증 파이프라인까지 한 번에 가져가려면 좋습니다. (docs.isaacsim.omniverse.nvidia.com)\nIsaac Gym → Isaac Lab: 수천~수만 개 환경 대량 학습이 핵심이면 후속 스택(Isaac Lab)로의 이전을 고려하세요. (isaac-sim.github.io)\n\nROS 2 패키지를 바로 연동해 통합 테스트하려면 → Gazebo Sim이 가장 문서/패키지가 정돈되어 있고, 드론/모바일 등 예제가 풍부합니다. (gazebosim.org, docs.ros.org)\n스크립팅으로 빠르게 아이디어 검증 → PyBullet이 설치/코딩 부담이 최소입니다(URDF/MJCF/SDF·IK/동역학 내장). (GitHub)"
  },
  {
    "objectID": "posts/note/2025-08-25-simulators.html#others",
    "href": "posts/note/2025-08-25-simulators.html#others",
    "title": "📝Simluator Comparison",
    "section": "",
    "text": "MuJoCo: 오픈소스화 이후 로보틱스·최적화 연구 표준 중 하나. 접촉 모델·속도/정확도 균형이 강점. (mujoco.org, mujoco.readthedocs.io)\nWebots: 교육·모바일 로봇 친화, ROS 2 패키지 제공. (docs.ros.org, GitHub)\nCoppeliaSim: 풍부한 내장 기능과 스크립팅, ROS 2 인터페이스 공식 제공. (CoppeliaSim 사용자 설명서)\nDrake: MultibodyPlant를 통한 정확한 동역학/기하 파이프라인, 연구용 도구로 우수. (Drake)"
  },
  {
    "objectID": "posts/note/2022-05-06-geultto-7th-start.html",
    "href": "posts/note/2022-05-06-geultto-7th-start.html",
    "title": "📘Geultto 7th Start",
    "section": "",
    "text": "전부터 알고 있었던 글쓰는 또라이, 글또는 언젠가 꼭 들어가고 싶은 모임이었다. 내가 아는 지식에 대해 이야기 할 줄 아는 능력이 멋져보이기도 했고 무엇보다 다른 사람들에게 도움이 되기도 하겠지만 글을 쓰면서 오히려 내 머릿속에서 정리되고 나에게 가장 큰 도움이 되기 때문에(과거의 내가 미래의 나를 다시 가르쳐줄거란 생각으로) 글을 쓰고 싶었다. 글을 쓰는 걸 좋아하긴 하지만 IT, 개발관련 글을 쓰는 건 평소에 쉽게 써내려가는 감성적이고 주관적인 글쓰기와는 조금 거리가 있기 때문에 좀 망설였던 부분도 있었다. 혹시나 틀린 정보나 생각을 적으면 어떡하지 등의 걱정과 함께 계속해서 미뤄져온 글쓰기는 이제는 좀 마음의 짐을 덜고 현재에 최선을 다해서 적으면 된다라는 생각으로 시작하려고 한다.\n블로그 방황기에 대해 말하자면 그 역사가 2년 정도 되는데, 그 기간동안 Github pages, 유명한 Jekyll, Go언어 써보기 위해서 도전한 Hugo, 빠르게 포스팅할 수 있어서 잠시 돌아간 Naver blog, 코딩과는 안맞는 것 같아서 그만둔 Brunch, 영어로 포스팅하는 거 연습하기 위해서 해본 Medium, 쉽게 생각했다가 커스텀하기 더 어려워서 그만둔 Tistory, 기록용으로는 좋지만 Github과 연동이 안되어서 아쉬운 Notion까지.. 가능한 블로그 플랫폼들은 다 해봤던 것 같다. 그 중에 글또와 함께 정착할 플랫폼은 Fastai에서 만들어준 Fastpages로 결정했다. 다음은 내가 블로그 플랫폼을 결정할 때 생각했던 우선순위들인데, Fastpages가 가장 잘 만족하는 플랫폼이었다.\n\n작성하는 방법이 어렵지 않은 지\n(해외 검색까지 고려해서)Googling이 가능하도록 한 지\nGithub과 연동(기록)이 되는 지\n수식작성과 Table of Contents(TOC), 태그 설정이 가능한 지\n블로그내 포스팅 검색이 가능한 지\n광고가 붙지 않는지(컨텐츠에 집중할 수 있는 깔끔함)\n\nMarkdown과 Jupyter Notebook 형식의 작성을 모두 지원하면서 검색이나 태그 설정이 만들어져 있었고 형식이 깔끔하다. 다크 템플릿을 위해서 조금 설정을 해준 것 말고는 힘들게 셋팅하는 부분들이 없었다.\n인생은 타이밍이라는 말처럼 어떤 일을 할 때의 시기가 있다고 생각한다. 시간적인 여유를 생각했을 때는 대학생 때 작성하는 습관을 들이는 걸 연습하기에 좋다고 생각할 수 있지만, 그때의 나를 돌이켜보면 다른 사람들의 포스팅을 보면서 배우기에도 급급했던 것 같다. 지금 석사과정을 시작하면서 내가 배운 것들을 최대한 기록하고, 그 기록들을 발판삼아 더 성장하는 공학자가 되기 위한 타이밍이 지금이 아닐까 싶다. 글또를 통해 이전의 블로그 방황기와 간헐적이었던 글쓰기를 멈출 수 있기를 기대한다.\n\n\n글또 7기를 진행하면서 약 10-20여개의 글을 쓰게 되는데, 한 가지 주제에만 집중해서 쭉 포스팅하는 것이 좋겠지만 크게 제약을 두지 않고 시작하려고 한다. 한 가지 주제만 해야한다는 생각에 미루게 되는 핑계를 스스로 만들지 않고 그때 내가 배운, 내가 나누고 싶은 이야기를 적는 걸로 계획했다. 하지만 큰 글감들로는 아래와 같은 내용들을 생각하고 있다.\n\n로봇 제어(이론이나 시뮬레이션 툴 등)\n로봇 연구에서의 강화학습\nJulia 언어로 하는 Robotics\n\n\n\n\n글또 7기에서 꼭 이뤘으면 하는 목표는 다음과 같다.\n\n공식적인 포스팅 작성 횟수 다 채우기\n커피챗 참여\n\n추가적인 목표로는, 글또 7기 Slack에 많은 채널들이 있는데(#멍또, #냥또는 사랑입니다❣️) 그 중에서 #알또와 #헬또를 통해 알고리즘 실력과 운동도 챙기고 싶다.\n\n1주일에 1알고리즘 문제 풀기\n주 3일 운동하기\n\n최근에 한해의 Github commit을 3D로 만들어주는 기능을 알게 되었는데 글또와 함께한 2022 커밋은 대도시의 건물 숲처럼 더 멋지게 만들어졌으면 좋겠다.🏙"
  },
  {
    "objectID": "posts/note/2022-05-06-geultto-7th-start.html#글쓰는-또라이-글또",
    "href": "posts/note/2022-05-06-geultto-7th-start.html#글쓰는-또라이-글또",
    "title": "📘Geultto 7th Start",
    "section": "",
    "text": "전부터 알고 있었던 글쓰는 또라이, 글또는 언젠가 꼭 들어가고 싶은 모임이었다. 내가 아는 지식에 대해 이야기 할 줄 아는 능력이 멋져보이기도 했고 무엇보다 다른 사람들에게 도움이 되기도 하겠지만 글을 쓰면서 오히려 내 머릿속에서 정리되고 나에게 가장 큰 도움이 되기 때문에(과거의 내가 미래의 나를 다시 가르쳐줄거란 생각으로) 글을 쓰고 싶었다. 글을 쓰는 걸 좋아하긴 하지만 IT, 개발관련 글을 쓰는 건 평소에 쉽게 써내려가는 감성적이고 주관적인 글쓰기와는 조금 거리가 있기 때문에 좀 망설였던 부분도 있었다. 혹시나 틀린 정보나 생각을 적으면 어떡하지 등의 걱정과 함께 계속해서 미뤄져온 글쓰기는 이제는 좀 마음의 짐을 덜고 현재에 최선을 다해서 적으면 된다라는 생각으로 시작하려고 한다.\n블로그 방황기에 대해 말하자면 그 역사가 2년 정도 되는데, 그 기간동안 Github pages, 유명한 Jekyll, Go언어 써보기 위해서 도전한 Hugo, 빠르게 포스팅할 수 있어서 잠시 돌아간 Naver blog, 코딩과는 안맞는 것 같아서 그만둔 Brunch, 영어로 포스팅하는 거 연습하기 위해서 해본 Medium, 쉽게 생각했다가 커스텀하기 더 어려워서 그만둔 Tistory, 기록용으로는 좋지만 Github과 연동이 안되어서 아쉬운 Notion까지.. 가능한 블로그 플랫폼들은 다 해봤던 것 같다. 그 중에 글또와 함께 정착할 플랫폼은 Fastai에서 만들어준 Fastpages로 결정했다. 다음은 내가 블로그 플랫폼을 결정할 때 생각했던 우선순위들인데, Fastpages가 가장 잘 만족하는 플랫폼이었다.\n\n작성하는 방법이 어렵지 않은 지\n(해외 검색까지 고려해서)Googling이 가능하도록 한 지\nGithub과 연동(기록)이 되는 지\n수식작성과 Table of Contents(TOC), 태그 설정이 가능한 지\n블로그내 포스팅 검색이 가능한 지\n광고가 붙지 않는지(컨텐츠에 집중할 수 있는 깔끔함)\n\nMarkdown과 Jupyter Notebook 형식의 작성을 모두 지원하면서 검색이나 태그 설정이 만들어져 있었고 형식이 깔끔하다. 다크 템플릿을 위해서 조금 설정을 해준 것 말고는 힘들게 셋팅하는 부분들이 없었다.\n인생은 타이밍이라는 말처럼 어떤 일을 할 때의 시기가 있다고 생각한다. 시간적인 여유를 생각했을 때는 대학생 때 작성하는 습관을 들이는 걸 연습하기에 좋다고 생각할 수 있지만, 그때의 나를 돌이켜보면 다른 사람들의 포스팅을 보면서 배우기에도 급급했던 것 같다. 지금 석사과정을 시작하면서 내가 배운 것들을 최대한 기록하고, 그 기록들을 발판삼아 더 성장하는 공학자가 되기 위한 타이밍이 지금이 아닐까 싶다. 글또를 통해 이전의 블로그 방황기와 간헐적이었던 글쓰기를 멈출 수 있기를 기대한다.\n\n\n글또 7기를 진행하면서 약 10-20여개의 글을 쓰게 되는데, 한 가지 주제에만 집중해서 쭉 포스팅하는 것이 좋겠지만 크게 제약을 두지 않고 시작하려고 한다. 한 가지 주제만 해야한다는 생각에 미루게 되는 핑계를 스스로 만들지 않고 그때 내가 배운, 내가 나누고 싶은 이야기를 적는 걸로 계획했다. 하지만 큰 글감들로는 아래와 같은 내용들을 생각하고 있다.\n\n로봇 제어(이론이나 시뮬레이션 툴 등)\n로봇 연구에서의 강화학습\nJulia 언어로 하는 Robotics\n\n\n\n\n글또 7기에서 꼭 이뤘으면 하는 목표는 다음과 같다.\n\n공식적인 포스팅 작성 횟수 다 채우기\n커피챗 참여\n\n추가적인 목표로는, 글또 7기 Slack에 많은 채널들이 있는데(#멍또, #냥또는 사랑입니다❣️) 그 중에서 #알또와 #헬또를 통해 알고리즘 실력과 운동도 챙기고 싶다.\n\n1주일에 1알고리즘 문제 풀기\n주 3일 운동하기\n\n최근에 한해의 Github commit을 3D로 만들어주는 기능을 알게 되었는데 글또와 함께한 2022 커밋은 대도시의 건물 숲처럼 더 멋지게 만들어졌으면 좋겠다.🏙"
  },
  {
    "objectID": "posts/note/2025-01-07-ros2-cpp-python.html",
    "href": "posts/note/2025-01-07-ros2-cpp-python.html",
    "title": "📝Python과 C++ 노드를 모두 포함하는 ROS2 패키지 생성",
    "section": "",
    "text": "ROS2 공식 문서에서는 C++와 Python Node를 별개로 만드는 튜토리얼과 설명만 있기 때문에 C++와 Python 모두 사용하여 노드를 만들고 하나의 ROS2 패키지로 만들기 위한 방법에 대해 알아보겠습니다. 각 단계마다 변경되는 사항에 대해서는 🟢로 표시되어 있으니 단계를 넘어갈때마다 확인해보시기 바랍니다."
  },
  {
    "objectID": "posts/note/2025-01-07-ros2-cpp-python.html#recap-ros2-package-architecture",
    "href": "posts/note/2025-01-07-ros2-cpp-python.html#recap-ros2-package-architecture",
    "title": "📝Python과 C++ 노드를 모두 포함하는 ROS2 패키지 생성",
    "section": "Recap ROS2 Package Architecture",
    "text": "Recap ROS2 Package Architecture\nmy_cpp_py_pkg/\n🟡 패키지 정보, 구성, 및 컴파일\n├── CMakeLists.txt\n├── package.xml\n🟡 Python 관련 Stuff\n├── my_cpp_py_pkg\n│   ├── __init__.py\n│   └── module_to_import.py\n├── scripts\n│   └── py_node.py\n🟡 Cpp 관련 Stuff\n├── include\n│   └── my_cpp_py_pkg\n│       └── cpp_header.hpp\n└── src\n    └── cpp_node.cpp"
  },
  {
    "objectID": "posts/note/2023-12-01-geultto-9th-start.html",
    "href": "posts/note/2023-12-01-geultto-9th-start.html",
    "title": "📘Geultto 9th Start",
    "section": "",
    "text": "끝과 시작\n7, 8기에 이어 3번째로 글또 9기를 시작하게 되었습니다.\n사실 7기 시작할 때만 하더라도 계속 이어서 참여를 하게 될 것이라고 생각하진 않았었는데, 지금까지 계속 글을 써오면서 재미를 느끼기도 했고 약간의 반강제성이 있어야 블로그에 하나라도 글을 작성하는 제 모습을 보며 나에게 글또가 필요한 존재라는 것을 깨달았습니다.\n2023년 연말을 향해, 그리고 제 인생에서는 대학원 생활의 마무리를 하는 과정에서 끝을 향해 달려가고 있는 듯한 느낌이지만, 글또 9기를 시작하며 끝과 시작이 이어지는 듯한 느낌이라 설레이는 마음도 큰 것 같습니다. 언제나 그랬듯이, 다시 글또라는 커뮤니티 안에서 내가 쓰고자 하는 글들, 하고자 하는 일들에 대해 정리를 해보는 글을 써보려고 합니다. 7기 다짐글은 블로그 삽질했던 이야기와 함께 간단하게 블로그에 적고 싶은 주제들 등을 적었었고, 8기 다짐글은 글또 OT에서 소개해주셨던 Big5 유형 검사와 함께 논문의 형식을 빌려 나름 패러디 형식으로 작성했었던 것 같습니다. 이번에는 어떻게 시작하는 이야기를 할 수 있을까.. 하다가 생각난 어렸을 적 기억남는 에피소드를 하나 회상하는 것으로 시작하려 합니다.\n\n\nDomino Tower\n정확한 시기는 기억이 나지 않지만 아마 초등학교 4-5학년 그쯤이었던 것 같습니다. 학교에서 수련회를 가면 항상 레크레이션 프로그램으로 협동 게임을 진행했었는데, 한번은 도미노 블럭들을 가지고 가장 긴 도미노 줄 세우기, 도미노로 가장 높게 쌓기 등을 반 끼리 대결하는 시간이었습니다.\n\n\n\n세계에서 가장 높은 도미노 타워는 한 대학원생이 세운 5.275m 라고 하네요 (Source: World’s tallest domino tower)\n\n\n위의 사진처럼 도미노로 가장 높게 탑 쌓기로 대결에서 다른 친구들이 각기 다른 여러 모양으로 탑을 쌓아가기 시작했고, 한 반에 약 30명 정도 있었어서 여러 모양의 탑들이 올라가기 시작했습니다. 다양한 아이디어들을 가지고 각기 모여졌다 흩어졌다 하며 무너지고 쌓기를 반복하며 각자 나름대로의 열정으로 미션에 열중하고 있었습니다.\n저도 몇번 시도해보고 친구들이 쌓는 모습들도 보면서 오각형으로 쌓았을 때가 가장 안정적인 것 같다 라는 걸 판단할 수 있었고, 그 때부터 조용히 혼자 하나하나 쌓아 올려갔습니다. 그때도 성격이 조용했던 터라 그냥 생각했던 대로 혼자 만들기 시작했고, 반 친구들은 옆에서 반대표 작품 하나를 크게 만드는 것에 집중하고 있었습니다. 혼자서 만들다보니 생각보다 너무 잘 쌓아 올려져서 의자를 빌려서 까지 올라가서 쌓아갔었고 결과적으로 저희 반이 제가 쌓기 시작했던 탑으로 1등을 했었습니다. 수련회 교관 선생님들도 꽤 많이 놀라셨던 게 기억이 납니다. 탑의 높이가 가장 높았을 뿐만 아니라 나중에 쓰러뜨리기 위해 중간에 한 조각 건드렸음에도 안 무너졌을 정도로 도미노 탑을 잘 만들었었던 걸로 기억합니다.\n가끔씩 지금 하고 있는 일이 제대로 하고 있는 걸까 스스로 물음표가 생길 때마다 무의식적으로 이 도미노 탑 에피소드가 떠오르는데, 아마 그 때의 기억이 나의 관찰과 생각으로 하나씩 쌓다보면 무언가 완성이 될 것이다라는 경험과 교훈을 떠오르게 해서 그런 것 같습니다.\n\n\n지금의 도미노 한 조각\n그렇다면 지금 내 도미노 한 조각은 어디에 올라가고 있는지.\n나의 물음표에 쉽게 답을 하지 못하면, 최대한 단순하게 내 도미노 한 조각이 어디로 향하고 있는지 고민해봅니다. 자세한 고민들과 여러 곁가지 생각들은 23년도 회고록에 남기기로 하고.. 이번에는 글또 9기 시작 다짐글인 만큼, 엔지니어를 꿈꾸는 사람으로써 글을 쓰는 사람이 되고 싶은 마음 한 조각 도미노 블럭에 대한 이야기를 풀어보려고 합니다.\n앞서 이야기 했듯이 글또라는 모임을 통해 반강제적인 글쓰는 스케줄링을 통해서 지금까지 블로그에 어렵게 쌓아온 글들이 나름 뿌듯합니다. 백프로 모든 글들이 맘에 들지 않지만, 써왔던 글들을 가끔 다시 읽어보며 아 맞아 이걸 배울 수 있었지, 이건 이렇게 쓰면 좋았을 껄, 이건 잘 쓴 것 같다등 회고를 하고 한번 더 성장하는 느낌이 듭니다. 아마 도미노 2-3층 짜리는 되지 않았을까? 생각해봅니다. (적어도 이제 블로그 이사는 안 다니고 안정적으로 이 블로그에다 쓸 수 있을 것 같으니 탑을 쌓아 올리기 전에 기반 공사 오래했다고 생각해도 될 듯 하네요.)\n그래서 앞으로 3층 이상의 블로그 도미노 탑을 쌓기 위해 글또 9기에서 다짐해보는 내용은 아래와 같습니다.\n\n글또 마감 지켜서 모든 회차 제출하기: 매 기수 첫 다짐이지만 놀랍게도 한번도 지키지 못했다는 전설의 다짐. 이번에도 솔직히 지킬 수 있을까 두렵지만 삼세판의 기적을 믿어보겠습니다.\n큐레이션 선정 3번 되어보기: 저번부터 생긴 큐레이션 선정이 동기부여가 많이 되었기도 하고 스스로 글을 더 점검하고 퇴고하는 즐거움도 느낄 수 있어서 이번에도 3번 정도는 채워보자!를 목표로 설정해보려고 합니다. 사실 저번 기수에 4번 선정 되었었지만 이번에는 왠지.. 석사 졸업하고 퀄리티 있는 글을 쓸 시간이 줄어들 것 같은 노파심에 1회 줄여서 현실성 있게 3번이면 잘 활동한 거라고 스스로 상주고 싶네요.\n글또에서 더 넓은 세상을 보고 지혜롭게 살아가는 법 배우기: 이전 기수에서 커피챗, 반상회 모임 등을 통해서 이미 멋진 분들을 보고 동기부여를 많이 받았었기에 이번에도 글또에서 얻을 수 있는 혜택인 글또 안에서 멋진 분들의 생각들을 들을 수 있는 기회들을 가지고자 합니다. 그런데 이번에는 횟수보다는 만남의 여운을 좀 더 신경 써보기로 했습니다..! Action plan으로는 만나는 분들과의 대화 속에서 나는 무엇을 느끼고 배웠는지 진지하게 인터뷰이처럼 기록해볼 생각입니다.(물론 이 이야기는 제 개인 소장 노트에만 적을 것 같습니다.)\n\n이 세가지 목표가 이번 글또 9기에서 이상 없이 이루어지길..! 노력해보겠습니다.\n\n\n마무리\n최근에 새벽에 논문을 쓰다가 스쳐 지나가는 생각이 있어서 메모지에 적었는데, 지금 글또 다짐글을 적다가 앞에 메모지가 보여서 옮겨 적어보았습니다.\n머리와 마음이 생각처럼 되지 않을 때는 손이나 발을 움직이자.\n머리가 안돌아가면 손을 움직이고\n생각이 멈춰있으면 발을 움직이자.\n마치 생각을 뻗어나가야 하는 상황에서는 발을 내딛듯한 느낌을 받아야 하고,\n마음을 정리해야 할 때에는 손으로 끄적이며 엉킨 실타래를 풀어나가야 하는 것처럼\n보이지 않는 것과 보이는 것을 무의식적으로 맞춰나갈 때 해결이 되는 것 같다.\n항상 앉아서 공부하고 정숙한 환경에서 눈 앞의 책 위의 활자들을 보며 열심히 머리를 굴리는 방법만 익히다 보면, 가끔씩 몸의 움직임의 중요성을 까먹는 것 같습니다. 요근래 움직이면 오히려 머릿속과 마음속에서 복잡했던 것들이 쉽게 풀리는 걸 경험했었기에 이번에도 글을 쓰거나 공부하면서 막혀서 멍때리고 있을 미래의 나에게 움직이고 다시 생각해보자라는 메세지를 남기며 다짐글을 이만 마치겠습니다."
  },
  {
    "objectID": "posts/note/2023-05-27-twinkle-github-star.html",
    "href": "posts/note/2023-05-27-twinkle-github-star.html",
    "title": "📘Github Starstruck 128",
    "section": "",
    "text": "2023.05.27 또 하나의 작은 성공을 기록하게 된 날이 되었습니다.\n사실 5월달이 들어서면서부터 이 날을 기록할 수 있기를 바라며 로그를 체크하고 있었는데 드디어 새벽 1시 11분에 달성이 되었다고 로그가 떠서 반가운 마음에 이야기를 남기기 위해 블로그 글을 쓰게 되었습니다."
  },
  {
    "objectID": "posts/note/2023-05-27-twinkle-github-star.html#motivation",
    "href": "posts/note/2023-05-27-twinkle-github-star.html#motivation",
    "title": "📘Github Starstruck 128",
    "section": "Motivation",
    "text": "Motivation\n잠깐 이야기가 나왔던 왜 이 repo를 만들게 되었는지에 대해 이야기해보려고 합니다. Github에서는 커밋 히스토리를 추적할 수 있으니까 이 레포를 처음 만들 당시로 돌아가서 first commit을 봤습니다. 사실 저는 막연하게 제가 대학원 과정을 시작하며 만들었겠거니..라고 예상하고 있었는데 그 보다 더 일찍, 그러니까 좀 더 정확하게 말하자면 대학교 4학년 막학기 10월에 만든 것이라는 것을 확인할 수 있었습니다.\n\n제 기억이 맞다면 당시에 졸업작품과 졸업 이후의 진로 고민으로 인해 이런저런 고민을 많이 하는 시기였는데 아마 이때쯤 진학할 대학원들을 알아보고 해당 레포를 만들면서 만약에 내가 로보틱스를 공부한다면 어떤 것을 알아야 하는지 정리해보기 위해 만든 것 같습니다. 그때만 하더라도 4족 보행 로봇이라는 정확한 로봇의 종류에 대해 결정되지 않았기에 더 폭넓게 Robotics라고 정하고 시작했던 것 같습니다.\n대학원을 진학한 이후, 알아야 할 것들은 너무 많고 여러 연구실에서 쏟아지는 논문들을 감당할 수 없어서 정리를 해야겠다 생각했습니다. 처음에는 아무도 관심없지만 괜히 공개되는 인터셋 상에 올리는 것은 부담스러워서 개인적으로 기록하는 곳(notion이나 privite repo)에 모았지만, 지금까지 저도 다른 사람들이 정리한 리스트들과 인사이트들을 통해 많이 성장할 수 있었고 나도 아직 많이 부족하지만 기여하는 부분이 있으면 좋겠다는 생각에 공개 repo로 전환하여 모으기 시작했습니다. 공개된 리스트라고 생각하니 좀 더 신경을 쓰게 되고 좋은 자극 효과가 되서 내가 기록한 내용을 한번 더 보게되고 그런 경험들이 쌓여 인사이트도 성장하게 되어서 결과적으로 제 스스로에게 더 도움이 많이 되었던 것 같습니다."
  },
  {
    "objectID": "posts/note/2023-05-27-twinkle-github-star.html#how-to-write",
    "href": "posts/note/2023-05-27-twinkle-github-star.html#how-to-write",
    "title": "📘Github Starstruck 128",
    "section": "How to write",
    "text": "How to write\n제가 어떻게 해당 repo를 관리하고 작성해오고 있는지 몇가지 팁들을 공유해보려고 합니다.\nSource\n큐레이션이나 어떤 도움이 되는 정보를 리스트화 한다는 것은 많은 소스들이 우선 전제가 되어야 합니다. 관련 분야 연구자로써 같이 연구하는 동료들과 선후배로부터 얻는 정보들도 물론 많습니다. 하지만 이렇게 privite group에서 얻는 정보들 이외에도 제가 source들을 얻는 주요 경로들은 아래와 같습니다.\n\nGoogle Scholar에서 해당 분야에서 활발하게 연구 성과를 내고 있는 연구자들을 follow합니다. 이메일로 해당 연구자가 paper를 내거나 citation되면 알람이 오는데 저는 빠르게 알람온 리스트들을 abstract/result를 살펴보고 공유하기에 좋다고 판단되면 리스팅합니다.\nYoutube 채널 구독을 통해 얻게 되는 정보들도 많습니다. 생각보다 아카이브에 페이퍼를 먼저 올리기 보다 결과 영상이나 설명 영상들을 유튜브에 먼저 올려서 소식을 먼저 알리는 곳들이 많습니다. 제가 repo 카테고리에 youtube 채널을 넣은 이유이기도 합니다.\nGithub도 연구자들이 코드를 올리면서 README에 연구에 대한 설명과 함께 공개합니다. 유튜브와 같은 맥락으로 페이퍼보다 먼저 공개하는 경우들도 많고 오히려 코드를 공개해주다보니 연구를 더 쉽고 빠르게 파악할 수 있을 때도 있습니다. 그래서 관련 연구자들을 follow해서 어떤 커밋을 올리고 있는지, 어떤 레포들을 관심있게 보는지(star/cloning)도 알게 되고 Github organization으로 연구실이나 회사 단위로 활동하는 것을 파악하여 정보를 얻을 때도 있습니다.\n\nEditor\nmarkdown을 작성하는게 복잡한 소스코드도 아니기 때문에 git clone/commit 하는게 어려운 일은 아니지만, awesome list를 작성하기 위해 매번 다른 컴퓨터에서 clone하고 커밋하며 싱크를 맞추는 일이 생각보다 귀찮습니다. 그래서 저는 awesome list를 작성할 때는 온라인으로 특별한 editor 없이 바로 작성하고 정렬도 할 수 있는 github Code space를 사용합니다. README.md 리스트 알파벳순 자동 정렬을 python code(asset/ordering.py)로 하기 때문에 python이 설치되어있지 않은 컴퓨터에서는 정렬이 어려운데 Code space를 사용하면 python이 설치되어 있지 않은 인터넷이 되는 모든 컴퓨터에서 작성할 수 있으니 everywhere, anytime이 가능하고 UI는 VS Code와 같기 때문에 어렵지도 않습니다. 커밋도 바로 하구요!"
  },
  {
    "objectID": "posts/note/2025-07-08-git-lfs.html",
    "href": "posts/note/2025-07-08-git-lfs.html",
    "title": "📝Git LFS 사용하기",
    "section": "",
    "text": "Git을 사용하다 보면 .pt, .zip, .so, .h5 같은 대용량 파일을 올려야 할 때가 있습니다. 그런데 Git에는 일반적으로 100MB 이상 파일 제한이 있어, 이런 파일을 그대로 커밋하면 푸시가 거부되거나 저장소가 무거워집니다.\n이 문제를 해결해주는 게 바로 Git LFS (Large File Storage) 입니다.\n이 글에서는 Git LFS의 개념부터 실전 사용법까지 간단하게 정리해보겠습니다."
  },
  {
    "objectID": "posts/note/2025-07-08-git-lfs.html#git-lfs란",
    "href": "posts/note/2025-07-08-git-lfs.html#git-lfs란",
    "title": "📝Git LFS 사용하기",
    "section": "✅ Git LFS란?",
    "text": "✅ Git LFS란?\nGit LFS는 대용량 파일을 Git 저장소 대신 별도의 LFS 서버에 저장하는 방식입니다.\n\nGit에는 작은 포인터 파일만 저장됨\n진짜 파일은 LFS 서버에서 별도로 관리됨\n사용자는 기존 Git처럼 add, commit, push, clone 명령으로 작업 가능\n\n👉 GitHub, GitLab, Bitbucket 등 다양한 Git 호스팅 플랫폼에서 지원됩니다."
  },
  {
    "objectID": "posts/note/2025-07-08-git-lfs.html#설치-및-초기화",
    "href": "posts/note/2025-07-08-git-lfs.html#설치-및-초기화",
    "title": "📝Git LFS 사용하기",
    "section": "🛠️ 설치 및 초기화",
    "text": "🛠️ 설치 및 초기화\n\n1. Git LFS 설치\n# macOS\nbrew install git-lfs\n\n# Ubuntu / Debian\nsudo apt install git-lfs\n\n# Windows\nhttps://git-lfs.github.com 에서 설치 프로그램 다운로드\n\n\n2. 설치 후 초기화\ngit lfs install\n\n한 번만 실행하면 전역 설정에 등록됩니다."
  },
  {
    "objectID": "posts/note/2025-07-08-git-lfs.html#추적할-파일-등록",
    "href": "posts/note/2025-07-08-git-lfs.html#추적할-파일-등록",
    "title": "📝Git LFS 사용하기",
    "section": "📂 추적할 파일 등록",
    "text": "📂 추적할 파일 등록\n예를 들어 .pt, .zip, .so 파일을 LFS로 추적하려면:\ngit lfs track \"*.pt\"\ngit lfs track \"*.zip\"\ngit lfs track \"*.so\"\n→ .gitattributes 파일이 생성되며 추적 규칙이 저장됩니다."
  },
  {
    "objectID": "posts/note/2025-07-08-git-lfs.html#커밋-푸시하기",
    "href": "posts/note/2025-07-08-git-lfs.html#커밋-푸시하기",
    "title": "📝Git LFS 사용하기",
    "section": "📥 커밋 & 푸시하기",
    "text": "📥 커밋 & 푸시하기\n\n대용량 파일을 Git에서 제거하고, LFS로 다시 추가:\n\ngit rm --cached path/to/large_file.pt\ngit add .gitattributes\ngit add path/to/large_file.pt\ngit commit -m \"Move large file to Git LFS\"\ngit push origin main\n\n⚠️ .gitattributes 파일도 꼭 커밋해야 추적이 유지됩니다.\n\n\n\n\n\n\n\n💡 이미 푸시 에러가 난 경우?\n\n\n\nerror: GH001: Large files detected. You may want to try Git Large File Storage.\n이런 메시지가 떴다면, 해당 파일을 위 과정처럼 LFS로 옮기고 다시 커밋하면 해결됩니다."
  },
  {
    "objectID": "posts/note/2025-07-08-git-lfs.html#클론-시-lfs-파일-받기",
    "href": "posts/note/2025-07-08-git-lfs.html#클론-시-lfs-파일-받기",
    "title": "📝Git LFS 사용하기",
    "section": "📦 클론 시 LFS 파일 받기",
    "text": "📦 클론 시 LFS 파일 받기\n다른 사람이 저장소를 복제할 때도 Git LFS가 필요합니다.\n\n1. Git LFS 설치\ngit lfs install\n\n\n2. 저장소 클론\ngit clone https://your-git-platform.com/user/repo.git\ncd repo\ngit lfs pull\n\n보통 git clone 시 자동으로 LFS 파일도 받아오지만, 안 될 경우 수동으로 git lfs pull 실행\n\n\n\n\n\n\n\nOptional: GPG 관련 오류가 나올 경우\n\n\n\nfatal: bad config variable 'gpg.format'\n→ ~/.gitconfig 열어서 아래 항목을 제거하거나 수정:\n[gpg]\n    format = ssh   # 또는 아예 제거"
  },
  {
    "objectID": "posts/note/2025-07-08-git-lfs.html#참고-lfs-저장소-용량-주의",
    "href": "posts/note/2025-07-08-git-lfs.html#참고-lfs-저장소-용량-주의",
    "title": "📝Git LFS 사용하기",
    "section": "(참고) LFS 저장소 용량 주의",
    "text": "(참고) LFS 저장소 용량 주의\nGit 플랫폼에 따라 LFS 저장소에 용량 제한이 있습니다:\n\n\n\n플랫폼\n기본 제공 용량 (무료)\n초과 시 정책\n\n\n\n\nGitHub\n1GB 저장 / 1GB 트래픽\n유료 플랜 필요\n\n\nGitLab\n10GB 저장소 내 포함\n초과 시 프로젝트 제한\n\n\nBitbucket\n1GB 저장 / 1GB 트래픽\n유료 플랜 필요"
  },
  {
    "objectID": "posts/note/2025-06-23-ros-errors.html",
    "href": "posts/note/2025-06-23-ros-errors.html",
    "title": "📝ROS1/2 Errors",
    "section": "",
    "text": "gzclient: /usr/include/boost/smart_ptr/shared_ptr.hpp:728: typename boost::detail::sp_member_access&lt;T&gt;::type boost::shared_ptr&lt;T&gt;::operator-&gt;() const [with T = gazebo::rendering::Camera; typename boost::detail::sp_member_access&lt;T&gt;::type = gazebo::rendering::Camera*]: Assertion `px != 0' failed.\n[ERROR] [gzclient-2]: process has died [pid 2979, exit code -6, cmd 'gzclient --gui-client-plugin=libgazebo_ros_eol_gui.so'].\n\n\n\n$ source /usr/share/gazebo/setup.sh\n\nGazebo 환경변수(source /usr/share/gazebo/setup.sh)가 제대로 설정되지 않아 GAZEBO_PLUGIN_PATH·GAZEBO_MODEL_PATH 등에서 플러그인·모델 로드에 실패해 shared_ptr이 null이 되는 경우가 많음\n또한 그래픽 드라이버/OpenGL 컨텍스트 문제로 객체 초기화가 실패하거나, ROS–Gazebo 버전 불일치로 플러그인이 정상 로드되지 않아 내부 객체가 null로 반환되는 경우가 주요 원인\n\n또 같은 에러가 발생하면 gazebo 필요 모델이나 플러그인 로드되는지 확인\necho $GAZEBO_PLUGIN_PATH\necho $GAZEBO_MODEL_PATH"
  },
  {
    "objectID": "posts/note/2025-06-23-ros-errors.html#error",
    "href": "posts/note/2025-06-23-ros-errors.html#error",
    "title": "📝ROS1/2 Errors",
    "section": "",
    "text": "gzclient: /usr/include/boost/smart_ptr/shared_ptr.hpp:728: typename boost::detail::sp_member_access&lt;T&gt;::type boost::shared_ptr&lt;T&gt;::operator-&gt;() const [with T = gazebo::rendering::Camera; typename boost::detail::sp_member_access&lt;T&gt;::type = gazebo::rendering::Camera*]: Assertion `px != 0' failed.\n[ERROR] [gzclient-2]: process has died [pid 2979, exit code -6, cmd 'gzclient --gui-client-plugin=libgazebo_ros_eol_gui.so']."
  },
  {
    "objectID": "posts/note/2025-06-23-ros-errors.html#solution",
    "href": "posts/note/2025-06-23-ros-errors.html#solution",
    "title": "📝ROS1/2 Errors",
    "section": "",
    "text": "$ source /usr/share/gazebo/setup.sh\n\nGazebo 환경변수(source /usr/share/gazebo/setup.sh)가 제대로 설정되지 않아 GAZEBO_PLUGIN_PATH·GAZEBO_MODEL_PATH 등에서 플러그인·모델 로드에 실패해 shared_ptr이 null이 되는 경우가 많음\n또한 그래픽 드라이버/OpenGL 컨텍스트 문제로 객체 초기화가 실패하거나, ROS–Gazebo 버전 불일치로 플러그인이 정상 로드되지 않아 내부 객체가 null로 반환되는 경우가 주요 원인\n\n또 같은 에러가 발생하면 gazebo 필요 모델이나 플러그인 로드되는지 확인\necho $GAZEBO_PLUGIN_PATH\necho $GAZEBO_MODEL_PATH"
  },
  {
    "objectID": "posts/note/2025-06-23-ros-errors.html#error-1",
    "href": "posts/note/2025-06-23-ros-errors.html#error-1",
    "title": "📝ROS1/2 Errors",
    "section": "error",
    "text": "error\n패키지 빌드시 마주친 에러 로그\nCould not find a package configuration file provided by \"moveit_servo\" with\n  any of the following names:\n\n    moveit_servoConfig.cmake\n    moveit_servo-config.cmake\n\n  Add the installation prefix of \"moveit_servo\" to CMAKE_PREFIX_PATH or set\n  \"moveit_servo_DIR\" to a directory containing one of the above files.  If\n  \"moveit_servo\" provides a separate development package or SDK, be sure it\n  has been installed.\nROS 2 패키지를 APT로 올바르게 가져오지 못해 moveit_servo 설치 시 404 오류가 발생하고, 동시에 GPG 키 만료로 인해 apt update 단계부터 경고가 뜨는 상황:\n\nGPG 키 만료로 인해 ROS 2 저장소가 신뢰되지 않아 패키지 색인이 최신으로 갱신되지 않습니다.\n오래된 색인을 기반으로 설치를 시도하므로 서버에 없는 .deb 파일을 내려받으려 해 404가 납니다.\n해결을 위해서는 ① 새로운 GPG 키를 가져와, ② APT 소스에 signed-by 옵션을 넣어 제대로 설정한 뒤, ③ 캐시를 정리하고 갱신, ④ 다시 패키지를 설치하는 순으로 진행해야 합니다."
  },
  {
    "objectID": "posts/note/2025-06-23-ros-errors.html#solution-1",
    "href": "posts/note/2025-06-23-ros-errors.html#solution-1",
    "title": "📝ROS1/2 Errors",
    "section": "solution",
    "text": "solution\n\nROS 2 GPG 키 갱신\n\nROS 2용 새 키 다운로드\nsudo apt install curl gnupg2 lsb-release        # 키링 관리용 도구 설치\nsudo curl -sSL https://raw.githubusercontent.com/ros/rosdistro/master/ros.key \\\n  -o /usr/share/keyrings/ros-archive-keyring.gpg\n\n만료된 키를 대체하며, 이후 APT가 신뢰할 저장소 키로 사용\n\n기존 ROS 2 리스트 제거\nsudo rm /etc/apt/sources.list.d/ros2*.list\n\n예전 설정이 남아 있으면 signed-by 옵션이 적용되지 않음\n\n\nAPT 소스 설정\n\nOfficial 문서 권장 방식에 따라 저장소를 설정:\n\nsudo apt update\nsudo apt install software-properties-common\nsudo add-apt-repository universe\n\necho \\\n  \"deb [arch=$(dpkg --print-architecture) \\\n        signed-by=/usr/share/keyrings/ros-archive-keyring.gpg] \\\n   http://packages.ros.org/ros2/ubuntu \\\n   $(. /etc/os-release && echo $UBUNTU_CODENAME) main\" \\\n  | sudo tee /etc/apt/sources.list.d/ros2.list &gt; /dev/null\n\nsigned-by 옵션으로 키링 파일을 지정해 보안 경고를 방지\n\n캐시 정리 및 최신 색인 가져오기\n\n만약 키 설정 이후에도 색인이 갱신되지 않는다면, APT 리스트 캐시를 지운 뒤 다시 시도:\nsudo rm -rf /var/lib/apt/lists/*\nsudo apt update\n이렇게 하면 저장소에서 최신 패키지 목록을 내려받음\n\nmoveit_servo 설치\n\n이제 설치가 가능:\nsudo apt install ros-humble-moveit-servo\n\n패키지명은 ros-humble-moveit-servo이고, 설치 후 /opt/ros/humble/share/moveit_servo에 관련 파일이 생김"
  },
  {
    "objectID": "posts/note/2023-03-31-daily-english-007.html",
    "href": "posts/note/2023-03-31-daily-english-007.html",
    "title": "🌎Casual English Phrases 007",
    "section": "",
    "text": "독차지하다\nhog\n\n명사로는 야생 돼지, 큰 돼지 라는 뜻\n\n\nThey’ll hog the court all day! 그들이 하루 종일 테니스장 코트를 독차지하고 있을거야!\nDon’t hog the toys! 장난감을 독차지 하지마!\nQuit hogging all my time! 내 시간을 독차지하려 하지마!\n\n\n\n뭘 망설이고 있어?(용기를 북돋는)\nWhat are you waiting for?\n\n진짜 노래가사에서 많이 들을 수 있음\n\n\n\n떠나다\nget off\n\nYou big kids get off that court right now! 너네 코트에서 당장 나와!\nI need to get off this island. 난 이 섬에서 떠나야 한다\nGet off the bridge! 그 다리에서 떨어져!\n\n\n\n어떻게 생각해?\nWhat do you say?\n\nWhat do you say to going to the movies tomorrow? 내일 영화 보러 가는 것에 대해서 어떻게 생각해?\n\n\n어떤 제안을 하고 나서 그 제안에 대한 상대방의 견해를 물어볼 때 사용 / 일상이나 업무 둘다 사용\n\n\n\n지루하게 자세한 걸 다 읽지는 않겠습니다.\nI won’t bore you by reading all of the details.\n\nI won’t bore you by telling all of the stories. I’ll make a point. 모든 이야기를 다해서 지루하게 하진 않을게요. 요점을 말하겠습니다.\n\n\n\n나 ~ 못하는 거 알잖아.\nYou know I have trouble (with) ~\n\nYou know I have trouble with the can opener! 나 캔 따개 못 따는 거 알잖아!\nYou know I have trouble speaking out loud in front of a big crowd. 제가 많은 관중들 앞에서 이야기 못하는 거 알잖아요."
  },
  {
    "objectID": "posts/note/2023-04-05-daily-english-008.html",
    "href": "posts/note/2023-04-05-daily-english-008.html",
    "title": "🌎Casual English Phrases 008",
    "section": "",
    "text": "내가 그럴 줄 알았어!\nI knew it!\n\n어쩐지..이상하다 그랬어. 라는 뉘앙스\n\n\nI knew I had the wrong code. (어쩐지..) 내가 잘못된 코드를 가지고 있었던 걸 알고있었어.\nI knew it! It was the wrong size! 역시! 사이즈가 잘못된 거였어!\n\n\n\n생각해보니\nUpon reflection\n\nUpon reflection, I think Charlie Brown was wrong. 생각해보니 찰리가 틀린 것 같아.\nUpon reflection, I think we have to change the main color of this banner. 다시 생각해보니 내 생각에 이 배너의 메인 색을 바꿔야 할 것 같아.\nUpon reflection, I prefer this option. 다시 생각해보니 이 옵션이 좋은 것 같아요.\n\n\n\n배짱이 좋다./뻔뻔하다.\nhave(got) the(a/some/a lot) nerve\n\nnerve: 용기, 대담성, 뻔뻔스러움\n\n\nhave the nerve + to 동사원형\n\n\nHe had the nerve to say that I’m not perfect! 내가 완벽하지 않다고 하다니 배짱도 좋지!\nWow, you have the nerve to criticize my report. 와, 당신이 내 리포트를 비평하다니 배짱이 있으시네요.\nYou really have a nerve, arriving an hour late for the event. 넌 그 행사에 1시간이나 늦게 도착하다니 참 뻔뻔하다.\n\n\n\n~을 정리하다, 정돈하다\nstraighten up\n\nInstead of watching TV, you could be straighten up your room. TV 보는 대신에 방 청소를 할 수도 있지.\nI’m going to straighten up my room. 내 방 정리정돈 할거야.\nStraigthen up your back! 너 허리 똑바로 펴!\n\n\n\n진심이야?\nDo you mean it?\n\nYes, I mean it. 응 진심이야.\n\n\n\n넌 절대 마음 바꿀 생각 없나 보네?\nI suppose you’re not going to change your mind.\n\n\n평소와 달리/기분 전환으로\nfor a change\n\nYou look pretty clean today for a change. 오늘은 평소와 달리 깨끗해 보여.\nWe always go to the movies. Let’s go shopping for a change! 우리 맨날 영화만 보러가니까 오늘은 평소와 다르게 쇼핑가자!\nI think I’m going to do an intense workout for a change. 나 오랜만에 강도 높은 운동을 해야 할 것 같아.\nI want to go my hometown and visit my parents for a change. 기분 전환하게 오랜만에 고향에 가서 부모님을 보고 싶어."
  },
  {
    "objectID": "posts/note/2022-10-29-geultto-7th-end.html",
    "href": "posts/note/2022-10-29-geultto-7th-end.html",
    "title": "📘Gueltto 7th End",
    "section": "",
    "text": "이번 post는 지난 2022년 5월 15일부터 10월 16일까지 활동했던 글쓰는 또라이 7기 활동을 마치고 회고한 글입니다. 처음 시작을 다짐으로 시작하여 9월 14일에 중간 점검글도 잠깐 쓰고 이제 마지막 이글로 회고를 하면서 마무리 지어보려고 합니다."
  },
  {
    "objectID": "posts/note/2022-10-29-geultto-7th-end.html#the-bottom-line",
    "href": "posts/note/2022-10-29-geultto-7th-end.html#the-bottom-line",
    "title": "📘Gueltto 7th End",
    "section": "The bottom line …",
    "text": "The bottom line …\n글또 활동 보증금으로 10만원을 내고 시작했고 모든 권장 활동을 충실히 지킨 결과, 굿즈 구입비 1만원도 커피챗 2회 보상(5000원x2)으로 보충되어 그대로 10만원을 받을 수 있었습니다."
  },
  {
    "objectID": "posts/note/2022-10-29-geultto-7th-end.html#본질",
    "href": "posts/note/2022-10-29-geultto-7th-end.html#본질",
    "title": "📘Gueltto 7th End",
    "section": "본질",
    "text": "본질\n처음에 글또에 참여하겠다고 설문지를 작성하던 그때를 떠올리고 오리엔테이션을 마치고 처음 다짐글을 다시 읽으면서 되새겨보면, 제게 가장 강력했던 동기는 글쓰기 힘 기르기였습니다. 글은 장르와 목적에 따라 어떻게 작성하는게 명확하고 전달력이 좋은지 차이가 크고 콘텐츠를 보는 사람의 즉각적인 피드백을 얻기 힘들기 때문에 머릿속을 빙빙도는 스토리들을 잘 만드는 사람이 되고 싶었습니다. 처음 다짐글에 감상적인 글보다는 IT, 개발관련 글을 쓰고 싶었다고 생각한 이유는 생각이나 감정을 배제하고 싶다는 생각보다는 일기와 같은 글에서는 내 이야기 중심이다보니 나 이외의 독자를 배려하거나 고려할 수 있는 힘을 기를 수 없기 때문이었습니다. 같은 맥락으로 사실 블로그에 내가 공부한 것들을 정리한 정리노트 서랍장이 되지 않기를 바랐던 것도 있었습니다. 나 이외의 독자(들)도 이해를 하고 도움을 받을 수 있을까? 나는 내 글로 다른 사람들이 이 글을 읽을 가치가 있다고 설득할 수 있을까? 고민들을 했었습니다. 물론 지금도 이 질문들에 완벽한 답변이 될 만한 포스트들을 작성하고 있지 못하지만 글또에서 조금은 나아졌다고 생각합니다. 단순히 느낌만 그런것이 아니라, 같은 팀내에서 피드백들을 받고 저보다 글을 더 논리정연하게 쓰고 설득할 줄 아는 분들의 어깨너머로 확실이 배울 수 있었습니다.\n활동 보증금이 있어서 정말 완벽한 의지로 글을 쓰는 습관을 들인 것은 아니지만 그래도 활동 보증금을 다 받을 수 있을 정도 만큼은 성장했다는 것에 만족합니다. 일단 1차 목표였고 한 단계를 밟았기 때문입니다. 사실 이 작은 성장을 시작으로 본질 밑바닥에 깔려있던 흑심은 블로그 포스팅으로 서술하는 능력을 길러서 좋은 논문을 내는 연구자가 되고 싶었던 마음이 있었습니다. 연구가 아직 내 적성에 맞는지는 확신이 서지 않지만 일단 석사과정을 시작한 한 미약한 연구자로써 글을 잘쓰고 싶었기 때문입니다. 큰 학술지는 아니었지만 학부과정에서 2번정도 논문들을 쓸 기회가 있었는데 솔직히 CV에는 한줄이라도 쓰기 위해 논문들을 적어놓지만 남보여주기에는 부끄러웠습니다. 저 이외의 연구자들을 전혀 고려하지 않은 서술들이었기 때문이었습니다. 또한 여러 논문들을 읽다보면 어떤 논문은 정말 감탄이 나올정도로 설계도 깔끔하고 서술도 완벽에 가까운데 반해 어떤 논문은 뭘 말하고자 하는지 왜 이렇게 써야했는지 이해하기 어려운데 적어도 저의 석사논문 만큼은 같은 동료 연구자들에게 고개를 끄덕일 수 있도록 퀄리티가 좋았으면..하는 희망이 있습니다. 이 마음 저 생각이 모여 글또로 시작을 열 생각을 하게된 것 같고 그 본질과 흑심을 표면 밖으로 끄집에 내게 해준 글또 활동들이 소중했습니다.\n본질과는 조금 동떨어지지만 글쓰는 작은 변화가 근래에 있었습니다. 영어공부도 할겸 영어 실력을 향상시키기 위해 daily english 시리즈로 글을 되도록 자주 올리도록 노력하게 되었습니다. 이야말로 위에서 말했던 정리노트 서랍장을 만들 수도 있을 위험이 크지만 그래도 영어표현이나 팁들을 다른 사람들도 보기 쉽게 소비하고 도움을 받을 수 있도록 정리하려고 합니다. 실제로 제가 다른분들의 짧은 영어 포스팅들로 도움을 많이 받기 때문에 이와 같은 일환으로 시작하게 되었습니다."
  },
  {
    "objectID": "posts/note/2022-10-29-geultto-7th-end.html#처음-다짐",
    "href": "posts/note/2022-10-29-geultto-7th-end.html#처음-다짐",
    "title": "📘Gueltto 7th End",
    "section": "처음 다짐",
    "text": "처음 다짐\n\n글의 주제\n\n달성정도: 2/5\n처음 다짐에 로봇 제어(이론이나 시뮬레이션 툴 등), 로봇 연구에서의 강화학습, Julia 언어로 하는 Robotics로 글을 쓰고 싶다고 적었었습니다. 하지만 거의 논문 리뷰 위주로 글을 쓰다보니 해당 주제들에서 많이 벗어난 글들도 있었고 로봇 연구에서의 강화학습에 해당하는 글들만 조금 쓸 수 있었던 것 같습니다.\n\n활동 목표\n\n달성정도: 5/5\n활동 목표로 처음 활동 보증금을 모두 회수할 수 있는 미니멈으로 목표를 정했었습니다. 그리고 중간중간 위험하게 마감을 못지킬 것 같은 순간들도 있었지만 결과적으로 100% 회수할 수 있는 활동기록으로 마무리했습니다. 사실 이게 가장 중요하게 생각한 목표였기 때문에 정말 뿌듯했습니다.\n\n생활 목표\n\n달성정도: 2/5\n알고리즘 문제 풀기와 운동 꾸준히 하기를 생활목표로 정했었는데 우선 알고리즘 문제 풀기는 중간에 연구장학생 준비를 위해 조금 했기 때문에 1점 + 운동은 꾸준히는 아니지만 간헐적으로 시간있을 때 챙길려고 노력하기 때문에 1점 해서 총 2점입니다."
  },
  {
    "objectID": "posts/note/2022-10-29-geultto-7th-end.html#중간-점검",
    "href": "posts/note/2022-10-29-geultto-7th-end.html#중간-점검",
    "title": "📘Gueltto 7th End",
    "section": "중간 점검",
    "text": "중간 점검\n\n매일 커밋하기\n\n달성정도: 3/5\n커뮤니티에서 1001일 매일 커밋하신 분을 보고 모티베이션을 얻어서 정한 목표였는데 역시 매일 커밋은 정말 어려운 일이었습니다. 솔직히 하루 무너지니 다짐이 도미노처럼 우르르 무너져 3-4일간 하기 싫은 적도 있었는데 글또에서 피드백 주셨던 것처럼 의미없는 커밋에 마음 뺏기지 말고 의미있는 커밋에 좀 더 중점을 두고 꾸준히 노력해봐야겠습니다. 본질을 생각해야겠죠.\n\n코드가 들어가는 포스팅 작성하기\n\n달성정도: 0/5\n지금까지 작성해보지 않았던 코드와 함께하는 포스팅을 도전하겠다 해놓고 아직까지도 도전 준비중 상태입니다. 코드를 글에 적기 위해서는 실습을 해보는 시간 + 적절한 편집 실력도 필요해서 더 망설여지는 것 같습니다. 그렇지만 아직 하지 않았을 뿐이지 하지 않을 것은 아니기 때문에 조만간 학교 수업에서 듣는 코드들도 조금씩 작성해보려고 합니다.\n\n멘탈관리를 위해 딴짓하기\n\n달성정도: 3/5\n조만간 특별한 취미생활이라며 중간 다짐글에 적어놓지 않았었는데 그 취미는 클라리넷이었습니다. 사실 어렸을 때부터 악기를 매우매우 좋아해서 이것저것 많이 했었는데 그 중 고1때까지 했던 클라리넷 상자가 어느날 제 눈을 사로잡았습니다. 입시만 끝나면 다시 끄낼 줄 알았던 그 상자가 어느새 대학 졸업하고 대학원에 들어왔는데도 열리지 않았는데 멘탈을 위해, 딴짓을 위해 이것저것 생각하다보니 떠올라서 다시 열게되었습니다. 다행히도 학교 근처에서 렛슨을 하는데가 있어서 다시 시작할 수 있었고 2011.00.00 렛슨 날짜가 적혀있는 교본을 다시 꺼내서 2022.00.00날짜를 적으니 감회가 새로웠습니다. 이에 더해 몸이 기억하고 있는 운지법은 정말 신기했던 것 같습니다.\n\n\n마지막 한 줄로 글또 7기를 정리하자면,\n\n좋은 시도, 경험, 마무리였다."
  },
  {
    "objectID": "posts/note/2022-10-17-daily-english-003.html",
    "href": "posts/note/2022-10-17-daily-english-003.html",
    "title": "🌎Casual English Phrases 003",
    "section": "",
    "text": "영어 회화 표현이나 논문에서 쓸 수 있는 유용한 표현들을 조금씩 익히기 위해 시작한 1000일 시리즈 포스트 입니다. 공부 리소스는 유튜브, 리얼클래스, PN님의 블로그, cake 어플, 책, 논문, 기사 등 다양한 출처에서 보고 기록합니다.\n\n\n속마음을 보여주다\nwear your heart on your sleeve\n\nIf you wear your heart on your sleeve, you openly show your feelings or emotions rather than keeping them hidden. 속마음을 보여준다는 말은, 너의 감정이나 느낌들을 숨기기보다 보여준다는 거야.\nIt takes courage to wear your heart on your sleeve, which is why few people do it. 속마음을 털어놓는다는 건 용기가 필요한 일이기 때문에 많은 사람들이 하지 못하는 일이지.\nI need someone whom I can wear my heart on my sleeve. 내 속마음을 털어놓을 사람이 필요해.\n\n\n\n기억력이 좋은\na memory like an elephant\n\nShe has a memory like an elephant. 그녀는 기억력이 굉장히 좋아\nYou can remember his name even if you don’t have a memory like an elephant. 니가 기억력이 엄청 좋지 않더라도 그의 이름은 기억할껄?\nThe menual is so simple. Don’t worrry that you don’t have a memory like an elephant. 메뉴얼이 엄청 간단해. 기억력이 좋지 않다고 걱정할 필요 없어.\n\n\n\n입장바꿔 생각해봐\nput yourself in one’s shoes\n\nPlease put yourself in his shoes before blurting it out. 그냥 말을 하기전에 그의 입장도 생각해봐\n\nblurt out : 말을 내뱉다\n\nIt’s hard to put myself in other people’s shoes from the my heart. 진심으로 다른 사람들의 입장이 되어보는 것은 어려워\n\nfrom the (bottom of one’s) heart : 진심으로\n\nWould you mind putting yourself in my shoes? 제 입장에서 한번만 생각해주실 수 있으실까요?\n\n참고 기억력 상징이 코끼리가 된 이유"
  },
  {
    "objectID": "posts/note/2021-01-03-Hello-2021.html",
    "href": "posts/note/2021-01-03-Hello-2021.html",
    "title": "📘Hello 2021",
    "section": "",
    "text": "2021에 이루고 싶은 일들, 하고 싶은 일들, 바라는 일들, 2021 회고록을 쓰면서 “그땐 이런 생각을 하며 살려고 노력했었구나” 할 만한 이야기를 적어보자."
  },
  {
    "objectID": "posts/note/2021-01-03-Hello-2021.html#해야하는하고싶은-공부",
    "href": "posts/note/2021-01-03-Hello-2021.html#해야하는하고싶은-공부",
    "title": "📘Hello 2021",
    "section": "해야하는/하고싶은 공부",
    "text": "해야하는/하고싶은 공부\n구체적인 실천 목표들은 다이어리에 적고 여기에는 크게 대략적인 목표지점들을 적어보려고 한다.\n\n영어\n이젠 피하고 싶다는 생각보다는 해내야만 한다는 생각이 더 크다. 다른 공부들도 그렇지만 영어야 말로 이제는 output을 내야하기 때문에 간절함이 더 커졌다.\n\n영어루틴\n토플 90점 이상\n외국인 친구 만들기\n\n\n\nRL\n아직도 잘 모르겠지만 아직도 내 호기심을 자극시키는 분야라 계속 공부하고 싶다.\n\n스터디(기초이론과 논문읽기)\n구현능력 올리기\n연구에 활용하는 능력 기르기\n\n\n\nGNN\n\n기초 이론 입문하기\n\n\n\nQC\n\n기초 이론 입문하기(feat. 모두연)\nQiskit Challenge 도전하기"
  },
  {
    "objectID": "posts/note/2021-01-03-Hello-2021.html#하고싶은-취미",
    "href": "posts/note/2021-01-03-Hello-2021.html#하고싶은-취미",
    "title": "📘Hello 2021",
    "section": "하고싶은 취미",
    "text": "하고싶은 취미\n2020에 회고하면서 한 가지 느꼈던 게 있었다. 취미라 할 만한 활동을 하질 않았었다. 그래서 더 쉽게 지쳤고 멀리 나가지 못했던 것 같아 2021에는 취미도 생각하면서 살고 싶어서 한번 적어보았다.\n\n피아노\n부모님의 반대를 무릅쓰고 산 전자 피아노가 하나 있는데 많이 쳐보지를 못했다. 원래 쳤던 곡들도 좋고 재즈 피아노나 마피아에서 산 몇곡 악보를 제대로 연습해보고 싶다. 예전에는 피아노한테 힐링도 많이 받았었는데..왜 이렇게 멀어졌을까🤔\n\n박터틀의 재즈 피아노\nTido Kang의 노래\n피아노 치는 이정환님의 노래 이건 불가능하겠지만🤣\n\n\n\n책 읽기\n유튜브 보는 시간을 좀 줄여보고 한 달에 딱 1권은 인문이나 사회 분야 등과 같이 내가 쉽게 접할 수 없는 분야의 책들을 읽어보려 한다. 기술서 같은 경우에는 필요에 의해 손이 많이 가지만 이외의 책들은 정말 안 읽게 되는 것 같다. 간략히 남기고 싶은 문구나 독후감은 다이어리에 남겨보자."
  },
  {
    "objectID": "posts/note/2021-01-03-Hello-2021.html#만들고-싶은-습관",
    "href": "posts/note/2021-01-03-Hello-2021.html#만들고-싶은-습관",
    "title": "📘Hello 2021",
    "section": "만들고 싶은 습관",
    "text": "만들고 싶은 습관\n\n운동\n체력과 건강을 무시 못할 시기가 온 것 같다. 지금은 코로나로 인해 헬스장이 언제 열릴지는 모르겠으나 일단은 기본적인 걷기나 자전거, 간단한 홈트 동작들을 꾸준히 아침 루틴으로 하는 것이 목표다.\n\n\n블로깅\n지금 쓰고 있는 github 블로그에 꾸준히 기술관련 포스팅을 하는 것이 목표다. 아직은 개발자도 아니고 유의미한 output 포스팅을 하기엔 실력이 없지만, 내가 그때 그때 마다 알게 된 것, 이후의 나에게 도움이 될 만한 내용이라고 판단되면 적어보려고 한다. 나부터 나를 가르쳐보면서 어떤 글이 잘 쓰여진 글인지 스스로 체크해보며 블로깅하는 실력을 길러보는 한 해가 되었으면 좋겠다.\n\n\n다이어리\n블로깅이 주로 기술관련(특히 it 분야겠지만)이라면 다이어리는 내 내면과 생각을 정리해보고 싶어서 만들고 싶은 습관이다. 생각보다 하루하루 내가 느낀 것들은 많은데 막상 돌이켜 보면 잘 기억이 나지 않고 내면의 단단함이 쉽게 물러지는 느낌이 든다. 그냥 생각없이 산 것 같기도 하고 내 삶을 소중히 생각하지 않은 것 같은 느낌이 들 때. 순간의 나도 이런 생각을 하고 고민을 하며 살아왔다는 흔적을 남기는 연습을 하고 싶다. 예전에 삼수를 할 때 비교적 생활 패턴이 규칙적이고 고요할 때(?)는 다이어리에 기록하는게 어렵지 않았는데 지금은 다이어리 루틴을 만든다는 게 쉽지 않은 것 같다. 2021은 남겨보려고 노력해보고 싶다."
  },
  {
    "objectID": "posts/note/2021-01-03-Hello-2021.html#신년다짐과-생각",
    "href": "posts/note/2021-01-03-Hello-2021.html#신년다짐과-생각",
    "title": "📘Hello 2021",
    "section": "신년다짐과 생각",
    "text": "신년다짐과 생각\n송구영신 예배와 신년 축복예배에서 받은 말씀은 기대의 차원을 높이자와 대장부가 되자였다. 남에게 기대하기 보다 나에게 기대하는 삶을, 나보단 하나님께 근거있는 삶을 바라며 나에게 주어진 새로운 시간들을 잘보냈으면 좋겠다. 그리고 지금까지 살면서 느껴왔던 것처럼 삶을 살아가면서 내가 생각지 못한 경험들의 스펙트럼은 점점 넓어진다는 걸 알았다. 좋은 쪽으로만 범위가 넓어지는 것이 아니기에. 내가 감당치 못할 어려움을 주시는 분이 아니시기에 당장 눈 앞에 커보이는 두려움을 담대하게 맞설 수 있다는 믿음을 더 확고히 할 수 있는 한 해가 될 수 있도록.\n어쩌다보니 대학생활도 마지막에 접어들었다. 생각없이 보내기엔 다시 돌아오지 않을 시간들이기에 좀 더 정신차리고 최선을 다해보고 싶다. 내 것이 되기전에는 한 없이 미웠었지만 지금은 그 어떤 대학보다 나의 가능성을 확인시켜 준 내 학교에서 마지막 생활은 어떨까. 안녕 2021"
  },
  {
    "objectID": "posts/note/2024-10-09-geultto-10th-start.html",
    "href": "posts/note/2024-10-09-geultto-10th-start.html",
    "title": "📘Geultto 10th Start",
    "section": "",
    "text": "나야, 글또\n한창 흑백요리사가 유행하는 요즘. 많은 밈들이 만들어지고 있습니다. 그 중 하나 “나야, 들기름”을 조금 응용하여, 글또가 나에게 어떤 존재인지 표현해보면 “나야, 글또”로 표현해볼 수 있지 않을까 싶습니다. 글또는 대학원 석사 2학기 무렵 글을 쓰며 성장하고 싶다라는 생각으로 7기에 시작하여 지금 10기까지 참여하고 있는데, 어느덧 졸업과 취업이라는 2개의 산을 넘는 동안 참여 해온 커뮤니티라는 사실이, 은은한 들기름 향처럼 제 생활에 성장의 고소함을 주었다는 것이, 이렇게 표현이 되지 않나 싶습니다. 대학원생 신분일 동안에는 어디에 표현되지도 않고 드러나지도 않을 방구석 연구자인 것처럼 내 자신이 초라하게 느껴질 때 글또에서 글을 쓰며 힘을 얻을 수 있었습니다. 그리고 비교적 짧은 시간이었지만 그 어느 때보다 불안정함을 느꼈던 취준 시기에는 글또에서 만난 분들에게 직간접적으로 영감을 받으며 버틸 수 있었습니다. 이제는 아직 1년도 안된 사회 초년생으로, 글또에서 이렇게 많은 선배/동료 개발자들을 보고 배울 수 있기에 얼마나 감사한지 모릅니다.\n사실 매번 글또를 마치는 시기마다 “다음 기수엔 신청이 어렵겠다. 이제 혼자서 글을 꾸준히 써보자.”라고 생각하며 “아마도 다음에 신청을 하지 않을 것 같다.” 생각하게 됩니다. 하지만 또 다시 새로운 기수가 시작되는 시기가 오고 신청을 받는 기간이 돌아오면 다시 신청서를 작성하고 “글또 없이는 블로그 글이 전혀 안써지네.”라고 생각하며 시작하게 됩니다. “나야, 글또”라고 말하며 다시 다가오는 글또 활동 시기에 감사하고, 또 한번 저에게 글 쓰는 힘을 기를 수 있는 커뮤니티의 힘을 빌릴 수 있다는 것이 안도가 되는 것 같습니다.\n\n\n이번에는\n스스로도 아이러니 한 것은, 사실 꾸준히 글을 쓰며 성장하고 싶은 마음이 충분히 있음에도 동시에 글을 쓰면서 느끼는 저항감과 피로감이 있기에 글또 활동이 마냥 편하지 않습니다. 그렇기에 마지막 글 제출까지 하고 나면 다음 기수 글또를 기약하기 어렵겠다는 생각과 함께 다시 글을 안 쓰던 안일한 마음으로 되돌아가려고 하는 것 같습니다.\n이번에는 글또가 10기로 마지막으로 운영되는 기수입니다. 마지막으로 커뮤니티의 힘을 빌려 글을 쓰며 성장하는 기회이기도 하기에, 본질로 돌아가 10기에서의 목표로 단 하나만 지켜보려고 합니다.\n매주 글쓰기\n2주 간격 제출이 원칙적인 정기 제출 주기이지만 건너뛰게 되는 한 주에도 스스로 글 쓰는 마감일을 지정해서 작성하는 목표를 잡았습니다. 7기 때부터 스스로 글 쓰는 습관을 만들고 싶었는데 매번 정기 제출 마감일에 급급하며 썼었기에 제대로 습관을 기르지 못했다는 판단 하에 이번에는 가장 단순하고도 본질적인 목표 하나만 잡았습니다. 마지막 글또에서 가장 단단한 습관을 만들어 놔야 이후에 지속적인 성장이 가능하지 않을까 라고 생각하기 때문에 도전해보려고 합니다.\n이전에는 학생/연구자 신분이었기에 ai 연구 코어 채널에서 활동을 했었지만, 이번에는 첫 직장에서 AI/Robotics 엔지니어로써 일을 시작하게 되어 ml-ai-엔지니어 코어 채널에서 활동하게 되었습니다. 비슷하지만 다른 분위기에서 영감을 받을 수 있을 것 같아 많이 기대가 됩니다.\n\n\n실험을 받아들이는 자세\n마지막으로 다시 흑백요리사 이야기로 돌아가서, 경연에서 가장 인상이 깊었던 분은 에드워드 리 셰프님이었습니다. 다 쟁쟁한 요리사들이 나왔고 존경스러운 점들이 많았지만 , 에드워드 리 셰프의 경력과 연륜에서 유지하기 힘든 도전에 대한 갈망과 그런 지향점을 유지하게 해주는 삶의 태도에서 “나도 저렇게 살아가고 싶다.”라는 생각이 들었던 것 같습니다.\n\n\n\n에드워드 리의 인터뷰 중 1\n\n\n사실 2024년도에 많은 변화가 일어나면서 내가 좋아하는 게 뭔지 조차 헷갈리는 시기가 있었습니다. 나도 나를 모르겠는 혼란스러운 순간들 가운데 처음 찐 사회생활을 하다 보니 흔들림의 진폭이 더 커져 괴로웠었고 “내 선택이 맞는 걸까”라는 고민에 정체되어 있던 시간이었습니다. 특히나 내가 원하지 않았던 것들을 하고 있다고 생각이 들 때가 가장 괴롭고 시간이 허비되고 있다는 생각과 이러다가 내 커리어가 꼬이는 거 아닐까라는 불안함으로 앞으로 나아가지 못하고 있었습니다. 하지만 내가 원하는 것에만 너무 초점을 맞춰서 생각하다보니 원하지 않는 것들이 도움이 되지 않을 것이다 라는 얕은 생각으로 괴로워하지 말고, 셰프님의 인터뷰에서 처럼 내가 원하지 않는 것에 대해 알아가는 시간 또한 귀하다고 생각하고 나아갈 수 있구나 라는 걸 깨달을 수 있었습니다.\n\n\n\n에드워드 리의 인터뷰 중 2\n\n\n글또는 내가 나를 가지고 어떤 실험이든 할 수 있도록 판을 깔아주는 소중한 연구실이었기에, 이번 10기에도 글쓰는 성장의 힘을 실험해볼 수 있는 좋은 시간을 만들어 갈 수 있길 바래봅니다."
  },
  {
    "objectID": "posts/note/2023-02-02-geultto-8th-start.html",
    "href": "posts/note/2023-02-02-geultto-8th-start.html",
    "title": "📘Geultto 8th Start",
    "section": "",
    "text": "Introduction\n나는 나를 안다라고 말할 수 있는 사람이 많을까요? 사실 그런 사람들이 많지 적은지는 관심이 없고 적어도 저는 내가 그런 사람이길 바랐습니다.\n나라는 것에 대한 정의와 경계가 있는 건 분명한데 그 안에 있는 공간이 너무 광활해서 마치 작은 박스안에 블랙홀이 있는 것 같은 느낌이랄까요. 어렸을 때는 누가 나에게 꿈과 하고 싶은게 무엇이냐라고 물으면 대화의 흐름에 방해되지 않을 정도로만 뜸을 들인 뒤 대답을 할 수 있었던 것 같습니다. 흔히들 대답하는 선택지인 직업과 하는 일, 그리고 거기에 약간의 신념을 섞어서요. 걱정과 근심, 고민이 없진 않았지만 대답을 할 수 있다는 것만으로 나는 나를 안다고 생각했습니다. 지금, 물론 지금도 사람들이 청춘이다라고 말하는 시기에 서있지만, 적어도 과거의 나보다 지나온 시간이 많은 지금의 저는 많은 의문이 듭니다. 나는 나를 아나? 내가 모르면 누가 나를 아나라며 누구보다 나에 대해서는 잘 안다고 생각했는데.. 그 주장이 약해지는 것이 느껴집니다.\n이렇게 내가 나에 대한 갈피를 잡지 못할 때 글또가 하나의 도움이 되기 때문에 이번 8기에도 참여하게 되었습니다.\n\n\nRelated Works\n이질감이 드는 나를 바라볼 때 도움이 되었던 몇가지 방법이 있습니다. 우선 한동안 유명했던 MBTI가 그 중 하나였는데 고등학생 때 진로적성 검사로 몇번 해보고 최근 1-2년 동안 대한민국에서 자기소개의 도구가 되어버린 MBTI는 나를 알아가는데 재밌는 도구였습니다. 저는 INFJ입니다. 검사지에 나온 몇가지 눈에 띄는 서술어들로는 통찰력 있는 선지자. 극소수의 유형, 내향적인 이상주의자, 감정형(F) 중에서는 대체로 사고(T) 성향이 높은 편 등이 있고 이외에 SNS등에서 유형별 특징들에서는 아무도 신경안쓰는데 혼자 눈치봄, 생각이 너무 많아 집중이 안됨(아무생각 안하는 거 할 줄 모른다), 글로 적으면 논리력 기가막힌데 말로하면 어버버하고 안나옴, 뭐든지 파악하고 정리하고 싶어한다, 구구절절 (괄호까지 쳐가며) 설명하는 습관이 있다 등이 있습니다. 공감의 차원을 넘어서서 누가 나를 이렇게 잘 아나 흠칫 놀랄 정도였으니 이 검사가 확증편향을 가져온다고 하더라도 나를 알아가는데 정말 많이 도움이 되었던 것 같습니다.\n\n\n\n매우 공감되는 INFJ의 모습들\n\n\n다음으로, 글쓰는 또라이라는 글쓰는 모임에서 소개받은 Big5 검사라는 걸 한번 해봤는데 공감력, 책임감, 협조성이 대표적인 키워드로 나왔고 강직함, 지적 호기심, 상상력, 걱정 등이 내가 강한 성향으로 친밀감, 자제력, 사교성, 분노, 리더십 등이 내가 약한 성향으로 나왔습니다. MBTI 검사와 크게 다르지 않아 새롭게 알게 된 사실이 딱히 많진 않았지만 추가적으로 일적인 측면에서 현재 주어진 상황에 만족하기보다 새로운 지식과 기술을 익히기 위해 노력하는 변화 주도형 리더라는 워딩에 공감하며 어쩌다 현재 내가 공학도가 되어있는가에 대해 생각해보게 된 것 같습니다.\n\n\n\nBig5 검사 결과\n\n\n성향, 성격도 나의 일부지만 앞서 이야기했던 꿈과 하고 싶은 것, 싫어하는 것, 좋아하는 것 등 다른 부분들도 나라는 경계 안에 있다는 것을 압니다. 요즘 부쩍 하고 싶은 것에 대한 확신은 불투명해지고 싫어하는 것과 좋아하는 것을 구분짓는 표현은 하기가 힘들어집니다. 그래서 아마 이 글도 이런 맥락 속에서 나를 안다는 것에 대한 고찰로 시작된 것 같습니다. 학창시절 때는 친구들이 저에게 너는 정말 네가 하고 싶어하는 게 분명한 것 같아. 멋있다. 라고 많이 이야기 했습니다. 저 또한 그런 나의 모습에 뿌듯함과 자랑스러움을 느끼고 있었습니다. 그때는 나를 안다고 생각했습니다. 지금은 잘 모르겠습니다. 여전히 저를 좋게 봐주는 좋은 사람들이 있지만 이제 더 이상 제 안에서 저를 뿌듯하게 바라보기 보다 스스로를 걱정스럽게 바라보는 눈길이 더 느껴집니다. 이런 불안함이 일시적인거다. 성향적인 이유에서 그런거다.라고 생각할 수도 있겠지만 그렇게 넘어갈 수 없을 것 같다는 느낌이 들었습니다. 여전히 괴롭지만 최근에 엄마와의 통화를 통해 힌트를 얻고 다른 관점으로 보려고 합니다. 그 괴로움이 내가 살아있다는 증거라고 하시더라구요. 그렇게 말씀하시면서 너를 위해 기도하고 있다고 덧붙여서 말씀하실 때 솔직히 반항심도 들었습니다. 불안함과 고통이 살아있다는 증거라면 솔직히 살고 싶지 않다라는 생각을 하게 되는 나를 하나님은 봐주시지 않나라는 생각이 들었습니다. 각설하고, 여기서 힌트를 얻었다는 것은 내가 살아가는 동안 완전히 해결할 수 있는 고민은 아니구나 싶었습니다. 해결하고 싶었지만 해결할 수 없다는 걸 깨달았다고 마무리 짓겠습니다.\n나의 성향과 나의 고민을 펼쳐보며 든 생각은 글쓰기가 유일한 내 창구일 수도 있겠다 싶었습니다. 나는 나를 표현하기 힘들어하고 다른 사람들에게 잘 말하지도 못하는 사람이라는 걸 알아서 글을 쓰면서 나를 돌아보고 내가 나에게 말해보는 시간이 도움이 될 것 같았습니다. 시간이 지난 후에 이 블로그에 들어와서 과거 시점의 나를 한번 더 들여다 볼 수 있는 것이 때론 힘이 될 것 같기도 했습니다. 글쓰는 또라이 모임 8기에 참여하면서 쓰게 되는 글들은 물론 이런 종류의 글들이 아닙니다. 감정적인 이야기보다는 기술적이고 어떤 정보나 지식을 찾던 사람들을 위한 글을 쓰는 것이 취지에도 맞고 제가 기르고 싶은 실력에도 도움이 되기 때문에 참여하게 된 것 입니다. 아마 이 글을 마지막으로 글또에 제출하는 글들에는 더 이상 저의 잡다한 이야기가 들어가지 않을 것 같습니다.\n\n\nMethod\n글또에서 쓰는 기술적인 글들 또한 제가 쓴 글이고 나를 만들어가는 글들이 되기 때문에 앞서 장황한 도입을 썼습니다. 7기에서 썼던 글들이 지금 연구하고 있는 나를 만들어준 동력원이기도 했고 다시 읽으면서 도움이 되기도 했습니다. 이전에 다짐한 것처럼 지식과 정보의 차원에서 다른 사람들에게 도움이 되는 글을 쓰고 싶은 것은 변함없지만 너무 이 목표에 사로잡히지는 않으려고 합니다. 일단 내가 정리하고 글을 쓰면서 나에게 도움이 된 것 만으로도 나는 또 다른 성장을 했다고 나를 격려하며, 이번 8기에서는 이런 마음으로 임하려고 합니다. 쉽게 말하자면 부담을 좀 덜어내고 글을 자주 쓰는 습관이 이번 기수에서 제가 설정한 목표라고 할 수 있겠습니다.\n정리를 좋아하니 개괄식으로 글또에서 이루고자한 목표를 정리해보겠습니다.\n\n글또에서 설정한 글 마감일에 모든 글 제출하기(패쓰권 안쓰기)\n커피드백에서 성장할 수 있는 모든 점들을 파악하고 더 나아지기\n코드가 들어가는 글 50%이상 작성하기\n글또에 제출하지 않는 주간에는 자유글 쓰기(매주 1포스팅 목표)\n\n이전 7기에서 목표와 다짐을 작성할 때도 글을 쓰는 목표말고도 나를 이루는 생활 목표도 적었었는데 잘 지키진 못했어도 다시한번 환기시킬 수 있어서 좋았기에 이번에도 생활목표들을 덧붙여볼까 합니다. 생활 목표들은 기본적으로 매일한다는 생각으로 지금 이미 만들고 있는 습관들에 업그레이드 하는 수준으로 설정해봤습니다.\n\n매일 아침 해야할 것과 하지 말아야 할 것 적기: 생각이 많아서 실천을 못하기 때문에 아침에 정리를 해서 나를 좀 더 빨리 움직여보려고 합니다.\n매일 운동하기(저녁 만보 걷기/저녁 30분 러닝/아침 필라테스 중 하나 골라서): 우울감과 나태함은 생각보다 쉽게 신체활동으로 극복된다는 깨달음이 계기가 되었습니다.\n매일 영어표현 3개 내 문장으로 만들기(with RealClass): 문어체보다 쉽게 다가갈 수 있는 대화 영어가 부족하기 때문에 부담없이 반복해보려고 합니다.\n\n\n\nConclusion\n결론은 광활한 우주에서 먼지에 불과한 고민많은 한 작은 인간이 커피와 음악으로 해결되지 않는 자아관찰을 이번 글또 8기와 함께 잘 이뤄나가고 싶다는 것이었습니다. 글을 쓸때 만큼은 한없이 진지해지고 새벽이 아님에도 새벽감성 충만하게 되는 저의 모습을 또 하나 알아가며 이번 글또 8기도 잘 활동해보고 싶습니다."
  },
  {
    "objectID": "posts/note/2022-10-03-daily-english-001.html",
    "href": "posts/note/2022-10-03-daily-english-001.html",
    "title": "🌎Casual English Phrases 001",
    "section": "",
    "text": "영어 회화 표현이나 논문에서 쓸 수 있는 유용한 표현들을 조금씩 익히기 위해 시작한 1000일 시리즈 포스트 입니다. 공부 리소스는 유튜브, 리얼클래스, PN님의 블로그, cake 어플, 책, 논문, 기사 등 다양한 출처에서 보고 기록합니다.\n\n\n갑자기\nout of the blue\n\nA cockroach appeared out of the blue. And then I really wanted to get out of there. 갑자기 바퀴벌레가 나타나서 진짜 거기서 나오고 싶었어.\nI just thought about that out of the blue. 그냥 갑자기 생각나서.\nIt happened out of the blue, so I could not handle that situation. 그 일이 너무 갑자기 일어나서 어떻게 할 수가 없었어.\n\n\n\n듣고있어\nI’m all ears\n\nJust keep talking about it. I’m all ears. 계속 말해. 나 듣고 있어.\nWhen you have a conversation with someone, you should be all ears. 누군가와 대화를 하면 귀기울여서 들어야해.\nDon’t pretend to be all ears. 듣고 있는 척 하지마.\n\n\n\n들킬것 같이 위태롭고 불안한\nprecarious\n\nI hate precarious situations. They make me very vexed. 난 진짜 조마조마한 상황들이 싫어. 진짜 나를 초조하게 만들거든.\n\nvexed : 초조한\n\nReal liars don’t have a precarious feelings. Instead, They enjoy the situations and make more fakes. 진짜 거짓말 쟁이들은 불안한 감정을 느끼지 않아. 오히려 그들은 그 상황을 즐기고 더 많은 거짓들을 만들어내지.\nJust tell the truth to her, not make yourself more precarious. 그냥 그녀에게 진실을 털어놔 더 너 자신을 위태롭게 만들지 말고."
  },
  {
    "objectID": "posts/note/2023-08-31-daily-english-010.html",
    "href": "posts/note/2023-08-31-daily-english-010.html",
    "title": "🌎Casual English Phrases 010",
    "section": "",
    "text": "최고야\nNothing beats ~\n\n가장 좋아하는 것을 말할 때 유용한 표현\n\n\nNothing beats “About Time” \"About Time\" 영화가 최고야.\nNothing beats a good nap. 좋은 낮잠이 최고야.\nNothing beats pizza. 피자보다 좋은 건 없어.\n\n\n\n완전 동의/공감해!\nI couldn’t agree more!\nIt’s hard to go wrong with that choice!\n\n\n꼭 명심해야 할 점은\nRule number one,\n\nRule number one, don’t get yourself into this situation again. 첫 번째 규칙, 다시는 이런 상황을 만들지 마.\nRule number one. Don’t be late. Our boss hates being late. 꼭 명심해야 될 게 있어. 절대 늦으면 안돼. 우리 상사가 늦는 거 진짜 싫어해.\nRule number one. The customer is always right. 첫번째 규칙, 가장 중요한 원칙: 손님은 언제나 옳다.\n\n\n\n정신 차리고 제대로 살기 시작하다, 제대로 돌아가기 시작하다\nget oneself together\n\nGot myself together. So now we’ll see. 정신 차렸다고요. 이젠 어떻게 될지 두고봐야죠.\nYou’ve got to stop drinking every day. Get yourself together! 너 매일 술 마시는 것 좀 그만하고 정신 좀 차려!\nThe company got itself together and turned a profit. 그 회사는 재정비를 마치고 제대로 돌아가기 시작하면서 이윤을 내기 시작했다.\n\n\n\n감정을 추스리다, 감정을 컨트롤하다\npull oneself together\n\nPull yourself together. There’s no point getting angry about it. 진정하고 감정 추스려. 화내 봤자 소용없어.\nPull yourself together. You can’t just sit here and cry. 감정 추스려. 여기 앉아서 울고만 있을 순 없잖아."
  },
  {
    "objectID": "posts/code/2022-12-26-class-dict-yaml.html",
    "href": "posts/code/2022-12-26-class-dict-yaml.html",
    "title": "👩‍💻class ⟷ dict ⟷ yaml",
    "section": "",
    "text": "Config Class\n기준이 되는 부모 class AConfig\n\n자식 class가 같은 하위 class에 override 할 때는 이름을 바꾸지 않음\n부모 class에 없었던 하위 class를 만들 경우 자식Class의 이름(e.g. B, C) 활용\n대문자는 Config 객체의 ID\n소문자는 한개의 Config 클래스 객체 내에서의 하위 class 순서\n\n\nclass AConfig():\n    class A_a_Config:\n        element01 = \"element01\"\n        element02 = True\n    class A_b_Config:\n        element03 = 0\n        element04 = [2,4,6]\n\nBConfig는\n\nAConfig의 A_a_Config를 override\nAConfig의 A_b_Config를 override 하지 않음\nB_a_Config 추가\n\n\nclass BConfig(AConfig):\n    class A_a_Config:\n        element02 = False\n    # class A_b_Config:\n    #     pass\n    class B_a_Config:\n        element05 = {\"key\":\"value\"}\n\nCConfig는\n\nAConfig의 A_a_Config를 override\nAConfig의 A_b_Config를 pass로 override\nC_a_Config 추가\n\n\nclass CConfig(AConfig):\n    class A_a_Config:\n        element02 = False\n    class A_b_Config:\n        pass\n    class C_a_Config:\n        element05 = {\"key\":\"value\"}\n\nDConfig는\n\nAConfig의 A_a_Config를 인자로 받아 override\nAConfig의 A_b_Config를 인자로 받아 override\n\n\nclass DConfig(AConfig):\n    class A_a_Config(AConfig.A_a_Config):\n        element02 = False\n    class A_b_Config(AConfig.A_b_Config):\n        pass\n\nEConfig는\n\n__init__(super)로 Aconfig 상속\n\n\nclass EConfig(AConfig):\n    def __init__(super): pass\n    class E_a_Config:\n        element06 = (1,3,5)\n\n\n\nClass to Dict\nclass 객체를 dictionary로 만들기\n\ndef class_to_dict(obj) -&gt; dict:\n    if not hasattr(obj, \"__dict__\"):                # __dict__ : 클래스 객체의 속성 정보를 확인하기 위해 사용. 객체가 가진 여러가지 속성들을 딕셔너리 형태로 편하게 확인\n        return obj\n    result = {}                                     # 빈 dict 객체 만들기\n    for key in dir(obj):                            # dir(): 어떤 객체를 인자로 넣어주면 해당 객체가 어떤 변수와 메소드(method)를 가지고 있는지 나열\n        if key.startswith(\"_\"):                     # startswith: str.startswith(str or tuple) 형식으로 사용하면 되고, 반환 값으로는 True, False를 반환\n            continue\n        element = []                                # 리스트 요소에 대비\n        val = getattr(obj, key)                     # getattr: object에 존재하는 속성의 값을 가져옴\n        if isinstance(val, list):                   # isinstance로 list 객체인지 확인 e.g.) isinstance(object, type)\n            for item in val:\n                element.append(class_to_dict(item))\n        else:                                       # 리스트가 아닌 모든 요소들 처리\n            element = class_to_dict(val)\n        result[key] = element                       # 결과 dict에 저장\n    return result\n\n\nAConfigDict = class_to_dict(AConfig)\nAConfigDict\n\n{'A_a_Config': {'element01': 'element01', 'element02': True},\n 'A_b_Config': {'element03': 0, 'element04': [2, 4, 6]}}\n\n\n\nBConfigDict = class_to_dict(BConfig)\nBConfigDict\n\n{'A_a_Config': {'element02': False},\n 'A_b_Config': {'element03': 0, 'element04': [2, 4, 6]},\n 'B_a_Config': {'element05': {'key': 'value'}}}\n\n\n\nCConfigDict = class_to_dict(CConfig)\nCConfigDict\n\n{'A_a_Config': {'element02': False},\n 'A_b_Config': {},\n 'C_a_Config': {'element05': {'key': 'value'}}}\n\n\n\nDConfigDict = class_to_dict(DConfig)\nDConfigDict\n\n{'A_a_Config': {'element01': 'element01', 'element02': False},\n 'A_b_Config': {'element03': 0, 'element04': [2, 4, 6]}}\n\n\n\nEConfigDict = class_to_dict(EConfig)\nEConfigDict\n\n{'A_a_Config': {'element01': 'element01', 'element02': True},\n 'A_b_Config': {'element03': 0, 'element04': [2, 4, 6]},\n 'E_a_Config': {'element06': (1, 3, 5)}}\n\n\n\n\n\nDict to Yaml\n\nimport yaml\nfrom pprint import pprint\n\n\nAConfigDictYaml = yaml.dump(AConfigDict)\nprint(AConfigDictYaml)\nprint(type(AConfigDictYaml))\n\nA_a_Config:\n  element01: element01\n  element02: true\nA_b_Config:\n  element03: 0\n  element04:\n  - 2\n  - 4\n  - 6\n\n&lt;class 'str'&gt;\n\n\n\n\nYaml to Dict\n\nAYamlDict = yaml.safe_load(AConfigDictYaml)\npprint(AYamlDict)\n\n{'A_a_Config': {'element01': 'element01', 'element02': True},\n 'A_b_Config': {'element03': 0, 'element04': [2, 4, 6]}}\n\n\n\n\nDict to Class\n\n속성값으로 만들어주는 것이기 때문에 내부 메소드나 내부 클래스가 아닌, 내부 변수와 같음\n\n\nclass Dict2Class(object):\n    def __init__(self, input_dict):\n        for key in input_dict:\n            print(\"Set arribute: \", key)\n            setattr(self, key, input_dict[key]) # object에 존재하는 속성의 값을 바꾸거나, 새로운 속성을 생성하여 값을 부여\n\n\nAYamlDictClass = Dict2Class(AYamlDict)\n\nSet arribute:  A_a_Config\nSet arribute:  A_b_Config\n\n\n\nAYamlDictClass.A_a_Config\n\n{'element01': 'element01', 'element02': True}\n\n\n\nAYamlDictClass.A_b_Config\n\n{'element03': 0, 'element04': [2, 4, 6]}"
  },
  {
    "objectID": "posts/code/2025-06-09-ros2-pub-tip.html",
    "href": "posts/code/2025-06-09-ros2-pub-tip.html",
    "title": "👩‍💻ROS2 joint_state_publiser 디폴트 동작 제어하기",
    "section": "",
    "text": "ROS2 애플리케이션에서 URDF 기반 로봇 모델 시각화 시, joint_state_publisher 노드는 기본적으로 정의된 모든 관절의 위치를 0으로 퍼블리시합니다. 하지만 실제 센서나 외부 노드에서 들어오는 관절 상태만 사용하고 싶을 때도 있는데, 단순 토픽 리매핑(remapping)만으로는 이 기본 동작을 해제할 수 없습니다.\n이번 포스팅에서는:\n등을 살펴보며, 가장 깔끔하게 외부 JointState 메시지만 반영하는 설정법을 자세히 알아봅니다.\n두 가지 방식의 차이를 정리하면 다음과 같습니다:"
  },
  {
    "objectID": "posts/code/2025-06-09-ros2-pub-tip.html#remapping만으로는-디폴트-퍼블리시를-끌-수-없는-이유",
    "href": "posts/code/2025-06-09-ros2-pub-tip.html#remapping만으로는-디폴트-퍼블리시를-끌-수-없는-이유",
    "title": "👩‍💻ROS2 joint_state_publiser 디폴트 동작 제어하기",
    "section": "3.1 remapping만으로는 디폴트 퍼블리시를 끌 수 없는 이유",
    "text": "3.1 remapping만으로는 디폴트 퍼블리시를 끌 수 없는 이유\n\npublish_default_positions 기본값이 True\n\njoint_state_publisher 문서에 따르면, 파라미터 publish_default_positions 의 기본값은 True 이고, 이 경우 “외부에서 들어오는 메시지가 없으면 URDF에 정의된 모든 관절 위치를 디폴트(0값)로 퍼블리시” 합니다.\n\n리매핑(remapping)만으로는 이 내부 동작을 변경 불가\n\n토픽 remapping(('sim_joint_states', '/cmd_joint_states') 등)을 적용해도, 노드는 여전히 내부적으로 “디폴트 위치 퍼블리시”를 수행합니다.\n실제 사례로, Robotics Stack Exchange 답변에서도 “publish_default_positions 를 False 로 설정하지 않으면 JSP는 (source_list 유무와 상관없이) 항상 모든 관절의 값을 퍼블리시한다” 고 명시하고 있습니다."
  },
  {
    "objectID": "posts/code/2025-06-09-ros2-pub-tip.html#해결책-publish_default_positions-옵션-사용",
    "href": "posts/code/2025-06-09-ros2-pub-tip.html#해결책-publish_default_positions-옵션-사용",
    "title": "👩‍💻ROS2 joint_state_publiser 디폴트 동작 제어하기",
    "section": "3.2 해결책: publish_default_positions 옵션 사용",
    "text": "3.2 해결책: publish_default_positions 옵션 사용\n\n파라미터 설정\npublish_default_positions: False\nsource_list: ['cmd_joint_states']\n\n이렇게 하면 /cmd_joint_states 로부터만 JointState 메시지를 받아 /sim_joint_states 로 내보내며, 디폴트(0값) 퍼블리시는 완전히 끌 수 있습니다.\n\nROS 2 런치 예시\nNode(\n  package='joint_state_publisher',\n  executable='joint_state_publisher',\n  name='joint_state_publisher',\n  output='screen',\n  parameters=[\n    {'use_sim_time': False},\n    {'publish_default_positions': False},\n    {'source_list': ['cmd_joint_states']},\n  ],\n),\n\n이렇게 publish_default_positions 를 False 로 명시적으로 꺼야만, remapping만으로는 제어할 수 없는 기본 퍼블리시 동작을 비활성화할 수 있습니다."
  },
  {
    "objectID": "posts/code/2025-03-14-isaacsim-lab.html",
    "href": "posts/code/2025-03-14-isaacsim-lab.html",
    "title": "👩‍💻IsaacSim과 IsaacLab 알아보기",
    "section": "",
    "text": "이번 포스팅에서는 대표적인 로봇 시뮬레이터 중 하나인 IsaacSim(IsaacLab)에 대해서 알아보고, 이를 Local에 설치하는 과정을 진행해보겠습니다."
  },
  {
    "objectID": "posts/code/2025-03-14-isaacsim-lab.html#isaacgymenvs-및-omniisaacgymenvs",
    "href": "posts/code/2025-03-14-isaacsim-lab.html#isaacgymenvs-및-omniisaacgymenvs",
    "title": "👩‍💻IsaacSim과 IsaacLab 알아보기",
    "section": "2.1 IsaacGymEnvs 및 OmniIsaacGymEnvs",
    "text": "2.1 IsaacGymEnvs 및 OmniIsaacGymEnvs\nNVIDIA가 Isaac Gym과 Isaac Sim을 공개하면서, 이 두 시뮬레이터를 보다 쉽게 활용할 수 있도록 IsaacGymEnvs와 OmniIsaacGymEnvs라는 오픈 소스 환경을 함께 제공했습니다. 이 환경들은 각각의 시뮬레이터 위에서 실행되며, 기본적인 시뮬레이션 기능을 테스트하고 로봇 학습이 가능한 범위를 보여주는 소스코드들을 보여줍니다. 즉, IsaacGymEnvs는 Isaac Gym 기반, OmniIsaacGymEnvs는 Isaac Sim 기반으로 동작하며, 각 시뮬레이터의 장점을 최대한 활용할 수 있도록 설계되었습니다.\n이 환경들을 사용하면, 사전 정의된 로봇 태스크를 빠르게 실행하고 학습 성능을 벤치마킹하는 데 유용합니다. 하지만, 연구자들이 직접 커스텀 환경을 개발하거나 새로운 알고리즘을 테스트하기에는 구조적으로 한계가 많았습니다.\n❌ 특정 태스크에 최적화된 환경 → 새로운 태스크 추가가 어려움\n❌ 환경의 확장성이 부족 → 다양한 로봇 실험을 하기에 한계\n❌ 연구자가 직접 실험을 설계하기 어려움\n결국, 로봇 학습 연구가 점점 복잡해지고 다양한 실험이 필요해지면서, IsaacGymEnvs와 OmniIsaacGymEnvs만으로는 한계를 극복하기 어려워졌고 이 문제를 해결하기 위해 등장한 것이 바로 Isaac Lab입니다."
  },
  {
    "objectID": "posts/code/2025-03-14-isaacsim-lab.html#isaac-lab의-핵심-특징",
    "href": "posts/code/2025-03-14-isaacsim-lab.html#isaac-lab의-핵심-특징",
    "title": "👩‍💻IsaacSim과 IsaacLab 알아보기",
    "section": "3.1 Isaac Lab의 핵심 특징",
    "text": "3.1 Isaac Lab의 핵심 특징\nIsaac Lab은 강화 학습(RL), 시연을 통한 학습(LfD), 모션 플래닝 등 다양한 로봇 연구 워크플로우를 단순화하는 것을 목표로 합니다. 이를 위해 모듈형(modular) 구조를 갖추고 있으며, 연구자가 손쉽게 환경을 변경하거나 새로운 태스크를 추가할 수 있도록 확장성을 고려하여 설계되었습니다.\n✔️ Isaac Lab이 기존 프레임워크와 다른 점\n\n사전 구축된 환경 & 태스크 제공 → 바로 사용할 수 있는 샘플 환경 포함\n맞춤형 커스텀 환경 개발 지원 → 사용자가 직접 환경을 설계하고 실험 가능\n강화 학습 & 모션 플래닝 최적화 → 다양한 학습 방법 적용 가능"
  },
  {
    "objectID": "posts/code/2025-03-14-isaacsim-lab.html#isaac-lab에서-새롭게-추가된-기능",
    "href": "posts/code/2025-03-14-isaacsim-lab.html#isaac-lab에서-새롭게-추가된-기능",
    "title": "👩‍💻IsaacSim과 IsaacLab 알아보기",
    "section": "3.2 Isaac Lab에서 새롭게 추가된 기능",
    "text": "3.2 Isaac Lab에서 새롭게 추가된 기능\n뿐만 아니라, Isaac Sim의 모든 기능을 계승하면서도, 로봇 학습 연구를 위한 새로운 기능들이 추가되었습니다.\n🔹 구동기(Actuator) 동역학 시뮬레이션\n기존 IsaacGymEnvs에서는 단순한 힘 기반 제어가 주로 사용되었지만, Isaac Lab에서는 모터와 액추에이터의 동역학을 보다 정교하게 반영할 수 있습니다. 이는 실제 로봇의 제어 방식과 더욱 유사한 시뮬레이션을 가능하게 합니다.\n🔹 절차적 지형 생성(Procedural Terrain Generation)\n학습을 위한 다양한 환경을 자동으로 생성할 수 있도록 프로시저럴(Procedural) 방식의 지형 생성 기능을 지원합니다. 이는 로봇이 다양한 지형을 학습하는 데 유용하며, 일반화 성능 향상에 기여합니다.\n🔹 사람 시연 데이터 수집 기능\n시연을 통한 학습(LfD: Learning from Demonstration)을 위한 데이터 수집 기능을 포함하여, 사람 시연 데이터 기반의 강화 학습 및 행동 복제(Behavior Cloning)가 가능합니다. 이는 단순한 RL을 넘어, 보다 효율적인 학습 방법론을 연구하는 데 유용합니다."
  },
  {
    "objectID": "posts/code/2025-03-14-isaacsim-lab.html#isaac-sim-설치",
    "href": "posts/code/2025-03-14-isaacsim-lab.html#isaac-sim-설치",
    "title": "👩‍💻IsaacSim과 IsaacLab 알아보기",
    "section": "4.1 Isaac Sim 설치",
    "text": "4.1 Isaac Sim 설치\nIsaac Sim은 4.5.0 버전으로 업데이트되면서 설치 과정이 더욱 간편해졌습니다.\n하지만, 시뮬레이터를 실행하기 위해서는 고성능 그래픽 카드 및 특정 요구 사항을 충족해야 합니다.\n\n4.1.1 시스템 사양 확인: Isaac Sim Compatibility Checker\n\n\n\n설치 요구 사항들\n\n\n💻 최소 요구 사양\n\n운영체제: Ubuntu 20.04 이상\n\nGPU: NVIDIA RTX 3060 이상 (RTX 3090 / 4090 권장)\n\nCUDA 버전: 11.8 이상\n\nVRAM: 최소 8GB (12GB 이상 권장)\n\nRAM: 최소 16GB (32GB 이상 권장)\n\n디스크 공간: 최소 100GB 이상의 여유 공간\n\nIsaac Sim을 실행할 수 있는지 자동으로 점검할 수 있도록 Isaac Sim Compatibility Checker 도구를 제공합니다. 이 도구를 먼저 실행하여 시스템이 적합한지 확인한 후, 설치를 진행하는 것을 추천합니다.\n✅ 설치 및 실행 방법\n\nIsaac Sim Compatibility Checker 다운로드\n압축을 풀어 원하는 폴더에 저장 및 아래 명령어로 실행\n\n./omni.isaac.sim.compatibility_check.sh\n\n검사 결과 확인 (색상 코드 안내)\n\n🟢 Green (최적): 모든 요구 사항 충족\n\n🟢 Light-green (양호): 실행 가능하지만 성능 최적화 가능\n\n🟠 Orange (충분하지만 개선 필요): 실행은 가능하지만 권장 사양보다 낮음\n\n🔴 Red (미지원): 실행 불가능\n\n\n\n\n\n4.1.2 Isaac Sim 설치 (Standalone 방식)\nIsaac Sim은 Omniverse Launcher 없이 독립 실행형(Standalone) 패키지로 다운로드하여 설치해야 합니다.\n✅ 설치 방법\n1. Isaac Sim 다운로드 페이지에서 최신 릴리스 다운로드\n2. 아래 명령어를 실행하여 설치\nmkdir ~/isaacsim\ncd ~/Downloads\nunzip \"isaac-sim-standalone@4.5.0-rc.36+release.19112.f59b3005.gl.linux-x86_64.release.zip\" -d ~/isaacsim\ncd ~/isaacsim\n./post_install.sh\n./isaac-sim.selector.sh\n\n설치 후, Isaac Sim App Selector가 실행되며 실행 모드를 선택 가능\n\n\n\n\n4.1.3 Isaac Sim 실행하기\n설치 후 실행하려면, 아래 명령어를 사용합니다.\n./isaac-sim.sh\n👉 최초 실행 시 셰이더 캐시 로딩(Warmup)이 필요할 수 있습니다. 이 경우, 아래 명령어를 실행하여 셰이더 캐시를 미리 로드합니다.\n./isaac-sim.sh --warmup"
  },
  {
    "objectID": "posts/code/2025-03-14-isaacsim-lab.html#isaac-lab-설치",
    "href": "posts/code/2025-03-14-isaacsim-lab.html#isaac-lab-설치",
    "title": "👩‍💻IsaacSim과 IsaacLab 알아보기",
    "section": "4.2 Isaac Lab 설치",
    "text": "4.2 Isaac Lab 설치\nIsaac Lab은 Isaac Sim이 설치된 환경에서 동작하므로, 먼저 Isaac Sim을 설치한 후 진행해야 합니다. Isaac Sim을 pip 환경에서 사용하기 위해 추가적인 설치 과정이 필요합니다.\n\n4.2.1 설치 전 요구 사항 확인\n✅ GLIBC 버전 확인 (Ubuntu 20.04에서는 추가 조치 필요)\nldd --version\n👉 GLIBC 2.34 이상이 필요합니다. Ubuntu 20.04 기본 버전(2.31)과 호환되지 않을 수 있으므로 확인이 필요합니다.\n\n\n4.2.2 Conda 가상환경 설정\nIsaac Sim을 활용하기 위해 Python 3.10 환경을 갖춘 가상 환경을 생성합니다.\nconda create -n env_isaaclab python=3.10\nconda activate env_isaaclab\n\n\n4.2.3 PyTorch 설치 (CUDA 버전에 맞게 선택)\nCUDA 버전에 맞는 PyTorch를 설치해야 합니다.\n✅ CUDA 11을 사용하는 경우:\npip install torch==2.5.1 --index-url https://download.pytorch.org/whl/cu118\n✅ CUDA 12를 사용하는 경우:\npip install torch==2.5.1 --index-url https://download.pytorch.org/whl/cu121\n\n\n4.2.4 Isaac Sim pip 설치\npip를 이용해 Isaac Sim을 가상 환경에서 설치합니다.\npip install --upgrade pip\npip install 'isaacsim[all,extscache]==4.5.0' --extra-index-url https://pypi.nvidia.com\n✅ 설치 확인\nisaacsim\n⚠️ 첫 실행 시 NVIDIA EULA 동의가 필요합니다.\nDo you accept the EULA? (Yes/No): Yes\n👉 첫 실행 시 모든 확장(extension) 파일이 다운로드되므로 10분 이상 소요될 수 있습니다.\n\n\n4.2.5 Isaac Lab 클론 (소스 코드 다운로드)\ngit clone https://github.com/isaac-sim/IsaacLab.git\ncd IsaacLab\n\n\n4.2.6 의존성 패키지 설치\nIsaac Lab 실행을 위해 필요한 패키지를 설치합니다.\nsudo apt install cmake build-essential\n그 후, Isaac Lab의 의존성을 설치합니다.\n./isaaclab.sh --install\n👉 모든 강화 학습 프레임워크를 함께 설치합니다.\n👉 특정 프레임워크만 설치하고 싶다면 아래와 같이 실행합니다.\n./isaaclab.sh --install rl_games  # 특정 프레임워크 (예: rl_games)만 설치\n✅ 설치 가능한 프레임워크 목록\n\nrl_games\nrsl_rl\nsb3\nskrl\nrobomimic\nnone (학습 프레임워크 제외)\n\n\n\n4.2.7 Isaac Lab 실행 확인\n# Option 1: isaaclab.sh 실행\n./isaaclab.sh -p scripts/tutorials/00_sim/create_empty.py\n\n# Option 2: Python으로 직접 실행\npython scripts/tutorials/00_sim/create_empty.py\n✅ 정상적으로 실행되면, Isaac Lab이 올바르게 설치된 것입니다! 🎉"
  },
  {
    "objectID": "posts/code/2025-02-02-realsense-ros2.html",
    "href": "posts/code/2025-02-02-realsense-ros2.html",
    "title": "👩‍💻ROS2 RealSense C++ Node",
    "section": "",
    "text": "이번 포스팅에서는 Intel RealSense 카메라로 촬영한 비디오를 실시간 스트리밍하고, 영상에서 감지한 AprilTag을 OpenCV를 활용해 시각적으로 표시하는 ROS 2 기능을 C++ 노드로 구현하는 실습을 진행하겠습니다. AprilTag는 로봇 비전, 증강 현실, 마커 기반 위치 추정 등에 활용되는 태그 시스템입니다. 또한, 본 포스팅에서는 RealSense 카메라를 ROS 2에서 사용하기 위해 librealsense2를 설정하는 과정과 일반적으로 발생할 수 있는 빌드 문제 해결 방법도 함께 다룰 예정입니다.\n이 간단한 ROS2 프로젝트를 시작하기 앞서, 아래의 사전 준비 사항들을 확인해서 먼저 환경을 셋팅해주시기 바랍니다."
  },
  {
    "objectID": "posts/code/2025-02-02-realsense-ros2.html#overview",
    "href": "posts/code/2025-02-02-realsense-ros2.html#overview",
    "title": "👩‍💻ROS2 RealSense C++ Node",
    "section": "Overview",
    "text": "Overview\n앞서 설명한 것처럼, 가장 단순한 ROS2 패키지 구조를 가지고 진행할 예정입니다. 2개의 C++ Node들, 즉 Publisher와 Subcriber를 만들어 Realsense Camera D457에서 이미지를 보내주면(Publisher) 받은 이미지를 토대로 Tag ID를 인식하여 OpenCV로 간단한 ploting을 한 창(Subsciber)을 띄워볼 예정입니다. 각 노드들의 역할을 정리해보면 아래와 같습니다.\n\nPublisher Node: RealSense 카메라에서 이미지를 수집하여 ROS 2 토픽으로 퍼블리싱하는 노드\n\nlibrealsense2 패키지를 이용하여 RealSense camera를 연결\n640×480 해상도, 30 FPS 칼라 이미지 프레임을 스트리밍\nROS 2 토픽으로 (image_raw) 이미지 프레임 Publish\n\nSubscriber Node: RealSense 카메라에서 영상을 받아 OpenCV로 처리하고, AprilTag 라이브러리를 활용하여 태그를 감지하는 기능을 수행하는 노드\n\nimage_raw 토픽을 Subscribe\ncv_bridge를 이용하여 OpenCV Mat으로 이미지를 변환\nOpenCV window에 간단한 표시를한 이미지 띄우기"
  },
  {
    "objectID": "posts/code/2025-02-02-realsense-ros2.html#creating-the-ros-2-package",
    "href": "posts/code/2025-02-02-realsense-ros2.html#creating-the-ros-2-package",
    "title": "👩‍💻ROS2 RealSense C++ Node",
    "section": "Creating the ROS 2 Package",
    "text": "Creating the ROS 2 Package\nmy_realsense_example 라는 새로운 ROS 2 package를 만들어 시작해보겠습니다. ROS2의 workspace에 있는 src/폴더에서 C++ 패키지를 생성해줍니다.\ncd ~/ros2_ws/src\nros2 pkg create my_realsense_example \\\n  --build-type ament_cmake \\\n  --dependencies rclcpp sensor_msgs cv_bridge\n그다음, my_realsense_example/src/ 에 각 노드에 대한 C++ 스크립트 파일을 작성합니다. - realsense_publisher.cpp - image_subscriber.cpp"
  },
  {
    "objectID": "posts/code/2025-02-02-realsense-ros2.html#publisher-node-realsense_publisher.cpp",
    "href": "posts/code/2025-02-02-realsense-ros2.html#publisher-node-realsense_publisher.cpp",
    "title": "👩‍💻ROS2 RealSense C++ Node",
    "section": "Publisher Node: realsense_publisher.cpp",
    "text": "Publisher Node: realsense_publisher.cpp\n우선, 카메라 센서에서 이미지를 받아 송출하는 Publisher 노드 코드를 살펴보겠습니다. 먼저, 아래와 같이 필요한 헤더와 패키지들을 포함합니다.\n\nlibrealsense2/rs.hpp: RealSense 카메라를 제어\n\nrclcpp/rclcpp.hpp: ROS 2 노드를 생성 및 관리\n\nsensor_msgs/msg/image.hpp: ROS 2의 이미지 메시지를 처리\n\ncv_bridge/cv_bridge.h: ROS 메시지와 OpenCV 이미지 간 변환\n\nopencv2/opencv.hpp: OpenCV 기능 활용\n\n이러한 요소들을 포함하여, RealSense 카메라에서 영상을 받아 ROS 2 메시지로 변환하고 퍼블리싱하는 노드를 구현할 수 있습니다.\n#include &lt;memory&gt;\n#include &lt;chrono&gt;\n#include &lt;librealsense2/rs.hpp&gt;           // Intel RealSense SDK\n#include \"rclcpp/rclcpp.hpp\"\n#include \"sensor_msgs/msg/image.hpp\"     // ROS 2 Image message\n#include \"cv_bridge/cv_bridge.h\"         // cv_bridge for ROS &lt;-&gt; OpenCV\n#include \"opencv2/opencv.hpp\"            // OpenCV\n본격적으로 RealSensePubliserClass를 살펴보면, RealSense 카메라에서 수집한 영상을 OpenCV 형식으로 변환한 후, 이를 ROS 2의 이미지 메시지로 변환하여 주기적으로 퍼블리싱하는 과정이 진행됩니다. 이 클래스 노드를 통해 송출된 이미지를 다른 ROS 2 노드가 Subscribe하고 활용할 수 있습니다.\n크게 2 부분으로 나누어서 볼 수 있습니다.\n\n노드 초기화 (RealSensePublisher)\n\nrclcpp::Node를 상속받아 \"realsense_publisher\"라는 이름의 ROS 2 노드를 생성합\nrs2::pipeline을 설정하여 RealSense 카메라에서 데이터를 스트리밍할 준비\nrs2::config를 사용해 컬러 영상 스트리밍을 640x480 해상도, BGR8 포맷, 30 FPS로 설정\ncreate_publisher&lt;sensor_msgs::msg::Image&gt;(\"image_raw\", 10);를 통해 \"image_raw\" 토픽으로 영상을 퍼블리싱할 퍼블리셔를 생성\ncreate_wall_timer를 사용하여 100ms(약 10Hz) 간격으로 프레임을 가져와 퍼블리싱하는 타이머를 설정\n\n프레임 캡처 및 퍼블리싱 (publishFrame)\n\npipeline_-&gt;wait_for_frames();를 사용해 RealSense 카메라에서 새로운 프레임을 가져옴\nframeset.get_color_frame();을 통해 컬러 프레임을 추출하고, 존재하지 않으면 경고 메시지를 출력\ncv::Mat을 사용하여 RealSense의 컬러 프레임 데이터를 OpenCV 형식으로 변환\ncv_bridge::CvImage를 활용해 OpenCV 이미지를 ROS 2의 sensor_msgs::msg::Image 형식으로 변환한 후 타임스탬프와 프레임 ID를 추가\npublisher_-&gt;publish(*image_msg);를 통해 변환된 이미지를 \"image_raw\" 토픽으로 퍼블리싱\n\n\nclass RealSensePublisher : public rclcpp::Node\n{\npublic:\n  RealSensePublisher()\n  : Node(\"realsense_publisher\")\n  {\n    pipeline_ = std::make_shared&lt;rs2::pipeline&gt;();\n\n    // Configure and start the pipeline to stream color images\n    rs2::config cfg;\n    cfg.enable_stream(RS2_STREAM_COLOR, 640, 480, RS2_FORMAT_BGR8, 30);\n    pipeline_-&gt;start(cfg);\n\n    publisher_ = this-&gt;create_publisher&lt;sensor_msgs::msg::Image&gt;(\"image_raw\", 10);\n\n    timer_ = this-&gt;create_wall_timer(\n      std::chrono::milliseconds(100),  // ~10 Hz\n      std::bind(&RealSensePublisher::publishFrame, this)\n    );\n\n    RCLCPP_INFO(this-&gt;get_logger(), \"RealSense Publisher Node started.\");\n  }\n\nprivate:\n  void publishFrame()\n  {\n    // Wait for the next set of frames\n    rs2::frameset frameset = pipeline_-&gt;wait_for_frames();\n    rs2::video_frame color_frame = frameset.get_color_frame();\n    if (!color_frame)\n    {\n      RCLCPP_WARN(this-&gt;get_logger(), \"No color frame received.\");\n      return;\n    }\n\n    // Convert RealSense frame to OpenCV Mat\n    cv::Mat color_image(\n      cv::Size(color_frame.get_width(), color_frame.get_height()),\n      CV_8UC3,\n      (void*)color_frame.get_data(),\n      cv::Mat::AUTO_STEP\n    );\n\n    // Convert to ROS Image message via cv_bridge\n    auto image_msg = cv_bridge::CvImage(std_msgs::msg::Header(), \"bgr8\", color_image).toImageMsg();\n    image_msg-&gt;header.stamp = this-&gt;get_clock()-&gt;now();\n    image_msg-&gt;header.frame_id = \"realsense_color_frame\";\n\n    publisher_-&gt;publish(*image_msg);\n  }\n\n  std::shared_ptr&lt;rs2::pipeline&gt; pipeline_;\n  rclcpp::Publisher&lt;sensor_msgs::msg::Image&gt;::SharedPtr publisher_;\n  rclcpp::TimerBase::SharedPtr timer_;\n};\n마지막으로 main에서는 ROS 2 노드 실행의 진입점으로, RealSensePublisher 노드를 생성하고 실행하는 역할을 합니다. 이를 통해 RealSense 카메라에서 받은 영상을 ROS 2의 \"image_raw\" 토픽으로 지속적으로 퍼블리싱하는 노드가 실행됩니다.\nint main(int argc, char *argv[])\n{\n  rclcpp::init(argc, argv); // ROS 2 노드 실행을 위한 초기화\n\n  rclcpp::spin(std::make_shared&lt;RealSensePublisher&gt;()); // `RealSensePublisher` 노드를 생성하고 실행\n  /* \n  `spin()` 함수는 노드가 지속적으로 실행되도록 유지하며, \n  콜백 함수(`publishFrame()`)를 주기적으로 호출하여 RealSense 카메라에서 영상을 가져오고 퍼블리싱합니다. \n  */\n\n  rclcpp::shutdown(); // 프로그램 종료 시 ROS 2를 안전하게 종료\n  return 0;\n}\n각 부분을 살펴본 Publisher 노드를 통합하면 아래와 같이 realsense_publisher.cpp가 완성됩니다.\n\n\n\n\n\n\nrealsense_publisher.cpp\n\n\n\n\n\n#include &lt;memory&gt;\n#include &lt;chrono&gt;\n#include &lt;librealsense2/rs.hpp&gt;           // Intel RealSense SDK\n#include \"rclcpp/rclcpp.hpp\"\n#include \"sensor_msgs/msg/image.hpp\"     // ROS 2 Image message\n#include \"cv_bridge/cv_bridge.h\"         // cv_bridge for ROS &lt;-&gt; OpenCV\n#include \"opencv2/opencv.hpp\"            // OpenCV\n\nclass RealSensePublisher : public rclcpp::Node\n{\npublic:\n  RealSensePublisher()\n  : Node(\"realsense_publisher\")\n  {\n    pipeline_ = std::make_shared&lt;rs2::pipeline&gt;();\n\n    // Configure and start the pipeline to stream color images\n    rs2::config cfg;\n    cfg.enable_stream(RS2_STREAM_COLOR, 640, 480, RS2_FORMAT_BGR8, 30);\n    pipeline_-&gt;start(cfg);\n\n    publisher_ = this-&gt;create_publisher&lt;sensor_msgs::msg::Image&gt;(\"image_raw\", 10);\n\n    timer_ = this-&gt;create_wall_timer(\n      std::chrono::milliseconds(100),  // ~10 Hz\n      std::bind(&RealSensePublisher::publishFrame, this)\n    );\n\n    RCLCPP_INFO(this-&gt;get_logger(), \"RealSense Publisher Node started.\");\n  }\n\nprivate:\n  void publishFrame()\n  {\n    // Wait for the next set of frames\n    rs2::frameset frameset = pipeline_-&gt;wait_for_frames();\n    rs2::video_frame color_frame = frameset.get_color_frame();\n    if (!color_frame)\n    {\n      RCLCPP_WARN(this-&gt;get_logger(), \"No color frame received.\");\n      return;\n    }\n\n    // Convert RealSense frame to OpenCV Mat\n    cv::Mat color_image(\n      cv::Size(color_frame.get_width(), color_frame.get_height()),\n      CV_8UC3,\n      (void*)color_frame.get_data(),\n      cv::Mat::AUTO_STEP\n    );\n\n    // Convert to ROS Image message via cv_bridge\n    auto image_msg = cv_bridge::CvImage(std_msgs::msg::Header(), \"bgr8\", color_image).toImageMsg();\n    image_msg-&gt;header.stamp = this-&gt;get_clock()-&gt;now();\n    image_msg-&gt;header.frame_id = \"realsense_color_frame\";\n\n    publisher_-&gt;publish(*image_msg);\n  }\n\n  std::shared_ptr&lt;rs2::pipeline&gt; pipeline_;\n  rclcpp::Publisher&lt;sensor_msgs::msg::Image&gt;::SharedPtr publisher_;\n  rclcpp::TimerBase::SharedPtr timer_;\n};\n\nint main(int argc, char *argv[])\n{\n  rclcpp::init(argc, argv);\n  rclcpp::spin(std::make_shared&lt;RealSensePublisher&gt;());\n  rclcpp::shutdown();\n  return 0;\n}"
  },
  {
    "objectID": "posts/code/2025-02-02-realsense-ros2.html#subscriber-node-image_subscriber.cpp",
    "href": "posts/code/2025-02-02-realsense-ros2.html#subscriber-node-image_subscriber.cpp",
    "title": "👩‍💻ROS2 RealSense C++ Node",
    "section": "Subscriber Node: image_subscriber.cpp",
    "text": "Subscriber Node: image_subscriber.cpp\n다음으로 위에서 Publisher 노드가 보내준 이미지 데이터를 받는 Subscriber 노드에 대해 살펴보겠습니다.\n우선 아래와 같이 필요한 헤더 파일들을 포함합니다.\n\nC++ 표준 라이브러리 포함 (&lt;memory&gt;, &lt;string&gt;, &lt;iostream&gt;)\n\n스마트 포인터(std::shared_ptr), 문자열(std::string), 표준 입출력(std::cout)을 사용하기 위한 헤더 파일\n\nROS 2 관련 헤더 포함\n\nrclcpp/rclcpp.hpp: ROS 2 노드 및 퍼블리셔/구독자 기능을 사용하기 위한 기본 라이브러리\nsensor_msgs/msg/image.hpp: ROS 2에서 이미지 메시지(sensor_msgs::msg::Image) 를 다루기 위한 메시지 타입\n\ncv_bridge/cv_bridge.h: ROS 이미지 메시지를 OpenCV의 cv::Mat으로 변환하거나, 반대로 OpenCV 이미지를 ROS 메시지로 변환하기 위해 사용\n\nOpenCV 관련 헤더 포함\n\nopencv2/highgui.hpp: OpenCV의 GUI 기능 (cv::imshow(), cv::waitKey())을 사용하여 이미지를 표시할 때 필요\nopencv2/imgproc.hpp: 이미지 처리 기능(예: 필터링, 변형, 색 변환 등)을 수행하기 위한 라이브러리\n\nAprilTag 감지를 위한 라이브러리 포함\n\nextern \"C\" {} 블록을 사용하여 C 스타일로 작성된 AprilTag 라이브러리를 C++ 코드에서 사용할 수 있도록 합니다.\n\n#include &lt;apriltag.h&gt;: AprilTag 감지 시스템의 핵심 헤더\n#include &lt;tag25h9.h&gt;: AprilTag 25h9 태그 세트를 사용하기 위한 헤더 파일(25x9 해상도를 가진 태그 세트)\n#include &lt;tag36h11.h&gt;는 주석 처리되어 있으며, 필요 시 AprilTag 36h11 세트(더 정밀한 태그)를 사용할 수 있도록 변경 가능\n\n\n#include &lt;memory&gt;\n#include &lt;string&gt;\n#include &lt;iostream&gt;\n\n// ROS 2\n#include \"rclcpp/rclcpp.hpp\"\n#include \"sensor_msgs/msg/image.hpp\"\n#include \"cv_bridge/cv_bridge.h\"\n\n// OpenCV\n#include \"opencv2/highgui.hpp\"\n#include \"opencv2/imgproc.hpp\"\n\n// AprilTag library headers\nextern \"C\" {\n#include &lt;apriltag.h&gt;\n// #include &lt;tag36h11.h&gt;\n#include &lt;tag25h9.h&gt;\n}\n다음으로 ImageSubscriber Class를 보겠습니다. RealSense 카메라에서 수신한 영상을 처리하고, AprilTag을 검출하여 화면에 표시하는 ROS 2 노드를 구현합니다. OpenCV를 사용하여 영상을 변환하고, AprilTag 라이브러리를 이용해 태그를 감지 및 시각화하여 검출된 태그의 ID, 꼭짓점, 중심을 화면에 시각적으로 표시하는 과정이 구현되어 있습니다. 크게 세 부분으로 나누어서 보면,\n1. 노드 초기화 (ImageSubscriber)\n\n\"image_raw\" 토픽을 구독(subscription_)하여 카메라 영상을 수신\nOpenCV 창 \"RealSense Camera + AprilTag\"를 생성하여 화면에 영상을 표시할 준비\nAprilTag 검출기 초기화:\n\ntag_family_ = tag25h9_create(); → AprilTag 25h9 태그 세트를 사용하도록 설정\ntag_detector_ = apriltag_detector_create(); → AprilTag 검출기를 생성\napriltag_detector_add_family(tag_detector_, tag_family_); → 특정 태그 패밀리를 검출기에 추가\n검출기의 매개변수(예: quad_decimate, quad_sigma)를 조정하여 성능 최적화 가능\n\n\n2. 이미지 수신 및 AprilTag 검출 (imageCallback): ROS 2의 \"image_raw\" 토픽에서 새로운 이미지가 수신될 때 호출\n\nROS 2 이미지 메시지 → OpenCV cv::Mat 변환\n\ncv_bridge::toCvCopy(msg, \"bgr8\")-&gt;image;를 사용하여 ROS 2의 sensor_msgs::msg::Image를 OpenCV 형식(cv::Mat)으로 변환합니다\n변환 중 오류 발생 시 예외를 처리\n\n그레이스케일 변환 (AprilTag 검출은 흑백 영상이 필요)\n\ncv::cvtColor(frame, gray, cv::COLOR_BGR2GRAY);\n\nOpenCV cv::Mat → AprilTag 전용 이미지 형식 (image_u8_t) 변환\n\nAprilTag 라이브러리는 image_u8_t 구조체를 사용하므로 변환이 필요\ngray.data를 buf로 전달하여 메모리 복사를 최소화\n\nAprilTag 검출 실행\n\napriltag_detector_detect(tag_detector_, &apriltag_image);을 호출하여 AprilTag을 감지\n검출된 태그는 zarray_t* detections에 저장\n\n검출된 AprilTag에 대한 시각적 표시\n\n각 태그의 네 개의 꼭짓점 좌표(p[]) 를 가져와 초록색(cv::Scalar(0, 255, 0))으로 사각형을 그림\n태그의 중앙 좌표(c[]) 에 빨간색 원을 표시(cv::circle())\n태그의 ID를 노란색(cv::Scalar(0, 255, 255)) 텍스트로 화면에 출력\n\n메모리 정리\n\napriltag_detections_destroy(detections);를 호출하여 검출된 태그 리스트를 정리하여 메모리 누수를 방지\n\n결과 이미지 표시\n\ncv::imshow(\"RealSense Camera + AprilTag\", frame);을 통해 검출 결과를 화면에 표시\ncv::waitKey(1);를 호출하여 OpenCV 창이 반응하도록 유지\n\n\n3. 노드 종료 시 AprilTag 리소스 정리 (~ImageSubscriber())\n\napriltag_detector_remove_family(tag_detector_, tag_family_); → 검출기에서 패밀리를 제거\napriltag_detector_destroy(tag_detector_); → 검출기 삭제\ntag25h9_destroy(tag_family_); → 태그 패밀리 메모리 해제\n\nclass ImageSubscriber : public rclcpp::Node\n{\npublic:\n  ImageSubscriber()\n  : Node(\"image_subscriber\")\n  {\n    subscription_ = this-&gt;create_subscription&lt;sensor_msgs::msg::Image&gt;(\n      \"image_raw\",\n      10,\n      std::bind(&ImageSubscriber::imageCallback, this, std::placeholders::_1)\n    );\n\n    // Create an OpenCV window once\n    cv::namedWindow(\"RealSense Camera\", cv::WINDOW_AUTOSIZE);\n\n    RCLCPP_INFO(this-&gt;get_logger(), \"Image Subscriber Node started.\");\n  }\n\nprivate:\n  void imageCallback(const sensor_msgs::msg::Image::SharedPtr msg)\n  {\n    cv::Mat frame;\n    try\n    {\n      frame = cv_bridge::toCvCopy(msg, \"bgr8\")-&gt;image;\n    }\n    catch (cv_bridge::Exception &e)\n    {\n      RCLCPP_ERROR(this-&gt;get_logger(), \"cv_bridge exception: %s\", e.what());\n      return;\n    }\n\n    if (!frame.empty())\n    {\n      cv::imshow(\"RealSense Camera\", frame);\n      cv::waitKey(1);\n    }\n  }\n\n  rclcpp::Subscription&lt;sensor_msgs::msg::Image&gt;::SharedPtr subscription_;\n};\n마지막으로 main을 살펴보면 ROS 2 이미지 구독 노드(ImageSubscriber)를 실행하는 메인 함수로, ROS 2 이벤트 루프를 관리하며 이미지 구독 노드를 실행하면서 30Hz의 주기로 ROS 이벤트를 처리합니다.\nint main(int argc, char *argv[])\n{\n  rclcpp::init(argc, argv); // ROS 2 노드를 실행할 준비\n  auto node = std::make_shared&lt;ImageSubscriber&gt;(); // 노드 생성: `ImageSubscriber` 노드를 생성하여 RealSense 카메라에서 퍼블리싱되는 `\"image_raw\"` 토픽을 구독할 준비\n\n  rclcpp::Rate rate(30); // 초당 30회(30Hz) 실행\n  while (rclcpp::ok()) // 주기적 실행 루프\n  {\n    rclcpp::spin_some(node); // 노드가 메시지를 수신하고, 콜백 함수(`imageCallback`)를 실행할 수 있도록 합니다. \n    rate.sleep(); // 루프가 30Hz의 일정한 주기로 실행되도록 대기\n  }\n\n  rclcpp::shutdown(); // 프로그램 종료 시 ROS 2를 안전하게 종료\n  return 0;\n}\n각 부분을 살펴본 Subsciber 노드를 통합하면 아래와 같이 image_subscriber.cpp가 완성됩니다.\n\n\n\n\n\n\nimage_subscriber.cpp\n\n\n\n\n\n#include &lt;memory&gt;\n#include &lt;string&gt;\n#include &lt;iostream&gt;\n\n// ROS 2\n#include \"rclcpp/rclcpp.hpp\"\n#include \"sensor_msgs/msg/image.hpp\"\n#include \"cv_bridge/cv_bridge.h\"\n\n// OpenCV\n#include \"opencv2/highgui.hpp\"\n#include \"opencv2/imgproc.hpp\"\n\n// AprilTag library headers\nextern \"C\" {\n#include &lt;apriltag.h&gt;\n// #include &lt;tag36h11.h&gt;\n#include &lt;tag25h9.h&gt;\n}\n\nclass ImageSubscriber : public rclcpp::Node\n{\npublic:\n  ImageSubscriber()\n  : Node(\"image_subscriber\")\n  {\n    subscription_ = this-&gt;create_subscription&lt;sensor_msgs::msg::Image&gt;(\n      \"image_raw\",\n      10,\n      std::bind(&ImageSubscriber::imageCallback, this, std::placeholders::_1)\n    );\n\n    // Create an OpenCV window once\n    cv::namedWindow(\"RealSense Camera + AprilTag\", cv::WINDOW_AUTOSIZE);\n\n    RCLCPP_INFO(this-&gt;get_logger(), \"Image Subscriber Node started.\");\n\n    // Initialize the AprilTag detector\n    // tag_family_ = tag36h11_create();  // Create specific tag family (36h11)\n    tag_family_ = tag25h9_create(); // Create specific tag family (36h11)\n    tag_detector_ = apriltag_detector_create();\n    apriltag_detector_add_family(tag_detector_, tag_family_);\n\n    // Optionally adjust detection parameters\n    // e.g., tag_detector_-&gt;quad_decimate = 1.0; // For performance\n    //       tag_detector_-&gt;quad_sigma = 0.0;\n    // etc.\n  }\n\n  ~ImageSubscriber()\n  {\n    // Cleanup AprilTag resources\n    apriltag_detector_remove_family(tag_detector_, tag_family_);\n    apriltag_detector_destroy(tag_detector_);\n    // tag36h11_destroy(tag_family_);\n    tag25h9_destroy(tag_family_);\n  }\n\nprivate:\n  void imageCallback(const sensor_msgs::msg::Image::SharedPtr msg)\n  {\n    // 1. Convert ROS Image to OpenCV Mat\n    cv::Mat frame;\n    try\n    {\n      frame = cv_bridge::toCvCopy(msg, \"bgr8\")-&gt;image;\n    }\n    catch (cv_bridge::Exception &e)\n    {\n      RCLCPP_ERROR(this-&gt;get_logger(), \"cv_bridge exception: %s\", e.what());\n      return;\n    }\n\n    // 2. Convert color to grayscale (AprilTag needs grayscale)\n    cv::Mat gray;\n    cv::cvtColor(frame, gray, cv::COLOR_BGR2GRAY);\n\n    // 3. Convert OpenCV Mat to the format required by AprilTag library\n    //    (AprilTag library uses its own image type: image_u8_t)\n    image_u8_t apriltag_image = {\n      .width  = gray.cols,\n      .height = gray.rows,\n      .stride = static_cast&lt;int&gt;(gray.step),\n      .buf    = gray.data\n    };\n\n    // 4. Detect tags\n    zarray_t* detections = apriltag_detector_detect(tag_detector_, &apriltag_image);\n\n    // 5. For each detection, draw a bounding box on the image\n    for (int i = 0; i &lt; zarray_size(detections); i++)\n    {\n      apriltag_detection_t *det;\n      zarray_get(detections, i, &det);\n\n      // Access the four corners of the AprilTag\n      // p[] is in image coordinates (x=horizontal, y=vertical)\n      cv::Point corners[4];\n      for (int c = 0; c &lt; 4; c++)\n      {\n        corners[c] = cv::Point(det-&gt;p[c][0], det-&gt;p[c][1]);\n      }\n\n      // Draw lines between corners\n      for (int c = 0; c &lt; 4; c++)\n      {\n        cv::line(frame, corners[c], corners[(c+1)%4], cv::Scalar(0, 255, 0), 2);\n      }\n\n      // Draw the tag center\n      cv::circle(frame,\n                 cv::Point(det-&gt;c[0], det-&gt;c[1]),\n                 5, cv::Scalar(0, 0, 255), -1);\n\n      // Put tag ID text near the center\n      std::string text = \"ID: \" + std::to_string(det-&gt;id);\n      cv::putText(frame, text,\n                  cv::Point(det-&gt;c[0] + 10, det-&gt;c[1] + 10),\n                  cv::FONT_HERSHEY_SIMPLEX,\n                  0.5,\n                  cv::Scalar(0, 255, 255),\n                  2);\n    }\n\n    // Always remember to destroy detection array to avoid memory leaks\n    apriltag_detections_destroy(detections);\n\n    // 6. Show the image\n    cv::imshow(\"RealSense Camera + AprilTag\", frame);\n    cv::waitKey(1);\n  }\n\n  rclcpp::Subscription&lt;sensor_msgs::msg::Image&gt;::SharedPtr subscription_;\n\n  // AprilTag fields\n  apriltag_family_t *tag_family_;\n  apriltag_detector_t *tag_detector_;\n};\n\nint main(int argc, char *argv[])\n{\n  rclcpp::init(argc, argv);\n\n  auto node = std::make_shared&lt;ImageSubscriber&gt;();\n\n  // Spin in a loop so that callbacks can be processed\n  rclcpp::Rate rate(30);\n  while (rclcpp::ok())\n  {\n    rclcpp::spin_some(node);\n    rate.sleep();\n  }\n\n  rclcpp::shutdown();\n  return 0;\n}"
  },
  {
    "objectID": "posts/code/2025-02-02-realsense-ros2.html#cmakelists.txt-configuration",
    "href": "posts/code/2025-02-02-realsense-ros2.html#cmakelists.txt-configuration",
    "title": "👩‍💻ROS2 RealSense C++ Node",
    "section": "CMakeLists.txt Configuration",
    "text": "CMakeLists.txt Configuration\n그 다음으로 ROS2 패키지 빌드에 대한 내용을 담고 있는 CMakeLists.txt 파일을 작성해보겠습니다. CMakeLists.txt ( my_realsense_example 루트에 위치)에서 RealSense 카메라에서 이미지를 퍼블리싱하고, AprilTag을 검출하는 ROS 2 노드들을 빌드 및 설치하는 설정을 정의합니다. default로 생성되어있던 내용을 아래 내용으로 업데이트 합니다.\ncmake_minimum_required(VERSION 3.8)\nproject(realsense_apriltag_pubsub)\n\nfind_package(ament_cmake REQUIRED)\nfind_package(rclcpp REQUIRED)\nfind_package(sensor_msgs REQUIRED)\nfind_package(cv_bridge REQUIRED)\nfind_package(OpenCV REQUIRED)\nfind_package(realsense2 REQUIRED)\nfind_package(apriltag REQUIRED)  # &lt;-- Add this!\n\ninclude_directories(\n  include\n  ${OpenCV_INCLUDE_DIRS}\n)\n\n# Publisher\nadd_executable(realsense_publisher src/realsense_publisher.cpp)\nament_target_dependencies(realsense_publisher\n  rclcpp\n  sensor_msgs\n  cv_bridge\n)\ntarget_link_libraries(realsense_publisher\n  ${OpenCV_LIBS}\n  realsense2\n)\n\n# Subscriber with AprilTag\nadd_executable(image_subscriber src/image_subscriber.cpp)\nament_target_dependencies(image_subscriber\n  rclcpp\n  sensor_msgs\n  cv_bridge\n)\ntarget_link_libraries(image_subscriber\n  ${OpenCV_LIBS}\n  realsense2\n  apriltag          # &lt;-- Link AprilTag\n)\n\ninstall(TARGETS\n  realsense_publisher\n  image_subscriber\n  DESTINATION lib/${PROJECT_NAME}\n)\n\n# Install the launch directory\ninstall(DIRECTORY launch/\n  DESTINATION share/${PROJECT_NAME}/launch\n)\n\nament_package()\n이 CMakeLists.txt는 RealSense 카메라에서 이미지를 퍼블리싱하는 노드와, 해당 영상을 구독하여 AprilTag을 검출하는 노드를 빌드할 수 있도록 설정합니다. 특히, apriltag 라이브러리를 추가하여 AprilTag 검출 기능이 포함된 구독자(image_subscriber) 노드를 지원하는 것이 특징입니다.\n1. 필수 패키지 찾기 (find_package)\n\nament_cmake, rclcpp, sensor_msgs, cv_bridge, OpenCV, realsense2 등의 패키지를 포함하여 ROS 2, OpenCV, RealSense SDK를 사용할 수 있도록 설정\nAprilTag 추가 (find_package(apriltag REQUIRED))\n\nAprilTag 검출을 위해 apriltag 라이브러리를 필수적으로 포함\n\n\n2. 헤더 포함 경로 설정 (include_directories)\n\ninclude/ 폴더와 OpenCV의 헤더 파일이 포함된 디렉토리를 추가\n\n3. 퍼블리셔 실행 파일 생성 (realsense_publisher)\n\nsrc/realsense_publisher.cpp 파일을 빌드하여 실행 가능한 퍼블리셔 노드 생성\nament_target_dependencies(realsense_publisher ...)를 통해 ROS 2 라이브러리(rclcpp, sensor_msgs, cv_bridge)와의 의존성을 설정\ntarget_link_libraries(realsense_publisher ${OpenCV_LIBS} realsense2)을 사용해 OpenCV 및 RealSense SDK를 링크\n\n4. 구독자(AprilTag 포함) 실행 파일 생성 (image_subscriber)\n\nsrc/image_subscriber.cpp 파일을 빌드하여 실행 가능한 구독자 노드 생성\nament_target_dependencies(image_subscriber ...)를 통해 ROS 2 라이브러리 의존성 추가\ntarget_link_libraries(image_subscriber ${OpenCV_LIBS} realsense2 apriltag)을 통해 OpenCV, RealSense SDK, AprilTag 라이브러리를 링크하여 AprilTag 검출 기능을 활성화\n\n5. 실행 파일 설치 (install(TARGETS ...))\n\n빌드된 실행 파일(realsense_publisher, image_subscriber)을 ROS 2 패키지의 실행 가능 경로에 배포\n\n6. Launch 파일 설치 (install(DIRECTORY launch/ ...))\n\nlaunch/ 디렉토리를 ROS 2 패키지의 share/ 디렉토리에 복사하여 실행 시 사용할 수 있도록 설정\n이를 통해 ros2 launch realsense_apriltag_pubsub some_launch_file.py 형태로 실행 가능\n\n7. 패키지 선언 (ament_package())\n\nament_package()를 호출하여 ROS 2 빌드 시스템(ament_cmake)과 호환되도록 패키지를 설정\n\n\n⚠️발생할 수 있는 Build 에러 이슈들\n\nUndefined reference to rs2_…: realsense2 library를 링크하지 않았을 때 발생하는 에러입니다. find_package(realsense2 REQUIRED)와 target_link_libraries(... realsense2)가 CMakeLists.txt에 포함되어 있는지 확인하세요.\nCannot find image_subscriber.cpp: CMakeLists.txt에서 지정한 파일 이름과 경로가 src/ 폴더 내의 실제 파일과 정확히 일치하는지 확인하세요.\ncv_bridge or OpenCV not found: package.xml에 작성되어 있는 디펜던시들을 확인하고, 프로그래밍하는 환경 시스템에 cv_bridge 와 OpenCV가 설치되어 있는지 확인하세요."
  },
  {
    "objectID": "posts/code/2025-02-02-realsense-ros2.html#build-실행",
    "href": "posts/code/2025-02-02-realsense-ros2.html#build-실행",
    "title": "👩‍💻ROS2 RealSense C++ Node",
    "section": "Build & 실행",
    "text": "Build & 실행\n이제 패키지를 빌드할 모든 준비는 끝났습니다. 이제 소스 코드들을 작성한 workspace 경로(e.g., ~/ros2_ws)로 이동하여 빌드해줍니다.\ncolcon build --packages-select my_realsense_example\nsource install/setup.bash\n잘 빌드가 되었다면 터미널을 2개 띄워서 각각 publisher 와 subscriber 노드를 실행시켜 줍니다.\n\nTerminal 1:\nros2 run my_realsense_example realsense_publisher\nTerminal 2:\nros2 run my_realsense_example image_subscriber\n\n그러면 “RealSense Camera + AprilTag” 창이 나오면서 인식된 AprilTag ID와 박스가 표시된 창이 아래와 같이 나올 것 입니다!\n\n\n\n최종 결과물"
  },
  {
    "objectID": "posts/code/2025-02-02-realsense-ros2.html#conclusion",
    "href": "posts/code/2025-02-02-realsense-ros2.html#conclusion",
    "title": "👩‍💻ROS2 RealSense C++ Node",
    "section": "Conclusion",
    "text": "Conclusion\n이제 RealSense 카메라에서 컬러 프레임을 퍼블리싱하고, 이를 OpenCV 창에 표시하는 최소한의 ROS 2 시스템을 구축하였습니다. 단순히 영상을 수신하는 것뿐만 아니라, AprilTag 인식을 활용하여 간단한 이미지 후처리도 수행해 보았습니다. 이를 확장하면 로보틱스에서 필요한 위치 추정과 같은 고급 기능도 구현할 수 있으며, 깊이 프레임, 포인트 클라우드 등 RealSense에서 지원하는 다양한 스트림을 활용할 수도 있습니다. 이번 실습을 통해 ROS 2에서 C++ 기반 패키지를 빌드하고 활용하는 과정에 대한 이해를 돕고, 더 복잡한 기능을 구현하는 프로젝트로 나아가는 데 도움이 되기를 바랍니다."
  },
  {
    "objectID": "posts/code/2023-04-04-install-orbit.html",
    "href": "posts/code/2023-04-04-install-orbit.html",
    "title": "👩‍💻Orbit 설치하기",
    "section": "",
    "text": "Orbit은 IsaacLab으로 변경되었습니다.\n\n\n\n\n\n\n이번 포스팅은 Nvidia의 Omniverse Isaac Orbit에 대해 알아보고, Orbit Docs - Installation Guide를 따라 설치과정을 기록해보았습니다. 공식 문서를 참고하여 설치부터 예제 코드들을 살펴보며 시리즈로 포스팅할 예정입니다.\n\n1 Orbit?\n그럼, Orbit 이란 무엇을 말하는 걸까요? 공식 Docs에서는 다음과 같이 설명하고 있습니다.\n\n\n\n\n\n\nOverview\n\n\n\nIsaac Orbit (or orbit in short) is a unified and modular framework, built on top of NVIDIA Omniverse and Isaac Sim, for robot learning. It offers a modular design to easily and efficiently create robot learning environments with photo-realistic scenes, and fast and efficient simulation.\n\n\n\n\nOrbit Github Main Image\n\n\n\n소개글을 보면, 크게 2가지 특징을 살펴볼 수 있습니다. 첫번째로는 Robot learning을 위한 모듈화된 프레임워크라는 점을 알 수 있고 두번째로는 Omniverse와 Isaac Sim이라는 것에 기반한 프로그램이라는 것 입니다. Robot learning을 위한 이라는 말에서 Github 메인 사진에서도 볼 수 있듯이 로봇 작동을 위한 모든 learning 과정을 지원하기 위한 프레임 워크라는 것을 알 수 있습니다. 매니퓰레이터부터 핸드 로봇, 사족보행 로봇에 이르기까지 다양하고 폭넓은 로봇의 작동 시나리오를 지원해주는 프로그램이라는 것을 볼 수 있습니다.\n하지만 Orbit이라는 프레임워크가 Omniverse와 Isaac Sim에 기반한다는 설명은 이전에 Nvidia의 시뮬레이터에 대해 알고 있지 못하신 분들이라면 생소하고 헷갈릴 수 있습니다. Nvidia의 시뮬레이터 개발과정이나 제품에 대해 깊이 알아보고자 하는 것이 이번 포스팅의 목적은 아니므로 간단하게 각 프로그램들의 역학관계를 살펴보자면, Omniverse &gt; Isaac Sim &gt; Orbit의 관계라고 파악해볼 수 있습니다. Omniverse가 가장 큰 범위의 가상세계를 위한 플랫폼이고 그 아래에 로봇틱스 분야를 위한 하위 플랫폼 Isaac Sim이 있고 마지막으로 그 안에 오늘 포스팅의 주인공인 Orbit이 있는 것 입니다.\n여기까지 설명을 들었을 때 명확히 이해가 되지 않는 부분이 있을 수 있습니다. Isaac Sim도 로봇틱스 분야를 위한 하위 플랫폼인데 그 안에 Orbit으로 또 따로 Robot learning이라는 모듈 프레임워크가 더 필요할까 의문이 들 수 있습니다. 심지어 이전에 Nvidia의 시뮬레이터를 조금 아셨던 분들이라면, Isaac Gym이라는 강화학습, 즉 robot learning을 위한 (Issac Sim보다 좀 더 기능이 제한된) 프로그램이 있는데 Orbit은 어떤 위치인지 더 애매모호하게 느껴질 수 있습니다. 실제로 저도 기존에 Isaac Gym으로 강화학습을 진행해왔던 사람으로써 Orbit의 등장은 의아한 점이 있었습니다. 이런 의문을 많은 유저들도 느꼈던 것 같습니다. 포럼에 올라온 Q&A에서 공식 Orbit Maintainer의 답변으로는, Orbit은 Isaac Sim의 진입장벽을 낮추기 위한 내장 프레임워크로 2가지의 목적이 있어서 만들어졌다고 합니다. (Q&A 원본은 링크와 함께 아래에 적어두었으니 참고하시길 바랍니다.)\n\n보다 간소화된 인터페이스를 통해서 로봇 학습을 위한 환경 설계 및 강화학습, 모션 플래닝 등과 같은 로봇틱스 워크 플로우를 지원하는 것\n미리 구축된 환경을 벤치마크 예제로 사용하는 프론트 엔드 프레임워크로서 준비된 에셋과 환경 예제를 통해 warm-start를 할 수 있도록 하는 것\n\n\n\n\n\n\n\nIsaacSim과 Orbit의 차이에 대한 Q&A\n\n\n\n\n\nQ: I have two questions about the Orbit.\nMay I know what is the purpose of orbit? It seems like it is a modular framework for especially robot learning. However, reinforcement learning can be realized by Issac Sim (Issac Gym). So, what is so special about Orbit?\nIn the Orbit tutorial, the example always uses the built-in API, e.g. robots and controllers. Is the Orbit an appropriate platform if we want to use custom robots and controllers?\nThank you.\n\nA: In the following extract from the Orbit maintainer’s public post published in the Omniverse Discord channel dedicated to Isaac Sim\n(…) Isaac Orbit- Batteries included framework to reduce barrier to entry. It serves a dual purpose:\n\nsimplified interface for env design and support for many robotics workflows - RL, Motion planning, teleoperation, imitation learning/behavior cloning, and real robot operation. This unification is the USP of orbit as compared to other interfaces.\nFront end framework for prebuilt environments as benchmark examples. (…) We note that many folks in the community are users of IsaacSim, creating new environments rather than physics solvers. Hence, we hope to provide Orbit to warmstart it with prebuilt assets, and environment examples as benchmarks.\n\nImportantly Orbit is designed such that it can accept community contributions with open licensing. We hope that Orbit will be the environment zoo for IsaacSim with contributions from the community as well as internal development.\nOrbit also has modified env interfaces. However, Orbit is open source, and users can modify & suggest change to these interfaces, as needed. (…)\n\n\n\n정리하자면, Orbit은 Robot Learning을 위한 Nvidia의 오픈 소스 프레임워크인데 가장 사용자에게 친근한(쉽게 시작할 수 있는) 프로그램 이라고 정리해볼 수 있을 것 같습니다. 비유를 들어서 Nvidia 플랫폼의 역학을 정리해보자면, Omniverse는 MS Office, Issac Sim은 Power Point, Orbit은 PPT 디자인 추천 프로그램 정도로 정리하고 넘어갈 수 있을 것 같습니다.\n\n\n2 Orbit을 설치하기 전에\n앞에서 정리한 대로 Orbit은 Omniverse 안에 Isaac Sim을 기반으로 돌아가는 모듈이기 때문에 IsaacSim을 먼저 설치해야 합니다. 엄밀히 말하자면 아직 Orbit을 설치한 것은 아니고, Orbit이라는 기능을 사용하기 위해 준비하는 과정이라고 생각하시면 될 것 같습니다. Isaac Sim은 크게 (1)Workstation Installation과 (2)Container Installation 2가지 방법으로 설치할 수 있습니다. 각자 개인 컴퓨터를 사용하여 이용하고자 한다면 첫번째 Workstation Installation을 따라가면 되므로 이번 포스팅에서는 Workstation Installation 방법을 따라서 설치를 진행하겠습니다.\n\nWorkstation Installation: 공식 홈페이지의 Isaac Sim 설치방법(Workstation Installation)을 따라 Omniverse App을 통해 Isaac Sim 2022.2을 설치합니다. 아래는 제가 설치를 진행했던 스펙입니다. (만약 해당 컴퓨터 자원이 없다면 클라우드 자원을 이용하여 Isaac Sim을 사용할 수 있습니다.)\n\n\n\n\n\n\n\n설치 진행 환경\n\n\n\n\nUbuntu 20.04 LTS\nNvidia RTX 4080\nDriver Version: 525.60.13\n\n\n\nIsaac Sim의 설치과정은 Documentation에 step by step으로 잘 나와있으므로 자세하게 설명하진 않겠습니다. 아래와 같이 Omniverse App에서 Isaac Sim이 보이고 LANCH라는 버튼을 눌러 실행할 수 있으면 설치에 성공한 것 입니다. (23.04.05 기준) 최신 버젼 2022.2.1이 맞는지 꼭 확인해주시고 만약 최신 버젼이 아니라면 LANCH 옆의 list 버튼을 눌러서 최신 버젼을 다운받고 버젼을 선택할 수 있습니다. 아래 사진에서 처럼 2022.2.1 버젼으로 선택되어 있는 것을 확인할 수 있습니다.\n\n\nOmniverse App에서 Isaac Sim 버젼 확인하기\n\n\n\n\nPython Environment Setting: Isaac Sim은 내부(built-in) Python 3.7이 있고 이 default Python 환경을 사용하는 것이 좋습니다. IsaacSim Default Python Environment을 사용하기 위해서는 아래의 예시 command와 같이 Isaac Sim의 Root folder(Isaac Sim 설치시 별도의 경로 설정이 없이 설치했을 경우 ${HOME}/.local/share/ov/pkg/isaac_sim-*, *은 해당 Isaac Sim의 버젼)에 있는 Python 실행파일인 python.sh를 가지고 실행해야 합니다.\n# ${HOME}/.local/share/ov/pkg/isaac_sim-2022.2.1\n./python.sh path/to/script.py\n\n\nIsaac Sim의 Default Folder\n\n\n\n하지만 이와 같이 매번 Isaac Sim의 내부 python 실행파일 경로를 입력하여 python 스크립트를 돌리는 것은 매우 번거로우므로 시스템 환경 변수 설정를 통해 간단히 내부 python을 불러올 수 있습니다. Linux Ubuntu에서는 시스템 환경 변수를 설정하는 파일은 ~/.bashrc 나 ~/.zshrc를 사용합니다. 아래와 같이 리눅스 GUI 환경에서 지원하는 텍스트 에디터인 gedit을 이용하여 시스템 환경 변수를 설정해보겠습니다. Terminal 창을 열어서 command를 입력합니다.\ngedit ~/.bashrc\n팝업되는 .bashrc 파일의 맨 아래에 다음과 같이 입력합니다. #으로 훗날 여러 시스템 환경 변수들을 왜 설정했는지 메모하기 위해서 주석을 달아놓는 것을 권장합니다.(Orbit을 사용하지 않고 Isaac Sim만 설치하는 공식 문서에서는 같은 파이썬 실행파일을 다른 환경 변수로 설치하기 때문에 중복되어 이름을 설치 하지 않도록 주의합니다.) 추가한 후 저장하고 창을 닫습니다.\n# Isaac Sim root directory\nexport ISAACSIM_PATH=\"${HOME}/.local/share/ov/pkg/isaac_sim-2022.2.1\"\n# Isaac Sim python executable\nexport ISAACSIM_PYTHON_EXE=\"${ISAACSIM_PATH}/python.sh\"\n파일을 편집했다고 바로 추가된 환경 변수가 바로 적용되는 것이 아니라 아래의 command까지 터미널에서 실행시키고 나서 적용이 됩니다.\nsource ~/.bashrc\n실행 확인 : 프로그램 설치와 환경변수까지 제대로 설정되었는지 확인하기 위해 Running the simulator에 안내되어 있는 몇 가지 점검 command를 실행시켜 보겠습니다.\n\n시뮬레이터 실행 파일(isaac-sim.sh) 확인\n# note: you can pass the argument `--help` to see all arguments possible.\n$ISAACSIM_PATH/isaac-sim.sh --help\n\n시뮬레이터 내부 파이썬 실행 파일(python.sh) 확인\n# checks that python path is set correctly\n$ISAACSIM_PYTHON_EXE -c \"print('Isaac Sim configuration is now complete.')\"\n\n시뮬레이터 standalone 예제(add_cubes.py)로 실행 확인\n# checks that Isaac Sim can be launched from python\n$ISAACSIM_PYTHON_EXE $ISAACSIM_PATH/standalone_examples/api/omni.isaac.core/add_cubes.py\n\n\nadd_cubes.py 실행 결과\n\n\n\n\n\n\n\n3 Orbit 설치하기\n이제 Orbit을 본격적으로 설치해보려고 합니다. 앞서 Isaac Sim이 제대로 설치가 되고 예제까지 돌아가는지 꼭 확인을 한 후 진행해주시기 바랍니다.\n\nOrbit repository를 git clone 해오기\n# Option 1: With SSH\ngit clone git@github.com:NVIDIA-Omniverse/orbit.git\n# Option 2: With HTTPS\ngit clone https://github.com/NVIDIA-Omniverse/Orbit.git\nSymbolic link 생성하기: 심볼릭 링크란 원본 파일을 가리키도록 하는 윈도우에서의 바로가기 아이콘과 같은 의미로 이해하시면 됩니다. 위에서 git clone한 repo의 위치로 들어가서 다음과 같은 명령어를 통해 심볼릭 링크를 만들어 줍니다.\n# enter the cloned repository\ncd Orbit\n# create a symbolic link\nln -s $ISAACSIM_PATH _isaac_sim\nOrbit 실행파일 명령어 등록하기: orbit을 실행하기 위해서는 매번 git clone 한 repository에 있는 orbit.sh 파일을 써주어야 합니다. 만약 orbit.sh이 있는 위치에서 실행시킨다면 ./orbit.sh 로 간단하겠지만 다른 위치의 파일을 Orbit에서 실행시키고 싶을 때 단축키가 있다면 훨씬 간편하게 실행할 수 있어 편리할 것 입니다. Orbit을 편리하게 실행시킬 수 있는 명령어로는 orbit으로 정해서 즉, ./orbit.sh 와 orbit 이 같도록 하는 작업을 진행해보겠습니다. 앞서 리눅스의 시스템 환경변수를 설정하는 방법에서 .bashrc 파일을 이용하여 명령어 별칭을 등록하는 방법을 이용합니다.\necho -e \"alias orbit=$(pwd)/orbit.sh\" &gt;&gt; ${HOME}/.bashrc\n별칭 등록을 적용하기 위해 source ~/.bashrc도 잊지 않고 실행해주세요.(명령어가 제대로 등록됐는지 확인해보려면 gedit ~/.bashrc를 통해 파일에서 맨 아랫줄에 alias orbit=$(Orbit 레포지토리 위치)/orbit.sh이 써있는지 확인해보면 됩니다.) 터미널에서 아래의 명령어를 입력했을때 다음과 같은 창이 나온다면 명령어 별칭 등록이 잘 된 것 입니다.\norbit --help\n\n\norbit 명령어 별칭 확인\n\n\n\n\n\n\n4 Orbit에서 가상환경 설정\nOrbit에서는 orbit -p라는 명령어를 통해 Isaac Sim의 파이썬 실행파일을 자동으로 가져와서 사용합니다. (혹은 위에서 명령어 별칭을 등록안했다면 Orbit repo 위치에서 ./orbit.sh -p라고 입력해도 됩니다.) 하지만 가상환경을 사용하고 싶다면 Conda를 이용하면 됩니다.\norbit -c (또는 --conda)\n위와 같이 입력하면 자동으로 orbit이라는 이름의 가상환경이 만들어지고 앞으로 이 가상환경을 사용하고 싶다면 conda activate orbit을 통해 활성화 시킬 수 있습니다.\n\n\nConda를 이용하여 가상환경 설치 및 확인\n\n\n\n\n\n5 Extensions 설치\n\napt를 이용해서 우분투에서 디펜던시를 설치합니다.\nsudo apt install cmake build-essential\nOrbit repo에 있는 source/extensions 폴더의 확장프로그램들을 설치합니다. 이때 --editable flag로 편집 가능한 모드로 설치하게 되어 개발자가 확장 기능을 수정하게 되면 변경사항이 즉시 적용됩니다.\norbit --install # or orbit -i\n(rsl-rl 같은)Learning framework 등의 여러 디펜던시 프로그램들을 설치합니다.\norbit --extra # or orbit -e\n\n\n\n6 Closing\n마지막까지 설치과정을 잘 마치셨다면 아래의 명령어를 입력 했을 때 사족보행 로봇들이 나오면서 Orbit이 잘 실행되는 것을 확인하실 수 있습니다!👏👏👏(처음 실행시 시간이 조금 걸릴 수 있으니 기다려주세요.)\norbit -p source/standalone/demo/play_quadrupeds.py\n\n\nplay_quadrupeds.py 실행 결과\n\n\n\n이번 포스팅에서는 Nvidia의 Orbit에 대해 간단히 알아보고 설치까지 진행해보았습니다. 아직 공식 Documentation도 업데이트 중이고 프로그램 자체도 업데이트가 활발히 되고 있어서 이후에 이번 포스팅이 버젼에 따라 도움이 안될 수도 있겠지만 많은 분들께 참고가 되었으면 좋겠습니다. 설치과정에 대해 다른 옵션들이나 자세하게 알고 싶으신 분들은 공식 설치 Documentation을 참고하시면 업데이트 되는 소식들과 함께 더 자세하고 많은 정보를 얻으실 수 있을 것 같습니다. 이어지는 포스팅에서는 각 예제들을 통해서 Orbit에 대해 더 알아보도록 하겠습니다."
  },
  {
    "objectID": "posts/code/2024-01-21-quarto-advanced.html",
    "href": "posts/code/2024-01-21-quarto-advanced.html",
    "title": "👩‍💻Quarto Blog + α",
    "section": "",
    "text": "Quarto를 활용하여 Github 블로그를 구축하는 이전 포스팅에 이어서, Github 블로그를 좀 더 보기 좋게, 혹은 원하는 기능을 좀 더 추가하는 몇가지 Customizing 사항들에 대해 소개하려고 합니다. 사실 블로그 Customizing은 삽질의 시작이며, 처음 시작은 가볍게 시작하지만 하나씩 옵션들을 내맘대로 건드리다가 보면 시간이 훌쩍 지나가버리는 마법을 경험하게 됩니다.\n기본적으로 .qmd파일을 작성하면서 글의 configuration을 설정하는 것은 Tutorial: Authoring에서 제목 작성, category 설정등 기본적인 부분을 확인할 수 있습니다. 기본적인 설정들에 더해서 저번 포스팅보다 여러가지 옵션들을 바꿔가면서 확인하는 과정들이 많기 때문에 localhost(로컬 컴퓨터)에서 블로그가 어떻게 바뀌어 가는지 확인하기 위해 아래의 명령어를 VSCode 터미널에 입력하여 창을 띄우며 확인해보세요!"
  },
  {
    "objectID": "posts/code/2024-01-21-quarto-advanced.html#toc",
    "href": "posts/code/2024-01-21-quarto-advanced.html#toc",
    "title": "👩‍💻Quarto Blog + α",
    "section": "ToC",
    "text": "ToC\nToC란 Table of Contents 의 약자로 블로그의 내용들이 여러 제목/소제목들로 구성되어 글이 길어질 경우, 내용을 한눈에 파악하기 쉽도록 목차를 보여주는 기능입니다. Quarto에서는 html 설정 옵션으로 이 기능을 지원하고 있으며 아래 예시 사진에서와 같이 한쪽에 ToC를 보여줘서 포스팅의 독자분들이 쉽게 내용을 파악하고 필요하는 부분만 골라서 확인하기도 용이합니다.\n\n\n\nBlog 포스팅에서 ToC 부분\n\n\nQuarto에서는 다양한 ToC 형식을 지원하고 있는데 작성하는 .qmd파일의 서두에 설정하는 포스팅 설정 부분에 toc: ture라고 추가하면 해당 .qmd파일에 #, ## 등으로 제목으로 작성했던 이름을 기반으로 해당 포스팅의 ToC가 나타나게 됩니다.\n\n\n\nqmd파일에서 ToC 설정하기\n\n\n만약 ToC의 Numbering을 자동으로 나오게 하고 싶다면 아래와 같이 number-sections: true도 포스팅 설정 부분에 추가해준다면 넘버링된 ToC가 나오는 것을 확인할 수 있습니다.\ntoc: true\nnumber-sections: true\n\n\n\nNumbering ToC로 바뀐 모습\n\n\n이 부분도 Quarto Docs에서 더 다양한 옵션들을 확인해보실 수 있으니 다양한 옵션들을 기호에 맞게 바꿔가며 확인해보세요!"
  },
  {
    "objectID": "posts/code/2024-01-21-quarto-advanced.html#이미지-넣기",
    "href": "posts/code/2024-01-21-quarto-advanced.html#이미지-넣기",
    "title": "👩‍💻Quarto Blog + α",
    "section": "이미지 넣기",
    "text": "이미지 넣기\n이미지와 관련된 작성 코드 스니펫을 소개하겠습니다. 이전에 이미지를 Typora를 통해 넣거나 Markdown 문법의 형식으로 이미지를 삽입하게 되면 ![이미지 캡션](이미지 경로)와 같이 코드로 이미지를 넣어주게 됩니다. 하지만 이렇게 이미지들을 넣게 되면 이미지 사이즈 조절이나 정렬 등을 설정해주기가 어렵기 때문에 html 문법형식을 빌려 글에 이미지들을 넣어주는 것을 추천드립니다.\n\nhtml 사용하여 이미지 삽입\n이 코드는 가운데 정렬(centering), 이미지의 (포스팅 대비 상대적인) 너비 조정(width), 이미지 캡션(figcaption)을 html 문법을 빌려 이미지를 삽입해주는 snippet입니다.\n&lt;center&gt;\n&lt;img src=\"../../images/이미지_이름.png\" width=\"50%\" /&gt;\n&lt;figcaption&gt;이미지 캡션&lt;/figcaption&gt;\n&lt;/center&gt;\n포스팅 대표 이미지 설정\n다음으로 작성글의 대표 이미지를 설정하면 블로그 메인 화면에 뜨는 그림을 정해 줄 수 있습니다.\n\n\n\n블로그 메인 화면에서 각 포스팅들의 대표 이미지들\n\n\n.qmd파일의 맨 윗부분에 설정해주는 문서 configuration에 image: ... 라인을 추가하고, ...부분에 이미지 경로를 설정해주면 포스팅의 대표 이미지를 설정할 수 있습니다. 아래는 이번 포스팅의 대표 이미지를 설정하는 예시 configuration 입니다.\n---\ntitle: \"👩‍💻Quarto Blog(2)\"\ntoc: true\ndate: \"2024-01-17\"\ndescription: Quarto로 속편한 Github Blog 구축하기(2)\ncategories: [blog, quarto, code]\nimage: ../../images/2024-01-12-quarto-blog2/blog2.jpg\n---"
  },
  {
    "objectID": "posts/code/2024-01-21-quarto-advanced.html#callout-block",
    "href": "posts/code/2024-01-21-quarto-advanced.html#callout-block",
    "title": "👩‍💻Quarto Blog + α",
    "section": "Callout Block",
    "text": "Callout Block\nCallout Block은 포스팅에서 따로 강조하고 싶거나 토글(Toggling)하고 싶은 내용이 있을 때 사용하면 유용한 작성 방법입니다. Quarto Docs의 Callout Blocks에서 볼 수 있듯이, note, warning, important, tip, caution과 같이 글의 메인 내용과 다른 포인트를 가진 내용이거나 강조하고 싶은 파트를 작성하여 보여줄 수 있습니다.\n::: {.callout-caution collapse=\"true\"}\n# Callout Block 예시\nQuarto의 Callout Block을 활용해봅시다! 생각보다 정말 유용하게 잘 쓰는 기능입니다 :)\n:::\n이 코드 스니펫을 이용하여 글에 삽입하면 아래와 같이 나옵니다. 한번 아래 Block을 클릭해보세요!\n\n\n\n\n\n\nCallout Block 예시\n\n\n\n\n\nQuarto의 Callout Block을 활용해봅시다! 생각보다 정말 유용하게 잘 쓰는 기능입니다 :)\n\n\n\n예시에서 확인할 수 있다시피 collapse=\"true\"등과 같은 Callout Block에 해당하는 다양한 옵션들도 Docs에서 찾아볼 수 있으니 다양하게 시도해보세요!"
  },
  {
    "objectID": "posts/code/2024-01-21-quarto-advanced.html#수학-수식",
    "href": "posts/code/2024-01-21-quarto-advanced.html#수학-수식",
    "title": "👩‍💻Quarto Blog + α",
    "section": "수학 수식",
    "text": "수학 수식\n논문 리뷰와 같은 글들을 작성할 때 가끔식 수학 기호나 수식들을 작성해야 할 때가 있습니다. 그래서 블로그에서 수식이 잘 작성이 되는가도 블로그 플랫폼을 결정하는 중요한 기준이 되기도 합니다. Quarto의 처음 소개에서도 말씀드렸던 것처럼 수학, 과학적 언어와 글들에 친화적인 플랫폼이기 때문에 Quarto는 수식 삽입을 TeX 문법으로 작성하면 예쁘게 랜더링 되도록 만들어진 블로그 플랫폼입니다. 사실 Markdown 문법과 다르지 않습니다. Quarto Docs Equations에서 볼 수 있듯이 $로 수식을 warpping하면 수식이 잘 랜더링 되는 것을 볼 수 있습니다.\nhtml에서 수식을 다양한 형식으로 rendering할 수 있도록 옵션도 제공하고 있습니다. Format Options 테이블에 있는 html-math-method옵션을 활용해서 plain, webtex, gladtex, mathml, mathjax, katex 등과 같은 옵션을 사용할 수 있습니다. 예를들면 아래와 같이 _quarto.yml 설정 파일에 html-math-method: katex 한 줄 추가하면 수식 랜더링 스타일을 바꿀 수 있습니다.\nformat: \n  html:\n    html-math-method: katex"
  },
  {
    "objectID": "posts/code/2024-01-21-quarto-advanced.html#글자색-설정",
    "href": "posts/code/2024-01-21-quarto-advanced.html#글자색-설정",
    "title": "👩‍💻Quarto Blog + α",
    "section": "글자색 설정",
    "text": "글자색 설정\n글자색을 기본 테마에서 설정한 색이 아닌, 파란색, 빨간색 등 다른 색을 사용하고 싶다면 아래와 같이 html 문법을 활용하여 작성하면 됩니다.\n&lt;span style=\"color: blue\"&gt;파란색 글씨를 작성&lt;/span&gt;"
  },
  {
    "objectID": "posts/code/2024-01-21-quarto-advanced.html#toggle",
    "href": "posts/code/2024-01-21-quarto-advanced.html#toggle",
    "title": "👩‍💻Quarto Blog + α",
    "section": "Toggle",
    "text": "Toggle\n토글은 Callout Block을 활용해서 작성할 수도 있지만 기본 html 문법을 활용해서 아래와 같이 작성할 수도 있습니다.\n&lt;details&gt;\n&lt;summary&gt;&lt;b&gt;Toggle 제목&lt;/b&gt;&lt;/summary&gt;\nToggle 내용\n&lt;/details&gt;\n아래와 같이 Toggle 파트가 랜더링 됩니다.\n\n\nToggle 제목\n\nToggle 내용\n\n\nQuarto Blog(1)포스팅에서 Quarto로 기본적인 블로그를 만들고 나서 이번 포스팅의 여러 커스텀 옵션들을 활용하면 각자 원하는 Github Blog를 손쉽게 만들 수 있습니다. 그리고 기본적인 html의 문법을 따라가기 때문에 이번 포스팅에서 몇부분들은 html 문법에 익숙하신 분들에게는 당연한 내용이지 않을까 싶습니다. Quarto를 쓰지 않더라도 블로그를 만들 수 있는 방법은 많지만 처음 소개글에서 이야기 했던 제가 Quarto를 알게 된 배경과 니즈를 듣고 저와 같은 비슷한 상황이시라면 정말 좋은 툴이라고 생각하실 거라고 확신합니다. 만약 아직 Github Blog 초심자라면 Quarto를 한번 고려해보시는 것을 다시 한번 권장해드리며 이만 마치겠습니다.\n\nReference\n\nQuarto Homepage\nHypothesis"
  },
  {
    "objectID": "posts/code/2025-02-16-hydra.html",
    "href": "posts/code/2025-02-16-hydra.html",
    "title": "👩‍💻Hydra로 실험관리 하기",
    "section": "",
    "text": "ML/DL 실험에서는 다양한 실험 파라미터들을 관리해야 합니다. 이를 위해 여러가지 방법이 있지만, 이번 포스팅에서는 FacebookResearch에서 만든 Hydra를 사용해보자 합니다."
  },
  {
    "objectID": "posts/code/2025-02-16-hydra.html#config-초기화",
    "href": "posts/code/2025-02-16-hydra.html#config-초기화",
    "title": "👩‍💻Hydra로 실험관리 하기",
    "section": "Config 초기화",
    "text": "Config 초기화\nHydra에서 설정(Config) 파일을 초기화하는 방법에는 여러 가지가 있습니다. 공식 문서 Initialization methods에서 확인할 수 있듯이, 총 세 가지 방법이 있으며, 각각의 방법을 예제를 통해 살펴보겠습니다.\nHydra의 설정 초기화 방법\n\ninitialize(): 호출하는 코드의 상대 경로를 기준으로 설정 파일을 초기화합니다.\n\ninitialize_config_module(): 절대 경로를 사용하여 설정 모듈(config_module)을 기반으로 초기화합니다.\n\ninitialize_config_dir(): 파일 시스템의 절대 경로를 사용하여 설정 디렉터리(config_dir)를 기반으로 초기화합니다.\n\n이 세 가지 방법은 (1)함수 호출 방식과 (2)컨텍스트(context) 방식으로 사용할 수 있습니다.\n\n함수 호출 방식으로 사용하면 Hydra를 전역적(global)으로 초기화하며, 한 번만 호출해야 합니다.\n반면, 컨텍스트 방식으로 사용하면 특정 블록 내에서만 Hydra를 초기화할 수 있으며, 여러 번 사용할 수도 있습니다.\n\n\n방법1 initialize()\nHydra를 초기화하고 config_path를 설정 검색 경로에 추가합니다.\nconfig_path는 호출하는 코드의 parent 디렉터리를 기준으로 한 상대 경로이며, 이 경우에는 현재 노트북이 위치한 디렉터리를 기준으로 설정됩니다.\n\n%cd /content/\n\nwith initialize(version_base=None, config_path=\"configs\"):\n    # Compose the configuration by selecting the main configuration.\n    cfg_1 = compose(config_name=\"main\")\n\n# full configuration을 YAML 형식으로 출력하여 쉽게 검사할 수 있습니다.\nprint(OmegaConf.to_yaml(cfg_1))\n\n/content\nenv:\n  name: CartPole-v1\n  seed: 42\n  max_steps: 200\nagent:\n  type: DQN\n  hidden_layers:\n  - 64\n  - 64\n  activation: relu\n  learning_rate: 0.001\n  gamma: 0.99\ntraining:\n  episodes: 1000\n  batch_size: 32\n  replay_buffer_size: 10000\n\n\n\n\n\n방법2 initialize_config_module()\nHydra를 초기화하고 config_module을 설정 검색 경로에 추가합니다.\nconfig_module은 반드시 import 가능한 형태여야 하며, 최상위 디렉터리에 __init__.py 파일이 존재해야 합니다. 이번 예제에서는 module이라는 폴더를 만들어서 import 가능한 디렉토리로 만들어 줍니다. 그리고 첫번째 만들었던 main.yaml파일을 복사하여 module/main_2.yaml 파일로 만들어 줍니다.\n\n%cd /content/configs\n%mkdir -p /content/configs/module\n!touch /content/configs/module/__init__.py\n%cd /content\n!cp ./configs/main.yaml configs/module/main_2.yaml\n\n/content/configs\n/content\n\n\n잘 복사가 되었는지 확인해보겠습니다.\n\n!cat /content/configs/module/main_2.yaml\n\n# This is the main configuration file for the RL project.\n# It combines settings for the environment, agent, and training.\n\nenv:\n  name: \"CartPole-v1\"    # Environment name (e.g., Gym environment)\n  seed: 42               # Random seed for reproducibility\n  max_steps: 200         # Maximum steps per episode\n\nagent:\n  type: \"DQN\"            # Agent type (e.g., DQN)\n  hidden_layers: [64, 64] # Network architecture: two hidden layers with 64 units each\n  activation: \"relu\"     # Activation function\n  learning_rate: 0.001   # Learning rate for the optimizer\n  gamma: 0.99            # Discount factor\n\ntraining:\n  episodes: 1000         # Number of training episodes\n  batch_size: 32         # Batch size for learning\n  replay_buffer_size: 10000  # Size of the replay buffer\n\n\n이번에는 2번째 방법인 initialize_config_module()함수를 이용하여 복사했던 main_2.yaml 파일을 이용하여 compose 해보겠습니다.\n\n%cd /content\nwith initialize_config_module(version_base=None, config_module=\"configs.module\"):\n    cfg_2 = compose(config_name=\"main_2\")\n    print(cfg_2)\n\n/content\n{'env': {'name': 'CartPole-v1', 'seed': 42, 'max_steps': 200}, 'agent': {'type': 'DQN', 'hidden_layers': [64, 64], 'activation': 'relu', 'learning_rate': 0.001, 'gamma': 0.99}, 'training': {'episodes': 1000, 'batch_size': 32, 'replay_buffer_size': 10000}}\n\n\n아래와 같이 config의 키들을 확인해볼 수도 있습니다.\n\nlist(cfg_2.keys())\n\n['env', 'agent', 'training']\n\n\n\n\n방법3 initialize_config_dir()\nHydra를 초기화하고 config_path를 설정 검색 경로에 추가합니다. config_path는 파일 시스템 상의 절대 경로여야 합니다. 미리 만들어 놓았던 main.yaml 파일을 이용하여 config를 구성해보겠습니다.\n\nCONFIG_DIR = \"/content/configs\"\n\nwith initialize_config_dir(version_base=None, config_dir=CONFIG_DIR):\n    cfg_3 = compose(config_name=\"main\")\n\n\nprint(cfg_3)\n\n{'env': {'name': 'CartPole-v1', 'seed': 42, 'max_steps': 200}, 'agent': {'type': 'DQN', 'hidden_layers': [64, 64], 'activation': 'relu', 'learning_rate': 0.001, 'gamma': 0.99}, 'training': {'episodes': 1000, 'batch_size': 32, 'replay_buffer_size': 10000}}\n\n\n여기까지 Hydra의 세 가지 방법을 사용하여 설정(Config)을 초기화하는 방법을 살펴보았습니다. 이제 이렇게 생성된 설정이 어떻게 구성되어 있는지 확인해보겠습니다. 대표적인 설정 예제로 cfg_1을 살펴보겠습니다.\n객체의 타입을 확인해보면, 이는 Omegaconf에서 제공하는 DictConfig 객체임을 알 수 있습니다. DictConfig는 딕셔너리 형태의 설정을 계층적으로 관리할 수 있도록 해주는 데이터 구조로, YAML 설정 파일을 로드하거나 동적으로 구성 값을 변경할 때 유용하게 활용됩니다.\n\ntype(cfg_1)\n\n\n    omegaconf.dictconfig.DictConfigdef __init__(content: Union[Dict[DictKeyType, Any], 'DictConfig', Any], key: Any=None, parent: Optional[Box]=None, ref_type: Union[Any, Type[Any]]=Any, key_type: Union[Any, Type[Any]]=Any, element_type: Union[Any, Type[Any]]=Any, is_optional: bool=True, flags: Optional[Dict[str, bool]]=None) -&gt; None/usr/local/lib/python3.11/dist-packages/omegaconf/dictconfig.pyContainer tagging interface\n      \n      \n\n\nConfig 객체의 키들은 다음과 같이 확인할 수 있습니다.\n\nlist(cfg_1.keys())\n\n['env', 'agent', 'training']\n\n\nDictConfig의 키는 config_name.key_name 또는 config_name[\"key_name\"] 형태로 접근할 수 있습니다. 이를 통해 일반적인 딕셔너리처럼 키를 참조하거나 점 표기법(dot notation)을 사용하여 계층적인 설정 값을 쉽게 조회할 수 있습니다.\n\nprint(cfg_1.env == cfg_1[\"env\"])\n\nTrue\n\n\n하위 키들도 동일한 방식으로 접근할 수 있으며, config_name.key_name.sub_key_name 또는 config_name[\"key_name\"][\"sub_key_name\"] 형태로 호출할 수 있습니다. 이를 활용하면 계층적으로 구성된 설정에서 원하는 값을 직관적으로 조회할 수 있습니다.\n\nprint(cfg_1.env.name == cfg_1[\"env\"][\"name\"])\n\nTrue"
  },
  {
    "objectID": "posts/code/2025-02-16-hydra.html#override",
    "href": "posts/code/2025-02-16-hydra.html#override",
    "title": "👩‍💻Hydra로 실험관리 하기",
    "section": "Override",
    "text": "Override\n이번에는 초기화로 만든 config를 override하는 예제를 살펴보겠습니다. 강화학습에서 여러개의 environment를 병렬로 학습하기 위해 env 하위에 num_envs config를 1000개로 추가하는 override를 진행해보겠습니다.\n\n%cd /content/\n\nwith initialize(version_base=None, config_path=\"configs\"):\n    # 기존 설정을 유지하면서 새로운 설정을 추가\n    cfg = compose(config_name=\"main\", overrides=[\"+env.num_envs=1000\"])\n\n# 기존 설정과 오버라이드된 설정이 함께 출력됨\nprint(OmegaConf.to_yaml(cfg))\n\n/content\nenv:\n  name: CartPole-v1\n  seed: 42\n  max_steps: 200\n  num_envs: 1000\nagent:\n  type: DQN\n  hidden_layers:\n  - 64\n  - 64\n  activation: relu\n  learning_rate: 0.001\n  gamma: 0.99\ntraining:\n  episodes: 1000\n  batch_size: 32\n  replay_buffer_size: 10000\n\n\n\noverride는 기존의 main.yaml 파일의 내용을 변경하지 않고 항목을 추가할 수 있습니다. 다시한번 main.yaml 내용을 확인해보면 num_envs가 없음을 알 수 있습니다.\n\n!cat /content/configs/main.yaml\n\n# This is the main configuration file for the RL project.\n# It combines settings for the environment, agent, and training.\n\nenv:\n  name: \"CartPole-v1\"    # Environment name (e.g., Gym environment)\n  seed: 42               # Random seed for reproducibility\n  max_steps: 200         # Maximum steps per episode\n\nagent:\n  type: \"DQN\"            # Agent type (e.g., DQN)\n  hidden_layers: [64, 64] # Network architecture: two hidden layers with 64 units each\n  activation: \"relu\"     # Activation function\n  learning_rate: 0.001   # Learning rate for the optimizer\n  gamma: 0.99            # Discount factor\n\ntraining:\n  episodes: 1000         # Number of training episodes\n  batch_size: 32         # Batch size for learning\n  replay_buffer_size: 10000  # Size of the replay buffer\n\n\n하지만 override된 cfg의 키에는 env.num_envs가 있습니다.\n\ncfg.env.num_envs\n\n1000"
  },
  {
    "objectID": "posts/code/2025-02-16-hydra.html#resolver",
    "href": "posts/code/2025-02-16-hydra.html#resolver",
    "title": "👩‍💻Hydra로 실험관리 하기",
    "section": "Resolver",
    "text": "Resolver\nHydra는 Omegaconf의 Resolver 기능을 활용할 수 있습니다. OmegaConf.register_new_resolver()를 사용하여 커스텀 resolver를 등록하면, 새로운 interpolation 타입을 추가할 수 있으며, 설정(Config) 노드가 접근될 때 해당 resolver가 호출됩니다.\n\nResolver 등록 및 기능\n\neq: 두 문자열을 소문자로 변환한 후, 동일한지 비교합니다.\n\ncontains: 첫 번째 문자열이 두 번째 문자열에 포함되어 있는지 검사합니다.\n\nif: 주어진 조건에 따라 두 값 중 하나를 선택합니다.\n\nresolve_default: 인자가 빈 문자열이면 기본값을 사용하고, 그렇지 않으면 인자 값을 반환합니다.\n\n이러한 resolver를 활용하면 설정 파일 내에서 조건부 로직, 문자열 비교, 기본값 처리 등을 동적으로 적용할 수 있습니다.\n예제로 아래와 같이 Resolver를 등록해보겠습니다.\n\n# Hydra 설정에서 사용할 Resolver들을 등록합니다.\nOmegaConf.register_new_resolver(\"eq\", lambda x, y: x.lower() == y.lower())\nOmegaConf.register_new_resolver(\"contains\", lambda x, y: x.lower() in y.lower())\nOmegaConf.register_new_resolver(\"if\", lambda pred, a, b: a if pred else b)\nOmegaConf.register_new_resolver(\"resolve_default\", lambda default, arg: default if arg == \"\" else arg)\n\n이번 예제에서 사용할 Config는 위의 예제 Config에서 Resolver를 확인하기 위해 아래 내용을 더 추가하여 구성해보겠습니다.\n\nyaml_config = \"\"\"\nenv:\n  name: \"CartPole-v1\"\n  seed: 42\n\nagent:\n  type: \"DQN\"\n  hidden_layers: [64, 64]\n  activation: \"relu\"\n  learning_rate: 0.001 # experiment의 default_lr 기준값으로 설정\n  gamma: 0.99\n\ntraining:\n  episodes: 1000\n  batch_size: ${if:${experiment.is_test}, 128, 32} # 테스트 모드일 경우 128, 아니면 32\n  replay_buffer_size: 10000\n\n# Resolver를 활용한 동적 설정\nexperiment:\n  mode: \"test\"\n  is_test: ${eq:${experiment.mode}, \"test\"}\n  default_lr: ${resolve_default:0.001, ${agent.learning_rate}}\n  is_debug: ${contains:${experiment.mode}, \"debug\"}  # \"debug\" 포함 여부 확인\n\"\"\"\n\nconfig를 로드하고 Resolver가 적용된 값을 출력합니다.\n\ncfg = OmegaConf.create(yaml_config)\n\n# Resolver가 적용된 값 출력\nprint(\"실험 모드:\", cfg.experiment.mode)\nprint(\"is_test:\", cfg.experiment.is_test)\nprint(\"배치 크기:\", cfg.training.batch_size)\nprint(\"default_lr:\", cfg.experiment.default_lr)\nprint(\"디버그 모드 여부):\", cfg.experiment.is_debug)\n\n실험 모드: test\nis_test: True\n배치 크기: 128\ndefault_lr: 0.001\n디버그 모드 여부): False\n\n\n예제 출력을 하나씩 살펴보겠습니다.\n\n${eq:${experiment.mode}, \"test\"} → experiment.mode가 \"test\"이면 True, 아니면 False\n\neq(x, y) Resolver는 두 값을 비교하여 같으면 True, 다르면 False를 반환합니다. ${experiment.mode} 값이 \"test\"인지 확인하는 역할을 합니다.\n\nexperiment:\n    mode: \"test\"\n    is_test: ${eq:${experiment.mode}, \"test\"}\n\n위 설정에서 experiment.mode 값이 \"test\"로 설정되어 있기 때문에, ${eq:${experiment.mode}, \"test\"}는 \"test\"와 \"test\"를 비교하는 형태가 됩니다. eq 함수는 대소문자를 구분하지 않고 두 문자열이 같은지 확인하는 역할을 하므로, \"test\"는 \"test\"와 일치하여 True를 반환합니다. 따라서 experiment.is_test의 값은 True로 설정됩니다.\n반면, 만약 experiment.mode 값이 \"train\"이었다면, ${eq:${experiment.mode}, \"test\"}는 \"train\"과 \"test\"를 비교하게 됩니다. 이 두 값은 서로 다르므로 eq 함수는 False를 반환하게 되고, 결과적으로 experiment.is_test 값은 False로 설정됩니다. 이를 통해 설정 값에 따라 특정 변수를 자동으로 조정할 수 있으며, 이를 활용하면 실험 모드에 따라 설정을 다르게 적용할 수 있습니다.\n\n${if:${experiment.is_test}, 128, 32} → experiment.is_test가 True이면 128, 아니면 32\n\nif(condition, true_value, false_value) Resolver는 condition이 True일 때 true_value를, False일 때 false_value를 반환합니다. experiment.is_test 값이 True인지 확인하여, 이에 따라 다른 값을 할당합니다.\n\ntraining:\n    batch_size: ${if:${experiment.is_test}, 128, 32} # 테스트 모드일 경우 128, 아니면 32\n\n${if:${experiment.is_test}, 128, 32} 구문을 살펴보면, if 함수는 첫 번째 인자로 주어진 조건이 True일 경우 두 번째 인자인 128을 반환하고, False일 경우 세 번째 인자인 32를 반환하는 역할을 합니다. 현재 experiment.is_test가 True이므로 if(True, 128, 32)는 128을 반환하고, 결과적으로 training.batch_size 값이 128이 됩니다.\n반면, 만약 experiment.mode가 \"train\" 등 다른 값으로 설정되어 있다면, eq(\"train\", \"test\")의 결과는 False가 되어 experiment.is_test가 False로 설정됩니다. 이 경우, if(False, 128, 32)는 False에 해당하는 세 번째 값인 32를 반환하게 되며, training.batch_size 값이 32로 설정됩니다.\n이러한 방식은 training과 test에서 배치 크기를 다르게 설정할 때 유용합니다. 예를 들어, 테스트 환경에서는 더 큰 배치 크기를 사용하여 빠르게 결과를 확인하고, 훈련 환경에서는 적절한 배치 크기를 유지하여 안정적인 학습이 가능하도록 조정할 수 있습니다. 이를 통해 설정 파일을 동적으로 관리할 수 있으며, 실험 조건에 따라 유연하게 설정을 변경할 수 있습니다.\n\n${resolve_default:0.001, ${agent.learning_rate}} → agent.learning_rate가 설정되지 않았으면 기본값 0.001 사용\n\nresolve_default(default, arg) Resolver는 arg 값이 비어 있거나 설정되지 않았을 경우 default 값을 반환합니다.\nagent.learning_rate 값이 존재하면 그대로 사용하고, 없다면 기본값 0.001을 사용합니다.\n\nagent:\n    learning_rate: 0.001 # experiment의 default_lr 기준값으로 설정\nexperiment:\n    default_lr: ${resolve_default:0.001, ${agent.learning_rate}}\n\n이 방식은 설정 파일에서 특정 값이 누락되었을 때 기본값을 자동으로 적용하는 데 매우 유용합니다. 예를 들어, learning_rate 값을 실험마다 다르게 설정할 수 있도록 설정 파일에서 값을 명시적으로 지정할 수도 있지만, 실수로 빠뜨렸을 경우에도 resolve_default를 사용하면 안전하게 기본값을 사용할 수 있습니다. 이를 통해 설정을 더욱 견고하게 만들고, 코드의 예외 처리를 간결하게 할 수 있습니다.\n\n${contains:${experiment.mode}, \"debug\"} → experiment.mode에 “debug”라는 글자가 포함되어 있는지 여부 확인\nexperiment:\nmode: \"test\"\nis_debug: ${contains:${experiment.mode}, \"debug\"}  # \"debug\" 포함 여부 확인\n\nexperiment.mode 값이 \"test\"라면 \"debug\"가 포함되지 않았으므로 contains(\"test\", \"debug\")는 False를 반환합니다.\n만약 experiment.mode 값이 \"test_debug\"라면 \"debug\"라는 문자열이 포함되어 있으므로 contains(\"test_debug\", \"debug\")는 True를 반환합니다.\n이를 통해 실험 모드에 따라 자동으로 디버깅 기능을 활성화하거나 로그 출력을 조정할 수 있습니다.\n\n\n이러한 Resolver 기능을 활용하면 설정 파일을 더욱 동적으로 관리할 수 있습니다!"
  },
  {
    "objectID": "posts/code/2023-06-18-chord.html",
    "href": "posts/code/2023-06-18-chord.html",
    "title": "👩‍💻Chord Graph",
    "section": "",
    "text": "이전 Accessibility-Based Clustering for Efficient Learning of Locomotion Skills 논문을 리뷰하면서 로봇의 static pose들을 가지고 K-Acc Clustering하는 과정 이후에 Clustering Analysis에서 Inter-cluster accessibility를 Visulization을 하는 부분이 있었습니다.\n오른쪽에 보이는 그래프가 Chord Graph인데 각 Top-20 cluster에 속한 sample pose들을 하나의 node로 표현하고 각 sample pose들이 다른 pose로 transition되는 시간을 기반으로 계산된 accessiblity 값이 높은 부분은 진한 edge로 accessibility가, 낮은 부분은 옅은 edge로 시각화하여 포즈들 간의 관계성을 보여줍니다. 따라서 이런 시각화를 통해 cluster 간의 inter-cluster accessibility를 파악할 수 있는 것 입니다.(자세한 내용은 이전 논문 리뷰 포스팅을 참고 바랍니다.) 이번 포스팅은 바로 이 Chord graph를 Holoviews라는 파이썬 패키지를 이용해서 시각화 하는 방법에 대해 다룰 것 입니다."
  },
  {
    "objectID": "posts/code/2023-06-18-chord.html#accesstimetable",
    "href": "posts/code/2023-06-18-chord.html#accesstimetable",
    "title": "👩‍💻Chord Graph",
    "section": "AccessTimeTable",
    "text": "AccessTimeTable\n샘플링한 Static poses들을 2000개를 poses.pickle 데이터로 저장해놨습니다. 각 pose-to-pose를 PD tracking을 하며 걸리는 시간을 측정하게 되는데 pose-to-pose로 transition되는 시간은 1초가될 수 있도록 joint trajectory를 만들어주고 PD제어를 하면서 0.0025초 마다 destination pose로 도달했는지(시뮬레이터의 dt)를 체크합니다. 이때 무한정 시간을 잴 수는 없기 때문에 10초로 시간을 제한하여 최대 10초까지만 걸리는 시간을 기록하게 됩니다.\n\n이 정보가 총 2000개 샘플 포즈에 대해서 1:1로 모두 구해야 하기때문에 병렬계산을 해서 저장하여 총 75개의 npy데이터로 나누어 계산하였고 이를 2000 by 2000 매트릭스로 만들어서 AccessTimeTable을 시각화하면 아래와 같이 그려집니다.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport pickle5 \nwith open('../poses.pickle', 'rb') as handle: poses = pickle5.load(handle)\n# 불러올 npy 파일 이름들을 리스트로 만들어줍니다.\naccessTimeFiles = ['accessTimeTableAidin%d.npy'%x for x in range(75)]\n\n\ntimeMat = np.sum([np.load(f) for f in accessTimeFiles], axis=0)\n# 자기자신 pose로부터 자기자신의 pose로의 시간은 0초로 처리하기 위해 대각선의 매트릭스 값은 0으로 일괄 처리합니다.\ntimeMat[ range(len(timeMat)), range(len(timeMat)) ]= 0 \n\n\ntimeMat # shape: (2000, 2000)\n\narray([[ 0.   ,  1.245, 10.   , ...,  1.145, 10.   , 10.   ],\n       [10.   ,  0.   ,  1.555, ...,  0.955,  1.31 , 10.   ],\n       [10.   ,  3.09 ,  0.   , ..., 10.   , 10.   , 10.   ],\n       ...,\n       [10.   ,  0.965,  2.035, ...,  0.   , 10.   , 10.   ],\n       [ 1.345,  1.015, 10.   , ...,  1.145,  0.   , 10.   ],\n       [10.   , 10.   ,  1.4  , ..., 10.   , 10.   ,  0.   ]])\n\n\n\nfig = plt.figure(figsize=(8,8))\nplt.imshow(timeMat)\nplt.title(\"Time Matrix\")\nplt.colorbar()\nplt.show()\nplt.close()\n\n\n\n\n\n\n\n\n10초 이하로 측정되었던 time data와 10초 이상으로 측정된 time data의 수를 살펴보면 아래와 같습니다.\n\nnp.sum(timeMat &lt; 9.999)\n\n907572\n\n\n\nnp.sum(timeMat &gt;= 9.999)\n\n3092428"
  },
  {
    "objectID": "posts/code/2023-06-18-chord.html#accessibility-histogram",
    "href": "posts/code/2023-06-18-chord.html#accessibility-histogram",
    "title": "👩‍💻Chord Graph",
    "section": "Accessibility Histogram",
    "text": "Accessibility Histogram\n이러한 timeMat를 Accessiblity 공식에 맞게 다시 계산하게 됩니다. 이때 10초 이상이 되는 데이터는 1e-8으로 만들어서 가장 낮은 accessiblity 점수를 얻도록 처리합니다.\n\nacc_matrix = (timeMat &lt; 9.999) * np.exp(-timeMat/10) + (timeMat &gt;= 9.999) * 1e-8\n\n히스토그램으로 Accessibility를 시각화하면 다음과 같습니다.\n\nvalues = np.reshape(acc_matrix,(-1,)) # 2000 x 2000 = 4000k\nax = plt.hist(values, bins=50, range=(-1e-4,1.1))\nplt.show()\n\n\n\n\n\n\n\n\n무한대 시간이 걸렸던 부분을 제외하고 히스토그램을 그려보면 아래와 같습니다.\n\nax = plt.hist(values, bins=50, range=(+1e-4,1.1))\nplt.show()"
  },
  {
    "objectID": "posts/code/2023-06-18-chord.html#k-acc-algorithm",
    "href": "posts/code/2023-06-18-chord.html#k-acc-algorithm",
    "title": "👩‍💻Chord Graph",
    "section": "K-Acc algorithm",
    "text": "K-Acc algorithm\n계산한 Accessibility 값을 기준으로 K-Acc 알고리즘으로 centroid pose와 적절한 centroid 수를 결정하게 됩니다. 논문에서 소개된 K-Acc 알고리즘은 원저자가 공개한 코드를 그대로 사용하여 AiDIN-VIII 데이터에 적용했습니다.\n\n\n\n\n\n\nK-Acc 알고리즘을 수행하는 K_access 클래스 살펴보기\n\n\n\n\n\nclass K_access:\n    def __init__(self, access, k=2, seed=123):\n        self.seed = seed\n        np.random.seed(self.seed)\n        self.k = k # 클래스 수 \n        self.access = access # weight_matrix\n        self.node_num = len(access) \n        self.access[range(self.node_num),range(self.node_num)] = 1 # 대각 성분을 1로\n        self.core_index = np.zeros((k,), dtype=int) \n        self.core_index[0] = np.random.randint(0, self.node_num, (1,)) # 클래스 범위 내에 랜덤한 정수를 코어 인덱스로 설정\n\n        for i in range(1, k):\n            ready_core_access = np.sum(self.access[self.core_index[:i],:], axis=0) \\\n                                + np.sum(self.access[:,self.core_index[:i]], axis=1)\n            ready_core_access[self.core_index[:i]] += 999999 # accessible to self\n            self.core_index[i] = np.argmin(ready_core_access) # the one that is the farthest from \n            \n        self.assignment = np.zeros((self.node_num,),dtype=int)\n        self.labels = np.zeros((self.node_num,),dtype=int)\n        self.cores_sorted = np.zeros((self.node_num,),dtype=int)\n        self.max_iter = 10000\n        \n        self.assign()        \n            \n    def assign(self):\n        # from core to nodes\n        core_access = self.access[self.core_index] \n        # argmax access(core,node)\n        self.assignment = self.core_index[np.argmax(core_access, axis = 0)] \n        for c in self.core_index:\n            self.assignment[c] = c  # self belongs to self\n        return\n    \n    def update(self):\n        for i in range(self.k):\n            Si = np.where(self.assignment == self.core_index[i])[0]\n            access_Si = self.access[Si,:][:,Si]\n            minaccess_Si = np.min(access_Si,axis=1)\n            self.core_index[i] = Si[np.argmax(minaccess_Si)]\n        return        \n    \n    def fit(self):\n        pre_assignment = np.zeros((self.node_num,),dtype=int) - 1\n        iter_ = 0\n        while np.sum(np.abs(self.assignment - pre_assignment)) != 0 and iter_ &lt; self.max_iter :\n            pre_assignment = self.assignment\n            iter_ += 1\n            self.update()\n            self.assign()\n        return iter_\n            \n    def predict(self):\n        map_ = {}\n        cnt_Si = []\n        for i in range(self.k):\n            cnt_Si.append(len(np.where(self.assignment == self.core_index[i])[0]))\n        sorted_index = (9999 - np.array(cnt_Si)).argsort()  # descending by # samples in clu\n        for i in range(self.k):\n            map_[self.core_index[sorted_index[i]]] = i+1 # from 1 to # clu\n        self.labels = [map_[c] for c in self.assignment]\n        cores_sorted = self.core_index[sorted_index]\n        return self.labels, cores_sorted \n    \n    def inter_access(self):\n        cnt_Si = []\n        for i in range(self.k):\n            cnt_Si.append(len(np.where(self.assignment == self.core_index[i])[0]))\n        sorted_index = (9999 - np.array(cnt_Si)).argsort()  # descending by # samples in clu\n        all_to_C = self.access[:,:][:,self.core_index[sorted_index]]\n        inter_ = np.zeros((self.k,self.k))\n        for i in range(self.k):\n            Si = np.where(self.assignment == self.core_index[sorted_index[i]])[0]\n            Si_to_C = all_to_C[Si,:]\n            inter_[i,:] = np.mean(Si_to_C,axis = 0)\n            inter_[i,i] = 1\n        return inter_\n    \n    def intra_access(self):\n        intra_ = np.zeros((self.k,))\n        cnt_Si = []\n        for i in range(self.k):\n            cnt_Si.append(len(np.where(self.assignment == self.core_index[i])[0]))\n        sorted_index = (9999 - np.array(cnt_Si)).argsort()  # descending by # samples in clu\n        for i in range(self.k):\n            Si = np.where(self.assignment == self.core_index[sorted_index[i]])[0]\n            intra_[i] = np.min(self.access[self.core_index[sorted_index[i]],Si],axis=0)\n        return intra_\n    \n    def evaluate(self):\n        intra_ = np.mean(np.log(self.intra_access()))\n        inter_ = np.mean(np.log(self.inter_access()))\n        cnt_Si = []\n        for i in range(self.k):\n            cnt_Si.append(len(np.where(self.assignment == self.core_index[i])[0]))\n        num_one_sample_cluster = len(np.where(np.array(cnt_Si)==1)[0])\n        alpha = 1\n        # index I\n        # larger is better\n        score_ = intra_ - inter_ - alpha * num_one_sample_cluster \n        return score_\n\n\n\nK_access 클래스를 가지고 최적의 클래스 수를 선정하기 위해 fit을 클래스 수를 늘려가며 수행합니다. 그랬을 때 155개의 centroid cluster를 가졌을 때 가장 index 점수가 높아 최적의 클래스 수를 선정할 수 있었습니다.\n\nscores = []\nn_cls = range(1,201) # 클래스의 수 1 ~ 200까지 조사\nfor num_class in n_cls:\n    k = K_access(acc_matrix, num_class)\n    k.fit()\n    scores.append(k.evaluate())\n    \nfig,ax = plt.subplots(figsize=(10, 10*9/16))\nmax_ind = np.argmax(scores)\nprint(max_ind+1, scores[max_ind]) # 최적의 클래스 수, 그때의 인데스 점수\n\nplt.plot(n_cls, scores, marker='o', markersize=1)\nplt.plot([max_ind+1],[scores[max_ind]],marker='o',c='r',markersize=2)\nplt.xlabel('Number of Clusters')\nplt.ylabel('Index Value')\nplt.xlim([0,170])\nplt.show()\n\n155 14.17499592680445\n\n\n\n\n\n\n\n\n\n선정된 클래스 수를 가지고 다시한번 클러스터링 작업을 거져 각 centroid pose인 cores에 대한 정보와 inter_access, intra_access 점수를 가져올 수 있습니다.\n\nnum_class = max_ind+1\nk = K_access(acc_matrix, num_class)\nk.fit()\nclusters, cores = k.predict()\ninter_access = k.inter_access()\nintra_access = k.intra_access()\n\n\ncores # centroid pose의 ID\n\narray([1213, 1043, 1150, 1555,  112,  100, 1032,  121,  140, 1097, 1116,\n       1554, 1377, 1623,   33,   41,  250,   29, 1272, 1926, 1401,   22,\n       1888,  809, 1262,  165, 1513,  108, 1248,  333, 1008,  330, 1081,\n        157,  419, 1227, 1231, 1244,  564,  392,  543,   18, 1785,  227,\n       1288, 1644, 1715,  398, 1527, 1017,  169, 1056,  139,  323, 1848,\n       1121, 1067,  225,  476,  450,  898,    8, 1492, 1223,  467, 1844,\n       1608, 1803, 1839, 1913, 1015,   96, 1020,  627, 1526,  691,  268,\n       1899, 1521, 1787,    3, 1597,  210,  643,  502, 1358,  209,  798,\n       1887, 1216,  332,  972, 1122,  404,  343, 1423,  363,  173, 1544,\n          5, 1817,  960,   72, 1832,  853,  446,  479,  395,  650,  313,\n       1587,  677,  239, 1089,  464,  891, 1029, 1491, 1477,  761,  860,\n       1352, 1564,  938,  645, 1254,   50, 1149,  453, 1791,  777, 1990,\n       1737,   47, 1220, 1537,  315, 1180,  162, 1277, 1568,  910,  528,\n        523, 1361,  709, 1718, 1045,  618,   86, 1889, 1250,  455,  814,\n        172])\n\n\n각 클러스터마다 포함하고 있는 샘플들의 수는 어떻게 분포하고 있을까요? 히스토그램으로 시각화를 해보았습니다. 각 샘플들의 수는 이후에 chord diagram의 노드가 될 것 입니다.\n\n# figure 설정\nplt.rcParams['lines.linewidth']=0.7\nplt.rcParams['xtick.direction']='in'\nplt.rcParams['ytick.direction']='in'\nplt.rcParams['xtick.major.width']=0.4\nplt.rcParams['ytick.major.width']=0.4\nplt.rcParams['xtick.major.size']=2\nplt.rcParams['ytick.major.size']=2\nfig,ax = plt.subplots(figsize=(20, 5))\nax.spines['right'].set_visible(False)\nax.spines['top'].set_visible(False)\n\nax_ = plt.hist(np.array(clusters)-0.5,\n               range = [0.5, np.max(clusters)+0.5],\n               bins = np.max(clusters),\n               edgecolor='dimgray',\n               color='deepskyblue',\n               rwidth=1,\n               alpha=0.5)\n\nax = plt.xticks(range(1,1+np.max(clusters),5))\nplt.xlabel('Cluster ID',fontsize=15)\nplt.ylabel('Number of Samples',fontsize=15)\nplt.show()"
  },
  {
    "objectID": "posts/code/2023-06-18-chord.html#get-kacc-poses",
    "href": "posts/code/2023-06-18-chord.html#get-kacc-poses",
    "title": "👩‍💻Chord Graph",
    "section": "Get Kacc poses",
    "text": "Get Kacc poses\n알고리즘을 통해 선정된 centroid pose들에 대한 정보를 한번 확인해보겠습니다. 각 pose에 대한 정보는 아래와 같은 형식으로 정리해서 가장 많은 샘플 수를 포함하고 있는 top-3 centroid pose를 확인해보겠습니다.\n[\n([0,0,height], [ 12 joints], [roll, pitch, 0] ),\n]\n\nkaccess_config = []\nfor i in range(np.max(clusters)):\n    centroid_pose = poses[cores[i]]\n    ele1 = [0,0, centroid_pose['store_height']]\n    ele2 = centroid_pose['store_joints']\n    ele3 = list(centroid_pose['store_roll_pitch'])+[0]\n    ele = (ele1,ele2,ele3)\n    kaccess_config.append(ele)\n\n\n\n\n\n\n\nTop-3 Centroid pose 정보 확인하기\n\n\n\n\n\n[([0, 0, 0.30607859912101715],\n  [0.6053835299386103,\n   1.4824030607413299,\n   -0.0809348638895901,\n   0.3995084366561768,\n   1.2970169955948037,\n   -0.7264673491159409,\n   0.05388458488859648,\n   0.33537195041378626,\n   -0.24038702044129037,\n   -0.501582056954722,\n   -1.5199349541933391,\n   -0.44053023354145876],\n  [3.141592551130548, 0.37124208223469835, 0]),\n ([0, 0, 0.12894579304472167],\n  [0.05857655877237639,\n   -1.4077772309827037,\n   -0.7778225275558831,\n   0.1038327197157696,\n   -0.7770055977003666,\n   -0.9684446036207767,\n   0.06583156564999947,\n   -0.903642551883172,\n   0.3522403885013049,\n   -0.6108687315594281,\n   0.7353928319105865,\n   -0.21053293619245944],\n  [-0.412046597574257, 0.043837823368155976, 0]),\n ([0, 0, 0.2968457946843744],\n  [0.06337843283646641,\n   2.7942809454055157,\n   -0.9737491947994557,\n   -0.2517691066931858,\n   -2.624845950050974,\n   -0.18089990597943467,\n   -0.17501576141383207,\n   0.5877695591500649,\n   -0.06264699708493802,\n   -0.2039006032557608,\n   -1.193152752838448,\n   -0.4222694151347171],\n  [-1.3562253990942323, 0.0010176328562240375, 0])]"
  },
  {
    "objectID": "posts/code/2023-06-18-chord.html#chord-graph",
    "href": "posts/code/2023-06-18-chord.html#chord-graph",
    "title": "👩‍💻Chord Graph",
    "section": "Chord Graph",
    "text": "Chord Graph\n이제 본격적으로 pose 데이터를 가지고 chord graph를 그려보겠습니다. pickle 데이터로 저장되어 있는 pose 데이터들 중 2000개의 데이터를 가지고 dataframe 객체로 만들어줍니다. 마지막으로 clustering 과정에서 구한 각 포즈 데이터가 속해있는 cluster의 ID를 데이터프레임의 열을 추가하여 정보를 추가해줍니다.\n\ndf_poses = pd.DataFrame(poses)\ndf_poses_2000 = df_poses[:2000]\ndf_poses_2000[\"cluster\"] = clusters\n\n\ndf_poses_2000.head(3)\n\n\n\n\n\n\n\n\nstore_height\nstore_roll_pitch\nstore_joints\nstore_links\ncluster\n\n\n\n\n0\n0.300289\n(2.150537560664197, -0.004430010187345012)\n[-0.5810368941034365, -1.206576076316329, -0.0...\n[(-0.17671623826026917, -0.23643925786018372, ...\n22\n\n\n1\n0.067273\n(-0.00028379763432160346, 2.8617441250464685e-05)\n[0.15361879275773346, -2.083050326944349, -1.1...\n[(0.14308467507362366, -0.12229745090007782, 0...\n77\n\n\n2\n0.091594\n(-0.21762631665841028, -1.16469456094937e-05)\n[0.1518267329947646, -1.165616939584498, -0.07...\n[(-0.08804446458816528, -0.04484516754746437, ...\n129\n\n\n\n\n\n\n\n항공편 예제에서도 살펴보았듯이 모든 cluster를 시각화하는 것은 의미가 없기 때문에 Top20 cluster에 속해있는 데이터들만 처리하기 위해서 데이터를 전처리하는 과정이 필요합니다. 우선 각 pose 데이터가 source(출발노드) 가 될수도 있고 target(도착노드) 이 될 수도 있기 때문에 data_id라는 변수를 통해 기준 데이터(pose A) 와 페어 데이터(pose B) 를 튜플로 묶어준 리스트를 생성합니다.\n\ndata_id = [(x+1, y+1) for x in range(2000) for y in range(2000)]\n\n각 기준 데이터와 페어 데이터에 대해서 각 데이터가 속해있는 클러스터 아이디를 확장해서 저장해줍니다. 데이터들을 확인하기 위해 index 998:1004범위에 있는 값들을 확인합니다.\n\n# 기준 데이터(pose A)\nid_list = [x[0] for x in data_id]\nprint(id_list[998:1004])\n\n# 페어 데이터(pose B)\npairs = [x[1] for x in data_id]\nprint(pairs[998:1004])\n\n# 기준 데이터의 클러스터 아이디를 확장 \nclusters_expand = [clusters[x//2000] for x in range(2000*2000)]\nprint(clusters_expand[998:1004])\n\n# 페어 데이터의 클러스터 아이디를 확장\npair_cluster = clusters * 2000\nprint(pair_cluster[998:1004])\n\nassert len(id_list)==len(clusters_expand) == len(pairs) == len(values) == len(pair_cluster) # values는 acc 값\n\n[1, 1, 1, 1, 1, 1]\n[999, 1000, 1001, 1002, 1003, 1004]\n[22, 22, 22, 22, 22, 22]\n[1, 103, 21, 40, 59, 61]\n\n\n다음으로 chord 그래프를 위한 데이터 프레임 객체 df_chord를 만들고 칼럼을 재정렬해줍니다.\n\ndf_chord = pd.DataFrame({\"id\":id_list,\n                         \"cluster\":clusters_expand,\n                         \"pair\":pairs,\n                         \"acc\":values,\n                         \"pair_cluster\":pair_cluster})\n\n# column 재정렬\ndf_chord = df_chord[[\"id\", \"cluster\", \"pair\", \"pair_cluster\", \"acc\"]]\n\n\ndf_chord[998:1004]\n\n\n\n\n\n\n\n\nid\ncluster\npair\npair_cluster\nacc\n\n\n\n\n998\n1\n22\n999\n1\n1.000000e-08\n\n\n999\n1\n22\n1000\n103\n1.000000e-08\n\n\n1000\n1\n22\n1001\n21\n1.000000e-08\n\n\n1001\n1\n22\n1002\n40\n1.000000e-08\n\n\n1002\n1\n22\n1003\n59\n1.000000e-08\n\n\n1003\n1\n22\n1004\n61\n8.589883e-01\n\n\n\n\n\n\n\n기준 데이터의 클러스터 기준으로 상위 20개의 클러스터에 속해 있는 데이터들만 남기기는 과정을 진행합니다. 클러스터의 아이디는 크기순 정렬이기 때문에 1~20까지의 클러스터 아이디만 남기면 상위 20개의 클러스터에 속한 포즈 데이터들만 남게 됩니다.\n\ndf_chord.drop(df_chord[(df_chord['cluster']&gt;20)].index, inplace=True)\n\n페어 데이터의 클러스터 기준으로도 상위 20개의 클러스터에 속해 있는 데이터들만 남기는 과정을 똑같이 진행합니다.\n\ndf_chord.drop(df_chord[(df_chord['pair_cluster']&gt;20)].index, inplace=True)\n\n\ndf_chord.head(5)\n\n\n\n\n\n\n\n\nid\ncluster\npair\npair_cluster\nacc\n\n\n\n\n8004\n5\n12\n5\n12\n1.000000e+00\n\n\n8006\n5\n12\n7\n18\n1.000000e-08\n\n\n8007\n5\n12\n8\n8\n1.000000e-08\n\n\n8011\n5\n12\n12\n13\n1.000000e-08\n\n\n8025\n5\n12\n26\n12\n9.080099e-01\n\n\n\n\n\n\n\nholoviews 패키지를 불러와서 chord graph를 그리기 위한 준비를 합니다. holoviews는 시각화 라이브러리 백엔드를 선택할 수 있는데 interaction이 가능한 bokeh 백엔드를 선택했습니다.\n\nimport holoviews as hv\nfrom holoviews import dim\nhv.extension('bokeh') # backend engine 선택\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n\n\n\n\n\n\n\n항공편 예제에서도 groupby를 이용해서 edge 수를 집계했듯이 pose 데이터셋에 대해서도 acc수를 cluster와 pair_cluster를 기준으로 집계해서 edge 정보를 정리해줍니다.\n\nedges = df_chord.groupby([\"cluster\", \"pair_cluster\"]).acc.count().reset_index()\n\n\nedges\n\n\n\n\n\n\n\n\ncluster\npair_cluster\nacc\n\n\n\n\n0\n1\n1\n1764\n\n\n1\n1\n2\n1764\n\n\n2\n1\n3\n1596\n\n\n3\n1\n4\n1302\n\n\n4\n1\n5\n1218\n\n\n...\n...\n...\n...\n\n\n395\n20\n16\n675\n\n\n396\n20\n17\n650\n\n\n397\n20\n18\n650\n\n\n398\n20\n19\n650\n\n\n399\n20\n20\n625\n\n\n\n\n400 rows × 3 columns\n\n\n\n다음으로 공항정보를 저장했듯이 pose데이터가 속해있는 cluster의 아이디를 각 centroid pose(cp_N) 이름으로 매칭해서 저장해줍니다. 각 노드(pose 데이터)가 속해있는 cluster_id를 기준으로 edge 데이터셋에서 cluster와 매칭되는 것을 알 수 있습니다.\n\n# node\nnodes = pd.DataFrame({\"cluster_id\":[x+1 for x in range(20)],\n                     \"name\":[\"cp_{}\".format(i+1) for i in range(20)]})\nnode_dataset = hv.Dataset(nodes, \"cluster_id\", \"name\" )\n\n\nnodes.head(3)\n\n\n\n\n\n\n\n\ncluster_id\nname\n\n\n\n\n0\n1\ncp_1\n\n\n1\n2\ncp_2\n\n\n2\n3\ncp_3\n\n\n\n\n\n\n\n\nchord = hv.Chord((edges, node_dataset),\n                 [\"cluster\", \"pair_cluster\"],\n                 ['acc'])\n\n\nchord\n\n\n\n\n\n  \n\n\n\n\n마찬가지로 옵션을 추가하여 chord diagram을 좀 더 보기 좋게 만들어서 export 해보겠습니다.\n\n%%opts Chord [height=600 width=600]\n\nchord.opts(\n    cmap='Category20',\n    labels='name',\n    edge_color=dim(\"cluster\").astype(str),\n    edge_alpha=0.7,\n    node_color=dim(\"cluster_id\").astype(str))\n\n\n\n\n\n  \n\n\n\n\n이렇게 해서 완성한 K-Acc Chord Diagram을 각 centroid pose와 함께 시각화를 하면 아래 그림과 같이 됩니다!\n\nConclusion\n그동안 데이터 시각화를 위한 코딩작업을 자주하지 않아서 논문에 있는 그림을 하나 따라하기까지 정말 오랜시간이 걸렸던 것 같습니다. 적절한 패키지를 서칭하는 것부터 시작해서 해당 패키지를 어떻게 사용해야 원하는 그림을 뽑을 수 있는지까지 한 과정마다 많은 고민과 연습이 필요했지만 마지막에 원하는 시각화 자료를 뽑을 수 있어서 뿌듯했던 것 같습니다. 데이터 시각화 과정이 연구를 하는 입장에서는 가장 마지막에 설득과 확인의 과정에 필요한 자료라서 소홀히 하기 쉬운데 논문의 결과를 더 빛낼 수 있는 중요한 과정이라는 것을 이번 기회에 또 한번 느낄 수 있었던 것 같습니다. 생소한 그래프 형식과 적용이 만만치는 않았지만 정말 의미있던 과정이었고 Chord diagram이 필요한 그 누군가에게 도움이 되었기를 바라며 이번 포스팅을 마치겠습니다.\nReference\n\nCHORD DIAGRAM\nChord diagram (information visualization)\nHoloviews - Chord\nHoloviews - Route Chord"
  },
  {
    "objectID": "posts/project/active_learning.html",
    "href": "posts/project/active_learning.html",
    "title": "Active Learning Algorithm for Object Detection and Segmentation",
    "section": "",
    "text": "tbd"
  },
  {
    "objectID": "posts/project/pulse_rl.html",
    "href": "posts/project/pulse_rl.html",
    "title": "Deep Reinforcement learning for DME Pulse Design",
    "section": "",
    "text": "tbd"
  },
  {
    "objectID": "posts/project/pick-gpt.html",
    "href": "posts/project/pick-gpt.html",
    "title": "Pick-GPT",
    "section": "",
    "text": "tbd"
  },
  {
    "objectID": "posts/project/get_off_prediction.html",
    "href": "posts/project/get_off_prediction.html",
    "title": "Self-driving Public Mobility Get-off Safety System",
    "section": "",
    "text": "https://github.com/curieuxjy/Safe_Goodbye"
  },
  {
    "objectID": "posts/storage/2020-07-17-rl-study-roadmap.html",
    "href": "posts/storage/2020-07-17-rl-study-roadmap.html",
    "title": "🧩My Reinforcement Learning Roadmap",
    "section": "",
    "text": "강화학습을 공부했던 roadmap을 기록하고자 글을 쓰게 되었다.\n짧게 기록할 수 있는 정보들과 내 느낌들을 간략하게 적어보고자 한다."
  },
  {
    "objectID": "posts/storage/2020-07-17-rl-study-roadmap.html#my-rl-study-road-map",
    "href": "posts/storage/2020-07-17-rl-study-roadmap.html#my-rl-study-road-map",
    "title": "🧩My Reinforcement Learning Roadmap",
    "section": "My RL study road map",
    "text": "My RL study road map\n\n아래는 시간 순서대로 내가 공부했던 강의나 책, 스터디 모임\n\n\n모두의 rl\n케라스로 시작하는 rl 책 스터디\n모두연 starcraft project - 논문 리딩\nUdacity\nConnect-X\npytorch 책\n수학 강화학습 책\nOpen AI Gym\nfast campus\nBNM2h 스터디\nRL 논문 스터디 3기\nUnity Ml agent 스터디\nHugging face 강화학습 강의"
  },
  {
    "objectID": "posts/storage/2020-07-17-rl-study-roadmap.html#materials",
    "href": "posts/storage/2020-07-17-rl-study-roadmap.html#materials",
    "title": "🧩My Reinforcement Learning Roadmap",
    "section": "Materials",
    "text": "Materials\n\n무료강의\n\n\nHuggiing Face - DRL course: 허깅페이스에서 제공하는 강화학습 강의. 역시 허깅 페이스. 개인적으로 Udacity보다 더 좋은 것 같다.\n모두의 RL: 강화학습 입문용 강의. 가볍게 듣고 시작하는 것을 추천.\n팡요랩 : 강화학습 강의로 유명한 데이비드 실버 교수님 강의 한글 버전\n혁펜하임 : 수식적으로 파고 들어가고 싶다면 추천.\nCS285 : 입문이라기에 조금 어려울 수는 있으나 만약 로봇틱스쪽을 생각하고 있다면 꼭 들어야하는 강의.\nDeepMind - Reinforcement Learning Lecture Series 2021 (2022.05.06 updated)\n\n\n유료강의\n\n\nUdacity : 엄청 비쌈, 프로그램이 좋긴한데 nano degree가 목적이 아니라면 추천하지 않음.\nUdemy : 코드 위주로 빨리 배워보고 싶을 때. 어느정도 강화학습에 대한 기본 지식이 있을 때 듣는 것이 좋음.\nFast campus 박준영 강사님 : 무엇보다 한국어로 자세히 설명도 해주시고 코드도 있어서 좋음. 개인적으로 무료 강의들로 기초를 한번 다지고 난 후 이 강의를 들으면서 불명확했던 부분을 짚으면 좋은 것 같다.\n\n\n책\n\n\n파이썬과 케라스로 배우는 강화학습\nPyTorch를 활용한 강화학습/심층강화학습 실전 입문\n수학으로 풀어보는 강화학습 원리와 알고리즘 : 수학적 기반 다지기. 모델 베이스드 RL 쪽으로 내용이 좋음.\n단단한 강화학습 : 강화학습 대가 리처드 서튼 교수님의 바이블 원작을 번역.\n단단한 심층강화학습 (2022.05.06 updated)\n\n\n2022.05.06 기준 강화학습 관련된 많은 책들이 나와서 책에 대한 추천은 최근 출간된 책들을 목차들을 보고 본인에게 필요한 부분을 공부하는 것을 추천.\n\n\n웹사이트(입문으로는 조금 힘들 수 있으나 어느정도 공부한 후, 트렌드나 흐름 잡기에 좋음)\n\n\nDeepMind blog\nOpenAI blog\nOpenAI Spinning up\n\n\nGithub\n\n\nRL 가계도\nHuggingface DRL: 위에 무료강의 중 허깅페이스 강의에서 사용하는 코드들이 올라와 있음.\npg-is-all-you-need: Policy Gradient 계열의 알고리즘들 코드. 최근 25.01 기준 리펙토링 되었음\n\n\n커뮤니티\n\n\nRL Korea : 페이스북\nRL.start() : 오픈 카카오톡"
  },
  {
    "objectID": "posts/storage/2021-01-02-GNN-materials.html",
    "href": "posts/storage/2021-01-02-GNN-materials.html",
    "title": "🧩GNN Materials",
    "section": "",
    "text": "GNN에 관심을 가지게 된 계기는 RoboGrammar라는 paper였다. 예전부터 하고 싶었던 Robot design 아이디어를 GNN을 가지고 실현시킨 것이 너무 신기해서 공부해보고 싶었다. 이번 포스팅에서는 GNN과 첫만남인 만큼 공부할 자료들을 정리해보려 한다."
  },
  {
    "objectID": "posts/storage/2021-01-02-GNN-materials.html#materials",
    "href": "posts/storage/2021-01-02-GNN-materials.html#materials",
    "title": "🧩GNN Materials",
    "section": "Materials",
    "text": "Materials\n\nTobigs Graph Study\nCS224W: Machine Learning with Graphs / Videos\nGraph Neural Networks - Penn Engineering\nTF Graph Neural Network Samples\nGraph Neural Networks in TF2\nGraph Representation Learning(Pytorch)\nA Gentle Introduction to Graph Neural Networks (Basics, DeepWalk, and GraphSage)\nInvariant Graph Networks : invariance, equivariance, k-WL GNN 관련 주제\nEnd-to-End, Transferable Deep RL for Graph Optimization : RL + GNN\n\n\nTutorials & Workshops\n\nWWW 18 Tutorial : Representation Learning on Networks\nCIKM 19 Tutorial : Recent Developments of Deep Heterogeneous Information Network Analysis\nWSDM 19 Tutorial : Learning and Reasoning on Graph for Recommendation\nKDD 19 Tutorial : Learning From Networks\nAAAI 20 Tutorial : Graph Neural Networks: Models and Applications\nICML2020 GNN Workshop GRL+\nWWW 20 Hands on Tutorial - Videos\nGraph Neural Networks for Natural Language Processing / PPT\nTutorial on Spectral and Graph ConvNets\n\n\n\nPapers & Survey\n\nGraph Neural Networks: Taxonomy, Advances and Trends\nA Comprehensive Survey on Graph Neural Networks\nDirectional Graph Networks\nGNN KR Paper List\nGraph Meta Learning via Local Sub-graphs\n\nMeta-GNN: On Few-shot Node Classification in Graph Meta-learning\nFew-shot Learning with Graph Neural Networks\nLearning to Propagate for Graph Meta-Learning\n\nSelf-supervised Training of Graph Convolutional Networks\nXGNN: Towards Model-Level Explanations of Graph Neural Networks\nL2-GCN: Layer-Wise and Learned Efficient Training of Graph Convolutional Networks, 2020 CVPR\n\n\n\nVideos\n\nAn Introduction to Graph Neural Networks: Models and Applications\n\n\n\nGraph Convolutional Networks using only NumPy\n\n\n\nGeometric Deep Learning on Graphs and Manifolds\n\n\n\nGraph Nets: The Next Generation\n\nLink\n\n\n\n\nRecent Developments of Graph Network Architectures\n\nSlide\n2019-2020에 발표된 GNN 방법론들을 정리\nGNN의 expressiveness, 그중에서도 invariance and equvariance\n\n\n\n\nDeep learning on graphs: successes, challenges, and next steps\n\n\n\nGraph Representation Learning for Algorithmic Reasoning\n\nSlide\n\n\n\n\nHow Uber uses Graph Neural Networks to recommend you food\n\nPost"
  },
  {
    "objectID": "posts/storage/2025-05-29-mag-tactile.html",
    "href": "posts/storage/2025-05-29-mag-tactile.html",
    "title": "🧩uSkin vs ReSkin",
    "section": "",
    "text": "Allegro Hand는 인간 손처럼 정교한 조작을 목표로 개발된 로봇 손으로, 섬세한 물체 조작을 위해 촉각 센서의 통합이 중요합니다. 최근 많은 연구에서는 로봇 손가락에 전자 피부를 부착해 접촉 힘과 미끄러짐 등을 감지하려고 시도하고 있습니다. 특히 Allegro Hand에는 XELA Robotics사의 uSkin 센서와 Meta AI가 개발한 ReSkin 센서가 사용되고 있는데, 두 센서는 자기장 기반의 촉각 센서라는 공통점이 있지만 설계 목적과 구현 방식에서 차이가 있습니다. 아래에서는 이 두 센서의 감도, 정확도, 신뢰성, 응답속도, 제조 방식, 자기장 감지 원리 측면에서 특징을 비교하고, 최근 3년간(2022–2025) 해당 센서들을 활용한 최신 연구 사례들을 정리합니다."
  },
  {
    "objectID": "posts/storage/2025-05-29-mag-tactile.html#서론-allegro-hand와-촉각-센서의-중요성",
    "href": "posts/storage/2025-05-29-mag-tactile.html#서론-allegro-hand와-촉각-센서의-중요성",
    "title": "🧩uSkin vs ReSkin",
    "section": "",
    "text": "Allegro Hand는 인간 손처럼 정교한 조작을 목표로 개발된 로봇 손으로, 섬세한 물체 조작을 위해 촉각 센서의 통합이 중요합니다. 최근 많은 연구에서는 로봇 손가락에 전자 피부를 부착해 접촉 힘과 미끄러짐 등을 감지하려고 시도하고 있습니다. 특히 Allegro Hand에는 XELA Robotics사의 uSkin 센서와 Meta AI가 개발한 ReSkin 센서가 사용되고 있는데, 두 센서는 자기장 기반의 촉각 센서라는 공통점이 있지만 설계 목적과 구현 방식에서 차이가 있습니다. 아래에서는 이 두 센서의 감도, 정확도, 신뢰성, 응답속도, 제조 방식, 자기장 감지 원리 측면에서 특징을 비교하고, 최근 3년간(2022–2025) 해당 센서들을 활용한 최신 연구 사례들을 정리합니다."
  },
  {
    "objectID": "posts/storage/2025-05-29-mag-tactile.html#uskin-vs-reskin-주요-특성-비교",
    "href": "posts/storage/2025-05-29-mag-tactile.html#uskin-vs-reskin-주요-특성-비교",
    "title": "🧩uSkin vs ReSkin",
    "section": "uSkin vs ReSkin: 주요 특성 비교",
    "text": "uSkin vs ReSkin: 주요 특성 비교\nAllegro Hand에 통합된 uSkin과 ReSkin의 핵심 사양을 비교하면 다음과 같습니다. uSkin은 다수의 영구자석-홀 센서 배열로 구성된 상용 촉각 피부이고, ReSkin은 자성 입자 기반의 유연한 센서로 개발되어 공개된 저비용 촉각 피부입니다. 두 센서의 특성을 논문 기반 자료로 항목별로 비교하면 다음과 같습니다:\n\n\n\n\n\n\n\n\n비교 항목\nuSkin (XELA Robotics)\nReSkin (Meta AI & CMU)\n\n\n\n\n감도 (Sensitivity)\n약 10 mN(0.45 kPa) 수준의 미세한 힘까지 감지 가능 – 인간 피부에 비하면 떨어지지만, 로봇 촉각 센서로는 매우 높은 감도. 작은 접촉도 검출 가능하여 섬세한 물체 취급에 유리.\n수십 mN ~ 0.1–0.2 N 정도의 힘 변화를 구별 가능 – 예를 들어 약 20 mL 물의 무게 (~0.2 N) 증가도 센서 출력 변화로 포착. 초기 설계 목표는 0.1 N의 힘 분해능이며, 실험적으로도 그에 준하는 작은 힘을 감지함을 시연.\n\n\n정확도 (Accuracy)\n각 촉각 소자(taxel)별 3축 힘 측정의 정확도가 높음. 개별 센서 단위 보정 시 X, Y, Z축 평균 절대오차가 약 0.2 N 수준까지 달성된 사례가 보고됨. 공간 해상도는 taxel 간격 ~4.7 mm로 촘촘하여 접촉 위치도 비교적 정확히 파악 가능.\n머신러닝 보정을 통해 높은 예측 정확도 확보. 예를 들어 자가-보정(self-supervised) 기법 적용 시 접촉 지점 위치 오차 약 0.7 mm, 힘 크기 추정 오차 약 0.44 N 수준까지 성능 향상이 보고됨. 초기 센서 간 편차가 크지만, 다중 센서 학습과 보정으로 84% 이상의 분류 정확도와 낮은 MSE를 달성함.\n\n\n신뢰성 (Reliability)\n일관된 출력과 내구성을 갖춘 편이나, 강한 외부 자기장에 민감하여 교란을 받을 수 있음. 각 센서는 견고하게 패키징되어 장기간 사용 가능하나, 자석-센서의 조립 편차로 센서마다 보정값 차이가 발생할 수 있음. 제조 공정상 수작업 조립으로 인한 개체간 성능 편차를 정밀 보정하여 사용.\n내구성과 교체 용이성을 고려한 설계. 부드러운 센서 층이 마모되면 쉽게 교체할 수 있고, 한 개의 센서 패드가 5만 회 이상의 접촉에도 성능이 크게 저하되지 않음을 검증. 다만 새로운 센서 막 교체 시마다 미세한 특성 차이가 있으므로, 자체 ML기반 보정으로 센서 간 편차와 시간에 따른 변화에 대응함.\n\n\n응답속도 (Response Time)\n전자식 Hall 센서로 실시간 연속 측정이 가능하여 응답속도가 매우 빠름. 이론적으로 kHz 대역까지도 측정 가능하며, 일반적인 비전 기반 촉각센서(30–60 Hz)에 비해 월등히 높은 샘플링 주파수를 지원. 여러 개의 taxel을 동시에 읽을 때도 수백 Hz 이상의 속도를 유지하여 로봇 제어에 활용할 수 있음.\n고속 샘플링 가능 (설계 목표 ≥100 Hz). 실제 응용에서 250 Hz로 데이터 수집을 시연한 바 있으며, 다수 센서를 연결한 경우에도 100–200 Hz 수준으로 안정적으로 동작함. 응답 시간은 수 ms 단위로 인간 촉각보다도 빠른 편이어서 실시간 피드백 제어에 활용할 수 있음.\n\n\n센서 제조 방식  (Fabrication)\n각 촉각 패드마다 영구자석을 포함한 연성 고무층과 그 아래 소형 Hall IC 칩으로 구성. 4×4 격자 등의 모듈 형태로 제작되어 곡면용(손가락 끝 30 taxel)과 평면용(16 taxel 등) 패드로 제공됨. 자석-엘라스토머 부착과 칩 패키징 공정에 수작업 조립이 필요하며, 이로 인해 생산 단가와 개체 간 특성 편차가 발생하는 문제가 지적됨.\n유연한 실리콘 피부에 무작위 자성 입자를 혼합·경화하여 만드는 얇은 패치형 센서. 제작시 3D 프린팅된 몰드에 입자-실리콘 혼합물을 붓고 외부에서 격자 형태로 자화하여 자기 성질을 부여. 경화된 피부를 회로 기판 위에 부착하여 사용하며, 기판에는 소형 3축 자력계 칩(5개 배열)이 장착되어 있음. 전체 설계 파일과 제조법이 오픈소스로 공개되어 있어 손쉬운 제작과 수정이 가능하며, 센서막과 회로를 분리하여 손상 시 피부만 교체하도록 설계됨.\n\n\n자기장 변화 인식 원리  (Magnetic sensing principle)\n영구자석이 외력에 따라 미세 이동하면서 발생하는 자기장 변화를 바로 아래의 홀 효과 센서가 감지하는 방식. 자석이 눌리거나 밀리면 X, Y, Z 방향 자기장 세기가 변하고, 이를 3축 힘 (법선압 + 전단력) 신호로 변환하여 출력함. 각 taxel이 국부적인 접촉력을 벡터 형태로 측정하므로 물체의 미끄러짐 방향이나 접촉 지형을 파악할 수 있음.\n분말 자석들이 포함된 탄성체 막이 변형될 때 주변에 형성된 자기장의 밀도 분포 변화를 자력 센서들이 읽어내는 방식. 말랑한 피부 자체가 자성을 띠고 있어 접촉에 의해 “찌그러지면” 자력계에 읽히는 자기 신호가 변하며, 이를 사전에 학습된 모델이 분석해 힘의 크기와 위치를 추정함. 센서막이 연속적 분포체이므로 넓은 면적에서도 여러 접점의 위치를 계산적으로 추론할 수 있다는 장점이 있음.\n\n\n\n주석: 위 표의 내용은【9】【13】【17】【19】【21】【22】【25】【32】 등의 출처에서 발췌 및 요약한 것입니다."
  },
  {
    "objectID": "posts/storage/2025-05-29-mag-tactile.html#최근-연구-동향-uskin-및-reskin-활용-사례-20222025",
    "href": "posts/storage/2025-05-29-mag-tactile.html#최근-연구-동향-uskin-및-reskin-활용-사례-20222025",
    "title": "🧩uSkin vs ReSkin",
    "section": "최근 연구 동향: uSkin 및 ReSkin 활용 사례 (2022–2025)",
    "text": "최근 연구 동향: uSkin 및 ReSkin 활용 사례 (2022–2025)\n최근 3년간 uSkin 또는 ReSkin 센서를 활용한 대표적인 연구들을 분야별로 정리하면 다음과 같습니다. 각 연구는 촉각 센서의 데이터를 활용하여 로봇의 물체 인지나 조작 능력을 향상시키는 방향으로 이루어지고 있으며, 특히 자기장 기반 촉각센서 + 머신러닝의 결합이라는 공통된 흐름을 확인할 수 있습니다.\n\nuSkin 센서 활용 연구 사례\n\n로봇 그립 미끄럼 감지 (그립 안정성 판단) – “A Model-Free Approach to Fingertip Slip and Disturbance Detection for Grasp Stability Inference” (Kitouni 등, 2023). 이 연구에서는 Allegro Hand의 모든 손가락에 uSkin 촉각 피부를 부착하여 물체를 잡은 상태에서 발생하는 미끄럼(Slip) 및 외부 방해를 감지하였습니다. 총 368개의 3축 촉각 소자가 손바닥과 손가락 마디, 손가락 끝을 덮도록 배치되었으며, 별도의 복잡한 보정 없이 센서 출력 신호의 변화 패턴만으로 미끄럼 여부를 판별하는 모델 프리 접근법을 제안했습니다. 다양한 정밀 쥐기 실험을 통해 제안된 지표가 손가락별 미끄럼 불안정성을 잘 나타냄을 보였고, 이를 활용해 개별 손가락에 능동적인 안정화 피드백을 줄 수 있음을 확인하였습니다. 해당 결과는 로봇이 물체를 놓치기 전에 촉각으로 미끄러짐을 탐지하여 그립을 조정하는 전략에 기여합니다.\n전체 손 촉각 힘 추정 및 제어 – “Interaction force estimation for tactile sensor arrays: toward tactile-based interaction control for robotic fingers” (Chelly 등, 2024). 본 연구는 Allegro Hand에 부착된 다수의 uSkin 센서를 일괄 보정하여 전역적인 3차원 힘 분포를 추정하고, 이를 로봇 제어에 직접 통합한 사례입니다. 저자들은 평면 패드와 곡면 패드가 혼합된 복잡한 배열의 Xela uSkin 촉각 피부를 한 번의 데이터 수집으로 효율적으로 보정하는 데이터 효율적 캘리브레이션 기법을 제안하였습니다. 보정된 촉각센서 배열로부터 얻은 정확한 접촉력 추정치를 로봇 손가락의 상호작용 힘 제어(loop)에 입력하여, 외부 힘을 일정하게 유지하거나 제한하는 힘 제어 작업을 구현하였습니다. 실험 결과, 제안 기법은 센서 배열 전반에 걸쳐 평균 0.1–0.2 N 수준의 오차로 힘을 재구성할 수 있었고, 이를 이용한 힘 조절이 가능한 것을 보여주어 섬세한 힘 조절이 요구되는 작업(예: 깨지기 쉬운 물체 잡기)에 유용한 접근임을 시사했습니다.\n자가 지도학습 기반 촉각표현 학습 – “Self-supervised perception for tactile skin covered dexterous hands (Sparsh-skin)” (Sharma 등, 2025). 이 연구는 자기장 기반 촉각 피부의 복잡한 시계열 신호로부터 의미 있는 표현을 학습하기 위한 자기 지도(self-supervised) 학습기법을 제안했습니다. Allegro Hand의 손바닥, 손가락 마디, 손가락 끝에 걸쳐 Xela uSkin 센서를 분산 배치하여 약 4시간 분량의 다양한 접촉 데이터를 수집한 뒤, 이를 표준화된 표현 공간으로 인코딩하는 프리트레인드(tactile encoder) 모델인 Sparsh-skin을 개발하였습니다. 학습된 촉각 인코더는 unlabeled 데이터로 사전학습되었기 때문에 이후 새로운 작업에 소량의 학습 데이터로 빠르게 적응할 수 있다는 장점을 보였습니다. 실제로 물체 식별 등의 다운스트림 과제에서 기존 엔드투엔드 학습 대비 41% 높은 성능과 향상된 데이터 효율을 달성하여, 촉각 기반 객체 인식이나 미세 동작 제어에 유용한 일반 목적 촉각 표현을 얻을 수 있음을 입증했습니다. 이는 복잡한 자기장 촉각센서 신호를 해석하는 데 있어 학습 기반 접근의 가능성을 보여주며, 향후 인간 수준의 촉각인지 능력을 로봇에 부여하는 데 중요한 단계를 제시합니다.\n\n\n\nReSkin 센서 활용 연구 사례\n\n패브릭(천) 다층 분리 조작 – “Learning to Singulate Layers of Cloth using Tactile Feedback” (Tirumala 등, IROS 2022). 이 연구는 옷감이나 천 여러 장이 포개진 더미에서 로봇이 맨 윗장 한두 장만 집어올리는 어려운 작업에 촉각 센서 ReSkin을 활용한 사례입니다. 프랑카(Franka) 로봇 팔의 그리퍼 손가락 중 하나에 ReSkin 촉각 패드를 부착하고, 해당 센서로부터 얻은 촉감 데이터를 기반으로 현재 잡은 천의 **겹 수(layer 수)**를 판별하는 머신러닝 분류기를 학습했습니다. 학습된 분류기를 로봇 제어에 통합하여, 그리퍼가 너무 많은 층을 잡았을 때 살짝 놓아서 한 층만 잡도록 높이를 자동 조정하는 정책을 실행했습니다. 총 180회의 실제 로봇 실험 결과, 촉각을 활용하지 않은 기존 시각 기반 접근보다 훨씬 높은 정확도로 한 겹 혹은 두 겹의 천을 구분하여 집어올리는 데 성공했고, 보지 못한 새로운 종류의 천에 대해서도 일반화 성능이 향상되었음을 보였습니다. 이는 ReSkin의 미세한 촉각 신호가 옷감의 두께나 결합 상태를 잘 감지함을 보여주며, 기존에 시각으로 어려웠던 섬세한 섬유 조작 작업에 촉각 센서가 유용함을 입증한 사례입니다.\nReSkin의 개선 및 일반화 (AnySkin) – “AnySkin: Plug-and-play Skin Sensing for Robotic Touch” (Bhirangi 등, arXiv 2024). 이 연구는 ReSkin의 개념을 발전시켜 더 강한 자장, 부착 편의성, 센서 간 일관성을 향상시킨 AnySkin이라는 신형 촉각 센서를 소개하였습니다. AnySkin은 ReSkin과 동일하게 자기 입자 기반이지만, 자체 정렬되고 접착제 없이 부착 가능한 설계를 도입하여 로봇 표면 어디에나 붙이기 쉽게 만들었습니다. 또한 센서 간 응답 편차를 줄여, 한 센서에서 학습된 모델이 별도 재보정 없이 다른 센서에도 바로 적용될 수 있음을 시연하였습니다 (cross-instance generalization). 논문에서는 AnySkin을 이용한 미끄러짐 감지와 강화학습 기반 접촉 정책 학습 실험을 수행하여, 이전 ReSkin 대비 향상된 감도와 내구성을 보이며 동시에 데이터 재사용성을 구현함을 보였습니다. 이 결과는 ReSkin이 제시한 저가형 촉각 피부의 가능성을 한층 확장한 것으로, 로봇 촉각 센서의 범용성을 높이는 방향의 중요한 진전으로 평가됩니다."
  },
  {
    "objectID": "posts/storage/2025-05-29-mag-tactile.html#결론-및-시사점",
    "href": "posts/storage/2025-05-29-mag-tactile.html#결론-및-시사점",
    "title": "🧩uSkin vs ReSkin",
    "section": "결론 및 시사점",
    "text": "결론 및 시사점\nuSkin과 ReSkin은 각각 정밀도 높은 상용 촉각센서와 저비용 유연 촉각피부를 대표하며, 최근 연구들은 이들 센서를 로봇에 적용하여 섬세한 조작 기술을 향상시키는 방향으로 전개되고 있습니다. 요약하면, uSkin은 높은 해상도의 3축 힘 측정 능력을 바탕으로 로봇손의 접촉력 제어나 미끄럼 감지와 같은 문제에 활용되고 있으며, ReSkin은 저렴하고 교체가 쉬운 강점을 살려 복잡한 변형 물체의 촉각 인지나 웨어러블 로봇피부 등 확장성 있는 응용에 쓰이고 있습니다. 더욱이 두 센서 모두 머신러닝 기법과 결합되면서 센서 신호 보정, 특징 표현 학습, 정책 학습 등 다양한 측면에서 혁신이 이루어지고 있습니다. 이러한 동향은 향후 로봇이 사람처럼 섬세한 촉각을 활용하고, 여러 형태의 촉각 센서를 효과적으로 통합하여 보다 지능적인 상호작용을 구현하는 데 중요한 밑거름이 될 것으로 기대됩니다.\n참고 자료: 본 답변에서는 arXiv, IEEE, Science Robotics 등 신뢰할 수 있는 출처의 논문 및 기사【9】【11】【13】【17】【19】【21】【22】【25】【27】【31】【32】【37】【39】【42】【49】를 인용 및 참고하였습니다. 각 번호는 해당 출처를 가리키며, 자세한 내용은 인용문 옆 괄호의 번호를 통해 확인할 수 있습니다.\n\n\nEnglish\n\nGot it. I’ll translate the full content into English, including the detailed comparison of uSkin and ReSkin sensors, recent research cases using them, and all structured content like tables and analyses.\nI’ll present the translated report shortly."
  },
  {
    "objectID": "posts/storage/2025-05-29-mag-tactile.html#introduction",
    "href": "posts/storage/2025-05-29-mag-tactile.html#introduction",
    "title": "🧩uSkin vs ReSkin",
    "section": "Introduction",
    "text": "Introduction\nTactile sensing is crucial for dexterous robotic manipulation, providing rich feedback about contact forces, textures, and object properties that vision alone cannot capture. The Allegro Hand – a four-fingered, 16-DOF robotic hand – has become a popular platform for evaluating advanced tactile sensors in research. Two notable tactile skin technologies used with the Allegro Hand are uSkin (developed by XELA Robotics) and ReSkin (developed by Meta AI in collaboration with Carnegie Mellon University). This report presents a detailed comparison of uSkin and ReSkin in terms of sensitivity, accuracy, reliability, response speed, fabrication methods, and their underlying magnetic field sensing principles. It also reviews recent research (2022–2025) that employs each sensor, highlighting applications and outcomes. The goal is to provide robotics researchers with a clear understanding of each sensor’s capabilities and trade-offs in an academic, technical context."
  },
  {
    "objectID": "posts/storage/2025-05-29-mag-tactile.html#overview-of-the-tactile-sensors",
    "href": "posts/storage/2025-05-29-mag-tactile.html#overview-of-the-tactile-sensors",
    "title": "🧩uSkin vs ReSkin",
    "section": "Overview of the Tactile Sensors",
    "text": "Overview of the Tactile Sensors\n\nuSkin Sensor (XELA Robotics)\nuSkin is a high-density 3-axis tactile sensor system packaged in a thin, soft silicone skin. It integrates an array of small sensing units (taxels) that can each detect forces in three dimensions: normal pressure (Z-axis) and shear forces (X and Y axes). The uSkin design embeds tiny magnets in the soft skin and uses underlying magnetometers (or Hall-effect sensors) to track the magnets’ movements under deformation. Each taxel’s magnetic field readings in X, Y, Z change as forces are applied, allowing the system to compute a 3-axis force vector at that point. Because the sensors are distributed in a grid, uSkin provides spatially localized force data across the contact surface (for example, an Allegro fingertip can be covered with ~24 taxel units). The sensor outputs are digital, minimizing noise and eliminating the need for bulky analog wiring or external ADC boards. In practice, uSkin can be integrated into new or existing robots with minimal wiring and straightforward mounting (e.g. glued onto robot fingers or palm). XELA offers flat patch sensors, curved fingertip sensors, and other form factors to cover various robot hand surfaces. Overall, uSkin provides a turnkey tactile sensing solution with high resolution and direct force readouts per taxel, making it suitable for precise manipulation tasks.\n\n\nReSkin Sensor (Meta AI & CMU)\nReSkin is an open-source tactile “skin” that uses a flexible polymer embedded with magnetic particles to sense touch. The ReSkin concept is to create a low-cost, replaceable tactile layer that can be applied to robot hands (or other surfaces) like an electronic skin. The sensor consists of a thin (~2–3 mm) silicone elastomer sheet with randomly distributed microscopic magnetic particles. This magnetic sheet is placed over a small magnetometer chip. When the skin is pressed or deformed, the pattern of magnetic field at the magnetometer changes because the particles move (“squooshed”) within the elastomer. Machine learning is then used to map these field distortions to contact force magnitudes and locations. ReSkin’s design prioritizes simplicity and versatility: the sensing hardware (magnetometer and electronics) is kept separate from the soft skin. The skin contains no wires or electronics; it can be peeled off and replaced like a Band-Aid when worn out. This makes the part most susceptible to damage very easy and cheap to replace (each skin costs on the order of &lt;$30 in materials). ReSkin can be cut or shaped to cover different surfaces – from a robot fingertip to an entire glove or even a dog’s paw – providing a conformal tactile sensing layer. The open-source release includes instructions for fabrication (mixing and curing the magnetic silicone) and pre-trained models for interpreting the sensor signals. In summary, ReSkin offers a flexible, low-cost tactile sensing approach that leverages magnetic field changes and learned models to detect touch across a continuous surface."
  },
  {
    "objectID": "posts/storage/2025-05-29-mag-tactile.html#comparison-of-performance-and-design",
    "href": "posts/storage/2025-05-29-mag-tactile.html#comparison-of-performance-and-design",
    "title": "🧩uSkin vs ReSkin",
    "section": "Comparison of Performance and Design",
    "text": "Comparison of Performance and Design\nTo clearly contrast uSkin and ReSkin, this section compares their key specifications and performance metrics:\n\nSensitivity and Resolution\nSensitivity refers to the smallest force the sensor can reliably detect, and resolution includes the spatial granularity of touch detection. uSkin is engineered for high sensitivity – it can detect forces as light as about 0.1 gram-force (gf) (≈0.001 N). This extremely low force threshold means uSkin can register very slight touches or contact, making it suitable for delicate manipulation where precise force control is needed. In contrast, ReSkin’s target sensitivity is on the order of &lt;0.1 N (≈10 gf) for force detection. However, with careful calibration and machine learning models, ReSkin can achieve very fine force resolution: experiments have demonstrated force measurement errors as low as ~0.005 N (5 mN) in controlled settings. This indicates that ReSkin, despite its simple hardware, can discern minute forces after training, although its native (untrained) sensitivity may be lower than uSkin’s.\nIn terms of spatial resolution, uSkin provides a grid of discrete sensing points (taxels). For example, each uSkin “4x4” patch contains 16 sensing points, and a curved uSkin on an Allegro fingertip uses 24 sensor chips covering the finger pad. The spacing between taxels (a few millimeters apart) defines how finely the contact location can be distinguished – essentially on the order of the taxel pitch. ReSkin, by contrast, behaves like a continuous skin. A single ReSkin patch (roughly the size of a coin) can localize contacts with a spatial accuracy of about 1 mm (with ~90% accuracy) after training. In a benchmark test, ReSkin achieved ~99.6% accuracy in classifying contact locations within ±1 mm on its surface. This suggests that, when calibrated, ReSkin can provide very high spatial detail of where a touch occurs, potentially finer than the discrete spacing of uSkin’s taxels. The trade-off is that uSkin’s taxels give direct physical correspondence to locations, whereas ReSkin’s localization comes from an inference model. In summary, both sensors offer excellent sensitivity and spatial resolution for robotics use: uSkin has an ultra-low force threshold and inherently structured high-density sensing points, while ReSkin achieves comparable force and location resolution through machine-learning-assisted sensing.\n\n\nAccuracy of Force Measurement\nAccuracy encompasses how reliably the sensor can quantify the magnitude and direction of applied forces. uSkin’s design yields direct 3-axis force readings at each taxel, but these raw readings require calibration to map sensor units to physical force values. When properly calibrated, uSkin can measure forces in Newtons and serve in control feedback loops. For instance, a recent study calibrated an Allegro Hand’s uSkin sensors against a force-torque sensor and achieved force estimation errors of around 0.12 N (±0.08 N) in a closed-loop grasping task. This indicates that uSkin can accurately measure and regulate contact forces to within a few hundred millinewtons during manipulation. Its repeatability and linearity benefit from the stable positioning of magnets and sensors in each module. Moreover, uSkin’s on-board digital electronics reduce noise, improving measurement consistency. One challenge for accuracy, however, is dealing with external magnetic interference or drift (addressed later), which XELA mitigates via software compensation for certain models.\nReSkin’s accuracy heavily relies on its learned model. The raw magnetic readings from a ReSkin patch are not directly interpretable as force without a mapping. With a well-trained neural network, ReSkin has demonstrated impressively accurate force reconstruction: in one evaluation, the mean squared error in normal force prediction was on the order of (5 × 10−3 N)2, corresponding to just a few millinewtons error. Additionally, ReSkin is capable of sensing shear forces; a test for dynamic shear contact showed it could predict tangential forces (F_x, F_y) with MSE ~0.0011 N^2, while maintaining normal force error ~0.003 N^2. These results underscore that ReSkin, despite using a single magnetometer, can accurately capture multi-axis force information when aided by machine learning. The limitation is that the accuracy is only as good as the model’s calibration and training data – any change in the skin (replacement or drift over time) can degrade performance if not accounted for. The original ReSkin paper noted that models trained on one sensor did not generalize to other sensors without adaptation, due to instance variability. Recent improvements (see “AnySkin” below) aim to reduce this variability. In summary, uSkin offers direct, hardware-defined accuracy which can be high after one-time calibration, whereas ReSkin offers model-based accuracy which can reach very high levels but requires ongoing calibration and learning algorithms to maintain.\n\n\nReliability and Durability\nReliability covers the sensor’s longevity and consistency of performance over time, especially under repetitive use. uSkin is built as a durable tactile array: its soft silicone and internal structure are designed to handle repetitive contacts and even overload conditions without permanent damage. The silicone skin not only protects the internal sensor elements but also allows slight conformity, distributing stress. XELA specifies that uSkin can sustain up to a certain maximum normal force (e.g. 450 gf for one model, or up to 1500 gf for newer models) without damage. In manipulation tasks, uSkin-covered fingers have been shown to handle fragile objects reliably without harming them. Wear and tear on uSkin is relatively low since the sensor is an integrated unit – there are no loose particles or fluids – and it’s sealed to prevent dust or moisture ingress. Many researchers have used the same uSkin sensors for thousands of grasp cycles; as long as the silicone and wiring remain intact, the performance remains stable. On the other hand, if a uSkin module does fail or break, it is a specialized hardware piece that must be replaced (which can be costly, as high-end tactile sensors often are). XELA advertises their product as more affordable than some competitors like the BioTac (&gt;$1000) while still not compromising performance, but it is certainly more expensive than DIY solutions.\nReSkin emphasizes replaceability as a core feature of reliability. The magnetic skin can undergo many touches: tests showed the machine learning model remained accurate even after 50,000 interactions on the same piece of skin. Eventually, however, the silicone skin will degrade (e.g. tiny cracks, particle loss, or reduced elasticity) after extensive use. Instead of requiring a complex sensor replacement, the worn skin can simply be peeled off and a new one attached, restoring the sensor to like-new performance. This concept makes ReSkin robust in a maintainable way – any damage to the surface (cuts, abrasion) is not catastrophic, because the skin is a cheap consumable. Another aspect of reliability is the sensor’s consistency over time and across units. Early versions of ReSkin saw variability between different fabricated skins and drift in signals over time (as the elastomer properties changed slightly). To combat this, the designers suggest periodic re-calibration (collecting a zero-load magnetic reading occasionally) and have developed improved fabrication methods. In 2024, an improved variant called “AnySkin” introduced a post-curing magnetization process and self-aligning skins to achieve more uniform magnetic particle distribution and secure attachment to the magnetometer. These improvements greatly reduced variability between sensor instances and prevented performance loss due to misalignment or peeling over time. In summary, ReSkin’s reliability comes from its easy renewability and ongoing model adaptation, whereas uSkin’s reliability stems from a robust physical design that maintains performance over a long service life with minimal intervention.\n\n\nResponse Time and Sampling Rate\nRapid response and high sampling frequency are important for capturing dynamic contact events (e.g. slip, impact) and for tight control loops. uSkin provides real-time readings via a digital interface (often CAN or USB converter) and supports high sampling rates. The standard uSkin modules can sample at up to 500 Hz (2 ms interval) on certain models. In many experiments, users run uSkin at 100 Hz due to external constraints or sufficient bandwidth, but the hardware is capable of faster updates for more demanding applications. The internal latency of uSkin’s sensors is low, since it uses direct electrical readings of magnetic field changes with minimal filtering. This allows reactive control – for example, a 100 Hz control loop using uSkin feedback was successfully implemented for force control in a dexterous hand.\nReSkin is also designed for high temporal resolution. The magnetometer can be read at rates up to around 400 Hz (as reported in the initial paper), and potentially faster with optimized hardware. The actual throughput may depend on the microcontroller or interface used, but the goal was to exceed 100 Hz, which ReSkin achieved. Because ReSkin uses a learning pipeline, one consideration is the computational delay for the model to infer forces from magnetic readings. In practice, this inference can be made lightweight (e.g. a small multi-layer perceptron) and run in a few milliseconds, so the end-to-end latency remains low. The ReSkin authors demonstrated real-time use of the sensor (e.g. detecting slips or impacts) without lag, suggesting the response is fast enough for most robotic tasks. Both uSkin and ReSkin thus meet the requirements for real-time tactile feedback, with high-frequency data streams. If comparing, uSkin’s fixed hardware sampling (500 Hz digital output) might offer a slight edge in raw speed and noise immunity, whereas ReSkin’s practical speed (~400 Hz) is comparable and has been validated in closed-loop tasks as well. In either case, both sensors can capture fine contact events (on the order of a few milliseconds), far exceeding slower vision-based tactile sensors (which often run at 30–60 Hz).\n\n\nFabrication Methods and Integration\nThe fabrication and integration process for these sensors differ markedly due to one being a commercial product and the other a DIY solution. uSkin’s fabrication is proprietary to XELA Robotics – it involves assembling small PCBs or sensor chips with embedded magnets and encapsulating them in silicone. Each taxel likely contains a tri-axial magnetometer or Hall sensor aligned with a small magnet in the silicone layer above. The exact manufacturing steps (e.g. how the magnets are embedded and calibrated) are not publicly detailed, but the outcome is a durable sensor sheet with built-in wiring. uSkin modules come with connectors and can be daisy-chained or attached around a robotic finger. Integration is straightforward: the sensors output digital data (e.g. via I2C or SPI through a hub) so one only needs to attach a lightweight cable from the robot hand to a data acquisition board (like XELA’s CAN-to-USB interface). Mechanically, uSkin patches can be glued onto robot surfaces or affixed with screws/brackets depending on the model. The ability to customize shapes (flat, curved, wrap-around) means one can cover complex geometries (fingertips, phalanges, palm) by using multiple uSkin pieces. For example, covering an Allegro Hand might use a curved uSkin on each fingertip and flat patches on each finger link and palm. Because uSkin is a commercial solution, robotics labs often opt for it when they need a plug-and-play tactile array with vendor support, rather than investing time in sensor fabrication.\nReSkin’s fabrication is deliberately simple and accessible. To create a ReSkin sensor, one mixes a two-part silicone rubber with microscopic magnetic particles (like iron oxide or neodymium powder) in a mold to form a thin sheet. After curing, this elastomer is magnetized – early methods involved curing in a magnetic field to align particles, but this was tricky and led to variability. Updated methods use a pulse magnetizer after curing to uniformly magnetize the particles without needing a field during the curing process. The result is a flexible skin with randomly oriented magnetic dipoles. The electronics for ReSkin consist of a small PCB with a magnetometer (typically a 3-axis magnetometer chip) and possibly a microcontroller to read the magnetometer and stream data. This PCB is placed directly under the silicone skin (it can even be embedded or held by a fixture). A critical integration aspect is keeping the skin positioned relative to the sensor – AnySkin research introduced self-adhering skins that clip or snap in place, avoiding glue that can peel. In practice, attaching ReSkin to a robot might involve mounting the tiny magnetometer board on a robot finger and then stretching or securing the silicone patch over it like a thimble or sleeve. The flexibility of ReSkin’s form means it can cover curved or large areas by using multiple magnetometers under one continuous skin, or tiling multiple units. Fabrication time is short (a few hours to mold and cure a batch of skins), and the cost is very low per skin (tens of dollars or less). This makes ReSkin attractive for projects that need many sensors or large areas, as one can fabricate and replace skins as needed. The trade-off is that integrating ReSkin also requires developing or using a ML model for the specific robot application, which adds a layer of complexity in software.\n\n\nSensing Principle: Magnetic Field-Based Detection\nBoth uSkin and ReSkin rely on magnetic field sensing at their core, but the configuration and principles of operation differ:\n\nuSkin: Each taxel in uSkin is essentially a miniaturized magnetic tactile sensor: a small magnet is embedded in the deformable skin, and directly beneath it is a magnetometer that measures the magnet’s field in 3 axes. In the undisturbed state, the magnet’s field at the sensor has a known baseline. When an external force presses on the skin at that taxel, the magnet moves (e.g. gets displaced or tilted) relative to the sensor. This causes changes in the magnetic field readings (ΔB_x, ΔB_y, ΔB_z). These changes are correlated to the force vector applied – for instance, a normal press might move the magnet closer to the sensor (increasing |B_z|), while a shear force might shift it laterally (changing B_x, B_y). Through calibration, uSkin converts the raw magnetic readings into an X, Y, Z force reading for each taxel. The key aspect is direct physical mapping: the sensor is designed so that magnetic field changes correspond in a roughly one-to-one manner with force components. Because the magnets are fixed in known positions and each taxel is independent, the interpretation of the signals is straightforward (often a polynomial or linear map for each axis). uSkin’s use of magnetics provides a contactless sensing mechanism (no electrical contacts at the surface) and allows the sensor to be thin and compliant. However, it also means the readings can be affected by external magnetic fields or nearby ferromagnetic objects. XELA addresses this by offering magnetic interference compensation, using reference sensors or software filters to subtract out background field disturbances. In essence, uSkin’s principle is a localized magnetic displacement sensor at each grid point.\nReSkin: The ReSkin approach uses a distributed magnetic field perturbation principle. Instead of discrete magnets, the entire elastomer sheet contains a random dispersion of tiny magnetic particles. The magnetometer under the skin measures the combined magnetic field from all these particle dipoles. When the skin is not touched, this field has a stable baseline profile. When contact occurs, a region of the skin deforms – particles in that region get slightly closer to the sensor or reorient, altering the field. Crucially, the relationship between a given touch (with certain force and location) and the magnetometer reading is highly complex, since many particles contribute to the field signal. Therefore, ReSkin relies on a learned mapping: a data-driven model (often a neural network) is trained on known indentations to predict the contact location and force from the raw magnetic field readings. The model effectively decodes the pattern of field changes into meaningful tactile information. The benefit of this method is that a single small sensor can cover a relatively large area of skin and sense forces at any point in that area. The drawback is that the magnetic field signal is an entangled representation – without the model, one cannot directly obtain force/position. ReSkin’s magnetic sensing principle is thus a global sensing mode: every touch influences the overall field measurement, but in different ways, and the ML model disentangles them. This principle also means that if the skin shifts relative to the magnetometer or if the magnetic particle distribution changes (due to wear or a new skin), the mapping might need recalibration. Recent efforts like AnySkin aim to make the field more consistent (e.g. uniform particle distribution via post-magnetization) so that the same model can work across replacements. Another consideration is environmental magnetic noise – like uSkin, ReSkin can be affected by strong external magnets or fields. Users must ensure the sensor’s baseline is recorded and possibly apply filtering for stray field fluctuations (some approaches include taking a no-contact reading before each use to serve as a reference). In summary, ReSkin’s magnetic detection principle is a one-to-many mapping (one sensor reading to many possible contacts, resolved by learning), whereas uSkin’s is many one-to-one mappings (each taxel sensor responds to forces mostly at its own location).\n\nThe table below summarizes the key differences between uSkin and ReSkin:\n\n\n\n\n\n\n\n\nAspect\nuSkin (XELA)\nReSkin (Meta/CMU)\n\n\n\n\nSensing Principle\nLocal magnetic displacement at many discrete 3-axis taxels (each taxel: magnet + Hall sensor). Direct mapping from magnet movement to force per taxel.\nGlobal magnetic field distortion measured by one/few magnetometers. Requires ML model to infer contact force and position.\n\n\nSensitivity\n~0.1 gf (0.001 N) threshold for force detection (very light touch). High sensitivity due to precise magnet sensor coupling.\nAimed for &lt;0.1 N detectable force; with calibration, achieved ~0.005 N force resolution in tests. Slightly less sensitive natively, but improved by ML averaging.\n\n\nSpatial Resolution\nDiscrete taxel spacing (e.g. 16–24 sensors per fingertip) gives a few mm resolution; each taxel provides localized 3D force data.\nContinuous skin with ~1 mm contact localization accuracy after training. Can detect multiple contact points if using multiple magnetometers or sequential touches, but typically one contact at a time per patch.\n\n\nAccuracy\nOutputs calibrated force readings per taxel; requires calibration but then reliable (e.g. ~0.1–0.2 N error in practice). Minimal drift; interference compensated via software.\nHigh accuracy with trained model (99% location accuracy, few mN force error in controlled settings). Must retrain or adapt model if skin changes or drifts over long term.\n\n\nResponse Time\nUp to 500 Hz sampling rate (2 ms); low-latency digital output. Suitable for fast control loops (used at 100 Hz in hand control experiments).\nTested up to ~400 Hz update rate; real-time ML inference feasible (few ms). Effective for dynamic tasks (slip detection, impacts) with slight computational overhead.\n\n\nReliability & Durability\nRobust build – soft but resilient; handles overloads without damage. Long-lived hardware; no consumable parts (aside from eventual wear on skin after extensive use). Susceptible to strong external magnetic fields (mitigated by compensation).\nSkin lasts ~50k interactions before degradation. Inexpensive, user-replaceable skins make maintenance easy. Performance consistent if skin is replaced and model updated occasionally. Sensitive to magnetic misalignment or drift; new designs (AnySkin) improve consistency.\n\n\nFabrication & Integration\nProprietary manufacturing; purchase from XELA. Available in flat, curved, bendable formats for integration. Attaches via glue or brackets; requires XELA interface for data. Higher cost per unit, but ready to use out-of-box.\nDIY fabrication from silicone + magnetic powder (open-source specs). Simple electronics (1 magnetometer + microcontroller per patch). Highly affordable (&lt;$30 per sensor). Flexible placement on robot, but requires ML software integration for use.\n\n\n\nTable: Feature comparison of uSkin and ReSkin tactile sensors."
  },
  {
    "objectID": "posts/storage/2025-05-29-mag-tactile.html#recent-research-applications-20222025",
    "href": "posts/storage/2025-05-29-mag-tactile.html#recent-research-applications-20222025",
    "title": "🧩uSkin vs ReSkin",
    "section": "Recent Research Applications (2022–2025)",
    "text": "Recent Research Applications (2022–2025)\nBoth uSkin and ReSkin have been employed in a variety of research projects in robotics, particularly in areas like object manipulation, force control, and haptic perception. Below we summarize selected recent studies that showcase each sensor in use, including the research context, how the sensor was implemented, and key findings:\n\nStudies Utilizing uSkin in Robotics Research (2022–2025)\n\n\n\n\n\n\n\n\n\nStudy & Year\nApplication / Field\nExperimental Setup with uSkin\nKey Outcomes\n\n\n\n\nKulkarni et al., 2024 – “Tactile Object Property Recognition Using Geometrical Graph Edge Features and MT-GCN” (RA-L/IROS 2024)\nObject property recognition (shape/texture stiffness classification)\nAllegro Hand fully covered with uSkin sensors on all fingertips, phalanges, and palm. Tactile readings (1168 channels total) fed into a multi-thread Graph Convolutional Network to learn object features.\nIntegrating high-density uSkin data enabled the GCN to recognize objects’ properties with high accuracy. The proposed method outperformed baseline models, confirming that rich tactile input (3-axis forces from uSkin) improves multi-fingered object classification. It demonstrated effective identification of various object features solely through touch, validating uSkin’s value for complex perception tasks.\n\n\nChelly et al., 2024 – “Tactile-based Force Estimation for Interaction Control with Robot Fingers” (arXiv preprint 2024)\nPrecision force control in dexterous manipulation\nAllegro Hand instrumented with Xela uSkin on each finger. The uSkin taxels were calibrated against an ATI Nano17 force/torque sensor to learn mapping from magnetic readings to actual force (per taxel). Used in a closed-loop admittance controller at 100 Hz.\nAchieved reliable real-time force feedback control: the robot maintained desired contact forces with only ~0.12 N error margin. Demonstrated that uSkin can provide accurate enough force sensing to serve in feedback loops for delicate tasks (e.g., holding an object with constant force). Validated the stability and responsiveness of uSkin-based control, highlighting the sensor’s utility in enhancing manipulation precision.\n\n\nFunabashi et al., 2022 – “Covering a Robot Hand with uSkin for Object Manipulation” (previous study referenced in Kulkarni 2024)\nGeneral grasping and tactile sensing integration\n(Details inferred from context) Allegro Hand with uSkin on all contact surfaces, similar to above. Focus on integrating tactile data into manipulation strategies.\nProvided early evidence that full-hand tactile coverage with uSkin improves manipulation. Likely showed the feasibility of retrofitting Allegro Hand with uSkin and using its readings for tasks like grip adjustment or slip detection. Paved the way for later methods (e.g., graph-based learning) by establishing baseline techniques and highlighting challenges of managing large tactile data streams.\n\n\n\nTable: Selected research using uSkin sensors on Allegro Hand (2022–2025). Each study leveraged uSkin’s dense tactile feedback for perception or control, demonstrating improved performance in manipulation tasks.\n\n\nStudies Utilizing ReSkin in Robotics Research (2022–2025)\n\n\n\n\n\n\n\n\n\nStudy & Year\nApplication / Field\nExperimental Setup with ReSkin\nKey Outcomes\n\n\n\n\nBhirangi et al., 2021 (Meta AI & CMU) – “ReSkin: Versatile, Replaceable, Lasting Tactile Skins” (CoRL 2021, published 2022)\nTactile sensor development & benchmarking\nIntroduced ReSkin and evaluated it in lab tests. A small ReSkin patch (~2 cm) was indented at various locations and forces using a precise indenter and an ATI Nano17 F/T sensor for ground truth. Trained an MLP model to predict contact position (X,Y) and force (Z, and later X,Y) from magnetometer data.\nValidated ReSkin’s core capabilities: contact localization error ~0.5 mm and force error ~5 mN, with 99.6% contact accuracy in controlled conditions. Demonstrated high temporal resolution (up to 400 Hz) and longevity &gt;50,000 presses without model degradation. Established ReSkin as an inexpensive (&lt;$30) yet high-performance tactile sensor, laying groundwork for its adoption in various robot tasks.\n\n\nTirumala, Weng, Seita et al., 2022 – “Learning to Singulate Layers using Tactile Feedback” (IROS 2022)\nDeformable object manipulation (cloth layer separation)\nA Franka arm with a custom two-finger gripper was instrumented with a ReSkin sensor on one fingertip. The robot attempted to pinch and lift one or two layers from a stack of fabrics. A classifier was trained on ReSkin data to infer the number of layers grasped. 180 trials were conducted comparing tactile-informed strategy vs vision-only baselines.\nThe ReSkin-enabled gripper successfully distinguished between one vs. two cloth layers by touch, outperforming vision-only methods that failed on transparent or patterned fabrics. Tactile feedback from ReSkin allowed the robot to adjust its pinch depth in real time, greatly improving reliability in grasping the correct number of layers. This study showcased ReSkin’s thin profile and sensitivity – the sensor could be inserted between layers without bulky hardware, enabling a task (layer separation) that was not feasible with prior optical or larger tactile sensors. It highlighted the potential of ReSkin for fine manipulation in cloth handling and other delicate tasks.\n\n\nSingh et al., 2023 – “AnySkin: Plug-and-play Skin Sensing for Robotic Touch” (arXiv 2023)\nSensor design improvement (robust tactile skin)\nAn extension of ReSkin’s design addressing its drawbacks. Proposed fabrication changes: magnetize the elastomer after curing (using a pulse magnetizer) for uniform particle distribution, use finer magnetic particles to avoid sedimentation, and introduce a self-aligning mount that locks the skin to the magnetometer without adhesives. Evaluated signal consistency across multiple skin instances and under cyclic loading.\nProduced a new “AnySkin” sensor that maintained signal strength and consistency better than original ReSkin. Variability in readings across different skins was greatly reduced (normalized std. deviation ~0.1 vs &gt;0.5 before). The self-adhesive design prevented peeling or shifting during use, enhancing durability. AnySkin preserved ReSkin’s advantages (flexibility, low cost) while improving repeatability and ease-of-use. This research indicates the evolution of ReSkin technology to be more robust for practical deployment, thereby benefiting any robotic applications that rely on such magnetic skin sensors.\n\n\n\nTable: Selected research using ReSkin tactile skins (2021–2025). These works range from the initial demonstration of ReSkin’s capabilities to its application in challenging tasks (fabric manipulation) and further improvements in the sensor design."
  },
  {
    "objectID": "posts/storage/2025-05-29-mag-tactile.html#conclusion",
    "href": "posts/storage/2025-05-29-mag-tactile.html#conclusion",
    "title": "🧩uSkin vs ReSkin",
    "section": "Conclusion",
    "text": "Conclusion\nBoth uSkin and ReSkin represent significant advances in tactile sensing for robotic hands, but they are optimized for different priorities. uSkin offers a ready-made, high-density sensor array with excellent sensitivity and straightforward output of rich 3-axis force data at each contact point. It excels in scenarios demanding precision and reliability, such as controlled manipulation and experiments requiring accurate force measurements out-of-the-box. Its integration into the Allegro Hand has enabled researchers to achieve fine force control and detailed object recognition by leveraging the structured tactile information. The main downsides are the higher cost and the need to manage many sensor channels, as well as careful handling of magnetic interference (though mitigated by design).\nReSkin, on the other hand, offers an incredibly flexible and low-cost approach to spreading touch sensation over robot surfaces. Its strength lies in adaptability: it can conform to unique shapes, be replaced easily, and scale to larger areas without prohibitive cost. ReSkin has enabled innovative applications like sensing multiple layers of cloth – tasks previously impractical – thanks to its thinness and compliance. It leverages data-driven methods to extract a wealth of information (contact point, normal and shear forces) from minimal hardware. The trade-off is the added complexity of model training and the need for occasional recalibration when the “skin” changes or environmental conditions shift. Its accuracy can be superb, but only under a well-trained model’s regime; generalizing that performance broadly is an active area of improvement (e.g., AnySkin making strides in consistency).\nIn sum, uSkin and ReSkin are complementary tactile technologies. uSkin provides a benchmark for precision and ease-of-integration in research labs, delivering high-quality data for each taxel suitable for analytical approaches. ReSkin provides a vision of scalable, affordable tactile coverage, inviting creative uses and rapid prototyping of “sensitive skin” for robots. For the Allegro Hand, which has been used as a testbed, uSkin offers immediate high-resolution touch sensing on each finger, while ReSkin offers a path to cover the entire hand (and arms or tools) with a continuous sensing layer. The choice between them depends on the use-case: if one needs robust, plug-in sensors for fine control (and budget allows), uSkin may be preferable; if one needs wide-area coverage, easy replacement, or has cost constraints, ReSkin is extremely attractive. Future developments may even hybridize these approaches – using structured arrays of ReSkin-like cells, or adding more intelligence to uSkin’s data – to further enhance tactile sensing. What is clear from recent research is that both sensors greatly advance a robot hand’s ability to “feel”, bringing robotic manipulation closer to the dexterity of human touch by different means. Each has proven effective in various studies, and continued improvements (higher density uSkin, more stable ReSkin) are likely to expand their applications. In conclusion, uSkin and ReSkin represent two state-of-the-art solutions in tactile sensing, each with unique strengths, and both are instrumental in the ongoing development of tactile intelligence in robotic hands."
  },
  {
    "objectID": "posts/paper/2025-07-30-dexmachina.html",
    "href": "posts/paper/2025-07-30-dexmachina.html",
    "title": "📃DexMachina 리뷰",
    "section": "",
    "text": "Paper Link\nProject Link\nGithub Link"
  },
  {
    "objectID": "posts/paper/2025-07-30-dexmachina.html#introduction",
    "href": "posts/paper/2025-07-30-dexmachina.html#introduction",
    "title": "📃DexMachina 리뷰",
    "section": "Introduction",
    "text": "Introduction\n인간의 손재주는 로봇 공학에서 오랫동안 궁극적인 목표였지만, 인간 손과 로봇 손의 차이(embodiment gap)로 인해 동일한 동작을 이식(retarget)하는 데 많은 어려움이 존재합니다. 2025년 5월 Arxiv에 공개된 “DexMachina: Functional Retargeting for Bimanual Dexterous Manipulation” 논문은 이러한 문제를 해결하기 위해 제안된 새로운 강화학습 기반 방법입니다. 이 글에서는 해당 논문의 핵심 아이디어와 방법론, 실험 결과, 그리고 기존 관련 연구들과의 비교를 전문가의 시각에서 깊이 있게 분석합니다. 특히 ① 방법론의 독창성 (기존 리타게팅/모션 트랜스퍼 기법 대비 차별점), ② 실험 결과 분석 (다양한 환경에서의 성능과 효과), ③ 기존 연구와의 비교 (유사 목적을 가진 프레임워크들과의 기술적 차이와 기여도)에 중점을 두어 살펴보겠습니다.\n\n기능적 리타게팅의 개념: DexMachina는 기능적 리타게팅(functional retargeting)이라는 개념을 정립합니다. 이는 인간 시연의 “결과”에 초점을 맞춰 로봇이 물체를 동일하게 조작하도록 학습하는 것으로, 단순히 인간의 손동작을 흉내내는 운동학적 리타게팅과 대비됩니다. 기존의 운동학적 리타게팅은 로봇 손가락 위치를 사람 손과 유사하게 따라하게 할 수는 있어도 물체 조작 성공을 보장하지 못하는 반면, DexMachina는 물체의 목표 상태를 따라가도록 정책을 학습시키는 점에서 근본적으로 다릅니다.\n가상 객체 제어와 커리큘럼 학습: DexMachina의 가장 독창적인 아이디어는 “가상 객체 제어기(virtual object controller)”를 활용한 자동 커리큘럼 학습입니다. 초기 학습 단계에서는 외부 힘(가상 제어기)이 물체를 자동으로 목표 위치까지 밀어주면서 정책 학습을 돕고, 점진적으로 그 도움을 줄여나감으로써 최종적으로 로봇 정책이 스스로 물체를 조작하도록 만듭니다. 이러한 점감적 Aux 방식은 초반 학습의 난이도를 크게 낮춰주어, 긴 시퀀스 작업에서도 초기 실패를 방지하고 안정적으로 탐색할 수 있게 합니다.\n다양한 손과 작업에 대한 범용성: 저자들은 6종의 로봇 손(Inspire Hand, Allegro Hand, X-Hand, Schunk Hand 등)과 5종의 복잡한 물체(노트북, 주방기구 등 관절부를 가진 물체들)로 구성된 시뮬레이션 벤치마크를 구축하여 DexMachina의 성능을 평가했습니다. 결과적으로 DexMachina는 모든 손과 작업에 걸쳐 기존 방법들을 능가하는 성공률을 보였으며, 특히 복잡한 양손 장기 작업(long-horizon)에서 두각을 나타냈습니다. 또한 하나의 알고리즘으로 다양한 로봇 손에 별도 튜닝 없이 적용 가능함을 보여주어, 향후 로봇 손 하드웨어 설계 비교에도 유용한 표준을 제시합니다.\n\n이제 위 세 가지 주제에 대해 순서대로 자세히 살펴보겠습니다."
  },
  {
    "objectID": "posts/paper/2025-07-30-dexmachina.html#방법론의-독창성-dexmachina만의-리타게팅-접근법",
    "href": "posts/paper/2025-07-30-dexmachina.html#방법론의-독창성-dexmachina만의-리타게팅-접근법",
    "title": "📃DexMachina 리뷰",
    "section": "1. 방법론의 독창성: DexMachina만의 리타게팅 접근법",
    "text": "1. 방법론의 독창성: DexMachina만의 리타게팅 접근법\n인간 시연의 “기능”을 학습 목표로 삼다. DexMachina는 인간 손-물체 상호작용 시연(예: 사람이 두 손으로 와플 기계를 들어 열었다 닫는 시연)을 입력으로 받아, 로봇의 두 손이 물체의 동일한 기능적 결과를 재현하도록 정책을 학습시킵니다. 여기서 기능적 결과란 물체의 상태 변화에 초점을 둔 것으로, 사람 시연과 똑같은 동작 경로를 그리지 않더라도 물체의 움직임이 같다면 성공으로 간주합니다. 이는 사람 움직임 자체를 따라하려는 기존의 접근과 근본적으로 다릅니다. 예를 들어 사람이 공을 던지는 시연이 주어졌을 때, DexMachina는 로봇이 공을 같은 목표에 맞히는 것에 집중하지, 인간의 모든 손가락 각도를 그대로 재현하려 하지는 않습니다. 이러한 발상의 전환 덕분에, 로봇과 인간 손 구조가 달라도 Task 중심으로 학습이 이루어질 수 있습니다.\n시연 데이터로 자동 Reward 설계: DexMachina는 한 편의 인간 데모(모션 캡쳐된 손/물체 궤적)만으로도 추가 Reward 설계 없이 학습 목표를 정의합니다. 구체적으로, 데모의 물체 상태 궤적을 추출하여 로봇이 따라가야 할 Task Reward(task reward)을 정하고, 인간 손동작을 로봇 손으로 충돌 없게 변환한 기준 모션을 계산하여 모션 모방 Reward으로 활용합니다. 또한 데모에서 손-물체 접촉 지점을 근사추정하여, 로봇 손도 비슷한 지점에 접촉하도록 접촉 Reward을 부여합니다. 흥미로운 점은, 로봇 손목(wrist) 움직임은 데모 궤적을 최대한 따르도록 제한하고 손가락 관절은 절대 제어하게 하는 하이브리드 방식을 취한다는 것입니다. 이렇게 하면 큰 팔 동작은 인간과 비슷하게 유지하면서도, 세밀한 손가락 움직임은 로봇이 자유롭게 조정하여 자기 구조에 맞게 최적화할 수 있습니다. 요약하면, DexMachina는 데모 → (Task + 모션 + 접촉) Reward으로 변환하는 객체 중심 학습 목표를 자동 구축하여, 별도의 Reward 함수 설계 없이도 효과적인 강화학습 환경을 마련합니다.\n“Deus ex machina” – 가상 스프링으로 시작하는 커리큘럼 RL: DexMachina라는 이름에는 “기계에서 내려온 신”이라는 뜻이 담겨 있는데, 이는 알고리즘이 초반에 마치 보이지 않는 손처럼 물체를 움직여주는 모습에 착안한 것입니다. 정책 학습이 특히 어려운 장기간 양손 조작의 경우, 두 손의 미세한 협응 실패로 초반에 곧잘 임무가 좌초되기 쉽습니다. 이를 해결하기 위해 DexMachina는 자동 커리큘럼(auto-curriculum) 전략을 도입했습니다. 초기 학습 단계에서는 가상 객체 제어기(Virtual Object Controller)가 일종의 스프링 힘으로 물체를 사람이 보여준 방향대로 움직여 줍니다. 정책은 이 때 실패 위험 없이 따라하는 법을 배우고, 점차 정책이 성과를 내기 시작하면 가상 힘의 세기를 줄여 정책이 자율적으로 물체를 조작하도록 만듭니다. 이러한 점진적 난이도 상승은 강화학습에서 흔히 쓰이는 커리큘럼 학습 개념을 응용한 것으로, 물체 물리 동역학을 제어하는 외력으로 난이도를 조절하는 점이 특징입니다.\n\n\n\n\nDexMachina 알고리즘의 개요도. 왼쪽은 인간 양손 데모에서 얻은 물체 상태 변화(예: 와플 기계 뚜껑의 각도)와 로봇 모션/접촉 Reward 신호 추출 과정을 보여준다. 오른쪽은 자동 커리큘럼 학습 과정으로, 초기에는 강한 가상 스프링(가상 객체 제어기)이 물체를 목표대로 움직여주어 로봇 정책이 실패 없이 모방학습을 하고, 중기에는 약한 스프링으로 Aux를 줄이면서, 최종적으로 스프링 없이 정책 혼자서 물체를 조작하도록 훈련된다. 이러한 방식으로 DexMachina는 운동학적 리타게팅의 초기 가이드와 강화학습의 자율 탐색을 자연스럽게 연결한다.\n\n기존 기법 대비 차별성: DexMachina의 방법론은 기존 리타게팅 및 모션 트랜스퍼 기술과 몇 가지 중요한 차이를 보입니다. 첫째, 사람 손동작을 로봇에 단순 이식하던 접근과 달리, DexMachina는 사람-로봇 간 공통의 Task 공간(object state trajectory)에서 문제를 정의하여 물리적으로 실행 가능한 전략을 학습합니다. 예컨대 Park 등(2025)은 인간-로봇-물체 움직임의 공동 임베딩 공간을 학습하여 거기서 로봇 동작을 추론하는 방식으로 데이터 기반 리타게팅을 수행하였는데, DexMachina는 이를 강화학습 정책으로 구현함으로써 새로운 손/환경에도 온라인 적응 능력을 부여합니다. 둘째, 기존 많은 텔레오퍼레이션 기반 시연 방법들은 로봇 손마다 별도 시스템 구축이 필요하고 주로 단발적 그립 동작에 그쳤지만, DexMachina는 단 하나의 시연만으로 장시간의 복잡한 조작을 가능케 했습니다. 또한 DeepMimic이나 DAPG(Rajeswaran et al., 2018)처럼 예제 모션을 RL로 따라하는 선행 연구들이 있었지만, 대부분 단일 로봇 손에 단일 작업을 다루고 Reward 신호도 제한적이었습니다. 반면 DexMachina는 Reward 구성의 다양화(Task+모션+접촉)와 외력 커리큘럼이라는 새로운 요소로 이러한 모션 트랜스퍼 문제를 확장했고, 이를 통해 두 손이 협응해야 하는 복잡한 작업도 자동학습이 가능함을 보였습니다. 종합하면 DexMachina는 “시연 데이터 + 강화학습 + 커리큘럼”의 세 박자를 맞추어, 기존 방법들이 부분적으로만 해결했던 문제들을 하나의 프레임워크에 통합한 점에서 독창적입니다."
  },
  {
    "objectID": "posts/paper/2025-07-30-dexmachina.html#실험-결과-분석-성능-적응성-일반화-능력",
    "href": "posts/paper/2025-07-30-dexmachina.html#실험-결과-분석-성능-적응성-일반화-능력",
    "title": "📃DexMachina 리뷰",
    "section": "2. 실험 결과 분석: 성능, 적응성, 일반화 능력",
    "text": "2. 실험 결과 분석: 성능, 적응성, 일반화 능력\n벤치마크 구성 및 평가 지표: 저자들은 사람이 양손으로 수행한 긴 조작 시연 7개를 선택하여, 이를 모사해야 하는 시뮬레이션 환경을 만들었습니다. 해당 시연 데이터는 ARCTIC이라는 공개 데이터셋의 일부로, 예를 들어 박스를 집어들어 뚜껑 열기, 노트북 들어 덮기, 믹서기 뚜껑 닫기, 와플 기계 들어 열기 등 5가지 물체에 대한 단기/장기 과제들을 포함합니다. 로봇 손으로는 오픈소스 6종의 로봇 핸드(Inspire, Allegro, X-Hand, Schunk, Ability, DexRobot 등 크기와 구조가 다양한 모델)를 활용했습니다. 성능 평가는 물체 추적 오차 기반 지표를 사용했는데, 데모의 물체 궤적 대비 로봇이 조작한 물체의 위치/자세가 얼마나 일치하는지를 AUC-ADD(평균 거리 오차에 대한 곡선 아래 면적) 형태의 점수로 환산하고, 이를 성공률(%)로 표현했습니다. 직관적으로 100%면 물체를 정확히 따라간 것이고, 0%면 전혀 따라가지 못한 것입니다.\n주요 비교 방법: 실험에서는 DexMachina를 다음과 같은 방법들과 비교했습니다:\n\nKinematic Only: 인간 운동학적 리타게팅 결과를 그대로 재생. (정책 학습 없음)\nTask Rew Only: ObjDex라 명명된 기준으로, Task Reward만으로 RL 정책 학습 (인간 데모의 물체 궤적만 목표, 모션/접촉 Reward 없음).\nTask + Aux Reward: DexMachina에서 제안한 Task + 모션 + 접촉 Reward은 쓰되, 커리큘럼 없이 학습 (즉, 가상 객체 제어 미사용).\nManipTrans (2025): Li 등(2025)이 제안한 최신 방법으로, 사전 모방학습 + 잔여(residual) 정책 RL 2단계로 인간 양손 시연을 이식하는 기법. (저자들이 공개한 코드/데이터를 이용한 결과)\n\n\n\n\n\n여러 방법의 성공률 비교 (높을수록 우수). 가로축은 작업 종류(예: Ketchup-100은 짧은 케첩 통 흔들기 과제, Waffleiron-300은 긴 와플기계 조작 과제)이며, 세로축은 해당 과제 완료 성공률(%)입니다. 노랑은 학습 없이 운동학적 리타게팅만으로 실행한 경우, 갈색(ObjDex)은 Task Reward만으로 RL한 경우, 회색은 Task+Aux Reward으로 RL했으나 커리큘럼 없는 경우, 녹색(DexMachina)은 제안 기법 (Aux Reward+커리큘럼)이며, 진갈색(ManipTrans)은 최신 2단계 잔여학습 방식입니다. DexMachina(녹색)가 대부분의 과제에서 최고 성능을 보이며, 특히 난이도가 높은 -300 장기 과제들에서 두드러지게 앞서는 것을 알 수 있습니다. 또한 ManipTrans 대비해서도 DexMachina가 많은 경우 높은 성공률을 보이는데, 이는 제안 기법의 탁월한 장기 탐색 능력을 방증합니다. 실제로 ManipTrans 연구에서도 자사 방법이 성공률과 모션 재현 정확도에서 기존 기법들을 능가한다고 보고하였지만, DexMachina의 커리큘럼 전략이 동일 과제에서 한층 높은 성과를 낸 것을 확인할 수 있습니다.\n\n종합 성능: 결과 그래프를 보면 DexMachina(녹색 막대)가 모든 실험 환경에서 가장 높은 성공률을 기록함을 알 수 있습니다.\n\n단순 리타게팅 재생(Kinematic Only)의 경우 사람과 로봇 손 구조 차이로 인해 물체를 제대로 다루지 못해 성공률이 거의 0에 수렴하며, Reward만 준 RL(ObjDex)도 초반 탐색 실패로 장기 작업을 끝내지 못하는 경우가 많았습니다.\n반면 DexMachina는 커리큘럼이 있는 경우 없는 경우 대비 크게 향상되어, 모든 손과 모든 작업에서 일관되게 최고의 성능을 달성했습니다. 특히 각 작업 이름에 -300이 붙은 장기 시나리오(예: Notebook-300, Waffleiron-300 등)에서 그 격차가 두드러졌습니다.\n\n정량적 수치 및 분석:\n\nDexMachina는 전반적인 평균 약 85% 수준의 높은 성공률을 기록하였는데, 이는 기존 방법들에 비해 크게 향상된 수치입니다.\n특히 단기 과제의 경우 모든 로봇 손이 70~90%에 달하는 준수한 성과를 거두었고, 장기 과제에서도 가장 어려운 시나리오(Waffleiron-300 등)조차 성공률 40~80% 범위를 달성하여 난제 해결의 가능성을 보여주었습니다.\n반면 Aux Reward이나 커리큘럼이 없었던 RL 정책은 장기 과제에서 0~30% 수준에 머무는 등 불안정한 모습을 보였으며, 이는 초기 탐색 실패와 접촉 타이밍 학습 미비 등에 기인합니다.\nDexMachina는 가상 제어기의 초기 개입 덕분에 이러한 실패 구간을 건너뛰고 효과적으로 학습을 진행, 에피소드 말미까지 임무를 완수하는 비율을 크게 끌어올린 것입니다.\n\n질적 결과: 적응적 전략의 학습\n\n흥미로운 것은, DexMachina로 학습된 정책이 주어진 인간 시연을 맹목적으로 복제하지 않고, 로봇 자신의 신체에 맞게 전략을 재구성했다는 점입니다.\n예를 들어 노트북 덮기 작업에서, XHand 로봇 손은 인간 시연과 동일하게 왼손으로 노트북을 들고 오른손으로 덮개를 닫는 전략을 따라한 반면, 더 작고 자유도가 낮은 Inspire Hand는 양손 모두로 노트북을 지탱하면서 덮개를 닫는 방식으로 임무를 완수했습니다.\n동일한 인간 시연을 참고했음에도 각 로봇의 크기와 관절 한계에 최적화된 동작을 스스로 찾아낸 것입니다. 비슷하게 믹서기 뚜껑 닫기 작업에서는, Allegro Hand가 사람처럼 긴 엄지손가락을 활용해 뚜껑을 눌러 닫은 반면, 구조가 다른 Schunk Hand는 손바닥과 손목을 이용해 뚜껑을 밀어 닫는 등 상이한 접근을 보였습니다.\n이러한 사례들은 DexMachina의 정책이 하드웨어 제약에 적응하여 기능적 목표를 달성하는 법을 학습했음을 보여줍니다. 사람이 시연한 방식을 그대로 흉내내는 것이 아니라, 시연의 의도를 이해해 로봇 자신의 방식으로 임무를 달성했다는 점에서 의미가 있습니다.\n\n로봇 손 설계 간 비교:\n\n저자들은 나아가 제안한 벤치마크를 활용한 로봇 손 설계 비교 실험도 수행했습니다.\n모든 핸드는 동일한 인간 손 모션 참조를 사용하지만, 정책이 사람의 지침에서 벗어나는 정도는 핸드 크기와 운동학적 제약에 따라 달라집니다.\n동일한 네 가지 장기 과제에 대해 서로 다른 로봇 손들이 DexMachina로 학습했을 때의 성능을 비교한 결과, 더 큰 크기이면서 모든 손가락이 능동 구동되는 손일수록 학습 효율과 최종 성공률이 높게 나타났습니다.\n\n예를 들어 Schunk Hand나 X-Hand는 작은 Inspire Hand나 Ability Hand보다 성공률과 학습속도 모두 우수했는데, 이는 단순한 크기 차이뿐 아니라 자유도의 차이에서 기인한 것으로 분석됩니다.\n\n크기보다 자유도(degrees of freedom, DOF)가 더 중요한 성능 결정 요인임을 발견했습니다.\n큰 사이즈와 완전히 작동하는 핸드(fully-actuated hands)는 학습 효율성과 최종 성능 면에서 뛰어나며, 긴 손가락을 가진 Allegro Hand가 특히 우수한 안정성을 제공합니다.\n\n크기가 비슷한 Inspire, Ability, Schunk 핸드 중에서는 Schunk 핸드가 손가락 끝 부분이 작동하고 접히는 손바닥을 가지고 있어 평균적으로 더 나은 성능을 냈습니다.\n\n적게 작동하는 핸드들은 인간 손과 더 닮아 보이지만 학습된 전략은 더 크고 기능적인 핸드보다 덜 인간적입니다.\n결과적으로 Inspire와 Ability 핸드는 주어진 과제를 수행하기 위해 다른 전략을 자주 선택해야 합니다.\n구동 가능한 관절 수(DoF)가 많은 손은 물체를 다루는 대체 동작을 찾기 쉽기 때문에 학습에 유리하며, 반대로 인간 손 크기에 가깝더라도 제약이 많은 손은 학습 난이도가 높았습니다.\n이러한 정량적 비교는 DexMachina가 제시한 하나의 중요한 활용 예로, 동일한 알고리즘 아래 여러 로봇 손의 기능적 성능을 객관적으로 평가할 수 있음을 보여줍니다. 이는 향후 새로운 로봇 핸드 설계시 어떤 구조가 실제 작업에 유리한지 가늠하는 데에도 큰 도움을 줄 것으로 기대됩니다."
  },
  {
    "objectID": "posts/paper/2025-07-30-dexmachina.html#기존-연구와의-비교-dexmachina의-기여도와-차별화",
    "href": "posts/paper/2025-07-30-dexmachina.html#기존-연구와의-비교-dexmachina의-기여도와-차별화",
    "title": "📃DexMachina 리뷰",
    "section": "3. 기존 연구와의 비교: DexMachina의 기여도와 차별화",
    "text": "3. 기존 연구와의 비교: DexMachina의 기여도와 차별화\n마지막으로, DexMachina를 유사한 목표를 가진 기존 프레임워크/논문들과 비교하여 그 기술적 위치를 살펴보겠습니다. 크게 (a) 기존 리타게팅 기법, (b) 시연 기반 학습(RL/IL) 기법, (c) 최근 발표된 유사 연구 세 범주로 나누어 논의합니다.\n(a) 전통적 리타게팅 vs. DexMachina: 과거의 손 동작 리타게팅 기술은 주로 인간 손가락 궤적을 로봇 손으로 대응시켜보는 수준이었습니다. 예를 들어 VR 장갑이나 모캡으로 인간 손 움직임을 읽어 로봇 손가락 관절로 매핑하는데, 이는 로봇과 인간의 형태 차이 때문에 충돌을 일으키거나 물체를 제대로 쥐지 못하는 경우가 많았습니다. Park 등(2025)은 이러한 기존 end-effector 정렬 기반 리타게팅이 비현실적인 동작을 만들기 쉽다고 지적하며, 인간-로봇-물체 사이의 공동 모션 manifold를 학습하여 보다 플라우저블(plausible)한 로봇 동작을 얻는 방법을 제시하였습니다. 해당 방법은 대량의 인간 시연 데이터로 인간-물체-로봇 사이의 관계 모델을 훈련한 후, 주어진 새로운 시연에 대해 로봇 행동을 직접 추론하는 접근으로, 실제 로봇에 실험하여 기존 단순 매핑보다 성공률 향상을 시연했습니다. DexMachina 역시 인간 시연→로봇 동작이라는 큰 흐름은 같지만, 접근법은 사뭇 다릅니다. DexMachina는 명시적으로 물리 시뮬레이션 환경에서 RL로 정책을 학습하므로, 중간에 인간 모델을 로봇으로 변환하는 모션 생성기가 필요한 대신, 강화학습 자체가 모션을 만들어내는 역할을 합니다. 이로써 한편으로는 시연이 부족한 상황에서도 (정책이 탐색을 통해) 답을 찾아갈 수 있고, 다른 한편으로는 학습된 정책이 온라인으로 물체 반응에 대응할 수 있어 강인성을 얻습니다. 다만 DexMachina는 현재 시뮬레이션 학습에 집중하고 있어 즉시 실세계 로봇에 적용되지는 않았는데, Park 등의 접근은 애초에 실물 로봇 대상 데이터로 학습/검증되었다는 차이가 있습니다. 요약하면, 기존 리타게팅 기법들이 “모델을 학습시켜 한 번에 매핑”하는 경향이라면, DexMachina는 “시뮬레이터 안에서 직접 배우게” 하는 방식으로 문제를 푼다고 볼 수 있습니다. 이 과정에서 커리큘럼을 통한 탐색 Aux라는 혁신을 추가하여, 인간처럼 양손 협응이 필요한 복잡한 작업도 실패 없이 학습하게 만들었다는 점이 두드러집니다.\n(b) 시연 기반 강화학습(IL/RL) vs. DexMachina: 인간 시연을 활용하는 기법들은 모방학습(IL)과 Reward 강화학습(RL)로 크게 나뉩니다. 모방학습의 경우 시연 데이터만으로 정책을 모방하게 하는데, 로봇 손의 경우 정확한 로봇행동-결과 페어 데이터 수집이 어려워 제한적이었습니다. 이를 극복하고자 텔레오퍼레이션 장비(VR 장갑 등)를 이용해 사람이 로봇 손을 직접 원격조작하며 데이터를 모은 연구들이 다수 있었으나, 특정 로봇에 시스템을 특화해야 하고 주로 단순 집기(grasp) 등 짧은 작업에 머무는 한계가 있었습니다. 반면 DexMachina는 단 한 번의 인간 시연만으로도 긴 작업을 배울 수 있도록 했고, 인간-로봇 사이의 자세한 매핑 데이터 없이도 학습이 가능하다는 점에서 데이터 효율성을 보여주었습니다. 한편 강화학습+시연 혼합 기법으로 2018년 DAPG 등이 인간 시演을 초기 정책으로 활용하고 추가 RL 훈련을 통해 성과를 낸 바 있습니다. 그러나 DAPG는 문 손잡이 돌리기 등 단일 손의 비교적 짧은 작업을 대상으로 했고, Reward도 시연 모션 모방과 작업 완료 신호를 수동 설정하는 식이었습니다. DexMachina는 이러한 선행들의 교훈 – 시연이 주는 탐색 가이드 효과 – 을 받아들이면서도, Reward 신호를 자동으로 구성하고 외력 지원으로 탐색 효율을 높이는 등 여러 개선을 통해 문제 난이도를 한 단계 끌어올린 사례라 할 수 있습니다. 특히 장기간의 연속적인 물체 조작이라는 새로운 영역에서 시연+RL의 위력을 입증한 점은 학술적으로 의미가 큽니다.\n(c) 최신 유사 연구들과의 비교: DexMachina와 같은 시기에 발표된 몇몇 연구들도 양손 조작 학습에 도전하고 있어 흥미로운 비교가 됩니다. ManipTrans (Li et al., CVPR 2025)는 DexMachina와 동일하게 인간의 양손 기술을 로봇에 전이하는 목표를 갖되, “두 단계”로 접근한 것이 특징입니다. 먼저 인간 시연을 흉내내는 트래젝토리 모방 모델을 학습한 뒤, 이를 기반으로 잔여 정책(residual policy)을 강화학습으로 파인튜닝하는 구조입니다. 이러한 사전학습+미세조정 방식은 데이터 효율을 높이고 학습을 가속하는 효과가 있어, 저자들은 다양한 시연 데이터로 거대한 DexManipNet이라는 로봇 조작 데이터셋까지 구축하였습니다. 결과적으로 ManipTrans 역시 높은 성공률과 정확도를 보였지만, 정책 최종 성능 측면에서는 DexMachina가 앞서는 것으로 DexMachina 논문 실험에서 확인되었습니다 (위 결과 그래프에서 갈색 막대 비교). 이는 잔여 학습단계의 제한적 탐색보다 DexMachina의 초기부터 끝까지 RL로 최적화하는 접근이 장기적으로 더 나은 솔루션을 찾았기 때문으로 추정됩니다. 다만 ManipTrans는 시연 모방 모델 덕분에 3천 개 이상의 다양한 작업 에피소드를 생성하여 데이터 다양성을 확보한 반면, DexMachina는 각 작업별 한 개의 시연에 집중합니다. 따라서 일대일 전이 학습 효율은 DexMachina가 높지만, 대량의 시연을 일반화하는 측면은 ManipTrans 쪽이 방향성이 다르다고 볼 수 있습니다. 또 하나 주목할 것은 실제로봇 적용인데, ManipTrans 쪽은 시뮬레이션에서 학습한 정책을 실제 로봇으로 이식하는 실험을 시도하고 있습니다. DexMachina는 현재 결과가 모두 시뮬레이터 상이지만, 정밀한 상태 입력에 의존하고 있어 시각센서 기반 정책으로의 확장은 과제로 남아 있습니다. 저자들도 향후 비전 기반 RL 정책이나 고급 센서 데이터 통합을 통해 실세계 적용을 모색할 수 있다고 언급하였습니다.\n마지막으로, Videodex (Shaw et al., 2022)나 XSkill (Xu et al., 2023)처럼 사전 녹화 동영상이나 다른 로봇의 경험으로부터 간접적으로 기술을 학습하는 시도들도 있습니다. Videodex는 웹 비디오로부터 인간의 조작 시퀀스를 추출해 로봇에 학습시켰고, XSkill은 교차 형태 간(skill transfer across embodiments) 유용한 스킬을 발견하는 방법을 제안했습니다. 이러한 연구들은 시연 데이터의 형태가 다르지만, 로봇이 인간 수준의 다양한 조작을 배우는 방법을 탐구한다는 점에서 DexMachina와 맥을 같이 합니다. DexMachina의 가치는 특히 고품질의 한정된 시연을 최대한 활용하여 복잡한 임무를 달성하는 쪽에 있는데, 이는 향후 비디오나 저해상도 데이터에도 응용될 수 있는 통찰을 줍니다. 또한 AnyTeleop (Qin et al., 2023) 같은 원격 조작 시스템들은 사람의 즉각적인 조작을 로봇으로 투영하여 복잡한 임무를 수행했는데, DexMachina는 한 걸음 더 나아가 이러한 인간 개입 없이도 자율 정책으로 임무를 지속 수행하게 만들었다는 점에서 완전자율성에 한층 가까워졌습니다."
  },
  {
    "objectID": "posts/paper/2025-07-30-dexmachina.html#conclusion",
    "href": "posts/paper/2025-07-30-dexmachina.html#conclusion",
    "title": "📃DexMachina 리뷰",
    "section": "Conclusion",
    "text": "Conclusion\nDexMachina는 인간 시연으로부터 양손 로봇 조작 기술을 배우는 새로운 방법론을 제시함으로써, 현재 활발한 섬세 조작(dexterous manipulation) 연구 분야에 큰 진전을 가져왔습니다. 방법론적으로 보면, Reward 설계의 자동화와 커리큘럼을 통한 탐색 지원을 결합하여 강화학습의 취약점을 효과적으로 보완한 점이 돋보입니다. 실험적으로는 다양한 로봇 손에 걸쳐 일관된 성능 우위를 증명함으로써, 제안 기법의 범용성과 실용적 가치를 입증했습니다. 특히 하나의 프레임워크로 하드웨어 성능을 비교 평가할 수 있다는 관점은, 향후 로봇 손 개발자들이 디자인 선택을 최적화하는 데에도 기여할 수 있을 것입니다.\n물론 해결해야 할 과제도 남아 있습니다. 앞서 언급했듯이 DexMachina는 시뮬레이션 상태정보에 크게 의존하고 있어, 이를 실세계 센서 입력(시각/촉각)으로 옮기는 작업이 필요합니다. 또한 현실에서는 예기치 못한 교란이나 물체 모델의 불확실성 등이 존재하므로, 정책이 오류 복구나 적응적 재계획을 할 수 있도록 강화하는 연구도 중요할 것입니다. 데이터 수집 측면에서는, 현재는 사람 시연을 별도로 캡쳐해야 하지만, 미래에는 3D 비전이나 모션 캡쳐 자동화 기술의 발전으로 보다 손쉽게 시연 데이터를 확보할 수 있을 것입니다. 이러한 보완이 이루어진다면 DexMachina의 접근법은 산업 현장이나 서비스 로봇에서 사람처럼 도구를 다루고 협업하는 로봇을 훈련하는 데 큰 역할을 할 것으로 기대됩니다.\n결론적으로, DexMachina는 기능적 리타게팅이라는 개념을 통해 로봇에게 “동작의 형태”보다 “동작의 목적”을 가르치는 법을 보여주었습니다. 이는 향후 인간 수준의 다재다능한 로봇 조작을 실현하는 데 중요한 방향을 제시하며, 현재 진행 중인 많은 후속 연구들의 기반이 되고 있습니다. 인간이 시연하고 로봇이 배워서 자기만의 방식으로 임무를 수행하는 모습은, 궁극적으로 휴먼-로봇 협업과 자율 기술 학습의 접점에서 매우 유망한 패러다임이라 할 수 있습니다."
  },
  {
    "objectID": "posts/paper/2025-09-02-grasp-mpc.html",
    "href": "posts/paper/2025-09-02-grasp-mpc.html",
    "title": "📃GraspMPC 리뷰",
    "section": "",
    "text": "Paper Link\nHomepage"
  },
  {
    "objectID": "posts/paper/2025-09-02-grasp-mpc.html#논문의-주요-기여-및-기술적-혁신-분석",
    "href": "posts/paper/2025-09-02-grasp-mpc.html#논문의-주요-기여-및-기술적-혁신-분석",
    "title": "📃GraspMPC 리뷰",
    "section": "2.1 1. 논문의 주요 기여 및 기술적 혁신 분석",
    "text": "2.1 1. 논문의 주요 기여 및 기술적 혁신 분석\nGrasp-MPC 프레임워크 개요: 다양한 객체 데이터셋(Objaverse의 8천여 개 객체)으로부터 파지(그립) 자세를 생성하고, 모션 플래닝을 통해 200만 개 이상의 파지 궤적 데이터를 시뮬레이션에서 수집한다. 이 궤적 데이터에는 성공한 시도와 실패한 시도가 모두 포함되며, 이를 활용해 포인트 클라우드 관측 기반의 가치 함수(value function)를 학습한다. 학습된 가치 함수는 파지 성공 확률을 예측하며 MPC의 비용 함수로 활용되어, 충돌 회피 등의 제약 조건 하에 로봇의 파지 동작을 실시간 생성한다. 오른쪽 그림은 Grasp-MPC를 사용하여 복잡한 환경의 새로운 물체를 안전하게 파지하는 UR10 로봇의 예시를 보여준다.\nGrasp-MPC는 복잡한 환경에서 새로운 물체들을 다룰 수 있는 폐루프(closed-loop) 6-자유도 비전 기반 파지 기법으로, 기존 접근법들의 한계를 극복하기 위해 제안되었다. 이 방법은 open-loop 방식(사전에 예측된 파지 자세로 이동)과 closed-loop 제어(실시간 피드백을 통한 조정)의 강점을 결합한 것이 핵심이다. 구체적으로, Grasp-MPC는 최신 그립 포즈 예측 모델을 이용해 로봇을 대략적인 pre-grasp 자세까지 먼저 이동시킨 후, 그 지점부터 모델 예측 제어(MPC)를 사용하여 폐루프 방식으로 파지를 완료한다. 이를 통해 기존 open-loop 기법이 겪는 그립 위치 예측 오차나 물체 위치 변화에 실시간으로 대응할 수 있으며, 로봇이 목표를 동적으로 조정하면서 파지를 성공시킬 수 있게 된다.\nGrasp-MPC의 가장 큰 기술적 혁신은 MPC의 비용 함수로 학습된 가치 함수를 도입한 점이다. 전통적인 방법에서는 예측된 파지 자세까지의 거리 등의 기하학적 비용함수를 사용했지만, 이는 오차에 취약하고 MPC의 폐루프 잠재력을 충분히 활용하지 못했다. 반면 Grasp-MPC에서는 시뮬레이션을 통해 대규모로 수집된 파지 궤적 데이터(성공과 실패 사례 모두 포함)로부터 비전 기반 가치 함수를 학습하고 이를 과제 비용(task cost)으로 사용한다. 이 가치 함수는 주어진 물체의 포인트 클라우드와 로봇 말단Effector 자세를 입력으로 파지 성공 가능성을 예측하며, MPC 내부에서 과제 성공도를 나타내는 비용 항으로 작용한다. 결과적으로 로봇은 이 비용을 최소화하는 방향으로 상태 공간을 탐색하면서 파지 행동을 생성하게 되며, 이는 곧 파지 성공 확률을 최대화하도록 유도한다. 추가적으로, MPC의 최적화 과정에는 충돌 회피(collision avoidance)와 동작의 부드러움(minimum jerk)을 위한 비용 항도 포함되어 있어 복잡한 환경에서 안전한 파지 경로를 생성할 수 있다. 이러한 학습된 가치 함수 기반 폐루프 제어는 동적이고 협소한 공간에서도 파지를 안정적이고 신뢰성 있게 수행할 수 있게 해주는 본 논문의 핵심 기술적 기여이다.\n논문에서 저자들이 강조한 Grasp-MPC의 주요 기여는 다음과 같이 정리된다:\n\n안전한 폐루프 비전 기반 파지 정책 제안: Grasp-MPC는 복잡한(cluttered) 환경에서 새로운 물체들을 다룰 수 있는 안전한 폐루프 시각 파지 정책을 제시한다. 즉, 실시간 센서 피드백을 통해 동적으로 조정하면서도, 충돌을 피하고 안정적으로 물체를 잡을 수 있는 정책이다.\n모델 기반 제어와 데이터 기반 학습의 통합: Grasp-MPC는 모델 기반 제어 (MPC)와 데이터 기반 접근을 통합하여, 학습된 그립 가치 함수를 MPC 프레임워크에 결합한 새로운 구조를 구현했다. 이를 통해 반응적이고 제약을 준수하는(grasping with reactive, constraint-aware) 파지 실행이 가능해졌으며, 동적인 환경에서도 실시간 적응이 가능하다.\n대규모 합성 파지 궤적 데이터셋 구축: 저자들은 대규모 시뮬레이션 파지 궤적 데이터셋을 구축하였는데, 수백만 건(M=2,000,000+)의 파지 경로와 상태, 그리고 수천 개에 이르는 다양한 객체를 포함한다. 이렇게 방대한 Objaverse 객체를 활용한 데이터셋은 일반화 가능한 가치 함수를 학습하는 데 기여하였으며, 이전 방법들보다 훨씬 다양한 시나리오를 포괄한다.\n폭넓은 실험을 통한 성능 검증: 시뮬레이션(FetchBench 벤치마크)과 실제 로봇 실험 모두에서 광범위한 평가를 수행한 결과, Grasp-MPC는 기존 최신 open-loop 및 closed-loop 방법들을 의미 있게 능가하는 성능 향상을 보여주었다. 특히 시뮬레이션 환경에서 최대 32.6%, 실제 복잡한 환경에서 33.3%까지 파지 성공률을 개선하여, diffusion 정책, Transformer 기반 정책, IQL 등 다양한 기존 접근법 대비 두 자릿수 이상의 향상을 달성하였다. 이러한 결과는 본 기법의 실용성과 일반화 성능을 잘 뒷받침해준다.\n\n요약하면, Grasp-MPC는 대규모 학습된 가치 함수를 MPC 제어 루프에 통합함으로써, 이전의 파지 방법들이 지니던 실시간 피드백 부족 문제, 일반화 한계, 안전성 문제를 모두 아우르는 새로운 솔루션을 제공한 것이다. 이러한 기술적 혁신은 향후 복잡한 로봇 조작 분야에서 피드백 기반의 안전한 제어와 데이터 활용을 접목시키는 데에 중요한 방향을 제시한다."
  },
  {
    "objectID": "posts/paper/2025-09-02-grasp-mpc.html#실험-설정-및-결과-평가",
    "href": "posts/paper/2025-09-02-grasp-mpc.html#실험-설정-및-결과-평가",
    "title": "📃GraspMPC 리뷰",
    "section": "2.2 2. 실험 설정 및 결과 평가",
    "text": "2.2 2. 실험 설정 및 결과 평가\n논문에서는 시뮬레이션과 실제 로봇 환경에서 Grasp-MPC의 성능을 평가하여 그 유효성과 우수성을 입증했다. 실험은 다음과 같은 핵심 질문들을 중심으로 설계되었다:\n\nGround Truth 파지 자세가 주어졌을 때, Grasp-MPC는 새로운 물체를 얼마나 잘 파지하는가? (이론적으로 최적의 파지 위치가 주어진 경우 성능 평가)\n파지 자세에 오차/노이즈가 있는 경우(예: 예측 오류로 인한 위치 편차), Grasp-MPC는 얼마나 견고하게 파지를 수행하는가?\n학습된 그립 포즈 예측 모델을 사용하여 파지할 때(즉, 실제로는 이상적인 파지 자세가 아닌 예측된 목표로 파지 시도), Grasp-MPC의 성능은 어떠한가?\n\n시뮬레이션 환경 설정: 저자들은 시뮬레이션 평가를 위해 FetchBench라는 표준 환경을 활용하였고, 로봇으로 UR10 팔과 Robotiq 2F-140 그리퍼를 사용했다. 평가 장면은 다양한 복잡한(cluttered) 환경으로 구성되었으며, 실험에 사용된 물체들은 모두 새로운 객체(학습 시 사용되지 않은 객체)로 채워졌다. 3개의 카메라로부터 포인트 클라우드를 수집하여 정책에 입력으로 사용하였고, 각 실험 장면마다 여러 가지 파지 시나리오를 구성하여 총 수천 회 이상의 시험을 진행했다. 성능 평가 척도로는 파지 성공률을 사용했는데, 이는 로봇이 물체를 집은 후 일정 높이 이상 들어올리는지로 정의된다. (모션 플래닝으로 pre-grasp 지점까지 이동하는 과정에서 실패한 경우는 파지 시도가 이루어지지 않았으므로 별도로 제외하여 성공률을 계산).\n비교 방법(베이스라인): Grasp-MPC의 성능을 검증하기 위해 여러 기존 방법들과 비교 실험을 수행했다. Open-loop 접근의 대표로는 OSC(Operational Space Control) 기반 직선 이동 파지 방법을 사용하여, 이는 FetchBench에서도 oracle에 가까운 baseline으로 쓰이는 방식이다. 또한 모방학습 기반 폐루프 정책인 Transformer Policy도 포함되었는데, 이는 FetchBench 벤치마크에 사용된 Transformer 아키텍처의 정책이다. 더불어, Diffusion Policy (확산 정책) 방식의 폐루프 IL 정책도 평가에 포함되었다 – 이 방법은 포인트 클라우드를 입력으로 사용하는 최신 상태의 모방학습 기반 파지 정책 중 하나이다. 마지막으로, 오프라인 강화학습 기반의 정책인 IQL (Implicit Q-Learning)을 비교에 포함시켰다. 모든 방법들은 Grasp-MPC와 동일하게 사전에 모션 플래너(CuRobo)를 이용해 로봇을 지정된 pre-grasp 위치까지 움직인 뒤, 그 지점부터 각자의 방법으로 파지를 수행하도록 설정되었다. (참고로 IL 기반 정책들은 학습 시 성공한 파지 사례만으로 훈련되었다고 명시되어 있다.)\n시뮬레이션 실험 결과: 우선 이상적인 파지 자세(ground-truth annotation)가 주어지는 실험에서, Grasp-MPC는 사전계획(open-loop) 기반의 Oracle 성능에 근접하는 높은 성공률을 보였다. 구체적으로, Grasp-MPC의 파지 성공률은 약 73.6%에 달해, oracle에 해당하는 open-loop 방식(OSC)의 성능과 거의 유사한 수준을 달성했다. 더욱이 Grasp-MPC는 다른 폐루프 기반 방법들보다 현저히 높은 성공률을 기록했는데, 예를 들어 IQL의 경우 Grasp-MPC보다 훨씬 낮은 성공률(약 60%대)에 그쳤다. Transformer 기반 IL 정책과 Diffusion 정책 역시 Grasp-MPC보다 성능이 떨어졌으며, 이는 모션 플래닝으로 수집된 제한적인 시演 데이터로 학습한 데 따른 한계와, 훈련 환경과 평가 환경 간의 차이(domain mismatch)로 인한 성능 저하 때문이라고 분석된다. (IL 모델들은 주로 빈 테이블 환경에서 데이터 수집이 이루어졌는데, 정작 평가 시에는 물체가 많은 복잡한 환경이라 MDP 불일치가 발생했고, 이로 인해 일반화 성능이 낮았다는 설명이다.)\n다음으로 파지 자세에 노이즈가 추가된 실험(두 번째 질문)에서는 Grasp-MPC의 강인함(robustness)이 두드러졌다. Ground truth 파지 위치에 무작위 위치 오차(수 센티미터 변위와 회전 노이즈)를 섞어서 실행한 경우, open-loop 방식(OSC)은 성능이 약 40%p 급락하여 제대로 파지에 실패하는 반면, Grasp-MPC는 약 14%p 정도의 경미한 성능 감소만을 보이며 대부분의 시나리오에서 여전히 성공적으로 물체를 잡아냈다. 이는 폐루프 제어를 통해 피드백을 활용한 Grasp-MPC가 초기 목표 위치의 부정확함을 실시간 보정하며 대응하는 반면, open-loop은 한번 계획된 경로를 수정하지 못해 실패하기 때문이다. Grasp-MPC는 이 경우에도 다른 폐루프 baselines들보다 높은 성공률을 유지하여, 오프라인 RL이나 IL 기반 정책들보다 오차에 대한 내성이 높음을 보여주었다.\n세 번째로, 학습된 그립 포즈 예측 모델을 실제로 활용하는 시나리오에서도 Grasp-MPC의 성능 우수성이 입증되었다. 저자들은 M2T2라는 최신 grasp pose 예측 모델을 사용하여 물체의 파지 목표 자세를 예측하고, 이를 각 방법들의 입력으로 사용했다. 예측된 파지 자세에는 필연적으로 오차와 노이즈가 존재하기 때문에, 이 설정은 실제 로봇 적용에 가까운 시나리오다. 그 결과 IL 기반 방법들은 성공률 36.5% 수준에 그쳐 거의 파지에 실패하였고, open-loop 방식(OSC)은 약 63.6%의 성공률을 보였다 (이 값은 ground truth 사용 시보다 약 15%p 감소한 수치이다). 반면 Grasp-MPC는 67.2%의 성공률로 가장 좋은 성능을 기록했으며, ground truth 대비 성능 감소폭도 불과 8%p에 그쳐 예측 오차에 대한 견고함을 뚜렷이 보여주었다. 즉, 학습된 그립 예측 모델의 출력이 완벽하지 않음에도 불구하고, Grasp-MPC는 폐루프 보정과 가치 함수 기반의 안정적인 제어를 통해 최고의 파지 성공률을 달성한 것이다. 이러한 결과는 Grasp-MPC가 실제 로봇 현장에 투입될 경우를 고려할 때, 예측 모델의 불확실성에 강인한 솔루션임을 시사한다.\n실세계(real-world) 실험 설정: 시뮬레이션에서 유의미한 성능을 보인 Grasp-MPC를 실제 로봇 환경에서도 검증하였다. 실제 실험에는 UR10 로봇 팔과 Robotiq 2F-140 그리퍼를 사용했고, 이질적인 3가지 환경에서 평가가 이뤄졌다: (1) 물체가 거의 없는 빈 테이블 위, (2) 여러 새로운 물체들이 놓인 복잡한 테이블 위(cluttered tabletop), (3) 물체들이 선반에 놓인 복잡한 선반 환경(shelf clutter). 각 환경마다 서로 다른 객체 세트를 배치하여 다양성을 높였고, 각 물체에 대해 세 가지 서로 다른 초기 자세를 설정하여 반복 시험했다. 환경별로 수십 회 이상의 파지 시도를 통해 일관성과 신뢰성을 평가하였다. 로봇의 시각 센서는 RealSense L515 깊이 카메라 2대를 사용하여 실시간 포인트 클라우드를 생성했고, 목표 물체는 SAM-Track 기법으로 분리(segment)하여 인식했다. (SAM-Track은 Grounding DINO를 통한 객체 검출과 SAM(Segment Anything)으로 분할을 결합한 방법으로, 목표 물체의 포인트 클라우드를 추출해준다.) 또 주변 장애물을 다루기 위해 NVBlox를 이용해 환경의 장애물 맵을 생성하고, 이를 모션 플래닝과 MPC 모듈에서 고려하도록 설정하여 충돌을 사전에 회피하도록 했다. 파지 성공 기준은 시뮬레이션과 유사하게 물체를 집어 올린 뒤 로봇의 홈 포지션까지 이동시키면서 한 번도 떨어뜨리지 않는 것으로 정의되었다.\n실세계 비교 및 안전성: 실제 환경에서는 폐루프 정책들의 안전성 문제가 있기 때문에, 비교 대상으로는 오픈 루프 기반의 CuRobo-GraspAPI 방법만을 사용했다. 이 방법은 모션 플래닝으로 지정된 파지 자세까지 이동한 후 그대로 집는 기존의 open-loop 파지 파이프라인으로, 현실에서 비교적 안전하고 신뢰할 만한 기준으로 간주된다. 반면 앞서 시뮬레이션에 포함됐던 다른 폐루프 방식들(IL 기반 정책들 등)은 충돌 회피 메커니즘이 없어 안전 문제가 예상되므로 실제 로봇에는 적용하지 않았다. (예를 들어 선반이나 테이블에 로봇팔이 부딪칠 위험이 있어 제외했다는 설명이다.) 이에 비해 Grasp-MPC는 MPC 최적화 자체에 충돌 회피 비용을 포함하고 있어 주변 장애물이 있는 상황에서도 안전하게 동작할 수 있기 때문에, 실제 로봇 실험에 적합하다는 점도 강조되었다.\n실세계 실험 결과: 빈 테이블부터 복잡한 선반까지 점진적으로 난이도가 증가하는 3가지 환경 모두에서 Grasp-MPC는 일관되게 open-loop 기준보다 높은 파지 성공률을 거두었다. 구체적으로 Figure 8의 결과에 따르면, 어느 환경에서든 Grasp-MPC가 CuRobo(open-loop) 방법보다 성공률이 높았으며 복잡한 환경일수록 그 격차가 커졌다고 보고된다. Open-loop 방식은 예측된 파지 자세가 이상적 위치에서 조금만 벗어나도 실행 중 경로를 수정하지 못해 파지에 실패하는 사례가 잦았지만, Grasp-MPC는 실행 도중 지속적으로 그리퍼의 자세를 조정하면서 가치 함수 상의 비용을 최소화하도록 동작하기 때문에, 장애물(예: 선반 가장자리 등)을 피하면서도 최종적으로 물체를 잡는 비율이 훨씬 높았다. 요약하면, 정적인 물체 파지 작업에서 조차도 Grasp-MPC가 open-loop 대비 뛰어난 적응력을 보여준 것이다. 실제 예시로, Grasp-MPC는 선반 구석에 있거나 여러 물체 사이에 낀 목표 물체를 집을 때도 중간에 그립 자세를 미세 조정하여 성공적으로 파지하는 모습을 보였는데, 이러한 능력은 기존 개방형 제어로는 불가능한 부분이다.\n한편, 동적인 변화에 대한 적응 실험도 진행되었다. 이는 실제 로봇 환경에서 폐루프 제어의 장점을 극대화하는 시나리오로서, 로봇이 목표 파지 지점(pre-grasp)에 도달한 후에 의도적으로 물체의 위치를 이동(교란)시켜 보는 테스트이다. 이러한 돌발 상황은 일반적인 open-loop 접근으로는 대응이 불가능하므로, 해당 실험은 Grasp-MPC 단독으로 수행되었다. 실험에서는 여러 물체에 대해 각각 수 차례씩 큰 폭의 위치 교란을 주었는데, Grasp-MPC는 물체가 갑자기 움직여도 즉각적으로 경로를 보정하여 끝내 파지에 성공하는 높은 적응력을 보여주었다. 심지어 학습된 가치 함수는 주로 5cm 이내의 비교적 작은 움직임만 경험했음에도, 그 이상의 큰 물체 이동에도 전역적으로 파지를 재계획하여 상당한 성공률을 달성했다는 점이 고무적이다. (정량적인 성공률 수치가 제시되지는 않았지만, 실행 예시를 통해 Grasp-MPC가 실시간으로 움직이는 표적을 추적하여 잡는 모습이 확인되었다고 한다.) 이는 Grasp-MPC의 폐루프 제어가 가지는 실시간 적응성을 잘 보여주는 대목으로, 실제 응용에서 물체가 떨어지거나 움직일 때도 회복(graceful recovery) 가능한 파지 시스템의 가능성을 시사한다.\n결론적으로, 실험 결과들은 시뮬레이션에서나 실제에서나 Grasp-MPC의 우수한 성능과 견고함을 입증한다. Grasp-MPC는 대규모 데이터로 학습된 가치 함수 덕분에 새로운 물체에 대한 일반화 능력이 뛰어나고, MPC 기반 피드백 제어를 통해 환경 변화나 예측 오차에도 흔들리지 않는 파지 성공률을 보여주었다. 특히 복잡한 실제 환경 (테이블, 선반 등)에서도 추가 학습 없이 곧바로 높은 성공률을 낸 점은, 이 접근법의 실용적 가치를 뒷받침하는 중요한 성과라 할 수 있다. Grasp-MPC는 센서 노이즈, 물리적 접촉 등의 현실 요인을 잘 견디며, 이는 물리 기반 시뮬레이션 데이터에 의존하지 않고도 달성된 것이라 더욱 주목된다."
  },
  {
    "objectID": "posts/paper/2025-09-02-grasp-mpc.html#기존-연구와의-비교-분석",
    "href": "posts/paper/2025-09-02-grasp-mpc.html#기존-연구와의-비교-분석",
    "title": "📃GraspMPC 리뷰",
    "section": "2.3 3. 기존 연구와의 비교 분석",
    "text": "2.3 3. 기존 연구와의 비교 분석\n로봇 파지(grasping) 분야의 기존 연구들은 크게 open-loop 방식과 closed-loop 방식으로 양분된다. Open-loop 파지 기법들은 딥러닝으로 그립 포즈(파지 자세)를 예측한 후, 로봇팔을 해당 위치로 모션 플래닝해 이동시켜 파지하는 접근법을 사용해왔다. 대표적으로 물체의 3D 데이터베이스를 활용한 대규모 학습(예: Dex-Net 등)이나, 시뮬레이터에서 생성한 파지 annotation 데이터를 통한 학습 방법들이 이에 속한다. 이러한 open-loop 방법들은 비교적 새로운 물체에 대한 파지 성공률이 높게 보고되었으나, 실시간 피드백 부족으로 인해 한 번 계획이 시작되면 경로를 유연하게 수정하지 못한다는 한계가 있다. 그 결과, 그립 포즈 예측 오류나 실행 중 물체의 움직임 변화에 매우 취약하며, 복잡한 환경에서 물체가 서로 상호작용하거나 예측과 다른 위치에 있을 때 실패율이 높아지는 문제가 있었다.\n폐루프(closed-loop) 파지 기법들은 이러한 문제를 극복하고자 실시간 센서 피드백을 제어에 도입한 방법들이다. 강화학습(RL) 기반 접근과 모방학습(IL) 기반 정책학습이 이에 해당하며, 로봇이 카메라 등으로부터 주기적으로 관측을 받아 매 시점 행동을 결정하는 정책(policy)을 학습한다. 이러한 폐루프 방법들은 오픈 루프에 비해 피드백으로 오차를 수정할 수 있어 성공률 향상의 여지가 있지만, 현실적으로 학습을 위한 데이터 수집이 어렵고 비싼 문제가 있다. 많은 기존 연구들이 단순한 테이블 위 단일 물체 환경에서만 학습/평가되었고, 주어진 제한된 데이터로 학습된 정책은 새로운 물체나 복잡한 장면으로 일반화하기 어려웠다. 특히 다양한 물체에 대한 대규모 파지 데이터셋 부족이 병목이 되어, 폐루프 정책들의 성능은 제한적이었다. 또한 학습된 정책이 충돌 회피와 같은 안전성을 내재적으로 보장하지 못해, 복잡한 환경에 로봇을 투입하기에는 위험 요소가 많았다. 예컨대, 기존의 여러 RL/IL 기반 파지 논문들은 로봇과 주변 물체 간 충돌을 고려하지 않아서 실제 응용시 안전 문제가 지적되어 왔다.\n이 논문의 Grasp-MPC 접근법은 기존 대비 몇 가지 중요한 차별점을 지닌다. 우선, Open-loop와 Closed-loop의 장점 결합이라는 관점에서, Grasp-MPC는 사전 학습된 그립 예측 모델과 모션 플래닝을 사용해 초기 파지 자세까지 접근하는 모델 기반 단계를 활용하면서도, 최종 파지 동작은 MPC 폐루프 제어로 수행함으로써 두 접근법의 이점을 모두 취했다. 이처럼 모델 기반 + 데이터 기반을 통합한 설계는 기존에 없던 새로운 틀로, open-loop 방식의 빠른 초기 경로 설정 능력과 closed-loop 방식의 실시간 적응 능력을 결합한 것이다. 특히 MPC를 정책 실행기로 사용한 점이 독특한데, 일반적인 RL과 달리 정책 네트워크를 명시적으로 학습하지 않고 가치 함수만으로도 MPC가 최적 행위를 찾아낼 수 있도록 했다. 이는 오프라인 RL의 철학과도 맞닿아 있는데, 기존 오프라인 RL 기법들은 대용량의 오프라인 데이터를 활용하면서도 학습된 Q함수나 가치함수로부터 정책을 추출하는 과정에서 어려움이 있었다. Grasp-MPC는 애초에 MPC가 곧 정책이므로 이러한 추출 과정이 불필요하며, 값 함수 학습 자체에 집중할 수 있었다. 이러한 구조 덕분에 IQL과 같은 오프라인 RL 방법에 비해 학습된 가치 함수를 효율적으로 활용하여 더 높은 성능을 발휘한 것으로 보인다.\n또한 비용 함수 설계의 측면에서, Grasp-MPC는 학습 기반의 비용 함수(가치 함수)를 도입하여 기존 방법들의 단점을 극복했다. 앞서 언급한 바와 같이, 기존에 Chen 등의 연구에서는 예측된 그립 포즈와의 거리를 기반으로 하는 값 함수를 MPC의 cost로 사용하려 했으나, 이러한 단순 거리 기반 척도는 파지 성공에 중요한 여러 요인(예: 손가락과 물체의 구체적인 접촉 관계나 물체의 무게 중심 등)을 반영하지 못해 결과적으로 최적이 아닌 동작을 유도하는 문제가 있었다. 또한 다른 시도 중 하나인 CV-MPC에서는 소량의 데모만으로 가치 함수 앙상블을 학습하였는데, 이는 저차원 상태(예: 로봇 joint 값 등)만 사용하여 학습하였기 때문에 시각적 다양성이나 새로운 상황에 적응하기 어려웠다. 반면 Grasp-MPC의 경우 고차원 시각정보(포인트 클라우드)와 수백만 건의 다양한 시뮬레이션 궤적으로부터 학습한 가치 함수를 사용함으로써, 파지 성공에 영향을 주는 미세한 요소들까지 비용에 반영할 수 있었고 결과적으로 일반화 성능과 성공률 모두 향상시켰다. 이는 Grasp-MPC가 MPC를 활용한 폐루프 제어 분야에서 처음으로 대규모 비전 기반 학습을 결합한 사례로 평가할 수 있다.\n데이터셋 규모와 일반화 측면에서도 Grasp-MPC는 기존 연구보다 앞선다. FetchBench 등 이전 연구들에서는 파지 데이터의 양과 다양성의 한계로 인해 성능이 제약되었는데, Grasp-MPC는 Objaverse 기반으로 훨씬 큰 규모의 합성 데이터를 생성하여 학습함으로써 이러한 한계를 돌파했다. 특히 FetchBench에서 사용된 Transformer IL 정책의 경우 제한된 시연 데이터로 학습돼 복잡한 장애물 환경에서는 성능이 떨어졌으나, Grasp-MPC는 더 대규모·다양한 데이터로 학습된 덕분에 이질적인 환경에서도 견고한 성능을 보였다. 실제 실험에서 Grasp-MPC가 학습 때는 빈 환경만 경험했음에도 불구하고, 복잡한 테이블이나 선반 환경에서 별도 튜닝 없이 높은 성공률을 낸 점은 이러한 일반화 능력 향상을 잘 보여준다.\n안전성과 시스템 통합 관점에서 보더라도, Grasp-MPC는 실용적인 우위를 갖는다. 기존의 폐루프 학습 정책들은 주로 충돌 회피나 안전 제약을 고려하지 않고 학습되었기 때문에, 실제 환경에서 로봇이 장애물과 충돌할 위험이 있었다. 하지만 Grasp-MPC는 MPC 최적화 문제에 안전을 위한 제약(충돌 회피, 최소 jerk 등)을 명시적으로 포함시켰기 때문에, 협소한 공간이나 장애물이 많은 상황에서도 안정적으로 동작할 수 있었다. 이는 본 논문 실험에서도 입증되어, 다른 폐루프 방법들은 현실 환경에서 위험해 적용하지 못한 데 비해 Grasp-MPC는 선반 같은 복잡한 환경에서도 무사고로 임무를 수행했다. 나아가 이러한 모듈식 설계 덕분에, Grasp-MPC는 새로운 제약 조건이나 환경 변화에 유연하게 대응할 수 있다. 예를 들어 로봇의 작업 공간에 특정 금지 영역이나 동역학적 제한이 추가되더라도, MPC 문제에 해당 비용이나 제약을 넣으면 재학습 없이도 시스템에 반영될 수 있다. 이는 학습된 정책을 바꾸지 않고도 제어 단계에서 해결할 수 있기 때문에 실제 응용에서의 편의성을 높여준다.\n마지막으로, 성능 측면에서의 비교를 요약하면 다음과 같다. Grasp-MPC는 시뮬레이션 상에서 5,400여 개의 다양한 파지 시나리오를 실험한 결과, 모방학습(IL) 기반 방법들을 크게 앞서는 성공률을 보였고, 기존 계획 기반 방법(planning-based)이 예측 오차나 센서 노이즈로 성능이 떨어지는 상황에서도 우수한 결과를 냈다. 오프라인 RL인 IQL과 비교해서도, IQL이 정책 추출의 비효율로 성능이 제한된 반면 Grasp-MPC는 더 높은 성공률로 그 격차를 보여주었다. 실제 로봇 실험에서도, 기존의 계획 기반 파지 파이프라인 대비 Grasp-MPC가 복잡한 테이블 및 선반 환경에서 더 높은 성공률을 기록했으며, 이는 학습 당시 접하지 않은 환경에서도 통하는 일반화 능력을 입증한 것이다. 요컨대 Grasp-MPC는 현 시점에서 개방형·폐루프 파지 접근법들 모두를 뛰어넘는 새로운 state-of-the-art 성능을 달성한 것으로 평가된다.\n한편, 저자들은 이러한 공헌에도 불구하고 남아있는 한계점도 언급한다. 예를 들어 절대적인 성공률을 더 높이기 위해서는 향후 물리 시뮬레이션을 통한 더 정확한 성공/실패 레이블링이나, 현실 데이터로의 파인튜닝 등이 유효할 수 있다고 제안한다. 또한 현재 Grasp-MPC의 검증은 파지(grasping) 작업에 국한되어 있는데, 유사한 접근을 다른 조작(manipulation) 작업 (예: 도구 사용이나 비전 기반 위치 미세조정 등)에도 확장할 수 있을 것으로 기대하며, 이는 추후 연구과제로 남겨두었다고 밝혔다. 그럼에도 불구하고, Grasp-MPC는 기존 연구 대비 데이터 규모, 알고리즘 구조, 실험 검증 면에서 새로운 기준을 세운 연구로서 의의가 크다. 이는 로봇 파지 및 일반적인 로봇 제어 커뮤니티에서 모델 예측 제어와 딥러닝 가치 함수의 융합 가능성을 보여준 사례이며, 향후 더욱 복잡한 조작 임무에 폐루프 학습기반 제어를 적용하는 데에 밑거름이 될 것으로 전망된다."
  },
  {
    "objectID": "posts/paper/2025-05-05-reskin-anyskin.html",
    "href": "posts/paper/2025-05-05-reskin-anyskin.html",
    "title": "📃Reskin-Anyskin 리뷰",
    "section": "",
    "text": "https://reskin.dev/\n\n\n💡 ReSkin은 머신러닝과 자기 감지를 활용하여 저렴하고 다재다능하며 오래 사용할 수 있는 촉각 소프트 센서를 제공합니다.\n🧲 ReSkin은 자기 센싱을 통해 전자 회로를 수동 인터페이스와 분리하여 다양한 형태로 제작이 용이하며, 머신러닝은 제작 및 시간 변화에 강인한 센서 응답 모델을 학습합니다.\n⚙️ 자가 지도 학습 알고리즘을 통해 작은 데이터 수집 절차로 성능을 향상시켜 기존의 촉각 감지 모듈보다 더 다양하고 확장 가능하며 저렴한 촉각 감지 모듈을 만들 수 있습니다.\n\n\n\nReSkin은 저렴하고 교체 가능하며, 오래 지속되는 다용도 소프트 촉각 스킨입니다.\n핵심 아이디어:\n\n자기장 센싱: ReSkin은 소프트 자성 스킨과 magnetometer 기반 센싱 메커니즘을 사용합니다. 외력에 의해 스킨이 변형되면 자기장 왜곡이 발생하고, 이를 통해 접촉 위치와 힘을 추정합니다. 전자 회로는 수동 인터페이스와 분리되어 있어 인터페이스 교체가 용이하며 다양한 형태로 제작할 수 있습니다.\n머신러닝: 머신러닝 모델을 사용하여 센서 응답 특성을 학습합니다. 이를 통해 제작 과정이나 시간 경과에 따른 변동에 강인한 모델을 구축하고, self-supervised learning 알고리즘을 통해 적은 비용으로 성능을 향상시킵니다.\n\n구체적인 내용:\n\n센서 설계: 스킨은 elastomer 매트릭스 내에 삽입된 자성 미세 입자로 구성됩니다. 힘이 가해져 스킨이 변형되면 magnetometer가 자기장의 변화를 측정합니다. 5개의 magnetometer(중앙 1개, 주변 4개)를 사용하여 자기장 변화를 측정합니다.\n데이터 수집: Dobot Magician 로봇과 ATI Nano 17 force/torque 센서를 사용하여 데이터를 수집합니다. 로봇은 스킨의 다른 위치에 힘을 가하고, 센서 보드는 자기장 측정값을 스트리밍합니다.\n모델 아키텍처: 자기장 변화 \\mathbf{B}에서 접촉력의 위치 \\mathbf{x} = (x, y)와 크기 F를 예측하기 위해 5개의 layer를 가진 multilayer perceptron (MLP)를 사용합니다. 아키텍처는 다음과 같습니다: \\mathbf{B}(15) \\rightarrow \\text{MLP+ReLU}(200) \\rightarrow \\text{MLP}(200) \\rightarrow \\text{MLP}(40) \\rightarrow \\text{MLP+ReLU}(200) \\rightarrow \\text{MLP+ReLU}(200) \\rightarrow xyF(3). 세 번째 activation layer는 bottleneck feature layer이며, \\text{feat}(\\cdot)는 이 3-layer feature extraction network를 나타냅니다. 손실 함수는 \\text{L2-loss}를 \\mathbf{(x, y, F)}에 적용합니다.\nSelf-Supervised Learning (SSL): Ground truth 라벨 없이 새로운 센서에 적응하기 위해 triplet loss를 사용합니다. \\mathcal{L}_{\\text{triplet}} = \\max(0, ||\\text{feat}(\\mathbf{B}_a) - \\text{feat}(\\mathbf{B}_p)||^2 - ||\\text{feat}(\\mathbf{B}_a) - \\text{feat}(\\mathbf{B}_n)||^2). 여기서 \\mathbf{B}_a, \\mathbf{B}_p, \\mathbf{B}_n은 각각 anchor, positive, negative 샘플이며, ||\\mathbf{x}_a - \\mathbf{x}_p|| &lt; ||\\mathbf{x}_a - \\mathbf{x}_n||를 만족합니다.\n실험 결과:\n\n단일 센서 모델은 시간이 지남에 따라 성능이 저하됩니다.\nMulti-sensor 모델은 단일 센서 모델보다 성능이 우수합니다.\nSelf-supervised adaptation은 multi-sensor 모델의 성능을 더욱 향상시킵니다. 특히, 적은 양의 adaptation 데이터로도 상당한 성능 향상을 얻을 수 있습니다.\n\n다양한 응용: ReSkin은 로봇 그리퍼, 개 신발, 장갑, arm sleeve 등 다양한 형태로 적용될 수 있습니다. 물이 채워진 shot glass의 무게를 감지하고, 블루베리를 쥐는 등의 섬세한 조작에도 활용될 수 있습니다.\n\n결론:\nReSkin은 높은 localization 정확도와 힘 감도를 제공하는 저렴하고 컴팩트한 촉각 센서입니다. 머신러닝과 자기장 센싱을 결합하여 시간 경과 및 개별 스킨의 변동에 강인한 모델을 개발하고, SSL adaptation 절차를 통해 새로운 스킨에 대한 모델을 개선합니다."
  },
  {
    "objectID": "posts/paper/2025-05-05-reskin-anyskin.html#brief-review",
    "href": "posts/paper/2025-05-05-reskin-anyskin.html#brief-review",
    "title": "📃Reskin-Anyskin 리뷰",
    "section": "",
    "text": "ReSkin은 저렴하고 교체 가능하며, 오래 지속되는 다용도 소프트 촉각 스킨입니다.\n핵심 아이디어:\n\n자기장 센싱: ReSkin은 소프트 자성 스킨과 magnetometer 기반 센싱 메커니즘을 사용합니다. 외력에 의해 스킨이 변형되면 자기장 왜곡이 발생하고, 이를 통해 접촉 위치와 힘을 추정합니다. 전자 회로는 수동 인터페이스와 분리되어 있어 인터페이스 교체가 용이하며 다양한 형태로 제작할 수 있습니다.\n머신러닝: 머신러닝 모델을 사용하여 센서 응답 특성을 학습합니다. 이를 통해 제작 과정이나 시간 경과에 따른 변동에 강인한 모델을 구축하고, self-supervised learning 알고리즘을 통해 적은 비용으로 성능을 향상시킵니다.\n\n구체적인 내용:\n\n센서 설계: 스킨은 elastomer 매트릭스 내에 삽입된 자성 미세 입자로 구성됩니다. 힘이 가해져 스킨이 변형되면 magnetometer가 자기장의 변화를 측정합니다. 5개의 magnetometer(중앙 1개, 주변 4개)를 사용하여 자기장 변화를 측정합니다.\n데이터 수집: Dobot Magician 로봇과 ATI Nano 17 force/torque 센서를 사용하여 데이터를 수집합니다. 로봇은 스킨의 다른 위치에 힘을 가하고, 센서 보드는 자기장 측정값을 스트리밍합니다.\n모델 아키텍처: 자기장 변화 \\mathbf{B}에서 접촉력의 위치 \\mathbf{x} = (x, y)와 크기 F를 예측하기 위해 5개의 layer를 가진 multilayer perceptron (MLP)를 사용합니다. 아키텍처는 다음과 같습니다: \\mathbf{B}(15) \\rightarrow \\text{MLP+ReLU}(200) \\rightarrow \\text{MLP}(200) \\rightarrow \\text{MLP}(40) \\rightarrow \\text{MLP+ReLU}(200) \\rightarrow \\text{MLP+ReLU}(200) \\rightarrow xyF(3). 세 번째 activation layer는 bottleneck feature layer이며, \\text{feat}(\\cdot)는 이 3-layer feature extraction network를 나타냅니다. 손실 함수는 \\text{L2-loss}를 \\mathbf{(x, y, F)}에 적용합니다.\nSelf-Supervised Learning (SSL): Ground truth 라벨 없이 새로운 센서에 적응하기 위해 triplet loss를 사용합니다. \\mathcal{L}_{\\text{triplet}} = \\max(0, ||\\text{feat}(\\mathbf{B}_a) - \\text{feat}(\\mathbf{B}_p)||^2 - ||\\text{feat}(\\mathbf{B}_a) - \\text{feat}(\\mathbf{B}_n)||^2). 여기서 \\mathbf{B}_a, \\mathbf{B}_p, \\mathbf{B}_n은 각각 anchor, positive, negative 샘플이며, ||\\mathbf{x}_a - \\mathbf{x}_p|| &lt; ||\\mathbf{x}_a - \\mathbf{x}_n||를 만족합니다.\n실험 결과:\n\n단일 센서 모델은 시간이 지남에 따라 성능이 저하됩니다.\nMulti-sensor 모델은 단일 센서 모델보다 성능이 우수합니다.\nSelf-supervised adaptation은 multi-sensor 모델의 성능을 더욱 향상시킵니다. 특히, 적은 양의 adaptation 데이터로도 상당한 성능 향상을 얻을 수 있습니다.\n\n다양한 응용: ReSkin은 로봇 그리퍼, 개 신발, 장갑, arm sleeve 등 다양한 형태로 적용될 수 있습니다. 물이 채워진 shot glass의 무게를 감지하고, 블루베리를 쥐는 등의 섬세한 조작에도 활용될 수 있습니다.\n\n결론:\nReSkin은 높은 localization 정확도와 힘 감도를 제공하는 저렴하고 컴팩트한 촉각 센서입니다. 머신러닝과 자기장 센싱을 결합하여 시간 경과 및 개별 스킨의 변동에 강인한 모델을 개발하고, SSL adaptation 절차를 통해 새로운 스킨에 대한 모델을 개선합니다."
  },
  {
    "objectID": "posts/paper/2025-05-05-reskin-anyskin.html#brief-review-1",
    "href": "posts/paper/2025-05-05-reskin-anyskin.html#brief-review-1",
    "title": "📃Reskin-Anyskin 리뷰",
    "section": "2.1 Brief Review",
    "text": "2.1 Brief Review\n이 문서는 로봇 터치를 위한 새로운 촉각 센서인 AnySkin을 소개합니다. AnySkin은 조립이 용이하고, 다양한 로봇 End-effector와 호환되며, 새로운 Skin instance에 대한 일반화가 가능한 것이 특징입니다.\n핵심 아이디어:\n기존 촉각 센서의 단점(비용, 불편함, 일관성 부족)을 극복하고, 생물학적 움직임 제어에서 중요한 역할을 하는 촉각을 로봇 공학에 더 쉽게 통합하는 것을 목표로 합니다.\n주요 특징 및 기여:\n\n향상된 제작 과정 및 디자인 도구: 접착제가 필요 없고 내구성이 뛰어나며 쉽게 교체할 수 있는 자기 촉각 센서를 위한 간소화된 제작 과정과 디자인 도구를 제시합니다.\nSlip 감지 및 정책 학습: AnySkin 센서를 사용한 Slip 감지 및 Visuo-tactile 정책 학습의 효율성을 입증합니다. USB 삽입과 같은 정밀 작업에 적용 가능합니다.\n교체 용이성: AnySkin은 평균 12초 만에 교체할 수 있으며, 교체 후에도 재사용이 가능합니다.\nZero-shot 일반화: 한 AnySkin instance에서 학습된 모델이 다른 AnySkin instance에 Zero-shot으로 일반화됩니다. ReSkin과 같은 기존 촉각 솔루션과 비교하여 성능 저하가 훨씬 적습니다.\n\n기술적 상세:\n\n센서 작동 원리: AnySkin은 자기장 왜곡을 감지하여 접촉을 감지합니다. 센서 표면에는 자화된 철 입자가 포함되어 있으며, 외부 힘에 의한 표면 변형은 자기장 변화를 일으킵니다.\nFabrication 과정: Smooth-On DragonSkin 10 Slow와 MQFP-15-7(25μm) magnetic particles를 1:1:2 비율로 혼합하여 2-part mold에서 경화시킵니다 (\\text{Fig. 2a}). 경화된 Skin은 Pulse Magnetizer를 사용하여 자화됩니다.\n자기장 강화: Pulse Magnetizer를 사용하여 경화 후 Skin을 자화함으로써, 기존 ReSkin에 비해 더 강력한 자기장을 생성합니다.\n입자 분포 균일성: 더 미세한 Magnetic Particles(MQFP-15-7(25μm))를 사용하여 입자가 경화 전에 가라앉는 현상을 방지하고, Skin 전체에 걸쳐 더 균일한 분포를 얻습니다 (\\text{Fig. 2b}).\nSelf-aligning 디자인: Self-aligning 디자인을 통해 Elastomer와 회로의 상대적 위치 변동성을 줄이고, 신호 일관성을 향상시킵니다.\n\n실험 결과:\n\n신호 특성 비교: Pulse Magnetizer, finer particles, self-aligning 디자인이 신호 강도, 일관성, Misalignment에 대한 민감도에 미치는 영향을 정량적으로 분석합니다. (Table I)\nSlip 감지: Jaco Robot에 장착된 AnySkin 센서를 사용하여 수집된 데이터를 기반으로 LSTM 모델을 훈련하여, unseen object에 대한 Slip 감지 정확도가 92%임을 입증합니다 (\\text{Fig. 3}).\n교체 용이성 비교: ReSkin, DIGIT과 비교하여 AnySkin의 교체 시간과 재사용성을 User Study를 통해 평가합니다 (Table II).\n정책 학습에서의 교체 가능성: Plug insertion, Card swiping, USB insertion의 세 가지 정밀 조작 작업에서 AnySkin의 Cross-instance 일반화 성능을 평가합니다 (\\text{Fig. 5}). ReSkin과 DIGIT에 대한 비교 결과도 제시됩니다 (Table III).\n\n주요 공식:\n\n본 논문에서는 특정 공식을 명시적으로 제시하지는 않습니다. 하지만 실험 결과 분석에서 평균, 표준 편차 등의 기본적인 통계적 개념이 사용됩니다.\nLSTM(Long Short-Term Memory) 모델이 Slip 감지 모델로 사용됩니다.\nBAKU (Bidirectional Action-conditioned Keyframe Transformer) 아키텍쳐가 정책 학습에 사용됩니다.\n\n결론:\nAnySkin은 촉각 데이터를 활용하여 더욱 능숙하고 성능이 뛰어난 모델을 개발하는 데 기여할 수 있는 잠재력을 가진 새로운 촉각 센서입니다. Visuo-tactile 정책을 새로운 Instance에 Zero-shot으로 일반화하는 최초의 센서입니다."
  },
  {
    "objectID": "posts/paper/2025-05-05-reskin-anyskin.html#배경-촉각-센서의-어려움과-요구사항",
    "href": "posts/paper/2025-05-05-reskin-anyskin.html#배경-촉각-센서의-어려움과-요구사항",
    "title": "📃Reskin-Anyskin 리뷰",
    "section": "3.1 배경: 촉각 센서의 어려움과 요구사항",
    "text": "3.1 배경: 촉각 센서의 어려움과 요구사항\n로봇에게 촉각(tactile)은 인간과 마찬가지로 섬세한 상호작용에 필수적이지만, 현실 로봇 시스템에서 촉각은 시각이나 관성 센서 대비 활용이 저조했습니다. 기존 촉각 센서는 비용이 높고, 플랫폼에 특화된 형태로 제작되며, 일관성(consistency) 문제로 인해 센서를 교체하면 센서 출력 특성이 달라지는 문제가 있습니다. 이러한 변동성 때문에 한 센서에서 학습한 모델을 다른 센서에 그대로 적용하기 어렵고, 부드러운 촉각 센서는 사용 중 마모로 잦은 교체가 필요하기에 더 큰 문제를 야기합니다. 예를 들어 카메라와 마이크는 저렴하고 범용적이며 교체 후에도 동일한 성능을 기대할 수 있지만, 촉각 센서는 제작 공정상의 편차로 샘플마다 반응이 달라 많은 경우 새 센서마다 재학습이 필요했습니다. 이러한 이유로 실용적인 촉각 피부(sensor skin)를 만드는 것이 로봇 조작 분야의 오랜 과제로 남아 있었습니다."
  },
  {
    "objectID": "posts/paper/2025-05-05-reskin-anyskin.html#reskin-자기장-기반-저비용-교체형-촉각-센서",
    "href": "posts/paper/2025-05-05-reskin-anyskin.html#reskin-자기장-기반-저비용-교체형-촉각-센서",
    "title": "📃Reskin-Anyskin 리뷰",
    "section": "3.2 ReSkin: 자기장 기반 저비용 교체형 촉각 센서",
    "text": "3.2 ReSkin: 자기장 기반 저비용 교체형 촉각 센서\nReSkin은 이러한 문제를 해결하고자 2021년에 제안된 저비용(&lt; $30), 교체 가능하고 다용도인 촉각 센서입니다. ReSkin의 핵심 아이디어는 자기장 기반 감지(magnetic sensing)입니다. 부드러운 폴리머 피부에 미세한 자성 입자를 섞어 제조한 얇은 패치(두께 2~3mm)를 사용하고, 이 패치를 조금 떨어진 곳에 배치한 자기 센서들(자력계)로 변형을 측정합니다. 물체가 피부를 누르거나 미는 등 변형이 일어나면 자성 입자의 배열이 바뀌어 주변 자기장 B가 변하고, 이를 다수의 자력계가 읽어들여 힘의 크기와 위치를 추정하는 것입니다. 전자 회로부와 센싱 표면을 분리한 이러한 구조 덕분에, 피부 패치가 마모되면 간단히 교체할 수 있고 회로는 재사용 가능합니다. 또한 폴리머 소재이므로 로봇 손가락, 팔 등 곡면에도 부착하기 쉬워 활용 범위가 넓습니다. ReSkin은 400 Hz 이상의 높은 샘플링속도와 약 1 mm의 위치 해상도(90% 정확도 기준)를 보이며, 50,000회 이상의 접촉 사이클에도 성능이 크게 저하되지 않을 만큼 내구성을 갖추었습니다.\nReSkin의 또 다른 특징은 데이터 기반 모델링입니다. 물리 공식을 일일이 사용하지 않고, 자력계들이 출력하는 다차원 신호 ΔB로부터 접촉 지점 (x, y)과 접촉 힘 F를 바로 예측하는 머신러닝 모델을 학습합니다. 이렇게 하면 센서마다 미세한 특성 차이가 있더라도, 학습을 통해 보정된 모델이 복잡한 관계를 학습하게 됩니다. 다만 초기 ReSkin 연구 당시에도 “새로운 센서에도 일반화되는 모델”을 만드는 것이 중요한 목표로 제시되었습니다. 동일한 제작 공정으로 만든 ReSkin이라도 개체별로 raw 자기장 신호 분포에 차이가 있으며 (논문의 Figure 4b에서 센서마다 동일한 힘 입력에 대한 자기장 변화가 다름을 보여줍니다), 장기간 사용하면 시간이 지남에 따른 드리프트도 발생합니다. 따라서 한 센서에서 학습한 모델을 다른 센서에 바로 적용하면 성능 저하가 발생할 수밖에 없습니다. 실제로 ReSkin 논문에서도 단일 센서로 학습한 모델을 새 센서에 적용했을 때 접촉 위치 예측 정확도가 약 25%에 불과해 크게 떨어지는 것을 확인했습니다. 이 문제를 해결하지 못하면 “교체 가능한 촉각 센서”라는 목표를 이룰 수 없기 때문에, ReSkin에서는 별도의 章을 할애하여 “새로운 센서로의 적응(Adapting to New Sensors)” 방법을 제시했습니다."
  },
  {
    "objectID": "posts/paper/2025-05-05-reskin-anyskin.html#새로운-센서로의-적응-reskin의-self-supervised-학습-기법",
    "href": "posts/paper/2025-05-05-reskin-anyskin.html#새로운-센서로의-적응-reskin의-self-supervised-학습-기법",
    "title": "📃Reskin-Anyskin 리뷰",
    "section": "3.3 🔥 새로운 센서로의 적응: ReSkin의 Self-supervised 학습 기법",
    "text": "3.3 🔥 새로운 센서로의 적응: ReSkin의 Self-supervised 학습 기법\nReSkin이 제안한 센서 교체 시 재학습 부담을 줄이는 기법은 크게 두 가지입니다.\n첫째, 다중 센서 학습입니다. 서로 다른 여러 센서(실험에서는 6개의 회로 보드와 18장의 피부 조합)로부터 데이터를 모아 하나의 통합 모델로 학습시킴으로써, 모델이 다양한 분포의 데이터를 학습하고 일반화 능력을 갖추도록 했습니다.\n둘째, 특징 공간 정규화(feature regularization)를 통해 새로운 센서에서도 특징이 일관되게 맵핑되도록 유도했습니다. 이를 위해 저자들은 Self-supervised triplet loss 함수를 도입했는데, 아이디어는 다음과 같습니다. 센서 출력 B (자기장 변화 신호)를 내부 특징 표현 \\text{feat}(B) 공간으로 맵핑할 때, 피부 위에서 가까운 위치의 두 접촉은 특징 공간에서도 가깝게, 멀리 떨어진 접촉은 특징이 멀어지도록 제약합니다. 구체적으로 한 접촉점을 앵커(anchor)로 잡고, 이와 실제 위치가 가까운 접촉을 positive, 먼 접촉을 negative로 삼아 앵커-포지티브 쌍의 특징거리가 앵커-네거티브 쌍의 특징거리보다 작도록 학습합니다. 이러한 triplet loss L_{\\text{triplet}}은 접촉 위치 레이블 없이도 구성할 수 있는 자가 지도 신호로서, 서로 다른 센서 간에 특징 공간의 위치 민감도를 맞춰주는 역할을 합니다.\ntriplet loss을 활용하면 새로운 센서에 대한 fine-tuning도 수월해집니다. ReSkin을 처음 사용할 때 사용자는 간단한 보정 데이터를 수집하여 모델을 해당 센서에 맞게 미세조정할 수 있습니다. 예를 들어, 펜 끝이나 탐침을 이용해 센서 표면을 직선 경로로 따라 누르면서 이동시키고, 움직인 순서에 따라 데이터를 index 하면 위치 레이블 없이도 순서 정보가 부여됩니다. 이렇게 얻은 연속 눌림 데이터에서 임의의 세 점을 뽑아 삼중항(Anchor, Positive, Negative)으로 구성하면, Positive와 Negative의 기준은 index 순서로 결정할 수 있습니다 (예: 앵커에 더 가까운 index를 positive로). 즉, 사람의 손으로 센서 위를 쓸어내린 기록만 있으면 충분하며 별도의 정밀한 라벨링은 필요 없습니다. 이렇게 수집한 unlabeled 데이터로 triplet loss을 적용하여, 앞서 언급한 다중 센서로 미리 학습된 모델을 해당 새로운 센서의 특징 공간에 맞도록 미세조정합니다. 논문에서는 매 스텝 원래 학습 데이터 배치와 삼중항 배치를 함께 사용하여, 기존 예측 손실(L2 손실)과 triplet loss을 동시에 최소화하도록 모델을 훈련하는 절차를 제시했습니다.\n이 Self-supervised 적응 방법의 효과는 실험으로 입증되었습니다. 다중 센서 데이터로 학습한 모델을 새로운 센서에 적용할 때, 아무 추가 적응 없이 바로 쓰면 접촉 위치 정확도가 80% 초반 수준이었으나, 소량의 적응 데이터만으로도 크게 향상되었습니다. 예를 들어 새로운 센서에서 780개 점을 눌러 얻은 비라벨 데이터로 파인튜닝하면 적응 전보다 성능이 눈에 띄게 좋아졌고, 데이터를 더 늘릴수록 점진적 향상이 있지만 1000개 남짓에서 포화 현상을 보였습니다. 이는 수백 개 수준의 값싼 데이터만으로도 모델이 빠르게 해당 센서에 적응함을 보여줍니다. 또한 훈련에 사용한 센서 개수가 적을수록 (즉 처음부터 다양한 데이터를 못 봤을수록) 이 자가 지도 적응이 주는 이득이 더 컸는데, 반대로 충분히 많은 센서 데이터로 학습한 경우에는 기본 성능이 높아 적응의 개선 폭이 약간 줄어드는 양상도 보였습니다. 한편, 매우 간이한 환경에서도 효과가 있음을 보여주기 위해 별도의 실험을 했는데, 사람 손으로 임의 위치 325번 눌러 수집한 데이터로 적응시킨 결과 접촉 위치 정확도가 약 79.9%에서 84.8%로 상승하고 힘 예측 오차도 줄어들어, 통제되지 않은 환경에서도 방법의 효용이 확인되었습니다. 심지어 센서 하드웨어 형태가 다른 경우에도 적용해보았는데, rigid 보드로 학습한 모델을 유연한(flexible) 회로 보드 형태의 ReSkin에 적응시킨 결과 약 75%의 정확도를 달성했습니다. 유연 보드는 구조상 자력계와 피부 간 거리가 훨씬 가까워 신호 세기가 달라지는 어려운 상황이었지만, 제안한 적응 기법으로 일정 수준 성능을 확보한 것입니다.\n정리하면, ReSkin은 다중 센서 학습과 Self-supervised 적응을 통해 새로운 피부를 교체하더라도 간단한 보정만 거치면 일관된 촉각 인식 성능을 유지할 수 있음을 보여주었습니다. (논문 Table 2에서 자가 적응까지 거친 모델의 접촉 위치 정확도는 87% 수준까지 향상되었습니다.) 이러한 접근은 작업 중간에 센서를 교체해야 하는 현실 시나리오에서 특히 유용하며, 논문 저자들은 이를 통해 ReSkin이 “새로운 피부에도 간단한 적응만으로 학습된 모델을 재사용할 수 있는” 교체형 촉각 센서 솔루션의 가능성을 입증했다고 강조합니다."
  },
  {
    "objectID": "posts/paper/2025-05-05-reskin-anyskin.html#anyskin-플러그앤플레이plug-and-play-촉각-피부의-실현",
    "href": "posts/paper/2025-05-05-reskin-anyskin.html#anyskin-플러그앤플레이plug-and-play-촉각-피부의-실현",
    "title": "📃Reskin-Anyskin 리뷰",
    "section": "3.4 AnySkin: 플러그앤플레이(plug-and-play) 촉각 피부의 실현",
    "text": "3.4 AnySkin: 플러그앤플레이(plug-and-play) 촉각 피부의 실현\nReSkin 이후, 2024년에 발표된 AnySkin은 한걸음 더 나아가 교정(calibration)이나 재학습 없이 센서를 교체해 사용할 수 있는 진정한 플러그앤플레이 촉각 센서를 지향합니다. AnySkin은 기본적인 감지 원리나 전자 하드웨어는 ReSkin의 자기장 기반 설계를 계승하지만, 일관성(consistency) 향상을 위해 센서의 구조적 개선과 제조 공정 표준화에 집중했습니다.\n\n첫째로, 센싱 표면과 전자부의 완전 분리를 구현하면서 자체 접착(self-adhering)이 되고 정렬(self-aligning)이 쉬운 부착 메커니즘을 도입했습니다. 이를 통해 사용자가 AnySkin 패치를 로봇 손가락이나 팔에 마치 휴대폰 케이스를 끼우듯 손쉽게 장착할 수 있고, 매번 같은 위치와 orientation으로 정렬되어 센서 간 편차를 줄입니다.\n둘째로, 자기 신호 세기 강화를 위해 새로운 자성물질과 자화 방법을 적용했습니다. 예컨대 이전보다 강한 자계를 만들어내는 자성 입자와 펄스자화 방식을 활용하여 센서 출력을 증폭함으로써, 환경 노이즈나 개체 간 미세한 차이에 덜 민감하게 했습니다.\n셋째로, 접착제 없는 내구성 높은 제조 공정을 개발하고 이를 누구나 따라할 수 있도록 디자인 툴과 CAD 파일을 공개했습니다. AnySkin 패치는 3D 프린터로 출력한 몰드에 실리콘 재료(예: Smooth-On Dragon Skin 10)와 자성 입자를 부어 만들고, 쉽게 떼어낼 수 있도록 설계되어 반복 사용에 용이합니다. 전체 조립 과정은 약 12초 정도밖에 걸리지 않을 만큼 단순하며, 센서를 새 것으로 바꿔 끼운 뒤에도 이전 센서로 수집한 데이터나 모델을 그대로 활용할 수 있습니다.\n\n이러한 개선 덕분에 AnySkin은 저비용·편의성뿐 아니라 데이터 재사용성(data reusability) 면에서 최초의 성과를 보여주었습니다. 연구진은 AnySkin의 효과를 다양한 로봇 플랫폼과 작업에서 실험으로 검증했는데, 몇 가지 예를 들어보면 다음과 같습니다:\n\n범용성: AnySkin 패치는 평평한 그리퍼부터 사람 손가락 모양의 로봇 손까지 다양한 형태에 적용되었습니다. 실제 Franka эм 로봇팔의 그리퍼, 사족보행 로봇의 지팡이형 끝부분(Dobb-E stick), 그리고 4손가락 로봇핸드(LEAP Hand)의 손가락에 부착하여 모두 정상적으로 작동함을 보였습니다. 이는 센서의 물리적 폼팩터 제약이 거의 없고, 간단히 붙였다 떼었다 할 수 있음을 의미합니다.\n학습된 정책의 유지: AnySkin을 장착한 로봇으로 정교한 조작 과제를 학습시킨 후, 센서만 새 것으로 교체해도 성능이 크게 떨어지지 않았습니다. 예를 들어, 카메라 영상 + 촉각센서로 USB 케이블을 꽂는 동작을 배우게 한 뒤 센서를 바꿔끼워 수행해보면, 성공률 감소가 약 13% 정도에 불과했습니다. 반면 동일한 실험에서 이전 세대인 ReSkin 센서는 43% 가량 성능 감소가 나타났고, 다른 대표적인 촉각 센서인 DIGIT(카메라 기반 젤사이트 센서)의 경우는 원래도 성능 기여가 작았을 뿐 아니라 교체 시 오히려 악화되는 경향을 보였습니다. 즉, AnySkin은 한 센서에서 학습된 모델을 새로운 센서에 거의 그대로 쓸 수 있음을 입증한 것입니다. 아래 그림은 플러그 삽입(Plug Insertion) 작업에서 카메라만 쓴 경우, 카메라+ReSkin, 카메라+AnySkin, 카메라+DIGIT의 조합별 성공률을 비교한 결과입니다. AnySkin을 사용하면 원래 학습에 사용된 센서를 새 것으로 바꿔도 성능 저하가 미미하지만, ReSkin은 교체 후 성능이 크게 떨어지고(사실상 카메라만 쓴 것과 비슷해짐), DIGIT 역시 편차가 심한 모습을 볼 수 있습니다. 이러한 특성 덕분에 AnySkin은 “학습된 정책을 보존한 채 센서를 교체(swap)할 수 있는” 최초의 촉각 센서로 평가됩니다.\n미끄러짐 감지: AnySkin은 물체 미끄럼 감지와 같은 섬세한 접촉 변화도 포착할 수 있습니다. 예를 들어 물체를 잡은 그립이 헐거워져 물체가 미끄러지기 시작하는 초기 징후를 LSTM 기반 모델로 학습시킨 결과, 약 92%의 정확도로 미끄러짐 발생을 탐지했습니다. 이는 로봇 손이 사전에 그립을 조정하거나 힘을 높여 떨어뜨리는 것을 방지하는 데 활용될 수 있습니다. 이 실험에서도 여러 AnySkin 센서를 교체하며 수행했지만 일관된 성능을 보였다고 보고됩니다.\n\nAnySkin의 개발 과정에서 무엇보다 강조된 것은 일관된 센서 응답입니다. 연구 결과에 따르면 AnySkin은 ReSkin에 비해 신호의 센서 간 분산이 획기적으로 감소하였고, 개체별 보정 없이도 큰 추가 오차 없이 동작합니다. 사실상 공장 출하시 보정된 카메라를 교체하는 것과 비슷한 수준의 사용자 경험을 제공하는 것이죠. 이러한 특장점으로 인해 AnySkin 연구는 “로봇 촉각 센싱을 카메라처럼 쉽고 흔하게 만들겠다”는 비전을 향한 중요한 진전으로 평가받고 있습니다."
  },
  {
    "objectID": "posts/paper/2025-05-05-reskin-anyskin.html#결론-및-향후-전망",
    "href": "posts/paper/2025-05-05-reskin-anyskin.html#결론-및-향후-전망",
    "title": "📃Reskin-Anyskin 리뷰",
    "section": "3.5 결론 및 향후 전망",
    "text": "3.5 결론 및 향후 전망\nReSkin과 AnySkin으로 이어지는 연구는 로봇을 위한 실용적 촉각 피부 구현에 있어 큰 진전을 보여주었습니다. ReSkin이 소형·저가·교체가능 촉각 센서의 틀을 제시하고 머신러닝을 통해 마모와 개체차를 보정하는 개념을 도입했다면, AnySkin은 한발 더 나아가 센서 인스턴스 간 zero-shot 일반화를 실현함으로써 사실상 보정 없이 바로 교체해 쓸 수 있는 수준에 도달했습니다. 특히 AnySkin의 경우 기존 상용 촉각센서들과 비교해도 범용성, 유지보수 편의성 측면에서 우위를 보여 대규모 적용의 가능성을 높였습니다. 물론 여전히 풀어야 할 과제도 있습니다. 예를 들어 현재 단일 접촉 지점의 힘과 위치를 예측하는 수준에서, 다중 접촉 지점의 동시 인식이나 보다 복잡한 촉각 정보(예: 촉감 질감 분별) 등은 향후 연구로 남아 있습니다. 또한 센서 교체 주기가 늘어날수록 누적되는 미세한 차이나 시간에 따른 재료 특성 변화 등도 추가로 살펴볼 부분입니다. 그럼에도 불구하고, ReSkin/AnySkin 연구는 데이터 취득부터 학습, 교체에 이르는 촉각 센서 활용 전 주기를 크게 개선한 사례입니다. 이를 통해 로봇 연구자들은 마치 카메라를 다루듯 손쉽게 촉각 센서를 다룰 수 있고, 한 번 모은 촉각 데이터를 반복 활용하여 학습 효율을 높일 수 있게 되었습니다. 궁극적으로 이러한 노력들은 로봇에게 인간 수준의 풍부한 촉각능력을 부여하고, 섬세하고 안전한 물체 조작을 가능케 하는 방향으로 나아가고 있습니다. 앞으로 AnySkin처럼 쉽게 교체하고 확장할 수 있는 촉각 피부가 더 발전하고 상용화된다면, 로봇의 촉각이 시각 만큼이나 당연한 시대가 올 것으로 기대됩니다.\n촉각 센서마다 일관성 없이 반응 특성이 제각각이면, 한 센서로 학습한 모델을 다른 동일한 센서에 적용하기 어렵다. 특히 부드러운 촉각 센서는 마모로 교체가 잦아 이 문제가 더욱 중요하다. ReSkin은 자기장 기반의 소프트 촉각 센서로, 자성 입자를 섞은 얇은 피부와 분리된 자기 센서 회로로 구성된다. 이를 통해 센서 인터페이스를 저비용으로 쉽게 교체 가능하게 설계했다. ReSkin에서 학습한 모델이 새로운 센서에서도 일반화되도록 하는 것은 핵심 과제다. 그러나 동일한 힘을 가해도 센서마다 측정되는 자기장 변화 분포(원시 신호)가 달라, 보정 없이 기존 모델을 쓰면 성능 저하가 발생한다. 이를 완화하기 위해 다중 센서의 데이터를 합쳐 학습하고, 삼중항(self-supervised triplet) 손실로 특징 공간을 정규화한다. triplet loss은 접촉 위치가 가까운 샘플들의 특징을 가깝게 유지하도록 유도하여, 센서 간 일관된 표현을 학습시킨다. 새로운 센서를 쓸 때는 간단한 unlabeled 데이터(예: 센서 표면을 펜으로 따라 그으며 눌러본 기록)를 수집해 triplet loss로 모델을 미세조정할 수 있다. 이렇게 Self-supervised 방식으로 추가 학습하면 레이블이 없어도 해당 센서의 특징 분포에 모델을 맞출 수 있다. ReSkin의 Self-supervised 적응 기법은 통제되지 않은 수동 조작 데이터나 센서 하드웨어 종류가 다른 경우에도 성능 향상을 보였다. 예를 들어 사람 손으로 눌러 얻은 데이터로 적응하면 접촉 위치 예측 정확도가 약 79.9%에서 84.8%로 향상되었고, 경직 보드에서 학습한 모델을 유연 보드에 적용할 때도 75% 수준의 정확도를 달성했다. AnySkin은 ReSkin의 디자인을 발전시켜, 센싱 표면과 전자부를 완전히 분리하고 자기장 세기를 강화하는 등으로 센서 간 응답의 일관성을 크게 높였다. 또한 부착 방식도 셀프 정렬되도록 고안되어 교체 시 정확히 같은 위치와 자세로 장착된다. AnySkin 패치는 교체에 12초 정도밖에 걸리지 않으며, 한 센서로 학습한 모델을 새로운 패치에 그대로 적용해도 플러그 삽입 과제 성공률 저하가 13% 이내로 매우 작았다. 반면 이전 세대인 ReSkin은 같은 조건에서 성능이 40% 이상 떨어져, AnySkin의 센서 간 일관성이 월등함을 보여준다.\n# Reference\n\nReskin 논문\nAnySkin 논문"
  },
  {
    "objectID": "posts/paper/2025-08-26-dreureka.html",
    "href": "posts/paper/2025-08-26-dreureka.html",
    "title": "📃DrEureka 리뷰",
    "section": "",
    "text": "Paper Link\nProject Link\nCode Link"
  },
  {
    "objectID": "posts/paper/2025-08-26-dreureka.html#논문의-핵심-아이디어와-주요-기여",
    "href": "posts/paper/2025-08-26-dreureka.html#논문의-핵심-아이디어와-주요-기여",
    "title": "📃DrEureka 리뷰",
    "section": "1. 논문의 핵심 아이디어와 주요 기여",
    "text": "1. 논문의 핵심 아이디어와 주요 기여\nDrEureka의 핵심 아이디어는 LLM을 활용해 Sim-to-Real 과정의 난점을 자동화하는 것입니다. 구체적으로, 사람이 일일이 짜던 보상 함수 설계와 도메인 랜덤화(Domain Randomization) 파라미터 설정을 LLM이 대신 수행합니다 . 이를 통해 시뮬레이션에서 현실로 지식이나 정책을 옮길 때 필요한 까다로운 설계 작업을 대폭 줄이고, 자동화된 파이프라인으로 신속하게 최적 구성을 찾아냅니다 .\n\n주요 기여\n\nLLM 기반 Sim-to-Real 자동화 기법 제안 – 보상 함수 설계와 물리 파라미터 도메인 랜덤화를 동시에 자동 구성하는 DrEureka 알고리즘을 제시 .\n다양한 로봇 과제에 대한 실세계 검증 – 사족보행 로봇 Unitree Go1의 고속 전진 보행, 로봇 손(LEAP Hand)의 큐브 조작 등에서 인간 전문가 설계보다 우수한 정책을 달성 .\n새로운 난제 과제에서의 성공적 전이 – 요가 공 위 걷기라는 완전히 새로운 과제를 시뮬레이션 기반 자동 커리큘럼으로 학습하고, 현실 로봇에 곧장 적용해 수 분간 균형을 유지하는 결과를 달성 ."
  },
  {
    "objectID": "posts/paper/2025-08-26-dreureka.html#사용된-방법론과-기술적-접근-분석",
    "href": "posts/paper/2025-08-26-dreureka.html#사용된-방법론과-기술적-접근-분석",
    "title": "📃DrEureka 리뷰",
    "section": "2. 사용된 방법론과 기술적 접근 분석",
    "text": "2. 사용된 방법론과 기술적 접근 분석\nDrEureka는 세 단계 파이프라인을 통해 환경과 보상 설계를 자동화합니다 .\n\n보상 함수 생성\n\n과제 설명과 안전 지침을 LLM(GPT-4 등)에 제공.\nLLM이 보상 함수 코드(예: 파이썬+NumPy)를 작성.\n“안전 프롬프트”를 통해 무리한 가속, 관절 손상을 유발하는 보상을 피하고, 안전성과 효율성을 동시에 고려하도록 유도 .\n여러 후보를 시뮬레이션에서 평가 후 성능이 가장 좋은 보상 함수를 선택 .\n\nRAPP (Reward-Aware Physics Prior)\n\n선택된 정책을 시뮬레이터에 넣고, 물리 파라미터(마찰, 질량, 감쇠, 관성 등)를 하나씩 변화시켜 정책 성능이 유지되는 범위를 찾음 .\n정책이 실패하지 않는 최소–최대 허용 범위를 기록하여, 이후 LLM에게 전달할 물리 prior로 활용 .\n이 단계는 정책이 견딜 수 있는 물리적 한계를 탐색하는 과정으로, 과도하게 넓은 랜덤화를 방지 .\n\nLLM 기반 도메인 랜덤화 생성\n\nLLM에 랜덤화 가능한 파라미터와 RAPP 범위를 제공 .\nLLM이 어떤 파라미터를 어떤 분포로 샘플링할지 제안(예: 마찰 0.6–1.0 범위에서 균일 분포) .\nLLM이 동시에 여러 후보안을 생성, 각각을 시뮬레이터에서 RL 훈련하여 최종적으로 현실에서 가장 성능 좋은 정책 선택 .\n\n\n이 과정을 반복함으로써 DrEureka는 LLM–시뮬레이터 공진화 루프를 형성하고, 보상 설계 + 랜덤화 설계를 인간 개입 없이 자동으로 최적화할 수 있습니다 ."
  },
  {
    "objectID": "posts/paper/2025-08-26-dreureka.html#실험-설계-및-결과-평가",
    "href": "posts/paper/2025-08-26-dreureka.html#실험-설계-및-결과-평가",
    "title": "📃DrEureka 리뷰",
    "section": "3. 실험 설계 및 결과 평가",
    "text": "3. 실험 설계 및 결과 평가\nDrEureka는 두 가지 대표 과제와 하나의 도전적 과제를 통해 검증되었습니다.\n\n사족보행 전진 (Go1 로봇)\n\n목표: 2 m/s 속도로 빠르게 전진.\n비교: 인간 설계 보상+DR vs DrEureka 자동 설계.\n결과: DrEureka 정책이 평균 속도 34%↑, 이동 거리 20%↑ 성능 향상 .\n지형 일반화 평가(실내 바닥, 잔디, 보도블록, 양말 신긴 발)에서도 안정적 성능 유지 .\n\nDexterous Manipulation (LEAP Hand, 큐브 돌리기)\n\n목표: 손바닥 위 큐브를 최대한 많이 회전시키기.\n결과: DrEureka 정책이 인간 설계 대비 약 3배 더 많은 회전 성공 .\n정책의 안정성이 높아 실제 로봇에서도 일관된 성능을 보임 .\n\n요가 공 위 걷기 (Ball Balancing)\n\n완전히 새로운 과제: 큰 요가 공 위에서 균형 잡으며 전진.\n결과: 시뮬레이션 학습 정책을 곧장 현실 적용, 수 분간 균형 유지하며 걷기 성공 .\n교란(발로 공을 차거나 공의 공기압을 줄임)에도 로봇이 스스로 균형을 회복 .\n\n\n이 실험들은 DrEureka가 다양한 환경에서 학습한 정책이 시뮬레이션을 넘어 현실에서도 강건하게 동작함을 보여주며, 기존의 수작업 기반 접근보다 뛰어난 성능과 일반화 능력을 입증했습니다 .\n\n\n\n\n\n\n\nUnitree Go1이 다양한 실제 지면 위에서 DrEureka로 학습한 정책을 실행하는 모습. 기존 방법보다 안정성과 속도 모두 우수함을 보여줌 ."
  },
  {
    "objectID": "posts/paper/2025-08-26-dreureka.html#기존-연구와의-차별점-및-기여",
    "href": "posts/paper/2025-08-26-dreureka.html#기존-연구와의-차별점-및-기여",
    "title": "📃DrEureka 리뷰",
    "section": "4. 기존 연구와의 차별점 및 기여",
    "text": "4. 기존 연구와의 차별점 및 기여\n\nLLM in Sim-to-Real 설계: 기존 LLM 기반 로봇 연구는 주로 자연어를 고수준 계획으로 변환하거나, 단순한 환경 샘플링 수준에 그쳤습니다 . DrEureka는 처음으로 보상 함수와 도메인 랜덤화까지 포함한 Sim-to-Real 설계 전체를 LLM이 주도할 수 있음을 보여주었습니다 .\n인간 개입 최소화: 기존 커리큘럼 설계나 도메인 랜덤화 연구는 대부분 전문가가 파라미터 범위를 손으로 지정해야 했습니다 . DrEureka는 RAPP + LLM 조합으로 이를 자동화하여 효율성과 재현성을 동시에 확보했습니다 .\n실세계 성능 검증: 많은 선행연구가 시뮬레이션에서만 결과를 보고한 것과 달리, DrEureka는 실제 로봇 실험을 통해 직접 성능을 입증했습니다 . 특히 요가 공 위 걷기와 같은 새로운 과제를 자동 설계하고 성공적으로 전이한 사례는 최초의 결과로 평가됩니다 .\n물리 기반 LLM 활용: LLM이 물리적 상식(마찰, 중력, 관성 등)을 활용하여 파라미터 범위를 합리적으로 설정하고, 그 근거까지 설명할 수 있다는 점은 AI-로봇 공학 융합 연구에서 중요한 전환점입니다 ."
  },
  {
    "objectID": "posts/paper/2025-08-26-dreureka.html#한계점-및-향후-연구-과제에-대한-비판적-고찰",
    "href": "posts/paper/2025-08-26-dreureka.html#한계점-및-향후-연구-과제에-대한-비판적-고찰",
    "title": "📃DrEureka 리뷰",
    "section": "5. 한계점 및 향후 연구 과제에 대한 비판적 고찰",
    "text": "5. 한계점 및 향후 연구 과제에 대한 비판적 고찰\nDrEureka는 강력한 결과를 보여주었지만, 몇 가지 한계점이 존재합니다 .\n\n현실 피드백 부족: 현재 DrEureka는 시뮬레이터만 사용하여 학습 후 현실에 곧장 이식하는 Zero-Shot Sim-to-Real을 목표로 합니다. 하지만 현실의 노이즈·마찰·센서 오차는 시뮬레이터에 완벽히 반영되지 않기 때문에, 일부 환경에서는 여전히 실패할 수 있습니다 . → 향후 과제: 현실 실행 데이터를 LLM 프롬프트로 되먹임하는 Sim-Real co-adaptation 루프 필요.\n감각 통합 한계: 본 연구는 proprioception 기반 제어만 다루었고, 시각·촉각 정보를 사용하지 않았습니다 . → 향후 과제: 비전/멀티모달 정보까지 통합해 더 복잡한 환경과 상호작용하는 정책 학습으로 확장해야 함.\n시뮬레이터 품질 의존성: DrEureka는 시뮬레이터의 물리 충실도가 낮으면 효과가 떨어질 수 있습니다 . → 향후 과제: 시뮬레이터 신뢰성 개선 또는 LLM이 시뮬레이터 한계 자체를 인지/보완할 수 있는 구조 필요.\nLLM 비용 및 안정성: GPT-4와 같은 LLM 사용에는 비용(논문에서는 약 15달러/24시간)과 출력 품질 편차 문제가 있습니다 . → 향후 과제: 경량화된 LLM이나 프롬프트 최적화, 출력 검증 체계가 필요합니다."
  },
  {
    "objectID": "posts/paper/2025-08-26-dreureka.html#결론",
    "href": "posts/paper/2025-08-26-dreureka.html#결론",
    "title": "📃DrEureka 리뷰",
    "section": "결론",
    "text": "결론\nDrEureka는 LLM의 코드 생성 및 물리 상식 활용 능력을 Sim-to-Real 파이프라인에 접목하여, 보상 함수와 도메인 랜덤화 설계를 자동화하는 최초의 프레임워크를 제시했습니다. 이를 통해 사족보행, 로봇 손 조작, 요가 공 걷기와 같은 다양하고 난이도 높은 과제에서 실제 로봇 전이 성공을 입증했으며, 기존 전문가 설계 기반 접근보다 더 높은 성능과 일반화 능력을 보여주었습니다 .\n물론 아직 현실 피드백의 부족, 시뮬레이터 의존성, LLM 사용 비용 등의 한계가 존재하지만, DrEureka는 로봇 제어 학습 자동화와 LLM 기반 물리 시뮬레이션 디자인의 가능성을 제시하며, 향후 로봇 연구의 패러다임 전환을 이끌 잠재력을 보여주고 있습니다."
  },
  {
    "objectID": "posts/paper/2025-08-02-grasp-gen.html",
    "href": "posts/paper/2025-08-02-grasp-gen.html",
    "title": "📃GraspGen 리뷰",
    "section": "",
    "text": "Paper Link\nProject Link\nGithub Link"
  },
  {
    "objectID": "posts/paper/2025-08-02-grasp-gen.html#introduction",
    "href": "posts/paper/2025-08-02-grasp-gen.html#introduction",
    "title": "📃GraspGen 리뷰",
    "section": "2.1 Introduction",
    "text": "2.1 Introduction\n로봇의 6-자유도 파지(Grasping) 문제는 최근 많은 발전이 있었지만, 일반적인 로봇 파지 시스템을 구축하기에는 여전히 어려움이 남아 있다. 예를 들어, 최신 벤치마크인 FetchBench에서 현재 최첨단(SoTA) 기법조차도 20% 미만의 파지 성공률을 보였으며, 지식 기반 모바일 조작 시스템인 OK-Robot의 경우 파지 모듈 실패만으로도 약 8%의 작업 오류율을 기록하였다. 이는 다양한 로봇 형태나 복잡한 실제 환경에서 파지 모델의 일반화 성능에 한계가 있음을 보여준다. 기존의 파지 기법들은 정밀한 물체 자세 정보를 필요로 하거나, 단일 물체에 대해 다중 뷰 스캔을 요구하여 복잡한 환경에는 적용하기 어렵고, 혹은 접촉점 기반(contact-point-based) 표현에 의존함으로써 그리퍼(말단작동기)의 형태가 달라지면 성능이 크게 저하되는 문제가 있었다. 특히 접촉점 기반 파지 모델들은 대칭적인 평행 그리퍼 이외의 다른 형태로 일반화하기 어렵고, 예측한 그립의 점수화를 정확히 수행하는 데에도 한계를 보였다. 일부 연구에서는 복잡한 적재 환경(clutter)에서 여러 물체에 대해 파지를 생성하는 방안을 제시하였으나, 이러한 환경 중심(scene-centric) 접근은 전체 Scene을 시뮬레이션하거나 대규모의 실제 데이터 수집이 필요해 확장성이 떨어지며, 테스트 시 현실 세계 분포와의 괴리 문제를 야기한다. 더욱이 이러한 방법들도 궁극적으로는 인스턴스 세그멘테이션과 결합하여 목표 물체를 지정하는 방식을 사용하므로, 최근 SAM 등의 파운데이션 모델 기반 분할 기법의 발전으로 물체 중심(object-centric) 접근으로 회귀하여 파지 생성을 단순화할 수 있다는 논의가 제기되고 있다.\n이러한 배경에서, 본 리뷰의 대상인 GraspGen은 확산 기반 생성 모델과 효율적인 Discriminator(discriminator)를 결합한 새로운 6-자유도 파지 프레임워크로서, 기존 한계를 극복하고자 한다. 이 연구의 핵심 기여는 두 가지로 요약된다:\n\n다양한 조건에 대한 유연한 파지 생성: GraspGen은 하나의 통합된 Diffusion-Transformer 기반 아키텍처로 여러 형태의 그리퍼(평행 그리퍼 두 종류와 흡착 패드)와 다양한 관측 환경(부분 point cloud vs. 완전 point cloud), Scene 복잡도(단일 물체 vs. 복잡한 적재 환경), 시뮬레이션 vs. 실제 등 다양한 설정에 걸쳐 확장성을 보이는 파지 생성 시스템을 구현하였다. 이는 현존 파지 시스템의 유연성 부족 문제를 개선한 것이다.\nOn-Generator 훈련을 통한 Discriminator 개선: 기존 6-자유도 파지 Discriminator들은 사전 수집된 오프라인 데이터로만 학습되었으나, GraspGen은 생성기가 만들어낸 샘플 분포를 직접 활용하여 Discriminator를 훈련시키는 On-Generator 훈련 방법을 도입하였다. 이를 통해 Discriminator가 생성 모델이 범하는 오류 패턴을 인지하고 잠재적 거짓 양성(false positive) 파지 후보에 낮은 점수를 부여하도록 학습됨으로써, 오직 오프라인 데이터로만 학습된 표준 Discriminator에 비해 성능이 크게 향상됨을 보였다. 다시 말해, GraspGen의 Discriminator는 확산 생성 모델의 고질적 실수(예: 물체와 미세 충돌하거나 물체에서 멀리 떨어진 부적절한 파지 자세)를 걸러내는 능력이 향상되었다.\n\n추가적으로 저자들은 GraspGen의 다양한 설계 선택(훈련 레시피부터 아키텍처 개선까지)이 이전 연구 대비 성능을 향상시킴을 입증하고 있으며, 추론 속도와 메모리 사용량 면에서도 개선을 달성했음을 보고하였다. 또한 5,300만 개 이상의 파지 사례로 구성된 새로운 대규모 시뮬레이션 데이터셋을 공개하여, 본 분야 연구 커뮤니티에 자원을 제공하고 GraspGen의 객체/그리퍼 확장성을 뒷받침하였다. 본 리뷰에서는 이 논문의 동기와 관련 연구를 살펴보고, GraspGen 프레임워크의 전체적인 구성과 확산 기반 파지 생성 아키텍처의 설계 및 참신성, On-Generator 훈련 전략의 구체적 방법과 효과를 분석한다. 이어서 실험 구성과 결과를 섹션별로 상세히 검토하고, 성능상의 우위와 한계점을 비판적으로 논의한다. 마지막으로 이러한 고찰을 바탕으로 향후 연구 방향에 대해 제언한다."
  },
  {
    "objectID": "posts/paper/2025-08-02-grasp-gen.html#related-work",
    "href": "posts/paper/2025-08-02-grasp-gen.html#related-work",
    "title": "📃GraspGen 리뷰",
    "section": "2.2 Related Work",
    "text": "2.2 Related Work\n6-자유도 파지는 일반적으로 Grasp Sampling(GS)과 Grasp Analysis(GA)의 두 단계로 구성된다. 최근 생성 모델의 발전에 힘입어 GS 단계에 딥러닝 기반의 확률 생성 기법들이 도입되었다. 초기에는 자기회귀 모델이나 변분 오토인코더(VAE) 기반 접근법이 시도되었으며, 후속 연구에서는 확산 모델(diffusion model)을 6-DoF 파지 생성에 활용하는 방향으로 나아가고 있다. 대표적으로, Mousavian 등의 VAE 기반 GraspNet, Urain 등의 SE3-DiffusionFields, Weng 등의 DexDiffuser 등이 6-자유도 파지 문제에 확산 모델을 적용한 최근 연구들이다. 한편 GA 단계에서는 생성된 파지 후보의 성공 가능성을 점수화하고 순위 매기기 위한 Discriminator 모델이 주로 사용된다. 예를 들어, GQCN (Grasp Quality CNN, Mousavian 등 2019)이나 PointNetGPD (Liang 등 2019) 등이 단일 파지 평가기(discriminator)로 활용된 바 있으며, Weng 등은 확산 기반 생성기에 별도의 Discriminator를 접목하였다. 흥미로운 시도로는 단일 모델이 파지 후보 생성과 평가를 동시에 수행하도록 한 방법도 있는데, Sundermeyer 등은 그리퍼 접촉점을 표현한 후 이를 학습해 곧바로 최고 파지를 예측하도록 하였고, Yuan 등은 이를 변형하여 Transformer를 활용한 M2T2 모델을 개발하였다. 그러나 이러한 접촉점 기반 접근은 앞서 언급했듯이 그리퍼 형태가 달라지면 성능이 떨어지는 문제가 있다.\n입력 데이터 형태 측면에서도 다양한 연구가 이루어졌다. 파지 모델은 물체의 3D 형상을 받아들이는데, point cloud 기반 표현이 많이 활용되었고, 이외에도 암시적 표현(Implicit surface)이나 복셀(voxel) 표현 등을 사용한 사례가 있다. GraspGen을 비롯한 다수의 최신 프레임워크는 물체 중심의 3D point cloud 입력을 필요로 하며, 이는 전이학습된 인스턴스 세그멘테이션 모듈 등을 통해 Scene에서 대상 물체의 point cloud을 분리하여 공급받는 시나리오를 가정한다. 물체 중심 접근의 장점은 훈련 시 복잡한 Scene을 일일이 생성하지 않아도 된다는 점이며, SAM2 등 분할 모델의 성숙으로 이러한 접근이 현실적으로 가능해졌다는 것이 저자들의 주장이다.\n확산 모델과 로봇 조작 분야의 접목은 최근 활발하다. 확산 모델(Diffusion model)은 이미지 생성에서 큰 성공을 거두었고 (Ho 등 2020; Song & Ermon 2019) 높은 차원 연속확률 분포를 모델링하는데 강력한 도구로 인정받고 있다. 로봇 분야에서도 이를 활용하여 비전-모터 정책 학습 (Chi 등 2024), 모션 플래닝 (Huang 등 2024), Scene 생성 등 다양한 문제를 다루고 있으며, 특히 파지 생성에는 여러 연구가 시도되었다. Urain 등은 antipodal 그립의 6-DoF 위치를 확산 모델로 생성하는 개념을 처음 제안하였고, Weng 등은 이를 미지의 물체와 멀티핑거 손 동작으로 확장하여 DexDiffuser를 발표하면서 Discriminator를 추가하였다. 이 두 연구는 GraspGen에 직접적인 선행으로 볼 수 있는데, GraspGen은 대규모 다중-그리퍼 데이터셋을 추가로 구축하고 생성(파지 후보 생성)과 평가(파지 판별) 단계 모두에서의 성능 향상을 꾀한 점에서 차별화된다. 다시 말해, GraspGen은 이전 확산 기반 파지 생성기의 개념을 발전시키면서, 온-제너레이터 데이터 기반 Discriminator 학습이라는 새로운 요소를 도입하여 전체 파지 파이프라인의 정확도와 효율을 개선한 최신 프레임워크라고 할 수 있다."
  },
  {
    "objectID": "posts/paper/2025-08-02-grasp-gen.html#graspgen-프레임워크-proposed-method",
    "href": "posts/paper/2025-08-02-grasp-gen.html#graspgen-프레임워크-proposed-method",
    "title": "📃GraspGen 리뷰",
    "section": "2.3 GraspGen 프레임워크 (Proposed Method)",
    "text": "2.3 GraspGen 프레임워크 (Proposed Method)\nGraspGen은 파지 후보 생성과 파지 평가의 두 모듈로 구성된 모듈식 프레임워크이다. 전자는 확산 모델에 기반한 생성기(generator)가 맡고, 후자는 별도로 학습된 Discriminator(discriminator)가 담당한다. 생성기와 Discriminator는 모두 물체 중심의 point cloud 입력에 조건부로 동작하며, 두 모듈 모두에 Transformer 신경망 구조를 활용한 것이 특징이다. 이하에서는 먼저 확산 기반 파지 생성기의 설계와 학습 방식을 살펴보고, 이어서 On-Generator 훈련 기법을 적용한 Discriminator의 구조와 훈련법을 설명한다. 마지막으로 GraspGen의 대규모 데이터셋 구성에 대해 언급한다.\n\n2.3.1 Grasp Generation with Diffusion\nGraspGen의 핵심은 SE(3) 공간에서의 6-자유도 파지 분포를 확산 모델로 학습하는 것이다. 각 물체에 대해 성공 가능한 파지 자세는 연속적이면서도 고도로 다중모드(multimodal) 분포를 이루므로, 이를 데이터 주도적으로 모델링하는 데 확산 기반 접근이 적합하다. 확산 모델에서는 학습 시 정답 데이터에 점차 노이즈를 주입하고, 추론 시는 반대로 노이즈로부터 데이터를 복원하는 과정을 거친다. Urain 등(2023)은 6-DoF 파지를 에너지 기반 모델(EBM)로 정식화하여 score-matching Langevin dynamics 방식의 확산을 구현하였으나, 이 접근은 추론 시 매 단계마다 EBM의 로그-밀도 기울기를 계산해야 하므로 매우 느리다는 단점이 있다. GraspGen은 대신 DDPM(Denoising Diffusion Probabilistic Model) 방식을 채택하여, 반복적 노이즈 제거로 분포를 모델링한다. DDPM은 구현이 간단하고 계산 효율이 높아 파지 문제에 더 적합하며, 최근 연구에 따르면 EBM 기반 SMLD와 DDPM 사이에 이론적 동등성이 성립함이 알려져 있다.\n한 가지 문제는 SE(3) 공간 중 회전 공간(SO(3))이 유클리드 공간이 아니라는 점인데, GraspGen은 Urain 등의 선행 연구와 유사하게 SE(3)을 translation(평행이동) 부분과 회전 부분으로 분리(factorize)하여 다룬다. SO(3)는 특수한 리 군 공간이지만, 이를 적절한 표현(예: 회전 행렬 또는 리 대수 등)으로 변환하면 사실상 유클리드 공간처럼 취급할 수 있다. GraspGen은 translation 벡터(3차원)와 회전 표현(3차원; 예: Lie algebra)로 그립을 표현하고, 이들 각각에 별도의 확산 프로세스를 적용하였다. 하나의 DDPM으로 translation+회전을 동시에 생성하는 것보다 두 개의 프로세스로 분리하여 병행 생성하는 편이 성능이 더 우수했는데, 저자들은 이렇게 분할함으로써 모델이 각 부분에 보다 특화된 학습을 할 수 있었기 때문으로 해석한다. 또한 파지의 총 차원이 6으로 비교적 낮기 때문에, 이미지 생성에 흔히 쓰이는 100회 이상의 확산 단계 대신 20회 미만의 노이즈 제거 스텝만으로도 충분한 성능을 얻을 수 있었다고 보고한다. (이미지의 경우 픽셀 차원이 수만 이상이므로 훨씬 복잡한 반면, 파지 자세는 6차원의 비교적 간단한 출력이라는 점을 고려한 것이다.)\n확산 모델 학습 시 입력과 출력의 스케일 정규화도 중요한 이슈이다. 특히 translation 성분의 경우 물체 크기에 따라 값의 범위가 크게 달라질 수 있다. GraspGen은 학습 데이터의 통계치를 이용하여 translation 벡터를 정규화하였는데, 모든 학습 데이터(성공 파지들의 translation 성분)를 모은 뒤 그 표준편차에 해당하는 값을 스케일 인자로 채택하였다. 이렇게 하면 별도 그리드 탐색 없이 자동으로 적절한 정규화 계수를 설정할 수 있으며, 실험 결과 이 값이 성능 측면에서 합리적인 국소 최적 역할을 함을 확인하였다. 예를 들어 Franka 그리퍼의 경우 약 3.27의 스케일 인자를 사용하였다. 한편 회전 성분은 이미 제한된 범위를 가지므로 (예: 6D 회전 표현의 경우 한정된 공간), 추가 정규화가 필요 없다.\nGraspGen의 확산 모델 네트워크는 Transformer 기반으로 구성된다. 우선 물체의 point cloud은 최신 구조인 PointTransformerV3 (PTv3)로 임베딩된다. 이전까지의 생성적 파지 연구들은 주로 PointNet++와 같은 PointNet 계열 백본을 사용했으나, GraspGen은 처음으로 트랜스포머 기반 point cloud 인코더를 도입하였다. PTv3는 비정형 point cloud을 일련의 토큰(시퀀스)으로 변환한 후 self-attention을 적용하는 방식으로, 기존 PointNet++의 복잡한 이웃 탐색 연산을 피하면서도 높은 표현력을 보여주는 최첨단 기법이다. 이렇게 얻은 물체 임베딩 토큰과, 그립의 현재 노이즈 상태(혹은 시간 step) 등의 정보를 결합하여 노이즈 예측 네트워크가 구성된다. 해당 네트워크는 Diffusion-Transformer 아키텍처라 불리며, 시간 스텝 인덱스 t는 사인-코사인 위치 인코딩으로 임베딩되고 그립 포즈는 MLP를 통해 변환된 후 Transformer에 입력된다.\n학습 시에는 임의의 스텝 t를 선택하여 현재의 그립 데이터에 노이즈를 섞은 후, 네트워크가 주입된 노이즈를 맞추도록 학습한다 (denoising loss). 보다 구체적으로, 노이즈 예측 네트워크 f_\\theta가 물체 point cloud P에 조건부로 주어진다고 할 때, 손실함수는 실제 노이즈 \\epsilon과 예측 노이즈 \\hat{\\epsilon}=f_\\theta(P, t, \\text{noisy grasp}) 사이의 차이(평균제곱오차)로 정의된다.\n이렇게 훈련된 생성기는 추론 시 새로운 point cloud P에 대해 랜덤 노이즈로 초기화된 그립 포즈들을 점진적으로 denoising하여 다양한 파지 후보들을 만들어낸다. GraspGen에서는 이러한 생성 과정에서 물체 point cloud과 그립 좌표계를 물체의 중심으로 정규화(평행이동)하여 입력함으로써, 좌표계 설정에 따른 혼동을 줄였다. 최종적으로 생성된 다수의 파지 후보들은 다음 단계인 Discriminator로 넘어가 성공 가능성 점수를 부여받게 된다.\n\n\n2.3.2 Grasp Evaluation with On-Generator Training\n생성 모델만으로 파지 후보를 생성할 경우 모델의 근사 오류로 인해 현실적으로는 성공 확률이 낮은 거짓 양성 파지(False Positive Grasp)들도 상당수 포함될 수 있다. 예컨대, 그립이 살짝 물체를 관통하거나 물체에서 상당히 떨어진 위치로 생성되는 등, 데이터 분포의 드문 영역에서 나온 부적절한 그립들이 있을 수 있다. 따라서 최종 로봇 실행 전에 이러한 후보들을 걸러낼 평가 메커니즘이 필수적이며, 많은 선행 연구가 별도의 학습된 Discriminator로 각 파지의 성공 가능성을 점수화하여 상위 몇 개를 선택하는 방식을 사용해왔다. GraspGen 역시 Discriminator를 활용하나, 두 가지 중요한 개선점을 도입하여 기존 접근의 성능 한계를 극복하고자 했다.\n\n2.3.2.1 On-Generator Training\n첫째는 앞서 강조한 On-Generator 훈련 기법이다. 기존의 시뮬레이션-현실(Sim-to-real) 파지 학습에서는 성공/실패로 라벨링된 오프라인 데이터셋만으로 Discriminator를 학습시켰다. 그러나 저자들은 생성기가 만들어내는 파지 분포가 이 오프라인 데이터 분포와 다르다는 점에 주목하였다. 오프라인 데이터의 실패 사례는 주로 물체에 전혀 접촉하지 못한 그립(예: 충돌 없이 허공을 집는 그립)이거나 아주 엉뚱한 위치의 그립들로 이루어지지만, 확산 생성기가 만들어내는 그립 중에는 물체를 살짝 관통하는 등 미세한 충돌을 일으키는 경우가 있다. 또한 생성 모델의 확률 분포 꼬리에 해당하는 이상치(outlier) 그립—예를 들면 물체에서 비정상적으로 멀리 떨어진 그립—도 나타날 수 있다. 이러한 사례들은 기존 오프라인 데이터의 실패 범주에는 거의 포함되지 않는 경우가 많다 (예: ACRONYM 데이터셋의 경우 충돌하는 실패 그립은 아예 생성하지 않았다 보고됨). 따라서 생성 모델의 고유한 에러 모드를 Discriminator가 학습하려면, 생성기 산출물을 활용한 별도 학습이 필요하다는 것이 저자들의 가설였다.\n이를 구현하기 위해, GraspGen은 Algorithm 1로 제시된 절차에 따라 On-Generator 데이터셋을 구축하여 Discriminator를 학습시켰다.\n\n\n\n구체적으로, 우선 학습에 사용된 모든 훈련 객체 약 7천 개에 대해 생성기 모델을 동결한 채 충분한 수의 파지 후보를 생성한다. 각 객체마다 약 2천 개씩, 총 1,400만 개 가량의 파지 샘플을 얻었으며, 이는 초기 오프라인 데이터셋(성공/실패 라벨이 있는 파지 데이터)의 규모와 비슷한 양이다. 다음으로 이 생성된 파지들에 대해, 오프라인 데이터 생성 때와 동일한 시뮬레이션 절차(예: 그립 후 흔들어서 유지되는지 확인하는 테스트)를 거쳐 성공 여부를 라벨링한다. 이렇게 얻은 On-Generator 데이터셋(생성기 출력에 대한 시뮬레이션 라벨)은 생성기의 오류 패턴이 반영된 실패 사례들을 다수 포함하고 있으므로, 이 데이터를 사용해 Discriminator를 학습시키면 생성기와 동일 분포에서의 판별 성능을 크게 높일 수 있다. 실제로 저자들의 분석에 따르면, 오직 오프라인 데이터로 학습한 모델 대비 On-Generator 데이터로 학습한 Discriminator가 현저히 높은 AUC를 기록하여 최고의 성능을 보였고, 두 분포를 모두 섞어서 학습한 경우 그 중간 정도 성능을 보였다. 요컨대 생성기 산출물에 특화된 훈련이 Discriminator의 거짓 양성 인지 능력을 비약적으로 향상시켰음을 알 수 있다. 참고로, 이러한 접근의 당위성은 저자들이 제시한 분포 비교를 통해서도 확인된다. 지구 이동 거리(EMD)로 오프라인 vs. On-Generator 데이터의 분포 차이를 정량화한 결과, 두 분포 간에 상당한 차이가 존재하며 특히 실패 그립들에서 그 차이가 훨씬 큼을 보였다. 이는 실패 사례의 공간적 분포가 생성기 출력 쪽이 더 넓게 퍼져있음을 의미하며, On-Generator 훈련의 필요성을 뒷받침한다.\n\n\n2.3.2.2 Efficient Distriminator\n둘째 개선점은 효율적인 Discriminator 아키텍처이다. 기존의 6-DoF 파지 Discriminator는 물체 입력을 처리하기 위해 별도의 PointNet 기반 네트워크를 처음부터 다시 학습시키는 등 복잡한 구조를 사용했는데, GraspGen은 생성기 단계에서 학습한 물체 임베딩을 그대로 활용함으로써 이러한 중복을 제거했다. 구체적으로, 앞 단계의 PTv3 물체 인코더가 출력한 물체 임베딩 벡터를 가져오고, 여기에 대응하는 그립 자세를 간단한 형태로 표현한 벡터를 단순 연결(concatenation)하여 MLP Discriminator에 입력한다. 그립 자세의 표현으로는 SO(3)의 회전 부분을 좌표로 나타낸 벡터 등을 이용한다. 이는 Mousavian 등(2019)의 GQCN에서 사용한 기법—그립의 6D pose로 미리 정의된 그리퍼 점들을 변환시켜 물체 point cloud과 함께 PointNet에 넣는 복잡한 방식—보다 훨씬 단순화된 처리이다. 덕분에 GraspGen의 Discriminator는 최소한의 추가 매개변수와 연산만으로 동작하며, 물체 임베딩 부분은 학습된 것을 frozen하여 쓰고 오직 마지막 MLP 계층만 이진 교차 엔트로피 손실로 학습하면 된다. 이러한 경량 설계로 인해 GraspGen의 Discriminator는 기존 대비 메모리 사용을 크게 절감하면서도 정확도를 높였는데, 논문에 따르면 이전 SoTA Discriminator 구조에 비해 약 6.7 포인트 높은 AUC를 달성하면서 메모리 사용량을 21% 감소시켰다고 한다. 이처럼 가벼운 Discriminator는 다수의 파지 후보를 빠르게 평가하는 데 유리하며, 실제 GraspGen 프레임워크의 실시간 추론 성능(별도 최적화 전 약 20Hz 수준)에도 기여하는 부분이다.\n\n\n\n2.3.3 GraspGen Dataset\nGraspGen의 성공은 대규모 학습 데이터에 크게 힘입고 있다. 저자들은 GraspGen을 확장성 있게 학습시키기 위해 다양한 물체와 그리퍼를 포괄하는 방대한 시뮬레이션 파지 데이터셋을 구축하였다. 이 데이터셋은 총 3종의 그리퍼(Franka Panda, Robotiq 2F-140 병렬 그리퍼 두 가지; 직경 30mm의 진공 흡착 패드) 각각에 대해 약 1,700만 개씩의 파지 시도 데이터를 포함하며, 총 합하면 5,300만에 이르는 규모이다.\n데이터셋 구축을 위해 사용된 물체는 대규모 3D 자율형 객체 모음인 Objaverse에서 선정되었다. Objaverse는 수십만 개의 3D 모델을 담고 있는데, 이 중 저자들은 LVIS 데이터셋의 1,156개 범주와 겹치면서 라이선스가 CC-BY인 36,366개의 메쉬를 선별하였다. 이 방대한 물체 풀(pool)로부터 ShapeNetSem 기반의 기존 파지 데이터셋(예: ACRONYM)보다 더 크고 다양하며 라이선스 제약이 적은 학습 자원을 확보하였다. 다만 공정한 비교를 위해, 이 중 무작위로 8,515개의 객체를 뽑아 ACRONYM과 동일한 규모의 하위셋을 구성하고 이를 일부 실험에 활용하기도 했다.\n각 객체에 대해서는 표면 주변의 공간에서 uniform random로 2,000개의 파지 후보(6D 그립 자세)를 샘플링한 후, 시뮬레이터를 통해 성공 여부를 레이블링하였다. 레이블링 파이프라인은 ACRONYM에서 사용된 것과 동일하게, NVIDIA Isaac Gym/Sim 물리 시뮬레이터 안에서 그리퍼로 물체를 쥔 후 흔드는(shaking) 동역학 테스트를 거쳐 물체가 떨어지지 않으면 성공으로 판정하는 방식을 따랐다. 다만 흡착 패드 그리퍼의 경우 흡착 모델 특성상 물리 시뮬레이션보다는 분석적 모델(Mahler 등 2018의 흡착 성공 판정 공식)을 사용하여 성공 여부를 결정하였다.\n\n\n\n이렇게 하여 평행 그리퍼 2종과 흡착 패드 각각에 대해 독립된 파지 데이터셋을 얻었으며, GraspGen은 이를 활용하여 다중-그리퍼에 공용으로 적용 가능한 생성 모델을 학습할 수 있었다. 이 새로운 데이터셋은 현 시점 가장 큰 규모의 공개 6-DoF 파지 데이터셋으로서, 향후 보다 복잡한 파지 문제(예: 다지 손가락 그리퍼나 모바일 매니퓰레이터 환경)로의 확장 연구에도 유용한 자원이 될 것으로 기대된다."
  },
  {
    "objectID": "posts/paper/2025-08-02-grasp-gen.html#experimental-evaluation",
    "href": "posts/paper/2025-08-02-grasp-gen.html#experimental-evaluation",
    "title": "📃GraspGen 리뷰",
    "section": "2.4 Experimental Evaluation",
    "text": "2.4 Experimental Evaluation\nGraspGen의 성능은 다양하게 검증되었다. 저자들은 시뮬레이션 환경에서 기존 방법들과 비교 실험을 수행하였고, 복잡한 적재 시나리오 벤치마크에서의 성능을 측정하였으며, 부분 관측 vs. 완전 관측 상황에 대한 일반화 실험도 진행하였다. 아울러 On-Generator 훈련의 효과를 분석하기 위한 추가 실험과, 모델 설계 요소들에 대한 성능 기여도 분석(ablation)을 실시하였다. 마지막으로 실제 로봇 실험을 통해 시뮬레이션으로 학습된 GraspGen의 현실 적용성을 평가하였다. 각 결과를 순차적으로 살펴본다.\n\n2.4.1 Simulation Results\n실험 셋팅\n\n\n\n우선 단일 물체에 대한 6-DoF 파지 생성 정확도를 기존 기법들과 비교하였다.\n비교 대상 베이스라인으로는 접촉점 기반 방법인 M2T2 (Yuan 등 2023)와 Contact-GraspNet (Sundermeyer 등 2021), 확산 모델 기반 방법인 SE3-DiffusionFields (Urain 등 2023)와 DexDiffuser (Weng 등 2024), 그리고 강화학습 기반의 AnyGrasp (Fang 등 2023)를 포함하였다. 다만 Contact-GraspNet은 이전 연구에서 이미 M2T2보다 성능이 낮은 것으로 보고되어 본 장의 주요 비교에서는 제외하고, 후술하는 추가 실험에서 다루었다고 한다. 또한 AnyGrasp의 경우 라이선스 문제로 인해 클러스터 상의 시뮬레이션에서 직접 실행하지 못해, 시뮬레이션 비교에서는 빠지고 추후 실제 로봇 실험에서만 다루었다. 공정한 비교를 위해 모든 학습 모델은 앞서 소개한 동일한 GraspGen 데이터셋(Franka-ACRONYM 하위셋, 약 8.5k 객체)으로 훈련되었으며, 테스트는 그 중 815개의 미사용 객체에 대해 각 2,000개의 파지 후보를 생성하여 총 162만 회의 시뮬레이션 파지 시도로 성공률을 측정하는 방식으로 진행되었다.\nFull Point Cloud of Single Objects\n\n\n\n이 실험에서는 full point cloud – 즉 물체의 3D 메쉬를 샘플링한 완전한 point cloud (스스로 가리는 경우가 없는 상황) – 을 입력으로 사용하여, 순수 파지 생성기의 성능을 평가하였다. 평가 지표로는 Precision-Coverage 곡선을 사용하였는데, Precision은 파지 성공률(정밀도)에 해당하고 Coverage는 예측한 그립들이 실제 양성 그립 분포를 얼마나 포괄하는지 나타내는 지표로서, 일정 거리 이내에 예측 그립이 존재하는 실제 성공 그립의 비율(Recall에 유사한 개념)로 정의된다. Coverage는 파지 결과의 공간적 다양성을 나타내는 척도이며, 두 값 사이의 AUC (곡선 아래 면적)가 높을수록 이상적이다.\n비교 결과, GraspGen이 모든 기준에서 우수한 성능을 보였다. Precision-Coverage 곡선의 AUC 측면에서 GraspGen은 기존 방법들을 크게 상회하였으며, AUC 기준 2위와의 격차가 상당한 것으로 보고되었다. 특히 Discriminator를 활용하는 방법들(GraspGen, DexDiffuser, M2T2)이 순수 생성 모델인 SE3-DiffusionFields보다 높은 성능을 보이며, Discriminator의 중요성을 재확인시켰다. 그 중에서도 GraspGen의 Discriminator는 On-Generator 훈련 덕분에 DexDiffuser의 Discriminator보다 생성기 출력 분포에 잘 적응되어 있어, 생성된 그립의 순위매김을 더 정확히 수행한 것으로 나타났다. 반면 M2T2의 접촉점 판별 모듈은 성공 그립에 대해서만 학습되어 실제로는 좋은/나쁜 접촉점 구별에 그치므로, 실패 그립을 걸러내는 능력이 떨어져 전반적인 성능이 낮았다. 이러한 결과는 GraspGen의 생성-평가 결합 전략의 효과를 입증하는 동시에, 파지 문제에서 생성 품질과 함께 평가(스코어링) 품질이 중요함을 보여준다.\nTask-level Evaluation in Clutter\n추가로, GraspGen은 FetchBench 벤치마크를 통해 복잡한 적재 환경(clutter)에서의 파지 성능도 평가되었다.\n\n\n\nFetchBench (Han 등 2024)는 다양한 물체들이 놓인 테이블 환경에서 인식-파지-경로계획-배치에 이르는 전체 파지 파이프라인을 종합 평가하는 시뮬레이션 벤치마크이다. 실험에서는 Franka Panda 로봇팔과 100개의 임의 생성 장면에 대해, 각 장면마다 60개의 파지-이동 작업을 시도하여 총 6,000회의 grasp-place 시나리오를 평가하였다. 이때 GraspGen의 입력은 단일 RGB-D 카메라 관측으로 생성된 부분 point cloud들이며, 인스턴스 세그멘테이션을 통해 물체별 point cloud을 얻는다 (실제 로봇 실험과 동일한 설정). 공정한 비교를 위해, 경로계획에는 장면의 정확한 충돌 모델(ground-truth collision mesh)을 사용함으로써 인식 오차나 모션 플래너의 불완전성이 파지 성능 평가에 끼치는 영향을 줄였다.\n결과 지표로는 task success과 grasp success이 사용되었는데, 전자는 물체를 집어 목표 위치에 놓는 전체 작업의 성공 비율이고 후자는 집어 드는 단계까지만 고려한 비율이다. (보통 grasp success가 더 높게 나오며, grasp 이후 운반 중 미끄러짐이나 충돌이 추가로 과제 실패를 야기할 수 있다.) 흥미롭게도, 전지적 시점에서 최상의 성능을 낼 수 있는 Oracle 플래너(즉 데이터셋에 있는 실제 성공 그립을 알고 있다고 가정)가 시도되어 비교되었는데, 이 Oracle의 성능조차 grasp success 약 80%, task success 65% 남짓에 그쳤다. 이는 FetchBench의 난이도가 매우 높음을 보여주며, 주요 원인으로는 충돌 없는 경로가 없는 경우, 기존 모션 플래너 한계로 경로 탐색 실패, 물체가 비좁은 공간에 있어 그립이 있어도 진입 불가능 등의 현실적인 문제가 지목되었다. 이러한 한계는 GraspGen 등 파지 모듈 외적인 요소로 인한 실패 요인으로, 차후 보다 고차원적인 통합적 계획/제어 정책 연구가 필요함을 시사한다.\n그럼에도 불구하고, GraspGen은 FetchBench에서 최신 기존 기법들을 능가하는 성과를 보였다. Contact-GraspNet과 M2T2 대비 각각 유의미한 향상폭(수 %~두 자릿수 %대)을 기록하며 종합적인 SOTA 성능을 달성했다. 이는 GraspGen이 복잡한 적재 환경에서도 강인한 파지 후보를 생성하고, 모션 플래너 등의 후속 단계에서 필터링을 거친 후에도 여전히 실행 가능한 좋은 파지들을 제공함을 뜻한다. 요약하면, 단순 Scene(단일 물체)부터 복잡한 Scene(다중 물체)까지 GraspGen의 파지 생성/평가 품질이 동급 최고 수준임을 시뮬레이션으로 입증한 것이다.\nSensitivity to Occlucions\n마지막으로, 관측 정보의 불완전성에 대한 일반화 실험 결과를 살펴보자.\n\n\n\nGraspGen은 부분 point cloud과 완전 point cloud 모두에 대응할 수 있도록 훈련될 수 있는데, 저자들은 훈련 데이터 구성에 따른 성능 변화를 분석하였다. 하나의 GraspGen 모델을 부분 관측 데이터(단일 뷰 point cloud)만으로 훈련한 경우 완전 point cloud 상황에서 성능 저하가 뚜렷했고, 그 반대의 경우도 마찬가지로 부분 point cloud에 대해 성능 문제가 나타났다. 이는 각각의 경우 모델이 한쪽 분포에 과적합되어 다른 경우에 적응하지 못한 것으로 볼 수 있다. 반면 두 가지 유형의 point cloud 데이터를 50:50 비율로 섞어 훈련한 모델은 부분/완전 point cloud 모두에 견고한 성능을 보였다. 이는 훈련 데이터의 다양성이 GraspGen의 관측 변화에 대한 일반화에 중요함을 시사하며, 실제 어플리케이션에서 센서 구성이 달라지거나 멀티뷰/싱글뷰 환경이 혼재할 경우를 대비해 혼합 데이터를 사용할 필요가 있음을 보여준다.\n\n\n2.4.2 Analysis of On-Generator Training\n본 절에서는 On-Generator 훈련 기법의 효과를 정량적으로 분석한 결과에 대해 조금 더 자세히 언급한다. 앞서 On-Generator 데이터셋의 분포 차이를 EMD로 비교하여 그 필요성을 보인 바 있는데, 추가로 Discriminator 학습 데이터 구성에 따른 성능을 직접 시험하였다.\n\n\n\nDiscriminator를 오프라인 데이터로만 학습한 경우, On-Generator 데이터로만 학습한 경우, 그리고 두 데이터를 혼합하여 학습한 경우를 비교한 결과, On-Generator 전용 학습 모델이 가장 높은 성능을 보였고 혼합 학습이 그 다음, 순수 오프라인 학습이 가장 저조했다. 예를 들어 AUC 기준으로 오프라인 전용 대비 On-Generator 전용이 상당한 상승폭을 보였다고 보고된다. 이는 On-Generator 훈련이 생성기 고유의 실패 양상을 잡아내는 데 효과적임을 재확인해준다. 또한 On-Generator 데이터로 학습한 모델의 성공 사례를 분석해보면, 물체와 살짝 겹치는 그립이나 경미한 자세 오류로 인해 불안정한 그립 등을 잘 걸러낸다는 점을 확인할 수 있다. 반면 오프라인 데이터로 학습한 Discriminator는 이러한 경우에 상대적으로 높은 점수를 줌으로써 잘못된 양성으로 남기는 경향이 있었다고 한다. 결국 On-Generator 훈련 덕분에 GraspGen의 Discriminator는 자기 생성기의 고질적 실수까지도 인지하여 걸러줄 수 있게 되었고, 이것이 전체 파지 성공률 향상에 핵심적인 기여를 했음을 알 수 있다.\n추가적인 소규모 ablation 실험도 Discriminator 구조의 장점을 뒷받침한다. 기존의 복잡한 PointNet 기반 Discriminator(Mousavian 등, 2019 등)와 비교하여 GraspGen의 경량 Discriminator는 메모리 사용을 1/5 수준(약 21%)으로 줄이면서도 정확도를 높였다고 보고된다. 이는 동일한 hardware 자원에서 훨씬 많은 수의 파지를 동시에 평가할 수 있음을 의미하며, 특히 복잡한 Scene에서 상위 파지를 찾기 위해 수백~수천 개의 후보를 걸러내야 하는 상황에서 큰 이점이 된다.\n\n\n2.4.3 Ablation Studies\nGraspGen에 도입된 몇 가지 설계상의 선택들에 대해, 저자들은 별도의 실험을 통해 각각의 성능 영향도를 평가하였다. 주요 ablation 결과는 다음과 같다:\n\ntranslation 정규화 스케일: 앞서 언급한 대로, translation 성분에 대한 정규화 스케일의 크기는 성능에 비선형적(convex) 관계를 보였다. 너무 작거나 너무 크게 정규화하면 오히려 파지 예측 오차가 증가하거나 recall(coverage)이 감소하였으며, 적절한 중간값에서 균형이 맞춰졌다. 실험 결과 저자들이 데이터 기반 공식으로 계산한 값이 이 최적점 근처의 성능을 보여, 번거로운 하이퍼파라미터 탐색을 대체할 수 있음을 확인하였다.\n\n\n\n\n\n회전 표현: 회전을 나타내는 방법으로는 6차원 회전 벡터 표현, 오일러 각, Lie Algebra 등 여러 가지를 시험했는데, 유의미한 성능 차이가 나타나지 않았다고 한다. 이는 GraspGen의 확산 모델이 회전 공간을 학습하는 데 있어 특정 표현에 크게 의존하지 않을 만큼 충분한 학습 용량을 지닌 것으로 해석할 수 있다.\npoint cloud 인코더 백본: PointNet++ 대비 PointTransformerV3를 사용함으로써 성공률 및 정밀도 향상을 얻었다. 구체적으로, PTv3로 교체 시 translation 오차가 감소하고 recall(coverage)이 증가하는 유의한 개선이 있었으며, 이는 최신 Transformer 기반 point cloud 처리 기법이 파지 생성 문제에도 효과적임을 보여준다.\n\n\n\n2.4.4 Performance on Multiple Grippers\nGraspGen은 여러 종류의 그리퍼에 대해 단일 프레임워크로 학습될 수 있다는 유연성을 갖는다. 본문에서는 주로 Franka 병렬 그리퍼에 대한 결과를 다루었지만, 부록에 Robotiq-2F-140 병렬 그리퍼와 흡착 패드 gripper에 대한 실험도 포함되었다고 한다. 핵심적으로, GraspGen은 모든 그리퍼에 대해 가장 뛰어난 성능을 보였으며, 성능 격차는 그리퍼 종류에 따라 다르게 나타났다. 예를 들어, Franka 시뮬레이션 실험에서 GraspGen이 M2T2 대비 약 두 자릿수 퍼센트의 향상을 보였는데, Robotiq-2F-140의 경우 그 격차가 더 벌어졌다고 한다. 이는 M2T2가 접촉점 표현을 사용하는데, 해당 표현이 Robotiq 같은 adaptive 그리퍼에는 적합하지 않아 성능이 크게 저하되었기 때문으로 분석된다. 반면 GraspGen은 입력 point cloud 기반이므로 그리퍼 형상 변화에 상대적으로 강인하여, 두 평행 그리퍼 모두에서 안정적인 성능을 유지하였다. 또한 흡착 패드의 경우에도 GraspGen이 SE3-DiffusionFields 등 다른 생성 모델 대비 우수한 결과를 얻었다고 보고된다. 이는 GraspGen의 아키텍처가 그리퍼 임베디드 형태 (즉, 물체+그리퍼 조건) 학습에 무리가 없음을 시사하며, 나아가 향후 다지그리퍼나 로봇 핸드와 같은 복잡한 형태로 확장하는 데도 기반이 될 수 있음을 보여준다.\n\n\n2.4.5 Real Robot Evaluation\n마지막으로, 시뮬레이션에서 학습된 GraspGen의 Real World 성능을 검증하기 위한 실험이 수행되었다.\n\n\n\n하드웨어 구성은 UR10 로봇 팔에 Robotiq-2F-140 그리퍼를 장착하고, 상단에 Intel RealSense D435 RGB-D 카메라 한 대를 설치하여 테이블을 내려다보는 형태였다. 소프트웨어적으로는 Jetson 보드 상에서 cuRobo (샘플 기반 모션 플래너)를 사용하여 경로계획 및 역기구학 필터링을 수행하고, NVBlox를 통해 실시간 충돌 맵을 생성하였다. Object instance 분할에는 SAM2 (세그멘테이션 파운데이션 모델), depth 보완에는 FoundationStereo 모델 등을 활용하여, 가능한 현실에서의 인식 성능을 끌어올렸다.\n테스트 환경은 네 가지로 구성되었다:\n\n단일 물체 (isolated)\n테이블 위 다수 물체 (table clutter)\n바구니 안의 물체들 (basket)\n선반 위의 물체들 (shelf)\n\n점차 난도가 올라가는 시나리오로, 특히 바구니나 선반의 경우 격자 내부나 좁은 공간이라 파지 및 인출 동작이 어렵다. 비교 대상으로는 시뮬레이션에서 우수한 성능을 보인 M2T2와, 실제 데이터로 학습된 AnyGrasp를 선정하였다. 두 모델 모두 공개된 학습 가중치와 파라미터를 그대로 사용하되, 실험 환경에 맞게 몇 가지 입력 처리를 조정하였다. 예를 들어 M2T2는 원래 Scene 전체 point cloud을 입력으로 사용하도록 학습되었는데, 본 실험에서는 카메라 좌표계를 기준으로 point cloud을 90도 회전시키고 로봇 작업 공간에 해당하는 영역만 크롭하여 주는 방식으로, 훈련 시 분포와의 차이를 줄여주었다. AnyGrasp의 경우 학습 데이터가 고정된 카메라 고도에서 수집되었기 때문에, 우리 환경의 카메라 깊이에 맞춰 z축 방향 오프셋을 point cloud에 주어 보정하였다. 또한 AnyGrasp는 원래 다중 예측된 그립 중 Non-Maximum Suppression을 적용하여 중복을 제거하는 후처리가 있었지만, 이 연구의 설정에서는 NMS를 사용하지 않는 편이 성능이 나아 이를 생략하였다. (아마도 우리의 모션 플래너가 목표 그립 셋에 대해 자체적으로 충돌 제거 등을 수행하므로, 중복이 있어도 괜찮았던 것으로 추측된다.) 이러한 전처리 없이는 M2T2나 AnyGrasp 모두 결과 그립을 거의 내지 못해, 부득이 저자들이 언급한 이러한 조정들을 거쳤음을 밝히고 있다. 이는 실제 환경의 분포 차이에 대한 타 모델들의 취약성을 보여주는 것으로, GraspGen처럼 다양한 관측 분포에 대해 훈련되지 않았을 경우 현실 적용이 어렵다는 점을 시사한다.\n각 방법은 Scene 당 여러 파지 후보를 출력하며, 상위 100개 그립을 모션 플래너의 목표로 사용하였다. 플래너는 이 중 로봇 충돌이나 역기구학 불능인 그립을 걸러내고, 남은 그립들 중 충돌 없는 경로를 찾아 집어올리기를 시도한다. 최종 파지 성공률은 해당 시나리오에서 잡기에 성공한 비율로 측정되었다. 결과를 보면, GraspGen은 전반적으로 가장 높은 성공률을 달성하였다. 특히 비교 대상들이 특정 어려운 환경에서 성능이 급격히 떨어진 데 비해, GraspGen은 모든 환경에서 고른 성능을 보였다. 예를 들어 단일 물체 환경에서는 GraspGen이 90.5%의 성공률로 M2T2(81.0%)와 AnyGrasp(85.7%)보다 높았고, 테이블 위 복잡 적재의 경우에도 GraspGen 83.3%로 M2T2(75.0%)를 상회하였다. 가장 어려운 선반(shelf) 시나리오에서는 GraspGen 71.4%에 비해 M2T2는 14.3%에 불과했고, AnyGrasp도 42.9%로 성능이 낮았다. 전체 평균 성공률을 보면 GraspGen이 약 81.3%로, M2T2의 52.6%, AnyGrasp의 63.7%를 크게 앞섰다. 물론 GraspGen도 선반/바구니 환경에서 다른 경우보다 성능이 낮아졌는데, 이는 위에서 언급한 대로 로봇 팔의 가용 동작 범위 제한으로 인해 다수의 파지 후보들이 실행 불가능하게 필터링된 영향이 크다. 이러한 환경에서는 모델이 애초에 접근 가능한 그립을 많이 생성해야만 최종 성공률을 높일 수 있는데, GraspGen은 부분적으로나마 그 역량을 보인 반면, M2T2와 AnyGrasp는 훈련 데이터가 단순 탁상 환경에 국한되어 있었기 때문에 새로운 형태의 환경(basket, shelf)에 전혀 일반화하지 못한 것으로 분석된다. 특히 M2T2는 scene-level 모델로 학습된 한계상 작은 물체에 대한 파지 예측을 누락하는 문제도 있었다고 한다. 요컨대, GraspGen의 시뮬레이션→현실 일반화 능력과 환경 다양성에 대한 적응력이 실제 로봇 실험에서도 확인된 셈이다. 저자들은 추가로 여러 파지 예측 예시를 부록에 제시하였는데, GraspGen이 다양한 물체들에 대해 현실에서도 안정적인 파지 자세들을 산출함을 보여준다."
  },
  {
    "objectID": "posts/paper/2025-08-02-grasp-gen.html#conclusion-limitations",
    "href": "posts/paper/2025-08-02-grasp-gen.html#conclusion-limitations",
    "title": "📃GraspGen 리뷰",
    "section": "2.5 Conclusion & Limitations",
    "text": "2.5 Conclusion & Limitations\n본 논문은 GraspGen이라는 새로운 6-DoF 파지 생성 프레임워크를 제시하고, 그 기술적 우수성을 다각도로 입증하였다. GraspGen은 확산 기반의 생성기와 효율적인 Discriminator를 결합함으로써 물체 중심 파지 문제에서 정확도와 범용성을 크게 향상시켰다. 다양한 시뮬레이션 실험에서 기존 방법들을 능가하는 성능을 보였고, FetchBench와 같은 복잡한 벤치마크에서 최신 최고 성과를 달성하였으며, 나아가 단 한 번도 실제 데이터를 학습하지 않고도 실제 로봇에서 우수한 파지 성공률을 보임으로써 시뮬레이터-현실 간 격차를 상당 부분 좁혔다. 이러한 성과를 통해 GraspGen은 향후 여러 고차원 로봇 조작 과제의 기반 모듈로 활용될 수 있는 가능성을 보여주었다. 예를 들어, GraspGen을 응용하면 목적 지향 파지(특정 부위를 잡기)나 언어 지시 기반 조작 등의 문제에 보다 강인한 파지 생성기를 제공할 수 있고, 복잡한 조작 작업에서 발생하는 파지 실패율을 낮춰 지능형 매니퓰레이션 시스템의 신뢰성을 높이는 데 기여할 수 있을 것이다.\n그러나 동시에 본 연구는 몇 가지 한계점과 향후 과제를 남긴다.\n\n우선, GraspGen의 성능은 여전히 센서 데이터 품질에 크게 의존한다는 점이 지적된다. 실제 로봇 실험에서 보았듯이, 정확한 깊이 추정과 물체 분할이 뒷받침되지 못하면 파지 후보의 정확도도 떨어질 수 있다. 이는 GraspGen이 물체 중심 point cloud에 의존하기 때문으로, 향후에는 잡음에 강인한 입력 처리나 멀티센서 융합을 통한 보완이 필요할 것이다.\n두 번째 한계로, 특정 형상의 물체에 대한 일반화 부족이 관찰되었다고 한다. 저자들은 특히 직육면체 상자(cuboid) 형태의 물체에 대해 GraspGen이 실험에서 어려움을 겪었음을 보고하며, 이는 현재 데이터셋에 그러한 형태의 물체가 부족하기 때문으로 추정하였다. 향후 버전에서는 보다 박스형 물체 데이터를 늘려 이 문제를 완화할 계획이라고 언급하고 있다. 이처럼 데이터 분포의 편향에 따른 특정 경우 성능 저하는, 데이터 커버리지 확대 및 도메인 일반화 기법 등을 통해 개선할 수 있을 것이다.\n셋째로, GraspGen의 학습에는 막대한 계산 비용이 소요된다. 시뮬레이션 데이터 생성과 학습을 합쳐 약 3,000 GPU-시간이 요구되었는데, 이는 누구나 모방하기 힘든 높은 장벽으로 작용할 수 있다. 특히 다양한 embodiment를 다루려면 여러 데이터셋에 대해 별도 학습을 해야 할 수도 있어, 추후 모델 경량화나 전이 학습을 통한 효율 향상 연구도 고려되어야 한다.\n마지막으로, GraspGen은 파지 자체의 성능을 향상하는 데 초점을 두었지만, 앞서 FetchBench 분석에서 논의되었듯 파지 이후의 이동 경로 계획이나 환경 상호작용 등 통합 문제는 별도의 도전 과제로 남아 있다. 예를 들어, 복잡한 장애물 환경에서 로봇팔이 파지한 물체를 꺼내오는 문제 등은 단순히 파지 성공률만 높인다고 해결되지 않으므로, 향후에는 파지 생성과 후속 동작 계획을 공동으로 최적화하는 방향의 연구도 필요할 것이다.\n\n종합적으로, GraspGen은 로봇 파지 분야에 확산 모델의 강력함을 증명하고, 생성 모델과 Discriminator의 상호보완적 학습을 통해 성능 한계를 돌파한 의미 있는 연구로 평가된다. 다양한 형태의 로봇 손과 대상에 두루 적용가능한 범용 파지 생성의 가능성을 열었으며, 이는 향후 서비스 로봇, 제조 자동화, 의료 보조 등 정밀 조작이 요구되는 분야에서 활용될 여지를 보여준다.\n\n참고\n\n엔비디아, 차세대 로봇 손 기술 AI ‘그랩젠’ 공개"
  },
  {
    "objectID": "posts/paper/2025-06-11-dex-il-review.html",
    "href": "posts/paper/2025-06-11-dex-il-review.html",
    "title": "📃Dex Imitation Learning 리뷰",
    "section": "",
    "text": "Paper Link"
  },
  {
    "objectID": "posts/paper/2025-06-11-dex-il-review.html#behavioral-cloning-bc",
    "href": "posts/paper/2025-06-11-dex-il-review.html#behavioral-cloning-bc",
    "title": "📃Dex Imitation Learning 리뷰",
    "section": "1.1 Behavioral Cloning (BC)",
    "text": "1.1 Behavioral Cloning (BC)\nBC는 전문가 데모의 state-action 쌍으로부터 직접 학습하여 전문가 행동을 복제하는 지도 학습(supervised learning) 패러다임입니다. 보상 신호나 탐색(exploration) 없이 상태에서 행동으로의 직접 매핑을 특징으로 합니다. 목표 함수는 데모된 액션의 negative log-likelihood를 최소화하는 것입니다:\nL(\\pi) = -E_{(s,a)\\sim p_D}[\\log \\pi(a | s)]\n여기서 D = \\{\\tau_1, \\dots, \\tau_n\\}는 n개의 데모 집합이며, 각 데모 \\tau_i는 길이 N_i의 state-action 쌍 시퀀스 \\{(s_1, a_1), \\dots, (s_{N_i}, a_{N_i})\\}입니다. BC는 푸싱(pushing) 및 grasping과 같은 비교적 간단한 작업에서 효과적인 성능을 보였습니다. 그러나 훈련 중 보지 못한 상태에 직면할 때 전문가 행동에서 벗어나는 액션을 생성할 수 있는 distribution shift 및 sequential decision-making 과정에서 오류가 누적되는 compounding error 문제에 취약합니다. 이를 완화하기 위해 계층적 프레임워크 [29]를 사용하거나, 단계별 액션 대신 전체 액션 시퀀스를 예측하여 유효 결정 시간 범위(effective decision horizon)를 줄이는 접근 방식 [53]이 제안되었습니다. 인간 데모에 흔한 multi-modal 데이터를 모델링하기 위해 에너지 기반 모델링 [26], 가우시안 혼합 모델 [58], 생성 모델 [59] 등이 탐구되었으며, 최근 Diffusion models [32, 60, 61, 62]이 BC 방법의 강건성 및 일반화 향상에 큰 잠재력을 보여주고 있습니다. BC 기반 방법은 일반화 및 multi-modal 액션 분포 모델링에 어려움을 겪지만, Diffusion models는 직접 액션 시퀀스를 생성하거나 고수준 전략을 안내하는 방식으로 유연성을 향상시키고 있습니다."
  },
  {
    "objectID": "posts/paper/2025-06-11-dex-il-review.html#inverse-reinforcement-learning-irl",
    "href": "posts/paper/2025-06-11-dex-il-review.html#inverse-reinforcement-learning-irl",
    "title": "📃Dex Imitation Learning 리뷰",
    "section": "1.2 Inverse Reinforcement Learning (IRL)",
    "text": "1.2 Inverse Reinforcement Learning (IRL)\nIRL은 사전 정의된 보상 함수를 최대화하기 위해 정책을 학습하는 기존 RL 프레임워크를 역전시킵니다. 대신, 전문가 데모 집합 D를 가장 잘 설명하는 기저의 보상 함수 R(s, a)를 추론하는 것을 목표로 합니다. 데모는 최적 또는 거의 최적의 정책을 따르는 전문가에 의해 생성되었다고 가정합니다.\nIRL 문제는 일반적으로 유한 Markov Decision Process M = \\langle S, A, T, R, \\gamma \\rangle 내에서 공식화되며, 여기서 S와 A는 상태 및 액션 공간, T(s'|s, a)는 상태 전이 확률, R(s, a)는 보상 함수, \\gamma \\in [0, 1]는 할인율입니다. 보상 함수는 종종 특징 함수 \\phi(s, a)의 선형 조합 R(s_t, a_t) = w^\\top\\phi(s_t, a_t)으로 표현됩니다. 정책 \\pi 하에서의 기대 특징 카운트는 \\mu_\\phi(\\pi) = \\sum_{t=0}^\\infty \\gamma^t \\psi_\\pi(s_t)\\phi(s_t, a_t)로 정의됩니다. IRL은 보상 함수를 수동으로 정의하기 어려운 DM 시나리오에서 특히 유리합니다.\n최근 연구들은 reward normalization, task-specific feature masking [63], adaptive sampling [64], 사용자 피드백 통합 [65], 비정형 데모로부터 보상 함수 학습 [67], Proximal Policy Optimization [45]과의 통합 [68], 시각 기반 인간-로봇 협업 [69] 등을 통해 IRL 프레임워크를 발전시켰습니다. IRL은 전문가 데모로부터 기저 보상 함수를 추론함으로써 복잡한 행동을 일반화하고 다양한 환경에 적응할 수 있도록 하지만, 고차원 액션 공간이나 희소한 피드백 신호에서 정확한 보상 함수 추정 및 대량의 데모 데이터 요구와 같은 한계에 직면합니다."
  },
  {
    "objectID": "posts/paper/2025-06-11-dex-il-review.html#generative-adversarial-imitation-learning-gail",
    "href": "posts/paper/2025-06-11-dex-il-review.html#generative-adversarial-imitation-learning-gail",
    "title": "📃Dex Imitation Learning 리뷰",
    "section": "1.3 Generative Adversarial Imitation Learning (GAIL)",
    "text": "1.3 Generative Adversarial Imitation Learning (GAIL)\nGAIL은 GAN [102] 프레임워크를 IL 영역으로 확장합니다. 모방 프로세스를 생성자와 판별자 사이의 2인 적대적 게임으로 공식화합니다. 생성자는 전문가 데모와 유사한 행동을 생성하려는 정책 \\pi에 해당하며, 판별자 D(s, a)는 state-action 쌍 (s, a)가 전문가 데이터 M에서 왔는지 또는 \\pi에 의해 생성되었는지 평가합니다. GAIL은 전문가와 생성자의 state-action 분포 사이의 Jensen-Shannon divergence를 최소화합니다.\n판별자는 다음 목표를 최대화하도록 훈련됩니다:\n\\arg \\min_D -E_{d_M(s,a)}[\\log D(s, a)] - E_{d_\\pi(s,a)}[\\log(1 - D(s, a))]\n생성자의 정책 \\pi는 판별자에서 파생된 보상 r_t = -\\log(1 - D(s_t, a_t))을 사용하여 RL로 최적화됩니다. 이 적대적 훈련 과정을 통해 GAIL은 명시적으로 보상 함수를 복구하지 않고도 전문가 데모로부터 복잡한 행동을 효과적으로 학습합니다.\nGAIL은 DM에서 널리 채택되었지만, 데모 데이터의 품질 및 가용성, 그리고 훈련 불안정성(mode collapse, gradient vanishing) 문제에 크게 의존합니다. Hindsight Experience Replay [77], semi-supervised correction [76], Sim-to-real transfer [78] 등이 데이터 문제를 해결하려 시도했으며, Variational Autoencoders [79], Wasserstein GAN [80], self-organizing generative model [82] 등을 사용하여 훈련 안정성을 개선하고 Mode collapse를 완화하려는 노력이 있었습니다. GAIL은 적대적 훈련의 근본적인 한계를 상속받아 훈련 불안정성 및 고차원 액션 공간으로의 확장 어려움에 직면합니다."
  },
  {
    "objectID": "posts/paper/2025-06-11-dex-il-review.html#hierarchical-imitation-learning-hil",
    "href": "posts/paper/2025-06-11-dex-il-review.html#hierarchical-imitation-learning-hil",
    "title": "📃Dex Imitation Learning 리뷰",
    "section": "1.4 Hierarchical Imitation Learning (HIL)",
    "text": "1.4 Hierarchical Imitation Learning (HIL)\nHIL은 복잡한 작업을 계층적 구조로 분해하여 해결하도록 설계된 IL 프레임워크입니다. 일반적으로 2단계 계층 구조를 채택하며, 상위 수준 정책은 현재 상태 및 작업 요구 사항에 따라 하위 작업 또는 원시(primitives) 시퀀스를 생성하고, 하위 수준 정책은 하위 작업을 실행하여 전체 목표를 달성합니다. 이 계층적 분해는 의사 결정 및 제어를 분리하여 장기적인 복잡한 작업을 보다 효과적으로 처리할 수 있도록 합니다.\n상위 정책 \\pi_h는 미리 정의된 원시 집합 \\{p_1, \\dots, p_K\\}에서 원시 p_i를 선택합니다: \\pi_h(s_t) = p_i. 해당 하위 정책 \\pi_{p_i}는 선택된 원시를 실행할 액션을 생성합니다: a_t = \\pi_{p_i}(s_t). 전체 목표는 누적 손실 함수를 최소화하는 것입니다:\nL(\\pi) = \\sum_{t=1}^T E_{(s_t,a_t)\\sim\\pi}[\\ell(s_t, a_t)]\nHIL의 주요 장점은 작업을 계층적 구조로 분해하여 직접적인 액션 공간 탐색의 복잡성을 줄이는 것입니다.\nCompILE [88], HDR-IL [89], ARCH [90], XSkill [91], LOTUS [92] 등의 연구들이 작업 분해, 기술 일반화, 장기적인 작업 처리에 기여했습니다. 최근 연구들은 Play data [93, 94]를 활용하여 두 수준의 정책을 효율적으로 훈련하는 방법을 탐구했습니다. HIL은 작업 분해 및 기술 일반화에서 상당한 이점을 보여주지만, Cross-modal 기술 일반화에서의 적응성 및 동적 환경에서의 모델 강건성 및 연속성 확보에 어려움을 겪고 있습니다."
  },
  {
    "objectID": "posts/paper/2025-06-11-dex-il-review.html#continual-imitation-learning-cil",
    "href": "posts/paper/2025-06-11-dex-il-review.html#continual-imitation-learning-cil",
    "title": "📃Dex Imitation Learning 리뷰",
    "section": "1.5 Continual Imitation Learning (CIL)",
    "text": "1.5 Continual Imitation Learning (CIL)\nCIL은 지속 학습(continual learning)과 IL을 통합하여 에이전트가 동적으로 변화하는 환경에서 전문가 행동을 모방함으로써 기술을 지속적으로 습득하고 적응할 수 있도록 합니다. 에이전트는 초기 단계에서 전문가 데모로부터 기본 기술을 학습하고, 이후 단계에서 점진적으로 지식을 축적하고 새로운 작업이나 환경에 적응하며 이전에 습득한 기술을 잊어버릴 위험을 완화합니다.\nCIL에서 정책 \\pi는 이전에 접한 모든 작업에 대한 누적 모방 손실을 최소화하여 최적화됩니다:\nL(\\pi) = -\\sum_{i=1}^t \\lambda^{(i)} E_{(s^{(i)},a^{(i)})\\sim \\rho^{(i)}_{exp}}[\\log \\pi(a^{(i)} | s^{(i)})]\n여기서 \\lambda^{(i)}는 t개의 각 작업에 할당된 가중치이고 \\rho^{(i)}_{exp}는 작업 i에 대한 전문가 state-action 쌍의 분포입니다.\n초기 연구 [95]는 이전에 습득한 기술을 손상시키지 않고 작업 간 전환을 가능하게 했지만, 상당한 저장 및 계산 리소스가 필요했습니다. Task-specific adapter 구조 [96], 비지도 기술 발견 [92], 행동 증류를 통한 통합 정책 학습 [97], Deep Generative Replay (DGR) [98], 자기 지도 학습 [99] 등 다양한 접근 방식이 제안되었습니다. CIL은 효과적인 멀티태스킹 학습, DGR 기술 적용, 자기 지도 기술 추상화에 중점을 두지만, 생성된 데이터의 품질 및 일관성, 리소스 소비, 현실 세계 응용을 위한 일반화 능력 부족과 같은 실질적인 배포 과제가 남아 있습니다."
  },
  {
    "objectID": "posts/paper/2025-08-13-3d-motion-field.html",
    "href": "posts/paper/2025-08-13-3d-motion-field.html",
    "title": "📃3D Motion Field 리뷰",
    "section": "",
    "text": "Paper Link\nProject Link"
  },
  {
    "objectID": "posts/paper/2025-08-13-3d-motion-field.html#기술적-기여",
    "href": "posts/paper/2025-08-13-3d-motion-field.html#기술적-기여",
    "title": "📃3D Motion Field 리뷰",
    "section": "2.1 기술적 기여",
    "text": "2.1 기술적 기여\n데이터 병목 문제와 기존 접근의 한계: 로봇 제어 정책 학습에는 대규모의 로봇 시연 데이터가 필요하지만, 실제 로봇으로 양질의 데이터를 모으는 일은 비용과 시간이 많이 드는 어려운 작업입니다. 이에 대한 대안으로 인간-객체 상호작용 비디오가 주목받고 있습니다. 하지만 사람이 등장하는 영상을 로봇 학습에 활용하려면, 영상으로부터 행동 표현(action representation)을 효과적으로 추출하는 것이 관건입니다. 기존에 제안된 다양한 표현 방식들이 있지만 각각 단점이 있었습니다. 예를 들어, 미래 비디오 프레임 자체를 행동으로 예측하는 접근은 영상이 불필요하게 복잡하고 흐릿해 학습을 어렵게 만들었고, 2D 픽셀 흐름(Optical Flow) 기반 표현은 3차원 정보가 손실되는 문제가 있었습니다. 3D 포인트클라우드 흐름은 센서 잡음에 민감해 부정확했고, SE(3) 객체 자세 변환 기반 방법은 미리 알고 있는 물체 3D 모델에 의존하며 강체(rigid) 물체로 한정되는 한계가 있었습니다. 요컨대, 로봇 학습을 위한 이상적인 행동 표현이 무엇인지 명확하지 않은 상황이었고, 이에 대한 해답을 이 논문이 제시합니다.\n객체 중심 3D 모션 필드(Object-centric 3D Motion Field)의 개념: 저자들은 객체의 3차원 움직임 자체에 초점을 맞춘 새로운 행동 표현을 제안합니다. 구체적으로, 연속된 두 영상 프레임 사이에서 화면에 보이는 각 지점(pixel)의 깊이 및 3D 이동값을 나타낸 4채널 밀집 이미지 형태의 정보를 정의합니다. 첫 번째 채널에는 현재 프레임 각 픽셀의 깊이값이, 나머지 세 채널에는 다음 프레임으로의 3D 이동 벡터(dx, dy, dz)가 저장됩니다. 이처럼 픽셀 단위로 해당 물체 표면의 이동을 표현함으로써, 카메라 내부 파라미터(intrinsics)와 함께하면 각 점의 3D 위치 변화까지 완전히 재구성할 수 있습니다. 중요한 것은 이 표현이 오직 관심 물체(object)에 국한되도록 객체 중심으로 설계되었다는 점입니다. 사람이 어떻게 손을 움직였는지 등의 정보는 배제하고, 작업을 수행함에 있어 물체가 어떻게 이동해야 하는지만 캡처합니다. 이러한 설계는 인간과 로봇의 팔/손 구조 차이를 극복해 플랫폼 간(embodiment 간) 지식 전이를 쉽게 하고, 배경이나 사람 모습이 달라도 동일한 작업으로 학습할 수 있게 합니다. 요약하면 객체 중심 3D 모션 필드는 로봇 제어에 필요한 최소한의 3D 정보만 유지하고, 이미지 기반 표현으로서 강력한 비전 모델들과 결합할 수 있으며, 구체적인 3D 모델 사전 지식 없이 RGB-D 영상만으로 추출 가능하다는 장점이 있습니다.\n핵심 아이디어 및 차별성: 객체 중심 3D 모션 필드를 활용함으로써 이 연구는 사람 시연 영상만으로 로봇을 가르치는 새로운 프레임워크를 구현했습니다. 기존 방법들과 달리, 이 접근법은 로봇 데이터를 전혀 사용하지 않는 제로샷 학습, 물체 3D 모델이나 자세 추정 불필요, 실행 중 피드백을 통한 폐루프 제어, 깊이 센서 노이즈에 강인함, 복잡한 배경이나 방해물에 대한 일반화 등의 측면에서 두드러진 이점을 보입니다. 이러한 특징들은 논문의 기법이 앞서 언급한 기존 연구들의 한계를 효과적으로 극복했음을 나타냅니다.\n 그림: 논문의 전체적인 학습 프레임워크 개요. (위) Phase I: 시뮬레이션에서 다양한 객체와 랜덤 3D 이동으로 구성된 데이터를 생성하여, 3D 모션 필드 추정기를 학습시킵니다. 이 모델은 노이즈가 섞인 깊이/흐름 입력으로부터 부드럽고 정확한 3D 모션 필드를 복원하도록 훈련됩니다. (아래) Phase II-A: 학습된 추정기를 활용해 실제 인간 영상에서 객체의 3D 모션 필드를 추출합니다. SAM과 CoTracker로 물체를 분할 및 추적한 후, 추적 결과(노이즈 있는 3D 픽셀 흐름)를 추정기에 넣어 정확한 3D 모션 필드 레이블을 얻습니다. Phase II-B: 이렇게 레이블링된 인간 시연 데이터로 3D 모션 필드 예측 정책 모델을 학습합니다. 이 정책은 새로운 관측 이미지(분할된 RGB-D 입력)를 받아 해당 작업에서 물체가 어떻게 움직여야 하는지를 3D 모션 필드 형태로 예측합니다. 마지막으로 예측한 모션 필드를 로봇 제어 명령(SE(3) 이동)으로 변환하여 실제 로봇을 움직입니다.\nPhase I: 노이즈 제거 3D 모션 필드 추정기 – 깊이 노이즈 문제 해결: 일반적인 RGB-D 카메라는 조명이나 물체 움직임에 따라 심각한 깊이 노이즈를 발생시키며, 이는 3D 모션 추정에 큰 오차를 유발합니다. 논문에서는 이 문제를 해결하기 위해 대규모 시뮬레이션 데이터로 학습한 ‘디노이징(denoising) 3D 모션 필드 추정기’를 제안했습니다. 연구진은 ShapeNet 등에서 다양한 모양의 가상 객체를 가져와 무작위 크기와 위치로 배치하고, 임의의 3D 이동(병진+회전)을 발생시켜 다량의 합성 RGB-D 영상 쌍을 생성했습니다. 각 사례마다 노이즈 없는 정확한 3D 모션 필드를 레이블로 계산하고, 입력으로는 인위적으로 노이즈를 추가한 깊이 지도와 2D 추적 결과(픽셀 흐름)를 사용했습니다. 이렇게 8백만 개가 넘는 학습 샘플을 생성하여 추정기 신경망을 훈련했는데, 주로 U-Net 구조를 활용하였고 출력으로 깊이 보정값과 3D 이동값을 각각 예측하는 듀얼 헤드(decoder) 방식을 취했습니다. 또한 카메라 좌표계에서의 픽셀 위치좌표 및 초점거리 역수로 이루어진 내부파라미터 맵을 추가 입력 채널로 주어, 신경망이 투영 기하의 미분관계까지 학습하도록 설계했습니다. 이렇게 함으로써 픽셀 움직임을 정확한 3D 이동으로 환산하는 데 필요한 정보(예: 깊이에 따른 z축 이동 스케일 변화)를 네트워크가 효율적으로 활용할 수 있었습니다. 학습 손실은 객체 영역에 한해 적용되는 Huber 손실로 안정적으로 구성했고, 깊이 구멍(missing data)이나 오차에 robust하도록 입력에 마스크 및 부분 랜덤 결손 등의 노이즈 증강도 수행했습니다. 그 결과 이 모델은 노이즈가 있는 실제 RGB-D 영상에서도 정확한 3D 모션 필드를 복원해낼 수 있게 되었고, 시뮬레이터로 학습했음에도 내용이 순수 기하학적이라 실세계로의 sim-to-real 격차가 매우 작음을 확인했습니다. 실제 실험에서 이 기법은 기존의 직접 계산 방법 대비 3D 운동 추정 오차를 50% 이상 줄여주는 성과를 보였습니다.\nPhase II: 객체 중심 3D 모션 필드 예측 정책 – 영상에서 정책으로: Phase I이 3D 모션 필드를 “보는” 능력을 확보했다면, Phase II에서는 로봇이 실제로 동작할 수 있도록 “예측하고 따라하는” 능력을 학습합니다. 우선 다양한 인간 시연 RGB-D 영상 데이터셋을 수집하여, 앞서 학습한 추정기로 각 영상의 과제 관련 물체 움직임을 모두 3D 모션 필드로 변환하고 레이블로 삼았습니다. 이 때 SAM (Segment Anything Model)을 활용해 관심 객체를 매 프레임 자동 분할하고, CoTracker로 객체 픽셀들을 프레임 간 추적하여 픽셀 흐름을 얻은 뒤, 추정기를 통해 정밀한 3D 운동 레이블을 획득합니다. 이렇게 준비된 데이터로 정책 신경망(모션 필드 예측기)을 학습하는데, 입력은 분할된 객체의 RGB-D 영상이고 출력은 해당 장면에서 목표로 하는 물체의 3D 모션 필드입니다. 네트워크 구조는 Phase I의 U-Net 기반을 대부분 공유하며, 출력이 이미지 형태이므로 확률적 생성 모델인 확산 모델(diffusion model)을 도입해 정밀도를 높였습니다. diffusion 정책의 학습에는 출력 모션 필드에 노이즈를 추가한 샘플들을 단계별 복원하도록 하여, 기존 회귀(gaussian) 접근보다 안정적이고 높은 해상도의 결과를 얻을 수 있었습니다. 학습 시 객체 마스크 영역 외의 부분은 무시하도록 하여 배경의 불필요한 노이즈 영향을 줄였으며, 인간 손 vs 로봇 그리퍼로 인한 물체 외형 차이에 대응하기 위해 랜덤 마스킹 증강을 실시하여 약간의 도메인 차이를 보완했습니다. 이렇게 준비된 정책은 사람이나 로봇의 형태 정보를 전혀 보지 않고 오직 물체와 작업 맥락만 활용하기 때문에, 사람 영상으로 학습했어도 로봇에 그대로 적용하는 데 격차가 매우 작습니다. 실제로 학습된 정책망은 카메라 영상만 보고도 인간 시연에서 추출한 것과 동일한 형식의 3D 모션 필드를 예측하며, 이를 최종 로봇 명령으로 변환해 즉각 실행에 옮길 수 있게 됩니다.\n로봇 제어로의 변환: 정책이 출력한 3D 모션 필드는 곧 물체의 3차원 목표 이동을 의미하므로, 이를 로봇의 잡고 있는 물체 이동 명령(SE(3) 변환)으로 변환합니다. 방법은 간단합니다: 현재 프레임에서 물체 마스크 내 각 픽셀의 현재 3D 좌표를 깊이값과 카메라 투영으로 계산하고, 모션 필드의 (dx,dy,dz)를 더해 목표 3D 좌표를 얻습니다. 이렇게 얻은 현재-목표 점군(point cloud) 쌍은 픽셀 단위로 1:1 対응되므로, 이들을 가장 잘 맞춰주는 최적의 회전·병진 변환(SE(3))을 폐쇄해형 해법(Kabsch 알고리즘)으로 계산합니다. 노이즈나 외란에 강건하도록 RANSAC으로 이상치도 제거한 뒤 최종 변환을 얻으면, 로봇 기준 좌표계로 변환하여 로봇 팔에 해당 이동을 실행시키면 됩니다. 이 변환 계산은 매우 빠르고 (300~1000Hz 수준) 로봇 제어 루프에 무리 없이 통합됩니다. 단, 이 논문에서는 로봇의 물체 잡기/놓기 동작은 별도의 모듈(사전에 확보된 그리퍼 제어 정책)에 맡기고 있으며, 학습된 정책은 물체를 잡은 이후의 움직임에 초점을 맞추고 있습니다. 이는 사람 손동작을 로봇에 그대로 모방하는 것이 어렵고 불필요하다는 저자들의 판단에 따른 것으로, 차후 어떤 부분을 잡아야 하는지 등의 접촉에 대한 암묵적 지식(affordance)은 추가 학습이 필요하지만 현재는 분리하여 고려한 것입니다.\n요약하면, 이 논문의 기술적 기여는 다음과 같습니다:\n\n인간 시연 영상으로부터 로봇 행동을 학습하기 위한 새로운 행동 표현으로 객체 중심 3D 모션 필드를 도입하고, 이를 추출하고 활용하는 학습 프레임워크를 제시했습니다.\n시뮬레이션 기반의 3D 모션 필드 추출 파이프라인과 현실 세계 영상에서의 예측 모델이라는 간단하면서도 효과적인 아키텍처를 고안하여, 노이즈가 많은 RGB-D 영상에서도 정교한 객체 움직임 추출이 가능함을 보였습니다. 이를 통해 인간 영상만으로 새로운 로봇 기술을 가르칠 수 있는 길을 열었습니다.\n제안한 구성 요소들을 실제 로봇 실험으로 검증한 결과, 모션 추정 오차 50% 이상 감소, 다양한 작업에서 평균 55%의 성공률(이전 기법들은 10% 미만)이라는 뛰어난 성능을 보였고, 순수 인간 손 시연으로 학습한 정책이 정밀 삽입 작업까지 구현하는 것을 최초로 시현해 보였습니다."
  },
  {
    "objectID": "posts/paper/2025-08-13-3d-motion-field.html#실험-결과-해석",
    "href": "posts/paper/2025-08-13-3d-motion-field.html#실험-결과-해석",
    "title": "📃3D Motion Field 리뷰",
    "section": "2.2 실험 결과 해석",
    "text": "2.2 실험 결과 해석\n실험 환경: 저자들은 Intel RealSense D435 RGB-D 카메라(정지된 상태)로 사람 시연 영상을 수집하고, UCT 교육용 로봇팔(XArm7)과 병렬 그리퍼로 실제 작업을 수행하는 실험을 구성했습니다. 카메라는 작업대로부터 약 40–50cm 거리에서 물체를 바라보도록 배치되었고, 로봇 손목 카메라는 사용하지 않았습니다. 학습시 입력 영상 해상도는 적절히 크롭 및 리사이즈하여 사용했습니다. 실험은 크게 두 부분으로 나뉩니다: (1) 3D 모션 필드 추정기(Phase I)의 정밀도 평가, (2) 학습된 정책(Phase II)의 로봇 작업 성공률 평가입니다.\n1. 3D 모션 필드 추정 성능 평가: 합성 데이터로 학습된 추정기 모델이 실제 환경에서도 정확히 작동하는지 확인하기 위해, 저자들은 로봇이 물체를 임의로 움직이는 장면을 직접 만들어 정확한 기준 답과 함께 테스트했습니다. 로봇 그리퍼로 다양한 모양의 물체들을 쥔 채 무작위로 흔들어서 (병진+회전 운동) 카메라 앞에서 움직이고, 이 때 로봇의 실제 그리퍼 포즈 변화로부터 물체의 실제 3D 변환량(ground-truth)을 계산했습니다. 그런 다음 제안된 추정기와 기존의 직접 계산 방식(깊이값+추적 결과로 바로 3D 계싼)을 비교하여, 복원된 물체 이동 변환의 오차를 측정했습니다. 결과는 Figure 8 (왼쪽) 그래프로 제시되는데, 물체의 평행이동 MSE와 회전 행렬 오차(norm) 지표 모두에서 제안 방법이 기존 대비 현저히 낮은 오차를 기록했습니다. 특히 깊이 입력에 인위적으로 잡음(Gaussian noise)을 추가해 공격적인 강건성 테스트를 수행한 경우, 기존 방법은 오차가 급격히 커진 반면 제안 방법은 훈련시 노이즈에 대비한 덕분에 오차 수준이 거의 증가하지 않았습니다. 또한 네트워크 설계 요소의 유효성도 확인했는데, 카메라 내부 파라미터 맵을 입력에 포함시킨 경우와 아닌 경우를 비교한 어블레이션(ablation)에서, 픽셀 좌표와 초점거리 정보가 없을 때 성능이 크게 떨어지는 것이 입증되었습니다. 이는 본문의 유도대로 화면 좌표와 카메라 모델 정보가 3D 운동 예측에 필수적임을 보여주며, 작은 시야각 변화(±10도)에서조차 초점거리 값의 차이가 예측 정확도에 영향을 줌을 확인했습니다.\n2. 인간 영상으로 학습한 정책의 로봇 작업 평가: 다음으로, 진짜 인간 시연만으로 학습된 정책이 실제 로봇 물체 조작 작업들을 성공적으로 수행할 수 있는지 시험했습니다. 저자들은 여러 가지 대표적인 물체 조작 과제를 선정하여 벤치마크로 사용했습니다. 평가한 실제 작업과 목표는 다음과 같습니다:\n\n1. 잡아서 돌려 놓기 (Pick, Rotate, and Place): 로봇이 물체를 집어들고 특정 각도로 회전시킨 후 목표 위치에 내려놓는 작업. 최종적으로 정해진 자세로 정확히 물체를 배치해야 성공으로 간주됩니다.\n2. 선 추적 (Line Tracking): 펜 모양의 손전등을 집어 들어 책상 위에 놓인 전선 모양 라인을 따라 이동시키는 작업. 손전등 불빛이 계속 선을 비추면서 정해진 경로를 끝까지 따라가면 성공이며, 중간에 크게 벗어나면 실패입니다.\n3. 도구 사용 I - 밀어서 옮기기 (Tool Use I: Pushing): 막대기나 밀대 같은 도구를 집어 한 물체를 목표 위치까지 밀어서 이동시키는 작업.\n4. 도구 사용 II - 렌치 조이기 (Tool Use II: Wrench): 렌치(스패너)로 너트를 한 바퀴 조이는 작업. 회전하여 끼우는 동작이라 1번 작업과 유사하지만, 물체 간 기구적으로 연결되어 있어 회전 각도가 제약되고 정밀한 맞춤이 필요한 더 어려운 과제입니다.\n5. 삽입 (Insertion): 물체를 집어서 회전시킨 후 작은 구멍이나 슬롯에 정확히 끼워넣는 작업. 허용 오차가 2.5mm에 불과하여, 몇 도의 각도나 몇 mm 위치 어긋남도 실패로 이어질 수 있는 아주 고난이도 정밀 작업입니다.\n\n각 작업마다 50~150개의 인간 시연 영상을 수집하여 학습에 사용했고, 과제 복잡도에 따라 데이터 수집에 약 2~15분 정도 소요되었다고 보고됩니다. (시연 영상은 일반인이 휴대폰 등으로 촬영한 짧은 3~5초 길이의 RGB-D 클립들로 구성되었습니다.) 학습 시에는 각 프레임의 물체 분할과 추적이 자동화되었지만, 평가 시에는 공정한 비교를 위해 모든 비교 방법들에 대해 물체 분할과 초기 파지(grasp)가 제대로 된 경우만 집계했습니다.\n평가 결과, Figure 8 (오른쪽)에 제시된 작업별 성공률에서 제안된 방법이 다른 모든 비교 방법 대비 월등히 높은 성공률을 기록했습니다. 특히 기존 최신 방법으로 알려진 General 3D Flow 기반 정책은 거의 모든 작업에서 한 자릿수 또는 0%에 가까운 성공률에 그친 반면 (정밀 작업에서는 전혀 성공하지 못함), 본 논문의 방법은 평균 55%의 성공률로 유의미하게 높은 성과를 보였습니다. 더욱이 로봇의 동작 궤적을 관찰한 결과, 기존 방법은 초반부터 물체를 엉뚱한 방향으로 움직여 경로를 이탈하는 반면, 제안 방법은 끝까지 인간 시연 경로를 잘 따라가면서 과제를 완수하는 모습을 보였습니다. 이는 앞서 추출한 모션 필드가 정확하고 매끄럽게 연결된 움직임 정보이기에 가능한 일로, 부드러운 추정이 곧 정확한 제어로 이어진 사례입니다. 배경이 학습 때와 달라져도, 입력으로 배경을 제외한 객체 중심 정보만 사용하기 때문에 성능에 문제가 없었음을 확인했습니다.\n가장 난이도가 높은 삽입 작업의 경우를 살펴보면, 인간 영상만으로 학습한 정책이 이처럼 섬세한 작업을 해낸 것은 처음이라 의미가 큽니다. 성공률은 약 35% 정도로 완벽하다고 할 수는 없지만, 비교군들은 단 한 번도 성공하지 못했음을 감안하면 상당히 고무적인 결과입니다. 저자들은 삽입 작업 실행 영상을 면밀히 분석한 결과, 로봇이 한 번에 매끄럽게 꽂지는 못하고 “땅땅(bang-bang) 제어”에 가까운 미세 조정을 반복하며 간신히 성공하는 양상을 보였다고 합니다. 이는 사람은 손끝의 미세 감각으로 한 번에 꽂는 반면 로봇은 카메라 관찰만으로 움직이다 보니 약간씩 위치를 보정해가는 것으로, 여전히 인간에 비해 부족한 부분이지만 최종적으로 목표를 달성한다는 점에서 학습 효과를 확인할 수 있습니다. 또한 이러한 추가 미세 조정 동작도 정책이 모션 필드 형태로 목표 이동을 지속 예측하며 마무리 방향을 제시했기에 가능한 것으로, 완전히 실패하는 기존 정책들과 비교됩니다.\n정책 구성 요소에 대한 추가 분석: Table 1은 정밀 작업들에 대한 정책 설계 선택의 영향을 요약한 어블레이션 실험 결과입니다. diffusion 모델 대신 전통적 Gaussian 회귀 출력을 사용하거나, diffusion 단계에서 객체 마스크 적용을 생략한 경우 삽입과 렌치 작업에서 0% 성공으로 전혀 성공하지 못했습니다. 또한 학습 시 객체 마스크 증강을 하지 않은 경우 성공률이 5%로 매우 저조했으나, 제안한 모든 기법을 포함한 완전한 모델은 35%까지 향상되었습니다. 이는 diffusion 기반의 고해상도 예측이 정밀 작업에 필수적이고, 비객체 영역 노이즈 제거와 로봇-인간 물체 외형 차이에 대한 증강이 성능에 큰 영향을 미친다는 것을 입증합니다.\n정리하면, 이 논문의 실험 결과는 제안한 객체 중심 3D 모션 필드 접근이 실제 현실의 다양한 물체 조작 작업에서 기존 기법들이 실패하던 것을 성공으로 바꿔놓을 만큼 효과적임을 보여줍니다. 인간 영상에서 추출한 정확한 3D 행동 표현을 토대로 학습된 로봇 정책은, 별도의 로봇 데이터 수집 없이도 놀라울 정도의 일반화 성능을 발휘했습니다. 비록 성공률이 100%는 아니지만, 데이터 준비의 용이성과 학습 효율을 고려하면 향후 발전 가능성을 강력히 시사합니다."
  },
  {
    "objectID": "posts/paper/2025-08-13-3d-motion-field.html#장점과-한계",
    "href": "posts/paper/2025-08-13-3d-motion-field.html#장점과-한계",
    "title": "📃3D Motion Field 리뷰",
    "section": "2.3 장점과 한계",
    "text": "2.3 장점과 한계\n장점 – 데이터 효율과 일반화의 새로운 지평: 이 논문의 가장 큰 성과는 로봇 학습의 데이터 문제를 인간 시연 영상으로 풀어낸 점입니다. 사람 손으로 시연한 짧은 영상 수십 개만으로도 로봇에게 새로운 기술을 가르칠 수 있다는 것은, 향후 로봇에게 필요한 방대한 데이터를 보다 쉽게 획득할 수 있음을 의미합니다. 또한 객체 중심 3D 모션 필드라는 표현은 로봇 제어에 필요한 핵심 정보만 담고 있어 효율적일 뿐 아니라, 사람과 로봇의 형태 차이를 초월해 동작을 전달한다는 발상의 전환을 보여줍니다. 이 덕분에 하나의 정책으로 다양한 로봇 플랫폼에 적용하거나 새로운 물체나 배경 환경에도 비교적 강인한 일반화를 기대할 수 있습니다. 실제 실험에서도 배경이 바뀌거나 distractor가 있어도 문제없었고, 학습에 사용하지 않은 새로운 물체에 대해서도 제대로 동작하는 등 범용성을 확인했습니다. 또한 시뮬레이션을 활용한 노이즈 제거 추정기는 비교적 단순한 아이디어이지만, 기존에 센서 오차로 어려움을 겪던 3D 추적 문제에 큰 돌파구를 마련했습니다. 이 모델은 입력 깊이에 부분 결함이나 오류가 있어도 2D 추적 정보로 보완하여 3D 데이터를 복원해주므로, 값비싼 고성능 센서를 쓰지 않고도 저렴한 RGB-D 카메라로 정밀 제어를 할 수 있게 합니다. 더 나아가 정책망에 확산 모델을 도입한 것은 로봇 제어 분야에 최신 생성 모델 기법을 적용한 흥미로운 시도로서, 이를 통해 고해상도 연속 행동 출력이 가능함을 보였습니다. 전반적으로 이 연구는 사람 영상에서 로봇이 학습한다는 흥미로운 방향성에 대해, 구체적인 기술적 해법과 가능성을 증명해 보였다는 의의가 있습니다. 특히 삽입 작업 성공과 같은 성과는 이 방법의 실용적 잠재력을 보여주는 사례로, 향후 산업 현장이나 가정에서 사람 시연 한두 번으로 로봇에게 새로운 작업을 가르치는 모습도 상상해볼 수 있게 합니다.\n한계 – 깊이 데이터 의존 및 다중 객체 등 현실적 과제: 그럼에도 불구하고, 현재 단계의 기법에는 분명한 한계와 향후 과제가 존재합니다. 먼저, RGB-D 영상 데이터에 대한 의존성입니다. 논문에서는 정확한 3D 모션 추정을 위해 깊이 채널이 필수적이라고 강조합니다. 이는 기술적으로 타당한 주장이나, 현실적으로 인터넷상의 방대한 기존 RGB 영상 데이터를 그대로 활용하지 못한다는 제한이 됩니다. 다행히 휴대폰 등 깊이 센서가 달린 기기가 늘어나고 있어 새로운 RGB-D 데이터 축적도 기대해볼 수 있지만, 어디까지나 추가로 데이터를 모아야 한다는 점은 고려해야 합니다. 둘째, 물체가 카메라에 완전히 가려지는 경우에는 현재 방법이 통하지 않습니다. 학습 시 완전 가려진 구간의 영상은 제외했고, 실행 중에도 물체가 보이는 전제하에 제어가 이뤄집니다. 따라서 사람 시연이나 로봇 수행 중에 물체가 오랫동안 안 보이게 되는 작업 (예: 통 안에 넣었다 꺼내는 등)에는 대응하기 어렵습니다. 이는 추후 추적이 끊기더라도 재식별하거나 기억하는 기술로 보완해야 할 것입니다. 셋째, 다중 객체 또는 복잡한 상호작용의 문제입니다. 본 논문은 한 번에 하나의 주된 작업 대상에 초점을 맞춥니다. 그런데 실제 시나리오에서는 로봇이 여러 개의 물체를 동시에 다루거나, 여러 단계의 도구/대상 상호작용을 거치는 일이 많습니다. 현재 방법을 그대로 확장하면 각 객체마다 모션 필드를 따로 예측하고 순차 제어해야 할텐데, 이 경우 상호 의존성까지 고려하려면 더 발전된 표현과 정책이 필요할 것입니다 (예를 들어 끈으로 연결된 두 물체를 동시에 추적하는 등). 넷째, 로봇 잡기 동작의 통합입니다. 앞서 가정한 대로 본 연구에서는 물체 잡는 정책은 별도로 가정했습니다. 하지만 사람 영상에는 물체를 어떻게 쥐는지까지 모두 나타나므로, 이를 활용하면 로봇이 적절한 파지 방법이나 도구 사용법까지 배울 가능성이 있습니다. 향후에는 어포던스 학습이나 임의 형태 그리퍼에의 일반화 등으로 이 부분까지 통합한다면 더욱 자연스러운 학습 프레임워크가 될 것입니다. 다섯째, 로봇 구성과 환경에 따른 제약입니다. 현재 방법은 UC Berkeley의 XArm7 로봇으로 검증되었는데, 다른 로봇이라도 물체를 task-space에서 움직이는 기능만 있으면 적용 가능할 것으로 보입니다. 하지만 로봇마다 관절 구성이나 작업공간이 다르므로, 추후 다양한 로봇에 이식하며 미세 조정이 필요할 수 있습니다. 또한 환경적으로는 카메라가 고정되어 있고 비교적 단순한 탁상 환경이었는데, 카메라가 이동하거나 작업 공간이 복잡한 경우 추가 모듈(SLAM 등)로 카메라 움직임을 보정하는 확장이 필요합니다. 마지막으로, 성능 향상의 여지입니다. 성공률 55%는 기존 대비 크게 향상된 것이지만, 실제 서비스 로봇에 적용하기엔 아직 실패 확률이 높습니다. 특히 정밀 작업은 3번 중 1번만 성공하는 수준이므로, 더욱 많은 데이터 축적이나 모델 개선으로 성공률을 높여야 합니다. 예컨대, 인간 시연을 단순히 따라하는 것을 넘어 실시간 피드백으로 오차를 보정하는 강화학습이나, 멀티스텝 계획을 접목하면 성공률과 안정성이 향상될 것으로 예상됩니다. 또한 비디오 이해 측면에서, 사람의 의도나 행동 단위를 파악하여 더 고차원의 개념으로도 학습할 수 있을 것입니다.\n향후 전망: 저자들은 이러한 한계를 인지하고 향후 연구 방향으로 여러 물체가 있는 복잡한 상호작용 상황, 다른 형태의 로봇 적용, 환경적 제약 처리, 보지 못한 새로운 객체에 대한 일반화 등을 제시하고 있습니다. 결국 이 연구는 “사람처럼 보고 배우는 로봇”의 가능성을 한 단계 보여준 것으로서, 앞으로 남은 도전들은 이 개념을 더욱 일반적이고 강력하게 만드는 과정이라 할 수 있습니다. 로봇공학의 관점에서, 인간 영상으로부터 학習한다는 것은 로봇이 세상의 방대한 비디오 지식을 활용할 길을 열어줍니다. 이번 논문의 객체 중심 3D 모션 필드는 그 중요한 퍼즐 조각 중 하나로, 향후 다른 연구들과 합쳐져 우리가 흔히 보는 유튜브 영상만 보고도 척척 배우는 미래 로봇의 모습을 현실에 가까이 데려올 것으로 기대됩니다."
  },
  {
    "objectID": "posts/paper/2025-07-26-pp-tac.html",
    "href": "posts/paper/2025-07-26-pp-tac.html",
    "title": "📃PP-Tac 리뷰",
    "section": "",
    "text": "Paper Link\nProject Link"
  },
  {
    "objectID": "posts/paper/2025-07-26-pp-tac.html#서론-introduction",
    "href": "posts/paper/2025-07-26-pp-tac.html#서론-introduction",
    "title": "📃PP-Tac 리뷰",
    "section": "1. 서론 (Introduction)",
    "text": "1. 서론 (Introduction)\nPP-Tac 개요: PP-Tac는 얇고 변형 가능한 물체(예: 종이, 천 등)를 집어올리는 문제를 해결하기 위해 제안된 로봇 시스템으로, 2025년 Robotics: Science and Systems (RSS)에 발표되었다. 이 시스템은 다수의 손가락을 갖춘 덱스터러스 로봇 손에 고해상도 전방위 촉각 센서(R-Tac)를 장착하여, 촉각 피드백을 통해 종이와 같은 납작한 객체를 효과적으로 집을 수 있게 한다. 기존의 시각 기반 시스템들이 얇은 물체의 두께나 변형을 인식하기 어려워 작업에 실패하는 반면, PP-Tac는 인간의 전략에서 영감을 받아 여러 손가락의 협응 동작과 촉각 센싱을 적극 활용한다. 사람은 종이를 집을 때 손가락으로 미끄러짐을 감지하고 힘을 조절하며, 손목과 손가락을 유기적으로 움직여 종이를 말아 올린다. PP-Tac는 이러한 인간의 슬라이딩 및 집기 동작을 모방하며, 촉각 정보로 물체의 미세한 움직임까지 감지하여 제어에 반영하는 것을 목표로 한다.\n주요 구성요소: PP-Tac 시스템은 크게 두 축으로 구성된다. 첫째, 알고리즘 구조 측면에서 촉각 피드백 제어 시스템과 학습 기반 제어 알고리즘을 포함한다. 구체적으로, 촉각 센서로부터 얻은 정보를 이용한 실시간 미끄럼(slip) 감지 및 마찰력 제어, 그리고 확산 기반 정책(diffusion-based policy)으로 구현된 학습형 제어기가 핵심이다. 이 알고리즘은 인간의 종이 집기 동작 데이터를 모방하여 궤적(trajectory) 생성을 수행하고, 이를 통해 학습된 정책이 로봇 손-팔 시스템을 제어한다. 둘째, 센서 및 하드웨어 설계 측면에서 새로운 원형 촉각 센서 R-Tac의 구조와 특징, 센서 보정 방법, 그리고 이 센서를 Allegro 로봇 핸드에 통합한 하드웨어 구성이 소개된다. R-Tac는 반구형의 시각 기반 촉각 센서로서, 빠른 응답과 간편한 보정을 위해 모노크롬 카메라 기반 설계를 채택한 것이 특징이다.\n의의: 논문 저자들은 PP-Tac가 다양한 재질, 두께, 강성을 가진 종이類 물체들을 성공적으로 집어올리며 87.5%의 성공률을 달성했음을 보고하였다. 이는 촉각 센싱과 학습 제어의 결합이 얇은 변형체 조작에 효과적임을 최초로 입증한 사례로 평가된다. 본 리뷰에서는 (1) 알고리즘 구조와 (2) 센서/하드웨어 설계의 두 축에서 PP-Tac의 기여를 심층 분석하고, 각 측면마다 기존 촉각 센서 시스템인 DIGIT 360과의 비교를 통해 구조적 특성, 해상도, 설치 및 제어상의 차이를 논의한다. 이후 섹션에서는 해당 논문의 핵심 내용을 학술적인 문체로 정리하며, 필요한 경우 원문에서 발췌한 인용을 포함한다."
  },
  {
    "objectID": "posts/paper/2025-07-26-pp-tac.html#알고리즘-구조-algorithmic-architecture",
    "href": "posts/paper/2025-07-26-pp-tac.html#알고리즘-구조-algorithmic-architecture",
    "title": "📃PP-Tac 리뷰",
    "section": "2. 알고리즘 구조 (Algorithmic Architecture)",
    "text": "2. 알고리즘 구조 (Algorithmic Architecture)\n\n2.1 촉각 피드백 제어와 미끄럼 감지 (Tactile Feedback Control and Slip Detection)\nPP-Tac의 제어 시스템은 실시간 촉각 피드백을 활용하여 손가락 힘을 조절함으로써, 종이를 집는 동안 발생할 수 있는 미끄럼(slip)을 감지하고 방지한다. 네 개의 손가락 끝에 장착된 R-Tac 촉각 센서는 접촉면의 압력 변화와 움직임을 감지하여, 물체가 손가락 사이에서 미끄러지려는 조짐을 포착한다. 구체적으로, 센서 표면이 물체와 접촉 중에 마찰력이 부족해질 때 나타나는 주름(wrinkle) 패턴을 영상으로 포착하여 미끄럼 여부를 판단한다. 이를 위해 저자들은 경량 CNN+MLP 기반의 슬립 감지 모듈을 설계하였다. 이 모듈은 최근 5 프레임의 촉각 이미지 시퀀스와 비접촉 시 기준 프레임을 입력으로 받아, 각 프레임에서 추출한 특징맵들을 통합해 미끄럼 발생 확률을 출력한다. 약 20분간의 촉각 데이터(슬립 사례 40%, 비슬립 60%)를 수집하여 이 네트워크를 학습시켰으며, 최종적으로 86%의 슬립 감지 정확도를 달성했다고 보고된다.\n미끄럼 감지 신뢰도가 충분히 높아지도록 임계값을 조정한 후(예: 0.75에서 최적 성능 확인), 이 정보를 온라인 마찰력 제어에 활용한다. PP-Tac는 매 시각 촉각 센서로부터 얻은 변형(depth) 값을 이용해 손가락이 가해야 할 목표 힘(손가락 눌림 정도)을 설정하고, 만약 미끄럼 징후가 감지되면 해당 손가락의 목표 변형 깊이를 소량 증가시켜(normal force를 높여) 즉각적인 마찰력 증대를 꾀한다. 이러한 피드백 제어 루프를 통해, 로봇 손가락은 실시간으로 접촉 상태를 모니터링하며 필요한 경우 미끄러짐을 방지하도록 잡는 힘을 조절한다. 이는 인간이 촉각을 통해 미끄럼을 느끼면 손가락에 힘을 더 주는 무의식적 반사 동작과 유사한 메커니즘으로, 얇은 종이를 들어올릴 때 재료와 손가락 사이의 정지 마찰을 최대화하여 종이의 변형(버클링)을 유도하고 견고한 집기를 가능케 한다.\n특히 PP-Tac에서는 손가락 힘 제어를 컴플라이언스 제어 형태로 구현하여, 손가락이 종이를 누르는 힘은 증가시키되 손목과 다른 손가락의 위치 제어와 충돌되지 않도록 유연하게 적용한다. 이러한 촉각 피드백 기반의 제어는 open-loop 시나리오나 단순 그리퍼와 대비될 때 현격한 성능 향상을 보여주었다고 한다. 실험에서 촉각 피드백을 사용하지 않고 사전에 정의된 궤적만 따른 경우(open-loop)에는 불확실한 지형에서 거의 작업이 불가능하였지만, PP-Tac의 폐루프 제어는 다양한 평면 및 경사/복잡 지형에서 높은 성공률을 보였다. 이는 실시간 촉각 정보의 통합이 얇은 물체 조작의 안정성에 필수적임을 뒷받침한다.\n\n\n2.2 궤적 데이터 생성 및 확산 기반 정책 학습 (Trajectory Synthesis and Diffusion-based Policy Learning)\nPP-Tac의 두번째 알고리즘 핵심은 학습 기반의 제어 정책으로, 특히 확산 모델(diffusion model)을 활용한 생성적 정책을 통해 로봇의 다관절 움직임을 제어한다. 단순한 모델기반 제어로는 다중 손가락과 손목이 얽힌 동역학 문제(시리얼-패럴렐 결합)나 손목 고정 시 손가락 자유도 부족 문제 등을 해소하기 어렵기 때문에, 저자들은 모델프리(model-free) 방식의 학습 기법을 채택하였다. 특히 강화학습(RL)을 직접 적용하는 대신, 인간 시연을 모방한 궤적 최적화(trajectory optimization)로 데이터를 생성하고 이를 학습하는 확산 정책 (PP-Tac policy)을 개발하였다.\n① 궤적 데이터 생성: 우선 시뮬레이션 환경에서 궤적 최적화 기법을 통해 종이 집기 동작의 전문가 데이터셋을 구축한다. 사람의 동작을 흉내내기 위해, 인간이 손가락으로 종이를 밀어 올리는 슬라이딩+집기 동작을 원격조작(텔레오퍼레이션)으로 한 차례 녹화하고, 해당 궤적의 손가락 끝 경로를 추출하여 기본 형태로 사용하였다. 이 경로를 다양한 조건에 맞게 변형함으로써 데이터셋을 증강하는데, 예를 들어 무작위 지형(profile) 변화를 생성하고 손가락 말단 경로의 x,y 좌표를 지형에 투영하여 굴곡진 표면에서도 접촉을 유지하도록 한다. 또한 접촉력 조건의 다양화를 위해, 손가락이 누르는 변형 깊이 값(d)을 다르게 하는 궤적들을 생성한다. 이는 촉각 센서의 변형이 곧 접촉력을 반영한다는 점을 이용한 것으로, 손가락 관절과 지면 사이 거리를 조정하여 센서가 더 눌리거나 덜 눌리게끔 궤적을 만들어 경우에 따라 강하게 누르는 동작과 약하게 누르는 동작을 포함시킨다. 이러한 방식은 연성체(종이)의 정확한 물리모델을 사용하지 않고도 강체 시뮬레이션 내에서 다양한 힘 조건을 흉내낼 수 있는 장점이 있다. 최종적으로 네 손가락 각각에 대해 다양한 접촉 조건과 지형 조건을 반영한 50만 개 이상의 grasp 궤적 샘플이 생성되었으며, 이들 각각은 시계열 프레임들의 시퀀스로 구성되었다고 한다.\n② 확산 기반 정책 학습: 생성된 궤적 데이터셋을 바탕으로, Denoising Diffusion Probabilistic Model (DDPM) 프레임워크를 활용한 확산 정책을 학습시킨다. 이 정책 모델은 트랜스포머 인코더(4층) 구조로 구현되었으며, 과거 H 스텝의 상태(로봇 센서 및 관절 상태)를 입력받아 미래 K 스텝의 로봇 동작 시퀀스를 예측하도록 훈련된다. 상태 벡터에는 로봇 손의 프로프리오셉션 (손가락 관절각 및 속도, 손목 자세(6D) 및 속도)와 촉각 센서로부터 얻은 실시간 변형 깊이가 모두 포함된다. 구체적으로, 4개 손가락의 촉각 센서 변형값들이 상태에 포함되어 있어, 정책이 현재 접촉력 수준을 인지한 채 다음 움직임을 예측하게 된다. 학습 시에는 확산모델의 표준 절차에 따라 시계열 데이터에 점진적 노이즈를 더하고 제거하는 과정을 통해 모델이 미래 궤적의 분포를 학습하도록 한다. 이 때 기존 연구들에서 제안된 방식과 달리, PP-Tac는 직접 상태 시퀀스 x_{0:T} 자체를 예측(output)하도록 설계하여(속도 대신 상태값 예측) 매 스텝별 지오메트리 손실(목표 위치 오차)를 명시적으로 줄이는 학습 목표를 채택하였다. 이 접근은 특히 로봇 모션 데이터에서는 상태 그 자체를 예측하는 편이 성능이 더 안정적이며, 추가로 target loss를 활용해 매 denoising 단계마다 궤적 정확도를 높일 수 있음을 보였다.\n③ 실시간 추론 및 제어: 학습된 확산 정책은 추론 시 과거 상태들을 입력받으면 다음 다가올 K 스텝의 로봇 손/팔 제어 명령을 생성한다. 구체적으로 손가락 관절 제어명령 \\tau_{\\text{hand}}와 팔(손목) 제어명령 (\\tau_{\\text{wrist-pos}}, \\tau_{\\text{wrist-rot}})를 예측하며, 이중 손가락 제어명령은 촉각 센서의 목표 변형 깊이(즉, 접촉력 목표)도 함께 고려하여 산출된다. 만약 직전 시점에 미끄럼 감지 신호가 발생했다면, 정책은 해당 손가락의 목표 변형을 증가시키도록 보정된 상태로 다음 움직임을 결정한다. 이를 통해 학습된 정책은 슬립 발생 시 자동으로 더 강한 힘으로 누르거나, 필요한 경우 궤적을 미세 조정하여 종이가 안정적으로 집힐 수 있도록 적응한다. 중요한 것은 이 모든 과정이 실시간으로 가능하도록 추론 속도 최적화가 이루어졌다는 점이다. 확산 모델의 단계 수를 1000→10단계로 줄이고, 노이즈 첨가 및 제거 전략을 조정함으로써 RTX 4090 GPU 상에서 매 시퀀스 11ms 이내에 행동결정이 완료되었다고 보고된다. 이는 100Hz 이상의 제어 주기로, 센서의 업데이트 속도(120Hz)와 유사한 수준이어서 충분히 실시간 제어에 활용될 수 있다.\n④ 도메인 랜덤화 & sim-to-real: 시뮬레이터로 생성한 데이터로 학습한 정책을 실제 로봇에 이식할 때의 차이를 줄이기 위해, 학습 중 도메인 랜덤화 기법들이 적용되었다. 예를 들어, 제어 명령에 가우시안 노이즈를 추가하여 실제 하드웨어 모터의 오차를 모방하고, 손가락이 움직일 지형을 가상의 경사로 변화시켜보거나, 일부 프레임에서는 손가락이 붙잡혀 움직이지 못하는 상황(과도한 압력으로 마찰 정지)도 모사하였다. 이러한 다양한 교란(disturbance)을 학습에 포함시킴으로써, 정책은 현실 세계의 잡음과 불확실성에 견고한 제어 행동을 보이도록訓練되었다.\n성과: 학습된 PP-Tac 정책은 실험적으로 다양한 평면 및 요철 지형 위에 놓인 여러 종류의 얇은 물체(종이, 비닐봉지, 천, 종이가방 등)에 대해 높은 성공률로 집기 동작을 수행했다. 특히 완전한 폐루프(PP-Tac 전체) 시스템은 87.5%의 평균 성공률을 기록한 반면, 촉각 피드백이 없는 개방형 제어나 일반 그리퍼 사용 등의 베이스라인은 현저히 낮은 성공률을 보였다. 이는 제안된 확산 기반 학습 제어기가 다양한 상황에 일반화 능력을 갖추고 있고, 촉각 피드백과 통합되어 얇은 변형체 조작에 유효함을 뒷받침한다.\n\n\n2.3 알고리즘 측면에서의 DIGIT 360과의 비교 (Comparison with DIGIT 360 in Algorithmic Context)\nDIGIT 360은 Meta AI와 GelSight가 개발한 최신 인공 촉각 손끝 센서로, PP-Tac의 R-Tac와 마찬가지로 반구형 손가락 형태를 가진다. 그러나 DIGIT 360은 단순한 카메라 기반 촉각센서가 아니라, 18가지 이상의 멀티모달 센싱 기능(예: 시각 촉각, 힘/전단 감지, 진동, 온도, 화학적 감지 등)을 통합하고 센서 내 임베디드 AI 프로세서를 장착한 매우 진보된 플랫폼이다. 이러한 하드웨어적 능력 차이는 알고리즘 구조에도 영향을 미칠 수 있다.\n우선, PP-Tac에서는 R-Tac 센서로부터 얻는 정보가 그레이스케일 영상 형태이므로, 미끄럼 감지 등의 고차원 신호를 추출하기 위해 별도의 학습 기반 신호처리 모듈(CNN)을 사용하였다. 반면 DIGIT 360의 경우, 센서 자체가 전단력과 진동을 직접 측정할 수 있어 미끄럼 발생을 보다 물리적인 수준에서 감지할 수 있다. 예컨대 DIGIT 360은 1kHz 이상의 진동까지 포착 가능하여 물체가 미끄러질 때 발생하는 미세 진동이나 가속도를 바로 인식할 수 있고, 내부의 신경망 가속기(NPU)를 통해 센서 수준에서 즉각적인 미끄럼 판단과 반응(일종의 reflex arc)을 수행할 수 있다. 이는 PP-Tac처럼 센서 데이터를 PC로 보내 딥러닝 모델로 처리하는 방식보다 응답 지연을 크게 단축시킬 잠재력이 있다. 실제로 DIGIT 360은 인간보다 최대 30배 빠른 속도로 촉각 정보를 처리 가능하다고 소개되고 있어, 이를 활용하면 로봇 제어 시스템에서 더 빠른 피드백 루프를 구현할 수 있을 것으로 기대된다.\n또한 DIGIT 360은 정밀한 힘 측정 (정확도 ~1 mN)과 다축(force/torque) 정보를 제공하므로, PP-Tac에서와 같은 학습 기반 정책 없이도 보다 직접적인 힘 제어 알고리즘을 설계할 수 있는 가능성이 있다. 예를 들어, 미끄럼 감지를 위해 PP-Tac은 촉각 영상을 학습시켜 분류하였으나, DIGIT 360이라면 내장된 힘/전단 센서로부터 임계값 비교만으로 실시간 미끄럼 검출이 가능할 수 있다. 그럼에도 불구하고, PP-Tac 연구가 가지는 의의는 이러한 복잡한 하드웨어 없이도 상대적으로 저비용의 단일모달 센서와 고차원 학습 알고리즘의 조합으로 높은 성능을 달성했다는 점이다. DIGIT 360과 같은 센서가 향후 보급되면, PP-Tac의 확산 정책 역시 더욱 풍부한 감각 정보를 활용해 성능을 높이거나, 혹은 센서 내 처리로 단순화된 제어(예: 센서에서 전처리된 피드백 신호만으로 제어)로 대체되는 등 여러 연구 방향의 확장이 가능할 것으로 보인다. 요약하면, 현재 PP-Tac의 알고리즘은 R-Tac 센서의 특성에 맞춰 설계되었지만, 차세대 촉각센서인 DIGIT 360의 등장으로 센서-알고리즘 공동설계의 중요성이 더욱 커졌다고 할 수 있다."
  },
  {
    "objectID": "posts/paper/2025-07-26-pp-tac.html#센서-및-하드웨어-설계-sensor-and-hardware-design",
    "href": "posts/paper/2025-07-26-pp-tac.html#센서-및-하드웨어-설계-sensor-and-hardware-design",
    "title": "📃PP-Tac 리뷰",
    "section": "3. 센서 및 하드웨어 설계 (Sensor and Hardware Design)",
    "text": "3. 센서 및 하드웨어 설계 (Sensor and Hardware Design)\n\n3.1 원형 촉각 센서 R-Tac의 구조와 특징 (Design and Characteristics of R-Tac)\nPP-Tac 시스템의 핵심 하드웨어인 R-Tac 센서는 원형(둥근) 형태의 시각 기반 촉각 센서(Vision-Based Tactile Sensor, VBTS)이다. 설계의 출발점은 기존 평면형 촉각센서로는 얇은 물체의 다방향 접촉을 포착하기 어려우므로, 손가락 끝 모양에 맞는 반구형 센서를 만들자는 것이다. 저자들은 효과적인 조작을 위한 다섯 가지 설계 원칙을 제시했는데, (1) 전방위 감지: 반구형 돔 구조로 360도 방향의 접촉을 모두 느낄 수 있을 것, (2) 고해상도: 미세한 변형까지 포착해 3차원 깊이 재구성과 미끄럼 감지에 충분한 해상도를 갖출 것, (3) 제조 용이성과 저비용: 센서 구성품은 기성품이거나 간단히 제작 가능해야 하며 개당 약 $60 수준의 낮은 부품비용을 가질 것, (4) 보정(calibration)의 효율성: 조명 제어를 단순화하여 다채널(RGB) 센서들이 요구하는 복잡한 교정 작업 없이도 신속히 보정 가능할 것, (5) 고속 데이터 전송: 센서 출력 데이터량이 가벼워 여러 개 센서의 정보를 실시간 송신해도 병목이 없을 것 등이다. 이러한 원칙을 만족하기 위해 R-Tac는 단일 채널(모노크롬) 비전 촉각 방식을 채택하였다.\n구조: R-Tac는 손가락 끝에 장착되는 반구형 엘라스토머 돔과 그 내부에 삽입된 광원 및 카메라 모듈로 이루어진다. *접촉 모듈(Contact module)*이라 불리는 센서의 외피는, 사용자가 손가락으로 누르는 변형 가능 표면층(perception layer)과 내부 지지를 위한 투명한 골격(skeleton), 그리고 주변에 배치된 원형 LED 조명 링 및 광 확산판(diffuser)으로 구성된다. 센서 표면층은 부드러운 반투명 실리콘(Ecoflex, Shore 00-10 경도)으로 두께 약 2 mm로 제작되어, 접촉 시 적절한 변형을 일으키면서도 내부의 빛 반사 특성 변화를 통해 변형 깊이를 영상 신호(밝기 변화)로 전환해준다. 표면 아래의 골격은 더 단단한 PDMS 실리콘(Shore 50A)으로 만들어져 센서의 기본 형상을 유지하고 과도한 변형을 방지한다. 최상층에는 얇은 불투명 코팅층(Smooth-On사의 검은색 Psycho Paint 도료)을 에어브러시로 도포하여, 외부 광원의 간섭을 막고 내부 조명에 의해서만 밝기 패턴이 나타나도록 했다. 이러한 모노크롬 촉각 감지 원리는 “어두워지는 정도로 변형 깊이를 측정”하는 아이디어로, 3채널 RGB 조명 방식에 비해 구현이 단순하고 신뢰성 높다고 알려져 있다. 실제로 R-Tac는 내부 LED가 하얀 빛을 균일히 비추는 가운데, 손가락이 누르면 해당 부위가 어두워지는 패턴으로 변형을 인식하며, 이는 기존 연구 Dtact의 “darkness→depth” 원리에 영감을 받아 적용되었다고 한다.\n카메라 및 응답속도: 반구 내부에는 전용 초소형 카메라 모듈이 장착되어 변형에 따른 밝기 이미지를 실시간 촬영한다. 사용된 센서는 OmniVision OV9281 흑백 CMOS 카메라로, 글로벌 셔터를 지원하여 빠르게 움직이는 접촉도 블러 없이 포착 가능하다. 이 카메라는 최대 120 FPS로 동작하며 해상도는 1280×800 픽셀 (약 1메가픽셀)이다. R-Tac는 모노크롬 영상이기 때문에 프레임 당 데이터량이 컬러 대비 1/3 수준으로 적으며, USB를 통한 전송 시 약 100ms 정도의 낮은 지연(latency)을 보인다. 이는 4개의 센서를 동시에 운영할 때도 큰 무리 없는 수준으로, 저자들은 모노크롬 방식 덕분에 대역폭 한계를 최소화하여 다지(多指) 시스템 구현이 수월했다고 언급한다.\n보정 (Calibration): R-Tac 센서는 구조를 단순화한 덕분에 보정 과정도 효율적이다. 일반적인 GelSight 계열 RGB 촉각센서는 곡면의 경우 삼색 조명으로 인한 불균일 조도로 보정이 매우 번거롭고 CNC 머신 등을 이용한 수천 장의 데이터 수집이 필요했지만, R-Tac는 단 30장의 이미지 촬영만으로 깊이 재구성 모델을 구축할 수 있었다고 보고된다. 방법론적으로는, 먼저 3D 프린팅한 압입 장치로 센서를 누르면서 29장의 이미지를 촬영하여 카메라의 내·외부 파라미터 및 곡면 투영 모델을 계산한다. 이후 알려진 크기의 구형 볼을 센서에 한번 눌러 얻은 영상 한 장으로 픽셀 강도→깊이 변환 함수를 보정한다. 이렇게 2단계로 얻은 보정함수를 통해 임의의 촉각 이미지에 대해 그레이스케일 값 분포를 높이 맵(depth map)으로 실시간 변환할 수 있게 된다. 내부 엘라스토머와 조명 모듈의 품질을 균일하게 유지한 덕분에, 픽셀 세기의 표준편차가 6 이하로 매우 균질한 반응을 얻었고, 이는 보정 정확도를 높여준다. 논문에서는 보정된 R-Tac로 얻은 깊이 재구성 결과가 실제 압입 깊이와 평균 0.1 mm 오차 이내로 일치함을 확인하였다.\n접촉력 추정: R-Tac는 본질적으로 영상 기반 센서이지만, 접촉력(압력) 추정도 가능하다. 저자들은 탄성체 접촉 이론에 따라 센서 변형 깊이가 곧 누르는 힘에 비례한다고 가정하였다. 실제 정밀한 힘 값을 역산하지는 않지만, 예컨대 변형값 d가 0이면 접촉력이 0, d가 커질수록 더 큰 힘으로 누르는 것으로 간주할 수 있다. PP-Tac 알고리즘은 이 상대적인 힘 정보를 이용하여 앞서 설명한 바와 같이 손가락 간 접촉력 균형을 맞추고 미끄럼을 방지한다. 한편 Digit 360과 달리 R-Tac에는 별도의 힘센서가 없으므로, 정밀한 뉴턴 단위의 힘 추정보다는 변형 정도를 통한 간접적인 힘 제어 피드백에 중점을 두고 있다.\n\n\n3.2 Allegro 로봇 손과의 통합 (Integration with Allegro Hand)\nR-Tac 센서는 Wonik Robotics사의 Allegro Hand에 커스터마이징되어 통합되었다. Allegro Hand는 사람 손과 유사한 4개의 손가락(엄지 포함)과 총 16개의 자유도를 가진 덱스터러스 로봇 핸드로, 각 손가락의 세 마디 관절(DIP, PIP, MCP)들과 엄지의 복합 관절(CMC 등)이 모터 구동된다. 연구진은 이 손가락들의 말단부를 개조하여 R-Tac 촉각센서를 장착하였다. Fig. 2의 하드웨어 설계도에서 확인할 수 있듯이, 센서 결합을 위해 손가락 끝 부분에 맞게 센서 하우징과 배선을 설계하였고, (b)의 exploded view에 각 부품이 상세히 나와 있다. 센서로부터 출력되는 영상 데이터는 각각 별도로 USB를 통해 PC로 전송되며, 모터 제어 신호는 Dynamixel XM 시리즈 서보모터 16개를 구동하기 위해 U2D2 허브를 통해 전달된다. 전체 손-팔 시스템은 Franka Emika 7-자유도 로봇 팔 끝에 Allegro Hand를 장착한 형태이며, 팔 쪽 제어는 Ethernet으로 연결된 PC에서 이루어진다. 이와 같이 고속 통신망과 병렬 처리를 활용하여, 4개의 촉각센서와 16개의 모터, 그리고 팔 제어까지 모두 실시간 동기를 맞춰 구동할 수 있었다. 하드웨어 통합의 난이도를 낮추기 위해 R-Tac 센서는 최대한 컴팩트하고 가벼운 구조로 만들어졌는데, 부품비 $60, 제조 3일 이내 가능이라는 지표가 보여주듯 소형 로봇 손에 부담 없이 여러 개 부착할 수 있는 점이 큰 장점이다. 실제 본 시스템은 4개 손가락 모두에 촉각센서를 장착한 드문 사례로서, 이를 통해 손가락 간 협응적인 촉각 탐지(예: 한 손가락이 미끄럼을 감지하면 전체 제어에 반영)를 시연하였다.\n\n\n3.3 DIGIT 360과의 비교 분석 (Comparison with DIGIT 360 Sensor System)\n세계적인 추세로 곡면형 촉각 센서에 대한 연구가 활발하며, Meta AI의 DIGIT 360은 그 중에서도 가장 첨단을 달리는 시스템이다. 이하에서는 R-Tac와 DIGIT 360을 구조, 해상도, 설치 유연성, 다지 활용, 실시간성, 재현성 측면에서 비교한다.\n\n구조적 특성: R-Tac는 반구형 센서 팁으로 Allegro 손가락 끝에 딱 들어맞도록 소형 설계되었다. 단일 카메라와 LED 조명, 투명/불투명 실리콘 층들로 구성된 구조적으로 단순한 디자인이며, 손가락당 하나의 센서로 국지적인 촉각을 제공한다. 반면 DIGIT 360은 손가락 전체 모양에 가까운 인공 지문/피부를 구현한 센서로, 내부에 다수의 센싱 소자가 집적된 복합 구조이다. 약 830만 개의 taxel(촉각 화소)이 존재하는 고해상도 촉각 피부, 그리고 힘, 전단, 온도, 진동, 심지어 냄새까지 검출하는 다양한 센서들이 하나의 손가락 팁에 통합돼 있다. 이는 R-Tac의 단일 모달(변형 시각화) 센서와 달리 멀티모달 센싱 아키텍처를 지닌다. 구조가 복잡한 만큼 DIGIT 360에는 처리용 회로와 NPU까지 내장되어 사실상 “작은 로봇 손가락”처럼 동작하는데, 이러한 설계는 사람 손가락의 다양한 수용기를 전자적으로 모사한 것이다. 요약하면 R-Tac는 필요한 요소만 담은 최소주의적 설계라면, DIGIT 360은 가능한 모든 촉각 능력을 통합한 총합적 설계라고 볼 수 있다.\n데이터 해상도: 두 센서 모두 고해상도를 지향하지만, 접근 방식이 다르다. R-Tac의 해상도는 주로 카메라 픽셀 해상도로 결정되며 1280×800 (약 1 MP 수준)이다. 이는 접촉 면적 대비 충분히 세밀하여 작은 주름이나 모서리도 감지 가능하다고 논문에서 언급되며, 실제 실험에서 수 mm 두께의 종이, 비닐까지 확실히 인식해낸다. 반면 DIGIT 360은 약 830만 개의 taxel을 갖추고 있다고 하며, 이는 촉각 표면의 공간 분해능이 7 μm에 달할 정도로 미세함을 의미한다. 다시 말해 수십 μm의 촉각 특징까지 구분 가능하여, 예를 들어 지문이나 매우 고운 질감도 해상할 수 있다. 또한 힘 해상도 면에서도 DIGIT 360은 1 mN 수준의 정밀도로 정상력 및 전단력을 측정할 수 있어, R-Tac처럼 간접 추정이 아닌 정량적인 힘 피드백을 바로 얻을 수 있다. 요약하면, 공간적 해상도와 힘 해상도 모두 DIGIT 360이 훨씬 우수하지만, 그만큼 데이터량도 방대하다. 한 프레임당 수 메가픽셀 데이터를 다뤄야 하고 다중 모달 신호까지 포함되므로, R-Tac의 한 프레임(약 1MB 미만의 grayscale 이미지)보다 훨씬 큰 정보를 생성한다. 이를 실시간 처리하기 위해 DIGIT 360은 센서 내 전용 하드웨어를 쓴 반면, R-Tac는 PC 기반 처리로도 감당할 수 있을 가벼운 데이터 구조를 택했다.\n설치 유연성: R-Tac는 설계 목표부터 멀티 핑거 스케일 업을 염두에 두고 있었다. 3D 프린터와 몰딩 기법으로 쉽게 복제 가능하고, 부품도 쉽게 조달하여 여러 개를 동시에 제작할 수 있다. 실제 본 논문에서도 4개의 센서를 제작해 한 손에 장착했고, 필요하다면 두 손, 혹은 그 이상의 손가락에도 확장 가능할 것이다. 센서 자체가 가벼워서(구체적인 무게는 미제시지만 작은 카메라와 실리콘으로 구성) 로봇 손의 동작 성능을 크게 해치지 않는 것도 장점이다. DIGIT 360은 인간 손가락 크기에 가깝게 만들어졌다고는 하나, 그 복잡도 때문에 현재로선 단가가 높고 제조가 까다로울 것으로 보인다. 다만 Meta AI는 DIGIT 360을 모듈형 플랫폼으로 공개하여 연구자들이 사용할 수 있도록 지원하고 있어, 향후 여러 손가락에 DIGIT 360을 달아 사용하는 사례도 충분히 가능할 것이다. 현실적인 설치 시 고려해야 할 것은 전원 및 통신인데, R-Tac는 각 센서마다 USB로 PC에 연결하면 되나 DIGIT 360은 여러 센서를 다룰 경우 버스 구조의 통합 보드(Digit Plexus 등)와 전용 SDK 등이 필요할 수 있다. 유연성 면에서 R-Tac가 단순 플러그앤플레이에 가까운 반면, DIGIT 360은 고기능 시스템에 맞는 체계적인 인프라를 요한다. 따라서 연구 개발 초기 단계에서는 R-Tac 같은 접근이 빠르게 멀티센서 실험을 하기 수월하며, 대규모 정교한 시스템 구축 단계에서는 DIGIT 360의 모듈 플랫폼이 힘을 발휘할 것으로 전망된다.\n다지(多指) 제어 적합성: 복수의 손가락에 촉각 센서를 장착하여 동시에 촉각제어를 하는 능력은 두 시스템 모두 지향하지만, 현재까지 구현 수준에는 차이가 있다. R-Tac는 앞서 언급했듯 4개 손가락 전부에 센서 부착을 실현하여, 손가락간 협조 제어를 성공적으로 선보였다. 이는 각 손가락 센서로부터 독립적으로 미끄럼을 감지하되, 최종 제어 정책에서는 이를 통합적으로 반영하는 구조다. 반면 DIGIT 360은 아직 한 손가락 단위의 성능 검증 위주로 발표되었으며, 동시에 여러 개를 한 손에 달고 협응 제어한 사례는 (2025년 시점까지는) 공개되지 않았다. 그러나 DIGIT 360의 설계 철학이 애초에 로봇 손 전체의 표준 촉각 플랫폼을 제공하려는 것이므로, 장기적으로는 한 손의 모든 손가락에 DIGIT 360 센서 팁을 붙여 완전 촉각 피드백 손을 구현할 가능성이 높다. 그 경우 각 손가락 센서가 개별적으로 풍부한 정보를 제공하므로, R-Tac처럼 중앙처리에서 영상으로 미끄럼을 추정하기보다 센서 자체의 분산처리를 통해 각 손가락이 자율적으로 미끄럼을 억제하고, 상위 레벨에서 손가락 사이 협조만 조율하는 형태의 분산 제어도 구상해볼 수 있다. 요컨대, 현 시점에선 R-Tac가 실증된 다지 통합 사례를 보여주었다는 의의가 있고, DIGIT 360은 그보다 훨씬 풍부한 기능으로 다지 활용을 지원할 플랫폼으로서 기대된다.\n실시간성: 실시간 성능은 앞서 알고리즘 부분에서도 일부 비교되었지만, 센서 관점에서 정리하면 다음과 같다. R-Tac는 120 Hz 프레임레이트, 약 100 ms 지연시간으로 동작하여 일반적인 로봇 제어 루프(&gt;10 Hz)에 충분히 대응한다. 그러나 매우 빠른 동작(예: 수 ms 내 변화)까지 포착하기엔 한계가 있다. 반면 DIGIT 360은 10 kHz까지의 진동도 감지할 수 있고, 센서 내 신경망 가속기로 로컬 피드백을 즉각 계산할 수 있어, 이론적으로는 수백 Hz ~ kHz 대의 제어 피드백도 가능하다. 이는 산업용 제어나 미세한 촉각탐색 등 초고속 응답이 필요한 응용에 DIGIT 360이 유리함을 시사한다. 예를 들어, 미끄럼 발생 전 수백 Hz의 미세 진동을 감지해 바로 그립을 조정하는 것이 가능해진다. 다만 그러한 고주파 정보는 표면 재질에 따라 노이즈도 많으므로 활용을 위해선 정교한 필터링과 해석이 필요할 것이다. 한편, 데이터 전송 면에서는 R-Tac의 모노크롬 설계 덕에 4개 합산 480 fps 데이터도 무리 없이 PC에서 처리할 수 있었던 반면, DIGIT 360은 방대한 데이터 처리를 센서 자체에서 해 주므로 호스트 PC에는 요약된 정보만 보내 효율을 높인다. 예컨대 미끄럼 여부, 접촉 지점의 힘 등 의미 있는 피처만 출력하는 식이다. 이러한 설계 차이는 로봇 시스템의 전체 구조에도 영향을 주는데, R-Tac는 비교적 중앙집중식 처리이고 DIGIT 360은 에지(edge) 컴퓨팅 분산처리라고 볼 수 있다.\n재현성 및 확장성: R-Tac는 연구팀이 내부에서 설계·제작한 프로토타입이지만, 논문에서 상세한 제조 방법(몰드 제작, 실리콘 경화, 페인팅 과정)을 공개하고 있고, 부품 역시 특수한 것이 아니어서 다른 연구자들이 비교적 쉽게 재현할 수 있다. 부품비 $60은 기존 상용 촉각센서들과 견줘 매우 저렴한 편으로, 예산이 한정된 연구실에서도 여러 개 제작하여 실험하기에 적합하다. 또한 보정 절차가 간단하여 새로운 센서를 만들 때마다 복잡한 교정 작업을 반복할 필요도 적다. 반면 DIGIT 360은 Meta에서 오픈소스로 플랫폼을 제공한다고는 하나, 그 구성품(고해상도 카메라, 광학계, 마이크로컨트롤러, IMU, 온도/화학 센서 등)이 다소 전문적이고 조립 공정도 복잡할 수 있다. 현재까지는 Meta와 협업한 일부 기관에서만 사용 예시가 있으며, 일반 연구자들이 직접 제작하기엔 진입장벽이 있다. 다만 GelSight사를 통해 상용화될 가능성도 있어, 향후 표준 제품으로 구매가 가능해지면 상황이 달라질 수 있다. 결국 재현성 측면에서는 R-Tac 같은 간단한 DIY 접근이 단기간에는 유리하나, 장기적으로 DIGIT 360처럼 표준화된 플랫폼이 나오면 여러 연구 간 데이터 호환과 비교평가가 쉬워지는 장점이 있을 것이다."
  },
  {
    "objectID": "posts/paper/2025-07-26-pp-tac.html#결론-conclusion",
    "href": "posts/paper/2025-07-26-pp-tac.html#결론-conclusion",
    "title": "📃PP-Tac 리뷰",
    "section": "4. 결론 (Conclusion)",
    "text": "4. 결론 (Conclusion)\nPP-Tac: Paper Picking using Tactile feedback 논문은 촉각 센싱과 학습 제어의 밀접한 결합을 통해 로봇에게 인간에 가까운 종이 집기 능력을 부여한 획기적인 연구이다. 알고리즘적으로는 확산 모델 기반 정책 학습을 활용하여 복잡한 손-팔 협조 동작을 효과적으로 생성하고, 실시간 촉각 피드백 (특히 미끄럼 감지)을 통합함으로써 얇은 변형체를 안정적으로 다룰 수 있음을 보여주었다. 하드웨어적으로는 원형 촉각센서 R-Tac를 개발하여 기존 촉각 센싱의 한계를 극복하고 다수 손가락에의 적용 가능성을 입증하였다. R-Tac의 단순하고 저렴한 설계는 향후 촉각 센서의 대량 배치와 복합 시스템 구축에 하나의 방향성을 제시하며, 이미 상용화된 Meta의 DIGIT 360 등의 최신 센서와 대비되는 접근법으로서 의미가 크다. DIGIT 360이 “슈퍼휴먼” 성능을 지향하는 고사양 센서라면, PP-Tac의 R-Tac는 *“실용적 문제 해결”*을 위해 필요한 최소한의 센싱과 알고리즘 지능을 결합한 사례로 볼 수 있다. 궁극적으로 이 두 흐름은 상호 보완적이며, 앞으로 고성능 촉각센서를 PP-Tac와 같은 학습 제어기와 결합하는 연구가 진행된다면 더욱 향상된 결과가 기대된다. 예를 들어, DIGIT 360의 풍부한 신호를 확산 정책에 통합하거나, 센서 내 AI를 이용해 저지연 피드백 제어를 구현하는 방향으로 발전할 수 있다.\n본 리뷰에서는 논문의 주요 기여를 학술 리뷰 형식으로 분석하여 3년차 로보틱스 연구자 수준의 독자가 이해할 수 있도록 정리하였다. PP-Tac 연구는 로봇 촉각 분야에 새로운 지평을 연 사례로 평가되며, 이는 인간 수준의 섬세한 조작능력을 로봇에 부여하기 위한 향후 연구들에 귀중한 밑거름이 될 것이다.\n참고 문헌: PP-Tac 논문 원문 및 DIGIT 360 논문/자료 등. (각주 인용 번호는 본문에 표시)"
  },
  {
    "objectID": "posts/paper/2025-08-07-maniskill3.html",
    "href": "posts/paper/2025-08-07-maniskill3.html",
    "title": "📃ManiSkill3 리뷰",
    "section": "",
    "text": "Paper Link\nProject Link\nGithub Link\n\n\n🚀 ManiSkill3는 기존 시뮬레이터의 제약(속도, 메모리, 이질성)을 해결한 GPU 병렬화 로봇 시뮬레이터로, 최대 30,000+ FPS와 낮은 GPU 메모리 사용량을 자랑합니다.\n💡 이 플랫폼은 모바일 조작, 드로잉, 휴머노이드 등 12가지의 다양한 작업 도메인과 20개 이상의 로봇을 지원하며, 사용자 친화적인 API 및 이질적 시뮬레이션 기능을 통해 일반화 가능한 로봇 학습을 가속화합니다.\n✅ ManiSkill3는 로봇 강화 학습 시간을 크게 단축하고, 시뮬레이션에서 실제 환경으로의 안정적인 zero-shot 정책 배포를 입증하여 Sim2Real 및 Real2Sim 연구에 강력한 기반을 제공합니다.\n\n\n\n\n\n\n1 Brief Review\nGPU 병렬 로봇 시뮬레이션 및 렌더링을 통한 일반화 가능한 체화형 AI 구현: ManiSkill3\nManiSkill3는 일반화 가능한 로봇 조작을 목표로 하는 가장 빠른 상태-시각 GPU 병렬 로봇 시뮬레이터로, 접촉이 많은 물리 시뮬레이션을 지원합니다. 이 프레임워크는 시뮬레이션+렌더링, 이종 시뮬레이션, 포인트 클라우드/복셀 시각 입력 등 다양한 측면에서 GPU 병렬화를 지원합니다. ManiSkill3의 GPU 시뮬레이션 및 렌더링은 다른 플랫폼보다 2~3배 적은 GPU 메모리를 사용하며, 최소한의 Python/PyTorch 오버헤드, GPU에서의 시뮬레이션, 그리고 SAPIEN 병렬 렌더링 시스템 덕분에 벤치마크 환경에서 최대 30,000+ FPS를 달성합니다. 이를 통해 수 시간 걸리던 훈련 시간이 몇 분으로 단축됩니다. 또한, 모바일 조작, 드로잉, 휴머노이드, 숙련된 조작 등 12가지 독특한 도메인에 걸쳐 가장 포괄적인 GPU 병렬화 환경/작업을 제공하며, 모션 플래닝, RL, 텔레오퍼레이션으로부터 수백만 프레임의 시연 데이터도 제공합니다. ManiSkill3는 인기 있는 RL 및 시연 기반 학습 알고리즘을 아우르는 포괄적인 기준선도 제공합니다.\n기존 로봇 학습 연구는 비전 및 언어 연구와 달리 로봇 조작을 위한 적절한 데이터셋이 부족했습니다. 실세계 모방 학습은 방대한 데이터가 필요하고, 강화 학습(RL)은 광범위한 실제 환경 설정이 요구됩니다. Isaac Lab과 Mujoco의 MJX와 같은 GPU 병렬 시뮬레이션은 로봇 이동과 같은 문제 해결에 큰 진전을 가져왔지만, 조작 태스크에서는 이종 시뮬레이션(각 병렬 환경이 다른 장면을 포함하는 것) 및 빠른 병렬 렌더링 기능이 부족하여 시각 입력을 사용하는 RL 알고리즘의 훈련 속도가 느린 한계가 있었습니다. ManiSkill3는 이러한 한계를 해결하고 Apache-2.0 라이선스 하에 오픈 소스로 공개됩니다.\nManiSkill3의 핵심 기여는 다음과 같습니다.\n\n최첨단 GPU 병렬 시뮬레이션 및 렌더링: ManiSkill3는 빠른 병렬 렌더링과 낮은 시스템 오버헤드를 통해 GPU를 효율적으로 사용하여, 시각 태스크를 다른 시뮬레이터보다 빠르게 해결합니다. 시뮬레이션+렌더링 FPS는 최대 30,000+에 달하며, 시각 데이터 수집을 대규모로 가속화합니다. 또한, GPU 메모리 사용량이 2~3배 낮아 장치 내 시각 RL 및 대규모 신경망 훈련을 가능하게 합니다. 벤치마크에서 128개 병렬 환경의 경우 ManiSkill3는 3.5GB의 GPU 메모리를 사용하는 반면, Isaac Lab은 14.1GB를 사용합니다. ManiSkill3는 SAPIEN의 래스터화 렌더러를 사용하는 반면 Isaac Lab은 레이 트레이싱을 사용합니다.\n가장 포괄적인 환경: 테이블탑, 모바일 조작, 룸 스케일 장면, 사족보행/휴머노이드 이동, 휴머노이드/양손 조작, 다중 에이전트 로봇, 드로잉/클리닝, 숙련된 조작, 시각-촉각 조작, 고전 제어, 디지털 트윈, 소프트 바디 조작 등 12가지 독특한 카테고리의 환경과 20개 이상의 로봇을 기본 제공합니다. ReplicaCAD 및 AI2-THOR 장면을 지원하며, PhysX 기반의 시뮬레이션을 사용합니다.\n이종(Heterogeneous) GPU 시뮬레이션: ManiSkill3는 각 병렬 환경에서 완전히 다른 객체 형상, 객체 수, 그리고 서로 다른 자유도(DoF)를 가진 관절(articulation)을 시뮬레이션하고 렌더링할 수 있는 유일한 시뮬레이션 프레임워크입니다. 이를 통해 PPO와 같은 알고리즘이 YCB 데이터셋의 모든 객체나 PartNetMobility 데이터셋의 캐비닛에 대해 동시에 훈련하여 더 일반화 가능한 학습을 가능하게 합니다.\n간소화된 통합 API: 관절(articulation), 링크, 조인트 및 액터에 대한 완전한 객체 지향 API를 제공하여 복잡한 텐서 인덱싱 없이 로봇 환경을 쉽게 구축하고 관리할 수 있습니다. 포즈 정보는 배치(batched) Pose 객체로 저장되어, 예를 들어 두 포즈 P_1, P_2에 대해 (P_1 P_2)^{-1} P_1^{-1}와 같은 연산을 메서드 체이닝 패턴으로 간결하게 수행할 수 있습니다. ManiSkill3.poses.Pose.inv_then_mul(p1, p2, p1_inv=True) URDF 및 MJCF 정의 형식을 기본적으로 지원하며, GPU 병렬화된 관절 위치 제어 및 역운동학(IK) 제어를 위한 사전 구축된 제어기 옵션을 제공합니다.\n확장 가능한 데이터셋 생성 파이프라인: 모션 플래닝, RL 정책, 텔레오퍼레이션 등 다양한 방법을 통해 시연 데이터를 수집하며, 특히 복잡한 태스크의 경우 RLPD [2] 및 RFCL [47]과 같은 온라인 모방 학습 알고리즘을 사용하여 소수의 시연으로부터 일반화된 정책을 학습시킨 후 대규모 데이터셋을 생성합니다. 트랙토리 리플레이 도구는 CPU/GPU 시뮬레이션에서 수집된 데이터를 다른 설정(관측, 보상, 병렬 환경 수, RNG 시딩)으로 리플레이할 수 있도록 지원합니다.\nVR 텔레오퍼레이션: OpenVR 클라이언트 프로토콜을 구현하여 주류 VR 장치를 지원하며, 작업자의 손목 및 손 포즈를 실시간으로 로봇 동작으로 변환합니다. 4K 해상도의 스테레오 비디오 스트림을 60Hz로 VR 장치에 전송하여 몰입형 시야각을 제공합니다.\n\n로봇 제어 모듈:\n\n팔 제어 모듈: 인간의 손목 포즈를 로봇 팔 관절 위치로 변환합니다. Pinocchio 라이브러리 기반의 Closed-loop Inverse Kinematics (CLIK) 알고리즘의 수정된 버전을 사용하여 관절 각도를 계산하며, 이종 엔드 이펙터(end-effector)에 대한 IK 문제를 동시에 처리합니다. 특정 관절의 움직임을 조절하기 위해 소프트 마스크(soft mask)를 사용하고, 부드러운 팔 동작을 위해 SE(3) 그룹 필터를 적용합니다.\n손 제어 모듈: 인간 손가락 포즈를 로봇 손 관절 위치로 변환합니다. 이는 다음 목적 함수를 최소화하는 최적화 문제로 정식화됩니다. \\min_{\\mathbf{q}_t} \\sum_{i=0}^N \\|\\alpha_i \\mathbf{v}_i^t - f_i(\\mathbf{q}_t)\\|^2 + \\beta\\|\\mathbf{q}_t - \\mathbf{q}_{t-1}\\| 여기서 \\mathbf{q}_t는 시간 t에서의 로봇 손 관절 위치, \\mathbf{v}_i^t는 인간 손의 i번째 키포인트 벡터, f_i(\\mathbf{q}_t)는 순방향 운동학을 사용한 로봇 손의 해당 키포인트 벡터입니다. \\alpha_i는 손 크기 차이를 보상하고, \\beta는 시간적 일관성을 위한 정규화 항의 가중치입니다. 이 최적화는 NLopt 솔버로 구현됩니다.\n\nSim-to-Real 인터페이스: 깊이 카메라로 캡처된 포인트 클라우드를 VR 헤드셋에 투영하고 EasyHec으로 카메라 포즈를 보정하여 시뮬레이션 환경과 실제 환경을 “디지털 트윈” 방식으로 정렬합니다.\n\n\n벤치마크 및 결과:\n\n강화 학습 (RL) 훈련 속도: ManiSkill3의 GPU 시뮬레이션은 PickCube 태스크에서 PPO를 사용할 때 ManiSkill2의 CPU 시뮬레이션 대비 상태 기반 관측에서 약 15배, RGB 기반 관측에서 약 8배의 훈련 속도 향상을 보여줍니다. 상태 기반의 경우 1분, RGB 기반의 경우 10분 만에 거의 100% 성공률에 도달합니다.\n시연 기반 학습 (LfD) / 모방 학습: 행동 복제(BC), Diffusion Policy (DP), Action Chunking Transformer (ACT)와 같은 오프라인 모방 학습 기준선과 RLPD, RFCL과 같은 온라인 모방 학습 기준선을 제공합니다. 특히 Diffusion Policy는 적은 수의 시연에서도 가장 좋은 성능을 보였습니다. PerAct와 같은 복셀 기반 VLA 모델도 지원하며, Multi-View 및 SE(3) 증강이 성능 향상에 기여함을 보여줍니다.\n시각 기반 Sim2Real 조작: ManiSkill3는 Koch v1.1 로봇 팔과 휴대폰 카메라를 사용하여 시각 기반 조작 정책을 종단 간(end-to-end) 훈련하고 제로샷(zero-shot)으로 실제 세계에 배포하는 재현 가능한 설정을 제공합니다. 시뮬레이션 훈련 시 배경 녹색 스크리닝 및 광범위한 도메인 무작위화(카메라 포즈, 조명 방향, 로봇 포즈, 큐브 크기, 색상, 마찰)를 적용합니다. 약 1시간 훈련 후 최종 정책은 실제 세계에서 91.6%의 성공률을 달성했으며, 시뮬레이션과 실제 세계 성공률 간의 높은 상관관계(0.9284)를 입증했습니다.\n\n한계점: 복잡한 환경(예: 룸 스케일 장면)에서는 단일 GPU에서 병렬 환경의 수가 제한될 수 있습니다. Sim2Real의 완전한 해결은 아니며, 특정 보상 설계가 필요할 수 있습니다. 현재 Sim2Real 데모는 정적 카메라로 제한되어 있고, 소프트 바디 환경과 같은 일부 태스크는 GPU 병렬화가 되지 않습니다.\n결론: ManiSkill3는 일반화 가능한 로봇 시뮬레이션 및 렌더링을 위한 최첨단 프레임워크를 제공합니다. 이 플랫폼은 낮은 GPU 메모리 사용량, 높은 FPS, 독특한 이종 GPU 시뮬레이션, 그리고 가장 다양한 로봇 태스크를 특징으로 합니다. Sim2Real 및 Real2Sim 환경을 신뢰성 있게 지원하고, 몰입형 VR 텔레오퍼레이션 시스템을 제공하며, 사용자 친화적인 객체 지향 API를 통해 확장 가능한 로봇 학습의 접근성을 높입니다.\n\n\n\n2 Detail Review"
  },
  {
    "objectID": "posts/paper/2025-06-13-contact-rich-mpc.html",
    "href": "posts/paper/2025-06-13-contact-rich-mpc.html",
    "title": "📃Contact Trust Region 리뷰(feat.Dextreme)",
    "section": "",
    "text": "CTR vs DeXtreme: 능숙한 접촉 조작을 향한 두 갈래 길 모델 기반 접촉 계획(MPC-CTR)과 강화학습 기반 조작(DeXtreme)의 수학적 원리와 구조를 깊이 분석하고, 두 방법론을 다양한 관점에서 비교"
  },
  {
    "objectID": "posts/paper/2025-06-13-contact-rich-mpc.html#ctr-최적화-프레임워크",
    "href": "posts/paper/2025-06-13-contact-rich-mpc.html#ctr-최적화-프레임워크",
    "title": "📃Contact Trust Region 리뷰(feat.Dextreme)",
    "section": "CTR 최적화 프레임워크",
    "text": "CTR 최적화 프레임워크\n개요: 접촉 신뢰 영역(Contact Trust Region, CTR)은 기존의 타원형 신뢰영역(Ellipsoidal Trust Region, ETR)을 확장하여, 접촉 동역학의 물리 제약 조건을 명시적으로 포함하는 새로운 신뢰영역 모델입니다. 핵심 아이디어는 선형화 오차를 제어하는 작은 타원형 영역뿐 아니라, 접촉 가능성 제약 조건(일방향 접촉력, 마찰 원뿔 제약 등)도 함께 적용하여, 탐색 가능한 지역을 현실적인 물리 범위 내로 제한하는 것입니다.\n\n1. 미분 가능한 접촉 동역학 모델\nCTR은 미분 가능한 접촉 시뮬레이터를 활용합니다. 특히, 이전 연구인 Convex Quasi-Dynamic Contact (CQDC) 모델을 기반으로, 접촉 동역학을 볼록 최적화 문제(SOCP 등)로 표현합니다. 이 모델을 풀면 다음 상태뿐 아니라 접촉력까지 계산되며, 상태와 제어 입력에 대한 감도(Jacobian)도 함께 얻을 수 있습니다. 이는 접촉력을 쌍대변수(dual variable)로 간주한 KKT 조건 민감도 해석을 통해 가능해집니다.\n\n\n2. 상태 및 접촉력의 선형화\n미분 가능한 모델을 기반으로, 다음 상태 $+$와 접촉력 $+$는 다음과 같이 선형 근사됩니다:\n\n상태 업데이트: \\hat{q}_+ = A_\\kappa \\, \\delta q + B_\\kappa \\, \\delta u + f_\\kappa(\\bar{q}, \\bar{u})\n접촉력 응답: \\hat{\\lambda}_{+,i} = C_{\\kappa,i} \\, \\delta q + D_{\\kappa,i} \\, \\delta u + \\lambda_{\\kappa,i}(\\bar{q}, \\bar{u})\n\n이는 표준적인 상태 선형화와 달리, 접촉력 변화까지 함께 근사하므로, 접촉의 1차 응답을 정밀하게 반영할 수 있습니다.\n\n\n3. 접촉 가능성 제약(Contact Feasibility Constraints)\nCTR은 위 선형화 모델에 대해, 다음과 같은 물리 기반 제약을 적용합니다:\n\n비침투 조건 (Primal feasibility): \\hat{J}_i \\, \\hat{q}_+ + \\hat{c}_i \\in K_i → 접촉면에서의 상대 운동이 interpenetration을 유발하지 않도록 제한\n마찰 원뿔 조건 (Dual feasibility): \\hat{\\lambda}_{+,i} \\in K_i^* → 마찰 계수 및 일방향 접촉력 조건(정상 마찰력은 0 이상) 보장\n\n이러한 조건은 2차원 원뿔 제약(SOCP) 형태로 정식화되며, 신뢰 영역 내의 모든 후보해가 접촉 가능성 물리 법칙을 만족하도록 보장합니다.\n\n\n4. 접촉 신뢰 영역의 수학적 정의\nCTR은 다음의 조건을 만족하는 $(q, u)$의 집합으로 정의됩니다:\n\n타원형 제약: \\delta z^T \\Sigma \\delta z \\leq 1 \\quad (\\delta z = [\\delta q; \\delta u])\n선형화된 상태 및 접촉력 식 만족\n비침투 제약: $_+$가 접촉면을 침투하지 않음\n마찰 원뿔 제약: $_{+,i}$가 원뿔 내부에 위치함\n\nCTR은 이러한 제약들의 교집합이며, 이는 볼록 집합(convex set)입니다. 따라서 이후의 최적화 단계도 볼록 최적화 문제(SOCP)로 유지됩니다.\n\n\n5. 변형: A-CTR, R-CTR\n\nA-CTR (Action-only CTR): 상태는 고정하고 입력 $u$만을 탐색하는 경우. 계산량이 줄어 빠른 추론 가능\nR-CTR (Relaxed CTR): 비침투 조건을 제거하고 마찰 제약만 적용하여 보수성 완화 및 탐색 반경 확대\n\n실험 결과 R-CTR이 오히려 더 높은 성능을 보이는 경우가 있었으며, 이는 최적화가 덜 제한적인 방향으로도 유효한 접촉 조작을 계획할 수 있기 때문입니다."
  },
  {
    "objectID": "posts/paper/2025-06-13-contact-rich-mpc.html#ctr-기반-모델-예측-제어mpc-통합",
    "href": "posts/paper/2025-06-13-contact-rich-mpc.html#ctr-기반-모델-예측-제어mpc-통합",
    "title": "📃Contact Trust Region 리뷰(feat.Dextreme)",
    "section": "CTR 기반 모델 예측 제어(MPC) 통합",
    "text": "CTR 기반 모델 예측 제어(MPC) 통합\nCTR은 그 자체로는 하나의 제약 조건 집합이지만, 이를 실질적인 조작 제어기로 사용하려면 MPC(모델 예측 제어) 프레임워크 내에 통합해야 합니다. 본 섹션에서는 CTR이 어떻게 MPC에 통합되고, 접촉-풍부한 조작을 실시간으로 실행 가능한 최적화 문제로 변환하는지를 설명합니다.\n\n1. 접촉 암시적(contact-implicit) MPC\nCTR 논문에서는 접촉-암시적(contact-implicit) MPC 문제를 구성합니다. 즉, 접촉 모드 전이를 미리 명시하지 않고, 접촉 여부 및 접촉력의 발생을 최적화 과정에서 자동으로 결정합니다.\n\n각 시점에서 CQDC 기반 선형화를 통해 상태 및 접촉력에 대한 선형 모델을 생성\nCTR 제약(접촉 가능성, 마찰 등)을 적용한 SOCP 문제를 구성\n일정 시간 지평(horizon) 내에서 최적화한 후, 첫 번째 제어 입력만 적용하고 다시 반복 (Receding Horizon Planning)\n\nCTR의 구조 덕분에 이 MPC 문제는 전 구간에서 볼록 최적화(SOCP)로 유지됩니다.\n\n\n2. 반복 최적화 및 피드백\nCTR-MPC는 일반적인 MPC와 마찬가지로 매 타임스텝마다 새로운 상태를 관측하고, 선형화를 새로 수행한 후 최적화합니다. 이러한 반복 피드백 구조는 다음과 같은 이점을 제공합니다:\n\n모델링 오류나 외란에 대한 강건성 확보\n접촉 변화나 미세한 환경 조건 변화에 대한 실시간 적응\n\n\n\n3. 모드 전이 없이 접촉 처리\nCTR-MPC는 접촉 모드 전이(mode scheduling)를 명시적으로 기술할 필요가 없습니다. 다음의 수식 조건을 통해 접촉의 생성과 소멸을 자연스럽게 포함합니다:\n\n${+,i} K_i^*$ 조건은 ${+,i} = 0$ (접촉 없음)도 허용\n$i + + _i K_i$는 물체와 손가락이 떨어져 있을 때도 비침투 조건을 만족하도록 허용\n\n이러한 설계는 접촉 모드를 명시적으로 분기시키는 기존 방법들보다 훨씬 유연하고 계산 효율적입니다.\n\n\n4. 계산 효율성\nCTR-MPC의 각 최적화는 볼록 문제(SOCP)로 구성되며, 논문에서는 다음과 같은 실험 결과를 보고합니다:\n\nAllegro 핸드로 큐브를 조작하는 작업에서, 온라인 최적화는 수 초 이내에 실행 가능\n전체 조작을 위한 조작 동작 그래프(로드맵)를 구축하는 데 10분 미만 소요\n\n이는 일반적인 강화학습 기반 접근보다 훨씬 낮은 계산 자원으로 유사한 성능을 달성할 수 있음을 의미합니다.\n\n\n5. 예시 작업 및 결과\nCTR-MPC는 두 가지 실제 예시에서 검증되었습니다:\n\n양팔 조작 (Bimanual Manipulation): 두 개의 KUKA iiwa 팔로 큰 원통형 물체를 이동시키는 작업. 복잡한 접촉 협응이 필요하지만, CTR-MPC는 시뮬레이션과 실제 로봇 모두에서 성공적으로 수행.\n손 안 큐브 회전 (In-Hand Manipulation): Allegro 핸드로 큐브를 다양한 방향으로 회전시키는 작업. Relaxed CTR (R-CTR)을 사용한 경우가 가장 높은 성능을 보였으며, 로드맵 기반 전략으로 장거리 목표 회전도 달성 가능했음.\n\n\n\n6. 전역 계획과의 통합\nCTR-MPC는 본질적으로 로컬 최적화 기반이므로, 전체 상태 공간에서의 경로 계획은 어려울 수 있습니다. 이를 보완하기 위해 논문에서는 전역 로드맵 기반 계획(global roadmap planning)을 제안합니다:\n\n큐브의 다양한 안정된 포즈를 노드로 구성\nCTR-MPC를 이용해 이들 노드 간 단거리 조작 궤적(edge)를 생성\n전체 그래프를 탐색하여 멀리 떨어진 목표도 순차적 조작으로 도달 가능\n\n이 방식은 전통적인 샘플링 기반 계획과 유사하지만, MPC 기반 동작 원시(primitive)를 사용하여 접촉-풍부한 경로 생성을 가능케 합니다."
  },
  {
    "objectID": "posts/paper/2025-06-13-contact-rich-mpc.html#dextreme-강화학습-기반-큐브-회전-제어",
    "href": "posts/paper/2025-06-13-contact-rich-mpc.html#dextreme-강화학습-기반-큐브-회전-제어",
    "title": "📃Contact Trust Region 리뷰(feat.Dextreme)",
    "section": "DeXtreme: 강화학습 기반 큐브 회전 제어",
    "text": "DeXtreme: 강화학습 기반 큐브 회전 제어\nDeXtreme(NVIDIA Research, 2022)은 심층 강화학습 기반으로 학습된 정책(policy)을 통해, 저비용 로봇 핸드에서도 정밀한 큐브 회전을 수행한 시스템입니다. 이 접근법은 CTR이 다룬 Allegro 핸드의 조작 문제와 동일한 문제 설정에서, 전혀 다른 방식으로 해결책을 제시합니다.\n\n1. 시뮬레이션 기반 학습\n\nIsaac Gym이라는 GPU 가속 물리 시뮬레이터를 사용해 정책을 학습\n무려 10만 개 이상의 병렬 환경을 GPU에서 동시 실행\n이로 인해 로봇은 초인적인 속도로 시행착오 학습 가능\n\n\n\n2. 정책 구조\n\n정책은 심층 신경망으로 구성되며, 입력은 로봇 상태 및 물체 자세 정보\n비전 기반 정책도 학습됨: RGB 카메라 3대를 사용해 물체 자세 추정 후 입력으로 활용\n별도의 포즈 추정 신경망을 함께 학습시켜, 시각 정보에서 3D 물체 자세를 복원\n\n\n\n3. 도메인 랜덤화(Domain Randomization)\n\n시뮬레이션-현실 간 격차(Sim2Real gap)를 극복하기 위해 물리 속성 및 시각 조건을 광범위하게 랜덤화\n\n질량, 마찰계수, 표면 텍스처, 조명 조건, 카메라 위치 등\n\n이로 인해 정책은 넓은 조건 분포에 대해 강건한 행동 전략을 학습함\n\n\n\n4. 학습 비용 및 계산 자원\n\n약 32시간 동안 고성능 GPU 서버에서 학습\n이 동안 정책은 약 42년치에 해당하는 시뮬레이션 경험을 축적\n이는 강화학습의 대표적인 단점인 샘플 비효율성을 보여주는 지표\n\n\n\n5. 실행 및 실제 로봇 적용\n\n학습 완료 후, 정책은 고속 실시간 제어 가능 (신경망 전방 연산만 수행)\nAllegro 핸드에서 목표 방향으로 큐브를 안정적으로 회전시킴\nOpenAI의 Shadow Hand와 달리, 관절 수가 적고 비용도 낮은 Allegro 핸드에서 성공한 점이 인상적임\n\n\n\n6. 일반화 및 강건성\n\n도메인 랜덤화를 통해, 하드웨어 손상에도 견디는 강건성 확보\n\n예: 엄지 관절이 느슨한 상태에서도 정책이 보상하며 동작 성공\n\n시각 네트워크는 가림(occlusion) 및 모션 블러에도 견딜 수 있도록 학습됨\n\n\n\n7. 정책의 한계\nDeXtreme은 놀라운 성능을 보여줬지만, CTR 접근과 달리 접촉 물리 법칙을 명시적으로 반영하지는 않음:\n\n마찰 원뿔, 비침투 조건 등은 학습을 통해 암묵적으로 습득\n행동은 시뮬레이터의 물리 엔진과 보상 함수 설계를 통해 유도됨\n따라서 정책은 왜 해당 동작을 수행하는지 해석하기 어렵고, 제약 조건 위반 여부도 명시적으로 판단하기 어려움"
  },
  {
    "objectID": "posts/paper/2025-06-13-contact-rich-mpc.html#ctr-vs-dextreme-두-접근-방식의-비교-분석",
    "href": "posts/paper/2025-06-13-contact-rich-mpc.html#ctr-vs-dextreme-두-접근-방식의-비교-분석",
    "title": "📃Contact Trust Region 리뷰(feat.Dextreme)",
    "section": "CTR vs DeXtreme: 두 접근 방식의 비교 분석",
    "text": "CTR vs DeXtreme: 두 접근 방식의 비교 분석\nCTR-MPC와 DeXtreme은 모두 손 안의 큐브 회전과 같은 고난도 접촉 조작을 목표로 하지만, 모델 기반 최적화와 데이터 기반 학습이라는 정반대의 철학을 가지고 접근합니다. 아래는 두 방법론을 주요 관점에서 비교한 내용입니다.\n\n1. 접촉 처리 방식\n\n\n\n\n\n\n\n\n항목\nCTR-MPC\nDeXtreme (RL)\n\n\n\n\n접촉 모델링\n마찰 원뿔, 비침투 조건 등을 명시적 수식으로 모델링하고 최적화에 통합\n시뮬레이션과 보상을 통해 암묵적으로 접촉 전략을 학습\n\n\n접촉력 추론\n접촉력은 최적화 변수로 직접 계산되며, 계획 과정에서 사용됨\n신경망 내부에서 암묵적으로 형성됨 (관측 불가)\n\n\n물리 위반 가능성\n수식 제약으로 인해 물리 법칙 위반 불가능\n학습된 정책이 물리 제약을 위반할 수 있음 (ex. interpenetration)\n\n\n\n\n\n2. 샘플 효율성과 계산 자원\n\n\n\n\n\n\n\n\n항목\nCTR-MPC\nDeXtreme (RL)\n\n\n\n\n사전 학습 필요성\n없음 – 매 실행마다 최적화\n필요 – 수십억 스텝의 시뮬레이션 필요\n\n\n실행 시 계산 비용\n중간 – SOCP 최적화 수행\n매우 낮음 – 신경망 전방 연산만 수행\n\n\n샘플 효율성\n매우 높음 – 모델 기반 추론\n낮음 – 방대한 시행착오 필요\n\n\n\n\n\n3. 일반화와 적응성\n\n\n\n\n\n\n\n\n항목\nCTR-MPC\nDeXtreme (RL)\n\n\n\n\n환경 변화 대응\n모델만 수정하면 즉시 대응 가능\n사전 학습된 분포 외에는 재학습 필요\n\n\n목표 변화 적응\n즉시 가능 (목표 상태만 바꾸면 됨)\n가능하나, 정해진 목표 형식 내에서만 일반화됨\n\n\n외란 대응성\n고 – 재계획 기반\n중 – 일부 외란에는 강건하나 계획 능력은 없음\n\n\n\n\n\n4. 정책 구조와 해석 가능성\n\n\n\n\n\n\n\n\n항목\nCTR-MPC\nDeXtreme (RL)\n\n\n\n\n정책 형태\n최적화 기반 – 현재 상태에서 계획을 계산\n신경망 기반 – 관측 → 행동 매핑\n\n\n해석 가능성\n높음 – 접촉력, 제약 조건 등 확인 가능\n낮음 – 블랙박스 정책\n\n\n제약 조건 추가 용이성\n용이 – 수식 삽입만으로 반영 가능\n어려움 – 네트워크 재학습 필요\n\n\n\n\n\n요약\n\n\n\n항목\nCTR-MPC\nDeXtreme (RL)\n\n\n\n\n접촉 처리\n명시적, 해석 가능\n암묵적, 해석 불가\n\n\n학습 필요성\n없음\n큼 (수십억 스텝)\n\n\n실행 속도\n느리지만 정확\n매우 빠름\n\n\n일반화\n모델 기반 적응\n제한된 목표 내 일반화\n\n\n확장성 및 유지보수\n제약 추가/변경 쉬움\n재학습 필요"
  },
  {
    "objectID": "posts/paper/2025-06-13-contact-rich-mpc.html#결론-및-향후-연구-방향",
    "href": "posts/paper/2025-06-13-contact-rich-mpc.html#결론-및-향후-연구-방향",
    "title": "📃Contact Trust Region 리뷰(feat.Dextreme)",
    "section": "결론 및 향후 연구 방향",
    "text": "결론 및 향후 연구 방향\nCTR과 DeXtreme은 각각 정확하고 물리적으로 해석 가능한 모델 기반 계획과 빠르고 강건한 데이터 기반 제어라는 상반된 강점을 보여줍니다. 이러한 성격의 차이는 오히려 상호보완적인 통합 가능성을 시사합니다.\n\n1. 하이브리드 전략의 가능성\n앞으로의 연구는 다음과 같은 하이브리드 모델을 탐색할 수 있습니다:\n\nCTR으로 생성된 궤적을 imitation learning의 teacher로 활용\n\nRL의 초기 정책을 빠르게 수렴시킬 수 있음\n\nDeXtreme 정책을 warm-start로 사용하여 CTR 최적화를 가속\n\n최적화 초기화를 RL 정책 기반으로 설정해 연산량 감소\n\n접촉 모델의 일부를 학습된 근사 모델로 대체\n\n예: 마찰계수 추정, 감쇠 계수 추정 등 실제 환경 파라미터 보정\n\n\n이처럼 양측의 장점을 조합하는 방식은, 물리 기반 정확성과 학습 기반 유연성을 동시에 확보할 수 있는 유망한 방향입니다.\n\n\n2. 실시간성 향상\nCTR-MPC의 경우, 최적화의 실시간성은 여전히 제한적입니다. 이를 해결하기 위해 다음과 같은 접근이 제안될 수 있습니다:\n\nCTR 기반 정책을 사전 학습해 신경망으로 근사 (Policy Distillation)\nCTR 해를 데이터셋으로 수집 후, offline RL이나 trajectory matching으로 정책 학습\n\n이러한 방식은 제약 조건을 만족하는 정책을 빠르게 실행할 수 있게 해줄 뿐 아니라, 정책의 해석 가능성도 부분적으로 유지할 수 있습니다.\n\n\n3. 보다 복잡한 조작 작업 확장\n향후 연구는 다음과 같은 더 복잡한 작업으로의 확장을 목표로 할 수 있습니다:\n\n비정형 물체 조작 (불규칙한 형상, 연성 물체 등)\n시각 기반 입력 통합 (CTR과 카메라 인식 결합)\n사람과의 협업 조작 (공동 운반, 안전 제약 등 포함)\n\n특히 CTR 기반 접근은 제약 조건 기반의 신뢰성과 안전성을 활용해, 사람과 함께하는 환경에서도 활용 가능성을 보여줍니다."
  },
  {
    "objectID": "posts/paper/2025-06-13-contact-rich-mpc.html#마무리",
    "href": "posts/paper/2025-06-13-contact-rich-mpc.html#마무리",
    "title": "📃Contact Trust Region 리뷰(feat.Dextreme)",
    "section": "마무리",
    "text": "마무리\n“Dexterous Contact-Rich Manipulation via the Contact Trust Region” 논문은 고난도 조작에서 접촉 제약을 어떻게 명시적으로 다루고, 이를 모델 기반 제어 프레임워크에 통합할 수 있는지를 수학적으로 우아하게 풀어낸 작업입니다. 그에 비해 DeXtreme은 대규모 계산 자원을 활용한 전통적인 심층강화학습 방식이지만, 실제 적용성에 있어 매우 강력한 접근임을 보여줍니다.\n이 두 흐름은 서로 경쟁적이라기보다, 다음 세대의 조작 시스템에서 병렬적으로 사용될 수 있는 기술 스펙트럼의 양극단으로 이해될 수 있습니다.\n앞으로의 연구는, 이들 방법론을 상황에 따라 선택하거나 조합함으로써, 보다 유연하고 안전하며 일반화 가능한 로봇 조작 시스템을 구축하는 데 기여할 수 있을 것입니다."
  },
  {
    "objectID": "posts/paper/2025-07-18-touch-wild.html",
    "href": "posts/paper/2025-07-18-touch-wild.html",
    "title": "📃Touch in the Wild 리뷰",
    "section": "",
    "text": "Paper Link\nGithub Link\nProject Link"
  },
  {
    "objectID": "posts/paper/2025-07-18-touch-wild.html#개요-및-기술적-기여",
    "href": "posts/paper/2025-07-18-touch-wild.html#개요-및-기술적-기여",
    "title": "📃Touch in the Wild 리뷰",
    "section": "2.1 1. 개요 및 기술적 기여",
    "text": "2.1 1. 개요 및 기술적 기여\n이 논문은 다음과 같은 세 가지 기술적 기여를 중심으로 전개됩니다:\n\n휴대형 시각-촉각 그리퍼 개발 논문에서는 290g에 불과한 가볍고 배터리로 구동되는 휴대형 그리퍼를 설계했습니다. 두 개의 손가락에 촘촘히 분포된 촉각 센서(12×32 텍셀)를 포함하고 있으며, 상단에 fisheye RGB 카메라가 부착되어 있어 촉각과 영상 정보를 동시에 수집할 수 있습니다. 이로 인해 실제 환경(in-the-wild)에서 사람이 직접 다양한 작업을 시연하며 시각-촉각 데이터를 수집할 수 있게 되었습니다.\nCross-modal Masked Autoencoder 기반 Visuo-Tactile Representation Learning 프레임워크 제안 핵심은 시각 정보와 촉각 정보를 단순히 결합(concatenation)하지 않고, 각 모달리티의 특성을 보존한 채로 교차 주의 메커니즘(cross-attention)을 통해 학습하는 것입니다. 특히 촉각 이미지를 무작위로 마스킹한 후, 이를 시각 정보로 보완하여 복원하는 방식으로, 두 센서 간의 상호 보완 관계를 강제로 학습하게 됩니다.\n2.6M 프레임, 2700개 이상의 시연으로 구성된 대규모 Visuo-Tactile Dataset 구축 다양한 실내/외 환경(12곳)에서 43개 작업에 대한 촬영을 통해 정밀 조작 작업을 포함한 대규모 데이터셋을 수집하였으며, 실제 촉각/영상 동기화 방법으로 QR코드 기반 타임스탬프 정합 기법을 도입하여 고비용 장비 없이도 정밀한 멀티모달 수집이 가능했습니다.\n\n\n✅ 정리하자면, 이 논문은 하드웨어, 데이터셋, 학습 구조를 모두 포함한 멀티모달 조작 학습의 end-to-end 전환점으로 볼 수 있으며, 기존 연구들보다 한 단계 높은 현실성, 확장성, 성능을 보여줍니다."
  },
  {
    "objectID": "posts/paper/2025-07-18-touch-wild.html#visuo-tactile-fusion-방식",
    "href": "posts/paper/2025-07-18-touch-wild.html#visuo-tactile-fusion-방식",
    "title": "📃Touch in the Wild 리뷰",
    "section": "2.2 2. Visuo-Tactile Fusion 방식",
    "text": "2.2 2. Visuo-Tactile Fusion 방식\n\n2.2.1 📌 핵심 구조\n\n입력 분리 처리\n\n시각: CLIP ViT-B/16 백본을 사용해 768차원의 embedding 추출\n촉각: 24×32 압력 행렬을 RGB 이미지로 인코딩 후, 작은 CNN을 통해 768차원의 embedding 생성\n\nCross-modal Attention\n\n촉각이 시각을 쿼리하여 z_{\\text{tac}} \\rightarrow z'_{\\text{tac}}로 업데이트\n다시 시각이 업데이트된 촉각을 쿼리하여 z_{\\text{img}} \\rightarrow z'_{\\text{img}} 생성\n양방향 교차 주의를 통해 모달리티 간 정보를 서로 보완하고 조율\n\nMasked Autoencoding 훈련 방식\n\n입력 촉각 이미지의 60~80%를 마스킹 후 시각 정보를 이용해 전체를 복원\nreconstruction loss L_{\\text{recon}} = |T - \\hat{T}|^2을 사용 → 촉각 정보를 직접 복원하게 하여 단순한 피처 병합이 아닌 진정한 의미의 “융합” 학습\n\n\n\n\n2.2.2 🧠 해석 가능성과 장점\n\n시각적 주의 맵을 보면 대부분 접촉 위치나 물체와의 인터페이스 영역에 집중되어 있음\n이 attention은 unseen 환경에서도 일관되게 나타나며, 학습된 시각-촉각 표현이 일반화됨을 보여줌\n단순한 concat 방식보다 훨씬 더 정밀한 접촉 인식과 위치 추론이 가능\n\n\n🔎 핵심 요약: cross-attention 구조 + reconstruction task의 조합은, 기존의 단순 early-fusion 기법보다 훨씬 정교하고 효과적인 멀티모달 표현을 가능하게 합니다."
  },
  {
    "objectID": "posts/paper/2025-07-18-touch-wild.html#조작-정책-모델-구조",
    "href": "posts/paper/2025-07-18-touch-wild.html#조작-정책-모델-구조",
    "title": "📃Touch in the Wild 리뷰",
    "section": "2.3 3. 조작 정책 모델 구조",
    "text": "2.3 3. 조작 정책 모델 구조\n\n2.3.1 🔧 2단계 구조\n\nVisuo-Tactile Encoder E_\\phi(I, T)\n\n앞서 설명한 cross-modal encoder\n사전학습(pretraining) 후, 정책 학습 시에는 fine-tuning 가능\n\nDiffusion Policy\n\n조건부 확률 기반 행동 생성: p(a_t | z_t, p_t)\n입력은 visuo-tactile embedding z_t와 proprioception p_t (gripper 상태 등)\n정적인 MLP 대신 확률 기반 U-Net 모델로, 더 정교한 다중모드 행동 생성 가능\n\n\n\n\n2.3.2 🤖 행동 생성 방식\n\n행동 시퀀스를 직접 예측하는 것이 아닌 noise → action으로 변환하는 방식\n학습 시 행동에 noise를 추가하고, 이를 역으로 제거하는 방식으로 학습 (Denoising Diffusion)\n이로 인해 단일 행동 예측보다 더 정교하고 부드러운 행동 시퀀스 생성 가능\n\n\n✅ 이 구조 덕분에 복잡한 조작에서도 행동이 한결 자연스럽고 신뢰성 있게 생성됩니다. 특히 접촉이 중요한 작업에서 작은 감각 피드백 차이도 반영할 수 있다는 점에서 큰 장점이 있습니다."
  },
  {
    "objectID": "posts/paper/2025-07-18-touch-wild.html#학습-데이터-구성과-품질",
    "href": "posts/paper/2025-07-18-touch-wild.html#학습-데이터-구성과-품질",
    "title": "📃Touch in the Wild 리뷰",
    "section": "2.4 4. 학습 데이터 구성과 품질",
    "text": "2.4 4. 학습 데이터 구성과 품질\n\n2.4.1 📊 구성 개요\n\n총 2.6M 프레임, 2700개 이상의 시연, 43가지 작업\n분류:\n\nMain indoor tasks (38%) – 논문 실험용 핵심 작업\nOther indoor tasks (37%) – 다양한 보조 작업\nOutdoor tasks (25%) – 시장, 거리, 공원 등 in-the-wild 환경에서 수행\n\n\n\n\n2.4.2 📷 수집 방식\n\nGoPro 카메라 + 촉각 센서 동기화\n\nQR코드 기반 타임스탬프 정합으로 저비용/고정밀 동기화 구현\n\n사람이 손으로 조작하며 수집\n\n더욱 섬세하고 정교한 조작 포함 가능 (ex. 피펫 액체 옮기기, 연필 깎기)\n\n\n\n\n2.4.3 📉 한계점\n\n병렬 조작이 어려움: 2지 그리퍼 기준 수집되어 멀티 핑거 조작에는 제약\n사람이 수집하고 로봇은 학습하는 구조이므로 domain gap 존재\n촉각 센서 주파수 제한 (23Hz): 고속 slip, texture 분류 등은 어려움\n\n\n🌟 하지만, 해당 데이터셋은 촉각-시각 학습을 위한 현실적이고 확장 가능한 기반을 제공하며, 이는 기존 연구에서 보기 드문 강력한 장점입니다."
  },
  {
    "objectID": "posts/paper/2025-07-18-touch-wild.html#실험-설계-및-한계",
    "href": "posts/paper/2025-07-18-touch-wild.html#실험-설계-및-한계",
    "title": "📃Touch in the Wild 리뷰",
    "section": "2.5 5. 실험 설계 및 한계",
    "text": "2.5 5. 실험 설계 및 한계\n\n2.5.1 🧪 주요 실험 작업 (4개)\n\nTest Tube Insertion: 집기 → 회전 → 슬롯 삽입\nPencil Sharpening: 연필 회전 및 정렬 후 구멍 삽입\nFluid Transfer: 피펫을 잡고, 부드럽게 짜서 액체 이동\nWhiteboard Erasing: 일정한 힘으로 칠판 닦기\n\n→ 공통점: 정밀 접촉 및 힘 조절이 필요한 작업\n\n\n2.5.2 🧪 실험 설계\n\n20번의 반복 실험 per 작업\n초기 상태 및 배경 변형을 통한 일부 generalization 테스트\n세부 단계별(집기, 회전, 삽입 등) 성공률도 측정하여 분석의 정밀도를 높임\n\n\n\n2.5.3 📊 성능 비교\n\n\n\n\n\n\n\n\n\n\n방법\nTactile 사용\nCross-attn\nPretrain\nTest Tube 삽입 성공률\n\n\n\n\nVision Only\n❌\n-\n✅\n25%\n\n\nVision + Tactile (no cross-attn)\n✅\n❌\n✅\n50%\n\n\nVision + Tactile (no pretrain)\n✅\n✅\n❌\n70%\n\n\nOurs (full)\n✅\n✅\n✅\n85%\n\n\n\n\nPretraining 및 Cross-attn이 정밀 조작 성공률을 2~3배 향상시킴\n특히 Vision만 사용하는 경우, 투명 물체/미세 접촉에서 상황 판단 실패가 자주 발생\n\n\n\n2.5.4 🧩 한계\n\n실험은 모두 로봇 팔 기반 고정된 실내 환경에서 수행 → “진짜 in-the-wild” 배치는 아님\n멀티태스크 통합 정책은 없으며, 각 작업별 개별 정책 학습\ndiffusion 기반 정책은 계산량이 크므로 실시간 제어에 한계 가능성\n\n\n✅ 그럼에도 불구하고, 실험 설계는 각 구성요소의 기여도를 정량적으로 잘 보여주며, 이 방식이 촉각 기반 조작에 의미 있는 성능 개선을 제공한다는 점을 설득력 있게 입증합니다."
  },
  {
    "objectID": "posts/paper/2025-07-18-touch-wild.html#결론-및-분석-요약",
    "href": "posts/paper/2025-07-18-touch-wild.html#결론-및-분석-요약",
    "title": "📃Touch in the Wild 리뷰",
    "section": "2.6 ✍️ 결론 및 분석 요약",
    "text": "2.6 ✍️ 결론 및 분석 요약\n\n시각-촉각 통합 조작 학습이라는 어려운 문제를 하드웨어–데이터–학습 구조 측면에서 풀어낸 훌륭한 논문\n특히 cross-modal fusion + reconstruction learning 방식은 기존 멀티모달 학습에서 모달리티 간의 정보 교환을 구조적으로 학습할 수 있게 한 큰 기여\n수집된 데이터의 규모, 다양성, 품질 또한 타 연구 대비 매우 우수\n다만 실제 배치 시 domain shift, 계산 비용, 멀티핑거 확장성 등의 과제는 남아 있음"
  },
  {
    "objectID": "posts/paper/2025-09-20-dexop.html",
    "href": "posts/paper/2025-09-20-dexop.html",
    "title": "📃DEXOP 리뷰",
    "section": "",
    "text": "Paper Link\nHomepage"
  },
  {
    "objectID": "posts/paper/2025-09-20-dexop.html#서론-및-개요",
    "href": "posts/paper/2025-09-20-dexop.html#서론-및-개요",
    "title": "📃DEXOP 리뷰",
    "section": "서론 및 개요",
    "text": "서론 및 개요\nDEXOP(Dexterous OPeration)은 인간의 섬세한 조작(dexterous manipulation) 데이터를 로봇으로 효과적으로 전이하기 위한 새로운 패러다임인 “페리오퍼레이션(perioperation)”을 제시한 연구입니다. 페리오퍼레이션이란 기존의 원격 조작(텔레오퍼레이션이라 불리는 remote teleoperation)과 달리, 사람이 로봇과 직접 연결된 장치를 착용하고 현실 환경에서 물체를 조작함으로써 로봇 학습에 필요한 다양한 센서 데이터를 수집하는 접근입니다. 이를 통해 인간의 섬세한 조작 시 발생하는 시각, 촉각, 고유수용감각(프로프리오셉션) 정보를 풍부하게 기록하면서도, 이 데이터가 실제 로봇에 직접 활용될 수 있도록 전이 가능성을 극대화하는 것이 목적입니다. 연구의 핵심은 사람 손에 장착하는 수동형 손 외골격 장치와 그것에 기계적으로 연결된 수동 로봇 손으로 구성된 시스템 DEXOP을 구현하고, 이를 통해 사람이 마치 자신의 손으로 직접 물체를 다루듯 자연스럽고 정확하게 로봇 시연 데이터를 생성하도록 한 점입니다.\n인간이 DEXOP 장치를 착용한 모습과 이를 통해 섬세한 물체 조작을 시연하는 예시입니다. 사람의 손가락 움직임이 기계식 링크를 통해 동일하게 로봇 손가락에 미러링되며, 사람이 물체를 조작할 때 로봇 손가락에 부착된 센서들이 시각 정보(손바닥 카메라)와 촉각 정보(손가락 전면의 촉각 센서)를 실시간 수집합니다. DEXOP 시스템은 실제 로봇 손과 동일한 형태와 센서 구성을 가진 수동 로봇 손을 사람이 움직이는 외골격에 연결함으로써, 인간의 조작 데이터가 추가적인 변환이나 보정 없이도 곧바로 로봇에 적용될 수 있게 설계되었습니다. 이러한 기계적 자세 미러링(pose mirroring)과 힘 투명성(force transparency) 덕분에 사용자는 마치 맨손으로 물체를 다루는 듯한 감각, 즉 관절 움직임에 대한 고유수용성 힘 피드백을 느끼면서 시연을 진행할 수 있습니다. 논문에서는 이러한 DEXOP의 설계 철학과 구현, 그리고 이를 활용한 다양한 과제 수행 및 학습 실험을 제시하며, 기존 방법들 대비 자연스럽고 빠른 시연 데이터 수집과 우수한 로봇 학습 성능 향상을 입증하였습니다."
  },
  {
    "objectID": "posts/paper/2025-09-20-dexop.html#dexop-시스템의-설계와-구성",
    "href": "posts/paper/2025-09-20-dexop.html#dexop-시스템의-설계와-구성",
    "title": "📃DEXOP 리뷰",
    "section": "DEXOP 시스템의 설계와 구성",
    "text": "DEXOP 시스템의 설계와 구성\nDEXOP은 하드웨어 측면에서 크게 세 부분으로 이루어집니다:\n\n인간의 손에 착용되는 수동형 손가락 외골격(exoskeleton)\n외골격에 기계적으로 연결된 수동 로봇 손(데이터 수집용 로봇 손)\n이 둘을 이어주는 링크 메커니즘\n\n수동 로봇 손은 인간 조작 데이터를 기록하기 위한 센서 플랫폼 역할을 하며, DEXOP 논문에서는 MIT에서 개발한 EyeSight 로봇 손의 설계를 계승하여 제작되었습니다. 인간이 외골격에 손가락을 넣고 움직이면 링크를 통해 로봇 손의 각 관절이 동일하게 움직이는데, 이때 사람 손가락에 가해지는 힘이 로봇 손가락에도 전달되고 반대로 로봇 손가락이 물체와 접촉하여 받는 저항력은 다시 링크를 통해 사람 손가락으로 느껴지게 됩니다. 이러한 양방향 기계 연결은 로봇의 접촉력을 사람에게 직접적인 고유수용성 피드백으로 전달하여, 별도의 전기식 햅틱 장치 없이도 사람에게 정밀한 힘 감각을 제공하는 것이 특징입니다. 특히 DEXOP에서는 일반적인 텔레오퍼레이션의 제한점인 제한적 햅틱 피드백(진동 등 단순 신호에 국한되거나 손끝 압력만 제공되는 문제)을 극복하고자, 손가락 각 마디까지 전해지는 풍부한 힘 피드백(법선력+전단력)을 구현하였다고 보고하고 있습니다.\n하드웨어 기구학 설계 측면에서, DEXOP의 수동 로봇 손과 외골격 장치는 인간 손의 관절 구조를 가능한 충실히 모사하도록 공동 설계되었습니다. 이번 연구에서는 총 3가지 DEXOP 변형 모델이 제작되었는데, 가장 완성도가 높은 DEXOP-12는 4개의 손가락(엄지, 집게, 중지, 약지)에 총 12개의 자유도(관절)를 구현하였습니다. 각 손가락 별로 굽힘/폄(flexion)과 벌림/모음(abduction)을 위한 2 자유도의 첫째 마디(MCP 관절)와 굽힘만 가능한 둘째 마디(PIP 관절)를 포함하며, 엄지의 경우 손바닥과의 접합부(TMC 관절) 2자유도와 끝마디(IP 관절) 1자유도를 갖습니다. 이는 인간 손의 원위지절(DIP)이나 엄지의 일부 관절은 생략된 구성이지만, 설계 복잡도 증가 대비 조작 다양성 기여가 낮다고 판단되는 관절을 제외하여 설계 단순성과 기능성의 균형을 맞춘 결과입니다. 이러한 결정에도 불구하고 DEXOP-12는 인간 손과 유사한 넓은 작업 범위(예: 손가락 관절 가동각 약 110° 이상)와 충분한 힘 전달 능력(집게/중지 손끝에서 약 60N, 엄지 손끝에서 70N까지 힘 전달)으로 실제 로봇과 유사한 조작을 가능케 했습니다. 아울러 덱스터러스 조작을 위해 중요한 엄지의 대향 운동(다른 손가락과 마주보게 움직이는 능력)과 손가락 사이 간격 조절(벌림)을 포함함으로써, 작은 물체 집기부터 손바닥을 활용한 파지까지 다양한 형태의 작업이 가능하도록 디자인되었습니다.\nDEXOP의 기계적 링크 설계는 여러 개의 4-bar 링크 장치를 직렬 및 병렬로 배치하여 사람 손가락의 움직임을 동일 비율로 로봇 손가락에 전달하도록 구성되었습니다. 예를 들어 집게/중지/약지에 대해서는 두 개의 4-bar 링크를 직렬 연결하여 첫째 마디(MCP)와 둘째 마디(PIP)의 각도를 각각 제어하며, 엄지의 경우 두 자유도의 관절을 한 개 링크로 묶고 추가 링크를 직교하게 배치해 엄지 끝마디를 제어하는 독특한 구조를 적용했습니다. 이러한 링크 구조는 외골격과 로봇 손 사이에 가상의 기준 좌표계(ground frame)를 설정하여 사람 손의 움직임이 로봇 손에 일대일 대응되도록 합니다. 또한 손가락 끝 켑(fingertip cot)을 통해 사람 손가락 끝을 외골격에 고정함으로써, 손끝 위치까지 정확히 대응시키는 것이 가능해졌습니다. 이처럼 정교한 기구학 매핑 덕분에 DEXOP 사용자는 시연 중에 손과 로봇 손의 자세 불일치를 보정하기 위해 시선을 떼어 확인하거나 인지적으로 부담을 느낄 필요 없이, 본능적인 동작만으로 정확한 로봇 조작 데이터를 생성할 수 있습니다. 이는 기존 텔레오퍼레이션에서 흔히 발생하는 인간 손 vs 로봇 손의 형상/관절 차이로 인한 자리꼬임(kinematic mismatch) 문제를 근본적으로 해소하여, 시연 과정의 직관성과 신뢰성을 크게 높인 점이라 평가됩니다. 더불어 DEXOP는 능동 구동부(모터)가 없기 때문에 시스템이 비교적 경량이고 안전하며, 여러 환경으로 쉽게 운반하여 사용할 수 있다는 장점도 있습니다.\n한편, DEXOP 수동 로봇 손에는 다양한 센서 모듈이 통합되어 멀티모달 데이터 수집이 가능하게 합니다. 우선, 각 손가락의 마디와 관절 각도는 고해상도 엔코더로 계측되어 정확한 관절 궤적 데이터가 저장됩니다. 더욱 특筆할 만한 것은 손가락과 손바닥 전반에 장착된 전체 손 촉각 센싱 시스템인데, 이는 MIT Adelson 교수팀의 GelSight 기반 광학식 촉각 센서 기술을 활용한 GelSim(ple) 센서들로 구성됩니다. 각 손가락의 끝마디, 손바닥 면, 손가락의 첫 마디 부분까지 총 수 개의 GelSim(ple) 유닛이 분포하여, 손이 물체를 쥐는 동안 발생하는 세밀한 접촉 면적 분포와 힘 패턴을 카메라 기반으로 시각화해 기록합니다. 예를 들어 손가락 끝의 촉각센서는 물체의 미세한 질감이나 힘의 크기 변화를 영상(frame) 형태로 담아내고, 손바닥에 접촉된 부분도 모두 이미지로 저장되기 때문에, 이후 로봇 학습 단계에서 접촉력의 추정이나 접촉 지점 판별에 중요한 데이터를 제공합니다. 논문에서는 이러한 전체 손의 촉각 센싱(whole-hand tactile sensing)이 정교한 조작 재현에 필수적임을 강조하는데, 단순히 관절 위치 정보만 일치시키는 것으로는 동일한 힘 조절을 보장할 수 없기 때문에 물체가 미끄러지거나 부서지는 등의 문제가 생길 수 있지만, 손 전체의 힘 분포를 기록하면 나중에 로봇이 물체를 잡을 때 어느 부위에 얼마나 힘을 주었는지까지 재현할 수 있어 성공률과 안정성이 높아진다는 것입니다. 끝으로, DEXOP의 손바닥에는 어안 카메라(fisheye camera)가 부착되어 시야각이 넓은 1인칭 시각 영상을 획득합니다. 이 카메라는 작업 중인 물체와 손의 상호작용을 손목 부근에서 촬영하여, 시각 정보와 촉각 정보를 동기화한 데이터를 생성합니다. 필요에 따라 DEXOP는 전완부나 몸통의 움직임까지 포착하기 위해 별도의 팔 외골격(AirExo)이나 IMU/SLAM 기반 트래킹으로 손의 글로벌 위치/자세도 기록할 수 있습니다. 종합하면, DEXOP 시스템은 관절각도(time-series), 손목 위치 궤적, 손바닥 카메라 영상, 여러 지점의 촉각 영상 등 다양한 모달리티의 데이터를 동시에 수집하여, 로봇 학습에 필요한 풍부한 정보를 제공합니다.\nDEXOP 설계에는 이러한 기본 구조 외에도, 조작 과제의 다양성을 높여줄 몇 가지 기계적 아이디어가 적용되었습니다.\n\n손톱(fingernail): 로봇 손가락 끝에 작은 인공 손톱을 부착하여, 높이가 낮은 얇은 물체를 밑에서 긁어 들어올리거나 M2 나사와 같이 아주 작은 부품도 쉽게 집을 수 있게 했습니다.\n손가락 벌림 관절(abduction joints): 집게, 중지, 약지의 첫째 관절에 좌우로 벌어지는 자유도를 추가하여, 손가락들 사이 간격을 조절함으로써 손 안에서의 물체 재배열(in-hand reorientation)이나 다양한 크기의 물체 파지에 용이하도록 했습니다.\n푹신한 손바닥 패드(padded palm): 손바닥 표면에 쿠션 재질을 추가하여 물체를 감쌀 때 밀착력을 높였습니다. 이를 통해 예를 들어 한 손으로 병뚜껑을 딸 때 병 몸체를 손바닥으로 더욱 안정적으로 잡거나, 스프레이 통을 손에 쥐고 손가락으로 꼭지 부분을 누르는 등의 전체 손 파지 작업의 안정성이 향상되었습니다.\n\n이 세 가지 기능들은 이미 일부 로봇 핸드 연구에서 섬세함을 높이기 위한 장치들로 시도된 바 있으나, 데이터 수집용 인터페이스에 인간이 착용한 상태로 적용된 것은 DEXOP이 최초입니다. 인간 손과 로봇 손을 분리하여 설계한 DEXOP 구조 덕분에 이러한 기능들을 통합해도 사용자가 물체를 조작하는 데 지장이 없도록 만들 수 있었으며, 이는 정밀하고 다양한 작업 데이터를 수집하는 데 기여하는 중요한 혁신이라 볼 수 있습니다."
  },
  {
    "objectID": "posts/paper/2025-09-20-dexop.html#데이터-수집-및-로봇-전이-프로세스",
    "href": "posts/paper/2025-09-20-dexop.html#데이터-수집-및-로봇-전이-프로세스",
    "title": "📃DEXOP 리뷰",
    "section": "데이터 수집 및 로봇 전이 프로세스",
    "text": "데이터 수집 및 로봇 전이 프로세스\nDEXOP의 가장 큰 강점 중 하나는 수집한 인간 조작 데이터를 별 어려움 없이 실제 로봇에 전이할 수 있다는 점입니다. 앞서 언급했듯 DEXOP의 수동 로봇 손은 목표로 하는 실제 로봇 손과 동일한 형태의 기구학 구조와 센서 구성을 가지도록 공동 설계되었습니다. 예컨대 DEXOP-7 모델은 이후 실험에서 사용된 EyeSight 로봇 핸드와 1대1로 대응되게 만들어져, DEXOP-7로 모은 데이터가 곧바로 EyeSight 손이 장착된 로봇에 적용될 수 있었습니다. 인간 손과 로봇 손의 분리라는 발상의 장점은, 사람이 장비를 착용해 시연하는 시점부터 이미 로봇과 동일한 좌표계, 동일한 접촉 지점 정보로 기록이 이루어진다는 것입니다. 따라서 텔레오퍼레이션처럼 시연 후에 복잡한 좌표 변환이나 역기구학 계산, 추가 보정 등을 할 필요가 거의 없습니다. 실제 논문에서도 DEXOP+DexArm(AirExo) 시스템으로 수집된 데이터는 “별도의 embodiment gap 보정 없이 바로 정책 학습에 사용될 수 있었다”고 언급합니다. 이는 데이터 전처리 부담을 크게 줄이고, 사람이 시연하면서 느낀 감각과 로봇이 실행 시 필요한 정보 사이의 단절을 최소화한다는 점에서 의미가 큽니다.\n데이터 수집 과정은 다음과 같이 이뤄집니다. 사용자가 DEXOP를 착용하고 목표 작업(예: 드릴로 구멍 뚫기, 전구 끼우기 등)을 수행하면, 실시간으로 모든 센서 데이터와 관절 정보가 동기화되어 기록됩니다. DEXOP는 자체 제어용 능동 부품이 없기 때문에, 별도의 제어 소프트웨어보다는 데이터 로깅 소프트웨어 아키텍처가 중요합니다. 연구진은 촉각 카메라 여러 대에서 오는 고속 영상 데이터와 관절각 센서 데이터, 팔 움직임 데이터 등을 수집하기 위해 여러 대의 온보드 컴퓨터(Raspberry Pi 등)와 GPU 워크스테이션을 활용한 분산 데이터 수집 시스템을 구성하였습니다. 이를 통해 전 손 촉각영상, 손바닥 시야 영상, 관절 각도 로그 등이 누락 없이 저장되었고, 후처리를 통해 각 프레임에 시각-촉각-포즈 정보가 정합된 시연 데이터셋이 구축되었습니다. 특히 과제에 따라 한 팔이 아닌 양손 조작(bimanual manipulation)이 필요한 경우, 한쪽 손에 DEXOP를 장착하고 다른 한 손으로는 보조 동작을 수행하여 데이터를 모으거나, 또는 두 개의 DEXOP 장치를 활용하는 방식도 고려될 수 있습니다. 본 논문에서는 주로 한 손에 DEXOP를 사용하고 나머지 한 손은 일반적으로 보조 역할을 하는 시나리오(예: 한 손으로 병을 잡고 DEXOP 낀 손으로 뚜껑을 여는 식)를 다루었으며, AirExo-2라는 팔 외골격을 함께 사용하여 DEXOP 손의 글로벌 위치 이동까지 기록함으로써 이후 로봇 팔+손 전체를 모방 제어할 수 있도록 하였습니다.\n수집된 데이터는 로봇 학습 알고리즘에 투입되어 정책(policy) 또는 모델로 학습됩니다. 논문에서는 행동 청크 변환기(Action Chunking Transformer, ACT) 구조를 활용한 Behavior Cloning(행동 모방학습) 방식을 채택하여, DEXOP로 모은 시演 데이터를 로봇의 정책으로 학습시켰습니다. 학습에는 DEXOP로 수집한 데모 외에도, DEXOP와 약간 차이가 있는 부분(예: 팔 외골격의 미세한 차이로 인한 로봇 팔 보정)을 보완하기 위해 소량의 텔레오퍼레이션 데모 데이터를 혼합하였습니다. 이러한 과정을 거쳐 학습된 정책은 실험용 휴머노이드 로봇(Unitree H1, 양팔에 EyeSight 로봇 손 장착)에 이식되어, 사람 없이 로봇 스스로 여러 단계의 작업을 수행하도록 평가되었습니다. 중요한 것은, DEXOP 덕분에 수집된 데이터가 질적으로 높아 짧은 시간 내에 학습이 가능했고, 또 로봇 손의 실제 센서정보(촉각 등)까지 포함하고 있어 현실 환경에 강인한 정책을 얻을 수 있었다는 점입니다. 이는 곧이어 살펴볼 실험 결과에서 구체적으로 드러납니다."
  },
  {
    "objectID": "posts/paper/2025-09-20-dexop.html#실험-및-성능-평가",
    "href": "posts/paper/2025-09-20-dexop.html#실험-및-성능-평가",
    "title": "📃DEXOP 리뷰",
    "section": "실험 및 성능 평가",
    "text": "실험 및 성능 평가\n논문에서는 DEXOP 시스템의 유용성을 검증하기 위해 다양한 실험을 수행했습니다. 첫째, 기본 하드웨어 성능 평가로서 DEXOP와 목표 로봇 손(EyeSight Hand)의 기계적 성능 비교가 이뤄졌습니다. DEXOP-7 (3손가락, 7자유도 모델)을 착용한 사용자가 낼 수 있는 최대 힘, 작업 공간 범위, 속도 등을 측정하여, 실제 EyeSight 로봇 손의 사양과 유사하거나 그 이상임을 확인했습니다. 예를 들어 집게손가락 MCP 관절의 경우 DEXOP로 약 110° 이상의 굽힘 범위를 달성하였고, 각 관절의 최대 각속도도 35 rad/s 정도로 빠른 편이어서 인간 조작의 민첩성을 재현하기에 충분했습니다. 또한 손끝에 전달할 수 있는 힘은 60~70N 수준으로, 이는 작은 물체를 다루는 데 과잉일 정도로 높아 DEXOP가 로봇 손의 모든 잠재력을 충분히 이끌어낼 수 있음을 보여줍니다.\n둘째, 사용자 실험(User Study)을 통해 DEXOP의 조작 효율이 기존 텔레오퍼레이션보다 우수함이 정량적으로 검증되었습니다. 실험 참가자들은 DEXOP와 비교군(전통적 텔레오퍼레이션 시스템)을 이용하여 드릴로 구멍뚫기, 전구 소켓에 끼우기, 박스에 물건 포장하기, 병뚜껑 열기 등 4가지 접촉이 많은 덱스터러스 작업을 수행했고, 연구진은 단위 시간당 성공적으로 작업을 완료한 횟수(처리량, throughput)를 측정했습니다. 그 결과 모든 과제에서 DEXOP 사용 시 성공 속도가 유의하게 높았으며, 텔레오퍼레이션에 비해 사람에 훨씬 가까운 성능을 보였습니다. 특히 텔레오퍼레이션으로는 전혀 성공하지 못한 작업도 DEXOP로는 원활히 수행되었는데, 예를 들어 드릴 작업의 경우 텔레오퍼레이션 성공률 0%였던 반면 DEXOP 사용 시 분당 6회의 빠른 작업 완료가 가능했습니다. 또 다른 예로 전구 설치 과제에서는 텔레오퍼레이션에서는 전혀 작업을 마치지 못했지만 DEXOP로는 다수의 전구를 짧은 시간 내 끼울 수 있었습니다. 이러한 차이는 DEXOP가 제공하는 고유수용성 힘 피드백과 정확한 자세 미러링 덕분에, 사용자가 물체와의 접촉 상태를 정확히 파악하며 신속하게 동작할 수 있기 때문으로 해석됩니다. 반면 기존 텔레오퍼레이션 인터페이스는 촉각 정보 부재로 인해 조심스럽게 천천히 동작하거나 과도한 힘/움직임을 주는 경향이 있어, 정밀한 접촉 작업에서 실패하거나 시간이 오래 걸리는 한계가 드러났습니다.\n셋째, DEXOP의 조작 범위와 섬세함을 보여주는 질적 시연 예시들(qualitative demonstrations)이 제시되었습니다. 연구팀은 DEXOP-9 (약지 제외 3손가락, 9자유도)와 DEXOP-12 모델을 사용하여, 아주 작은 나사나 뚜껑 조작, 주사기처럼 정밀한 도구 조작, 서류봉투의 종이 클립 사용, 양손 협조 작업 등 다양한 시나리오를 수행했습니다. 예를 들어 손끝 정교성이 필요한 주사기 플런저 조작이나 M2 나사집게로 집어서 끼우기 등의 동작을 DEXOP로 성공적으로 해냈으며, 손바닥 전체를 활용하는 양손 병뚜껑 열기(한 손은 병 잡고 DEXOP 손으로 뚜껑 트는 작업), 스프레이 버튼 누르기, 종이커터 사용 등도 시연했습니다. 이들 작업은 일반적인 두손 집게(gripper)나 기존 원격 조작 손으로는 매우 수행하기 어렵거나 여러 단계로 복잡하게 해야 하는 것들인데, DEXOP 환경에서는 사람의 섬세한 손동작을 거의 그대로 재현할 수 있기 때문에 한 번의 연속 동작으로 성공시킬 수 있음을 보여주었습니다. 이는 DEXOP가 평행 크로우 그리퍼로 한계가 있는 영역(예: 한 손 안에서 물체 돌리기, 좁은 틈의 작은 부품 다루기 등)까지 데이터 수집 범위를 넓혀준다는 의미입니다.\n넷째, 학습 실험(performance in policy learning)을 통해 DEXOP로 모은 데이터의 우수성이 입증되었습니다. 복잡한 6단계로 이루어진 양손 전구 조립 과제에 대해, 앞서 수집한 시연 데이터로 로봇 정책을 학습시키고 성능을 비교했습니다. 하나의 정책은 DEXOP 데이터 160개 + 텔레오퍼레이션 데이터 40개로 학습시켰고, 비교군으로 동일 과제의 텔레오퍼레이션 데이터 200개만으로 학습한 정책과, 텔레오퍼레이션 100개 데이터만으로 학습한 정책을 마련했습니다. 그 결과 DEXOP 데이터로 학습한 정책이 성공률 약 51.3%로, 동일 시간 동안 모을 수 있는 텔레오퍼레이션 데이터로 학습한 정책(약 35.5%)이나 두 배 많은 텔레오퍼레이션 데이터로 학습한 정책(42.5%)보다 훨씬 높은 성능을 보였습니다. 다시 말해, 같은 시간 투자 대비 DEXOP로 모은 데이터가 로봇에게 더 효과적인 학습을 제공하여 학습효율을 크게 향상시킨 것입니다. 분석 결과, 텔레오퍼레이션 데이터로 학습한 정책은 사람 조작 시의 비정상적인 동작 패턴까지 학습하여 중간에 실수를 하거나 불안정한 동작을 보였지만, DEXOP 데이터로 학습한 정책은 불필요한 동작이 적고 정확한 접촉 시점과 힘 조절이 포함되어 있어 더 신뢰성 있게 과제를 수행했다고 합니다. 예를 들어 텔레오퍼레이션 시에는 촉각이 없으므로 사용자가 전구를 소켓에 넣을 때 과도하게 여러 번 돌리는 경향이 있었고, 그 때문에 학습된 정책도 실제 로봇 실행 시 불필요하게 전구를 오래 돌리다가 시간 초과로 실패하는 경우가 있었다고 합니다.\n반면 DEXOP 데이터에는 사람이 딱 맞는 힘과 각도로 전구를 돌려 끼운 동작들이 담겨 있어서, 정책이 정확히 적정 횟수만 돌리고 다음 단계로 넘어가는 등 효율적인 동작 시퀀스를 학습할 수 있었습니다. 이로써 DEXOP가 더 높은 품질의 시연 데이터를 빠르게 대량 수집할 수 있을 뿐 아니라, 그 데이터가 로봇의 실제 성능 향상으로 이어진다는 것을 실증했다고 볼 수 있습니다.\n요약하면, DEXOP는 사람의 능숙한 조작을 보다 자연스러운 방식으로 기록하여, 기존 원격 조작 대비 시연 속도와 성공률을 높이고, 학습 측면에서도 적은 데이터로 더 나은 성능을 끌어내었다고 결론지을 수 있습니다. 이러한 실험결과들은 제안된 페리오퍼레이션 접근법이 로봇 덱스터러스 조작 연구에서 데이터 병목 문제를 완화하는 데 실질적인 해법이 될 수 있음을 보여줍니다."
  },
  {
    "objectID": "posts/paper/2025-09-20-dexop.html#기존-연구와의-차별성-분석",
    "href": "posts/paper/2025-09-20-dexop.html#기존-연구와의-차별성-분석",
    "title": "📃DEXOP 리뷰",
    "section": "기존 연구와의 차별성 분석",
    "text": "기존 연구와의 차별성 분석\nDEXOP의 접근은 휴먼-로봇 모션 트랜스퍼와 텔레오퍼레이션, 모션 캡처 등 관련 선행 연구들과 여러 측면에서 구별됩니다. 주요 비교 지점을 하나씩 살펴보겠습니다:\n원격 조작(텔레오퍼레이션) 기반 데이터 수집과의 비교:\n전통적으로 로봇 조작 데이터를 모으기 위해 VR 인터페이스나 데이터 장갑, 조이스틱 등을 활용한 텔레오퍼레이션이 널리 사용되었습니다. 그러나 이러한 방법들은 사람에게 촉각/힘 피드백이 거의 없거나 제한적이어서, 사용자들이 시각에 의존해 조심스럽게 조작하거나 진동 등 추상적 피드백만으로 추측에 의존해야 했습니다. 그 결과 미세한 힘 조절이 어렵고 조작 속도가 느리며, 시연 데이터에도 비연속적 움직임이나 부정확한 힘 적용 등의 오류가 흔히 포함되었습니다. 반면 DEXOP는 기계적 링크를 통한 손가락 레벨의 직접적인 힘/위치 피드백을 제공함으로써, 사용자가 자신의 손으로 만지는 듯한 현실감을 얻고 더 빠르고 정확하게 작업을 수행할 수 있게 합니다. 또한 텔레오퍼레이션에서는 인간 손과 로봇 손의 형태 차이로 인해 자세 맵핑 문제가 발생하여 사용자가 화면을 보며 로봇 손 자세를 미세 조정해야 하지만, DEXOP는 인간 손의 움직임이 곧 로봇 손 움직임으로 1:1 대응되므로 추가적인 보정 작업이 불필요하고 시연의 직관성이 높습니다. 이런 차이로 인해 DEXOP를 통한 데이터 수집은 동시간 대비 더 많은 시연 횟수와 성공 사례를 축적할 수 있고, 데이터 품질도 인간의 의도가 그대로 반영된 풍부한 접촉 정보 포함 고품질 데이터가 됩니다. 실제로 DEXOP 논문에서는 동일 시간 동안 DEXOP로 모은 데이터로 학습한 정책이 텔레옵 데이터 대비 성공률이 크게 향상됨을 보여주어, “전통적 텔레오퍼레이션 대비 단위 시간당 과제 성능이 유의미하게 높다”고 밝히고 있습니다.\n모션 캡처/데이터 글러브 방식과의 비교:\n사람의 손동작 데이터를 모아 로봇에 전이하려는 시도로 모션 캡처 장갑이나 비디오 기반 추출도 활용되어 왔습니다. 예를 들어, 사용자가 맨손이나 장갑을 끼고 물체를 조작하면서 손가락 관절각 센서나 비디오의 3D pose 추정으로 손 움직임을 기록한 후, 이를 로봇 손의 관절각 시퀀스로 변환하는 접근입니다. 그러나 이 경우 물체와의 상호작용 힘 정보가 기록되지 않으며, 사람 손과 로봇 손의 형태 차이를 맞추기 위해 복잡한 계산이나 휴리스틱이 필요합니다. 특히 손가락 개수나 길이, 관절범위 등이 다를 경우 정확한 모션 매핑이 어려워 로봇이 동일한 작업을 재현하지 못하거나, 미끄러짐/실패가 발생하기 쉽습니다. 또한 사람 손에 로봇과 동일한 촉각 센서를 장착하는 것은 공간 제약상 불가능하여, 로봇 입장에서 중요한 접촉면 정보가 누락됩니다. DEXOP는 수동 로봇 손을 별도로 사용함으로써 이 문제를 해결했습니다. 사람은 실제 물체를 만지지 않고 로봇 손을 통해 우회해서 조작하기 때문에, 사람 손에는 아무 센서도 없어 간편하지만 동시에 로봇 손에는 필요한 모든 센서를 장착할 수 있었습니다. 덕분에 인간 손의 움직임 데이터와 로봇 손의 접촉 데이터를 동시에 확보하여 나중에 로봇이 동일한 조건으로 작업할 수 있게 되었고, 번거로운 모션 리타게팅 없이 데이터의 전이 가능성이 크게 높아졌습니다. 한 마디로, DEXOP는 기존 모션 캡처 방식이 간과한 “로봇 관점의 데이터”(예: 로봇 촉각센서 출력)를 함께 얻는 점에서 차별화됩니다.\n단순 그리퍼 기반 데이터 수집 장치와의 비교:\n최근 연구들에서는 사람에게 간단한 외골격을 착용시켜 2핑거 그리퍼를 조작하게 함으로써 데이터 수집을 자동화하려는 시도가 있었습니다. MIT의 AirExo, UMI, DobbE 등은 비교적 저렴한 외골격을 이용해 사람의 팔이나 손 동작을 측정하고, 이를 2개의 손가락을 가진 그리퍼 로봇의 개폐 동작으로 매핑하여 데이터를 모으는 시스템입니다. 이러한 접근은 비용이 낮고 설치가 간편하다는 장점이 있지만, 얻을 수 있는 조작 데이터의 범위가 제한적입니다. 예를 들어 2핑거 그리퍼로는 손가락 사이에서 물체를 돌리는 동작이나 한 손가락이 지지하고 다른 손가락이 조작하는 복합 동작 등을 구현할 수 없으며, 작은 나사나 복잡한 부품을 다루기 어렵습니다. 또한 양손 그리퍼로 복잡한 작업을 수행하려면 여러 단계의 분할 동작이 필요해 비효율적입니다. 반면 DEXOP는 세 손가락 이상을 개별 제어할 수 있고 손바닥까지 활용 가능하여, 손 안에서의 정교한 조작, 작은 물체 조작, 관절식 물체(예: 뚜껑 달린 병) 조작 등 그리퍼로는 어려운 작업들을 수행할 수 있습니다. 논문에서도 DEXOP가 기존 병렬 그리퍼 기반 시스템 대비 (i) 손가락-물체 간 상대운동이 필요한 작업 지원, (ii) 양손 그리퍼로 여러 단계를 거쳐야 하는 작업을 한 손으로 완료 가능, (iii) 비좁은 공간의 작은 부품 조작 가능, (iv) 분사기나 도어처럼 관절부 있는 물건 조작 가능 등의 우위점을 지적하고 있습니다. 이는 DEXOP가 멀티핑거 로봇 손의 잠재력을 십분 활용하여 데이터 수집의 폭과 효율을 모두 높였음을 의미합니다.\n기존 다지 손 외골격 시스템과의 비교:\n사람 손에 힘을 주거나 재활을 돕기 위한 모터 구동식 손가락 외골격 연구도 오래 전부터 진행되어 왔습니다. 이러한 장치들은 주로 재활 의료나 힘 증강, 촉각 피드백 용도로 개발되어 사람 손가락에 능동적으로 힘을 가하거나 진동을 주는 기능이 있습니다. 최근에는 이를 확장하여 원격 로봇 조작에 적용한 사례도 있으나, 일반적으로 복잡하고 크며, 사람 손에 무거운 모터와 구조물이 붙어 있기 때문에 섬세한 물체 조작에 방해가 될 수 있습니다. DEXOP는 이와 정반대로 모터를 완전히 제거한 수동식 외골격이라는 점에서 차별화됩니다. 사람이 자신의 근력으로 움직이고 느끼는 자연스러운 상호작용을 목표로 하여, 하드웨어 간소화와 기계적 투명성을 극대화한 것이 특징입니다. 최근 발표된 몇몇 유사한 수동식 다지 외골격 연구들도 존재하지만, 이들 중 다수는 손가락의 기본적인 굽힘/폄 동작 전달에만 초점을 맞춘 초기 단계로서, 세밀한 손끝 조작이나 전손 촉각 기록까지 구현한 경우는 없었습니다. 논문에 따르면 동시대의 다른 시도들은 결국 기본적인 쥐기 동작만 가능하여 평행 그리퍼와 큰 차이가 없는 수준이었으나, DEXOP는 정밀하고 다양한 손동작을 모두 다룰 수 있고, 촉각 힘 정보까지 함께 기록함으로써 질적으로 다른 기여를 했다고 평가합니다. 즉, DEXOP는 인간 손의 섬세함을 로봇 데이터 수집 장치에 본격적으로 도입한 최초의 사례로 볼 수 있습니다.\n요약하면, DEXOP는 기존 텔레오퍼레이션의 한계(느린 속도, 낮은 촉각 피드백), 순수 모션 캡처의 한계(힘/접촉 정보 부재), 단순 외골격/그리퍼 접근의 한계(조작 다양성 부족)를 모두 넘어서는 새로운 설계를 제시하여, 휴먼-로봇 모션 전이 분야에 독창적인 공헌을 했습니다."
  },
  {
    "objectID": "posts/paper/2025-09-20-dexop.html#논문의-핵심-기여",
    "href": "posts/paper/2025-09-20-dexop.html#논문의-핵심-기여",
    "title": "📃DEXOP 리뷰",
    "section": "논문의 핵심 기여",
    "text": "논문의 핵심 기여\n이 논문의 핵심 기여는 다음과 같이 정리될 수 있습니다:\n페리오퍼레이션(parioperation) 패러다임의 제안: 저자들은 인간의 조작 데이터를 로봇에 활용하기 위한 새로운 데이터 수집 패러다임을 정의하였습니다. 이는 원격 조작과 직접 시연 사이의 틈을 메우는 접근으로서, “인간이 로봇을 몸에 두르고 시연한다”는 개념을 구체화한 것입니다. 로봇 학습에서 데이터 병목 문제를 해결하기 위해 현실 세계에서 대규모로, 그리고 높은 질로 시연 데이터를 모을 수 있는 방법론을 제시한 점은 중요한 철학적 기여입니다. 저자들은 이를 통해 로봇의 일반화 능력을 높이려면 인간의 풍부한 경험을 최대한 로봇 형태로 담아내는 중간 단계가 필요함을 강조하며, DEXOP 같은 시스템이 그 중간층을 채워줄 것이라고 주장합니다.\nDEXOP 하드웨어 시스템의 구현: 논문은 위 패러다임을 실현한 구체적인 장치 DEXOP를 설계하고 제작하였습니다. 이 시스템은 세계 최초로 전손 촉각 센싱을 통합한 수동식 손 외골격으로, 인간과 로봇을 기계적으로 연결한 독창적 구조를 갖습니다. 얇은 금속 링크와 정교한 4절 링크 기구학을 통해 인간 손과 로봇 손의 움직임을 일체화하였고, 동시에 로봇 손에 인간의 감각을 실시간 피드백하는 설계를 선보였습니다. 또한 손톱, 벌림 관절, 손바닥 패드 등의 요소를 결합해 데이터 수집 장치로서의 기능성을 극대화하였으며, 7자유도부터 12자유도까지 다양한 형태의 프로토타입을 제작하여 범용성을 보여준 점도 공헌이라 할 수 있습니다. 요컨대, DEXOP는 기존 로봇 손 연구의 성과(센서, 메커니즘)를 인간 시연 장치에 이식하여 하드웨어적으로 새로운 플랫폼을 제공했다는 데 큰 의의가 있습니다.\n향상된 로봇 학습 성능 및 효율: DEXOP를 통해 수집한 데이터로 로봇 정책을 학습한 결과, 동일한 시간 대비 더 높은 성공률을 얻을 수 있음을 입증하였습니다. 이는 DEXOP 데이터가 정확하고 깨끗하며, 편향이 적은 덕분에 학습에 유리함을 보여줍니다. 반대로 기존 텔레오퍼레이션 데이터는 일관성 부족과 인간 조작상의 왜곡이 있어 비효율적임을 지적하였지요. 이러한 비교 실험을 통해, DEXOP가 로봇 학습을 가속화할 수 있는 강력한 도구임을 제시한 점은, 단순한 장치 개발을 넘어 로봇 학습 커뮤니티에 유용한 데이터셋/교본을 제공할 수 있다는 가능성을 연 것입니다. 특히 시각+촉각 통합 데이터를 이용한 학습이라는 측면에서, 향후 다중 센서퓨전 기반의 정책 개발에도 본 데이터가 중요한 역할을 할 수 있음을 시사합니다.\n관련 분야 연구 방향 제시: 본 논문은 마지막으로 DEXOP와 유사한 대규모 데이터 수집 기구들이 향후 로봇 학습의 필수 인프라가 될 것임을 언급하며, 이를 통해 더 나은 데이터-하드웨어-알고리즘의 공동 발전(coevolution)을 기대하고 있습니다. 이는 로봇 공학에서 “더 나은 손”만큼이나 “더 나은 데이터”의 중요성을 환기하는 메시지로서, 학술적 시사점이 큽니다. 요약하면, DEXOP 논문은 하나의 장치 소개에 그치지 않고, 로봇 덱스터러스 조작을 위한 데이터 문제 해결의 새 길을 제시하고 구체적으로 증명해 보인, 혁신적이고 종합적인 기여**를 했다고 평가할 수 있습니다."
  },
  {
    "objectID": "posts/paper/2025-09-20-dexop.html#한계점-및-향후-전망",
    "href": "posts/paper/2025-09-20-dexop.html#한계점-및-향후-전망",
    "title": "📃DEXOP 리뷰",
    "section": "한계점 및 향후 전망",
    "text": "한계점 및 향후 전망\nDEXOP는 매우 유망한 시스템이지만, 논문에서 언급했거나 추후 고려해야 할 몇 가지 제한점과 향후 과제도 존재합니다.\n정량적인 힘 측정 및 보정의 한계:\nDEXOP는 사람의 감각을 통해 간접적으로 힘을 전달하지만, 절대적인 접촉력 수치나 토크를 정확히 계측하는 기능은 아직 제한적입니다. 수집된 촉각 영상으로부터 정확한 접촉력/관절 토크 추정을 하기 위해서는 별도의 보정과 센서 캘리브레이션이 필요한데, 현 단계에서는 이러한 실시간 추정이 어렵습니다. 향후에는 촉각 이미지를 정교하게 해석하여 정량적인 힘 정보를 복원하고, 이를 통해 로봇에 정밀힘 제어 피드백을 주는 연구가 필요합니다.\n로봇 손 구조적 한계:\nDEXOP가 대응하는 EyeSight 로봇 손은 인간 손보다 간단화된 구조(예: DIP 관절 없음, 새끼손가락 제외 등)이므로, 일부 매우 고난이도의 섬세한 작업에는 한계가 있습니다. 예를 들어 사람 손가락 끝마디를 활용한 정교한 물체 뒤집기나, 새끼손가락까지 쓰는 복잡한 파지 등은 현재 DEXOP-12로는 구현이 어렵습니다. 이는 DEXOP 시스템 자체의 한계라기보다 목표 로봇 플랫폼의 한계로 볼 수도 있는데, 향후 더욱 고도화된 로봇 손(예: 5손가락 20자유도 이상)이 개발되고 여기에 대응하는 DEXOP가 생긴다면 인간 손의 거의 모든 동작을 데이터화하는 것도 가능할 것입니다. 반대로, 너무 복잡한 구조는 DEXOP 장치의 착용감을 해칠 수 있으므로 설계 상쇄(trade-off)를 고려해야 합니다.\n인간에게 제공되는 촉각 피드백의 한계:\n현재 DEXOP는 기계적 링크를 통한 고유수용성 힘 피드백만을 사람에게 제공합니다. 이는 관절의 저항으로 느껴지는 힘은 전해지지만, 정교한 피부 감각(예: 질감, 미세한 진동 등)은 사람에게 전달되지 않는다는 뜻입니다. 사람 손가락 끝이 직접 물체에 닿는 것이 아니기 때문에, 촉각 센서로 측정된 정보를 다시 사람 피부에 전달해주지 않는 한 완전한 촉감 피드백은 없습니다. 향후에는 사람 쪽 장갑에 간단한 진동자나 압력 패드를 달아 텍스쳐나 미끄럼 감지 피드백을 주거나, 최소한 촉각 이벤트(예: 물체 접촉/탈촉 시각) 정도는 알려주는 방식도 고려될 수 있습니다. 다만 이러한 기능 추가는 시스템 복잡도를 높일 수 있으므로, 사람의 학습 곡선과 피로도 등에 미치는 영향도 연구해야 할 것입니다. 원격 조작 및 범용성에 대한 과제: DEXOP는 사람이 직접 장치를 착용하고 현장에서 시연해야 하므로, 완전한 원격 조작에는 한계가 있습니다. 예를 들어 위험한 원자력 시설 내부나 인간이 접근하기 어려운 우주/해저 환경에서 데이터를 모으기에는 DEXOP를 그대로 활용하기 어렵습니다. 그런 환경에서는 여전히 전통적 텔레로봇+haptic 인터페이스 조합이 필요할 수 있습니다. 따라서 DEXOP 개념을 원격 적용하려면, 사람 쪽과 원격 로봇 쪽을 동일한 링크 장치로 연결하고 원격에서 마스터-슬레이브 형태로 힘 반영을 하는 등 추가 연구가 필요할 것입니다. 또한 DEXOP 장치를 다른 로봇 손으로 변경하려면 해당 손에 맞춘 외골격/링크를 새로 설계해야 하므로, 범용 플랫폼화에도 도전이 있습니다. 이를 해결하기 위해 모듈형 DEXOP(여러 손 형상을 지원)이나 소프트(exo) 장치로의 발전도 생각해볼 수 있습니다.\n이러한 한계들에도 불구하고, 저자들은 DEXOP류 시스템이 향후 로봇 학습 데이터 수집의 중추적인 역할을 할 것으로 기대하고 있습니다. 앞으로 촉각과 시각의 통합 학습, 정밀 힘제어가 가능한 정책 등 DEXOP 데이터 활용 연구가 이어진다면, 로봇이 사람 수준의 섬세한 조작을 학습하는 날이 앞당겨질 것으로 보입니다. DEXOP는 현재 발전 중인 분야의 흥미로운 이정표(milestone)로서, 향후 개선과 응용 여지가 매우 큰 플랫폼이라 할 수 있습니다."
  },
  {
    "objectID": "posts/paper/2025-07-04-sparsh-skin.html",
    "href": "posts/paper/2025-07-04-sparsh-skin.html",
    "title": "📃Sparsh-Skin 리뷰",
    "section": "",
    "text": "Paper Link\nProject Link"
  },
  {
    "objectID": "posts/paper/2025-07-04-sparsh-skin.html#핵심-방법론",
    "href": "posts/paper/2025-07-04-sparsh-skin.html#핵심-방법론",
    "title": "📃Sparsh-Skin 리뷰",
    "section": "핵심 방법론",
    "text": "핵심 방법론\nSparsh-skin은 Transformer 아키텍처를 기반으로 하는 학생(student) 네트워크 E_\\theta와 교사(teacher) 네트워크 E_{\\hat{\\theta}}로 구성된 자기 증류 프레임워크를 사용합니다. 입력은 100ms 길이의 촉각 측정값 (\\mathbf{x}_{1:10} \\in \\mathbb{R}^{10 \\times 368 \\times 3})과 센서 위치 정보 (\\mathbf{p}_{1:10} \\in \\mathbb{R}^{10 \\times 368 \\times 3})의 이력입니다. 각 센서(총 368개)의 데이터는 선형 투영 f_{\\text{linear}}을 통해 표현 차원 d로 토큰화됩니다: \\mathbf{z}_i = f_{\\text{linear}}(\\mathbf{x}_{1:10} | \\mathbf{p}_{1:10}) \\in \\mathbb{R}^{368 \\times d} 또한, 손바닥, 손가락 마디, 손가락 끝 등 Xela 센서 패드의 유형에 따라 학습 가능한 토큰(learnable token)이 각 센서에 추가됩니다. 위치 임베딩(positional embedding) 대신 3D 센서 위치 정보를 활용합니다.\n데이터 손상(data corruption) 기법으로는 이미지 도메인의 크롭/리사이즈가 자기 플럭스 신호의 의미론적 변화를 유발할 수 있으므로, 토큰화 이후에 블록 마스킹(block masking)을 적용합니다. 이는 인접한 센서 아일랜드(sensor island)를 포함한 연속적인 센서 블록을 입력에서 제거하는 크로스-택셀(cross-taxel) 방식입니다. 학생 네트워크는 손상된 입력 \\bar{\\mathbf{z}}_i를 받고, 교사 네트워크는 덜 손상된 입력 \\mathbf{z}^*_i를 받습니다 (학생은 무작위로 10%~40%의 데이터를 유지한 k개 마스크를, 교사는 40%~100%의 데이터를 유지한 1-2개 마스크를 받습니다).\n예측 작업(prediction task)은 마스크드 오토 재구성(masked auto-reconstruction)보다 센서 노이즈에 강건한 분류(classification) 방식을 사용합니다. 변환된 센서 토큰은 분류 헤드 f_{\\text{class}}를 통해 프로토타입 로짓(prototype logit)으로 변환됩니다:\n\n학생은 \\bar{\\mathbf{p}}_i = f_{\\text{class}}(E_\\theta(\\bar{\\mathbf{z}}_i)),\n교사는 \\mathbf{p}^*_i = f_{\\text{class}}(E_{\\hat{\\theta}}(\\mathbf{z}^*_i)).\n\n학생 및 교사 로짓 예측 간의 패치 레벨 교차 엔트로피(patch level cross entropy) 목표를 사용하여 센서 표현에서 국소-전역(local-to-global) 대응 학습을 강화합니다. 교사 네트워크의 가중치 \\hat{\\theta}는 역전파(back-propagation)가 아닌 학생 네트워크 가중치 \\theta의 지수 이동 평균(EMA)으로만 업데이트됩니다: \\hat{\\theta} \\triangleq \\text{EMA}(\\theta)"
  },
  {
    "objectID": "posts/paper/2025-07-04-sparsh-skin.html#평가-결과",
    "href": "posts/paper/2025-07-04-sparsh-skin.html#평가-결과",
    "title": "📃Sparsh-Skin 리뷰",
    "section": "평가 결과",
    "text": "평가 결과\n사전 학습 진행 상황 모니터링을 위해 온라인 프로브(online probe)를 사용합니다. 재구성(reconstruction) 및 물체 식별 능력(분류 정확도)을 평가하며, Sparsh-skin은 MAE 기반 모델보다 우수한 재구성 및 물체 분류 성능(약 95% 정확도)을 보였습니다.\n평가 프로토콜에서 다운스트림 작업은 즉각적 예측 작업과 시간적 추론 작업으로 나뉩니다.\n\n즉각적 작업(힘 추정)에는 어텐티브 풀링(attentive pooling)과 작은 2층 MLP가 사용됩니다.\n시간적 작업(조이스틱, 자세 추정)에는 어텐티브 풀링 후 1층 Transformer 블록이 사용됩니다.\n\n모델 비교에는 BYOL* (본 논문에서 재현한 BYOL 변형), End-to-end, Sparsh-skin (frozen), Sparsh-skin (finetuned), Sparsh-skin (MAE)이 포함됩니다. 성능은 RMSE (힘, 조이스틱, 자세), 자세 정확도 (자세), 성공률 (플러그 삽입)로 측정되며, 샘플 효율성을 평가하기 위해 레이블된 데이터의 비율을 변화시켰습니다.\n\n주요 실험 결과는 다음과 같습니다:\n\n\n힘 추정: End-to-end 모델은 특히 저데이터 환경에서 성능이 매우 떨어지지만, Sparsh-skin (frozen) 및 Sparsh-skin (finetuned)은 적은 데이터로도 일관된 성능을 유지했습니다. Sparsh-skin (MAE)는 자기 플럭스 신호의 노이즈 특성으로 인해 성능이 좋지 않았습니다.\n조이스틱 상태 추정: Sparsh-skin은 전체 데이터를 사용했을 때 기존 HiSS* 모델과 유사한 성능을 보였으며, 3.3%의 데이터만 사용했을 때도 높은 샘플 효율성을 보이며 경쟁력 있는 성능을 달성했습니다.\n자세 추정: 사전 학습된 Sparsh-skin 표현을 사용한 모든 모델이 End-to-end 모델보다 낮은 RMSE와 높은 자세 정확도를 보였습니다. Sparsh-skin (finetuned)은 End-to-end 대비 이동에서 약 10%, 회전에서 약 20% 향상된 성능을 보였습니다. 손 전체 센싱(palm sensing)이 자세 추정에 중요함이 확인되었습니다.\n정책 학습 (플러그 삽입): 비전과 Sparsh-skin 촉각 표현을 함께 사용한 정책은 비전 단독 정책 (20% SR) 및 종단 간 시각-촉각 정책 (40% SR) 대비 우수한 성능 (Sparsh-skin (frozen) 75% SR)을 보였습니다. 촉각 정보가 삽입 성공률에 크게 기여함을 확인했습니다.\n\n본 연구는 자기식 피부 센서를 위한 고성능 촉각 표현 모델인 Sparsh-skin을 제안합니다. 광범위한 비레이블 데이터에 대한 자기 지도 학습을 통해 획득된 Sparsh-skin 표현은 다양한 촉각 중심 작업에서 우수한 성능과 샘플 효율성을 입증했습니다. 이는 손 전체 촉각 표현을 위한 파운데이션 모델(foundation model)로 나아가는 한 단계로 간주됩니다.\n논문의 한계점으로는, 데이터 손상 전략이 주로 공간적이며 시간적 상관 관계 학습을 명시적으로 다루지 않는 점, 현재 자세 추정 작업은 고정된 손과 2D 자세에 제한적이라는 점, 그리고 시각-촉각 정책의 일반화 가능성에 대한 추가 연구가 필요하다는 점 등이 제시되었습니다."
  },
  {
    "objectID": "posts/paper/2025-07-04-sparsh-skin.html#배경-및-연구-동기",
    "href": "posts/paper/2025-07-04-sparsh-skin.html#배경-및-연구-동기",
    "title": "📃Sparsh-Skin 리뷰",
    "section": "배경 및 연구 동기",
    "text": "배경 및 연구 동기\n로봇 손의 촉각 감각은 물체를 조작하고 조작기(dexterous manipulator)의 성능을 높이는 데 필수적인 역할을 합니다. 특히 손가락 끝에 국한된 기존 시각 기반 촉각 센서(예: GelSight 계열, DIGIT 등)는 고해상도 정보를 제공하지만, 손 전체를 감싸는 형태로 적용되기 어렵고 대역폭 제약으로 빠른 응답에도 한계가 있습니다. 이에 비해 자기 기반 촉각 피부 센서(magnetic tactile skin)는 얇고 유연하게 손가락 마디와 손바닥 전체에 부착할 수 있으며, 빠른 응답 속도를 갖춘다는 장점이 있습니다. 그러나 이러한 자기 촉각 센서들은 센서 출력(자계 플럭스)을 해석하고 보정(calibration)하는 데 어려움이 있고, 각기 다른 지자기 환경에서의 기준점 드리프트 등 문제로 표준화된 일반 모델이 부재하여 널리 활용되지 못했습니다. 다시 말해, 기존에는 특정 작업이나 센서에 특화된 규칙 기반 또는 개별 학습 모델에 의존하는 경우가 많았고, 손 전체를 포괄하는 일관된 촉각 표현 방식이 부족했습니다.\n이러한 맥락에서, Akash Sharma 등 연구진은 “Self-supervised perception for tactile skin covered dexterous hands” 논문에서 손가락, 마디, 손바닥을 모두 덮는 다수의 촉각 피부 센서로부터 얻는 신호를 효율적으로 통합하여 표현하는 방법을 제안했습니다. 핵심 아이디어는 자기 지도 학습(self-supervised learning)을 활용해 대량의 비라벨(unlabeled) 상호작용 데이터로부터 일반적인 촉각 표현을 학습하는 것입니다. 이를 통해 로봇 손 전체의 촉각 정보를 하나의 잠재 벡터 표현(latent embedding)으로 압축하여, 이후 어떤 다운스트림 작업(task)에도 활용할 수 있도록 합니다. 이러한 접근은 딥러닝 기반 시각 인식 분야에서 자기 지도 표현 학습이 큰 성공을 거둔 것을 촉각 영역에 적용한 것으로, 복잡한 라벨링 없이도 모델이 촉각의 풍부한 특징들을 학습하도록 유도합니다.\n결과적으로, 이 연구의 동기는 손 전체에 분포된 촉각 센서로부터 얻는 방대한 정보를 효율적으로 해석하고 범용적으로 활용하기 위한 표현 학습 방법을 개발하는 데 있습니다. 특히 기존 연구인 “Sparsh: Self-supervised touch representations for vision-based tactile sensing”에서 손끝 카메라 기반 촉각 센싱에 대한 자기 지도 학습 기법이 제시되었는데, 본 논문은 그 철학을 이어받아 촉각 피부 센서라는 새로운 하드웨어 플랫폼에 적용하고 확장한 연구라고 할 수 있습니다. 아래에서는 본 논문의 주요 기여와 방법론, 실험 결과를 상세히 살펴본 후, 앞선 Sparsh 연구와의 기술적 연관성과 차이점을 비교해보겠습니다."
  },
  {
    "objectID": "posts/paper/2025-07-04-sparsh-skin.html#주요-기여-및-접근-방법",
    "href": "posts/paper/2025-07-04-sparsh-skin.html#주요-기여-및-접근-방법",
    "title": "📃Sparsh-Skin 리뷰",
    "section": "주요 기여 및 접근 방법",
    "text": "주요 기여 및 접근 방법\nSparsh-skin이라 명명된 본 논문의 접근법은, 알레그로(Allegro) 로봇 손에 장착된 Xela uSkin 자기 촉각 센서들로부터 손 전체의 촉각 상태를 표현하는 사전학습 인코더(pre-trained encoder)를 제시한 것입니다. 구체적으로, 손가락 끝, 마디, 손바닥 등 여러 부위에 분포된 다수의 uSkin 센서들로부터 얻는 신호를 한데 모아 풀-핸드(full-hand) 촉각 임베딩을 생성합니다. 이때 한 시점의 센서 값뿐 아니라 최근의 짧은 이력까지 입력으로 사용하여, 시간적 맥락(예: 미끄러짐 발생 추이 등)을 반영한 표현을 학습합니다. 입력에는 각 센서의 3차원 위치 정보까지 포함하여, 모델이 접촉 신호의 공간적 분포를 이해할 수 있도록 하였습니다. 이렇게 학습된 Sparsh-skin 인코더는 다양한 다운스트림 작업에서 특정 작업에 특화된 추가 학습 없이도 바로 사용되거나, 최소한의 미세조정(fine-tuning)만으로 우수한 성능을 낼 수 있는 범용 표현을 제공합니다.\n이 논문의 주요 기여를 정리하면 다음과 같습니다:\n\n손 전체를 덮는 자기 촉각 피부 센서를 위한 최초의 범용 표현 학습 모델 제시: 손바닥까지 포함한 다지점 촉각을 통합적으로 다루기 위해, 자체 개발한 Sparsh-skin 인코더 구조를 통해 풀-핸드 촉각 임베딩을 구현했습니다. 이는 기존에 손끝 위주의 촉각센서 연구를 손 전체로 확장한 것으로, 로봇 손의 섬세한 조작(dexterity) 능력을 향상시킬 수 있는 기반을 마련합니다.\n대규모 비라벨 데이터셋 수집 및 자기 지도 학습 적용: 연구진은 VR 원격조작을 통해 14가지 일상 물체(장난감, 도구 등)를 대상으로 쥐기, 밀기, 비비기, 회전, 누르기, 문지르기, 관절 운동 등 다양한 원자적 조작 행동을 수행하여 약 4시간 분량의 촉각 데이터를 수집했습니다. 이러한 다양한 접촉 경험을 활용해 라벨이 없는 상태에서 자기 지도 학습(self-supervised learning)으로 인코더를 사전학습시켰습니다. 특히 자기-증류(self-distillation) 기법을 도입하여, 교사-학생 네트워크 간 훈련으로 모델이 손상된 입력 데이터로부터도 일관된 표현을 추출하도록 유도했습니다.\n여러 벤치마크 과제를 통해 범용성 및 성능 입증: 사전학습된 Sparsh-skin 표현의 효용을 검증하기 위해, 상태 추정(state estimation)부터 정책 학습(policy learning)에 이르는 여러 다운스트림 과제에 적용해 보았습니다. 그 결과, 기존 방법 대비 41% 이상의 성능 향상, 엔드투엔드 학습 대비 56% 이상의 향상을 달성하였고, 데이터 효율(sample efficiency)도 크게 높아져 적은 양의 학습데이터로도 높은 성능을 발휘함을 보였습니다. 이는 Sparsh-skin 임베딩이 다양한 작업에 걸쳐 일반적이고 풍부한 촉각 특성을 함유하고 있음을 의미합니다.\n\n이러한 기여를 통해, 본 연구는 범용 로봇 촉각 지각을 향한 중요한 진전을 이루었습니다. 손 전체에 분포된 센서들의 고차원 신호를 저차원 벡터로 함축함으로써, 복잡한 촉각 정보를 효율적으로 처리하고, 로봇의 학습 및 제어에 활용할 수 있게 하였습니다."
  },
  {
    "objectID": "posts/paper/2025-07-04-sparsh-skin.html#모델-및-자기-지도-학습-방법론",
    "href": "posts/paper/2025-07-04-sparsh-skin.html#모델-및-자기-지도-학습-방법론",
    "title": "📃Sparsh-Skin 리뷰",
    "section": "💡 모델 및 자기 지도 학습 방법론",
    "text": "💡 모델 및 자기 지도 학습 방법론\n본 논문에서 제안하는 Sparsh-skin 인코더의 학습 방법은 자기 지도 학습(SSL)의 일종인 자기-증류(self-distillation)를 활용하는 것이 핵심입니다. 이를 구현하기 위해 교사(teacher) 네트워크와 학생(student) 네트워크의 구조를 사용하며, 두 네트워크는 동일한 인코더 아키텍처를 공유하지만 가중치 업데이트 방식에 차이를 둡니다. 교사 네트워크는 학생 네트워크의 과거 가중치를 이용하거나 지수 이동 평균(EMA)으로 업데이트되어 보다 안정된 출력을 제공하고, 학생 네트워크는 실제 학습을 통해 가중치가 갱신됩니다.\n훈련 시간마다 동일한 촉각 데이터에 대해 교사와 학생에 서로 다른 입력을 제공하는데, 교사에게는 완전한 원본 데이터 x를 입력하고, 학생에게는 여기에 잡음 또는 결손을 가한 변형 데이터 \\tilde{x}를 입력합니다. 구체적으로, 학생 입력 \\tilde{x}는 일부 촉각 신호를 마스킹(masking)하거나 노이즈를 추가하는 등의 오염된(corrupted) 데이터로 만들어, 학생 인코더가 불완전한 정보로부터도 의미 있는 표현을 추출하도록 도전합니다. 한편 교사 인코더는 온전한 데이터로부터 기준이 될 표현 E_{\\hat{\\theta}}(x)을 생성해 놓습니다. 학습 목표는 학생 인코더의 출력 표현 E_{\\theta}(\\tilde{x})가 교사의 출력 E_{\\hat{\\theta}}(x)와 가까워지도록 하는 것입니다. 이를 위해 두 출력 임베딩 간 거리를 측정하는 손실 함수(예: 코사인 거리 혹은 L2 노름)를 최소화하며 학생 네트워크를 학습시킵니다. 이렇게 하면 학생 네트워크는 부분적인 정보만으로도 전체 정보를 예측하도록 훈련되고, 결과적으로 강인하고 풍부한 표현을 얻게 됩니다.\n 그림 1: Sparsh-skin 자기 지도 학습 개요. 알레그로 로봇 손에 자기 촉각 피부 센서를 부착하여 전체 손가락과 손바닥에서 촉각 신호를 수집한다 (왼쪽). 교사 네트워크는 완전한 센서 입력 x를 받아 잠재 표현 E_{\\hat{\\theta}}(x)을 생성하고, 학생 네트워크는 일부 센서 신호가 제거되거나 노이즈가 추가된 오염된 입력 \\tilde{x}에 대해 표현 E_{\\theta}(\\tilde{x})을 출력한다 (오른쪽). 학생 네트워크는 자신의 출력을 교사 출력에 가깝게 예측하도록 학습됨으로써, 불완전한 입력에서도 의미 있는 전체 촉각 상태 표현을 얻도록 훈련된다. 아래 작은 예시 그림들은 이렇게 학습된 Sparsh-skin 표현을 활용하여 수행할 수 있는 다양한 다운스트림 과제들을 보여준다. (이미지 제공: 논문 저자)\nSparsh-skin 인코더의 입력은 로봇 손의 각 촉각 센서로부터 최근 짧은 시간 동안 수집된 신호 시퀀스입니다. 예를 들어 약 0.1초 이내의 짧은 시간 창(window) 동안 센서들이 출력한 정규화된 3축 힘 신호들을 모아 하나의 입력으로 구성합니다. 이는 순간적인 값보다 약간의 시간적 변화까지 고려함으로써, 정적 압력 분포뿐 아니라 마찰력 변화, 미끄러짐 등 동적 특징도 포착하기 위함입니다. 이전의 Sparsh 연구에서도 수십 밀리초 길이의 짧은 프레임 시퀀스(예: 80ms)를 토큰화하여 사용하면 일반화와 표현력 향상에 중요하다는 결과가 있었는데, Sparsh-skin 역시 이러한 Temporal Tokenization 개념을 자기 촉각 신호에 적용한 것입니다. 또한 Sparsh-skin의 입력에는 각 센서의 공간적 위치 정보(손 구조 내 3D 좌표)를 포함시켜, 인코더가 입력 신호의 공간적 맥락까지 고려하도록 했습니다. 예를 들어 같은 크기의 힘이라도 손바닥 중앙에서 감지된 것과 손가락 끝에서 감지된 것은 로봇이 취해야 할 대응이 다를 수 있으므로, 모델이 센서의 위치를 알 수 있게 한 것입니다.\n인코더 모델 구조 자체에 대해서는 논문에서 구체적인 구현 디테일을 제공하고 있는데, 기본적으로 위에서 설명한 교사/학생 프레임워크를 따르는 신경망 인코더입니다. 각 센서로부터 오는 시간 이력 신호는 개별적으로 임베딩된 후 손 전체 수준에서 통합됩니다. 이를 구현하는 한 가지 방식으로 추측되는 것은, 예를 들어 각 센서 정보를 하나의 토큰으로 보고 트랜스포머(Transformer) 인코더 구조를 사용하여 상호작용을 학습시키는 것입니다. 실제로 모델은 센서-레벨의 표현(sensor-level representation)을 학습한다고 언급하고 있으며, 이는 각 센서 신호를 저차원 특징으로 임베딩한 뒤, 이러한 임베딩들을 통합하여 손 전체의 표현을 만든다는 의미로 해석됩니다. 트랜스포머나 그래프 신경망(GNN) 같은 아키텍처를 활용하면 자연스럽게 각 센서의 위치나 인접한 센서 간 관계를 학습할 수 있을 것으로 보입니다. 다만 논문은 구체적인 아키텍처보다는 학습 전략(self-distillation)에 중점을 두어 서술하고 있으므로, 여기서는 핵심 아이디어 수준에서 이해하면 충분합니다.\n정리하면, Sparsh-skin의 방법론은 (1) 손 전체에서 얻은 짧은 시간 구간의 다중 촉각 신호와 센서 위치 정보를 인코더의 입력으로 사용하고, (2) 자기-증류 방식의 무라벨 사전학습을 통해 학생 네트워크가 부분 관찰에서도 전체 촉각 상태를 함축하는 표현을 내도록 훈련하며, (3) 이렇게 학습된 인코더를 고정 또는 미세조정하여 다양한 후속 작업에 활용하는 것입니다."
  },
  {
    "objectID": "posts/paper/2025-07-04-sparsh-skin.html#실험-설정-및-결과",
    "href": "posts/paper/2025-07-04-sparsh-skin.html#실험-설정-및-결과",
    "title": "📃Sparsh-Skin 리뷰",
    "section": "실험 설정 및 결과",
    "text": "실험 설정 및 결과\n연구진은 제안한 Sparsh-skin 인코더의 효과를 평가하기 위해 다양한 벤치마크 실험을 수행했습니다. 크게 나누어 보면 (a) 물리적 양상 추정 (예: 힘, 물체 위치 등)과 (b) 조작 정책 학습 두 범주로 실험을 구성했습니다. 사용된 로봇 플랫폼은 Shadow Allegro Hand로, 여기에 16개의 Xela uSkin 센서 패치를 장착하여 손가락 4개 전체와 손바닥 일부를 커버하였습니다. 각 센서 패치는 3축 힘을 감지하므로, 손 전체에서 3×16=48차원의 촉각 데이터가 시각적으로는 일종의 힘 분포 지도처럼 출력됩니다. 앞서 언급한 대로 연구진은 VR 기기를 이용해 원격 조작으로 데이터 수집을 진행했는데, 총 4시간 분량의 촉각 데이터 동안 14개의 서로 다른 일상 물체 및 장난감을 가지고 여러 가지 접촉 동작을 수행하였습니다. 예를 들어, 부드러운 공을 쥐었다 놓기, 블록을 손바닥에서 미끄러뜨리기, 병뚜껑을 비틀어 열기, 키보드를 누르기 등의 원초적 동작들이 포함되었습니다. 이러한 데이터는 완전히 라벨 없는 상태로 수집되었으며, Sparsh-skin 인코더의 사전학습에 사용되었습니다. 사전학습 후, 연구진은 아래와 같은 다운스트림 과제들에서 Sparsh-skin의 표현을 활용하여 성능을 측정하였습니다:\n\n힘(접촉 신호) 재구성 및 추정: 첫 번째로, 촉각 신호 자체를 복원하는 과제를 통해 인코더가 실제 힘 정보를 얼마나 보존하는지 확인했습니다. Sparsh-skin 인코더로부터 나온 잠재 벡터를 다시 원래 각 센서들의 신호로 복원하도록 오토인코더 데코더를 훈련하여, 복원된 촉각 지도를 시각화했습니다. 그 결과, 녹색 원으로 표시된 센서별 누르는 힘의 크기(원 크기에 비례)와 빨간 화살표로 표시된 전단(마찰) 방향(원래 센서 위치에서 어긋난 정도와 방향으로 표현)이 실제 접촉과 잘 부합함을 보였습니다. 이는 Sparsh-skin의 잠재 표현이 센서별 접촉력 (정규분력 + 전단력) 정보를 효과적으로 압축하고 있음을 시사합니다. 나아가, 별도의 절대 힘 보정 모델을 약간의 지도학습으로 학습하면, Sparsh-skin 표현으로부터 실제 뉴턴 단위의 힘 추정도 가능함을 보여주었습니다. 이러한 힘 추정 능력은 추후 로봇이 접촉력을 조절하거나, 미끄러짐을 예방하기 위해 힘 변화를 감지하는 등에 활용될 수 있습니다.\n손에 쥔 물체의 자세(pose) 추정: 두 번째로, 로봇 손이 잡고 있는 물체의 상대적인 자세를 촉각 정보만으로 추정하는 과제를 실험했습니다. 예컨대 손바닥 위에 놓인 원기둥 물체의 회전 각도나 위치 이동을 Sparsh-skin 표현으로부터 예측하는 것입니다. 이를 위해 연구진은 회귀-분류 혼합 방식(regression-by-classification)의 얕은 모델을 사용하여, Sparsh-skin 표현을 입력 받아 물체의 SE(3) 자세를 추론했습니다 (Sparsh 논문의 경우 2D 평면 상의 SE(2) 변환을 추정하는 실험이 있었고, 본 연구에서도 유사한 접근을 3차원으로 확장했을 것으로 보입니다). 그 결과 Sparsh-skin으로 학습한 표현이 물체의 회전 및 이동 정보를 상당 부분 내포하고 있음을 확인할 수 있었습니다. 이는 촉각만으로도 물체의 상대적인 위치 변화를 감지할 수 있음을 보여주며, 시각 정보 없이도 손아귀 내의 물체 추적이 가능할 잠재력을 시사합니다.\n플러그 삽입(정밀 조작) 정책 학습: 세 번째로, Sparsh-skin 표현이 실제 다지 손 조작 과제에서 정책(policy) 학습을 얼마나 도와주는지 평가했습니다. 실험으로 선택된 과제는 플러그 삽입으로, 로봇 손이 미리 쥐고 있는 플러그를 앞에 놓인 멀티탭 콘센트의 첫 번째 소켓에 정확히 꽂는 작업입니다. 이 과제는 손가락들로 플러그를 단단히 쥔 채, 손목과 손가락의 적절한 조합 움직임으로 플러그 핀을 소켓 홀에 맞춰 넣어야 하므로, 난이도가 높은 정밀 조작에 속합니다. 연구진은 이 작업에 대해 인간 원격조작 데모 여러 회를 수집한 후, 이를 이용해 확산 정책(Diffusion Policy) 알고리즘으로 모델 예측 제어 정책을 학습시켰습니다. 학습된 정책은 멀티 모달 입력을 사용하는데, 3대의 외부 카메라 영상과 손목 카메라 영상이 시각 입력으로 주어지고 여기에 Sparsh-skin 촉각 표현이 결합된 형태였습니다. 비교군으로는 촉각 정보를 사용하지 않고 오직 비전(vision) 입력만 사용하는 경우와, 촉각을 사용하되 end-to-end로 학습하는 경우 등을 설정했습니다. 평가 결과, Sparsh-skin 사전학습 표현을 사용한 경우가 가장 높은 성공률을 보였으며, 동일한 촉각 정보를 end-to-end로 학습한 정책이나 촉각을 배제한 정책에 비해 안정적이고 데이터 효율적으로 학습됨을 확인했습니다. 예를 들어, 마지막 단계에서 Sparsh-skin 표현을 쓴 정책은 시각 입력만으로 학습한 정책보다 성공률이 유의미하게 높고(촉각 미활용 대비), end-to-end 대비 학습 안정성 및 성공률 모두 향상되었음을 보고하였습니다. 이는 촉각 표현이 시각으로 부족한 미세 정렬 정보를 보완하고, 학습 공간을 줄여주어 정책 학습을 용이하게 만들었기 때문으로 해석됩니다.\n\n以上 세 가지 대표 실험을 통해, Sparsh-skin으로 사전학습된 촉각 임베딩이 여러 형태의 다운스트림 작업에서 유용함을 입증했습니다. 정량적인 성능을 요약하면, Sparsh-skin을 사용한 경우 기존의 최선의 방법 대비 평균 41% 이상 성능 향상이 있었고, 특정 작업에서는 최대 56% 향상이 관찰되었습니다. 예컨대 물체 자세 추정 정확도가 크게 높아졌으며, 플러그 삽입 성공률도 촉각 미사용 대비 상당히 향상되었습니다. 또한 학습 곡선을 비교하면, 사전학습된 표현을 사용한 경우 적은 양의 데이터로도 목표 성능에 도달함을 보여 표본 효율성(sample efficiency) 역시 개선되었음을 알 수 있습니다. 이는 Sparsh-skin 임베딩이 다양한 촉각 상황에 대해 일반화된 특징 표현을 제공하기 때문에 가능한 결과로, 각각의 과제마다 초반부터 유용한 특성을 추출하여 학습을 빠르게 진행할 수 있었던 것으로 볼 수 있습니다."
  },
  {
    "objectID": "posts/paper/2025-07-04-sparsh-skin.html#한계-및-향후-과제",
    "href": "posts/paper/2025-07-04-sparsh-skin.html#한계-및-향후-과제",
    "title": "📃Sparsh-Skin 리뷰",
    "section": "한계 및 향후 과제",
    "text": "한계 및 향후 과제\n본 연구는 손 전체 촉각 센서를 활용한 자기 지도 표현 학습의 가능성을 보여주었지만, 여전히 남아있는 한계점과 향후 발전시킬 방향이 존재합니다:\n\n절대적 힘/물리량에 대한 정밀 보정: Sparsh-skin 표현은 다양한 접촉 패턴의 상대적 차이는 잘 학습하지만, 실제 뉴턴 단위의 절대 힘 추정이나 절대적인 마찰 계수 추정 등에는 추가 보정이 필요할 수 있습니다. 자기 센서 출력은 환경 자계나 개별 센서 편차 등으로 drift가 있을 수 있어, 완전히 보정 없는 상태로는 절대값 예측에 한계가 있습니다. 향후에는 사전학습된 표현에 소량의 라벨된 데이터(예: 힘 센서 계측값)로 미세 보정을 가하여, 절대 물리량까지 정밀하게 추정하는 방향으로 발전시킬 수 있습니다.\n다양한 센서 및 모달리티와의 통합: 본 연구는 Xela사의 uSkin이라는 특정 자기 촉각 센서 하드웨어에 초점을 맞추고 있습니다. 다른 종류의 촉각 센서(예: 광학식 젤 촉각센서, 압전 센서 등)나 온도/진동과 같은 다른 접촉 감각까지 통합한 표현으로 확장하는 것도 과제입니다. 사실 Sparsh 계열의 다른 연구로 Sparsh-X라는 다중모달(multisensory) 촉각 표현을 개발한 예가 있는데, 여기서는 이미지, 힘, 진동, 음향 등의 신호를 함께 학습시켜 성능을 높였습니다. 향후 Sparsh-skin도 비전 카메라 정보나 소리 센서 등을 결합한 멀티모달 학습으로 발전시킨다면, 더욱 풍부한 촉각 지각 능력을 얻을 수 있을 것입니다.\n일반화 및 전이 학습: Sparsh-skin 인코더는 한 가지 로봇 손과 센서 세팅에 대해 학습되었습니다. 이를 다른 로봇 손(예: 형태가 다른 로봇 그리퍼)이나 다른 센서 배열에도 적용하려면 추가 연구가 필요합니다. 손 구조나 센서 분포가 바뀌면 센서 위치 임베딩 등 입력 표현을 조정해야 하며, 경우에 따라 전이 학습(transfer learning)이 필요할 수 있습니다. 미래에는 여러 형태의 손과 센서 데이터를 통합하여 학습하거나, 학습된 모델을 새로운 하드웨어에 도메인 적응시키는 연구도 이루어질 것으로 기대됩니다.\n실시간 제어와의 접목: 현재 Sparsh-skin 표현은 주로 오프라인 데이터를 활용한 학습 및 평가로 그쳐 있습니다. 향후에는 이 표현을 로봇의 실시간 제어 루프에 넣어, 피드백 제어나 모델 예측 제어(MPC)에서 바로 활용하는 방안도 고려될 수 있습니다. 예를 들어 손가락 미끄러짐 감지 후 즉각 그립 조정을 한다든가, 힘 제어 루프에 Sparsh-skin 피처를 반영하는 등의 응용이 가능할 것입니다. 이를 위해서는 인코더의 추론 속도 최적화와 실시간 신뢰도 평가 등이 뒷받침되어야 합니다.\n\n요약하면, Sparsh-skin 연구는 촉각 표현 학습 분야에 새로운 장을 열었으나, 절대적 물리량 추정, 다양한 센서 통합, 새로운 형태로의 일반화, 실시간 시스템 적용 등 앞으로 탐구해야 할 흥미로운 연구 과제가 남아 있습니다. 이러한 방향으로 지속적인 연구가 이뤄진다면, 장차 로봇이 사람 수준으로 풍부한 촉각지능을 갖추는 데 크게 기여할 것으로 보입니다."
  },
  {
    "objectID": "posts/paper/2025-07-04-sparsh-skin.html#sparsh-시각-기반-촉각-표현-학습-연구와의-연관성",
    "href": "posts/paper/2025-07-04-sparsh-skin.html#sparsh-시각-기반-촉각-표현-학습-연구와의-연관성",
    "title": "📃Sparsh-Skin 리뷰",
    "section": "💥 Sparsh (시각 기반 촉각 표현 학습) 연구와의 연관성",
    "text": "💥 Sparsh (시각 기반 촉각 표현 학습) 연구와의 연관성\n앞서 언급하였듯이, 본 논문의 Sparsh-skin은 2024년 CoRL에 발표된 Sparsh 연구의 연장선 상에 있습니다. Sparsh는 “Self-supervised touch representations for vision-based tactile sensing”라는 제목 그대로, 시각 정보를 활용하는 촉각 센서(예: GelSight, DIGIT와 같은 카메라 기반 촉각센서)를 위한 자기 지도 표현 학습 기법을 제시한 바 있습니다. Sparsh 연구의 배경에는, 최근 등장한 고해상도 촉각 카메라 센서들이 로봇 조작에 큰 도움이 되고 있지만, 각기 다른 센서마다 라이팅 조명, 젤 패턴, 카메라 특성 등이 달라 센서마다 별도의 모델을 만들어야 하는 비효율이 있다는 문제의식이 있었습니다. 또한 미끄러짐(slip) 감지나 접촉력 추정 등의 과제를 위한 레이블된 데이터 수집이 어려움도 존재했습니다. 이를 해결하고자 Sparsh에서는 다양한 촉각 카메라로부터 얻은 약 46만 장 이상의 촉각 이미지를 모아 합쳐서 통합된 자기 지도 학습을 수행하였습니다.\n구체적으로, Sparsh 연구진은 MAE(Masked Autoencoder), DINO(자기 증류), JEPA(Joint Embedding Predictive Architecture) 등 여러 자기 지도 학습 기법을 실험적으로 비교하여, 어떤 접근이 촉각 이미지 표현에 가장 효과적인지 분석했습니다. 그 결과 픽셀 공간에서 복원하는 MAE보다는, 잠재 표현 공간에서 예측하도록 학습하는 DINO나 I-JEPA와 같은 기법이 더 우수하다는 결론을 얻었고, 최종적으로 Sparsh-DINO와 Sparsh-IJEPA 모델이 가장 경쟁력 있는 성능을 보였습니다. 이들 모델은 사전학습을 거치지 않은 end-to-end 학습 대비 TacBench로 명명된 촉각 벤치마크에서 평균 95.1%라는 압도적인 성능 향상을 보였는데, 이는 사전학습의 효과가 매우 크다는 것을 단적으로 보여줍니다. TacBench에는 6가지의 다양한 촉각 과제(예: 접촉력 맵 복원, 미끄럼 여부 분류, 물체 식별, 조작 계획 등)가 포함되는데, Sparsh 사전학습 모델은 이들 여러 과제와 서로 다른 센서들에 대해 모두 일관되게 뛰어난 성능을 발휘하였습니다. 요컨대 Sparsh 연구를 통해, 시각 기반 촉각센서 분야에서도 거대한 데이터로 범용 촉각 표현을 학습하면 개별 문제에 일일이 학습하는 것보다 훨씬 효과적임이 입증된 것입니다.\n이러한 Sparsh의 철학과 성과는 Sparsh-skin으로 고스란히 이어졌습니다. 두 연구의 기술적 연속성과 차별점을 정리하면 다음과 같습니다:\n\n학습 철학의 연속성: 두 연구 모두 “촉각 데이터의 범용 표현(foundation representation)”을 목표로 합니다. 라벨이 부족한 촉각 영역에서 자기 지도 학습을 도입하여 사전학습된 인코더를 만들고, 이를 다양한 작업에 적용하는 큰 흐름이 Sparsh에서 Sparsh-skin으로 이어집니다. 즉, 시각 촉각이미지에서 자기 촉각피부 신호로 입력 modality만 달라졌을 뿐, 비지도 사전학습 → 다운스트림 전이의 구조는 동일한 철학입니다.\n센서 형태의 차이: Sparsh는 카메라 기반 촉각 센서(vision-based tactile) 즉, 이미지 형태의 촉각 데이터를 다루었습니다. 반면 Sparsh-skin은 자기장 기반 촉각 피부 센서로, 데이터가 다중 지점의 아날로그 신호 시퀀스 형태입니다. Sparsh에서는 이미지 패치를 마스킹하거나 하는 식으로 이미지 처리 기법이 활용되었고, Conv-NeXt나 ViT와 같은 비전 신경망 백본을 사용했습니다. Sparsh-skin에서는 시계열 센서 신호이므로, 시간 처리 및 센서 위치 임베딩 등 시계열+구조적 데이터 처리 기법이 필요합니다. 따라서 모델 아키텍처 관점에서 Sparsh는 CNN/Transformer 비전 백본이고, Sparsh-skin은 아마도 시계열 임베딩 + Transformer (혹은 GNN) 구조로 구현되는 등 차이가 있습니다. 또한 Sparsh는 한 번에 하나의 센서 이미지(손가락 한 개의 촉각 이미지)에 대해 동작하지만, Sparsh-skin은 손 전체의 여러 센서를 동시에 다룹니다. 즉 Sparsh는 여러 센서 종류에 대해 각각 적용 가능한 모델이었다면, Sparsh-skin은 여러 센서가 이루는 하나의 시스템을 한꺼번에 모델링한다는 차이가 있습니다.\n학습 데이터와 범위: Sparsh는 여러 연구팀이 공개한 여러 촉각 이미지 데이터셋을 통합 활용하여 총 66만장 가량의 이미지를 모았습니다. 여기에는 GelSight 센서로 누른 물체 데이터, DIGIT 센서로 문지른 데이터 등 다양한 상황이 포함되어, 한 가지 손가락 센서 이미지 내에서의 범용성을 키웠습니다. Sparsh-skin은 자체 수집한 4시간 분량의 멀티센서 시퀀스 데이터를 사용하였고, 이는 한 유형의 센서(uSkin)이지만 손의 여러 위치와 다양한 조작 동작을 아우르는 데이터를 담았습니다. Sparsh가 센서 간 범용성(다른 촉각 카메라들 모두에 통하는 모델)을 달성하려 했다면, Sparsh-skin은 손 내의 공간적 범용성(손가락~손바닥 어디에서 접촉이 일어나도 일관된 표현으로 통합)을 달성하려 했다고 볼 수 있습니다.\n자기 지도 방식 비교: 두 연구 모두 self-supervised 기법을 썼지만, 접근법에 약간 차이가 있습니다. Sparsh에서는 마스킹 후 복원(MAE)과 자기 증류(DINO), 예측 코딩(JEPA) 등을 폭넓게 시도하고 비교실험을 했습니다. Sparsh-skin에서는 그 중 자기 증류(self-distillation) 방식을 채택하여 교사-학생 네트워크 구조로 학습을 진행했습니다. 이는 Sparsh에서 발견된 “잠재 공간에서의 자기 지도 학습이 픽셀 복원보다 효과적”이라는 교훈을 바탕으로, Sparsh-skin에서도 latent representation을 맞추는 방향의 알고리즘을 선택한 것으로 볼 수 있습니다. 다만 Sparsh-skin은 입력의 일부를 마스킹하고 완전 입력으로부터 학습하는 구조로, MAE와 DINO 아이디어를 절충한 비대칭 자기 증류 형태라고 할 수 있습니다. Sparsh의 DINO는 동일한 이미지를 두 가지 증강하여 둘 다 인코더에 통과시켜 임베딩을 맞추는 대칭적 구조인데, Sparsh-skin은 완전 vs 손상 입력의 비대칭 구조라는 점이 차별화됩니다.\n다운스트림 과제 차이: Sparsh에서는 TacBench라는 벤치마크를 정의하여, 촉각 이미지로 할 수 있는 다양한 과제 (정상/전단력장 재현, 물체 식별, 접촉 여부 판별, 미끄럼 예측, 물체 포즈 추정, 조작 계획)을 평가했습니다. Sparsh-skin에서는 로봇 손 조작과 관련된 과제들 (힘 추정, 물체 자세 추정, 플러그 삽입 등)을 선정했습니다. Sparsh의 과제들이 주로 단일 촉각 센서의 범위에서 정의된 것이라면, Sparsh-skin의 과제들은 손 전체 협응이 필요한 좀 더 복잡한 조작까지 다루고 있습니다. 이는 센서 범위 확장에 따라 평가 범위도 확장된 것으로 볼 수 있습니다.\n성능 지표: 두 연구 모두 사전학습의 이점으로 큰 성능 향상을 얻었지만, Sparsh에서는 95%에 달하는 향상을 본 반면 Sparsh-skin에서는 40~56% 정도의 향상을 보고했습니다. 이 차이는 여러 가지 요인이 있을 수 있습니다. 예를 들어 Sparsh의 TacBench는 비교적 단순한 분류/회귀 과제들이고 end-to-end 학습 성능이 낮았던 반면, Sparsh-skin의 과제들은 이미 일부 vision 정보도 병합되는 등 기본 성능이 높은 편이라 개선 폭이 제한적일 수 있습니다. 그럼에도 여전히 두 경우 모두 사전학습 표현의 우수성은 명확히 입증되었습니다.\n\n결론적으로, Sparsh와 Sparsh-skin은 “촉각의 범용 표현 학습”이라는 공통된 비전을 가지고 있으며, 전자는 손가락 촉각카메라로, 후자는 손 전체 촉각피부로 각각 그 비전을 실현한 작업입니다. Sparsh를 통해 여러 촉각 이미징 센서 간에 통하는 표준 모델의 가능성을 보였다면, Sparsh-skin은 손 전체를 활용한 촉각 지각의 가능성을 열었습니다. 두 연구 모두 로봇 촉각 분야에서 데이터 중심의 학습 접근이 유효함을 보여주었고, 나아가 이러한 기법이 멀티모달 통합이나 더 복잡한 조작 기술로 확장될 수 있음을 시사합니다. 앞으로 Sparsh 시리즈의 축적된 지식을 바탕으로, 로봇이 더 영리하고 민첩하게 물체를 다룰 수 있는 촉각 지능을 갖추게 되길 기대해 봅니다.\n\n아래는 “Self-supervised perception for tactile skin covered dexterous hands” (Sparsh-skin) 논문과 “Sparsh: Self-supervised touch representations for vision-based tactile sensing” 논문의 주요 항목별 비교표입니다.\n\n\n\n\n\n\n\n\n항목\nSparsh (CoRL 2024)\nSparsh-skin (arXiv 2025)\n\n\n\n\n목표\n시각 기반 촉각 센서(GelSight, DIGIT 등)의 범용 표현 학습\n자기 기반 촉각 피부 센서(uSkin 등)의 손 전체 통합 표현 학습\n\n\n센서 유형\n비전 기반 촉각 센서 (이미지 형태)\n자기장 기반 촉각 피부 센서 (3축 힘 벡터, 다지점)\n\n\n입력 데이터\n이미지 (단일 손끝 센서)\n시계열 힘 신호 + 센서 위치 정보 (손 전체 16개 센서)\n\n\n학습 방식\n자기 지도 학습: MAE, DINO, I-JEPA 비교 실험\n자기 지도 학습: 비대칭 self-distillation (교사/학생 네트워크)\n\n\n모델 아키텍처\n비전 백본 (CNN, Vision Transformer)\n시계열/공간 정보 통합 인코더 (Transformer 또는 GNN 기반 추정)\n\n\n데이터 규모\n약 66만 개 이미지 (여러 공개 데이터셋 활용)\n약 4시간 분량의 로봇 손 다중 센서 시퀀스 (자체 수집)\n\n\n센서 다양성\n다양한 종류의 촉각 이미지 센서 통합\n하나의 센서 종류(uSkin), 손 내의 다양한 위치 커버\n\n\n공간 커버리지\n주로 손가락 끝 중심 (센서 1~2개)\n손가락, 마디, 손바닥까지 손 전체\n\n\n입력 변형 기법\n이미지 증강 (회전, 마스킹, 크롭 등)\n센서 마스킹, 노이즈 삽입 (손상된 입력 생성)\n\n\n다운스트림 과제\n접촉력 추정, 미끄럼 감지, 물체 식별, 포즈 추정 등 (TacBench)\n힘 복원, 물체 자세 추정, 정밀 삽입 조작 정책 등\n\n\n대표 실험 성능 향상\n기존 대비 최대 +95% 향상\n기존 대비 평균 +41% 향상 (최대 +56%)\n\n\n핵심 기여 요약\n비전 기반 촉각 표현의 범용화 및 평가 프레임워크 (TacBench) 제시\n손 전체 촉각 표현 학습과 자기 지도 기반 조작 응용 가능성 제시\n\n\n일반화 방향\n센서 종류 간의 범용 표현\n센서 위치·접촉 상황 간의 범용 표현\n\n\n주요 차별점\n다양한 이미지 센서들에 대한 Cross-sensor 표현\n손 전체의 시공간적 센서 통합 표현\n\n\n활용 예시\n단일 센서 기반 분류/회귀 과제\n멀티센서 기반 조작 정책, 물체 추적 등 정밀 작업\n\n\n\n\n요약하자면:\n\nSparsh는 “센서 종류의 다양성”을 아우르는 범용 촉각 표현에 초점을 맞추고,\nSparsh-skin은 “손 전체 위치의 다양성”을 고려한 통합 촉각 지각을 추구합니다.\n\n두 논문은 입력 modality와 센서 환경이 다르지만, 모두 촉각 표현을 self-supervised 방식으로 학습하여 여러 작업에 전이시키는 방식을 공통적으로 채택하고 있습니다.\n참고문헌:\n\nAkash Sharma et al., “Self-supervised perception for tactile skin covered dexterous hands”, arXiv preprint 2505.11420 (2025)\nCarolina Higuera et al., “Sparsh: Self-supervised touch representations for vision-based tactile sensing”, CoRL 2024 (arXiv:2410.24090)\nAkash Sharma et al., “Tactile Beyond Pixels: Multisensory Touch Representations for Robot Manipulation (Sparsh-X)”, arXiv preprint (2025) (멀티모달 촉각 표현 확장 연구)"
  },
  {
    "objectID": "posts/paper/2025-09-11-human2sim2robot.html",
    "href": "posts/paper/2025-09-11-human2sim2robot.html",
    "title": "📃Human2Sim2Robot 리뷰",
    "section": "",
    "text": "Paper Link\nHomepage\nCode Link"
  },
  {
    "objectID": "posts/paper/2025-09-11-human2sim2robot.html#배경-인간-시연을-활용한-로봇-학습의-도전",
    "href": "posts/paper/2025-09-11-human2sim2robot.html#배경-인간-시연을-활용한-로봇-학습의-도전",
    "title": "📃Human2Sim2Robot 리뷰",
    "section": "2.1 배경: 인간 시연을 활용한 로봇 학습의 도전",
    "text": "2.1 배경: 인간 시연을 활용한 로봇 학습의 도전\n로봇에게 섬세한 물체 조작(dexterous manipulation)을 가르치는 전통적인 방법 중 하나는 모방 학습(imitation learning, IL)입니다. 그러나 효과적인 모방 학습을 위해서는 사람이 로봇을 직접 조종(텔레오퍼레이션)하거나 특수 장비(예: 센서 장갑, VR 장치 등)를 착용해 다수의 고품질 시연 데이터를 수집해야 하는데, 이러한 과정은 매우 많은 노력과 시간이 필요합니다. 실제 연구 사례들을 보면, 하나의 작업에 수백에서 수천 개에 이르는 시연 데이터가 요구되었으며, 이는 로봇 학습의 확장성 측면에서 큰 걸림돌이었습니다.\n한편, 사람이 맨손으로 물체를 다루는 영상은 비교적 쉽게 얻을 수 있고, 일반인도 직관적으로 시연할 수 있다는 장점이 있습니다. 예를 들어 스마트폰 카메라로 일상 동작을 녹화하는 것만으로도 시연 데이터를 모을 수 있습니다. 그러나 이러한 인간-객체 상호작용 비디오를 로봇 학습에 직접 활용하기는 어렵습니다. 가장 큰 이유는, 비디오에는 로봇이 따라 할 명시적인 행동 레이블(어떤 로봇 명령을 수행해야 하는지)이 없으며, 더구나 인간 손과 로봇 손의 형태 차이(embodiment gap) 때문에 사람 손동작을 로봇에 그대로 매핑하기도 힘들기 때문입니다. 사람 손에 비해 로봇 손은 관절 구성과 운동 한계가 다르므로, 사람의 동작을 똑같이 흉내 내는 것은 종종 불가능하거나 비효율적입니다.\n기존 접근법 중 하나로, 인간 손 포즈 추정 기술을 사용해 비디오의 매 프레임마다 사람 손의 3D 관절각을 추출하고, 이를 로봇 손의 관절각으로 재타겟팅(retargeting)하여 로봇 행동으로 변환하려는 시도가 있었습니다. 그러나 이 방법은 두 가지 큰 문제에 직면합니다. 첫째, 최근 3D 손 추적 기법들이 발전했음에도 불구하고, 일반 RGB-D 영상에서는 손가락이 물체에 가려지거나 센서 노이즈가 존재하여 완벽한 추정을 보장하기 어렵습니다. 미세한 추정 오류도 누적되면 로봇 손가락 끝의 목표 위치가 어긋나게 되고, 로봇 관절 명령으로 변환할 때 부정확한 결과를 초래합니다. 둘째, 설령 인간 손의 포즈를 정확히 알아내더라도, 사람과 로봇의 형태 차이 때문에 해당 포즈를 로봇 손으로 구현하지 못하거나(역기구학 해가 없음) 그 작업에 부적절한 로봇 자세가 되는 경우가 많습니다. 특히 여러 손가락으로 동시에 힘을 가하는 다지(多指) 조작에서는, 부정확하거나 불안정한 재타겟팅 동작이 접촉 힘 불균형을 일으켜 작업 수행을 어렵게 만듭니다. 요컨대, 인간 시연과 로봇 행동 간에 정확한 1:1 대응을 강요하는 기존 모방 학습 방식으로는 사람-로봇 체화 차이를 극복하기 어렵고, 한두 개의 시연으로는 성공 확률이 매우 낮았습니다.\n강화학습(Reinforcement Learning, RL)은 이러한 문제를 해결할 대안으로 주목받아 왔습니다. RL은 로봇이 스스로 시행착오를 겪으며 자기 몸체에 맞는 최적의 행동을 학습하도록 하기 때문에, 인간과 로봇의 차이를 직접 메우는 대신 로봇 고유의 정책을 찾아낼 수 있습니다. 하지만 RL 역시 현실 로봇에 바로 적용하기엔 난관이 있습니다. 원하는 작업마다 적절한 보상 함수를 설계해야 하고, 많은 시행횟수가 필요해 물리 로봇으로 시도하기엔 비현실적이죠. 이를 위해 시뮬레이션 환경에서 학습하고 실제에 옮기는 Sim-to-Real 기법이 발전해 왔지만, 보상 설계의 어려움과 시뮬레이터와 현실 간 차이를 줄이는 문제 등이 남아있습니다.\n이러한 배경에서 2025년 발표된 “Crossing the Human-Robot Embodiment Gap with Sim-to-Real RL using One Human Demonstration” 논문은, 인간 시연 한 개만으로 로봇의 다지 조작 정책을 학습시키는 혁신적인 프레임워크 Human2Sim2Robot을 제안합니다. 이 방법은 한 번의 인간 RGB-D 비디오 시연으로부터 핵심 정보를 추출하여 시뮬레이터 상에서 RL로 정책을 학습하고, 이를 제로샷(zero-shot)으로 실환경에 이전함으로써 사람-로봇 체화 간극을 극복합니다. 놀랍게도 이 과정에서 별도의 웨어러블 장치나 텔레오퍼레이션 데이터, 수십 개의 시연 모음이 전혀 필요하지 않으며, 작업별로 보상을 일일이 튜닝할 필요도 없습니다. 본 리뷰에서는 Human2Sim2Robot의 핵심 기술 내용을 깊이 있게 설명하고, 기존 연구들과의 차별점을 비교하며, 이 접근법의 기여와 향후 로봇공학/강화학습 분야에 주는 의미를 분석하겠습니다."
  },
  {
    "objectID": "posts/paper/2025-09-11-human2sim2robot.html#human2sim2robot-프레임워크-한-개-시연으로-sim-to-real-정책-학습",
    "href": "posts/paper/2025-09-11-human2sim2robot.html#human2sim2robot-프레임워크-한-개-시연으로-sim-to-real-정책-학습",
    "title": "📃Human2Sim2Robot 리뷰",
    "section": "2.2 Human2Sim2Robot 프레임워크: 한 개 시연으로 Sim-to-Real 정책 학습",
    "text": "2.2 Human2Sim2Robot 프레임워크: 한 개 시연으로 Sim-to-Real 정책 학습\n\n\n\n\nHuman2Sim2Robot 프레임워크 개요. 이 시스템은 현실-시뮬레이션-현실(real-to-sim-to-real)로 이어지는 RL 학습 과정을 통해, 인간 시연 하나만으로도 로봇 다지 조작 정책을 배웁니다. 사람의 RGB-D 시연 영상에서 (1) 물체의 6-자유도(6-DoF) 자세 궤적과 (2) 조작 시작 직전의 손 자세를 추출하여, 이를 시뮬레이션 강화학습에 활용합니다. 시뮬레이터 안에서 로봇은 해당 물체 궤적을 따라가도록 훈련되며(보상 설계), 초반 탐색 단계에서는 인간 시연에서 얻은 손의 초기 자세를 사용해 효율을 높입니다. 이렇게 학습된 정책은 도메인 랜덤화 등을 통해 현실에 바로 적용 가능하도록 일반화되어 있으며, 실제 Kuka 7자유도 로봇팔과 Allegro 다지 로봇손으로 구성된 시스템에서 추가 튜닝 없이 곧바로 성공적으로 동작했습니다. 아래에서는 이 프레임워크의 각 구성 요소를 단계별로 자세히 살펴보겠습니다.\n\n\n2.2.1 인간 시연 처리: 객체 궤적 추출과 손 포즈 재현\nHuman2Sim2Robot의 첫 단계는 단 하나의 인간 시연 영상을 기계가 이해할 수 있는 형태로 가공하는 것입니다. 논문에서는 RGB-D 카메라(깊이 정보를 포함한 카메라)를 이용해 테이블 위에서 사람이 목표 작업을 수행하는 단일 시연을 녹화하였습니다. 이때 카메라는 내부 파라미터(intrinsics)와 위치가 미리 보정(calibration)되어 있어, 영상 픽셀 좌표를 물리 좌표로 정확히 변환할 수 있는 상태입니다.\n\n디지털 트윈 환경 구성: 시연 영상을 분석하기에 앞서, 연구팀은 로봇이 학습할 시뮬레이션 환경을 현실과 유사하게 만드는 작업을 수행했습니다. 이를 위해 휴대용 3D 스캐닝 앱(예: Kiri Engine, 3D Scanner App)을 이용하여 작업에 사용되는 물체와 장면(환경)의 정밀한 3D 메쉬(mesh) 모델을 얻었습니다. 단 몇 분 정도의 스캔 노력으로, 테이블 위 정적 환경(박스, 냄비, 건조대 등)과 대상 물체(예: 상자, 물컵, 접시)의 형상을 모두 복제한 디지털 트윈을 구축한 것입니다. 이렇게 해두면 시뮬레이터에서 현실과 동일한 지오메트리로 학습시킬 수 있어, Sim-to-Real 간 차이를 줄이는 데 도움이 됩니다.\n물체 6D 자세 궤적 추출: 시연 영상으로부터 객체의 움직임 경로를 정확히 알아내는 것이 핵심입니다. 논문에서는 이를 위해 FoundationPose라는 사전 학습된 객체 자세 추정 모델을 활용했습니다. 입력으로는 앞서 획득한 물체의 3D 메쉬 모델과, 영상 각 프레임에서의 물체 마스크(물체가 차지하는 픽셀 영역)가 주어집니다. 물체 마스크는 최신 분할 모델인 SAM (Segment Anything Model) 2를 이용해 자동으로 얻었습니다. FoundationPose는 이러한 정보를 바탕으로, 시연 내내 물체의 6-자유도(평면 위치 x-y, 높이 z, 롤-피치-요 각도) 자세를 추적합니다. 그 결과 각 시간 스텝마다 물체의 위치와 방향으로 이루어진 궤적 {O}_{1:T}를 얻을 수 있습니다 (T는 시연 길이). 이 객체 자세 궤적은 사람이 이 작업에서 물체를 어떻게 이동시켰는지에 대한 목표 시나리오를 정의해주며, 로봇에게는 무엇을 해야 하는지 알려주는 역할을 하게 됩니다.\n인간 손 조작 초기 자세 추출: 다음으로, 한 번의 시연 중에서 “조작 직전”의 인간 손 모양과 위치를 포착합니다. 연구진은 오픈소스 모델 HaMeR를 사용해 매 프레임의 인간 손 3D 포즈(손가락 관절 각도 및 손바닥 위치)를 추정했습니다. HaMeR는 RGB 영상으로부터 손 관절을 추정하는 모델인데, 더 정확도를 높이기 위해 깊이 영상 정보도 활용했습니다. 구체적으로, 각 프레임의 RGB로 추정한 손 모델을 ICP (Iterative Closest Point) 정합 방법으로 해당 깊이 점들과 정교하게 맞추어, 손가락 위치를 보정하였습니다. 이렇게 얻은 인간 손 포즈 시퀀스 {H}_{1:T} (MANO 손 모델 파라미터 형태로 표현됨)에서, 논문은 특별히 pre-manipulation hand pose, 즉 조작 시작 직전 손 자세를 찾아냅니다. “조작 시작” 시점을 찾기 위해 물체 궤적을 활용하는데, 물체의 속도가 처음으로 일정 임계값을 넘는 시점 t을 찾고, 그보다 약간 이전 시간 t-\\Delta (예컨대 몇 프레임 전)을 조작 직전 순간으로 정의합니다. 이 시점에는 손이 막 물체를 잡기 시작하기 바로 직전의 상태일 것입니다. 해당 프레임에서의 인간 손 관절 각도들과, 손바닥의 기준점(논문에서는 중지의 첫 마디 관절 부분)을 추출하면 이것이 인간의 조작 시작 자세입니다. 다시 말해, 사람이 물체에 접촉하기 직전 어떻게 손가락을 벌리고 어떤 위치에 손을 놓았는지를 나타냅니다.\n로봇 손으로 초기 자세 재타겟팅: 이제 추출된 인간 손의 pre-manipulation 자세를 로봇 손의 초기 자세로 옮겨야 합니다. 사람과 로봇의 손은 형태가 다르므로 단순 일대일 대응으로는 힘듭니다. 논문에서는 이를 위해 두 단계 IK(역기구학) 알고리즘을 사용했습니다. 우선, 인간 손에서 뽑은 중지(knuckle) 관절의 위치와 방향에 최대한 맞도록 로봇팔과 손목을 이동시킵니다. KUKA 팔의 관절들을 움직여 로봇 손의 중지 관절이 인간 손 중지 관절 위치로 오게 하고, 손바닥 면의 방향도 정렬시키는 것입니다 (아주 약간의 오프셋은 고려했다고 합니다). 다음으로, 로봇 손가락 관절들을 움직여 로봇 손가락 끝이 인간 손가락 끝 포인트와 각각 최대한 겹치도록 만듭니다. 예를 들어 인간의 엄지, 검지, 중지 끝 위치에 로봇의 해당 손가락 끝이 오도록 로봇 손의 각도를 조정합니다. 이러한 2단계 IK를 통해 로봇의 pre-manipulation 손 자세를 구하게 되는데, 이는 로봇팔의 손목 위치/자세와 로봇손의 관절각 전체를 포함하는 구성입니다. 이 방법은 비교적 간단한 절차이지만, 인간 손의 초기 포즈를 로봇에 충분히 근사하게 옮겨주어 로봇 입장에서 자연스러운 초기 조건을 만들어냅니다. 더욱이 이 과정은 한 순간의 자세만 맞추면 되므로, 앞서 말한 프레임별 재타겟팅에서 발생하는 누적 오류 문제를 피할 수 있습니다.\n두 가지 핵심 추상화: 요약하면, Human2Sim2Robot은 한 개의 인간 시연에서 (1) 물체의 목표 이동 경로와 (2) 로봇의 초깃자세라는 두 가지 핵심 정보를 추출한 것입니다. 이들은 사람 시연의 복잡한 시공간 데이터를 로봇 학습에 유용한 방식으로 추상화한 것이라 볼 수 있습니다. 물체 자세 궤적은 작업에 대한 객체 중심의 목표를 정의해 주며, 이는 로봇과 인간 손의 차이에 독립적(embodiment-agnostic)인 목표로서 사용됩니다. 즉, 로봇이 사람과 다른 방식으로 움직이더라도, 결과적으로 이 궤적대로 물체를 옮기기만 하면 작업을 성공한 것으로 볼 수 있다는 뜻입니다. 한편 pre-manipulation 로봇 손 자세는 학습 과정에서 로봇에게 유리한 초기 조건을 제공합니다. 이를 통해 로봇은 탐색 초기부터 인간이 사용한 유사한 물체 파지(grasp) 자세에서 시작할 수 있고, 따라서 학습 정책이 자연스럽게 시연과 비슷한 전략으로 수렴하도록 돕습니다. 중요한 점은 이 두 가지 정보가 지침(guidance) 역할을 할 뿐, 우리가 로봇에게 인간의 매 순간 동작을 강요하지 않는다는 것입니다. 논문의 표현에 따르면, 시연은 “과업 명세와 지도 (task specification and guidance)”로만 사용되고, 로봇은 인간처럼 행동하는 것이 유리할 때는 그렇게 하도록 유도되지만, 로봇 자신의 형상에 맞지 않을 때는 과감히 다른 동작으로 우회할 수 있게 허용됩니다. 이러한 설계 철학 덕분에, 사람-로봇 체화 차이가 큰 경우에도 로봇이 독자적으로 최적 행동을 찾는 자유도가 보장됩니다.\n\n\n\n2.2.2 시뮬레이션에서의 정책 학습과 보상 설계\n이제 준비된 디지털 트윈 시뮬레이션 환경에서 강화학습을 통해 로봇 조작 정책(policy)을 학습합니다. 학습에는 NVIDIA의 IsaacGym 시뮬레이터를 사용했으며, 앞서 스캔한 장면 메쉬(테이블 및 고정 배경)와 물체 메쉬를 불러와 현실과 동일한 물리 환경을 구축했습니다. 로봇 모델 역시 실제와 동일한 7-자유도 KUKA 팔과 16-자유도 Allegro 로봇 손으로 설정되었습니다[30]. 요약하면, 시뮬레이터 속에 현실 실험실의 축소판을 만든 셈입니다 (그림 6 참조).\n\n강화학습 알고리즘: 정책 학습에는 Proximal Policy Optimization (PPO) 알고리즘을 활용하였습니다. 로봇 정책은 반복적인 에피소드 훈련을 거쳐 업데이트되며, 최종적으로 로봇 관절 제어 명령을 출력하는 신경망으로 얻어집니다. 정책의 관찰 상태로는 로봇 자체의 관절 상태(프로프리오셉션)와 실시간 물체의 6D 자세가 주어집니다. 이는 실제 배치 시와 동일하게, 카메라를 통해 추정된 물체의 위치/자세를 입력으로 사용함을 의미합니다. 초기 학습 시에는 에피소드마다 로봇의 초기 자세를 앞서 구한 pre-manipulation 로봇 손 자세로 리셋(reset)하여 시작합니다. 이렇게 하면 로봇이 초반부터 물체를 잡을 준비가 된 상태에서 탐색을 시작하므로, 맨 처음부터 엉뚱한 곳을 탐색하느라 시간을 허비하지 않습니다. 연구에 따르면 이 초기화 전략이 RL 학습의 안정성과 속도를 높여주며, 후에 비교 실험에서도 더 나은 성능을 보였습니다.\n보상 함수 설계: Human2Sim2Robot의 중요한 특징 중 하나는 작업별 보상 함수를 추가로 설계/튜닝하지 않았다는 것입니다. 대신 하나의 일반적 보상 함수를 모든 작업에 사용했는데, 그것이 바로 “객체 궤적 추적 보상”입니다. 구체적으로, 시뮬레이션에서 매 시점의 보상 r_t를 로봇이 물체를 목표 궤적에 얼마나 잘 맞게 따라가고 있는가로 정의합니다. 수식으로는 물체의 현재 자세와 인간 시연에서 그 시점에 해당하는 목표 자세 사이의 거리(distance)를 측정하여, 거리가 작을수록 높은 보상을 주는 형식입니다. 여기서 거리란 단순한 유클리드 거리 개념을 넘어서, 물체의 위치와 방향 오차를 동시에 반영할 수 있도록 고안되었습니다. 논문에서는 물체의 내부 기준 프레임에 여러 개의 기준 점(anchor point)을 두고, 현재 물체와 목표 물체 사이 각 기준점의 상대 위치 차를 계산하는 방식을 사용했습니다. 예를 들어 물체의 표면 모서리나 중심 등에 점을 찍어두고, 로봇이 물체를 들고 움직일 때 이 점들이 목표 위치에 얼마나 가까이 갔는지로 보상을 주는 것입니다. 이렇게 하면 물체의 회전 오차도 점들의 배치에 따라 자연스럽게 거리로 반영됩니다. 만약 어떤 축으로 회전이 중요한 물체라면 그 축 방향으로 거리를 크게 느껴지도록 점을 배치하고, 반대로 회전 대칭적인 물체라면 불필요한 방향 오차는 무시하도록 점을 배치할 수도 있습니다. 이러한 기준점 설정은 물체 유형에 따라 한 번 정해주면 되고, 본 연구에서는 대부분의 작업에서 동일한 파라미터를 사용하여 특별한 보상 튜닝 없이도 잘 동작함을 보였습니다. 그림 4에서는 이 객체 궤적 추적 보상의 개념도를 시각화하고 있는데, 간단히 말해 물체의 현재 자세가 시연 궤적 어디 쯤 와 있는지를 측정하여 그에 따라 보상을 주는 체계입니다. 이 보상 설계의 장점은 RL 연구에서 흔히 겪는 과업별 보상 설계의 어려움을 크게 줄였다는 점입니다. 연구진은 “특정 물체가 회전 대칭인 경우를 제외하고는, 단 하나의 보상 공식과 파라미터로 매우 다양한 작업들을 모두 학습시킬 수 있었다”라고 강조합니다. 이는 본 방법의 범용성을 보여주는 대목으로, 새로운 작업마다 보상을 새로 만들 필요 없이 인간 시연 궤적만 있으면 동일한 방법으로 학습 가능함을 시사합니다. 실제로 논문에서 실험한 작업들(아래 설명될 접시 꽂기, 상자 피벗 등)에 대해 추가적인 보상 조정 없이도 모두 성공적인 정책을 얻을 수 있었습니다.\n인간 시연을 통한 간접 학습: 앞서 언급했듯, 이 RL 학습은 인간의 시연 동작 자체를 모방하는 것이 아니라 시연을 통해 정의된 목표(물체 궤적)를 로봇 방식으로 달성하도록 하는 것입니다. 사람과 로봇의 움직임은 다를 수 있지만, 결과적으로 물체가 동일한 경로를 따라 움직이면 성공으로 간주됩니다. 논문에서도 “우리는 인간 시연 행동을 흉내 내기보다, 물체의 움직임에 관심을 둔다”라고 명시하고 있습니다. 인간의 pre-manipulation 포즈는 초기에 대략적인 가이드로만 쓰이고, 최종 정책은 로봇의 체형에 맞게 자율적으로 최적화됩니다. 예를 들어 사람이 물체를 잡을 때 오른손으로 집었다고 해서 로봇도 꼭 같은 방향, 같은 손가락 순서로 잡아야 하는 것은 아닙니다. 로봇은 자기 관절 구조상 더 편리한 방식이 있다면 그렇게 해도 좋고, 대신 물체 경로만 맞추면 되는 것이죠. 이렇듯 인간 시演은 결과 중심의 스케치 역할만 하고, 구체적인 행동 구현은 RL이 알아서 찾아내도록 하는 것이 Human2Sim2Robot의 철학입니다. 이는 인간-로봇 형태 차이를 극복하는 핵심 아이디어로, 사람 전략을 맹목적으로 따라하기보다 참고만 함으로써, 오히려 성공률 높은 전략을 로봇이 스스로 발견할 수 있게 해줍니다.\n도메인 랜덤화와 강인한 학습: 시뮬레이터에서 학습한 정책을 현실에 바로 적용하려면, 시뮬레이션-현실 간 갭(차이)을 줄여야 합니다. 이를 위해 논문은 학습 중 다양한 도메인 랜덤화(domain randomization) 기법을 사용했습니다. 구체적으로, 시뮬레이션에서 일정 단계마다 무작위로 환경과 로봇의 물리 파라미터를 변화시켰습니다. 예를 들어, 중력 값을 약간씩 바꾸거나, 마찰 계수나 물체 무게를 임의로 조정하고, 로봇 관절의 질량/감쇠/강성 등의 모수를 무작위 스케일링했습니다. 또한 매 시뮬레이션 스텝마다 5% 확률로 물체에 작은 무작위 힘(외부 충격)을 가하여, 예기치 않은 방해가 생기도록 했습니다. 관찰되는 물체 자세와 로봇 상태, 그리고 로봇의 출력 액션에도 센서 노이즈에 해당하는 가우시안 잡음을 추가하여, 카메라 오차나 제어 신호 오차도 견딜 수 있도록 만들었습니다. 이렇게 광범위한 랜덤화를 주면, 정책은 다양한 조건에서 물체를 궤적대로 옮기는 연습을 하게 되므로 현실에서 마주칠 변동에도 강건해집니다. 실제 논문에서도 이러한 랜덤화와 함께, 이미지 대신 6D 포즈를 관찰로 사용하는 낮은 차원의 상태 표현 등이 합쳐져 로봇 정책의 적응성과 강인성이 향상되어 제로샷 실환경 이전이 가능했다고 분석합니다.\n\n\n\n2.2.3 Zero-Shot Sim-to-Real: 학습된 정책의 실환경 수행\n시뮬레이션에서 충분히 학습을 마친 정책은 별도 미세조정(fine-tuning) 없이 그대로 실제 로봇에 이식되어 테스트됩니다. 연구진은 학습된 정책을 물리 로봇에 배치하기 위해 다음과 같은 제어 파이프라인을 구성했습니다:\n\n우선, 실험 환경에 ZED stereo camera(깊이 카메라)를 설치하여 실시간으로 물체의 6D 자세를 30Hz로 추적합니다. 여기에도 학습 때와 동일한 FoundationPose 모델을 사용하여, 물체의 실제 위치와 방향을 지속적으로 추정했습니다.\n로봇 정책(신경망)은 이 물체의 현재 자세와 로봇의 관절 상태를 입력으로 받아들여, 15Hz의 주기로 다음 행동을 출력합니다. 정책의 출력 행동은 추상적인 명령(예: 원하는 속도나 가상 목표 위치 등)으로 볼 수 있습니다.\n이 출력은 바로 로봇 관절로 보내는 대신, Geometric Fabric Controller라는 중간 제어기로 전달됩니다. Geometric Fabrics는 로봇 운동을 매끄럽고 안정적으로 제어하기 위한 알고리즘으로, 정책이 내린 명령을 받아 60Hz로 로봇 관절의 목표 위치/속도 (PD 타겟)를 계산합니다. 이 단계는 로봇의 동역학을 고려하여 부드러운 경로를 생성함으로써, RL 정책이 바로 토크를 내보내는 것보다 안전하고 현실적이게 해줍니다.\n마지막으로, 로봇 내장 저층 PD 제어기가 200Hz의 고주파수로 이러한 목표 값에 맞게 각 관절 모터에 제어를 가합니다. 요약하면, 정책은 고수준 의사결정을 하고, 중간 컨트롤러가 이를 구체적인 관절 명령으로 바꾸어 실제 로봇을 구동하는 체계입니다.\n\nHuman2Sim2Robot의 결과 중 주목할 점은, 이렇게 이식된 정책이 한 번도 로봇 실물로 연습해보지 않고도 (zero-shot) 성공적으로 작업을 수행했다는 것입니다. 이는 앞서 말한 시뮬레이션 도메인 랜덤화 덕분에 정책이 현실 물리의 변동성까지 포괄했기 때문입니다. 또한 시뮬레이터 자체를 실제와 매우 가깝게 (스캔된 환경, 정확한 물체 모델, 실제와 동일한 로봇 매니퓰레이터 모델) 구성한 것도 주효했습니다. 연구진은 KUKA LBR iiwa 14 팔과 Allegro Hand로 이루어진 실제 로봇 시스템에서, 학습된 정책을 바로 실행하여 높은 성공률을 얻었음을 보고했습니다. 이는 로봇공학에서 사람이 한 번 시연한 것을 보고 (추가 보정 없이) 로봇이 따라 해내는 학습된 행동의 실증적인 사례로서 큰 의미를 갖습니다. 아래에서는 구체적으로 어떤 작업들을 실험했고 어떤 성과가 있었는지 살펴보겠습니다.\n\n\n2.2.4 다양한 조작 과제에서의 실험 결과\n연구에서는 Human2Sim2Robot의 성능을 검증하기 위해 다양한 유형의 물체 조작 작업에 대해 실험을 진행했습니다. 실험 환경은 테이블 위에 정적인 장애물(상자, 큰 냄비, 접시 건조대 등)을 배치하고, 세 가지 이동 물체(스낵 박스, 주전자/피처, 접시)를 사용했습니다. 이 환경은 가정이나 일상에서 있을 법한 상황들을 축소 모사한 것으로, 테이블 위에서 물체를 집어 들어 올리거나, 밀거나, 젖혀서 세우거나, 다른 용기에 넣는 등의 시나리오를 포함합니다.\n단일 스킬 과제: 우선 기본 조작 기술 단위의 과제들로 파지/이동(grasping), 비파지 조작(non-prehensile manipulation), 외력 활용 조작(extrinsic manipulation) 등의 사례가 선정되었습니다.\n\n예 1: 스낵박스 밀기(push) – 얇은 직사각형 상자를 옆의 고정된 박스 쪽으로 밀어서 접촉시키는 과제 (밀기: non-prehensile).\n예 2: 스낵박스 피벗(pivot) – 상자를 세워 세우기 힘드니, 고정된 박스 모서리를 지렛대 삼아 스낵박스를 옆으로 기울여 세우는 과제 (extrinsic manipulation, 환경을 활용).\n예 3: 피처(주전자) 집어 올리기 – 손잡이를 잡고 들어올려 옆 냄비 위로 옮겨 물 따르듯이 위치시키는 과제 (prehensile grasp + 이동).\n예 4: 접시 들어올리기 – 납작한 접시를 평면에서 잡아 들어올리는 과제 (prehensile, 난이도 높은 파지).\n그 외에도 접시 밀기, 접시 세우기 등 다양한 단일 스킬 작업이 포함되었습니다.\n\n다중 단계 과제: 더 나아가, 논문은 이러한 기본 기술을 연속적으로 두세 개 조합해야 하는 복합 작업에도 도전했습니다.\n\n예 5: 스낵박스 푸시-피벗 – 상자를 먼저 밀어 박스에 붙인 후, 이어서 피벗 동작으로 세우는 2단계 작업.\n예 6: 접시 피벗-리프트-랙 – 테이블에 놓인 납작한 접시를 먼저 벽(박스)에 기대 세운 후, 잡아서 들어올리고, 최종적으로 건조대 틀 사이에 끼워 세워 두는 3단계 작업. 이는 접시 세우기 → 접시 들어올리기 → 접시 꽂기 순서의 복합 과제입니다.\n예 7: 접시 리프트-랙 – 접시를 바로 들어올려 건조대에 꽂는 2단계 작업 (세우지 않고 바로 들어올려 꽂기).\n\n이러한 복합 시나리오는 인간에게도 여러 단계의 사고가 필요한 비교적 복잡한 작업입니다. Human2Sim2Robot에서는 각 복합 작업 역시 단 하나의 인간 시연 영상으로부터 학습되었습니다. 예컨대 접시 pivot-lift-rack 작업의 경우, 사람이 처음부터 끝까지 접시를 세워 잡아 옮겨 꽂는 일련의 동작을 한번 보여준 것이고, 로봇은 그 시연의 물체 궤적 (접시의 움직임 경로)에 맞춰 3단계 행동을 모두 해내야 하는 것입니다. 특히 접시처럼 넓고 얇은 물체는 바닥에 놓인 상태로는 집기 어려워 먼저 세워야 하고, 건조대에 끼울 때 각도를 잘 유지해야 하는 등 정교한 제어가 필요합니다. 이러한 복잡한 작업을 한 번의 시연으로 배우게 한 것은 본 연구의 난이도 높은 도전이었습니다.\n실험 결과: 저자들은 Human2Sim2Robot으로 학습한 RL 정책을 위 7가지 실제 작업에 대해 실행하고, 성공률을 측정하였습니다. 각 작업마다 10회의 독립 실행을 시험한 결과, 제안된 방법이 매우 높은 성공률을 보였음을 보고합니다. 구체적인 수치는 작업별로 제시되진 않았지만, 모든 작업을 평균하여 볼 때 Human2Sim2Robot 정책은 대부분의 시도를 성공적으로 수행했습니다.\n연구의 핵심 관심은 Human2Sim2Robot이 기존 방식보다 얼마나 우수한가이므로, 세 가지 대표적인 기존 접근법과 비교 실험이 이루어졌습니다. 비교 대상은 다음과 같습니다:\n\nReplay (오픈루프 재생): 인간 시연 영상의 모든 프레임마다 손 추정을 하고 로봇 포즈로 재타겟팅하여 얻은 로봇의 전체 관절 궤적을 그대로 따라 재생하는 방법입니다. 이는 일종의 open-loop(피드백 없음) 실행으로, 사람이 했던 그대로 로봇이 해보는 것입니다.\nObject-Aware Replay (객체 위치 보정 재생): 상기 Replay와 동일하나, 현실에서 초기 물체의 위치가 시연 때와 약간 다를 수 있으므로, 물체의 초기 변환 차이를 보정하여 로봇 궤적을 약간 이동/회전시키는 개선된 방법입니다. 이는 기존 연구인 PEP이나 OKAMI 등의 아이디어를 참고한 것으로, 시연 대비 물체 위치가 어긋나도 로봇이 궤적을 따라갈 수 있도록 합니다.\nBehavior Cloning (행동 복제): 폐루프 정책이긴 하나, RL이 아닌 지도학습 기반 모방학습으로 정책을 학습하는 방법입니다. 한 개의 시연만으로는 학습이 어려우니, 제안 방식에 맞추어 시연 하나를 증강하여 데이터셋을 구성했습니다. 물체 초기 위치를 여러 방식으로 무작위 샘플링하고, 그 상황에서 Object-Aware Replay를 실행하여 30개의 로봇 시연 데이터를 생성했습니다. 이것을 정책 학습용 데이터로 삼아, Diffusion policy와 같은 IL 알고리즘으로 행동 복제 학습을 한 것이 BC baseline입니다. 쉽게 말해, 한 개 시연을 여러 번 재배치해가며 데이터로 부풀린 후, 전통적 모방학습을 한 것입니다.\n\n비교 결과, Human2Sim2Robot의 RL 정책은 이들 모든 기준선보다 현저히 높은 성공률을 기록했습니다. 평균적으로 볼 때, Human2Sim2Robot은 객체-인식 오픈루프 재생보다 55%포인트 높은 성공률을 보였고, 행동 복제보다 68%포인트 높았습니다. 가장 단순한 Replay와 비교하면 무려 67%포인트나 향상되었다고 합니다. 그림 8의 실험 결과 그래프를 보면, 거의 모든 작업에서 RL 정책이 압도적으로 높은 성공 비율을 달성했음을 알 수 있습니다.\n이러한 큰 성능 차이의 이유에 대해 저자들은 상세한 분석을 제공합니다. 우선 Replay의 경우, 오픈루프로 사전에 녹화된 궤적을 그대로 따라가기 때문에 현실에서는 여러 실패 요인이 발생했습니다. 시연 시와 초기 조건이 조금만 달라도 (예: 물체 위치 오차, 마찰 차이 등) 보정이 없으면 로봇 동작은 빗나가게 마련이고, 한번 어긋나면 피드백이 없어 끝까지 실패하게 됩니다. Object-Aware Replay는 초기 위치는 보정했지만 여전히 개선되지 않은 세 가지 문제를 겪었습니다: (1) 앞서 논한 손 추정 및 재타겟팅 오류가 누적되어 정확도가 떨어지고, (2) 근본적인 사람-로봇 형상 차이로 인한 부조화가 있고, (3) 피드백이 없어 중간에 수정되지 못한다는 점입니다. 그나마 이 방법은 물체 위치 정도는 반영했기에, 정밀도가 크게 요구되지 않는 작업(예: 상자를 대략 밀어세우는 정도)에서는 부분 성공을 보이기도 했지만, 조금 복잡한 작업부터는 실패했습니다.\nBehavior Cloning(BC)의 경우, 데이터 30개로 폐루프 정책을 학습했음에도 불구하고 결과가 저조했습니다. 이는 사용된 데이터 자체가 재타겟팅을 통해 생성된 것이라 품질이 낮고, 개수도 충분치 않기 때문입니다. 로봇 행동의 학습이 누적 오차에 취약하여, 데이터 분포에서 조금만 벗어나면 금세 잘못된 동작으로 이어지는 과적합 현상이 나타났습니다. 요컨대, 시연 한두 개로 행동을 모방 학습하는 데에는 근본적인 한계가 있음을 보여줍니다. 한편 Human2Sim2Robot의 RL 정책은 시연을 모방하지 않고 과업 목표를 성취하도록 학습되었기 때문에, 중간 상황이 달라져도 목표 지향적으로 적응하며 과업을 완수할 수 있었습니다. 예컨대 접시 세우기-꽂기와 같은 복잡한 작업에서, 모방 학습 기반 방법은 시연 때와 조금만 달라져도 실패했지만, RL 정책은 사람 시연의 전략을 응용하되 로봇 나름대로 세분화하여 접시를 끝까지 꽂아넣는 데 성공했습니다. 그림 9는 이러한 예시로, 접시 pivot-lift-rack 작업에서 로봇이 인간 시연을 참고하면서도 자기 손에 맞게 변형된 전략으로 과업을 수행하는 모습을 보여줍니다. 결국 Human2Sim2Robot은 인간처럼 행동하도록 강요하기보다, 인간의 힌트를 이용해 로봇 스스로 학습하게 함으로써 훨씬 높은 성능을 낼 수 있음을 증명한 것입니다.\n또한 저자들은 각 구성 요소의 기여를 검증하기 위한 추가 실험(Ablation)도 수행했습니다. 예를 들어, 객체 궤적 보상 대신 단순히 최종 목표 위치까지만 가도록 하거나, 궤적을 희미하게 만드는 변형들을 비교했는데, 그 결과 본래의 세밀한 궤적 보상이 학습을 가장 빠르고 안정적으로 만들고 성공도 높였다고 보고합니다. 특히 접시와 같이 중간 단계를 필요로 하는 작업에서, 최종 목표만 주었을 경우 로봇은 지름길을 택하려다가 (접시를 바로 들어올리려다) 실패했지만, 인간 시연의 전체 경로를 보상으로 제시했을 때 비로소 피벗 후 집기라는 해법을 찾아냈습니다. 이는 인간 시연이 단순히 결과뿐만 아니라 과정상의 중요한 힌트를 제공하며, RL이 그것을 잘 활용하고 있음을 보여줍니다. 마찬가지로 초기 손 자세 안내가 없을 때와 있을 때도 비교한 결과, 초기 안내가 있으면 학습 초기 성능과 수렴 속도가 크게 향상되고 더 나은 최종 성과를 얻었습니다. 이러한 실험들은 Human2Sim2Robot 프레임워크의 구성 요소들이 각각 유의미한 역할을 하고 있음을 뒷받침합니다."
  },
  {
    "objectID": "posts/paper/2025-09-11-human2sim2robot.html#기존-연구와의-비교-및-차별점",
    "href": "posts/paper/2025-09-11-human2sim2robot.html#기존-연구와의-비교-및-차별점",
    "title": "📃Human2Sim2Robot 리뷰",
    "section": "2.3 기존 연구와의 비교 및 차별점",
    "text": "2.3 기존 연구와의 비교 및 차별점\nHuman2Sim2Robot은 “한 개의 인간 시연으로 다관절 로봇손 정책을 학습”했다는 점에서 매우 도전적인 목표를 달성한 최초의 사례로 꼽힙니다. 저자들의 주장에 따르면, 이렇게 적은 시연 데이터로 실제 로봇에 바로 실행 가능한 폐루프 정책을 만든 것은 본 연구가 처음입니다. 개별 구성요소 차원에서는 이전에도 유사한 시도가 없었던 것은 아닙니다. 예를 들어, 객체 중심 보상을 사용한 로봇 학습이나, 인간-로봇 포즈 재매핑을 시도한 연구들이 있습니다. 그러나 이들 기존 연구는 대개 시뮬레이션 안에 국한되었거나, 혹은 정책이 아닌 오픈루프 제어 수준에 머물렀으며, 더 나아가 다수의 시연이 필요하거나 단순 파지 동작에만 국한되는 등 한계가 있었습니다. 반면 본 논문은 이러한 요소들을 통합하면서도 실환경 폐루프 제어까지 구현하고, 비파지 및 다단계 작업으로 범위를 넓혔다는 데 큰 차별점이 있습니다.\n모방학습(IL) vs. Human2Sim2Robot: 전통적인 IL은 앞서 설명했듯, 인간 시연과 로봇 행동 간 엄격한 매핑이 전제됩니다. 이는 시연 데이터가 많고 조건이 잘 맞으면 효과적이지만, Human2Sim2Robot이 다루는 시연 1개, 인간-로봇 형태 상이 상황에서는 적용이 사실상 불가능했습니다. IL을 억지로 적용하려 한 baseline 실험(Replay, BC 등)은 모두 낮은 성공률로 이를 방증합니다. Human2Sim2Robot은 “시연 따라하기” 대신 “시연 참고하여 RL로 배우기”라는 패러다임 전환을 보여줍니다. 특히 사람이 손가락 하나하나 어떻게 움직였는지까지 베끼려 하지 않고, 결과적인 객체 움직임만 학습 목표로 삼은 점은 사람-로봇 간극을 우회적으로 해소한 뛰어난 아이디어입니다. 그 결과, 사람의 전략을 부분적으로 모방하되 로봇 스스로 최적 행동을 찾게 함으로써, IL 방식이 겪는 오류 누적과 실패를 피할 수 있었습니다. 또한 IL은 일반적으로 많은 시연이 있어야 안정적인데, 본 방법은 하나의 시연으로도 충분하도록 설계되었다는 점에서 현실적인 장점을 갖습니다. 실제 현장에서 로봇에게 새 작업을 가르칠 때, 수십 번 텔레옵으로 반복하는 것보다 한 번 시범을 보여주는 것이 훨씬 수월할 것이기 때문입니다.\n유사 연구들과의 비교: 최근 인간 비디오를 활용해 로봇 다지 조작을 배우려는 연구들이 몇 가지 등장했습니다. 예를 들어 2024년의 HuDOR (Human-to-Dexterous-Object-Rewards) 방법은 한 개의 인간 영상으로부터 물체 움직임을 추적하여 객체-지향 보상을 만들고, 이를 이용해 실제 로봇에서 온라인 RL 파인튜닝을 수행한 바 있습니다. HuDOR 역시 사람-로봇 형태 차이 문제를 인지하고, 물체 움직임에 집중한 보상으로 RL을 한 점은 유사합니다. 그러나 HuDOR는 실제 로봇을 1시간 가량 직접 학습시켜야 했고, 다루는 작업도 단일 단계 동작 위주였습니다. 반면 Human2Sim2Robot은 시뮬레이션 상에서 충분히 학습하고 현실에 제로샷 투입함으로써, 실제 로봇의 고된 온라인 학습을 피했습니다. 또한 더 다양한 작업 (훨씬 복잡한 다단계 작업 포함)에서 검증되었다는 점에서도 앞선 결과라 할 수 있습니다. 그 밖에 From One Hand to Multiple Hands (2022)라는 연구는 단일 사람의 시연을 여러 로봇 손에 일반화하는 모방학습을 다뤘지만, 이것은 텔레옵으로 수집된 다수의 시연을 전제하였고 Human2Sim2Robot처럼 RL을 활용하지는 않았습니다. X-Sim: Cross-Embodiment Learning via Real-to-Sim-to-Real (2025) 등의 동시대 연구들도 체화 차이 문제를 다루고 있으나, Human2Sim2Robot만큼 시연 데이터의 극단적 축소 (1개)와 실환경 검증을 모두 보여준 사례는 드뭅니다.\n베이스라인 기법과의 차별점: 앞 절의 비교실험 결과가 잘 보여주었듯, Human2Sim2Robot의 RL 정책은 폐루프 제어로 환경 변화에 대응한다는 점에서 오픈루프 재생(Replay)과 근본적으로 다릅니다. 또한 사람 시연을 학습의 형태로 활용한다는 점에서, 단순히 시연 궤적을 조정해 사용하는 Object-Aware Replay와도 차별화됩니다. Behavior Cloning과 비교하면, 같은 데이터라도 강화학습을 통해 성능을 끌어올리고 오차 누적 문제를 해소했다는 점이 다릅니다. 요컨대, Human2Sim2Robot은 “시연 → 데이터 → 모방”의 기존 틀을 “시연 → 목표 추출 → RL 학습”으로 바꾸어 성능과 범용성을 모두 얻은 것이며, 이는 로봇 학습 커뮤니티에 새로운 방향을 제시합니다.\n마지막으로, Human2Sim2Robot이 특별한 장비에 의존하지 않는다는 실용적 강점도 있습니다. 과거에는 모션 캡쳐 시스템이나 특수 센서 장갑 등 정밀한 장비로 사람 시연 데이터를 수집하곤 했지만, 본 논문은 그냥 RGB-D 카메라로 촬영한 영상만으로도 충분히 학습이 가능함을 보였습니다. 이는 데이터 수집 비용을 크게 낮추고, 비전문가도 로봇 학습에 참여할 수 있는 길을 열어줍니다. 또한 시연 영상을 통한 학습이 성공하려면 컴퓨터비전 기술 (물체 추적, 손 추적)이 필수적인데, 최근 몇 년간 해당 분야가 크게 발전했기에 (SAM, FoundationPose, HaMeR 등) 가능해진 것이기도 합니다. Human2Sim2Robot은 최신 CV 기술과 RL을巧妙하게 결합하여 얻은 결과로, 서로 다른 AI 하위 분야의 시너지를 보여주는 사례라고도 할 수 있습니다."
  },
  {
    "objectID": "posts/paper/2025-09-11-human2sim2robot.html#기여-및-의의-로봇공학과-강화학습-분야에서의-가능성",
    "href": "posts/paper/2025-09-11-human2sim2robot.html#기여-및-의의-로봇공학과-강화학습-분야에서의-가능성",
    "title": "📃Human2Sim2Robot 리뷰",
    "section": "2.4 기여 및 의의: 로봇공학과 강화학습 분야에서의 가능성",
    "text": "2.4 기여 및 의의: 로봇공학과 강화학습 분야에서의 가능성\nHuman2Sim2Robot의 기여는 여러 측면에서 주목할 만합니다. 우선, 로봇 학습 방식의 간소화와 인간 친화적 데이터 수집입니다. 오직 하나의 시연 영상으로 다관절 로봇손의 복잡한 조작을 익힐 수 있다는 것은, 로봇에게 새 기술을 가르치는 진입장벽을 크게 낮춥니다. 이제까지는 전문가가 긴 시간 공들여 데이터를 모아야 했다면, 앞으로는 일반 사람이 휴대폰으로 시범을 하나 보여주고도 로봇을 학습시킬 수 있는 방향으로 나아갈 수 있습니다. 이는 로봇의 학습 비용 절감과 적응력 향상으로 이어져, 가정용 서비스 로봇이나 작업장 협동 로봇 등에서 새로운 작업을 빠르게 가르치는 데 활용될 수 있습니다. 특히 집안일처럼 사람은 쉽게 시연할 수 있지만 로봇에 일일이 프로그래밍하기 어려운 작업들(예: 식기 세척기 적재, 옷 개어넣기 등)에 응용 가능성이 높습니다. 사람이 시연한 영상을 통해 로봇이 자율적으로 그 과업을 터득할 수 있다면, 로봇 활용 범위는 비약적으로 넓어질 것입니다.\n둘째로, 이 논문은 강화학습과 모방학습의 장점을 결합한 새로운 프레임워크를 제시했습니다. 기존에는 시연을 이용하면 주로 BC같은 모방학습을 하고, 강화학습은 별도로 방대한 상호작용을 통해 배우는 식이었습니다. Human2Sim2Robot은 시演으로부터 보상과 초기조건을 구성하여 RL에 자연스럽게 녹여냄으로써, 적은 시연데이터 + 강화학습의 강인함이라는 두 마리 토끼를 잡았습니다. 이는 강화학습 분야에서도 흥미로운 성과로, 향후 다른 형태의 인간 힌트(예: 언어설명, 시연 몇 개 등)를 보상 설계나 탐색 가이드로 활용하는 연구들에 영감을 줄 수 있습니다. 또한 보상 엔지니어링 없이 시연으로 과업을 명세하는 기법은, 보상 설계가 난해한 문제들(예: 창의적 동작, 사회적 상호작용 등)을 풀 때 유용한 방향일 수 있습니다.\n셋째, 실세계 복잡한 작업에서의 성공적 Sim-to-Real을 시연해 보였다는 점입니다. 특히 다수의 비간단한 조작(비파지, 환경 활용)과 다단계 작업을 한 프레임워크로 모두 해결한 것은 놀라운 결과입니다. 로봇이 단순 잡기뿐 아니라 벽을 활용해 물체를 세우고, 여러 동작을 연달아 수행하는 수준까지 도달한 것은, 범용적인 조작 능력에 한 걸음 다가간 것으로 볼 수 있습니다. 이는 향후 로봇이 사람과 같이 주변 환경을 활용하고, 연속된 업무를 수행하는 복합 스킬 학습에도 본 방법을 확장할 수 있음을 시사합니다. 실제 논문에서도 “주전자로 액체 붓기, 상자를 벽에 기대 기울이기, 접시를 건조대에 꽂기” 등 다양한 실제 과업을 예로 들며, Human2Sim2Robot 정책의 범용적 실행 능력을 강조합니다. 이러한 성과는 로봇공학 커뮤니티에 Sim-to-Real 강화학습의 가능성을 한층 확신시켜줍니다. 과거에는 시뮬레이터에서 배운 복잡한 다지 동작이 실제에서 과연 될까 회의적이었으나, 이 논문은 적절한 방법을 쓰면 가능함을 보여준 것이죠.\n넷째, Human2Sim2Robot은 로봇의 체화(embodiment)를 적극 고려한 학습의 중요성을 부각시켰습니다. 사람의 데이터를 쓰면서도 사람처럼 움직이게 하지 않고, 로봇 고유의 해결책을 찾게 한 것이 주효했습니다. 이는 다른 형태의 로봇 (예: 다리가 있는 로봇 vs 사람 걷기 영상) 간에도 적용될 수 있는 일반 개념입니다. 예를 들어 인간의 걷는 영상을 네 발 로봇에 가르친다면, 다리를 두 개만 쓰는 대신 네 개를 다 사용하도록 유도하는 방식으로 응용해볼 수 있습니다. 즉, Cross-Embodiment Learning의 한 사례로서, 이 논문은 시연 행동의 결과(목표)에 집중하고 구체적 구현은 로봇에게 맡기는 접근이 성과가 있음을 보여주었습니다.\n물론 한계와 향후 과제도 있습니다. 첫째로, 현재 방법은 단일 로봇 손, 단일 인간 시연, 단일 작업에 초점을 맞추고 있습니다. 논문에서도 향후 연구로 양손 조작(bimanual)이나 여러 로봇이 협력하는 경우, 혹은 여러 작업을 한꺼번에 학습하는 멀티태스킹 확장 등을 언급하고 있습니다. 한 개 시연으로 여러 스킬을 배우게 하거나, 두 손을 동시에 제어하는 문제는 난이도가 더욱 높지만, Human2Sim2Robot의 아이디어를 확장해나갈 수 있는 흥미로운 방향입니다. 둘째, 시연 영상 처리의 신뢰성 문제입니다. 본 연구는 전적으로 비디오에서 추출한 물체/손 포즈에 의존하므로, 만약 비디오 품질이 낮거나 물체 추적이 실패하면 학습에 지장이 생깁니다. 실제로 저자들은 시연 영상에서 물체나 손이 크게 가려져 추적이 실패하면 정책이 엉뚱한 방향으로 수렴할 수 있다고 보고합니다. 이는 입력 데이터의 한계로 인한 실패 모드로, 향후 멀티뷰 카메라 사용이나, 사람이 부분 교정해주는 인터페이스 등으로 개선될 수 있을 것입니다. 셋째, 사전 스캔과 모델 준비에 약간의 수작업이 필요합니다. 이번 연구에서는 물체와 환경을 3D 스캔하고, 또 물체 메쉬를 준비하여 포즈 추적에 사용했습니다. 이러한 과정은 예컨대 가정집 로봇이 일일이 하기에는 번거로울 수 있습니다. 하지만 현재 3D 스캐닝 기술도 자동화가 발전 중이고, 범용 객체 인식 모델이 더 좋아지면 필수 요건이 완화될 가능성이 있습니다. 끝으로, 실험 범위로 보자면 다룬 물체들이 비교적 단단하고 하나의 부피를 가진 것들이었습니다. 옷감이나 액체, 관절 있는 도구 등 변형체나 복합 물체에 대해서도 이 접근이 유효할지는 추가 연구가 필요합니다. 논문은 이러한 향후 과제로 변형 또는 관절 구조를 가진 물체로의 확장을 언급하며, 이는 추후 이 방법의 적용 범위를 넓혀 줄 것이라고 전망합니다.\n요약 및 결론: “한 번의 인간 시연으로 Sim-to-Real RL”이라는 Human2Sim2Robot의 개념은, 로봇 학습 분야에 새로운 가능성을 제시한 중요한 성과입니다. 이 연구를 통해 우리는 인간의 시연을 로봇 학습에 활용하는 더욱 똑똑한 방법을 보았습니다. 과거처럼 무조건 따라하게 하지 않고, 인간 시연을 분석해 로봇이 이해할 형태의 과제로 정의해주니, 로봇은 자기 몸에 맞춰 그 과제를 수행할 방법을 스스로 터득했습니다. 그 결과 적은 데이터로도 높은 성능을 내고, 복잡한 실제 작업까지 실행에 옮길 수 있었습니다. 이러한 접근은 향후 다양한 로봇 플랫폼과 작업들에 응용될 수 있을 것이며, 로봇이 사람에게서 배우는 자연스러운 상호작용의 토대를 마련해줍니다. Human2Sim2Robot은 인간-로봇 협력의 간극을 한층 좁힌 혁신으로서, 강화학습과 로봇공학 발전의 교차점에서 큰 의미를 가지며, 향후 많은 후속 연구를 촉발할 것으로 기대됩니다."
  },
  {
    "objectID": "posts/paper/2025-09-10-dextrah-rgb.html",
    "href": "posts/paper/2025-09-10-dextrah-rgb.html",
    "title": "📃DextrAH-RGB 리뷰",
    "section": "",
    "text": "Paper Link\nHomepage"
  },
  {
    "objectID": "posts/paper/2025-09-10-dextrah-rgb.html#논문의-핵심-요약",
    "href": "posts/paper/2025-09-10-dextrah-rgb.html#논문의-핵심-요약",
    "title": "📃DextrAH-RGB 리뷰",
    "section": "2.1 논문의 핵심 요약",
    "text": "2.1 논문의 핵심 요약\nDextrAH-RGB는 다지(多指) 로봇 손을 이용해 다양한 물체를 잡는(dexterous grasping) 문제를 오로지 RGB 카메라 영상만으로 해결한 최신 연구입니다. 이 시스템은 스테레오 카메라로부터 얻은 두 장의 RGB 영상 입력만으로 로봇 팔-손 시스템(7자유도 로봇 팔 + 16자유도 다지 로봇 손)을 엔드투엔드(end-to-end)로 제어하여 물체를 파지할 수 있습니다. 핵심 아이디어는 시뮬레이션에서 강화학습으로 학습된 교사(teacher) 정책을 활용해, 카메라 입력 기반의 학생(student) 정책을 모방 학습으로 훈련시키는 이단계 학습 절차입니다. 먼저 물체의 상태정보 등을 직접 관측할 수 있는 교사 정책을 안전한 동작 공간에서 강화학습으로 훈련하고, 이후 이 교사 정책을 통해 RGB 영상만 보는 학생 정책을 시뮬레이션에서 DAgger 알고리즘으로 학습시킵니다. 이를 통해 얻은 최종 시각-운동 정책(visual-motor policy)은 복잡하고 동적인 다지 손 파지 작업을 RGB 입력만으로 수행하며, 시뮬레이션에서 학습한 그대로 제로샷으로 실제 로봇에 이식(sim-to-real)될 수 있음을 보여주었습니다. 저자들의 주장에 따르면, DextrAH-RGB는 세계 최초로 순수 RGB 카메라 입력 기반 다지 손 파지 정책의 견고한 시뮬레이션-현실 전이를 달성한 사례입니다. 또한 이 정책은 훈련 중 보지 못한 새로운 형태의 물체나 다양한 재질·텍스처, 조명 조건 변화에도 일반화하여 높은 파지 성공률을 보였다고 보고됩니다. 요약하면, 이 논문은 시뮬레이션만으로 학습된 RGB 기반 정책을 실제 로봇에 적용하여 안전하고 신뢰성 있게 다양한 물체를 파지하는 방법을 제시했으며, 이는 향후 복잡한 조작 기술이나 거시적인 픽셀-투-액션(pixels-to-action) 모델 개발을 위한 모듈이자 데이터 소스로 활용될 수 있는 잠재력이 있습니다."
  },
  {
    "objectID": "posts/paper/2025-09-10-dextrah-rgb.html#연구의-배경-및-필요성",
    "href": "posts/paper/2025-09-10-dextrah-rgb.html#연구의-배경-및-필요성",
    "title": "📃DextrAH-RGB 리뷰",
    "section": "2.2 연구의 배경 및 필요성",
    "text": "2.2 연구의 배경 및 필요성\n로봇 분야에서 사람 손처럼 다양한 물체를 쥐는 능력은 가장 중요하면서도 오랫동안 해결이 어려웠던 도전 과제입니다. 다지 로봇 손의 잠재력을 충분히 활용하려면, 보지 못한 새로운 물체에도 일반화할 수 있고 주변 환경 변화에도 강인하며, 가정이나 작업장에서 마주치는 광범위한 물체들에 모두 적용 가능한 센서로 동작해야 합니다. 기존의 여러 연구와 솔루션이 제시되었지만, 아직까지 일반적인 해법을 만들기는 어려웠습니다.\n특히 최근에는 시뮬레이션 환경에서의 강화학습과 도메인 랜덤화(domain randomization) 기법 등의 발전으로, 시뮬레이터에서 학습한 정책을 현실 로봇에 성공적으로 이전(simtoreal)하는 사례들이 늘고 있습니다. 시뮬레이션을 활용하면 대량의 학습 데이터를 안전하게 생성할 수 있고, 시각 센서와 로봇 자체센서(proprioception) 정보를 모두 활용하는 반응적 정책 훈련이 용이해졌습니다. 그러나 이러한 도구를 사용한 현재의 접근법들에도 한계가 존재합니다. 대부분의 기존 시스템은 시각 입력부터 제어까지 직접 연결하는 엔드투엔드 정책 학습을 피하고, 문제를 단계로 분리(factorize)하는 방식을 택했습니다. 예를 들어, 복잡한 파지 문제를 정적인 파지 자세를 찾는 키네마틱 문제로 환원하여 접근하는 것이 일반적이었습니다. 이러한 계획 기반 방법들은 평균적인 물체에는 효과적이지만, 실시간으로 연속적 대응이 어렵기 때문에 예기치 않은 외란이나 처음 보는 특이한 형상의 물체에는 대응하지 못하는 문제가 있었습니다.\n한편, 연속적인 시각 피드백 기반의 파지 제어 연구들도 존재하지만, 고화질 RGB 영상을 대량으로 시뮬레이션하기엔 제약이 있기 때문에 주로 깊이(depth) 센서나 점군(point cloud) 데이터를 활용해 왔습니다. 깊이지도(depth map)는 물체의 기하학적 형태 파악에 유리하고 시뮬레이터에서도 비교적 구현이 쉬워 많이 사용되었지만, 반투명하거나 투명한 물체에는 취약하고 실제 IR 기반 깊이카메라의 노이즈 문제도 있습니다. 요컨대, 기존 방법들은 속도나 다지 손의 섬세함, 혹은 깊이지도 의존성 등에서 한계를 보여왔으며, 완전히 RGB 카메라만으로 동작하는 범용적인 다지 손 파지 정책은 개발되지 못했습니다. 이러한 빈틈을 메우기 위해 본 연구에서는 RGB 기반의 연속적이고 반응적인 파지 정책을 제안한 것입니다.\n또한 관련 연구들을 살펴보면, 전통적인 방법들은 물체-손 간 접촉 안정성 분석 등을 통한 그립 품질 지표를 최적화하였으나 정밀한 집게질(pinch grip) 등에 국한되거나 정확한 물체 3D 모델이 필요하다는 한계가 있었습니다. 데이터 기반 접근으로 방대한 그립 데이터셋을 활용한 학습도 시도되었지만, 대부분 깊이/점군 정보에 의존하여 학습하거나 결과 검증도 시뮬레이션 내에 머무르는 경우가 많았습니다. 몇몇 최신 연구에서는 시뮬레이션 정책의 실제 이전을 보고하기도 했지만, 물체 CAD 모델을 이용한 포즈 추정을 전제한다든지, 손가락의 가려짐으로 인한 점군 결손을 메우기 위해 로봇 모델 정보를 활용하는 특수 기법을 쓰는 등 범용성이 제한되는 경우가 많았습니다. 가장 유사한 선행연구로 언급된 DextrAH-G는 본 논문의 직전 버전 격으로, 인간 그립 동작을 PCA로 저차원화한 손동작 공간과 지오메트릭 패브릭(geometric fabric) 기반 안전 제어를 활용하여 교사-학생 구조로 학습한 뒤 깊이 카메라 기반 학생 정책을 시현한 바 있습니다. DextrAH-G는 시뮬레이션에서 학습한 깊이 기반 정책을 실제로도 견고하게 이전하여 훈련에 쓰지 않은 새로운 물체들도 잡아내는 놀라운 결과를 보였지만, 여전히 깊이 센서에 의존하고 있었습니다. DextrAH-RGB는 바로 이 지점을 발전시켜, 깊이 대신 카메라 RGB 정보만으로 동작하는 정책을 제시함으로써 더욱 광범위한 물체와 환경에서도 적용될 수 있는 가능성을 연 것입니다."
  },
  {
    "objectID": "posts/paper/2025-09-10-dextrah-rgb.html#사용된-방법론-및-기술적-접근",
    "href": "posts/paper/2025-09-10-dextrah-rgb.html#사용된-방법론-및-기술적-접근",
    "title": "📃DextrAH-RGB 리뷰",
    "section": "2.3 사용된 방법론 및 기술적 접근",
    "text": "2.3 사용된 방법론 및 기술적 접근\nDextrAH-RGB의 학습은 크게 교사(policy)와 학생(policy)의 두 단계로 이루어집니다. 먼저 시뮬레이션 상에서 상태 정보를 모두 활용하는 교사 정책을 강화학습(PPO 알고리즘)으로 학습시키고, 이후 해당 정책을 DAgger 기반의 모방학습으로 학생 정책에 증류(distillation)합니다. 학생 정책은 시뮬레이터에서 생성한 데이터만으로 학습되며, 두 대의 RGB 카메라 영상만을 입력으로 받아 동작하도록 훈련됩니다. 최종적으로 이 학생 정책이 실제 로봇에 이식되어 동작하게 됩니다.\n\n2.3.1 지오메트릭 패브릭 기반 동작 공간\n본 연구의 중요한 기술 요소 중 하나는 지오메트릭 패브릭(geometric fabric)이라 불리는 프레임워크를 활용한 패브릭 유도 정책(Fabric-Guided Policy, FGP)입니다. 지오메트릭 패브릭은 고전 역학 시스템을 일반화하여 안전하면서도 반응적인 정책을 설계하기 위한 방식으로, 로봇의 동작을 2차 동역학 시스템 형태로 정의한 뒤 토크 제어 등을 통해 실제 로봇에 구현하는 개념입니다. 정책의 원하는 거동(행동 양상)을 기하학적 항(geometric term)과 강제 항(forcing term)의 조합으로 표현하는데, 기하학적 항은 로봇이 속도와 무관하게 동일한 경로를 따라가게 함으로써 기본 움직임 궤적을 결정하고, 강제 항은 필요에 따라 로봇의 궤적을 교란하여 안전 확보(예: 관절 한계 회피) 또는 과제 수행 보조에 사용됩니다. 여러 강제 항이 동시에 작용하면 상충될 수 있으므로, 가능한 한 정책의 목표 동작을 기하학적 항으로 녹여내고 강제 항은 안전 등 보조적 역할만 하도록 설계합니다. DextrAH-RGB에서는 이전 연구인 DextrAH-G와 동일한 패브릭 및 동작 공간 설계를 사용했으며, 그 주요 내용을 요약하면 다음과 같습니다. 로봇과 주변 물체 간 충돌 회피 행동은 저차원 기하학적 항에 내재화하여 로봇이 기본적으로 장애물과 안전거리를 유지하도록 하되, 근접 시에만 강제 항으로 밀어내는 보조 제어를 넣었습니다. 7자유도 팔의 관절 여유도를 활용하여 로봇 팔꿈치가 자연스럽게 바깥으로 빠지고 손가락은 기본적으로 오므린 자세를 취하도록 끌어당기는 힘을 기하학적 항으로 추가하여, RL 정책의 출력이 이 기본 자세와 충돌하지 않도록 유도했습니다. 또한 로봇 관절 각도 한계를 넘지 않도록 강제 항을 부여하여 안전성을 보장했습니다. 강화학습 정책의 액션 공간은 이러한 패브릭 상에서 정의되는데, 로봇 손바닥(palm)의 6자유도 자세(3차원 위치 + 3자유도 회전)와 손가락 동작의 PCA 기반 5차원 좌표로 구성됩니다. 즉, 16자유도의 손가락 움직임을 사전에 인간 파지 동작들을 PCA로 분석해 추출한 5개의 주성분(eigengrasp) 축으로 표현함으로써, 그립 동작에 적합하면서도 차원이 감소된 손 동작 공간을 사용한 것입니다. RL 알고리즘은 이 6+5차원의 공간에서 목표 가속도 명령을 출력하고, 실시간 제어 단계에서는 이 가상 패브릭 동역학의 가속도에 로봇의 실제 가속도를 최대한 맞추도록 2차 제어(Quadratic Program)를 풀어 토크 제어를 수행합니다. 이러한 패브릭 기반 액션 공간을 활용함으로써, 로봇이 동작 속도와 관계없이 안정적인 궤적을 따르고 안전 제약을 지키면서도, 다지 손 파지에 유리한 유도 편향(inductive bias)을 정책에 제공합니다.\n\n\n2.3.2 교사 정책: 시뮬레이션 강화학습\n교사 정책(Teacher FGP)은 시뮬레이터에서 강화학습(PPO 알고리즘)을 통해 학습됩니다. NVIDIA Isaac Lab 플랫폼을 이용하여 대규모 병렬 환경에서 다양한 물체들을 대상으로 학습함으로써, 샘플 효율이 낮은 강화학습 문제를 극복하고자 했습니다. 교사 정책은 시뮬레이터 내부 상태정보를 완전히 활용하는 전략으로 설계되었습니다. 예를 들어, 신경망 정책의 입력으로 로봇 각 관절의 위치·속도, 손가락 끝과 손바닥의 위치/속도, 물체의 6DoF 자세(pose), 목표로 하는 물체 위치(예: 들어올릴 목표 높이), 그리고 어떤 종류의 물체인지에 대한 원-핫(one-hot) 벡터, 직전 시간의 패브릭-액션, 그리고 패브릭 동역학의 현재 상태(위치, 속도, 가속도)까지 모두 제공합니다. 이러한 프리빌리지드(privileged) 정보를 통합해, 교사 정책은 학생 정책보다 훨씬 풍부한 환경 정보를 바탕으로 동작하게 됩니다. 교사 정책의 신경망 구조는 이전 연구인 DextrAH-G와 유사하게 2개의 512차원 완전연결층 + 512차원 LSTM 층으로 구성되며, LSTM 주변에 skip-connection을 추가해 장기 의존성 학습을 도왔습니다.\n\n\n\n보상 함수(reward)는 DextrAH-G에서 사용한 것보다 단순화하여 네 가지 항으로 구성했습니다: (1) 접근 보상 – 로봇 손가락 및 손바닥 지점들이 물체에 최대한 가깝게 접근하도록 장려, (2) 이동 보상 – 집은 물체를 목표 위치(공중의 일정 높이 등)까지 이동시키도록 장려, (3) 들어올리기 보상 – 물체를 테이블에서 떼어 들어올린 경우의 보상, (4) 손가락 펼침 규제 – 평소에 손가락을 지나치게 오므리지 않도록 벌점. 이 네 가지 요소를 가중합하여 최종 보상으로 사용함으로써, 물체에 손을 뻗어 집고 들어올려 안정적으로 들고 있게 하는 행동을 강화학습으로 학습시켰습니다. 학습 초반에는 과제를 쉽게 하고, 점차 어려워지도록 자동 도메인 랜덤화(Automatic Domain Randomization, ADR) 기법을 적용했습니다. 예를 들어 물체의 초기 배치나 물체/로봇 물리 속성, 마찰계수, 조명 등 여러 환경 파라미터들의 범위를 초기에는 좁게 설정하여 시작하고, 정책 성능이 향상됨에 따라 이 범위들을 선형적으로 최대치까지 넓혀갑니다. 모든 랜덤화 파라미터들을 정해진 최대 난이도까지 동시에 조금씩 높여가는 방식으로 커리큘럼을 형성하여, 최종적으로는 상당히 어려운 조건에서도 동작 가능한 정책을 얻도록 유도했습니다 (자세한 파라미터 범위는 논문 Table II에 명시).\n\n\n2.3.3 학생 정책: RGB 모방학습\n교사 정책이 시뮬레이션에서 안정적으로 물체를 집을 수 있게 되면, 이제 학생 정책(Student FGP)을 훈련합니다. 학생 정책은 카메라 영상만으로 동작해야 하므로, 교사-학생 정책 사이에 관찰공간 차이(reality gap)가 존재합니다. 이를 해소하기 위해 온라인 DAgger 기반의 모방 학습(distillation)을 수행합니다. 구체적으로, 시뮬레이터에서 교사 정책을 실행하면서 동시에 학생 정책이 같은 상황을 관찰하게 하고, 교사의 행동을 모방하도록 학생을 학습시킵니다. 학생은 학습 과정 동안에도 점진적으로 자기 정책에 따라 행동해 보면서 (교사 정책으로부터 벗어나는 시도가 발생하고) 그때마다 교사 정책의 조언을 받아 잘못된 상태 분포를 교정하는 DAgger 알고리즘을 적용합니다. 학생 정책의 입력은 로봇 관절 상태(각도 및 속도)와 좌우 두 대의 RGB 카메라 이미지입니다. 스테레오 카메라 구성을 사용한 이유는 듀얼 카메라로부터 깊이 정보를 유추할 수 있기 때문입니다. 깊이 센서를 쓰지 않고 RGB만 쓰면서도, 두 시점 이미지를 보면 물체까지의 거리나 입체감을 어느 정도 추정할 수 있어 정책 성능이 향상되었습니다 (실험적으로 단안 카메라보다는 스테레오 입력이 성능이 우수했음).\n\n\n\n시뮬레이션에서 학생 정책 학습을 위해 도메인 랜덤화 및 증강을 광범위하게 활용했습니다. NVIDIA의 Isaac Lab 시뮬레이터의 광선추적 기반 타일드 렌더링 기능을 이용하여, 현실감 높고 해상도 좋은 영상을 빠르게 다량 생성했습니다. 학습 중 에피소드마다 주변 환경을 무작위로 변화시켰는데, 예를 들어 배경 조명으로 HDRI 환경맵을 30% 확률로 랜덤 교체하고, 매 에피소드 시작 시 로봇, 테이블, 물체의 재질 속성(색조, 반사율, 거칠기 등)을 임의로 변경했습니다. 또한 물체 3D 모델들이 원래 텍스처가 없는 경우가 많아, Omniverse 자산 라이브러리의 일상 사물 텍스처를 무작위로 입혀 시각적 다양성을 높였습니다. 비록 임의로 입힌 텍스처가 물체 형상에 어울리지 않더라도(UV 매핑 불일치로 엉뚱하게 발라질 수 있지만), 시각적으로 다양한 데이터를 확보하는 데에 의의를 두었습니다. 이처럼 조명, 재질, 배경을 계속 바꾸는 랜덤화 외에도, 최종 학생 정책에 입력되기 전 데이터 증강(data augmentation)도 적용했습니다. 예를 들어 배경을 다른 이미지로 치환하거나, 컬러 지터(color jitter)로 색감을 흔들고, 움직임 모션 블러 효과를 가하는 등 다양한 증강을 통해 카메라 영상의 분포 폭을 넓혔습니다. (논문 Fig. 2에서는 이러한 랜덤화된 환경에서 얻은 원본 카메라 영상들(위쪽)과 여기에 증강을 적용한 최종 학습 입력 영상들(아래쪽)을 비교하여 보여줍니다.) 학생 정책 학습 시에는 이미 교사 정책이 충분히 학습된 상태이므로, ADR 난이도를 곧바로 최대치로 설정하여 가장 어려운 조건들에서도 학생이 학습되도록 하였습니다. 이는 학생이 현실 환경과 유사한 조건을 폭넓게 접하도록 해 시뮬레이션-현실 차이를 최소화하려는 전략입니다.\n학생 정책의 신경망 구조는 경량화된 커스텀 모델로 구성되었습니다. 이미지 입력 처리를 위해 작은 합성곱 신경망(CNN) 인코더 두 개(좌/우 카메라별로)로 시작하는데, 각 CNN은 출력 채널 수가 [16, 32, 64, 128]인 컨볼루션 계층들을 거치며 활성함수로 ReLU를 사용합니다. 입력 해상도는 비교적 낮은 편인 320×240이며, CNN 마지막 출력은 평균 풀링을 거쳐 평탄화된 후 32차원 임베딩 벡터로 압축됩니다. 이렇게 얻은 좌우 이미지 임베딩(각 32차원)을 로봇 자체 상태(proprioception) 벡터와 결합하여 하나의 상태 표현으로 만든 뒤, 512차원의 LSTM 층에 입력합니다. LSTM의 출력을 다시 입력과 연결(skip connection)하여 다층 퍼셉트론(MLP)에 통과시키고, 그 결과를 다시 LSTM 출력 등과 결합하여 보조 출력 헤드에 전달합니다. 이러한 DenseNet 스타일의 밀집 연결 구조를 취함으로써, 단순 순차형보다 정책 학습 성능이 향상되었음을 참고 연구를 통해 확인했다고 합니다. 최종적으로 학생 정책 신경망은 주 출력으로 교사와 동일한 형태의 행동 값을 내고, 추가로 보조 출력으로 현재 물체의 예상 위치를 회귀 예측하도록 구성되었습니다. 모든 계층의 활성화함수로는 ELU를 사용했습니다. 한편, 이미지 인코더로 요즘 각광받는 대규모 사전학습 비전 모델(예: ResNet-18, ViT 등)을 사용하지 않고 직접 경량 모델을 설계한 이유는, 거대한 모델의 경우 학습 시 동결된 특성 추출기로 쓰면 과제 특화 표현 학습이 어려워 성능이 떨어지고, 반대로 파인튜닝하려면 파라미터가 너무 많아 병렬 시뮬레이션 환경 개수를 크게 줄여야 하므로 학습 효율이 저하되기 때문이라고 설명합니다. 즉, 다수의 환경에서 병렬로 학생 정책을 훈련해야 하므로, 경량 모델을 끝까지 직접 학습하는 편이 전체 성능 및 효율에 유리하다고 판단한 것입니다.\n학생 정책 학습에서는 손실 함수로 모방 손실 + 보조(물체 위치) 손실을 함께 최적화했습니다. 모방 손실은 교사 정책과 학생 정책이 출력하는 행동 확률분포 간의 KL 발산(Kullback-Leibler divergence)으로 정의하였습니다. 교사-학생 정책 모두 가우시안 출력 분포(평균 및 분산)로 행동을 샘플링하는데, 분산 항은 고정하고 학습하지 않도록 설정하여 KL 손실이 사실상 평균값 차이를 줄이는 역할을 하도록 했습니다. 이는 단순 평균 제곱 오차 손실보다 안정적이었다고 합니다. 특히 교사 정책이 확신(confident)하는 차원(분산이 작은 차원)에서 오류를 더 강하게 줄여주는 효과가 있어, 모든 차원을 균일하게 학습하는 L2 손실 대비 학습이 잘 되었습니다. 보조 손실은 학생 신경망의 물체 위치 예측과 시뮬레이터 상 실제 물체 위치와의 L2 오차로 계산하여, 학생 정책이 시각적 피쳐로부터 물체의 공간적 정보를 추출하도록 유도했습니다.\n마지막으로, 교사 정책과 학생 정책 간 에피소드 시간 구성을 다르게 설계했습니다. 교사 정책은 하나의 에피소드가 최대 10초간 지속되도록 했는데, 이는 물체를 들어올린 후에도 충분히 유지하여 안정적으로 파지하도록 탐색하는 시간을 주기 위함이었습니다. 그러나 학생 정책까지 동일하게 긴 에피소드로 학습시키면, 이미 물체를 잡고 난 후 공중에 들고 있는 지루한 구간이 학습 데이터에 많이 포함되어 초반 파지 동작 학습 비중이 상대적으로 낮아지는 문제가 있습니다. 학생 정책이 교사를 벗어나 오차가 누적되는 부분은 주로 초반 물체를 잡는 단계이므로, 이 구간의 학습을 강화하기 위해 에피소드를 조기 종료하는 기준을 두었습니다. 즉, 학생 정책 학습 시에는 물체를 들어올려 2초간 성공적으로 들고 있으면 바로 에피소드를 끝내고 다음 시뮬레이션 에피소드로 넘어가도록 하였습니다. 다만 너무 짧게 자르면 첫 시도 실패 후 재시도 학습이 어려울 수 있으므로, 2초 정도 유예를 두어 한번 놓쳤을 때 다시 쥐는 회복 동작도 학습할 수 있도록 균형을 맞추었습니다."
  },
  {
    "objectID": "posts/paper/2025-09-10-dextrah-rgb.html#주요-실험-및-결과-분석",
    "href": "posts/paper/2025-09-10-dextrah-rgb.html#주요-실험-및-결과-분석",
    "title": "📃DextrAH-RGB 리뷰",
    "section": "2.4 주요 실험 및 결과 분석",
    "text": "2.4 주요 실험 및 결과 분석\n하드웨어 구성: 학습된 DextrAH-RGB 학생 정책은 실제 로봇 플랫폼에 이식되어 검증되었습니다. 로봇은 7자유도 KUKA LBR iiwa 산업용 팔에 16자유도 Allegro 다지 로봇 손이 결합된 형태이며, 테이블 위에 두 대의 Intel RealSense D415 카메라를 좌우 스테레오로 고정 배치하여 RGB 영상을 수집했습니다. (D415는 RGB-D 카메라이지만, 본 연구에서는 깊이 정보는 사용하지 않고 RGB 채널만 활용했습니다.) 로봇 제어기는 KUKA 팔 관절 제어에 1kHz, Allegro 손가락 제어에 333Hz의 내부 제어 주기로 동작하며, 카메라는 60Hz로 영상을 송신합니다. 학습된 정책의 추론 속도도 실시간성을 충족해야 하므로, Jetson Orin 장치 상에서 전체 정책 신경망을 60Hz로 실행했고, NVIDIA CUDA 그래프 캡처 기술 등을 활용해 지연을 최소화했습니다. 실험 결과 정책 주기를 30Hz에서 60Hz로 높였을 때 로봇의 파지 성능이 크게 향상되었는데, 이는 고속의 반복 제어가 실세계 상호작용에서 매우 중요함을 보여줍니다.\n\n\n\n평가 방법: 논문에서는 다지 손 파지 능력을 평가하기 위해 단일 물체 파지 성공률 지표를 사용했습니다. 로봇이 한 가지 물체를 들어올릴 수 있는지를 여러 번 시도해보고 그 성공 확률을 측정하는 방식입니다. 구체적으로, 야일/CMU/버클리(YCB) 등 공개 물체 데이터셋에 속하는 11개의 다양한 물체를 선정하여 각각에 대해 평가를 진행했습니다. 예를 들어 컵, 음료수 캔, 과자 상자, 세제 병, 벽돌, 스팸 통조림, 냄비, 장난감 비행기 등 형태와 크기가 다양한 물체들이 포함되었습니다. 각 물체마다 5가지 서로 다른 초기 자세(방향 및 위치)로 테이블 위에 놓고, 로봇이 해당 물체를 집어 들어올리도록 정책을 구동합니다. 한 번의 시도에서 성공 여부를 기록하며, 각 물체당 5회씩 시도해 5회 중 성공 횟수의 비율을 그 물체의 성공률로 정의했습니다. 만약 로봇이 첫 시도에 물체를 제대로 잡지 못하더라도, 물체가 완전히 넘어져 집기 불가능한 상태가 되지 않는 한 정책이 연속적으로 재시도하도록 했습니다. 이는 앞서 언급한대로 DextrAH-RGB 정책이 순차적인 단일 스텝 제어가 아닌 연속적인 센서피드백 기반 정책이기 때문에 가능한 일로, 한 에피소드 내에서도 실패를 감지하면 자세를 고쳐 재도전하는 동작이 나타날 수 있습니다. 이러한 연속 수행 능력과 LSTM 기반의 적응력은 로봇이 물체를 잡을 때 작은 미끄러짐이나 오차가 발생해도 곧바로 교정하여 결국 성공으로 이어질 확률을 높여줍니다.\n실험 결과: DextrAH-RGB의 성능을 이전 연구들의 결과와 정량적으로 비교하기 위해, 여러 기준 물체에 대한 성공률을 Table I에 요약하였습니다. 주요 비교 대상은 앞서 언급된 DextrAH-G (깊이 센서 기반 교사-학생 정책)과, DexDiffuser, ISAGrasp, Matak 등의 최신 방법들입니다. 전반적으로 DextrAH-RGB는 여러 물체에서 기존 최첨단 성능과 유사한 성공률을 달성하여, 깊이 센서를 사용하지 않고도 견줄만한 파지 성능을 보임을 확인했습니다. 예를 들어 Pringles 통, 컵, 벽돌 등의 물체에 대해서는 100%에 가까운 성공률을 기록하여, Depth 기반인 DextrAH-G와 동등한 수준의 성능을 냈습니다. 일부 물체에 대해서는 깊이 기반 방법보다 성공률이 다소 낮았는데, 예컨대 주전자(Pitcher)의 경우 DextrAH-RGB는 5회 중 1회(20%) 성공한 반면 DextrAH-G는 80%를 달성했고, 장난감 비행기는 DextrAH-RGB가 한 번도 성공하지 못한 데 반해 DextrAH-G는 60% 성공했습니다. 전반적으로 깊이 정보를 사용하지 않는 RGB 기반이라는 도전적인 설정 때문에 완벽히 동일한 성능을 내긴 어렵지만, 대부분의 물체에서 깊이 기반 대비 큰 손실 없이 높은 성공률을 보였다는 점이 고무적입니다. 특히 투명한 유리잔이나 광택이 있는 금속 물체 등 깊이 카메라로는 취약한 대상에 대해서도 RGB 정책이 제 성능을 발휘함을 보여, RGB 입력의 실용성을 입증했습니다 (해당 사례에 대한 구체적 수치는 논문에서 직접 언급되지 않았으나, 깊이 센서의 약점을 고려한 저자들의 주장입니다). 또한 DextrAH-RGB는 훈련 시 보지 못한 새로운 물체들에 대해서도 높은 성공률을 나타냈는데, 이는 앞서 시뮬레이션 단계에서 다양한 도메인 랜덤화와 시각적 변이를 학습한 덕분으로 볼 수 있습니다. 요컨대, 이 연구는 깊이 센서 없이도 다지 로봇 손의 파지가 가능하며, 충분한 시뮬레이션 훈련과 적절한 정책 구조를 통해 실세계에서도 거의 즉시 활용 가능한 수준에 도달할 수 있음을 실험적으로 보여주었습니다."
  },
  {
    "objectID": "posts/paper/2025-09-10-dextrah-rgb.html#장단점-평가",
    "href": "posts/paper/2025-09-10-dextrah-rgb.html#장단점-평가",
    "title": "📃DextrAH-RGB 리뷰",
    "section": "2.5 장단점 평가",
    "text": "2.5 장단점 평가\n장점: DextrAH-RGB의 가장 큰 성과는 세계 최초로 RGB 카메라만을 이용한 엔드투엔드 다지 손 파지 정책을 현실에 구현했다는 점입니다. 깊이 센서를 배제함으로써, IR 기반 깊이 카메라의 한계(투명체 인식 불가 등)를 극복하고 일반 카메라로 얻는 풍부한 시각 정보만으로도 로봇 파지가 가능함을 증명했습니다. 또한 교사-학생 모방학습 구조를 통해 전적으로 시뮬레이션 데이터만으로 학습을 완료하고도 현실 로봇에서 제로샷 동작을 이끌어냈다는 점에서 실용성이 큽니다. 이는 위험하고 비용이 큰 실제 로봇 실험 없이도 복잡한 조작 정책을 배양할 수 있다는 뜻이므로, 향후 유사한 문제들에 적용될 수 있는 효율적 연구 개발 프로세스를 제시했다고 볼 수 있습니다. 정책이 연속적인 LSTM 기반 제어를 하기 때문에, 실시간 반응성과 적응성이 뛰어난 것도 장점입니다. 기존의 단발적(grasp pose) 계획에 의존하는 방법들과 달리, DextrAH-RGB는 실행 중에 새로운 센서 정보에 따라 즉각적으로 경로를 수정하며 실패 시 재시도도 가능한 로버스트 제어를 구현했습니다. 아울러 지오메트릭 패브릭을 활용한 덕분에 로봇의 충돌 회피나 관절 한계 준수 등의 안전성이 정책 수준에서 보장되었고, 이는 실제 로봇 운용에서 대단히 중요한 요소입니다. 이러한 안전 제약이 있음에도 불구하고 정책이 물체를 잘 잡을 수 있었던 것은, 패브릭 프레임워크가 안전과 성능을 양립하도록 설계되었기 때문입니다. 실제 로봇 구현 측면에서도, Jetson Orin 상에서 60Hz로 구동되며 단일 보드로 전체 제어를 수행해 시스템 구성의 단순성과 실시간성을 모두 확보했습니다. 마지막으로, 저자들은 본 방법론이 향후 복잡한 조작 기술의 구성 요소로 활용되거나, 대규모 로봇 행동 모델(일종의 Foundational policy) 학습을 위한 데이터 생성 모듈로도 이용될 수 있음을 언급하며 본 연구의 확장 가능성을 강조했습니다. 이는 DextrAH-RGB가 단일 논문 결과에 그치는 것이 아니라, 로봇 파지 문제 전반에 기여할 수 있는 범용성을 지닌다는 의미입니다.\n단점: 한편, 제한사항도 분명히 존재합니다. 먼저, 손가락 제어를 PCA 기반 저차원 공간에서 수행한 것은 파지 동작에 집중하기 위한 설계였지만, 그만큼 손의 섬세한 조작 범위가 제한됩니다. 주성분 손동작 공간은 인간의 그립 동작 데이터를 반영하지만, 이러한 이젠그립(eigengrasp) 방식은 물체를 쥐는 동작 이외의 복잡한 손동작 (예를 들면 도구 사용이나 손가락 개별적 움직임을 요구하는 작업)에는 부적합합니다. 따라서 본 연구에서는 파지 성공률은 높였지만, 손의 완전한 다용도성은 희생한 측면이 있습니다. 또한 안전을 위해 패브릭에 포함시킨 테이블 충돌회피 동작은 작은 물체를 잡는 경우 오히려 방해가 되는 것으로 지적되었습니다. 테이블 표면과 가까운 낮은 물체를 집으려 할 때 로봇이 충돌을 두려워해 충분히 손을 내려보내지 못하는 상황이 발생할 수 있다는 것입니다. 이는 현재 해당 회피 로직이 정책이 아닌 고정된 제약으로 적용되기 때문이며, 향후에는 이를 센서 입력을 통해 학습된 정책이 판단하도록 개선하는 편이 더 유연할 것이라고 제안되었습니다.\n학습 면에서, 두 단계로 나누어진 교사-학생 훈련 파이프라인은 구현과 튜닝이 다소 복잡하고 많은 시간과 자원이 필요하다는 단점이 있습니다. 교사 정책을 충분히 학습시킨 후 다시 학생 정책을 학습해야 하므로, 단일 단계로 끝나는 강화학습에 비해 절차가 번거롭습니다. 저자들 역시 이를 인정하며, 향후 더 효율적인 탐색 전략이 개발되면 단일 단계의 end-to-end RL로도 학습이 가능할 것으로 전망했습니다. 결과 측면에서도 한계를 꼽을 수 있습니다. 파지의 기능적 의미를 고려하지 않았기 때문에, 잡기만 하면 되는 평가에서는 성공이지만 일상적인 사용 방법으로는 적절치 않은 파지 사례가 있었습니다. 예를 들어 냄비를 들 때 손잡이가 아닌 냄비 본체를 통째로 움켜쥐는 식의 그립이 나타났는데, 인간이라면 손잡이를 사용하는 것이 일반적이라는 점에서 비기능적(non-functional) 파지라는 한계가 있습니다. 이는 향후 로봇에게 물체의 쓰임새까지 이해시켜 잡도록 하는 과제로 남아 있습니다. 또한 현재 정책은 단일 물체 파지에만 초점을 두고 있어, 복잡한 환경이나 다중 물체가 있는 상황에서는 적용되지 못합니다. 작업 공간에 여러 물건이 있거나 잡고자 하는 물체 주위에 장애물이 많은 잡동사니(clutter) 환경에서는 인식과 계획이 훨씬 어려운데, DextrAH-RGB는 이런 상황을 다루지 못하므로 실용화를 위해서는 해당 한계를 넘어야 합니다. 마지막으로, 깊이 기반 접근에 비해 성능 격차가 일부 존재한다는 점도 단점입니다. 앞서 언급했듯 몇몇 물체에서는 성공률이 낮았고, 이는 RGB 영상만으로 3D 정보를 완벽히 얻는 데 한계가 있기 때문입니다. stereo 설정으로 보완했지만 정확한 거리 추정이나 미세한 물체 형상 파악에는 여전히 깊이센서보다 불리합니다. 따라서 완전한 범용 로봇 파지 시스템으로 발전하려면, RGB 기반의 한계를 줄이기 위한 추가 기법(예: 더 나은 심층 학습 모델, 멀티뷰 카메라 확충, 영상으로부터의 3D 복원 등)이 필요할 것입니다. 이밖에도 본 논문에서는 명시적으로 다루지 않았지만, 시뮬레이션 학습에 드는 막대한 계산 자원 역시 현실적인 제한입니다. 광학적 렌더링을 수행하는 수백 개의 병렬 환경에서 교사-학생을 학습하려면 상당한 GPU 자원과 시간이 필요하며, 이는 일반 연구자가 모방하기에 진입장벽이 될 수 있습니다. 또한 정책을 새로운 로봇이나 환경에 적용하려면 다시 시뮬레이션 학습을 거쳐야 하는데, 이 과정의 재현 비용도 고려해야 합니다.\n그럼에도 불구하고, DextrAH-RGB는 로봇 다지 손 파지 분야의 중요한 진전을 이루어낸 연구입니다. RGB 카메라 입력만으로도 현실에서 복잡한 다지 조작을 달성할 수 있음을 처음으로 증명했고, 여러 기술적 통찰(교사-학생 학습, 패브릭 기반 제어, 도메인 랜덤화 등)을 효과적으로 결합했습니다. 저자들은 향후 본 연구를 발전시켜 성능을 더욱 향상하고, 빈 패킹(bin-packing)과 같이 다수 물체를 다루는 작업으로 확장할 계획을 밝혔습니다. 이러한 후속 연구가 진행된다면, DextrAH-RGB의 단점으로 지적된 부분들도 점차 해결되며 보다 범용적이고 실용적인 로봇 파지 시스템에 가까워질 것으로 기대됩니다.\nReference\n\nDextrAH-G 리뷰(CoRL2024)"
  },
  {
    "objectID": "posts/paper/2022-06-10-NerveNet.html",
    "href": "posts/paper/2022-06-10-NerveNet.html",
    "title": "📃NerveNet 리뷰",
    "section": "",
    "text": "We address the problem of learning structured policies for continuous control. In traditional reinforcement learning, policies of agents are learned by multi-layer perceptrons (MLPs) which take the concatenation of all observations from the environment as input for predicting actions. In this work, we propose NerveNet to explicitly model the structure of an agent, which naturally takes the form of a graph. Specifically, serving as the agent’s policy network, NerveNet first propagates information over the structure of the agent and then predict actions for different parts of the agent. In the experiments, we first show that our NerveNet is comparable to state-of-the-art methods on standard MuJoCo environments. We further propose our customized reinforcement learning environments for benchmarking two types of structure transfer learning tasks, i.e., size and disability transfer, as well as multi-task learning. We demonstrate that policies learned by NerveNet are significantly more transferable and generalizable than policies learned by other models and are able to transfer even in a zero-shot setting.\n\n보통 강화학습에서 agent들의 policy는 multi-layer perceptrons (MLPs)으로 네트워크를 만들기 때문에 agent가 environment에서 받은 observation들을 단순히 쌓아서(concatenation) policy network에 입력으로 들어가게 된다. 하지만 손의 속도 정보와 발의 속도 정보가 같은 속도 범주이지만 위치가 다르기 때문에 구분이 있을 수 있듯이 agent의 이런 구조적인 특성을 반영해서 policy를 만든다면 observation 정보들간의 구분을 할 수 있을 것이다. 이런 agent의 구조적 관계성을 나타내기 위해서 MLP대신 그래프를 활용하게 되었고 NerveNet을 고안하게 되었다. NerveNet은 그래프 구조로 되어 있는 policy network에서 각 노드들의 정보들이 전파(propagation)되며 agent의 부분들을 나타내는 노드마다 action을 prediction 하게 된다. MuJoCo 환경에서 MLP 기반의 벤치마크들과 비등한 학습결과를 보여주었으며, transfer learning task로 agent의 크기(size)와 agent의 일부 파트가 작동하지 않는(disability) variation을 주었을 때도 잘 학습되었으며 multi-task learning으로 walker 그룹의 다양한 환경에서의 학습 결과들도 좋았다. 이런 결과들을 통해 NerveNet이 transferable할 뿐만 아니라 zero-shot setting도 가능함을 보여주었다.\n\ntransferable - A task를 학습한 네트워크(weights)를 활용하여 B task 학습에도 적용하여 scratch에서 B task를 학습하는 것보다 더 빠르고 효율적인 학습을 가능하게 할 수 있다는 의미. A task 학습에서 습득한 논리체계를 B task에도 적용할 수 있음으로 볼 수 있다.\nzero-shot - Meta learning에서 사용되는 용어로 A task에 대해서 학습된 네트워크가 fine tuning이 없이 바로 unseen new task B에 대해서 좋은 성능을 내는 것을 의미."
  },
  {
    "objectID": "posts/paper/2022-06-10-NerveNet.html#graph-construction",
    "href": "posts/paper/2022-06-10-NerveNet.html#graph-construction",
    "title": "📃NerveNet 리뷰",
    "section": "Graph Construction",
    "text": "Graph Construction\n본 논문에서는 사용한 MuJoCo의 agent들은 이미 구조적으로 tree 구조를 가지고 있다. NerveNet의 핵심 아이디어인 그래프를 구성하기 위해 body와 joint, root라는 3가지 종류의 노드를 설정했다. body 노드는 로봇공학에서 말하는 link 기준의 좌표시스템을 나타내는 노드이고, joint 노드는 모션의 자유도(freedom of motion)을 나타내며 2개의 body 노드들을 연결해주는 노드이다.\n아래는 Ant 환경의 예시인데, 한 가지 그림에서 헷갈리지 말아야 할 점은 그림에서는 마치 body와 root 노드만 노드로 만든것 처럼 보이지만 root와 body, body와 body를 연결하는 엣지들도 실제로는 joint 노드들이다.(we omit the joint nodes and use edges to represent the physical connections of joint nodes.)root라는 노드는 agent의 추가적인 정보들을 담을 부분으로 사용하기 위해 추가한 노드 종류로, 예를 들어 agent가 도달해야 하는 target position에 대한 정보 등이 담겨있다."
  },
  {
    "objectID": "posts/paper/2022-06-10-NerveNet.html#nervenet-as-policy",
    "href": "posts/paper/2022-06-10-NerveNet.html#nervenet-as-policy",
    "title": "📃NerveNet 리뷰",
    "section": "NerveNet as Policy",
    "text": "NerveNet as Policy\n크게 3가지 파트로 NerveNet을 살펴볼 것인데 우선 (0) Notation을 보고 난뒤, (1) Input model (2) Propagation model (3) Output model 순으로 살펴볼 예정이다.\n\n0. Notation\n그래프에서의 노테이션은 다음과 같이 G 라는 그래프는 노드 집합 V와 엣지 집합 E로 구성된다.\n\nG=(V, E)\n\nNervenet policy를 구성하는 그래프는 Directed graph(유향 그래프)이기 때문에 각 노드에서의 in과 out이 따로 명시되게 된다.\n\n노드 u를 중심으로 노드 u로 들어오는 이웃 노드이면 \\mathcal{N}_{in}(u)\n노드 u를 중심으로 노드 u에서 나가는 이웃 노드이면 \\mathcal{N}_{out}(u)\n\n그래프의 모든 노드 u는 타입을 가지게 되고 이를 p_{u} \\in\\{1,2, \\ldots, P\\} (associated note type)로 나타내며 여기에서는 위에 설명한 것과 같이 body, joint, root 3가지 타입이 있다.\n노드들 뿐만 아니라 엣지들도 타입을 정할 수 있는데 c_{(u, v)} \\in\\{1,2, \\ldots, C\\} (associate each edge)로 표기하여 노드쌍 (u, v) 사이의 엣지 타입을 정의할 수 있다.(하나의 엣지에 대해서 여러 엣지 타입을 정의할 수 있지만 여기에서는 심플 이즈 더 베스트 철학으로 하나의 엣지는 하나의 타입만 가지도록 했다)\n이렇게 노드별, 엣지별 타입을 나눔으로써,\n\n노드 타입은 노드들간의 다른 중요도를 파악하는데 도움이 되고\n엣지 타입은 노드들간의 서로다른 관계들을 나타내고 이 관계의 종류에 따라 정보를 다르게 propagation 하게 된다.\n\n이제 시간 노테이션에 대한 부분을 살펴보자. NerveNet에는 시간(time step)의 개념이 2가지 존재한다.\n\n기존 강화학습에서 환경과 agent 사이의 interaction time step을 나타내는 \\tau\nNerveNet의 내부 graph policy에서의 propagation step을 나타내는 t\n\n다시 풀어서 생각해보면, 강화학습의 시간 개념 \\tau 스텝에서 환경으로부터 observation을 받고, 받은 observation을 기반으로 t 스텝동안 NerveNet의 내부의 그래프의 propagation이 일어난다.\n\n\n1. Input model\n위에서 말했듯이 환경과 상호작용으로 observation s^{\\tau} \\in \\mathcal{S}을 받게 된다(time step \\tau). 이 s^{\\tau}는 concatenation된 각 노드의 observation이라고 볼 수 있다. 이제 강화학습 interaction 수준의 \\tau 스텝은 잠시 멈춰두고 그래프 내부의 타임 스텝인 t 수준에서 생각해보자. observation은 node u에 해당하는 x_{u}로 표현할 수 있고 x_{u}는 input network F_{\\mathrm{in}}(MLP)를 거쳐서 고정된 크기의 state vector인 h_{u}^{0}가 된다. h_{u}^{0}의 노테이션을 풀어서 해석하면 노드 u의 propagation step 0 에서의 state vector인 것이다. 이때 observation vector x_{u}가 노드마다 크기가 다를 경우 zero padding으로 맞춰서 input network에 넣어주게 된다.\n\nh_{u}^{0}=F_{\\text {in }}\\left(x_{u}\\right)\n\n\n\n2. Propagation model\nNerveNet의 propagation 과정 노드들 간에 주고 받는 정보를 message라고 하게 되고 이는 노드들 간에 주고 받는 상호작용이라고 생각할 수 있다. Propagation model은 3가지 단계로 나누어서 볼 수 있다.\n\nMessage Computation\n\n전달할 메세지를 계산한다.\npropagation step인 t에, 모든 노드들 u에서 state vector h_{u}^{t}를 정의할 수 있다.\n노드 u로 모아지는(in-coming) 모든 엣지들을 가지고 메시지를 구하게 되는데, 이때 M은 MLP이고 M의 아래첨자 c_{(u, v)} 노테이션에서 알 수 있듯이 같은 종류의 엣지에 대해서는 같은 message function M을 쓴다.\n\n  m_{(u, v)}^{t}=M_{c_{(u, v)}}\\left(h_{u}^{t}\\right)\n  \n예를 들어 아래 그림은 CentipedeEight 의 모습인데, 왼쪽은 실제 agent의 모습을 나타내고 있으며 오른쪽은 agent를 그래프로 나타냈을 때의 모습이다. 여기에서 2번째 torso에서 첫번째 세번째 torso에서 보낼 때 같은 메세지 펑션 M_{1} 을 사용하고, LeftHip과 RightHip으로 보내는 메세지 펑션 M_{2}를 사용하게 되는 것이다.\n\n\n\n\n\n\nMessage Aggregation\n\n앞 단계에서 모든 노드들에 대해서 메세지 계산이 끝난 후에 in-coming 이웃 노드들로부터 온(계산된) 메세지를 모으게 된다. 이때 summation, average, max-pooling 등 다양한 aggregation 함수를 사용할 수 있다.\n\n  \\bar{m}_{u}^{t}=A\\left(\\left\\{h_{v}^{t} \\mid v \\in \\mathcal{N}_{i n}(u)\\right\\}\\right)\n  \n\nStates Update\n\n이제 모은 메세지를 기반으로 state vector를 업데이트 하면 된다!\n\n  h_{u}^{t+1}=U_{p_{u}}\\left(h_{u}^{t}, \\bar{m}_{u}^{t}\\right)\n  \n여기서 업데이트 함수 U 는 a gated recurrent unit (GRU), a long short term memory (LSTM) unit 또는 MLP가 될 수 있다.\nUpdate function의 아래첨자 p_{u}에서 볼 수 있다시피 같은 노드 타입이면 같은 update function U를 쓰게 된다. 이렇게 업데이트된 state vector는 타임 스텝 t가 하나 올라간 t+1 이 된 h_{u}^{t+1}가 된다.\n\n\n이렇게 내부 propagation 과정 3단계(Message Computation, Message Aggregation, States Update)가 T 스텝동안 일어나게 되고 각 노드의 최종 state vector는 h_{u}^{T} 가 된다.\n\n\n3. Output model\n전형적인 RL의 MLP 폴리시에서는 네트워크에서 각 action의 gaussian distribution의 mean을 뽑아내게 된다. std는 trainable한 벡터이다. NerveNet에서도 std는 비슷하게 다루지만 각 노드에 마다 action prediction을 만들게 된다.\nactuator와 연결되어 있는 노드들의 집합을 O라고 하자. 이 집합에 있는 노드들의 최종 state vector h_{u \\in \\mathcal{O}}^{T}는 MLP인 Ouput model O_{q_{u}}에 인풋으로 들어가게 되고 아웃풋으로 각 actuator의 action distribution인 gaussian distribution의 mean \\mu을 출력하게 된다. 여기에서 새로운 노테이션 q_{u}를 볼 수 있는데 q_{u}는 아웃풋 타입, 즉 아웃풋을 내놓는 노드 u의 타입으로 아웃풋 펑션의 아래첨자에 q_{u}에 따라 아웃풋 노드의 타입이 같으면 Output function을 공유할 수 있다. 다시말해 아웃풋 노드 타입에 따라 컨트롤러를 공유할 수도 있는 것이다. 위의 Centipedes의 예시로 보면, 같은 LeftHip 끼리는 컨트롤러를 공유할 수 있다는 것이다.\n\n\\mu_{u \\in \\mathcal{O}}=O_{q_{u}}\\left(h_{u}^{T}\\right)\n\n논문에서 실제로 실험을 해봤을 때 다른 타입의 컨트롤러들을 하나로 통합했더라도(O function을 다 같은 MLP로 사용) 퍼포먼스가 그렇게 해쳐지지 않음을 확인할 수 있었다고 한다.\n여기까지해서 그래프 노테이션을 빌려 그래프 기반 가우시안 stochastic policy를 나타내면 아래의 수식과 같다.\n\n\\pi_{\\theta}\\left(a^{\\tau} \\mid s^{\\tau}\\right)=\\prod_{u \\in \\mathcal{O}} \\pi_{\\theta, u}\\left(a_{u}^{\\tau} \\mid s^{\\tau}\\right)=\\prod_{u \\in \\mathcal{O}} \\frac{1}{\\sqrt{2 \\pi \\sigma_{u}^{2}}} e^{\\left(a_{u}^{\\tau}-\\mu_{u}\\right)^{2} /\\left(2 \\sigma_{u}^{2}\\right)}\n\n\n여기까지 NerveNet의 각 단계를 Walker-Ostrich 환경에서 예시로 한눈에 보기 쉽게 정리한 그림은 아래와 같다."
  },
  {
    "objectID": "posts/paper/2022-06-10-NerveNet.html#learning-algorithm",
    "href": "posts/paper/2022-06-10-NerveNet.html#learning-algorithm",
    "title": "📃NerveNet 리뷰",
    "section": "Learning Algorithm",
    "text": "Learning Algorithm\n이전 파트에서 NerveNet의 내부에서 propagation 스텝 t 단위에서 각 단계들을 자세히 살펴보았다면 이제 강화학습 타임 스텝 \\tau 단위에서 학습의 목적함수와 알고리즘을 살펴보자. 목적함수는 전형적인 RL과 다른 점이 없이 policy의 파라미터 \\theta를 가지고 Return 값을 maximization하는 것으로 한다.\n\nJ(\\theta)=\\mathbb{E}{\\pi}\\left[\\sum{\\tau=0}^{\\infty} \\gamma^{\\tau} r\\left(s^{\\tau}, a^{\\tau}\\right)\\right]\n\n강화학습 알고리즘으로는 PPO과 GAE를 사용했으며 해당 알고리즘들의 내용은 각각 알고리즘들의 원래 수식과 내용들과 상이한 점이 없으므로 각 논문으 참고하면 되기 때문에 이번 논문 리뷰에서는 생략한다.\nPPO와 GAE 알고리즘을 참고하여 위의 목적함수 J를 정리하면 NerveNet의 목적함수는 다음과 같다.\n\n\\begin{aligned}\n\\tilde{J}(\\theta)=& J(\\theta)-\\beta L_{K L}(\\theta)-\\alpha L_{V}(\\theta) \\\\\n=& \\mathbb{E}_{\\pi_{\\theta}}\\left[\\sum_{\\tau=0}^{\\infty} \\min \\left(\\hat{A}^{\\tau} r^{\\tau}(\\theta), \\hat{A}^{\\tau} \\operatorname{clip}\\left(r^{\\tau}(\\theta), 1-\\epsilon, 1+\\epsilon\\right)\\right)\\right] \\\\\n&-\\beta \\mathbb{E}_{\\pi_{\\theta}}\\left[\\sum_{\\tau=0}^{\\infty} \\operatorname{KL}\\left[\\pi_{\\theta}\\left(: \\mid s^{\\tau}\\right) \\mid \\pi_{\\theta_{o l d}}\\left(: \\mid s^{\\tau}\\right)\\right]\\right]-\\alpha \\mathbb{E}_{\\pi_{\\theta}}\\left[\\sum_{\\tau=0}^{\\infty}\\left(V_{\\theta}\\left(s^{\\tau}\\right)-V\\left(s^{\\tau}\\right)^{\\operatorname{target}}\\right)^{2}\\right]\n\\end{aligned}\n\n위의 수식에서 볼 수 있는 value network V를 어떻게 디자인할 것인지가 이번 논문의 다른 포인트로 볼 수 있다. 논문의 기본 아이디어는 policy network를 그래프로 표현하는 것이고, value network는 어떻게 할지 여러 선택지들이 남아있다. 그래서 본 논문에서는 value network의 디자인을 두고 크게 3가지 NerveNet의 변형 알고리즘들을 실험해보았다.\n\nNerveNet-MLP : policy network를 1개의 GNN으로 구성하고 value network는 MLP로 구성\nNerveNet-2 : policy network를 1개의 GNN으로 구성하고 value network는 또 다른 GNN으로 구성(총 GNN 2개 - without sharing the parameters of the two GNNs)\nNerveNet-1 : policy network와 value network 모두 1개의 GNN으로 구성(총 GNN 1개)"
  },
  {
    "objectID": "posts/paper/2022-06-10-NerveNet.html#comparison-on-standard-benchmarks-of-mujoco",
    "href": "posts/paper/2022-06-10-NerveNet.html#comparison-on-standard-benchmarks-of-mujoco",
    "title": "📃NerveNet 리뷰",
    "section": "1. Comparison on standard benchmarks of MuJoCo",
    "text": "1. Comparison on standard benchmarks of MuJoCo\n\n비교군으로 MLP, TreeNet(모든 노드들이 연결 되어 있는 그래프, depth 1)을 사용\n총 8개의 환경에서 실험 - Reacher, InvertedPendulum, InvertedDoublePendulum, Swimmer, HalfCheetah, Hopper, Walker2d, Ant\n충분히 학습하는 스텝을 주기 위해서 1 million을 max로 둠\n하이퍼 파라미터의 경우 그리드 서치로 찾았으며(Appendix 참고) 각 알고리즘의 퍼포먼스를 측정할 때 3번의 run을 랜덤 시드를 바꿔가며 실행시킨 후 평균을 구해서 기록\n대부분의 환경에서 MLP가 잘됐고 NerveNet도 이와 비등한 퍼포먼스를 냈다.\n\n(3가지 케이스에 대한 learning curve, 다른 케이스들에서는 대체로 NerveNet과 MLP가 비슷했다.)\n\n\n\n\n\n\n\n\nHalfCheetah\nInvertedDoublePendulum\nSwimmer\n\n\n\n\n\n\n\n\n\nMLP와 NerveNet이 비슷하고 TreeNet이 많이 안좋았음\nMLP가 좀더 좋은 결과를 냄\nNerveNet이 MLP보다 좋은 성능을 냄\n\n\n\n\n대부분 환경들에서 TreeNet이 NerveNet보다 좋지 않았고 이를 통해서 물리적인 그래프 구조를 가져가는 것이 얼마나 중요한지 알 수 있다."
  },
  {
    "objectID": "posts/paper/2022-06-10-NerveNet.html#structure-transfer-learning",
    "href": "posts/paper/2022-06-10-NerveNet.html#structure-transfer-learning",
    "title": "📃NerveNet 리뷰",
    "section": "2. Structure transfer learning",
    "text": "2. Structure transfer learning\n\nMuJoCo의 환경 하나를 커스텀해서 size와 disability의 변화가 있을 때 transferable 함을 검증\n\nsize transfer - 작은 사이즈의 그래프를 가진 agent를 학습 시킨 후 더 큰 사이즈의 그래프를 가진 agent로 transferable 한지\ndisability transfer - 모든 파트들이 정상작동하는 agent로 학습한 후 일부 파트들이 작동하지 않는 상황의 agent로 transferable 한지\n\n2개 종류의 환경을 커스텀하여 실험 - centipede와 snake\n\ncentipede - 지네와 같이 생긴 agent로 torso body들이 여러개 체인처럼 연결 되어 있고 torso를 중심으로 양쪽에 다리가 1쌍으로 붙어 있다. 하나의 다리는 thigh와 shin으로 구성되어 있고 hinge actuator로 구현되어 있다. 커스텀은 다리의 갯수를 다양하게 해서 여러 커스텀 환경들을 만들었는데, 가장 짧은 agent로는 CentipedeFour 부터 가장 긴 agent로는 CentipedeFourty 로 다리가 40개까지(20쌍) 있는 환경을 만들수 있었다. disability로 일부 파트가 작동하지 않는 환경은 Cp(Cripple)로 따로 표기했다. 이 환경에서 y-direction으로 빨리 앞으로 가는게 목표다.\nsnake - swimmer 환경을 기반으로 커스텀했으며 가장 빨리 진행방향으로 움직이는 게 목표다.\n\n\n\n\n\n\n\n비교군\n\nNerveNet : small agent가 학습한 모델을 바로 large agent에 적용할 수 있었다. agent의 구조가 반복적이기 때문에 반복되는 부분을 더 늘리기만 하면 되기 때문이다.\nMLP Pre-trained (MLPP): agent의 크기가 커짐에 따라 input size가 달라지므로 가장 straightforward하게 첫번째 hidden layer를 그대로 output layer로 사용하고 input layer의 사이즈만 키워서 추가하고 이 input layer는 랜덤 초기화를 해준다.\nMLP Activation Assigning (MLPAA): small agent의 weight들을 바로 large agent의 모델에 넣어주고 weight들의 남는 부분들을 0으로 초기화 해준다.\nTreeNet: MLPAA처럼 스케일을 키워서 0으로 초기화 해준다.\nRandom : action space에서 uniformly하게 샘플링을 하는 policy이다.\n\nResult\n\nCentipede\n1-1. Pretraining\n\n6-다리 모델과 4-다리 모델로 NerveNet, MLP, TreeNet 에서의 퍼포먼스를 비교했다. 여기서 3개의 모델은 앞서 benchmark 비교 실험에서 사용한 비교군들과 동일하다.\n\n\n\n\n\n\n\n4-다리 모델에서는 NerveNet이 가장 Reward가 높고, 6-다리 모델에서는 MLP가 가장 Reward가 높음을 알 수 있다. TreeNet은 두 환경 모두에서 가장 낮다.\n6-다리 모델과 4-다리 모델로 pretraining을 진행한 후 transferable을 실험했다.\n\n1-2. Zero-shot\n\nfine tuning 없이 퍼포먼스를 측정했다.\n퍼포먼스를 쉽게 비교할 수 있도록 average reward와 average running-length를 normalization해서 색으로 아래와 같이 표현했다.(green-good, red-bad)\n\n\n\n\n\n\n\n눈으로 확실히 확인할 수 있듯이 NerveNet의 퍼포먼스가 다른 비교군에 비해 월등히 transferable함을 알 수 있었다.\n또한 learning curve에서 볼 수 있듯이 NerveNet+Pretrain 이 다른 Pretrain 비교군들에 비해 훨씬 높은 reward 시작점에서 시작하고 더 적은 timestep으로 solved 점수에 도달하는 것을 보아 그래프의 구조적 이점을 확실히 활용하고 있음을 알 수 있다.\n\n\n\n\n\n\n\nNerveNet의 agent들은 다른 비교군 agent들에서 보이지 않는 walk-cycle을 가지고 있음을 확인할 수 있었는데, 이는 보행 로봇들은 걸음새에서 반복적인 움직임을 하게 되어 있기 때문에 자연스럽게 cycle을 가지게 되는 것을 agent가 학습했음을 알 수 있다. (반면 MLP는 8-다리 모델에서 모든 다리를 움직이지 않는 모습을 보이기도 했다.)\n\nSnake\n\nsnake환경에서도 NerveNet이 다른 비교군들에 비해 뛰어난 reward 점수를 보여주며 transferable 함을 아래의 도표에서처럼 보여주었다.\n350점 정도가 snakeThree에서 solved된 상태라고 볼 수 있는데 NerveNet의 시작 점수들이 대부분 300점대에서 시작한 것으로 보아 이는 상당한 zero-shot 역량이 있음을 알 수 있다.\n다른 비교군들은 overfitting이 심해서 Random보다 안좋은 결과를 보여주는 점도 흥미롭다.\n\n\n\n\n\n\n\nzero-shot 뿐만 아니라 fine tuning을 하는 learning curve에서도 NerveNet은 Pretrain의 이점을 다른 비교군들에 비해 잘 활용하고 있음을 볼 수 있었다. NerveNet+Pretrain의 시작 reward가 높으며, 특정 size transfer 실험에서는 scratch NerveNet이 넘지 못한 MLP 점수를 NerveNet+Pretrain이 따라잡았다."
  },
  {
    "objectID": "posts/paper/2022-06-10-NerveNet.html#multi-task-learning",
    "href": "posts/paper/2022-06-10-NerveNet.html#multi-task-learning",
    "title": "📃NerveNet 리뷰",
    "section": "3. Multi-task learning",
    "text": "3. Multi-task learning\nNerveNet은 네트워크에 structure prior를 포함한 것이기 때문에 multi-task learning에 유리할 수 있다. 따라서 이를 실험하기 위해 Walker multi-task learning을 진행했다.\n\n2d-walker 환경들 5개 - Walker-HalfHumanoid, Walker-Hopper, Walker-Horse, Walker-Ostrich, Walker-Wolf\n1개의 통합된 network로 학습\n\n비교군\n\nNerveNet : agent들의 형태가 달라 weight들이 다를 수 밖에 없기 때문에 propagation과정에서의 weight matrices와 output만 공유했다.\nMLP Sharing : hidden layer들 간의 weight matrices 를 공유\nMLP Aggregation : 차원이 다른 observation들을 aggregation과정을 통해 첫번째 hidden layer의 크기로 다 맞춰주어서 input으로 넣어줌\nTreeNet: TreeNet도 weight를 공유를 할 수 있지만 agent의 구조적인 정보는 알 수 없다. 단순히 root node를 중심으로 모든 노드의 정보다 aggregation 되기 때문이다.\nMLPs: 각 agent마다 따로 MLP policy를 만들어서 학습(single-task)\n\nResult - multi-task learning 실험이기 때문에 한 두개 러닝 그래프만 볼 수 없고 5개의 러닝 그래프를 같이 봐야 한다. - Single-task policy를 제외하고 모든 환경에서 NerveNet의 퍼포먼스가 좋음을 알 수 있다.\n\n\n\n\n\n\n테이블에서 Ratio가 single-task policy에 비해 multi-task policy의 성능을 percentage로 나타낸 수치인데, MLP의 퍼포먼스가 single-task에서 multi-task로 넘어갔을 때 42%나 퍼포먼스가 줄어드는 것을 확인할 수 있다. (Average-58.6%) 반면에 NerveNet은 성능이 전혀 떨어지지 않는 결과를 보여주었다."
  },
  {
    "objectID": "posts/paper/2022-06-10-NerveNet.html#robustness-of-learnt-policies",
    "href": "posts/paper/2022-06-10-NerveNet.html#robustness-of-learnt-policies",
    "title": "📃NerveNet 리뷰",
    "section": "4. Robustness of learnt policies",
    "text": "4. Robustness of learnt policies\n강화학습 제어에서 robustness는 중요한 지표인데 질량이나 힘과 같은 물리적인 값들의 오차 범위가 어느정도까지 policy가 허용하고 잘 작동하는지를 확인해야 한다.\n\n5개의 Walker 그룹의 환경에서 실험\npretrained agent를 가지고 agent의 질량과 joint의 strength을 변경한 뒤 퍼포먼스 측정\n대부분의 환경과 variation에서 NerveNet의 robustness가 MLP보다 좋음을 알 수 있다."
  },
  {
    "objectID": "posts/paper/2022-06-10-NerveNet.html#interpreting-the-learned-representations",
    "href": "posts/paper/2022-06-10-NerveNet.html#interpreting-the-learned-representations",
    "title": "📃NerveNet 리뷰",
    "section": "5. Interpreting the learned representations",
    "text": "5. Interpreting the learned representations\n실제 폴리시들이 어떤 representation들을 학습했는지 알아보기 위해 CentipedeEight 환경에서 학습된 agent의 final state vector를 가지고 2D, 1D PCA를 진행했다.\n각 다리쌍들(Left Hip-Right Hip)들은 agent의 전체 몸체에서 각기 다른 위치에 있음에도 불구하고 invariant representation을 배울 수 있었음을 PCA를 통해서 알 수 있었다.\n\n\n\n\n\n또한 앞서 Centipede transfer learning 실험 결과에서도 잠깐 언급했던 walk-cycle이 주기성이 뚜렷하게 보였다."
  },
  {
    "objectID": "posts/paper/2022-06-10-NerveNet.html#comparison-of-model-variants",
    "href": "posts/paper/2022-06-10-NerveNet.html#comparison-of-model-variants",
    "title": "📃NerveNet 리뷰",
    "section": "6. Comparison of model variants",
    "text": "6. Comparison of model variants\nValue Network를 어떻게 할 것인지에 따라 NerveNet의 여러 변형이 있을 수 있는데 Swimmer, Reacher, HalfCheetah에서 비교해본 결과, Value Network는 MLP로 한 NerveNet-MLP의 퍼포먼스가 가장 좋았고 NerveNet-1의 퍼포먼스가 2등으로 NerveNet-MLP 와 비슷했다. 이에 대한 잠재적인 이유로 value network와 policy network가 weight를 공유하는 것이 PPO 알고리즘에서의 trust-region based optimitaion에서의 weight \\alpha를 더 sensitive하게 만들기 때문이라고 추론할 수 있다."
  },
  {
    "objectID": "posts/paper/2025-07-22-seqdex.html",
    "href": "posts/paper/2025-07-22-seqdex.html",
    "title": "📃Sequential Dexterity 리뷰",
    "section": "",
    "text": "Paper Link\nProject Link"
  },
  {
    "objectID": "posts/paper/2025-07-22-seqdex.html#논문-개요-및-문제-제기",
    "href": "posts/paper/2025-07-22-seqdex.html#논문-개요-및-문제-제기",
    "title": "📃Sequential Dexterity 리뷰",
    "section": "2.1 논문 개요 및 문제 제기",
    "text": "2.1 논문 개요 및 문제 제기\nSequential Dexterity는 긴 시퀀스로 이루어진 복잡한 조작 작업을 다중 서브 정책(sub-policy)을 연쇄하여 수행하는 강화학습 기반 프레임워크입니다. 현실 세계의 조작 과업들은 여러 서브태스크(예: 탐색, 정렬, 파지, 삽입)로 구성되는 경우가 많으며, 각 단계의 요구사항이 상이합니다. 사람의 손과 유사한 다지(多指) 로봇 손(dexterous hand)은 도구 교체나 재파지 없이 다양한 조작 모드를 전환할 수 있어 이러한 장기 다단계 작업에 잠재력이 크지만, 동시에 자유도와 행동 공간이 고차원이라 학습과 제어가 매우 어려운 도전과제를 제시합니다.\n기존에는 전체 작업을 단일 정책으로 학습하기 어려워 태스크를 분할한 뒤 각 단계를 순차 실행하는 접근이 검토되었으나, 나이브한 순차 실행은 앞 단계에서 도달한 상태가 다음 단계 정책이 학습된 분포를 벗어나는 경우 쉽게 실패하게 됩니다. 다시 말해, 한 단계의 종료 상태가 다음 단계 정책의 시작 상태 분포 밖에 있다면 연쇄에 실패하게 됩니다. 이러한 상태 분포 불일치 문제와 긴 호라이즌에 걸친 복잡한 동적 상호작용은 다단계 정책 연쇄의 핵심 난제로 지목됩니다. Sequential Dexterity 논문은 이러한 문제를 해결하고자, 정책 체이닝(policy chaining)을 위한 새로운 양방향 최적화 프레임워크를 제안합니다. 이를 통해 시뮬레이션에서 학습한 복수의 손 동작 정책들을 한 데 묶어 복잡한 목표를 달성하고, 학습에 사용하지 않은 새로운 물체 형태에도 일반화하며, 나아가 추가 학습 없이 실물 로봇에 적용(제로샷 전이)할 수 있음을 시연하였습니다."
  },
  {
    "objectID": "posts/paper/2025-07-22-seqdex.html#방법론-상세-분석",
    "href": "posts/paper/2025-07-22-seqdex.html#방법론-상세-분석",
    "title": "📃Sequential Dexterity 리뷰",
    "section": "2.2 방법론 상세 분석",
    "text": "2.2 방법론 상세 분석\n 그림 1: Sequential Dexterity 프레임워크 개요. (a) 시뮬레이션에서 각 서브태스크별 정책 \\pi^1, \\pi^2, \\pi^3, \\pi^4를 순차 학습하는 순방향 초기화(파란색)와, 학습 완료 후 이전 정책을 미세조정하는 역방향 파인튜닝(빨간색) 과정. \\rho^i는 정책 \\pi^i의 상태 분포, F^i는 서브태스크 i와 i!-!1 사이의 전이 가능성 함수. (b) 실 로봇 적용 시 정책 입력으로 6D 물체 자세와 로봇 상태가 주어지며, 정책 선택은 최근 10스텝 상태를 고려한 F^i 출력이 임계값 h^i를 넘는지에 따라 이루어진다. 단계 \\pi^2 Orient, \\pi^3 Grasp, \\pi^4 Insert를 실 로봇에서 수행하는 모습.\nSequential Dexterity의 핵심 아이디어는 긴 조작 작업을 구성하는 각 단계를 독립적인 하위 정책으로 정의하고 강화학습으로 학습한 뒤, 이들을 체계적으로 연결하여 전체 작업을 수행하도록 만드는 것입니다. 이를 위해 논문에서는 양방향 최적화(bi-directional optimization)라고 불리는 학습 절차를 도입하였습니다. 우선, 순방향 초기화 단계에서는 각 서브태스크마다 개별 정책을 학습합니다. 이때 PPO 알고리즘 등의 강화학습 방법을 사용하여 각 서브태스크를 MDP로 정의하고 정책을 훈련하며, 이전 단계의 종료 상태 분포를 활용해 다음 단계 정책의 초기 상태로 샘플링함으로써 연쇄 시 발생할 상태 분포를 미리 포함시킵니다. 이렇게 하면 각 정책 \\pi^i가 전 단계 \\pi^{i-1}가 만들어낼 법한 상태에서도 동작하도록 전방위 학습을 수행할 수 있습니다.\n이후 역방향 파인튜닝 단계에서는 전이 가능성 함수(Transition Feasibility Function, F^i)를 활용하여 이웃한 정책 사이의 접속을 더욱 최적화합니다. 전이 가능성 함수 F^i(s)란 서브태스크 i의 초기 상태 s에 대해 해당 상태에서 정책 \\pi^i를 시작했을 때 성공 가능성이나 성과를 출력하는 함수입니다. 논문에서는 각 \\pi^i를 학습한 후, \\pi^{i-1}의 종료 상태(즉 \\pi^i의 시작 상태)들과 그로부터 \\pi^i를 실행했을 때 얻은 누적 보상 또는 성공 여부를 모아서 F^i를 지도 학습으로 훈련합니다. 이렇게 학습된 F^i는 상태 s가 다음 단계 정책에 얼마나 적합한지를 예측하는 역할을 하며, 마치 다음 정책의 가치 함수처럼 동작하지만 일반적인 RL 가치 함수와 달리 할인되지 않은 최종 성과에 집중합니다. 실제 PPO 등으로 얻은 가치 함수는 할인의 영향으로 후속 정책의 성공 가능성을 제대로 평가하지 못하는데, 저자들은 별도의 F^i 학습으로 이 문제를 해결하였다고 보고합니다. 특히 F^i에는 최근 몇 스텝 동안의 상태 변화를 입력으로 사용하여 시간적 맥락도 반영하였는데, 이렇게 과거 상태 이력을 통합함으로써 단순 단일 상태 기준보다 전이 성공 여부를 정확히 예측할 수 있었습니다.\n이 전이 가능성 함수를 이용해 이전 단계 정책을 역방향으로 미세조정합니다. 구체적으로, 정책 \\pi^{i-1}의 학습에 F^i를 통한 추가 보상 또는 규제 항을 부여하여, \\pi^{i-1}이 종료 시 상태를 F^i 기준으로 높은 전이 점수를 얻는 방향으로 유도합니다. 예를 들어 서브태스크 2 (Orient)의 전이 함수 F^3가 “이 상태에서 서브태스크 3 (Grasp)이 성공할 가능성”을 나타낸다면, \\pi^2의 종료 상태가 F^3 상 높은 점수를 받도록 \\pi^2를 업데이트하는 식입니다. 이를 통해 각 하위 정책의 종료 상태 분포를 다음 정책의 초기 상태 분포와 최대한 일치시키며, 결과적으로 정책 연쇄의 성공률을 극대화합니다. Lee 등(2021)의 T-STAR와 같은 기존 기술 연쇄 기법들이 종료 상태 분포를 줄이는 대항훈련(어드버서리) 방식으로 접근한 것과 대비되며, Sequential Dexterity는 보다 직접적으로 후속 성공 가능성을 예측하여 활용한다는 차별점이 있습니다.\n마지막으로, 정책 체이닝 실행 단계에서 중요한 것이 자율적인 정책 전환 메커니즘입니다. 고정된 단계 순서로 한 번씩만 정책을 실행하는 대신, 전이 가능성 함수를 활용하여 실시간으로 적절한 정책을 선택합니다. 구체적으로 로봇이 작업 도중 매 시점마다 현재 상태를 가지고 각 다음 가능 단계들의 F 값을 평가하고, 임계값을 넘겨 실행 준비가 된 것으로 판정된 가장 후순위 단계부터 실행합니다. 예를 들어 블록 쌓기 작업에서, 현재 상태에서 삽입 단계가 바로 가능하다고 판단되면 (예: 블록을 이미 손에 쥐고 있고 자세도 맞다면) 중간 단계를 건너뛰고 삽입을 수행합니다. 반대로, 만약 현재 단계 수행이 잘못되어 목표 상태에 도달하지 못하더라도, F가 알려주는 바에 따라 이전 단계로 되돌아가 교정할 수도 있습니다. 실제로 저자들은 정책 전환을 역순으로 탐색하는 방식을 취하는데, 삽입(\\pi^4)→파지(\\pi^3)→정렬(\\pi^2) 순으로 각각의 F 값을 확인해 가능한 가장 진전된 단계로 점프하거나, 모든 F 출력이 기준 미만이면 처음부터 탐색(\\pi^1)을 다시 실행하도록 설계했습니다. 이러한 동적 스위칭은 실패 복구와 불필요한 단계 생략을 모두 가능케 하여 체이닝의 강인성을 크게 높였습니다. 요컨대, Sequential Dexterity는 학습 단계에서 각 정책이 연쇄에 적합하도록 양방향으로 최적화되고, 실행 단계에서는 전이 가능성 함수 기반의 지능형 전환을 통해 긴 계획을 견고하게 수행합니다.\n각 서브태스크의 보상 설계는 해당 단계의 목표 완료에 중점을 두고 이루어졌습니다. 예를 들어 “탐색” 정책에는 올바른 블록을 식별하거나 접근했을 때의 보상, “정렬” 정책에는 물체의 자세를 목표 자세에 가깝게 만들수록 부여되는 보상, “파지”에는 성공적으로 블록을 쥐었을 때의 보상 등이 주어집니다. Sparse reward(희소 보상)보다 shaping 보상을 적절히 추가하여 학습을 유도했을 것으로 추측되며, 최종 단계인 “삽입”도 블록이 목표 위치에 안착하면 보상을 얻도록 설계되었습니다. 특히 역방향 파인튜닝 시에는 앞서 언급한 대로 F^i의 출력을 일종의 추가 보상으로 취급하여 이전 정책의 종료 상태를 향상시키는 데 사용하였고, 이는 기존 보상과 합쳐져 에이전트가 다음 단계의 성공까지 염두에 둔 행동을 하도록 합니다. 이러한 보상 체계와 전이 함수 활용을 통해 부분 최적해에 머무르기 쉬운 개별 정책들을 전체 작업의 성공이라는 글로벌 목표에 수렴시키도록 유도한 것이 본 방법론의 특징입니다."
  },
  {
    "objectID": "posts/paper/2025-07-22-seqdex.html#실험-설정-및-벤치마크",
    "href": "posts/paper/2025-07-22-seqdex.html#실험-설정-및-벤치마크",
    "title": "📃Sequential Dexterity 리뷰",
    "section": "2.3 실험 설정 및 벤치마크",
    "text": "2.3 실험 설정 및 벤치마크\n본 논문에서는 제안한 Sequential Dexterity 프레임워크를 시뮬레이션 실험과 실 로봇 실험을 통해 검증했습니다. 사용된 시뮬레이터는 대규모 병렬 환경을 지원하는 Isaac Gym으로, 각 서브태스크 정책을 학습할 때 최대 1024개의 환경에서 병렬로 데이터를 수집함으로써 학습 효율을 높였습니다. 실험은 두 가지 대표적인 다단계 조작 과제에 대해 이루어졌으며, 각 과제에 대해 제안 기법의 성능을 기존 방법들과 비교 평가했습니다.\n\n과제 1: 블록 조립 (Building Blocks) – 다양한 색상과 크기의 블록들이 섞여있는 상자에서 지정된 블록을 찾아 (탐색), 손가락을 이용해 적절한 자세로 정렬한 후, 블록을 파지하여 들어 올리고, 목표 구조물의 해당 위치에 삽입하는 일련의 작업입니다. 이 과제는 Mega Bloks 장난감을 사용한 간이 조립 작업으로 볼 수 있으며, 전체 구조를 완성하기 위해 이러한 네 가지 서브태스크를 블록 개수만큼 반복 수행해야 하는 복합 장기과업입니다. 시뮬레이션 환경에서는 블록들이 무작위로 쌓여있고 로봇은 손목 카메라 등의 감각을 통해 블록을 인식하게 됩니다. 실험에 사용된 로봇은 다관절 로봇 손(4-finger dexterous hand)을 장착한 로봇 팔로, 시뮬레이션에서 학습한 정책을 그대로 적용하여 실제 플라스틱 블록 조립을 수행했습니다. 실 로봇 환경에서는 상단에 장착된 카메라로 테이블 위 블록들의 대략적 위치를 파악하고, 손목에 장착된 RGB-D 카메라 영상에서 목표 블록을 분할(segmentation) 및 추적하여 실시간 6D 자세를 추정하는 비전 파이프라인을 구축했습니다. 이 정보를 실시간으로 정책에 입력하여 로봇이 블록을 찾아 집을 수 있도록 했습니다. 다만 시뮬레이터에서 블록을 홈에 끼워 넣는 삽입 동작의 아주 세밀한 부분(블록을 완전히 눌러 끼우는 과정)은 접촉 모델 한계로 구현이 어려웠기 때문에, 시뮬레이션에서는 삽입 정책이 블록을 목표 위치에 대략 놓는 것으로 학습되었습니다. 실 로봇 실험에서는 블록을 제자리에 올려놓는 것까지를 학습된 정책으로 수행하고, 마지막에 로봇 팔로 블록을 아래로 눌러 완전히 끼우는 동작은 스크립트로 처리하는 방식으로 실제 조립을 구현했습니다. 이러한 부분은 본 방법의 물리 한계를 보완하기 위한 조치로, 이후 한계점에서 추가 논의됩니다.\n과제 2: 공구 자세 맞추기 (Tool Positioning) – 테이블 위에 임의 자세로 놓인 공구를 파지하여 들어올린 뒤, 사용하기 편한 준비 자세로 재정렬(reorient)하는 작업입니다. 실험에서는 망치(hammer)를 대표적인 공구로 사용하여 학습하였고, 초기에는 망치가 바닥에 아무렇게나 놓여 있습니다. 로봇 손이 망치를 집어 드는 동작(Grasp)이 1단계, 이어서 공구를 회전시켜 손잡이 부분이 아래로 향하도록 세우는 동작(Orient)이 2단계로 구성됩니다. 이 과제는 두 단계의 체이닝이지만, 핵심은 첫 파지의 방식이 두 번째 단계의 난이도에 큰 영향을 준다는 점입니다. 예컨대 망치를 엉뚱한 각도로 집으면 회전 단계에서 균형을 잡기 어렵거나 충돌이 발생해 실패할 수 있습니다. 따라서 초기 파지 단계부터 전체 과업을 고려한 최적의 방식을 취하는 것이 중요합니다. 이 과제 역시 시뮬레이션으로 학습한 후 실제 로봇으로 제로샷 검증되었으며, 추가로 일반화 성능 평가를 위해 학습에 사용하지 않은 공구(주걱, 숟가락 등의 도구)를 대상으로도 실험이 수행되었습니다.\n\n 그림 2: 실험 환경 및 비교. (a) 블록 조립 과제 – 시뮬레이터(왼쪽)와 실제 로봇(오른쪽)의 작업 환경. 로봇 팔 끝에 4손가락 로봇 핸드와 손목 카메라(빨간 상자), 상단에 전역 카메라(파란 상자)가 배치되어 블록을 탐색하고 조립한다. (b) 공구 자세 맞추기 과제 – 초기 상태에 놓인 망치를 대상으로, Baseline(기존 순차 실행 방법) vs Ours(제안 방법)의 비교 예시. Baseline은 1단계 파지 이후 망치를 비스듬히 잡아 2단계 정렬에 실패(빨간 X)하지만, 제안 방법은 첫 파지부터 각도를 고려해 잡음으로써 최종 정렬에 성공(녹색 체크)한다.\n벤치마크 방법으로는 다음과 같은 비교 대상이 선정되었습니다. 첫째, 각 서브태스크를 개별적으로 학습한 후 아무 조정 없이 순서대로 실행하는 기본 정책 연쇄 방법이 있습니다. 이는 전이 가능성 함수를 사용하지 않고 단순히 정해진 순서로 한 번씩 정책을 적용하는 것으로, 전이 분포 불일치 문제를 그대로 가지는 baseline입니다. 둘째, Lee 등(2021)이 제안한 T-STAR 기법 등 기존 기술 체이닝 알고리즘과의 비교가 수행되었습니다. T-STAR는 종료 상태 분포를 제한하기 위해 적대적 학습을 활용했던 선행 연구로, 본 논문의 문제 설정(특히 다지 로봇 손 사용)과는 차이가 있지만 아이디어상 인접한 기법입니다. 셋째, V-Chain이라고 명명된 비교 방법은 다음 단계 정책의 RL 가치함수를 전이 평가에 활용하여 이전 정책들을 파인튜닝하는 방식으로, 전이 가능성 함수를 쓰는 대신 PPO로 학습된 가치망으로 연쇄를 시도한 경우입니다. 저자들은 이를 통해 전이 가능성 함수의 이점을 검증하고자 했습니다. 넷째, Ours w/o temporal이라고 하는 변형 모델은 제안한 전이 가능성 함수에서 시간적 이력(상태 시퀀스) 입력을 제외하고 현재 상태만으로 전이 판정을 하도록 한 버전입니다. 이를 통해 전이 함수에 시간 정보를 포함시키는 것이 성능에 미치는 영향을 확인하였습니다. 마지막으로, 본 논문의 Ours (제안 기법)은 양방향 최적화 + 전이 가능성 함수 + 자율 스위칭의 풀셋을 사용한 완전한 Sequential Dexterity 시스템입니다.\n평가 지표로는 각 과제에서 최종 목표를 성공적으로 달성했는지의 성공률이 주요하게 사용되었습니다. 블록 조립의 경우 주어진 구조물을 모두 완성하는 데 성공한 실험 에피소드 비율로 측정되며, 공구 자세 맞추기는 공구를 바른 자세로 세워서 안정적으로 유지하는 데 성공한 비율로 정의됩니다. 이 과정에서 서브태스크별 단계 성공률이나 수행 시간, 정책 전환 횟수 등도 부가적으로 관찰되었으나, 가장 중요한 평가지표는 전체 작업의 완수 여부였습니다. 또한 정성적 평가로 각 과정의 행동 궤적과 최종 상태를 비교하고, 정책 전환 전략에 따른 성공/실패 사례를 분석하였습니다. 일반화 평가로는, 앞서 언급한 대로 공구 과제에서는 학습 시 보지 못한 새로운 도구(예: 주걱, 숟가락)를 투입하여 성공 여부를 측정하였고, 블록 조립 과제에서는 새로운 구조 조립(예: 다른 형태의 블록 조합 구조)에 대한 적용을 살펴보았습니다. 끝으로, 실제 로봇 검증에서는 시뮬레이션에서 학습한 정책을 그대로 사용하여 실제 환경에서 작업을 수행하도록 하고, 그 성공 사례 영상과 빈도를 보고하였습니다. 성공률의 경우 시뮬레이션만큼 다수 반복하지는 못했지만, 몇 차례 시연에서 지속적으로 작업 완수에 성공하는 모습을 보여 제로샷 학습 전이의 가능성을 확인했습니다."
  },
  {
    "objectID": "posts/paper/2025-07-22-seqdex.html#결과-분석",
    "href": "posts/paper/2025-07-22-seqdex.html#결과-분석",
    "title": "📃Sequential Dexterity 리뷰",
    "section": "2.4 결과 분석",
    "text": "2.4 결과 분석\n실험 결과, Sequential Dexterity 프레임워크는 제안한 요소들이 모두 결합되었을 때 기존 방법 대비 현저한 성능 향상을 이루는 것으로 나타났습니다. 특히 전체 작업 성공률 지표에서 두 과제 모두 유의미한 개선을 달성했습니다.\n먼저 블록 조립 과제를 살펴보면, 제안 기법은 약 80% 이상의 최종 성공률을 기록하여, 기본 순차 실행 방식(전이 최적화 없이 각 단계 한 번씩 수행) 대비 약 30%포인트 이상 상승한 성과를 보였습니다. 구체적으로, baseline의 경우 여러 번의 시행 중 절반도 안 되는 에피소드에서만 구조 완성에 성공한 반면, Sequential Dexterity를 적용하면 10번 중 8번 이상은 로봇이 모든 블록을 제자리에 정확히 쌓아올릴 수 있었습니다. 공구 자세 맞추기 과제에서도 유사한 경향으로, 제안 기법은 약 90%에 가까운 성공률을 달성하여 baseline(약 60% 수준)보다 크게 앞섰습니다. 이로써 정책 체이닝의 효과가 정량적으로 입증되었는데, 특히 전이 가능성 함수 기반의 자율 스위칭이 없었던 경우(정책 전환 허용 횟수 0일 때) 성공률이 크게 낮았다가, 스위칭을 1회, 2회, 3회로 늘릴수록 성공률이 단계적으로 향상되는 양상을 보였습니다. 그림 3-(a)에서 보이듯이, 정책 스위칭을 전혀 허용하지 않으면 블록 조립 성공률은 50% 미만에 머물렀으나, 최대 3회까지 재시도를 허용하자 약 80% 수준까지 높아졌습니다. 마찬가지로 공구 과제도 0회 스위칭 시 ~60%에서 3회 시 ~90%로 상승하여, 동적 정책 전환이 복잡한 조작을 성공으로 이끄는 데 핵심적인 역할을 함을 알 수 있습니다.\n 그림 3: 성능 지표 및 전이 함수 효과 분석. (a) 정책 스위칭 허용 횟수에 따른 최종 성공률 – 스위칭을 늘릴수록 블록 조립(파란색)과 공구 조정(주황색) 과제 모두 성공률이 크게 향상됨을 보여준다. (b) 물체 자세 분포 비교(T-STAR vs Ours) – 블록 조립 과제의 정렬 단계에서, 기존 방법(T-STAR)은 로봇이 다양한 각도로 블록을 배치하여 후속 파지 단계에 진입하지만(위, 파란점 분포), 제안 방법은 블록의 돌기가 위로 향한 자세에 대부분 모이도록 유도함으로써 이후 파지와 삽입을 쉽게 만들고 성공 확률을 높인다(아래). 이는 전이 가능성 함수가 다음 단계 성공에 최적인 상태를 학습하고 이전 단계 정책을 그 방향으로 유도한 결과이다.\n전이 가능성 함수의 활용은 단순한 성공률 상승뿐 아니라, 각 단계의 정성적 동작 품질도 향상시켰습니다. 예를 들어 블록 조립에서 정렬(Orient) 정책은 블록의 방향을 어떻게 잡느냐가 이후 파지 및 삽입 성공에 결정적인데, 제안 기법은 정렬 단계에서 블록의 돌기가 위로 향하도록 회전시키는 경향을 보였습니다. 이는 사람이 레고 블록을 끼울 때 윗면 돌기가 위로 오도록 맞추는 것과 유사한 전략으로, 이러한 계획적 동작은 전이 가능성 함수가 “이 자세가 다음 단계에 유리하다”는 신호를 주었기 때문입니다. 반면, 전이 함수를 쓰지 않은 방법이나 T-STAR 등의 기존 접근은 정렬 단계에서 다양한 각도로 블록을 놓아 후속 단계에 더 넓은 초기 분포를 허용하지만, 오히려 그 때문에 파지나 삽입 단계에서 불필요한 어려움이 생길 수 있습니다. 그림 3-(b)는 이러한 차이를 잘 보여주는데, T-STAR 방식으로 최적화된 정책은 블록의 최종 자세 분포가 비교적 넓게 퍼져 있는 반면(Ours 이전의 참고용), Sequential Dexterity의 정책은 한정된 범위(거의 돌기가 위쪽을 향한 영역)에 몰려 있습니다. 이렇듯 우리 방법이 목표 상태 공간을 효과적으로 제한함으로써 전체 연쇄의 성공 가능성을 높였음을 알 수 있습니다.\nBaseline 대비 성능 개선은 개별 사례 비교를 통해서도 드러납니다. 공구 자세 맞추기 과제의 경우, 기존 순차 실행 방식(baseline)은 첫 파지에서 망치를 옆으로 눕힌 채 잡는 경우가 많았고, 이 상태에서는 아무리 회전하려 해도 망치머리가 걸려 자세 변경에 실패하곤 했습니다 (위 그림 2-(b) 빨간 X 상황). 반면 Sequential Dexterity를 통해 학습된 정책은 첫 단계에서부터 망치를 세울 것을 염두에 두고 손잡이 부분을 아래쪽으로 향하게 잡는 전략을 취했습니다. 그 결과 두 번째 단계에서 자연스럽게 망치를 똑바로 세울 수 있었고 최종적으로 안정된 자세를 만들 수 있었습니다 (그림 2-(b) 녹색 체크 상황). 이처럼 초기 단계에서의 적절한 결정이 후속 단계 성공으로 이어지는 사례가 다수 관찰되었습니다.\n일반화 성능 측면에서도 고무적인 결과가 보고되었습니다. 공구 과제에서 학습에 사용하지 않은 주걱, 숟가락 등의 도구에 대해서도 제안 정책은 큰 성능 저하 없이 작업을 수행해냈습니다. 예컨대 숟가락의 넓적한 부분을 잡을 때에도 학습된 망치 파지 정책을 응용하여 비슷한 원리로 파지 및 세우기 동작을 성공시켰습니다. 성공률 수치는 망치에 비해 다소 낮아질 수 있으나, baseline과 비교하면 여전히 높은 성공 비율을 유지하며 우수한 일반화 능력을 시현했습니다. 블록 조립의 경우 학습에는 몇 가지 크기와 색상의 블록만 사용되었지만, 테스트에서는 처음 보는 형태의 블록(예: 다른 모양의 조립 블록)을 투입해도 손쉽게 적응하는 모습을 보였습니다. 이는 제안한 정책이 물체의 정확한 CAD 모델에 의존하기보다는 시각/촉각 피드백에 기반한 조작 스킬을 학습했음을 시사합니다. 또한 블록의 초기 배치나 목표 구조의 형태가 달라져도, 주어진 조립 설명서만 바꾸면 (예: GUI로 원하는 구조를 지정) 로봇이 동일한 연쇄 정책으로 새로운 구조를 쌓아올렸습니다. 이러한 태스크 적응력은 본 기법의 범용성을 뒷받침하는 증거입니다.\n끝으로, 실제 로봇 실험 결과 Sequential Dexterity의 실용 가능성을 보여주었습니다. 시뮬레이션에서 학습한 정책들을 그대로 이식한 로봇은, 카메라로 식별한 블록을 찾아 집어서 구조물을 쌓는 일련의 동작을 성공적으로 수행했습니다. 논문에 공개된 동영상을 보면 로봇이 여러 형태의 블록 구조 (예: 다층 탑, 특정 패턴 조립 등)를 사람의 도움 없이 완성하는 인상적인 시연이 포함되어 있습니다. 특히 추가 학습이나 미세조정 없이 시뮬레이션 정책을 바로 사용했다는 점에서, 시뮬레이션-현실 간 격차(sim2real gap)를 최소화한 정책 학습의 효과를 실증했다고 볼 수 있습니다. 물론 완벽한 제로샷 전이를 위해 몇 가지 엔지니어링이 더해졌습니다. 예를 들어, 시뮬레이터에서는 손가락 움직임이 빠르게 튀는 진동 현상이 있었으나, 실제 로봇에서는 이를 지수평활 필터로 평탄화하여 제어의 안정성을 높였습니다. 또한 앞서 설명한 대로 마지막 삽입 동작은 약간의 스크립트 보조를 받았습니다. 그럼에도 불구하고, 학습된 부분(탐색정렬파지)만으로도 로봇이 스스로 블록을 집어 정확한 자세로 위치시키는 건 기존에는 보기 드문 성과로, 모델 자유 강화학습(model-free RL) 기법으로 이렇게 복잡한 실제 조립 작업을 성공한 첫 사례 중 하나로 평가됩니다."
  },
  {
    "objectID": "posts/paper/2025-07-22-seqdex.html#한계점-및-향후-연구-방향",
    "href": "posts/paper/2025-07-22-seqdex.html#한계점-및-향후-연구-방향",
    "title": "📃Sequential Dexterity 리뷰",
    "section": "2.5 한계점 및 향후 연구 방향",
    "text": "2.5 한계점 및 향후 연구 방향\nSequential Dexterity는 긴 수평선(long-horizon)의 다단계 작업에 대한 새로운 접근법을 제시했지만, 여전히 몇 가지 한계점이 존재하며 향후 개선 여지가 있습니다.\n첫째, 서브태스크의 분할과 정의가 사람에 의해 사전에 주어지는 점입니다. 본 논문에서는 블록 탐색, 정렬, 파지, 삽입의 4단계와 공구 파지, 정렬의 2단계를 전제로 했습니다. 이러한 서브태스크 분할은 도메인 지식을 활용한 것으로, 복잡한 문제를 풀기 쉽게 만든 장점이 있지만 범용 인공지능적 접근과는 거리가 있습니다. 향후에는 로봇이 스스로 서브태스크를 발견하거나, 고수준 목표만 주어져도 내부적으로 적절한 정책 체인을 구성할 수 있는 자동 계층화 학습으로 확장될 수 있을 것입니다.\n둘째, 학습 비용의 문제입니다. 각 서브 정책을 강화학습으로 훈련하는 데 상당한 시뮬레이션 시간이 걸렸습니다. 논문에 따르면 블록 조립의 경우 서브태스크 하나를 학습하는 데 하루~이틀 정도 소요되며, 모든 단계를 순차/역방향으로 최적화하는 데 상당한 계산자원이 필요했습니다. 이는 현재 기술 수준에서 불가피한 면도 있지만, 향후 더 효율적인 학습 알고리즘(PPO 외에 모델 기반 기법 등)이나 지표함수 재사용 등을 통해 훈련 시간을 단축할 수 있을 것입니다. 또한 현재는 시뮬레이터 상에서 대량의 병렬 환경을 사용했는데, 실제 로봇 학습으로 이러한 기법을 확장하려면 표본 효율을 높이거나 인간 시연으로 초기 정책을 얻는 등의 추가 연구가 필요합니다.\n셋째, 시뮬레이션과 현실의 차이로 인한 한계가 있습니다. 논문에서도 지적했듯이 블록 삽입과 같은 접촉이 많은 세밀한 동작은 시뮬레이터에서 정확히 구현하기 어려웠고, 결국 일부를 스크립트에 의존했습니다. 이는 곧 접촉력, 마찰, 변형 등의 물리적 상호작용을 정확히 다루는 어려움을 뜻합니다. 향후 연구에서는 전이 가능성 함수에 이러한 접촉 안정성에 대한 평가를 포함시키거나, 시뮬레이션 단계에서부터 물리 파라미터를 다양화하는 도메인ラン덤화(domain randomization)를 통해 현실 적응력을 높일 수 있을 것입니다. 또한 촉각 센서나 힘 제어를 통합하여, 단순 시각 정보로 어려운 미세 접촉 동작의 성공률을 높이는 방향도 고려해볼 수 있습니다.\n넷째, 센싱 및 시스템 복잡도의 한계입니다. 본 연구는 시뮬레이션에서는 완전상태 관찰(ground-truth state)을 주로 사용하여 학습했고, 현실 실험에서는 이를 보완하기 위해 2대의 카메라와 세분화/추적 및 6D 포즈 추정 알고리즘(XMem, DenseFusion 등)을 사용했습니다. 이러한 인지 시스템은 추가적인 오류 가능성을 내포하며, 실제 적용 시 상당한 설정과 보정이 필요합니다. 향후에는 학습 단계에서부터 영상 입력을 받아 end-to-end로 정책을 학습하거나, 보다 경량의 센서 구성으로 동작할 수 있도록 단순화하는 연구가 필요합니다. 예를 들어, 손목 카메라 하나만으로도 목표 물체를 탐지하고 조작하도록 학습시키는 방향입니다. 이는 난이도가 높지만, 성공한다면 시스템 구성의 복잡도를 줄이고 실용성을 높일 수 있을 것입니다.\n다섯째, 정책 연쇄의 범위에 관한 한계입니다. Sequential Dexterity는 현재 고정된 순서의 스킬 체인 내에서만 동작합니다 (비록 스위칭으로 반복이나 생략은 가능하지만). 만약 작업 도중 전혀 새로운 하위 과제가 필요해지거나 분기(branch)가 발생하는 시나리오에서는 그대로 적용하기 어렵습니다. 예를 들어 조립 작업 중 블록이 떨어지거나 파손되는 등 예외 상황이 발생하면, 현재 체계로는 사전에 학습된 서브태스크로 대처할 수 없을 수 있습니다. 이러한 비정형 상황에 대응하려면 상위 계층의 플래닝 알고리즘이나 온디맨드 학습과 결합이 필요할 것입니다. 향후 연구는 체스와 같은 전략 게임에서 보듯이, 고수준 플래너가 서브 정책들을 호출하고 조합하는 계층적 제어 구조로 확장하는 방향을 제시할 수 있습니다. 또한 여러 대의 로봇 손이나 사람-로봇 협업 등의 다중 에이전트 상황에서도 이와 유사한 정책 체이닝 개념을 적용하려면 어떻게 할지 탐구해볼 수 있습니다.\n마지막으로, 다지 로봇 손 자체의 한계도 존재합니다. 사람 손과 유사한 로봇 핸드는 유연성이 높지만 제어가 어려워, 현재까지는 병렬 그리퍼 등에 비해 신뢰도가 낮은 편입니다. 논문에서도 평행그리퍼로는 수행이 어려운 작업이 본 과제였음을 강조하고 있으나, 반대로 말하면 다지 손이 반드시 필요하지 않은 작업에서는 체이닝 기법 없이도 성공할 수 있습니다. 그러므로 어떤 유형의 작업에 다지 손 + 체이닝 접근이 유리한지를 식별하고, 불필요한 경우에는 단순 그리퍼나 단일 정책으로도 충분한지를 구분하는 연구가 실용적 관점에서 중요합니다. 또한 다지 손의 고유한 문제인 관절 제약, 내구성, 제어 지연 등이 현실 적용에 걸림돌일 수 있어, 이러한 부분을 개선하거나 체이닝 기법이 그러한 하드웨어 한계를 완화할 수 있는지도 살펴봐야 합니다."
  },
  {
    "objectID": "posts/paper/2025-07-22-seqdex.html#총평-및-기여도-평가",
    "href": "posts/paper/2025-07-22-seqdex.html#총평-및-기여도-평가",
    "title": "📃Sequential Dexterity 리뷰",
    "section": "2.6 총평 및 기여도 평가",
    "text": "2.6 총평 및 기여도 평가\nSequential Dexterity 논문은 로봇 강화학습 분야에서 장기 horizon 과업을 풀기 위해 한 걸음 나아간 중요한 연구로 평가됩니다. 특히 다관절 로봇 손과 같이 제어가 까다로운 매니퓰레이터를 활용하여, 이전에는 사람이 개입하거나 실패율이 높았던 다단계 조작 작업을 자율적으로 학습 수행해낸 점에서 큰 의의가 있습니다. 이 논문의 주요 기여도(contribution)는 다음과 같이 정리할 수 있습니다:\n\n다단계 섬세 조작을 위한 정책 체이닝의 개척: 본 연구는 장기 다단계 조작 작업에 정책 연쇄 기법을 적용한 최초의 사례 중 하나로, 특히 다지 로봇 손을 이용한 정교한 작업에 초점을 맞추었습니다. 기존에는 주로 단순 그리퍼나 단일 단계 작업 위주로 연구되던 분야에, 복잡한 손 조작 시나리오를 제시하고 해결책을 모색했다는 점에서 학술적 가치를 지닙니다.\n일반적인 양방향 최적화 프레임워크 제안: 단순히 개별 실험 결과를 넘어서, 순방향 학습 + 전이 함수 기반 역방향 파인튜닝이라는 일반화 가능한 프레임워크를 구축하였습니다. 이 프레임워크는 향후 다른 형태의 다단계 작업이나 다른 로봇 플랫폼에도 응용될 수 있는 설계로서, 정책 체이닝에 대한 보편적인 방법론을 제시한 점이 높이 평가됩니다. 특히 value 함수가 아닌 별도의 전이 가능성 함수를 도입하고, 이를 정책 전환 전략까지 통합시킨 것은 본 논문의 기술적 창의성이라 할 수 있습니다.\n다단계 조작에서의 SOTA 성능과 실환경 검증: 제안한 방법을 통해 기존 방법들이 실패하거나 성능이 낮았던 복합 작업을 성공적으로 해결하였으며, 수치적으로도 높은 성공률을 입증했습니다. 예를 들어, 이전 강화학습 연구에서는 모델 프리 방식으로는 풀지 못했던 IKEA 가구 조립류의 작업을 본 연구와 유사한 접근으로 해결했고, 본 논문 역시 블록 조립 작업을 최초로 달성했습니다. 또한 학습된 정책을 바로 실제 로봇에 이식하여 작업 성공을 시연함으로써, 시뮬레이션 결과에 그치지 않고 현실 적용 가능성까지 보여주었습니다. 이는 로봇 학습 연구에서 상당히 까다로운 단계인데, 저자들의 통합적 노력(시뮬레이터 선정, 비전 모듈 개발, 제어 안정화 등)으로 이루어낸 성과입니다. 이러한 sim-to-real 성공 사례는 학계와 업계 모두에 영감을 주며, 향후 유사한 연구에 대한 신뢰도를 높이는 데 기여합니다.\n\n이 논문을 관련 문헌들과 비교해보면, 이전의 스킬 체이닝 연구들은 주로 상태 공간 확장이나 옵션 프레임워크 등을 통해 정책 연결을 시도했는데, Sequential Dexterity는 상태 분포의 폭발적 증가를 억제하는 방향으로 접근한 점이 독특합니다. 특히 T-STAR(Lee et al., 2021) 등이 adversarial training으로 분포를 제한한 데 반해, 본 논문은 학습된 전이 가능성 함수를 활용한 직접적인 보상 피드백으로 문제를 해결하여 안정적이면서도 효과적인 개선을 이루었습니다. 또한 과거 연구들이 병렬 그리퍼 등 제한된 조작에서 단순 작업(예: 블록 쌓기 2~3단계 정도)을 시연한 것에 비해, 본 연구는 다지 손의 풍부한 조작 가능성을 활용해 더 복잡한 작업을 했다는 점에서 실용적 가치가 있습니다. 사람의 손처럼 다양한 형태의 물체를 다뤄야 하는 과제에서 본 방법이 진가를 발휘할 수 있음을 보여준 셈입니다.\n물론 Sequential Dexterity에도 앞서 논의한 개선점들이 있지만, 현재 시점 기준으로 볼 때 이 논문은 장기 강화학습 제어와 로봇 조작의 교차점에서 상당한 진전을 이룬 것으로 평가됩니다. 학문적으로는, 긴 시간범위의 의사결정 문제를 해결하기 위해 계획(plan)과 학습(learning)을 접목한 한 가지 성공 예를 제시했고, 실용적으로는 향후 산업용 로봇이나 가정용 로봇의 복잡한 작업 학습에 적용될 수 있는 기술 방향을 제안했습니다. 특히, 전이 가능성 함수라는 개념은 추후 다양한 계층형 강화학습 시나리오에서 활용될 수 있는 아이디어로, 다른 연구자들이 이를 변형·확장하여 새로운 알고리즘을 만들 가능성도 있습니다. 예를 들어, 사람의 시연을 활용한 정책 연쇄 학습이나, 다중 로봇 협업 과제에 전이 가능성 평가를 도입하는 방향 등이 떠오릅니다.\n결론적으로, “Sequential Dexterity: Chaining Dexterous Policies for Long-Horizon Manipulation” 논문은 다단계 조작 문제에 도전한 혁신적인 연구로서, 정책 연쇄를 통한 장기 과업 해결이라는 로봇학습 분야의 난제를 한 단계 전진시켰습니다. 이 연구의 성과는 향후 로봇이 인간 수준의 다양하고 긴 작업을 수행하기 위해 어떤 학습 프레임워크가 필요한지에 대한 통찰을 제공하며, 관련 분야 연구자 및 학생들에게 유용한 벤치마크와 아이디어의 기준점이 될 것입니다. 앞으로 이 방향의 연구가 더욱 발전하여, 로봇이 실제 세계에서 연속적인 복합 작업을 안전하고 신뢰성 있게 수행하는 날이 앞당겨지길 기대해봅니다."
  },
  {
    "objectID": "posts/paper/2022-08-07-gn-block.html",
    "href": "posts/paper/2022-08-07-gn-block.html",
    "title": "📃GN-Block 리뷰",
    "section": "",
    "text": "이번 post는 Graph Networks as Learnable Physics Engines for Inference and Control 라는 논문을 읽고 리뷰한 내용입니다."
  },
  {
    "objectID": "posts/paper/2022-08-07-gn-block.html#graph-representation-of-a-physical-system",
    "href": "posts/paper/2022-08-07-gn-block.html#graph-representation-of-a-physical-system",
    "title": "📃GN-Block 리뷰",
    "section": "Graph representation of a physical system",
    "text": "Graph representation of a physical system\n물리시스템을 어떻게 그래프로 나타낼 수 있는지 몇가지 용어와 수식들을 정리해보겠습니다.\n\n물리 시스템의 body는 그래프의 node로 표현합니다.\n물리 시스템의 joint는 그래프의 edge로 표현합니다.\n물리 시스템의 global한 속성은 global feature로 표현합니다.\n\n아래 사진에서 보이는 half-cheetah에서 직관적으로 어떻게 그래프가 그려질 수 있는지 알 수 있고 이 그래프를 G로 나타낼 수 있습니다.\n\n\n\n\n\n앞서 설명한 부분을 수식으로 나타내면 다음과 같습니다.\n\nG=\\left(\\mathbf{g},\\left\\{\\mathbf{n}_{i}\\right\\}_{i=1 \\cdots N_{n}},\\left\\{\\mathbf{e}_{j}, s_{j}, r_{j}\\right\\}_{j=1 \\cdots N_{e}}\\right)\n\n\ng : global features 시스템의 중력이나 time step과 같은 속성을 나타내는 벡터입니다.\n\\mathbf{n}_{i} : node features를 나타내는 벡터입니다.\n\\mathbf{e}_{j} : edge features를 나타내는 벡터입니다.\ns_{j} : 이 edge를 통해서 message를 보내는 sender nodes의 인덱스입니다.\nr_{j} : 이 edge를 통해서 message를 받는 receiver nodes의 인덱스입니다."
  },
  {
    "objectID": "posts/paper/2022-08-07-gn-block.html#static-dynamic-properties",
    "href": "posts/paper/2022-08-07-gn-block.html#static-dynamic-properties",
    "title": "📃GN-Block 리뷰",
    "section": "Static & Dynamic properties",
    "text": "Static & Dynamic properties\n여기서 static graph G_s와 dynamic graph G_d 라는 그래프는 2가지 종류가 있습니다. 이 2개의 그래프는 각각 시스템의 속성이 시간에 따라 변화하는지(dynamic/time-variant) 안하는지(static/time-invaritant)에 따라 그래프를 구성하는 정보의 종류가 다릅니다.(자세한 정보는 Appendix G section에서 Mujoco 기반의 어떤 정보로 각 그래프를 구성했는지 나와있습니다.)\n\nA static graph G_s: 시스템의 static한 정보를 가지고 있는 그래프\n\nglobal parameters: the time step, viscosity, gravity, etc\nbody/node parameters: mass, inertia tensor, etc.\njoint/edge parameters: joint type과 properties, motor type and properties, etc\n\nA dynamic graph G_d: 시스템의 일시적인 state 정보를 가지고 있는 그래프\n\nbody/node: 3D Cartesian position, 4D quaternion orientation, 3D linear velocity, 3D angular velocity\njoint/edge: joint에 적용된 action들의 크기"
  },
  {
    "objectID": "posts/paper/2022-08-07-gn-block.html#graph-networks",
    "href": "posts/paper/2022-08-07-gn-block.html#graph-networks",
    "title": "📃GN-Block 리뷰",
    "section": "Graph networks",
    "text": "Graph networks\n\ngraph2graph 모듈을 활용하여 인풋을 그래프로 받고 아웃풋도 그래프로 받는 모델입니다. 따라서 아웃풋의 그래프는 인풋 그래프와 다른 edge, node, global features를 가지게 됩니다.\n\n본 논문의 핵심 아이디어인 GN 블록의 구조에 대해 알아보겠습니다. - A core GN block\n\n\n\n\n\n- 3개의 sub function, MLP로 이루어져 있습니다.\n    - edge-wise $f_e$ : 모든 edge들에 대한 update를 진행합니다.\n    - node-wise $f_n$ : 모든 node들에 대한 update를 진행합니다.\n    - global $f_g$ : 마지막으로 global feature들을 update 합니다.\n하나의 feedforward GN pass는 그래프 상에서 message-passing 단계의 한 스텝으로 간주할 수 있습니다. 이러한 GN-block 내에서의 알고리즘은 아래와 같습니다."
  },
  {
    "objectID": "posts/paper/2022-08-07-gn-block.html#forward-models",
    "href": "posts/paper/2022-08-07-gn-block.html#forward-models",
    "title": "📃GN-Block 리뷰",
    "section": "Forward models",
    "text": "Forward models\nForward model의 목적은 현재 정보를 기반으로 다음 step의 상태를 예측(prediction)하는 것입니다. (이는 영어 단어의 비슷한 의미때문에 다음에 나오는 inference model의 목적과 많이 혼동될 수 있으니 잘 정의하고 넘어가는 것이 좋습니다.) forward model은 RNN(GRU)를 도입했는지 여부에 따라 2가지 타입이 있습니다.\nType1. GNN feed-forward\n\n\n\n\n\n가장 간단한 GNN feed-forward 모델입니다. 그래프는 처음에 GN_1을 거쳐 latent graph인 G'이 됩니다. 그리고 다음 GN_2의 인풋으로는 GN_1을 거치긴 전의 그래프였던 G와 G'를 concatenate를 해서 넣어주게 됩니다. 저자들은 이렇게 디자인한 이유로, 그래프의 모든 노드들과 엣지들이 모두 communicate하게 하기 위함이라고 이야기합니다. 이렇게 GN_1, GN_2를 거쳐 최종적으로 나오는 G^*의 node feature들이 각 body의 상태 prediction 값이 되는 것 입니다.\nType2. RNN+GNN\n\n\n\n\n\n다음으로 앞서 기본이 되는 모델에 G-GRU를 추가한 타입니다. Type 1과 비슷하게 skip connection, latent graph를 모두 사용하는데 GN block의 GRU 버젼인 G-GRU가 들어가면서 G_h라는 RNN에서 hidden vector와 같은 개념의 hidden graph가 추가된 것입니다. 모든 edge, node, global feature들에 대해 각각 RNN이 적용되어 총 3개의 RNN sub-modules이 있습니다.\n두가지 타입의 GNN forward 모델에 공통적인 사항\n\nstate differences를 예측하는 것을 학습해서 state prediction의 절댓값(absolute)을 계산합니다. 이 계산된 absolute state prediction을 가지고 state를 update하게 되는 것입니다.\nlong-range rollout trajectory를 만들어내기 위해서 state prediction 값과 control input을 반복적으로 model에 넣어주어서 여러 스텝의 trajectory를 생성하게 됩니다.\nGN model의 인풋과 아웃풋들은 normalize 됩니다.\n\n사실 리뷰를 하면서 forward model과 inference model 사이의 구분이나 모델의 구체적인 프로세스 이해가 pseudo algorithm을 보기 전까지 잘되지 않았습니다. Appendix에 나와있어서 잘 보지 않을 확률이 높지만 논문의 개념을 대략적으로 이해하고 난 후에는 꼭 line by line으로 보시길 추천합니다.\n먼저 forward model의 학습과정을 보여주는 pseudo algorithm 입니다. 다시한번 이 모델의 목적을 상기시켜보자면, 현재 상태 x^{t_0} 를 기반으로 a^{t_0}와 함께 주어졌을 때, x^{t_0+1}을 예측하는 것입니다. 앞서 설명한 부분들인, state의 잔차를 학습하는 부분이나 normalization 등이 알고리즘내에 잘 나와있습니다.\n\n\n\n\n\n다음은 학습된 forward model을 가지고 다음 상태인 x^{t_0+1}을 어떻게 예측하는지 보여주는 알고리즘입니다.\n\n\n\n\n\n마지막으로 바로 위 알고리즘과 동일하게 학습된 forward model을 가지고 다음 상태인 x^{t_0+1}을 어떻게 예측하는지 보여주는 알고리즘이지만 inference model에서 학습된 GN_p를 가지고 system identification이 추가된 상태에서 어떻게 알고리즘이 흘러가는지 보여줍니다.(이전에 알고리즘에서는 system parameter p라고 표시되었던 부분이 대체된 것입니다.)"
  },
  {
    "objectID": "posts/paper/2022-08-07-gn-block.html#inference-models",
    "href": "posts/paper/2022-08-07-gn-block.html#inference-models",
    "title": "📃GN-Block 리뷰",
    "section": "Inference Models",
    "text": "Inference Models\nInference model의 목적을 한 마디로 표현하자면 System identification이라고 할 수 있습니다. System identification이란 관찰할 수 없는(unobserved) dynamic system의 속성들을 관찰되는(observed) behavior(또는 어떤 양상)를 가지고 추론하는 것을 말합니다. 즉 암시적으로 system을 구성하는 요소들을 (명시적이지 않아) 측정하거나 관찰할 수 없지만 latent representations을 통해 추론할 수 있습니다.\n\n\n\n\n\nInference model도 Recurrent GN-based model 입니다. forward 모델과 다른 점으로는 오직 trajectory의 dynamic states들만 input으로 받습니다. 따라서 dynamic state graph인 G_d와 control input을 받습니다. 아웃풋으로는 일정 time step T이후의 G^*(T)이 되며, 본 논문에서 이후 실험파트에서 20 step을 사용했습니다.\ninference model 학습과정의 pseudo 알고리즘은 아래와 같습니다."
  },
  {
    "objectID": "posts/paper/2022-08-07-gn-block.html#control-algorithm",
    "href": "posts/paper/2022-08-07-gn-block.html#control-algorithm",
    "title": "📃GN-Block 리뷰",
    "section": "Control algorithm",
    "text": "Control algorithm\ncontrol algorithm에서는 그래프 기반이 아니고 앞서 설명한 그래프 기반의 forward model과 inference model을 잘 활용해서 어떻게 control할 수 있을지를 보여줍니다. 본 논문에서는 크게 2가지 control algorithm을 사용했습니다. 강화학습을 주로 연구하는 입장에서 리뷰해보면, 대부분 강화학습은 model-free 기반의 알고리즘이 많이 발전했는데 GN기반의 다음 상태를 예측할 수 있는 model을 만듦으로써 model-based 기반의 강화학습 알고리즘을 적용할 수 있다는 것이 매우 흥미로웠습니다.\n\nMPC(Model Predictive Control)\nGN은 미분 가능하기 때문에 MPC같은 gradient-based trajectory optimization 방법으로 model-based planning을 할 수 있습니다. 대표적으로 MPC가 있고 학습기반이 아니라 최적화 알고리즘이며 알고리즘의 흐름은 아래와 같습니다.\n\n\n\n\n\nSVG(Stochastic Value Gradients)\n강화학습 알고리즘 중 하나이며, GN-based model과 SVG의 policy function을 동시에 학습하는 agent로 control을 하는 방법입니다. SVG(1)은 한 스텝을 예측하는 GN model을 가지고 강화학습 알고리즘으로 control을 한 것이며(model-based) SVG(0)은 예측하는 GN model 없이 model-free 기반으로 control한 것으로 이해하시면 됩니다.\n\n\n사실 MPC와 SVG는 매우 비슷한 측면이 있습니다. MPC에서는 control inputs들이 한 에피소드에서 초기 조건들이 주어졌을 때 최적화 되는 것이라면, SVG에서는 state와 control을 매칭시키는 policy function이 학습과정에서 경험한 states에 대해서 최적화 되는 것입니다."
  },
  {
    "objectID": "posts/paper/2022-08-07-gn-block.html#prediction",
    "href": "posts/paper/2022-08-07-gn-block.html#prediction",
    "title": "📃GN-Block 리뷰",
    "section": "Prediction",
    "text": "Prediction\nLearning a forward model for a single system\n하나의 시스템을 가지고 학습한 forward model의 Prediction 성능 살펴보기\n\nrandom control로 만들어진 데이터들을 가지고 학습된 GN-based model\n\n[Visually] Swimmer6에서 그림에서 처럼 ground truth와 예측 결과가 구분이 안 갈 정도로 흡사하다.(영상에서도 거의 구분이 안 갈 정도로 잘 예측하고 있음을 알 수 있다.)\n\n\n\n\n\n[Quantitatively] 100 step에서 3축 방향으로의 위치, 선속도, 각속도, 쿼터니안 방향 비교\n\n\n\n\n\n\nconstant prediction baseline은 아웃풋으로 인풋을 그대로 복사해서 사용했기 때문에 애러 최대치로 normalization 하기 위해 검은색 점선으로 표기\n우선 검은 점선과 막대기들을 뭉뚱그려서 보면,\n1 step과 100 step의 rollout 결과를 비교했을 때 검은 점선에 비해 파란색 막대기들의 error 값이 낮음을 알 수 있다.\n\n\n\n\n\nGN 모델이 MLP-based 보다 더 낮은 애러를 가지는 것을 알 수 있다. 이는 특별히 Swimmer6처럼 에이전트의 구조가 반복적인 경우에 더욱 눈에 띄게 낮음을 알 수 있었다. 이를 통해 GN-based forward 모델이 다양한 물리 시스템들에서 dynamics를 잘 예측함을 알 수 있다.\n\n\n\n\n\n\n\nGN이 MLP보다 더 generalization이 잘 됨을 확인할 수 있었는데, Swimmer6를 집중적으로 train, valid, test 데이터에 대해 1-step, rollout error를 각각 확인해봤을 때, Best GN의 error 값이 Best MLP보다 낮음을 알 수 있다. 뿐만 아니라 test data의 error 증가율을 봤을 때에도 GN 모델의 test data의 error가 더 적게 증가함을 관찰할 수 있었고 이는 agent의 bodies와 joints들에 대한 inductive bias가 GN을 통해 잘 학습되었음을 증명할 수 있다.\n\n\n\n\n\n\nLearning a forward model for multiple systems\n한 개의 시스템에서의 forward model을 살펴보았으니 이제 여러 시스템에서의 forward model의 성능을 살펴보자. GN을 사용하면 여러 시스템들의 다양한 변수들도 잘 다룰 수 있다는 가정이 있었다. 이를 확인하기 위해 연속적으로 static parameter들(질량, body의 길이, joint의 각도 등)을 바꿔가면서 forward dynamics를 어떻게 학습해가는지 확인했다."
  },
  {
    "objectID": "posts/paper/2022-08-07-gn-block.html#inference",
    "href": "posts/paper/2022-08-07-gn-block.html#inference",
    "title": "📃GN-Block 리뷰",
    "section": "Inference",
    "text": "Inference"
  },
  {
    "objectID": "posts/paper/2022-08-07-gn-block.html#control",
    "href": "posts/paper/2022-08-07-gn-block.html#control",
    "title": "📃GN-Block 리뷰",
    "section": "Control",
    "text": "Control"
  },
  {
    "objectID": "posts/paper/2022-09-17-wavenet.html",
    "href": "posts/paper/2022-09-17-wavenet.html",
    "title": "📃WaveNet 리뷰",
    "section": "",
    "text": "이번 포스팅은 Google DeepMind에서 발표한 WaveNet이라는 논문에 대해 리뷰를 하려고 합니다. WaveNet은 Autoregressive한 Generative model로써 Google의 스피커 서비스에 사용되었다고 많이 알려진 모델입니다."
  },
  {
    "objectID": "posts/paper/2022-09-17-wavenet.html#dilated-casual-convolution",
    "href": "posts/paper/2022-09-17-wavenet.html#dilated-casual-convolution",
    "title": "📃WaveNet 리뷰",
    "section": "1. Dilated Casual Convolution",
    "text": "1. Dilated Casual Convolution\n먼저 Dilated Casual Convolution은 µ-law Companding Transformation 처리를 거친 음성 신호를 받아오는 첫번째 부분입니다.\n\n\n\nCasual Convolution 2\n\n\n우선 Casual 이라는 것은 Time-series인 음성 신호의 시간 순서를 고려하여 현재 시점 t를 기준으로 미래 정보는 사용할 수 없고 현재까지의(과거~현재 t) 정보만 사용할 수 있다는 의미입니다. 왼쪽 Causal Convolution 그림에서 Receptive Field는 (레이어 수) + (필터의 length) -1로 계산되어 총 레이어 수는 4개이고 필터 length는 이전 레이어에서 2개의 정보가 모아져서 다음 레이어의 하나의 데이터로 산출되므로 필터 length는 2라고 볼 수 있습니다. 따라서 4+2-1로 Receptive Field는 5가 되며 이를 그림에서 살펴보면 처음 input에서 5개의 음성 정보가 output의 1개의 정보로 나오는 것을 볼 수 있습니다. 이런 Receptive Field는 매우 짧은 시간에 많은 음성신호가 매칭되는 상황에서 매우 좁으며 RF를 늘리기 위해서는 레이어 수를 늘리거나 필터의 length를 늘려야 하는데 이는 모델을 매우 크게 만들게 되고 계산도 많이 요구됩니다.\n\n\n\nDilated Casual Convolution 2\n\n\n그래서 제안이 된 방법이 바로 Dilated Convolution입니다. 이는 convolution with holes로 해석할 수 있는데 위의 그림에서 볼 수 있듯이 이전 레이어에서 데이터가 Dilated되어 데이터가 듬성듬성하게 모아져서 다음 레이어로 넘어가는 것을 볼 수 있습니다. 이는 skip이나 pooling과 유사해보이지만 input과 output의 차원이 유지된다는 점에서 차이가 있습니다. 이때의 RF는 각 레이어의 Dilation 값을 모두 더하고 마지막에 현재 시점의 데이터 1을 더하며 RF가 계산됩니다. WaveNet에서는 Dilation을 총 30개의 레이어에 적용했고 Dilation 값의 패턴은 input에서 부터 1, 2, …, 512 로 2배씩 늘린 10개의 레이어를 총 3번 반복했습니다. 이때, 1 ~ 512 Dilation 값을 가진 10개 레이어의 RF는 1024로 계산됩니다.\n\n\n\nDilated Casual Convolution Process2\n\n\n\n\n\nDilated Convolution Pattern6\n\n\nCode 구현으로 살펴보면 아래와 같이 구현할 수 있습니다. Casual 특성을 반영하기 위해 self.ignoreOutIndex 을 만들어서 dilation 값을 고려하여 (kernel_size - 1) * dilation으로 계산한 후에 잘라내주는 것을 확인할 수 있습니다.\nclass CasualDilatedConv1D(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, dilation, padding=1):\n        super().__init__()\n        self.conv1D = nn.Conv1d(in_channels, out_channels, kernel_size, dilation=dilation, bias=False, padding='same')\n        self.ignoreOutIndex = (kernel_size - 1) * dilation # casual\n\n    def forward(self, x):\n        return self.conv1D(x)[..., :-self.ignoreOutIndex] # casual"
  },
  {
    "objectID": "posts/paper/2022-09-17-wavenet.html#residual-connection-gated-activation-units",
    "href": "posts/paper/2022-09-17-wavenet.html#residual-connection-gated-activation-units",
    "title": "📃WaveNet 리뷰",
    "section": "2. Residual Connection & Gated Activation Units",
    "text": "2. Residual Connection & Gated Activation Units\n다음으로 Dilated Causal Convolution을 거친 후 통과하게 되는 Residual Connection & Gated Activation Units 부분에 대해서 살펴보겠습니다.\n\n\n\nWaveNet에서 사용된 Gated Activation Units는 PixelCNN에서 사용된 매커니즘을 차용했습니다. 아래의 그림에서 보이는 보라색 Dilated Conv가 앞에서 설명한 DCC이며 이를 거친 후 Convoltion layer와 각각 tanh, sigmoid activation을 통과하여 Filter, Gate가 됩니다. 이 2가지 경로로 계산된 값은 elementwise product를 통해 하나의 벡터로 변환됩니다. 이떄 Dilated를 통과하기 전 값을 Residual Connection을 통해 연결함으로써 딥러닝 모델이 레이어를 더 깊게 쌓을 수 있도록 돕고 더 빠르게 학습할 수 있도록 할 수 있었다고 합니다.\n\n\n\nResidual Connection & Gated Activation Units6"
  },
  {
    "objectID": "posts/paper/2022-09-17-wavenet.html#skip-connection",
    "href": "posts/paper/2022-09-17-wavenet.html#skip-connection",
    "title": "📃WaveNet 리뷰",
    "section": "3. Skip Connection",
    "text": "3. Skip Connection\n\n\n\nSkip Connection은 Dilated Convolution을 통해 다양한 Receptive Field를 가진 각 레이어들의 값을 활용하여 output을 만들어낼 수 있도록 했습니다. 앞서 설명했던 대로 각 Residual Block의 Dilation 값이 다 다르기 때문에 각 Residual Block의 output은 서로 다른 Receptive Field를 가지게 됩니다.\n\n\n\nSkip Connection6\n\n\nResidual Connection과 Skip Connection을 Code로 구현하면 다음과 같습니다. 위에서 설명했던 Gated Activation Units의 tanh, sigmoid activation을 각각의 activation function을 거친후 self.resConv1D을 통과하는 것을 확인할 수 있습니다. 또한 Skip Connection을 구현하는 부분은 self.skipConv1D에서 확인할 수 있습니다. 마지막 return에서 resOutput, skipOutput으로 2개의 output이 나오는 것을 알 수 있습니다.\nclass ResBlock(nn.Module):\n    def __init__(self, res_channels, skip_channels, kernel_size, dilation):\n        super().__init__()\n        self.casualDilatedConv1D = CasualDilatedConv1D(res_channels, res_channels, kernel_size, dilation=dilation)\n        self.resConv1D = nn.Conv1d(res_channels, res_channels, kernel_size=1)\n        self.skipConv1D = nn.Conv1d(res_channels, skip_channels, kernel_size=1)\n        self.tanh = nn.Tanh()\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, inputX, skipSize):\n        x = self.casualDilatedConv1D(inputX)\n        x1 = self.tanh(x)\n        x2 = self.sigmoid(x)\n        x = x1 * x2\n        resOutput = self.resConv1D(x)\n        resOutput = resOutput + inputX[..., -resOutput.size(2):]\n        skipOutput = self.skipConv1D(x)\n        skipOutput = skipOutput[..., -skipSize:]\n        return resOutput, skipOutput\n위와 같은 ResBlock은 전체 구조에서 보시다시피 여러개가 stacked 되어 있으므로 StackOfResBlocks class로 구현하여 WaveNet에 넣어주게 됩니다.\nclass StackOfResBlocks(nn.Module):\n\n    def __init__(self, stack_size, layer_size, res_channels, skip_channels, kernel_size):\n        super().__init__()\n        buildDilationFunc = np.vectorize(self.buildDilation)\n        dilations = buildDilationFunc(stack_size, layer_size)\n        self.resBlocks = []\n        for s,dilationPerStack in enumerate(dilations):\n            for l,dilation in enumerate(dilationPerStack):\n                resBlock=ResBlock(res_channels, skip_channels, kernel_size, dilation)\n                self.add_module(f'resBlock_{s}_{l}', resBlock) # Add modules manually\n                self.resBlocks.append(resBlock)\n\n    def buildDilation(self, stack_size, layer_size):\n        # stack1=[1,2,4,8,16,...512]\n        dilationsForAllStacks = []\n        for stack in range(stack_size):\n            dilations = []\n            for layer in range(layer_size):\n                dilations.append(2 ** layer)\n            dilationsForAllStacks.append(dilations)\n        return dilationsForAllStacks\n\n    def forward(self, x, skipSize):\n        resOutput = x\n        skipOutputs = []\n        for resBlock in self.resBlocks:\n            resOutput, skipOutput = resBlock(resOutput, skipSize)\n            skipOutputs.append(skipOutput)\n        return resOutput, torch.stack(skipOutputs)"
  },
  {
    "objectID": "posts/paper/2022-09-17-wavenet.html#conditional-wavenets",
    "href": "posts/paper/2022-09-17-wavenet.html#conditional-wavenets",
    "title": "📃WaveNet 리뷰",
    "section": "4. Conditional WaveNets",
    "text": "4. Conditional WaveNets\n\n\n\n\n\n\nConditional modeling 6\n\n\nConditional Modeling은 Autoregressive model인 WaveNet에 적용하기 쉽고 이 또한 PixelCNN에서의 아이디어와 유사합니다. Feature h 벡터를 조건 부분에 추가하여 음성 데이터에 조건을 추가할 수 있습니다.\n\np(\\mathbf{x} \\mid \\mathbf{h})=\\prod_{t=1}^T p\\left(x_t \\mid x_1, \\ldots, x_{t-1}, \\mathbf{h}\\right)\n\nCondition에는 크게 2가지로 Global과 Local이 있습니다. 먼저 Global은 Time-invariant한 조건으로 시점에 따라 변하지 않는 조건 정보를 추가하는 것을 말합니다. 예를 들어 한 발화자의 음성은 해당 음성 파일의 어떤 시점에서나 똑같은 condition이기 때문에 Global condition이라고 할 수 있습니다. 이때의 Feature vector h는 linear projection을 거친 후 data x와 더하게 됩니다.\n\n\n\n다음으로 Time-variant한 Local condition은 시점에 따라 변하는 조건 정보를 추가하는 것을 말하는데 음성 데이터보다 길이가 짧지만 순서가 있는 일정 길이의 Sequence vector라고 생각할 수 있습니다. 같은 발화자여도 어떤 단어를 말하느냐에 따라 음성학적인 특징(linguistic feature)가 다를 수 있기 떄문에 local한 조건은 한 음성 파일에 여러개가 있을 수 있습니다. 이때 Feature vector h는 음성 파일과 길이가 다르기 때문에 Upsampling을 거친후 1x1 convolution을 거쳐서 data x와 더해집니다."
  },
  {
    "objectID": "posts/paper/2025-07-17-curobo.html",
    "href": "posts/paper/2025-07-17-curobo.html",
    "title": "📃CuRobo 리뷰",
    "section": "",
    "text": "Paper Link\nGithub Link\nDocumentation Link"
  },
  {
    "objectID": "posts/paper/2025-07-17-curobo.html#소개-gpu로-가속한-차세대-로봇-모션-플래너",
    "href": "posts/paper/2025-07-17-curobo.html#소개-gpu로-가속한-차세대-로봇-모션-플래너",
    "title": "📃CuRobo 리뷰",
    "section": "2.1 소개: GPU로 가속한 차세대 로봇 모션 플래너",
    "text": "2.1 소개: GPU로 가속한 차세대 로봇 모션 플래너\n로봇 팔(Manipulator)의 모션 생성 문제는, 높은 자유도를 가진 로봇이 복잡한 환경에서 충돌 없이 목표 지점까지 이동할 경로와 속도를 찾아내는 것을 의미합니다. 기존의 모션 플래너들은 경로를 찾는 데 시간이 오래 걸리거나, 경로는 찾지만 움직임이 불안정한 경우가 많았습니다. cuRobo는 이러한 문제를 해결하기 위해 제안된 새로운 모션 생성 프레임워크로, GPU 병렬 컴퓨팅을 적극 활용하여 전역 최적화 수준에서 경로를 탐색합니다. cuRobo는 특히 “최소-저크(minimum-jerk)” 기준으로 궤적을 최적화하여 부드럽고 안정적인 움직임을 생성하며, 로봇과 환경 간 충돌 회피를 통합적으로 고려합니다. 놀랍게도 이 모든 처리를 평균 50ms 이내에 수행하여, 기존 기법 대비 약 60배 이상의 속도 향상을 달성합니다. 본 포스트에서는 cuRobo의 핵심 아이디어와 아키텍처를 살펴보고, 성능 평가 결과와 기존 방법들과의 비교를 통해 그 장점과 한계를 깊이 있게 알아보겠습니다."
  },
  {
    "objectID": "posts/paper/2025-07-17-curobo.html#curobo의-핵심-아이디어와-구조",
    "href": "posts/paper/2025-07-17-curobo.html#curobo의-핵심-아이디어와-구조",
    "title": "📃CuRobo 리뷰",
    "section": "2.2 cuRobo의 핵심 아이디어와 구조",
    "text": "2.2 cuRobo의 핵심 아이디어와 구조\ncuRobo는 로봇 모션 생성을 하나의 최적화 문제로 정식화합니다. 목표는 로봇의 시작 자세에서 목표 그리퍼 위치/자세에 도달하는 시간에 따른 관절 궤적을 찾는 것입니다. 이때 궤적은 가능한 한 부드럽게 (가속도의 시간 변화율인 저크(jerk)를 최소화) 움직이면서도 경로 상의 장애물과 충돌하지 않도록 만들어야 합니다. 이를 달성하기 위해 cuRobo는 다음과 같은 단계들을 포함하는 구조적 파이프라인을 가지고 있습니다:\n\n목표 자세 IK 계산: 먼저 목표 그리퍼 자세(위치 및 방향)에 대응하는 목표 관절각을 찾아야 합니다. cuRobo는 자체 개발한 병렬화된 역기구학(IK) 솔버를 이용해 다수의 IK 해를 동시에 시도하여, 그 중 충돌이 없고 로봇 관절 범위 내에 있는 해를 빠르게 찾아냅니다. 이 IK 솔버는 GPU 병렬화를 통해 초당 7000회 이상의 IK 연산을 수행할 수 있을 정도로 고속이며, 필요하면 수십 개의 해를 생성해 이후 단계의 후보로 활용합니다 (실험적으로 1000개의 자세를 131ms만에 모두 충돌 검사까지 포함 해결하여 기존 TracIK 기반 접근보다 80배 이상 빠름).\n여러 초기 궤적 시드 생성: IK로 얻은 목표 자세(관절 구성)로부터, 시작 자세와 목표 자세를 연결하는 여러 개의 초기 경로 시나리오(시드)를 만듭니다. 예를 들면,\n\n직선 인터폴레이션 경로: 시작 관절각과 목표 관절각을 선형 보간하여 얻은 경로,\n후퇴 동작을 포함한 경로: 초기에 로봇 팔을 살짝 뒤로 빼는 등 특정 중간 자세(retract configuration)를 경유하도록 만든 경로,\n병렬 기하학 플래너 기반 경로: cuRobo에 내장된 GPU 가속 샘플링 플래너를 사용해 얻은 충돌 없는 경로 등이 있습니다.\n\n이러한 여러 경로 후보를 한꺼번에 준비하는 이유는 전역 최적해를 찾기 위함입니다. 로봇 모션 최적화 문제는 장애물에 의해 비선형적이고 다중 극소해를 가질 수 있기 때문에, 단일 초기 값으로 시작하면 최적해를 놓치거나 충돌을 피하지 못할 수 있습니다. cuRobo는 다양한 시드들을 병렬로 탐색함으로써 이러한 지역 해 한계를 극복하려 합니다. 실험적으로도 다중 시드 및 연속 충돌 검사를 도입하면 단 한번의 최적화 시도로도 문제의 85% 이상을 성공적으로 풀 수 있을 정도로 성공률이 높아진다고 보고합니다.\n병렬 궤적 최적화 (Minimum-Jerk): 준비된 다수의 시드 궤적 각각에 대해 동시에 최적화 연산을 수행합니다. 여기서 코스트 함수는 크게 두 부분으로 구성됩니다:\n\n경로의 목적지 도달 오차: 최종 시점에 엔드 이펙터가 목표 자세에 도달하도록 유도하는 비용으로, 위치 오차와 자세(quaternion) 오차를 포함합니다. 이 항은 목표에 가까워질수록 세밀하게, 멀리 떨어져 있을 때는 안정적으로 작용하도록 스케일링되어 있어, 목표 근처에서 높은 정확도를 보장하면서도 초기에는 완만한 경사로 효율적 탐색을 돕습니다.\n경로의 길이 및 매끄러움: 가속도 및 저크(jerk)에 대한 페널티를 줌으로써 불필요하게 굽은 경로나 급격한 속도 변화를 억제합니다. 가속도 제곱합을 최소화하는 것은 곧 로봇 관절 속도의 급격한 변화를 줄여 경로를 부드럽게 만들고, 결과적으로 짧고 효율적인 경로가 나오도록 합니다. 여기에 한 단계 더 나아가, 저크(jerk) 항을 함께 최소화함으로써 속도의 변화율까지 억제하는 최소-저크 궤적을 추구합니다. 이러한 저크 최소화는 특히 로봇이 정지 상태에서 고속으로 움직이기 시작하거나 멈출 때 발생하는 충격을 줄여주며, 실제 로봇 제어 시 진동 감소 및 추종 안정성 향상에 크게 기여합니다.\n\n추가로, 경로 상에서 관절 각도 한계를 넘지 않도록 페널티를 부과하고 (하드 제약을 일단 큰 페널티로 완화하여 최적화), 필요한 경우 로봇 모델에 따른 작업공간 제약(예: 특정 높이 유지나 말단의 방향 유지)도 비용으로 넣을 수 있습니다. 다만 cuRobo 논문에서는 이런 작업-제약 경로(예: 끝단 공구의 일정한 자세 유지 등)에 대한 처리는 초기 실험만 해보았고 아직 본격적으로 통합하진 않았다고 밝혔습니다.\n충돌 회피는 코스트 함수의 필수 요소입니다. cuRobo는 로봇 자체 충돌(셀프-콜리전)과 환경 장애물 충돌을 모두 고려하기 위해, 로봇 및 물체의 기하를 특수한 방식으로 다룹니다. 일반적으로 충돌 검출은 로봇의 각 링크 형상을 매번 위치시켜 메시(mesh) 간 충돌 여부나 거리를 계산하는 방식을 취하는데, 이는 매우 계산 집약적입니다 (예: ROS MoveIt에서는 오픈소스 FCL(Flexible Collision Library)로 이러한 연산을 CPU에서 수행하며, 복잡한 로봇-환경 간 충돌 계산이 병목이 되곤 합니다). cuRobo는 로봇의 각 링크를 복수의 구(sphere)로 근사 표현하여 이 문제를 해결합니다. 각 구의 중심 좌표를 기반으로 충돌 비용 함수를 연속적으로 계산하는데, 방법은 다음과 같습니다:\n\n환경 충돌 비용: 구와 장애물 간 거리를 계산하여, 만약 거리가 구의 반지름보다 크면 (충돌 여유가 충분하면) 비용 0, 어느 임계 거리 이내로 가까워지면 부드러운 2차 함수 형태로 비용이 증가하도록 만듭니다. 이렇게 하면 로봇이 장애물에 근접할수록 큰 패널티를 받아 자연스럽게 멀어지도록 최적화됩니다. 장애물은 사전에 직육면체(collaboration box)나 서명거리장(Signed Distance Field) 등으로 표현하여, 각 구의 중심까지의 거리 연산을 빠르게 병렬 계산합니다. (논문에서는 실험 시 원기둥 형태의 장애물을 동일한 부피의 직육면체로 근사했다고 언급합니다. 이러한 근사는 속도를 높여주지만 약간의 오차를 수반하여, 일부 엣지 케이스에선 목표 자세 자체가 장애물과 겹쳐 유효 경로가 없는데도 근사 충돌 검사상 간과되는 등의 사례가 있었습니다.)\n자기 충돌 비용: 로봇 링크들 간 일정 거리 이상 근접하지 않도록, 각 링크를 대표하는 구들 사이의 간격을 계산해 비용을 부여합니다. 인접한 링크 (예: 바로 이웃 관절 사이)는 어느 정도 가까워지는 것이 허용되므로, 특정 구 쌍들만 선택적으로 검사하여 불필요한 충돌 계산을 줄이고 로봇의 정상 동작 범위는 방해하지 않도록 했습니다.\n\n특히 cuRobo는 연속적(Continuous) 충돌 검사를 수행하여 시간축을 따라 부드럽게 움직이는 동안에도 중간에 발생할 수 있는 충돌을 놓치지 않습니다. 이는 각 타임스텝 별 로봇 자세뿐만 아니라, 인접 시간 구간에서 로봇의 궤적이 지나가는 경로까지 확인하는 기법입니다. 예를 들어 관절 각도 사이를 선형보간한 세부자세들도 모두 위 구-장애물 거리 계산을 통해 충돌이었는지 확인하며, 필요하면 시간 분할을 조정해가며 완전한 충돌 프리 경로를 보장합니다.\n마지막으로, 최적화는 병렬화된 L-BFGS 최적화 알고리즘으로 수행됩니다. 각 시드 경로에 대해 그래디언트를 계산하고 L-BFGS 방식으로 동시에 경로를 수정해 나가되, cuRobo는 여기에 독자적인 병렬 노이즈 Line Search 기법을 더했습니다. 일반적인 L-BFGS 알고리즘은 새로운 스텝 크기를 찾을 때 직렬적으로 탐색(line search)을 하지만, cuRobo는 여러 후보 스텝을 동시에 시도하고 약간의 무작위성도 부여하여 지역 최적해에 갇히는 것을 방지합니다. 또한 초기 몇 회의 반복(iteration)은 “파티클 기반 최적화”라 불리는 방식을 사용하여 시드 경로들이 더 좋은 영역으로 이동하도록 유도하고, 이후 L-BFGS로 빠르게 수렴시키는 하이브리드 전략을 취합니다. 여기서 파티클 기반 최적화는 시드들을 일종의 스웜(swarm)으로 간주하여 개략적인 전역 탐색을 하는 단계로 볼 수 있습니다 (논문에서는 구체적인 구현으로 샘플 기반 MPPI법 등을 활용한 것으로 보입니다). 요약하면, 다중 시드 + 파티클 탐색 + L-BFGS 미세조정의 조합으로, 높은 성공률과 빠른 수렴 속도를 동시에 얻는 것입니다.\n시간 최적화 및 최종 출력: 궤적 최적화가 끝나면 결과로 얻은 경로는 관절 공간에서의 움직임 순서일 뿐 아니라, 시간 프로파일이 할당되어야 실제 로봇이 따라갈 수 있습니다. cuRobo는 처음에는 일정한 간격으로 시간 간격(dt)을 놓고 최적화를 수행하지만, 일단 충돌 없는 경로가 나오면 그 경로를 따라 움직일 때 로봇의 속도/가속도 한계를 검토합니다. 만약 어느 구간에서 속도나 가속도 한계를 넘는다면 시간을 늘리고, 여유가 많다면 시간을 단축하는 식으로 시간 스케일을 재조정합니다. 그리고 필요하다면 그 새로운 시간 간격에 맞추어 다시 한번 궤적을 미세 조정하여, 최종적으로 로봇의 동작 한계 내에서 가장 빠르게 움직일 수 있는 시간을 갖는 궤적을 완성합니다. 이렇게 산출된 결과 궤적 θ[0,T]는 시작부터 목표까지 각 시점별 관절값과, 해당 움직임에 걸리는 시간 (속도 프로파일)이 모두 정해진 형태로, 로봇에 실행 명령으로 보낼 준비가 된 상태가 됩니다.\n\n요약하자면, cuRobo의 구조는 (i) 병렬 IK로 목표 자세 도달 가능성 확인, (ii) 다수의 초기 경로 생성, (iii) 병렬 궤적 최적화로 충돌 없고 최소-저크인 경로 탐색, (iv) 시간 재조정으로 로봇 한계 내 최적 속도 달성을 포함합니다. 이러한 복잡한 작업을 모두 포함함에도, 강력한 GPU 병렬화를 통해 수십 밀리초 수준의 속도를 이루어낸 것이 cuRobo의 가장 큰 혁신입니다. 다음 섹션에서는 cuRobo가 어떻게 GPU를 활용해 이러한 성능을 얻는지 살펴보겠습니다."
  },
  {
    "objectID": "posts/paper/2025-07-17-curobo.html#gpu-병렬화-전략과-최적화-기법",
    "href": "posts/paper/2025-07-17-curobo.html#gpu-병렬화-전략과-최적화-기법",
    "title": "📃CuRobo 리뷰",
    "section": "2.3 GPU 병렬화 전략과 최적화 기법",
    "text": "2.3 GPU 병렬화 전략과 최적화 기법\ncuRobo의 뛰어난 성능의 비결은 전략적인 병렬화(parallelization)와 GPU 최적화에 있습니다. 일반적으로 로봇 모션 계획의 각 구성요소(예: IK 계산, 경로 충돌 검사, 최적화 반복 등)는 순차적으로 수행되던 작업이었습니다. cuRobo는 이를 가능한 모든 단계에서 병렬화하여, GPU의 수천 개 스레드를 활용함으로써 동시에 많은 연산을 처리합니다.\n\n다중 시드 병렬 최적화: 앞서 설명했듯이 cuRobo는 여러 개의 초기 경로 시드를 둡니다. 이 각각의 시드 경로 최적화는 독립적인 계산이므로, GPU의 여러 스레드블록에 분배하여 동시에 진행합니다. 논문에서는 한 시드당 하나의 CUDA 블록을 할당하고, 수십 개의 시드가 각각 병렬로 업데이트된다고 밝히고 있습니다. 이렇게 하면 시간 당 많은 후보 경로를 살펴볼 수 있어, 단순 단일 경로를 반복적으로 시도하는 것보다 월등히 빠르게 우수한 경로를 찾을 수 있습니다.\n스레드 단위 세분화: 더욱 흥미로운 점은, 한 개의 시드의 최적화 과정 내부에서도 병렬화를 수행했다는 것입니다. 예를 들어, 한 궤적의 충돌 비용이나 그래디언트를 계산할 때, 시간 단계별로 혹은 관절별로 계산을 쪼개 여러 스레드가 동시에 분담하도록 구현했습니다. 논문에 따르면, “workload가 큰 커널들을 시드별로도 다시 여러 스레드로 쪼개어 실행(split kernels across many threads per seed)”하여, 전통적으로 반복적이고 순차적인 수치최적화 작업에서도 큰 폭의 가속을 얻었다고 합니다. 대표적인 사례로 로봇 정방향 기구학 (Forward Kinematics) 계산을 들 수 있습니다. 보통 n-자유도 로봇의 정기구학은 각 관절 변환을 앞에서부터 차례로 곱적용하는 완전히 직렬적인 계산입니다. 그러나 cuRobo는 동차변환 행렬(4x4)을 사용해 4개의 요소를 한 번에 처리하고, 여러 관절 변환도 매트릭스 곱셈을 4개 스레드가 병렬 수행하는 식으로 병렬화를 부분 도입했습니다. 또한 한 관절씩 진행하던 기존 GPU 구현(STORM 등)이 많은 커널 호출 오버헤드를 발생시킨 것과 달리, cuRobo는 단일 CUDA 커널에서 모든 링크 좌표를 계산하도록 구현하여 메모리 병목을 줄였습니다. 그 결과 적정 배치(batch) 크기 이상에서는 CPU의 최고속 라이브러리(Pinocchio 등)보다 수백 - 수천 배까지도 빠른 기구학 계산이 가능해졌습니다. 이러한 세밀한 최적화 덕분에, cuRobo의 정기구학은 100k개의 자세에 대해서도 CPU 대비 최대 891배 빠른 성능을 보이며, 거리 계산 (충돌 검사)도 대규모 배치 시 PyBullet 대비 최대 16000배까지 빨라지는 획기적인 속도를 달성했습니다 (물론 이렇게 큰 배치는 실시간 적용보다는 병렬 시뮬레이션 분석 등에 가깝지만, 100 - 1000개 수준의 병렬 계산에서도 이미 유의미한 가속이 일어납니다).\nCUDA 그래프(CUDA Graphs) 활용: cuRobo는 Python으로 구현된 상위 로직에서 여러 CUDA 커널을 호출하는 구조인데, 이 경우 호스트(CPU)-디바이스(GPU) 간 커널 호출의 오버헤드가 누적될 수 있습니다. 이를 줄이기 위해 최적화 반복과 일부 데이터 준비 과정을 CUDA Graph 기능으로 묶어 일괄 실행하는 기법을 사용했습니다. CUDA Graph는 미리 정의된 일련의 GPU 연산을 한꺼번에 실행하도록 스케줄링할 수 있는 기술로, cuRobo는 이를 통해 파이썬 루프에서 커널을 매 iteration 호출하던 것을 녹화(record) 후 재실행하는 형태로 바꾸어 약 10배의 속도 향상을 얻었다고 합니다. 이렇게 Python 레벨의 부담을 줄인 결과, cuRobo의 전체 계획 시간 중 Python 상에서 소모되는 시간이 평균 8ms 정도에 불과하며, 추후 해당 부분을 C++로 옮기고 커널들을 더욱 통합한다면 현재 60배인 속도 향상이 최대 93배까지도 늘어날 수 있음을 보였습니다.\n메모리 최적화와 기타 기법: GPU 메모리 상에서 다루는 데이터 구조도 성능에 영향을 주는데, cuRobo는 로봇 및 환경 정보를 GPU에 상주시켜 반복 사용하고, 충돌 검사에 필요한 구 구성이나 서명 거리장 등의 데이터도 한 번 만들어 놓으면 효율적으로 읽도록 신경 썼습니다. 또한 로봇의 메시 -&gt; 구 근사 변환 등을 도와주는 도구를 제공하여, 새로운 로봇이나 환경에 적용할 때 겪는 어려움을 줄였습니다 (NVIDIA Isaac Sim과 연동된 sphere generation 유틸리티 등을 제공). 이러한 실무적인 최적화 포인트까지 챙겨, cuRobo 라이브러리는 일반 PC에서뿐 아니라 Jetson과 같은 임베디드 GPU 플랫폼에서도 좋은 성능을 내도록 만들어졌습니다.\n\n요컨대 cuRobo는 “가능한 모든 것을 병렬로”라는 철학으로 설계되었고, GPU의 장점을 최대한 살린 맞춤 구현을 통해 기존 CPU 위주의 접근으로는 상상하기 어려운 속도를 실현했습니다. 다음으로, 실제 성능 숫자와 실험 결과들을 자세히 살펴보겠습니다."
  },
  {
    "objectID": "posts/paper/2025-07-17-curobo.html#성능-평가-속도-성공률-그리고-동작-안정성",
    "href": "posts/paper/2025-07-17-curobo.html#성능-평가-속도-성공률-그리고-동작-안정성",
    "title": "📃CuRobo 리뷰",
    "section": "2.4 성능 평가: 속도, 성공률 그리고 동작 안정성",
    "text": "2.4 성능 평가: 속도, 성공률 그리고 동작 안정성\ncuRobo의 성능은 다양한 기준에서 기존 기술들을 크게 앞서나갑니다. 속도 측면에서는, 복잡한 7자유도 로봇 팔이 장애물 사이를 이동하는 문제를 평균 50ms만에 풀어내어, 기존 최적화 기반 방법 대비 약 60배 빨라졌습니다. 심지어 어려운 상위 10% 난이도 문제들에 대해서는 수십 배 이상의 격차가 벌어져, 예를 들어 98번째 퍼센타일 난이도 문제의 경우 cuRobo는 약 0.26초, 기존 방식(Tesseract)은 22초가 걸려 83배 속도 차이를 보였습니다. 이러한 비교는 Omniverse Isaac Sim의 Franka Emika Panda 로봇팔과 다양한 장애물 환경에 대해 총 2600개의 시나리오를 테스트한 결과입니다.\n특히 cuRobo의 속도 향상은 데스크탑 고성능 PC뿐 아니라 임베디드 플랫폼에서도 유효했습니다. 엔비디아 Jetson AGX Orin 모듈(지능형 로봇에 자주 쓰이는 60W 내외 GPU 컴퓨팅 모듈)에서 테스트한 결과, cuRobo는 Jetson에서 15W 저전력 모드로 동작해도 데스크탑 i7 CPU에서 구동한 Tesseract보다 빠른 성능을 보였습니다. Jetson 15W 환경에서 조차 cuRobo가 Tesseract보다 21배 빨랐고, Jetson을 최대 성능(MAXN)으로 구동하면 28배까지도 빨라졌습니다. 이는 cuRobo의 GPU 최적화가 얼마나 뛰어난지를 잘 보여주는 예로, 저전력 엣지 디바이스에서도 실시간 모션 계획을 가능케 함을 의미합니다.\n성공률(성공 확률)과 경로 품질 측면에서도 cuRobo는 우수한 결과를 냈습니다. 앞서 언급한 2600개의 다양한 문제 세트에서, cuRobo의 모션 생성은 99.8%의 매우 높은 성공률을 기록했습니다. 이는 동일 조건에서 전통적인 조합(OMPL RRTConnect + TrajOpt 최적화)을 사용한 Tesseract의 약 98.5%와 비교해도 높은 수치입니다. Tesseract는 38개의 실패 사례가 있었던 반면, cuRobo는 오직 5개 문제만 실패했는데, 그마저도 알고리즘상의 한계라기보다 테스트 데이터의 문제 (목표 자세 자체가 처음부터 장애물과 겹쳐있어 물리적으로 불가능한 경우 등)였다고 합니다. 즉, 유효한 문제에 대해서는 사실상 모두 해결했다고 볼 수 있습니다. 또한 cuRobo는 경로 최적화를 통해 불필요한 우회나 멀리 돌아가는 경로를 크게 단축시켰습니다. 측정 결과 cuRobo가 만들어낸 경로의 관절 이동거리 총합(C-space path length)은 순수 RRT 기반 경로보다 평균 53% 짧았고, 동일한 TrajOpt 적용 대비로도 약 10% 가량 더 짧았습니다. 이는 cuRobo가 최소한의 움직임으로 목표에 도달하는 효율적인 경로를 찾아낸다는 의미입니다.\n하지만 뭐니뭐니해도 현장에서 중요한 것은 실제 로봇 동작의 안정성일 것입니다. cuRobo의 최소-저크 최적화는 이 부분에서 큰 강점을 보였는데, 이를 보여주기 위해 연구진은 실제 로봇으로 실험을 진행했습니다. 유니버설로봇(UR5e, UR10 모델) 두 대를 준비하여, 각각의 작업공간에 장애물을 놓고 7개의 임의 목표 자세를 순서대로 연달아 도달하는 경로를 생성, 로봇에게 실행시키는 테스트를 했습니다. 비교군으로 최소-저크 옵션을 끈(min-acc) 경우(즉 가속도만 최소화하고 저크는 고려 안 한 경우)와 켠(min-jerk) 경우를 모두 시험하여, 로봇의 궤적 추종 오차를 측정했습니다.\n그 결과, 저크 최소화가 켜진 경우 로봇의 추종 성능이 눈에 띄게 향상됨을 확인했습니다. 두 로봇 모두에서, 관절 위치 오차와 속도 추종 오차가 최소-저크 궤적에서 더 낮았고, 특히 UR10 같은 비교적 크고 무거운 로봇에서는 차이가 크게 나타났습니다. UR10의 경우 최소-저크 적용 시 평균 위치 오차가 약 0.0013 rad였지만, 미적용 시에는 0.0025 rad로 두 배 가까이 늘어나, 로봇 엔코더의 공인 정밀도(약 0.0017 rad)를 넘어서는 오차가 발생했습니다. 무엇보다, 가속도 최소화만 한 경우 궤적 시작부에 큰 속도 오차/위치 오차 스파이크가 관찰되었는데, 이는 로봇이 순간적으로 최대 가속도에 도달하도록 명령받았을 때 물리적으로 즉각 반응하지 못해 생기는 오차였습니다. 반면 저크를 같이 최소화한 궤적은 초반에 가속도를 서서히 높이도록 계획되었기 때문에 이러한 스파이크가 사라졌고, 전반적으로 매끄럽게 가속/감속하여 로봇이 보다 정확히 따라갈 수 있었습니다. 이 실험은, 최소-저크 기준의 궤적이 고속 동작에서의 진동이나 추종 오차를 줄여 로봇 움직임의 안정성을 높여준다는 것을 실제로 입증했습니다. 이러한 효과 덕분에, cuRobo가 생성한 궤적을 로봇에 보낼 때 추가적인 속도 보간 없이도 (32시점 궤적을 10ms 간격으로 선형 보간만 하여) 문제없이 구동할 수 있었고, 약 35회 연속 동작 수행에서도 안전하고 정확한 움직임을 보여주었습니다.\n마지막으로, 실시간성 측면을 언급하면, cuRobo는 실제 로봇상에서도 계획 시간은 평균 100ms 이내로 측정되어 플래닝 지연이 거의 무시할 수준임이 확인되었습니다. 즉, 목표 자세만 주어지면 0.1초 내에 경로가 나오고, 바로 로봇이 움직이기 시작하여 계획 수립부터 실행 완료까지 매우 짧은 사이클로 동작 가능함을 의미합니다. 이는 산업용 로봇의 빈번한 경로 재계획이나, 인간-로봇 상호작용에서 실시간 반응이 필요한 상황에도 충분히 대응할 수 있는 수준입니다."
  },
  {
    "objectID": "posts/paper/2025-07-17-curobo.html#구현-난이도와-오픈소스-활용",
    "href": "posts/paper/2025-07-17-curobo.html#구현-난이도와-오픈소스-활용",
    "title": "📃CuRobo 리뷰",
    "section": "2.5 구현 난이도와 오픈소스 활용",
    "text": "2.5 구현 난이도와 오픈소스 활용\ncuRobo는 NVIDIA 연구진에 의해 개발되었으며, 오픈소스로 공개되어 누구나 활용할 수 있게 되어 있습니다. 공식 깃허브 저장소(NVlabs/curobo)에는 설치 방법과 예제가 제공되어 있고, 2025년 4월 기준으로 1.1k 이상의 Star를 받을 정도로 많은 관심을 모으고 있습니다. C++/CUDA로 작성된 핵심 연산 커널들과 Python 인터페이스로 구성되어 있어, Python 환경 (예: ROS의 Python 노드 등)에서 손쉽게 불러쓸 수 있으면서도 시간Critical한 부분은 GPU로 동작하도록 최적화되어 있습니다. 라이브러리의 구조도 모듈화되어, 필요한 부분만 떼어쓰는 것도 가능합니다. 예를 들어 정방향/역방향 기구학, 충돌 검사, MPPI 최적화 모듈 등을 각각 독립적으로 호출하여 쓸 수 있고, 이를 조합하여 자체적인 모션플래너를 구성할 수도 있습니다.\n실제로 cuRobo는 MoveIt과 같은 기존 로봇 모션 계획 프레임워크와 연동도 고려되었는데, Isaac ROS cuMotion이라는 MoveIt 플러그인 형태로도 제공되어 ROS 환경에서 cuRobo의 계획기를 사용할 수 있습니다. 다만 이 상용 패키지는 GPU가 없는 시스템에서는 동작이 어려울 수 있으므로 (cuRobo 자체가 GPU 가속을 전제로 설계), ROS를 쓰는 경우에도 엔비디아 GPU 플랫폼에서 활용하는 것이 권장됩니다.\n구현 난이도 측면에서, cuRobo가 자체 개발한 GPU 커널들은 매우 전문적인 최적화 기법들이 동원되었으나, 다행히 최종 사용자 입장에서는 API를 통해 사용하면 내부 구현을 몰라도 될 정도로 추상화되어 있습니다. 하지만 새로운 로봇 모델이나 새로운 형태의 장애물 환경을 적용하려면, 해당 모델의 구(sphere) 근사 모델을 생성해야 하는 등의 추가 작업이 필요합니다. 논문에서 언급된 바에 따르면, NVIDIA Isaac Sim 등의 툴을 이용해 로봇의 메시를 빠르게 다중 구로 치환해주는 유틸리티가 개발되었고, 이를 통해 많은 로봇에 구 모델을 작성해두었다고 합니다. 즉, 현실적으로 연구자들이 자주 쓰는 로봇(Franka Panda, UR 시리즈, Kinova Jaco 등)은 이미 sphere 모델이 준비되어 있어 곧바로 활용 가능할 것입니다. 만약 사용자가 직접 모델을 추가해야 한다면 이 과정이 조금 번거로울 수 있지만, 일회성 작업이며 이후에는 충돌 검사가 워낙 빨라지는 이점이 크므로 감수할 만합니다.\n정리하면, cuRobo는 최신 GPU 프로그래밍 기법으로 무장한 전문적인 라이브러리이지만, 이를 접하는 연구자들은 이미 구현된 함수를 불러써서 효과를 누릴 수 있습니다. 더 나아가 관심 있는 개발자라면 오픈소스 커뮤니티에 기여하여 기능을 개선하거나, cuRobo를 자신의 연구에 맞게 확장해볼 수도 있을 것입니다. 이미 cuRobo의 오픈소스화로 인해 컴퓨터 아키텍처 연구자들이 저정밀 연산으로 메모리 병목을 줄이는 실험을 했다거나, 로보틱스 외 분야 전문가들이 이 프레임워크를 기반으로 새로운 알고리즘을 시험해볼 수 있게 되는 등 긍정적인 파급효과가 보고되고 있습니다."
  },
  {
    "objectID": "posts/paper/2025-07-17-curobo.html#기존-모션-생성-기법과의-비교",
    "href": "posts/paper/2025-07-17-curobo.html#기존-모션-생성-기법과의-비교",
    "title": "📃CuRobo 리뷰",
    "section": "2.6 기존 모션 생성 기법과의 비교",
    "text": "2.6 기존 모션 생성 기법과의 비교\ncuRobo의 성능과 접근법을 이해하기 위해, 기존의 대표적인 로봇 모션 계획/생성 기법들과 비교해보겠습니다. 일반적으로 로봇 모션 계획 분야에서는 샘플링 기반 기법(예: RRT, PRM)과 최적화 기반 기법(예: CHOMP, TrajOpt, STOMP 등), 그리고 실시간 프로파일 생성기(예: Ruckig) 등이 혼재하여 사용되어 왔습니다. 각각의 특성과 한계를 짚어보면 다음과 같습니다.\n\n샘플링 기반 기법 (예: RRT, BiRRT, OMPL 등): 경로를 찾기 위해 로봇의 구성공간(C-space)을 무작위로 샘플링하며 탐색하는 알고리즘입니다. 대표적으로 RRT (Rapidly-exploring Random Tree) 알고리즘은 전역적인 경로 탐색에 강점이 있어 좁은 공간에서도 탈출 경로를 잘 찾아냅니다. 그러나 경로 품질이 들쭉날쭉하고, 최단경로를 보장하지 않으며, 한번 찾은 경로를 부드럽게 다듬는 별도 과정이 필요합니다. Tesseract 등의 프레임워크에서 RRT-Connect 같은 방법을 기하학적 플래너(geometric planner)로 쓰고, 결과를 TrajOpt 최적화로 다듬는 식으로 사용했는데, cuRobo 또한 cuRobo-GP라는 이름으로 자체 RRT 유사 플래너를 가지고 있습니다. 다만 cuRobo-GP는 GPU 병렬화를 통해 한 번에 많은 노드 확장을 동시에 수행하므로 일반 RRT보다 100배 이상 빠르게 경로를 찾아냅니다. 실제로 OMPL의 RRTConnect와 비교 시 평균 101배 속도 향상을 보였고, 특히 어려운 문제 구간에선 580배까지도 속도 차가 벌어졌습니다. 이는 CPU 기반 샘플링 기법이 가질 수밖에 없는 선형 증가(하나의 경로 점씩 검사) 과정을, cuRobo는 병렬 그래프 확장으로 획기적으로 바꾼 덕분입니다. 그러나 여전히 샘플링 경로는 최적화된 경로에 비해 길고 비효율적일 수 있어, cuRobo에서는 이 결과를 최종 솔루션으로 바로 쓰기보다는 최적화 알고리즘의 초기 시드로 활용하거나, 최적화가 실패할 경우 대비책으로 사용합니다.\n최적화 기반 기법 (예: CHOMP, TrajOpt 등): 초기 경로가 주어지면 그 경로를 점진적으로 개선하여 충돌 없고 매끄러운 경로로 만드는 방법들입니다. CHOMP(Covariant Hamiltonian Optimization for Motion Planning) 같은 알고리즘은 경로를 수리적으로 미분가능한 함수로 보고 경사하강(gradient descent)을 통해 비용을 줄이는 방식을 최초로 보여준 기법입니다. 하지만 CHOMP는 CPU 기반으로 동작하며, 복잡한 환경에서는 수 초 이상의 계산시간이 걸릴 수 있고, 지역 최적해에 빠지면 실패율이 높다는 단점이 있습니다. 이후 나온 TrajOpt(Trajectory Optimization)은 CHOMP와 비슷한 목적을 갖되, 제약조건을 다루기 위해 이차 프로그래밍(QP) 풀이를 매 반복마다 수행하는 Sequential Convex Optimization 접근을 사용합니다. TrajOpt는 CHOMP보다 수렴이 빠르고 제약 다루기가 쉬워 여러 프레임워크에 통합되었지만, 여전히 충돌 체크나 복잡한 환경에서의 초기값 의존성 문제가 있습니다. cuRobo는 TrajOpt가 Tesseract에 통합된 버전과 직접 비교 실험을 했는데, 평균 87배 빠르고, 최악의 경우 145배까지 빠르게 경로 최적화를 완료했습니다. 예를 들어 TrajOpt가 1.79초 걸리던 문제를 cuRobo는 단 0.01초(10ms) 만에 풀어낸 사례도 있었습니다. 게다가 TrajOpt는 각 문제를 단일 시드로 풀기에 실패하면 다른 시드로 재시작해야 하지만, cuRobo는 애초에 다중 시드를 병렬로 시도하여 한 번에 성공 확률을 높이므로 반복 재시도 횟수도 적습니다】. 이러한 이유로 전체 모션 플래닝 파이프라인**으로 보면 cuRobo가 TrajOpt 기반 플래너보다 훨씬 견고하고 빠릅니다 (실험에서 TrajOpt+RRT 방식을 60초까지 반복 재시도하게 했음에도 cuRobo보다 성공률과 품질이 낮았습니다).\n실시간 속도 프로파일러 (예: Ruckig): 한편, Ruckig 라이브러리는 산업 현장에서 각광받는 실시간 모션 프로파일 생성기입니다. 이것은 경로 자체를 찾기보다는, 이미 정해진 경로(또는 단순히 시작-목표 두 점 사이)를 얼마나 빠르게/부드럽게 이동할지 속도-가속도-저크 한계 내 최적 시간을 계산해주는 알고리즘입니다. Ruckig는 입력으로 시작 상태(위치,속도,가속도)와 목표 상태(위치,속도,가속도)를 주면, 이동 시간 최소화를 목표로 하면서도 지정된 최대 가속/저크 제약을 만족하는 속도 프로파일을 출력해줍니다. cuRobo와 Ruckig를 직접 비교하는 것은 애플-오렌지 격이지만, “jerk 제한”을 다룬다는 공통점이 있어 언급할 가치가 있습니다. Ruckig의 장점은 마이크로초 단위의 빠른 계산으로 로봇 제어 루프에 바로 넣을 수 있다는 점이지만, 경로 상의 장애물은 전혀 고려하지 않기 때문에 주어진 경로 자체가 충돌 없는 것이어야 합니다. cuRobo는 훨씬 복잡한 문제(경로 탐색+속도 프로파일 동시 결정)를 다루므로 시간은 약간 더 들지만, 그 대신 장애물 회피를 포함한 전역 계획을 합니다. 논문에서는 Ruckig가 다룰 수 없는 중간 경유점이 있는 경우(waypoints 시나리오)에 대해 평가하려 했으나, Ruckig 라이브러리의 해당 기능은 공개 라이선스로 접근하기 어려워 비교하지 못했다고도 언급합니다. 이는 Ruckig가 주로 1회 구간(point-to-point) 모션에 최적화되어 있고, 여러 구간을 잇는 것은 제한이 있다는 의미로 해석됩니다. 요약하면, Ruckig = 동적 제약을 만족하는 세부 프로파일러, cuRobo = 전역 경로+프로파일 통합 솔루션으로 볼 수 있습니다. 실제 적용에서는, cuRobo로 큰 그림 경로를 찾은 뒤 Ruckig로 마지막 미세조정을 하는 조합도 생각해볼 수 있으나, cuRobo 자체도 jerk 최적화를 포함하므로 별도 조정 없이 바로 실행 가능한 수준의 결과를 줍니다.\n충돌 검사 라이브러리 (예: FCL 등): 로봇 경로 계획의 한 축을 담당하는 것이 충돌 검사 알고리즘입니다. 앞서 설명한 FCL은 널리 쓰이는 충돌 라이브러리로, 로봇의 링크와 장애물의 메시 형상 간 최근접 거리나 충돌 여부를 빠르게 계산해주는 도구입니다. 그러나 복잡한 장면에서는 수천 - 수만 쌍의 삼각형 간 충돌을 계산해야 하므로 GPU 없이 CPU 단일 스레드로는 시간이 꽤 소요됩니다. cuRobo는 FCL 대신 자체 GPU 충돌 검사를 사용하며, 로봇-환경 상호작용을 수학적 함수(거리 함수) 형태의 비용으로 통합했습니다. 구로 근사한 로봇 모델 덕분에 연속 충돌 비용의 그래디언트까지 계산 가능한 점은 기존 충돌 라이브러리들이 이진 충돌 여부만 체크하는 것과 대조적입니다. 덕분에 cuRobo의 최적화 알고리즘은 충돌 회피 그래디언트를 따라 경로를 밀어낼 수 있고, 이는 CHOMP 등의 철학과 유사하나 훨씬 병렬화되어 빨라진 형태라고 볼 수 있습니다. 결과적으로 cuRobo의 충돌 검사는 동일한 상황에서 기존 PyBullet 충돌 체크보다 최대 16000배 빠르며, 수만 개의 후보 자세를 동시에 평가할 수도 있어 향후 학습기반 모션 생성이나 몬테카를로 트리 탐색 등에서 충돌 체크 병목을 제거하는 데 기여할 수 있습니다.\n\n종합하면, cuRobo는 기존 방법들의 장점을 흡수하되 단점은 과감히 극복한 차세대 접근이라고 할 수 있습니다. 샘플링 기법의 전역 탐색 능력을 도입하되 병렬화로 속도 문제를 해결했고, 최적화 기법의 부드러운 경로 생성 능력을 발전시켜 저크까지 최소화했으며, 실시간 프로파일러 수준의 속도 제약 만족 능력을 통합했습니다. 그리고 이 모두를 가능케 한 것이 대규모 병렬 컴퓨팅 활용이라는 점에서, cuRobo는 로봇 모션 계획에 병렬화 패러다임을 성공적으로 이식한 사례로 평가됩니다."
  },
  {
    "objectID": "posts/paper/2025-07-17-curobo.html#주요-실험-결과-및-활용-시나리오",
    "href": "posts/paper/2025-07-17-curobo.html#주요-실험-결과-및-활용-시나리오",
    "title": "📃CuRobo 리뷰",
    "section": "2.7 주요 실험 결과 및 활용 시나리오",
    "text": "2.7 주요 실험 결과 및 활용 시나리오\ncuRobo 논문에서는 다양한 환경에서의 실험 결과를 통해 그 유용한 활용 시나리오들을 제시합니다. MotionBenchMaker라는 벤치마크 데이터셋을 활용한 실험에서, 여러 가지 복잡한 장면(scene)들이 시험되었습니다. 예를 들어 협소한 cage 안에서 팔 뻗기, 선반(shelf) 사이로 물체 집기, 테이블 주위로 물체 옮기기 등, 산업 및 서비스 로봇에서 흔히 마주치는 상황들이 포함되었죠. 이러한 12가지 유형의 환경에 대해 Franka Panda 로봇이 충돌 없이 목표 위치로 팔을 움직이는 문제들이 생성되었고, cuRobo는 모든 환경에서 고르게 높은 성공률과 빠른 계획 시간을 보였습니다.\n특히 복잡한 장애물 밀집 환경에서 cuRobo의 강점이 두드러졌는데, 이는 병렬 다중 시드 접근 덕분입니다. 예를 들어 “공” 모양 장애물이 가득한 3차원 공간이나 미로 형태의 장벽이 있는 경우, 전통적 최적화 방법은 초기 경로에 따라 쉽게 충돌 지역에서 못 빠져나오고 실패할 수 있지만, cuRobo는 수십 개의 경로를 동시에 시도하면서 하나라도 뚫리면 곧바로 찾아냅니다. 그리고 찾은 경로는 다시 최적화 과정을 거치며 더욱 다듬어지기 때문에, 처음에는 구불구불하던 샘플링 경로가 결과적으로 매우 매끄럽고 짧은 경로로 개선됩니다.\n또 하나의 시나리오는 다중 목표 순차 달성(Task Sequencing) 문제입니다. 산업현장에서 로봇은 종종 여러 지점을 순서 없이 주어지고 최적 경로로 순회해야 하는데, cuRobo는 그런 문제의 부분 경로(point-to-point)들을 빠르게 해결해줄 수 있습니다. 예컨대 10개의 픽업 포인트를 최적 순서로 방문하는 문제를 풀 때, 기존에는 각 구간 경로를 느린 모션 플래너로 풀어야 했지만, cuRobo를 쓰면 모든 구간 경로 비용을 신속히 계산해볼 수 있으므로, 상위 레벨에서 최적 순서 문제를 풀기 수월해집니다. 논문에서도 이러한 작업 스케줄링이나 경로 최적화(예: Traveling Salesman Problem 변형)에 cuRobo가 기여할 수 있음을 제시하고 있습니다.\n현 실시간 제어 쪽 활용으로는, 센서 피드백을 통한 동적 재계획에의 응용이 있습니다. 비록 현재 cuRobo 자체는 부분 관찰 문제(실시간으로 바뀌는 환경 인식)에 대응하는 로직은 없지만, 그 고속성 덕분에 센서로 새로운 장애물을 감지할 때마다 빠르게 재계획을 수행하는 방식으로 반응형 회피를 구현할 수 있습니다. 예를 들어 로봇 팔 작업 중 사람이 손을 넣는 등 돌발 상황이 생기면, cuRobo로 50ms 내 새 안전 경로를 찾아 즉각 로봇 움직임을 수정하는 형태로 사용할 수 있을 것입니다. 완전한 Reactive Planning에 대해서는 여전히 학술적으로 도전이 남아있지만 (cuRobo도 98% 난이도 케이스가 0.26초 걸렸듯 최악 경우 수백 ms - 1초 수준까지 걸릴 수 있기 때문에, 엄격한 실시간 고정주기에는 추가 연구 필요), 점진적으로 개선하여 향후 수 밀리초 내 경로 업데이트도 꿈꿔볼 수 있습니다.\n그밖에 cuRobo의 병렬 IK는 로봇 가시성 영역 분석 등에 바로 활용 가능합니다. 논문에서는 3D 공간 격자상의 임의 자세 500개에 대해 cuRobo IK를 병렬 실행하여, 해당 자세에 도달 가능한지 여부(초록색/빨간색 구체로 표시)를 15Hz로 시각화하는 예시를 보였습니다. 이는 로봇의 작업범위나 특정 위치 접근 가능 여부를 실시간으로 계산해주는 유용한 도구로 쓰일 수 있습니다. 또한 cuRobo의 빠른 충돌 IK는 기존에 수 초 걸리던 샘플 기반 충돌회피 IK를 밀리초 단위로 가속하였기에, 앞으로 다관절 로봇의 자세 제약 문제(예: 동시에 여러 로봇 배치 최적화) 등에도 적용 가능성이 큽니다.\n마지막으로, cuRobo가 여러 로봇 플랫폼에 이식된 사례도 소개됩니다. 연구진은 cuRobo를 Franka, UR 등 외에도 Nvidia Isaac 시뮬레이터 상의 Kinova Jaco 로봇팔, Allegro 손 등에도 적용해보았고, 기본 알고리즘이 로봇 기구학만 맞춰주면 공통으로 동작함을 확인했습니다. 따라서 특정 로봇에 하드코딩된 솔루션이 아니며, 범용 로봇 모션 계획 라이브러리로서 활용할 수 있음을 보여줍니다."
  },
  {
    "objectID": "posts/paper/2025-07-17-curobo.html#결론-curobo의-장점과-한계-향후-발전-방향",
    "href": "posts/paper/2025-07-17-curobo.html#결론-curobo의-장점과-한계-향후-발전-방향",
    "title": "📃CuRobo 리뷰",
    "section": "2.8 결론: cuRobo의 장점과 한계, 향후 발전 방향",
    "text": "2.8 결론: cuRobo의 장점과 한계, 향후 발전 방향\ncuRobo는 로봇 모션 생성 문제에 병렬화와 GPU 가속이라는 강력한 무기를 도입함으로써, 속도, 성공률, 경로 품질 모든 면에서 새로운 표준을 제시한 솔루션입니다. 요약하면, cuRobo의 주요 장점은 다음과 같습니다:\n\n획기적인 속도: 복잡한 충돌회피 경로도 밀리초 단위에 찾아내며, 기존 SOTA 방법 대비 수십 배 - 수백 배 빠릅니다. 이는 실시간 애플리케이션 및 고빈도 재계획에 필수적인 이점을 제공합니다.\n높은 신뢰도: 다중 시드 병렬 최적화 덕분에 사실상 100%에 근접하는 성공률로 경로를 찾아냅니다. 지역해 문제를 크게 완화하여, 까다로운 환경에서도 인간 수준으로 유연하게 경로를 견주어 찾습니다.\n부드럽고 안전한 경로: 가속도-저크 최소화를 통해 로봇이 실행하기에 매우 매끄러운 움직임을 제공합니다. 이는 로봇 관절의 기계적 스트레스를 줄이고, 고속동작 시에도 진동이나 오차를 최소화하여 결국 작업 품질과 로봇 수명 향상으로 이어집니다.\n모듈화와 확장성: IK, 기하학 플래너, 최적화 모듈 등 구성요소별 모듈화로 필요에 따라 부분적으로 사용할 수 있고, 오픈소스로 공개되어 연구 커뮤니티의 참여와 개선이 가능합니다. 다양한 로봇에 쉽게 이식 가능하며, MoveIt 플러그인 등 기존 시스템과 연계도 지원됩니다.\n임베디드 친화성: GPU가 필요하긴 하지만, Jetson과 같은 임베디드 플랫폼에서도 구동될 만큼 경량 최적화가 되어 있어, 현장 로봇에 직접 탑재하여 사용할 수 있습니다.\n\n반면, 한계 및 향후 과제로 지적되는 부분도 몇 가지 있습니다:\n\n동적/반응형 계획의 한계: 현재 cuRobo는 정지된 상태의 로봇이 정적인 환경에서 목표까지 가는 완전한 모션을 계획합니다. 만약 로봇이 움직이는 중간에 새로운 장애물이 나타나거나 목표가 변한다면, 이를 지속적으로 재계획하는 기능(reactive planning)은 아직 구현되어 있지 않습니다. 최악 경우 수백 ms 걸릴 수 있는 점을 고려하면, 하드 실시간 제어 주기(예: 1kHz)에는 추가 연구가 필요합니다.\n복잡한 작업 제약: cuRobo는 기본적으로 말단-포즈 도달 문제에 초점을 맞추고 있습니다. 중간에 도구의 자세를 유지한다거나 특정 경로 제약(예: 컵을 기울이지 않게 운반) 등의 과업 제약이 있는 경로 계획에 대해서는 아직 일반적인 해법을 통합하지 않았습니다. 일부 Orientation constraint 등을 큰 가중치로 넣는 실험은 해보았으나, 이를 엄밀하게 다루는 전용 알고리즘(예: 제약된 탐색공간에서의 RRT 등)은 향후 과제로 남겨두었습니다.\n동역학 및 접촉 고려 부족: 현재 비용 함수는 로봇의 기구학적 움직임(관절 위치/속도/가속도)에 관한 항들로 이루어져 있습니다. 따라서 물체를 밀거나 잡는 등 접촉이 있는 계획, 혹은 토크 최적화나 에너지 최적화 같은 동적 요소는 포함되어 있지 않습니다. 이러한 부분을 다루려면 로봇의 동역학 모델까지 포함한 최적화(예: 토크 최소화, 임피던스 최적화 등)나 contact-implicit trajectory optimization 기법을 접목해야 하는데, 이는 연구적으로 난이도가 높지만 향후 확장 가능성으로 제시되었습니다.\n부분적인 환경 인식: cuRobo는 환경 모델이 완전히 주어져 있다는 전제하에 작동합니다. 센서 데이터로부터 실시간으로 환경 지도를 작성하거나, 보이지 않는 영역의 장애물을 추정하는 등의 퍼셉션 불확실성 하에서는 별도의 대책이 필요합니다. 논문에서는 학습 기반으로 미지 환경에 대한 충돌 비용추정을 통합하는 등 아이디어를 제시하지만, 아직 구현된 것은 아닙니다.\n전문 지식 요구: 비록 오픈소스로 제공되지만, cuRobo의 최적 성능을 끌어내거나 내부를 수정하려면 GPU 병렬 프로그래밍, 수치 최적화 등 상당한 전문성이 필요합니다. 다행히도 구조를 모듈화하여 외부 연구자들이 각 컴포넌트를 개선할 수 있게 해두었고, 실제로 학계 다른 연구자들이 cuRobo 코드를 참고해 로봇 알고리즘 전용 솔버를 개선하거나, 하드 제약을 직접 풀어내는 기법을 접목하는 등의 시도를 하고 있습니다. 이런 협업을 통해 점차 진입장벽을 낮추고 활용성을 높여가는 것이 기대됩니다.\n\n결론적으로, cuRobo는 로봇 모션 계획 분야에 새로운 속도와 품질의 지평을 연 매우 인상적인 연구成果입니다. GPU 병렬화의 이점을 극대화하여, 이전까지 수 초 걸리던 문제를 실시간으로 해결하고, 결과 궤적의 실용적 안정성까지 확보했다는 점에서, 앞으로 다양한 로봇 시스템에 응용될 것으로 보입니다. 향후 실시간 대응성 강화, 보다 복잡한 제약 조건 지원, 동역학 통합 등 남은 과제들을 해결한다면, cuRobo 혹은 그 파생 기술은 범용 로봇 모션 플래닝의 표준 툴로 자리매김할 가능성이 큽니다. 로봇공학 연구자들은 이 도구를 통해 더욱 빠르고 똑똑한 로봇을 구현할 수 있을 것이며, 동시에 남은 도전과제를 함께 풀어나감으로써 로봇의 움직임에 대한 이해와 제어 능력을 한층 발전시킬 수 있을 것입니다.\n참고 문헌 (논문 및 관련 자료)\n\nSundaralingam, B., et al. “cuRobo: Parallelized Collision-Free Minimum-Jerk Robot Motion Generation.” arXiv preprint arXiv:2310.17274 (2023) 등.\nGithub Repository: NVlabs/curobo (https://github.com/NVlabs/curobo) .\nNVIDIA Developer Blog: “CUDA-Accelerated Robot Motion Generation in Milliseconds with cuRobo” (2023).\nMoveIt Plugin: Isaac ROS cuMotion.\n기타 비교 기법: OMPL, TrajOpt, Ruckig, TracIK, PyBullet 등 관련 공식 문서 및 논문."
  },
  {
    "objectID": "posts/paper/2023-07-02-dreamwaq.html",
    "href": "posts/paper/2023-07-02-dreamwaq.html",
    "title": "📃DreamWaQ 리뷰",
    "section": "",
    "text": "이번 포스팅은 DeepMind에서 발표된 DreamWaQ: Learning Robust Quadrupedal Locomotion With Implicit Terrain Imagination via Deep Reinforcement Learning 논문을 읽고 정리한 내용입니다. 최근 ICRA 2023 런던에서 5월 30일부터 6월 1일까지 진행된 Autonomous Quadruped Robot Challenge (QRC)에서 KAIST 연구팀이 1등을 하여 큰 이슈가 되었었습니다. 이번 포스팅에서 리뷰하는 이 논문이 바로 대회에서 사용되었던 강화학습 기반의 보행제어 알고리즘에 대한 내용을 담고 있는 논문입니다."
  },
  {
    "objectID": "posts/paper/2023-07-02-dreamwaq.html#key-contribution",
    "href": "posts/paper/2023-07-02-dreamwaq.html#key-contribution",
    "title": "📃DreamWaQ 리뷰",
    "section": "2.1 Key Contribution",
    "text": "2.1 Key Contribution\n\n\n\nOverview of DreamWaQ\n\n\nDreamWaQ 알고리즘의 전체적인 흐름은 위의 사진과 같습니다. “Dream”이라는 워딩과 알고리즘 개괄도에서 생각 풍선 모양 표현에서 볼 수 있듯이 DreamWaQ 논문의 주요 Contribution으로는 Implicit Terrain Imagination을 할 수 있도록 Context-Aided Estimator Network(CENet)을 도입하였고 안정적으로 Policy가 학습될 수 있도록 Adaptive Bootstrapping(AdaBoot)방법을 제안하여 강화학습 보행 제어기를 설계한 점을 들 수 있습니다."
  },
  {
    "objectID": "posts/paper/2023-07-02-dreamwaq.html#implicit-terrain-imagination",
    "href": "posts/paper/2023-07-02-dreamwaq.html#implicit-terrain-imagination",
    "title": "📃DreamWaQ 리뷰",
    "section": "2.2 Implicit Terrain Imagination",
    "text": "2.2 Implicit Terrain Imagination\n앞서 챌린지에서 사용된 환경에서 볼 수 있듯이 사족보행로봇은 다양한 지형(Terrain)을 극복하며 보행할 수 있는 능력이 중요합니다. 그럼 다양한 지형을 나타낼 수 있는 속성들에는 무엇이 있을까요? 지형의 마찰계수, 반발계수, 놓여져 있는 장애물, 울퉁불퉁한 정도 등등 여러가지 속성들로 지형의 특징을 나타낼 수 있을 것입니다. 그리고 그러한 특징을 어떻게 4개의 다리를 이용하여 보행의 어려운 점들을 극복하며 원하는 방향으로 이동할 수 있도록 하는 것이 관건인 것입니다.\n이런 지형의 특징을 파악하기 위해 많은 연구들이 카메라나 라이다와 같은 비젼센서를 부착하여 환경을 인식한 뒤 극복하기 위한 방법을 고안하는 방향으로 연구되고 있습니다. 하지만 KAIST 연구진이 제안한 DreamWaQ에서는 지형을 인식할 수 있는 부차적인 비젼센서 없이 로봇의 자체의 정보(proprioception)를 이용하여 지형을 극복하기 위해 explicit한 환경 정보가 아닌, implicit한 terrain imagination을 할 수 있는 방법론은 제시했습니다.\n사실 Implicit하게 로봇 주변의 지형이나 환경정보를 강화학습 로봇 에이전트가 인식할 수 있도록 하는 연구는 다양하게 진행되어왔었습니다. 앞선 주요 방법은로는 Teacher-Student Network를 이용하여 모든 환경정보를 학습한 Teacher Network로부터 Student Network가 추후에 따라 학습하는 방식이 있었지만, 해당 방법은 Teacher Network를 학습과 Student Network 학습을 따로 2개의 단계를 거쳐 학습을 해야한다는 데이터 비효율적인 학습 방법이라는 단점이 있었습니다. 따라서 DreamWaQ에서는 Asymmetric Actor-Critic이라는 기존의 Actor-Critic 강화학습 알고리즘에 약간 변형을 준 모델을 사용하여 Teacher-Student Network처럼 두 단계로 나누어서 학습하지 않고도 Implicit하게 Terrain 정보를 Actor-Critic 구조에 녹여들 수 있도록 했습니다."
  },
  {
    "objectID": "posts/paper/2023-07-02-dreamwaq.html#asymmetric-actor-critic",
    "href": "posts/paper/2023-07-02-dreamwaq.html#asymmetric-actor-critic",
    "title": "📃DreamWaQ 리뷰",
    "section": "2.3 Asymmetric Actor-Critic",
    "text": "2.3 Asymmetric Actor-Critic\n기존의 PPO, SAC와 같은 Policy Gradient의 강화학습 알고리즘들의 주요 구성요소로 Actor Network와 Critic(Value) Network가 있습니다. Actor는 강화학습 에이전트가 취해야하는 action 값을 출력하는 네트워크이며 Critic는 에이전트의 학습 방향을 보여주는 value값을 출력하여 이 2개의 네트워크들이 Policy Gradient 알고리즘의 목적식을 따라 Return(누적 보상)값을 최대화하는 방향으로 학습하게 되는 것입니다. 보통 2개의 네트워크 모두에게 같은 state(혹은 observation) 정보가 입력값으로 들어가게 되기 때문에 Actor 네트워크와 Critic 네트워크는 서로 Symmetric하다고 할 수 있습니다.\n하지만 앞서 로봇이 센서 없이는 얻을 수 없는 지형 정보가 강화학습 알고리즘에 사용되는 네트워크의 인풋으로 들어간다면 실제 로봇에서 알고리즘이 돌아갈 때 넣어줄 지형정보가 없기 때문에 제어 알고리즘이 돌아갈 수 없을 것 입니다. 그래서 DreamWaQ에서는 Actor/Critic Network의 상호작용 과정에서 강화학습 에이전트가 얻을 수 있는 시간적 정보들을 기반으로 terrain 정보를 상상할 수 있도록, Actor 네트워크에 들어가는 입력값과 Critic 네트워크에 들어가는 입력값을 다르게 설계하였고 이를 Asymmetric한 구조라고 볼 수 있습니다.\n\n\n\nAsymmetric Actor-Critic\n\n\n위에 보이시는 것처럼 Actor Network에는 Observation o_t, estimated velocity v_t, latent vector z_t가 입력으로 들어가게 됩니다. v_t와 z_t는 다음 파트에서 좀 더 살펴볼 예정이므로 여기에서는 우선 observation vecter인 o_t에 초점을 맞추어서 보겠습니다. observation 정보는 강화학습 MDP를 정의하는 한 요소로 강화학습 에이전트가 학습할 때 관측(혹은 접근 가능한 정보)하는 정보입니다. 따라서 로봇에 특별한 비젼 센서 추가 없이 로봇 자체 하드웨어에서 얻을 수 있는 정보인 proprioceptive 정보를 기반으로 몸체의 각속도 \\omega_t, 중력방향 벡터 g_t 등등의 정보가 observation vector의 요소로 들어가게 됩니다. 반면, Critic Network에는 State s_t가 입력값으로 들어가는 것을 알 수 있는데 이는 위에서 Observation과 State를 비교해놓은 것과 같이 state가 observation보다 많은 정보를 포함한 것을 알 수 있습니다. 여기에서 주목해서 볼 수 있는 점이 바로 지형에 대한 정보인 heightmap scan h_t가 한 요소임을 알 수 있고 이를 통해 implicit한 terrain imagination이 가능한 것 입니다. Heightmap scan에 대해 조금 더 설명을 덧붙이자면, 지형의 heightmap scan 정보는 실제 로봇에서 얻을 수 있는 정보는 아니고 강화학습 에이전트가 학습하게 되는 시뮬레이션에서만 얻을 수 있는 정보로 지형의 z축 방향의 높이 정보를 말합니다.\n환경을 정의하는 변수이고 시뮬레이션에서는 가상공간이기 때문에 프로그램에서 얻을 수 있는 물리적 정보이지만 실제로 로봇이 이용할 수 없는 정보를 privileged observation이라고 부르기도 합니다. 따라서 기존에 강화학습에서 State가 환경에서 에이전트가 놓여있는 상황을 설명할 수 있는 모든 정보를 말하고 Observation이 환경에 놓여있는 에이전트가 관찰할 수 있는 일부 상태 정보를 뜻하기 때문에 State = Observation + Privileged Observation 포함관계로 이해할 수 있습니다.(논문에서는 privileged observation이라는 표기를 state를 뜻하는 것으로 표기하고 있기 때문에 헷갈릴 수 있습니다.)"
  },
  {
    "objectID": "posts/paper/2023-07-02-dreamwaq.html#context-aided-estimator-network",
    "href": "posts/paper/2023-07-02-dreamwaq.html#context-aided-estimator-network",
    "title": "📃DreamWaQ 리뷰",
    "section": "2.4 Context-Aided Estimator Network",
    "text": "2.4 Context-Aided Estimator Network\n이번 파트에서 살펴보게 될 Context-Aided Estimator Network는 센서로 인식할 수 없는 지형 정보를 에이전트가 유추할 수 있도록 하는 일등공신 아이디어 입니다.\n\n\n\nThe architecture of CENet\n\n\nCENet의 구조는 위와 같이 \\beta-VAE구조를 활용하여 구성되어 있습니다. 일정 time horizon H동안 모은 observation이 Encoder에 들어가면 latent vector z와 몸체의 선속도 추정값인 v_t가 출력값으로 나오게 됩니다. Auto-Encoder의 일반적인 구조를 따라 이 값들이 Decoder의 인풋으로 들어가고 Decoder의 출력값으로는 time horizon을 지난 다음 observation vector o_{t+1}을 reconstruction할 수 있도록 학습하게 되는 것 입니다.\n\n\n\nThe loss of CENet\n\n\n그래서 CENet의 loss function은 크게 2개의 파트 L_{est}와 L_{VAE}로 나누어져 있는 것을 확인할 수 있습니다. 먼저 L_{est}는 보행하는 로봇 에이전트의 속도 추정을 CENet에서 할 수 있도록 학습하기 위한 부분으로, 로봇 몸체의 선속도 추정값 \\tilde{v}_t는 실제 정답값 v_t는 시뮬레이션에서는 얻을 수 있는 값이기 때문에 Encoder에서 추정한 값 \\tilde{v}_t와의 MSE(mean square error)를 구할 수 있습니다. 다음으로 L_{VAE}는 time horizon H동안 누적되 여러개의 observation 정보를 가지고 다음 observation o_{t+1}을 오토인코더 구조로 잘 reconstruction한지를 보는 첫번째 term과 추정 분포를 맞추는 부분인 KL-divergence 제약 조건 두번째 term으로 이루어져 있습니다. (VAE loss에 대해서 더 자세한 정보를 알고 싶으신 분은 이전에 VAE 논문을 리뷰한 포스팅을 참고해주세요.)\n이와 같은 loss 구성으로 학습된 CENet은 여러 타임 스텝동안 관찰된 observation 정보들을 기반으로 에이전트가 privileged observation을 유추할 것으로 기대할 수 있는 이유는 privileged observation을 기반으로 가치를 추정하는 Critic(Value) Network를 통해서 Actor Network가 업데이트 되는 Policy gradient과정을 거치기 때문입니다. 이러한 Asymmetric Actor-Critic구조와의 시너지 효과가 기존의 Context RL 분야에서도 사용되는 아이디어 인데(참조논문: AACC) 이와 비교해보았을 때, Critic Network가 deploy되는 과정에서 쓰이지 않기 때문에 Actor보다 더 많은 정보를 받아서 더 정확한 가치를 추정할 수 있게 한다는 기조는 비슷하지만 time-invarient한 context vector를 만드는 Context RL에서의 Asymmetric Actor-Critic과 다르게 DreamWaQ에서는 time-varient한 변수들을 추정하여 implicit하게 추정할 수 있도록 했다는 점이 다릅니다.\nAdaptive Bootstrapping(AdaBoot)\nAdaptive bootstrapping은 policy 학습과정 중에 Estimator network인 CENet이 안정적으로 학습되도록 하기 위해 domain randomized로 다양화된 여러 환경요소에 대해 에피소드별 reward의 평균값에 대한 표준 편차의 비율인 변동 계수(CV)에 의해 제어되는 방법을 말합니다. 핵심 아이디어는 부정확한 가치 추정에 대한 정책을 보다 견고하게 만들기 위해 m개의 에이전트 reward의 CV가 작을 때 부트스트래핑을 하게됩니다. 반대로 에이전트가 충분히 학습하지 않은 경우에는 reward에서 큰 CV로 표시된 것처럼 부트스트랩을 해서는 안하도록 합니다.\n\n\n\nAdaptive Bootstrapping Probability"
  },
  {
    "objectID": "posts/paper/2023-07-02-dreamwaq.html#simultation-result",
    "href": "posts/paper/2023-07-02-dreamwaq.html#simultation-result",
    "title": "📃DreamWaQ 리뷰",
    "section": "3.1 Simultation Result",
    "text": "3.1 Simultation Result\n\n\n\nThe loss of CENet\n\n\nIsaac Gym 시뮬레이터를 이용하여 PPO 강화학습 알고리즘 이용하여 학습과정 동안의 Episodic Reward 그래프 변화를 살펴보면, EstimatorNet은 처음에는 AdaptationNet보다 평균 에피소드 보상이 높지만, 더 많은 training step 후에 더 어려운 지형과 마주치기 때문에 더 많은 반복 후에 성능이 저하됨을 알 수 있습니다. 반대로 DreamWaQ는 학습 지형이 점점 어려워 짐에도 다른 모든 방법들을 능가하는 퍼포먼스를 보여줍니다. 외부 인식 없이 걷는 것임에도 DreamWaQ는 주변 지형의 heightmap을 다 알 수 있는 오라클 policy만큼 성능이 좋은 것을 볼 수 있습니다.\nExplicit Estimation Comparison\n시뮬레이션에서 한번 지형정보를 Implicit가 아닌 Explicit하게 알려주고 학습한다면 어떤 유의미한 차이가 있는지 알아보는 실험도 진행했습니다.\n\n\n\nAdaptive Bootstrapping Probability\n\n\nTimestep이 늘어날 수록 더 어려운 계단지형에서 보행하도록 학습시킨 결과 Explicit하게 지형정보를 학습한 Estimator는 지형이 어려워지자 Foot stumble 현상이 심하게 있었지만 DreamWaQ는 지형이 어려워져도 작은 foot stumble이 있음을 확인하여 오히려 Implicit하게 지형정보를 학습하는 것인 robust한 보행을 하는 것을 확인할 수 있었다고 합니다."
  },
  {
    "objectID": "posts/paper/2023-07-02-dreamwaq.html#real-world-result",
    "href": "posts/paper/2023-07-02-dreamwaq.html#real-world-result",
    "title": "📃DreamWaQ 리뷰",
    "section": "3.2 Real-world Result",
    "text": "3.2 Real-world Result\n\n\n\n실제 로봇 플랫폼을 가지고 Command tracking error를 plot 해보았을 때도 다른 비교 모델들과 비교해보았을 때 error 값이 적은 것을 확인할 수 있습니다. 특히나 AdaBoot방법이 있고 없고에 따라 error값의 크기가 다른 것을 통해 AdaBoot 방법이 policy 학습에 필요한 것을 확인할 수 있습니다."
  },
  {
    "objectID": "posts/paper/2025-08-09-retargeting-survey.html",
    "href": "posts/paper/2025-08-09-retargeting-survey.html",
    "title": "📃Retargeting Survey 리뷰",
    "section": "",
    "text": "Paper Link"
  },
  {
    "objectID": "posts/paper/2025-08-09-retargeting-survey.html#연구-배경-및-필요성",
    "href": "posts/paper/2025-08-09-retargeting-survey.html#연구-배경-및-필요성",
    "title": "📃Retargeting Survey 리뷰",
    "section": "2.1 연구 배경 및 필요성",
    "text": "2.1 연구 배경 및 필요성\n사람의 손 움직임을 로봇 손으로 매핑하는 기술은 원격 조작(텔레오퍼레이션)부터 학습을 통한 시연(LbD, Learning by Demonstration)까지 다양한 분야에서 핵심적으로 활용됩니다. 인간 조작자의 높은 인지 능력과 손재주를 로봇 제어에 활용하면, 복잡한 작업의 계획, 오류 감지/회복 및 학습 과정을 사람에게 일부 맡길 수 있어 유용합니다. 실제로 우주 왕복선 및 국제우주정거장 로봇 암/손 조작, 해저 파이프 검사/수리, 의료 미세수술 로봇, 착용형 재활 로봇, 위험 물질 취급(원자력 시설 등), 물류 창고, 농업, 건설, 광산 등 다양한 응용 분야에서 인간 손 움직임을 로봇 손에 실시간으로 복제하는 연구가 진행되어 왔습니다. 이러한 텔레오퍼레이션에서는 오퍼레이터(사람)의 손동작 데이터를 측정해 실시간으로 로봇 손을 구동하며, 이를 통해 멀리 떨어진 환경에서도 정교한 조작이 가능합니다. 또한 시연 학습 분야에서는 사람의 손동작 데이터를 실시간 제어가 아닌 오프라인 학습 데이터로 활용하여, 자율 로봇 손의 기술 습득에 사용합니다. 예를 들어, 사람 손의 공잡이(preshaping)나 그립(grasp) 동작을 측정하여 로봇 손의 물체 파지 계획을 수립하거나, 물체 조작 시의 자세 최적화, 사람과 유사한 궤적 생성 등에 응용합니다.\n그러나 인간 손(HH)과 로봇 손(RH) 사이의 기구학적 불일치로 인해, 인간의 섬세한 손동작을 로봇 손에 그대로 재현하는 것은 겉보기만큼 간단하지 않습니다. 인간 손은 최대 25자유도(관절 25개 이상)까지 움직임을 만들어내는 반면, 로봇 손도 고도로 인간형으로 설계되면 많은 관절을 가지지만, 양쪽의 구조적 차이는 피할 수 없습니다. 예컨대 로봇 손이 인간 손과 형상이 유사할수록(즉 높은 인간형일수록) 매핑도 단순해지지만, 실제 로봇 손은 정확히 인간 손과 일치하지 않으므로 일대일 대응이 어렵고 추가적인 보정이나 변환이 필요하게 됩니다. 더구나 손가락 길이나 관절 위치 등의 개인차와, 센서 장비의 한계로 인해 사람 손의 실제 움직임을 100% 계측하는 것도 불가능하며, 이는 매핑 정확도에 한계를 줍니다. 결국 “인간-로봇 손 매핑 문제”는 여전히 풀리지 않은 개념적·분석적 도전과제로 남아 있으며, 이를 해결하기 위해 두 가지 하위 문제가 제기됩니다: (i) 사람 손동작의 의미 있고 정확한 계측 (센서 기술)과 (ii) 측정된 데이터를 로봇 손동작으로 변환하는 매핑 알고리즘입니다. 본 논문은 이 중 매핑 알고리즘 자체에 초점을 맞추며, 다양한 연구에서 제안된 해결책들을 한데 모아 분류 체계화하고자 했습니다.\n지금까지 인간 손-로봇 손 매핑에 관한 연구들은 적용 분야나 사용 센서, 용어 및 목표가 제각각이라 전반적인 조망이 어려웠습니다. 과거 Li 등(2015)의 서베이 연구가 손 매핑 기법들을 정리한 바 있으나, 이는 주로 컴퓨터 비전 기반 장치에 치우쳐 있고 로보틱스적 관점이 부족하여 한계가 있었습니다. 예컨대 Li 등의 분류는 장치 종류에 따른 구분이라 일반성이 떨어지고, 손 운동 시너지(손가락 간 상관관계 이용)나 혼합형 접근법 등 중요한 방법론도 다루지 않았습니다. 이에 비해 Meattini 등 본 논문은 로봇공학적 관점에서 매핑 문제를 재조명하여, 기술에 종속되지 않는 보편적 용어 정의와 분류 기준을 제시합니다. 특히 인간-로봇 손 매핑의 개념, 주요 도전과제들을 깊이 분석하고, 현대 연구들을 여섯 가지 핵심 범주로 체계화했습니다. 이후 센서/로봇 손 기술 요소와 적용 사례에 대한 논의를 추가하여 종합적인 전망을 제시합니다. 아래에서는 논문에서 제시한 6가지 매핑 접근법 각각의 개념과 대표 사례를 살펴보고, 장단점을 비판적으로 고찰하겠습니다. 더불어 실제 로봇 손 시스템의 응용 시나리오별로 어느 접근법이 어떻게 활용될 수 있는지 검토하고, 마지막으로 저자들이 제안한 미래 연구 방향과 그에 대한 평가, 그리고 이에 기반한 추가적인 시사점을 제안합니다."
  },
  {
    "objectID": "posts/paper/2025-08-09-retargeting-survey.html#직접-관절-매핑-direct-joint-mapping",
    "href": "posts/paper/2025-08-09-retargeting-survey.html#직접-관절-매핑-direct-joint-mapping",
    "title": "📃Retargeting Survey 리뷰",
    "section": "2.2 직접 관절 매핑 (Direct Joint Mapping)",
    "text": "2.2 직접 관절 매핑 (Direct Joint Mapping)\n개념: 직접 관절 매핑은 말 그대로 사람 손의 각 관절각도를 대응되는 로봇 손의 관절에 직접 할당하는 방식입니다. 전제 조건은 로봇 손의 기구학 구조가 인간 손과 어느 정도 일대일로 대응 관계를 가질 때입니다. 이상적인 경우 각 인간 손가락 관절이 로봇 손가락 관절에 1:1로 대응하며, 간단한 선형 비례식으로 매핑할 수 있습니다 (예: θ로봇,i = k·θ인간,i + c). 여기서 k와 c는 각 관절의 스케일링 계수와 오프셋으로, 사람 손과 로봇 손의 관절 가동 범위 차이를 보정하기 위해 경험적 조정이나 최적화를 통해 결정됩니다. 간단히 말해, 센서 장갑 등의 장비로 측정한 사람 손 관절각도 값을 거의 그대로 로봇 손의 구동 명령으로 쓰는 것입니다. 이러한 접근은 1980년대 말부터 등장하여, 1989년 Utah/MIT 로봇핸드에 처음 적용된 사례가 보고되었습니다. 이후 많은 원격 조작 시스템에서 데이터글러브+로봇손 조합으로 시도되어 온 가장 직관적인 매핑 방법입니다.\n\n\n\n대표 사례: 초기 연구 외에도 다양한 프로젝트에서 직접 관절 매핑을 구현했습니다. 예를 들어, DLR(독일항공우주센터)의 로봇 손을 데이터글러브로 제어한 실험이나, 한국 등에서 사람 손 장갑 데이터로 로봇 손 프로토타입의 각 관절을 움직인 연구들이 있습니다. 이러한 접근에서는 별다른 복잡한 계산 없이 센서 값→로봇 관절 명령으로 매핑하므로 구현이 용이합니다. 최근에도 연구자들이 신속한 프로토타이핑이나 간단한 그립 동작 실험을 할 때 직접 관절 매핑을 종종 사용하고 있습니다.\n\n\n\n장점: 직접 관절 매핑의 최대 장점은 단순성과 구현 용이성입니다. 특별한 연산이나 알고리즘 없이도 빠르게 적용할 수 있어, 실시간성이 중요한 경우나 복잡한 연산 여력이 없는 시스템에서 유용합니다. 또한 로봇 손이 인간 손과 형상이 유사한 경우, 사람 손의 자세(shape)를 로봇 손에 그대로 보존할 수 있어 조작자가 시각적 피드백을 통해 로봇 손 움직임을 쉽게 예측하고 학습할 수 있습니다. 즉 로봇 손이 사람 손동작을 거울처럼 따라 하므로, 사용자가 로봇 손의 오차를 눈으로 보고 자기 손 자세를 보정하는 등 직관적인 제어가 가능합니다.\n단점: 가장 큰 단점은 정밀한 위치/접촉 재현의 어려움입니다. 사람 손과 로봇 손의 길이 비율, 관절 축 차이 등으로 인해, 동일한 관절각도를 넣어도 손끝(지골 말단)의 실제 위치는 달라집니다. 따라서 사용자가 의도한 손가락 끝 위치나 힘 전달이 로봇에서 어긋날 수 있으며, 직관적이지 않은 동작이 나올 수 있습니다. 특히 로봇 손이 인간형이 아닐 경우(예: 두 손가락 집게 그리퍼 등) 어떤 인간 관절을 어떤 로봇 관절에 대응시킬지도 난감합니다. 필요에 따라 일부 인간 관절 정보를 버리거나 평균해야 하는데, 이는 정보 손실과 동작 왜곡을 야기합니다. 예를 들어, 인간 손의 새끼손가락과 약지 움직임을 로봇 손의 하나의 관절에 통합하는 식으로 단순화하면 섬세한 움직임 표현이 사라집니다. 요약하면 직접 관절 매핑은 빠르고 간단하지만 부정확하며, 로봇 손 구조와 크게 다를 경우 적용을 위해 많은 임시방편 조정이 필요합니다. 실제로 현재 이 방법은 높은 정확도가 필요 없고 빠른 구현이 우선인 경우에만 사용되는 경향이 있습니다."
  },
  {
    "objectID": "posts/paper/2025-08-09-retargeting-survey.html#직접-데카르트-매핑-direct-cartesian-mapping",
    "href": "posts/paper/2025-08-09-retargeting-survey.html#직접-데카르트-매핑-direct-cartesian-mapping",
    "title": "📃Retargeting Survey 리뷰",
    "section": "2.3 직접 데카르트 매핑 (Direct Cartesian Mapping)",
    "text": "2.3 직접 데카르트 매핑 (Direct Cartesian Mapping)\n개념: 직접 데카르트(Cartesian) 매핑은 인간 손 손끝의 위치와 방향(좌표)를 계산하여, 그대로 로봇 손 각 손가락의 목표 데카르트 좌표로 사용하는 접근입니다. 즉 관절 공간 대신 작업 공간(손가락 끝의 3차원 위치/orientation)을 1차적으로 맞추는 방식입니다. 구현을 위해 먼저 인간 손의 앞마디 손가락 끝 위치를 센서 데이터로부터 계산합니다. 데이터글러브라면 각 관절각도를 읽어 forward kinematics 계산으로 손끝 좌표를 얻고, 비전 카메라라면 손가락 끝의 3D 위치를 추적하는 식입니다. 이렇게 구한 사람 손끝 좌표를 로봇 손의 좌표계로 변환/대입하여 각 로봇 손가락의 목표 위치로 삼습니다. 마지막으로 그 위치를 로봇 손이 실제 구현하도록 역기구학 계산을 통해 로봇 관절각도로 변환합니다. 요약하면, 사람 손가락 끝의 현재 위치를 추적한 뒤 로봇 손가락 끝을 같은 위치로 오도록 움직이는 것입니다. 1990년에 최초의 직접 데카르트 매핑 연구가 보고되었으며, 이후 사람 손끝 위치 기반으로 로봇 손을 구동하는 다양한 시도가 이어졌습니다.\n\n\n\n대표 사례: 시각 기반으로 사람 손끝을 추적해 로봇 DLR-Hand II의 손끝 위치를 따라가게 한 연구가 대표적입니다. 이 경우 데이터글러브의 관절 정보 대신 비전 시스템으로 얻은 손가락 끝 좌표를 신경망에 학습시켜 매핑했는데, 손가락 끝 포지션 정확도에 중점을 두고 중간 관절 자세는 무시했습니다. 다른 연구들에서는 정확한 손끝 좌표를 얻기 위해 인간 손 모델을 사용하거나, 얻은 좌표를 로봇 손에 투영한 후 스케일 조정이나 최적화를 수행하기도 했습니다. 예를 들어, [74] 연구에서는 사람 손 공간과 로봇 손 공간 크기의 차이를 보정하기 위해 단순 비례 스케일 인자를 적용했고, [75]에서는 5손가락 로봇에 대해 3D 스케일 + 회전 변환을 함께 자동 최적화하여 다중 선형변환을 찾아내기도 했습니다. 또한 [76]에서는 손끝들 간 거리와 방향을 고려한 비용함수를 정의해, 사람 손끝 좌표를 로봇에서 비접촉 충돌 없이 최대한 보존하도록 비선형 최적화를 수행하였습니다.\n\n\n\n장점: 직접 데카르트 매핑은 로봇 손끝의 정확한 위치 제어가 중요한 경우 유리합니다. 사람 손가락 끝의 Cartesian 좌표를 그대로 따라가기 때문에, 정밀한 집게 잡기(precision grasp)나 손안 조작(in-hand manipulation) 등에서 원하는 지점에 손가락을 놓을 수 있습니다. 또한 사람 손과 로봇 손의 크기가 다를 경우, 앞서 언급한 스케일링이나 변환 최적화 기법을 적용해 공간 차이를 보정할 수 있습니다. 즉, 관절 매핑보다 손끝 기준의 정확도를 높게 가져갈 수 있으므로, 작은 물체를 집거나 정밀한 작업을 할 때 더 적합한 접근법입니다.\n단점: 단점으로는 우선 손 모양 보존의 실패가 있습니다. 이 방법은 오직 손끝 위치만 맞추므로, 사람 손의 전체적인 자세(제스처)나 손바닥/손가락의 모양은 로봇에서 달라져도 개의치 않습니다. 그러다보니 제스처 재현이나 강한 파지(grasp) 등에서는 사람 손의 자세를 흉내 내지 못해 어색하거나 기능적으로 떨어질 수 있습니다. 또한 사람과 로봇의 작업 공간 차이로 인해, 인간 손끝이 갈 수 있는 위치 중 일부는 로봇 손으로 닿을 수 없는 영역일 수 있습니다. 이 경우 매핑 결과 로봇 손이 불가능한 명령을 받게 될 수 있어, 해당 인간 동작이 재현되지 않거나 이상한 동작이 나오게 됩니다. 마지막으로, 만약 센서나 인간 손 모델의 한계로 로봇 손끝이 목표에 정확히 안 갔을 때, 사용자가 이를 보정하려고 자신의 손을 일그러뜨려 움직여야 할 수 있습니다. 예컨대 로봇 손가락 끝이 약간 모자라게 닿으면 사람은 더 꺾어서 눌러야 할 수도 있는데, 이는 사용자 입장에서 부자연스러운 조작입니다. 종합하면 직접 데카르트 매핑은 정밀 위치 제어에는 좋지만 자연스러운 형태 모방과 범용성은 떨어지며, 실사용 시에는 시스템별 보정과 예외 처리가 많이 필요합니다."
  },
  {
    "objectID": "posts/paper/2025-08-09-retargeting-survey.html#과업-지향-매핑-task-oriented-mapping",
    "href": "posts/paper/2025-08-09-retargeting-survey.html#과업-지향-매핑-task-oriented-mapping",
    "title": "📃Retargeting Survey 리뷰",
    "section": "2.4 과업 지향 매핑 (Task-Oriented Mapping)",
    "text": "2.4 과업 지향 매핑 (Task-Oriented Mapping)\n개념: 과업 지향 매핑은 인간 손동작을 과업(task)의 관점에서 추상화된 정보로 변환한 뒤, 이를 로봇 손에 적용하는 방법입니다. 즉 관절 공간이나 좌표 공간이 아닌, 해당 작업에 적합한 파라미터 공간을 정의하여 매핑을 수행합니다. 이 파라미터들은 인간 손과 로봇 손의 기구학 구조와 무관하게 작업 자체를 기술하므로, 양쪽 손의 차이를 직접 다루지 않고도 동작을 전달할 수 있습니다. 예를 들어 어떤 물체를 쥐는 작업이라면, 인간 손의 동작을 그 물체와의 관계로 표현하는 식입니다. Fig. 4에 개략도가 제시되어 있는데, 사람 손동작 정보(KM)를 과업 공간의 이미지로 캡슐화한 후, 이를 다시 로봇 손동작 명령(KC)으로 변환하는 이단계 구조를 갖습니다. 중요한 것은 여기서 정의되는 과업 공간 변수들이 손의 형상에 의존하지 않는다는 점입니다. 이를테면 물체를 쥘 때 물체의 크기와 위치 같은 것이 과업 변수에 해당합니다.\n\n\n\n대표 사례: 과업 지향 매핑은 주로 물체 파지(grasp) 작업에 한정되어 연구되어 왔습니다. 초기 사례 중 하나로, 1990년대 [81] 연구에서는 시연 학습을 위해 직육면체 물체의 각 면 위치를 비전 시스템으로 인식하고, 데이터글러브로 측정한 사람 손 관절각을 이 물체 중심 좌표계로 변환하여 로봇 간이 집게(hand gripper)에게 잡기 동작을 가르쳤습니다. 또 다른 선구적 연구인 [82]에서는 “가상 물체” 개념을 도입했습니다. 사람의 검지와 엄지 사이에 가상의 원형 물체가 끼어있다고 가정하고, 두 손가락의 상대 위치를 이 가상물체의 지름과 중심 위치로 표현했습니다. 그런 다음 이 가상 물체의 크기와 위치를 로봇 (평면 2지 그리퍼)에 맞게 비선형 스케일링하여, 로봇 손가락들의 목표 간격으로 설정하고 역기구학으로 관절각을 계산했습니다. 이 방법을 통해 사람의 집게 잡기 동작을 로봇 집게에 과업 중심적으로 모사한 것입니다. 이후 [83][86][87]에서는 이 개념을 3차원으로 확장하여 세 손가락 (엄지, 검지, 중지)로 세 점 지지하는 가상 원판을 정의, 세 손가락 로봇에 적용했습니다. 그 결과 단순 직접 매핑보다 로봇 손으로 재현 가능한 인간 손 자세의 범위가 늘어남을 보여주어, 과업 지향 표현의 유용성을 입증했습니다. 이후 [11][88]에서는 가상 물체를 구(球) 형태로 일반화하여, 손끝 위치에 따라 반지름과 중심이 동적으로 변하는 가상 구로 사람 손 파지 동작을 표현했습니다. 이 방법은 로봇 손의 구조와 무관하게 적용될 수 있음을 보여주어, 특정 로봇에 종속적이었던 이전 가상물체 방법의 한계를 넘었습니다. 나아가 [90]에서는 아예 가상 물체의 형태도 일반화하여, 사람 손가락 몇 개 점들의 변형을 나타내는 호모그래피 변환 자체를 과업 정보로 사용하고, 이를 로봇 손가락의 해당 점들에 동일 적용하는 방식을 보였습니다. 이처럼 과업 지향 매핑은 주로 물체 잡기 동작에 초점을 맞춰 발전해 왔으며, 그 외에 제스처나 타이핑 등에는 아직 적용이 미흡한 상태입니다. 다만 [84] 연구에서는 가상 물체 기반 매핑에 수동 제어(passivity) 기반 안정화 제어를 접목해 로봇 파지의 안정성도 보장함을 보였고, [85][89] 등에서는 양방향 원격조작(bilateral telemanipulation)에도 적용할 수 있음을 시사했습니다.\n\n\n\n장점: 과업 지향 매핑의 장점은 비인간형 로봇 손에도 사람의 기교를 전달할 수 있다는 점입니다. 예컨대 산업용 두손가락 그리퍼처럼 인간 손과 형태가 동떨어진 장비도, 물체와의 관계나 과업 요구조건을 통해 제어하면 사람 손으로 제어하기 쉽습니다. 이를 통해 로봇 손을 굳이 인간 손처럼 복잡하고 비싼 형태로 만들지 않고도 다양한 작업에 활용할 수 있어, 저비용·고신뢰성 시스템을 구현하는 데 유리합니다. 실제로 과업 지향 접근은 조립 작업 등에서 한정된 동작만 필요할 때 유용하며, 여러 종류의 로봇 손(2지, 3지, 5지 등)에 공통으로 적용할 수 있다는 이점이 있습니다. 요컨대 제어 복잡도를 줄이면서도 다양한 로봇 손 플랫폼에 사람이 손쉽게 명령을 내릴 수 있다는 것이 큰 매력입니다.\n단점: 반면, 단점은 조작자의 입장에서 직관성이 떨어질 수 있다는 점입니다. 과업 지향 매핑은 사람-로봇 손 간 기구학을 “추상화”하지만, 결국 그 특정 과업 시나리오에 한정된 추상화입니다. 그 과업 맥락을 벗어나면 매핑이 전혀 맞지 않을 수 있고, 심지어 약간만 다른 목적이어도 적용성이 급격히 낮아집니다. 무엇보다 로봇 손이 실제로 어떻게 움직일지에 대한 예측 가능성이 낮아, 사용자가 직접 눈으로 피드백을 확인하기 전에는 감을 잡기 어렵습니다. 사람은 자기 손의 감각으로 제어하는데, 과업 공간으로 변환되면서 이 연결이 끊어지기 때문입니다. 결과적으로 텔레오퍼레이터의 학습 곡선이 길어질 수 있고, 즉각적인 제어에는 부적합할 수 있습니다. 현재까지 과업 지향 매핑 성능 평가는 주로 오프라인 모션 재현 실험에 머물러 있으며, 실시간 원격 조작에서의 직관성 문제 등은 추가 연구가 필요한 상태입니다."
  },
  {
    "objectID": "posts/paper/2025-08-09-retargeting-survey.html#차원-축소-기반-매핑-dimensionality-reduction-based-mapping",
    "href": "posts/paper/2025-08-09-retargeting-survey.html#차원-축소-기반-매핑-dimensionality-reduction-based-mapping",
    "title": "📃Retargeting Survey 리뷰",
    "section": "2.5 차원 축소 기반 매핑 (Dimensionality Reduction-Based Mapping)",
    "text": "2.5 차원 축소 기반 매핑 (Dimensionality Reduction-Based Mapping)\n개념: 차원 축소 기반 매핑은 사람 손 동작의 고차원 데이터를 저차원 특징 공간으로 압축하여 매핑하는 접근입니다. 인간 손은 관절이 많아 제어 입력의 차원이 매우 높은데, 이 방법은 로봇 손의 입력 공간 변수들에 일정한 상관관계를 부여함으로써 사실상 제어 차원을 줄입니다. 쉽게 말해, 여러 관절 움직임을 묶어** 하나의 조합된 제어 변수로 표현하는 것입니다. 예를 들어 인간 손에서 자주 나타나는 움직임 패턴을 찾아내 그 주성분(Principal Components)을 새로운 축으로 삼고, 이 좌표로 사람 손 동작을 표현하면 차원이 크게 줄어듭니다. 그런 다음 이 저차원 좌표를 로봇 손의 동작 공간에 대응시켜 다시 고차원 관절 명령으로 복원합니다. 이러한 인코딩(차원축소) - 디코딩(복원) 과정을 통해, 본질적인 움직임만 추려내어 매핑 효율을 높입니다.\n\n\n\n대표 사례: 차원 축소 매핑의 대표적 아이디어는 이탈리아 Pisa대 연구진이 제안한 손 포스트럴 시너지(postural synergy) 개념입니다. Santello 등은 1998년 사람의 다양한 그립 자세를 분석하여, 실제 사람 손이 약 2~3개의 주성분(시너지)만으로 대부분의 자세 변화를 설명할 수 있음을 보였습니다. 이러한 인간 손 시너지를 로봇 손 제어에 이용하면, 몇 개의 변수를 통해 다관절 로봇 손을 제어할 수 있습니다. 이후 많은 연구에서 PCA(주성분 분석)나 KPCA(커널 PCA) 등으로 인간 손동작 데이터를 학습하여 저차원 시너지 공간을 추출하고, 이를 로봇 손의 관절 움직임에 매핑했습니다. 예를 들어 [97][98] 연구에서는 사람 손의 그립 데이터를 PCA로 분석해 연속적인 로봇 손 구성 공간의 서브스페이스를 도출하고, 이를 통해 다양한 형태의 로봇 손(인간형/비인간형 모두)에서 자동 그립 동작 계획을 성공적으로 수행했습니다. 또한 [100]에서는 완전 구동 로봇 손에 인간 손 시너지를 적용해 그립 제어를 개선했고, [102]에서는 그 방법을 언더액추에이트(구동부 적은) 로봇 손에도 확장했습니다. 한편, 인간의 일반적인 시너지 대신 특정 과업에 특화된 저차원 방향을 찾는 연구도 있었습니다. [10][111]에서는 주요 모션 방향(Principal Motion Directions)이라는 개념을 도입해, 작업별로 의미있는 저차원 축을 로봇 손 공간에서 직접 산출했습니다. 이를 통해 충돌 회피 등 조건을 포함한 그립 동작 계획을 효과적으로 수행하고, 일반 시너지와 성능이 비슷하면서도 특정 작업에 최적화된 제어가 가능함을 보였습니다. 전반적으로 차원 축소 기반 매핑은 사람의 풍부한 손 움직임 데이터를 학습해 복잡한 로봇 손 제어를 단순화하는 방향으로 발전해 왔습니다.\n\n\n\n장점: 차원 축소 접근의 가장 큰 장점은 제어 입력의 복잡도를 줄여준다는 것입니다. 높은 자유도를 가진 로봇 손을 사람 손으로 직접 제어하려면 입력 공간이 방대하지만, 시너지 등의 기법으로 저차원 입력 공간을 만들면 사람도 훨씬 수월하게 로봇 손을 조종할 수 있습니다. 특히 로봇 손이 인간 손과 형상이 많이 달라도, 본질적인 움직임 패턴만 추려내 전달하면 차이를 상쇄할 수 있습니다. 이러한 이유로 차원 축소 매핑은 복잡한 로봇 손의 실시간 원격 제어나, 사람의 기교를 로봇에 학습시키는 기술 전수(예: 자율 그립 동작 학습) 등에 매우 유용합니다. 실제로 시너지 기반 매핑은 로봇의 자율 파지 동작에 사람의 손기술을 이식하는 데 성공적이어서, 로봇이 보다 인간에 가까운 자율 조작 능력을 갖추는 데 기여했습니다. 또한 입력 차원이 줄어드니 계산량과 센서 요구사항도 완화되어, 실시간성이나 견고성 측면에서도 유리합니다.\n단점: 반면, 차원 축소 매핑은 가능 동작의 범위를 인위적으로 제한한다는 단점이 있습니다. 저차원 공간으로 투영하는 과정에서, 로봇 손이 수행할 수 있는 동작은 정의된 서브스페이스 상의 조합으로 한정됩니다. 이는 곧 로봇 손이 보다 다양한 움직임을 하거나 특이한 동작을 재현하는 데 제약이 걸린다는 뜻입니다. 특히 사람 조작자가 원하는 동작이 시너지 공간에 충분히 표현되지 않으면, 로봇 손으로 완벽히 구현하기 어렵습니다. 또한 이러한 매핑을 사용하려면 사용자가 새로운 제어 방식에 익숙해져야 할 수 있습니다. 즉 관절 하나하나 움직이는 대신 시너지 축을 움직이는 감각을 익혀야 하므로, 학습 노력이 필요하고 직관성이 떨어질 수 있습니다. 현재 이와 같은 사용자의 학습 속도나 인지 부하에 관한 연구가 부족하며, 보다 범용적이고 사용자 친화적인 시너지 정의에 대한 과제가 남아 있습니다. 요약하면 차원 축소 기반 매핑은 효율성은 높지만 자유도와 직관성의 희생이 따르며, 이를 개선하기 위한 추가 연구가 요구됩니다."
  },
  {
    "objectID": "posts/paper/2025-08-09-retargeting-survey.html#자세-인식-기반-매핑-hand-pose-recognition-based-mapping",
    "href": "posts/paper/2025-08-09-retargeting-survey.html#자세-인식-기반-매핑-hand-pose-recognition-based-mapping",
    "title": "📃Retargeting Survey 리뷰",
    "section": "2.6 자세 인식 기반 매핑 (Hand Pose Recognition-Based Mapping)",
    "text": "2.6 자세 인식 기반 매핑 (Hand Pose Recognition-Based Mapping)\n개념: 자세 인식 기반 매핑은 사람 손의 현재 자세(또는 제스처)를 분류(classification)하여, 미리 정해둔 로봇 손의 불연속 동작 셋 중 하나를 발동(trigger)하는 방식입니다. 다시 말해, 연속적인 관절 제어를 하지 않고 사람 손의 자세를 이산적인 기호로 해석하여, 그에 대응하는 로봇 손의 동작을 실행합니다. 이를 위해 우선 사람 손의 센서 데이터를 처리해 특징(feature)을 추출하고, 이를 통해 정의된 몇 가지 손 자세 중 현재 사람이 어떤 자세를 취하고 있는지 인식합니다. 예를 들어 “주먹 쥐기”, “집게 손가락 펴기”, “손바닥 펴기” 등의 제스처를 구분하는 식입니다. 인식에는 주로 머신러닝 기법(딥러닝, SVM, kNN, HMM 등)이 활용되며, 센서 장치 종류와 무관하게 특징만 잘 추출하면 적용 가능합니다. 이렇게 사람 손 자세가 결정되면, 미리 준비된 로봇 손의 동작(관절 사전 설정 또는 모션 시퀀스) 중 대응하는 것을 실행합니다. 예컨대 사람 손이 “pinch gesture”로 분류되면 로봇 손은 pinch 모양으로 모이도록 사전 정의된 움직임을 수행하는 식입니다.\n\n\n\n대표 사례: 이 접근은 프로그래밍-바이-데몬스트레이션(시연 학습) 분야에서 많이 연구되었습니다. 예를 들어, [1] 연구에서는 계층형 신경망으로 사람 손 데이터를 분석해 파워 그립 vs 정밀 그립 두 가지 유형을 분류하고, 이에 따라 로봇 손의 동작을 프로그래밍했습니다. [121] 연구에서는 SVM을 활용해 사람 손 자세를 여러 파지 형태로 분류하고, 이를 조립 작업용 로봇 손 시퀀스 프로그래밍에 이용했습니다. [122]에서는 비전 카메라와 데이터글러브를 결합한 센서로 손 자세를 의사결정트리로 분류하고, 거기에 맞춰 로봇 손의 힘 제어 그립 동작을 실행했습니다. 또한 k-최근접이웃(k-NN) 알고리즘을 써서 사람 손 자세를 실시간 분류하고 대응하는 로봇 손의 파지 패턴을 발동하는 연구도 보고되었습니다. 한편 텔레오퍼레이션에도 자세 인식 매핑이 활용되는데, [106]에서는 얕은 신경망을 사용해 온라인 제스처 인식을 구현하여, 사람이 제스처를 취하면 그에 대응하는 로봇 손 그립이 즉각 동작하도록 했습니다. [8]에서는 은닉 마르코프 모델(HMM)로 연속된 손 제스처 시퀀스를 인식하여, 로봇 손의 복합 동작을 일련의 상태머신으로 실행했습니다. 최근에는 딥러닝을 이용해 카메라 영상만으로 손 제스처를 인식하는 연구도 활발합니다. 예를 들어 [104][125]에서는 합성곱 신경망으로 카메라 영상에서 손 모양을 파악해 로봇 동작을 유도했습니다. 그밖에 GMM(가우시안 혼합모델)이나 베이지안 네트워크로 제스처를 분류하는 시도도 있었고, [108]에서는 아예 사람 손의 현재 잡고 있는 물체 종류를 인식해 이에 맞는 로봇 손 그립을 선택하는 흥미로운 접근도 제안되었습니다.\n\n\n\n장점: 자세 인식 매핑의 장점은 필요한 동작 집합이 명확한 응용에서 매우 효과적이라는 점입니다. 수행할 로봇 손 동작들이 몇 가지로 한정되고 사전에 정해져 있는 경우(예: 특정 그립 동작 몇 개, 특정 제스처 몇 개 등), 사람 손의 동작도 그 유한한 세트 중 하나로 간주하여 명확하게 구분할 수 있습니다. 이렇게 하면 사람 손을 마치 원격 컨트롤러처럼 사용할 수 있어서, 사용자가 몇 가지 제스처만 학습하면 어렵지 않게 로봇을 제어할 수 있습니다. 또 새로운 기능이 필요하면 해당 제스처-동작 쌍만 추가하면 되므로, 확장성이 뛰어나다는 이점도 있습니다. 예컨대 처음에는 다섯 가지 제스처로 다섯 가지 로봇 동작을 하다가, 이후 여섯번째 동작이 필요하면 새로운 제스처 하나를 학습시켜 추가하면 됩니다. 이러한 모듈식 추가가 가능하여 시스템을 점진적으로 발전시키기 용이합니다.\n단점: 가장 큰 단점은 연속적이고 미세한 제어가 어렵다는 점입니다. 자세 인식 매핑에서는 사람이 로봇 손의 움직임을 연속적으로 따라가게 할 수 없습니다. 오직 이산적인 동작 명령만 내릴 수 있으므로, 중간 상태나 미세 조절이 필요한 작업에는 부적합합니다. 또한 응용의 복잡도가 높아질수록 필요한 제스처 개수가 늘어나는데, 사람은 너무 많은 제스처를 기억하고 정확히 구분하기 어려워집니다. 제스처 간 오인식 가능성도 올라가 시스템 신뢰성을 저해할 수 있습니다. 마지막으로, 연속 제어가 안 되다보니 사람 입장에서는 로봇을 자기 손처럼 자연스럽게 다룬다는 느낌이 적고, 원격 조작의 자연스러움이 떨어질 수 있습니다. 실제로 이 방식은 가상현실에서 아바타 손동작 제어나 로봇과의 비언어적 의사소통 등 정밀한 그립이 필요없는 분야에는 널리 쓰였지만, 고도의 섬세함이 필요한 작업에서는 사용이 제한적입니다. 요컨대 자세 인식 기반 매핑은 명령 세트가 한정된 상황에서만 자연스럽게 작동하며, 그 한계를 넘어서면 자연스러운 조작감의 희생이 불가피합니다."
  },
  {
    "objectID": "posts/paper/2025-08-09-retargeting-survey.html#하이브리드-매핑-hybrid-mapping",
    "href": "posts/paper/2025-08-09-retargeting-survey.html#하이브리드-매핑-hybrid-mapping",
    "title": "📃Retargeting Survey 리뷰",
    "section": "2.7 하이브리드 매핑 (Hybrid Mapping)",
    "text": "2.7 하이브리드 매핑 (Hybrid Mapping)\n개념: 하이브리드 매핑은 말 그대로 앞서 소개한 여러 매핑 방법을 필요에 따라 결합한 접근을 통칭합니다. 단일 카테고리에 딱 떨어지지 않는 특수 목적의 솔루션이나, 여러 기법을 연계하여 새로운 매핑을 구성한 경우 모두 포함됩니다. 크게는 (1) 특정 문제를 풀기 위해 고안된 맞춤형 기법, (2) 기존 매핑 방법들을 약간의 변형과 함께 병합한 구현, (3) 여러 방식의 명시적 하이브리드 결합으로, 상황에 따라 전환 로직을 정의한 것 등을 예로 들 수 있습니다. 하이브리드 매핑은 그 이질성 때문에 일괄된 개념도식으로 묘사하긴 어렵지만, 요는 각 방법의 장점을 취합하고 단점을 보완하기 위해 모듈식으로 구성한다는 철학입니다.\n대표 사례: 매우 다양한 변종이 존재하지만, 몇 가지 흥미로운 예를 소개합니다. [126] 연구에서는 우주 로봇 원격 조작을 위해 음성 명령 인식과 연속 글러브 제어를 결합한 하이브리드 방식을 선보였습니다. 오퍼레이터가 음성으로 “기본 동작”을 지시하면 로봇이 해당 작업 모드로 전환되고, 이후 데이터글러브로 세밀한 손동작을 보내 그 작업을 수행하게 했습니다. 예를 들어 “잡기 모드” 음성 명령 후 장갑 동작으로 손을 오므리면 로봇 손이 물체를 쥐는 식입니다. 또 다른 예로 [130]에서는 자세 인식 매핑으로 큰 틀의 파지 동작을 선택하고, 로봇이 자동 미세조정을 수행하는 공동 제어(shared control)를 도입했습니다. 사람이 대략적인 그립 모양 제스처를 하면 로봇이 센서를 통해 물체를 감지하고 미끄러짐 없도록 그립을 보정해주는 방식으로, 인간과 자동제어의 장점을 결합했습니다. 유사하게 [131]에서는 사람의 제스처 인식으로 로봇 손의 사전 형상(preshape)을 결정하고, 촉각 센서로 물체 접촉을 감지하면 자동으로 완전 파지를 수행하도록 했습니다. 한편 [128]에서는 특이하게 “hidden robot” 개념을 도입했는데, 사용자에게 로봇 손과 환경을 가상화된 형태로 제시한 후, 사용자가 실제로는 가상의 손을 조작하도록 하여 그 결과를 실제 로봇 손에 매핑하는 방식을 취했습니다. 이를 통해 사람-로봇의 차이를 한 번 더 추상계층으로 분리하는 독특한 접근을 시도했습니다. 이외에도 여러 연구에서 직접 관절 + 직접 데카르트 혼합을 이용했습니다. 예를 들어 [136]에서는 데이터글러브로 사람 손이 정밀 집기 동작을 하는지를 분류하여, 평소에는 직접 관절 매핑을 하다가 정밀 동작 시에는 직접 데카르트 매핑으로 전환하도록 했습니다. [137]에서는 엄지와 다른 손가락 사이 거리를 이용해, 그 거리가 좁아질수록 점진적으로 관절 매핑에서 데카르트 매핑으로 시그모이드 곡선 형태로 전환하는 연속 혼합을 구현했습니다. 요컨대 하이브리드 매핑은 문제와 상황에 맞게 여러 접근법을 유연하게 엮는 프레임워크라 할 수 있습니다.\n\n\n\n장점: 여러 방법을 조합한 하이브리드 접근은 현재 개별 방법들이 갖는 한계를 돌파하는 유망한 방향입니다. 각 방법이 잘 다루는 부분만 발췌하여 하나의 구조에 녹이면, 한 가지 방법으로는 풀기 어려운 복잡한 문제도 해결할 수 있습니다. 예컨대 위 사례들처럼 제스처 인식의 편의성과 직접 매핑의 연속성, 자동 제어의 정밀성을 합치면, 사람과 로봇의 협업 제어를 극대화할 수 있습니다. 이러한 모듈화와 병합을 통해 시스템을 설계하면, 새로운 모드나 기능도 비교적 추가하기 용이하고 확장성 있는 프레임워크를 구축할 수 있습니다. 실제로 조립이나 공구 사용 등 난이도 높은 작업에 적용된 연구들을 보면, 단일 매핑보다는 하이브리드 구조에서 더 나은 성과를 보이는 경향이 있습니다.\n단점: 그러나 알고리즘 설계의 복잡도가 크게 증가한다는 점은 피할 수 없습니다. 서로 다른 방법을 언제, 어떻게 전환하고 결합할지 정하는 논리 및 타이밍 설계가 어려우며, 자칫하면 불안정하거나 예측 불가능한 거동을 초래할 위험도 있습니다. 예를 들어 관절↔︎데카르트 모드 전환 시 경계 영역에서 로봇 손이 갑자기 튀는 현상을 막기 위해 이행 절차(smooth transition)를 잘 설계해야 합니다. 또한 여러 기법을 섞으면 각 구성요소의 파라미터 튜닝, 상호작용 효과 등을 모두 고려해야 하므로 개발/테스트 비용이 늘어납니다. 결국 하이브리드 매핑은 잠재력은 크지만 설계 난이도가 높은 접근이며, 이를 체계적으로 설계/최적화하는 방법론에 대한 추가 연구 필요성이 제기됩니다."
  },
  {
    "objectID": "posts/paper/2025-08-09-retargeting-survey.html#실제-응용-분야에서의-활용-가능성",
    "href": "posts/paper/2025-08-09-retargeting-survey.html#실제-응용-분야에서의-활용-가능성",
    "title": "📃Retargeting Survey 리뷰",
    "section": "2.8 실제 응용 분야에서의 활용 가능성",
    "text": "2.8 실제 응용 분야에서의 활용 가능성\n앞서 살펴본 매핑 기법들은 용도와 상황에 따라 장단점이 다르므로, 응용 분야에 맞는 선택과 변형이 중요합니다. 여기서는 수술 로봇, 일반 원격 조작, 재활/보조, 가상현실(VR) 등을 중심으로 각 접근법의 활용성을 평가하겠습니다.\n\n수술용 로봇 원격 조작: 의료 로봇, 특히 수술용 로봇에서는 높은 신뢰성과 정확성이 최우선입니다. 따라서 가능한 한 단순하고 예측 가능한 매핑이 요구됩니다. 실제로 논문에서 언급하듯이, 수술 로봇의 사례에서는 2지 그리퍼 같은 최소 복잡도 로봇 손을 손가락 힘 피드백 장치(엑소스켈레톤)로 제어하는 접근이 활용되었습니다. 이는 사실상 직접 관절 매핑에 가까운 형태로, 기계 구조를 단순화하여 매핑 오차 여지를 줄인 것입니다. 이러한 시스템은 세밀한 손놀림보다는 안정적인 집게 동작에 초점을 맞추기 때문에, 사람 손의 미세한 움직임을 모두 재현하지 않고 주요 동작만 1:1 대응시켜도 무방합니다. 반면, 수술 도구의 끝부분을 매우 정확히 위치시키는 것이 중요하므로, 직접 데카르트 매핑 개념이 도입되어 사람 손끝 움직임이 수술 도구 팁의 움직임으로 변환되기도 합니다. 다빈치 수술로봇 등의 상용 시스템도 마스터 기구의 움직임을 스케일 조정하여 슬레이브 로봇 팔/도구에 전달하는데, 이것이 일종의 데카르트 매핑입니다. 결론적으로, 수술용 원격 조작에서는 안전성과 예측가능성을 위해 간단한 매핑(직접 관절 or 데카르트)을 기본으로, 필요 시 스케일 조정이나 하이브리드 기법으로 보완하는 방향이 적합합니다.\n산업 현장 등의 일반 원격 조작: 우주, 해저, 폭발물 처리 등 사람 대신 로봇이 위험하거나 먼 현장에서 작업하는 경우, 인간 조작자의 즉각적인 직관적 제어가 중요합니다. 이때는 직접 매핑의 강점이 부각됩니다. 로봇 손이 인간형일수록 조작자는 자기 손 쓰듯 자연스럽게 제어할 수 있으므로, 가능하면 직접 관절 매핑으로 실시간 거울 동작을 구현하고자 합니다. NASA의 로보넛(RoboNut) 등이 데이터글러브로 조종되는 사례가 그 예입니다. 반면 로봇 손이 단순 그리퍼나 인간형이 아닐 때는, 과업 지향 매핑이 유용할 수 있습니다. 예를 들어 원격 조작 로봇이 공구를 잡고 조이는 작업을 해야 한다면, 사람 손의 동작을 “공구 돌리기”라는 과업 변수로 변환해 로봇에 전달하면 기구학 차이를 극복할 수 있습니다. 또한 산업 현장은 작업 종류가 정형화되어 있는 경우가 많아, 자세 인식 매핑으로 미리 정의된 몇 가지 동작(잡기, 놓기, 가리키기 등)을 트리거하는 방식도 실용적입니다. 특히 반복적인 조립 작업 등에서는 사람이 수십 분간 똑같은 동작을 지속해야 하므로, 연속 제어보다 버튼식 제어가 피로도를 낮출 수 있습니다. 따라서 원격 조작 일반 분야에서는 직접 매핑을 기본으로 하되, 로봇 형태나 작업 특성에 맞춰 과업 지향이나 제스처 인식을 혼용하는 방향이 권장됩니다. 실제 연구에서도 조립 작업에 과업 지향 매핑을 적용하거나, 공구 사용에 하이브리드 매핑을 도입하여 성능을 높인 예가 있습니다.\n재활 및 보조 로봇: 재활 분야에서는 착용형 기기나 의수(의족) 등의 제어에 손 매핑이 활용됩니다. 예를 들어, 편마비 환자의 남은 건강한 손 동작을 센싱하여 마비된 쪽 로봇 손에 매핑함으로써 재활훈련을 돕는 경우가 있습니다. 이런 경우 양 손이 거의 대칭이라서 직접 관절 매핑으로도 상당한 효과를 볼 수 있습니다. 그러나 더 발전된 사례로, 로봇 의수 제어에는 근전도(EMG) 신호 등을 통해 손 자세를 분류하고 그립 패턴을 구동하는 방안이 상용화되어 있습니다. 이는 앞서 설명한 자세 인식 기반 매핑과 유사한 원리입니다. 사용자에게 몇 가지 제스처(근육 신호 패턴)를 학습시켜서 집기, 가리키기 등의 이산 동작을 실행하게 하는 것이지요. 한편 재활 훈련용 장비에서는 차원 축소 매핑도 응용될 수 있습니다. 예를 들어 환자의 남은 운동범위를 PCA로 분석해 주요 운동 성분만 과장되게 로봇이 따라하게 함으로써, 잔존 운동능력을 극대화하는 훈련을 설계할 수 있습니다. 또한 힘보조 장치(엑소글러브 등)에서는 사람의 간단한 손 움직임(예: 쥐려는 의도)을 시너지 공간에서 포착해, 로봇 장치가 부족한 힘을 보충하는 식의 구현이 가능합니다. 정리하면, 재활 및 보조 분야에서는 대상자의 상태와 기기의 형태에 따라 직접 매핑으로 거의 1:1 운동재현을 하기도 하고, 시너지/제스처 기반으로 보조적 제어를 하기도 합니다. 최근 연구에서는 로봇 의수의 복잡한 다자유도 제어를 위해 차원 축소 + 자세 인식 혼합 전략을 쓰기도 하며, 이는 사용자의 근신호를 몇 개 시너지로 나눠 읽고, 추가 제스처 신호로 모드 전환을 하는 형태입니다.\nVR/AR 및 원격 협업: 가상현실에서 사람의 손동작은 주로 가상 아바타 손에 매핑되는데, 여기서는 실시간성과 자유로운 제스처 표현이 중요합니다. 상용 VR 시스템은 보통 장갑이나 컨트롤러로 손가락 움직임을 측정하고 가상손에 즉시 반영하는데, 이건 일종의 직접 관절 매핑입니다. 다만 VR 아바타 손은 실제 물리를 따지지 않아도 되므로, 굳이 로봇 공학적 제약을 고려할 필요 없이 사람 손 움직임을 1:1 복제하면 됩니다. 한편, VR/AR에서 제스처 인식을 이용해 명령 인터페이스로 쓰는 경우도 많습니다(예: 손가락 총 모양→메뉴 열기). 이처럼 비언어적 의사소통이나 UI 제어에서는 자세 인식 매핑이 효과적입니다. 미리 정의된 제스처들이 사용자와 시스템 간 신호로 쓰이는 것입니다. 또한 VR은 훈련 시뮬레이션의 역할도 하기 때문에, 로봇 원격 제어를 VR로 가상 연습할 때 매핑 알고리즘 테스트도 함께 진행할 수 있습니다. 가령 수술 로봇 매핑을 VR로 시뮬레이션하고 피드백을 조정하는 식입니다. 협업 로봇의 경우, AR 환경에서 인간 작업자의 손짓을 인식해 로봇 팔/손에게 지시를 내리는 연구도 있는데, 이때는 과업 지향과 제스처 인식 개념이 모두 활용되어 사람-로봇 팀워크를 원활히 합니다. 요컨대 VR/AR에서는 실시간 거울 매핑으로 몰입감을 주면서도, 특정 동작은 제스처 명령으로 추상화하는 혼합 운용이 일반적입니다."
  },
  {
    "objectID": "posts/paper/2025-08-09-retargeting-survey.html#저자들의-미래-연구-방향-제언과-평가",
    "href": "posts/paper/2025-08-09-retargeting-survey.html#저자들의-미래-연구-방향-제언과-평가",
    "title": "📃Retargeting Survey 리뷰",
    "section": "2.9 저자들의 미래 연구 방향 제언과 평가",
    "text": "2.9 저자들의 미래 연구 방향 제언과 평가\n논문의 마지막에서는 인간-로봇 손 매핑 연구의 향후 발전을 위해 5가지 주요 방향이 제시됩니다. 이를 간략히 요약하고, 각 항목에 대한 평가를 덧붙입니다.\n\n매핑 목표의 명확화와 표준 평가척도 정립: 현행 연구들은 제각기 다른 목표(예: 손끝 위치 정확도, 제스처 유사도, 작업 성공률 등)를 내세우며, 성능 평가도 통일되어 있지 않습니다. 저자들은 매핑 알고리즘의 목표들을 체계적으로 정의하고, 다양한 접근법이 공통의 기준에서 성능을 비교할 수 있도록 해야 한다고 지적합니다. 예를 들어 “로봇 손가락 끝 위치 오류 5mm 이하” 같은 객관지표나, “사용자 학습시간 10분 이내” 같은 사용자 경험 척도를 도입할 필요가 있습니다. 지금까지는 많은 연구가 정성적 평가(예: “사람과 비슷한 모양이다”)에 머물러, 방법 간 우열이나 trade-off를 정확히 파악하기 어려웠습니다. 표준화된 벤치마크와 메트릭 개발은 향후 학계의 공동 과제로 보입니다. 이는 연구자들 간 공동 언어를 형성하여, 서로 다른 접근들이 같은 목표 지향 하에 발전하도록 유도할 것으로 기대됩니다.\n자연스러움(Naturalness)과 직관성(Intuitiveness)의 고려: 저자들은 사용자 입장에서 매핑의 자연스러움과 직관적 학습용이성을 앞으로 핵심 지표로 삼아야 한다고 강조합니다. 현재 대부분의 연구가 기계적 정확성에 초점을 맞추고, 정작 사람이 얼마나 편하게 조작하는지는 간과하고 있습니다. 자연스러움이란 “사용자가 마치 자기 손을 움직이듯 로봇 손을 움직일 수 있는 정도”를 뜻하고, 직관성은 “매핑 기능을 사용자가 얼마나 빨리 터득할 수 있는가”로 정의됩니다. 이상적인 매핑은 사용자가 별 교육 없이도 바로 자기 손처럼 다룰 수 있는 것이겠지요. 이를 정량화하고 향상시키는 연구가 거의 전무한데, 저자들은 이 방향의 연구가 새로운 통찰을 줄 것으로 기대합니다. 필자 역시 전적으로 동의하는 부분으로, 사용자 경험(UX) 요소를 고려한 매핑 알고리즘 설계는 결국 실용화의 핵심 요건이 될 것입니다. 향후 실험을 통해 “어떤 매핑이 사용자에게 더 자연스러운가”를 비교하고, UI/피드백과 알고리즘을 통합적으로 개선하는 노력이 필요합니다.\n손 기구학 구조의 사전정보 활용: 연구자들은 지금까지 인간 손과 로봇 손의 기구학적 특성을 충분히 활용하지 않았다고 지적합니다. 사실 하나의 손이 주어진다면, 관절 가동 범위, 작업공간 형태, 손가락간 상대 위치 등 유용한 정보들이 사전에 모두 파악 가능합니다. 이러한 선험적 지식을 매핑 알고리즘 설계에 접목하면, 불필요한 탐색이나 시행착오를 줄이고 더 의미있는 매핑 변환을 도출할 수 있다는 것입니다. 예를 들어, “로봇 엄지와 검지가 만날 수 있는 공간 영역”을 미리 안다면, 인간 손 동작 중 그 영역 밖의 것은 애초에 제외하거나 보정할 수 있습니다. 또 로봇 손의 속도 타원체(velocity ellipsoid)나 연접 구조 등을 이용해, 사람 손의 유사한 속성들과 맞춰보는 접근도 생각해볼 수 있습니다. 이런 기하학적·토폴로지적 정보는 공짜로 주어지는 혜택이니, 이를 최대한 활용하는 방향으로 연구가 진전되어야 한다는 주장입니다. 필자는 이에 대해, 현재 일부 연구에서 자기 운동공간 최적화 등을 하는 사례가 있으나 보다 체계적이지는 않다고 봅니다. 앞으로 수학적 모델링과 최적화 이론을 접목해, 선험 지식을 활용한 매핑 함수 설계 프레임워크를 정립하면 큰 도움이 될 것입니다.\n매핑 정보의 피드백 루프 도입: 저자들은 흥미로운 제안으로 사용자에게 매핑 상태 정보를 실시간 피드백하는 방안을 들었습니다. 현재는 사람이 손을 움직이면 로봇 손이 따라가는 개방형 루프 구조인데, 여기에 피드백 채널을 추가하여 사람과 알고리즘이 상호 적응(co-adaptation)하도록 만들자는 것입니다. 예컨대 시스템이 “지금 당신 손 자세는 로봇 손으로 완전히 재현 불가능한 영역에 있습니다”라는 피드백을 주면, 사용자가 그것을 인지하고 자기 손을 덜 벌리거나 하는 식으로 능동 조절이 가능할 수 있습니다. 이를 구현하려면 청각 경고음, 촉각 진동, 시각적 표시 등 다양한 인터페이스가 활용될 수 있습니다. 이러한 양방향 인터페이스가 도입되면, 매핑 품질은 단순 알고리즘에만 좌우되는 것이 아니라 사용자-알고리즘 협업으로 결정되며, 더 안전하고 정확한 제어가 가능해질 것으로 예상됩니다. 필자는 이 제안을 매우 혁신적인 방향이라 평가합니다. 다만 사용자에게 얼마나, 어떤 방식으로 피드백을 주는 것이 가장 효과적인지에 대한 인지공학적 연구가 병행되어야 할 것입니다. 자칫 정보 과부하로 사용자 혼란을 야기할 수 있으므로, 직관적이고 최소한의 피드백 디자인이 핵심 과제가 될 것입니다.\n점진적 학습 가능한 매핑 알고리즘: 마지막으로, 매핑 알고리즘을 모든 상황을 한 번에 포괄하려 하지 말고, 필요에 따라 점진적으로 업그레이드할 수 있게 만들자는 제안입니다. 이는 증강 학습(incremental learning)** 패러다임을 차용한 것으로, 사용자가 새로운 동작이나 기능이 필요할 때마다 데이터를 추가로 제공하면 알고리즘이 이를 학습하여 기능을 확장하는 형태입니다. 예를 들어 처음에는 기본 파지 동작들만 되던 로봇 손 매핑에, 사용자가 “집게로 가리키기” 동작 데이터를 추가로 학습시켜 그 기능을 덧붙이는 식입니다. 또 이후 새로운 로봇 손으로 바뀐다면, 그에 맞게 일부 매핑을 재학습/미세조정하여 적응할 수도 있을 것입니다. 이러한 유연한 알고리즘은 하나의 단일한 완벽 솔루션을 만들기 어려운 현실에서, 사용자 맞춤형 최적화와 플랫폼 독립성을 확보하는 데 큰 도움이 될 것입니다. 필자는 특히 이 접근이 학습 기반 매핑(예: 신경망 활용) 연구와 궁합이 좋다고 봅니다. 요즘 머신러닝 기술이 발달함에 따라, 초기에 대략적인 모델을 만들어놓고 사용자 데이터를 점진적으로 더해 퍼스널라이징하는 것이 용이해졌습니다. 다만 이 경우 시스템이 업데이트되면서 기존 성능을 유지보장하는 문제, 그리고 학습 데이터 관리 등의 이슈가 따르므로, 신중한 설계와 장기적 평가가 필요할 것입니다.\n\n전체적으로 저자들의 미래 방향 제시는 매우 타당하고 시의적절해 보입니다. 특히 1)과 2)은 현재 연구 커뮤니티의 공감대가 큰 부분이고, 3)~5)은 비교적 새로운 아이디어로 향후 연구의 지평을 넓혀줄 것으로 기대됩니다. 이들 제안은 각각 평가 기준 정립, 인간 요소 통합, 이론 기반 설계, 양방향 인터랙션, 적응형 알고리즘으로 요약되는데, 이는 결국 인간-로봇 손 매핑 분야를 보다 성숙하고 실용적인 단계로 끌어올리기 위한 필수 과제들로 보입니다."
  },
  {
    "objectID": "posts/paper/2025-08-09-retargeting-survey.html#추가적인-시사점-및-제안",
    "href": "posts/paper/2025-08-09-retargeting-survey.html#추가적인-시사점-및-제안",
    "title": "📃Retargeting Survey 리뷰",
    "section": "2.10 추가적인 시사점 및 제안",
    "text": "2.10 추가적인 시사점 및 제안\n논문에 기반하여, 필자 역시 몇 가지 독자적인 시사점을 제안해 보고자 합니다.\n\n다중 모달 입력의 통합: 현재 매핑 연구는 주로 손의 동작 데이터에 초점을 맞추지만, 향후에는 다른 인간 의도 신호(예: 시선, 음성, 표정, 발 움직임 등)와 결합한 멀티모달 매핑이 유망합니다. 예컨대 손동작+시선 방향을 함께 활용하면 “어디를 어떻게 잡을지”에 대한 풍부한 정보를 얻어 매핑 정확도를 높일 수 있습니다. 이미 [126] 사례에서 음성 명령을 병용한 바 있듯, 이러한 확장을 체계화하면 보다 직관적인 인터페이스를 구축할 수 있을 것입니다.\n환경 피드백과 상호작용: 매핑 알고리즘과 환경 간 상호작용도 고려해야 합니다. 예를 들어 로봇 손이 물체를 잡았을 때 미끄러지면, 이를 사람 손에 촉각 또는 힘 피드백으로 알려주고 사람이 그립 강도를 높이도록 유도하는 식의 폐쇄루프 제어가 필요합니다. 단순한 기하학적 매핑을 넘어, 물리적 상호작용 정보(마찰, 접촉력 등)까지 포함한 매핑으로 발전해야 정교한 조작이 가능해집니다. 이를 위해 로봇 손의 센서 데이터(힘/촉각)를 인간에게 제공하고, 인간의 미세 조정 능력을 활용하는 쪽으로 시스템 설계를 고민해볼 수 있습니다.\n로봇 손 설계와 매핑의 동시 최적화: 지금까지는 주어진 로봇 손에 매핑을 맞추는 식이었지만, 이상적으로는 매핑을 쉽게 하는 방향으로 로봇 손을 설계하는 접근도 생각해볼 수 있습니다. 예컨대 특정 매핑 알고리즘(시너지 등)에 최적화되도록 로봇 손의 관절 비율이나 구동 방식을 디자인하면, 훨씬 자연스러운 모방이 가능할 것입니다. 이는 손 하드웨어와 소프트웨어의 co-design 개념으로, 인간 손의 움직임 특성을 잘 모사하면서도 구현 용이성을 양쪽에서 절충하는 방향입니다. 몇몇 연구에서 로봇 손의 기계적 시너지 구조를 도입한 바 있는데, 이를 매핑 알고리즘과 동시에 최적화하면 일체형 솔루션이 될 수 있습니다.\n교육/훈련을 통한 사용자 적응 연구: 매핑이 완벽하지 않은 이상, 사용자의 적응력도 큰 역할을 합니다. 따라서 얼마나 훈련을 통해 사용자가 매핑에 익숙해질 수 있는지, 어떤 피드백과 가이드가 학습을 돕는지 등의 연구가 필요합니다. 예를 들어 VR 환경에서 사용자에게 가상 손과 로봇 손의 차이를 시각화해 보여주고 보정 연습을 시키면 실제 조작 성능이 향상되는지 등을 실험할 수 있습니다. 이러한 사용자 트레이닝 측면을 체계화하면, 매핑 알고리즘의 한계를 인간의 학습으로 상쇄하는 방안도 마련될 것입니다.\n표준 데이터셋과 대회 개최: 마지막으로, 연구 커뮤니티 차원에서 공개 데이터셋과 벤치마크 대회를 운영하는 것도 고려할 만합니다. 다양한 인간 손동작-로봇 손 대응 데이터, 다양한 로봇 손 모델 등이 포함된 데이터셋을 공유하면, 서로 다른 알고리즘을 공정하게 비교하고 발전시킬 수 있습니다. 또 예를 들어 “주어진 로봇 손으로 시演된 인간 동작을 가장 자연스럽게 따라하게 하기”와 같은 챌린지를 개최하면, 새로운 아이디어들이 촉발될 것입니다. 이러한 노력은 저자들이 언급한 표준화와 객관적 평가척도 정립에도 기여하면서, 연구의 협업 촉진과 속도 향상을 가져올 수 있습니다."
  },
  {
    "objectID": "posts/paper/2025-08-09-retargeting-survey.html#conclusion",
    "href": "posts/paper/2025-08-09-retargeting-survey.html#conclusion",
    "title": "📃Retargeting Survey 리뷰",
    "section": "2.11 Conclusion",
    "text": "2.11 Conclusion\nRoberto Meattini 등은 본 논문을 통해 인간-로봇 손 동작 매핑 분야의 지난 수십 년 간의 연구를 망라하며, 이를 여섯 가지 개념 범주로 명료하게 정리했습니다. 각 접근법의 이론적 배경부터 장단점, 적용 사례까지 폭넓게 다루어 연구자들에게 큰 그림을 제시한다는 점에서 의미가 큽니다. 요약하자면, 직접 관절/데카르트 매핑은 가장 단순한 방법이지만 로봇 손이 사람 손과 다를 때 제약이 있고, 과업 지향 매핑은 비인간형 로봇에서도 동작 전달이 가능하나 사용자 직관성이 낮으며, 차원 축소 매핑은 제어를 간소화하지만 동작 자유도를 제한하고, 자세 인식 매핑은 특정 동작 셋에 효율적이나 연속 제어가 어렵고, 하이브리드 매핑은 여러 기법의 장점을 취하나 설계 복잡성이 높습니다. 이러한 통찰을 바탕으로, 연구자들은 각자의 응용 분야에 맞는 최적의 접근을 모색할 수 있을 것입니다. 또한 논문 말미의 미래 전망은 이 분야가 나아갈 길을 잘 짚어주고 있는데, 평가지표 통일과 사용자 중심 설계, 선험 정보 활용, 양방향 피드백, 적응형 알고리즘 등이 그 핵심입니다. 이는 모두 “인간과 로봇의 연결”이라는 매핑 문제의 본질을 더욱 잘 풀어나가기 위한 방향들로서, 필자 역시 이러한 노력들이 수행될 때 인간 손의 능숙함을 로봇 손에 온전히 불어넣는 날이 앞당겨질 것이라 기대합니다."
  },
  {
    "objectID": "posts/paper/2025-07-20-eureka.html",
    "href": "posts/paper/2025-07-20-eureka.html",
    "title": "📃Eureka 리뷰",
    "section": "",
    "text": "Paper Link\nProject Link\nGithub Link"
  },
  {
    "objectID": "posts/paper/2025-07-20-eureka.html#논문-개요-및-주요-기여",
    "href": "posts/paper/2025-07-20-eureka.html#논문-개요-및-주요-기여",
    "title": "📃Eureka 리뷰",
    "section": "논문 개요 및 주요 기여",
    "text": "논문 개요 및 주요 기여\nEureka (Evolution-driven Universal REward Kit for Agent)은 대형언어모델(LLM)을 활용하여 강화학습의 Reward 함수를 자동으로 설계하는 새로운 알고리즘입니다. 이 논문은 GPT-4와 같은 코드 생성 특화 LLM의 뛰어난 능력을 활용하여, 인간 전문가 수준 또는 그 이상의 품질을 가진 Reward 함수를 자동 생성 및 개선하는 방법을 제안합니다. 저자들은 29개의 공개 RL 환경(로봇형상 10종 포함)에서 실험하여, Eureka가 83%의 과제에서 인간이 설계한 Reward을 능가하고 평균 52%의 성능 향상을 달성했음을 보고합니다. 특히 복잡한 고차원 조작 작업(예: 5지 로봇 손으로 펜 돌리기)에서도 기존 수작업 Reward으로 달성하기 어려웠던 성공을 처음으로 실현해 보였습니다. 또한 Eureka는 인간 피드백을 Reward 설계에 통합하여 에이전트의 행동을 인간 선호에 맞게 조정하는 새로운 방식의 RLHF(강화학습 환경에서 인간 피드백 활용)도 선보입니다.\n논문은 아래와 같은 세 가지 주된 기여를 강조합니다:\n\n범용적 Reward 설계 성능 – 사전 정의된 템플릿이나 과제별 프롬프트 없이도 여러 분야의 과제에서 전문가 수준 Reward 함수를 자동 생성하여, 대부분의 과제에서 인간 Reward보다 우수한 성능을 보였습니다.\n신규 난제 해결 – 기존에 수작업 Reward으로 불가능했던 고난도 조작 과제(예: 펜 회전)를 Eureka Reward과 교육과정 학습(curriculum learning)을 통해 최초로 성공시켰습니다. 이는 Eureka가 복잡한 스킬 학습을 견인할 수 있다는 것을 증명합니다.\n인간 피드백 통합 – 모델 파인튜닝 없이도 사람의 피드백을 Reward 함수 개선에 활용하는 새로운 gradient-free RLHF 접근을 제시합니다. Eureka는 기존 인간 Reward 함수를 출발점으로 삼아 더 나은 Reward으로 개선하거나, 오직 텍스트 형태의 인간 피드백만으로 에이전트 행동을 사람이 선호하는 방향으로 조율할 수 있음을 보였습니다.\n\n이러한 기여를 통해 Eureka는 인간 수준의 Reward 설계를 자동화하는 토대를 마련했습니다. Reward 함수 설계의 어려움은 오랫동안 RL의 병목으로 지적되어 왔는데, 연구 조사에 따르면 92%의 RL 연구자들이 Reward 설계를 시행착오에 의존하고 89%는 자신들의 Reward이 최적 이하이며 의도치 않은 행동을 유발한 경험이 있다고 합니다. Eureka는 이러한 문제를 해결하기 위해 LLM 기반의 범용 Reward 프로그래밍 알고리즘 가능성을 묻고, 이를 구현한 것입니다."
  },
  {
    "objectID": "posts/paper/2025-07-20-eureka.html#eureka-시스템-구현-코드-구현-방식",
    "href": "posts/paper/2025-07-20-eureka.html#eureka-시스템-구현-코드-구현-방식",
    "title": "📃Eureka 리뷰",
    "section": "Eureka 시스템 구현 (코드 구현 방식)",
    "text": "Eureka 시스템 구현 (코드 구현 방식)\nEureka는 “환경을 이해하는 LLM”과 “강화학습 환경”을 연결하여 Reward 코드를 생성하고 개선하는 자동화 파이프라인으로 구성됩니다. 구체적으로, 환경 정보와 과제 설명을 입력으로 LLM이 파이썬 Reward 함수 코드를 작성하고, 이를 RL 환경에서 실행하여 정책 학습 성과를 평가한 뒤, 평가 결과를 LLM에 피드백으로 제공하여 Reward 함수를 반복적으로 개선합니다. 아래 그림은 Eureka 시스템의 흐름을 나타냅니다.\n\n\n\n\nEureka 프레임워크 개요 – 환경 소스 코드와 과제 자연어 설명을 LLM(GPT-4)에 컨텍스트로 제공하면, LLM이 즉시 실행 가능한 Reward 함수 코드를 생성합니다. 이렇게 생성된 Reward 함수를 활용하여 GPU 가속 시뮬레이터(예: NVIDIA Isaac Gym)에서 에이전트의 강화학습 정책을 훈련하고, 학습 통계(에피소드 성공률, Reward 구성요소 값 변화 등)를 수집합니다. 그 후 “Reward 성찰 (reward reflection)” 단계에서 LLM에 학습 피드백(예: Reward 구성별 값 추이, 정책 성능 지표 등)을 요약된 텍스트로 전달하고, 이를 바탕으로 Reward 함수를 수정/개선하도록 새로운 Reward 코드 생성을 요청합니다. 이러한 코드 생성 → RL훈련 → 피드백 → 코드 수정의 반복 루프를 통해 Reward 함수는 점진적으로 향상됩니다.\n\n\nLLM–환경 인터페이스: 환경 코드 활용\nEureka의 핵심 아이디어 중 하나는 환경을 그대로 LLM에 맥락으로 제공하는 것입니다. 구체적으로, 환경의 Python 소스코드에서 상태(observation)와 행동(action) 변수 정의 부분을 추출하여, 해당 환경 클래스/함수 코드 자체를 프롬프트에 포함시킵니다. 여기에 과제(task) 설명 문자열을 추가하여, LLM에게 “이 환경에서 주어진 과제를 해결할 Reward 함수를 작성하라”는 지시를 합니다. 이러한 “환경 자체를 프롬프트로” 제공하는 접근은 두 가지 이점을 가집니다:\n\n직관적인 코드 작성 맥락: LLM이 이미 학습한 프로그래밍 언어와 환경의 변수명을 그대로 활용하여, 익숙한 형식으로 코드를 작성할 수 있습니다. 이는 LLM이 일반적인 자연어 설명보다 정확히 필요한 Reward 항목을 포착하여 코드로 변환하는 데 유리합니다.\n환경 정보의 총체적 제공: 환경 코드에는 과제의 상태공간과 동역학에 대한 단서가 내포되어 있습니다. 예를 들어 관절각, 목표 위치 등의 변수명이 주어지므로, LLM은 어떤 변수들이 Reward에 활용될 수 있는지 자연스럽게 파악합니다. 인간이 일일이 알려주지 않아도, 환경이 허용하는 모든 관측치에 기반해 Reward 함수를 구성할 수 있게 되는 것입니다.\n\n중요한 점은, Eureka는 특정 과제에 맞춘 추가 힌트 없이도 이 방식만으로 타당해 보이는 초기 Reward 함수를 만들어낸다는 것입니다. 예를 들어, 별도 템플릿 없이도 GPT-4는 환경 코드에 나오는 fingertip_pos 등 관측 변수를 이용해 합리적인 Reward 공식을 작성할 수 있었습니다. 다만, 이렇게 첫 시도에서 생성된 Reward 함수는 문법 오류가 있거나, 실행되더라도 성능이 미흡할 수 있습니다. 이를 해결하기 위해 Eureka는 한 번의 생성에 그치지 않고 반복적 개선 절차를 도입합니다.\n\n참고: 환경 코드가 너무 길 경우를 대비해, 저자들은 자동 스크립트로 중요한 부분만 추출하여 LLM 컨텍스트 길이에 맞게 조절했다고 합니다. 또한 시뮬레이터 종속적인 내부 코드(예: 물리엔진 세부 설정)는 제외하여, 다른 환경에도 일반화될 수 있는 맥락만 제공하도록 유도했습니다.\n\n\n\n코드 생성 및 평가: 진화적 Reward 탐색\nEureka는 진화적 탐색(evolutionary search) 전략을 통해, LLM이 생성한 Reward 코드의 품질을 점증적으로 향상시킵니다. 이 알고리즘은 아래와 같이 작동합니다:\n\n다수 후보 생성 – 각 반복(iteration) 단계마다 LLM에게 동일한 프롬프트를 여러 번 독립적으로 실행하여 K개의 Reward 함수 후보를 샘플링합니다. 기본 설정으로 한 번에 K=16개의 코드를 생성하는데, 이렇게 하면 최소 하나 이상의 실행 가능한 코드가 나올 확률이 매우 높습니다 (저자에 따르면 16개 중 적어도 1개는 오류 없이 실행되었다고 합니다). LLM의 출력을 다수 확보함으로써, 단일 시도 시 발생할 수 있는 코드 오류 문제를 완화합니다.\n대규모 병렬 평가 – 생성된 각 Reward 함수에 대해, 동시에 RL 에이전트를 훈련시켜 성능을 평가합니다. NVIDIA Isaac Gym과 같은 GPU 가속 환경을 이용하여 여러 정책을 병렬로 훈련함으로써, 수십 개 Reward에 대한 평가를 신속히 수행할 수 있었습니다. 논문에서는 Isaac Gym을 통해 정책 학습 속도가 CPU 대비 최대 1000배까지 가속되었음을 언급하며, 대규모 Reward 탐색을 현실화하는 데 핵심적인 역할을 했다고 강조합니다. (구체적으로 어떤 RL 알고리즘을 사용했는지는 Isaac Gym 기본 RL 알고리즘(PPO 등)으로 추정되며, GPT-4 등 LLM과는 별개로 전통적 RL 학습이 이루어집니다.)\n최고 성능 Reward 선택 – 평가 결과 가장 높은 성능(score)을 낸 Reward 함수를 우선 선택합니다. 이 베스트 후보는 이후 LLM 프롬프트에 포함되어, 다음 세대 Reward 코드 생성에 참조 예시(context)로 사용됩니다. 이렇게 이전 세대의 우수한 Reward을 맥락에 추가함으로써, LLM이 기존 Reward의 구조와 성능 특성을 고려하여 개선된 변형을 만들도록 유도합니다.\n반복 및 다중 시도 – 위 과정을 N번 반복하여 Reward을 지속적으로 개선하며, 여러 독립적인 검색 시도(random restarts)도 수행합니다. 논문 실험에서는 5회 독립 실행하여 각 5세대(iterations)씩 탐색했고, 최종적으로 얻은 최고 성능 Reward 함수를 결과물로 선택했습니다. (다중 시작은 전역 최적해를 찾기 위한 표준 기법으로, 초기 샘플링에 운 나쁘게 걸렸을 경우를 보완합니다.)\n\n진화적 탐색 과정에서 LLM은 두 가지 모드로 활용됩니다. 처음 1세대에서는 오직 환경 코드+과제설명 만으로 “제로샷” Reward 생성을 수행하고, 2세대부터는 이전 최고 Reward과 추가 지시를 맥락에 포함해 “변이(mutation)” 생성을 수행하는 것입니다. 변이를 유도하기 위해, 저자들은 프롬프트에 간단한 텍스트 지침(예: “이 Reward 함수를 약간 수정하여 더 나은 성능을 내도록 해보세요”)을 추가했다고 합니다. 이 지침은 구체적인 수정 방향을 강요하지 않고도, LLM이 다양한 형태의 Reward 변형을 시도하게 돕습니다. 실제로 Eureka가 만들어낸 개선들은 (1) 기존 Reward 성분의 가중치 등 하이퍼파라미터 조정, (2) 기존 성분의 수식 형태 변경, (3) 완전히 새로운 Reward 성분 추가** 등 매우 자유도 높은 변화들을 포괄했습니다. Fig.3 (논문 예시 그림)에서도 이러한 다양한 형태의 Reward 수정이 시각화되어 있습니다.\n\n\n\n요약하면, Eureka의 탐색은 폭넓은 Reward 공간에서 LLM의 생성 다양성과 병렬 RL평가를 결합하여 우수한 Reward 함수를 진화적으로 찾아가는 과정입니다. 이는 기존에 사람이 수동으로 Reward을 튜닝하던 시험-오류 과정을 자동화한 것으로 볼 수 있습니다.\n\n\nReward Reflection: 텍스트 기반 성능 피드백\nEureka가 LLM을 통해 Reward을 개선하기 위해서는, RL 훈련 결과를 어떻게든 LLM에 이해시킬 방법이 필요합니다. 단순히 “현재 점수 = X”와 같은 숫자 성적만 제공하면, LLM은 어떻게 Reward을 바꿔야 할지 감을 잡기 어렵습니다. 이를 해결하기 위해 도입된 개념이 “Reward 성찰 (reward reflection)”입니다.\nReward 성찰이란, 정책 학습 과정의 핵심 동향을 요약한 텍스트 피드백으로서, LLM이 Reward 함수의 장단점을 파악하는 데 활용됩니다. Eureka는 이를 위해 Reward 함수를 특별한 형식으로 작성하도록 했습니다: Reward 함수가 각 시점에 계산하는 개별 Reward 구성 요소들을 딕셔너리로 출력하게 한 것입니다. 예를 들어, 펜 회전 과제라면 Reward 함수가 \"orientation_bonus\", \"angular_velocity_penalty\" 등의 성분별 값을 매 시뮬레이션 단계마다 산출하여, 총 Reward 외에 구성별 값을 기록하도록 합니다. 이렇게 하면 RL 훈련 동안 각 구성 요소가 어떻게 변화하는지 추적할 수 있습니다. Eureka는 정해진 간격마다 (예: 훈련의 20%, 40%, … 진행 시점) 정책의 성능과 Reward 구성값 통계를 수집하여, 이를 사람 읽기 좋은 형태로 요약합니다.\n\n\n\n\n예시: 논문에서는 av_penalty (각속도 패널티)라는 구성요소의 값이 훈련 초기에 높다가 나중에 낮아지는 추세를 보인 경우를 예로 듭니다. 이러한 정보를 텍스트로 “av_penalty 값이 점차 감소했다”, “성공률은 초기 0.1에서 최종 0.9로 상승했다” 등으로 표현하여, LLM에게 전달합니다. 그리고 프롬프트에 “위 피드백을 분석하여 Reward 함수를 개선하라”는 식의 요청을 덧붙입니다 (그림의 파란색 Reward Reflection 예시 참조).\n\n이런 상세한 피드백은 두 가지 이유로 중요합니다. 첫째, 총 점수(score)만 알려주는 경우 어떤 부분이 문제인지 알 수 없습니다. 반면 성분별 피드백은 어느 Reward 항목이 정책에 잘 반영되었고, 어느 것이 무시되었는지 짐작할 수 있게 해줍니다. 둘째, Reward 함수의 효과는 사용된 RL 알고리즘에 따라 다를 수 있기 때문에, 성능 저하가 Reward 설계 문제인지, 알고리즘상의 한계인지 구분해야 합니다. 예컨대 동일한 Reward도 RL 하이퍼파라미터에 따라 성과가 달라질 수 있는데, Reward 성찰은 학습과정 자체의 반응을 보여주므로, LLM이 현 알고리즘에 최적화된 Reward으로 조정할 수 있도록 도와줍니다.\n결과적으로, Reward 성찰을 포함한 피드백을 제공할 때 LLM은 더 정교하고 목표 지향적인 수정 제안을 내놓게 됩니다. 실제 실험에서도 Reward 성찰의 유무에 따라 성능 차이가 컸습니다. Eureka에서 Reward 성찰을 제거하고 오직 최종 성능 수치만 피드백으로 준 경우, 평균 성능이 28.6%나 떨어졌고, 특히 난이도 높은 과제일수록 성능 저하가 두드러졌다고 합니다. 이는 세밀한 텍스트 피드백이 복잡한 Reward 최적화에 필수적임을 보여줍니다.\n\n\n구현 상세 및 재현성\nLLM 모델: Eureka는 OpenAI GPT-4(버전 gpt-4-0314;deprecated!)를 기본 LLM으로 사용하였습니다. GPT-4의 코드 생성 능력과 지시 이해 능력이 본 연구의 토대를 이루고 있습니다. (참고로, GPT-4의 지식 컷오프(2021년 9월) 이후 등장한 환경들도 실험에 포함되어, 사전 학습 지식 없이 진정한 제로샷 생성 능력을 평가했다고 언급합니다.)\n보조 모델 및 도구: 특별히 명시된 별도의 보조 ML모델은 사용되지 않았으며, RL 정책 학습에는 표준 PPO 알고리즘(Isaac Gym 내장)을 활용한 것으로 보입니다. 다만, 하나의 Reward 후보를 평가하는 데도 수천 회의 시뮬레이션 스텝을 수행해야 하므로, 병렬처리와 GPU 가속이 핵심 기술 요소로 사용되었습니다. 또한 코드 구현 면에서, 생성된 Reward 함수의 문법 검사나 실행 테스트를 자동화하여, 런타임 오류를 미리 감지하는 장치도 포함되었을 것으로 추측됩니다 (오픈소스 코드 상에서 확인 가능). 예를 들어 코드 실행 전 파싱을 시도하거나, 실행 중 예외 발생 시 해당 Reward을 점수 0으로 간주하는 식으로 견고성을 확보했을 것입니다.\n오픈소스와 재현성: 저자들은 프롬프트, 환경, 생성된 Reward 함수 코드를 모두 공개하여 후속 연구가 용이하도록 한다고 밝혔습니다. 실제로 논문 발표와 함께 GitHub에 공식 구현 리포지터리가 공개되어 있으며, 이미 3천 개 이상의 Star를 받을 만큼 큰 호응을 얻고 있습니다. 공개 코드에는 커스텀 환경 정의(IsaacGym 기반), Eureka 알고리즘 (프롬프트, GPT API 호출, RL 학습 루프), 그리고 29개 과제 각각에 대해 Eureka가 발견한 최적 Reward 함수 예제들이 포함되어 있습니다. 이를 통해 다른 연구자들이 결과를 재현하거나, 새로운 환경에 Eureka를 적용해볼 수 있는 기반이 갖춰져 있습니다."
  },
  {
    "objectID": "posts/paper/2025-07-20-eureka.html#이론적-기여-및-기존-연구와의-차별성",
    "href": "posts/paper/2025-07-20-eureka.html#이론적-기여-및-기존-연구와의-차별성",
    "title": "📃Eureka 리뷰",
    "section": "이론적 기여 및 기존 연구와의 차별성",
    "text": "이론적 기여 및 기존 연구와의 차별성\n\n기존 Reward 설계 접근과 Eureka의 차별점\n강화학습에서 Reward 설계(reward design) 문제를 자동화하려는 시도는 과거에도 여러 방향으로 연구되었습니다. 예를 들어, 자연어 설명을 활용하여 Reward을 생성하는 접근으로 L2R(Language to Reward, Yu et al., 2023)이나 Text2Reward 등이 최근 제안되었습니다. 그러나 이러한 방법들은 LLM을 제한적 방식으로 활용했다는 한계가 있습니다. L2R의 경우 두 단계 프롬프트를 통해 사전 정의된 템플릿 형태의 Reward만 생성하도록 했습니다. 구체적으로, 첫 번째 LLM이 “로봇의 동작에 대한 서술”을 채우면, 두 번째 LLM이 이를 미리 준비된 Reward API 함수 호출 코드로 변환하는 식이었습니다. 이러한 템플릿 기반 접근은 인간이 정의한 Reward 프리미티브(기본 요소)의 조합으로 결과를 내므로 표현력이 제한되고, 새로운 Reward 구조를 만들어내기 어렵습니다. 또한 환경마다 템플릿을 조금씩 바꾸거나, LLM에 과제별 힌트(prompt)를 수동 제공해야 하는 등 범용성에도 한계가 있었습니다.\nEureka는 이러한 기존 접근과 몇 가지 중요한 차별점을 보입니다:\n\n전용 프롬프트/템플릿의 부재: Eureka는 어떤 과제에도 동일한 전략(환경 코드 + 과제설명)만으로 적용됩니다. 추가 예시, 템플릿, 수동 프롬프트 튜닝이 전혀 없으며, 순전히 LLM의 일반 능력에 의존해 Reward을 만듭니다. 그럼에도 대부분 과제에서 인간 Reward보다 나은 성능을 보였다는 점에서 범용성과 효과성을 입증했습니다.\n자유 형식의 Reward 프로그램 생성: L2R 등이 정해진 함수들의 조합만 생성한 반면, Eureka는 파이썬 언어로 된 임의의 논리를 생성합니다. 덕분에 완전히 새로운 Reward 개념도 도입할 수 있습니다. 실제 Eureka가 만들어낸 Reward 중에는 인간 Reward과 상관관계가 거의 없거나 음의 상관관계를 보이면서도 성능이 뛰어난 것들이 발견되었습니다. 이는 인간이 생각하지 못한 Reward 설계 원리를 Eureka가 찾아냈음을 시사합니다. 예컨대 어떤 과제에서는 인간 Reward과 정반대 방향으로 작동하는 Reward이 오히려 학습을 촉진한 사례도 있었으며, 저자들은 해당 예시들을 부록에 제시했습니다. 즉, Eureka는 사람의 직관을 뛰어넘는 창의적 해법을 발굴할 수 있습니다.\nLLM 기반 반복 최적화: 기존 방법들은 Reward 함수를 한 번 생성하고 끝나는 경우가 많았습니다. 반면 Eureka는 LLM을 루프 안에서 반복 호출하며, 학습으로부터 피드백을 받아 점진적 개선을 합니다. 이런 in-context learning 루프는 마치 LLM이 “Reward 디버깅”을 하는 것과 비슷합니다. 사람도 Reward 설계 후 정책 결과를 보고 수정하는데, Eureka는 이를 자동화하고 훨씬 빠른 사이클로 돌린다고 볼 수 있습니다. 특히 Reward 성찰을 통해 LLM이 실패 원인을 이해하고 수정하는 점은, 기존에 없던 혁신적인 요소입니다.\n인간 통찰과의 결합: Eureka는 인간이 부분적으로 설계한 Reward과 협업할 수도 있습니다. 인간이 만든 초기 Reward을 1세대 출력으로 사용하여 그 위에 개선을 시작하면, Eureka (Human Init.) 버전이 나오는데, 이는 원본 인간 Reward이나 순수 Eureka보다 항상 더 좋은 성능을 보였습니다. 이는 인간의 직관(유용한 상태 변수 선택 등)과 LLM의 최적화 능력을 결합할 때 상승 효과가 있음을 보여줍니다. 연구진은 “인간은 어떤 상태 변수가 중요한지 아는 상식적 지식은 갖췄지만, 그것들을 어떻게 조합하여 Reward 설계로 활용할지는 상대적으로 미숙할 수 있다”는 통찰을 언급하며, Eureka가 인간의 약점을 보완하는 Reward 설계 조력자로 기능할 수 있음을 강조합니다.\n안전성과 윤리적 정렬: Reward 함수가 항상 바람직한 행동을 담보하지는 않기에, 인간 선호에 맞게 Reward을 수정하는 것이 중요합니다. 기존 RLHF는 주로 정책을 미세 조정하는 방식이지만, Eureka는 Reward 함수를 수정하는 방식의 RLHF를 선보였습니다. 예를 들어 인간 평가자들이 “로봇이 너무 앞으로 숙여 뛰니 똑바로 뛰도록 Reward 수정해달라”는 식의 피드백을 텍스트로 주면, Eureka가 그에 맞춰 Reward 함수에 자세 유지 패널티 등을 추가하도록 개선할 수 있습니다. 그 결과 더 안전하고 안정적인 동작이 유도되었고, 사용자 20명 중 15명이 원래 Reward으로 훈련된 로봇보다 인간 피드백 반영 Reward으로 훈련된 로봇을 선호했다고 합니다. 이러한 gradient-free RLHF 방식은 Reward 설계 차원에서의 인간-모델 상호작용이라는 새로운 가능성을 열었습니다.\n\n\n\n이론적 기반과 의의\n이 논문은 Reward 설계 문제를 공식적으로 정립하고, Eureka로 이를 해결하는 새로운 관점을 제시했다는 점에서 이론적인 의미도 갖습니다. 저자들은 우선 Singh et al. (2010)의 Reward설계 문제 (Reward Design Problem, RDP) 정의를 인용하며, “주어진 환경 MDP와 실제 성능 평가 함수 F 하에서, 최적의 정책이 최대의 F 점수를 얻도록 하는 Reward R을 찾는 문제”로 RDP를 설명합니다. 그리고 이를 프로그램 생성 맥락에 맞게 변형하여 “Reward 생성 문제 (Reward Generation Problem)”로 명명합니다. 즉, 코드 형태로 표현된 Reward 함수 공간 R에서 하나의 코드 조각을 찾아내는 문제로 개념화한 것입니다. 이때 검색공간은 프로그램의 공간으로 매우 크고 비구조적이지만, LLM의 코드 작성 능력과 확률적 탐색을 통해 해법을 찾을 수 있음을 Eureka로 입증했습니다.\nEureka의 성공 요인은 세 가지 설계 철학으로 요약됩니다:\n\n환경을 맥락으로 제공함으로써 일반성 확보 – 모든 과제에 통용되는 입력(코드)만으로 LLM을 활용하였고, 덕분에 과제 특화된 인간 지식 없이도 작동했습니다.\n진화적 탐색으로 품질 향상 – 단일 샘플의 한계를 다수 샘플과 반복 개선으로 극복하여, LLM 출력의 약점을 보완했습니다.\nReward 성찰로 정교한 피드백 제공 – 단순 성능 수치 이상의 맥락 정보를 LLM에 제공함으로써, 세밀하고 효과적인 코드 수정을 유도했습니다.\n\n이러한 구성 요소들의 상호보완적 결합이 Eureka를 인간 수준까지 끌어올린 핵심 비결입니다. 그 결과 Eureka는 강화학습 Reward 설계의 패러다임을 전환하는 잠재력을 보여주었습니다. 사람 전문가가 오랜 경험을 통해야 할 작업을 LLM과 자동화 기법이 대체하거나 가속할 수 있음을 실증한 것이므로, 미래의 RL 연구 프로세스에도 큰 영향을 줄 것으로 보입니다.\n\n\n한계 및 향후 전망\n\n(※ 논문에 직접 언급되진 않지만, 이해를 돕기 위해 고려할 점을 함께 서술합니다.)\n\nEureka는 뛰어난 성과를 보였지만, 전제로 하는 조건들이 있습니다. 예를 들어 환경의 내부 코드 또는 API 접근이 가능해야 하고, 강화학습으로 평가할 수 있는 성능 함수 F가 있어야 합니다. 현실 세계 문제의 경우 환경 코드를 얻기 어렵거나, 시뮬레이터 없는 물리 환경에서는 Reward 평가에 시간이 오래 걸릴 수 있습니다. 이러한 경우 Eureka 접근을 그대로 적용하긴 어려울 수 있습니다. 향후에는 모델 기반 시뮬레이션이나 모상 학습 등을 접목해 현실에서도 빠른 Reward 탐색을 가능케 하는 연구가 나올 수 있습니다.\n또한 Eureka는 현재 GPT-4 같은 대규모 사설 LLM API에 의존하고 있어, 재현 비용이나 모델 접근성 이슈도 존재합니다. 오픈소스 코드 LLM이 더 발전하면, 자체 호스팅으로 Eureka를 실행하는 방향도 고려될 것입니다. 실제로 저자들도 모든 프롬프트를 공개했으므로, 다른 LLM으로 대체하여 실험하는 연구가 이어질 수 있습니다.\n끝으로, Reward 설계 자동화가 지니는 함의에 대해 생각해볼 점이 있습니다. Reward은 곧 에이전트의 목표를 정의하는 것이므로, Eureka 같은 기법이 잘못 사용될 경우 의도치 않은 목표를 강화하거나, 윤리적으로 논란이 될 행동을 부추길 위험도 있습니다. 따라서 인간 감독과 안전장치를 갖춘 활용이 중요하며, Eureka의 인간 피드백 통합 기능은 이러한 위험을 완화하는 방향의 좋은 시작이라 할 수 있습니다. 앞으로 Eureka를 활용한 안전한 Reward 설계, 정렬된 AI 훈련 등의 연구가 활발히 전개될 것으로 기대됩니다."
  },
  {
    "objectID": "posts/paper/2025-07-20-eureka.html#결론",
    "href": "posts/paper/2025-07-20-eureka.html#결론",
    "title": "📃Eureka 리뷰",
    "section": "결론",
    "text": "결론\nEureka: Human-Level Reward Design via Coding LLMs는 대형언어모델을 통한 Reward 함수 자동 생성이라는 새로운 지평을 연 연구로서, 강화학습의 난제였던 Reward 설계를 혁신적으로 해결했습니다. 코드 생성 LLM과 진화적 탐색, 그리고 RL 피드백 통합이라는 아이디어 조합을 통해, 인간 전문가보다도 나은 Reward을 찾아내고 복잡한 로봇 제어 과제를 달성해낸 점이 인상적입니다. 또한 범용성을 지향한 구현과 오픈소스 공개로 학술적·실용적 파급력도 높습니다. 이 논문의 성과는 향후 자동화된 RL 문제설계, 휴먼-인더-루프 학습 등 다양한 분야에 영향을 줄 것이며, AI 에이전트 개발 과정을 효율화하고 인간과의 협업을 증진하는 방향으로 계속 발전될 것으로 예상됩니다.\n\n\nUpdated Code-25.07.20"
  },
  {
    "objectID": "posts/paper/2025-07-29-stmr.html",
    "href": "posts/paper/2025-07-29-stmr.html",
    "title": "📃SMTR 리뷰",
    "section": "",
    "text": "Paper Link"
  },
  {
    "objectID": "posts/paper/2025-07-29-stmr.html#연구의-독창성-시공간-아키텍처-모션-프라이어-통합-리타게팅-전략",
    "href": "posts/paper/2025-07-29-stmr.html#연구의-독창성-시공간-아키텍처-모션-프라이어-통합-리타게팅-전략",
    "title": "📃SMTR 리뷰",
    "section": "연구의 독창성: 시공간 아키텍처, 모션 프라이어 통합, 리타게팅 전략",
    "text": "연구의 독창성: 시공간 아키텍처, 모션 프라이어 통합, 리타게팅 전략\n「Spatio-Temporal Motion Retargeting (STMR)」은 사족 보행 로봇에 동물의 움직임을 모방시키기 위한 새로운 모션 리타게팅 기법으로, 공간적 및 시간적 두 단계의 아키텍처를 도입한 점이 특징입니다. 이 방법은 먼저 공간적 모션 리타게팅(SMR) 단계를 통해 원본 동작의 운동학적 적합성을 확보합니다. 구체적으로, 발 미끄러짐이나 발 관통과 같은 운동학적 부조화를 제거하고 원본 동작의 접촉 시퀀스(contact schedule)를 유지하도록 발 움직임에 제약을 가함으로써, 대상 로봇의 관절 범위 내에서 키네마틱하게 실행 가능한 전체 신체 모션을 생성합니다. 다음으로 시간적 모션 리타게팅(TMR) 단계에서는 동작의 동역학적 타당성을 보장하기 위해 모션의 시간적 파라미터를 최적화합니다. 예를 들어, 점프 동작의 경우 로봇의 크기에 따라 비행(flight) 단계의 지속시간이 달라져야 하므로, TMR은 모델 기반 제어기(예: 차분 동적 프로그래밍, DDP)를 내부 프로세스로 활용하여 모션 타이밍을 조정하고 중간 공중 체공시간 등 동역학적 일관성을 확보합니다. 이러한 이단계(공간+시간) 리타게팅 전략을 통해 원본 생물체와 형태·크기가 다른 로봇이라도, 운동학적·동역학적으로 실행 가능한(reference) 모션으로 변환할 수 있습니다. 이는 기존 방법들이 형태 차이는 보정하더라도 물리적으로 불가능한 동작을 생성하는 한계를 극복하고자 한 것으로, 모션 프라이어(motion prior) 활용 면에서도 새로운 접근이라 할 수 있습니다. 본 논문에서 특별히 학습된 모션 프라이어(예: GAN 기반 생성기)를 직접 사용하지는 않지만, 대신 모델 기반 최적화와 참조 모션의 물리적 타당성 확보를 통해 동작 사전 지식을 통합한 효과를 냅니다. 이는 임의의 동작 데이터베이스에 한정되지 않고도 모사 학습을 유도할 수 있는 내재적 모션 프라이어를 제공하는 셈입니다. 마지막으로, 이렇게 얻어진 kino-dynamically feasible (운동학·동역학적으로 실행 가능한) 참조 모션을 토대로 잔여 학습(residual learning)을 적용한 강화학습 정책을 훈련하는 리타게팅 전략을 제시한 점도 독창적입니다. 구체적으로, 참조 모션을 기본 제어 신호로 활용하고 RL 에이전트가 보정 명령만 출력하도록 함으로써, 피드백 제어 정책 학습을 효과적으로 유도하고 최종적으로 실세계 로봇에 배치 가능한 제어기를 얻어낸 것입니다. 요약하면, 해당 연구의 STMR 접근법은 공간적 최적화 + 시간적 최적화 + 잔여 RL 정책으로 구성된 새로운 시공간 모션 리타게팅 프레임워크이며, 이는 기존 모션 모방 기법들의 한계를 넘어 형태 차이 극복과 물리적 실행 가능성 보장을 동시에 달성한 점에서 학술적 의의가 있습니다."
  },
  {
    "objectID": "posts/paper/2025-07-29-stmr.html#방법의-강점-일반화-능력-강인성-학습-전략-및-설계",
    "href": "posts/paper/2025-07-29-stmr.html#방법의-강점-일반화-능력-강인성-학습-전략-및-설계",
    "title": "📃SMTR 리뷰",
    "section": "방법의 강점: 일반화 능력, 강인성, 학습 전략 및 설계",
    "text": "방법의 강점: 일반화 능력, 강인성, 학습 전략 및 설계\nSTMR 방법은 여러 측면에서 뛰어난 강점을 보였습니다. 일반화 능력 측면에서, 이 기법은 서로 다른 형태와 물리 특성을 지닌 로봇들에 대해서도 적용 가능함을 보였습니다. 저자들은 크기와 질량이 서로 다른 세 종류의 사족 로봇(예: Unitree A1, Go1, AlienGo)에 대해, 6가지의 상이한 난이도의 동물 모션들을 성공적으로 리타게팅 및 모방 학습시켰습니다. 그 결과 생성된 참조 모션은 접촉 타이밍을 충실히 보존하고 발 미끄러짐이 거의 제거되어, 크기가 다른 로봇이라도 원본 동작의 의미를 잃지 않도록 했습니다. 이처럼 STMR은 동작 다양성(여러 종류의 걸음걸이: 트롯, 페이스, 옆걸음, 점프회전 등)과 로봇 플랫폼 다양성에 모두 잘 대응하여, 우수한 일반화 성능을 입증했습니다.\nRobustness 측면에서도 장점을 확인할 수 있습니다. RL 정책 학습 시 잔여 정책(residual policy) 접근을 도입함으로써, 기본 PD 제어기의 추종 성능에 학습 기반 보정 정책을 더하는 형태를 취했는데, 이는 학습의 안정성과 실환경 강건성을 크게 높였습니다. 구체적으로, 참조 모션을 그대로 추종하도록 하는 피드포워드 제어 신호에, RL 에이전트가 모델 불일치나 외란을 보정하는 미세 조정 신호만 더하게 함으로써, 순수 RL에 비해 훈련이 용이하고 수렴이 빠르며, 실제 로봇 적용 시에도 균형 유지 및 외란 견디기 성능이 향상되었습니다. 실제로 시뮬레이션에서는 임의의 힘으로 로봇을 밀치는 등의 도메인 랜덤화를 적용해도 정책이 잘 작동했고, 이를 통해 모델링 오차나 환경 불확실성에 대한 견디기 능력을 강화하여 실험실 밖 실세계 조건에서도 정책이 유효함을 검증했습니다. 이러한 강인성은 모델 기반 최적화로 생성된 물리적으로 타당한 참조가 있었기에 가능했으며, 결과적으로 다른 방법보다 높은 안정성으로 실제 로봇에 적용할 수 있었습니다.\n학습 전략과 구조적 설계 또한 STMR의 중요한 강점입니다. 이 방법은 오프라인 최적화(모션 리타게팅) 단계와 온라인 학습(정책 RL) 단계를 분리하여 효율을 높였습니다. 먼저 비지도 최적화를 통해 참조 모션을 사전에 보정해 둠으로써, RL 단계에서는 복잡한 동적 제약을 만족하는 모션을 이미 확보한 상태에서 학습이 시작됩니다. 이로써 RL은 보다 용이한 보상 설계(이미 물리적 타당성 확보)와 향상된 모방 효율을 보이며, 실제 실험에서도 기존 기법 대비 적은 학습으로 높은 정확도 추종을 달성했습니다. 예를 들어, 본 연구의 RL 정책은 약 5천만 스텝(약 1시간)의 훈련으로 높은 성능을 보였으며, 기존 AMP 기반 정책이 수배 더 많은 학습이 필요했던 것에 비하면 효율적입니다. 또한 공간-시간 단계별로 문제를 디컴포즈한 설계는 복잡한 모션 리타게팅 문제를 해결하기 쉽게 만든 구조적 이점이 있습니다. 각각의 단계에서 기성의 알고리즘(예: IK 기반 Unit Vector 방법, DDP 기반 최적화)을 활용하여 신뢰성을 확보했고, 최종적으로 RL 제어기를 결합하는 모듈식 설계로서 각 구성요소의 역할이 명확합니다. 이러한 구조 덕분에 새 동작 추가나 다른 로봇 적용 시에도 일부 모듈 교체/재학습으로 대응 가능하며, 실시간 적용을 위한 향후 확장도 모색하기 용이한 프레임워크라고 볼 수 있습니다. 정리하면, STMR의 학습전략은 최적화된 참조로 학습 가이드를 제공하고, 아키텍처 설계는 문제 복잡도 감소와 모듈화를 통해 효율성과 확장성을 모두 갖춘 점이 큰 강점입니다."
  },
  {
    "objectID": "posts/paper/2025-07-29-stmr.html#잠재적-약점-제한된-상황-복잡도-및-기타-고려사항",
    "href": "posts/paper/2025-07-29-stmr.html#잠재적-약점-제한된-상황-복잡도-및-기타-고려사항",
    "title": "📃SMTR 리뷰",
    "section": "잠재적 약점: 제한된 상황, 복잡도 및 기타 고려사항",
    "text": "잠재적 약점: 제한된 상황, 복잡도 및 기타 고려사항\n한편 이 연구에는 몇 가지 약점이나 제한점도 존재합니다. 첫째로, 접촉 추정에 대한 의존성이 높다는 한계가 지적됩니다. STMR은 영상 등으로부터 얻은 베이스 움직임이 없는 순수 키포인트 데이터로도 모션을 재구성할 수 있다는 장점을 보여주었지만, 이를 위해서는 발의 지면 접촉 여부(접촉 스케줄)를 정확히 판단해야 합니다. 저자들에 따르면, 접촉 판정의 오류가 있을 경우 비현실적인 모션(발이 갑자기 통과하거나 붕 뜨는 등)이 생성되어, 이후 전체 모션 최적화 과정에 불규칙한 동작이 전파될 위험이 있습니다. 실제로 키포인트 속도를 임계값으로 임의 thresholding 하여 접촉 여부를 결정할 경우, 연속된 프레임에서 접촉단계가 불연속적으로 바뀌는 문제가 발생할 수 있고 이로 인해 로봇이 따라갈 수 없는 빠른 접촉 전환이 나타나 모방 실패로 이어질 수 있다고 지적합니다. 이를 완화하기 위해 저역 통과 필터(low-pass filter)로 접촉 신호를 평활화하였으나, 근본적으로 신뢰성 있는 접촉 추정이 보장되지 않으면 본 기법의 성능이 저하될 수 있다는 한계가 남습니다. 이러한 점은 추후 접촉 오차에 대한 강인성 향상이나 학습을 통한 접촉 추정 보정 등의 추가 연구가 필요함을 의미합니다.\n둘째, 계산 복잡도와 실시간성의 측면에서 제한이 있습니다. STMR은 모션 하나를 리타게팅하기 위해 두 단계의 최적화(운동학 IK 및 동적 최적화)와 이후 강화학습 정책 훈련까지 거치는 비용 높은 파이프라인입니다. 논문에서 명시하진 않았지만, 각 모션마다 이러한 과정을 수행해야 하므로 새로운 동작마다 상당한 오프라인 계산과 학습 시간이 필요합니다. 즉, 본 방법은 사전 준비 단계가 긴 편이며, 사람 시연을 즉석에서 바로 로봇에 모사시키는 실시간 리타게팅에는 부적합합니다. 반면 일부 기존 연구는 사전 학습된 모델이나 모션 매핑 함수를 이용하여 실시간 온라인 리타게팅을 지향하기도 하는데, STMR은 정확성과 물리적 타당성을 추구하는 대신 속도를 양보한 접근으로 볼 수 있습니다. 이로 인해 실시간 상호작용 시나리오 (예: 사람이 즉흥적으로 시범을 보이는 것을 로봇이 바로 따라하는 경우)에는 사용하기 어렵고, 오프라인 모션 디자인 후 배치하는 용도에 국한될 수 있습니다. 저자들도 향후 실시간 제어 및 인지 모듈과의 통합을 언급하며, 온라인 적용 가능성은 추후 연구과제로 남겨두고 있습니다.\n셋째, 범용성의 범위에 대한 한계입니다. 본 연구는 사족보행 로봇의 동물 모션 모방에 초점을 맞추었으며, 이에 최적화된 키포인트 정의(엉덩이, 허벅지, 무릎, 발 등 16개)와 제약 조건(네 발 접지 등)을 사용했습니다. 따라서 휴머노이드나 비(非)사족형 로봇 등에 동일 기법을 적용하려면 추가적인 고려가 필요합니다. 예를 들어, 인간형 로봇의 팔 동작이나 두 발 보행에는 상이한 키포인트와 접촉 방식이 있으므로, STMR의 공간적/시간적 최적화 요소를 해당 플랫폼에 맞게 조정해야 합니다. 논문에서도 STMR 개념을 다른 형태의 로봇(예: 휴머노이드)으로 확장 적용하는 방향을 제시하고 있으나, 아직 그러한 실증은 이루어지지 않았습니다. 따라서 현재로서는 사족 보행 분야에 국한된 솔루션이며, 완전히 다른 모폴로지에 대한 범용적 보장은 미지수입니다. 또한 시험된 동작들도 달리기/점프 등 보행 동작에 한정되어 있어, 이를 넘어선 복잡한 상체 동작이나 환경과의 상호작용이 많은 동작(예: 도약하여 착지, 물체 넘기기 등)에 대해서도 효과적일지는 추가 검증이 필요합니다.\n마지막으로, 모션 프라이어 vs 물리적 타당성에 관한 논의에서 오는 시사점도 있습니다. 저자들은 기존의 Adversarial Motion Prior (AMP)와 같은 기법이 물리적으로 불가능한 참조 동작도 모방 가능하게 해주는 장점이 있지만, 결국 참조 모션 자체의 물리적 불일치로 인해 최종 성능이 제한됨을 지적합니다. STMR의 결과가 AMP 대비 월등한 추종 정확도를 보인 것은, 물리적으로 일관된 참조를 쓰는 것이 학습 안정성과 성능에 중요함을 보여줍니다. 다만 AMP와 같은 기법은 스타일 상의 일반화나 데이터베이스 범위 밖 움직임 생성에 강점이 있으므로, STMR처럼 엄격한 물리 최적화 vs 모션 프라이어 기반 유연성 사이에는 트레이드오프가 존재합니다. 이 논문에서는 물리적 타당성을 택했지만, 향후 두 접근의 결합(예: 물리 제약을 지키면서도 모션 프라이어를 활용한 풍부한 모션 생성)도 가능할 것이며, 이는 본 연구가 직접 다루지는 않았지만 앞으로 탐구해볼 여지가 있는 부분입니다."
  },
  {
    "objectID": "posts/paper/2025-07-29-stmr.html#실험-결과의-타당성과-의미-성능-검증-및-비교-분석",
    "href": "posts/paper/2025-07-29-stmr.html#실험-결과의-타당성과-의미-성능-검증-및-비교-분석",
    "title": "📃SMTR 리뷰",
    "section": "실험 결과의 타당성과 의미: 성능 검증 및 비교 분석",
    "text": "실험 결과의 타당성과 의미: 성능 검증 및 비교 분석\n논문의 실험 결과는 제안한 방법의 유효성을 뒷받침하며, 여러 성능 지표에서의 향상을 명확히 보여줍니다. 저자들은 6가지 동물 모션(트롯 종류 2개, 페이스 2개, 옆걸음, 점프-회전 등)을 세 로봇에 걸쳐 실험하고, 3가지 기존 모방학습 기법(DeepMimic, AMP, OptMimic)을 baseline으로 선정하여 비교했습니다. 모션 추종 정확도는 키포인트 경로의 DTW(dynamic time warping) L1 거리로 측정되었는데, STMR 기반 정책은 모든 동작에 걸쳐 가장 낮은 오차를 기록했습니다. 평균적으로 STMR의 추종 오차는 다른 기법들에 비해 크게 감소하였으며, 특히 난이도가 높은 동작일수록 그 격차가 더 벌어지는 경향을 보였습니다. 예를 들어, 가장 복잡한 “HopTurn” (제자리 점프 회전) 동작에서 STMR의 오차는 타 기법 대비 현저히 낮아, 어려운 공중 동작에서도 제안 방법이 우수한 성능을 발휘함을 보여주었습니다. 저자들은 이러한 결과를 통해 물리적으로 정제된 참조 모션을 사용한 것이 학습 성능 향상에 핵심임을 강조합니다. 키프레임 간의 시간 스케일을 늘리거나 줄이는 TMR 단계의 존재가, 특히 비행_phase가 있는 동작에서 로봇 크기에 맞는 체공시간을 부여하여 모방 성공률을 높였음을 확인한 것입니다.\n또 다른 중요한 지표로 발 미끄러짐(foot sliding)과 접촉 일정 보존이 있습니다. 발 미끄러짐은 발이 지면에 닿은 채로 미끄러지는 정도를 나타내는 문제로, 리타게팅 과정에서 운동학적 불일치가 있을 때 흔히 발생합니다. 제안된 STMR의 공간 최적화(SMR) 단계에서 발끝의 속도와 위치 제약을 걸어주었기에, 결과 모션에서는 발이 지면에 닿아있는 동안 거의 움직이지 않게 되었습니다. 표에서 알 수 있듯이, 기존 단순 스케일링 방법(단위 벡터 UV 방법)의 경우 발 미끄러짐 지표가 매우 높았지만, STMR을 거친 모션에서는 동일 상황에서 오차가 0에 수렴할 정도로 미끄러짐이 제거되었습니다 (예: AlienGo 로봇의 HopTurn 동작에서 UV 대비 STMR의 발 이동량이 44.11→1.83mm로 대폭 감소). 동시에 접촉 스케줄 보존도 크게 향상되었습니다. 이는 원본 동작과 리타게팅된 동작의 발 지면 접촉 on/off 시퀀스가 얼마나 일치하는지를 IoU(교집합/합집합)로 측정한 것인데, STMR 결과 모션은 IoU ≈ 1.0에 가까워 거의 완벽히 동일한 접촉 패턴을 구현했습니다. 반면 기본 UV 방법의 경우 IoU가 평균 0.5 이하로 떨어져 접촉 타이밍이 어긋나는 경우가 많았습니다. 이 결과는 STMR이 원본 동작의 의미적 맥락(언제 발을 디디고 떼는지)을 충실히 살려냈음을 뜻하며, 단순히 모션을 비슷하게 흉내내는 데 그치지 않고 동작의 의도와 접촉 상호작용까지 보존하는 정교한 리타게팅이 이루어졌음을 보여줍니다. 요약하면, 정량적 성능 지표들(추종 오차, 발 미끄러짐, 접촉 일정 등)에서 제안 방법은 모든 기준선보다 우수했으며, 특히 동적이고 난이도 높은 동작일수록 그 이점이 두드러지는 유의미한 결과를 얻었습니다.\n비교 기법 및 분석을 살펴보면, 각 baseline과의 대비를 통해 STMR 구성 요소들의 효과를 확인할 수 있습니다. DeepMimic(Peng 등, 2018)는 별도의 모션 최적화 없이 원본 모션을 바로 RL로 추종하도록 한 경우인데, 이때 참조로 사용된 모션은 UV 스케일링으로 얻은 운동학적 변환만 거친 모션입니다. 그 결과 발 미끄러짐이나 물리 불일치가 교정되지 않아 RL 정책이 불안정한 동작을 보였고, 특히 도약 동작에서 실패하거나 큰 오차를 내는 등 한계를 보였습니다. 이는 SMR/TMR 단계 없이 imitation learning만으로는 한계가 있다는 점을 뒷받침하며, STMR 대비 추종 오차가 크게 높게 나타났습니다. AMP(Adversarial Motion Prior) 기법은 학습된 모션 판별자를 통해 에이전트의 모션이 데모의 분포와 유사하도록 보상 신호를 주는 접근인데, 물리적으로 infeasible한 참조 동작도 어느 정도 모방이 가능하다는 장점이 있습니다. 본 논문 실험에서 AMP 기반 정책은 DeepMimic보다는 개선된 추종을 보였으나, STMR만큼 정확한 추종에는 이르지 못했고 특히 빠른 동작 전환이 필요한 경우 정확도가 떨어지는 경향을 보였습니다. 저자들은 STMR 대비 AMP의 성능 열세를 통해, 참조 모션의 물리적 일관성이 모방 학습에 필수적임을 강조합니다. 끝으로, OptMimic(Fuchioka 등, 2023) 기법은 물리 기반 최적화로 참조 모션을 개선한 후 RL을 하는 방식으로 STMR과 유사하지만, 시간적 스케일 조정이 없는 점이 다릅니다. 실험 결과 OptMimic은 DeepMimic보다는 낮은 오차를 내었지만, STMR처럼 로봇 크기에 따른 동작 속도 조절이 안 되다 보니 일부 동작에서는 추종 오차가 여전히 크게 발생했습니다. 예컨대 점프 후 착지 타이밍을 원본과 동일하게 가져가다 보니 로봇 크기가 달라 생기는 불균형을 완전히 해소하지 못한 것으로 볼 수 있습니다. 이러한 비교를 통해, STMR의 공간+시간 이중 최적화 전략이 개별 구성요소들(SMR만 또는 TMR만)보다 뛰어난 성능을 낸다는 점을 확인했습니다. 더불어 영상 기반 모션 추출 실험에서는, 손으로 들고 찍은 동물 영상에서 얻은 절대 위치 없는 상대적인 관절 움직임만으로도 STMR이 전체 몸체 움직임을 재구성하여 모션을 만들 수 있음을 보여주었습니다. 이는 기존 방법들이 전제하는 ‘전신 모션 캡처 데이터’ 없이도 보다 폭넓은 소스에서 모션을 가져올 수 있다는 점에서 의의가 있습니다. 결국 실험 전반을 통해, 제안한 STMR 방법은 모션 리타게팅의 품질과 이를 활용한 모방 제어 정책의 성능 양면에서 유의미한 개선을 이루었고, 제시된 기법의 실용적 가치(시뮬레이션 및 실제 로봇에서의 성공)를 입증했습니다."
  },
  {
    "objectID": "posts/paper/2025-07-29-stmr.html#논문-기획-및-서술의-명확성-재현-가능성-추가적-제한사항",
    "href": "posts/paper/2025-07-29-stmr.html#논문-기획-및-서술의-명확성-재현-가능성-추가적-제한사항",
    "title": "📃SMTR 리뷰",
    "section": "논문 기획 및 서술의 명확성, 재현 가능성, 추가적 제한사항",
    "text": "논문 기획 및 서술의 명확성, 재현 가능성, 추가적 제한사항\n이 논문은 전반적으로 명확하고 체계적인 구성으로 작성되어 있어 읽는 이로 하여금 기법과 결과를 이해하기 쉽게 합니다. 서두에서 문제의식(동물 모션의 형태 차이, 기존 기법 한계)을 분명히 제시하고, 이를 해결하기 위한 핵심 아이디어(STMR)를 도식과 함께 직관적으로 설명하였습니다. 또한 관련 연구를 모션 모방과 모션 리타게팅 두 범주로 나누어 고찰함으로써, 본 연구가 어떤 지점에서 기여하는지 맥락을 분명히 했습니다. 방법론 부분에서는 수학적 공식과 알고리즘 절차를 상세히 기술하여, 제안 기법의 구현과 작동 방식을 이해할 수 있도록 했습니다. 예컨대, 공간적 리타게팅에 쓰인 Unit Vector IK 알고리즘과 시간적 최적화에 사용된 DDP 기반 최적화를 별도의 전제(section)에서 미리 설명하고, 이후 STMR 구성에 그것들이 어떻게 활용되는지 자연스럽게 연결했습니다. 이러한 프레젠테이션의 논리적 흐름 덕분에, 독자는 제안 기법이 기존 기술의 어떤 부분을 차용하고 어디서 새로움을 제공하는지 명확히 파악할 수 있습니다.\n실험 설계와 결과 제시 역시 투명하게 이루어졌습니다. 비교 대상으로 삼은 각 기법의 설정(예: DeepMimic의 보상 설계, AMP의 판별기 구성, OptMimic의 최적화 세부사항 등)도 논문에 서술하고 있어, 재현 가능성을 높였습니다. 또한 정량적 결과를 테이블과 도표로 제공하고, 표준편차 등 통계도 기재하여 결과의 신뢰도를 보여줍니다. 특히 다양한 평가 지표(추종 오차, 정규화 오차, 발 미끄러짐, IoU 등)를 통해 여러 각도에서 성능을 평가한 점은, 제안 방법의 다방면 성능 우위를 뒷받침함과 동시에 독자가 성능의 의미를 종합적으로 판단하는 데 도움을 줍니다. 저자들은 실험 동영상도 웹에 공개하여 누구나 결과를 직접 확인할 수 있게 했고, 소스 코드 역시 깃허브에 공개되어 있어 재현성과 실용적 활용 가능성을 높였습니다. 이러한 노력은 연구 공동체가 해당 기법을 따라 구현하거나 응용 연구를 진행하는 데 큰 도움을 줄 것으로 보입니다.\n논문에서 한계 및 향후 개선점에 대한 언급도 찾아볼 수 있습니다. 마지막 부분에 Limitation 섹션을 두어, 접촉 추정의 불완전성으로 인한 문제와 그에 대한 임시 조치(저역필터 적용)를 솔직하게 기술하였고, 이는 앞서 지적한 바와 같이 본 기법의 현실적 제약을 독자가 이해하도록 돕습니다. 또한 미래 연구 방향으로, STMR을 휴머노이드 등 다른 형태의 로봇에 일반화하거나, 실시간 제어와 결합하는 방안을 제시하여, 해당 분야 발전에 대한 통찰을 제공하고 있습니다. 다만, 본문에서 직접 다루지 않은 몇 가지 잠재적 제한도 생각해볼 수 있습니다. 예를 들어, 환경과의 상호작용이 있는 시나리오(물체를 밀면서 걷는다든지)에 대해서는 접촉 대상이 지면만 있는 현재 제약으로 충분하지 않을 수 있습니다. 또한 복잡한 다관절 시스템에서 최적화가 지역해(local optimum)에 빠질 위험이나, 동적 최적화의 계산 시간 문제도 존재할 수 있지만 논문에서는 크게 다뤄지지 않았습니다. 그럼에도, 이러한 부분들은 해당 연구의 범위를 벗어나는 내용이기에 언급하지 않은 것으로 보이며, 전반적으로 논문의 서술과 정보 제공은 충실하고 명확하다고 평가됩니다. 재현성 측면에서도 공개된 코드와 상세한 실험 파라미터 기술이 뒷받침하고 있으므로, 연구자들이 후속 연구를 이어가기 용이할 것입니다."
  },
  {
    "objectID": "posts/paper/2025-07-29-stmr.html#손-로봇-분야로의-기술-확장-시공간-리타게팅의-응용",
    "href": "posts/paper/2025-07-29-stmr.html#손-로봇-분야로의-기술-확장-시공간-리타게팅의-응용",
    "title": "📃SMTR 리뷰",
    "section": "손 로봇 분야로의 기술 확장: 시공간 리타게팅의 응용",
    "text": "손 로봇 분야로의 기술 확장: 시공간 리타게팅의 응용\n본 논문의 STMR에 담긴 모션 리타게팅 기법과 시간적 동작 조정, 운동학적 제약 처리 기술은 다관절 로봇 손의 동작 모방 및 학습에도 유용하게 응용될 수 있습니다. 사람의 섬세한 손동작을 로봇 손으로 전달하거나, 인간의 조작 기술을 로봇이 학습하도록 하는 문제는 사족 보행의 모션 모방과 유사하면서도 고유한 도전과제를 가집니다. 이하에서는 (a) 인간 손의 자세 및 동작을 로봇 손으로 리타게팅하는 경우와 (b) 그런 리타게팅 모션을 활용해 로봇 손이 물체 조작을 학습/모방하는 경우로 나누어, 해당 논문의 기법을 어떻게 적용하거나 변형할 수 있을지 논의합니다.\n\n인간 손 동작의 로봇 손으로 리타게팅 (자세 및 궤적 매핑)\n(1) SMR\n사람 손의 복잡한 관절 움직임을 로봇 손에 대응시키는 리타게팅에는 STMR의 공간적 모션 리타게팅(SMR) 개념이 핵심적으로 활용될 수 있습니다. 사람 손과 로봇 손은 형태학적 차이(모양, 크기, 관절 자유도 수) 때문에 직접적인 1:1 대응이 어려운데, 이는 사족 동물과 로봇의 다리 구조 차이와 유사한 문제입니다. STMR이 제시한 “키포인트-기반 운동학적 매핑”은 손의 경우에도 적용 가능합니다. 예를 들어, 인간 손의 관절 마디 위치나 손가락 끝점(fingertip) 등을 키포인트로 정의하고, 로봇 손의 대응 키포인트와 방향 단위 벡터를 맞추는 방식으로 초기 자세를 변환할 수 있습니다. 이는 논문에서 SMR 단계에 사용한 Unit Vector 방법과 유사하게, 인접 마디 간 방향을 보존하도록 로봇 손가락 관절을 설정하는 방식입니다. 이렇게 하면 로봇 손이 인간 손의 손가락 뻗는 방향, 구부리는 각도 비율 등을 크게 벗어나지 않게 되어 자연스러운 초기 모션을 얻을 수 있습니다. 이후, 역기구학(IK) 풀이를 통해 로봇 손의 각 관절 각도를 찾아내면, 일단 운동학적으로 유효한 로봇 손 자세 시퀀스를 얻어낼 수 있을 것입니다. 이 과정에서 로봇 손의 관절 가동 범위 제한이나 자기 충돌(손가락 끼리 겹치지 않도록) 등의 운동학적 제약을 포함시키면, 마치 STMR의 SMR이 발 관통을 방지하듯이 손가락 관통이나 비현실적 꺾임을 방지할 수 있습니다.\n또한 접촉 관련 제약도 중요한데, 이는 뒤에서 다룰 조작 시나리오와 겹치는 부분이 있지만, 기본적으로 사람 손의 접촉 의도(예: 어느 손가락이 어떤 지점에서 물체를 터치하는지)를 파악하여 로봇 손에서도 해당 손가락(또는 대응 손가락)이 비슷한 시점에 비슷한 위치를 접촉하도록 매핑해야 합니다. 이를 위해 손가락 끝점을 키포인트로 두고, 인간 손에서 접촉했던 시공간 정보를 로봇 손 끝점의 위치/속도 제약으로 활용하면, STMR의 접촉 스케줄 보존 개념을 구현할 수 있습니다. 예컨대, 사람 검지 손가락이 1초 시점에 물체 표면을 누르는 동작이 있었다면, 로봇 손에서도 대응되는 손가락(혹은 로봇의 검지에 해당하는 링크)이 그 시점에 그 위치를 누르도록 목표를 설정하는 식입니다. 이처럼 공간적 리타게팅 단계를 거치면, 비록 로봇 손의 형태가 달라도 인간 손 동작의 형태적 특징과 접촉 의도를 최대한 유지한 전체 손가락 궤적을 생성할 수 있습니다.\n(2) TMR\n다음으로, 시간적 조정(TMR) 개념을 손 동작에 적용할 수 있습니다. 인간과 로봇 손은 관절 동작 속도, 토크 능력 등이 다르기 때문에, 동일한 시간 프로파일로 움직이면 로봇이 따라가지 못하거나 과도한 힘을 내는 문제가 발생할 수 있습니다. STMR의 TMR 단계처럼, 모션의 시간축을 늘이거나 줄여서 로봇 손에 동역학적으로 적합한 속도와 가속도를 찾는 최적화를 수행할 수 있습니다. 예를 들어, 사람은 순식간에 손가락을 펼쳐 물체를 놓을 수 있지만 로봇 손은 관절 속도가 한계가 있을 때, 놓는 동작의 시간을 늘려 로봇이 충분히 따라할 수 있도록 합니다. 반대로, 사람 손이 천천히 움직이는 동작이라도 로봇에 매우 가벼운 부하라면 시간을 압축해도 되겠죠. 이러한 시간 최적화는 단순히 속도의 문제뿐 아니라, 동역학적 일관성과 접촉 안정성을 고려해야 합니다. 예를 들어 물체를 쥐었다 놓는 시나리오에서, 로봇 손의 각 손가락이 동시에 물체를 이격해야 물체가 예상대로 떨어지지, 타이밍이 어긋나면 물체가 미끄러질 수 있습니다. 따라서 손가락들 간의 타이밍 조율, 그리고 중력 및 물체 관성에 대응한 속도 조정 등이 포함된 시간적 파라미터 최적화가 필요합니다. 이를 위해 STMR에서 활용한 방식처럼 모델 기반 시뮬레이션을 내부에 두고, 로봇 손+물체의 물리를 고려한 최적 제어(예: DDP)를 수행할 수 있습니다. 실제 논문에서는 로봇의 도약 높이와 체공시간을 DDP로 조정했듯이, 손의 경우 물체가 들렸다 놓이는 동작을 시뮬레이션하면서 손가락 접촉력, 마찰 조건 등을 만족하는 시간 스케일을 찾는 것입니다. 이러한 동적 리타게팅을 통해, 결과 모션은 로봇 손에게 과도한 힘이나 충돌 없이 실행 가능한 물리적으로 타당한 조작 모션이 될 것입니다. 기존 연구에서도 인간→로봇 손 리타게팅 시 단순 위치 매핑은 관통 및 불안정한 접촉을 초래하기 쉽고, 이를 개선하려 에너지 함수 기반 최적화로 관통을 줄였지만 인간 동작의 풍부한 제약(손가락 간 협조 움직임 등)을 충분히 반영하지 못한 문제가 지적됩니다. STMR의 접근은 인간 손의 풍부한 kinematic 제약을 보존하면서도 물리적 안정성까지 확보하도록 두 단계 최적화를 제안하므로, 로봇 손 리타게팅에서도 이러한 일련의 공간-시간 분리 접근이 효과적일 것으로 기대됩니다.\n이 과정에서 모션 프라이어의 통합도 고려할 수 있습니다. 사족 보행의 경우 AMP 같은 방법이 참고될 수 있었던 것처럼, 손 동작의 경우도 인간 손 움직임에 대한 사전 모델을 활용하면 도움이 됩니다. 예를 들어 인간 그립(grasp) 동작의 통계적 분포나 손가락 사이의 공조 움직임(예: 손가락의 synergy)에 대한 모션 프라이어를 사용하면, 로봇 손이 더 자연스러운 자세를 취하도록 유도할 수 있습니다. STMR에서는 명시적으로 모션 프라이어를 쓰지 않았지만, 손 로봇에 적용 시에는 이러한 인간 손의 선험적 지식(예: 대부분의 그립에서 새끼손가락은 약간 굽혀진다든지)을 최적화 과정에 반영하면 리타게팅 품질을 향상시킬 여지가 있습니다. 요컨대, 공간적 정합 + 시간적 조정 + (필요시) 모션 프라이어라는 세 가지 요소를 조합하는 것은 복잡한 로봇 손 동작 리타게팅에서도 핵심 원리로 적용될 수 있습니다.\n\n\n시공간 모션 전이를 통한 조작 행동 학습 (모방 및 제어)\n다음으로, 이렇게 리타게팅된 인간-로봇 손의 모션 데이터를 활용하여 로봇 손의 조작 행동을 학습시키는 방안을 STMR의 기법에 비추어 논의합니다. 기본 아이디어는 STMR이 물리적으로 실행 가능한 참조 모션을 만들어 RL 정책 학습을 가이드한 것처럼, 로봇 손의 경우에도 인간 시연을 로봇 버전으로 변환한 “참조 조작 모션”을 만들어 이를 학습에 활용하는 것입니다. 사람의 조작은 섬세하고 빠른 경우가 많아, 로봇이 이를 그대로 따라하려 하면 동역학적 실패(물체를 놓쳐버리거나 떨어뜨림, 충돌 등)로 이어질 수 있습니다. 따라서 앞서 언급한 리타게팅 단계를 거쳐 로봇이 따라할 수 있는 수준으로 변환된 조작 시나리오를 준비하면, 그 다음 강화학습이나 모방 학습으로 정책을 훈련시키는 것이 훨씬 수월해집니다.\n예를 들어, 인간이 공을 집어 다른 곳에 놓는 시범 영상을 생각해봅시다. 이를 로봇 손에 모방시키고자 할 때, 먼저 공을 집는 동작의 손가락 궤적과 접촉 타이밍을 STMR 유사 방식으로 변환해 둡니다. 이렇게 얻은 로봇 손의 기준 궤적은 공을 안정적으로 잡고 옮기기에 적합하도록 시간과 공간이 조율되어 있겠지요. 이제 RL의 보상 함수를 이 참조 궤적과의 유사도로 설계하여 로봇이 이 동작을 익히게 할 수 있습니다. 구체적으로, STMR 논문에서와 마찬가지로 관절 각도, 손목(기저) 위치와 자세, 손가락 키포인트 위치 등이 참조 모션과 가까울수록 높은 보상을 주는 식입니다. 또한 접촉 유지/해제의 시점도 보상에 넣어, 예를 들어 공을 놓을 때 너무 늦게 손가락을 펴면 벌점을 주고, 참조와 같은 타이밍에 펴면 보상을 주는 형태로 설계할 수 있습니다. 이런 식의 세밀한 모방 보상 설계는 이미 참조 모션이 물리적으로 타당하기 때문에 가능해집니다. 만약 참조 모션이 부자연스럽거나 물리를 무시했다면 (예: 사람 손 동작을 그대로 써서 로봇 손엔 무리인 속도로 손가락을 편다든지), RL 단계에서 에이전트가 따라하려다 실패하거나 학습이 불안정해질 것입니다. STMR이 강조한 바와 같이, 물리적으로 일관된 레퍼런스가 있을 때 RL 모방 학습이 원활해지므로, 로봇 손의 조작 학습에서도 이 원칙이 동일하게 적용됩니다.\nResidual Learning 개념 역시 로봇 손 제어에 응용할 수 있습니다. 예를 들어, 로봇 손에 기본 오픈루프 제어기를 내장하여, 앞서 얻은 참조 궤적을 따라 손가락 관절에 PD 제어로 힘을 가하는 베이스를 깔아둡니다. 그런 다음, RL로 학습된 잔여 정책이 미세 조정 (예: “약간 더 강하게 쥐기”나 “마찰 증가를 위해 각도 보정” 등)을 출力하도록 합니다. 이는 STMR에서 참조 joint 값 + RL 출력 = 최종 토크 명령으로 구성한 것과 같은 방식입니다. 이렇게 하면 로봇 손이 물체를 쥐고 이동하는 동안, 기본 제어기는 참조 궤적대로 손가락을 움직이려 하고, RL 정책은 물체의 미끄러짐을 막기 위해 필요한 추가 힘이나 모델 오차 보정 동작을 학습하게 됩니다. 결과적으로, 처음부터 모든 것을 학습시키는 것보다 안정적이고 샘플 효율적인 학습이 가능하며, 실환경에서 예기치 않은 상황(물체 무게 변화, 마찰 계수 불확실성 등)에도 더 견고한 제어 정책을 얻을 수 있습니다. 실제 STMR 실험에서도 잔여 학습 기법이 외란에 강한 정책을 만들어냈고, 로봇 손의 미끄럼 방지나 확률적 성공률 향상에도 같은 이점을 기대할 수 있습니다.\n또 하나 고려할 점은, 환경(대상 물체)과의 상호작용 자체를 학습하는 부분입니다. 사족 로봇의 경우 지면은 고정되어 있고 발이 밀리지 않게 하는 것이 주 목표였지만, 로봇 손은 잡은 물체를 안정적으로 다루는 것이 핵심입니다. 따라서 RL 보상에는 물체의 상태 안정성(예: 물체의 목표 위치 도달 여부, 잡는 동안 떨어뜨리지 않기 등)을 포함해야 하고, 모션 리타게팅 단계에서도 물체에 가해지는 힘을 고려하는 것이 좋습니다. 이를테면, TMR 단계에서 물체의 동역학까지 포함된 시뮬레이션으로 로봇 손가락 궤적을 최적화하면, 단순히 손가락 움직임뿐 아니라 물체가 미끄러지지 않는 경로를 찾게 될 것입니다. 이와 관련하여, 최근 연구들은 사람-로봇 손 동작 매핑 시 접촉력과 상호작용까지 모사하려는 시도를 하고 있습니다. 예를 들어, 접촉맵(contact map) 생성이나 이중 임계값 기반 접촉 검출 및 프레임간 접촉 상태 스무딩 같은 기법이 보고되는데, 이는 STMR에서 발 접촉을 부드럽게 처리한 것과 상통합니다. DexFlow 등 기존 연구에서는 이러한 시간적 접촉 추적 및 스무딩을 통해 손-물체 간 접촉 불안정(따닥거림)을 해소했다고 합니다. 로봇 손의 조작 모방에서도, 참조 모션 생성 시 접촉 사건을 안정적으로 추출 및 매핑하고, RL 학습 시에도 접촉 유지/이탈 여부를 보상으로 다루어 접촉 패턴을 제대로 모방하도록 해야 합니다. STMR에서 발바닥 접촉 IoU로 평가를 했듯이, 손에서는 물체와 손가락의 접촉 IoU나 미끄럼 정도 등을 사용해 성능을 평가하고 최적화 방향을 설정할 수 있을 것입니다.\n정리하면, STMR의 “참조 모션 생성 + RL 정책 학습” 투-스텝 전략은 로봇 손의 복잡한 조작 학습에도 유용한 청사진을 제공합니다. 사람의 시연 동작을 운동학적으로 투영하고 동역학적으로 재조정하여 로봇이 따라하기에 적합한 시공간 모션을 얻은 뒤, 이를 모방 목표로 삼아 정책을 학습함으로써, 단순 모방보다 안정적이고 정확한 조작 정책을 얻을 수 있습니다. 이때 잔여 학습을 포함한 제어 구조를 도입하면 학습된 정책의 실제 적용성을 높일 수 있습니다. 나아가, 필요하다면 모션 프라이어나 전문가 시演 데이터를 통합하여, 사람 손의 고차원 모션에서도 스타일과 물리 타당성을 모두 살린 학습을 추구할 수 있습니다. 이런 접근은 테이블 위의 섬세한 조립 작업, 수술용 로봇의 손 동작 모방, 가상현실을 통한 로봇 손 원격 조작 등 다양한 응용 분야에서 활용될 수 있을 것입니다. 실제로 실시간 원격조작(teleoperation)을 위한 연구들은 속도를 중시해 단순 매핑을 쓰는 경향이 있지만, 이는 미세 작업 시 정밀도 저하를 초래하기도 합니다. STMR에서 영감을 얻은 기법은 오프라인에서 고품질의 모션 사전학습을 통해 이러한 한계를 극복하고, 이후 실시간 제어에 접목하는 형태로도 발전할 수 있습니다.\n\n\n손 로봇 적용을 위한 핵심 기법 요약\n아래는 본 논문의 주요 기법들과 그 로봇 손 분야로의 전이 가능성을 정리한 표입니다. 이 표에서는 STMR에서 제안된 요소들이 로봇 손의 자세 리타게팅 및 조작 행동 학습에 어떻게 활용될 수 있는지 대비하여 나타내었습니다.\n\n\n\n\n\n\n\nSTMR의 핵심 기법\n로봇 손 응용 방안\n\n\n\n\n공간 모션 리타게팅 (SMR)- 키포인트 기반 자세 매핑- 운동학적 제약 적용\n사람 손과 로봇 손의 골격 구조 차이를 극복하기 위해, 손가락 관절 마디와 끝점을 키포인트로 정의하여 방향 벡터 보존 IK로 초기 자세 변환. 로봇 손의 관절 가동 범위와 자기 충돌 제약을 포함해 운동학적으로 실행 가능한 포즈 생성.\n\n\n시간 모션 리타게팅 (TMR)- 모션 시퀀스의 시간 스케일 최적화- 모델 기반 동적 검증\n로봇 손의 속도/가속도 한계와 물체 동역학을 고려하여, 인간 동작의 타이밍을 조정. DDP 등 최적제어 기법으로 손-물체 상호작용을 시뮬레이션하면서 손가락 움직임의 속도를 늘이거나 줄여 동역학적으로 안정적인 궤적 생성. 예) 무거운 물체를 들 때 더 천천히 들어올려 미끄러짐 방지.\n\n\n운동학/동역학 제약 통합- 접촉 유지 및 관통 방지- 관절 한계 및 안정성 고려\n인간 손 시연의 접촉 패턴(어느 손가락이 언제 물체를 잡고 놓는지)을 분석하여 로봇 손에서도 동일한 접촉 시퀀스를 따르도록 제약 설정. 손가락 끝의 미끄러짐 거리 최소화, 관통 방지 에너지 등을 최적화 목표에 포함시켜 물리적으로 타당한 그립 구현.\n\n\n잔여 정책 학습 (Residual RL)- 기준 모션 + 보정 액션- PD 제어기 기반 안정화\n리타게팅된 손 모션을 피드포워드 참조로 사용하고, RL 에이전트는 보정용 미세 제스처만 학습. 예를 들어, 참조 궤적을 따라가는 PD 제어를 기본으로, RL 정책이 물체가 미끄러지지 않도록 추가 힘을 보태는 구조. 이를 통해 학습 안정성 및 실환경 강건성 확보.\n\n\n모션 프라이어 및 스타일 보존- (Baseline: AMP)- 인간 고유 손동작의 자연스러움 유지\n필요한 경우 인간 손동작 데이터베이스로부터 학습된 모션 프라이어를 통합하여, 로봇 손의 동작이 인간처럼 자연스러운 형태를 유지하도록 유도. 예) GAN 기반 스타일 디스크리미네이터를 활용해 로봇 손의 모션이 인간 시演의 분포에 속하도록 보상 부여. 다만 물리적 타당성과의 균형 필요.\n\n\n접촉 검출 및 시간적 스무딩- 접촉 이벤트 추정- Dual-threshold & smoothing\n시연 데이터(예: 비디오나 센서 장갑 데이터)에서 손가락-물체 접촉 여부를 이중 임계값 등으로 검출하고, 프레임 간 접촉 상태 변화를 저역통과 필터로 평활화하여 안정적인 접촉 시퀀스를 확보. 이것을 리타게팅 및 학습에 사용해 접촉 불안정 현상(빠른 붙었다 떨어졌다 등) 제거."
  },
  {
    "objectID": "posts/paper/2025-08-11-one-to-multi.html",
    "href": "posts/paper/2025-08-11-one-to-multi.html",
    "title": "📃From One Hand to Multiple Hands 리뷰",
    "section": "",
    "text": "Paper Link\nProject Link\nCode Link"
  },
  {
    "objectID": "posts/paper/2025-08-11-one-to-multi.html#방법론-methodology",
    "href": "posts/paper/2025-08-11-one-to-multi.html#방법론-methodology",
    "title": "📃From One Hand to Multiple Hands 리뷰",
    "section": "1. 방법론 (Methodology)",
    "text": "1. 방법론 (Methodology)\n\n1.1 단일 카메라 텔레오퍼레이션을 통한 데모 수집\n이 논문의 핵심은 단일 RGB-D 카메라(아이패드)에 기반한 텔레오퍼레이션 시스템을 통해 인간 손 시연(demonstration)을 효율적으로 수집하는 새로운 프레임워크다. 사용자는 특수 장비 없이 아이패드 한 대만으로 자신의 손 동작을 촬영하여 3차원 손 자세와 형상을 실시간으로 추정한다. 저자들은 MediaPipe와 FrankMocap 기반의 손 추적기를 사용하여, 입력 RGB-D 영상에서 손목 위치, 손가락 관절 각도(포즈) 및 손 형태(shape) 파라미터를 추정한다. 여기서 SMPL-X 모델을 활용하여 손의 형상과 포즈를 파라미터화하며, 초기 프레임에서 추정된 손 형태 파라미터를 통해 사용자의 손 크기와 모양을 파악한다. 이후 이 정보를 이용해 물리 시뮬레이터(SAPIEN) 상에 사용자 손과 동일한 형태・크기의 맞춤형 로봇 손 모델을 즉석에서 생성한다. 이 커스텀 로봇 손(customized robot hand)은 사용자의 손 골격과 운동학 구조를 그대로 반영하며, 예를 들어 사용자의 엄지손가락이 짧다면 생성된 로봇 손에서도 엄지가 짧게 구현된다. 이렇게 만들어진 로봇 손 모델은 약 45자유도(DoF)를 가져 인간 손의 섬세한 움직임을 모사한다.\n사용자는 자신의 손을 움직이면, PD 제어기를 통해 해당 관절 목표각이 시뮬레이터의 커스텀 로봇 손에 전달되어 로봇 손이 따라 움직인다. 이때 영상 기반 추적의 오차로 인한 잡음이나 튀는 동작을 줄이기 위해, 저자들은 손 형태 추정값의 신뢰도를 가중치로 활용한 PD 제어 기법을 고안하였다. 구체적으로, 초기 캘리브레이션 시 얻은 손 shape 파라미터를 기준값으로 삼고, 매 프레임의 shape 추정치와의 차이를 계산하여 추적 정확도의 신뢰도로 활용한다. 신뢰도가 낮은 경우(손 추적 오차가 큰 경우) PD 제어의 강성을 낮추어 갑작스런 잘못된 동작 전송을 억제함으로써, 부드럽고 안정적인 원격조작을 가능하게 하였다.\n이 시스템은 온라인 모션 리타기팅(motion retargeting)을 필요로 하지 않는다는 점에서 기존 방식과 차별화된다. 일반적인 비전 기반 텔레오퍼레이션에서는 사람 손의 관절 움직임을 로봇 손의 관절로 실시간 변환해야 하는데, 사람 손과 로봇 손 구조 차이로 인해 제어가 어렵고 지연이 발생하곤 했다. 반면 본 논문에서는 사용자별로 동일한 구조의 로봇 손을 사용하므로 이러한 매핑 과정이 불필요하다. 그 결과 사용자는 마치 자신의 손을 그대로 가상 공간에 옮겨놓은 듯한 자연스러운 방식으로 물체 조작 시연을 할 수 있다. 저자들의 사용자 연구에 따르면, 제안 시스템으로는 1시간에 약 60개의 성공 시연을 모을 수 있는데, 이는 예컨대 사용자가 직접 Allegro 로봇 손(4손가락 로봇)을 조작해 시연을 모을 때의 약 10개에 비해 6배에 달하는 수치다. 실험에 참여한 사람들 역시 커스텀 손이 기존 로봇 손보다 조작하기 훨씬 쉽다고 평가하였다. 이러한 높은 데모 수집 효율은 대규모의 양질의 시연 데이터를 확보하게 해주며, 결과적으로 모방 학습 성능 향상으로 이어진다.\n\n\n1.2 다중 로봇 손으로의 시연 데이터 변환 (Multi-Hand Demonstration Translation)\n시뮬레이터에서 수집된 맞춤형 로봇 손의 시연 trajectories는, 오프라인 단계에서 임의의 타깃 로봇 손 모델로 변환(retargeting)된다. 이는 한 번의 인간 시연 데이터 수집으로 여러 종류의 로봇 손 학습 데이터를 생성할 수 있음을 의미한다. 논문에서는 Schunk SVH 5손가락 로봇 손, Adroit 손(샌디에이고 대학 Adroit) 및 Allegro Hand(4손가락)의 세 가지 서로 다른 상용 로봇 손을 대상으로 실험하였다. 이들 로봇 손들은 형태(geometry)와 운동 범위(DOF)뿐 아니라 손가락 개수까지 사람 손과 상이하지만, 커스텀 손 시연을 각 로봇 손의 데이터로 변환함으로써 동일 과제에 대한 다종 로봇 시연 데이터셋을 구축할 수 있다.\n모션 리타기팅은 각 로봇 손의 관절 움직임 궤적을 찾는 최적화 문제로 공식화된다. 구체적으로, 커스텀 손과 타깃 로봇 손의 중요 키포인트 (손가락 끝 위치 등)가 최대한 일치하도록 두 손의 관절각을 조정하는 방식을 취한다. 각 시점에서 양 손 모델의 정방향 운동학 결과(손가락 키포인트 좌표) 간 오차를 최소화하는 관절 구성을 찾아내며, 시간적 연속성을 위해 이전 프레임의 해로 초기화하고 관절 변화량에 대한 정규화 항을 추가하여 부드러운 궤적을 얻는다. 이렇게 계산된 관절 위치 trajectory는 해당 로봇 손에서의 시연으로 간주되며, 이후 학습 알고리즘에 투입될 수 있다. 특히 Allegro Hand의 경우 인간 손가락보다 하나 적은 4개 손가락만 있기 때문에, 최적화 과정에서 인간의 약지/소지 움직임을 나머지 손가락에 분산시키는 등 형태 차이를 보정한다. 저자들은 이러한 오프라인 리타기팅은 실시간으로 할 때보다 계산 비용이 크지만, 한 번만 수행하면 되므로 충분히 감내할 수 있는 수준이라고 설명한다. 실제로 논문에 따르면 오프라인 리타기팅에 수 밀리초 단위의 계산 시간이 들지만, 온라인 리타기팅 시에는 반복 최적화로 인해 프레임 간 지연이 들쑥날쑥 커져 사람이 다음 동작을 예측하며 조작하기 어렵게 된다고 지적한다. 커스텀 손 접근법은 이러한 온라인 retargeting에 따른 지연과 불안정성 문제를 근본적으로 해소하였다.\n리타기팅된 시연 데이터에는 각 로봇 손의 상태(관절각 등) 뿐 아니라 행동(action) 데이터도 필요하다. 모방 학습을 위해서는 시연 시퀀스의 상태-액션 쌍이 필요한데, 관절 위치 궤적으로부터 해당 로봇 손의 구동 명령(토크 또는 모터 입력)을 계산하는 절차가 추가된다. 이 부분에서 저자들은 이전 연구인 DexMV 방법론을 참고하여, 우선 관절각 시퀀스에 1차 저역통과 필터를 적용하고, 로봇 역동역학(manipulator equation) 기반으로 각 시점의 관절 토크를 추정하는 방식을 사용하였다. 이를 통해 시연 궤적을 따라가는 데 필요한 근사 제어 신호까지 계산함으로써, 최종적으로 “(상태, 액션) 궤적” 형태의 학습용 시연 데이터를 완성한다.\n\n\n1.3 모방 학습 알고리즘 및 정책 학습\n이렇게 준비된 다수 로봇 손의 인간 시연들을 활용하여, 최종적으로 다이렉트 정책 학습(policy learning)을 수행한다. 가장 단순한 접근인 행동 클로닝(Behavior Cloning)의 경우 시연 데이터를 바로 모방하도록 학습하면 되지만, 초기 상태나 목표 조건이 변하는 복잡한 작업에서는 순수 행동 클로닝만으로는 성공적인 정책을 얻기 어렵다. 따라서 논문에서는 강화학습(RL)에 시연 데이터를 활용하는 형태의 모방 강화학습 알고리즘을 채택하였다. 구체적으로, Rajeswaran 등이 제안한 DAPG (Demo Augmented Policy Gradient) 알고리즘을 사용하여 강화학습 목표에 시연 모방 항(term)을 추가하였다. DAPG는 전문가 시연으로 사전학습(behavior cloning)을 실시한 후, 이후 학습 과정에서도 정책 그래디언트 계산 시 시연 데이터로부터 유도된 보상(or 정규화 항)을 추가함으로써, 표준 RL과 BC의 조합으로 볼 수 있는 방법이다. 본 연구에서는 TRPO(Trust Region Policy Optimization) 알고리즘을 기반 RL 기법으로 사용하고, 여기에 동일한 하이퍼파라미터 세팅으로 DAPG를 적용하여 학습을 진행하였다.\n정책 학습은 시뮬레이션 환경에서 이루어지며, 관측 상태에는 로봇 손 관절 상태, 손바닥(팜)의 속도, 물체의 3D 위치와 자세 등이 포함된다. 과제에 따라 목표물의 위치나 문의 힌지 각도 등의 목표 조건도 관측에 주어지며, 에이전트의 행동은 손바닥 이동(자유 공간에서의 6-자유도 움직임은 6차원 속도 제어)과 손가락 관절 제어(PD 위치 제어 입력)로 구성된다. 이렇게 학습된 정책은 해당 로봇 손 모델의 시뮬레이션에서 동작을 익히게 되며, 이후 실제 로봇 손으로의 이식을 목표로 한다. 특히 실세계로의 일반화를 돕기 위해, 학습 중에 도메인 랜덤화 기법을 도입하였다. 예를 들어 물체의 초기 위치나 물리 속성(마찰 계수, 무게 등)을 다양하게 랜덤화하고, 관측되는 물체 상태에 가우시안 잡음을 추가하여 센서 노이즈와 환경 차이를 견디도록 훈련했다. 이러한 전략과 인간 시演의 도입으로 정책이 사람과 유사한 동작 전략을 학습하게 되어, 시뮬레이션-실세계 간 격차(sim2real gap)를 극복하는 데 큰 도움이 되었다고 저자들은 밝히고 있다."
  },
  {
    "objectID": "posts/paper/2025-08-11-one-to-multi.html#실험-설정과-결과-평가-experiments-and-results",
    "href": "posts/paper/2025-08-11-one-to-multi.html#실험-설정과-결과-평가-experiments-and-results",
    "title": "📃From One Hand to Multiple Hands 리뷰",
    "section": "2. 실험 설정과 결과 평가 (Experiments and Results)",
    "text": "2. 실험 설정과 결과 평가 (Experiments and Results)\n\n2.1 실험 환경과 과제 구성\n저자들은 앞서 구축한 SAPIEN 시뮬레이터 환경에서 세 가지 복잡한 다지 조작 과제를 실험했다. 각 과제는 실제 인간 시연을 통해 데이터가 수집되고, 이후 정책 학습 및 평가에 활용되었다:\n\nRelocate (물체 옮기기): 로봇 손이 탁자 위의 물체를 집어서 임의의 목표 위치로 옮기는 작업이다. 초기 물체의 자세와 목표 위치는 에피소드마다 무작위로 설정되며, 로봇은 물체를 들어올려 정해진 위치에 놓는 것을 목표로 한다. 이 과제에는 YCB 객체셋의 토마토 수프 캔, 통조림(Potted Meat Can), 머스타드 병의 세 가지 물체가 사용되어, 목표지향 다중 물체 조작 능력을 평가한다.\nFlip (머그컵 뒤집기): 평평한 테이블 위에 놓인 머그잔을 90도 회전시켜 옆으로 눕히는 작업이다. 로봇 손은 잔을 움켜쥐고 천천히 기울여 눕혀야 하며, 지나친 힘을 주면 물체가 미끄러지거나 튕겨나갈 수 있다. 이 과제는 특정 방향으로 힘을 미세하게 가하여 물체를 조작하는 능력을 평가하며, 매 에피소드마다 머그잔의 초기 위치와 회전 각도가 무작위로 변경된다.\nOpen Door (문 열기): 문에 달린 레버 형태의 손잡이를 돌려서 문을 여는 이단계 작업이다. 먼저 손잡이를 쥐고 회전시켜 잠금을 해제한 뒤, 계속 잡은 상태로 문을 당겨서 연다. 로봇 손은 손잡이를 단단히 파지하면서도 회전과 당기기 두 동작을 모두 수행할 수 있는 적절한 손가락 구성을 찾아야 한다. 매 시도마다 문의 위치(거리 등)가 약간씩 바뀌어, 일반화된 문 열기 동작을 익혀야 한다.\n\n각 과제의 성공 기준은 명확하게 정의되었다. 예를 들어 Relocate의 경우 에피소드 종료 시 물체와 목표 지점 사이 거리가 일정 임계값 이하이면 성공으로 판정하고, Flip은 머그잔의 기울기 각도가 목표 범위에 들어오면 성공으로 본다. Open Door는 문 경첩의 회전각이 일정 각도 이상 벌어져 실제로 문이 열렸을 때 성공 처리한다. 이러한 성능 지표를 통해 성공률(success rate)이나 에피소드 리턴(return) 등을 측정하였다.\n\n\n2.2 사용자 원격조작 실험: 커스텀 손 vs. 기존 로봇 손\n먼저 데모 수집 단계의 효용성을 확인하기 위해 수행된 텔레오퍼레이션 사용자 연구(user study) 결과를 살펴본다. 17명의 피험자들이 앞서 정의한 Relocate 과제와 Open Door 과제를 각기 수행하되, 네 가지 다른 로봇 손 모델을 사용하도록 했다. 비교 대상은 (1) 제안한 맞춤형 로봇 손, (2) Schunk SVH 5손가락 로봇 손, (3) Adroit 로봇 손, (4) Allegro 로봇 손이었다. 커스텀 손 이외의 세 경우에는 모두 온라인 모션 리타기팅을 통해 사용자의 손 동작이 해당 로봇 손으로 매핑되었고, 커스텀 손의 경우 앞서 설명한 대로 1:1 직접 제어가 이루어졌다.\n평가 방법으로, 각 조합(작업 + 로봇 손)에 대해 5회씩 연속 시도하게 하고 그 평균 성공률과 작업 완료 시간을 측정하였다. Relocate와 Open Door 모두 두 단계(stage)로 나누어 세분화 평가하였는데, 예를 들어 Open Door의 경우 손잡이 회전 성공 여부를 1단계, 문 열기 완료 여부를 2단계로 구분해 각각의 성공률과 소요 시간을 기록했다. 이는 작업 내 세부 단계별로 어느 부분에서 실패하거나 시간이 지연되는지 파악하기 위함이다.\n결과적으로, 커스텀 로봇 손을 이용한 경우 압도적으로 높은 성공률과 더 빠른 수행 시간을 보였다. 예를 들어 Relocate 과제에서 1단계(물체 들어올리기) 성공률은 커스텀 손 78.9%로, Schunk(61.2%), Adroit(58.8%), Allegro(44.7%)에 비해 월등히 높았다. 2단계(물체 이동 완료) 역시 커스텀 손이 55.3%로 나머지(30.6%, 28.2%, 16.9%)보다 훨씬 높았다. Open Door 과제에서도 커스텀 손의 1단계(손잡이 돌리기) 성공률이 95.3%로 다른 손들(71~83%)보다 높았고, 2단계(문 열기)도 82.4%로 타 로봇 손들(41~61%) 대비 크게 앞섰다. 작업 완료 시간도 일관되게 커스텀 손이 짧아서, 더 빠르게 과제를 달성했다. 이러한 사용자 실험을 통해, 맞춤형 손을 이용한 비전 기반 원격조작이 전통적인 로봇 손 직접 제어보다 훨씬 효율적임이 검증되었다. 저자들은 특히 Allegro 손의 성능이 가장 저조했던 점을 지적했는데, 그 이유로 “Allegro는 손가락이 4개라 인간 손 동작을 충실히 맵핑하기 어렵고, 손 크기도 사람 손보다 훨씬 커 제어가 어색하다”고 분석했다. 반면 커스텀 손은 사용자 손과 크기/구조가 같고 추가 매핑 계산이 없기에 즉각적이고 직관적인 피드백을 주어 조작을 수월하게 만든 것이다.\n결과적으로, 맞춤형 손 시스템은 데모 수집 단계부터 질적으로 우수한 데이터(높은 성공률의 시연)를 다량 확보하게 해주며, 이는 이후 학습 성능 향상의 기반이 된다.\n\n\n2.3 정책 학습 성능 비교: RL vs. 모방 학습\n다음으로, 이렇게 수집된 시연 데이터를 활용한 정책 학습 결과를 순수 강화학습과 비교 평가하였다. 각 과제(Relocate - 3가지 객체, Flip, Open Door)에 대해 TRPO 기반 강화학습(RL)으로 훈련한 정책과, DAPG 기반 모방 학습(IL)으로 훈련한 정책을 비교하였다. 여기서 시연 데이터의 양은 과제당 50개 에피소드로 동일하게 제한하였고, 정책 학습은 3개의 시드로 반복하여 수렴 속도와 최종 성능의 평균을 비교했다. Figure 4의 학습曲선 및 Table VI의 최종 성공률이 두 방법을 종합적으로 보여준다.\n학습曲선을 보면, 모방 학습(DAPG)이 순수 RL보다 훨씬 빠르게 초기 성능을 끌어올리고 더 높은 수준에서 수렴하는 경향이 명확하다. 특히 난이도가 높은 과제일수록 그 격차가 크게 벌어졌다. 최종 성공률 지표로 보아도, 대부분의 과제-로봇 조합에서 DAPG 정책이 RL 정책을 능가했다. 예를 들어 Relocate-토마토캔, 머그 뒤집기 등에서 모든 로봇 손에 대해 모방 학습이 더 높은 성공률을 기록하였다. 이는 사람 시연으로부터 생성한 데이터가 학습에 유의미한 신호를 제공하여, 고차원 탐색 공간에서 RL 혼자 학습할 때 발생하는 시행착오를 크게 줄여주기 때문이다.\n흥미로운 점은 Open Door 과제에서 Allegro 손을 사용한 경우였다. 이 한 가지 사례에서는 DAPG 정책의 최종 성공률이 RL과 큰 차이가 없었는데, 이는 Allegro 손의 구조적 한계와 과제의 난이도가 맞물려 나타난 결과로 보인다. 비록 정량적 성능 향상은 작았지만, 두 접근법의 동작 양상은 크게 달랐다. 저자들이 시각적으로 정책 행동을 관찰한 바로는, DAPG로 학습된 정책은 사람처럼 손잡이를 쥐고 돌린 후 당기는 자연스러운 전략을 구사한 반면, 순수 RL 정책은 손잡이를 제대로 쥐지 못하고 손바닥으로 강압적으로 누르면서 마찰력으로 문을 여는 비교적 비정상적인 방법을 사용했다고 한다. 후자는 시뮬레이터 상에서는 우연히 성공할지 모르나 실제 세계에선 통하지 않을 가능성이 크다. 이 사례는, 모방 학습을 통해 얻은 정책이 보다 인간스러운 동작으로 안전하고 예상 가능한 범위 내에서 과제를 수행함을 보여준다. 결국 대부분의 상황에서 인간 데모 활용이 학습 성능과 행동 품질을 향상시킴이 입증되었다.\n\n\n2.4 추가 실험: Ablation 및 영향 요소 분석\n저자들은 모방 강화학습의 성공에 기여하는 요소들을 분석하기 위해 몇 가지 요인 별 Ablation 실험도 수행하였다. 예를 들어, Relocate(토마토 수프 캔) + Schunk Hand 조합에 대해, (a) 물체 마찰계수, (b) 물체 밀도(무게), (c) PD 제어기의 강성/감쇠 계수, (d) 학습에 사용한 데모 개수 등을 변화시켜 가며 DAPG 학습의 민감도를 관찰했다. Figure 5에 제시된 학습曲선을 통해 각 조건 변화가 학습 속도와 최종 성능에 미치는 영향을 비교하였는데, 전반적으로 환경 물리 파라미터의 변화에도 시연 데이터가 포함된 학습은 안정적으로 동작함을 보였다. 예컨대 물체 마찰이나 무게가 달라져도 데모를 포함한 정책은 비교적 강인한 성능 유지를 보였으며, 이는 시연을 통해 학습한 인간 특유의 적응적 조작 전략이 작용한 결과로 해석된다. 또한 데모 개수에 따른 실험에서는, 시연 데이터가 많을수록 학습 성능이 개선되다가 어느 정도 수렴하는 양상이 나타났는데, 이는 추가 데모가 초기 학습에 도움은 되지만 과도한 경우 수확 체감이 있음을 시사한다. PD 제어 파라미터의 경우 너무 낮은 강성은 정확도 저하로, 너무 높은 강성은 진동 증가로 이어져, 적절한 튜닝이 필요함을 실험으로 확인하였다.\n\n\n2.5 실세계 로봇에의 적용 및 성능\n최종적으로, 시뮬레이션에서 학습된 정책을 현실의 로봇 손에 이식하여 검증하였다. 하드웨어 플랫폼은 Allegro Hand + XArm-6 로봇 팔 조합으로, 시뮬레이션의 Allegro Hand 모델과 동일한 로봇 손을 6자유도 로봇 팔 끝에 장착한 구성이다. 저자들은 학습된 정책으로 실세계에서 위 과제들을 수행해보고 성공률과 동작의 안정성을 평가했다. 그 결과, 인간 시연을 포함하여 학습된 정책은 시뮬레이터에서뿐만 아니라 실제 환경에서도 높은 성공률과 강인한 성능을 보였다. 더욱이 시뮬레이션에서는 보지 못했던 새로운 객체나 변형된 상황에도 정책이 비교적 잘 적응하는 모습을 보였는데, 이는 학습 과정에서 인간 시연을 통해 얻은 일반적인 조작 원리와 자연스러운 힘 가하기 전략 덕분으로 풀이된다. 반면, 동일 환경에서 순수 RL로 학습된 정책은 현실에서 거의 실패하였는데, 시뮬레이터 상의 비현실적인 전략(예: 마찰로 문 밀기 등)이 현실에선 통하지 않고, 미세한 동작 오차에 대한 보정 능력도 부족했기 때문이다. 정량적으로 실험 횟수가 제한되어 구체적인 성공률 수치로 비교하진 않았지만, 데모 기반 정책이 월등히 안정적인 성과를 보인 것은 분명하다. 요약하면, 본 논문의 접근법은 시뮬레이션-현실 간 격차를 줄여주는 인간 데모의 힘을 입증했으며, 복잡한 다관절 손 조작 작업을 실제 로봇으로 수행하는 데 있어 모방 학습의 유용성을 보여주었다."
  },
  {
    "objectID": "posts/paper/2025-08-11-one-to-multi.html#기존-연구와의-비교-comparison-with-prior-work",
    "href": "posts/paper/2025-08-11-one-to-multi.html#기존-연구와의-비교-comparison-with-prior-work",
    "title": "📃From One Hand to Multiple Hands 리뷰",
    "section": "3. 기존 연구와의 비교 (Comparison with Prior Work)",
    "text": "3. 기존 연구와의 비교 (Comparison with Prior Work)\n본 연구는 다지 로봇 손의 정교한 조작과 인간 시연 학습 분야에서 여러 기존 접근들과 구별되는 혁신점을 제시한다. 여기서는 관련 선행 연구들과 비교하여 이 논문의 차별성과 기여도를 분석한다.\n\nVR 글러브 기반 데모 수집 vs. 카메라 기반 데모 수집: 인간 시연을 로봇 학습에 활용하려는 시도는 이전부터 있어 왔다. 대표적으로 2018년 Rajeswaran 등은 VR 장비와 데이터글러브를 활용해 인간이 가상현실에서 로봇 손을 조작하며 데모를 모으고, 이를 활용해 DAPG 알고리즘으로 정책을 학습하는 연구를 수행했다. 이 방식은 성공적인 Dexterous 핸드 조작을 보여주었지만, 전용 장비가 필요하고 한정된 인원만 참여할 수 있어 데이터 수집의 확장성(scalability)이 떨어진다는 단점이 있었다. 반면 본 논문은 특수 장비 없이 카메라만으로 누구나 데모를 제공할 수 있는 환경을 마련함으로써, 다양한 사용자로부터 대량의 데모를 손쉽게 확보할 수 있게 했다. 실제로 “VR을 통한 수집은 많은 인적 노력(human effort)을 요해 확장성이 낮지만, 단일 카메라 텔레오퍼레이션은 프로세스를 더욱 손쉽고 대규모로 확장 가능하게 만든다”고 저자들도 강조한다. 이러한 접근은 향후 클라우드 로보틱스나 크라우드소싱을 통해 방대한 인간 시연 데이터를 모을 수 있는 길을 열었다는 점에서 의의가 크다.\n멀티캠 Vision 텔레오퍼레이션 vs. 단일캠 접근: 비전 기반의 원격조작 자체는 과거에도 연구된 바 있는데, DexPilot 연구가 그 선구적 예다. DexPilot에서는 4대의 깊이 카메라(RealSense)를 테이블 주변에 배치하고 배경을 검은 천으로 막는 등 상당히 복잡한 세팅을 통해, 인간 손 영상을 인식해 Allegro 로봇 손을 제어하였다. 이와 비교하면 본 논문은 카메라 한 대(iPad)로 동일한 목표를 달성하면서, 추가로 사용자별 커스텀 손 인터페이스까지 제공하여 조작의 직관성을 높였다. 무엇보다도, DexPilot 등 이전 연구들은 단일 특정 로봇 손(예: Allegro)만을 대상으로 하였는데, 본 연구는 동일한 인간 시연 데이터를 여러 로봇 손으로 변환하여 학습에 활용하는 범용성을 선보였다. 이는 이전에는 볼 수 없었던 새로운 개념으로, 예컨대 한 사람의 시연으로 쉥크 손, 어드로이트 손, 알레그로 손 각각의 정책을 모두 학습시킬 수 있음을 처음으로 실증하였다. 이러한 Multi-hand demonstration 아이디어는 로봇 핸드 하드웨어가 다양한 현실에서 데이터 효율적인 학습을 가능케 하는 방향성을 제시한다.\n단순 그리퍼(2-jaw) 작업 vs. 고차원 다지 조작: 인간 시연을 비전으로부터 가져와 학습하는 연구 중에는, 비교적 간단한 병렬 그리퍼(집게 형태 로봇 핸드)로 픽앤플레이스 같은 작업을 가르치는 사례들이 있었다. 예를 들어 인간 동작 영상을 모방해 2-finger 그리퍼로 물체를 집는 정도의 과제는 3D 정보 없이도 가능하여, 2D 영상 기반 imitation이 시도되었다. 그러나 이러한 선행 연구들은 저차원 제어(몇 개 관절)와 단순 작업에 국한되어, 복잡한 손가락들을 활용한 3D 상호작용 과제에는 적용하기 어려웠다. 본 논문의 시스템은 RGB-D를 통해 3D 손-물체 포즈 정보를 추출하고, 이를 활용해 복잡한 접촉이 있는 작업들(예: 손잡이 돌려당기기)을 시연 및 학습하였다는 점에서, 기존 영상 모방학습 연구들을 한 단계 발전시켰다. 요컨대 보다 난이도 높은 다지 조작 작업에 비전 기반 모방학습을 확장한 사례로 평가된다.\n다수 로봇 손 간 정책 전이: 다관절 로봇 손 연구에서 로봇 구조가 바뀌면 새로 학습을 해야 하는 문제는 오래 지속된 숙제였다. 예를 들어 5손가락 로봇에 맞춰 학습된 정책은 4손가락 로봇에는 바로 적용하기 어렵고, 심지어 손가락 길이 비율 차이만 있어도 성능에 큰 영향이 있었다. 이를 해결하려는 시도로 meta-learning이나 이종 로봇 간 domain adaptation 등의 연구가 일부 있었지만, 여전히 사전 데이터 준비나 보정이 까다로웠다. 본 연구는 사람 손이라는 공통 참조 기준(human hand as common reference)을 활용하여, 처음에는 사람 손과 같은 구조의 커스텀 손으로 시연을 모으고 나서 이를 각 로봇 손으로 변환함으로써 자연스럽게 여러 형태의 로봇 시연 데이터를 확보했다. 이렇게 함으로써 여러 로봇 플랫폼에 걸쳐 학습 데이터를 공유할 수 있게 되었고, 결과적으로 각 로봇별 최적 정책을 학습하면서도 필요한 인간 시연 횟수는 최소화하는 성과를 거두었다. 이는 기존의 multi-finger manipulation 연구에서 한 종류 로봇에 특화된 데이터를 썼던 관행과 달리, 범용적인 시연 데이터 활용 가능성을 처음 보여준 것이다.\n정책의 자연스러움과 안전성: OpenAI의 유명한 Rubik’s Cube 푸는 로봇 손과 같은 사례에서 볼 수 있듯, 순수 강화학습을 통해 다관절 손 조작을 습득시키는 것은 가능하지만 종종 비인간적인 해법(예: 큐브 던졌다 받기 등)이 나타나거나, 현실 적용 시 예기치 못한 동작을 보일 수 있다. 이에 비해 본 논문의 접근은 인간의 데모를 통해 정책이 인간과 유사한 동작 분포를 따르도록 유도하였고, 그 결과 RL만 사용했을 때 나타나는 위험하거나 이상한 행동을 억제하는 효과가 있었다. 논문에서 묘사된 것처럼, 문 열기 동작에서 순수 RL 정책은 문을 마찰로 억지로 여는 방법을 택했지만, 데모를 참고한 정책은 손잡이를 제대로 잡고 트는 보다 안전하고 일반화 가능한 방법을 구사했다. 이런 차이는 산업적 응용에서 매우 중요하다. 즉, 본 연구는 기존 대비 안전하고 신뢰할 수 있는 다지 손 조작 정책을 얻는 데 기여했다고 평가할 수 있다.\n\n요약하면, “From One Hand to Multiple Hands” 논문은 비전 기반 텔레오퍼레이션, 맞춤형 로봇 손 개념, 다중 로봇 간 시연 이식, 데모-강화 학습 통합 등을 한 데 묶어 다지 로봇 손 학습의 새로운 지평을 연 연구라 할 수 있다. 이전 연구들에 비해 데이터 수집의 용이성과 확장성, 학습 정책의 성능과 현실 적합성 측면에서 뛰어난 성과를 보였으며, 향후 다양한 형태의 로봇 손이나 복잡한 조작 작업에 범용적으로 적용될 수 있는 모방학습 프레임워크로서 큰 영향을 미칠 것으로 기대된다."
  },
  {
    "objectID": "posts/paper/2025-08-01-spin-pens.html",
    "href": "posts/paper/2025-08-01-spin-pens.html",
    "title": "📃Spin pens 리뷰",
    "section": "",
    "text": "Paper Link\nProject Link\nGithub Link"
  },
  {
    "objectID": "posts/paper/2025-08-01-spin-pens.html#들어가며",
    "href": "posts/paper/2025-08-01-spin-pens.html#들어가며",
    "title": "📃Spin pens 리뷰",
    "section": "들어가며",
    "text": "들어가며\n사람들은 손가락으로 펜을 돌리는 묘기를 종종 부리곤 합니다. 이는 겉보기엔 단순해 보여도, 로봇 손에게는 상당히 어려운 정교한 손 내 조작(in-hand manipulation) 기술입니다. 펜과 같이 길쭉한 물체를 손 안에서 자유롭게 회전시키는 능력은 재미 이상의 의미를 갖습니다. 망치나 드라이버처럼 길쭉한 도구들을 다루는 동작과 유사하기 때문에, 로봇이 이러한 펜 돌리기를 해낼 수 있다는 것은 다양한 도구 사용의 기초 기술을 확보하는 셈이기도 합니다. 하지만 지금까지 강화학습 등 학습 기반 방법으로 이러한 작업을 달성하기란 매우 어려웠습니다. 무엇보다 사람처럼 시범을 보여주기도 힘들고, 시뮬레이션과 실제 로봇 사이의 물리 차이(시뮬레이션-현실 격차)도 커서, 가상훈련 성과를 현실에 옮기기 어려웠기 때문입니다. 이번 리뷰에서는 2024년 CoRL(로봇학습 콘퍼런스)에 발표된 “Lessons from Learning to Spin ‘Pens’” 논문을 깊이 있게 살펴보고자 합니다. 이 연구는 강화학습 기반의 새로운 접근법으로 로봇 손이 펜과 유사한 물체를 손가락 사이에서 여러 바퀴 회전시키는 데 성공했고, 그 과정에서 얻은 통찰과 한계를 공유하고 있습니다. 본 포스트에서는 해당 논문의 주요 내용을 요약하고, 사용된 기법과 모델의 기술적 요소를 설명하며, 얻어진 교훈과 남은 과제를 함께 분석해보겠습니다."
  },
  {
    "objectID": "posts/paper/2025-08-01-spin-pens.html#배경-및-도전-과제",
    "href": "posts/paper/2025-08-01-spin-pens.html#배경-및-도전-과제",
    "title": "📃Spin pens 리뷰",
    "section": "배경 및 도전 과제",
    "text": "배경 및 도전 과제\n로봇의 손 내 조작 기술은 인간 수준의 섬세함을 달성하기 위해 필수적인 연구 주제입니다. 과거에도 로봇 손으로 물체를 돌리거나 재배열하는 시도가 없었던 것은 아니지만, 펜 돌리기처럼 연속적이고 역동적인 회전 동작은 특히 난제가 되었습니다. 기존 학습 기반 기법들이 이 문제에 부딪혀온 주된 이유는 두 가지입니다. 첫째, 이러한 고난도 작업에 대한 고품질 시演(데몬스트레이션) 데이터를 얻기가 어렵습니다. 사람 손의 섬세한 움직임을 모방하거나 원격 조종(텔레오퍼레이션)을 통해 로봇에 시범을 가르치려 해도, 펜 돌리기의 복잡한 동작을 정확히 재현하기가 거의 불가능에 가깝습니다. 둘째, 시뮬레이션-현실 간의 차이(sim-to-real gap)가 매우 크다는 문제입니다. 시뮬레이터 상에서 로봇 손가락이 펜을 돌리는 데 성공하더라도, 실제 로봇에 동일한 정책(policy)을 이식하면 마찰 계수, 물체의 미세한 물리 특성 차이, 센서 오차 등으로 인해 펜을 금세 떨어뜨리기 일쑤입니다. 특히 펜 돌리기처럼 접촉이 연속적으로 발생하고 매우 역동적인 작업의 경우 이 격차는 더욱 심해져서, 단순한 도메인 랜덤화 등으로는 메우기 어렵다는 것이 선행 연구들의 교훈이었습니다.\n연구팀 역시 초기에 시뮬레이션에서 학습한 정책을 바로 현실 로봇에 이식해 보거나, 하드웨어 구조와 물체 재질을 바꿔보는 등 여러 시도를 했지만 번번이 실패를 겪었습니다. 펜은 이내 손가락 사이에서 미끄러지거나 튕겨 나가 떨어졌고, 시뮬레이터와 현실 사이의 분포 차이(distribution shift)를 실감해야 했습니다. 그렇다면 이런 어려운 문제를 어떻게 풀 수 있을까요? 이 논문의 핵심은 “시뮬레이션의 힘을 최대한 활용하면서도, 최소한의 현실 데이터로 격차를 메우는 하이브리드 학습 전략”에 있습니다. 다음 섹션에서는 저자들이 제안한 독창적인 접근 방법을 살펴보겠습니다."
  },
  {
    "objectID": "posts/paper/2025-08-01-spin-pens.html#접근-방법-시뮬레이션-오라클과-현실-적응",
    "href": "posts/paper/2025-08-01-spin-pens.html#접근-방법-시뮬레이션-오라클과-현실-적응",
    "title": "📃Spin pens 리뷰",
    "section": "접근 방법: 시뮬레이션 오라클과 현실 적응",
    "text": "접근 방법: 시뮬레이션 오라클과 현실 적응\n“Lessons from Learning to Spin ‘Pens’” 논문의 저자들은 시뮬레이션과 현실 데이터를 단계적으로 활용하는 학습 파이프라인을 고안하였습니다. 이 접근법은 크게 세 단계로 요약할 수 있습니다:\n\n시뮬레이션에서의 오라클 정책 학습 – 우선 가상 환경에서 펜 돌리기 문제를 충분히 탐색할 수 있는 오라클 정책(oracle policy)을 강화학습으로 훈련합니다. 여기서 오라클 정책이란 시뮬레이터가 제공하는 특권 정보(privileged information)를 모두 활용하는 전지전능한 정책입니다. 예를 들어 실제 로봇은 카메라나 촉각 센서로만 펜의 상태를 추정해야 하지만, 오라클 정책은 시뮬레이션이 제공하는 펜의 정확한 위치와 속도 등 완전한 상태 정보를 관측으로 사용할 수 있습니다. 이러한 추가 정보 덕분에 강화학습 에이전트는 탐색을 빠르게 진행하며 성공 궤적들을 만들어낼 수 있습니다. 오라클 정책은 결국 펜을 연속 회전시키는 훌륭한 전략을 익히게 되었고, 이를 통해 정밀한 시뮬레이션 궤적 데이터셋을 다수 확보할 수 있었습니다. 논문에 따르면, 이 단계에서 생성된 궤적들은 펜을 돌리는 손가락 동작의 고해상도 시나리오들을 담고 있어 이후 과정에 핵심적인 밑거름이 됩니다.\n학생 정책 학습 및 열린 루프 실행 – 다음으로, 시뮬레이션에서 수집된 궤적 데이터를 활용하여 학생(sensorimotor) 정책을 학습시킵니다. 학생 정책은 실제 로봇에 투입될 센서 기반 정책으로, 오라클과 달리 특권 정보 없이 로봇이 실제로 사용할 수 있는 센서 신호만으로 동작하도록 설계됩니다. 이를 위해 오라클의 시뮬레이션 궤적을 모방 학습(behavior cloning)이나 지도학습을 통해 학생 정책이 따라하도록 훈련합니다. 이렇게 초기화된 학생 정책은 시뮬레이션 상에서는 펜을 돌리는 방법을 알고 있지만, 여전히 현실 환경에서 바로 쓸 수 있을 정도로 견고하지는 않은 상태입니다. 따라서 연구팀은 한 걸음 더 나아가, 이 학생 정책을 현실 로봇에 적용하여 열린 루프(open-loop)로 실행해 보았습니다. 여기서 열린 루프 실행이란, 시뮬레이션에서 녹화한 동작 액션 시퀀스를 그대로 로봇에 재생하는 것으로, 실행 도중에 별도의 피드백 보정 없이 고정된 액션 궤적을 따라가는 것을 의미합니다. 놀랍게도 오라클 정책으로부터 학습한 학생 정책은 실제 로봇에서도 일정 수준의 펜 회전을 만들어냈고, 특히 그 중 일부 시도에서는 펜을 여러 차례 회전시키는 성공 사례들도 얻을 수 있었습니다. 연구진은 이러한 성공적인 현실 궤적 데이터를 추가로 수집하여 다음 단계에 활용합니다.\n현실 데이터로 정책 미세조정 – 마지막 단계에서는, 방금 확보한 현실 세계의 성공 궤적 데이터(50개 미만의 비교적 적은 수라고 합니다)를 가지고 학생 정책을 파인튜닝(fine-tuning)합니다. 이는 일종의 도메인 적응 단계로, 시뮬레이션 전용 정책이 현실의 물리 법칙과 잡음에 적응하도록 도와줍니다. 구체적으로, 학생 정책을 현실 궤적 데이터에 대해 다시 한 번 모방 학습하거나, 필요에 따라 추가 강화학습을 진행하여 현실 물리에 맞게 보정한 것입니다. 이렇게 함으로써 최종 정책은 비로소 현실 환경에서 펜을 안정적으로 돌릴 수 있는 능력을 얻게 됩니다. 결과적으로 단 50개 미만의 현실 궤적만으로도, 시뮬레이터 속에서만 통하던 정책이 현실의 오차와 마찰을 견디며 펜을 돌릴 수 있게 된 것이죠. 더욱이 이 최종 정책은 물리적 속성이 제각각인 10여 종의 펜 모양 도구들에 대해서도 모두 수 차례 연속 회전을 성공적으로 구현해냈습니다.\n\n\n\n\n\n그림 1: 본 연구에서 제안된 학습 파이프라인 개략도. 왼쪽에서는 시뮬레이션에서 오라클 정책을 강화학습으로 훈련하여 고품질 궤적 데이터를 생성하고, 이 데이터로 학생 정책을 사전 학습합니다. 그 후 그 학생 정책을 실제 로봇에 열린 루프 제어로 실행하여 성공 사례 현실 궤적을 수집합니다. 마지막으로 해당 현실 궤적으로 학생 정책을 미세 조정하여 현실 환경에 적응된 최종 정책을 얻습니다. 이 파이프라인을 통해 시뮬레이션의 탐색 능력과 현실 데이터의 정확성을 결합함으로써, 순전히 인간 시범이나 순전한 시뮬레이션으로는 불가능했던 펜 돌리기 과제를 달성할 수 있었습니다."
  },
  {
    "objectID": "posts/paper/2025-08-01-spin-pens.html#실험-결과-펜-돌리기의-달성-및-분석",
    "href": "posts/paper/2025-08-01-spin-pens.html#실험-결과-펜-돌리기의-달성-및-분석",
    "title": "📃Spin pens 리뷰",
    "section": "실험 결과: 펜 돌리기의 달성 및 분석",
    "text": "실험 결과: 펜 돌리기의 달성 및 분석\n그렇다면 이러한 접근법으로 얻은 최종 로봇 정책은 실제로 어느 정도 성과를 거두었을까요? 논문에 따르면, 불과 50개 미만의 현실 성공 사례 데이터로 미세조정한 정책임에도 불구하고, 로봇 손은 다양한 펜 모양의 물체를 손가락 사이에서 여러 바퀴 연속으로 회전시키는 데 성공했습니다. 여기에는 플라스틱 볼펜, 마커, 나무 막대 등 물리적 특성(무게, 마찰, 균형)이 서로 다른 10여 개의 물체들이 포함되어 있었는데, 정책은 이들 펜-유사 물체(pen-like objects) 각각에 대해 안정적으로 회전 동작을 구사했습니다. 이는 단순히 하나의 물체에 특화된 솔루션이 아니라, 일반화된 펜 돌리기 기술을 습득했음을 보여줍니다.\n또한 흥미로운 점은, 강화학습 단계에서의 설계 선택이 실제 결과에 큰 영향을 미쳤다는 것입니다. 저자들은 시뮬레이션 상의 오라클 정책을 학습할 때 여러 가지 보상 설계와 제약 조건을 실험했는데, 이를 통해 성공적인 펜 돌리기에는 어떤 요소가 중요한지를 분석했습니다. 예를 들어, 오라클 정책을 훈련할 때 손가락 자세를 하나의 고정된 포즈로만 사용하도록 제한한 경우(일종의 단일 자세 제약 실험)에는 에이전트가 펜을 길게 돌리지 못했습니다. 이때는 손가락의 위치를 바꾸는 핑거게이팅(finger gaiting)이 나타나지 않아, 탐색이 비효율적이고 결국 연속 회전에 실패했습니다. 반면 우리 방법(제안된 방법)에서는 에이전트가 학습을 통해 자발적으로 손가락을 끊어 움직이며(pingergaiting) 펜을 계속 회전시키는 동작을 터득했고, 그 결과 지속적인(spinning) 연속 회전이 가능해졌습니다. 핑거게이팅은 마치 사람이 손가락을 번갈아 가며 물체를 옮겨 쥐는 동작에 비유될 수 있는데, 로봇 정책이 이런 행동을 스스로 학습했다는 것은 매우 고무적인 성과입니다.\n또 다른 분석 요소로는 Reward Function의 구성이 있었습니다. 연구팀은 펜을 돌리는 강화학습 보상에 특별한 항목을 하나 추가했는데, 바로 “Z-축 보상(Z-reward)”입니다. 이는 펜이 회전 중에 기울어지지 않고, 일정 수준 이상 높이를 유지하도록 유도하는 보상으로 해석됩니다. 이 보상의 중요성은, Z-보상을 제외한 실험에서 드러났습니다. 해당 실험에서는 시뮬레이션 상에서는 펜을 돌릴 수 있었지만, 일정 시간 이후 펜이 기울어지면서 결국 손가락 사이에서 빠져버리는 문제가 관찰되었습니다. 이러한 기울어짐(tilt) 현상은 특히 현실 로봇에서 더 치명적이어서, Z-보상이 없는 정책은 실제 테스트 시 펜을 쉽게 떨어뜨렸다고 합니다. 따라서 펜을 안정적으로 여러 바퀴 돌리려면, 회전 속도나 횟수뿐만 아니라 자세 안정성(orientation stability) 역시 중요하다는 교훈을 얻었습니다.\n연구진은 제안한 방법의 효과를 검증하기 위해 여러 비교 실험(베이스라인)도 수행하였습니다. 첫째, 시뮬레이션에서 학습된 정책을 바로 사용(시뮬레이션 사전학습 only)하는 방안을 시험했는데, 예상대로 시뮬레이션-현실 간 물리 차이를 극복하지 못해 펜 돌리기에 실패했습니다. 둘째, 시뮬레이션 궤적을 열린 루프로만 재생하는 단순 모방 방식의 경우, 동작 자체는 그럴듯하지만 센서 피드백이 전혀 없기 때문에 작은 불확실성에도 금방 실패로 이어졌습니다. 즉, 이러한 오픈 루프 재생은 재활용 불가능한 일회성 묘기에 그치고 일반화된 정책이 될 수 없었습니다. 마지막으로, 비전(distillation) 기반 접근도 검토되었는데, 이는 아마도 시뮬레이션에서 시각적 관찰로 학습한 정책을 distill하여 현실에 적용하려 한 시도였습니다. 그러나 이 경우 물체(펜)가 회전하며 움직일 때 카메라 기반 인식에 오차가 커지고, 훈련 분포에서 벗어난(OOD) 시각 정보가 입력되면서 정책 성능이 불안정해졌습니다. 결과적으로 물체가 화면에서 흔들려 보이는(oscillates) 상황에서 비전 정책은 큰 에러를 일으켰고, 펜 돌리기를 유지하지 못했습니다. 이러한 비교 실험들은 본 논문의 혼합 접근법이 왜 필요한지 잘 뒷받침해줍니다. 요컨대, 시뮬레이션 학습만으로는 부족하지만, 그렇다고 현실 데이터만으로 처음부터 학습하기엔 탐색이 불가능한 이 딜레마 상황에서, 시뮬레이션의 성공 경험을 최대한 활용하고 최소한의 현실 경험으로 보완하는 것이 핵심이라는 점을 입증한 셈입니다."
  },
  {
    "objectID": "posts/paper/2025-08-01-spin-pens.html#배운-교훈-및-한계",
    "href": "posts/paper/2025-08-01-spin-pens.html#배운-교훈-및-한계",
    "title": "📃Spin pens 리뷰",
    "section": "배운 교훈 및 한계",
    "text": "배운 교훈 및 한계\n이 연구를 통해 얻은 교훈(lessons)들은 펜 돌리기 과제에만 국한되지 않고, 일반적인 로봇 강화학습과 시뮬레이션 활용에 시사하는 바가 큽니다. 저자들은 논문에서 개발 과정에서 느낀 주요 교훈을 다음과 같이 정리하였습니다:\n\n탐색을 위한 시뮬레이션 설계의 중요성: 시뮬레이션에서 강화학습을 성공시키려면 탐색이 충분히 이루어지도록 환경과 보상을 꼼꼼히 설계해야 합니다. 초기 상태 분포를 적절히 다양하게 만들어 에이전트가 여러 상황을 접하게 하고, 학습을 돕는 특권 정보를 활용하는 등 세심한 디자인이 필요했습니다. 이러한 노력이 뒷받침되어야 비로소 시뮬레이터 상에서 어려운 기술이 발현될 수 있다는 것입니다.\n단순한 Sim-to-Real은 통하지 않는다: 접촉이 많은 고난이도 동작일수록, 시뮬레이션 결과를 그대로 현실에 가져오는 것은 거의 실패한다고 볼 수 있습니다. 연구진이 촉각, 시각 센서 등을 개별적으로 배제해보는 등 여러 방법을 시도해봤지만, 물리 엔진과 실제 세계의 근본적 차이는 남았습니다. 광범위한 도메인 랜덤화조차도 이 격차를 완전히 메꾸지 못했고, 결국 현실 데이터의 직접적인 활용이 불가피하였습니다.\n그래도 시뮬레이션은 유용하다: 비록 시뮬레이션 결과만으로 완성품을 얻을 순 없지만, 시뮬레이터는 여전히 새로운 기술을 탐색하는 데 필수적입니다. 펜 돌리기와 같은 역동적 스킬은 인간이 로봇을 원격으로 조종하며 가르치기에는 거의 불가능에 가깝습니다. 이런 경우 강화학습을 통해 시뮬레이션에서 성공 사례를 찾아내는 과정이 있었기에, 초기 정책과 궤적을 확보할 수 있었습니다. 이는 다른 복잡한 로봇 기술 학습에도 시사하는 바가 있으며, 시뮬레이션을 탐색 도구로 적극 활용하되 그 한계를 인지하는 균형 잡힌 접근이 필요합니다.\n현실 데이터는 생각보다 적게 필요하다: 희망적인 소식은, 시뮬레이터에서 학습한 정책을 잘 활용하면 소량의 현실 성공 데이터로도 충분히 정책을 보정할 수 있다는 점입니다. 이번 연구에서는 불과 50여 개 미만의 실행 궤적으로도 정책을 현실에 적응(fine-tuning)시킬 수 있음을 보여주었습니다. 즉, 시뮬레이션으로 기본기를 익혀 놓으면 이후 현실에서는 몇십 차례의 실험만으로도 높은 수준의 성능을 얻어낼 수 있다는 뜻입니다. 이는 로봇 학습 분야에서 데이터 효율성 측면으로 큰 의미가 있습니다.\n\n이러한 교훈들과 더불어, 본 연구에서 드러난 몇 가지 한계점도 짚고 넘어가겠습니다. - 첫째, 최종 정책은 시각 센서에 의존하지 않는 proprioceptive(고유감각) 기반 정책입니다. 이는 펜의 위치나 움직임을 로봇 손의 관절 센서 등으로만 추정한다는 의미인데, 이러한 접근은 물체의 형태나 주변 환경 변화를 인지하진 못하기 때문에 범용성 면에서는 한계가 있습니다. - 둘째, 논문에서 보고된 실패 사례들을 보면 현 방법의 하드웨어적 한계도 나타납니다. 예를 들어, 로봇 손의 제어 주파수(control frequency)가 충분히 높지 않아 빠르게 떨어지는 물체를 붙잡지 못하는 경우가 있었습니다. 펜이 손가락에서 살짝 이탈할 때 재빨리 대응해야 하지만, 현재 시스템의 속도론 역부족이었다는 것입니다. 또한 펜이 돌면서 무게중심(center of mass)이 미세하게 변해 균형이 깨지는 경우도 있었는데, 이런 상황에서는 시스템이 불안정해져 실패가 발생했습니다. 이러한 문제들은 하드웨어 성능이나 제어 알고리즘을 개선해야만 극복할 수 있는 부분으로 보입니다. - 마지막으로, 비록 여러 종류의 펜을 다뤘다고는 하나 모든 형태의 도구로 일반화되었다고 보긴 어렵습니다. 펜 돌리기는 비교적 대칭적이고 길쭉한 물체라 가능한 면도 있는데, 전혀 다른 모양이나 질감의 물체를 다룰 때도 이 접근법이 유효할지는 추가 검증이 필요합니다. 향후에는 더 복잡한 조작 과제(예: 공중에서 던졌다 받기, 비대칭 물체 다루기 등)에도 이번 기법을 확장하여 테스트해볼 수 있을 것입니다."
  },
  {
    "objectID": "posts/paper/2025-08-01-spin-pens.html#결론",
    "href": "posts/paper/2025-08-01-spin-pens.html#결론",
    "title": "📃Spin pens 리뷰",
    "section": "결론",
    "text": "결론\n“Lessons from Learning to Spin ‘Pens’” 논문은 로봇 강화학습 연구에서 하나의 이정표가 될 만한 흥미로운 성과를 보여주었습니다. 이들은 어려운 펜 돌리기 과제를 해결하는 과정에서, 시뮬레이션의 탐색 능력과 현실의 정확한 피드백을 결합하는 창의적인 전략을 선보였습니다. 이를 통해 소량의 현실 데이터만으로도 복잡한 기술을 학습할 수 있음을 증명했고, 기존 방법들이 처했던 시뮬레이션-현실 격차의 함정을 효과적으로 극복했습니다. 특히 강화학습으로 로봇 손에 핑거게이팅과 같은 인간 유사 전략이 자발적으로 발현된 점, 그리고 연속 회전이라는 난제를 달성해냈다는 점은 주목할 만합니다. 물론 아직 해결해야 할 한계들도 존재하지만, 저자들이 공유한 교훈들은 향후 로봇 학습 연구에 귀중한 지침이 될 것입니다. 궁극적으로, 이 연구는 “어렵다고 여겨진 로봇 기술도 올바른 학습 전략을 통해 극복 가능하다”는 희망을 보여주었습니다. 앞으로 펜 돌리기를 넘어서, 로봇이 더욱 다양한 도구를 능숙하게 다루는 모습을 기대해 봐도 좋겠습니다."
  },
  {
    "objectID": "posts/paper/2025-08-19-dexpilot.html",
    "href": "posts/paper/2025-08-19-dexpilot.html",
    "title": "📃DexPilot 리뷰",
    "section": "",
    "text": "Paper Link\nProject Link"
  },
  {
    "objectID": "posts/paper/2025-08-19-dexpilot.html#연구-개요-및-기여",
    "href": "posts/paper/2025-08-19-dexpilot.html#연구-개요-및-기여",
    "title": "📃DexPilot 리뷰",
    "section": "1. 연구 개요 및 기여",
    "text": "1. 연구 개요 및 기여\nDexPilot은 고자유도(23 DoF)의 다지능 로봇 손–팔 시스템을 저비용·시각 기반으로 원격 조작하기 위한 시스템이다. 전통적인 텔레오퍼레이션 시스템은 고자유도 로봇 제어시 고가의 센서(글러브, 마커, 모션캡처 등)를 요구하지만, DexPilot은 실제 인간의 맨손 움직임만으로 23자유도의 Allegro 로봇 손과 로봇 팔을 직접 모사·제어한다.\n주요 기여로는\n\n마커나 장갑 없이 순수 RGB-D 카메라로 인간 손을 추적하여 로봇 손에 전사하는 시각 기반 글러브-프리 텔레오퍼레이션 구현,\n손 끝(fingertip) 위치 및 방향을 보존하면서 인간 손 관절 상태를 Allegro 손 관절로 매핑하는 새로운 비용 함수 및 투영(projection) 기법 제안,\n정밀한 집기(pinching)와 다중 단계 조작을 포함한 다양한 과제(지폐 추출, 서랍 열기, 약병 개봉 등)에서 23DoF 시스템 조작을 시연,\n두 명의 파일럿으로 진행한 실험에서 속도 및 성공률 지표로 시스템 성능을 평가이다.\n\n이 결과 고자유도 로봇 조작용 대용량 상태·행동(상태/액션) 데이터 수집이 가능하며, 향후 머신러닝 기반 조작 정책 학습에 유용한 데이터셋을 제공할 수 있다."
  },
  {
    "objectID": "posts/paper/2025-08-19-dexpilot.html#시스템-구성-및-손-추적-방법",
    "href": "posts/paper/2025-08-19-dexpilot.html#시스템-구성-및-손-추적-방법",
    "title": "📃DexPilot 리뷰",
    "section": "2. 시스템 구성 및 손 추적 방법",
    "text": "2. 시스템 구성 및 손 추적 방법\nDexPilot의 하드웨어는 KUKA LBR iiwa7 협동로봇 팔과 Wonik Allegro 손으로 구성되며, Allegro 손 끝에는 Biotac 촉각 센서를 장착하였다. 사람 파일럿 영역에는 검은색 천으로 덮인 테이블 위에 4대의 Intel RealSense D415 RGB-D 카메라가 배치되어, 인간 손을 여러 시점에서 관찰한다.\n시스템은 세 개의 처리 스레드로 병렬 실행된다.\n\n학습 스레드는 4개 카메라의 융합된 포인트 클라우드로부터 손의 자세 및 관절각을 추정하는 신경망을 실행하며, 이를 통해 얻은 초기 추정값을 하위 모듈에 제공한다.\n추적 스레드는 DART(Differentiable Articulated Rigid-body Tracker)를 사용하여 인간 손 모델의 6자유도 위치 및 20개 관절(각 손가락당 4개: 1 abduction, 3 flexion)의 자세를 지속적으로 최적화 추적한다. 이때, 신경망이 제공한 손 위치/관절각 예측이 초기값(prior)으로 사용되어 로컬 미니마로 빠지는 것을 방지한다.\n제어 스레드는 Riemannian Motion Policy(RMP) 기반의 제어 방정식을 계산하여 Allegro 손바닥의 목표 위치·자세와 팔 동작을 생성한다. 전체 시스템의 엔드-투-엔드 지연(latency)은 약 1초 정도로 보고되었다.\n\n시각 기반 손 추적을 위해 DexPilot은 두 단계의 딥러닝 모델과 DART 최적화를 결합하였다.\n\n첫 번째 단계에서는 파일럿이 착용한 컬러 장갑(glove)을 활용하여 학습 데이터를 얻는다. 장갑의 손가락 끝과 손바닥에 서로 다른 색의 점을 부착하고, 4대의 RGB 카메라로 관찰한 RGB 영상을 ResNet-50 기반의 회귀 네트워크(GloveNet)를 통해 색점의 2D 위치를 추정한다. 이렇게 얻은 2D 좌표에 깊이(depth)를 결합해 3D 위치를 계산하고, 그로부터 손의 포즈(세 점의 위치)와 분할(segmentation)을 구한다. 이 정보를 이용해 DART가 손 모델을 세분화(segmented point cloud)에 맞추어 최적화하도록 함으로써, 초기에는 장갑을 쓴 상태에서 정확한 손 관절각 어노테이션을 생성한다.\n두 번째 단계에서는 장갑 없이 생 데이터를 사용한다. 4개 카메라의 깊이 영상을 융합하여 테이블 평면을 제거한 후, 남은 손·팔 포인트클라우드를 PointNet++ 기반 네트워크에 입력한다. 이 네트워크는 손 부분을 분리하고(손분할), 손뼈의 23개 주요 관절점(keypoints; 손가락당 4개, 손바닥 후면 3개)를 3D 좌표로 회귀한다. 첫 단계의 손바닥 컬러 장갑 방식으로 생성된 어노테이션을 학습에 사용하여, 실제 맨손 데이터에서도 손 관절 포즈를 예측한다. 또한, 23개 키포인트를 20차원 관절각으로 변환하기 위한 JointNet(2층 완전연결망)도 함께 학습시켰다. 이 딥 네트워크들 덕분에 DART 추적이 장기간 안정적으로 수행되며, 검증 셋에서 평균 키포인트 오차는 약 9.7mm, 관절각 오차는 약 1.33°로 보고되었다. 결과적으로, DexPilot은 카메라 포인트클라우드→키포인트→관절각 추정→DART 미세조정의 파이프라인을 통해 인간 손의 포즈와 관절 상태를 실시간으로 얻어낸다."
  },
  {
    "objectID": "posts/paper/2025-08-19-dexpilot.html#인간-로봇-손-매핑-전략-및-수학적-모델링",
    "href": "posts/paper/2025-08-19-dexpilot.html#인간-로봇-손-매핑-전략-및-수학적-모델링",
    "title": "📃DexPilot 리뷰",
    "section": "3. 인간-로봇 손 매핑 전략 및 수학적 모델링",
    "text": "3. 인간-로봇 손 매핑 전략 및 수학적 모델링\n인간 손과 Allegro 로봇 손은 관절 수, 관절축 배치, 손가락 길이 등이 크게 다르기 때문에 단순한 대응(mapping)이 불가능하다. DexPilot은 정밀 조작 관점에서 손끝(fingertip) 작업 공간(task-space) 을 최우선시하여 두 손의 동작을 연결한다.\n\n\n\n손끝을 잇는 위치와 방향 정보가 인간·로봇 손의 주요 조작을 결정한다고 보고, 이들 사이 거리를 최소화하는 최적화 기반 매핑(cost function)을 설계하였다. 구체적으로, 인간 손 자세 q_h와 Allegro 손 관절 q_a에 대해 다음과 같은 비용 함수를 정의:\n C(q_h, q_a) = \\frac{1}{2}\\sum_{i=1}^N s(d_i)\\,|r_i(q_a) - f(d_i)\\,\\hat{r}_i(q_h)|^2 \\;+\\; \\gamma|q_a|^2, \n\n여기서 r_i(q)는 손바닥(origin)으로부터 i번째 손끝까지의 벡터(또는 손가락 간 벡터)로, 각각 Allegro 손(r_i(q_a))과 인간 손 모델(\\hat{r}_i(q_h))의 작업 공간에서 계산된다.\n\\hat{r}_i(q_h)=r_i(q_h)/d_i는 정규화된 인간 손 벡터이며, d_i=|r_i(q_h)|\ns(d_i)는 가중치 함수로서 인간 손의 엄지가 i번째 벡터(r_i(q_h))와 가깝게 접촉할 때 손끝 간 거리에 더 큰 중요도를 부여한다. 예를 들어, 임계거리 \\epsilon 이하로 가까워지면 엄지와 손끝이 대응되는 벡터 집합 S1일 때 s(d_i)=200, 손끝 쌍(S2)에 대해서는 s(d_i)=400 등으로 급격히 증가시킨다. 반면 거리가 \\epsilon 이상이면 s(d_i)=1으로 작게 준다.\nf(d_i)는 거리 조절 함수로서, 보통 f(d_i)=\\beta d_i (증폭계수 \\beta=1.6)로 손끝 간 거리를 그대로 복사하지만, 임계거리 이하일 때 손가락끼리 겹치지 않도록 일정 거리(\\eta_1,\\eta_2)를 강제한다. 예를 들어 엄지-주요 손가락 사이가 너무 가까워지면 \\eta_1=0.1\\;\\mathrm{mm}로 접촉 거리를 유지시켜 핀치 집기를 가능하게 하며, 주요 손가락 간에는 \\eta_2=30\\;\\mathrm{mm}로 일정 거리를 확보한다.\n마지막으로 \\gamma|q_a|^2 항(정규화 항)은 Allegro 손을 완전히 펼친 상태(q_a=0)로 유도하여 중복성(redundancy)을 완화하고 기괴한 최소해(예: 손가락이 손바닥에 파고드는 현상)를 방지한다. 이때 사용하는 벡터 집합 S1, S2는 표 I에 정의된 것처럼 “엄지와 주요 손가락(검지·중지·약지) 사이 벡터”와 “엄지와 각각 매핑된 두 주요 손가락 사이 벡터”로 구성한다. 또한, 해 공간 크기를 줄이기 위해 Allegro 손의 검지·중지·약지 각각에 대해 원위관절(distal joint)의 각도를 중간관절(medial joint)과 같게 고정하는 제약을 두었다. 이와 같이 설계된 비용 함수를 매 프레임마다 최소화하면 인간 손의 손끝 배치와 유사한 Allegro 손 구성이 생성된다.\n\n최적화는 NLopt 라이브러리의 SLSQP(순차적 이차계획법) 알고리즘으로 실시간 수행된다. 초기 프레임에는 Allegro 각도를 모두 0(완전 펼침)으로 시작하고, 이후 매 프레임은 이전 프레임 해를 초기값으로 사용하여 연속성을 유지한다. 인간 손 모델과 Allegro 손의 순방향 기구학 계산에는 Orocos KDL 라이브러리를 사용하였다. 최적화 결과로 얻은 Allegro 관절각은 고주파 노이즈를 억제하기 위해 1차 저역 통과 필터를 거쳐 출력한다. 결과적으로 이 리타겟팅 모듈은 인간 파일럿이 손을 구부리거나 엄지와 손가락 사이 거리를 조절할 때, 그 손끝 동작이 로봇 손에서도 자연스럽게 재현되도록 동작한다."
  },
  {
    "objectID": "posts/paper/2025-08-19-dexpilot.html#손-리타겟팅-모듈-동작-원리와-제약조건",
    "href": "posts/paper/2025-08-19-dexpilot.html#손-리타겟팅-모듈-동작-원리와-제약조건",
    "title": "📃DexPilot 리뷰",
    "section": "4. 손 리타겟팅 모듈 동작 원리와 제약조건",
    "text": "4. 손 리타겟팅 모듈 동작 원리와 제약조건\nDexPilot의 리타겟팅 모듈은 추적 스레드 내부에서 작동하며, 인간 손 추적 결과를 Allegro 손 제어 명령으로 변환하는 실시간 최적화 엔진이다. 매 주기마다 앞서 계산된 인간 손 관절각을 입력으로 하여 위의 비용 함수를 최소화하며, 이때 s(d_i)나 f(d_i) 등의 기법으로 엄지-검지 간 픽스쳐 동작을 강제한다. 예를 들어, 지폐를 핀치할 때와 같이 엄지와 검지 사이 거리가 작아져 d_i&lt;\\epsilon이 되면 손끝 간 거리를 유지하도록 f(d_i)가 작아지며, 동시에 가중치 s(d_i)가 커져 해당 손끝 벡터 항이 비용에 크게 반영된다. 이러한 투영(projection) 기법은 카메라 기반 추적의 오차에도 불구하고 정확한 핀치 자세를 유도할 수 있게 해주지만, 후술할 작은 물체 놓기 등의 상황에서는 손가락을 너무 오래 유지하게 만드는 부작용도 발생할 수 있다.\n리타겟팅 최적화는 실시간으로 실행되어야 하므로, 계산 복잡도를 줄이고 솔루션의 연속성을 보장하는 여러 제약조건도 적용된다. 먼저 \\gamma|q_a|^2 정규화 항을 통해 해 공간의 중복성을 억제하며, 동일한 효과로 앞서 언급한 검지·중지·약지의 distal=medial 고정 제약도 도입한다. 이와 함께, 최적화 초기값을 이전 결과로 설정하여 연산 비용과 진동을 완화한다. 마지막으로 로봇과 카메라 좌표계 정합(calibration)을 통해 원하는 초기 손 자세(펼친 손, 손바닥 평행)를 시스템에 맞추어 파일럿의 손과 로봇 손이 일치하도록 설정한다. 종합하면, DexPilot의 리타겟팅 모듈은 비선형 최적화 기반이며, 손끝 위치·방향 작업 공간을 보존하기 위한 비용 함수에 의해 인간 손동작을 Allegro 관절값으로 변환한다. 추가적인 필터링과 제약을 통해 부드럽고 물리적으로 타당한 움직임을 보장하며, 이를 통해 인간 파일럿의 손 제스처는 정교하게 로봇 손으로 복제된다."
  },
  {
    "objectID": "posts/paper/2025-08-19-dexpilot.html#실험-설정-및-성능-평가",
    "href": "posts/paper/2025-08-19-dexpilot.html#실험-설정-및-성능-평가",
    "title": "📃DexPilot 리뷰",
    "section": "5. 실험 설정 및 성능 평가",
    "text": "5. 실험 설정 및 성능 평가\nDexPilot 시스템의 성능은 다양한 조작 과제(task)에서 측정되었다. 사용된 실험 장비는 앞서 설명한 KUKA iiwa7+Allegro 손, 4대의 Intel RealSense D415 카메라이다[9]. 실험에서 파일럿(조종사)은 테이블 위에서 정해진 물체를 조작해야 했으며, 실험 과제는 총 15가지가 제시되었다(표 II 참조). 여기에는 단순 물체 옮기기(pick-and-place)부터, 동전 내지 지폐를 지갑에서 꺼내기(그림 11), 서랍 열기 및 티백 꺼내기(그림 12), 땅콩통 뚜껑 풀기(그림 13) 같은 다단계 작업들이 포함.\n각 과제마다 파일럿 2명이 5회 연속 시도하며 성공률을 측정했고, 완료 시간(mean completion time)도 기록했다. 결과적으로, DexPilot은 대부분 과제에서 높은 성공률을 보였다(Fig. 15). 특히 단순 피킹/플레이스 작업이나 비교적 큰 물체 조작 작업들은 대부분 성공률 90–100%에 달했다. 평균 완료 시간은 과제 난이도와 복잡도에 따라 다양했는데, 멀티스텝 작업(예: 서랍 속 물건 꺼내기)일수록 수 분이 소요되었다. 전반적으로 시스템은 정밀 집기·파지, 다지 간 조작, 비파지(non-prehensile) 동작 등을 모두 수행할 수 있는 충분한 유연성과 안정성을 보였다. 정성적 평가에서도 DexPilot의 성능을 확인할 수 있다. 예를 들어 그림 11의 지갑 과제에서 파일럿은 지폐를 손가락 사이에 핀치한 채로 성공적으로 지갑 바깥으로 끄집어냈으며, 이때 로봇 손도 지폐를 놓치지 않고 유지했다. 그림 12에서는 서랍을 열고 티백을 잡아 당기기 위한 손가락의 회전 및 접촉 동작이 명확히 구현되었으며, 그림 13의 땅콩통 뚜껑 과제에서는 뚜껑을 반복 회전시키는 동작이 로봇에도 그대로 전달되었다. 이처럼 작은 물체를 집거나 돌리는 정밀 동작 뿐 아니라, 두 손가락으로 물체를 잡은 상태에서 남은 손가락을 이용해 추가 조작을 수행하는 복합 조작(compound manipulation)도 모두 사람이 행하듯 수행 가능함을 보였다.\n그러나 작은 물체를 다루는 작업에서는 한계도 관찰되었다. 예를 들어 크기가 작은 블록(pick blocks small)이나 컨테이너 속 물체 뽑기(Container) 등의 작업은 완료 시간이 길거나 성공률이 낮았다. 특히 작은 블록을 쥐었다가 놓는 과정에서, 앞서 설명한 투영 기법이 손가락 간 거리를 강하게 조절하여 물체를 늦게 놓게 만들거나 손가락이 간섭하는 현상이 발생했다. 결과적으로 작은 블록 옮기기 과제의 경우 성공률이 상대적으로 현저히 낮았고, 완료 시간이 매우 길어졌다. 이러한 현상은 장갑 기반 추적 데이터의 부정확성이나 투영 파라미터 조정에 기인한 것으로 분석된다."
  },
  {
    "objectID": "posts/paper/2025-08-19-dexpilot.html#기존-방법과-비교-및-기술적-한계",
    "href": "posts/paper/2025-08-19-dexpilot.html#기존-방법과-비교-및-기술적-한계",
    "title": "📃DexPilot 리뷰",
    "section": "6. 기존 방법과 비교 및 기술적 한계",
    "text": "6. 기존 방법과 비교 및 기술적 한계\nDexPilot은 글러브·마커를 사용하지 않는 순수 시각 기반 시스템이라는 점에서 독창적이다. 기존 상용 시스템들(예: CyberGlove, HaptX)은 높은 정확도의 관절 추정과 촉각 피드백을 제공하지만, 장비 비용과 부피가 크고 사용자의 자유로운 움직임을 제한한다. 반면 DexPilot은 저렴한 RGB-D 카메라 네트워크만으로 23DoF 제어를 가능하게 하였고, 이는 종래의 글러브나 모션캡처 없이 복잡 조작을 수행한 사례로는 드물다. 기존 학술 연구와 비교해 보면, Li 등은 딥러닝으로 섀도우 핸드(Shadow Hand) 관절각을 추정하였으나 시스템 전체 적용과 정밀 집기에는 한계가 있었다. Antotsiou 등은 시뮬레이션 상의 간단한 조작 작업만 보였던 반면, DexPilot은 실제 물리적 환경에서 손끝 접촉과 연관된 복잡 작업을 수행했다. 이처럼 DexPilot은 시각-모델 추적, 최적화 기반 리타겟팅, 임피던스 제어를 결합하여 현장작업에 필요한 수준의 조작 성능을 보여주었다는 점에서 새로운 패러다임을 제시했다.\n그럼에도 몇 가지 기술적 한계가 보고되었다.\n\n첫째, 관찰 영역(workspace)이 카메라 범위로 제한되어 있어 넓은 공간에서의 조작에는 부적합하다. 실험에서는 카메라가 관절 거리 1m 이내에서 좋은 품질을 보였으나, 범위를 벗어나면 깊이 센싱 정확도가 급격히 떨어진다.\n둘째, 앞서 언급한 리타겟팅 투영 기법의 부작용이다. 엄지-검지 핀치 유지 시 잡은 물체를 늦게 놓거나 손가락끼리 간섭이 발생할 수 있으며, 이는 작은 물체 작업에서 효율을 떨어뜨린다. 현재는 이 기능을 옵션으로 끌 수 있도록 하였으나, 궁극적으로는 손 추적 정확도를 높여 이러한 보정이 필요 없도록 해야 한다.\n셋째, 촉각 피드백 부재로 인해 미세 조작이 어렵다. DexPilot에는 촉각 센서가 있어 데이터를 기록할 수 있으나, 파일럿에게는 힘/촉각 정보가 돌아가지 않는다. 이로 인해 물체가 미끄러질 때 직관적으로 감지하기 어려워 조작 실패율이 높아질 수 있다. 향후 촉각 피드백 전달이나 반자동 힘 제어(잡기 강도 자동 조절) 기술을 결합한다면 이 부담을 줄일 수 있을 것이다.\n넷째, 추적 및 제어 지연이다. 전체 시스템의 응답 지연은 약 1초이며, RMP 제어의 파라미터 튜닝과 네트워크 인퍼런스 지연을 최적화할 수 있다. 마지막으로 고정밀 삽입 작업(예: 페그-인-홀)은 아직 완벽히 수행되지 못했다. 실제로 NIST 삽입 과제를 시도했지만, 매우 협소한 간격(0.1mm)에서는 성공률이 10% 이하로 저조했다. 이는 카메라 해상도, 손 추적 정밀도, 제어 응답 속도 등 다양한 요인이 복합적으로 작용한 결과로, 한계를 극복하기 위해 더 정밀한 추적과 자동 제어 보조 기능이 필요하다.\n\n종합하면, DexPilot은 저비용 시각 기반 방식으로 고자유도 로봇 손을 조작 가능하게 한 획기적인 시스템이지만, 카메라 관측 범위, 손 추적 정확도, 촉각 부재 등 실제 활용 시 고려해야 할 한계점들도 동시에 지니고 있다. 이러한 한계들을 개선하면 앞으로 보다 정교한 텔레오퍼레이션과 로봇 학습 응용에 큰 기여를 할 수 있을 것이다."
  },
  {
    "objectID": "posts/paper/2025-06-04-neural-feels.html",
    "href": "posts/paper/2025-06-04-neural-feels.html",
    "title": "📃Neural feels with neural fields 리뷰",
    "section": "",
    "text": "🤖 이 논문은 로봇이 손 안에서 물체를 조작하는 동안 물체의 자세와 형태를 인식하는 NeuralFeels를 소개합니다.\n🧠 NeuralFeels는 비전과 촉각 센싱을 결합하여 신경 필드를 온라인으로 학습하고, 자세 그래프 최적화를 통해 이를 추적합니다.\n📈 이 방법은 객체 재구성과 자세 추적 성능을 크게 향상시키며, 특히 시각적 가림이 심한 상황에서 강점을 보입니다."
  },
  {
    "objectID": "posts/paper/2025-06-04-neural-feels.html#이-논문은-무엇을-다루고-있나",
    "href": "posts/paper/2025-06-04-neural-feels.html#이-논문은-무엇을-다루고-있나",
    "title": "📃Neural feels with neural fields 리뷰",
    "section": "2.1 1. 👋 이 논문은 무엇을 다루고 있나?",
    "text": "2.1 1. 👋 이 논문은 무엇을 다루고 있나?\n로봇이 물체를 손으로 잡고 움직일 때, 단순히 눈으로 보는 정보만으로는 부족한 경우가 많습니다. 특히 손가락으로 가려진 부분이나 접촉하는 면은 시각 정보만으로는 관찰할 수 없죠.\n이 논문에서는 이런 in-hand manipulation(손 안에서 조작) 상황에서, 📷 시각 정보(RGB-D) 와 ✋ 촉각 정보(GelSight) 를 통합하여,\n\n3D 물체 형상(Shape) 과\n접촉 상태(Contact) 를 실시간으로 추론하는 모델인 NeuralFeels를 제안합니다.\n\n핵심 개념은 단순합니다:\n\n시각이 놓치는 부분은 촉각으로 보완하자. 그리고 이 정보를 Neural Field 형태로 부드럽게 표현하자."
  },
  {
    "objectID": "posts/paper/2025-06-04-neural-feels.html#배경-지식-neural-field와-촉각-센서",
    "href": "posts/paper/2025-06-04-neural-feels.html#배경-지식-neural-field와-촉각-센서",
    "title": "📃Neural feels with neural fields 리뷰",
    "section": "2.2 2. 🔧 배경 지식: Neural Field와 촉각 센서",
    "text": "2.2 2. 🔧 배경 지식: Neural Field와 촉각 센서\n\n2.2.1 🔹 Neural Field란?\nNeural Field는 공간의 연속적인 물리량(예: 밀도, 색, 거리 등)을 예측하는 신경망 기반 함수 표현입니다. 대표적인 예가 NeRF(Neural Radiance Fields)로, 한 점의 위치와 시점을 입력으로 받아 해당 점의 색과 밀도를 예측합니다.\n이 논문에서는 NeRF 대신 Signed Distance Function(SDF) 기반 Field를 사용합니다. SDF는 어떤 점이 물체의 표면에서 얼마나 떨어져 있는지를 나타내는 스칼라 값입니다.\n\n0이면 표면 위,\n음수면 내부,\n양수면 외부.\n\nNeuralFeels는 이 SDF를 학습하여 물체 형상을 연속적으로 표현합니다.\n\n\n2.2.2 🔹 GelSight 센서란?\nGelSight는 물체 표면의 미세한 형상과 접촉 강도를 고해상도로 추출할 수 있는 촉각 센서입니다. 물리적으로는 젤 같은 투명한 물질에 고무막을 덮고, 그 아래에 카메라를 설치하여 변형된 표면을 시각적으로 읽어내는 장치입니다."
  },
  {
    "objectID": "posts/paper/2025-06-04-neural-feels.html#neuralfeels의-구조-이해하기",
    "href": "posts/paper/2025-06-04-neural-feels.html#neuralfeels의-구조-이해하기",
    "title": "📃Neural feels with neural fields 리뷰",
    "section": "2.3 3. 🧠 NeuralFeels의 구조 이해하기",
    "text": "2.3 3. 🧠 NeuralFeels의 구조 이해하기\nNeuralFeels는 크게 두 개의 neural field로 구성됩니다:\n\n\n\n\n\n\n\n\n\n컴포넌트\n역할\n입력\n출력\n\n\n\n\n🔵 Shape Field\n3D 형상 추정 (SDF 예측)\nRGB-D + Tactile Depth\nSDF 값\n\n\n🔴 Contact Field\n손가락-물체 접촉 부위 예측\n손가락 위치 + SDF\n접촉 확률\n\n\n\n\n2.3.1 ✨ Shape Field: 형상을 그리는 촉각\n\n기본적으로 RGB-D를 통해 관찰된 시점의 점들을 SDF supervision으로 사용합니다.\n촉각으로 측정된 표면은 occluded region의 SDF ground-truth로 활용됩니다.\n손가락으로 가려진 영역도 촉각으로 재구성 가능한 게 포인트입니다.\n\n\n\n2.3.2 ✨ Contact Field: 손끝의 압력을 확률로\n\n손가락 링크의 위치를 기준으로 공간 샘플링.\nSDF가 0에 가까운 위치 중, 실제로 접촉한 tactile evidence가 있는 곳에 contact 확률을 높이도록 학습."
  },
  {
    "objectID": "posts/paper/2025-06-04-neural-feels.html#어떻게-학습하고-평가했나",
    "href": "posts/paper/2025-06-04-neural-feels.html#어떻게-학습하고-평가했나",
    "title": "📃Neural feels with neural fields 리뷰",
    "section": "2.4 4. ⚙️ 어떻게 학습하고 평가했나?",
    "text": "2.4 4. ⚙️ 어떻게 학습하고 평가했나?\n\n2.4.1 🧾 데이터셋: Visuo-Tactile In-Hand Manipulation Dataset\n\n6가지 일상 물체 (컵, 병, 상자 등)\n다관절 로봇 손으로 다양한 조작 (돌리기, 들기, 눌러보기)\nRGB-D 영상 + Gelsight 촉각 정보 + 손-물체 포즈 정보\n\n\n\n2.4.2 🧪 실험 평가 항목\n\nSDF 재구성 정확도 (Chamfer Distance)\n접촉 예측 정확도 (Contact Classification)\nOccluded 영역 복원 성능 비교"
  },
  {
    "objectID": "posts/paper/2025-06-04-neural-feels.html#실험-결과-요약",
    "href": "posts/paper/2025-06-04-neural-feels.html#실험-결과-요약",
    "title": "📃Neural feels with neural fields 리뷰",
    "section": "2.5 5. 📊 실험 결과 요약",
    "text": "2.5 5. 📊 실험 결과 요약\n\n\n\n평가 항목\n기존 방법\nNeuralFeels\n성능 향상\n\n\n\n\nSDF 오차 ↓\n0.86 mm\n0.54 mm\n-37%\n\n\n접촉 예측 정확도 ↑\n75.3%\n91.7%\n+16%\n\n\nOcclusion 복원 품질\n낮음\n우수함\n✅\n\n\n\n\n2.5.1 🔍 주요 인사이트\n\nVision-only는 물체의 뒤나 접촉면을 거의 추론 못함.\n촉각 정보를 supervision으로 넣자 hidden surface 복원 능력이 극적으로 향상됨."
  },
  {
    "objectID": "posts/paper/2025-06-04-neural-feels.html#기술적-통찰",
    "href": "posts/paper/2025-06-04-neural-feels.html#기술적-통찰",
    "title": "📃Neural feels with neural fields 리뷰",
    "section": "2.6 6. 💡 기술적 통찰",
    "text": "2.6 6. 💡 기술적 통찰\n\n2.6.1 ✔️ 왜 좋은 아이디어인가?\n\n촉각 정보를 “단순 피드백”이 아니라 “지각 학습의 supervision”으로 사용한 점이 탁월합니다.\nNeRF 기반의 3D 표현력과 tactile의 세밀한 접촉 감지를 결합해, 기존보다 훨씬 현실감 있는 지각이 가능해졌습니다.\n\n\n\n2.6.2 ✔️ 특히 눈에 띄는 부분\n\nContact Field는 단순 contact point를 넘어서 “접촉 확률 분포”로 표현됩니다.\n이는 Grasp Refinement, Slip Detection, Force Control 등 downstream task에 매우 유용합니다."
  },
  {
    "objectID": "posts/paper/2025-06-04-neural-feels.html#한계점-및-고민거리",
    "href": "posts/paper/2025-06-04-neural-feels.html#한계점-및-고민거리",
    "title": "📃Neural feels with neural fields 리뷰",
    "section": "2.7 7. ⚠️ 한계점 및 고민거리",
    "text": "2.7 7. ⚠️ 한계점 및 고민거리\n\n2.7.1 🛠️ 하드웨어 의존성\n\nGelsight 센서는 고가이며 설치 복잡 → 실사용 시스템 구축 난이도 ↑\n\n\n\n2.7.2 🧠 추론은 빠르나 학습은 느림\n\nInference는 30Hz 이상 가능하지만, 학습은 한 객체당 수 시간 소요됨\n\n\n\n2.7.3 🔄 제어 시스템과 통합은 미완성\n\nperception 모듈은 훌륭하지만, 실시간 manipulation loop과 연결된 완전한 policy는 아직 제안되지 않음"
  },
  {
    "objectID": "posts/paper/2025-06-04-neural-feels.html#그리고-우리는-어떤-질문을-던질-수-있을까",
    "href": "posts/paper/2025-06-04-neural-feels.html#그리고-우리는-어떤-질문을-던질-수-있을까",
    "title": "📃Neural feels with neural fields 리뷰",
    "section": "2.8 8. 🤔 그리고 우리는 어떤 질문을 던질 수 있을까?",
    "text": "2.8 8. 🤔 그리고 우리는 어떤 질문을 던질 수 있을까?\n\n저가형 센서에서도 같은 방식이 가능할까? 예: ReSkin, uSkin처럼 범용성 높은 자성 기반 센서로도 SDF 학습이 가능할까?\n실시간 업데이트 가능성은? 현재는 offline 학습 후 추론만 실시간. 실시간 online update가 된다면 slip feedback 등에 바로 반영 가능.\nGeneralization은 어떻게 보장할까? 물체가 바뀌었을 때, 손 모양이 달라졌을 때 얼마나 robust한가?"
  },
  {
    "objectID": "posts/paper/2025-06-04-neural-feels.html#향후-연구로-이어질-수-있는-아이디어",
    "href": "posts/paper/2025-06-04-neural-feels.html#향후-연구로-이어질-수-있는-아이디어",
    "title": "📃Neural feels with neural fields 리뷰",
    "section": "2.9 9. 🌱 향후 연구로 이어질 수 있는 아이디어",
    "text": "2.9 9. 🌱 향후 연구로 이어질 수 있는 아이디어\n\nPolicy-level 학습 통합: SDF + Contact Field를 조건으로 하는 강화학습 기반 manipulation policy 학습\nDomain Adaptation 연구: tactile 없는 상황에서 pre-trained model을 어떻게 활용할 수 있을까?\nSimulation to Real Transfer: GelSight 시뮬레이터를 통한 대규모 학습 → 실제 환경 적용"
  },
  {
    "objectID": "posts/paper/2025-06-04-neural-feels.html#마무리",
    "href": "posts/paper/2025-06-04-neural-feels.html#마무리",
    "title": "📃Neural feels with neural fields 리뷰",
    "section": "2.10 10. 📌 마무리",
    "text": "2.10 10. 📌 마무리\nNeuralFeels는 시각과 촉각이라는 이질적인 두 감각을 하나의 신경 표현 안에 통합한 인상적인 연구입니다. 특히 그 통합 방식을 Neural Field로 추상화하여 연속적이고 해석 가능한 형태로 만든 점은 향후 로봇 촉각지각 연구의 중요한 이정표가 될 수 있습니다.\n촉각 센서의 발전과 함께 이런 멀티모달 field 기반 방법은 더욱 빛을 발할 것으로 기대됩니다. 로봇이 ‘보는’ 것에서 ‘느끼는’ 존재로 진화해 가는 흐름을 이 논문이 잘 보여주고 있죠."
  },
  {
    "objectID": "posts/paper/2025-06-04-neural-feels.html#참고자료",
    "href": "posts/paper/2025-06-04-neural-feels.html#참고자료",
    "title": "📃Neural feels with neural fields 리뷰",
    "section": "2.11 🧾 참고자료",
    "text": "2.11 🧾 참고자료\n\n논문 링크 (arXiv)\nGelSight 기술 개요\nNeRF 개념 설명 블로그\nOriginal Paper\nProject Homepage"
  },
  {
    "objectID": "posts/paper/2024-11-10-ipo.html",
    "href": "posts/paper/2024-11-10-ipo.html",
    "title": "📃IPO 리뷰",
    "section": "",
    "text": "오늘은 “IPO: Interior-point Policy Optimization under Constraints”라는 논문에 대해서 리뷰해보려고 합니다. 흔히 강화학습(Reinforcement Learning)을 처음 개념을 공부하고 나면, 강화학습의 문제를 MDP(Markov Decision Process)로 정의한다는 것을 떠올릴 수 있습니다. 이때 강화학습의 핵심인 Reward, 즉 보상을 잘 설정해주어야 Agent가 원하는 방향대로 학습을 하게 됩니다. 보상은 Agent가 해야하는 행동 양식의 (+)가 되는 방향을 나타내는 지 표이며 우리가 원하는 행동을 Encourage(장려)하는 역할을 하게 됩니다.\n이번 논문에서는 기본적인 강화학습의 MDP가 아닌 Constraint라는 개념을 넣어서 생각을 해보려고 합니다. Constraint(제약)은 가장 단순하게는 -Reward 라고 생각해볼 수 도 있습니다. 우리가 Agent가 하지 않았으면 하는 행동을 정의함으로써 negative reward를 준다고 볼 수 있는 것이죠. (마치 Gradient Ascent가 Gradient Discent의 반대로 생각해볼 수 있듯이요.) 따라서 Reward와 Constraint는 서로 (+)/(-) 부호적인 성격이 다르지만 Agent에게 학습의 방향을 제시하는 신호라는 측면에서는 공통점을 가지고 있습니다.\n조금 더 Constraint에 대해서 자세히 살펴보겠습니다. Constraint는 제약이 발생되는 시점에 따라 2가지로 나누어서 생각해 볼 수 있습니다.\n\n\n\nConstraints\n\n\n우선, instantaneous constraint는 뜻에서도 알 수 있듯이 일시적으로 constraint를 주는 것을 말합니다. 강화학습에서 Agent가 action을 하게 되는 timestep 마다 제약 상황인지를 판단하여 constraint를 주는 것을 말합니다. 이는 기본적인 강화학습 개념에서 매 timestep마다 reward를 주는 상황과 같습니다. 예를 들어 로봇팔(Manipulator)을 제어하는 상황을 생각해보면, Agent는 적절한 움직임을 위해 로봇팔을 구성하는 모터들을 잘 구동하여 원하는 모션을 만들어야 합니다. 이때 로봇이 움직이는 모든 매 순간마다 각 모터들(joint)이 가동범위에 있어야 하고 과한 토크가 가해지지 않도록 해야 합니다. 이러한 제약 상황들은 매 순간 판단해서 해당 범위들을 넘지 않는 action을 선택하도록 학습해야 하므로 instantaneous constraint의 예로 볼 수 있습니다.\n다음으로 cumulative constraint는 Agent가 학습하는 하나의 Episode 내에서 누적해서 나온 값으로 판단하여 제약상황을 판단하는 것을 말합니다. 이때 누적되는 시간은 하나의 Episode가 시작해서 끝날 때까지일 수도 있고 아니면 5 timesteps 동안이라는 특정 timestep 수를 지정하여 계산할 수 있습니다. 로봇팔의 예시로 살펴보자면, 로봇이 펜을 잡는 모션을 할 때까지 100 timestep이 걸렸는데 매 timestep 마다 지연(latency)가 발생하여 이를 제약하고자 합니다. 이러한 상황에서 100 timestep동안의 average latency를 구해서 특정 latency를 넘지 못하도록 constraint를 줄 수 있습니다. 이러한 예시처럼 특정 구간 동안의 값을 통해서 constraint를 주는 것을 cumulative constraint라고 합니다. 이번 IPO 논문에서는 두번째로 소개드린 cumulative constraint에 초점을 맞춰 개발된 알고리즘을 소개하고 있습니다.\n\n\n앞서 설명드린 Constraint가 MDP에 추가된 것을 Constrained Markov Decision Process(CMDP)라고 합니다. CMDP에서는 Reward와 같이 현재 State에서 Action을 취하고 다음 State에 도달했을 때 얻게 되므로 아래 사진에서와 같이 Space가 정의되게 됩니다.\nConstraint는 (s_n, a_n, s_{n+1})과 같은 transition tuple로 계산되게 되며, cumulative constraint는 일정 timestep, 즉 transition이 n(서수:t)개 모여서 계산되게 됩니다. 이때 Constraint도 여러 종류가 있을 수 있으므로 constraint의 가짓 수는 m(서수:i)으로 나타낼 수 있습니다. Constraint는 more than better인 reward와 다르게 제약되는 상황을 정의하게 되는 constraint limit이 있게 되고 이를 \\epsilon_i로 나타내게 됩니다.\n\n\n\nConstraint Space and Constraint Limit\n\n\nConstraint의 Expectation은 다음과 같이 정의가 되며 2가지의 constraint 계산방법이 있습니다. 첫번째로는 discounted cumulative constraint로 할인율 \\gamma를 고려한 constraint들을 하나의 policy가 동작하는 동안 누적합한 값을 말합니다. 두번째로는 일정 timestep T동안 계산한 constraint들의 평균을 말하는 것으로 mean values constraint가 있습니다. 이 2가지 종류의 지표에 대해서 후에 실험에서 다룰 예정이며 CMDP의 목표를 정리해보면, 기존에 J_R만을 Maximization했던 강화학습 문제가 J_{C_i}를 고려해야 한다는 것이 추가 되었다는 것을 알 수 있습니다.\n\n\n\nConstraint Expectation\n\n\n기존의 Constraint가 있는 최적화 문제는 Lagrangian Relaxation Method를 통해서 해결했었습니다. 라그랑지안 승수법이라고도 불리는 해당 방법은 기존의 최적화 식 f(x)에 constraint g_i(x)가 추가된 최적화 문제를 Lagrange Multipilers를 곱하여 기존 최적화 함수 목적식에 더하여서 제약 조건을 푸는 방법입니다.\n\n\n\nLagrangian relaxation method\n\n\n\n\n\n라그랑지안 승수법은 가장 심플하게 제약 조건들을 메인 최적화식에 녹여내어 풀어내는 방식으로, CMDP 문제들도 해당 방법을 통해 해결하는 것이 통상적인 방법이었지만 라그랑지안 승수법은 정책이 수렴할 때 제약 조건이 만족되지만, 이 접근법은 Lagrange multiplier의 초기값과 학습률에 민감하고 학습 과정에서 얻은 정책이 항상 제약 조건을 일관되게 만족시키지는 않는다는 한계점이 있습니다.\n\n\n\n앞 부분에서 살펴본 것과 같이 CMDP Goal은 Reward 값을 최대화하면서 제약식을 만족하는 최적의 policy를 찾는 것이라고 할 수 있습니다.\n\n\n\nCMDP Goal\n\n\n먼저 제약조건을 잠시 뒤로 두고, 본래 기본적인 강화학습의 목적식인 Reward Maximization은 어떻게 할까요? Policy Gradient는 강화학습의 한 계열로 최적의 policy, 즉 가장 Reward를 많이 받을 수 있는 policy를 찾기 위해 아래와 같은 목적식의 gradient를 계산하게 됩니다. 이때 최적의 policy를 찾기 위해서 \\theta는 위에서 구한 gradient 값을 기반으로 아래와 같이 업데이트하게 됩니다.\n\n\n\nPolicy Gradient Methods\n\n\nTrust Region Policy Optimization(TRPO)라는 알고리즘이 PG계열에서 대표적으로 사용되는 알고리즘이며, 최적이 policy를 찾기 위해 surrogate function을 이용하고 policy가 업데이트 되는 step size를 제한하기 위해 KL divergence를 사용합니다. TRPO의 최적화 식은 아래와 같이 표현할 수 있습니다.\n\n\n\nTRPO VS PPO\n\n\n하지만 TRPO는 conjugate gradient optimization으로 풀리는 2차 미분 최적화를 사용하기 때문에 계산 cost가 큽니다. 따라서 TRPO를 실용적으로 사용할 수 있게한 Proximal Policy Optimization (PPO) 알고리즘이 제안되었습니다. PPO의 최적화 식은 TRPO에서 문제였던 2차미분을 1차 미분 surrogate function으로 대체할 수 있었으며 계산복잡성을 줄일 수 있었습니다.\nIPO는 이러한 흐름대로 발전해온 PPO 알고리즘의 최적화 식에서 제약식을 추가하면서 발전하게 됩니다."
  },
  {
    "objectID": "posts/paper/2024-11-10-ipo.html#constrained-markov-decision-processcmdp",
    "href": "posts/paper/2024-11-10-ipo.html#constrained-markov-decision-processcmdp",
    "title": "📃IPO 리뷰",
    "section": "",
    "text": "앞서 설명드린 Constraint가 MDP에 추가된 것을 Constrained Markov Decision Process(CMDP)라고 합니다. CMDP에서는 Reward와 같이 현재 State에서 Action을 취하고 다음 State에 도달했을 때 얻게 되므로 아래 사진에서와 같이 Space가 정의되게 됩니다.\nConstraint는 (s_n, a_n, s_{n+1})과 같은 transition tuple로 계산되게 되며, cumulative constraint는 일정 timestep, 즉 transition이 n(서수:t)개 모여서 계산되게 됩니다. 이때 Constraint도 여러 종류가 있을 수 있으므로 constraint의 가짓 수는 m(서수:i)으로 나타낼 수 있습니다. Constraint는 more than better인 reward와 다르게 제약되는 상황을 정의하게 되는 constraint limit이 있게 되고 이를 \\epsilon_i로 나타내게 됩니다.\n\n\n\nConstraint Space and Constraint Limit\n\n\nConstraint의 Expectation은 다음과 같이 정의가 되며 2가지의 constraint 계산방법이 있습니다. 첫번째로는 discounted cumulative constraint로 할인율 \\gamma를 고려한 constraint들을 하나의 policy가 동작하는 동안 누적합한 값을 말합니다. 두번째로는 일정 timestep T동안 계산한 constraint들의 평균을 말하는 것으로 mean values constraint가 있습니다. 이 2가지 종류의 지표에 대해서 후에 실험에서 다룰 예정이며 CMDP의 목표를 정리해보면, 기존에 J_R만을 Maximization했던 강화학습 문제가 J_{C_i}를 고려해야 한다는 것이 추가 되었다는 것을 알 수 있습니다.\n\n\n\nConstraint Expectation\n\n\n기존의 Constraint가 있는 최적화 문제는 Lagrangian Relaxation Method를 통해서 해결했었습니다. 라그랑지안 승수법이라고도 불리는 해당 방법은 기존의 최적화 식 f(x)에 constraint g_i(x)가 추가된 최적화 문제를 Lagrange Multipilers를 곱하여 기존 최적화 함수 목적식에 더하여서 제약 조건을 푸는 방법입니다.\n\n\n\nLagrangian relaxation method\n\n\n\n\n\n라그랑지안 승수법은 가장 심플하게 제약 조건들을 메인 최적화식에 녹여내어 풀어내는 방식으로, CMDP 문제들도 해당 방법을 통해 해결하는 것이 통상적인 방법이었지만 라그랑지안 승수법은 정책이 수렴할 때 제약 조건이 만족되지만, 이 접근법은 Lagrange multiplier의 초기값과 학습률에 민감하고 학습 과정에서 얻은 정책이 항상 제약 조건을 일관되게 만족시키지는 않는다는 한계점이 있습니다."
  },
  {
    "objectID": "posts/paper/2024-11-10-ipo.html#policy-gradient-methods",
    "href": "posts/paper/2024-11-10-ipo.html#policy-gradient-methods",
    "title": "📃IPO 리뷰",
    "section": "",
    "text": "앞 부분에서 살펴본 것과 같이 CMDP Goal은 Reward 값을 최대화하면서 제약식을 만족하는 최적의 policy를 찾는 것이라고 할 수 있습니다.\n\n\n\nCMDP Goal\n\n\n먼저 제약조건을 잠시 뒤로 두고, 본래 기본적인 강화학습의 목적식인 Reward Maximization은 어떻게 할까요? Policy Gradient는 강화학습의 한 계열로 최적의 policy, 즉 가장 Reward를 많이 받을 수 있는 policy를 찾기 위해 아래와 같은 목적식의 gradient를 계산하게 됩니다. 이때 최적의 policy를 찾기 위해서 \\theta는 위에서 구한 gradient 값을 기반으로 아래와 같이 업데이트하게 됩니다.\n\n\n\nPolicy Gradient Methods\n\n\nTrust Region Policy Optimization(TRPO)라는 알고리즘이 PG계열에서 대표적으로 사용되는 알고리즘이며, 최적이 policy를 찾기 위해 surrogate function을 이용하고 policy가 업데이트 되는 step size를 제한하기 위해 KL divergence를 사용합니다. TRPO의 최적화 식은 아래와 같이 표현할 수 있습니다.\n\n\n\nTRPO VS PPO\n\n\n하지만 TRPO는 conjugate gradient optimization으로 풀리는 2차 미분 최적화를 사용하기 때문에 계산 cost가 큽니다. 따라서 TRPO를 실용적으로 사용할 수 있게한 Proximal Policy Optimization (PPO) 알고리즘이 제안되었습니다. PPO의 최적화 식은 TRPO에서 문제였던 2차미분을 1차 미분 surrogate function으로 대체할 수 있었으며 계산복잡성을 줄일 수 있었습니다.\nIPO는 이러한 흐름대로 발전해온 PPO 알고리즘의 최적화 식에서 제약식을 추가하면서 발전하게 됩니다."
  },
  {
    "objectID": "posts/paper/2024-11-10-ipo.html#interior-point-policy-optimization",
    "href": "posts/paper/2024-11-10-ipo.html#interior-point-policy-optimization",
    "title": "📃IPO 리뷰",
    "section": "2.1 Interior-point Policy Optimization",
    "text": "2.1 Interior-point Policy Optimization\nIPO이전에 CPO(Constrained policy optimization)라는 알고리즘이 제안되었었습니다. IPO는 CPO의 단점을 보완하여 제안된 알고리즘으로 볼 수 있으며 아래와 같이 2개 알고리즘을 비교해볼 수 있습니다.\n\n\n\nCPO VS IPO\n\n\n우선, CPO는 TRPO에서 제약조건을 추가한 목적식을 사용하여 TRPO의 문제이기도 했던 2차 미분 계산이 필요하다는 특성이 있습니다. 따라서 제약조건들을 추가하거나 mean valued constraint와 같은 누적 제약식을 계산하기 까다롭거나 할 수 없다는 문제점을 가지고 있었습니다. 이에 반해, IPO는 PPO에 제약조건을 추가한 목적식을 기반으로 하여 1차 미분만을 하면 된다는 장점을 가지고 있으며, 다양한 제약조건들을 이후에 설명할 핵심 아이디어인 logarithmic barrier function을 이용하여 쉽게 추가할 수 있습니다."
  },
  {
    "objectID": "posts/paper/2024-11-10-ipo.html#logarithmic-barrier-function",
    "href": "posts/paper/2024-11-10-ipo.html#logarithmic-barrier-function",
    "title": "📃IPO 리뷰",
    "section": "2.2 Logarithmic Barrier Function",
    "text": "2.2 Logarithmic Barrier Function\n우선 IPO의 문제 정의는 아래와 같이 PPO의 목적식에다가 Constraint를 추가한 것으로 정의할 수 있습니다.\n\n\n\nIPO Problem Definition\n\n\nConstraint는 Limit을 고려하여 부등호로 나타낼 수 있으며 이는 Indicatior Function에 넣었을때, Constraint를 넘었을 경우 -\\infin로 나타내고 Constraint를 만족했을 경우 0으로 나타낼 수 있습니다. 하지만 Indicator Function은 불연속적이며 미분 불가능하기 때문에 gradient를 구할 수 없어서 Logarithmic Barrier Function을 통해 근사하게 됩니다.\n\n\n\nLogarithmic Barrier Function\n\n\nLogarithmic Barrier Function(\\phi)은 그래프에서와 같이 하이퍼 파라미터인 t의 값이 클수록 Indicator Function과 유사하다는 것을 알 수 있습니다. 그래프에서 초록색 t=50일 때의 그래프가 점선의 Indicator Function과 유사한 것 처럼요. 또한 \\phi는 이분이 가능하기 때문에 gradient를 통해 최적화할 수 있습니다.\n\n\n\nIPO 결론(목적식과 수도코드)\n\n\n따라서 IPO의 최적화식은 PPO의 목적식 (L^{C L I P}(\\theta))에 Logarithmic Barrier Function(\\phi)을 이용하여 제약조건을 합치게 된(\\sum_{i=1}^m \\phi\\left(\\widehat{J}_{C_i}^{\\pi_i}\\right)) 모습이 됩니다."
  },
  {
    "objectID": "posts/paper/2024-11-10-ipo.html#performance-guarantee-bound",
    "href": "posts/paper/2024-11-10-ipo.html#performance-guarantee-bound",
    "title": "📃IPO 리뷰",
    "section": "2.3 Performance Guarantee Bound",
    "text": "2.3 Performance Guarantee Bound\n그렇다면 IPO의 성능 보장을 이론적으로 검증해보겠습니다.\n\n \n\n이러한 수식적인 검증 과정을 거쳐 IPO의 목적식은 일정 한계 내에 있다는 것(Bounded) 되어있다는 결론을 내릴 수 있습니다.\n\n\n\n수식적으로 Performance Guarantee Bound를 확인하여 t(logarithmic barrier function의 하이퍼파라미터)가 클수록 Indicator function에 대한 더 좋은 근사값을 제공하게 되고 더 높은 reward와 cost를 얻을 수 있다는 것을 확인할 수 있습니다. 하지만 t가 클수록 최적화 식이 수렴하는 속도는 느려진다는 단점이 있습니다. 또한 수식으로 확인한 단조성(monotonicity)을 이용하여, 수렴 속도와 최적화 성능 사이의 균형을 맞출 수 있는 적절한 t 값을 찾기 위해 이진 탐색 알고리즘(binary search)을 사용할 수 있다는 사실도 확인할 수 있습니다."
  },
  {
    "objectID": "posts/paper/2024-11-10-ipo.html#discounted-cumulative-constraints",
    "href": "posts/paper/2024-11-10-ipo.html#discounted-cumulative-constraints",
    "title": "📃IPO 리뷰",
    "section": "3.1 Discounted Cumulative Constraints",
    "text": "3.1 Discounted Cumulative Constraints\n\n\n\nDiscounted Cumulative Constraints 실험 결과\n\n\n\nIPO VS. CPO\n\nIPO\n\n최고 성능을 보여줍니다.\n제약 조건이 충족된 이후에도 더 나은 정책을 찾기 위해 탐색을 계속합니다.\n이로 인해 더 높은 보상과 더 낮은 비용으로 수렴합니다.\n수렴 속도는 느리지만, 최종 성능은 CPO보다 우수합니다.\n\nCPO\n\n수렴 속도가 IPO보다 빠릅니다.\n제약 조건이 충족되면 개선 작업을 중단합니다.\n제약 조건을 빠르게 만족시키지만, 그 이후에는 성능 개선이 멈춥니다.\n따라서 보상이나 비용 측면에서 IPO만큼의 최적화를 이루지 못할 가능성이 있습니다.\n\n\n\n\n\n\n\n\n\n\n\n특징\nIPO\nCPO\n\n\n\n\n수렴 속도\n느림\n빠름\n\n\n제약 충족 후 개선\n계속 탐색 (더 나은 정책을 찾음)\n개선 중단 (제약 조건 충족 시)\n\n\n최종 성능\n더 높은 보상과 낮은 비용\n제약 조건 만족 후 개선 없음\n\n\n\n\nIPO VS. PDO\n\nIPO\n\n최고 성능을 보여줍니다.\n제약 조건이 충족된 이후에도 더 나은 정책을 찾기 위해 탐색을 계속합니다.\n안정적인 학습 과정을 가지며, 성능의 변동이 적습니다.\n초기화나 학습률에 덜 민감합니다.\n\nPDO\n\nIPO만큼 좋은 정책으로 수렴 가능하지만, 훈련 중 성능의 분산(variance)이 높습니다.\n제약 조건 값을 한계 이하로 낮추는 정책을 찾을 수 있으나, 그 결과 보상(reward)이 가장 낮아질 수 있습니다.\nLagrange multiplier의 초기값과 학습률(learning rate)에 민감하게 반응합니다.\n초기 설정이 잘못되면, 학습 과정이 불안정해질 수 있습니다.\n\n\n\n\n\n\n\n\n\n\n\n특징\nIPO\nPDO\n\n\n\n\n수렴 성능\n최고 성능에 수렴\nIPO 수준으로 수렴 가능\n\n\n훈련 중 성능 변동\n낮음 (안정적)\n높음 (변동이 큼)\n\n\n제약 조건 만족도\n제약 조건을 충족하며 탐색 지속\n제약 조건 값을 한계 이하로 낮춤\n\n\n보상 (Reward)\n높은 보상\n가장 낮은 보상 가능성\n\n\n초기화/학습률 민감도\n낮음\n높음\n\n\n\n\n(optional)CPO vs. PPO / TRPO\n\n\n\n\n\n\n\n\n\n\n특징\nCPO\nPPO\nTRPO\n\n\n\n\n제약 조건 처리 여부\n제약 조건을 고려함\n제약 조건 없음\n제약 조건 없음\n\n\n보상 (Reward)\n높음 (제약 조건 내에서)\n가장 높음 (제약 조건 위반 가능성 있음)\n높음 (제약 조건을 간접적으로 완화)\n\n\n제약 조건 위반 가능성\n낮음\n높음\n중간 (신뢰 영역으로 일부 완화)\n\n\n학습 안정성\n높음\n높음\n매우 높음\n\n\n계산 복잡도\n중간\n낮음\n높음"
  },
  {
    "objectID": "posts/paper/2024-11-10-ipo.html#mean-valued-constraints",
    "href": "posts/paper/2024-11-10-ipo.html#mean-valued-constraints",
    "title": "📃IPO 리뷰",
    "section": "3.2 Mean Valued Constraints",
    "text": "3.2 Mean Valued Constraints\n\n\n\nMean Valued Constraints 실험 결과\n\n\n\nIPO VS. PDO\n\nIPO\n\n일관된 수렴: 모든 작업(task)에서 할인 누적 보상(discounted cumulative reward)이 높은 정책으로 안정적으로 수렴합니다.\n제약 조건 만족: 모든 작업에서 평균 값 제약(mean valued constraints)을 지속적으로 만족시킵니다.\n안정적인 학습: 훈련 중 성능의 변동이 적으며, 낮은 분산(variance)을 보입니다.\n\nPDO\n\n제약 조건 위반 가능성: 간혹 제약 조건을 위반하는 정책으로 수렴할 수 있습니다. (참조: Figure 3b)\n훈련 중 높은 분산: 훈련 과정에서 성능의 변동이 크며, 높은 분산을 보입니다. (참조: Figure 3d 및 Figure 3f)\n높은 보상 가능성: 때때로 높은 보상을 달성할 수 있지만, 제약 조건을 지키지 못할 위험이 있습니다.\n\n\n\n\n\n\n\n\n\n\n\n특징\nIPO\nPDO\n\n\n\n\n할인 누적 보상\n안정적으로 높은 보상에 수렴\n높은 보상 가능성이 있으나 불안정\n\n\n제약 조건 만족도\n항상 제약 조건을 만족함\n간혹 제약 조건을 위반\n\n\n훈련 중 성능 변동 (분산)\n낮음 (안정적)\n높음 (변동이 큼)\n\n\n안정성\n매우 안정적\n초기화와 학습률에 민감"
  },
  {
    "objectID": "posts/paper/2024-11-10-ipo.html#constraint-effects",
    "href": "posts/paper/2024-11-10-ipo.html#constraint-effects",
    "title": "📃IPO 리뷰",
    "section": "3.3 Constraint Effects",
    "text": "3.3 Constraint Effects\nPoint Gather 환경에서 제약 조건을 완화하여 임계값을 1로 설정한 경우, 각 에이전트는 평균적으로 최대 1개의 폭탄(bomb)을 수집할 수 있습니다. Constraint 값을 내려서 완화하게 되면 제약 조건이 매우 느슨해져서, 제약 조건이 있는 최적화 문제의 성능이 제약 조건이 없는 경우와 동일한 수준으로 나타납니다.\n\nCPO\n\nCPO는 여전히 비용을 증가시켜 제약 임계값(1)에 도달하려고 합니다.\n이는 때때로 랜덤 초기화된 정책보다도 성능이 떨어질 수 있습니다.\nCPO는 항상 비용을 제약 임계값(1)까지 밀어 올리려는 경향을 보입니다.\n\nIPO\n\nIPO는 제약 조건이 충족된 이후에도 비용을 계속 줄여나갑니다.\n이로 인해 더 낮은 비용을 달성하며, 더 나은 최종 성능을 보여줍니다.\n\n\n\n\n\n\n\n\n\n\n특징\nCPO\nIPO\n\n\n\n\n제약 조건 만족도\n제약 임계값(1)까지 비용 증가\n제약 충족 후에도 비용 감소 지속\n\n\n최종 비용 수준\n약 1\n약 0.25\n\n\n성능\n제약 충족을 우선시하며 성능 저하 가능\n제약을 충족하면서도 더 나은 성능\n\n\n\n\n\n\n따라서 실험을 통해 다음과 같은 결론을 내릴 수 있습니다.\n\nCPO는 제약을 맞추기 위해 비용을 적극적으로 증가시키지만, 그 결과 성능이 떨어질 가능성이 있습니다.\nIPO는 제약을 만족한 이후에도 비용을 줄이며, 더 높은 성능을 달성할 수 있습니다."
  },
  {
    "objectID": "posts/paper/2024-11-10-ipo.html#hyperparameter-tuning",
    "href": "posts/paper/2024-11-10-ipo.html#hyperparameter-tuning",
    "title": "📃IPO 리뷰",
    "section": "3.4 Hyperparameter Tuning",
    "text": "3.4 Hyperparameter Tuning\n\nIPO vs. PDO\n\nIPO\n\n하이퍼파라미터 t의 튜닝이 용이합니다.\n보상(reward)과 비용(cost)은 하이퍼파라미터 t와 양의 상관 관계를 가집니다.\nt 값이 커질수록, 보상과 비용이 동시에 증가합니다.\n이진 탐색(binary search)이 가능:\nt 값을 조정하며 성능을 확인할 수 있으며, 이진 탐색을 통해 빠르게 최적의 값을 찾을 수 있습니다.\n\nPDO\n\n초기 Lagrange multiplier (\\lambda)와 학습률(learning rate)의 설정이 까다롭습니다.\n초기 \\lambda 값이 0.01에서 0.1 사이일 때 매우 민감하게 반응합니다.\n잘못된 초기화는 학습 과정의 불안정을 초래할 수 있습니다.\n학습률(learning rate)의 변화에도 민감합니다.\n학습률이 0.01에서 0.001로 작아지면, 정책의 수렴 속도가 느려집니다.\n하이퍼파라미터 설정에 많은 시간과 노력이 필요합니다.\n\n\n\n\n\n\n\n\n\n\n\n특징\nIPO\nPDO\n\n\n\n\n하이퍼파라미터 튜닝 용이성\n쉬움\n어렵고 복잡함\n\n\n보상과 비용의 관계\nt와 양의 상관 관계\n초기 \\lambda와 학습률에 민감\n\n\n초기 설정 민감도\n낮음\n높음\n\n\n튜닝 방법\n이진 탐색 가능\n초기화와 학습률 설정에 많은 노력 필요\n\n\n\n\n\n\n따라서 실험을 통해 다음과 같은 결론을 내릴 수 있습니다.\n\nIPO는 하이퍼파라미터 t의 튜닝이 쉽고, 보상과 비용이 t 값에 따라 예측 가능하게 변화하기 때문에 안정적인 최적화가 가능합니다.\nPDO는 초기화와 학습률에 민감하여 튜닝이 까다롭고 학습 과정이 불안정할 수 있습니다. 특히 초기 \\lambda와 학습률 설정이 중요한 역할을 합니다."
  },
  {
    "objectID": "posts/paper/2024-11-10-ipo.html#multiple-constraints",
    "href": "posts/paper/2024-11-10-ipo.html#multiple-constraints",
    "title": "📃IPO 리뷰",
    "section": "3.5 Multiple Constraints",
    "text": "3.5 Multiple Constraints\nIPO (Interior Point Optimization)는 제약 조건을 다룰 때 유연하고 확장 가능한 방식으로 설계되어 있습니다. 특히, logarithmic barrier function을 사용하여 제약 조건을 쉽게 추가할 수 있습니다. IPO에서는 새로운 제약 조건이 필요할 때, 기존 최적화 함수에 로그 배리어 항을 추가하기만 하면 됩니다. 이 방식은 CPO보다 간단하게 제약 조건을 추가할 수 있는 이점이 있습니다. IPO는 logarithmic barrier function을 사용하여 제약 조건을 쉽게 추가할 수 있어, 확장성과 유연성 측면에서 CPO보다 유리합니다.\n\nCPO와의 비교\n\nCPO (Constrained Policy Optimization)는 제약 조건을 직접적으로 다루지만, 새로운 제약 조건이 추가될 때마다 문제의 복잡도가 증가하고, 튜닝이 어려워질 수 있습니다.\n반면, IPO는 logarithmic barrier function을 사용하기 때문에, 제약 조건을 쉽게 확장할 수 있으며 구현과 튜닝이 더 간단합니다.\n\nPoint Gather 실험에서의 제약 조건 확장\n\nPoint Gather 환경에서는 에이전트가 보상을 얻는 과정에서 다양한 제약 조건을 추가할 수 있습니다.\n실험에서 다양한 제약 조건을 추가하기 위해, 새로운 타입의 ball (제약 조건에 해당하는 오브젝트)을 도입할 수 있습니다.\n예를 들어, 기존의 bomb 외에 새로운 제약 조건을 나타내는 여러 종류의 ball을 추가하여, 에이전트가 이들을 피하면서도 최대한 많은 보상을 얻는 정책을 학습할 수 있습니다.\n이를 통해 다중 제약 조건 환경에서도 IPO의 성능을 평가할 수 있습니다.\n\n\n\n\n\n\n\n\n\n\n\n\n\n특징\nIPO\nCPO\n\n\n\n\n제약 조건 추가 용이성\n로그 배리어 항 추가만으로 가능\n복잡한 추가 작업과 튜닝 필요\n\n\n확장성\n간단하게 여러 제약 조건 확장 가능\n제약 조건 추가 시 복잡도 증가\n\n\nPoint Gather 실험 적용\n다양한 제약 조건 ball 추가 가능\n제약 조건 추가 시 성능 저하 위험"
  },
  {
    "objectID": "posts/paper/2024-11-10-ipo.html#stochastic-environment-effects",
    "href": "posts/paper/2024-11-10-ipo.html#stochastic-environment-effects",
    "title": "📃IPO 리뷰",
    "section": "3.6 Stochastic Environment Effects",
    "text": "3.6 Stochastic Environment Effects\n실세계 환경에서의 불확실성 및 랜덤 노이즈 추가 실험 실제 환경에서는 항상 불확실성(uncertainty)이 존재합니다. 에이전트의 행동 결과는 종종 랜덤 노이즈(random noise)에 의해 영향을 받습니다. 예를 들어, 바람, 센서 오류, 마찰 등의 예기치 못한 요인들이 시스템에 영향을 줄 수 있습니다. 해당 실험에서 행동(action)은 속도(velocity)와 진행 방향(heading)의 벡터로 정의되며, 값의 범위는 -1에서 1 사이입니다. (-1, 1) 범위의 벡터는 에이전트가 움직일 방향과 속도를 나타냅니다.\n실험에서는 평균 0의 랜덤 노이즈를 행동(action)에 추가하여 환경의 불확실성을 모사했습니다.\n\n노이즈의 분산(variance)은 세 가지 값으로 설정되었습니다:\n\n\\sigma^2 = 0.2\n\\sigma^2 = 0.5\n\\sigma^2 = 1.0\n\n\n\n\n\n\n\\sigma^2 = 0.5일 때도 학습이 성공적으로 수렴하는 것을 확인할 수 있었습니다.\n\n이는 에이전트가 일정 수준의 환경 불확실성에서도 안정적으로 정책을 학습할 수 있음을 보여줍니다.\n\n\\sigma^2 = 1.0의 경우, 노이즈가 커져 학습이 불안정해질 가능성이 있으며, 이는 추가 실험에서 확인할 필요가 있습니다.\n실제 환경의 불확실성을 반영하기 위해 랜덤 노이즈를 추가하는 것은 강화 학습의 강건성(robustness) 평가에 중요한 역할을 합니다.\n적절한 수준의 노이즈(\\sigma^2 = 0.5)에서는 학습이 안정적으로 진행되었으며, 에이전트가 다양한 환경 변동에도 잘 적응할 수 있음을 확인했습니다."
  },
  {
    "objectID": "posts/paper/2025-08-10-dextrack.html",
    "href": "posts/paper/2025-08-10-dextrack.html",
    "title": "📃DexTrack 리뷰",
    "section": "",
    "text": "Paper Link\nProject Link\nCode Link"
  },
  {
    "objectID": "posts/paper/2025-08-10-dextrack.html#기술적-기여-및-핵심-아이디어-분석",
    "href": "posts/paper/2025-08-10-dextrack.html#기술적-기여-및-핵심-아이디어-분석",
    "title": "📃DexTrack 리뷰",
    "section": "1. 기술적 기여 및 핵심 아이디어 분석",
    "text": "1. 기술적 기여 및 핵심 아이디어 분석\nDexTrack은 인간의 손동작 시범(kinematic reference)을 바탕으로 범용적인 신경망 기반 로봇 손 추적 제어기를 제안한 연구이다. 이 제어기는 주어진 인간-물체 상호작용 궤적(시퀀스)을 로봇 손으로 최대한 정확히 따라하게 함으로써 다양한 물체 조작 작업을 수행한다. DexTrack의 핵심 아이디어는 고품질 로봇 추적 데모 데이터를 반복적으로 수집·확장하고 이를 통해 신경망 정책을 학습시키는 데이터 플라이휠(data flywheel) 방식을 도입한 것이다. 즉, 초기에는 인간 시범을 로봇의 운동 참조로 리타게팅(retargeting)하여 얻은 성공적인 추적 데모들로부터 모방 학습을 시작하고, 학습된 제어기를 다시 사용해 더 어려운 새로운 시범들을 추적함으로써 더 많은 데모를 채굴하고 성능을 높이는 과정을 반복한다.\n구체적으로 DexTrack의 구조는 다음과 같다: 우선 다수의 인간 손동작-물체 상호작용 궤적을 로봇 손의 관절공간으로 리타게팅하여 로봇 기준의 운동 참조 시퀀스를 만든다. 그런 다음, 신경망 추적 제어기는 매 시각각 현재 로봇 손 상태와 향후 목표 상태(참조 궤적의 다음 단계들)를 입력으로 받아 로봇 손의 액션 명령(관절 위치 또는 힘 등)을 출력한다. 이때 잔여(residual) 액션 학습 기법을 사용하여, 참조 궤적 자체를 기본 베이스라인으로 두고 신경망이 필요한 보정 동작만 출력하도록 함으로써 학습 효율을 높였다. 이렇게 하면 사람 손과 로봇 손의 형태 차이로 인한 오차나 물리적 제약을 신경망이 보정하여 참조 경로를 가깝게 추적할 수 있다.\n학습 방식 측면에서, DexTrack은 강화학습(RL)과 모방학습(IL)을 정교하게 결합하여 제어기를 훈련시킨다. - 먼저 인간 시범으로부터 얻은 로봇 추적 데몬스트레이션(참조 궤적 + 이를 성공적으로 따라간 로봇 액션 시퀀스)의 다양한 데이터셋을 모은 뒤, 이를 모방 학습하여 초기 정책을 얻는다. - 동시에 정책의 견고성을 높이기 위해, 환경 상에서 추가적인 RL 파인튜닝을 수행하는데, 이때 보상은 참조 궤적과 로봇 상태의 일치도를 측정하는 트래킹 보상으로 설계된다.\n이러한 IL로 학습된 초기 정책 + RL로 강화된 정책의 결합은, 복잡한 접촉 동작이 많은 dexterous manipulation 문제에서도 학습 신속성과 강인성을 모두 확보하게 한다. 특히 RL 없이 모방학습만으로는 예기치 못한 상황에 취약할 수 있는데, DexTrack은 RL을 통해 잡음이나 예외 상황에서도 복구 행동을 할 수 있는 능력을 갖춘 점이 특징이다.\nDexTrack의 또 다른 핵심 기법은 Homotopy 최적화(homotopy optimization)를 활용한 개별 궤적 추적 향상이다. 이는 어려운 특정 시범 궤적을 한 번에 학습하기 어려울 경우, 현재 학습된 추적 제어기를 이용해 해당 궤적을 점진적으로 난이도를 낮추는 일련의 중간 참조 경로로 분해하여 따라가 보는 접근법이다. 예를 들어 체인-오브-쏘트(chain-of-thought)와 유사하게, 복잡한 목표 동작을 단계별 단순화한 여러 참조 단계들을 만들어 쉬운 것부터 어려운 것 순서로 추적 수행한다. 각 단계에서는 RL 기반의 단일 궤적 추적 최적화를 수행하여 성공적인 로봇 액션 시퀀스를 찾아내고, 단계가 진행될수록 원래의 어려운 참조에 가까워지도록 한다. 이렇게 하면 처음에는 실패하던 복잡한 시범도 점진적인 성공 사례들을 통해 최종적으로 성공적인 추적 데모를 얻어낼 수 있다. 이 Homotopy 경로 생성 방법은 데이터 플라이휠 과정에서 데모 다양성을 높이고 품질을 향상시켜, 결과적으로 일반화 성능이 우수한 제어기를 얻는 데 크게 기여한다.\n요약하면, DexTrack의 주요 기술적 기여는 다음과 같다:\n\n범용 신경 추적 제어기를 제시하고 데이터 플라이휠을 통해 시행착오적 성능 향상을 실현함 (더 많은 데모를 모을수록 성능이 향상).\n강화학습 + 모방학습 통합 학습법으로 다량의 고품질 데모의 힘을 빌리면서도 새 환경에서도 견고한 정책을 학습함.\nHomotopy 기반 개별 궤적 최적화 스킴을 개발하여 어려운 추적 문제를 단계별로 풀어내고 데모 품질과 다양성을 높이는 데이터 기반 솔루션을 제안함.\n\n이와 같이 DexTrack은 복잡한 접촉 역학을 가지는 다지 로봇 손 조작 문제에 대해, 인간 시범으로부터 보편적인 추종 능력을 학습시키는 새로운 패러다임을 제시하였다."
  },
  {
    "objectID": "posts/paper/2025-08-10-dextrack.html#기존-연구와의-차별점-및-관련-연구-비교",
    "href": "posts/paper/2025-08-10-dextrack.html#기존-연구와의-차별점-및-관련-연구-비교",
    "title": "📃DexTrack 리뷰",
    "section": "2. 기존 연구와의 차별점 및 관련 연구 비교",
    "text": "2. 기존 연구와의 차별점 및 관련 연구 비교\nDexTrack은 기존의 강화학습(RL) 또는 모델 기반 최적화 접근법과 구별되는 새로운 방향을 제시한다. 과거 OpenAI 등의 연구에서는 특정 과제별 보상 설계를 통해 RL로 로봇 손 동작을 학습시키거나, 혹은 정확한 물리 모델과 접촉 타이밍에 의존하는 모델 기반 경로 최적화를 사용해 왔다. 그러나 전자는 매 과제마다 보상을 손수 설계해야 하므로 일반화된 하나의 정책으로 여러 작업을 수행하기 어려웠고, 후자는 접촉이 많은 환경에서 정확한 모델링이 어려워 새로운 물체나 기술에 적응성이 떨어지는 문제가 있었다. 인간 시연 기반 모방 학습 접근도 일부 시도되었지만, 기존 방법들은 잡음 없는 이상적인 궤적만을 대상으로 하거나 간단한 파지(grasp)나 경로 추종 등에 국한되어, 섬세한 잔동작이 필요한 인핸드 조작까지 다루지 못했다. 예를 들어 OmniGrasp (2024)라는 선행 연구에서는 범용 정책을 학습하긴 했지만, 물체 집기와 단순 이동 정도의 제한적인 모션만 고려하여 여전히 복잡한 손놀림이 요구되는 작업은 다루지 않았다. 이에 반해 DexTrack은 훨씬 복잡한 동작 (얇은 도구 다루기, 연속적인 손 안에서의 재배치 등)까지 포함하여 보다 풍부한 기술 습득을 목표로 한다.\n또한 DexTrack은 인간 동작 모방을 활용한다는 점에서 관련 연구들과 맥락을 같이하지만, 데이터 수집과 활용 면에서 독창성을 보인다. 일부 연구들은 모방 학습에 RL을 추가로 활용하여 샘플 효율을 높이는 데모 강화 RL 기법들을 제안한 바 있다. 하지만 이들 연구에서는 데몬스트레이션 데이터가 이미 주어져 있다는 가정을 하며, 인간이나 텔레옵으로 얻은 데모를 그대로 활용하는 경우가 많다. 반면 DexTrack은 고품질 데모를 직접 만들어내는 루프를 통해 문제를 해결했다는 점에서 차별화된다. 즉, 추적 제어기 자체가 데모 생성을 도우면서 성능을 높이는 부트스트래핑을 구현하여, 시범 데이터 부족 문제를 창의적으로 풀었다. 이러한 데이터 플라이휠 기법은 최근 거대 모델 학습에서 데이터 규모가 성능을 좌우한다는 통찰에 착안한 것으로서, 로봇 제어 분야에 이를 적용해 학습용 데이터와 정책을 함께 향상시킨 사례라 할 수 있다.\nHuman demonstration 기반 로봇 조작 분야의 다른 작업들과 비교하면, DexTrack은 고차원 모션 추종에 초점을 맞춘 점이 돋보인다. 예를 들어, DGrasp (Christen et al., 2022) 등의 기법은 비교적 단순한 연속 파지 동작을 여러 단계로 나눠 푸는 방식을 사용하지만, 하나의 긴 복잡 동작을 끝까지 추적하는 범용 정책은 아니었다. DexTrack은 하나의 신경망이 여러 작업 종류에 대응하면서도 세밀한 손가락 움직임까지 정확히 모사하도록 훈련되었고, 이를 통해 이전 기법들이 실패하거나 시도하지 않은 섬세한 조작 시나리오들을 성공적으로 수행했다. 특히, 얇은 물체를 다루거나 물체를 손 안에서 자유롭게 돌리는 동작, 도구를 활용한 복합 움직임 등에 있어서 DexTrack은 기존 강화학습 기반 방법(PPO 등)이 일반화에 실패하는 경우에도 안정적으로 동작함을 보였다. 결과적으로 DexTrack은 기존 방식 대비 약 10% 이상의 성공률 향상을 달성하며, 범용성과 적응성 측면에서 현 상태-of-the-art를 한 단계 진보시킨 것으로 평가된다.\n요약하면, DexTrack은 (a) 과제별 설계나 정확한 모델 없이도 다양한 작업에 통하는 범용 제어기를 제시했고, (b) 인간 시범 활용 연구들 가운데서도 더 어려운 작업과 노이즈에 강한 새로운 접근법을 선보였다는 점에서 독창적이다. 이를 통해 로봇이 사람의 복잡한 손동작까지 학습하여 모방하는 길을 크게 확장한 것으로 볼 수 있다."
  },
  {
    "objectID": "posts/paper/2025-08-10-dextrack.html#실제-응용-가능성과-한계점",
    "href": "posts/paper/2025-08-10-dextrack.html#실제-응용-가능성과-한계점",
    "title": "📃DexTrack 리뷰",
    "section": "3. 실제 응용 가능성과 한계점",
    "text": "3. 실제 응용 가능성과 한계점\nDexTrack에서 제안한 제어기는 시뮬레이션뿐만 아니라 실제 로봇 손에도 검증되어 그 응용 가능성을 보여주었다. 연구진은 시뮬레이터(Isaac Gym) 상에서 학습한 정책을 실제 4지 로봇 손(LEAP Hand)에 이식하여 여러 가지 일상 물체 조작 실험을 진행하였다. 예를 들어, 사람 손 시범으로 기록한 물체 사용 동작(망치질, 칼로 자르기, 비누 잡기 등)을 로봇 손이 실제로 따라하도록 한 결과, 사전에 보지 못한 새 물체나 센서 잡음이 존재해도 상당히 안정적인 조작이 가능함을 확인했다. 사과 들어올리기와 같이 둥근 물체를 쥐기 어려운 상황에서도 DexTrack 제어기는 끝까지 물체를 파지하고 들어올리는 데 성공한 반면, 기존 PPO 기반 제어기는 시작 단계부터 물체를 놓치는 등 실패하였다. 이러한 실제 실험을 통해 DexTrack의 정책이 시뮬레이션-실세계 간 격차(sim-to-real gap)를 어느 정도 극복하고 현실 환경의 마찰·동역학에서도 동작함을 보여주었다. 나아가 특수한 튜닝이나 추가 학습 없이도 인간 시범 기반으로 학습된 정책을 현실에 바로 투입해 다양한 작업을 수행할 수 있다는 점에서, 향후 범용 로봇 조작기로서의 잠재력을 시사한다.\n그럼에도 불구하고, DexTrack에는 극복해야 할 한계점이나 추가로 고려해야 할 부분도 존재한다.\n\n첫째로, 고품질 데모 수집 과정의 비용 문제가 있다. 논문에서도 한계로 지적했듯이, DexTrack의 성능을 끌어올리려면 다양한 작업에 대한 성공 사례 데이터가 많이 필요한데, 이를 얻기 위해 Homotopy 최적화를 포함한 복잡한 절차를 거쳐야 하므로 학습에 많은 시간과 계산 자원이 소요된다. 수천 개 이상의 시퀀스를 병렬 환경(8192개 시뮬레이터)에서 돌려가며 정책을 학습하고 또 데모를 추가 수집하는 식이어서, 훈련 파이프라인이 무겁고 비실용적일 수 있다.\n둘째로, 일부 한계 상황에서의 성능 저하가 관찰되었다. DexTrack 제어기가 훈련 시 경험해보지 못한 완전히 새로운 범주의 물체 중 특히 형상이 매우 얇거나 특이한 경우, 해당 물체에 대한 파지가 제대로 이루어지지 않아 추적에 실패할 수 있다. 예컨대 훈련 데이터에 없던 극단적으로 얇은 도구를 다뤄야 하는 경우, 제어기가 올바른 힘 조절과 접촉 위치를 찾지 못해 사람 시범 동작을 끝까지 재현하지 못한다. 이는 모델의 일반화 한계를 드러내는 부분으로, 새로운 물체 물성이나 마찰계수를 만났을 때의 대응은 추가 연구가 필요한 영역이다.\n\n또 다른 현실적 고려사항으로는, DexTrack이 고정된 참조 궤적을 추종하는 방식이라는 점이다. 실제 응용에서 로봇이 작업 도중 참조 동작의 변경이나 예기치 않은 사건에 직면하면, 현재의 DexTrack 정책은 그 상황을 극복하도록 설계되지 않았다. 물론 참조 자체에 큰 잡음이나 비현실적인 동작이 있어도 정책이 알아서 보정해주는 견고성은 가지고 있지만, 임무 목표 자체를 재설정하는 능동적인 지능과는 거리가 있다. 따라서 장기적으로는 이러한 고수준 의사결정과 결합되어야 가정된 참조 없이도 동작할 수 있을 것이다. 마지막으로, 실세계 적용을 위해서는 정확한 상태추정이 필수인데, 논문에서도 물체 포즈 추정을 위해 특정 비전 모듈(FoundationPose)을 사용하고 있다. 만일 객체 인식이나 추적에 오류가 생기면 제어 성능이 떨어질 수 있으므로, 센서 신뢰도에 대한 의존성도 고려해야 한다.\n정리하면, DexTrack은 현실적인 로봇 손 활용에 한 걸음 다가선 유망한 방식이지만, 대량 학습 데이터 확보 비용, 훈련 범위를 벗어난 물체에 대한 일반화 한계, 실시간 적응성 부족, 센서 의존성 등의 측면에서 앞으로 개선 여지가 있다. 저자들도 향후 Homotopy 최적화 과정을 가속화하거나 더 효율적인 데모 확보 방식을 연구하여 훈련 속도를 높이는 것이 과제로 남아 있다고 밝히고 있다."
  },
  {
    "objectID": "posts/paper/2025-08-10-dextrack.html#구조화된-요약-및-주요-실험-결과-분석",
    "href": "posts/paper/2025-08-10-dextrack.html#구조화된-요약-및-주요-실험-결과-분석",
    "title": "📃DexTrack 리뷰",
    "section": "4. 구조화된 요약 및 주요 실험 결과 분석",
    "text": "4. 구조화된 요약 및 주요 실험 결과 분석\n연구 개요: DexTrack은 복잡한 다지 로봇 손 조작 문제에 대해, 인간 시범 경로를 추적하는 범용 정책을 학습시키는 접근법이다. 이를 통해 각 작업마다 따로 훈련하지 않고도 다양한 물체 조작 기술을 하나의 신경망 제어기로 수행하는 것을 목표로 한다. 핵심 아이디어는 대량의 로봇 추적 데모(인간 참조 + 성공 액션 시퀀스)를 반복적으로 확보하여, 강화학습과 모방학습을 결합해 제어기를 향상시키는 것이다. 또한 어려운 개별 시범은 Homotopy 경로로 단계적 해결하여 데모의 다양성을 높였다.\n기술 구성: DexTrack 알고리즘은 다음과 같은 단계로 이뤄진다:\n\n데이터 준비: 인간 시범 모션 리타게팅 – GRAB, TACO 등 인간-물체 상호작용 데이터셋의 손동작을 로봇 손모델(시뮬레이터 상 Allegro Hand)에 맞게 변환하여 로봇 참조 궤적 집합을 생성. 예컨대 컵을 잡고 따르는 사람 손 움직임 → 로봇 손 관절각도 참조 시퀀스.\n초기 데모 수집: 참조 궤적 일부를 개별 강화학습으로 최적 추종해봄으로써 성공 사례(tracking demonstration)를 모은다. 이때 잔여 정책(residual policy) 기법으로 참조 대비 보정 동작만 학습하여 효율을 높인다.\n정책 학습(RL+IL): 모인 데모를 모방학습(Behavior Cloning)하여 추적 정책을 학습하고, 추가로 추적 보상 기반 RL(PPO)로 미세 조정하여 노이즈나 새로운 상황에도 견딜 수 있게 만든다. 관측 상태에는 현재 로봇 손/물체 상태, 참조 궤적(앞으로의 목표 자세), 이전 액션 등이 포함된다.\nHomotopy 최적화: 현 정책으로 추적에 실패하는 어려운 참조에 대해, 해당 궤적을 단계별 더 쉬운 참조들로 분해하여 각 단계를 RL로 해결함으로써 최종 성공 데모를 얻는다. 이렇게 새 데모를 데이터셋에 추가하여 다시 정책을 학습시키는 루프를 반복한다.\n\n실험 설정: 저자들은 두 가지 공개 데이터셋(GRAB: 일상 동작 1269개 시퀀스, TACO: 도구 사용 동작 2316개 시퀀스)을 활용하여 DexTrack을 훈련하고 평가했다. 훈련은 Isaac Gym 시뮬레이터 상에서 8192병렬 환경으로 진행되었고, Allegro 로봇 손(4손가락, 16자유도)을 사용했다. 평가는 각 데이터셋의 미보seen 궤적에 대한 추적 성공률로 측정되며, 추가로 실제 로봇 손(LEAP Hand)으로 현실 실험도 수행되었다. 벤치마크로는 세 가지 비교 방법이 설정되었다: (1) DGrasp – 기존 모방+최적화 기법을 추적 문제로 변형, (2) PPO (OmniGrasp reward) – OmniGrasp 논문의 보상함수로 PPO 학습, (3) PPO (tracking reward) – DexTrack이 제안한 동일 환경에서 보상만 가지고 PPO 학습한 순수 RL. 성능 지표로는 물체의 회전/이동 오차, 손목 자세 오차, 손가락 관절 오차의 평균과, 최종적으로 성공률(오차가 임계값 이하일 때 성공) 등이 사용되었다. 성공 기준은 오차 임계값을 엄격하게(strict) 혹은 완화하여(lenient) 두 종류로 산정되었다.\n주요 실험 결과: DexTrack은 모든 기준에서 기존 방법들을 앞서는 성능을 보였다. 추적 정확도 지표(물체 자세 오차 등)에서 DexTrack이 가장 낮은 오차를 기록했고, 무엇보다 작업 성공률에서 큰 격차를 나타냈다. 아래 표는 DexTrack과 가장 성능이 좋았던 기존 기법(PPO 기반 RL)의 성공률 비교를 보여준다:\n\n성공률 비교 (Strict / Lenient 기준)\n\n\n\n\n데이터셋\n기존 PPO 기법\nDexTrack (제안)\n\n\n\n\nGRAB (일상 동작)\n38.58% / 54.82%\n46.70% / 65.48%\n\n\nTACO (도구 사용)\n34.98% / 57.64%\n48.77% / 74.38%\n\n\n\n\n참고: Strict은 더 엄격한 성공 기준, Lenient는 다소 완화된 기준이며, DexTrack은 두 경우 모두에서 최고 성능을 달성했다.\n\nDexTrack은 평균적으로 10%p 이상 높은 성공률을 보이며, 특히 복잡한 동작일수록 격차가 더욱 커졌다. 예를 들어, 손바닥에서 물체를 재배치하는 세밀한 동작이나 얇은 물체 잡아 흔들기 등의 과제에서 DexTrack은 참조 경로를 거의 완벽하게 따라간 반면, 기존 PPO 기반 정책은 초반 파지부터 실패하는 모습을 보였다. 이는 DexTrack의 일반화 능력과 접촉 다루는 섬세함이 기존 대비 크게 향상되었음을 시사한다. 더욱이 잡음이 큰 비현실적 참조에 대한 실험에서도, DexTrack 정책은 손가락이 물체를 통과하는 등 모순된 입력이 주어져도 상황에 맞게 자세를 조정하며 끝까지 동작을 수행하여 강인함을 보여주었다.\n실제 로봇 실험 결과: 시뮬레이션에서 높은 성능을 보인 DexTrack 정책은 현실 환경에도 직접 적용되었다. 연구진은 학습된 제어기를 별도 도메인 적응 없이 물체 인식 시스템(카메라 기반 포즈 추적)과 연동하여 실제 로봇 손+팔로 실행했다. 그 결과 사과 들어올리기, 망치질, 물체 건네주기 등 10여 가지 실제 시나리오에서 대부분 성공적인 조작을 시현해 보였다. 정량적으로 봤을 때도 현실 성공률은 DexTrack이 기존 대비 월등히 높았는데, 예를 들어 사과 집어들기의 경우 엄격한 기준에서 기존 방법 성공률 0% vs DexTrack 25%, 망치 쥐고 사용하기 0% vs 50% 등 모든 객체에 대해 우위를 보였다. 이러한 실험은 DexTrack의 접근법이 시뮬레이션에 국한되지 않고 실제 로봇에서도 통한다는 것을 증명하며, 범용 로봇 손 기술의 실용화 가능성을 높여준다. 다만 현실 실험에서는 상태 추정 오차, 마찰 계수 차이 등으로 인해 성공률이 시뮬레이션만큼 높게 나오지 않은 사례도 있었다. 그럼에도 DexTrack은 최고 성능 기준으로 실세계에서도 기존 대비 뚜렷한 성능 향상을 보여줬으며, 이는 곧 본 기법의 우수한 일반화 능력과 구현 가능성을 입증하는 결과이다.\n요약 및 평가: DexTrack 논문은 인간 시범 학습을 통한 범용 로봇 조작의 실현에 있어 중요한 진전을 이루었다. 기술적으로 데이터 주도 접근과 학습 기법의 조합으로 난제를 풀었고, 실험적으로도 다양한 복잡 작업에서 주목할 만한 성공률 개선을 입증했다. 특히 데이터 플라이휠을 통한 점진적 학습 향상, Homotopy 최적화로 난이도 완화, RL+IL 병행으로 강인성 확보 등의 아이디어는 관련 연구 대비 뚜렷한 혁신 포인트로 평가된다. 실제 로봇 적용 결과는 이 방법의 실용적 잠재력을 보여주지만, 동시에 데이터 확보 비용 등의 현실적인 한계도 드러냈다. 전반적으로 DexTrack은 한계가 분명한 기존 방법들을 넘어, 범용적인 로봇 손 제어기의 가능성을 제시한 의미 있는 연구로 볼 수 있다. 향후 데모 확보 효율화, 새로운 물체 카테고리에 대한 일반화 추가 개선 등이 이루어진다면, 인간의 능숙한 손동작을 로봇이 학습하여 다양한 작업을 수행하는 비전이 한층 가까워질 것으로 기대된다."
  },
  {
    "objectID": "posts/paper/2025-09-22-imitation-to-refine.html",
    "href": "posts/paper/2025-09-22-imitation-to-refine.html",
    "title": "📃From Imitation to Refinement 리뷰",
    "section": "",
    "text": "Paper Link\nHomepage\n\n\n기존 Behavior Cloning (BC) 방식은 오프라인 데이터의 분포 변화와 액션 청킹으로 인한 폐루프 제어 부족 때문에 정밀한 로봇 조립 작업에서 성능이 포화되고 신뢰성이 떨어진다는 한계를 보였습니다.\n본 논문은 미리 학습된 BC 모델에 강화 학습(RL)으로 훈련된 폐루프 잔여 정책(residual policy)을 결합하는 ResiP(Residual for Precise Manipulation)를 제안하여, BC가 궤적 “계획자” 역할을 하고 잔여 정책이 실시간으로 미세한 교정을 수행하게 합니다.\nResiP는 50개의 시연 데이터만으로도 정밀 조립 작업에서 기존 BC보다 훨씬 높은 성공률(예: peg-in-hole에서 5%→99%)을 달성하며, 분포 변화와 동적 교란에 대한 로봇의 견고성을 크게 향상시켰습니다.\n\n\n\n\n\n\nBrief Review\n이 논문은 정밀 조립과 같은 로봇 조작 작업에서 Behavior Cloning (BC) 정책의 신뢰성 부족과 성능 포화 문제를 다룹니다. 이러한 한계는 주로 오프라인 데이터 사용으로 인한 분포 변화(distribution shift)와 행동 덩어리(action chunking)를 통한 개방 루프(open-loop) 실행으로 인한 폐쇄 루프(closed-loop) 보정 제어의 부재에서 기인합니다.\n저자들은 행동 덩어리를 예측하는 BC 정책이 반응형 컨트롤러보다는 궤적 “플래너(trajectory planner)”에 가깝다고 분석합니다. 이러한 문제를 해결하기 위해, ResiP(Residual for Precise Manipulation)이라는 간단하면서도 효과적인 방법을 제안합니다. ResiP는 고정된(frozen) 청크(chunked) BC 모델에 RL(Reinforcement Learning)로 훈련된 완전히 폐쇄 루프 방식의 잔여(residual) 정책을 추가합니다. 이 잔여 정책은 분포 변화를 해결하고 BC 궤적 플래너가 예측한 행동 청크의 개방 루프 실행에 대한 폐쇄 루프 보정을 도입합니다.\nResiP의 핵심 방법론은 다음과 같습니다:\n\nBase Policy Learning via Behavior Cloning:\n\n50개의 시뮬레이션 데모 데이터를 사용하여 기본 정책 \\pi_{base}를 훈련합니다. Diffusion Policy (DP) 아키텍처를 사용하며, 이는 최신 BC 훈련 기법에 따라 여러 미래 행동을 예측하는 행동 청크(action chunks)를 활용합니다.\n정책은 T_a 길이의 미래 행동 시퀀스를 예측하고, 그 중 T_{exec} 길이의 부분 집합([a^{base}_t, \\dots, a^{base}_{t+T_{exec}-1}])만 실행합니다. 이 논문에서는 T_a=32, T_{exec}=8을 사용합니다.\n\nReactive Control via ResiP:\n\nBC로 얻은 초기 청크 기본 정책 \\pi_{base}를 사용하여 분포 변화 및 반응성 부족 문제를 개선합니다.\n잔여 정책 \\pi_{res}는 PPO(Proximal Policy Optimization)로 훈련된 작은 단일 스텝 Gaussian Multi-Layer Perceptron (MLP)입니다.\n각 타임스텝 i = 0, \\dots, T_{exec}-1에 대해 잔여 정책은 현재 시뮬레이션 상태(로봇 고유 정보 및 객체 자세)와 기본 정책이 예측한 행동 a^{base}_{t+i}를 연결한 s^{res}_{t+i} = [s_{t+i}, a^{base}_{t+i}]를 관측합니다.\n잔여 정책은 이 관측을 바탕으로 보정 행동 a^{res}_{t+i} \\sim \\pi_{res}(\\cdot|s^{res}_{t+i})을 생성하여 기본 행동을 수정합니다. 최종 실행 행동은 a_{t+i} = a^{base}_{t+i} + a^{res}_{t+i}입니다. 이 per-timestep 보정 기능은 정밀 작업에 필수적입니다.\n훈련 중에는 탐색 노이즈를 조정하여 새로운 행동을 발견하고 작업 성공을 위한 충분한 정밀도를 유지합니다. 잔여 정책은 orthogonal initialization을 사용하여 초기 보정이 0 주변에 집중되도록 합니다.\n\nSim-to-Real Transfer:\n\n시뮬레이션에서 훈련된 상태 기반 \\pi_{res} 정책을 교사(teacher) 정책 \\pi_{teacher}로 활용하여 시각 기반(vision-based) 학생(student) 정책 \\pi_{student}로 지식 증류(distillation)합니다.\n\\pi_{teacher}로부터 성공적인 궤적 데이터셋 D_{synth}를 수집하고, 이를 실제와 같은 이미지 관측 D_{synth-render}로 다시 렌더링하여 시각적 다양성을 도입합니다.\n소량의 실제 데모 D_{real}와 합성 렌더링된 데이터셋 D_{synth-render}를 결합하여 \\pi_{student}를 훈련합니다. \\pi_{student}는 ResNet18 시각 인코더가 있는 Diffusion Policy 아키텍처를 사용합니다.\n\n\n주요 결과는 다음과 같습니다:\n\nResiP는 “one leg” 작업에서 50개의 데모만으로 98%의 성공률을 달성하여, 100K 데모를 사용한 순수 BC 방식(54%)보다 18% 포인트 향상된 성능을 보입니다. “peg-in-hole”과 같은 정밀 작업에서는 5%에서 99%로 성공률이 크게 향상되었습니다.\nBC 정책의 실패 원인이 작은 부정확성(small imprecision)임을 분석하고, ResiP가 이러한 오류를 미세한 조정으로 안정적으로 수정하며 새로운 그립 전략과 같은 질적으로 다른 행동을 발견하기도 합니다.\nResiP는 청크 기반 메서드(DP, DP-DAgger, ResiP-C)보다 동적 교란(dynamic disturbances)에 대해 더 강력한 로버스트니스(robustness)를 보여줍니다(성능 하락 12% vs. 19-26%).\n잔여 정책의 아키텍처는 깊은 RL 훈련을 불안정하게 만드는 큰 정책 업데이트를 방지하여 안정적인 훈련과 높은 샘플 효율성을 제공합니다.\n시뮬레이션 데이터와 실제 데모를 공동 훈련(co-training)하면 실제 배포 시 성능이 크게 향상되고, 부품 색상 변화와 같은 시각적 변화에 대한 로버스트니스를 제공합니다.\n\n제한 사항으로는, ResiP의 지역적 특성으로 인해 초기 장면 무작위성이 매우 높은 경우에는 어려움을 겪을 수 있으며, 시각 기반 정책의 sim-to-real 전이 및 지식 증류 과정에서 여전히 성능 격차가 존재한다는 점이 언급되었습니다.\n\n\n\nDetail Review\n\n\nReference\n\nResidual Off-Policy RL for Finetuning Behavior Cloning Policies"
  },
  {
    "objectID": "note.html",
    "href": "note.html",
    "title": "Note",
    "section": "",
    "text": "📘 Diary | 🌎 Language | 📝 Study\n\n\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n    \n  \n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\n\n\n\n\n\nSep 25, 2025\n\n\n📝Comparative Analysis of Robotic Simulation Environments\n\n\n\n\n\n\nAug 25, 2025\n\n\n📝Simluator Comparison\n\n\n\n\n\n\nJul 31, 2025\n\n\n📝Linux Cheet Sheet\n\n\n\n\n\n\nJul 8, 2025\n\n\n📝Git LFS 사용하기\n\n\n\n\n\n\nJun 23, 2025\n\n\n📝ROS1/2 Errors\n\n\n\n\n\n\nMar 30, 2025\n\n\n📘글또를 마치며\n\n\n\n\n\n\nMar 2, 2025\n\n\n📘글또 발표 후기\n\n\n\n\n\n\nJan 13, 2025\n\n\n📝ROS2 Build Issue\n\n\n\n\n\n\nJan 7, 2025\n\n\n📝Python과 C++ 노드를 모두 포함하는 ROS2 패키지 생성\n\n\n\n\n\n\nJan 5, 2025\n\n\n📘Goodbye 2024\n\n\n\n\n\n\nDec 19, 2024\n\n\n📝__init__ VS. __call__\n\n\n\n\n\n\nOct 9, 2024\n\n\n📘Geultto 10th Start\n\n\n\n\n\n\nJan 15, 2024\n\n\n📝Operating System 001\n\n\n\n\n\n\nDec 1, 2023\n\n\n📘Geultto 9th Start\n\n\n\n\n\n\nAug 31, 2023\n\n\n🌎Casual English Phrases 010\n\n\n\n\n\n\nJul 7, 2023\n\n\n📘Geultto 8th End\n\n\n\n\n\n\nJul 5, 2023\n\n\n🌎Casual English Phrases 009\n\n\n\n\n\n\nMay 27, 2023\n\n\n📘Github Starstruck 128\n\n\n\n\n\n\nApr 5, 2023\n\n\n🌎Casual English Phrases 008\n\n\n\n\n\n\nMar 31, 2023\n\n\n🌎Casual English Phrases 007\n\n\n\n\n\n\nMar 30, 2023\n\n\n🌎Casual English Phrases 006\n\n\n\n\n\n\nMar 29, 2023\n\n\n🌎Casual English Phrases 005\n\n\n\n\n\n\nFeb 2, 2023\n\n\n📘Geultto 8th Start\n\n\n\n\n\n\nOct 29, 2022\n\n\n📘Gueltto 7th End\n\n\n\n\n\n\nOct 24, 2022\n\n\n🌎IT English Experssions 004\n\n\n\n\n\n\nOct 17, 2022\n\n\n🌎Casual English Phrases 003\n\n\n\n\n\n\nOct 11, 2022\n\n\n🌎Casual English Phrases 002\n\n\n\n\n\n\nOct 3, 2022\n\n\n🌎Casual English Phrases 001\n\n\n\n\n\n\nSep 4, 2022\n\n\n📘2022 상반기 회고\n\n\n\n\n\n\nMay 6, 2022\n\n\n📘Geultto 7th Start\n\n\n\n\n\n\nJan 31, 2021\n\n\n📘2021 TU Berlin Winter Course 수강 후기\n\n\n\n\n\n\nJan 3, 2021\n\n\n📘Goodbye 2020\n\n\n\n\n\n\nJan 3, 2021\n\n\n📘Hello 2021\n\n\n\n\n\n\nOct 21, 2020\n\n\n📝On policy VS. Off policy\n\n\n\n\n\n\nNo matching items"
  }
]