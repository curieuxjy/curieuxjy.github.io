[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Jung Yeon Lee",
    "section": "",
    "text": "My name is Jung Yeon Lee, and I am deeply passionate about robotics and reinforcement learning. I enjoy exploring innovative learning methods, solving complex problems, and contributing to the advancement of technology that fosters closer collaboration between humans and robots.\nMy guiding principle is Stop Wishing, Start Doing, and I am committed to working toward a future where humans and robots coexist harmoniously and dynamically.\n     \n\nMy Interest Fields are\n\nMachine learning | Deep learning | Reinforcement learning\nBioinspired-Robots | Simulations for Robotics\nOn-device AI | Quantum computing\n\n\nSkills and Tools:\n\n                                                     \n\n\n\n\nEducation\n\nM.Eng. 2022.03~2024.02\n\nM.S. in MECHANICAL ENGINEERING, SungKyunKwan University(SKKU)\nResearcher at Robotics Innovatory Lab. Quadrupedal Walking Robot Team\n\n\n\n2021.01.11~01.29\n\nTU Berlin Winter University Online : Machine learning using Pyhon - Theory and Application\n\nGrade : 1.0(the best score)\n5 credit points according to the ECTS(European Credit Transfer System)\n\n\n\n\nB.Eng. 2018.03~2022.02\n\nB.S. in ENGINEERING MECHANICAL & SYSTEM DESIGN ENGINEERING, HongIk University\nIntegrated Major in Design Engineering\nCompleted Accereditation Board for Engineering Education of Korea(ABEEK)\nUndergraduate research student at Autonomous Navigation Lab"
  },
  {
    "objectID": "posts/paper/2024-12-22-rotating-without-seeing.html",
    "href": "posts/paper/2024-12-22-rotating-without-seeing.html",
    "title": "📃Rotating without Seeing 리뷰",
    "section": "",
    "text": "이번 포스팅에서 리뷰할 논문은 Rotating without Seeing: Towards In-hand Dexterity through Touch 입니다. RSS(Robotics: Science and Systems) 2023 학회에서 발표된 해당 논문은 사람이 시각 없이 촉각만으로 손안에서 물체를 정교하게 조작하는 능력을 로봇 핸드에 구현하고자, 손바닥, 손가락 관절, 손끝 전체에 넓게 분포된 저비용의 이진 촉각 센서를 활용하여, 시뮬레이션에서 강화학습으로 학습한 정책을 실제 로봇 손에 적용하고, 이를 통해 학습한 물체뿐만 아니라 학습하지 않은 새로운 물체까지 조작할 수 있는 시스템인 Touch Dexterity를 제안합니다."
  },
  {
    "objectID": "posts/paper/2024-12-22-rotating-without-seeing.html#problem-formulation",
    "href": "posts/paper/2024-12-22-rotating-without-seeing.html#problem-formulation",
    "title": "📃Rotating without Seeing 리뷰",
    "section": "2.1 Problem Formulation",
    "text": "2.1 Problem Formulation\nTouch Dexterity는 강화학습 방법으로 제어를 하기 때문에 강화학습의 문제 정의 방식인 MDP(Markov Decision Process)의 요소들, State, Action, Reward 순으로 확인해보겠습니다.\n\n2.1.1 State\nState는 Hand Robot Agent의 상태를 나타내는 요소들로 이루어집니다. Allegro hand 로봇의의 joint position(16 차원), sensor observation(16차원), 이전 position target(16차원), 그리고 rotation axis(2차원)로 구성되어 있습니다. 핸드 로봇의 관절(joint) 부분들이 작은 모터들 16개로 이루어져 있고, FSR(Force Sensing Resistor) 센서들도 총 16개가 아래 그림처럼 손가락과 손바닥에 분포되어 있어 State 벡터의 차원들이 다음과 같이 구성되게 됩니다. 이렇게 구성된 State가 학습 시에 Policy Network의 Input으로 들어가게 되는데 1 time step 정보만으로는 학습하기에 부족한 정보량이기 때문에 현재 시점 기준 이전 스텝 2 time step을 합쳐(concatenation), 총 3 time step 을 쌓아서 policy network에 input으로 넣어줍니다.\n\n\n\nState 구성요소\n\n\n\n\n2.1.2 Action\nHand Agent가 움직이는 Action은 로봇의 각 관절(joint) 모터들이 움직이는 것으로 생각할 수 있습니다. 따라서 Policy network에서는 16차원의 모터와 관련된 어떠한 command 정보가 나오게 됩니다. 하지만 Policy network의 output인 a_t를 바로 쓰는 것이 아닌 PD Controller에 적용하기 위한 값으로 변환하는 과정을 거치게 됩니다. 결과적으로 PD Controller에서 사용하는 값은 \\tilde{q}_{t+1} (현재 time step이 t 이므로 앞으로 제어할 position의 time step 첨자는 t+1)인 것 입니다.\n하지만 여기서 \\tilde{q}_{t+1}을 바로 적용할 경우 생기는 문제가 있습니다. policy network output 값들이 연속적인 시간 순으로 봤을때 갭이 큰 값들이 나타나게 되면 부드러운 움직임을 가질 수 없습니다. 따라서 해당연구에서는 Exponential moving average 방법을 사용하여 smoothing하는 과정을 거치게 됩니다.\n\n\n\nAction이 적용되는 과정\n\n\n아래 그래프는 논문에서 제시한 파라미터(\\eta, 2 consecutive steps)로 랜덤한 포인트들을 가지고 smoothing하는 모습을 보여줍니다.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Parameters\neta = 0.8  # Smoothing factor\nsteps = 2  # Step size for x-axis\nn_points = 200  # Number of points\n\n# Generate data\nx = np.arange(0, n_points, steps)\ndata = np.sin(x / 5) + np.random.normal(0, 0.3, len(x))  # Random data with noise\nema = []\n\n# Calculate EMA\nfor i, point in enumerate(data):\n    if i == 0:\n        ema.append(point)  # Initialize EMA with the first data point\n    else:\n        ema.append(eta * point + (1 - eta) * ema[-1])\n\n# Plot\nplt.figure(figsize=(8, 3))\nplt.plot(x, data, label=\"Data\", marker=\"o\", linestyle=\"--\", alpha=0.6)\nplt.plot(x, ema, label=\"Exponential Moving Average (EMA)\", linewidth=2)\nplt.xlabel(\"Step\")\nplt.ylabel(\"Value\")\nplt.title(f\"Exponential Moving Average (eta={eta}, step={steps}) \")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n이렇게 최종적으로 계산된 Action 값으로 Hand Agent의 모션이 만들어지게 됩니다.\n\n\n2.1.3 Reward\n보상함수는 아래와 같이 6개의 term들로 구성되어 있습니다. 각 6개의 reward term들은 linear weighted sum이 되어 해당 timestep에서의 최종 reward가 됩니다.\n\n\n\nReward Function\n\n\n\nReward of rotation r_{rot}\n\n회전 축 k의 법선 평면 \\Pi에서 샘플링된 단위 벡터의 회전 각도 \\Delta \\theta로 정의된 회전 보상입니다.\n\n\n\nReward Function\n\n\n계산하는 과정\n\n법선 평면 \\Pi에서 단위 벡터 v를 임의로 샘플링하며, 이 벡터는 object에 부착된 것으로 간주할 수 있습니다.\n다음 상태에서의 해당 벡터 v'를 가져와 \\Pi에 투영(projection)합니다: v'_p = \\text{Proj}(v', \\Pi)\n\\Delta \\theta \\in [-\\pi, \\pi)는 축 k에 대해 v'_p와 v 사이의 부호 있는 거리로 정의됩니다.\n\nobject의 움직임이 매우 복잡한 경우, 시뮬레이터가 제공하는 각속도가 매우 노이즈가 심하기 때문에 보상에 이 각속도를 보상함수에 사용할 경우 특정 자세에서 진동하는 등 바람직하지 않은 object 움직임 패턴이 나타날 수 있습니다.\n이 유한 차분(finite difference)을 보상으로 사용하는 것이 서로 다른 실행에서도 일관된 회전 동작을 생성할 수 있습니다.\n\nPenalty of object’s velocity r_{vel}\n\n손이 object를 안정적으로 회전시키도록 장려하며, 훈련된 정책의 transferability을 향상시킵니다.\n\nPenalty of falling r_{fall}\n\nobject가 손바닥에서 떨어질 때 적용되는 negative penalty입니다.\n\nPenalty of the work controller r_{work}\n\n컨트롤러의 일(work)의 양을 패널티로 부과합니다. 이 reward term의 torque \\tau는 t에서 PD 컨트롤러가 출력한 토크입니다. 이 페널티는 손가락 움직임의 부드러움을 향상시키는 데 도움을 줍니다.\n\nPenalty of torque r_{torque}\n\n큰 토크 출력값에 패널티를 부과합니다.\n\nReward of distance r_{dist}\n\n거리 보상으로, 손끝이 객체에 가까이 가서 상호작용하도록 장려합니다.\n\nd(x_{\\text{tip}}, x_{\\text{obj}})는 손끝 위치 x_{\\text{tip}}와 객체 위치 x_{\\text{obj}} 사이의 거리입니다.\n\\epsilon은 작은 양으로, 분모가 0이 되는 것을 방지합니다.\nc_2와 c_3는 보상의 클리핑 범위를 정의하는 하한과 상한입니다.\n\n\n\n\n\n2.1.4 Reset\n불필요한 exploration를 줄이고 학습 과정을 가속화하기 위해 object가 초기 위치(즉, 손바닥의 중심)에서 너무 많이 벗어날 경우 에피소드를 리셋합니다. 또한, object의 주요 축이 회전 축에서 너무 많이 벗어날 경우에도 에피소드를 리셋합니다."
  },
  {
    "objectID": "posts/paper/2024-12-22-rotating-without-seeing.html#domain-randomization",
    "href": "posts/paper/2024-12-22-rotating-without-seeing.html#domain-randomization",
    "title": "📃Rotating without Seeing 리뷰",
    "section": "2.2 Domain Randomization",
    "text": "2.2 Domain Randomization\n강화학습의 Sim2Real Gap을 줄이기 위해 학습 단계에서 Domain Randomization을 적용하는데 해당 연구에서는 2가지 Domain Randomization을 진행합니다.\n\n물리적 랜덤화:\n\nrotation하는 object의 초기 위치, 질량, 형태, 마찰을 랜덤화하여 학습된 정책이 다양한 종류의 객체를 처리할 수 있도록 합니다.\nPD 컨트롤러의 게인을 랜덤화하여 실제 환경에서 PD 컨트롤러의 불확실성을 모델링합니다.\n각 촉각 센서를 랜덤화하는 것도 고려합니다. 활성화된 접촉 센서(출력이 1인 경우)에 대해, 확률 p로 출력을 0으로 뒤집습니다.\n지수 지연 모델(exponential delay)을 통해 접촉 센서의 신호 지연을 모델링합니다.\n\n비물리적 랜덤화\n\npolicy의 observation과 출력된 action에 화이트 노이즈를 주입하여 작은 외란에도 강인하도록 만듭니다."
  },
  {
    "objectID": "posts/paper/2024-12-22-rotating-without-seeing.html#training-procedure",
    "href": "posts/paper/2024-12-22-rotating-without-seeing.html#training-procedure",
    "title": "📃Rotating without Seeing 리뷰",
    "section": "2.3 Training Procedure",
    "text": "2.3 Training Procedure\nProximal Policy Optimization (PPO) 알고리즘을 사용하며 policy 네트워크와 value 네트워크 모두에 다층 퍼셉트론(MLP)을 사용했습니다.\n\n훈련 설정:\n\n이점(advantage) 클립 임계값 ϵ=0.2= 0.2와 KL 발산 임계값 0.020.02를 사용\n네트워크에서 활성화 함수로 ELU를 사용\n정책 네트워크는 학습 가능한 상태 독립적인 표준편차를 가지는 가우시안 분포를 출력\n\n비대칭 관찰(asymmetric observation):\n\n정책 및 가치 네트워크의 학습 난이도를 줄이기 위해 asymmetric observation 을 사용\n\n가치 네트워크: 입력에 접촉력, object의 ground-truth pose, 물리적 파라미터와 같은 특권 정보를 추가\n정책 네트워크: 현재 상태와 함께 3개의 과거 상태를 입력으로 사용하며, 특권 정보는 접근할 수 없음\n\n\n시뮬레이션 설정:\n\nIsaacGym 시뮬레이션에서 시간 간격(dt)은 0.01667초로 설정하고, 2 sub step을 사용\n8192개의 병렬 환경에서 시뮬레이션을 실행\n정책 네트워크가 출력하는 행동(제어 목표)은 6단계 동안 실행되며, 이는 실제 환경에서 10Hz의 제어 주파수에 해당\n\n\n\n\n\nTraining Process"
  },
  {
    "objectID": "posts/paper/2024-12-22-rotating-without-seeing.html#real-world-system-setup",
    "href": "posts/paper/2024-12-22-rotating-without-seeing.html#real-world-system-setup",
    "title": "📃Rotating without Seeing 리뷰",
    "section": "3.1 Real-world System Setup",
    "text": "3.1 Real-world System Setup\n\n\n\nOverview\n\n\n하드웨어 구성은 XArm 로봇 팔과 16자유도(16-DOF)를 가진 Allegro Hand에 접촉 센서 배열을 장착한 형태로 이루어져 있습니다. 손바닥과 손가락 끝을 포함한 Allegro Hand의 여러 부위에 부착된 16개의 접촉 센서로 구성됩니다.\n사용된 접촉 센서는 외부 힘이 표면에 가해질 때 저항이 변하는 Force-Sensing Resistor(FSR) 기반입니다. STM32F 마이크로컨트롤러를 사용하여 각 센서의 아날로그 전압 신호를 수집하고, 이를 디지털 신호로 변환하여 호스트로 전달합니다. 이 접촉 센서는 연속적인 접촉력 측정을 출력할 수 있지만, 신호는 일반적으로 비선형적이고 노이즈가 많습니다. 따라서 이를 제어에 사용하기 전에 적절한 전처리가 필요합니다. 선택된 임계값 \\theta_{\\text{th}}에 따라 이 측정값을 이진화(binarize)하고 이 신호를 사용합니다.\n이진 신호를 사용하는 장점:\n\n시뮬레이션과 실제 로봇 간의 차이를 줄이고, Sim2Real 전이 절차를 단순화할 수 있습니다.\n이진화된 측정값은 임계값을 조정하여 쉽게 보정(calibrate)할 수 있습니다."
  },
  {
    "objectID": "posts/paper/2024-12-22-rotating-without-seeing.html#simulation-setup",
    "href": "posts/paper/2024-12-22-rotating-without-seeing.html#simulation-setup",
    "title": "📃Rotating without Seeing 리뷰",
    "section": "3.2 Simulation Setup",
    "text": "3.2 Simulation Setup\n이 논문에서는 IsaacGym 시뮬레이터 사용합니다. 각 접촉 센서는 손가락과 손바닥 링크의 고정된 링크로 시뮬레이션됩니다. 시뮬레이터는 매 시뮬레이션 단계에서 각 센서 링크에 대한 순 접촉력 F=[Fx,Fy,Fz]F = [F_x, F_y, F_z]를 제공합니다. \\|F\\|을 시뮬레이션된 접촉력 측정값으로 사용한 다음, 이 측정값을 다른 임계값 \\tilde{\\theta}_{\\text{th}}으로 이진화합니다.\n\n\n\n중요한 점은 센서의 부모 링크에서 제공되는 힘은 순 접촉력에 기여하지 않는다는 것입니다. 시뮬레이션에서 실제 환경과 유사한 동작을 보장하기 위해 이 센서들의 임계값 \\tilde{\\theta}_{\\text{th}}을 조정합니다. 시뮬레이션에서는 \\tilde{\\theta}_{\\text{th}} = 0.01N을 사용합니다."
  },
  {
    "objectID": "posts/paper/2024-12-22-rotating-without-seeing.html#benchmark-in-hand-rotation",
    "href": "posts/paper/2024-12-22-rotating-without-seeing.html#benchmark-in-hand-rotation",
    "title": "📃Rotating without Seeing 리뷰",
    "section": "3.3 Benchmark: In-hand Rotation",
    "text": "3.3 Benchmark: In-hand Rotation\n이 논문에서는 시스템의 손재주(dexterity)를 연구하기 위해, 시스템을 사용하여 손 안에서 회전하는 작업(in-hand rotation task)을 목표로 합니다. 이 task는 object가 손바닥에 초기화된 상태로 시작하며, 로봇 손은 주어진 회전 축을 따라 객체를 회전시켜야 합니다. 손 안에서 객체를 회전하는 동안, object의 움직임은 손끝 회전(finger-tip rotation)보다 훨씬 더 복잡하며 특히, 손 안에서 조작하는 동안 객체는 손바닥에서 미끄러지거나 구를 수 있습니다.\n이와 같은 복잡한 움직임 패턴 때문에, 성공적인 조작을 위해 촉각 센서나 비전(vision) 시스템의 명시적인 피드백이 필요합니다. 그렇지 않으면, 현재 객체의 상태를 추론할 수 없으며, 객체를 안전하게 밀거나 회전시키는 데 실패할 수 있습니다."
  },
  {
    "objectID": "posts/paper/2023-07-02-dreamwaq.html",
    "href": "posts/paper/2023-07-02-dreamwaq.html",
    "title": "📃DreamWaQ 리뷰",
    "section": "",
    "text": "이번 포스팅은 DeepMind에서 발표된 DreamWaQ: Learning Robust Quadrupedal Locomotion With Implicit Terrain Imagination via Deep Reinforcement Learning 논문을 읽고 정리한 내용입니다. 최근 ICRA 2023 런던에서 5월 30일부터 6월 1일까지 진행된 Autonomous Quadruped Robot Challenge (QRC)에서 KAIST 연구팀이 1등을 하여 큰 이슈가 되었었습니다. 이번 포스팅에서 리뷰하는 이 논문이 바로 대회에서 사용되었던 강화학습 기반의 보행제어 알고리즘에 대한 내용을 담고 있는 논문입니다."
  },
  {
    "objectID": "posts/paper/2023-07-02-dreamwaq.html#key-contribution",
    "href": "posts/paper/2023-07-02-dreamwaq.html#key-contribution",
    "title": "📃DreamWaQ 리뷰",
    "section": "2.1 Key Contribution",
    "text": "2.1 Key Contribution\n\n\n\nOverview of DreamWaQ\n\n\nDreamWaQ 알고리즘의 전체적인 흐름은 위의 사진과 같습니다. “Dream”이라는 워딩과 알고리즘 개괄도에서 생각 풍선 모양 표현에서 볼 수 있듯이 DreamWaQ 논문의 주요 Contribution으로는 Implicit Terrain Imagination을 할 수 있도록 Context-Aided Estimator Network(CENet)을 도입하였고 안정적으로 Policy가 학습될 수 있도록 Adaptive Bootstrapping(AdaBoot)방법을 제안하여 강화학습 보행 제어기를 설계한 점을 들 수 있습니다."
  },
  {
    "objectID": "posts/paper/2023-07-02-dreamwaq.html#implicit-terrain-imagination",
    "href": "posts/paper/2023-07-02-dreamwaq.html#implicit-terrain-imagination",
    "title": "📃DreamWaQ 리뷰",
    "section": "2.2 Implicit Terrain Imagination",
    "text": "2.2 Implicit Terrain Imagination\n앞서 챌린지에서 사용된 환경에서 볼 수 있듯이 사족보행로봇은 다양한 지형(Terrain)을 극복하며 보행할 수 있는 능력이 중요합니다. 그럼 다양한 지형을 나타낼 수 있는 속성들에는 무엇이 있을까요? 지형의 마찰계수, 반발계수, 놓여져 있는 장애물, 울퉁불퉁한 정도 등등 여러가지 속성들로 지형의 특징을 나타낼 수 있을 것입니다. 그리고 그러한 특징을 어떻게 4개의 다리를 이용하여 보행의 어려운 점들을 극복하며 원하는 방향으로 이동할 수 있도록 하는 것이 관건인 것입니다.\n이런 지형의 특징을 파악하기 위해 많은 연구들이 카메라나 라이다와 같은 비젼센서를 부착하여 환경을 인식한 뒤 극복하기 위한 방법을 고안하는 방향으로 연구되고 있습니다. 하지만 KAIST 연구진이 제안한 DreamWaQ에서는 지형을 인식할 수 있는 부차적인 비젼센서 없이 로봇의 자체의 정보(proprioception)를 이용하여 지형을 극복하기 위해 explicit한 환경 정보가 아닌, implicit한 terrain imagination을 할 수 있는 방법론은 제시했습니다.\n사실 Implicit하게 로봇 주변의 지형이나 환경정보를 강화학습 로봇 에이전트가 인식할 수 있도록 하는 연구는 다양하게 진행되어왔었습니다. 앞선 주요 방법은로는 Teacher-Student Network를 이용하여 모든 환경정보를 학습한 Teacher Network로부터 Student Network가 추후에 따라 학습하는 방식이 있었지만, 해당 방법은 Teacher Network를 학습과 Student Network 학습을 따로 2개의 단계를 거쳐 학습을 해야한다는 데이터 비효율적인 학습 방법이라는 단점이 있었습니다. 따라서 DreamWaQ에서는 Asymmetric Actor-Critic이라는 기존의 Actor-Critic 강화학습 알고리즘에 약간 변형을 준 모델을 사용하여 Teacher-Student Network처럼 두 단계로 나누어서 학습하지 않고도 Implicit하게 Terrain 정보를 Actor-Critic 구조에 녹여들 수 있도록 했습니다."
  },
  {
    "objectID": "posts/paper/2023-07-02-dreamwaq.html#asymmetric-actor-critic",
    "href": "posts/paper/2023-07-02-dreamwaq.html#asymmetric-actor-critic",
    "title": "📃DreamWaQ 리뷰",
    "section": "2.3 Asymmetric Actor-Critic",
    "text": "2.3 Asymmetric Actor-Critic\n기존의 PPO, SAC와 같은 Policy Gradient의 강화학습 알고리즘들의 주요 구성요소로 Actor Network와 Critic(Value) Network가 있습니다. Actor는 강화학습 에이전트가 취해야하는 action 값을 출력하는 네트워크이며 Critic는 에이전트의 학습 방향을 보여주는 value값을 출력하여 이 2개의 네트워크들이 Policy Gradient 알고리즘의 목적식을 따라 Return(누적 보상)값을 최대화하는 방향으로 학습하게 되는 것입니다. 보통 2개의 네트워크 모두에게 같은 state(혹은 observation) 정보가 입력값으로 들어가게 되기 때문에 Actor 네트워크와 Critic 네트워크는 서로 Symmetric하다고 할 수 있습니다.\n하지만 앞서 로봇이 센서 없이는 얻을 수 없는 지형 정보가 강화학습 알고리즘에 사용되는 네트워크의 인풋으로 들어간다면 실제 로봇에서 알고리즘이 돌아갈 때 넣어줄 지형정보가 없기 때문에 제어 알고리즘이 돌아갈 수 없을 것 입니다. 그래서 DreamWaQ에서는 Actor/Critic Network의 상호작용 과정에서 강화학습 에이전트가 얻을 수 있는 시간적 정보들을 기반으로 terrain 정보를 상상할 수 있도록, Actor 네트워크에 들어가는 입력값과 Critic 네트워크에 들어가는 입력값을 다르게 설계하였고 이를 Asymmetric한 구조라고 볼 수 있습니다.\n\n\n\nAsymmetric Actor-Critic\n\n\n위에 보이시는 것처럼 Actor Network에는 Observation o_t, estimated velocity v_t, latent vector z_t가 입력으로 들어가게 됩니다. v_t와 z_t는 다음 파트에서 좀 더 살펴볼 예정이므로 여기에서는 우선 observation vecter인 o_t에 초점을 맞추어서 보겠습니다. observation 정보는 강화학습 MDP를 정의하는 한 요소로 강화학습 에이전트가 학습할 때 관측(혹은 접근 가능한 정보)하는 정보입니다. 따라서 로봇에 특별한 비젼 센서 추가 없이 로봇 자체 하드웨어에서 얻을 수 있는 정보인 proprioceptive 정보를 기반으로 몸체의 각속도 \\omega_t, 중력방향 벡터 g_t 등등의 정보가 observation vector의 요소로 들어가게 됩니다. 반면, Critic Network에는 State s_t가 입력값으로 들어가는 것을 알 수 있는데 이는 위에서 Observation과 State를 비교해놓은 것과 같이 state가 observation보다 많은 정보를 포함한 것을 알 수 있습니다. 여기에서 주목해서 볼 수 있는 점이 바로 지형에 대한 정보인 heightmap scan h_t가 한 요소임을 알 수 있고 이를 통해 implicit한 terrain imagination이 가능한 것 입니다. Heightmap scan에 대해 조금 더 설명을 덧붙이자면, 지형의 heightmap scan 정보는 실제 로봇에서 얻을 수 있는 정보는 아니고 강화학습 에이전트가 학습하게 되는 시뮬레이션에서만 얻을 수 있는 정보로 지형의 z축 방향의 높이 정보를 말합니다.\n환경을 정의하는 변수이고 시뮬레이션에서는 가상공간이기 때문에 프로그램에서 얻을 수 있는 물리적 정보이지만 실제로 로봇이 이용할 수 없는 정보를 privileged observation이라고 부르기도 합니다. 따라서 기존에 강화학습에서 State가 환경에서 에이전트가 놓여있는 상황을 설명할 수 있는 모든 정보를 말하고 Observation이 환경에 놓여있는 에이전트가 관찰할 수 있는 일부 상태 정보를 뜻하기 때문에 State = Observation + Privileged Observation 포함관계로 이해할 수 있습니다.(논문에서는 privileged observation이라는 표기를 state를 뜻하는 것으로 표기하고 있기 때문에 헷갈릴 수 있습니다.)"
  },
  {
    "objectID": "posts/paper/2023-07-02-dreamwaq.html#context-aided-estimator-network",
    "href": "posts/paper/2023-07-02-dreamwaq.html#context-aided-estimator-network",
    "title": "📃DreamWaQ 리뷰",
    "section": "2.4 Context-Aided Estimator Network",
    "text": "2.4 Context-Aided Estimator Network\n이번 파트에서 살펴보게 될 Context-Aided Estimator Network는 센서로 인식할 수 없는 지형 정보를 에이전트가 유추할 수 있도록 하는 일등공신 아이디어 입니다.\n\n\n\nThe architecture of CENet\n\n\nCENet의 구조는 위와 같이 \\beta-VAE구조를 활용하여 구성되어 있습니다. 일정 time horizon H동안 모은 observation이 Encoder에 들어가면 latent vector z와 몸체의 선속도 추정값인 v_t가 출력값으로 나오게 됩니다. Auto-Encoder의 일반적인 구조를 따라 이 값들이 Decoder의 인풋으로 들어가고 Decoder의 출력값으로는 time horizon을 지난 다음 observation vector o_{t+1}을 reconstruction할 수 있도록 학습하게 되는 것 입니다.\n\n\n\nThe loss of CENet\n\n\n그래서 CENet의 loss function은 크게 2개의 파트 L_{est}와 L_{VAE}로 나누어져 있는 것을 확인할 수 있습니다. 먼저 L_{est}는 보행하는 로봇 에이전트의 속도 추정을 CENet에서 할 수 있도록 학습하기 위한 부분으로, 로봇 몸체의 선속도 추정값 \\tilde{v}_t는 실제 정답값 v_t는 시뮬레이션에서는 얻을 수 있는 값이기 때문에 Encoder에서 추정한 값 \\tilde{v}_t와의 MSE(mean square error)를 구할 수 있습니다. 다음으로 L_{VAE}는 time horizon H동안 누적되 여러개의 observation 정보를 가지고 다음 observation o_{t+1}을 오토인코더 구조로 잘 reconstruction한지를 보는 첫번째 term과 추정 분포를 맞추는 부분인 KL-divergence 제약 조건 두번째 term으로 이루어져 있습니다. (VAE loss에 대해서 더 자세한 정보를 알고 싶으신 분은 이전에 VAE 논문을 리뷰한 포스팅을 참고해주세요.)\n이와 같은 loss 구성으로 학습된 CENet은 여러 타임 스텝동안 관찰된 observation 정보들을 기반으로 에이전트가 privileged observation을 유추할 것으로 기대할 수 있는 이유는 privileged observation을 기반으로 가치를 추정하는 Critic(Value) Network를 통해서 Actor Network가 업데이트 되는 Policy gradient과정을 거치기 때문입니다. 이러한 Asymmetric Actor-Critic구조와의 시너지 효과가 기존의 Context RL 분야에서도 사용되는 아이디어 인데(참조논문: AACC) 이와 비교해보았을 때, Critic Network가 deploy되는 과정에서 쓰이지 않기 때문에 Actor보다 더 많은 정보를 받아서 더 정확한 가치를 추정할 수 있게 한다는 기조는 비슷하지만 time-invarient한 context vector를 만드는 Context RL에서의 Asymmetric Actor-Critic과 다르게 DreamWaQ에서는 time-varient한 변수들을 추정하여 implicit하게 추정할 수 있도록 했다는 점이 다릅니다.\nAdaptive Bootstrapping(AdaBoot)\nAdaptive bootstrapping은 policy 학습과정 중에 Estimator network인 CENet이 안정적으로 학습되도록 하기 위해 domain randomized로 다양화된 여러 환경요소에 대해 에피소드별 reward의 평균값에 대한 표준 편차의 비율인 변동 계수(CV)에 의해 제어되는 방법을 말합니다. 핵심 아이디어는 부정확한 가치 추정에 대한 정책을 보다 견고하게 만들기 위해 m개의 에이전트 reward의 CV가 작을 때 부트스트래핑을 하게됩니다. 반대로 에이전트가 충분히 학습하지 않은 경우에는 reward에서 큰 CV로 표시된 것처럼 부트스트랩을 해서는 안하도록 합니다.\n\n\n\nAdaptive Bootstrapping Probability"
  },
  {
    "objectID": "posts/paper/2023-07-02-dreamwaq.html#simultation-result",
    "href": "posts/paper/2023-07-02-dreamwaq.html#simultation-result",
    "title": "📃DreamWaQ 리뷰",
    "section": "3.1 Simultation Result",
    "text": "3.1 Simultation Result\n\n\n\nThe loss of CENet\n\n\nIsaac Gym 시뮬레이터를 이용하여 PPO 강화학습 알고리즘 이용하여 학습과정 동안의 Episodic Reward 그래프 변화를 살펴보면, EstimatorNet은 처음에는 AdaptationNet보다 평균 에피소드 보상이 높지만, 더 많은 training step 후에 더 어려운 지형과 마주치기 때문에 더 많은 반복 후에 성능이 저하됨을 알 수 있습니다. 반대로 DreamWaQ는 학습 지형이 점점 어려워 짐에도 다른 모든 방법들을 능가하는 퍼포먼스를 보여줍니다. 외부 인식 없이 걷는 것임에도 DreamWaQ는 주변 지형의 heightmap을 다 알 수 있는 오라클 policy만큼 성능이 좋은 것을 볼 수 있습니다.\nExplicit Estimation Comparison\n시뮬레이션에서 한번 지형정보를 Implicit가 아닌 Explicit하게 알려주고 학습한다면 어떤 유의미한 차이가 있는지 알아보는 실험도 진행했습니다.\n\n\n\nAdaptive Bootstrapping Probability\n\n\nTimestep이 늘어날 수록 더 어려운 계단지형에서 보행하도록 학습시킨 결과 Explicit하게 지형정보를 학습한 Estimator는 지형이 어려워지자 Foot stumble 현상이 심하게 있었지만 DreamWaQ는 지형이 어려워져도 작은 foot stumble이 있음을 확인하여 오히려 Implicit하게 지형정보를 학습하는 것인 robust한 보행을 하는 것을 확인할 수 있었다고 합니다."
  },
  {
    "objectID": "posts/paper/2023-07-02-dreamwaq.html#real-world-result",
    "href": "posts/paper/2023-07-02-dreamwaq.html#real-world-result",
    "title": "📃DreamWaQ 리뷰",
    "section": "3.2 Real-world Result",
    "text": "3.2 Real-world Result\n\n\n\n실제 로봇 플랫폼을 가지고 Command tracking error를 plot 해보았을 때도 다른 비교 모델들과 비교해보았을 때 error 값이 적은 것을 확인할 수 있습니다. 특히나 AdaBoot방법이 있고 없고에 따라 error값의 크기가 다른 것을 통해 AdaBoot 방법이 policy 학습에 필요한 것을 확인할 수 있습니다."
  },
  {
    "objectID": "posts/paper/2022-09-17-wavenet.html",
    "href": "posts/paper/2022-09-17-wavenet.html",
    "title": "📃WaveNet 리뷰",
    "section": "",
    "text": "이번 포스팅은 Google DeepMind에서 발표한 WaveNet이라는 논문에 대해 리뷰를 하려고 합니다. WaveNet은 Autoregressive한 Generative model로써 Google의 스피커 서비스에 사용되었다고 많이 알려진 모델입니다."
  },
  {
    "objectID": "posts/paper/2022-09-17-wavenet.html#dilated-casual-convolution",
    "href": "posts/paper/2022-09-17-wavenet.html#dilated-casual-convolution",
    "title": "📃WaveNet 리뷰",
    "section": "1. Dilated Casual Convolution",
    "text": "1. Dilated Casual Convolution\n먼저 Dilated Casual Convolution은 µ-law Companding Transformation 처리를 거친 음성 신호를 받아오는 첫번째 부분입니다.\n\n\n\nCasual Convolution 2\n\n\n우선 Casual 이라는 것은 Time-series인 음성 신호의 시간 순서를 고려하여 현재 시점 t를 기준으로 미래 정보는 사용할 수 없고 현재까지의(과거~현재 t) 정보만 사용할 수 있다는 의미입니다. 왼쪽 Causal Convolution 그림에서 Receptive Field는 (레이어 수) + (필터의 length) -1로 계산되어 총 레이어 수는 4개이고 필터 length는 이전 레이어에서 2개의 정보가 모아져서 다음 레이어의 하나의 데이터로 산출되므로 필터 length는 2라고 볼 수 있습니다. 따라서 4+2-1로 Receptive Field는 5가 되며 이를 그림에서 살펴보면 처음 input에서 5개의 음성 정보가 output의 1개의 정보로 나오는 것을 볼 수 있습니다. 이런 Receptive Field는 매우 짧은 시간에 많은 음성신호가 매칭되는 상황에서 매우 좁으며 RF를 늘리기 위해서는 레이어 수를 늘리거나 필터의 length를 늘려야 하는데 이는 모델을 매우 크게 만들게 되고 계산도 많이 요구됩니다.\n\n\n\nDilated Casual Convolution 2\n\n\n그래서 제안이 된 방법이 바로 Dilated Convolution입니다. 이는 convolution with holes로 해석할 수 있는데 위의 그림에서 볼 수 있듯이 이전 레이어에서 데이터가 Dilated되어 데이터가 듬성듬성하게 모아져서 다음 레이어로 넘어가는 것을 볼 수 있습니다. 이는 skip이나 pooling과 유사해보이지만 input과 output의 차원이 유지된다는 점에서 차이가 있습니다. 이때의 RF는 각 레이어의 Dilation 값을 모두 더하고 마지막에 현재 시점의 데이터 1을 더하며 RF가 계산됩니다. WaveNet에서는 Dilation을 총 30개의 레이어에 적용했고 Dilation 값의 패턴은 input에서 부터 1, 2, …, 512 로 2배씩 늘린 10개의 레이어를 총 3번 반복했습니다. 이때, 1 ~ 512 Dilation 값을 가진 10개 레이어의 RF는 1024로 계산됩니다.\n\n\n\nDilated Casual Convolution Process2\n\n\n\n\n\nDilated Convolution Pattern6\n\n\nCode 구현으로 살펴보면 아래와 같이 구현할 수 있습니다. Casual 특성을 반영하기 위해 self.ignoreOutIndex 을 만들어서 dilation 값을 고려하여 (kernel_size - 1) * dilation으로 계산한 후에 잘라내주는 것을 확인할 수 있습니다.\nclass CasualDilatedConv1D(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, dilation, padding=1):\n        super().__init__()\n        self.conv1D = nn.Conv1d(in_channels, out_channels, kernel_size, dilation=dilation, bias=False, padding='same')\n        self.ignoreOutIndex = (kernel_size - 1) * dilation # casual\n\n    def forward(self, x):\n        return self.conv1D(x)[..., :-self.ignoreOutIndex] # casual"
  },
  {
    "objectID": "posts/paper/2022-09-17-wavenet.html#residual-connection-gated-activation-units",
    "href": "posts/paper/2022-09-17-wavenet.html#residual-connection-gated-activation-units",
    "title": "📃WaveNet 리뷰",
    "section": "2. Residual Connection & Gated Activation Units",
    "text": "2. Residual Connection & Gated Activation Units\n다음으로 Dilated Causal Convolution을 거친 후 통과하게 되는 Residual Connection & Gated Activation Units 부분에 대해서 살펴보겠습니다.\n\n\n\nWaveNet에서 사용된 Gated Activation Units는 PixelCNN에서 사용된 매커니즘을 차용했습니다. 아래의 그림에서 보이는 보라색 Dilated Conv가 앞에서 설명한 DCC이며 이를 거친 후 Convoltion layer와 각각 tanh, sigmoid activation을 통과하여 Filter, Gate가 됩니다. 이 2가지 경로로 계산된 값은 elementwise product를 통해 하나의 벡터로 변환됩니다. 이떄 Dilated를 통과하기 전 값을 Residual Connection을 통해 연결함으로써 딥러닝 모델이 레이어를 더 깊게 쌓을 수 있도록 돕고 더 빠르게 학습할 수 있도록 할 수 있었다고 합니다.\n\n\n\nResidual Connection & Gated Activation Units6"
  },
  {
    "objectID": "posts/paper/2022-09-17-wavenet.html#skip-connection",
    "href": "posts/paper/2022-09-17-wavenet.html#skip-connection",
    "title": "📃WaveNet 리뷰",
    "section": "3. Skip Connection",
    "text": "3. Skip Connection\n\n\n\nSkip Connection은 Dilated Convolution을 통해 다양한 Receptive Field를 가진 각 레이어들의 값을 활용하여 output을 만들어낼 수 있도록 했습니다. 앞서 설명했던 대로 각 Residual Block의 Dilation 값이 다 다르기 때문에 각 Residual Block의 output은 서로 다른 Receptive Field를 가지게 됩니다.\n\n\n\nSkip Connection6\n\n\nResidual Connection과 Skip Connection을 Code로 구현하면 다음과 같습니다. 위에서 설명했던 Gated Activation Units의 tanh, sigmoid activation을 각각의 activation function을 거친후 self.resConv1D을 통과하는 것을 확인할 수 있습니다. 또한 Skip Connection을 구현하는 부분은 self.skipConv1D에서 확인할 수 있습니다. 마지막 return에서 resOutput, skipOutput으로 2개의 output이 나오는 것을 알 수 있습니다.\nclass ResBlock(nn.Module):\n    def __init__(self, res_channels, skip_channels, kernel_size, dilation):\n        super().__init__()\n        self.casualDilatedConv1D = CasualDilatedConv1D(res_channels, res_channels, kernel_size, dilation=dilation)\n        self.resConv1D = nn.Conv1d(res_channels, res_channels, kernel_size=1)\n        self.skipConv1D = nn.Conv1d(res_channels, skip_channels, kernel_size=1)\n        self.tanh = nn.Tanh()\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, inputX, skipSize):\n        x = self.casualDilatedConv1D(inputX)\n        x1 = self.tanh(x)\n        x2 = self.sigmoid(x)\n        x = x1 * x2\n        resOutput = self.resConv1D(x)\n        resOutput = resOutput + inputX[..., -resOutput.size(2):]\n        skipOutput = self.skipConv1D(x)\n        skipOutput = skipOutput[..., -skipSize:]\n        return resOutput, skipOutput\n위와 같은 ResBlock은 전체 구조에서 보시다시피 여러개가 stacked 되어 있으므로 StackOfResBlocks class로 구현하여 WaveNet에 넣어주게 됩니다.\nclass StackOfResBlocks(nn.Module):\n\n    def __init__(self, stack_size, layer_size, res_channels, skip_channels, kernel_size):\n        super().__init__()\n        buildDilationFunc = np.vectorize(self.buildDilation)\n        dilations = buildDilationFunc(stack_size, layer_size)\n        self.resBlocks = []\n        for s,dilationPerStack in enumerate(dilations):\n            for l,dilation in enumerate(dilationPerStack):\n                resBlock=ResBlock(res_channels, skip_channels, kernel_size, dilation)\n                self.add_module(f'resBlock_{s}_{l}', resBlock) # Add modules manually\n                self.resBlocks.append(resBlock)\n\n    def buildDilation(self, stack_size, layer_size):\n        # stack1=[1,2,4,8,16,...512]\n        dilationsForAllStacks = []\n        for stack in range(stack_size):\n            dilations = []\n            for layer in range(layer_size):\n                dilations.append(2 ** layer)\n            dilationsForAllStacks.append(dilations)\n        return dilationsForAllStacks\n\n    def forward(self, x, skipSize):\n        resOutput = x\n        skipOutputs = []\n        for resBlock in self.resBlocks:\n            resOutput, skipOutput = resBlock(resOutput, skipSize)\n            skipOutputs.append(skipOutput)\n        return resOutput, torch.stack(skipOutputs)"
  },
  {
    "objectID": "posts/paper/2022-09-17-wavenet.html#conditional-wavenets",
    "href": "posts/paper/2022-09-17-wavenet.html#conditional-wavenets",
    "title": "📃WaveNet 리뷰",
    "section": "4. Conditional WaveNets",
    "text": "4. Conditional WaveNets\n\n\n\n\n\n\nConditional modeling 6\n\n\nConditional Modeling은 Autoregressive model인 WaveNet에 적용하기 쉽고 이 또한 PixelCNN에서의 아이디어와 유사합니다. Feature h 벡터를 조건 부분에 추가하여 음성 데이터에 조건을 추가할 수 있습니다.\n\np(\\mathbf{x} \\mid \\mathbf{h})=\\prod_{t=1}^T p\\left(x_t \\mid x_1, \\ldots, x_{t-1}, \\mathbf{h}\\right)\n\nCondition에는 크게 2가지로 Global과 Local이 있습니다. 먼저 Global은 Time-invariant한 조건으로 시점에 따라 변하지 않는 조건 정보를 추가하는 것을 말합니다. 예를 들어 한 발화자의 음성은 해당 음성 파일의 어떤 시점에서나 똑같은 condition이기 때문에 Global condition이라고 할 수 있습니다. 이때의 Feature vector h는 linear projection을 거친 후 data x와 더하게 됩니다.\n\n\n\n다음으로 Time-variant한 Local condition은 시점에 따라 변하는 조건 정보를 추가하는 것을 말하는데 음성 데이터보다 길이가 짧지만 순서가 있는 일정 길이의 Sequence vector라고 생각할 수 있습니다. 같은 발화자여도 어떤 단어를 말하느냐에 따라 음성학적인 특징(linguistic feature)가 다를 수 있기 떄문에 local한 조건은 한 음성 파일에 여러개가 있을 수 있습니다. 이때 Feature vector h는 음성 파일과 길이가 다르기 때문에 Upsampling을 거친후 1x1 convolution을 거쳐서 data x와 더해집니다."
  },
  {
    "objectID": "posts/paper/2023-03-12-wasabi.html",
    "href": "posts/paper/2023-03-12-wasabi.html",
    "title": "📃WASABI 리뷰",
    "section": "",
    "text": "이번 포스팅은 WASABI: Learning Agile Skills via Adversarial Imitation of Rough Partial Demonstrations 논문을 읽고 정리한 내용입니다. 4족 보행 로봇 연구에서 많은 연구 성과들을 발표하는 스위스의 ETH Robotic System Lab과 독일의 Max Plank Institude for Intelligent Systems에서 발표한 논문으로, 강화학습에서 중요한 부분들 중 하나인 reward design에 대한 고민을 generatvie adversarial method(WGAN, Wasserstein GAN)를 통해 해결할 수 있음을 보여주었습니다.\n보행 로봇의 모션 제어에서 기본적인 보행뿐만 아니라 다양한 다이나믹한 모션을 수행하도록 로봇의 퍼포먼스를 끌어올리는 방향으로 연구가 활발하게 진행되고 있습니다. 여기서 말하는 다이나믹한 모션들로는 로봇이 공중에서 한바퀴 돌아야 하는 backflip과 같은 기존의 전통적인 보행 제어 연구를 기반으로 rule-based로 제어하기에는 매우 어려운 모션들을 말합니다. 로봇이 이런 모션들을 수행하도록 수학적으로 자세히 명시하고 그리고 모든 물리적 환경요소들을 고려하여 제어하기 어려울 때, 강화학습이라는 인공지능 프레임 워크를 이용하여 reward라는 보상체계를 기준으로 trial-and-error를 통해 모션을 학습하도록 하는 것이 직관적으로 매우 좋은 해결책으로 보입니다.\n하지만 다이나믹한 모션을 각 task로 정의하고 우리가 원하는 방향대로 로봇이 모션들을 학습되기 위해서는 reward를 잘 정의해주어야 하는데 이 과정이 만만치 않게 까다롭고 어려우며, 오히려 수학적인 동역학 모델을 기반으로 제어할 때보다 분석적인 접근이 어렵기 때문에 reward design이라는 과제를 해결해야만 우리가 원했던 다이나믹 모션들을 강화학습을 이용하여 로봇이 수행할 수 있을 것 입니다. 바로 이 부분을 생성모델로 유명한 GAN 모델들 중 하나인 WGAN을 이용하여 해결하고자 했으며 해당 논문에서 가장 흥미로웠던 접근법은 강화학습의 policy를 GAN의 generator 관점으로 바라보고 reward를 추론하도록하는 프레임 워크를 만들었다는 점이었습니다. (이후 관련해서 더 논문들을 찾아보니 생성모델과 강화학습은 닮은 점이 많은 것 같습니다. 관련해서 흥미롭게 읽었던 다른 논문 Connecting Generative Adversarial Networks and Actor-Critic Methods도 관심이 있으시다면 가볍게 읽어보시는 것을 추천드립니다.)"
  },
  {
    "objectID": "posts/paper/2023-03-12-wasabi.html#gan",
    "href": "posts/paper/2023-03-12-wasabi.html#gan",
    "title": "📃WASABI 리뷰",
    "section": "GAN",
    "text": "GAN\n적대적 신경망에 대해 기본적인 이론부터 시작해보겠습니다. GAN은 생성 모델을 학습하기 위한 방법론 중 하나로 Generative, 어떠한 새로운 데이터 생성을 하는, Adversarial 게임과 같이 Discriminator와 Generator라는 2개의 알고리즘 모듈이 경쟁을 하며 학습을 하는 방법론 입니다. 아래 사진에서 보이는 예시로 보면 진짜 모나리자 그림이라는 Real example을 보고 이를 모사한 작품을 파는 화가를 Generator라고 생각해볼 수 있습니다. 그러면 미술 작품 감별사인 Discriminator는 이 작품이 진짜 모나리자 그림인지 아니면 화가가 모사한 가짜 모나리자 인지 판단하게 됩니다. 당연히 Generator 입장에서는 Discriminator가 감별하기 어렵게 점점 더 진짜같은 모나리자를 그리게 되고(new data) Discriminiator 입장에서는 진짜와 가짜 사이에 더 자세하고 민감한 차이를 찾아내어 Generator의 모사품을 찾아내려고 할 것 입니다.\n\n이러한 GAN의 학습 과정에는 지도 학습과 비지도 학습이 모두 들어있습니다. 우선 Discriminator 입장에서는 진짜와 가짜 라벨을 가진, 인풋 데이터가 들어오면 2개의 카테고리들 중 하나를 선택하는 지도학습을 하게 됩니다. Generator는 비지도 학습으로 latent code라는 일종의 trigger 요소인 어떤 벡터를 인풋으로 받으면 진짜 data distribution과 가까운 데이터인 new data를 생성하게 됩니다.\n\n잠깐 data distribution이라는 개념이 GAN에서는 중요한 개념이므로 Probability Distribution(확률 분포)을 간단하게 짚고 넘어가겠습니다. 확률 분포란 어떤 사건을 대변하는 랜덤 변수들의 확률 분포라고 볼 수 있습니다. 주사위를 총 6번 던져서 1, 2, 3, 5가 각각 1번씩 그리고 6이 2번 나왔다면 아래와 같은 확률 분포 그래프를 그릴 수 있고, 이때의 Expectation(기댓값)을 구해보면 1 \\cdot \\frac{1}{6} + 2 \\cdot \\frac{1}{6} + 3 \\cdot \\frac{1}{6} + 4 \\cdot \\frac{0}{6} + 5 \\cdot \\frac{1}{6} + 6 \\cdot \\frac{2}{6} = \\frac{23}{6} \\eqsim 3.8 임을 알 수 있습니다.\n\n이미지를 데이터 포인트 x라고 하고 우리가 가지고 있는 사람 얼굴 이미지 데이터 셋의 분포가 왼쪽의 분포와 같다고 한다면, 여러개의 모드(mode)가 있는데 가장 높은 확률의 mode에서는 금발 여성의 얼굴이 있고 상대적으로 낮은 확률로 흑발의 안경 쓴 남자의 얼굴 이미지가 있음을 알 수 있습니다. 또한 mode가 아닌 매우 낮은 확률을 보이는 분포의 꼬리 부분을 보면 매우 이상한 얼굴 이미지들이 나오는 것을 알 수 있습니다.\n\n바로 우리가 가지고 있는 이미지 데이터 셋 분포(빨강색)과 유사한 데이터 분포(파란색)를 학습하는 것이 생성 모델의 목표이고 이를 Discriminator와 Generator를 가지고 학습하도록 하는 것이 GAN입니다.\n\nDiscriminator의 Objective Function(V)을 보면, 먼저 첫번째 term은 데이터 x는 true dataset distribution인 p_{data}에서 샘플링 되었을 때 Discriminator는 이를 진짜라고 판별해야 하고 이는 output 1(true label)을 출력해야하는 방향으로 학습되어야 합니다. 두번째 term은 fake dataset distribution인, 즉 generator가 만든 데이터일 경우에 가짜라고 판별해야 하고 output 0(fake label)을 출력해야 합니다. 따라서 2개의 term을 모두 maxmization하는 것이 Discriminator의 목표이기 때문에 \\text{max}_DV(\\cdot)이 됩니다.\n\nGenerator의 Objective Function을 보면, 첫번째 true dataset distribution에서 샘플링 되는 부분은 Generator와 상관이 없습니다. 두번째 term에서 Generator에서 나온 ouput new data를 Discriminator에게 넘겨주었을 때 1(true label)로 착각하도록 만들어야 하므로 \\text{min}_GV(\\cdot)이 됩니다."
  },
  {
    "objectID": "posts/paper/2023-03-12-wasabi.html#wgan",
    "href": "posts/paper/2023-03-12-wasabi.html#wgan",
    "title": "📃WASABI 리뷰",
    "section": "WGAN",
    "text": "WGAN\n위에서 설명한 기본적인 GAN을 잘 학습했을 때 확률분포를 그려보면 다음과 같이 Discriminator의 판별 분포가 빨간색 그래프처럼 그려지는 것을 알 수 있습니다. 완벽하게 true distribution인 p_{data}에 대해서는 1을, generated distribution p_G에 대해서는 0을 보여주고 있지만 이런 상황에서는 유의미한 학습이 일어나기 힘듭니다.\n\nOptimal한 Discriminator를 가정하고 Objective function을 다시보면 p_{data}와 p_G가 너무 멀리 떨어져 있어서 사실상 계산된 V(\\cdot)값이 0이기 때문입니다. 따라서 Generator가 두 분포가 가깝도록 만드는 방향으로 학습을 해야 하는데 Classic GAN의 Objective Function에는 이러한 정보를 알려줄 수 있는 부분이 수학적으로 모델링이 되어 있지 않습니다.\n\n따라서 분포들간의 먼 정도를 모델링할 수 있는 WGAN(Wasserstein GAN)이 제안되었고 이에 대해서는 수학적으로 매우 딥한 내용이 있지만 본 포스팅에서는 간단하게 개념적으로 공사장의 포크레인을 이용하여 이해하고 넘어가겠습니다. Wassertein Distance는 Earth mover’s distance라고도 불리는데 이름에서 직관적으로 이해할 수 있듯이, 두 분포를 어떤 흙더미라고 생각하고 우리가 Generated Distribution에 있는 흙들을 Real Distribution의 모양대로 흙들을 옮긴다고 했을 때 드는 cost가 distance로 정의된다고 볼 수 있습니다. (수학적으로 더 궁금하신 분들은 Implicit DGM 29 | Wasserstein Distance with GAN을 추천합니다.) 본 연구에서는 이 WGAN을 이용하여 reward 디자인을 했습니다."
  },
  {
    "objectID": "posts/paper/2023-03-12-wasabi.html#rl-with-gan",
    "href": "posts/paper/2023-03-12-wasabi.html#rl-with-gan",
    "title": "📃WASABI 리뷰",
    "section": "RL with GAN",
    "text": "RL with GAN\nGAN 내용을 설명할 때 이미지 생성 분야의 예시가 직관적이고 쉽기 때문에 이를 가지고 설명하다 보니 문득 그래서 강화학습에서 어떻게 GAN을 사용하는데? 라는 의문이 생길 수 있습니다. 다시 강화학습에서의 여러 어려움들 중 Task reward를 잘 정의해주기가 어렵다는 점을 상기시켜보면 Task reward를 Discriminator가 결정해줄 수 있지 않을까라는 아이디어를 떠올려볼 수 있습니다. 모션의 reference가 될 수 있는 demonstration의 일련의 state들이 true distribution이 되고, policy에서 나오는 일련의 state들이 generated distribution이 되어서, Discriminator가 두 분포를 못 구분할 정도를 task reward로 정의한다면 policy가 demonstration에서 나타난 다이나믹한 모션들을 따라하도록 학습할 수 있는 지표가 될 수 있을 것 입니다. 이전에 locomotion이나 backflip 등의 각각의 모션마다 task reward를 hand design 할 때는 각 모션에서 보행 로봇의 발이 어떻게 움직여야 하는지, 몸체의 속도가 어떠해야 하는지 일일이 reward로 고려하고 여러 reward term들을 weighted sum하는 방식이었지만 이 GAN 방식을 이용하면 각 모션에 대한 demonstration의 state들을 보고 어떤 모션을 어떻게 따라해야하는지 agent의 policy가 알아서 task reward를 높이는 방향으로 학습할 수 있는 것 입니다."
  },
  {
    "objectID": "posts/paper/2023-03-12-wasabi.html#problem-definition",
    "href": "posts/paper/2023-03-12-wasabi.html#problem-definition",
    "title": "📃WASABI 리뷰",
    "section": "Problem Definition",
    "text": "Problem Definition\n이전에 AMP 방식에서 모션의 자연스러움을 학습하기 위해 Motion data가 매우 well-defined 되어 있어야 한다고 했습니다. 하지만 이러한 Motion data(혹은 demonstration)을 얻기는 어렵고 특히나 보행과 같이 이미 많이 연구가 되어왔고 동물들의 모습에서도 많이 관찰될 수 있는 task와는 다르게 다이나믹한 backflip하는 모션 task들은 참고할 데이터들도 매우 적고 만들어내기도 어렵습니다. 이런 문제 상황을 본 연구에서는 Rough하고 Partial한 demonstration만 있는 문제로 파악하고 Rough한 모션 데이터라는 것은 실제 로봇이나 동물이 움직여서 얻은 데이터가 아닌 사람이 로봇을 단순히 들고 움직여서 얻은 데이터를 말하며 Partial하다는 것은 로봇의 모션 데이터라고 해서 로봇을 구성하고 있는 모든 joint들의 움직임에 대한 데이터가 아닌 로봇의 몸체에 대한 정보만 있는 모션데이터만 있는 것을 말합니다.\n\n말로만 들으면 잘 와닿지 않기 때문에 위에 사진에서 한 연구자가 backflip하는 demonstration 데이터를 얻기 위해 로봇을 들고 손으로 그냥 한번 뒤집어주는 모습을 보면서 다시한번 설명을 해보겠습니다. 앞서 설명했듯이 로봇이 backflip하는 작동을 해서 데이터를 얻지 않고 사람이 단순히 로봇을 들고 원하는 모션의 demonstration 데이터를 얻습니다. 여기서 Backflip demonstration 데이터는 로봇의 12개의 joint들에 대한 정보는 없이 몸체에 대한 정보(base linear, angular velocity, projected gravity, base height)만을 포함하게 됩니다. 여기서 demonstration 데이터에 대한 놀라운 점은 로봇이 직접 움직여서 얻은 데이터도 아니고 실제 동물의 모션 데이터도 아니기 때문에 물리적으로도 시간적으로도 로봇 플랫폼에서는 사실상 따라하기 어려운 데이터라는 것입니다. 이런 demo 데이터만 있다고 문제상황을 가정한 이유는 backflip과 같이 다이나믹하고 다양한 모션에 대해서는 reference가 될 만한 motion data를 well-defined하기 어렵기 때문입니다.\n이쯤에서 다시한번 AMP와 WASABI를 다시 비교해보면, 두가지 방법 모두 expert의 action이 없이도 reference가 될 수 있는 motion data(혹은 demonstration)를 가지고 reward engineering을 잘해서 모션 제어를 할 수 있었다는 점에서 공통점이 있습니다. 하지만 AMP는 well-defined한 모션 데이터가 있어야 가능한 방법론인 반면 WASABI는 로봇의 몸체에 대한 partial한 모션 데이터만 있으면 학습할 수 있었고 AMP는 모션의 주요 reward를 디자인한 것이 아니라 자연스러움을 위한 보조적인 style reward 디자인을 했고 WASABI는 각 모션에 대한 task reward를 디자인 한 것이 큰 차이점이라고 볼 수 있습니다."
  },
  {
    "objectID": "posts/paper/2023-03-12-wasabi.html#reward-design",
    "href": "posts/paper/2023-03-12-wasabi.html#reward-design",
    "title": "📃WASABI 리뷰",
    "section": "Reward Design",
    "text": "Reward Design\nPartial하고 Rough한 모션 demo들을 가지고 어떻게 하면 다이나믹한 모션에 대한 reward를 정의할 수 있을까요?\n\nWASABI에서 제안한 전체적인 알고리즘 구조는 아래와 같습니다. r^I, r^R, r^T 라는 각각의 reward가 합쳐지는 것을 볼 수 있는데요 이제부터 각각의 reward가 어떤 의미와 목적을 가지고 있는 것인지 살펴보겠습니다.\n\n\nImitation(Task) Reward\n우선, task reward는 다이나믹 모션의 demo를 잘 모방(imitate)할 수 있도록 해야할 것 입니다. 그래서 imitation reward 혹은 task reward로 불리며 여기서 WGAN 방법을 이용해서 정의하게 되는 부분입니다. 다시한번 이야기하지만 우리가 backflip을 하는 학습을 하기 위해서 로봇의 몸체를 공중에 올리고 pitch 방향으로의 회전을 360도 해야해!라고 말해주는 imitation reward function(hand-designed)을 사용하는 것이 아니라 demo(true) distribution을 보고 이를 따라가는 generated distribution을 policy가 학습할 수 있도록 하는 것이 이 방법의 핵심입니다.\n\n잠깐 앞에서 이야기 했듯이 우리가 사용하는 demo 데이터는 well-defined한 데이터가 아닌 사람이 로봇을 들고 모은 데이터이기 때문에 로봇의 base에 대한 데이터(O)로 한정적입니다. 하지만 policy에서 generated된 observation 데이터(S)는 로봇의 각 joint에 대한 정보 등 더 많은 정보가 있는 vector space이기 때문에 true distribution과 generated distribution을 비교가능한 상태로 만들어주기 위해 Mapping function \\phi를 사용하여 맞춰줍니다. 쉽게 생각하자면 정보량이 더 많은 S를 차원이 적은 O로 맞춰주기 위해 joint position, velocity, last action과 같은 부분을 가리고 data distribution을 Discriminator에게 넘겨주는 것으로 볼 수 있습니다.\n\nmapping function을 통해 차원을 맞춘 \\phi(s) 와 o는 GAN의 objective function에서 Discriminator의 인풋으로 들어가는 seq. of states(observations)이며 아래와 같이 일정 time horizon H동안 모아진 states 벡터들로 볼 수 있습니다. 이러한 seq. of states들을 가지고 Discriminator가 만든 reward distribution을 각각 LSGAN(Least Squares GAN)과 WGAN의 objective function으로 아래와 같이 나타내 볼 수 있습니다. 여기서 LSGAN은 WGAN의 비교군이 되는 또 다른 GAN의 알고리즘이며 LSGAN의 Objective function을 해석해보면, policy에서 나온 state history를 가지고 나온 reward distribution은 -1에 가깝도록 demo를 통해 나온 reward distribution은 +1에 가깝도록 하는 것으로 볼 수 있습니다. 반면, WGAN은 이 두 분포간의 wasserstein distance 줄이도록하는 방향으로 학습합니다. 두 가지 GAN 모두 policy에서 나온 seq. of states로 나온 task reward distribution을 demo의 seq. of states로 나온 task reward distribution을 맞춰가도록 학습하는 것은 공통적입니다.\n\n\n이렇게 Discriminator를 통해 나온 task reward는 바로 사용되는 것이 아니고 zero-mean unit-variance로 만들어주는 과정을 한번 거친 후 비로소 Task(Imitation) Reward로 만들어집니다.\n\n\n\nRegularization Reward\n이전에 AMP에서의 Style reward의 역할을 WASABI에서는 Regularization Reward가 대신한다고 볼 수 있습니다. 이 reward는 task-dependent하지 않은 task-agnostic한 term들로 이루어져 있어서 backflip 모션을 하든 locomotion 모션을 하든 로봇의 자연스럽고 에너지 효율적인 모션을 위해 부가적으로 더해지는 reward라고 볼 수 있습니다.\n\n\n\nTermination Reward\n마지막으로 agent가 모션을 충분히 학습하기도 전에 episode를 더 빨리 끝내는 것이 이득이라 판단하고 학습이 잘 이루어지지 않는 경우를 방지하고자 Termination Reward를 추가해주었습니다. T는 episode를 너무 빨리 끝내버린 경우에 대해서 0 또는 1로 판단하는 인디케이터 역할을 하게 되고, termination에 대한 고려는 Imitation reward의 분포에서 나온 \\sigma와 할인율 \\gamma를 고려하여 다음과 같이 정해주게 됩니다.\n\n\n\nTotal Reward\n앞서 설명한 Imitation reward r^I, Regularization reward r^R, Termination reward r^T를 모두 합산하여 Total reward가 계산되게 되고 이를 Agent에게 학습 피드백으로 보내주게 됩니다. 이때 r^I와 r^T는 모션 task 마다 다르게 정의될 수 있는 부분이므로 task-related한 부분이라고 볼 수 있으며 r^R는 어떤 모션 task인지 상관없이 항상 동일한 reward term이기 때문에 task-agnostic한 부분이라고 할 수 있습니다. 물론 여기서 해당 연구의 contribution이 두드러진 부분은 Imitation reward r^I이라고 할 수 있습니다."
  },
  {
    "objectID": "posts/paper/2023-03-12-wasabi.html#induced-imitation-reward-distributions",
    "href": "posts/paper/2023-03-12-wasabi.html#induced-imitation-reward-distributions",
    "title": "📃WASABI 리뷰",
    "section": "Induced Imitation Reward Distributions",
    "text": "Induced Imitation Reward Distributions\n우선 Imitation Reward Distribution이 정말 의미있게 학습을 했는가(Informative한 reward distribution을 만들어 냈는가)를 보기 위해 reward distribution을 시각화해보았습니다. 먼저 Informative한 분포라는 것은 어떤 분포를 말하는가를 짚어볼 필요가 있습니다. 아래 사진의 오른쪽 2개의 분포 그래프에서 평평한 분포(파란색)보다는 뾰족한 분포(초록색)가 여러 x값들에 대해 분별적인 y값(확률)을 가지고 있기 때문에 더 informative하다고 할 수 있습니다.(더 자세한 내용은 정보이론을 살펴보셔도 좋을 것 같습니다.)\n왼쪽의 2개의 그래프는 각각 LSGAN과 WGAN(WASABI)를 가지고 학습했을 때, O의 요소들 중 고정된 pitch rate(\\dot\\theta)와 height(z)를 가지고 Imitation reward 분포를 시각화한 그래프입니다. LSGAN보다 WGAN으로 학습한 분포가 reward range도 더 넓고 더 구분되는 분포를 가지고 있는 것을 볼 수 있습니다. 마지막으로 세번째 그래프는 학습 과정 중에 r^I의 분포를 그린 것으로 LSGAN은 -1과 1, 각각으로 reward targeting을 하게 되는 objective function을 가지고 있었기 때문에 넓고 다양한 reward distribution을 가지지 못한 모습을 볼 수 있고 그에 반해 WGAN은 약 -5~2 정도의 range를 가지는 넓은 reward distribution을 가지고 있는 것을 확인할 수 있었다고 합니다."
  },
  {
    "objectID": "posts/paper/2023-03-12-wasabi.html#learning-to-mimic-rough-demonstrations",
    "href": "posts/paper/2023-03-12-wasabi.html#learning-to-mimic-rough-demonstrations",
    "title": "📃WASABI 리뷰",
    "section": "Learning to Mimic Rough Demonstrations",
    "text": "Learning to Mimic Rough Demonstrations\n그럼 정말로 Demo 모션 데이터들을 얼만큼 잘 따라 학습할 수 있었을까요? 이에 대한 지표는 단순히 reward가 높다고 판단할 수 있는 것이 아니라 모션의 유사성을 판단할 수 있는 다른 metric이 필요합니다.\n\nDynamic Time Warping\nDynamic Time Warping이란 각 데이터의 시간의 길이도 다르고 데이터 포인트의 수도 다른 2개의 시계열 데이터를 비교할 때 사용하는 방법으로 기존의 Euclidean distance라면 측정할 수 없거나 정확한 비교가 어려운 점을 DTW를 이용하면 시간적인 밀림이나 소실된 데이터 포인트까지 고려하여 시계열 데이터 간의 유사도를 판단할 수 있습니다. 바로 이 방법을 이용해서 사람이 들고 만들었던 demo의 모션 데이터와 실제 학습 후 policy에서 만들어낸 모션 데이터 간의 차이를 측정해보았습니다.\n\n\\tau_\\pi는 policy에서 만들어진 trajectory를, \\tau_M은 demo에서 따온 trajectory를 말하며 아래의 실험 결과표는 각각 WASABI와 LSGAN에서의 4 task에 대한 DTW를 구한 값을 나타내고 있습니다. DTW가 낮을수록 demo 데이터와의 유사성이 높은 것이며 잘 모션을 따라 학습했다고 볼 수 있습니다.(아래 Stand Still은 단순히 가만히 서 있는 모션의 데이터와 demo 데이터 간의 DTW 값을 나타낸 것이며 비교를 위한 DTW의 최대 상한선을 나타낸 것으로 볼 수 있습니다.)\n\n\n\nHandcrafted Task Reward\n또 다른 지표로는, 해당 모션 task에 대한 Handcrafted task reward로 점수를 매겼을 때 그 점수가 더 높다면 해당 모션을 잘 학습했다고 판단하는 지표가 있습니다. 예를 들어 STANDUP은 몸체의 pitch angle이 90도에 가깝고 몸체의 높이가 높고 몸체의 z축이 중력방향에 수직이 되는 상태라면 해당 모션을 잘 수행하고 있다고 볼 수 있을 것 입니다. 이처럼 우리가 원하는 모션에 대한 Handcrafted task reward를 계산해서 학습 iteration 마다 그려보면 오른쪽 그림과 같이 WASABI를 가지고 학습한 reward 점수가 대체적으로 LSGAN에 비해 높은 것을 알 수 있습니다. 아래 표에서는 학습을 끝낸 후 각 task에 대한 handcrafted reward 점수이며 맨 아래 점수는 최고 상한 기준 점수로 볼 수 있습니다. 표에서 볼드체로 표시된 부분은 roll-out을 했을 때 모션을 눈으로 확인한 결과 잘 수행했다고 판단한 경우를 나타내면 WASABI로 학습한 4가지 task 모두에서 성공적인 학습 결과를 볼 수 있었다는 것을 볼 수 있습니다."
  },
  {
    "objectID": "posts/paper/2023-03-12-wasabi.html#evaluation-on-real-robot",
    "href": "posts/paper/2023-03-12-wasabi.html#evaluation-on-real-robot",
    "title": "📃WASABI 리뷰",
    "section": "Evaluation on Real Robot",
    "text": "Evaluation on Real Robot\n학습이 시뮬레이션에서만 멈춘다면 당연히 의미가 없는 것이므로 실제 로봇을 가지고 해당 policy의 학습 결과를 확인해봐야 합니다. 따라서 WASABI로 학습한 policy를 가지고 실제 로봇으로 작동을 해보고 이때 10개의 marker를 이용해서 모션 데이터를 얻어 DTW를 측정해보았습니다. 그 결과 표에서 볼 수 있듯이 Sim-to-Real의 퍼포먼스 차이가 거의 없었고 실제 로봇에서도 4가지 task 모두 다 잘 수행하는 것을 확인할 수 있었습니다. 이 부분은 실험영상에서 직접 확인할 수 있습니다.\n\nLeap\n\nWave\n\nStand up\n\nBackflip"
  },
  {
    "objectID": "posts/paper/2023-03-12-wasabi.html#cross-platform-imitation",
    "href": "posts/paper/2023-03-12-wasabi.html#cross-platform-imitation",
    "title": "📃WASABI 리뷰",
    "section": "Cross-platform Imitation",
    "text": "Cross-platform Imitation\n사실 강화학습은 특정 로봇 플랫폼에서 학습한 결과를 다른 configuration을 가진 로봇 플랫폼에 바로 적용하기 어렵습니다. 하지만 WASABI 알고리즘은 처음에 Rough하고 Partial한 demo 데이터를 가지고 학습했기 때문에 다른 로봇 플랫폼에 적용해보는 것이 가능했으며 기존에 Solo 8 로봇 플랫폼을 가지고 학습한 policy를 단순히 로봇 플랫폼의 크기 차이만을 고려하여 base height를 0.25m 조금 더 큰값으로 수정해서 Anymal-C 로봇 플랫폼에 적용했을 때 특별한 추가적인 학습 과정없이도 적용할 수 있었다고 합니다. 이때에도 DTW 값을 찍어서 확인한 결과, 낮은 DTW 값과 함께 시뮬레이션으로 roll-out을 했을 때에 로봇이 잘 작동되는 것을 확인할 수 있었다고 합니다."
  },
  {
    "objectID": "posts/paper/2022-06-10-NerveNet.html",
    "href": "posts/paper/2022-06-10-NerveNet.html",
    "title": "📃NerveNet 리뷰",
    "section": "",
    "text": "We address the problem of learning structured policies for continuous control. In traditional reinforcement learning, policies of agents are learned by multi-layer perceptrons (MLPs) which take the concatenation of all observations from the environment as input for predicting actions. In this work, we propose NerveNet to explicitly model the structure of an agent, which naturally takes the form of a graph. Specifically, serving as the agent’s policy network, NerveNet first propagates information over the structure of the agent and then predict actions for different parts of the agent. In the experiments, we first show that our NerveNet is comparable to state-of-the-art methods on standard MuJoCo environments. We further propose our customized reinforcement learning environments for benchmarking two types of structure transfer learning tasks, i.e., size and disability transfer, as well as multi-task learning. We demonstrate that policies learned by NerveNet are significantly more transferable and generalizable than policies learned by other models and are able to transfer even in a zero-shot setting.\n\n보통 강화학습에서 agent들의 policy는 multi-layer perceptrons (MLPs)으로 네트워크를 만들기 때문에 agent가 environment에서 받은 observation들을 단순히 쌓아서(concatenation) policy network에 입력으로 들어가게 된다. 하지만 손의 속도 정보와 발의 속도 정보가 같은 속도 범주이지만 위치가 다르기 때문에 구분이 있을 수 있듯이 agent의 이런 구조적인 특성을 반영해서 policy를 만든다면 observation 정보들간의 구분을 할 수 있을 것이다. 이런 agent의 구조적 관계성을 나타내기 위해서 MLP대신 그래프를 활용하게 되었고 NerveNet을 고안하게 되었다. NerveNet은 그래프 구조로 되어 있는 policy network에서 각 노드들의 정보들이 전파(propagation)되며 agent의 부분들을 나타내는 노드마다 action을 prediction 하게 된다. MuJoCo 환경에서 MLP 기반의 벤치마크들과 비등한 학습결과를 보여주었으며, transfer learning task로 agent의 크기(size)와 agent의 일부 파트가 작동하지 않는(disability) variation을 주었을 때도 잘 학습되었으며 multi-task learning으로 walker 그룹의 다양한 환경에서의 학습 결과들도 좋았다. 이런 결과들을 통해 NerveNet이 transferable할 뿐만 아니라 zero-shot setting도 가능함을 보여주었다.\n\ntransferable - A task를 학습한 네트워크(weights)를 활용하여 B task 학습에도 적용하여 scratch에서 B task를 학습하는 것보다 더 빠르고 효율적인 학습을 가능하게 할 수 있다는 의미. A task 학습에서 습득한 논리체계를 B task에도 적용할 수 있음으로 볼 수 있다.\nzero-shot - Meta learning에서 사용되는 용어로 A task에 대해서 학습된 네트워크가 fine tuning이 없이 바로 unseen new task B에 대해서 좋은 성능을 내는 것을 의미."
  },
  {
    "objectID": "posts/paper/2022-06-10-NerveNet.html#graph-construction",
    "href": "posts/paper/2022-06-10-NerveNet.html#graph-construction",
    "title": "📃NerveNet 리뷰",
    "section": "Graph Construction",
    "text": "Graph Construction\n본 논문에서는 사용한 MuJoCo의 agent들은 이미 구조적으로 tree 구조를 가지고 있다. NerveNet의 핵심 아이디어인 그래프를 구성하기 위해 body와 joint, root라는 3가지 종류의 노드를 설정했다. body 노드는 로봇공학에서 말하는 link 기준의 좌표시스템을 나타내는 노드이고, joint 노드는 모션의 자유도(freedom of motion)을 나타내며 2개의 body 노드들을 연결해주는 노드이다.\n아래는 Ant 환경의 예시인데, 한 가지 그림에서 헷갈리지 말아야 할 점은 그림에서는 마치 body와 root 노드만 노드로 만든것 처럼 보이지만 root와 body, body와 body를 연결하는 엣지들도 실제로는 joint 노드들이다.(we omit the joint nodes and use edges to represent the physical connections of joint nodes.)root라는 노드는 agent의 추가적인 정보들을 담을 부분으로 사용하기 위해 추가한 노드 종류로, 예를 들어 agent가 도달해야 하는 target position에 대한 정보 등이 담겨있다."
  },
  {
    "objectID": "posts/paper/2022-06-10-NerveNet.html#nervenet-as-policy",
    "href": "posts/paper/2022-06-10-NerveNet.html#nervenet-as-policy",
    "title": "📃NerveNet 리뷰",
    "section": "NerveNet as Policy",
    "text": "NerveNet as Policy\n크게 3가지 파트로 NerveNet을 살펴볼 것인데 우선 (0) Notation을 보고 난뒤, (1) Input model (2) Propagation model (3) Output model 순으로 살펴볼 예정이다.\n\n0. Notation\n그래프에서의 노테이션은 다음과 같이 G 라는 그래프는 노드 집합 V와 엣지 집합 E로 구성된다.\n\nG=(V, E)\n\nNervenet policy를 구성하는 그래프는 Directed graph(유향 그래프)이기 때문에 각 노드에서의 in과 out이 따로 명시되게 된다.\n\n노드 u를 중심으로 노드 u로 들어오는 이웃 노드이면 \\mathcal{N}_{in}(u)\n노드 u를 중심으로 노드 u에서 나가는 이웃 노드이면 \\mathcal{N}_{out}(u)\n\n그래프의 모든 노드 u는 타입을 가지게 되고 이를 p_{u} \\in\\{1,2, \\ldots, P\\} (associated note type)로 나타내며 여기에서는 위에 설명한 것과 같이 body, joint, root 3가지 타입이 있다.\n노드들 뿐만 아니라 엣지들도 타입을 정할 수 있는데 c_{(u, v)} \\in\\{1,2, \\ldots, C\\} (associate each edge)로 표기하여 노드쌍 (u, v) 사이의 엣지 타입을 정의할 수 있다.(하나의 엣지에 대해서 여러 엣지 타입을 정의할 수 있지만 여기에서는 심플 이즈 더 베스트 철학으로 하나의 엣지는 하나의 타입만 가지도록 했다)\n이렇게 노드별, 엣지별 타입을 나눔으로써,\n\n노드 타입은 노드들간의 다른 중요도를 파악하는데 도움이 되고\n엣지 타입은 노드들간의 서로다른 관계들을 나타내고 이 관계의 종류에 따라 정보를 다르게 propagation 하게 된다.\n\n이제 시간 노테이션에 대한 부분을 살펴보자. NerveNet에는 시간(time step)의 개념이 2가지 존재한다.\n\n기존 강화학습에서 환경과 agent 사이의 interaction time step을 나타내는 \\tau\nNerveNet의 내부 graph policy에서의 propagation step을 나타내는 t\n\n다시 풀어서 생각해보면, 강화학습의 시간 개념 \\tau 스텝에서 환경으로부터 observation을 받고, 받은 observation을 기반으로 t 스텝동안 NerveNet의 내부의 그래프의 propagation이 일어난다.\n\n\n1. Input model\n위에서 말했듯이 환경과 상호작용으로 observation s^{\\tau} \\in \\mathcal{S}을 받게 된다(time step \\tau). 이 s^{\\tau}는 concatenation된 각 노드의 observation이라고 볼 수 있다. 이제 강화학습 interaction 수준의 \\tau 스텝은 잠시 멈춰두고 그래프 내부의 타임 스텝인 t 수준에서 생각해보자. observation은 node u에 해당하는 x_{u}로 표현할 수 있고 x_{u}는 input network F_{\\mathrm{in}}(MLP)를 거쳐서 고정된 크기의 state vector인 h_{u}^{0}가 된다. h_{u}^{0}의 노테이션을 풀어서 해석하면 노드 u의 propagation step 0 에서의 state vector인 것이다. 이때 observation vector x_{u}가 노드마다 크기가 다를 경우 zero padding으로 맞춰서 input network에 넣어주게 된다.\n\nh_{u}^{0}=F_{\\text {in }}\\left(x_{u}\\right)\n\n\n\n2. Propagation model\nNerveNet의 propagation 과정 노드들 간에 주고 받는 정보를 message라고 하게 되고 이는 노드들 간에 주고 받는 상호작용이라고 생각할 수 있다. Propagation model은 3가지 단계로 나누어서 볼 수 있다.\n\nMessage Computation\n\n전달할 메세지를 계산한다.\npropagation step인 t에, 모든 노드들 u에서 state vector h_{u}^{t}를 정의할 수 있다.\n노드 u로 모아지는(in-coming) 모든 엣지들을 가지고 메시지를 구하게 되는데, 이때 M은 MLP이고 M의 아래첨자 c_{(u, v)} 노테이션에서 알 수 있듯이 같은 종류의 엣지에 대해서는 같은 message function M을 쓴다.\n\n  m_{(u, v)}^{t}=M_{c_{(u, v)}}\\left(h_{u}^{t}\\right)\n  \n예를 들어 아래 그림은 CentipedeEight 의 모습인데, 왼쪽은 실제 agent의 모습을 나타내고 있으며 오른쪽은 agent를 그래프로 나타냈을 때의 모습이다. 여기에서 2번째 torso에서 첫번째 세번째 torso에서 보낼 때 같은 메세지 펑션 M_{1} 을 사용하고, LeftHip과 RightHip으로 보내는 메세지 펑션 M_{2}를 사용하게 되는 것이다.\n\n\n\n\n\n\nMessage Aggregation\n\n앞 단계에서 모든 노드들에 대해서 메세지 계산이 끝난 후에 in-coming 이웃 노드들로부터 온(계산된) 메세지를 모으게 된다. 이때 summation, average, max-pooling 등 다양한 aggregation 함수를 사용할 수 있다.\n\n  \\bar{m}_{u}^{t}=A\\left(\\left\\{h_{v}^{t} \\mid v \\in \\mathcal{N}_{i n}(u)\\right\\}\\right)\n  \n\nStates Update\n\n이제 모은 메세지를 기반으로 state vector를 업데이트 하면 된다!\n\n  h_{u}^{t+1}=U_{p_{u}}\\left(h_{u}^{t}, \\bar{m}_{u}^{t}\\right)\n  \n여기서 업데이트 함수 U 는 a gated recurrent unit (GRU), a long short term memory (LSTM) unit 또는 MLP가 될 수 있다.\nUpdate function의 아래첨자 p_{u}에서 볼 수 있다시피 같은 노드 타입이면 같은 update function U를 쓰게 된다. 이렇게 업데이트된 state vector는 타임 스텝 t가 하나 올라간 t+1 이 된 h_{u}^{t+1}가 된다.\n\n\n이렇게 내부 propagation 과정 3단계(Message Computation, Message Aggregation, States Update)가 T 스텝동안 일어나게 되고 각 노드의 최종 state vector는 h_{u}^{T} 가 된다.\n\n\n3. Output model\n전형적인 RL의 MLP 폴리시에서는 네트워크에서 각 action의 gaussian distribution의 mean을 뽑아내게 된다. std는 trainable한 벡터이다. NerveNet에서도 std는 비슷하게 다루지만 각 노드에 마다 action prediction을 만들게 된다.\nactuator와 연결되어 있는 노드들의 집합을 O라고 하자. 이 집합에 있는 노드들의 최종 state vector h_{u \\in \\mathcal{O}}^{T}는 MLP인 Ouput model O_{q_{u}}에 인풋으로 들어가게 되고 아웃풋으로 각 actuator의 action distribution인 gaussian distribution의 mean \\mu을 출력하게 된다. 여기에서 새로운 노테이션 q_{u}를 볼 수 있는데 q_{u}는 아웃풋 타입, 즉 아웃풋을 내놓는 노드 u의 타입으로 아웃풋 펑션의 아래첨자에 q_{u}에 따라 아웃풋 노드의 타입이 같으면 Output function을 공유할 수 있다. 다시말해 아웃풋 노드 타입에 따라 컨트롤러를 공유할 수도 있는 것이다. 위의 Centipedes의 예시로 보면, 같은 LeftHip 끼리는 컨트롤러를 공유할 수 있다는 것이다.\n\n\\mu_{u \\in \\mathcal{O}}=O_{q_{u}}\\left(h_{u}^{T}\\right)\n\n논문에서 실제로 실험을 해봤을 때 다른 타입의 컨트롤러들을 하나로 통합했더라도(O function을 다 같은 MLP로 사용) 퍼포먼스가 그렇게 해쳐지지 않음을 확인할 수 있었다고 한다.\n여기까지해서 그래프 노테이션을 빌려 그래프 기반 가우시안 stochastic policy를 나타내면 아래의 수식과 같다.\n\n\\pi_{\\theta}\\left(a^{\\tau} \\mid s^{\\tau}\\right)=\\prod_{u \\in \\mathcal{O}} \\pi_{\\theta, u}\\left(a_{u}^{\\tau} \\mid s^{\\tau}\\right)=\\prod_{u \\in \\mathcal{O}} \\frac{1}{\\sqrt{2 \\pi \\sigma_{u}^{2}}} e^{\\left(a_{u}^{\\tau}-\\mu_{u}\\right)^{2} /\\left(2 \\sigma_{u}^{2}\\right)}\n\n\n여기까지 NerveNet의 각 단계를 Walker-Ostrich 환경에서 예시로 한눈에 보기 쉽게 정리한 그림은 아래와 같다."
  },
  {
    "objectID": "posts/paper/2022-06-10-NerveNet.html#learning-algorithm",
    "href": "posts/paper/2022-06-10-NerveNet.html#learning-algorithm",
    "title": "📃NerveNet 리뷰",
    "section": "Learning Algorithm",
    "text": "Learning Algorithm\n이전 파트에서 NerveNet의 내부에서 propagation 스텝 t 단위에서 각 단계들을 자세히 살펴보았다면 이제 강화학습 타임 스텝 \\tau 단위에서 학습의 목적함수와 알고리즘을 살펴보자. 목적함수는 전형적인 RL과 다른 점이 없이 policy의 파라미터 \\theta를 가지고 Return 값을 maximization하는 것으로 한다.\n\nJ(\\theta)=\\mathbb{E}{\\pi}\\left[\\sum{\\tau=0}^{\\infty} \\gamma^{\\tau} r\\left(s^{\\tau}, a^{\\tau}\\right)\\right]\n\n강화학습 알고리즘으로는 PPO과 GAE를 사용했으며 해당 알고리즘들의 내용은 각각 알고리즘들의 원래 수식과 내용들과 상이한 점이 없으므로 각 논문으 참고하면 되기 때문에 이번 논문 리뷰에서는 생략한다.\nPPO와 GAE 알고리즘을 참고하여 위의 목적함수 J를 정리하면 NerveNet의 목적함수는 다음과 같다.\n\n\\begin{aligned}\n\\tilde{J}(\\theta)=& J(\\theta)-\\beta L_{K L}(\\theta)-\\alpha L_{V}(\\theta) \\\\\n=& \\mathbb{E}_{\\pi_{\\theta}}\\left[\\sum_{\\tau=0}^{\\infty} \\min \\left(\\hat{A}^{\\tau} r^{\\tau}(\\theta), \\hat{A}^{\\tau} \\operatorname{clip}\\left(r^{\\tau}(\\theta), 1-\\epsilon, 1+\\epsilon\\right)\\right)\\right] \\\\\n&-\\beta \\mathbb{E}_{\\pi_{\\theta}}\\left[\\sum_{\\tau=0}^{\\infty} \\operatorname{KL}\\left[\\pi_{\\theta}\\left(: \\mid s^{\\tau}\\right) \\mid \\pi_{\\theta_{o l d}}\\left(: \\mid s^{\\tau}\\right)\\right]\\right]-\\alpha \\mathbb{E}_{\\pi_{\\theta}}\\left[\\sum_{\\tau=0}^{\\infty}\\left(V_{\\theta}\\left(s^{\\tau}\\right)-V\\left(s^{\\tau}\\right)^{\\operatorname{target}}\\right)^{2}\\right]\n\\end{aligned}\n\n위의 수식에서 볼 수 있는 value network V를 어떻게 디자인할 것인지가 이번 논문의 다른 포인트로 볼 수 있다. 논문의 기본 아이디어는 policy network를 그래프로 표현하는 것이고, value network는 어떻게 할지 여러 선택지들이 남아있다. 그래서 본 논문에서는 value network의 디자인을 두고 크게 3가지 NerveNet의 변형 알고리즘들을 실험해보았다.\n\nNerveNet-MLP : policy network를 1개의 GNN으로 구성하고 value network는 MLP로 구성\nNerveNet-2 : policy network를 1개의 GNN으로 구성하고 value network는 또 다른 GNN으로 구성(총 GNN 2개 - without sharing the parameters of the two GNNs)\nNerveNet-1 : policy network와 value network 모두 1개의 GNN으로 구성(총 GNN 1개)"
  },
  {
    "objectID": "posts/paper/2022-06-10-NerveNet.html#comparison-on-standard-benchmarks-of-mujoco",
    "href": "posts/paper/2022-06-10-NerveNet.html#comparison-on-standard-benchmarks-of-mujoco",
    "title": "📃NerveNet 리뷰",
    "section": "1. Comparison on standard benchmarks of MuJoCo",
    "text": "1. Comparison on standard benchmarks of MuJoCo\n\n비교군으로 MLP, TreeNet(모든 노드들이 연결 되어 있는 그래프, depth 1)을 사용\n총 8개의 환경에서 실험 - Reacher, InvertedPendulum, InvertedDoublePendulum, Swimmer, HalfCheetah, Hopper, Walker2d, Ant\n충분히 학습하는 스텝을 주기 위해서 1 million을 max로 둠\n하이퍼 파라미터의 경우 그리드 서치로 찾았으며(Appendix 참고) 각 알고리즘의 퍼포먼스를 측정할 때 3번의 run을 랜덤 시드를 바꿔가며 실행시킨 후 평균을 구해서 기록\n대부분의 환경에서 MLP가 잘됐고 NerveNet도 이와 비등한 퍼포먼스를 냈다.\n\n(3가지 케이스에 대한 learning curve, 다른 케이스들에서는 대체로 NerveNet과 MLP가 비슷했다.)\n\n\n\n\n\n\n\n\nHalfCheetah\nInvertedDoublePendulum\nSwimmer\n\n\n\n\n\n\n\n\n\nMLP와 NerveNet이 비슷하고 TreeNet이 많이 안좋았음\nMLP가 좀더 좋은 결과를 냄\nNerveNet이 MLP보다 좋은 성능을 냄\n\n\n\n\n대부분 환경들에서 TreeNet이 NerveNet보다 좋지 않았고 이를 통해서 물리적인 그래프 구조를 가져가는 것이 얼마나 중요한지 알 수 있다."
  },
  {
    "objectID": "posts/paper/2022-06-10-NerveNet.html#structure-transfer-learning",
    "href": "posts/paper/2022-06-10-NerveNet.html#structure-transfer-learning",
    "title": "📃NerveNet 리뷰",
    "section": "2. Structure transfer learning",
    "text": "2. Structure transfer learning\n\nMuJoCo의 환경 하나를 커스텀해서 size와 disability의 변화가 있을 때 transferable 함을 검증\n\nsize transfer - 작은 사이즈의 그래프를 가진 agent를 학습 시킨 후 더 큰 사이즈의 그래프를 가진 agent로 transferable 한지\ndisability transfer - 모든 파트들이 정상작동하는 agent로 학습한 후 일부 파트들이 작동하지 않는 상황의 agent로 transferable 한지\n\n2개 종류의 환경을 커스텀하여 실험 - centipede와 snake\n\ncentipede - 지네와 같이 생긴 agent로 torso body들이 여러개 체인처럼 연결 되어 있고 torso를 중심으로 양쪽에 다리가 1쌍으로 붙어 있다. 하나의 다리는 thigh와 shin으로 구성되어 있고 hinge actuator로 구현되어 있다. 커스텀은 다리의 갯수를 다양하게 해서 여러 커스텀 환경들을 만들었는데, 가장 짧은 agent로는 CentipedeFour 부터 가장 긴 agent로는 CentipedeFourty 로 다리가 40개까지(20쌍) 있는 환경을 만들수 있었다. disability로 일부 파트가 작동하지 않는 환경은 Cp(Cripple)로 따로 표기했다. 이 환경에서 y-direction으로 빨리 앞으로 가는게 목표다.\nsnake - swimmer 환경을 기반으로 커스텀했으며 가장 빨리 진행방향으로 움직이는 게 목표다.\n\n\n\n\n\n\n\n비교군\n\nNerveNet : small agent가 학습한 모델을 바로 large agent에 적용할 수 있었다. agent의 구조가 반복적이기 때문에 반복되는 부분을 더 늘리기만 하면 되기 때문이다.\nMLP Pre-trained (MLPP): agent의 크기가 커짐에 따라 input size가 달라지므로 가장 straightforward하게 첫번째 hidden layer를 그대로 output layer로 사용하고 input layer의 사이즈만 키워서 추가하고 이 input layer는 랜덤 초기화를 해준다.\nMLP Activation Assigning (MLPAA): small agent의 weight들을 바로 large agent의 모델에 넣어주고 weight들의 남는 부분들을 0으로 초기화 해준다.\nTreeNet: MLPAA처럼 스케일을 키워서 0으로 초기화 해준다.\nRandom : action space에서 uniformly하게 샘플링을 하는 policy이다.\n\nResult\n\nCentipede\n1-1. Pretraining\n\n6-다리 모델과 4-다리 모델로 NerveNet, MLP, TreeNet 에서의 퍼포먼스를 비교했다. 여기서 3개의 모델은 앞서 benchmark 비교 실험에서 사용한 비교군들과 동일하다.\n\n\n\n\n\n\n\n4-다리 모델에서는 NerveNet이 가장 Reward가 높고, 6-다리 모델에서는 MLP가 가장 Reward가 높음을 알 수 있다. TreeNet은 두 환경 모두에서 가장 낮다.\n6-다리 모델과 4-다리 모델로 pretraining을 진행한 후 transferable을 실험했다.\n\n1-2. Zero-shot\n\nfine tuning 없이 퍼포먼스를 측정했다.\n퍼포먼스를 쉽게 비교할 수 있도록 average reward와 average running-length를 normalization해서 색으로 아래와 같이 표현했다.(green-good, red-bad)\n\n\n\n\n\n\n\n눈으로 확실히 확인할 수 있듯이 NerveNet의 퍼포먼스가 다른 비교군에 비해 월등히 transferable함을 알 수 있었다.\n또한 learning curve에서 볼 수 있듯이 NerveNet+Pretrain 이 다른 Pretrain 비교군들에 비해 훨씬 높은 reward 시작점에서 시작하고 더 적은 timestep으로 solved 점수에 도달하는 것을 보아 그래프의 구조적 이점을 확실히 활용하고 있음을 알 수 있다.\n\n\n\n\n\n\n\nNerveNet의 agent들은 다른 비교군 agent들에서 보이지 않는 walk-cycle을 가지고 있음을 확인할 수 있었는데, 이는 보행 로봇들은 걸음새에서 반복적인 움직임을 하게 되어 있기 때문에 자연스럽게 cycle을 가지게 되는 것을 agent가 학습했음을 알 수 있다. (반면 MLP는 8-다리 모델에서 모든 다리를 움직이지 않는 모습을 보이기도 했다.)\n\nSnake\n\nsnake환경에서도 NerveNet이 다른 비교군들에 비해 뛰어난 reward 점수를 보여주며 transferable 함을 아래의 도표에서처럼 보여주었다.\n350점 정도가 snakeThree에서 solved된 상태라고 볼 수 있는데 NerveNet의 시작 점수들이 대부분 300점대에서 시작한 것으로 보아 이는 상당한 zero-shot 역량이 있음을 알 수 있다.\n다른 비교군들은 overfitting이 심해서 Random보다 안좋은 결과를 보여주는 점도 흥미롭다.\n\n\n\n\n\n\n\nzero-shot 뿐만 아니라 fine tuning을 하는 learning curve에서도 NerveNet은 Pretrain의 이점을 다른 비교군들에 비해 잘 활용하고 있음을 볼 수 있었다. NerveNet+Pretrain의 시작 reward가 높으며, 특정 size transfer 실험에서는 scratch NerveNet이 넘지 못한 MLP 점수를 NerveNet+Pretrain이 따라잡았다."
  },
  {
    "objectID": "posts/paper/2022-06-10-NerveNet.html#multi-task-learning",
    "href": "posts/paper/2022-06-10-NerveNet.html#multi-task-learning",
    "title": "📃NerveNet 리뷰",
    "section": "3. Multi-task learning",
    "text": "3. Multi-task learning\nNerveNet은 네트워크에 structure prior를 포함한 것이기 때문에 multi-task learning에 유리할 수 있다. 따라서 이를 실험하기 위해 Walker multi-task learning을 진행했다.\n\n2d-walker 환경들 5개 - Walker-HalfHumanoid, Walker-Hopper, Walker-Horse, Walker-Ostrich, Walker-Wolf\n1개의 통합된 network로 학습\n\n비교군\n\nNerveNet : agent들의 형태가 달라 weight들이 다를 수 밖에 없기 때문에 propagation과정에서의 weight matrices와 output만 공유했다.\nMLP Sharing : hidden layer들 간의 weight matrices 를 공유\nMLP Aggregation : 차원이 다른 observation들을 aggregation과정을 통해 첫번째 hidden layer의 크기로 다 맞춰주어서 input으로 넣어줌\nTreeNet: TreeNet도 weight를 공유를 할 수 있지만 agent의 구조적인 정보는 알 수 없다. 단순히 root node를 중심으로 모든 노드의 정보다 aggregation 되기 때문이다.\nMLPs: 각 agent마다 따로 MLP policy를 만들어서 학습(single-task)\n\nResult - multi-task learning 실험이기 때문에 한 두개 러닝 그래프만 볼 수 없고 5개의 러닝 그래프를 같이 봐야 한다. - Single-task policy를 제외하고 모든 환경에서 NerveNet의 퍼포먼스가 좋음을 알 수 있다.\n\n\n\n\n\n\n테이블에서 Ratio가 single-task policy에 비해 multi-task policy의 성능을 percentage로 나타낸 수치인데, MLP의 퍼포먼스가 single-task에서 multi-task로 넘어갔을 때 42%나 퍼포먼스가 줄어드는 것을 확인할 수 있다. (Average-58.6%) 반면에 NerveNet은 성능이 전혀 떨어지지 않는 결과를 보여주었다."
  },
  {
    "objectID": "posts/paper/2022-06-10-NerveNet.html#robustness-of-learnt-policies",
    "href": "posts/paper/2022-06-10-NerveNet.html#robustness-of-learnt-policies",
    "title": "📃NerveNet 리뷰",
    "section": "4. Robustness of learnt policies",
    "text": "4. Robustness of learnt policies\n강화학습 제어에서 robustness는 중요한 지표인데 질량이나 힘과 같은 물리적인 값들의 오차 범위가 어느정도까지 policy가 허용하고 잘 작동하는지를 확인해야 한다.\n\n5개의 Walker 그룹의 환경에서 실험\npretrained agent를 가지고 agent의 질량과 joint의 strength을 변경한 뒤 퍼포먼스 측정\n대부분의 환경과 variation에서 NerveNet의 robustness가 MLP보다 좋음을 알 수 있다."
  },
  {
    "objectID": "posts/paper/2022-06-10-NerveNet.html#interpreting-the-learned-representations",
    "href": "posts/paper/2022-06-10-NerveNet.html#interpreting-the-learned-representations",
    "title": "📃NerveNet 리뷰",
    "section": "5. Interpreting the learned representations",
    "text": "5. Interpreting the learned representations\n실제 폴리시들이 어떤 representation들을 학습했는지 알아보기 위해 CentipedeEight 환경에서 학습된 agent의 final state vector를 가지고 2D, 1D PCA를 진행했다.\n각 다리쌍들(Left Hip-Right Hip)들은 agent의 전체 몸체에서 각기 다른 위치에 있음에도 불구하고 invariant representation을 배울 수 있었음을 PCA를 통해서 알 수 있었다.\n\n\n\n\n\n또한 앞서 Centipede transfer learning 실험 결과에서도 잠깐 언급했던 walk-cycle이 주기성이 뚜렷하게 보였다."
  },
  {
    "objectID": "posts/paper/2022-06-10-NerveNet.html#comparison-of-model-variants",
    "href": "posts/paper/2022-06-10-NerveNet.html#comparison-of-model-variants",
    "title": "📃NerveNet 리뷰",
    "section": "6. Comparison of model variants",
    "text": "6. Comparison of model variants\nValue Network를 어떻게 할 것인지에 따라 NerveNet의 여러 변형이 있을 수 있는데 Swimmer, Reacher, HalfCheetah에서 비교해본 결과, Value Network는 MLP로 한 NerveNet-MLP의 퍼포먼스가 가장 좋았고 NerveNet-1의 퍼포먼스가 2등으로 NerveNet-MLP 와 비슷했다. 이에 대한 잠재적인 이유로 value network와 policy network가 weight를 공유하는 것이 PPO 알고리즘에서의 trust-region based optimitaion에서의 weight \\alpha를 더 sensitive하게 만들기 때문이라고 추론할 수 있다."
  },
  {
    "objectID": "posts/paper/2022-10-02-vae.html",
    "href": "posts/paper/2022-10-02-vae.html",
    "title": "📃VAE 리뷰",
    "section": "",
    "text": "이번 포스트는 생성모델에서 유명한 Variational Auto-Encoder(VAE)를 다루고 있는 Auto-Encoding Variational Bayes라는 논문 리뷰입니다. 이번 포스트를 정리하면서 가장 많이 인용하고 도움을 받은 오토 인코더의 모든 것를 보시면 훨씬 더 자세하고 깊은 이해를 하실 수 있습니다. 포스트의 순서는 아래와 같이 진행됩니다."
  },
  {
    "objectID": "posts/paper/2022-10-02-vae.html#regularization-term",
    "href": "posts/paper/2022-10-02-vae.html#regularization-term",
    "title": "📃VAE 리뷰",
    "section": "2.1 Regularization term",
    "text": "2.1 Regularization term\nELBO term을 나누었을 때 나왔던 첫번째 Regularization term에 대해 보겠습니다. True posterior를 추정하기 위한 q_\\phi(\\mathrm{z} \\mid \\mathrm{x})은 KL 값을 계산하기 쉽도록 하기 위해 Multivariate gaussian distribution으로 설계합니다. 또한 앞서 이야기했던 것 처럼 controller 부분인 p(z)는 다루기 쉬운 분포이어야 하기 때문에 정규분포로 만들어 줍니다. 그러면 논문의 Appendix F.1에서 볼 수 있듯이 가우시안 분포들 사이의 KL 값은 mean과 std를 사용하여 다음과 같이 쉽게 계산될 수 있습니다.\n\n\n\nRegularization term6"
  },
  {
    "objectID": "posts/paper/2022-10-02-vae.html#reconstruction-error-term",
    "href": "posts/paper/2022-10-02-vae.html#reconstruction-error-term",
    "title": "📃VAE 리뷰",
    "section": "2.2 Reconstruction error term",
    "text": "2.2 Reconstruction error term\nELBO의 두번째 term인 Reconstruction error에 대해 살펴보겠습니다. Reconstruction error의 expectation 표현을 integral로 표현하면 다음과 같고 이는 몬테카를로 샘플링을 통해 L개의 z_{i, l}를 가지고 평균을 내서 구할 수 있습니다. 여기에서 index i는 데이터 x의 넘버링이고 index l은 generator의 distribution에서 샘플링하는 횟수에 대한 넘버링입니다. VAE는 한정된 몬테카를로 샘플링을 통해 효과적으로 optimization을 수행합니다.\n\n\n\nRecontruction error term6\n\n\n\n2.2.1 Reparametrization Trick\n위에서 Reconstruction error를 구하기 위해 샘플링하는 과정에서 backpropation을 하기 위해 Reparametrization trick을 사용하게 됩니다. 단순히 정규분포에서 샘플링 하면 random node인 z에 대해서 gradient를 계산할 수 없기 때문에 random성을 정규분포에서 샘플링 되는 ϵ으로 만들어주고 이를 reparametrization을 해주어서 deterministic node가 된 z를 backpropagation 할 수 있게 됩니다.\n\n\n\nReparametrization trick6\n\n\n# sampling by re-parameterization technique\nz = mu + sigma * tf.random_normal(tf.shape(mu), 0, 1, dtype=tf.float32)\nz를 샘플링하는 generator의 distribution은 Bernoulli로 디자인할 경우 NLL이 Cross Entropy가 되며 Gaussian 분포로 디자인할 경우 MSE가 되어서 보통 계산하기 용이한 2개의 분포 중 하나를 사용하게 됩니다. 모델 디자인의 조건은 데이터의 분포에 따라 결정되는데 데이터의 분포가 continuous 하다면 Gaussian 분포에 가깝기 때문에 Gaussian으로 디자인하고, 데이터의 분포가 discrete 하다면 Bernoulli분포에 가깝기 때문에 Bernoulli로 디자인합니다.\n\n\n\nTypes of generator distributions6"
  },
  {
    "objectID": "posts/paper/2022-06-26-keep-learning.html",
    "href": "posts/paper/2022-06-26-keep-learning.html",
    "title": "📃Legged Robots that Keep on Learning 리뷰",
    "section": "",
    "text": "Legged robots are physically capable of traversing a wide range of challenging environments but designing controllers that are sufficiently robust to handle this diversity has been a long-standing challenge in robotics. Reinforcement learning presents an appealing approach for automating the controller design process and has been able to produce remarkably robust controllers when trained in a suitable range of environments. However, it is difficult to predict all likely conditions the robot will encounter during deployment and enumerate them at training-time. What if instead of training controllers that are robust enough to handle any eventuality, we enable the robot to continually learn in any setting it finds itself in? This kind of real-world reinforcement learning poses a number of challenges, including efficiency, safety, and autonomy. To address these challenges, we propose a practical robot reinforcement learning system for fine-tuning locomotion policies in the real world. We demonstrate that a modest amount of real-world training can substantially improve performance during deployment, and this enables a real A1 quadrupedal robot to autonomously fine-tune multiple locomotion skills in a range of environments, including an outdoor lawn and a variety of indoor terrains."
  },
  {
    "objectID": "posts/paper/2022-06-26-keep-learning.html#system-process",
    "href": "posts/paper/2022-06-26-keep-learning.html#system-process",
    "title": "📃Legged Robots that Keep on Learning 리뷰",
    "section": "System Process",
    "text": "System Process\n\n\n\n\n\n\n위의 사진에 보이는 공원과 같은 새로운 환경에서 먼저 로봇 agent가 첫번째 시도로 locomotion task를 진행한다.\n만약에 땅이 고르지 못해서 agent의 학습된 policy를 활용할 수 없는 상황이 되어서 넘어지게 되는 상황이 될 수 도 있다.\n이때 reset controller를 이용해서 빠르게 다시 일어난다.\n실제 task에서 좀 더 몇 번 시도를 하면서 1~3의 과정을 몇 번 반복하게 되고 이 과정에서 policy가 업데이트 되게 된다.\n업데이트가 되면서 policy는 새로운 test 환경에서 제대로 작동할 수 있게 된다."
  },
  {
    "objectID": "posts/paper/2022-06-26-keep-learning.html#how",
    "href": "posts/paper/2022-06-26-keep-learning.html#how",
    "title": "📃Legged Robots that Keep on Learning 리뷰",
    "section": "How",
    "text": "How\n\n강화학습의 reward 가 robot의 on-board 센서로 측정되는 값들로만 디자인 되어야 실제 Real-world에서 작동하면서 fine tuning을 할 수 있다.\nAgile한 behavior를 학습하기 위해서 Motion imitation 기법을 활용했다.\n로봇의 넘어지고 나서 빠르게 정상자세로 회복할 수 있도록 Recovery policy를 학습했다.\n강화학습 알고리즘들 중에서 REDQ(Randomized Ensembled Double Q-Learning) 라는 알고리즘을 사용했는데, 이 알고리즘은 여러개 Q-network들의 앙상블을 통해 randomization을 해서 Q-learning 계열의 알고리즘들의 sample-efficiency와 안정성을 향상시킨 알고리즘이다."
  },
  {
    "objectID": "posts/paper/2022-06-26-keep-learning.html#main-contribution",
    "href": "posts/paper/2022-06-26-keep-learning.html#main-contribution",
    "title": "📃Legged Robots that Keep on Learning 리뷰",
    "section": "Main Contribution",
    "text": "Main Contribution\n\n본 논문의 주요 contribution은 다음과 같다.\n\n\n4족 보행 로봇의 agile한 locomotion skill을 real-world에서 학습하기 위한 fine-tuning 자동화 시스템을 제안하였다.\n처음으로 자동화 reset과 on-board 상태 추정을 통해 real-world에서 fine-tuning이 될 수 있음으로 보였다.\nA1 로봇을 가지고 dynamic skill들을 학습해서 외부 잔디에서 앞으로, 뒤로 pacing을 하고 3가지 다른 지형 특징을 가진 환경에서 side-stepping을 할 수 있었다."
  },
  {
    "objectID": "posts/paper/2022-06-26-keep-learning.html#details-with-hash-tags",
    "href": "posts/paper/2022-06-26-keep-learning.html#details-with-hash-tags",
    "title": "📃Legged Robots that Keep on Learning 리뷰",
    "section": "Details with Hash tags",
    "text": "Details with Hash tags\n\n원 논문의 II. Related Work section 참고\n\n#Cumbersome controller designs\n\n이전의 로봇 controller들은 footstep planning, trajectory optimization, model-predictive control (MPC) 등의 조합으로 만들어지고 있었다. 그러나 이런 방법들은 로봇의 동역학과 각 로봇마다 다르고 각 skill마다 다른 많은 요소들을 고려해야 하기 때문에 정말 어려웠다.\n\n#Sim2Real\n\ntrial-and-error라는 데이터에 매우 의존성이 높은 강화학습 알고리즘의 특성과 하드웨어의 safety 이슈 때문에 보통 로봇 강화학습 agent는 시뮬레이션 기반으로 학습된다. 하지만 시뮬레이션에서 학습하면서 실제로 만나보지 않은 real-world의 모든 조건들을 예상하고 학습하기란 사실상 불가능하며 가장 robust한 policy라고 할지라도 모든 상황에 대해 generalization 되었다고 할 수 없다.\n\n#Real-world\n\n이전에 복잡한 motion들을 학습하게 하기 위해서 environment의 다양한 장치들로 다양한 상태 정보를 만들어서 사용했지만 본 연구에서는 real-world에서 작동하고 있는 로봇에서 fine-tuning을 해야 하기 때문에 로봇의 on-board에서 받을 수 있는 모든 state estimation 정보들을 가지고만 진행했으며 motion capture나 외부 장치들을 별도로 사용하지 않았다.\nscratch부터 실제 환경에서 단순한 구조의 로봇들로 walking gaits들을 학습하는게 아니라, A1 로봇으로 pacing, side stepping 등 매우 자연스럽고 조금은 불안정하고 세밀한 balancing이 요구되는 skill들을 학습할 수 있었다. (기존의 연구들은 balancing에 매우 신경쓴 나머지 느리고 부자연스러운 walking gaits 에 치중한 면이 있었다.) 본 논문의 연구에서 motion imitation과 실제 환경에서의 fine-tuning 이 이런 다이나믹한 task들을 성공시키는데 매우 중요한 역할을 했다. 또한 실제 환경에서 로봇이 작동하면서 넘어질 때, manual하게 로봇의 reset하거나 recovery시키지 않고 강화학습으로 자동적으로 reset 할 수 있는 controller를 만들어서 사용했다.\n\n#Few-shot adaptation\n\n기존의 Adaptation structure라는 구조를 만들어서 학습시켜서 latent 또는 explicit한 환경에 대한 descriptor로 adaptive한 policy를 만드는 연구들이 있었으나, 이 기법들 또한 결국 training에서 경험했던 것들을 기반으로 adaptive함을 보이는 것이므로 실제 test 환경이 이 허용 범위에서 많이 벗어날 경우 제대로 작동안되는 것은 똑같다. 따라서 강화학습으로 지속적인 적응적인 학습능력을 보장해서 어떤 test 환경에서든 잘 작동할 수 있도록 했다.\n\n#RL Algorithm\n\n강화학습 알고리즘으로는 기존의 vision 기반 매니퓰레이터들에서 grasping 작업을 하는 task들에서 많이 쓰인 off-policy model-free RL 기법들을 참고하여 fixed되어 있는 매니퓰레이터들보다 더 challenging한 floating-based 보행 로봇의 locomotion에 적용해서 성공시켰다."
  },
  {
    "objectID": "posts/paper/2022-06-26-keep-learning.html#overview",
    "href": "posts/paper/2022-06-26-keep-learning.html#overview",
    "title": "📃Legged Robots that Keep on Learning 리뷰",
    "section": "Overview",
    "text": "Overview\n아래 사진의 전체 시스템의 개략도에서 볼 수 있듯이 각각의 policy는 하나의 desired skill을 학습하게 된다. 즉 하나의 policy는 forward를, 다른 policy는 backward를, 마지막 다른 policy는 reset을 담당하여 학습하게 된다. 이렇게 다양한 task를 수행할 수 있도록 만든 프레임워크 이기 때문에 Multitask framework인 것이다.\n\n\n\n\n\n\nPseudo Algorithm\n시스템 개략도에서 봤듯이 논문에 나와있는 시스템 전체를 보여주는 Algorithm2 알고리즘은 크게 2개의 과정으로 진행된다.\n\n\n\n\n\n\nAgent의 policy는 시뮬레이션에서 pretrained 한다. (Algorithm 2 line 2~7)\n\n각 에피소드가 끝날 때마다 학습된 recovery policy가 로봇을 다음 rollout을 할 수 있도록 준비시켜준다.\n각 skill을 위한 policy들은 독립적으로 학습되고 recovery policy도 마찬가지로 독립적으로 학습된다.\n\nFine-tuning을 실제 물리적인 환경에서 진행하면서 training process를 계속 이어나갈 수 있다. (Algorithm 2 line 8~14)\n\n시뮬레이션과 실제 환경의 차이를 고려하여 각 policy들의 replay buffer는 초기화 시켜준다.(Algorithm 2 line 12)"
  },
  {
    "objectID": "posts/paper/2022-06-26-keep-learning.html#motion-imitation-off-policy-rl",
    "href": "posts/paper/2022-06-26-keep-learning.html#motion-imitation-off-policy-rl",
    "title": "📃Legged Robots that Keep on Learning 리뷰",
    "section": "Motion Imitation & Off-policy RL",
    "text": "Motion Imitation & Off-policy RL\n\n\n\n\n\nMotion Imitation\nMotion Imiation 방법을 이용하여 reference motion clip들의 skill들을 모방 학습하도록 했는데 이는 Learning Agile Robotic Locomotion Skills by Imitating Animals라는 논문에서 제시한 방법을 따라했다. (Algorithm 1 line1~4)\nReference motion M이 주어지면 agent의일련의 pose들과 비교하여 section III-B에서 소개될 reward function을 기반으로 학습한다. - 이 방법을 통해 reference motion data만 바꿔주면 바로 다른 여러 skill들을 배울 수 있다. - recovery policy를 학습하기 위해서 standing pose를 모방하도록 할 수 있다.(III-C 참고)\nOff-policy RL\noff-policy 알고리즘인 REDQ algorithm 사용했다.(Algorithm 1 line5~9) - SAC 알고리즘을 더 발전시킨 알고리즘 - time step에 대한 gradient step비율을 증가시켜서 강화학습 알고리즘의sample efficiency를 높였다. - 너무 많은 gradient step을 할 경우에 일어날 수 있는 overestimation issue를 앙상블 기법을 이용해서 완화할 수 있었다."
  },
  {
    "objectID": "posts/paper/2022-06-26-keep-learning.html#a.-state-action-spaces",
    "href": "posts/paper/2022-06-26-keep-learning.html#a.-state-action-spaces",
    "title": "📃Legged Robots that Keep on Learning 리뷰",
    "section": "A. State & Action Spaces",
    "text": "A. State & Action Spaces\n\nState space\n\nState는 연속적인 3 timesteps에서 얻은 아래 정보들로 정의했다.\n\nRoot orientation (read from the IMU)\nJoint angles\nPrevious actions\n\nPolicy는 위에서 말한 Proprioceptive input 뿐만 아니라 a goal g_t에 대한 정보도 input으로 받게 된다.\n\ng_t는 future timesteps에서의 reference motion에서 계산된 Target pose (root position, root rotation, joint angles)의 정보를 포함한다.\n4 future target poses 는 현재 timestep에서 약 1초 정도 이후의 pose들이다.\n\n\nAction space\n\nAction은 12 joints들에 대한 PD position targets 이다.\n33Hz의 주파수로 command가 적용된다.\n자연스러운 움직임을 위해 PD targets을 low-pass filter를 로봇에 적용하기 전에 통과시켜준다."
  },
  {
    "objectID": "posts/paper/2022-06-26-keep-learning.html#b.-reward-function",
    "href": "posts/paper/2022-06-26-keep-learning.html#b.-reward-function",
    "title": "📃Legged Robots that Keep on Learning 리뷰",
    "section": "B. Reward Function",
    "text": "B. Reward Function\n\n\\begin{gathered}r_{t}=w^{\\mathrm{p}} r_{t}^{\\mathrm{p}}+w^{\\mathrm{v}} r_{t}^{\\mathrm{v}}+w^{\\mathrm{e}} r_{t}^{\\mathrm{e}}+w^{\\mathrm{rp}} r_{t}^{\\mathrm{rp}}+w^{\\mathrm{rv}} r_{t}^{\\mathrm{rv}} \\\\w^{\\mathrm{p}}=0.5, w^{\\mathrm{v}}=0.05, w^{\\mathrm{e}}=0.2, w^{\\mathrm{rp}}=0.15, w^{\\mathrm{rv}}=0.1\\end{gathered}\n\n\n\nr_{t}^{\\mathrm{p}} : 로봇의 joint rotation 값들을 reference motion의 joint rotation과 맞추도록 하는 reward term\n\n  r_{t}^{\\mathrm{p}}=\\exp \\left[-5 \\sum_{j}\\left\\|\\hat{q}_{t}^{j}-q_{t}^{j}\\right\\|^{2}\\right]\n  \n\n\\hat{q}_{t}^{j} : 시점 t에 reference motion의 j번째 joint의 local rotation\nq_{t}^{j} : 로봇의 j번째 joint local rotation\n\nr_{t}^{\\mathrm{v}} : joint velocities\nr_{t}^{\\mathrm{e}} : end-effector positions\n로봇이 reference root motion을 잘 tracking 하게 하기 위한 reward term\n\nr_{t}^{\\mathrm{rp}} : root pose reward\nr_{t}^{\\mathrm{rv}} : root velocity reward\n\n\n\n이전부터 강조해왔듯이, 실제 환경에서 fine-tuning과정을 진행하기 위해서 on-board 센서들의 값을 이용해서 reward function을 디자인하였고 실제 물리적인 환경에서 구동할 때 이를 상태 추정 기법을 이용해서 reward를 구하게 된다. 따라서 아래의 상태 추정 방법(State Estimation)이 fine-tuning의 성능을 결정하는 중요한 부분이 된다.\n\nReal-world에서 로봇의 linear root velocity를 잘 추정하기 위해서 Kalman filter를 사용했다.\n\n칼만 필터는 IMU 센서에서 acceleration과 orientation 값들을 읽어서 foot contact sensors로 값들을 보정한다.\n처음에 발 끝의 속도를 0으로 생각해서 각 다리의 joint velocities를 고려하여 몸체의 속도를 계산하고 IMU으로부터 추정했던 값을 보정한다.\n\n이렇게 계산된 linear velocity를 로봇의 position 추정값에 통합시킨다.\n\n\n\n\n\n\n위의 그래프들에 볼 수 있듯이(아래에서 위 방향으로),\n\nangular velocity와 orientation 센서 값들은 매우 정확했다.\nlinear velocity는 매우 정확하진 않았지만 허용가능했다.(reasonable)\nposition drifts는 상당히 벗어나는 부분이 있었지만, 각 에피소드에서 reward function을 계산할 정도로의 적합한 값들을 보여주었다."
  },
  {
    "objectID": "posts/paper/2022-06-26-keep-learning.html#c.-reset-controller",
    "href": "posts/paper/2022-06-26-keep-learning.html#c.-reset-controller",
    "title": "📃Legged Robots that Keep on Learning 리뷰",
    "section": "C. Reset Controller",
    "text": "C. Reset Controller\n\nreset policy를 시뮬레이션에서 학습하기 위해 다양한 initial states에서 시작하도록 했다.\n\n→ 로봇을 random한 height & orientation에서 떨어뜨려서 아래 사진에서 볼 수 있듯이 다양한 initial states를 설정\n\n\n\n\n\n\nMotion imitation 목적함수를 수정해서 single, streamlined reset policy를 학습시켰다.\nReference motion을 가지고 로봇이 정확히 어떻게 일어나야 할지를 알려주는 것이 아니라, 아래와 같은 방법으로 reset policy를 학습시켰다.\n\n\npolicy가 rolling right side up을 위한 reward만을 가지고 학습한다.\n만약 로봇이 upright하는데 성공하면 이후에 motion imitation reward를 추가시켜서 학습니다.\n\n이때의 reference motion은 standing pose가 되고 로봇이 똑바로 설 수 있도록 학습시킨다.\n\n\n\n이런 방식으로 학습된 reset policy는 다양한 test 지형에서 fine-tuning 없이도 잘 동작했다.(tranfered well)"
  },
  {
    "objectID": "posts/paper/2022-06-26-keep-learning.html#a.-simulation-experiments",
    "href": "posts/paper/2022-06-26-keep-learning.html#a.-simulation-experiments",
    "title": "📃Legged Robots that Keep on Learning 리뷰",
    "section": "A. Simulation Experiments",
    "text": "A. Simulation Experiments\n\nagent의 policy를 먼저 특정 시뮬레이션 셋팅에서 학습시킨 후에 학습된 시뮬레이션과 또 다른 시뮬레이션 환경 셋팅에 “deployed”한 후 결과를 살펴보았다.\nLearned forward pacing gait가 테스트 환경들에서 얼마나 빨리 적용되는지 확인해보았다.\nStandard dynamics randomization (mass, inertia, motor strength, friction, latency 변동)으로 Pre-train을 flat ground에서 진행했다.\n\nThe test terrains\ntest 환경들로는 총 3가지로 실험하였으며 pre-training 과정의 시뮬레이션 셋팅과 유사한 test 환경 [1]과 pre-training 과정의 시뮬레이션 셋팅과 다소 다른 test 환경 [2], [3]에서 진행됐다.\n\na flat ground\nrandomized heightfield : 랜덤하게 지형의 높이를 설정한 울퉁불퉁한 지형\na low friction surface : 낮은 마찰계수를 가지는 지형, 빙판길과 같은 미끄러운 지형(Training 과정에서 경험한 마찰계수 분포와 한참 동떨어진 마찰계수를 가지고 있음)\n\n비교군\n\nlatent space : 호율적인 다양한 dynamics parameters에 대한 학습을 하기 위해 latent space에 표현된 behaviors을 학습\nRMA: dynamics randomization한 모델. 위에서 언급한 Adaptation Module을 가지고 학습\nVanilla SAC : Soft Actor-Critic 알고리즘으로 학습\nOurs(REDQ): 10개의 Q-functions을 가지고 randomly sample 2로 학습\n\n\n\n\n\n\n\n실험 결과를 살펴보면, RMA는 training 환경에서만 높은 성능을 보여주어 Adaptation Module의 한계점을 명확히 보여주었다. SAC에 비해서 REDQ(Ours)가 sample efficiency가 좋을 뿐만 아니라 수렴하는 Return 값도 높았다."
  },
  {
    "objectID": "posts/paper/2022-06-26-keep-learning.html#b.-real-world-experiments",
    "href": "posts/paper/2022-06-26-keep-learning.html#b.-real-world-experiments",
    "title": "📃Legged Robots that Keep on Learning 리뷰",
    "section": "B. Real-World Experiments",
    "text": "B. Real-World Experiments\n시뮬레이션에서 학습된 Agent를 4개의 real-world 환경(Outdoor 1개, Indoor 3개)에서 test 했다. 모든 (real-world) test 지형 실험은 시뮬레이션의 flat ground에서 pre-training된 agent로 실험한 것이었으며, 처음에 buffer를 5000 samples로 초기화 해주고 시작한 다음 test real world 환경에서 policy를 fine-tuning 해주었다.\n\nOutdoor grassy lawn:\n\nslippery surface를 가지고 있어서 발이 잔디에서 미끄러지거나 흙에 빠질 수 있다.\n앞 혹은 뒤로 움직이는 pacing gait를 fine-tuning 하도록 했다.(pacing gait: 좌나 우의 2개의 다리가 한번에 움직이는 걸음새)\nPre-trained forward pacing policy는 매우 조금만 앞으로 갈 수 있었고, pre-trained backward pacing policy는 잘 넘어지는 경향이 있었다.\n작동한 지 약 2시간 만에, 로봇은 (아주 조금의 넘어짐은 있었지만) 지속적이고 안정적으로 앞 혹은 뒤로 pacing gait를 할 수 있었다.\n\n\n\n\n\n\nIndoor\n\nCarpeted room: 높은 마찰계수를 가지는 지형으로 (카펫이 푹신하므로) 로봇의 고무로 마감되어 있는 발이 시뮬레이션에서 학습된 것과 다르게 안정적이지 않은 컨택을 하게 될 수 있다.\n\n\n\n\n\n\n\nDoormat with crevices: 매트 표면에 발이 빠질 수도 있는 환경이다.\n\n\n\n\n\n\n\nMemory foam: 4cm 정도의 두께의 메모리폼으로 발이 매트리스에 빠지고 평평하고 딱딱한 바닥과 비교했을 때 이 환경에서는 gait(걸음새)가 상당히 변화가 많이 일어날 수 있다.\n\n\n\n\n\n\n\nIndoors에서는, pre-trained side stepping policy가 움직일 때 매우 불안정했고 motion을 끝내기 전에 넘어졌다.\n그러나 각 지형 셋팅에서 2.5 시간 이내로 로봇이 비틀거림 없이 skill을 수행할 수 있었다."
  },
  {
    "objectID": "posts/paper/2022-06-26-keep-learning.html#c.-semi-autonomous-training",
    "href": "posts/paper/2022-06-26-keep-learning.html#c.-semi-autonomous-training",
    "title": "📃Legged Robots that Keep on Learning 리뷰",
    "section": "C. Semi-autonomous training",
    "text": "C. Semi-autonomous training\n\n\n\n\n\n\n전반적인 모든 실험들에서, the recovery policy는 100% 성공적이었다.\n본 논문에서 제시된 방법으로 학습된 reset controller와 Unitree에서 제공한 built-in rollover controller를 비교해보았다.\n\nOn hard surfaces : 두 가지 controllers 모두 효과적으로 잘 작동했지만 built-in 컨트롤러는 learned policy에 비해 상당히 느렸다.\nOn the memory foam : built-in 컨트롤러는 더 성능이 좋지 못했다."
  },
  {
    "objectID": "posts/storage/2020-07-17-rl-study-roadmap.html",
    "href": "posts/storage/2020-07-17-rl-study-roadmap.html",
    "title": "🧩My Reinforcement Learning Roadmap",
    "section": "",
    "text": "강화학습을 공부했던 roadmap을 기록하고자 글을 쓰게 되었다.\n짧게 기록할 수 있는 정보들과 내 느낌들을 간략하게 적어보고자 한다."
  },
  {
    "objectID": "posts/storage/2020-07-17-rl-study-roadmap.html#my-rl-study-road-map",
    "href": "posts/storage/2020-07-17-rl-study-roadmap.html#my-rl-study-road-map",
    "title": "🧩My Reinforcement Learning Roadmap",
    "section": "My RL study road map",
    "text": "My RL study road map\n\n아래는 시간 순서대로 내가 공부했던 강의나 책, 스터디 모임\n\n\n모두의 rl\n케라스로 시작하는 rl 책 스터디\n모두연 starcraft project - 논문 리딩\nUdacity\nConnect-X\npytorch 책\n수학 강화학습 책\nOpen AI Gym\nfast campus\nBNM2h 스터디\nRL 논문 스터디 3기\nUnity Ml agent 스터디\nHugging face 강화학습 강의"
  },
  {
    "objectID": "posts/storage/2020-07-17-rl-study-roadmap.html#materials",
    "href": "posts/storage/2020-07-17-rl-study-roadmap.html#materials",
    "title": "🧩My Reinforcement Learning Roadmap",
    "section": "Materials",
    "text": "Materials\n\n무료강의\n\n\nHuggiing Face - DRL course: 허깅페이스에서 제공하는 강화학습 강의. 역시 허깅 페이스. 개인적으로 Udacity보다 더 좋은 것 같다.\n모두의 RL: 강화학습 입문용 강의. 가볍게 듣고 시작하는 것을 추천.\n팡요랩 : 강화학습 강의로 유명한 데이비드 실버 교수님 강의 한글 버전\n혁펜하임 : 수식적으로 파고 들어가고 싶다면 추천.\nCS285 : 입문이라기에 조금 어려울 수는 있으나 만약 로봇틱스쪽을 생각하고 있다면 꼭 들어야하는 강의.\nDeepMind - Reinforcement Learning Lecture Series 2021 (2022.05.06 updated)\n\n\n유료강의\n\n\nUdacity : 엄청 비쌈, 프로그램이 좋긴한데 nano degree가 목적이 아니라면 추천하지 않음.\nUdemy : 코드 위주로 빨리 배워보고 싶을 때. 어느정도 강화학습에 대한 기본 지식이 있을 때 듣는 것이 좋음.\nFast campus 박준영 강사님 : 무엇보다 한국어로 자세히 설명도 해주시고 코드도 있어서 좋음. 개인적으로 무료 강의들로 기초를 한번 다지고 난 후 이 강의를 들으면서 불명확했던 부분을 짚으면 좋은 것 같다.\n\n\n책\n\n\n파이썬과 케라스로 배우는 강화학습\nPyTorch를 활용한 강화학습/심층강화학습 실전 입문\n수학으로 풀어보는 강화학습 원리와 알고리즘 : 수학적 기반 다지기. 모델 베이스드 RL 쪽으로 내용이 좋음.\n단단한 강화학습 : 강화학습 대가 리처드 서튼 교수님의 바이블 원작을 번역.\n단단한 심층강화학습 (2022.05.06 updated)\n\n\n2022.05.06 기준 강화학습 관련된 많은 책들이 나와서 책에 대한 추천은 최근 출간된 책들을 목차들을 보고 본인에게 필요한 부분을 공부하는 것을 추천.\n\n\n웹사이트(입문으로는 조금 힘들 수 있으나 어느정도 공부한 후, 트렌드나 흐름 잡기에 좋음)\n\n\nDeepMind blog\nOpenAI blog\nOpenAI Spinning up\n\n\nGithub\n\n\nRL 가계도\nHuggingface DRL: 위에 무료강의 중 허깅페이스 강의에서 사용하는 코드들이 올라와 있음.\npg-is-all-you-need: Policy Gradient 계열의 알고리즘들 코드. 최근 25.01 기준 리펙토링 되었음\n\n\n커뮤니티\n\n\nRL Korea : 페이스북\nRL.start() : 오픈 카카오톡"
  },
  {
    "objectID": "posts/project/quad_recovery.html",
    "href": "posts/project/quad_recovery.html",
    "title": "Deep Reinforcement Learning for Fall-Recovery Control on Non-Flat Terrain of Quadruped Robots",
    "section": "",
    "text": "tbd"
  },
  {
    "objectID": "posts/project/kompanion.html",
    "href": "posts/project/kompanion.html",
    "title": "Smart Device for Dog Triaining",
    "section": "",
    "text": "https://github.com/curieuxjy/K-ompanion"
  },
  {
    "objectID": "posts/project/traffic_signal_od.html",
    "href": "posts/project/traffic_signal_od.html",
    "title": "Traffic Sign Detection and Recognition Task",
    "section": "",
    "text": "https://github.com/curieuxjy/traffic_sign_object_detection"
  },
  {
    "objectID": "posts/code/2025-01-10-fitting-ellipse.html",
    "href": "posts/code/2025-01-10-fitting-ellipse.html",
    "title": "👩‍💻타원 Fitting",
    "section": "",
    "text": "그동안 Opencv 패키지에서 ellipse fitting을 해주는 함수 fitEllipse만 가져다가 썼다면, 어떤 원리로 타원이 피팅되고 경우에 따라 옵션들을 바꿔야 한다면 어떤 선택지들이 있는지 알아보겠습니다. 먼저 간단한 타원을 표현하는 수학적인 표현식부터 간단한 scratch code로 타원을 fitting해보고, opencv에서는 어떤 함수들을 제공하고 있는지 확인해보겠습니다.\n\n\n\nimage source\n\n\n\nMore\n\nReference\n\nfitellipse.cpp\nCreating Bounding rotated boxes and ellipses for contours\nPython Implementation of Ellipse Fitting\nOpenCV 타원 추정 Ellipse Fitting\nOpenCV’s fitEllipse() sometimes returns completely wrong ellipses"
  },
  {
    "objectID": "posts/code/2024-01-21-quarto-advanced.html",
    "href": "posts/code/2024-01-21-quarto-advanced.html",
    "title": "👩‍💻Quarto Blog + α",
    "section": "",
    "text": "Quarto를 활용하여 Github 블로그를 구축하는 이전 포스팅에 이어서, Github 블로그를 좀 더 보기 좋게, 혹은 원하는 기능을 좀 더 추가하는 몇가지 Customizing 사항들에 대해 소개하려고 합니다. 사실 블로그 Customizing은 삽질의 시작이며, 처음 시작은 가볍게 시작하지만 하나씩 옵션들을 내맘대로 건드리다가 보면 시간이 훌쩍 지나가버리는 마법을 경험하게 됩니다.\n기본적으로 .qmd파일을 작성하면서 글의 configuration을 설정하는 것은 Tutorial: Authoring에서 제목 작성, category 설정등 기본적인 부분을 확인할 수 있습니다. 기본적인 설정들에 더해서 저번 포스팅보다 여러가지 옵션들을 바꿔가면서 확인하는 과정들이 많기 때문에 localhost(로컬 컴퓨터)에서 블로그가 어떻게 바뀌어 가는지 확인하기 위해 아래의 명령어를 VSCode 터미널에 입력하여 창을 띄우며 확인해보세요!"
  },
  {
    "objectID": "posts/code/2024-01-21-quarto-advanced.html#toc",
    "href": "posts/code/2024-01-21-quarto-advanced.html#toc",
    "title": "👩‍💻Quarto Blog + α",
    "section": "ToC",
    "text": "ToC\nToC란 Table of Contents 의 약자로 블로그의 내용들이 여러 제목/소제목들로 구성되어 글이 길어질 경우, 내용을 한눈에 파악하기 쉽도록 목차를 보여주는 기능입니다. Quarto에서는 html 설정 옵션으로 이 기능을 지원하고 있으며 아래 예시 사진에서와 같이 한쪽에 ToC를 보여줘서 포스팅의 독자분들이 쉽게 내용을 파악하고 필요하는 부분만 골라서 확인하기도 용이합니다.\n\n\n\nBlog 포스팅에서 ToC 부분\n\n\nQuarto에서는 다양한 ToC 형식을 지원하고 있는데 작성하는 .qmd파일의 서두에 설정하는 포스팅 설정 부분에 toc: ture라고 추가하면 해당 .qmd파일에 #, ## 등으로 제목으로 작성했던 이름을 기반으로 해당 포스팅의 ToC가 나타나게 됩니다.\n\n\n\nqmd파일에서 ToC 설정하기\n\n\n만약 ToC의 Numbering을 자동으로 나오게 하고 싶다면 아래와 같이 number-sections: true도 포스팅 설정 부분에 추가해준다면 넘버링된 ToC가 나오는 것을 확인할 수 있습니다.\ntoc: true\nnumber-sections: true\n\n\n\nNumbering ToC로 바뀐 모습\n\n\n이 부분도 Quarto Docs에서 더 다양한 옵션들을 확인해보실 수 있으니 다양한 옵션들을 기호에 맞게 바꿔가며 확인해보세요!"
  },
  {
    "objectID": "posts/code/2024-01-21-quarto-advanced.html#이미지-넣기",
    "href": "posts/code/2024-01-21-quarto-advanced.html#이미지-넣기",
    "title": "👩‍💻Quarto Blog + α",
    "section": "이미지 넣기",
    "text": "이미지 넣기\n이미지와 관련된 작성 코드 스니펫을 소개하겠습니다. 이전에 이미지를 Typora를 통해 넣거나 Markdown 문법의 형식으로 이미지를 삽입하게 되면 ![이미지 캡션](이미지 경로)와 같이 코드로 이미지를 넣어주게 됩니다. 하지만 이렇게 이미지들을 넣게 되면 이미지 사이즈 조절이나 정렬 등을 설정해주기가 어렵기 때문에 html 문법형식을 빌려 글에 이미지들을 넣어주는 것을 추천드립니다.\n\nhtml 사용하여 이미지 삽입\n이 코드는 가운데 정렬(centering), 이미지의 (포스팅 대비 상대적인) 너비 조정(width), 이미지 캡션(figcaption)을 html 문법을 빌려 이미지를 삽입해주는 snippet입니다.\n&lt;center&gt;\n&lt;img src=\"../../images/이미지_이름.png\" width=\"50%\" /&gt;\n&lt;figcaption&gt;이미지 캡션&lt;/figcaption&gt;\n&lt;/center&gt;\n포스팅 대표 이미지 설정\n다음으로 작성글의 대표 이미지를 설정하면 블로그 메인 화면에 뜨는 그림을 정해 줄 수 있습니다.\n\n\n\n블로그 메인 화면에서 각 포스팅들의 대표 이미지들\n\n\n.qmd파일의 맨 윗부분에 설정해주는 문서 configuration에 image: ... 라인을 추가하고, ...부분에 이미지 경로를 설정해주면 포스팅의 대표 이미지를 설정할 수 있습니다. 아래는 이번 포스팅의 대표 이미지를 설정하는 예시 configuration 입니다.\n---\ntitle: \"👩‍💻Quarto Blog(2)\"\ntoc: true\ndate: \"2024-01-17\"\ndescription: Quarto로 속편한 Github Blog 구축하기(2)\ncategories: [blog, quarto, code]\nimage: ../../images/2024-01-12-quarto-blog2/blog2.jpg\n---"
  },
  {
    "objectID": "posts/code/2024-01-21-quarto-advanced.html#callout-block",
    "href": "posts/code/2024-01-21-quarto-advanced.html#callout-block",
    "title": "👩‍💻Quarto Blog + α",
    "section": "Callout Block",
    "text": "Callout Block\nCallout Block은 포스팅에서 따로 강조하고 싶거나 토글(Toggling)하고 싶은 내용이 있을 때 사용하면 유용한 작성 방법입니다. Quarto Docs의 Callout Blocks에서 볼 수 있듯이, note, warning, important, tip, caution과 같이 글의 메인 내용과 다른 포인트를 가진 내용이거나 강조하고 싶은 파트를 작성하여 보여줄 수 있습니다.\n::: {.callout-caution collapse=\"true\"}\n# Callout Block 예시\nQuarto의 Callout Block을 활용해봅시다! 생각보다 정말 유용하게 잘 쓰는 기능입니다 :)\n:::\n이 코드 스니펫을 이용하여 글에 삽입하면 아래와 같이 나옵니다. 한번 아래 Block을 클릭해보세요!\n\n\n\n\n\n\nCallout Block 예시\n\n\n\n\n\nQuarto의 Callout Block을 활용해봅시다! 생각보다 정말 유용하게 잘 쓰는 기능입니다 :)\n\n\n\n예시에서 확인할 수 있다시피 collapse=\"true\"등과 같은 Callout Block에 해당하는 다양한 옵션들도 Docs에서 찾아볼 수 있으니 다양하게 시도해보세요!"
  },
  {
    "objectID": "posts/code/2024-01-21-quarto-advanced.html#수학-수식",
    "href": "posts/code/2024-01-21-quarto-advanced.html#수학-수식",
    "title": "👩‍💻Quarto Blog + α",
    "section": "수학 수식",
    "text": "수학 수식\n논문 리뷰와 같은 글들을 작성할 때 가끔식 수학 기호나 수식들을 작성해야 할 때가 있습니다. 그래서 블로그에서 수식이 잘 작성이 되는가도 블로그 플랫폼을 결정하는 중요한 기준이 되기도 합니다. Quarto의 처음 소개에서도 말씀드렸던 것처럼 수학, 과학적 언어와 글들에 친화적인 플랫폼이기 때문에 Quarto는 수식 삽입을 TeX 문법으로 작성하면 예쁘게 랜더링 되도록 만들어진 블로그 플랫폼입니다. 사실 Markdown 문법과 다르지 않습니다. Quarto Docs Equations에서 볼 수 있듯이 $로 수식을 warpping하면 수식이 잘 랜더링 되는 것을 볼 수 있습니다.\nhtml에서 수식을 다양한 형식으로 rendering할 수 있도록 옵션도 제공하고 있습니다. Format Options 테이블에 있는 html-math-method옵션을 활용해서 plain, webtex, gladtex, mathml, mathjax, katex 등과 같은 옵션을 사용할 수 있습니다. 예를들면 아래와 같이 _quarto.yml 설정 파일에 html-math-method: katex 한 줄 추가하면 수식 랜더링 스타일을 바꿀 수 있습니다.\nformat:\n  html:\n    html-math-method: katex"
  },
  {
    "objectID": "posts/code/2024-01-21-quarto-advanced.html#글자색-설정",
    "href": "posts/code/2024-01-21-quarto-advanced.html#글자색-설정",
    "title": "👩‍💻Quarto Blog + α",
    "section": "글자색 설정",
    "text": "글자색 설정\n글자색을 기본 테마에서 설정한 색이 아닌, 파란색, 빨간색 등 다른 색을 사용하고 싶다면 아래와 같이 html 문법을 활용하여 작성하면 됩니다.\n&lt;span style=\"color: blue\"&gt;파란색 글씨를 작성&lt;/span&gt;"
  },
  {
    "objectID": "posts/code/2024-01-21-quarto-advanced.html#toggle",
    "href": "posts/code/2024-01-21-quarto-advanced.html#toggle",
    "title": "👩‍💻Quarto Blog + α",
    "section": "Toggle",
    "text": "Toggle\n토글은 Callout Block을 활용해서 작성할 수도 있지만 기본 html 문법을 활용해서 아래와 같이 작성할 수도 있습니다.\n&lt;details&gt;\n&lt;summary&gt;&lt;b&gt;Toggle 제목&lt;/b&gt;&lt;/summary&gt;\nToggle 내용\n&lt;/details&gt;\n아래와 같이 Toggle 파트가 랜더링 됩니다.\n\n\nToggle 제목\n\nToggle 내용\n\n\nQuarto Blog(1)포스팅에서 Quarto로 기본적인 블로그를 만들고 나서 이번 포스팅의 여러 커스텀 옵션들을 활용하면 각자 원하는 Github Blog를 손쉽게 만들 수 있습니다. 그리고 기본적인 html의 문법을 따라가기 때문에 이번 포스팅에서 몇부분들은 html 문법에 익숙하신 분들에게는 당연한 내용이지 않을까 싶습니다. Quarto를 쓰지 않더라도 블로그를 만들 수 있는 방법은 많지만 처음 소개글에서 이야기 했던 제가 Quarto를 알게 된 배경과 니즈를 듣고 저와 같은 비슷한 상황이시라면 정말 좋은 툴이라고 생각하실 거라고 확신합니다. 만약 아직 Github Blog 초심자라면 Quarto를 한번 고려해보시는 것을 다시 한번 권장해드리며 이만 마치겠습니다.\n\nReference\n\nQuarto Homepage\nHypothesis"
  },
  {
    "objectID": "posts/code/2023-04-23-orbit-existing-scripts.html",
    "href": "posts/code/2023-04-23-orbit-existing-scripts.html",
    "title": "👩‍💻Orbit Existing Scripts",
    "section": "",
    "text": "Orbit은 IsaacLab으로 변경되었습니다.\nOrbit 시리즈의 두번째 포스팅으로 이번에는 Orbit에서 제공하는 scripts를 살펴보며 Orbit으로 어떤 프로그래밍을 할 수 있을지 살펴보겠습니다. 공식 Documents에서 Running existing scripts를 따라가보며 진행될 예정입니다."
  },
  {
    "objectID": "posts/code/2023-04-23-orbit-existing-scripts.html#modules",
    "href": "posts/code/2023-04-23-orbit-existing-scripts.html#modules",
    "title": "👩‍💻Orbit Existing Scripts",
    "section": "2.1 Modules",
    "text": "2.1 Modules\nOrbit이 기반하고 있는 Isaac Sim Simulator을 먼저 키기 위해 SimulationApp class를 이용하여 시뮬레이터 앱을 설정해줍니다. 이때 headless는 시뮬레이터의 GUI를 띄우지 않고 실행하는 옵션을 가리킵니다.\n\"\"\"Launch Isaac Sim Simulator first.\"\"\"\n\nfrom omni.isaac.kit import SimulationApp\n\nimport argparse\n# add argparse arguments\nparser = argparse.ArgumentParser(\"Welcome to Orbit: Omniverse Robotics Environments!\")\nparser.add_argument(\"--headless\", action=\"store_true\", default=False, help=\"Force display off at all times.\")\nargs_cli = parser.parse_args()\n\n# launch omniverse app\nconfig = {\"headless\": args_cli.headless}\nsimulation_app = SimulationApp(config)\n다음으로 import하는 orbit의 core모듈을 살펴보겠습니다.\n\nprim_utils : 현재 USD stage에 prim을 생성하기 위한 모듈. (자세한 prim에 대한 내용은 다음 포스팅에서 다루도록 하겠습니다. 우선 간단하게 시뮬레이터의 오브젝트들을 prim으로 본다고 생각하고 진행하겠습니다.)\nSimulationContext : 타임라인 관련 이벤트를 처리하고(simulator 일시 중지, 재생, 단계적 실행 또는 중지 등), stage를 구성하며(stage 단위나 상/하 방향과 같은 설정), physicsScene prim을 생성합니다(중력 방향 및 크기, 시뮬레이션 시간 간격 크기, 고급 솔버 알고리즘 설정과 같은 물리 시뮬레이션 매개 변수를 제공). 여기서 physicsScene prim은 물리 시뮬레이션을 위한 초기화 및 설정을 담당하는 객체를 말하며, SimulationContext는 이러한 physicsScene prim을 생성하고 물리 시뮬레이션 설정을 관리하며, 타임라인 관련 이벤트 처리와 스테이지 설정을 담당합니다.\nset_camera_view : stage에서 카메라 prim의 위치와 대상을 설정하고, 해당 prim의 경로를 지정합니다. 여기서 카메라 prim은 stage 카메라로 사용될 객체를 의미하며, 위치(location)와 대상(target)은 각각 카메라의 위치와 카메라가 바라보는 대상의 위치를 지정하는 것을 의미합니다. 경로(path)는 스테이지에서 해당 카메라 prim을 가리키는 이름이나 경로를 의미합니다.\n\n이외의 모듈중에서 kit_utils는 시뮬레이터에서 제공하는 ground를 불러오기 위한(create_ground_plane) 모듈이며 로봇의 발 위치 마커를 표시하기 위해 markers 모듈에서 PointMarker, StaticMarker를 불러옵니다.\n다음으로 robot 모듈에서 4족보행로봇들을 위한 configuration들을 불러올 수 있습니다.\n# from core\nimport omni.isaac.core.utils.prims as prim_utils\nfrom omni.isaac.core.simulation_context import SimulationContext\nfrom omni.isaac.core.utils.viewports import set_camera_view\n\nimport omni.isaac.orbit.utils.kit as kit_utils\nfrom omni.isaac.orbit.markers import PointMarker, StaticMarker\n\nfrom omni.isaac.orbit.robots.config.anymal import ANYMAL_B_CFG, ANYMAL_C_CFG\nfrom omni.isaac.orbit.robots.config.unitree import UNITREE_A1_CFG\nfrom omni.isaac.orbit.robots.legged_robot import LeggedRobot"
  },
  {
    "objectID": "posts/code/2023-04-23-orbit-existing-scripts.html#helpers",
    "href": "posts/code/2023-04-23-orbit-existing-scripts.html#helpers",
    "title": "👩‍💻Orbit Existing Scripts",
    "section": "2.2 Helpers",
    "text": "2.2 Helpers\nMain 코드에서 시뮬레이터의 Scene을 구성하기 위해서 Ground-plane, Lights들을 편하게 구성하기 위해서 helper 함수 design_scene()를 만들어줍니다. 앞서 import했던 kit_utils를 이용해서 ground를 불러오고 prim_utils를 이용하여 빛 설정을 해줍니다.\ndef design_scene():\n    \"\"\"Add prims to the scene.\"\"\"\n    # Ground-plane\n    kit_utils.create_ground_plane(\n        \"/World/defaultGroundPlane\",\n        static_friction=0.5,\n        dynamic_friction=0.5,\n        restitution=0.8,\n        improve_patch_friction=True,\n    )\n    # Lights-1\n    prim_utils.create_prim(\n        \"/World/Light/GreySphere\",\n        \"SphereLight\",\n        translation=(4.5, 3.5, 10.0),\n        attributes={\"radius\": 2.5, \"intensity\": 600.0, \"color\": (0.75, 0.75, 0.75)},\n    )\n    # Lights-2\n    prim_utils.create_prim(\n        \"/World/Light/WhiteSphere\",\n        \"SphereLight\",\n        translation=(-4.5, 3.5, 10.0),\n        attributes={\"radius\": 2.5, \"intensity\": 600.0, \"color\": (1.0, 1.0, 1.0)},\n    )"
  },
  {
    "objectID": "posts/code/2023-04-23-orbit-existing-scripts.html#main",
    "href": "posts/code/2023-04-23-orbit-existing-scripts.html#main",
    "title": "👩‍💻Orbit Existing Scripts",
    "section": "2.3 Main",
    "text": "2.3 Main\n메인 코드에서는 먼저 SimulationContext를 이용하여 시뮬레이터의 시간관련 설정 등을 진행합니다. dt가 0.005라는 것은 시간 단위가 second로 시간간격이 0.005초로 설정하는 것을 말합니다. 또한 backend는 이후 포스팅에서도 설명하겠지만 물리 시뮬레이터에서 반환되는 tensor들을 어떤 backend로 캐스팅할 것인지 설정하는 부분입니다. 현재 orbit에서는 torch만 지원하고 있습니다.\n시뮬레이터의 시점이 되는 카메라를 설정하고 각 로봇을 어디에 놓을지 정하는 translation 파라미터와 함께 spawning하고 마지막으로 helper 함수로 만들어주었던 design_scene()을 이용하여 ground와 light를 설정합니다.\n\"\"\"Imports all legged robots supported in Orbit and applies zero actions.\"\"\"\n\n# Load kit helper\nsim = SimulationContext(stage_units_in_meters=1.0, physics_dt=0.005, rendering_dt=0.005, backend=\"torch\")\n# Set main camera\nset_camera_view(eye=[3.5, 3.5, 3.5], target=[0.0, 0.0, 0.0])\n\n# Spawn things into stage\n# -- anymal-b\nrobot_b = LeggedRobot(cfg=ANYMAL_B_CFG)\nrobot_b.spawn(\"/World/Anymal_b/Robot_1\", translation=(0.0, -1.5, 0.65))\nrobot_b.spawn(\"/World/Anymal_b/Robot_2\", translation=(0.0, -0.5, 0.65))\n# -- anymal-c\nrobot_c = LeggedRobot(cfg=ANYMAL_C_CFG)\nrobot_c.spawn(\"/World/Anymal_c/Robot_1\", translation=(1.5, -1.5, 0.65))\nrobot_c.spawn(\"/World/Anymal_c/Robot_2\", translation=(1.5, -0.5, 0.65))\n# -- unitree a1\nrobot_a = LeggedRobot(cfg=UNITREE_A1_CFG)\nrobot_a.spawn(\"/World/Unitree_A1/Robot_1\", translation=(1.5, 0.5, 0.42))\nrobot_a.spawn(\"/World/Unitree_A1/Robot_2\", translation=(1.5, 1.5, 0.42))\n\n# design props\ndesign_scene()\n시뮬레이터를 초기화하는 reset을 먼저 진행해줍니다. 각 로봇의 handle 또한 초기화를 시켜줍니다. 로봇의 각 정보를 담기 위한 buffer도 reset을 해서 본격적인 실행을 준비합니다.\n# Play the simulator\nsim.reset()\n# Acquire handles\n# Initialize handles\nrobot_b.initialize(\"/World/Anymal_b/Robot.*\")\nrobot_c.initialize(\"/World/Anymal_c/Robot.*\")\nrobot_a.initialize(\"/World/Unitree_A1/Robot.*\")\n# Reset states\nrobot_b.reset_buffers()\nrobot_c.reset_buffers()\nrobot_a.reset_buffers()\n4족 보행 로봇은 제어를 할 때 발의 움직임이 매우 중요합니다. 발의 위치에 마커를 위치시켜서 발에 대한 정보를 얻기 위해 marker를 설정해줍니다. 발의 x, y, z 축을 시각화하기 위해서 StaticMarker를 이용하여 설정하고 발이 contact 포인트를 보기 위해 PointMarker를 설정합니다.\n# Debug visualization markers.\n# -- feet markers\nfeet_markers: List[StaticMarker] = list()\nfeet_contact_markers: List[PointMarker] = list()\n# iterate over robots\nfor robot_name in [\"Anymal_b\", \"Anymal_c\", \"Unitree_A1\"]:\n    # foot\n    marker = StaticMarker(f\"/World/Visuals/{robot_name}/feet\", 4 * robot_c.count, scale=(0.1, 0.1, 0.1))\n    feet_markers.append(marker)\n    # contact\n    marker = PointMarker(f\"/World/Visuals/{robot_name}/feet_contact\", 4 * robot_c.count, radius=0.035)\n    feet_contact_markers.append(marker)\n각 로봇은 action을 하게되고 이를 로봇을 제어한다고 볼 수 있습니다. get_default_dof_state()은 로봇의 각 객체에 있는 method로 각 로봇의 standing 자세에 대한 joint position(dof) 정보가 들어있습니다.\n# dummy action\nactions = torch.zeros(robot_a.count, robot_a.num_actions, device=robot_a.device)\n\n# Define simulation stepping\nsim_dt = sim.get_physics_dt()\nsim_time = 0.0\ncount = 0\n# Simulate physics\nwhile simulation_app.is_running():\n    # If simulation is stopped, then exit.\n    if sim.is_stopped():\n        break\n    # If simulation is paused, then skip.\n    if not sim.is_playing():\n        sim.step(render=not args_cli.headless)\n        continue\n    # reset\n    if count % 1000 == 0:\n        # reset counters\n        sim_time = 0.0\n        count = 0\n        # reset dof state\n        for robot in [robot_a, robot_b, robot_c]:\n            dof_pos, dof_vel = robot.get_default_dof_state()\n            robot.set_dof_state(dof_pos, dof_vel)\n            robot.reset_buffers()\n        # reset command\n        actions = torch.zeros(robot_a.count, robot_a.num_actions, device=robot_a.device)\n        print(\"&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Reset!\")\n    # apply actions\n    robot_b.apply_action(actions)\n    robot_c.apply_action(actions)\n    robot_a.apply_action(actions)\n    # perform step\n    sim.step()\n    # update sim-time\n    sim_time += sim_dt\n    count += 1\n시뮬레이터 초기화할 때 reset했던 buffer에 각 로봇의 정보를 담습니다. marker로 설정해두었던 foot_marker와 contact_marker도 각 로봇에서 불러와서 state update를 진행합니다.\n# note: to deal with timeline events such as stopping, we need to check if the simulation is playing\nif sim.is_playing():\n    # update buffers\n    robot_b.update_buffers(sim_dt)\n    robot_c.update_buffers(sim_dt)\n    robot_a.update_buffers(sim_dt)\n    # update marker positions\n    for foot_marker, contact_marker, robot in zip(\n        feet_markers, feet_contact_markers, [robot_b, robot_c, robot_a]\n    ):\n        # feet\n        foot_marker.set_world_poses(\n            robot.data.feet_state_w[..., 0:3].view(-1, 3), robot.data.feet_state_w[..., 3:7].view(-1, 4)\n        )\n        # contact sensors\n        contact_marker.set_world_poses(\n            robot.data.feet_state_w[..., 0:3].view(-1, 3), robot.data.feet_state_w[..., 3:7].view(-1, 4)\n        )\n        contact_marker.set_status(torch.where(robot.data.feet_air_time.view(-1) &gt; 0.0, 1, 2))"
  },
  {
    "objectID": "posts/code/2022-12-14-gpu-status.html",
    "href": "posts/code/2022-12-14-gpu-status.html",
    "title": "👩‍💻Linux GPU 상태 확인하기",
    "section": "",
    "text": "1. nvidia-smi\nwatch -d -n 0.5 nvidia-smi\n\nwatch : 명령어를 주기적으로 실행\n-d : 차이를 보여줌\n-n : 주기적으로 실행할 시간 간격\n\n\n\n2. gpustat\nsudo apt install gpustat\ngpustat -i\noptions\n--color : Force colored output (even when stdout is not a tty)\n--no-color : Suppress colored output\n-u, --show-user : Display username of the process owner\n-c, --show-cmd : Display the process name\n-p, --show-pid : Display PID of the process\n-P, --show-power : Display GPU power usage and/or limit (draw or draw,limit)\n-i, --interval : Run in watch mode (equivalent to watch gpustat) if given. Denotes interval between updates.\n--json : JSON Output (Experimental, #10)\n\n\n3. gpumonitor\n\nGithub mountassir/gmonitor에서 설치방법 확인\n\n$ cd gmonitor\n$ mkdir build\n$ cd build\n$ cmake ..\n$ make\n$ sudo make install\n\\# use default\ngmonitor\n\n\\# Monitor the most recent state only\ngmonitor -d 1\n\n\\# Monitor current and history states for 4 GPUs.\ngmonitor -d 0 -g 0123\n\n\\# Monitor both current and previous states for all GPUs, refresh every 3 seconds.\ngmonitor -d 0 -r 3\n\n\n4. glance\n\n설치 sudo apt-get install -y python-pip; sudo pip install glances[gpu]\n실행 sudo glances\n\n\n\n[+] Jupyter Lab에서 GPU 상태 확인하기\n\nhttps://developer.nvidia.com/blog/gpu-dashboards-in-jupyter-lab/\n\n$ pip install jupyterlab-nvdashboard\n\n# If you are using Jupyter Lab 2 you will also need to run\n$ jupyter labextension install jupyterlab-nvdashboard"
  },
  {
    "objectID": "posts/code/2022-12-21-torch-tensor-inverted.html",
    "href": "posts/code/2022-12-21-torch-tensor-inverted.html",
    "title": "👩‍💻torch.Tensor vs torch.tensor",
    "section": "",
    "text": "둘다 data(input)에 대해서 Tensor 객체로 만들어 주는 기능은 동일\n\n\n\n\ntorch.tensor() : function\n\ndocs: https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor\ntorch.tensor(data)에서 data는 필수 argument. (data 없을 시 TypeError)\ntorch.tensor는 항상 data를 복사\nint 입력 시 int 그대로 입력\n입력받은 데이터를 새로운 메모리 공간에 복사해 Tensor 객체 생성 (call by value)\n\ntorch.Tensor() : class\n\ndocs: https://pytorch.org/docs/stable/tensors.html#torch.Tensor\n이미 생성된 객체를 tensor로 바꾸고 싶을 때 사용\n빈 Tensor 객체를 만들 때 사용\nint 입력 시 float로 변환\nTensor 객체로 데이터를 입력할 경우 입력 받은 메모리 공간 그대로 사용 (call by reference)\nlist나 numpy 등 다른 자료형으로 입력 받을 경우 값을 복사하여 Tensor 객체 생성(call by value)\n\n\n\n\n\n\nimport torch\n\n\noriginal_data = torch.tensor([1])\nnew_data = torch.tensor(original_data)\nprint(f\"original : {original_data} {original_data.dtype}\")\nprint(f\"new : {new_data} {new_data.dtype}\")\n\n \n# original data를 수정\noriginal_data[0] = 2\nprint(f\"original : {original_data} {original_data.dtype}\")\nprint(f\"new : {new_data} {new_data.dtype}\")\n\noriginal : tensor([1]) torch.int64\nnew : tensor([1]) torch.int64\noriginal : tensor([2]) torch.int64\nnew : tensor([1]) torch.int64\n\n\n/tmp/ipykernel_847781/1124094978.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  new_data = torch.tensor(original_data)\n\n\n\nint에서 int로\ncall by value, 새로운 Tensor 객체이므로 변화 x\n\n\n\n\n\noriginal_data = torch.Tensor([1])\nnew_data = torch.Tensor(original_data)\nprint(f\"original : {original_data} {original_data.dtype}\")\nprint(f\"new : {new_data} {new_data.dtype}\")\n\n \n# original data를 수정\noriginal_data[0] = 2\nprint(f\"original : {original_data} {original_data.dtype}\")\nprint(f\"new : {new_data} {new_data.dtype}\")\n\noriginal : tensor([1.]) torch.float32\nnew : tensor([1.]) torch.float32\noriginal : tensor([2.]) torch.float32\nnew : tensor([2.]) torch.float32\n\n\n\nint에서 float으로\ncall by reference, shallow copy\n\n\noriginal_data = [1]\nnew_data = torch.Tensor(original_data)\nprint(f\"original : {original_data}\")\nprint(f\"new : {new_data} {new_data.dtype}\")\n\n# original data를 수정\noriginal_data[0] = 2\nprint(f\"original : {original_data}\")\nprint(f\"new : {new_data} {new_data.dtype}\")\n\noriginal : [1]\nnew : tensor([1.]) torch.float32\noriginal : [2]\nnew : tensor([1.]) torch.float32\n\n\n\nint에서 float으로\ncall by value, deep copy"
  },
  {
    "objectID": "posts/code/2022-12-21-torch-tensor-inverted.html#공통점",
    "href": "posts/code/2022-12-21-torch-tensor-inverted.html#공통점",
    "title": "👩‍💻torch.Tensor vs torch.tensor",
    "section": "",
    "text": "둘다 data(input)에 대해서 Tensor 객체로 만들어 주는 기능은 동일"
  },
  {
    "objectID": "posts/code/2022-12-21-torch-tensor-inverted.html#차이점",
    "href": "posts/code/2022-12-21-torch-tensor-inverted.html#차이점",
    "title": "👩‍💻torch.Tensor vs torch.tensor",
    "section": "",
    "text": "torch.tensor() : function\n\ndocs: https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor\ntorch.tensor(data)에서 data는 필수 argument. (data 없을 시 TypeError)\ntorch.tensor는 항상 data를 복사\nint 입력 시 int 그대로 입력\n입력받은 데이터를 새로운 메모리 공간에 복사해 Tensor 객체 생성 (call by value)\n\ntorch.Tensor() : class\n\ndocs: https://pytorch.org/docs/stable/tensors.html#torch.Tensor\n이미 생성된 객체를 tensor로 바꾸고 싶을 때 사용\n빈 Tensor 객체를 만들 때 사용\nint 입력 시 float로 변환\nTensor 객체로 데이터를 입력할 경우 입력 받은 메모리 공간 그대로 사용 (call by reference)\nlist나 numpy 등 다른 자료형으로 입력 받을 경우 값을 복사하여 Tensor 객체 생성(call by value)"
  },
  {
    "objectID": "posts/code/2022-12-21-torch-tensor-inverted.html#torch.tensor",
    "href": "posts/code/2022-12-21-torch-tensor-inverted.html#torch.tensor",
    "title": "👩‍💻torch.Tensor vs torch.tensor",
    "section": "",
    "text": "import torch\n\n\noriginal_data = torch.tensor([1])\nnew_data = torch.tensor(original_data)\nprint(f\"original : {original_data} {original_data.dtype}\")\nprint(f\"new : {new_data} {new_data.dtype}\")\n\n \n# original data를 수정\noriginal_data[0] = 2\nprint(f\"original : {original_data} {original_data.dtype}\")\nprint(f\"new : {new_data} {new_data.dtype}\")\n\noriginal : tensor([1]) torch.int64\nnew : tensor([1]) torch.int64\noriginal : tensor([2]) torch.int64\nnew : tensor([1]) torch.int64\n\n\n/tmp/ipykernel_847781/1124094978.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  new_data = torch.tensor(original_data)\n\n\n\nint에서 int로\ncall by value, 새로운 Tensor 객체이므로 변화 x"
  },
  {
    "objectID": "posts/code/2022-12-21-torch-tensor-inverted.html#torch.tensor-1",
    "href": "posts/code/2022-12-21-torch-tensor-inverted.html#torch.tensor-1",
    "title": "👩‍💻torch.Tensor vs torch.tensor",
    "section": "",
    "text": "original_data = torch.Tensor([1])\nnew_data = torch.Tensor(original_data)\nprint(f\"original : {original_data} {original_data.dtype}\")\nprint(f\"new : {new_data} {new_data.dtype}\")\n\n \n# original data를 수정\noriginal_data[0] = 2\nprint(f\"original : {original_data} {original_data.dtype}\")\nprint(f\"new : {new_data} {new_data.dtype}\")\n\noriginal : tensor([1.]) torch.float32\nnew : tensor([1.]) torch.float32\noriginal : tensor([2.]) torch.float32\nnew : tensor([2.]) torch.float32\n\n\n\nint에서 float으로\ncall by reference, shallow copy\n\n\noriginal_data = [1]\nnew_data = torch.Tensor(original_data)\nprint(f\"original : {original_data}\")\nprint(f\"new : {new_data} {new_data.dtype}\")\n\n# original data를 수정\noriginal_data[0] = 2\nprint(f\"original : {original_data}\")\nprint(f\"new : {new_data} {new_data.dtype}\")\n\noriginal : [1]\nnew : tensor([1.]) torch.float32\noriginal : [2]\nnew : tensor([1.]) torch.float32\n\n\n\nint에서 float으로\ncall by value, deep copy"
  },
  {
    "objectID": "posts/code/2022-12-21-torch-tensor-inverted.html#torch.tensor-2",
    "href": "posts/code/2022-12-21-torch-tensor-inverted.html#torch.tensor-2",
    "title": "👩‍💻torch.Tensor vs torch.tensor",
    "section": "torch.tensor()",
    "text": "torch.tensor()\n\na = torch.tensor([0,1,10])\nprint(f\"Shape of tensor: {a.shape}\")\nprint(f\"Datatype of tensor: {a.dtype}\")\nprint(f\"Device tensor is stored on: {a.device}\")\n\nShape of tensor: torch.Size([3])\nDatatype of tensor: torch.int64\nDevice tensor is stored on: cpu\n\n\n\n~a\n\ntensor([ -1,  -2, -11])\n\n\n\nb = torch.tensor([True, False, True])\nprint(f\"Shape of tensor: {b.shape}\")\nprint(f\"Datatype of tensor: {b.dtype}\")\nprint(f\"Device tensor is stored on: {b.device}\")\n\nShape of tensor: torch.Size([3])\nDatatype of tensor: torch.bool\nDevice tensor is stored on: cpu\n\n\n\n~b\n\ntensor([False,  True, False])\n\n\n\nc = torch.tensor([0.13, 1.5, -.116])\nprint(f\"Shape of tensor: {c.shape}\")\nprint(f\"Datatype of tensor: {c.dtype}\")\nprint(f\"Device tensor is stored on: {c.device}\")\n\nShape of tensor: torch.Size([3])\nDatatype of tensor: torch.float32\nDevice tensor is stored on: cpu\n\n\n\n~c\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nInput In [10], in &lt;cell line: 1&gt;()\n----&gt; 1 ~c\n\nTypeError: ~ (operator.invert) is only implemented on integer and Boolean-type tensors"
  },
  {
    "objectID": "posts/code/2022-12-21-torch-tensor-inverted.html#torch.tensor-3",
    "href": "posts/code/2022-12-21-torch-tensor-inverted.html#torch.tensor-3",
    "title": "👩‍💻torch.Tensor vs torch.tensor",
    "section": "torch.Tensor()",
    "text": "torch.Tensor()\n\na = torch.Tensor([0,1,10])\nprint(f\"Shape of tensor: {a.shape}\")\nprint(f\"Datatype of tensor: {a.dtype}\")\nprint(f\"Device tensor is stored on: {a.device}\")\n\nShape of tensor: torch.Size([3])\nDatatype of tensor: torch.float32\nDevice tensor is stored on: cpu\n\n\n\na\n\ntensor([ 0.,  1., 10.])\n\n\n\n~a\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nInput In [13], in &lt;cell line: 1&gt;()\n----&gt; 1 ~a\n\nTypeError: ~ (operator.invert) is only implemented on integer and Boolean-type tensors\n\n\n\n\nb = torch.Tensor([True, False, True])\nprint(f\"Shape of tensor: {b.shape}\")\nprint(f\"Datatype of tensor: {b.dtype}\")\nprint(f\"Device tensor is stored on: {b.device}\")\n\nShape of tensor: torch.Size([3])\nDatatype of tensor: torch.float32\nDevice tensor is stored on: cpu\n\n\n\nb\n\ntensor([1., 0., 1.])\n\n\n\n~b\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nInput In [16], in &lt;cell line: 1&gt;()\n----&gt; 1 ~b\n\nTypeError: ~ (operator.invert) is only implemented on integer and Boolean-type tensors\n\n\n\n\nc = torch.Tensor([0.13, 1.5, -.116])\nprint(f\"Shape of tensor: {c.shape}\")\nprint(f\"Datatype of tensor: {c.dtype}\")\nprint(f\"Device tensor is stored on: {c.device}\")\n\nShape of tensor: torch.Size([3])\nDatatype of tensor: torch.float32\nDevice tensor is stored on: cpu\n\n\n\nc\n\ntensor([ 0.1300,  1.5000, -0.1160])\n\n\n\n~c\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nInput In [19], in &lt;cell line: 1&gt;()\n----&gt; 1 ~c\n\nTypeError: ~ (operator.invert) is only implemented on integer and Boolean-type tensors\n\n\n\n결론: torch.tensor()로만 ~연산 사용 가능\nReference\n\ntorch.Tensor와 torch.tensor의 차이"
  },
  {
    "objectID": "posts/code/2020-07-13-install-mujoco-win10.html",
    "href": "posts/code/2020-07-13-install-mujoco-win10.html",
    "title": "👩‍💻Install Mujoco in Windows10",
    "section": "",
    "text": "Mujoco\n\nMujoco에서 License tab으로 들어가서 Personal Student Software License를 받는다.\n이메일로 Account number를 받는다.(spam도 확인하자)\n다시 License로 가서 Account number와 Computer Id를 입력한다.\n\n\n\n\n\n다시 이메일로 MuJoCo Pro Personal Activation Key(mjkey.txt)를 받는다.\n.mujoco라는 파일을 C:\\Users\\사용자명\\에 만든다.\nProducts에서 mjpro150 win64을 다운받아 .mujoco에 압축을 풀어준다.(C:\\Users\\사용자명\\.mujoco\\mjpro150)\n\n\n\n\n\n다운받은 mjkey.txt 파일을 C:\\Users\\사용자명\\.mujoco와 C:\\Users\\사용자명\\.mujoco\\mjpro150\\bin 에 옮긴다.\ncmd창을 열어서 cd 명령어로 C:\\Users\\사용자명\\.mujoco\\mjpro150\\bin 경로로 이동한 다음 simulate ../model/humanoid.xml명령어를 입력한다.\nHumanoid Simulation 창이 나오면 성공이다.\n\n\n\n\n\n\n\n\nMujoco-py\n\nSET PATH=C:\\Users\\사용자명\\.mujoco\\mjpro150\\bin;%PATH%;으로 경로설정을 해준다.\nmujoco-py-1.50.1.0 파일을 다운받아 C:\\Users\\사용자명\\.mujoco에 압축을 풀어준다.(C:\\Users\\사용자명\\.mujoco\\mujoco-py-1.50.1.0)\ncmd 창에서 python setup.py install를 입력하여 설치한다.\nC:\\Users\\사용자명\\.mujoco\\mujoco-py-1.50.1.0&gt; python setup.py install\n혹시 여기서 error가 난다면 전에 mujoco-py를 설치해서 버젼이 안 맞아 나는 것일 수도 있다. pip list로 mujoco-py의 버젼을 확인해보고 다른 버젼이라면 pip uninstall mujoco-py를 해준후 다시 설치한다.\ncmd 창에서 python examples\\body_interaction.py를 입력하여 잘 실행되는지 확인한다.\nC:\\Users\\사용자명\\.mujoco\\mujoco-py-1.50.1.0&gt; python examples\\body_interaction.py\n처음에 실행화면이 뜨는 시간이 오래걸리지만 이후에는 실행창이 빨리 나왔다.\n\n\n\n\n\n\n\nReference\n\nMuJoCo 설치 (윈도우 10 version)"
  },
  {
    "objectID": "posts/note/2021-01-31-ML-tu-berlin.html",
    "href": "posts/note/2021-01-31-ML-tu-berlin.html",
    "title": "📘2021 TU Berlin Winter Course 수강 후기",
    "section": "",
    "text": "사실 ML/DL 카테고리에는 공부내용만 적을려고 했으나 관련 수업을 들은 후기도 여기에 정리하면 좋을 것 같아 여기에 분류했다. 후에 수업에서 배운 내용들도 차근차근히 정리해볼 예정이다.\n\n\n학교에서 지원해주는 교환학생이라던가 해외 협업 수업은 언제나 하고 싶었지만 항상 발목을 잡은 건 영어였던 것 같다. 영어..읽고 쓰는 것도 좋아하고 듣는 것도 다 알아듣진 못해도 어느정도는 되는데 항상 ’말하기’가 문제였다. 사실 천성 내향형 인간이라 한국어로도 말은 원래 잘 못하는데 영어는 더더욱 자신이 없었다. 게다가 학교에서 지원하는 프로그램들은 내가 영어를 잘하는지 증명을 해야하는데 토플 준비만했을 뿐 정작 시험은 안봤다. 계속 다른 일에 우선순위가 밀리고 시험비도 그냥 한번 시도해보기엔 만만치 않아서가 변명이라면 변명이었다. 어쨌든 자격증이 없으니 지원자격자체가 안됬었다.\n그런데 영어 자격증이 없어도 지원할 수 있는 프로그램이 하나 생겼고 코로나로 인해 온라인이긴 했지만 지원했다. 마지막 4학년만 남았기에 이번이 마지막일 수도 있다는 생각도 있었다. 들을 수 있는 수업이 4개가 있었는데 데이터 사이언스/시각화분야 수업과 머신러닝 이론/적용분야 수업 중에 고민을 했다. 결과적으로는 머신러닝 이론/적용을 선택했다. 이유는 아래와 같았다.\n\nSyllabus에 이미 알고 있는 내용들이 좀 더 많았기에 혹시 놓치더라도 뒤쳐지지 않을 것 같았음 + 영어로 소통하는 것에 좀 더 집중할 수 있음\nFeature Engineering 부분이나 Unsupervised Learning과 같은 실제 적용은 많이 해보지 않은 부분들을 다지는 기회\nLecturer 중에 전공분야가 Reinforcement Learning이신 분이 있어서 (혹시라도 친해지면) 이것저것 물어볼 수 있지 않을까라는 생각\n\n그래서 학교에 지원을 넣고 합격이 될까 안될까 걱정반 설렘반이었는데, 담당 교수님과 편하게 카톡영상통화로 면접을 보고 난후 흔쾌히 허락해주셨다. 다행이었다. 이제 독일 학교에 지원서를 작성하는데 문제가 하나 있었다. 지원서 영어실력을 증명하라는 칸이 있었는데 위에 써놓은 것과 같이 난 자격증이 없었기에 뭘 제출하라는 말인지 이해하지 못했다. 근데 컨택해주신 교수님이 분명 영어 자격증없어도 가능하다고 하셨던 것 같은데..뭘까…고민을 하다가 학교에서 이수한 과목들 목록을 뽑았다. 그중에는 영국인 교수님이 수업하신 (물론 영어로 진행된) 영국문화 알아가기라는 교양 수업도 있었고, 필수 이수과목인 대학영어, 전공과목이지만 영어로 진행한 수업들도 있었다. 그래서 학기별 내가 들은 ’영어’와 관련된 수업 목록들을 하나하나 설명하는 보고서(?)를 작성하여 “이런 영어로 진행된 수업들을 잘 이수하였으니 난 영어로 소통가능하다”라고 어필했다. 다행히도 그 보고서로 나의 영어 실력이 증명이 되어서 간신히 수업등록을 마칠 수 있었다. 이렇게 우여곡절 끝에 수업등록을 하니 영어자격증을 빨리 따야겠다는 생각만 더 절실해졌다.😂(이제 조금 여유가 생긴 2월달에 토플 시험 볼 예정이다.)\n\n\n\n\n수업이 시작되고 역시나 걱정되는 건 영어로하는 의사소통이었다. 급하게 영어 말하기 세포를 깨우기 위해 Facebook 지인분들께 방법도 여쭤보고 혼자서 엄청 중얼중얼 거렸다. 단기간에 실력적인 향상 효과는 미미했지만 마음의 준비는 할 수 있었다. 근데 확실히 수업이 막상 시작하니 난관은 말하기보다 듣기에 있었다. 다양한 나라의 다양한 억양은 듣기 힘들다는 이야기를 듣긴 했어도 직접 체험해보니 잘 안들리는 황당함이란.. 다양한 나라의 사람들과 소통하고 싶다는 포부로 시작했던 처음 마음과 달리, 수업하는 동안은 ’잘’듣는 것만으로도 나에게 도전이었던 것 같다. 그런데 한편으로는 완벽한 영어를 구성하지 않는, 나와같이 영어가 제2외국어이신 분들과 수업을 하니 말할때는 부담없이 나도 막(?)말했던 것 같다. 어쨌든 걱정했던 ’말하기’는 생각보다 난관이 아니었고, ’듣기’가 난관이었다는 이야기다. 같이 수업을 듣는 분들은 약 20명 정도 였는데 첫시간에 자기소개할 때 들어보니 다들 박사과정이거나 직장인이었다..?!😲 학사도 졸업 못한 나랑은 너무나 대비되는 분들이라 신기하기도 하고 걱정도 됐다. (학교는 왜 이런 프로그램인지 왜 안알려줬지) 그분들의 능력은 나중에 프로젝트 발표하실 때 확연히 느낄 수 있었는데 프로젝트 완성도나 주제가 넘사벽이었다.\n수업은 대부분 강의 시청 + exercise + 질의 응답으로 이루어졌기에 내가 적극적으로 이야기하고 질문할 부분이 많진 않았다. 그리고 앞서 적었던 것처럼 수업내용이 나한테는 완전히 새로운 분야는 아니어서 그런지 그럭저럭 따라갈 수 있었다. (벌써 수업 마쳤다고 기억이 미화된건지는 모르겠지만) 그리고 모르는게 있으면 구글링으로 잘 정리해놓은 훌륭하신 한국인 분들의 포스팅, 가지고 있는 책들로 메꿔갔기에 수업을 따라가는 건 많이 어렵진 않았던 것 같다. 진짜 다른분들이 잘 정리해놓으신 포스팅에 도움을 많이 받아서 항상 드는 생각이지만 언젠가는 나도 저분들처럼 도움되는 포스팅을 하고 싶다라는 생각이 든다.\n사실 등록 전에는 몰랐던 Group Work는 수업계획서에서 처음 봤을 때 당황스러웠다. 그러나 걱정됐던 부분과 달리 팀구성도 잘됐고, 내가 주도해서 그런지 주제도 내가 원하는 걸로 진행하게되었다. 워낙 짧은 시간(3일 정도)에 완성해야하는 프로젝트였기에 마음과 달리 여러가지 기법들을 사용하기 보다는 심플하고 충실하게 ‘배운것만’ 응용해서 프로젝트를 마무리했다. 그래도 팀원분들과 영어로 의견 조율하는게 잠깐이었지만 재밌었고, 이번 경험으로 영어가 더이상 공포의 대상이 아닌 세상 다른 사람들과 ’소통’할 수 있는 도구라는 생각이 확실히 각인된 것 같다. 정말 ’언어’라는 생각을 체험해보는 기회였달까. 그렇게 더듬더듬거리는 영어지만 의견과 생각을 나누는 경험은 소중했고 마지막 프로젝트까지 잘 마칠 수 있었다. 나중에 Lecturer 분들께서 코멘트 해주실 때도 프로젝트 지적을 당했다라는 생각보다, 진정으로 우리 프로젝트를 봐주시고 인정해주신다는 생각이 들어서 감사했다. 그리고 사실은 진짜로 우리팀이 프로젝트에서 중요한 포인트를 빼먹었었는데 짚어주시는 거 보고 많이 배웠다.(한편으로는 말도 안되는 영어로 발표하는 거 들으시고도 이해하셨다는게 신기했다🤣) 아쉽게도 발표할 때가 처음이자 마지막으로 Lecturer 분들과 이야기한 순간이었다.(아 첫시간에 자기소개 빼고!) 프로젝트 깃헙은 여기에 있다.\n수업 마지막에 봤던 시험은 생각보다 어렵지 않았기에 사실 후기에 적을 내용이 별로 없다. 한국 시험에서는 선택형이어도 꼬아서 내거나 복수 정답지들을 만들어서 난이도를 높이는데, 여기 수업은 깔끔하게 중복 답안지 없다라고 말해주고 선택지들도 기본중의 기본이었다. 그나마 분별력을 가질 수 있는 부분은 프로그래밍 파트였으나 이 부분마저 코드를 외워서 치는게 아니라 오픈북이어서 그렇게 어렵진 않았던 것 같다. 게다가 시험시간은 3시간이었기에 타임 리밋도 거의 없는거나 다름없었다. 물론 시험보기 전에는 시험이 어떻게 나올지 예측할 수 없었기에 전날밤을 새워가며 공부하긴 했었다.\n\n\n\n마지막 날에 발표를 마치고 Lecturer분들의 ML/DL 트렌드에 관한 짧은 발표와 같이 수업들은 분들과 약간의 담소를 나누며 훈훈하게 수업이 끝났다. 약 3주간동안 저녁 시간에 열심히 배웠던 수업이 막을 내렸다.\n항상 가장 괴로울 때 가장 많이 배우는 것 같다. 사실 지나고 보면 그 고통이 어느정도 미화되는 것도 있고 위에 ‘어렵지 않았다.’, ’잘 마무리됐다.’라고 써놓기도 했지만, 그걸 하고 있던 순간에는 힘들었다. 아무리 알고 있던 내용이들이라도 그 내용들을 다시 다지는 것도 어렵고, 직접 코드를 써보는 것도 어렵고, 마무리 하기까지 불확실성과 두려움 때문에 괴로웠던 건 팩트다. 근데 항상 그러면서 배우는 것 같다. 그래서 이번 코스를 통해 배운 걸 정리해보자면,\n\nML은 확실히 DL보다 수학적인 이론이 어려웠음\n영어가 시험이나 부담이 아닌 ’언어’라는 것. 나를 표현할 수 있는 툴이라는 것\n전에 조금이라도 공부했던 것들이 나중에 도움이 ’크게’될 수 있다는 것\n내가 알고있는 것들을 꾸준하게 포스팅하는 습관을 길러야 하겠다는 생각\n해보지 않고 추측만으로 하는 생각들로 두려워하지 말 것\n\n수업을 마치고 certificate를 받았다. 사실 certificate에는 옵션이 있었는데, grade를 받을 것인지/pass or fail을 받을 것인지 내가 선택할 수 있었다. 등록 당시엔 근자감으로 grade를 받겠다고 했다가 수업 듣는 동안 과거의 나를 반성했다. 사실 수업하는 동안 그렇게 적극적이지도 않았고, 프로젝트 발표를 하고 지적을 받으면서 든 생각은 ‘최고 grade까지는 못받고 중간이라도 했으면..’ 했다. 그런데 생각지 못했던 최고 grade를 받을 수 있었다. 학점이 다가 아니긴 하지만 그래도 최선을 다했다는 것을 어느정도 뒷받침해줄 수 있기 때문에 뿌듯했다. 그래서 위에 배운 점들에 하나 더 추가하자면, ’최선을 다해서 어찌저찌해보면 뭐라도 한다.’라는 것. 식상한 멘트와 교훈이지만 매번 도전하면서 느끼는 바이기도 하다. 이렇게 2021년도 1월은 TU Berlin의 Machine Learning using Python: Theory and Application 코스와 함께 시간을 보냈다."
  },
  {
    "objectID": "posts/note/2021-01-31-ML-tu-berlin.html#tu-berlin-winter-university-online-2021",
    "href": "posts/note/2021-01-31-ML-tu-berlin.html#tu-berlin-winter-university-online-2021",
    "title": "📘2021 TU Berlin Winter Course 수강 후기",
    "section": "",
    "text": "사실 ML/DL 카테고리에는 공부내용만 적을려고 했으나 관련 수업을 들은 후기도 여기에 정리하면 좋을 것 같아 여기에 분류했다. 후에 수업에서 배운 내용들도 차근차근히 정리해볼 예정이다.\n\n\n학교에서 지원해주는 교환학생이라던가 해외 협업 수업은 언제나 하고 싶었지만 항상 발목을 잡은 건 영어였던 것 같다. 영어..읽고 쓰는 것도 좋아하고 듣는 것도 다 알아듣진 못해도 어느정도는 되는데 항상 ’말하기’가 문제였다. 사실 천성 내향형 인간이라 한국어로도 말은 원래 잘 못하는데 영어는 더더욱 자신이 없었다. 게다가 학교에서 지원하는 프로그램들은 내가 영어를 잘하는지 증명을 해야하는데 토플 준비만했을 뿐 정작 시험은 안봤다. 계속 다른 일에 우선순위가 밀리고 시험비도 그냥 한번 시도해보기엔 만만치 않아서가 변명이라면 변명이었다. 어쨌든 자격증이 없으니 지원자격자체가 안됬었다.\n그런데 영어 자격증이 없어도 지원할 수 있는 프로그램이 하나 생겼고 코로나로 인해 온라인이긴 했지만 지원했다. 마지막 4학년만 남았기에 이번이 마지막일 수도 있다는 생각도 있었다. 들을 수 있는 수업이 4개가 있었는데 데이터 사이언스/시각화분야 수업과 머신러닝 이론/적용분야 수업 중에 고민을 했다. 결과적으로는 머신러닝 이론/적용을 선택했다. 이유는 아래와 같았다.\n\nSyllabus에 이미 알고 있는 내용들이 좀 더 많았기에 혹시 놓치더라도 뒤쳐지지 않을 것 같았음 + 영어로 소통하는 것에 좀 더 집중할 수 있음\nFeature Engineering 부분이나 Unsupervised Learning과 같은 실제 적용은 많이 해보지 않은 부분들을 다지는 기회\nLecturer 중에 전공분야가 Reinforcement Learning이신 분이 있어서 (혹시라도 친해지면) 이것저것 물어볼 수 있지 않을까라는 생각\n\n그래서 학교에 지원을 넣고 합격이 될까 안될까 걱정반 설렘반이었는데, 담당 교수님과 편하게 카톡영상통화로 면접을 보고 난후 흔쾌히 허락해주셨다. 다행이었다. 이제 독일 학교에 지원서를 작성하는데 문제가 하나 있었다. 지원서 영어실력을 증명하라는 칸이 있었는데 위에 써놓은 것과 같이 난 자격증이 없었기에 뭘 제출하라는 말인지 이해하지 못했다. 근데 컨택해주신 교수님이 분명 영어 자격증없어도 가능하다고 하셨던 것 같은데..뭘까…고민을 하다가 학교에서 이수한 과목들 목록을 뽑았다. 그중에는 영국인 교수님이 수업하신 (물론 영어로 진행된) 영국문화 알아가기라는 교양 수업도 있었고, 필수 이수과목인 대학영어, 전공과목이지만 영어로 진행한 수업들도 있었다. 그래서 학기별 내가 들은 ’영어’와 관련된 수업 목록들을 하나하나 설명하는 보고서(?)를 작성하여 “이런 영어로 진행된 수업들을 잘 이수하였으니 난 영어로 소통가능하다”라고 어필했다. 다행히도 그 보고서로 나의 영어 실력이 증명이 되어서 간신히 수업등록을 마칠 수 있었다. 이렇게 우여곡절 끝에 수업등록을 하니 영어자격증을 빨리 따야겠다는 생각만 더 절실해졌다.😂(이제 조금 여유가 생긴 2월달에 토플 시험 볼 예정이다.)\n\n\n\n\n수업이 시작되고 역시나 걱정되는 건 영어로하는 의사소통이었다. 급하게 영어 말하기 세포를 깨우기 위해 Facebook 지인분들께 방법도 여쭤보고 혼자서 엄청 중얼중얼 거렸다. 단기간에 실력적인 향상 효과는 미미했지만 마음의 준비는 할 수 있었다. 근데 확실히 수업이 막상 시작하니 난관은 말하기보다 듣기에 있었다. 다양한 나라의 다양한 억양은 듣기 힘들다는 이야기를 듣긴 했어도 직접 체험해보니 잘 안들리는 황당함이란.. 다양한 나라의 사람들과 소통하고 싶다는 포부로 시작했던 처음 마음과 달리, 수업하는 동안은 ’잘’듣는 것만으로도 나에게 도전이었던 것 같다. 그런데 한편으로는 완벽한 영어를 구성하지 않는, 나와같이 영어가 제2외국어이신 분들과 수업을 하니 말할때는 부담없이 나도 막(?)말했던 것 같다. 어쨌든 걱정했던 ’말하기’는 생각보다 난관이 아니었고, ’듣기’가 난관이었다는 이야기다. 같이 수업을 듣는 분들은 약 20명 정도 였는데 첫시간에 자기소개할 때 들어보니 다들 박사과정이거나 직장인이었다..?!😲 학사도 졸업 못한 나랑은 너무나 대비되는 분들이라 신기하기도 하고 걱정도 됐다. (학교는 왜 이런 프로그램인지 왜 안알려줬지) 그분들의 능력은 나중에 프로젝트 발표하실 때 확연히 느낄 수 있었는데 프로젝트 완성도나 주제가 넘사벽이었다.\n수업은 대부분 강의 시청 + exercise + 질의 응답으로 이루어졌기에 내가 적극적으로 이야기하고 질문할 부분이 많진 않았다. 그리고 앞서 적었던 것처럼 수업내용이 나한테는 완전히 새로운 분야는 아니어서 그런지 그럭저럭 따라갈 수 있었다. (벌써 수업 마쳤다고 기억이 미화된건지는 모르겠지만) 그리고 모르는게 있으면 구글링으로 잘 정리해놓은 훌륭하신 한국인 분들의 포스팅, 가지고 있는 책들로 메꿔갔기에 수업을 따라가는 건 많이 어렵진 않았던 것 같다. 진짜 다른분들이 잘 정리해놓으신 포스팅에 도움을 많이 받아서 항상 드는 생각이지만 언젠가는 나도 저분들처럼 도움되는 포스팅을 하고 싶다라는 생각이 든다.\n사실 등록 전에는 몰랐던 Group Work는 수업계획서에서 처음 봤을 때 당황스러웠다. 그러나 걱정됐던 부분과 달리 팀구성도 잘됐고, 내가 주도해서 그런지 주제도 내가 원하는 걸로 진행하게되었다. 워낙 짧은 시간(3일 정도)에 완성해야하는 프로젝트였기에 마음과 달리 여러가지 기법들을 사용하기 보다는 심플하고 충실하게 ‘배운것만’ 응용해서 프로젝트를 마무리했다. 그래도 팀원분들과 영어로 의견 조율하는게 잠깐이었지만 재밌었고, 이번 경험으로 영어가 더이상 공포의 대상이 아닌 세상 다른 사람들과 ’소통’할 수 있는 도구라는 생각이 확실히 각인된 것 같다. 정말 ’언어’라는 생각을 체험해보는 기회였달까. 그렇게 더듬더듬거리는 영어지만 의견과 생각을 나누는 경험은 소중했고 마지막 프로젝트까지 잘 마칠 수 있었다. 나중에 Lecturer 분들께서 코멘트 해주실 때도 프로젝트 지적을 당했다라는 생각보다, 진정으로 우리 프로젝트를 봐주시고 인정해주신다는 생각이 들어서 감사했다. 그리고 사실은 진짜로 우리팀이 프로젝트에서 중요한 포인트를 빼먹었었는데 짚어주시는 거 보고 많이 배웠다.(한편으로는 말도 안되는 영어로 발표하는 거 들으시고도 이해하셨다는게 신기했다🤣) 아쉽게도 발표할 때가 처음이자 마지막으로 Lecturer 분들과 이야기한 순간이었다.(아 첫시간에 자기소개 빼고!) 프로젝트 깃헙은 여기에 있다.\n수업 마지막에 봤던 시험은 생각보다 어렵지 않았기에 사실 후기에 적을 내용이 별로 없다. 한국 시험에서는 선택형이어도 꼬아서 내거나 복수 정답지들을 만들어서 난이도를 높이는데, 여기 수업은 깔끔하게 중복 답안지 없다라고 말해주고 선택지들도 기본중의 기본이었다. 그나마 분별력을 가질 수 있는 부분은 프로그래밍 파트였으나 이 부분마저 코드를 외워서 치는게 아니라 오픈북이어서 그렇게 어렵진 않았던 것 같다. 게다가 시험시간은 3시간이었기에 타임 리밋도 거의 없는거나 다름없었다. 물론 시험보기 전에는 시험이 어떻게 나올지 예측할 수 없었기에 전날밤을 새워가며 공부하긴 했었다.\n\n\n\n마지막 날에 발표를 마치고 Lecturer분들의 ML/DL 트렌드에 관한 짧은 발표와 같이 수업들은 분들과 약간의 담소를 나누며 훈훈하게 수업이 끝났다. 약 3주간동안 저녁 시간에 열심히 배웠던 수업이 막을 내렸다.\n항상 가장 괴로울 때 가장 많이 배우는 것 같다. 사실 지나고 보면 그 고통이 어느정도 미화되는 것도 있고 위에 ‘어렵지 않았다.’, ’잘 마무리됐다.’라고 써놓기도 했지만, 그걸 하고 있던 순간에는 힘들었다. 아무리 알고 있던 내용이들이라도 그 내용들을 다시 다지는 것도 어렵고, 직접 코드를 써보는 것도 어렵고, 마무리 하기까지 불확실성과 두려움 때문에 괴로웠던 건 팩트다. 근데 항상 그러면서 배우는 것 같다. 그래서 이번 코스를 통해 배운 걸 정리해보자면,\n\nML은 확실히 DL보다 수학적인 이론이 어려웠음\n영어가 시험이나 부담이 아닌 ’언어’라는 것. 나를 표현할 수 있는 툴이라는 것\n전에 조금이라도 공부했던 것들이 나중에 도움이 ’크게’될 수 있다는 것\n내가 알고있는 것들을 꾸준하게 포스팅하는 습관을 길러야 하겠다는 생각\n해보지 않고 추측만으로 하는 생각들로 두려워하지 말 것\n\n수업을 마치고 certificate를 받았다. 사실 certificate에는 옵션이 있었는데, grade를 받을 것인지/pass or fail을 받을 것인지 내가 선택할 수 있었다. 등록 당시엔 근자감으로 grade를 받겠다고 했다가 수업 듣는 동안 과거의 나를 반성했다. 사실 수업하는 동안 그렇게 적극적이지도 않았고, 프로젝트 발표를 하고 지적을 받으면서 든 생각은 ‘최고 grade까지는 못받고 중간이라도 했으면..’ 했다. 그런데 생각지 못했던 최고 grade를 받을 수 있었다. 학점이 다가 아니긴 하지만 그래도 최선을 다했다는 것을 어느정도 뒷받침해줄 수 있기 때문에 뿌듯했다. 그래서 위에 배운 점들에 하나 더 추가하자면, ’최선을 다해서 어찌저찌해보면 뭐라도 한다.’라는 것. 식상한 멘트와 교훈이지만 매번 도전하면서 느끼는 바이기도 하다. 이렇게 2021년도 1월은 TU Berlin의 Machine Learning using Python: Theory and Application 코스와 함께 시간을 보냈다."
  },
  {
    "objectID": "posts/note/2024-12-19-init-vs-call.html",
    "href": "posts/note/2024-12-19-init-vs-call.html",
    "title": "📝__init__ VS. __call__",
    "section": "",
    "text": "Monkey Path를 공부하다가 한번 정리하면 좋을 것 같아 Python의 __init__과 __call__에 대해서 정리하고자 합니다.\n\n__init__은 인스턴스 초기화 시 실행\n__call__은 인스턴스 호출 시 실행\n\n\nclass A:\n\n    def __init__(self):\n        print('init')\n\n    def __call__(self):\n        print('call')\n\n    def myfunc(self):\n        print('my')\n\nprint(\"==== OUTPUT ====\")\n\na = A()\n\na()\n\na.myfunc()\n\n==== OUTPUT ====\ninit\ncall\nmy\n\n\n\nDecorator\n\n데코레이터는 자신이 수식할 함수나 메소드 내부에 받아 놓아야 함. 그러기 위해서 __init__에 데이터 속성 저장.\n데코레이터가 하는 일은 함수를 대리 호출.\n\n\nclass MyDecorator:\n    def __init__(self, data):\n        self.storage = data\n\n    def __call__(self):\n        print('data entered :', self.storage.__name__)\n        self.storage()\n        print('data exited :', self.storage.__name__)\n\n@MyDecorator\ndef printer():\n    print('I print the empty space.')\n\nprint('==== start ====')\nprinter()\n\n==== start ====\ndata entered : printer\nI print the empty space.\ndata exited : printer"
  },
  {
    "objectID": "posts/note/2024-12-23-linux-cheetsheet.html",
    "href": "posts/note/2024-12-23-linux-cheetsheet.html",
    "title": "📝Linx Cheet Sheet",
    "section": "",
    "text": "library 경로 설정\n\nconda env lib 연결시\n\n$ export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/usr/work/mylib\n\nvi ~/.bashrc (파일명 앞의 .은 숨김파일을 의미)\nexport LD_LIBRARY_PATH=/home/user/work/mylib:${LD_LIBRARY_PATH}\n\n\nGPU Memory 해제\n\n프로세스가 죽지 않을때 강제 종료\n\nps aux | grep python # 딥러닝 학습을 실행시킨 python 파일의 실행 ID를 찾기\nsudo kill -9 [ID_NUMBER]\n\n\nDocker\n\ndocker: Error response from daemon: could not select device driver \"\" with capabilities: [[gpu]].\nnvidia-container-toolkit 설치\n\n$ distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \\\n   && curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - \\\n   && curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list\n$ sudo apt-get update && sudo apt-get install -y nvidia-container-toolkit\n\n$ sudo systemctl restart docker\n\nGPU 설정\n\ndocker run -it --gpus '\"device=0,1,2,3\"'\n\ncontainer를 image로 저장\n\ndocker commit [CONTAINER_ID] [IMAGE_NAME]\n\n\n\n\nterminal zsh/bash\nchsh -s bin/bash\nchsh -s bin/zsh\n\n(@terminal)\n/bin/bash\n/bin/zsh\n\n\n용량 확인\ndf -h\n\n해당 경로에 있는 모든 파일과 폴더들을 용량 단위로 표기\n폴더가 1 순위, 파일이 2 순위로 나열\n경로에 있는 모든 폴더들과 각 폴더내에 있는 파일들을 모두 표기\n\ndu -ch [path] | sort -h\n\n해당 경로의 폴더 기준 트리 구조의 깊이 1만큼의 수준까지 탐색\n용량이 큰 폴더가 아래로, 가장 용량이 큰 10개만 보임\n\ndu -ch [path] --max-depth=1 | sort -h | tail -10\n\n현재 경로의 파일 용량\n\nls -lh\n\n\n파일 찾기\nfind [옵션] [경로] [표현식]\n\n# 현재 위치에서 log가 들어가는 파일 모두 찾기\nfind . -name \"*log*\"\n\n# 현재 디렉토리에서 .txt 확장자 파일 검색 후 모두 삭제\nfind . -name \"*.txt\" -delete\n\n\noptions\n\nP : 심볼릭 링크를 따라가지 않고, 심볼릭 링크 자체 정보 사용.\nL : 심볼릭 링크에 연결된 파일 정보 사용.\nH : 심볼릭 링크를 따라가지 않으나, Command Line Argument를 처리할 땐 예외.\nD : 디버그 메시지 출력.\n\n\n\n\n폴더나 파일 갯수 찾기\n\n현재 위치에서의 폴더 개수 ls -l | grep ^d | wc -l\n현재 위치에서의 파일 개수 ls -l | grep ^- | wc -l\n현재 폴더의 하위 파일 개수 find . -type f | wc -l\n\n\n\nmount 다중 경로 설정\n\n원래 마운트 지점에서 VFS(가상 파일 시스템) 노드를 생성\n\nmount --bind original-dir original-dir\n\n원래 마운트 지점을 공유로 표시\n\nmount --make-shared original-dir\n선택한 마운트 지점과 그 아래의 모든 마운트 지점에 대한 마운트 유형을 변경하려면 --make-shared 대신 --make-r shared 옵션 사용\n\n중복 생성\n\nmount --bind original-dir dupulicate-dir\n\n지우기\n\nsudo umount duplicate-dir\n\n\n\n\n원격과 로컬 사이 파일 복사\n\nscp 사용. scp [options] [source] [target]\n\n\n로컬에서 원격으로\n\nscp [source_path] [User]@[IP]:[target_path]\nscp /media/avery/source/test.txt avery@xxx.xx.x.xxx:mnt/\n\n원격에서 로컬로\n\nscp [User]@[IP]:[source_path] [target_path]\nscp avery@xxx.xx.x.xxx:mnt/test.txt /home/avery/Documents\n\n원격(A User)에서 원격(B User)으로\n\nscp [User]@[IP]:[source_path] [User]@[IP]:[target_path]\nscp ai@xxx.xx.x.xxx:mnt/test.txt avery@xxx.xx.x.xxx:/home/avery/Documents\n\n복수의 파일 전송\n\n여러 경로를 \" \"을 이용하여 묶어줌\nscp [options] [User]@[IP]:\"[file_1] [file_2]\" [target_path]\n\nOptions\n\n-r: 폴더를 복사할 때 사용(전송 대상을 폴더로 지정). 모든 폴더들을 재귀적으로 복사\n-P: ssh 포트 지정\n-i: identity file을 지정해서 사용\n-v: 상세 내용을 보면서 디버깅할 때 사용\n-p: 전송 시 파일의 수정 시간과 권한을 유지\n\n\n\n\n.cache 용량 해결\n\npip와 conda의 .cache 삭제\n\n#pip\npip cache purge\n\n#conda\nconda clean -a\n\n.cache 위치 변경\n\n# .bashrc를 vim으로 열어서 아래 변수를 추가하기 \nexport XDG_CACHE_HOME=\"/{disk_location}/{user_name}/.cache\"\n\n# 해당 경로 생성하기\nmkdir -p /{disk_location}/{user_name}/.cache\n\n\nvim 편집\n\n:%d: 전체 내용 삭제 (press Esc to ensure you’re in Normal mode first)\ni 입력 후, Ctrl + Shift + v: 새로운 내용 붙여넣기\n:wq: 저장\nNormal mode shortcuts:\n\ngg (go to the first line of the file)\ndG (delete from current line to the end of the file)\n\n\n\n\npython module 경로 파악\nimport module\nprint(module.__file__)"
  },
  {
    "objectID": "posts/note/2023-02-02-geultto-8th-start.html",
    "href": "posts/note/2023-02-02-geultto-8th-start.html",
    "title": "📘Geultto 8th Start",
    "section": "",
    "text": "Introduction\n나는 나를 안다라고 말할 수 있는 사람이 많을까요? 사실 그런 사람들이 많지 적은지는 관심이 없고 적어도 저는 내가 그런 사람이길 바랐습니다.\n나라는 것에 대한 정의와 경계가 있는 건 분명한데 그 안에 있는 공간이 너무 광활해서 마치 작은 박스안에 블랙홀이 있는 것 같은 느낌이랄까요. 어렸을 때는 누가 나에게 꿈과 하고 싶은게 무엇이냐라고 물으면 대화의 흐름에 방해되지 않을 정도로만 뜸을 들인 뒤 대답을 할 수 있었던 것 같습니다. 흔히들 대답하는 선택지인 직업과 하는 일, 그리고 거기에 약간의 신념을 섞어서요. 걱정과 근심, 고민이 없진 않았지만 대답을 할 수 있다는 것만으로 나는 나를 안다고 생각했습니다. 지금, 물론 지금도 사람들이 청춘이다라고 말하는 시기에 서있지만, 적어도 과거의 나보다 지나온 시간이 많은 지금의 저는 많은 의문이 듭니다. 나는 나를 아나? 내가 모르면 누가 나를 아나라며 누구보다 나에 대해서는 잘 안다고 생각했는데.. 그 주장이 약해지는 것이 느껴집니다.\n이렇게 내가 나에 대한 갈피를 잡지 못할 때 글또가 하나의 도움이 되기 때문에 이번 8기에도 참여하게 되었습니다.\n\n\nRelated Works\n이질감이 드는 나를 바라볼 때 도움이 되었던 몇가지 방법이 있습니다. 우선 한동안 유명했던 MBTI가 그 중 하나였는데 고등학생 때 진로적성 검사로 몇번 해보고 최근 1-2년 동안 대한민국에서 자기소개의 도구가 되어버린 MBTI는 나를 알아가는데 재밌는 도구였습니다. 저는 INFJ입니다. 검사지에 나온 몇가지 눈에 띄는 서술어들로는 통찰력 있는 선지자. 극소수의 유형, 내향적인 이상주의자, 감정형(F) 중에서는 대체로 사고(T) 성향이 높은 편 등이 있고 이외에 SNS등에서 유형별 특징들에서는 아무도 신경안쓰는데 혼자 눈치봄, 생각이 너무 많아 집중이 안됨(아무생각 안하는 거 할 줄 모른다), 글로 적으면 논리력 기가막힌데 말로하면 어버버하고 안나옴, 뭐든지 파악하고 정리하고 싶어한다, 구구절절 (괄호까지 쳐가며) 설명하는 습관이 있다 등이 있습니다. 공감의 차원을 넘어서서 누가 나를 이렇게 잘 아나 흠칫 놀랄 정도였으니 이 검사가 확증편향을 가져온다고 하더라도 나를 알아가는데 정말 많이 도움이 되었던 것 같습니다.\n\n\n\n매우 공감되는 INFJ의 모습들\n\n\n다음으로, 글쓰는 또라이라는 글쓰는 모임에서 소개받은 Big5 검사라는 걸 한번 해봤는데 공감력, 책임감, 협조성이 대표적인 키워드로 나왔고 강직함, 지적 호기심, 상상력, 걱정 등이 내가 강한 성향으로 친밀감, 자제력, 사교성, 분노, 리더십 등이 내가 약한 성향으로 나왔습니다. MBTI 검사와 크게 다르지 않아 새롭게 알게 된 사실이 딱히 많진 않았지만 추가적으로 일적인 측면에서 현재 주어진 상황에 만족하기보다 새로운 지식과 기술을 익히기 위해 노력하는 변화 주도형 리더라는 워딩에 공감하며 어쩌다 현재 내가 공학도가 되어있는가에 대해 생각해보게 된 것 같습니다.\n\n\n\nBig5 검사 결과\n\n\n성향, 성격도 나의 일부지만 앞서 이야기했던 꿈과 하고 싶은 것, 싫어하는 것, 좋아하는 것 등 다른 부분들도 나라는 경계 안에 있다는 것을 압니다. 요즘 부쩍 하고 싶은 것에 대한 확신은 불투명해지고 싫어하는 것과 좋아하는 것을 구분짓는 표현은 하기가 힘들어집니다. 그래서 아마 이 글도 이런 맥락 속에서 나를 안다는 것에 대한 고찰로 시작된 것 같습니다. 학창시절 때는 친구들이 저에게 너는 정말 네가 하고 싶어하는 게 분명한 것 같아. 멋있다. 라고 많이 이야기 했습니다. 저 또한 그런 나의 모습에 뿌듯함과 자랑스러움을 느끼고 있었습니다. 그때는 나를 안다고 생각했습니다. 지금은 잘 모르겠습니다. 여전히 저를 좋게 봐주는 좋은 사람들이 있지만 이제 더 이상 제 안에서 저를 뿌듯하게 바라보기 보다 스스로를 걱정스럽게 바라보는 눈길이 더 느껴집니다. 이런 불안함이 일시적인거다. 성향적인 이유에서 그런거다.라고 생각할 수도 있겠지만 그렇게 넘어갈 수 없을 것 같다는 느낌이 들었습니다. 여전히 괴롭지만 최근에 엄마와의 통화를 통해 힌트를 얻고 다른 관점으로 보려고 합니다. 그 괴로움이 내가 살아있다는 증거라고 하시더라구요. 그렇게 말씀하시면서 너를 위해 기도하고 있다고 덧붙여서 말씀하실 때 솔직히 반항심도 들었습니다. 불안함과 고통이 살아있다는 증거라면 솔직히 살고 싶지 않다라는 생각을 하게 되는 나를 하나님은 봐주시지 않나라는 생각이 들었습니다. 각설하고, 여기서 힌트를 얻었다는 것은 내가 살아가는 동안 완전히 해결할 수 있는 고민은 아니구나 싶었습니다. 해결하고 싶었지만 해결할 수 없다는 걸 깨달았다고 마무리 짓겠습니다.\n나의 성향과 나의 고민을 펼쳐보며 든 생각은 글쓰기가 유일한 내 창구일 수도 있겠다 싶었습니다. 나는 나를 표현하기 힘들어하고 다른 사람들에게 잘 말하지도 못하는 사람이라는 걸 알아서 글을 쓰면서 나를 돌아보고 내가 나에게 말해보는 시간이 도움이 될 것 같았습니다. 시간이 지난 후에 이 블로그에 들어와서 과거 시점의 나를 한번 더 들여다 볼 수 있는 것이 때론 힘이 될 것 같기도 했습니다. 글쓰는 또라이 모임 8기에 참여하면서 쓰게 되는 글들은 물론 이런 종류의 글들이 아닙니다. 감정적인 이야기보다는 기술적이고 어떤 정보나 지식을 찾던 사람들을 위한 글을 쓰는 것이 취지에도 맞고 제가 기르고 싶은 실력에도 도움이 되기 때문에 참여하게 된 것 입니다. 아마 이 글을 마지막으로 글또에 제출하는 글들에는 더 이상 저의 잡다한 이야기가 들어가지 않을 것 같습니다.\n\n\nMethod\n글또에서 쓰는 기술적인 글들 또한 제가 쓴 글이고 나를 만들어가는 글들이 되기 때문에 앞서 장황한 도입을 썼습니다. 7기에서 썼던 글들이 지금 연구하고 있는 나를 만들어준 동력원이기도 했고 다시 읽으면서 도움이 되기도 했습니다. 이전에 다짐한 것처럼 지식과 정보의 차원에서 다른 사람들에게 도움이 되는 글을 쓰고 싶은 것은 변함없지만 너무 이 목표에 사로잡히지는 않으려고 합니다. 일단 내가 정리하고 글을 쓰면서 나에게 도움이 된 것 만으로도 나는 또 다른 성장을 했다고 나를 격려하며, 이번 8기에서는 이런 마음으로 임하려고 합니다. 쉽게 말하자면 부담을 좀 덜어내고 글을 자주 쓰는 습관이 이번 기수에서 제가 설정한 목표라고 할 수 있겠습니다.\n정리를 좋아하니 개괄식으로 글또에서 이루고자한 목표를 정리해보겠습니다.\n\n글또에서 설정한 글 마감일에 모든 글 제출하기(패쓰권 안쓰기)\n커피드백에서 성장할 수 있는 모든 점들을 파악하고 더 나아지기\n코드가 들어가는 글 50%이상 작성하기\n글또에 제출하지 않는 주간에는 자유글 쓰기(매주 1포스팅 목표)\n\n이전 7기에서 목표와 다짐을 작성할 때도 글을 쓰는 목표말고도 나를 이루는 생활 목표도 적었었는데 잘 지키진 못했어도 다시한번 환기시킬 수 있어서 좋았기에 이번에도 생활목표들을 덧붙여볼까 합니다. 생활 목표들은 기본적으로 매일한다는 생각으로 지금 이미 만들고 있는 습관들에 업그레이드 하는 수준으로 설정해봤습니다.\n\n매일 아침 해야할 것과 하지 말아야 할 것 적기: 생각이 많아서 실천을 못하기 때문에 아침에 정리를 해서 나를 좀 더 빨리 움직여보려고 합니다.\n매일 운동하기(저녁 만보 걷기/저녁 30분 러닝/아침 필라테스 중 하나 골라서): 우울감과 나태함은 생각보다 쉽게 신체활동으로 극복된다는 깨달음이 계기가 되었습니다.\n매일 영어표현 3개 내 문장으로 만들기(with RealClass): 문어체보다 쉽게 다가갈 수 있는 대화 영어가 부족하기 때문에 부담없이 반복해보려고 합니다.\n\n\n\nConclusion\n결론은 광활한 우주에서 먼지에 불과한 고민많은 한 작은 인간이 커피와 음악으로 해결되지 않는 자아관찰을 이번 글또 8기와 함께 잘 이뤄나가고 싶다는 것이었습니다. 글을 쓸때 만큼은 한없이 진지해지고 새벽이 아님에도 새벽감성 충만하게 되는 저의 모습을 또 하나 알아가며 이번 글또 8기도 잘 활동해보고 싶습니다."
  },
  {
    "objectID": "posts/note/2024-10-09-geultto-10th-start.html",
    "href": "posts/note/2024-10-09-geultto-10th-start.html",
    "title": "📘Geultto 10th Start",
    "section": "",
    "text": "나야, 글또\n한창 흑백요리사가 유행하는 요즘. 많은 밈들이 만들어지고 있습니다. 그 중 하나 “나야, 들기름”을 조금 응용하여, 글또가 나에게 어떤 존재인지 표현해보면 “나야, 글또”로 표현해볼 수 있지 않을까 싶습니다. 글또는 대학원 석사 2학기 무렵 글을 쓰며 성장하고 싶다라는 생각으로 7기에 시작하여 지금 10기까지 참여하고 있는데, 어느덧 졸업과 취업이라는 2개의 산을 넘는 동안 참여 해온 커뮤니티라는 사실이, 은은한 들기름 향처럼 제 생활에 성장의 고소함을 주었다는 것이, 이렇게 표현이 되지 않나 싶습니다. 대학원생 신분일 동안에는 어디에 표현되지도 않고 드러나지도 않을 방구석 연구자인 것처럼 내 자신이 초라하게 느껴질 때 글또에서 글을 쓰며 힘을 얻을 수 있었습니다. 그리고 비교적 짧은 시간이었지만 그 어느 때보다 불안정함을 느꼈던 취준 시기에는 글또에서 만난 분들에게 직간접적으로 영감을 받으며 버틸 수 있었습니다. 이제는 아직 1년도 안된 사회 초년생으로, 글또에서 이렇게 많은 선배/동료 개발자들을 보고 배울 수 있기에 얼마나 감사한지 모릅니다.\n사실 매번 글또를 마치는 시기마다 “다음 기수엔 신청이 어렵겠다. 이제 혼자서 글을 꾸준히 써보자.”라고 생각하며 “아마도 다음에 신청을 하지 않을 것 같다.” 생각하게 됩니다. 하지만 또 다시 새로운 기수가 시작되는 시기가 오고 신청을 받는 기간이 돌아오면 다시 신청서를 작성하고 “글또 없이는 블로그 글이 전혀 안써지네.”라고 생각하며 시작하게 됩니다. “나야, 글또”라고 말하며 다시 다가오는 글또 활동 시기에 감사하고, 또 한번 저에게 글 쓰는 힘을 기를 수 있는 커뮤니티의 힘을 빌릴 수 있다는 것이 안도가 되는 것 같습니다.\n\n\n이번에는\n스스로도 아이러니 한 것은, 사실 꾸준히 글을 쓰며 성장하고 싶은 마음이 충분히 있음에도 동시에 글을 쓰면서 느끼는 저항감과 피로감이 있기에 글또 활동이 마냥 편하지 않습니다. 그렇기에 마지막 글 제출까지 하고 나면 다음 기수 글또를 기약하기 어렵겠다는 생각과 함께 다시 글을 안 쓰던 안일한 마음으로 되돌아가려고 하는 것 같습니다.\n이번에는 글또가 10기로 마지막으로 운영되는 기수입니다. 마지막으로 커뮤니티의 힘을 빌려 글을 쓰며 성장하는 기회이기도 하기에, 본질로 돌아가 10기에서의 목표로 단 하나만 지켜보려고 합니다.\n매주 글쓰기\n2주 간격 제출이 원칙적인 정기 제출 주기이지만 건너뛰게 되는 한 주에도 스스로 글 쓰는 마감일을 지정해서 작성하는 목표를 잡았습니다. 7기 때부터 스스로 글 쓰는 습관을 만들고 싶었는데 매번 정기 제출 마감일에 급급하며 썼었기에 제대로 습관을 기르지 못했다는 판단 하에 이번에는 가장 단순하고도 본질적인 목표 하나만 잡았습니다. 마지막 글또에서 가장 단단한 습관을 만들어 놔야 이후에 지속적인 성장이 가능하지 않을까 라고 생각하기 때문에 도전해보려고 합니다.\n이전에는 학생/연구자 신분이었기에 ai 연구 코어 채널에서 활동을 했었지만, 이번에는 첫 직장에서 AI/Robotics 엔지니어로써 일을 시작하게 되어 ml-ai-엔지니어 코어 채널에서 활동하게 되었습니다. 비슷하지만 다른 분위기에서 영감을 받을 수 있을 것 같아 많이 기대가 됩니다.\n\n\n실험을 받아들이는 자세\n마지막으로 다시 흑백요리사 이야기로 돌아가서, 경연에서 가장 인상이 깊었던 분은 에드워드 리 셰프님이었습니다. 다 쟁쟁한 요리사들이 나왔고 존경스러운 점들이 많았지만 , 에드워드 리 셰프의 경력과 연륜에서 유지하기 힘든 도전에 대한 갈망과 그런 지향점을 유지하게 해주는 삶의 태도에서 “나도 저렇게 살아가고 싶다.”라는 생각이 들었던 것 같습니다.\n\n\n\n에드워드 리의 인터뷰 중 1\n\n\n사실 2024년도에 많은 변화가 일어나면서 내가 좋아하는 게 뭔지 조차 헷갈리는 시기가 있었습니다. 나도 나를 모르겠는 혼란스러운 순간들 가운데 처음 찐 사회생활을 하다 보니 흔들림의 진폭이 더 커져 괴로웠었고 “내 선택이 맞는 걸까”라는 고민에 정체되어 있던 시간이었습니다. 특히나 내가 원하지 않았던 것들을 하고 있다고 생각이 들 때가 가장 괴롭고 시간이 허비되고 있다는 생각과 이러다가 내 커리어가 꼬이는 거 아닐까라는 불안함으로 앞으로 나아가지 못하고 있었습니다. 하지만 내가 원하는 것에만 너무 초점을 맞춰서 생각하다보니 원하지 않는 것들이 도움이 되지 않을 것이다 라는 얕은 생각으로 괴로워하지 말고, 셰프님의 인터뷰에서 처럼 내가 원하지 않는 것에 대해 알아가는 시간 또한 귀하다고 생각하고 나아갈 수 있구나 라는 걸 깨달을 수 있었습니다.\n\n\n\n에드워드 리의 인터뷰 중 2\n\n\n글또는 내가 나를 가지고 어떤 실험이든 할 수 있도록 판을 깔아주는 소중한 연구실이었기에, 이번 10기에도 글쓰는 성장의 힘을 실험해볼 수 있는 좋은 시간을 만들어 갈 수 있길 바래봅니다."
  },
  {
    "objectID": "posts/note/2021-01-03-Hello-2021.html",
    "href": "posts/note/2021-01-03-Hello-2021.html",
    "title": "📘Hello 2021",
    "section": "",
    "text": "2021에 이루고 싶은 일들, 하고 싶은 일들, 바라는 일들, 2021 회고록을 쓰면서 “그땐 이런 생각을 하며 살려고 노력했었구나” 할 만한 이야기를 적어보자."
  },
  {
    "objectID": "posts/note/2021-01-03-Hello-2021.html#해야하는하고싶은-공부",
    "href": "posts/note/2021-01-03-Hello-2021.html#해야하는하고싶은-공부",
    "title": "📘Hello 2021",
    "section": "해야하는/하고싶은 공부",
    "text": "해야하는/하고싶은 공부\n구체적인 실천 목표들은 다이어리에 적고 여기에는 크게 대략적인 목표지점들을 적어보려고 한다.\n\n영어\n이젠 피하고 싶다는 생각보다는 해내야만 한다는 생각이 더 크다. 다른 공부들도 그렇지만 영어야 말로 이제는 output을 내야하기 때문에 간절함이 더 커졌다.\n\n영어루틴\n토플 90점 이상\n외국인 친구 만들기\n\n\n\nRL\n아직도 잘 모르겠지만 아직도 내 호기심을 자극시키는 분야라 계속 공부하고 싶다.\n\n스터디(기초이론과 논문읽기)\n구현능력 올리기\n연구에 활용하는 능력 기르기\n\n\n\nGNN\n\n기초 이론 입문하기\n\n\n\nQC\n\n기초 이론 입문하기(feat. 모두연)\nQiskit Challenge 도전하기"
  },
  {
    "objectID": "posts/note/2021-01-03-Hello-2021.html#하고싶은-취미",
    "href": "posts/note/2021-01-03-Hello-2021.html#하고싶은-취미",
    "title": "📘Hello 2021",
    "section": "하고싶은 취미",
    "text": "하고싶은 취미\n2020에 회고하면서 한 가지 느꼈던 게 있었다. 취미라 할 만한 활동을 하질 않았었다. 그래서 더 쉽게 지쳤고 멀리 나가지 못했던 것 같아 2021에는 취미도 생각하면서 살고 싶어서 한번 적어보았다.\n\n피아노\n부모님의 반대를 무릅쓰고 산 전자 피아노가 하나 있는데 많이 쳐보지를 못했다. 원래 쳤던 곡들도 좋고 재즈 피아노나 마피아에서 산 몇곡 악보를 제대로 연습해보고 싶다. 예전에는 피아노한테 힐링도 많이 받았었는데..왜 이렇게 멀어졌을까🤔\n\n박터틀의 재즈 피아노\nTido Kang의 노래\n피아노 치는 이정환님의 노래 이건 불가능하겠지만🤣\n\n\n\n책 읽기\n유튜브 보는 시간을 좀 줄여보고 한 달에 딱 1권은 인문이나 사회 분야 등과 같이 내가 쉽게 접할 수 없는 분야의 책들을 읽어보려 한다. 기술서 같은 경우에는 필요에 의해 손이 많이 가지만 이외의 책들은 정말 안 읽게 되는 것 같다. 간략히 남기고 싶은 문구나 독후감은 다이어리에 남겨보자."
  },
  {
    "objectID": "posts/note/2021-01-03-Hello-2021.html#만들고-싶은-습관",
    "href": "posts/note/2021-01-03-Hello-2021.html#만들고-싶은-습관",
    "title": "📘Hello 2021",
    "section": "만들고 싶은 습관",
    "text": "만들고 싶은 습관\n\n운동\n체력과 건강을 무시 못할 시기가 온 것 같다. 지금은 코로나로 인해 헬스장이 언제 열릴지는 모르겠으나 일단은 기본적인 걷기나 자전거, 간단한 홈트 동작들을 꾸준히 아침 루틴으로 하는 것이 목표다.\n\n\n블로깅\n지금 쓰고 있는 github 블로그에 꾸준히 기술관련 포스팅을 하는 것이 목표다. 아직은 개발자도 아니고 유의미한 output 포스팅을 하기엔 실력이 없지만, 내가 그때 그때 마다 알게 된 것, 이후의 나에게 도움이 될 만한 내용이라고 판단되면 적어보려고 한다. 나부터 나를 가르쳐보면서 어떤 글이 잘 쓰여진 글인지 스스로 체크해보며 블로깅하는 실력을 길러보는 한 해가 되었으면 좋겠다.\n\n\n다이어리\n블로깅이 주로 기술관련(특히 it 분야겠지만)이라면 다이어리는 내 내면과 생각을 정리해보고 싶어서 만들고 싶은 습관이다. 생각보다 하루하루 내가 느낀 것들은 많은데 막상 돌이켜 보면 잘 기억이 나지 않고 내면의 단단함이 쉽게 물러지는 느낌이 든다. 그냥 생각없이 산 것 같기도 하고 내 삶을 소중히 생각하지 않은 것 같은 느낌이 들 때. 순간의 나도 이런 생각을 하고 고민을 하며 살아왔다는 흔적을 남기는 연습을 하고 싶다. 예전에 삼수를 할 때 비교적 생활 패턴이 규칙적이고 고요할 때(?)는 다이어리에 기록하는게 어렵지 않았는데 지금은 다이어리 루틴을 만든다는 게 쉽지 않은 것 같다. 2021은 남겨보려고 노력해보고 싶다."
  },
  {
    "objectID": "posts/note/2021-01-03-Hello-2021.html#신년다짐과-생각",
    "href": "posts/note/2021-01-03-Hello-2021.html#신년다짐과-생각",
    "title": "📘Hello 2021",
    "section": "신년다짐과 생각",
    "text": "신년다짐과 생각\n송구영신 예배와 신년 축복예배에서 받은 말씀은 기대의 차원을 높이자와 대장부가 되자였다. 남에게 기대하기 보다 나에게 기대하는 삶을, 나보단 하나님께 근거있는 삶을 바라며 나에게 주어진 새로운 시간들을 잘보냈으면 좋겠다. 그리고 지금까지 살면서 느껴왔던 것처럼 삶을 살아가면서 내가 생각지 못한 경험들의 스펙트럼은 점점 넓어진다는 걸 알았다. 좋은 쪽으로만 범위가 넓어지는 것이 아니기에. 내가 감당치 못할 어려움을 주시는 분이 아니시기에 당장 눈 앞에 커보이는 두려움을 담대하게 맞설 수 있다는 믿음을 더 확고히 할 수 있는 한 해가 될 수 있도록.\n어쩌다보니 대학생활도 마지막에 접어들었다. 생각없이 보내기엔 다시 돌아오지 않을 시간들이기에 좀 더 정신차리고 최선을 다해보고 싶다. 내 것이 되기전에는 한 없이 미웠었지만 지금은 그 어떤 대학보다 나의 가능성을 확인시켜 준 내 학교에서 마지막 생활은 어떨까. 안녕 2021"
  },
  {
    "objectID": "posts/note/2022-10-17-daily-english-003.html",
    "href": "posts/note/2022-10-17-daily-english-003.html",
    "title": "🌎Casual English Phrases 003",
    "section": "",
    "text": "영어 회화 표현이나 논문에서 쓸 수 있는 유용한 표현들을 조금씩 익히기 위해 시작한 1000일 시리즈 포스트 입니다. 공부 리소스는 유튜브, 리얼클래스, PN님의 블로그, cake 어플, 책, 논문, 기사 등 다양한 출처에서 보고 기록합니다.\n\n\n속마음을 보여주다\nwear your heart on your sleeve\n\nIf you wear your heart on your sleeve, you openly show your feelings or emotions rather than keeping them hidden. 속마음을 보여준다는 말은, 너의 감정이나 느낌들을 숨기기보다 보여준다는 거야.\nIt takes courage to wear your heart on your sleeve, which is why few people do it. 속마음을 털어놓는다는 건 용기가 필요한 일이기 때문에 많은 사람들이 하지 못하는 일이지.\nI need someone whom I can wear my heart on my sleeve. 내 속마음을 털어놓을 사람이 필요해.\n\n\n\n기억력이 좋은\na memory like an elephant\n\nShe has a memory like an elephant. 그녀는 기억력이 굉장히 좋아\nYou can remember his name even if you don’t have a memory like an elephant. 니가 기억력이 엄청 좋지 않더라도 그의 이름은 기억할껄?\nThe menual is so simple. Don’t worrry that you don’t have a memory like an elephant. 메뉴얼이 엄청 간단해. 기억력이 좋지 않다고 걱정할 필요 없어.\n\n\n\n입장바꿔 생각해봐\nput yourself in one’s shoes\n\nPlease put yourself in his shoes before blurting it out. 그냥 말을 하기전에 그의 입장도 생각해봐\n\nblurt out : 말을 내뱉다\n\nIt’s hard to put myself in other people’s shoes from the my heart. 진심으로 다른 사람들의 입장이 되어보는 것은 어려워\n\nfrom the (bottom of one’s) heart : 진심으로\n\nWould you mind putting yourself in my shoes? 제 입장에서 한번만 생각해주실 수 있으실까요?\n\n참고 기억력 상징이 코끼리가 된 이유"
  },
  {
    "objectID": "posts/note/2023-07-07-geultto-8th-end.html",
    "href": "posts/note/2023-07-07-geultto-8th-end.html",
    "title": "📘Geultto 8th End",
    "section": "",
    "text": "2번째로 참여한 글또 8기에서 마지막 글을 쓰는 날이 되었습니다. 지금까지 있었던 일들을 되돌아보며 마무리 회고를 해보려고 합니다. 이전에 회고할 때는 활동을 시작하며 작성했던 다짐글을 보며 그때 목표했던 것들을 잘 이루었는지 점검하는 방식으로 했었지만, 이번에는 자유롭게 활동했던 이야기들 중심으로 회고해보려고 합니다. 글또라는 공동체에서 했던 활동들을 주로 돌아보며 작성하겠지만 2023 상반기에 있던 개인의 일상사와 생각도 자연스럽게 담기는 회고가 될 것 같습니다.\n매번 시작할 때 예치금을 넣어놓고 예치금을 절대 까먹지 않으리라 했던 목표는 이뤘습니다. 사실 다짐글에서는 pass권도 안쓰고 모든 제출을 하는 것이 목표였지만 2번의 pass도 다 쓰고, 추가적으로 한번은 제출을 하지 못해서 만원 차감이 되었었습니다. 다행히도 커피드백 환급금으로 만원 손실은 매꿀 수 있었고 결과론적으로는 예치금은 사수할 수 있었습니다. 간단한 활동 결산표이지만 이번 글또 8기에서의 제 활동을 함축적으로 잘 보여주는 것 같습니다."
  },
  {
    "objectID": "posts/note/2023-07-07-geultto-8th-end.html#good-stories",
    "href": "posts/note/2023-07-07-geultto-8th-end.html#good-stories",
    "title": "📘Geultto 8th End",
    "section": "Good Stories",
    "text": "Good Stories\n\nWriting\n역시 글또의 본질, 글쓰기 활동에 대한 점검은 가장 중요한 부분이라고 생각합니다. 블로그 글을 쓰면서 1차로는 당연히 나에게 도움 되는 글을 쓰는 게 목적이었고 2차로는 다른 사람들에게도 도움이 되고 잘 읽힐 수 있는 글이 되는 것이 목표였습니다. 사실 이 생각은 이전에 7기를 마무리하며 썼던 글에서 이야기했던 것처럼 연구자로써 더 좋은 글을 쓰고 싶은 마음이 처음 동기가 되어 지금까지도 변함없는 생각입니다. 이전에 썼던 글들과 다르게, 이번 글또 활동에서부터 정말 청중들에게 발표하듯이 작성해보자라는 생각을 했었기 때문에 개괄식/~이다 문체를 피하고 서술식/~합니다 문체로 작성하는 연습을 했었습니다. 글의 길이가 길어지고 핵심까지 도달하는 시간이 좀 걸리는 글이지만 전체적인 독자의 이해도를 높일 수 있기 때문에 결과적으로 문체를 바꿔서 작성한 글들이 더 좋다는 생각이 들었습니다. 개인적으로 바뀐 문체로 작성한 글들이 퇴고를 할 때도 더 좋았기 때문에 앞으로도 문체를 유지하면서 글을 작성하려고 합니다. 형식 뿐만 아니라 내용과 주제 측면에서도 현재 연구하고 있는 분야와 연관된 논문들과 코드들을 중점적으로 다루면서 원하는 주제들을 재밌게 작성할 수 있었습니다. 이번 8기에서 작성한 총 9개의 글들을 통해 확실히 글쓰는 능력이 한층 성장되었다는 것을 느낄 수 있었습니다.\n스스로 느끼는 성장 히스토리가 가장 중요하지만 다른 사람들에게 어떻게 제 글이 보이는지도 많이 궁금했었습니다. 다른 사람들이 내 글을 볼 때 어떨까?라는 궁금증을 이전 7기 때까지는 없었던 운영진 분들의 감사한 수고로 생긴 큐레이션이라는 파트를 통해 확인해볼 수 있었습니다. 큐레이션을 통해 다른 분야들의 좋은 글들도 볼 수 있었고 객관적으로 제가 쓴 글을 본 독자들의 생각도 알 수 있었습니다. 이번 8기 활동을 하면서 총 4개의 제 글들이 큐레이션이 되었다는 사실이 지금도 얼떨떨하긴 합니다. 큐레이션이 되었다는 사실 자체가 좋은 글이다 라는 절대적인 판단 기준이 될 순 없지만, 글또에 실력 좋고 대단하신 분들이 2주 마다 작성하시는 많은 좋은 글들 중에 조금이라도 눈에 띄었다는 사실에 제가 쓴 글들에 대해 좋은 인정을 받은 것 같아 기분이 좋았습니다.\n\nCurated 1: WASABI\nCurated 2: K-Accessibility for RL\nCurated 3: Github Starstruck 128\nCurated 4: Chord Graph\n\n\n\n\n4번의 큐레이션 선정\n\n\n\n\nNetworking\n커피와 함께한 feedback&chat\n글을 쓰는 습관과 능력을 기를 수 있다는 것이 글또의 주된 매력적인 포인트지만, 글또에서 만나는 멋진 분들이 글또의 또 다른 매력인 것 같습니다. 이번 기수에서는 총 3회의 커피드백에 참여를 했었고 함께하신 분들의 이야기들을 들으면서 배울 점들도 많았고 내 이야기도 나누면서 느끼는 점도 많아서 다 소중한 시간이었습니다. 매번 모임마다 2-3시간은 기본으로 각자의 삶과 진로를 고민하는 이야기들을 나누면서 때론 나만 힘든 게 아니라는 사실에 위로도 받고 힘도 얻었으며, 때론 머릿속에서만 뱅뱅 돌고 있던 고민들을 입 밖으로 꺼내 이야기하면서 새삼 내가 이런 생각들을 가지고 있었구나 놀랐을 때도 있었습니다.\n\n\n\n3번의 커피드백 모임\n\n\n1, 3회차 때는 캐쥬얼하게 각자의 삶의 이야기나 세상 이야기들을 나누었지만, 커피드백이라는 이름에 맞게 2회차 때는 이때까지 글을 쓴 것들을 살펴보며 서로 좋게 생각한 점, 보완하면 좋을 것 같은 점들을 나누었었습니다. 아마 제일 오랫동안 서로 피드백을 주고 받았던 것으로 기억하는데(한 4시간 정도) 감사하게도 같이 모인 모든 분들이 처음부터 끝까지 진지하게 하나하나 각자의 글들을 읽어보고 좋은 생산적인 의견들을 주셔서 어떻게 글의 질을 높일 수 있을 지에 대한 많은 인사이트를 얻을 수 있었습니다. 글또 커피드백 참여가 정말 소중하다는 생각을 했었습니다.\n글또에서의 또 다른 만남들\n사실 오프라인 커뮤니티 모임에서 동기부여를 얻는 것을 매우 좋아합니다. 내가 잘 모르는 분야라도 집중해서 듣다 보면 익숙해지면서 새로운 세상을 알게 되는 것 같아 재밌고, 해당 분야에 열정이 많은 멋진 연사님들이 하시는 발표를 들으면 나도 모르게 좋은 에너지를 받아서 자주 오픈 세미나에 참석하는 편입니다. 그런 저에게 글또에서 하는 채널별 반상회는 정말 기다리던 행사였고 기대했던 만큼 너무나도 좋은 데이터 통합 반상회를 운영진 분들과 반상회 준비위 분들이 만들어 주셔서 정말 감사했습니다. 각자의 분야와 자리에서 데이터, AI 직군/연구에 대한 고민을 치열하게 했던 이야기들을 들으면서 스스로의 한계점에 지쳐있던 참에 힘을 받을 수 있었고, 데이터로 보는 글또 발표에서는 재밌고 공감 되는 부분들도 많아 많이 웃으면서 발표를 들을 수 있었습니다.\n\n\n\n글또 반상회와 글또 안에서의 네트워킹\n\n\n커피드백과 반상회 같은 공식적인 일정 말고도 슬랙을 통해 많은 분들을 만날 수 있었는데, 소심이 I이지만 이번에는 저도 용기 내서 몇 번 활동을 참여하거나 호스트 역할을 했습니다. 해외 대학원/취업을 고민하는 분들을 만나 잠시 접어두었던 꿈과 도전에 대해 생각해볼 수 있었고, 막막했던 코딩 테스트 준비를 위해 스터디 활동도 하고, 혼자 나름 커스텀한 키보드 자랑도 해보고, 진로 고민하는 시기(현재 진행 중..)에 해외 취업도 미쳤다 생각하고 도전해보자는 마음에 공고를 내고 사람들을 모아 보기도 했습니다. 돌아보니 뭔가 제가 이것저것 많이 활동을 했다는 게 신기합니다. 물론 모든 모임에서 제가 생각한 대로 생산적인 결과나 마무리를 하지 못했거나 부족한 점들이 있지만 좋은 사람들을 만나는 용기가 저에겐 쉽지 않은 시도였기 때문에 이후에도 내가 유지해야 할 인생의 태도라고 생각이 들었습니다."
  },
  {
    "objectID": "posts/note/2023-07-07-geultto-8th-end.html#bad-stories",
    "href": "posts/note/2023-07-07-geultto-8th-end.html#bad-stories",
    "title": "📘Geultto 8th End",
    "section": "Bad Stories",
    "text": "Bad Stories\n좋아하는 노래 중에 AJR의 100 Bad Days가 있는데 가끔씩 너무 힘들고 내가 너무 초라해져 보이거나 후회가 되는 일들이 있을 때, 노래 가사를 새겨보며 마음을 다잡습니다. 노래에서 “나쁜 이야기들이 나중에 재밌는 파티에서의 썰을 풀 소재가 된다”라는 메세지가 너무 심각해져있는 마음의 답답함과 긴장감을 풀어주는 느낌이고 “나쁜” 이야기(경험)이 꼭 나쁘지만은 않다는 점이 위로가 정말 많이 됩니다. 그런 면에서 회고 파트에서 Bad Stories를 기록하는 부분도 훗날 재밌는 안줏거리 혹은 지금의 나를 만든 한 페이지의 증거로 소중하다 생각하며 적어봅니다.\n꾸준함에 대한 반성\n글또 다짐글을 다시보며 든 생각은.. 정말 하나도 지킨 다짐이 없다는 생각이 들면서 너무 부끄러웠습니다. 작성했던 다짐들을 살펴보면 미래의 나에 대해 높은 기대감을 가지고 높은 꾸준함을 요구하는 목표들을 나열했었더군요. 마치 당장의 내가 180도 달라질 것을 기대하는 것 같은 느낌이 들 정도로, 지금 생각해보면 목표를 너무 크게 잡은 것이 아닌가하는 생각이 들었습니다. “모든 글 제출”, “제출 기간 외에도 작성하는 꾸준한 습관 기르기”등 목표에서 나에게 바라는 꾸준함의 기준은 굉장히 높은데 이를 조금 낮추어서 하나만 지켰더라도 좋지 않았을까..라는 페이스 조절에 대한 반성을 하게 되었습니다. 누가 강제로 시킨 약속이 아니더라도 스스로에게 한 약속에 대해 좀 더 엄격해질 필요성을 느끼며 지금은 실패한 역사로 남지만 훗날 꾸준히 글을 쓰는 사람이 되어있길 바래봅니다. 글을 쓰는 꾸준함 뿐만 아니라 생활 속에 고치고자 했던 작은 습관들도 포기하지 않고 좋은 방향으로 꾸준히 변화해가는 힘을 기르고 싶네요.\n현재를 살아가기\n저는 사서 고생이 아닌, 걱정을 하는 타입입니다. 걱정도 많고 미래의 불확실성에 대한 두려움이 정말 많은 사람이기 때문에 현재에 집중하지 못해서 상반기에 하고자 했던 일들을 많이 하지 못했습니다. Stop wishing, start doing이라는 좌우명을 설정해서 행동하는 사람이 되자는 취지로 스스로 격려를 하지만, 여전히 필요 없는 생각들로 집중력이 흐트러지고 앞으로 나아가지 못하는 것 같습니다. 내가 오늘, 지금 이 시간에 해야 하는 것을 단순하게 정리하고 집중하는 사람이 될 수 있도록 하반기에 다시 생활 리듬을 돌아보고 해야 할 것과 하지 말아야 할 것 목록을 만들어서 지켜보겠습니다. 그리고 불안한 마음들을 적어보고 이 감정이 현재 내가 개선할 수 있는 action plan으로 바꿀 수 있는 것인지 점검하는 시간도 하루를 마무리하면서 점검을 해보겠습니다."
  },
  {
    "objectID": "posts/note/2025-01-05-Goodbye-2024.html",
    "href": "posts/note/2025-01-05-Goodbye-2024.html",
    "title": "📘Goodbye 2024",
    "section": "",
    "text": "2024 정말 잊지 못할 한해였습니다.\n사람이 살아가면서 많은 마일스톤들이 있고, 그 마일스톤을 한해에 하나 담기도 어려운 법인데 2024는 3가지 마일스톤들이 지나간 해였기에 가장 기억에 남지 않을까 싶습니다.\n하나 하나의 키워드가 굵직굵직해서 솔직히 지금도 이게 저한테 한해동안 일어난 일이었던가 어떨떨하긴 한데, 돌이켜 보면 정말 그렇습니다. 그렇기에 2024년도 회고야 말로 정말 쓸것도 많고 다시 되짚어 볼 것도 많은 것 같습니다. 가장 기억이 나지 않는 1월부터 달력을 짚어보며 회고를 시작해보겠습니다."
  },
  {
    "objectID": "posts/note/2025-01-05-Goodbye-2024.html#월",
    "href": "posts/note/2025-01-05-Goodbye-2024.html#월",
    "title": "📘Goodbye 2024",
    "section": "1월",
    "text": "1월\n12월 폭풍 전야와 같은 논문 심사를 마치고 마지막 논문 편집을 하며 시간들을 보내고 있었습니다. 2년 동안 다사다난했지만 결국 이 논문 한편을 위해 달려왔다는 사실을 다시 깨닫게 되는 시간이었고, 내 자신이 나름 대견하다고 생각하며 마지막 졸업 논문 편집을 달리고 있었습니다. 비록 졸업 논문이기에 많은 사람들이 읽고 피드백을 주는 연구적인 임팩트가 큰 결과물이라고 볼 수는 없지만, 석사 기간동안 내가 공부한 것들과 주장하고자 하는 것들을 정리하는 시간이 지금까지도 큰 거름이 되는 것 같습니다.\n졸업을 하고 한동안은 나에게 주는 휴식 선물로 취직을 바로 할 생각이 없없지만, 말할 수 없는 사정에 의해 바로 취준을 시작했습니다. 그래서 아직 공채 기간이 시작되지 않았기에 스타트업들 위주로 열심히 지원서들도 쓰기 시작했던 기간이었습니다. 물론 석사 연구 내용과 유관한 직무들을 찾긴했었지만 막상 사회생활 전선으로 나아가니 나를 어필할 부분이 많지 않아 당황스럽기도 하고 조금은 좌절스럽기도 했지만, 다른 측면으로 나의 성과들을 돌아보고 정리할 수 있어서 좋았던 것 같습니다. 취준을 하면서 가장 막막했던 점은 코딩 테스트와 같은 스킬적인 측면을 보는 것들이 스트레스가 심했었는데, 아무래도 지금까지 연구 코드만 작성해오던 사람이라 알고리즘 능력에서는 많이 부족하기도 하고 시간 제한이 있는 상황이 익숙하지 않았던 것 같습니다."
  },
  {
    "objectID": "posts/note/2025-01-05-Goodbye-2024.html#월-1",
    "href": "posts/note/2025-01-05-Goodbye-2024.html#월-1",
    "title": "📘Goodbye 2024",
    "section": "2월",
    "text": "2월\n2월은 졸업식이 있었습니다. 대학교 졸업때는 코로나 시기였기에 친구들도 초대하지 못하고 바로 입학할 대학원으로 복귀해야 하는 상황이어서 졸업식을 잘 즐기지 못했던 아쉬움이 있었습니다. 하지만 석사 졸업은 연구실에 단 한명 졸업에다가 친구들도 초대해서 진정한 주인공 놀이를 할 수 있었던 것 같습니다.\n\n\n\n1월부터 여기저기 열심히 알아보던 취준은 한 스타트업에 합격이 되면서 마무리가 되었습니다. 사실 대기업과 스타트업 사이에서 많은 고민들이 있었지만 조금 더 많은 권한과 다이나믹한 환경에서 일하고 싶다는 생각에 스타트업을 선택했었습니다. 직무와 회사 위치 등 나쁘지 않은 조건으로 3월달부터 일하게 되어서 급하게 동생과 함께 일본 여행을 떠났었습니다.\n\n\n\n급하게 준비하는 여행이라 여행을 준비하는 설레는 마음이 크진 않았지만 그동안 긴 시간 공부하느라 수고 많았다고 스스로에게 상을 주는 여행이었기에 행복했던 시간이었습니다. 생각해보니 대학생때 교환학생을 할 수 있는 기회도, 여행을 갈 수 있는 시기들도 코로나 때문에 다 놓쳐버리고 코로나가 조금 잠잠해졌어도 대학원생이라는 신분에 걸맞게(?) 자유롭지 못했기에 해외 여행은 꽤 오랜시간 동안 마음 한켠에 소원으로만 있었습니다. 가까운 일본 조차 한번도 가본적 없었기에 바로 비행기 티켓을 끊고 떠났던 마음은 정말 최고의 상이었던 것 같습니다."
  },
  {
    "objectID": "posts/note/2025-01-05-Goodbye-2024.html#월-2",
    "href": "posts/note/2025-01-05-Goodbye-2024.html#월-2",
    "title": "📘Goodbye 2024",
    "section": "3월",
    "text": "3월\n진짜 직장인이 되었습니다. 사실 초기에 회사에 적응하는 게 쉽지 않았지만 나이대가 비슷하고 에너제틱한 회사 분위기 때문에 즐거웠던 것 같습니다. 물론 대학원생일 때와 차원이 다른 월급에 신났던 것도 사실이지만요. 마치 신학기에 들뜬 설렘과 긴장감이 교차하는 듯한 느낌을 직딩으로써 느끼는 한달이었습니다."
  },
  {
    "objectID": "posts/note/2025-01-05-Goodbye-2024.html#월-3",
    "href": "posts/note/2025-01-05-Goodbye-2024.html#월-3",
    "title": "📘Goodbye 2024",
    "section": "4월",
    "text": "4월\n4월은 회사에서 워크샵을 다녀왔었고 생각보다 대학교 MT같은 분위기에 재밌게 즐길 수 있었던 것 같습니다. 사실 첫 회사가 대학원 같은 성격이었어서 마치 대학원을 다시 입학한 느낌이긴 했습니다. 그래도 회사이기에 내가 어떻게 기여할 수 있고 어떻게 평가받을 것인지 계속 긴장했던 것 같습니다.\n또한 4월애 20년 이상 알고지낸 친한 언니의 결혼식이 있었는데 정말 감회가 새로웠습니다. 4-5살때부터 알고지내온 언니의 결혼식을 마주하고 나니 저도 벌써 그런 나이가 되었구나 새삼 깨달았고, 지금까지 학교생활만 해온 나에게 이후 삶에 대한 진지한 고민도 하게된 날이었습니다. 졸업하고 직장까지 가지게 되었으니 마치 결혼이 다음 숙제인 것 같은 느낌.. 그리고 나는 그런 인생의 동반자를 만날 수 있을까.. 이런 저런 고민들이 머리속에 떠올랐던 것 같습니다. 각설하고, 언니의 결혼식에서는 진심으로 축하와 행복을 기도했습니다."
  },
  {
    "objectID": "posts/note/2025-01-05-Goodbye-2024.html#월-4",
    "href": "posts/note/2025-01-05-Goodbye-2024.html#월-4",
    "title": "📘Goodbye 2024",
    "section": "5월",
    "text": "5월\n5월은 가정의 달이기도 하고 많은 휴일들이 있어서 행복했습니다. 또한 직딩으로써 처음으로 어버이날에 부모님께 용돈을 드릴 수 있어서 뿌듯했습니다. 28살이 되어서야 드린다는 게 죄송하긴 했지만 그래도 지금이라도 드릴 수 있어서 다행이다 싶었습니다.\n5월에 갑자기 대학 동기를 만나게 된 날이 있었는데, 이 날 제 꿈에 대해 다시 복기해보게 되었었습니다. 연말 회고에 적을 정도로 인상이 깊은 날이었는데, 퇴근을 하고 그 친구를 만나는 날 예전의 빛나는 눈빛이 사라진 것 같다라는 말을 듣게 되었습니다. 사실 회사에서 어떠한 이유로 점점 지쳐가고 있었고 어떻게 타파해야할지 몰라 힘든 상황이었는데 그런 상황을 모르는 친구가 그 이야기를 하자 다른 사람을 통해 진짜 내가 지쳐있구나 확인할 수 있었습니다. 그래서 어떤 방향을 찾아봐야겠다는 능동적인 태도를 취하게 되었고 나를 위한 다른 자리를 찾아보고자 이직을 준비하기 시작했습니다. 회사에서 무슨 보람을 찾는 거냐, 낭만을 쫓는 거 아닌가 싶을 수도 있었지만 친구의 말을 통해서 다시 마음속에 열정을 찾고 싶다는 생각이 들었던 것 같습니다."
  },
  {
    "objectID": "posts/note/2025-01-05-Goodbye-2024.html#월-5",
    "href": "posts/note/2025-01-05-Goodbye-2024.html#월-5",
    "title": "📘Goodbye 2024",
    "section": "6월",
    "text": "6월\n첫 회사에서의 첫 결과물을 내는 시기였습니다. 자세히 말할 수는 없지만 처음으로 사회에서 내가 참여한 프로젝트의 결실을 만들어 낸다는 사실이 많이 부담되기도 하고 정말 잘하고 싶다는 욕심도 많이 들어갔던 것 같습니다. 그 과정에서 미숙한 마음과 태도로 결과의 성패와 상관없이 실망한 부분들이 있었고 프로젝트가 마무리가 되는대로 퇴사를 하겠다는 의사를 전달하게 되었습니다. 퇴사와 별개로 마지막까지 제 역할을 잘 마무리하고 나온 것 같아 아쉬움은 없었습니다. 이때 얻은 교훈들은 지금도 많이 도움이 되고 있습니다.\n사실 이직하는 곳을 어느정도 정해놓았었기 때문에 이후 돈을 벌 걱정은 크지 않았습니다. 다만 이직을 처음하다보니 당황스러웠던 점들도, 두려웠던 점들도 많았습니다. 내가 할 수 있는 역할들과 별개로 다른 사람들과 같이 일한다는 건 많이 다르구나를 느꼈던 것 같습니다. 처음에는 이런 점들에 능숙하지 못한 스스로가 실망스러웠지만 이렇게 배워가며 성장하는 거겠지..라고 지금은 생각하고 있습니다."
  },
  {
    "objectID": "posts/note/2025-01-05-Goodbye-2024.html#월-6",
    "href": "posts/note/2025-01-05-Goodbye-2024.html#월-6",
    "title": "📘Goodbye 2024",
    "section": "7월",
    "text": "7월\n7월 중순까지 다니고 나서 퇴직을 하고, 졸업하고도 많이 놀지 못했으니 한달은 놀아야지! 하고 국내에서 전시회등을 돌아다니며 휴식을 취했었습니다. 버킷리스트 중 하나가 혼자 여행하는 것이 있었는데 서울 종로 한옥 숙소에서 약 4일간 여행을 했습니다.\n\n\n\n서울 태생이지만 바빠서 서촌과 같은 한옥의 정취를 느낄 수 있는 곳들을 가보지 못했었는데 이번 기회에 한옥 숙소에서 맘껏 즐기며 나를 돌아보는 시간을 가졌었습니다. 퇴사와 이직을 하면서 스스로도 단단하지 못함을 느꼈었는데, 왜 그랬는지 어떤 상황에서 내가 어떤 모습이었는지 등등 많은 생각들을 할 수 있었습니다. 이 때 내린 결론은 내가 하는 직무에서 너무 이상적인 Hero를 찾을려고 하지 말자는 결론을 내렸습니다. 주니어고 신입이니까 라는 이유로 내가 하는 직무에서 나를 리드해줄 누군가를 계속 찾느라 내 에너지가 고갈되어가고 있는 것 같았습니다. 사실 로보틱스 + AI를 하는 직무 자체가 신기술이고 빛나보여서 멋져보일지는 몰라도 실제 현장과 산업에서 프로젝트를 성공시킨 사례, 성공시켜본 경험이 있는 사람은 많지 않은 것 같습니다. 그래서 내가 보고 배울 수 있는 표본을 찾기 보다는 결국 내가 처음 찾아서 만들어가야 하는구나를 깨달았던 것 같습니다. 그래서 갑자기 뜬금 없지만 최애 밴드인 The Score의 Don’t need a hero 라는 노래가 이 교훈을 잘 담고 있는 노래라는 생각이 드네요."
  },
  {
    "objectID": "posts/note/2025-01-05-Goodbye-2024.html#월-7",
    "href": "posts/note/2025-01-05-Goodbye-2024.html#월-7",
    "title": "📘Goodbye 2024",
    "section": "8월",
    "text": "8월\n두번째 회사에 출근을 시작했습니다. 분위기와 사람들이 첫회사와 정말 정반대였습니다. 우선 거리부터 판교로 거의 2시간 출근 시간을 확보해야 한다는 것도 이전 회사가 40분 거리였다는 점이라 정말 달랐습니다. 마치 밸런스 게임의 정반대 선택지만 다 고른 것처럼 분위기, 사수, 일하는 방식.. 등등 완전 새로웠던 것 같습니다. 두번째 회사는 지금까지 잘 다니고 있습니다 :)"
  },
  {
    "objectID": "posts/note/2025-01-05-Goodbye-2024.html#월-8",
    "href": "posts/note/2025-01-05-Goodbye-2024.html#월-8",
    "title": "📘Goodbye 2024",
    "section": "9월",
    "text": "9월\n9월에는 점점 다가오는 시연날과 함께 현장도 바쁘게 오가며 열심히 배우고 열심히 일했던 것 같습니다. 첫회사에서도 그랬지만 입사하고 거의 3개월 내에 항상 시연해야하는 일이 있었습니다. 그래도 팀원들과 사수의 좋은 팀워크로 일하는 동안 외롭지는 않았었습니다."
  },
  {
    "objectID": "posts/note/2025-01-05-Goodbye-2024.html#월-9",
    "href": "posts/note/2025-01-05-Goodbye-2024.html#월-9",
    "title": "📘Goodbye 2024",
    "section": "10월",
    "text": "10월\n10월.. 잊지 못할 시연이 있었습니다. 시연 전날 새벽까지도 제가 맡은 부분이 잘 작동되지 않아 완전히 멘탈이 나갔었지만 팀장님과 팀원들의 도움과 끝까지 놓을 수 없었던(?) 멘탈을 붙잡아 결국 성공을 시켰습니다. 지금도 그 시연날을 생각하면 아찔하긴 하지만 결과적으로 좋은 결과를 냈기에 지금은 웃으면서 회고를 적을 수 있습니다. 원래 10월은 제 생일있는 달이기에 가장 좋아하는 달인데 24년도 10월은 시연 때문에 순간 최악의 달이 될 뻔 했습니다😅\n\n\n\n생일 말고도 또 한가지 경사가 있는 10월이었습니다! 석사 과정을 시작하면서 시작했던 4족 보행 로봇관련 연구자료들을 모은 awesome list repository가 512개 star를 달성했습니다! 사실 512개까지 모을 수 있을지 모르고 이전에 포스팅으로 자축을 다 한 상태였지만 이후로도 스타가 많이 늘어서 너무 기뻤던 하루였습니다.(현재 회고를 작성하고 있는 순간에는 벌써 606개의 스타가..!)"
  },
  {
    "objectID": "posts/note/2025-01-05-Goodbye-2024.html#월-10",
    "href": "posts/note/2025-01-05-Goodbye-2024.html#월-10",
    "title": "📘Goodbye 2024",
    "section": "11월",
    "text": "11월\n11월은 조금 한숨 쉬어가는 한달이었는데 지금까지 일과 공부에만 전념했던 생활에 조금 tweek을 준 기간이었습니다. 인간관계에 공부를 하는 시간을 가졌었는데 공개되는 회고에 적기에는 조금 민감할 수 있어서 이 부분은 개인적인 다이어리에만 쓰려고 합니다.\n직무적으로는 조금씩 자기효용감이 생기기 시작했던 것 같습니다. 두번째 회사에서의 적응도 어느정도 된 상태에서 내가 어떤 역할을 할 수 있는지, 그리고 회사에서 어떤 점을 바라고 있는지 align을 하면서 내가 성장할 수 있겠다라는 확신이 생겼었습니다. 특히나 회사에서 얻기 쉽지 않은 기회인 Facebook Meta와 협업하는 프로젝트에 내가 involve 할 수 있다는 사실에 감사하며 내가 할 수 있는 부분이 무엇인지 더 능동적인 태도를 가질 수 있게 되어서 좋았던 것 같습니다."
  },
  {
    "objectID": "posts/note/2025-01-05-Goodbye-2024.html#월-11",
    "href": "posts/note/2025-01-05-Goodbye-2024.html#월-11",
    "title": "📘Goodbye 2024",
    "section": "12월",
    "text": "12월\n연말이 다가오는 12월에 크리스마스 분위기를 한껏 느끼며 시간을 보냈었습니다. 첫번째 회사도 5개월, 두번째 회사도 5개월째 되는 달이라서 스스로 각 회사에서 느낀점이 어떤 부분에서 다른지 짚어보았습니다. 그 과정에서 내가 어떤 부분에서 취약하다고 느끼는지 파악해볼 수 있었습니다.\n12월에 갑자기 친구들의 추진력 덕분에 필리핀 여행을 다녀왔습니다. 딱 24년도 말일까지 필리핀에 있는 여행일정이었습니다.\n\n\n\n처음으로 동남아로 가족들이 아닌 친구들끼리 가는 여행이라 긴장도, 걱정도 많이했었지만 엑티비티도 많이하고 추억들을 많이 쌓을 수 있어서 정말 재밌었던 여행이었습니다. 여행을 하면서 내가 막연히 가지고 있는 걱정이 생각보다 많고 경험했을때 깨달을 수 있는 부분들이 많다는 걸 다시한번 짚어볼 수 있었습니다."
  },
  {
    "objectID": "posts/note/2025-01-05-Goodbye-2024.html#마무리",
    "href": "posts/note/2025-01-05-Goodbye-2024.html#마무리",
    "title": "📘Goodbye 2024",
    "section": "마무리",
    "text": "마무리\n한때 회고를 남기는 것에 대해 회의적일 때가 있었습니다. 그런데 기억한다는 점은 다음 단계로 나아가는 첫걸음이라는 생각이 들고 나서 다시 회고를 하고 있습니다. 글의 서두에도 적었듯이 2024년도가 정말 많은 일들이 있던 한해였기에 전체 인생에서도 기억에 남을 한해가 될 것 같습니다. 다가오는 2025년도에 가끔씩 24년도 회고를 보러올 것 같은데 그때에는 조금이라도 성장해있는 또 다른 내가 되어있길 바라며 회고를 이만 맺겠습니다."
  },
  {
    "objectID": "posts/note/2022-10-24-daily-english-004.html",
    "href": "posts/note/2022-10-24-daily-english-004.html",
    "title": "🌎IT English Experssions 004",
    "section": "",
    "text": "영어 회화 표현이나 논문에서 쓸 수 있는 유용한 표현들을 조금씩 익히기 위해 시작한 1000일 시리즈 포스트 입니다. 공부 리소스는 유튜브, 리얼클래스, PN님의 블로그, cake 어플, 책, 논문, 기사 등 다양한 출처에서 보고 기록합니다."
  },
  {
    "objectID": "posts/note/2022-10-24-daily-english-004.html#git으로-세련되게-협업하는-방법",
    "href": "posts/note/2022-10-24-daily-english-004.html#git으로-세련되게-협업하는-방법",
    "title": "🌎IT English Experssions 004",
    "section": "Git으로 세련되게 협업하는 방법",
    "text": "Git으로 세련되게 협업하는 방법\n\nConventional Commits\n\n커밋 메세지는 Subject (Title), Body, Footer로 구분\n구조\n\n&lt;type&gt;[optional scope]:&lt;description&gt;\n[optional body]\n[optional footer(s)]\n\n예시(http://www.conventionalcommits.org/)\n\nfix: prevent racing of requests                           ---&gt; Subject or Title\nIntroduce a request id and a reference to latest request. ---&gt; Body\nReviewed-by : Z                                           ---&gt; Footer(s)\nRefs : #123\n\n\nSubject(Title) 작성법\n커밋 메세지의 제목은 변경사항을 대표하는 텍스트이므로 대표적인 타입들이 있다.\n\nfeat: 코드에 새로운 기능(=feature) 추가\nfix: 버그 수정\nBREAKING CHANGE: 이전 버전과 호환되지 않는 변경 내역. !으로 표시할 수 있음. e.g.) feat!:\ndocs: 개발 문서 변경\nstyle: 들여쓰기, 따옴표, 세미콜론 등 코드 형식 및 스타일 변경\nci: CI/CD(continueout integration and deployment) 관련 코드 변경\nrefactor: 중복된 코드 제거, 변수명 변경, 코드 단순화 등 리팩터링\ntest: 테스트 관련 코드 변경\nbuild: 빌드 시스템 관련 코드 변경\nperf: 성능 개선 관련 코드 변경\nchore: 기타 코드 변경\n\n예시\n\nfix: remove deprecated features 수정: 권장되지 않는 기능 삭제\nfeat: add parameters to getImage 기능: getImage에 매개변수 추가\ndocs(readme): update build instructions 문서(readme): 빌드 지침 업데이트\nchore: update np dependencies to latest version 기타: npm 의존성 최신버전으로 업테이트\n\n나만의 예시 연습\n\ndocs(readme): add new papers \"THE TITLE OF PAPER\"\nfeat: add getJointPosition function\nrefactor: delete overlapped constants and variables of robot model\n\n\n\n5가지 커밋 작성법\n\n동사 원형으로 시작\n\n\n제목은 명령적 어조(Imperative Mood)의 동사원형으로 시작\nBody, Footer는 명령문이 아니어도 됨\n상황에 따라 과거형(e.g. Added) 또는 3인칭 단수 현재형(e.g. Adds)를 사용하기도 함\n커밋 메세지에 자주 등장하는 동사\n\nFix: 수정하다\nImprove: 개선하다\nHandle: 처리하다\nOptimize: 최적화하다\nUpdate: 업데이트하다\nImplement: 구현하다, 적용하다\nRefactor: 리펙터링하다\nAdd: 추가하다\nRevert: 되돌리다\nChange: 변경하다\nReplace: 대체하다\nMerge: 병합하다\nDocument: 문서를 작성하다\nBump: 버전을 올리다\nSimplify: 단순화시키다\nEnable: 가능하게 하다\nRun: 실행하다\nClean: 제거하다, 정리하다\nWrap: 감싸다, 그룹화하다\nDeploy: 배포하다\nModify: 변경하다\nRemove: 제거하다\nRename: 이름을 바꾸다\nMove: 이동하다, 이동시키다\n\n\n\n모두 소문자로 또는 첫 글자만 대문자로\n\n\nConventional Commits 형식에서는 모든 문자를 소문자로 작성. type: description에 맞추어 메세지 작성\nConventional Commits 형식을 적용하지 않는 경우, 일반적으로 앞부분만 대문자 사용하고 type 생략\n예시\n\n[1] docs: update build.md with detailed instructions\n[2] Update build.md with detailed instructions\n\n관사(a, an, the와 같은 Article) 생략\n\n\n커밋 타이틀은 최대 50글자 제한\n핵심 키워드만 활용하여 메세지를 작성하기 위해 관사 생략\n예시\n\n[X] Fix a typo in the header\n[O] Fix typo in header\n\n마침표와 같은 구두점(Punctuation Mark) 생략\n\n\n반드시 필요한 경우가 아니면 쉼표, 하이픈 등 생략\n예시\n\n[X] feat: implement google analytics.\n[O] feat: implement google analytics\n\n변경한 이유, 상세한 설명은 본문(Body)에\n\n\n코드 변경 사유와 상세 설명은 커밋 본문에 씀\n\n\n\nGit 주요 실무 영어\n\nSquash the last 3 commits: 최근 3개 커밋을 합치다\nPush commits to a repository: 리포지터리(코드 저장소)로 커밋을 전달하다\nMerge a feature branch into the base branch: 기능 브랜치를 기본 브랜치에 병합하다\nRevert a pull request: 풀 리퀘스트를 되돌리다(이전 상태로 되돌리다)\nRequest a review: 검토를 요청하다\nComment on a pull request: 풀 리퀘스트에 댓글을 남기다\nResolve a merge conflict: 병합 충돌을 해결하다\nRebase onto another branch: 다른 브랜치로 리베이스(base를 재설정하여 커밋 재적용)하다\nClone a repository: 리포지터리를 복제하다\nClose a pull request without merging it onto the branch: 풀 리퀘스트를 병합하지 않고 종료하다"
  },
  {
    "objectID": "posts/note/2023-05-27-twinkle-github-star.html",
    "href": "posts/note/2023-05-27-twinkle-github-star.html",
    "title": "📘Github Starstruck 128",
    "section": "",
    "text": "2023.05.27 또 하나의 작은 성공을 기록하게 된 날이 되었습니다.\n사실 5월달이 들어서면서부터 이 날을 기록할 수 있기를 바라며 로그를 체크하고 있었는데 드디어 새벽 1시 11분에 달성이 되었다고 로그가 떠서 반가운 마음에 이야기를 남기기 위해 블로그 글을 쓰게 되었습니다."
  },
  {
    "objectID": "posts/note/2023-05-27-twinkle-github-star.html#motivation",
    "href": "posts/note/2023-05-27-twinkle-github-star.html#motivation",
    "title": "📘Github Starstruck 128",
    "section": "Motivation",
    "text": "Motivation\n잠깐 이야기가 나왔던 왜 이 repo를 만들게 되었는지에 대해 이야기해보려고 합니다. Github에서는 커밋 히스토리를 추적할 수 있으니까 이 레포를 처음 만들 당시로 돌아가서 first commit을 봤습니다. 사실 저는 막연하게 제가 대학원 과정을 시작하며 만들었겠거니..라고 예상하고 있었는데 그 보다 더 일찍, 그러니까 좀 더 정확하게 말하자면 대학교 4학년 막학기 10월에 만든 것이라는 것을 확인할 수 있었습니다.\n\n제 기억이 맞다면 당시에 졸업작품과 졸업 이후의 진로 고민으로 인해 이런저런 고민을 많이 하는 시기였는데 아마 이때쯤 진학할 대학원들을 알아보고 해당 레포를 만들면서 만약에 내가 로보틱스를 공부한다면 어떤 것을 알아야 하는지 정리해보기 위해 만든 것 같습니다. 그때만 하더라도 4족 보행 로봇이라는 정확한 로봇의 종류에 대해 결정되지 않았기에 더 폭넓게 Robotics라고 정하고 시작했던 것 같습니다.\n대학원을 진학한 이후, 알아야 할 것들은 너무 많고 여러 연구실에서 쏟아지는 논문들을 감당할 수 없어서 정리를 해야겠다 생각했습니다. 처음에는 아무도 관심없지만 괜히 공개되는 인터셋 상에 올리는 것은 부담스러워서 개인적으로 기록하는 곳(notion이나 privite repo)에 모았지만, 지금까지 저도 다른 사람들이 정리한 리스트들과 인사이트들을 통해 많이 성장할 수 있었고 나도 아직 많이 부족하지만 기여하는 부분이 있으면 좋겠다는 생각에 공개 repo로 전환하여 모으기 시작했습니다. 공개된 리스트라고 생각하니 좀 더 신경을 쓰게 되고 좋은 자극 효과가 되서 내가 기록한 내용을 한번 더 보게되고 그런 경험들이 쌓여 인사이트도 성장하게 되어서 결과적으로 제 스스로에게 더 도움이 많이 되었던 것 같습니다."
  },
  {
    "objectID": "posts/note/2023-05-27-twinkle-github-star.html#how-to-write",
    "href": "posts/note/2023-05-27-twinkle-github-star.html#how-to-write",
    "title": "📘Github Starstruck 128",
    "section": "How to write",
    "text": "How to write\n제가 어떻게 해당 repo를 관리하고 작성해오고 있는지 몇가지 팁들을 공유해보려고 합니다.\nSource\n큐레이션이나 어떤 도움이 되는 정보를 리스트화 한다는 것은 많은 소스들이 우선 전제가 되어야 합니다. 관련 분야 연구자로써 같이 연구하는 동료들과 선후배로부터 얻는 정보들도 물론 많습니다. 하지만 이렇게 privite group에서 얻는 정보들 이외에도 제가 source들을 얻는 주요 경로들은 아래와 같습니다.\n\nGoogle Scholar에서 해당 분야에서 활발하게 연구 성과를 내고 있는 연구자들을 follow합니다. 이메일로 해당 연구자가 paper를 내거나 citation되면 알람이 오는데 저는 빠르게 알람온 리스트들을 abstract/result를 살펴보고 공유하기에 좋다고 판단되면 리스팅합니다.\nYoutube 채널 구독을 통해 얻게 되는 정보들도 많습니다. 생각보다 아카이브에 페이퍼를 먼저 올리기 보다 결과 영상이나 설명 영상들을 유튜브에 먼저 올려서 소식을 먼저 알리는 곳들이 많습니다. 제가 repo 카테고리에 youtube 채널을 넣은 이유이기도 합니다.\nGithub도 연구자들이 코드를 올리면서 README에 연구에 대한 설명과 함께 공개합니다. 유튜브와 같은 맥락으로 페이퍼보다 먼저 공개하는 경우들도 많고 오히려 코드를 공개해주다보니 연구를 더 쉽고 빠르게 파악할 수 있을 때도 있습니다. 그래서 관련 연구자들을 follow해서 어떤 커밋을 올리고 있는지, 어떤 레포들을 관심있게 보는지(star/cloning)도 알게 되고 Github organization으로 연구실이나 회사 단위로 활동하는 것을 파악하여 정보를 얻을 때도 있습니다.\n\nEditor\nmarkdown을 작성하는게 복잡한 소스코드도 아니기 때문에 git clone/commit 하는게 어려운 일은 아니지만, awesome list를 작성하기 위해 매번 다른 컴퓨터에서 clone하고 커밋하며 싱크를 맞추는 일이 생각보다 귀찮습니다. 그래서 저는 awesome list를 작성할 때는 온라인으로 특별한 editor 없이 바로 작성하고 정렬도 할 수 있는 github Code space를 사용합니다. README.md 리스트 알파벳순 자동 정렬을 python code(asset/ordering.py)로 하기 때문에 python이 설치되어있지 않은 컴퓨터에서는 정렬이 어려운데 Code space를 사용하면 python이 설치되어 있지 않은 인터넷이 되는 모든 컴퓨터에서 작성할 수 있으니 everywhere, anytime이 가능하고 UI는 VS Code와 같기 때문에 어렵지도 않습니다. 커밋도 바로 하구요!"
  },
  {
    "objectID": "posts/note/2023-12-01-geultto-9th-start.html",
    "href": "posts/note/2023-12-01-geultto-9th-start.html",
    "title": "📘Geultto 9th Start",
    "section": "",
    "text": "끝과 시작\n7, 8기에 이어 3번째로 글또 9기를 시작하게 되었습니다.\n사실 7기 시작할 때만 하더라도 계속 이어서 참여를 하게 될 것이라고 생각하진 않았었는데, 지금까지 계속 글을 써오면서 재미를 느끼기도 했고 약간의 반강제성이 있어야 블로그에 하나라도 글을 작성하는 제 모습을 보며 나에게 글또가 필요한 존재라는 것을 깨달았습니다.\n2023년 연말을 향해, 그리고 제 인생에서는 대학원 생활의 마무리를 하는 과정에서 끝을 향해 달려가고 있는 듯한 느낌이지만, 글또 9기를 시작하며 끝과 시작이 이어지는 듯한 느낌이라 설레이는 마음도 큰 것 같습니다. 언제나 그랬듯이, 다시 글또라는 커뮤니티 안에서 내가 쓰고자 하는 글들, 하고자 하는 일들에 대해 정리를 해보는 글을 써보려고 합니다. 7기 다짐글은 블로그 삽질했던 이야기와 함께 간단하게 블로그에 적고 싶은 주제들 등을 적었었고, 8기 다짐글은 글또 OT에서 소개해주셨던 Big5 유형 검사와 함께 논문의 형식을 빌려 나름 패러디 형식으로 작성했었던 것 같습니다. 이번에는 어떻게 시작하는 이야기를 할 수 있을까.. 하다가 생각난 어렸을 적 기억남는 에피소드를 하나 회상하는 것으로 시작하려 합니다.\n\n\nDomino Tower\n정확한 시기는 기억이 나지 않지만 아마 초등학교 4-5학년 그쯤이었던 것 같습니다. 학교에서 수련회를 가면 항상 레크레이션 프로그램으로 협동 게임을 진행했었는데, 한번은 도미노 블럭들을 가지고 가장 긴 도미노 줄 세우기, 도미노로 가장 높게 쌓기 등을 반 끼리 대결하는 시간이었습니다.\n\n\n\n세계에서 가장 높은 도미노 타워는 한 대학원생이 세운 5.275m 라고 하네요 (Source: World’s tallest domino tower)\n\n\n위의 사진처럼 도미노로 가장 높게 탑 쌓기로 대결에서 다른 친구들이 각기 다른 여러 모양으로 탑을 쌓아가기 시작했고, 한 반에 약 30명 정도 있었어서 여러 모양의 탑들이 올라가기 시작했습니다. 다양한 아이디어들을 가지고 각기 모여졌다 흩어졌다 하며 무너지고 쌓기를 반복하며 각자 나름대로의 열정으로 미션에 열중하고 있었습니다.\n저도 몇번 시도해보고 친구들이 쌓는 모습들도 보면서 오각형으로 쌓았을 때가 가장 안정적인 것 같다 라는 걸 판단할 수 있었고, 그 때부터 조용히 혼자 하나하나 쌓아 올려갔습니다. 그때도 성격이 조용했던 터라 그냥 생각했던 대로 혼자 만들기 시작했고, 반 친구들은 옆에서 반대표 작품 하나를 크게 만드는 것에 집중하고 있었습니다. 혼자서 만들다보니 생각보다 너무 잘 쌓아 올려져서 의자를 빌려서 까지 올라가서 쌓아갔었고 결과적으로 저희 반이 제가 쌓기 시작했던 탑으로 1등을 했었습니다. 수련회 교관 선생님들도 꽤 많이 놀라셨던 게 기억이 납니다. 탑의 높이가 가장 높았을 뿐만 아니라 나중에 쓰러뜨리기 위해 중간에 한 조각 건드렸음에도 안 무너졌을 정도로 도미노 탑을 잘 만들었었던 걸로 기억합니다.\n가끔씩 지금 하고 있는 일이 제대로 하고 있는 걸까 스스로 물음표가 생길 때마다 무의식적으로 이 도미노 탑 에피소드가 떠오르는데, 아마 그 때의 기억이 나의 관찰과 생각으로 하나씩 쌓다보면 무언가 완성이 될 것이다라는 경험과 교훈을 떠오르게 해서 그런 것 같습니다.\n\n\n지금의 도미노 한 조각\n그렇다면 지금 내 도미노 한 조각은 어디에 올라가고 있는지.\n나의 물음표에 쉽게 답을 하지 못하면, 최대한 단순하게 내 도미노 한 조각이 어디로 향하고 있는지 고민해봅니다. 자세한 고민들과 여러 곁가지 생각들은 23년도 회고록에 남기기로 하고.. 이번에는 글또 9기 시작 다짐글인 만큼, 엔지니어를 꿈꾸는 사람으로써 글을 쓰는 사람이 되고 싶은 마음 한 조각 도미노 블럭에 대한 이야기를 풀어보려고 합니다.\n앞서 이야기 했듯이 글또라는 모임을 통해 반강제적인 글쓰는 스케줄링을 통해서 지금까지 블로그에 어렵게 쌓아온 글들이 나름 뿌듯합니다. 백프로 모든 글들이 맘에 들지 않지만, 써왔던 글들을 가끔 다시 읽어보며 아 맞아 이걸 배울 수 있었지, 이건 이렇게 쓰면 좋았을 껄, 이건 잘 쓴 것 같다등 회고를 하고 한번 더 성장하는 느낌이 듭니다. 아마 도미노 2-3층 짜리는 되지 않았을까? 생각해봅니다. (적어도 이제 블로그 이사는 안 다니고 안정적으로 이 블로그에다 쓸 수 있을 것 같으니 탑을 쌓아 올리기 전에 기반 공사 오래했다고 생각해도 될 듯 하네요.)\n그래서 앞으로 3층 이상의 블로그 도미노 탑을 쌓기 위해 글또 9기에서 다짐해보는 내용은 아래와 같습니다.\n\n글또 마감 지켜서 모든 회차 제출하기: 매 기수 첫 다짐이지만 놀랍게도 한번도 지키지 못했다는 전설의 다짐. 이번에도 솔직히 지킬 수 있을까 두렵지만 삼세판의 기적을 믿어보겠습니다.\n큐레이션 선정 3번 되어보기: 저번부터 생긴 큐레이션 선정이 동기부여가 많이 되었기도 하고 스스로 글을 더 점검하고 퇴고하는 즐거움도 느낄 수 있어서 이번에도 3번 정도는 채워보자!를 목표로 설정해보려고 합니다. 사실 저번 기수에 4번 선정 되었었지만 이번에는 왠지.. 석사 졸업하고 퀄리티 있는 글을 쓸 시간이 줄어들 것 같은 노파심에 1회 줄여서 현실성 있게 3번이면 잘 활동한 거라고 스스로 상주고 싶네요.\n글또에서 더 넓은 세상을 보고 지혜롭게 살아가는 법 배우기: 이전 기수에서 커피챗, 반상회 모임 등을 통해서 이미 멋진 분들을 보고 동기부여를 많이 받았었기에 이번에도 글또에서 얻을 수 있는 혜택인 글또 안에서 멋진 분들의 생각들을 들을 수 있는 기회들을 가지고자 합니다. 그런데 이번에는 횟수보다는 만남의 여운을 좀 더 신경 써보기로 했습니다..! Action plan으로는 만나는 분들과의 대화 속에서 나는 무엇을 느끼고 배웠는지 진지하게 인터뷰이처럼 기록해볼 생각입니다.(물론 이 이야기는 제 개인 소장 노트에만 적을 것 같습니다.)\n\n이 세가지 목표가 이번 글또 9기에서 이상 없이 이루어지길..! 노력해보겠습니다.\n\n\n마무리\n최근에 새벽에 논문을 쓰다가 스쳐 지나가는 생각이 있어서 메모지에 적었는데, 지금 글또 다짐글을 적다가 앞에 메모지가 보여서 옮겨 적어보았습니다.\n머리와 마음이 생각처럼 되지 않을 때는 손이나 발을 움직이자.\n머리가 안돌아가면 손을 움직이고\n생각이 멈춰있으면 발을 움직이자.\n마치 생각을 뻗어나가야 하는 상황에서는 발을 내딛듯한 느낌을 받아야 하고,\n마음을 정리해야 할 때에는 손으로 끄적이며 엉킨 실타래를 풀어나가야 하는 것처럼\n보이지 않는 것과 보이는 것을 무의식적으로 맞춰나갈 때 해결이 되는 것 같다.\n항상 앉아서 공부하고 정숙한 환경에서 눈 앞의 책 위의 활자들을 보며 열심히 머리를 굴리는 방법만 익히다 보면, 가끔씩 몸의 움직임의 중요성을 까먹는 것 같습니다. 요근래 움직이면 오히려 머릿속과 마음속에서 복잡했던 것들이 쉽게 풀리는 걸 경험했었기에 이번에도 글을 쓰거나 공부하면서 막혀서 멍때리고 있을 미래의 나에게 움직이고 다시 생각해보자라는 메세지를 남기며 다짐글을 이만 마치겠습니다."
  },
  {
    "objectID": "posts/note/2022-10-11-daily-english-002.html",
    "href": "posts/note/2022-10-11-daily-english-002.html",
    "title": "🌎Casual English Phrases 002",
    "section": "",
    "text": "영어 회화 표현이나 논문에서 쓸 수 있는 유용한 표현들을 조금씩 익히기 위해 시작한 1000일 시리즈 포스트 입니다. 공부 리소스는 유튜브, 리얼클래스, PN님의 블로그, cake 어플, 책, 논문, 기사 등 다양한 출처에서 보고 기록합니다.\n\n\n속마음을 보여주다\nwear your heart on your sleeve\n\nIf you wear your heart on your sleeve, you openly show your feelings or emotions rather than keeping them hidden. 속마음을 보여준다는 말은, 너의 감정이나 느낌들을 숨기기보다 보여준다는 거야.\nIt takes courage to wear your heart on your sleeve, which is why few people do it. 속마음을 털어놓는다는 건 용기가 필요한 일이기 때문에 많은 사람들이 하지 못하는 일이지.\nI need someone whom I can wear my heart on my sleeve. 내 속마음을 털어놓을 사람이 필요해.\n\n\n\n고지식하거나 융통성 없는 성격\n약간 모범생 이미지\nsquare\nIf you are a SQUARE: You are an organized, logical, and hardworking person who likes structure and rules.\n\nBe there, or be square. : The expression be there or be square means that if one declines to attend an event, one is considered “uncool.” It implies that the event will be exciting.\nDon’t be square. 고리타분하게 굴지마\nShe is such a square - I’ve never met anyone so boring. 그녀는 진짜 고지식한 사람이야 이때껏 그사람처럼 지루한 사람은 처음봤어.\n\n\n\n위선적인\ntwo-faced\n\nDon’t trust her - I suspect she’s a bit two-faced. 그녀를 믿지마. 그녀가 조금 위선적인 사람인 것 같아.\nHe’s a two-faced cheater. 그는 두얼굴을 사진 사기꾼이야.\nHe had been devious and two-faced. 그는 기만적인고 양면적이었다.\n\ndevious : 정직하지 못한, 기만적인"
  },
  {
    "objectID": "posts/note/2023-03-30-daily-english-006.html",
    "href": "posts/note/2023-03-30-daily-english-006.html",
    "title": "🌎Casual English Phrases 006",
    "section": "",
    "text": "불만족스러운, 불편한\nnot (too) happy\n\nStomach’s not too happy, is it? 배가 고프구나?(아프구나)\nMy stomach’s not happy with me. 배가 아파.(고파)\n\n\n가끔씩 대화를 하다가 맥락을 서로가 알고 있는 상황이면 a 나 your 등을 생략하는 경우가 있음.(Stomach)\n\n\n\n유감스럽게도\nI’m afraid ~\n\nI’m afraid I haven’t heard any cookies calling you, plain or otherwise. 유감이지만 그냥 쿠키든 아니든 널 부르는 쿠키는 없었어.\nI’m afraid your suggestion is not accepted. 유감이지만 네 제안이 받아들여지지 않았어.\nI’m afriad I can’t go to your party. 미안하지만 네 파티에 못 갈 것 같아.\n\n\n\n속이다\nput (someone) on\n\nHe must be wondering if I was just putting him on. 내가 자기를 속이고 있는 건지 궁금해하고 있을 게 뻔해.\nDon’t believe it! She’s putting you on! 믿지마! 그녀는 너를 속이고 있어.\n\n\n\n기다리다 / 견디다, 버티다\nhang on\n\nHang on a few more minutes, I’m looking. 조금만 더 기다려, 찾고 있으니까.\nHang on a moment. 네 잠시만 기다리세요.\nIf you can just hang on for a little longer, we’ll finish soon. 네가 조금만 더 버텨준다면, 우리는 곧 끝낼거야.\n\n\n\n~로 만들어지다\nbe made of\n\nWhat’s it made of, rotten zombie eyes or something? 대체 뭐로 만든거야, 썩은 좀비 눈 같은 건가?\nThis desk is made of baobab wood. 이 책상은 바오밥 나무로 만들어졌다.\n\n\nbe made from : 재료의 형태를 알아 볼수없는 경우. Cheese is made from milk.\n\n\n\n역겨운\nsick\n\nJust looking at it almost makes me sick. 그냥 보기만 해도 구역질 나와.\nHe makes me sick! 그가 하는 짓은 역겨워!\nJust thinking about it makes me sick to the stomach. 그거 생각하는 것만으로도 정말 역겨워."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Curieux.JY",
    "section": "",
    "text": "Hello👋 I’m Jung Yeon. Thanks for visiting my blog.\n\n\nThis is where I document everything I explore with curiosity.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n👩‍💻타원 Fitting\n\n\n\n\n\n\nellipse\n\n\nopencv\n\n\ncode\n\n\n\n타원 Fitting에 대해 scratch부터 opencv까지 살펴보기\n\n\n\n\n\nJan 10, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n👩‍💻ROS2 RealSense C++ Node\n\n\n\n\n\n\nros2\n\n\nrealsense\n\n\ncode\n\n\n\nC++로 ROS2 RealSense 카메라 노드 만들기\n\n\n\n\n\nJan 10, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n📃Rotating without Seeing 리뷰\n\n\n\n\n\n\npaper\n\n\ntactile\n\n\nrl\n\n\nhand\n\n\n\nTowards In-hand Dexterity through Touch\n\n\n\n\n\nDec 22, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n📃IPO 리뷰\n\n\n\n\n\n\npaper\n\n\nrl\n\n\ncmdp\n\n\n\nInterior-point Policy Optimization under Constraints\n\n\n\n\n\nNov 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n📃VCGS 리뷰\n\n\n\n\n\n\ngrasp\n\n\npointcloud\n\n\nvae\n\n\npaper\n\n\n\nVariational Constrained Grasp Sample\n\n\n\n\n\nMar 17, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n👩‍💻Quarto Blog + α\n\n\n\n\n\n\nblog\n\n\nquarto\n\n\ncode\n\n\n\nQuarto로 속편한 Github Blog 구축하기(2)\n\n\n\n\n\nJan 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n👩‍💻Quarto Blog 기본 셋팅\n\n\n\n\n\n\nblog\n\n\nquarto\n\n\ncode\n\n\n\nQuarto로 속편한 Github Blog 구축하기(1)\n\n\n\n\n\nJan 7, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n📃DreamWaQ 리뷰\n\n\n\n\n\n\ncontext\n\n\nrl\n\n\npaper\n\n\n\nLearning Robust Quadrupedal Locomotion With Implicit Terrain Imagination via Deep Reinforcement Learning\n\n\n\n\n\nJul 2, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n👩‍💻Chord Graph\n\n\n\n\n\n\nchord\n\n\nvisualization\n\n\ncode\n\n\n\nHoloViews를 이용하여 Chord Graph 그리기\n\n\n\n\n\nJun 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n📃K-Accessibility 리뷰\n\n\n\n\n\n\nrl\n\n\nclustering\n\n\nquadruped\n\n\nrecovery\n\n\nbackflip\n\n\npaper\n\n\n\nAccessibility-Based Clustering for Efficient Learning of Locomotion Skills\n\n\n\n\n\nMay 7, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n👩‍💻Orbit Existing Scripts\n\n\n\n\n\n\norbit\n\n\nisaacsim\n\n\ncode\n\n\n\nIsaac Orbit Series 002\n\n\n\n\n\nApr 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n👩‍💻Orbit 설치하기\n\n\n\n\n\n\norbit\n\n\nisaacsim\n\n\ncode\n\n\n\nIsaac Orbit Series 001\n\n\n\n\n\nApr 4, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n📃WASABI 리뷰\n\n\n\n\n\n\nrl\n\n\ngan\n\n\nquadruped\n\n\nbackflip\n\n\npaper\n\n\n\nLearning Agile Skills via Adversarial Imitation of Rough Partial Demonstrations\n\n\n\n\n\nMar 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n👩‍💻class ⟷ dict ⟷ yaml\n\n\n\n\n\n\nconfig\n\n\nyaml\n\n\ncode\n\n\n\n여러 파라미터들을 기록하기 위한 config 관리\n\n\n\n\n\nDec 26, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n👩‍💻torch.Tensor vs torch.tensor\n\n\n\n\n\n\ntorch\n\n\ntensor\n\n\ninverted\n\n\ncode\n\n\n\ntorch Tensor 객체 생성 방법 비교와 Inverted 연산 확인하기\n\n\n\n\n\nDec 21, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n👩‍💻Linux GPU 상태 확인하기\n\n\n\n\n\n\nlinux\n\n\ngpu\n\n\ncode\n\n\n\n리눅스에서 GPU 상태를 확인하는 여러가지 방법을 알아봅니다.\n\n\n\n\n\nDec 14, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n📃Robust Recovery Controller 리뷰\n\n\n\n\n\n\nrl\n\n\ntrpo\n\n\ngae\n\n\nrecovery\n\n\nquadruped\n\n\npaper\n\n\n\nRobust Recovery Controller for a Quadrupedal Robot using Deep Reinforcement Learning\n\n\n\n\n\nOct 16, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n📃VAE 리뷰\n\n\n\n\n\n\ngenerative\n\n\nvae\n\n\npaper\n\n\n\nAuto-Encoding Variational Bayes\n\n\n\n\n\nOct 2, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n📃WaveNet 리뷰\n\n\n\n\n\n\nautoregressive\n\n\ngenerative\n\n\npaper\n\n\n\nA Generative Model for Raw Audio\n\n\n\n\n\nSep 17, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n📃GN-Block 리뷰\n\n\n\n\n\n\ngnn\n\n\nsystem identification\n\n\nmpc\n\n\nrl\n\n\npaper\n\n\n\nGraph Networks as Learnable Physics Engines for Inference and Control\n\n\n\n\n\nAug 7, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n📃Legged Robots that Keep on Learning 리뷰\n\n\n\n\n\n\nquadruped\n\n\nrl\n\n\nredq\n\n\npaper\n\n\n\nFine-Tuning Locomotion Policies in the Real World\n\n\n\n\n\nJun 26, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n📃NerveNet 리뷰\n\n\n\n\n\n\ngnn\n\n\nrl\n\n\npaper\n\n\n\nLearning Structured Policy with Graph Neural Networks\n\n\n\n\n\nJun 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n🧩GNN Materials\n\n\n\n\n\n\ngnn\n\n\nstorage\n\n\n\nGNN 자료들 모음\n\n\n\n\n\nJan 2, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n👩‍💻Import custom module\n\n\n\n\n\n\npython\n\n\ncode\n\n\n\ncustom module을 불러오는 방법\n\n\n\n\n\nJul 20, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n🧩My Reinforcement Learning Roadmap\n\n\n\n\n\n\nrl\n\n\nstudy\n\n\nstorage\n\n\n\n내가 공부했던 강화학습 Roadmap\n\n\n\n\n\nJul 17, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n👩‍💻Install Mujoco in Windows10\n\n\n\n\n\n\nmujoco\n\n\ncode\n\n\n\nRL에서 많이 쓰이는 시뮬레이션 Mujoco Windows10에 설치하기\n\n\n\n\n\nJul 13, 2020\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "post.html",
    "href": "post.html",
    "title": "Post",
    "section": "",
    "text": "📃 Paper Review | 🧩 Storage | 👩‍💻 Code\n\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n\n\n\n\n\nDate\n\n\nTitle\n\n\n\n\n\n\nJan 10, 2025\n\n\n👩‍💻타원 Fitting\n\n\n\n\nJan 10, 2025\n\n\n👩‍💻ROS2 RealSense C++ Node\n\n\n\n\nDec 22, 2024\n\n\n📃Rotating without Seeing 리뷰\n\n\n\n\nNov 10, 2024\n\n\n📃IPO 리뷰\n\n\n\n\nMar 17, 2024\n\n\n📃VCGS 리뷰\n\n\n\n\nJan 21, 2024\n\n\n👩‍💻Quarto Blog + α\n\n\n\n\nJan 7, 2024\n\n\n👩‍💻Quarto Blog 기본 셋팅\n\n\n\n\nJul 2, 2023\n\n\n📃DreamWaQ 리뷰\n\n\n\n\nJun 18, 2023\n\n\n👩‍💻Chord Graph\n\n\n\n\nMay 7, 2023\n\n\n📃K-Accessibility 리뷰\n\n\n\n\nApr 23, 2023\n\n\n👩‍💻Orbit Existing Scripts\n\n\n\n\nApr 4, 2023\n\n\n👩‍💻Orbit 설치하기\n\n\n\n\nMar 12, 2023\n\n\n📃WASABI 리뷰\n\n\n\n\nDec 26, 2022\n\n\n👩‍💻class ⟷ dict ⟷ yaml\n\n\n\n\nDec 21, 2022\n\n\n👩‍💻torch.Tensor vs torch.tensor\n\n\n\n\nDec 14, 2022\n\n\n👩‍💻Linux GPU 상태 확인하기\n\n\n\n\nOct 16, 2022\n\n\n📃Robust Recovery Controller 리뷰\n\n\n\n\nOct 2, 2022\n\n\n📃VAE 리뷰\n\n\n\n\nSep 17, 2022\n\n\n📃WaveNet 리뷰\n\n\n\n\nAug 7, 2022\n\n\n📃GN-Block 리뷰\n\n\n\n\nJun 26, 2022\n\n\n📃Legged Robots that Keep on Learning 리뷰\n\n\n\n\nJun 10, 2022\n\n\n📃NerveNet 리뷰\n\n\n\n\nJan 2, 2021\n\n\n🧩GNN Materials\n\n\n\n\nJul 20, 2020\n\n\n👩‍💻Import custom module\n\n\n\n\nJul 17, 2020\n\n\n🧩My Reinforcement Learning Roadmap\n\n\n\n\nJul 13, 2020\n\n\n👩‍💻Install Mujoco in Windows10\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Pick-GPT\n\n\n\nApr 12, 2024\n\n\n\n\n\n\n\n\n\n\n\nDeep Reinforcement Learning for Fall-Recovery Control on Non-Flat Terrain of Quadruped Robots\n\n\n\nDec 11, 2020\n\n\n\n\n\n\n\n\n\n\n\nSelf-driving Public Mobility Get-off Safety System\n\n\n\nNov 12, 2020\n\n\n\n\n\n\n\n\n\n\n\nDeep Reinforcement learning for DME Pulse Design\n\n\n\nNov 11, 2020\n\n\n\n\n\n\n\n\n\n\n\nActive Learning Algorithm for Object Detection and Segmentation\n\n\n\nNov 12, 2019\n\n\n\n\n\n\n\n\n\n\n\nSmart Device for Dog Triaining\n\n\n\nOct 12, 2019\n\n\n\n\n\n\n\n\n\n\n\nTraffic Sign Detection and Recognition Task\n\n\n\nOct 12, 2019\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/note/2022-05-06-geultto-7th-start.html",
    "href": "posts/note/2022-05-06-geultto-7th-start.html",
    "title": "📘Geultto 7th Start",
    "section": "",
    "text": "전부터 알고 있었던 글쓰는 또라이, 글또는 언젠가 꼭 들어가고 싶은 모임이었다. 내가 아는 지식에 대해 이야기 할 줄 아는 능력이 멋져보이기도 했고 무엇보다 다른 사람들에게 도움이 되기도 하겠지만 글을 쓰면서 오히려 내 머릿속에서 정리되고 나에게 가장 큰 도움이 되기 때문에(과거의 내가 미래의 나를 다시 가르쳐줄거란 생각으로) 글을 쓰고 싶었다. 글을 쓰는 걸 좋아하긴 하지만 IT, 개발관련 글을 쓰는 건 평소에 쉽게 써내려가는 감성적이고 주관적인 글쓰기와는 조금 거리가 있기 때문에 좀 망설였던 부분도 있었다. 혹시나 틀린 정보나 생각을 적으면 어떡하지 등의 걱정과 함께 계속해서 미뤄져온 글쓰기는 이제는 좀 마음의 짐을 덜고 현재에 최선을 다해서 적으면 된다라는 생각으로 시작하려고 한다.\n블로그 방황기에 대해 말하자면 그 역사가 2년 정도 되는데, 그 기간동안 Github pages, 유명한 Jekyll, Go언어 써보기 위해서 도전한 Hugo, 빠르게 포스팅할 수 있어서 잠시 돌아간 Naver blog, 코딩과는 안맞는 것 같아서 그만둔 Brunch, 영어로 포스팅하는 거 연습하기 위해서 해본 Medium, 쉽게 생각했다가 커스텀하기 더 어려워서 그만둔 Tistory, 기록용으로는 좋지만 Github과 연동이 안되어서 아쉬운 Notion까지.. 가능한 블로그 플랫폼들은 다 해봤던 것 같다. 그 중에 글또와 함께 정착할 플랫폼은 Fastai에서 만들어준 Fastpages로 결정했다. 다음은 내가 블로그 플랫폼을 결정할 때 생각했던 우선순위들인데, Fastpages가 가장 잘 만족하는 플랫폼이었다.\n\n작성하는 방법이 어렵지 않은 지\n(해외 검색까지 고려해서)Googling이 가능하도록 한 지\nGithub과 연동(기록)이 되는 지\n수식작성과 Table of Contents(TOC), 태그 설정이 가능한 지\n블로그내 포스팅 검색이 가능한 지\n광고가 붙지 않는지(컨텐츠에 집중할 수 있는 깔끔함)\n\nMarkdown과 Jupyter Notebook 형식의 작성을 모두 지원하면서 검색이나 태그 설정이 만들어져 있었고 형식이 깔끔하다. 다크 템플릿을 위해서 조금 설정을 해준 것 말고는 힘들게 셋팅하는 부분들이 없었다.\n인생은 타이밍이라는 말처럼 어떤 일을 할 때의 시기가 있다고 생각한다. 시간적인 여유를 생각했을 때는 대학생 때 작성하는 습관을 들이는 걸 연습하기에 좋다고 생각할 수 있지만, 그때의 나를 돌이켜보면 다른 사람들의 포스팅을 보면서 배우기에도 급급했던 것 같다. 지금 석사과정을 시작하면서 내가 배운 것들을 최대한 기록하고, 그 기록들을 발판삼아 더 성장하는 공학자가 되기 위한 타이밍이 지금이 아닐까 싶다. 글또를 통해 이전의 블로그 방황기와 간헐적이었던 글쓰기를 멈출 수 있기를 기대한다.\n\n\n글또 7기를 진행하면서 약 10-20여개의 글을 쓰게 되는데, 한 가지 주제에만 집중해서 쭉 포스팅하는 것이 좋겠지만 크게 제약을 두지 않고 시작하려고 한다. 한 가지 주제만 해야한다는 생각에 미루게 되는 핑계를 스스로 만들지 않고 그때 내가 배운, 내가 나누고 싶은 이야기를 적는 걸로 계획했다. 하지만 큰 글감들로는 아래와 같은 내용들을 생각하고 있다.\n\n로봇 제어(이론이나 시뮬레이션 툴 등)\n로봇 연구에서의 강화학습\nJulia 언어로 하는 Robotics\n\n\n\n\n글또 7기에서 꼭 이뤘으면 하는 목표는 다음과 같다.\n\n공식적인 포스팅 작성 횟수 다 채우기\n커피챗 참여\n\n추가적인 목표로는, 글또 7기 Slack에 많은 채널들이 있는데(#멍또, #냥또는 사랑입니다❣️) 그 중에서 #알또와 #헬또를 통해 알고리즘 실력과 운동도 챙기고 싶다.\n\n1주일에 1알고리즘 문제 풀기\n주 3일 운동하기\n\n최근에 한해의 Github commit을 3D로 만들어주는 기능을 알게 되었는데 글또와 함께한 2022 커밋은 대도시의 건물 숲처럼 더 멋지게 만들어졌으면 좋겠다.🏙"
  },
  {
    "objectID": "posts/note/2022-05-06-geultto-7th-start.html#글쓰는-또라이-글또",
    "href": "posts/note/2022-05-06-geultto-7th-start.html#글쓰는-또라이-글또",
    "title": "📘Geultto 7th Start",
    "section": "",
    "text": "전부터 알고 있었던 글쓰는 또라이, 글또는 언젠가 꼭 들어가고 싶은 모임이었다. 내가 아는 지식에 대해 이야기 할 줄 아는 능력이 멋져보이기도 했고 무엇보다 다른 사람들에게 도움이 되기도 하겠지만 글을 쓰면서 오히려 내 머릿속에서 정리되고 나에게 가장 큰 도움이 되기 때문에(과거의 내가 미래의 나를 다시 가르쳐줄거란 생각으로) 글을 쓰고 싶었다. 글을 쓰는 걸 좋아하긴 하지만 IT, 개발관련 글을 쓰는 건 평소에 쉽게 써내려가는 감성적이고 주관적인 글쓰기와는 조금 거리가 있기 때문에 좀 망설였던 부분도 있었다. 혹시나 틀린 정보나 생각을 적으면 어떡하지 등의 걱정과 함께 계속해서 미뤄져온 글쓰기는 이제는 좀 마음의 짐을 덜고 현재에 최선을 다해서 적으면 된다라는 생각으로 시작하려고 한다.\n블로그 방황기에 대해 말하자면 그 역사가 2년 정도 되는데, 그 기간동안 Github pages, 유명한 Jekyll, Go언어 써보기 위해서 도전한 Hugo, 빠르게 포스팅할 수 있어서 잠시 돌아간 Naver blog, 코딩과는 안맞는 것 같아서 그만둔 Brunch, 영어로 포스팅하는 거 연습하기 위해서 해본 Medium, 쉽게 생각했다가 커스텀하기 더 어려워서 그만둔 Tistory, 기록용으로는 좋지만 Github과 연동이 안되어서 아쉬운 Notion까지.. 가능한 블로그 플랫폼들은 다 해봤던 것 같다. 그 중에 글또와 함께 정착할 플랫폼은 Fastai에서 만들어준 Fastpages로 결정했다. 다음은 내가 블로그 플랫폼을 결정할 때 생각했던 우선순위들인데, Fastpages가 가장 잘 만족하는 플랫폼이었다.\n\n작성하는 방법이 어렵지 않은 지\n(해외 검색까지 고려해서)Googling이 가능하도록 한 지\nGithub과 연동(기록)이 되는 지\n수식작성과 Table of Contents(TOC), 태그 설정이 가능한 지\n블로그내 포스팅 검색이 가능한 지\n광고가 붙지 않는지(컨텐츠에 집중할 수 있는 깔끔함)\n\nMarkdown과 Jupyter Notebook 형식의 작성을 모두 지원하면서 검색이나 태그 설정이 만들어져 있었고 형식이 깔끔하다. 다크 템플릿을 위해서 조금 설정을 해준 것 말고는 힘들게 셋팅하는 부분들이 없었다.\n인생은 타이밍이라는 말처럼 어떤 일을 할 때의 시기가 있다고 생각한다. 시간적인 여유를 생각했을 때는 대학생 때 작성하는 습관을 들이는 걸 연습하기에 좋다고 생각할 수 있지만, 그때의 나를 돌이켜보면 다른 사람들의 포스팅을 보면서 배우기에도 급급했던 것 같다. 지금 석사과정을 시작하면서 내가 배운 것들을 최대한 기록하고, 그 기록들을 발판삼아 더 성장하는 공학자가 되기 위한 타이밍이 지금이 아닐까 싶다. 글또를 통해 이전의 블로그 방황기와 간헐적이었던 글쓰기를 멈출 수 있기를 기대한다.\n\n\n글또 7기를 진행하면서 약 10-20여개의 글을 쓰게 되는데, 한 가지 주제에만 집중해서 쭉 포스팅하는 것이 좋겠지만 크게 제약을 두지 않고 시작하려고 한다. 한 가지 주제만 해야한다는 생각에 미루게 되는 핑계를 스스로 만들지 않고 그때 내가 배운, 내가 나누고 싶은 이야기를 적는 걸로 계획했다. 하지만 큰 글감들로는 아래와 같은 내용들을 생각하고 있다.\n\n로봇 제어(이론이나 시뮬레이션 툴 등)\n로봇 연구에서의 강화학습\nJulia 언어로 하는 Robotics\n\n\n\n\n글또 7기에서 꼭 이뤘으면 하는 목표는 다음과 같다.\n\n공식적인 포스팅 작성 횟수 다 채우기\n커피챗 참여\n\n추가적인 목표로는, 글또 7기 Slack에 많은 채널들이 있는데(#멍또, #냥또는 사랑입니다❣️) 그 중에서 #알또와 #헬또를 통해 알고리즘 실력과 운동도 챙기고 싶다.\n\n1주일에 1알고리즘 문제 풀기\n주 3일 운동하기\n\n최근에 한해의 Github commit을 3D로 만들어주는 기능을 알게 되었는데 글또와 함께한 2022 커밋은 대도시의 건물 숲처럼 더 멋지게 만들어졌으면 좋겠다.🏙"
  },
  {
    "objectID": "posts/note/2025-01-07-ros2-cpp-python.html",
    "href": "posts/note/2025-01-07-ros2-cpp-python.html",
    "title": "📝Python과 C++ 노드를 모두 포함하는 ROS2 패키지 생성",
    "section": "",
    "text": "ROS2 공식 문서에서는 C++와 Python Node를 별개로 만드는 튜토리얼과 설명만 있기 때문에 C++와 Python 모두 사용하여 노드를 만들고 하나의 ROS2 패키지로 만들기 위한 방법에 대해 알아보겠습니다. 각 단계마다 변경되는 사항에 대해서는 🟢로 표시되어 있으니 단계를 넘어갈때마다 확인해보시기 바랍니다."
  },
  {
    "objectID": "posts/note/2025-01-07-ros2-cpp-python.html#recap-ros2-package-architecture",
    "href": "posts/note/2025-01-07-ros2-cpp-python.html#recap-ros2-package-architecture",
    "title": "📝Python과 C++ 노드를 모두 포함하는 ROS2 패키지 생성",
    "section": "Recap ROS2 Package Architecture",
    "text": "Recap ROS2 Package Architecture\nmy_cpp_py_pkg/\n🟡 패키지 정보, 구성, 및 컴파일\n├── CMakeLists.txt\n├── package.xml\n🟡 Python 관련 Stuff\n├── my_cpp_py_pkg\n│   ├── __init__.py\n│   └── module_to_import.py\n├── scripts\n│   └── py_node.py\n🟡 Cpp 관련 Stuff\n├── include\n│   └── my_cpp_py_pkg\n│       └── cpp_header.hpp\n└── src\n    └── cpp_node.cpp"
  },
  {
    "objectID": "posts/note/2021-01-03-Goodbye-2020.html",
    "href": "posts/note/2021-01-03-Goodbye-2020.html",
    "title": "📘Goodbye 2020",
    "section": "",
    "text": "구글 캘린더의 도움을 받아 조금은 늦은 2020 회고록을 적어봤다. 기억이 희미하고 상기하면서 왜곡된 추억을 회상할 수도 있겠지만, 2020을 보내고 2021을 맞이하기에 충분한 시간을 가지는 건 이 시간이 아니면 할 수 없기에 소중하게 생각하며 한 글자 한 글자 적어봤다. 월마다 1~2개의 이야기를 쓰게 될 것 같으니 한 해를 12개 정도의 이야기로 잘 풀어가봐야 겠다."
  },
  {
    "objectID": "posts/note/2021-01-03-Goodbye-2020.html#월---2월",
    "href": "posts/note/2021-01-03-Goodbye-2020.html#월---2월",
    "title": "📘Goodbye 2020",
    "section": "1월 - 2월",
    "text": "1월 - 2월\n독서실 알바를 했었다. 벌써 1년전 일이라서 잊고 있었던 일이었는데..2020년도 일이었다니 새삼 놀랍다. 아빠의 해외 출장에 마음이 헛헛해지기도 하고, 방학 때 쉽게 쳐질 수 있는 생활 패턴도 잡고, 돈도 벌고자 찾은 아르바이트였다. 그동안 대치동 학원 알바도 해보고 동네 수학학원 보조교사도 해봤었는데 이번에는 처음으로 대면(?)알바가 아니었다. 청소담당을 지원했었는데 당시 실장님 면접 때 여자인 내가 와서 고개를 갸우뚱하셨던게 기억에 남는다. 나름 아침형 인간인지라 사람이 오기전 6~8시 사이에 독서실 모든 공간을 청소기 돌리고 물걸레질을 부지런히 했었다.(부모님이 제일 반대하던 알바였는데.. 생각해보면 은근 말 안듣는 딸인 듯)\n그때 청소를 하면서 독서실 위에 놓여진 공시, 어학 책들을 보며, (삼수생이었던 못난 면모를 아직 벗어던지지 못해서 그런지 겨울만 되면 센치해지는 감성때문일수도) 열심히 공부하는 청춘들의 시간이 왜 안타까워보이기만 했다. 오지랖일순 있지만.. 취업시장이 좁아지고, 스펙과 자격증만 바라보게 되고, 좁은 독서실 자리로 향해야한다는 현실이 너무 답답했다. 각설하고 그때 공짜로 받은 자리에서 공부했던 걸 생각해보면 Udacity RL 코스를 열심히 했던 것 같다. TOEFL도 준비하겠다고 옆에 책을 쌓아두긴 했었지만 제대로 공부 안한건 2021에 그대로 업보로 받아 이어지고 있다.(으이구🤪)\n\n오랜만에 가족들과 속초여행을 갔었다. 당시에 코로나가 조금씩 심해지고 있었는데 그땐 “여름까지 코로나가 계속되면 안되는데..”라고 걱정하고 있었다. 왠걸..2020 한 해를 온전히 코로나랑 함께할 줄은 몰랐다.😥 친구들과도 갔었던 속초였지만 가족들과 함께했던 속초는 또 다른 모습으로 즐길 수 있어서 좋았다. 타임랩스로 바다위로 떠오르는 태양을 찍었던 기억이 생생하다. 물회는 맛있었고, 겨울바다의 소리는 정말 맑고 시원했다.\n\n사실 2019 겨울부터해서 2월까지 KPMG Ideation이라는 대회에 멋진 분들과 함께 준비하고 있었다. 다른 대회보다 달랐던 점은 정말 나에게는 도전 그 자체였기에 더 기억에 남는다. 그 동안 기술적인 성격의 대회들은 대부분 숫자로 표현된 성적으로 판단하는게 대부분이었는데 이 대회는 아이디어 제안 성격을 가지고 있었기에 엔지니어 마인드보다 기획자 마인드를 배우게 되었던 것 같다. 또한 잘 몰랐던 NLP 분야에 대해 공부할 수 있는 기회였고, 특허라는 분야, 변리사라는 직업에 대한 이야기 등 새로운 세상 이야기 좋아하는 나에게는 진짜 재밌었던 시간이었다. 잊지못할 해프닝이 있던 대회이기도 했는데, 본선 대회 당일 대회장에 도착하자 마자 집으로 돌아가라고 통보(?) 받았던 대회였기도 했다. 혹시라도 밝히고 싶지 않으실 수도 있으니까 최대한 말을 아끼고 내 마음속에 저장하겠지만, 정말 감사했던 점은 함께 해주셨던 팀원분들이 정말 다 멋진 분들이었다는 것이다. 밤을 새워가며 함께 나누었던 이야기들이나 생각들 모두 너무 좋았고 평생 남을만한 따뜻한 추억이 생겼다는게 행복했다."
  },
  {
    "objectID": "posts/note/2021-01-03-Goodbye-2020.html#월---6월",
    "href": "posts/note/2021-01-03-Goodbye-2020.html#월---6월",
    "title": "📘Goodbye 2020",
    "section": "3월 - 6월",
    "text": "3월 - 6월\n본격적으로 코로나로 인한 대혼란이 시작됐다. 상황이 좋아질 기미가 보이지 않고 개강이 미뤄졌다. 지금은 익숙하지만 처음접하게 된 온라인 수업은 정말 당황 그 자체였다. 등하교 3시간 통학러인 나에게는 시간을 세이브할 수 있는 장점도 있긴 했지만 집에서 하루종일 노트북만 보고 있으면서 “이게 뭐하는 건가..”싶은 생각이 든게 한 두번이 아니었다. 진짜 대학의 역할에 대해 진지하게 고민도 했었다. 그래도 바쁘게 1학기를 잘 보냈고 랩실 생활도 열심히 했던 것 같다. 아 생각해보니 이때부터 나그네처럼 랩실을 다녔던 생활을 마치고 정식으로 학부연구생으로 인정받아 돈을 받으면서 연구하게 되었다.\n\n종강을 하고 미래연구소 14기 서브튜터를 하게되었다. 내가 처음으로 딥러닝을 공부하게 된 곳에서 서브튜터로 일하게 되었다는 게 정말 신기하고 감사했다. 대학교 가자마자 우연히 보게된 글을 보고 (지금 생각해보면 겁도 없이 혼자 찾아간게 신기하지만) 미래연구소 1기로 딥러닝을 공부했다. 오랜만에 랩장님도 보고 몰라보게 커진 미래연구소 모습을 보면서 괜시리 뿌듯하기도 했다. 사실 메인튜터님이 많이 배려해주시기도 하시고 서브튜터 업무 자체는 크게 부담스럽지는 않았지만 돈을 받고 일하는 자리는 항상 긴장하게 되기 때문에 조금 스트레스를 받았던 것 같긴하다. 14기 분들 중 완전 입문자를 위한 파이썬 기초 스터디는 따로 혼자 운영해야 했기 때문에 좀 더 긴장했던 것 같기도 하다. 그래도 항상 DL 공부 관련해서 INPUT만 했던 입장에서 처음으로 OUTPUT을 하게되는 도전적인 경험이었고, 지식적인 면으로나 태도적인 면으로나 성장할 수 있었던 큰 도약점이 됬었다. 부족한 서브튜터를 만났지만 열심히 공부하셨던 14기 분들이 모두 성공하셔서 나중에 커뮤니티에서 만나뵈면 정말 행복할 것 같다."
  },
  {
    "objectID": "posts/note/2021-01-03-Goodbye-2020.html#월---8월",
    "href": "posts/note/2021-01-03-Goodbye-2020.html#월---8월",
    "title": "📘Goodbye 2020",
    "section": "7월 - 8월",
    "text": "7월 - 8월\n여름방학에 역시 많은 일들이 있었다. 우선 정말 다이나믹했던 2020 국제창작자동차대회 PostNomad팀으로 참가한 이야기.🦈 4학년 분들의 졸업프로젝트였지만 운좋게도 참여할 수 있는 기회가 주어져서 제어팀으로 합류할 수 있었다. 후에는 딥러닝 테스크 비중이 높은 비젼팀으로 옮겨가야만 했지만. 그동안은 학교내에서 팀을 꾸려서 대회에 나가기보다는 대외 스터디나 모임에서 만난 분들과 프로젝트들을 했었기 때문에 또 다른 느낌이었다. 대형학과이다 보니 사실상 같은 학과여도 서로 잘 모르기 때문에 졸업할 때까지 모르는 동기도 많다. 평소에 아싸생활을 하는 나로써는 더더욱 학과 사람들이랑 친해질 기회가 없었는데 처음으로 기계과 사람들과 함께할 수 있었던 시간이 아니었나 싶다. 지금 회고하는 이 시점에서는 대회 결과도 아쉽고😢 준비하는 과정에서 랩뷰만 써야하는 답답함, 협업하면서 느꼈던 어려움과 실망했던 점들도 많지만, 진짜 소중한 시간들이었다. 지하 작업실에서 회의하고, 함께 공구들을 나르고, 덥고 습한 K-City에서 다같이 고생하고 고민했던 모든 시간들이 감사하다. 이 회고록을 빌려 고백을 하자면.. 우리 팀에게 조금 더 잘하지 못한 게 죄송하다. 다들 열심히 하시고 항상 나를 배려해주셨던 것 같은데 그에 비해 나는 잘 따라가는 팀원은 아니었던 것 같다.\n\n랩실 연구를 비롯해서 TOEFL 공부까지 하느라 정말 열심히 살았는데, 사실 이때 제대로 공부하지 않아서 후에 고생하게 된 건 안비밀이다. 항상 느끼는 것이긴 한데 나는 한가지 일을 집중해서 끝내는 능력이 아직도 부족한 것 같다. 이에 더하여 2020 Korea Health Datathon에 참가하여 부비동 데이터셋으로 최종 4위를 했었다. NSML 플랫폼은 할말하않이긴 하지만 꾸준히 데이터톤 경험을 갖게 해주신 것만으로도 감사하다. 2019 대회에도 참여했었는데 그때보다 발전된 성적을 가질 수 있어서 좋았다.\n\n산티아고 순례길을 걷고 싶다는 버킷리스트가 생겼다. 그래서 하나씩 뭘 준비해야 하나 고민하는 중에 체력을 길러야 겠다는 생각을 했다. 이에 더해 코로나로 떨어진 활동성을 보충하고자 등하교를 따릉이로 하기 시작했다. 가는데에만 2시간 걸리는 여정이었지만 한강을 따라 가는 길이 나쁘지 않았기 때문에 충분히 좋은 도전이었다. 처음에는 다들 미친 짓(?)이라고 만류했었고 나도 반신반의 했었지만 막상 해보니 죽을 정도는 아니었고 자전거 타고 보는 한강은 다리아픈 것 따위 다 잊게 만들정도로 예뻤다. 아침은 아침대로, 저녁은 저녁대로, 맑으면 맑은대로, 흐리면 흐린대로. 때로는 상수나들목 공사때문에 당황하기도 했지만 길은 항상 찾으면 되는 거였다."
  },
  {
    "objectID": "posts/note/2021-01-03-Goodbye-2020.html#월---10월",
    "href": "posts/note/2021-01-03-Goodbye-2020.html#월---10월",
    "title": "📘Goodbye 2020",
    "section": "9월 - 10월",
    "text": "9월 - 10월\n2학기도 여전히 코로나로 인해 온라인으로 진행됐다. 학기중에는 거의 수업에 집중하다보니 딱히 회고록에 적을 내용이 없는 것 같아서 한빛미디어에서 “나는 리뷰어다”로 참여했던 경험을 적어볼까 한다. 리뷰어로 활동을 하면서 2개의 도전이 있었다. 첫째는 글쓰는 것 자체에 대한 도전이었는데, 예전에는(그러니까 고2때 까지만 하더라도) 글을 쓰는데 어려움이 없었던 것 같다. 그리고 나름 글을 잘 쓴다고 인정도 받았던 것 같은데 이후에 딱히 글을 쓸 기회가 없었고 쓰지 않다보니 리뷰를 쓴다는 것 자체가 어색했다. 그리고 “독후감”과는 다른 목적이 있는 “리뷰”라는 글에 대한 고민이 있었다. 출판사에서 책을 주면서 나에게 리뷰를 쓰기를 원하는 니즈는 분명 있는 것이고, 다른 사람들에게 책의 내용이 잘 어필이 되길 바라는 것일터였다. 그러면 단순히 책의 장단점을 나 혼자 판단하고 즐기고 끝나는 것이 아닌 다른 사람들에게 잘 전달될 수 있도록 표현해야 한다는 것이었다. 두번째로는 빠르게 기술서를 봐야한다는 도전이었다. 사실 IT전공도 아니고 원래는 1개의 책도 적어도 2~3개월을 봐야하는 거북이 속도인데 리뷰를 하려면 1달에 1권을 무조건 다 보고 리뷰까지 완성해야 했다. 몇개의 책들은 사실 다 보지도 못하고 리뷰 적기에 급급했던 것도 사실이다. 제대로 리뷰어로 활동하지 못해 관계자분들께 죄송하다. 그래도 몇몇 리뷰는 제대로 적었다는 것에 스스로 조금 위안을 삼아본다."
  },
  {
    "objectID": "posts/note/2021-01-03-Goodbye-2020.html#월",
    "href": "posts/note/2021-01-03-Goodbye-2020.html#월",
    "title": "📘Goodbye 2020",
    "section": "11월",
    "text": "11월\n지금까지 인생의 경험들 중에 최악과 최고를 다 뽑으라고 하면 2020.11월에 다 있다. 좋은 것부터 먼저. 우선 최고는 내 인생 처음으로 학회에서 내가 한 연구를 가지고 발표를 완성도 있게 마무리 할 수 있었고 인정도 받아서 우수논문상까지 받게 된 경험이다. 진짜 학회 발표 전 리허설하는 랩미팅에서 울면서 나가기도 했었고 발표 전날까지도 결과가 잘 나오지 않아 정말 힘들었다. “힘들었다”라는 4글자로 밖에 표현 못한다는 게 억울할 정도로 최고의 스트레스를 받은 시간이었다. 그래도 진짜 주변에 천사들을 심어 놓으신 것인지 기적적으로 도움도 받고 몇일 밤을 새서 어찌저찌 마무리 할 수 있었고 IPNT에서 구두발표도 잘 마무리하여 우수논문상도 받게되었다.(지금생각해도 기적이라고 말할 수 밖에 없다.) 진짜 힘들었던 만큼 최고의 성취감은 말로 할 수 없었다.\n다음으로 최악의 경험은 사실 최고의 경험과 관련이 깊다. 앞서 적은 “최고의 스트레스”가 복선이었다. 학회를 마치고 체력이 바닥으로 떨어질대로 떨어졌고 긴장은 완전히 풀어진 상태에서 몸이 엄청 아팠다. 감을 먹고 체한 탓도 있었지만 몸이 정말 아팠고 힘이 빠지면서 “이대로 죽을 수도 있겠구나..” 싶은 생각이 들었다. 인생 처음으로 자다가 새벽에 구급차를 불러 응급실에 갔던 게 최악의 경험이지 않나 싶다. 근데 유감스럽게도 구급대원분들이 오시면서 급속도로 괜찮아져서 정말 난처하고 민망했다. 나중에 새벽 4시쯤 엄마랑 응급실을 나와서 걸어서 집으로 돌아갔다. 이때의 일은 가족들에게도 큰 충격을 주어서 지금도 아빠는 잊을만 하면 이야기를 하시는데 하지말라고 장난스럽게 말을 하면서도 죄송한 마음이 들기도 한다. 웃프긴 하지만 이 일 이후로 내가 제일 좋아하는 “감”은 금지어가 되었다."
  },
  {
    "objectID": "posts/note/2021-01-03-Goodbye-2020.html#월-1",
    "href": "posts/note/2021-01-03-Goodbye-2020.html#월-1",
    "title": "📘Goodbye 2020",
    "section": "12월",
    "text": "12월\n사랑하는 양가의 조부모님들이 몇년전만 해도 다 살아계셨다. 하지만 근 2년 정도 매년 겨울에 사랑하는 분들을 떠나보내게 되었다. 외할아버지, 친할아버지, 그리고 외할머니까지. 올해 후반에 건강이 급속도로 나빠지신 외할머니가 결국 우리곁을 떠나셨다. 4분의 할머니 할아버지 가운데 가장 사랑의 표현도 아끼시지 않고 항상 전화도 먼저 걸어주셨던 멋진 할머니였다. 지금 이렇게 “할머니 사랑해요”라고 말하던게 이렇게 그리워할 것이었다면 살아계셨을때 왜 그렇게 어색해하고 표현하기 부끄러워 했는지. 코로나로 인해 좋았던 점 하나는 장례식에 손님들 없이 식구들끼리 할머니를 추억하면서 얼마나 멋진 분이셨는지 되새길수 있어서 좋았다. 진짜 멋진 분이셨다."
  },
  {
    "objectID": "posts/note/2021-01-03-Goodbye-2020.html#bnm2h",
    "href": "posts/note/2021-01-03-Goodbye-2020.html#bnm2h",
    "title": "📘Goodbye 2020",
    "section": "BNM2h",
    "text": "BNM2h\n2020년도에 감사했던 많은 일들이 있었지만, 지금 나름 뿌듯하고 보람찬 시간들을 회고할 수 있도록 도와준 많은 분들이 있었다. 스터디를 통해서 만나는 인연들이 대부분이다. 물론 많은 스터디에 참여해보고 도전 받을 수 있는 좋은 시간들이었다. 그래도 가장 애정가는 스터디는 아무래도 BNM2h가 아닐까 싶다. 그렇다. 사실 가장 애정가는 이유 중 하나는 아무래도 내가 만든 스터디였기에 가장 책임을 느꼈고 가장 스트레스도 많이 받고 가장 노력했음을 나 스스로도 느꼈기 때문일 것이다.\n처음에는 진짜 가벼운 마음으로 시작했다. 따지고 보면 2019년도에 Kaggle KR에서 스터디 리더를 뽑는다는 글을 보고 마침 캐글에서 Connect-X라는 강화학습 대회가 베타수준으로 시도하기 시작했던 때라 “내가 한번 캐글 강화학습 스터디 리더가 되어보자!”라는 생각과 패기로 시작했던 거였다. 패기는 패기였던 걸로.. 그렇게 시작했으나 함께할 팀원분들이 모집되지 않아 그냥 한 순간의 불꽃으로 끝날 뻔 했다. 다행이도 다른 리더분들 중에 나와 같은 처지였던 분이 계셨었고 감사하게도 같이 공부하자 먼저 손을 내밀어 주셔서 BNM2h라는 스터디가 생기게 되었다!🙌\n그때 그때 마다 인연이 되는대로 지금까지 이어져오고 있는 스터디에서 강화학습을 공부하고 있다. 최고의 스터디라고 자랑할 순 없지만 최애 스터디라고는 할 수 있다. 아직도 강화학습이라는 분야에 대해, 스터디에서 함께 공부하는 방식에 대해, 스터디 매니징하는 것에 대해, 감사함과 겸손함을 표현하는 것에 대해 한참 모자른 애송이이지만 매주 스터디에 나와주셔서 나에게 성장할 기회를 주고 그런 시간들을 함께 보내주시는 BNM2h 스터디원분들은 천사들이신 것 같다. 나도 그런 분들에게 조금이나마 도움이 되고 함께 성장하기 위해 조금 더 노력하는 사람이 되어야겠다고 느낀 한 해였다."
  },
  {
    "objectID": "posts/note/2021-01-03-Goodbye-2020.html#마무리",
    "href": "posts/note/2021-01-03-Goodbye-2020.html#마무리",
    "title": "📘Goodbye 2020",
    "section": "마무리",
    "text": "마무리\n물론 당연히도 여기에 적지 못한 많은 이야기들이 있다. 이야기들은 열정을 담기도 하고, 아쉬움을 담기도 하고, 기쁨을 담기도 하고, 슬픔을 담기도 한다. 그 이야기들은 지금의 나의 마음이나 기억 속 어딘가에 잘 살아있겠지.\n멋있는 개발자분들의 회고록 같은 것을 기대했으나 적고나서 읽어보니 아직은 어디에 내놓기 부끄러운 새벽감성의 회고록이니 그냥 조용히 블로그에 남기기로 생각했다.😂\n잘가요 2020"
  },
  {
    "objectID": "posts/note/2024-01-15-cs-study-001.html",
    "href": "posts/note/2024-01-15-cs-study-001.html",
    "title": "📝Operating System 001",
    "section": "",
    "text": "Originial Repository: https://github.com/gyoogle/tech-interview-for-developer를 공부하며 2차 편집한 내용입니다."
  },
  {
    "objectID": "posts/note/2024-01-15-cs-study-001.html#운영체제의-역할",
    "href": "posts/note/2024-01-15-cs-study-001.html#운영체제의-역할",
    "title": "📝Operating System 001",
    "section": "운영체제의 역할",
    "text": "운영체제의 역할\n\n\n\n\n\n\n\n\n\n\n1. 프로세스 관리\n2. 저장장치 관리\n3. 네트워킹\n4. 사용자 관리\n5. 디바이스 드라이버\n\n\n\n\n프로세스, 스레드 스케줄링 동기화 IPC 통신\n메모리 관리 가상 메모리 파일 시스템\nTCP/IP 기타 프로토콜\n계정 관리 접근권한 관리\n순차접근 장치 임의접근 장치 네트워크 장치\n\n\n\n\n\n각 역할에 대한 자세한 설명\n\n\n\n\n\n\n1. 프로세스 관리\n\n\n\n\n\n운영체제에서 작동하는 응용 프로그램을 관리하는 기능이다.\n어떤 의미에서는 프로세서(CPU) 관리하는 것이라고 볼 수도 있다. 현재 CPU를 점유해야 할 프로세스를 결정하고, 실제로 CPU를 프로세스에 할당하며, 이 프로세스 간 공유 자원 접근과 통신 등을 관리하게 된다.\n\n\n\n\n\n\n\n\n\n2. 저장장치 관리\n\n\n\n\n\n1차 저장장치에 해당하는 메인 메모리와 2차 저장장치에 해당하는 하드디스크, NAND 등을 관리하는 기능이다.\n\n1차 저장장치(Main Memory)\n\n프로세스에 할당하는 메모리 영역의 할당과 해제\n각 메모리 영역 간의 침범 방지\n메인 메모리의 효율적 활용을 위한 가상 메모리 기능\n\n2차 저장장치(HDD, NAND Flash Memory 등)\n\n파일 형식의 데이터 저장\n이런 파일 데이터 관리를 위한 파일 시스템을 OS에서 관리\nFAT, NTFS, EXT2, JFS, XFS 등 많은 파일 시스템들이 개발되어 사용 중\n\n\n\n\n\n\n\n\n\n\n\n3. 네트워킹\n\n\n\n\n\n네트워킹은 컴퓨터 활용의 핵심과도 같아졌다.\nTCP/IP 기반의 인터넷에 연결하거나, 응용 프로그램이 네트워크를 사용하려면 운영체제에서 네트워크 프로토콜을 지원해야 한다. 현재 상용 OS들은 다양하고 많은 네트워크 프로토콜을 지원한다.\n이처럼 운영체제는 사용자와 컴퓨터 하드웨어 사이에 위치해서, 하드웨어를 운영 및 관리하고 명령어를 제어하여 응용 프로그램 및 하드웨어를 소프트웨어적으로 제어 및 관리를 해야한다.\n\n\n\n\n\n\n\n\n\n4. 사용자 관리\n\n\n\n\n\n우리가 사용하는 PC는 오직 한 사람만의 것일까? 아니다.\n하나의 PC로도 여러 사람이 사용하는 경우가 많다. 그래서 운영체제는 한 컴퓨터를 여러 사람이 사용하는 환경도 지원해야 한다. 가족들이 각자의 계정을 만들어 PC를 사용한다면, 이는 하나의 컴퓨터를 여러 명이 사용한다고 말할 수 있다.\n따라서, 운영체제는 각 계정을 관리할 수 있는 기능이 필요하다. 사용자 별로 프라이버시와 보안을 위해 개인 파일에 대해선 다른 사용자가 접근할 수 없도록 해야 한다. 이 밖에도 파일이나 시스템 자원에 접근 권한을 지정할 수 있도록 지원하는 것이 사용자 관리 기능이다.\n\n\n\n\n\n\n\n\n\n5. 디바이스 드라이버\n\n\n\n\n\n운영체제는 시스템의 자원, 하드웨어를 관리한다. 시스템에는 여러 하드웨어가 붙어있는데, 이들을 운영체제에서 인식하고 관리하게 만들어 응용 프로그램이 하드웨어를 사용할 수 있게 만들어야 한다.\n따라서, 운영체제 안에 하드웨어를 추상화 해주는 계층이 필요하다. 이 계층이 바로 디바이스 드라이버라고 불린다. 하드웨어의 종류가 많은 만큼, 운영체제 내부의 디바이스 드라이버도 많이 존재한다.\n이러한 수많은 디바이스 드라이버들을 관리하는 기능 또한 운영체제가 맡고 있다."
  },
  {
    "objectID": "posts/note/2024-01-15-cs-study-001.html#멀티-프로세스",
    "href": "posts/note/2024-01-15-cs-study-001.html#멀티-프로세스",
    "title": "📝Operating System 001",
    "section": "멀티 프로세스",
    "text": "멀티 프로세스\n\n하나의 프로그램을 여러개의 프로세스로 구성하여 각 프로세스가 병렬적으로 작업을 수행하는 것\n\n장점 : 안전성 (메모리 침범 문제를 OS 차원에서 해결)\n단점 : 각각 독립된 메모리 영역을 갖고 있어, 작업량 많을 수록 오버헤드 발생. Context Switching으로 인한 성능 저하\nContext Switching이란?\n\n프로세스의 상태 정보를 저장하고 복원하는 일련의 과정\n즉, 동작 중인 프로세스가 대기하면서 해당 프로세스의 상태를 보관하고, 대기하고 있던 다음 순번의 프로세스가 동작하면서 이전에 보관했던 프로세스 상태를 복구하는 과정을 말함\n→ 프로세스는 각 독립된 메모리 영역을 할당받아 사용되므로, 캐시 메모리 초기화와 같은 무거운 작업이 진행되었을 때 오버헤드가 발생할 문제가 존재함"
  },
  {
    "objectID": "posts/note/2024-01-15-cs-study-001.html#멀티-스레드",
    "href": "posts/note/2024-01-15-cs-study-001.html#멀티-스레드",
    "title": "📝Operating System 001",
    "section": "멀티 스레드",
    "text": "멀티 스레드\n\n하나의 응용 프로그램에서 여러 스레드를 구성해 각 스레드가 하나의 작업을 처리하는 것\n\n스레드들이 공유 메모리를 통해 다수의 작업을 동시에 처리하도록 해줌\n장점 : 독립적인 프로세스에 비해 공유 메모리만큼의 시간, 자원 손실이 감소 전역 변수와 정적 변수에 대한 자료 공유 가능\n단점 : 안전성 문제. 하나의 스레드가 데이터 공간 망가뜨리면, 모든 스레드가 작동 불능 상태 (공유 메모리를 갖기 때문)\n\n멀티스레드의 안전성에 대한 단점은 Critical Section 기법을 통해 대비함\n\n하나의 스레드가 공유 데이터 값을 변경하는 시점에 다른 스레드가 그 값을 읽으려할 때 발생하는 문제를 해결하기 위한 동기화 과정\n상호 배제, 진행, 한정된 대기를 충족해야함"
  },
  {
    "objectID": "posts/note/2024-01-15-cs-study-001.html#외부-인터럽트",
    "href": "posts/note/2024-01-15-cs-study-001.html#외부-인터럽트",
    "title": "📝Operating System 001",
    "section": "외부 인터럽트",
    "text": "외부 인터럽트\n입출력 장치, 타이밍 장치, 전원 등 외부적인 요인으로 발생\n\n전원 이상, 기계 착오, 외부 신호, 입출력"
  },
  {
    "objectID": "posts/note/2024-01-15-cs-study-001.html#내부-인터럽트",
    "href": "posts/note/2024-01-15-cs-study-001.html#내부-인터럽트",
    "title": "📝Operating System 001",
    "section": "내부 인터럽트",
    "text": "내부 인터럽트\nTrap이라고 부르며, 잘못된 명령이나 데이터를 사용할 때 발생\n\n0으로 나누기가 발생, 오버플로우, 명령어를 잘못 사용한 경우 (Exception)"
  },
  {
    "objectID": "posts/note/2024-01-15-cs-study-001.html#소프트웨어-인터럽트",
    "href": "posts/note/2024-01-15-cs-study-001.html#소프트웨어-인터럽트",
    "title": "📝Operating System 001",
    "section": "소프트웨어 인터럽트",
    "text": "소프트웨어 인터럽트\n프로그램 처리 중 명령의 요청에 의해 발생한 것 (SVC 인터럽트)\n\n사용자가 프로그램을 실행시킬 때 발생\n소프트웨어 이용 중에 다른 프로세스를 실행시키면 시분할 처리를 위해 자원 할당 동작이 수행된다."
  },
  {
    "objectID": "posts/note/2024-01-15-cs-study-001.html#인터럽트-발생-처리-과정",
    "href": "posts/note/2024-01-15-cs-study-001.html#인터럽트-발생-처리-과정",
    "title": "📝Operating System 001",
    "section": "인터럽트 발생 처리 과정",
    "text": "인터럽트 발생 처리 과정\n주 프로그램이 실행되다가 인터럽트가 발생했다.\n\n\n\nHandling Interrupts\n\n\n현재 수행 중인 프로그램을 멈추고, 상태 레지스터와 PC 등을 스택에 잠시 저장한 뒤에 인터럽트 서비스 루틴으로 간다.\n(잠시 저장하는 이유는, 인터럽트 서비스 루틴이 끝난 뒤 다시 원래 작업으로 돌아와야 하기 때문)\n만약 인터럽트 기능이 없었다면, 컨트롤러는 특정한 어떤 일을 할 시기를 알기 위해 계속 체크를 해야 한다.\n(이를 폴링(Polling)이라고 한다)\n폴링을 하는 시간에는 원래 하던 일에 집중할 수가 없게 되어 많은 기능을 제대로 수행하지 못하는 단점이 있었다.\n즉, 컨트롤러가 입력을 받아들이는 방법(우선순위 판별방법)에는 두가지가 있다.\n\n폴링 방식\n\n사용자가 명령어를 사용해 입력 핀의 값을 계속 읽어 변화를 알아내는 방식\n인터럽트 요청 플래그를 차례로 비교하여 우선순위가 가장 높은 인터럽트 자원을 찾아 이에 맞는 인터럽트 서비스 루틴을 수행한다. (하드웨어에 비해 속도 느림)\n\n인터럽트 방식\n\nMCU 자체가 하드웨어적으로 변화를 체크하여 변화 시에만 일정한 동작을 하는 방식\nDaisy Chain\n병렬 우선순위 부여\n\n\n인터럽트 방식은 하드웨어로 지원을 받아야 하는 제약이 있지만, 폴링에 비해 신속하게 대응하는 것이 가능하다. 따라서 ‘실시간 대응’이 필요할 때는 필수적인 기능이다.\n즉, 인터럽트는 발생시기를 예측하기 힘든 경우에 컨트롤러가 가장 빠르게 대응할 수 있는 방법이다."
  },
  {
    "objectID": "posts/note/2024-01-15-cs-study-001.html#fork",
    "href": "posts/note/2024-01-15-cs-study-001.html#fork",
    "title": "📝Operating System 001",
    "section": "Fork",
    "text": "Fork\n\n새로운 Process를 생성할 때 사용.\n그러나, 이상한 방식임.\n\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;unistd.h&gt;\n\nint main(int argc, char *argv[]) {\n    printf(\"pid : %d\", (int) getpid()); // pid : 29146\n    \n    int rc = fork();                    // 주목\n    \n    if (rc &lt; 0) {\n        exit(1);\n    }                                   // (1) fork 실패\n    else if (rc == 0) {                 // (2) child 인 경우 (fork 값이 0)\n        printf(\"child (pid : %d)\", (int) getpid());\n    }\n    else {                              // (3) parent case\n        printf(\"parent of %d (pid : %d)\", rc, (int)getpid());\n    }\n}\n\npid : 29146\nparent of 29147 (pid : 29146)\nchild (pid : 29147)\n\n을 출력함 (parent와 child의 순서는 non-deterministic함. 즉, 확신할 수 없음. scheduler가 결정하는 일임.)\n[해석]\nPID : 프로세스 식별자. UNIX 시스템에서는 PID는 프로세스에게 명령을 할 때 사용함.\nFork()가 실행되는 순간. 프로세스가 하나 더 생기는데, 이 때 생긴 프로세스(Child)는 fork를 만든 프로세스(Parent)와 (almost) 동일한 복사본을 갖게 된다. 이 때 OS는 위와 똑같은 2개의 프로그램이 동작한다고 생각하고, fork()가 return될 차례라고 생각한다. 그 때문에 새로 생성된 Process (child)는 main에서 시작하지 않고, if 문부터 시작하게 된다.\n그러나, 차이점이 있었다. 바로 child와 parent의 fork() 값이 다르다는 점이다. 따라서, 완전히 동일한 복사본이라 할 수 없다.\n\nParent의 fork()값 =&gt; child의 pid 값\nChild의 fork()값 =&gt; 0\n\nParent와 child의 fork 값이 다르다는 점은 매우 유용한 방식이다.\n그러나! Scheduler가 부모를 먼저 수행할지 아닐지 확신할 수 없다. 따라서 아래와 같이 출력될 수 있다.\n\npid : 29146\nchild (pid : 29147)\nparent of 29147 (pid : 29146)"
  },
  {
    "objectID": "posts/note/2024-01-15-cs-study-001.html#wait",
    "href": "posts/note/2024-01-15-cs-study-001.html#wait",
    "title": "📝Operating System 001",
    "section": "wait",
    "text": "wait\n\nchild 프로세스가 종료될 때까지 기다리는 작업\n\n위의 예시에 int wc = wait(NULL)만 추가함.\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;unistd.h&gt;\n#include &lt;sys/wait.h&gt;\n\nint main(int argc, char *argv[]) {\n    printf(\"pid : %d\", (int) getpid()); // pid : 29146\n    \n    int rc = fork();                    // 주목\n    \n    if (rc &lt; 0) {\n        exit(1);\n    }                                   // (1) fork 실패\n    else if (rc == 0) {                 // (2) child 인 경우 (fork 값이 0)\n        printf(\"child (pid : %d)\", (int) getpid());\n    }\n    else {                              // (3) parent case\n        int wc = wait(NULL)             // 추가된 부분\n        printf(\"parent of %d (wc : %d / pid : %d)\", wc, rc, (int)getpid());\n    }\n}\n\npid : 29146\nchild (pid : 29147)\nparent of 29147 (wc : 29147 / pid : 29146)\n\nwait를 통해서, child의 실행이 끝날 때까지 기다려줌. parent가 먼저 실행되더라도, wait ()는 child가 끝나기 전에는 return하지 않으므로, 반드시 child가 먼저 실행됨."
  },
  {
    "objectID": "posts/note/2024-01-15-cs-study-001.html#exec",
    "href": "posts/note/2024-01-15-cs-study-001.html#exec",
    "title": "📝Operating System 001",
    "section": "exec",
    "text": "exec\n단순 fork는 동일한 프로세스의 내용을 여러 번 동작할 때 사용함.\nchild에서는 parent와 다른 동작을 하고 싶을 때는 exec를 사용할 수 있음.\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;unistd.h&gt;\n#include &lt;sys/wait.h&gt;\n\nint main(int argc, char *argv[]) {\n    printf(\"pid : %d\", (int) getpid()); // pid : 29146\n    \n    int rc = fork();                    // 주목\n    \n    if (rc &lt; 0) {\n        exit(1);\n    }                                   // (1) fork 실패\n    else if (rc == 0) {                 // (2) child 인 경우 (fork 값이 0)\n        printf(\"child (pid : %d)\", (int) getpid());\n        char *myargs[3];\n        myargs[0] = strdup(\"wc\");       // 내가 실행할 파일 이름\n        myargs[1] = strdup(\"p3.c\");     // 실행할 파일에 넘겨줄 argument\n        myargs[2] = NULL;               // end of array\n        execvp(myarges[0], myargs);     // wc 파일 실행.\n        printf(\"this shouldn't print out\") // 실행되지 않음.\n    }\n    else {                              // (3) parent case\n        int wc = wait(NULL)             // 추가된 부분\n        printf(\"parent of %d (wc : %d / pid : %d)\", wc, rc, (int)getpid());\n    }\n}\nexec가 실행되면,\nexecvp( 실행 파일, 전달 인자 ) 함수는, code segment 영역에 실행 파일의 코드를 읽어와서 덮어 씌운다.\n씌운 이후에는, heap, stack, 다른 메모리 영역이 초기화되고, OS는 그냥 실행한다. 즉, 새로운 Process를 생성하지 않고, 현재 프로그램에 wc라는 파일을 실행한다. 그로인해서, execvp() 이후의 부분은 실행되지 않는다."
  },
  {
    "objectID": "posts/note/2024-01-15-cs-study-001.html#process-management",
    "href": "posts/note/2024-01-15-cs-study-001.html#process-management",
    "title": "📝Operating System 001",
    "section": "Process Management",
    "text": "Process Management\n\nCPU가 프로세스가 여러개일 때, CPU 스케줄링을 통해 관리하는 것을 말함\n\n이때, CPU는 각 프로세스들이 누군지 알아야 관리가 가능함\n프로세스들의 특징을 갖고있는 것이 바로 Process Metadata\n\nProcess Metadata\n\nProcess ID\nProcess State\nProcess Priority\nCPU Registers\nOwner\nCPU Usage\nMemeory Usage\n\n\n이 메타데이터는 프로세스가 생성되면 PCB(Process Control Block)이라는 곳에 저장됨"
  },
  {
    "objectID": "posts/note/2024-01-15-cs-study-001.html#pcbprocess-control-block",
    "href": "posts/note/2024-01-15-cs-study-001.html#pcbprocess-control-block",
    "title": "📝Operating System 001",
    "section": "PCB(Process Control Block)",
    "text": "PCB(Process Control Block)\n\n프로세스 메타데이터들을 저장해 놓는 곳, 한 PCB 안에는 한 프로세스의 정보가 담김\n\n\n\n\nPCB\n\n\n다시 정리해보면?\n프로그램 실행 → 프로세스 생성 → 프로세스 주소 공간에 (코드, 데이터, 스택) 생성 \n→ 이 프로세스의 메타데이터들이 PCB에 저장"
  },
  {
    "objectID": "posts/note/2024-01-15-cs-study-001.html#pcb가-왜-필요한가요",
    "href": "posts/note/2024-01-15-cs-study-001.html#pcb가-왜-필요한가요",
    "title": "📝Operating System 001",
    "section": "PCB가 왜 필요한가요?",
    "text": "PCB가 왜 필요한가요?\n\nCPU에서는 프로세스의 상태에 따라 교체작업이 이루어진다. (interrupt가 발생해서 할당받은 프로세스가 waiting 상태가 되고 다른 프로세스를 running으로 바꿔 올릴 때)\n이때, 앞으로 다시 수행할 대기 중인 프로세스에 관한 저장 값을 PCB에 저장해두는 것이다."
  },
  {
    "objectID": "posts/note/2024-01-15-cs-study-001.html#pcb는-어떻게-관리되나요",
    "href": "posts/note/2024-01-15-cs-study-001.html#pcb는-어떻게-관리되나요",
    "title": "📝Operating System 001",
    "section": "PCB는 어떻게 관리되나요?",
    "text": "PCB는 어떻게 관리되나요?\n\nLinked List 방식으로 관리함\nPCB List Head에 PCB들이 생성될 때마다 붙게 된다. 주소값으로 연결이 이루어져 있는 연결리스트이기 때문에 삽입 삭제가 용이함.\n즉, 프로세스가 생성되면 해당 PCB가 생성되고 프로세스 완료시 제거됨\n\n이렇게 수행 중인 프로세스를 변경할 때, CPU의 레지스터 정보가 변경되는 것을 Context Switching이라고 한다."
  },
  {
    "objectID": "posts/note/2024-01-15-cs-study-001.html#context-switching",
    "href": "posts/note/2024-01-15-cs-study-001.html#context-switching",
    "title": "📝Operating System 001",
    "section": "Context Switching",
    "text": "Context Switching\n\nCPU가 이전의 프로세스 상태를 PCB에 보관하고, 또 다른 프로세스의 정보를 PCB에 읽어 레지스터에 적재하는 과정\n\n보통 인터럽트가 발생하거나, 실행 중인 CPU 사용 허가시간을 모두 소모하거나, 입출력을 위해 대기해야 하는 경우에 Context Switching이 발생\n즉, 프로세스가 Ready → Running, Running → Ready, Running → Waiting처럼 상태 변경 시 발생!"
  },
  {
    "objectID": "posts/note/2024-01-15-cs-study-001.html#context-switching의-overhead란",
    "href": "posts/note/2024-01-15-cs-study-001.html#context-switching의-overhead란",
    "title": "📝Operating System 001",
    "section": "Context Switching의 OverHead란?",
    "text": "Context Switching의 OverHead란?\noverhead는 과부하라는 뜻으로 보통 안좋은 말로 많이 쓰인다.\n하지만 프로세스 작업 중에는 OverHead를 감수해야 하는 상황이 있다.\n프로세스를 수행하다가 입출력 이벤트가 발생해서 대기 상태로 전환시킴\n이때, CPU를 그냥 놀게 놔두는 것보다 다른 프로세스를 수행시키는 것이 효율적\n즉, CPU에 계속 프로세스를 수행시키도록 하기 위해서 다른 프로세스를 실행시키고 Context Switching 하는 것\nCPU가 놀지 않도록 만들고, 사용자에게 빠르게 일처리를 제공해주기 위한 것이다."
  },
  {
    "objectID": "posts/note/2024-01-15-cs-study-001.html#ipc-종류",
    "href": "posts/note/2024-01-15-cs-study-001.html#ipc-종류",
    "title": "📝Operating System 001",
    "section": "IPC 종류",
    "text": "IPC 종류\n\n익명 PIPE\n\n파이프는 두 개의 프로세스를 연결하는데 하나의 프로세스는 데이터를 쓰기만 하고, 다른 하나는 데이터를 읽기만 할 수 있다.\n한쪽 방향으로만 통신이 가능한 반이중 통신이라고도 부른다.\n따라서 양쪽으로 모두 송/수신을 하고 싶으면 2개의 파이프를 만들어야 한다.\n매우 간단하게 사용할 수 있는 장점이 있고, 단순한 데이터 흐름을 가질 땐 파이프를 사용하는 것이 효율적이다. 단점으로는 전이중 통신을 위해 2개를 만들어야 할 때는 구현이 복잡해지게 된다.\n\nNamed PIPE(FIFO)\n\n익명 파이프는 통신할 프로세스를 명확히 알 수 있는 경우에 사용한다. (부모-자식 프로세스 간 통신처럼)\nNamed 파이프는 전혀 모르는 상태의 프로세스들 사이 통신에 사용한다.\n즉, 익명 파이프의 확장된 상태로 부모 프로세스와 무관한 다른 프로세스도 통신이 가능한 것 (통신을 위해 이름있는 파일을 사용)\n\n\n하지만, Named 파이프 역시 읽기/쓰기 동시에 불가능함. 따라서 전이중 통신을 위해서는 익명 파이프처럼 2개를 만들어야 가능\n\nMessage Queue\n\n입출력 방식은 Named 파이프와 동일함\n다른점은 메시지 큐는 파이프처럼 데이터의 흐름이 아니라 메모리 공간이다.\n사용할 데이터에 번호를 붙이면서 여러 프로세스가 동시에 데이터를 쉽게 다룰 수 있다.\n\n공유 메모리\n\n파이프, 메시지 큐가 통신을 이용한 설비라면, 공유 메모리는 데이터 자체를 공유하도록 지원하는 설비다.\n프로세스의 메모리 영역은 독립적으로 가지며 다른 프로세스가 접근하지 못하도록 반드시 보호돼야한다. 하지만 다른 프로세스가 데이터를 사용하도록 해야하는 상황도 필요할 것이다. 파이프를 이용해 통신을 통해 데이터 전달도 가능하지만, 스레드처럼 메모리를 공유하도록 해준다면 더욱 편할 것이다.\n공유 메모리는 프로세스간 메모리 영역을 공유해서 사용할 수 있도록 허용해준다.\n프로세스가 공유 메모리 할당을 커널에 요청하면, 커널은 해당 프로세스에 메모리 공간을 할당해주고 이후 모든 프로세스는 해당 메모리 영역에 접근할 수 있게 된다.\n\n중개자 없이 곧바로 메모리에 접근할 수 있어서 IPC 중에 가장 빠르게 작동함\n\n\n\n메모리 맵\n\n공유 메모리처럼 메모리를 공유해준다. 메모리 맵은 열린 파일을 메모리에 맵핑시켜서 공유하는 방식이다. (즉 공유 매개체가 파일+메모리)\n주로 파일로 대용량 데이터를 공유해야 할 때 사용한다.\n\n소켓\n\n네트워크 소켓 통신을 통해 데이터를 공유한다.\n클라이언트와 서버가 소켓을 통해서 통신하는 구조로, 원격에서 프로세스 간 데이터를 공유할 때 사용한다.\n서버(bind, listen, accept), 클라이언트(connect)\n\n\n이러한 IPC 통신에서 프로세스 간 데이터를 동기화하고 보호하기 위해 세마포어와 뮤텍스를 사용한다. (공유된 자원에 한번에 하나의 프로세스만 접근시킬 때)"
  },
  {
    "objectID": "posts/note/2023-03-31-daily-english-007.html",
    "href": "posts/note/2023-03-31-daily-english-007.html",
    "title": "🌎Casual English Phrases 007",
    "section": "",
    "text": "독차지하다\nhog\n\n명사로는 야생 돼지, 큰 돼지 라는 뜻\n\n\nThey’ll hog the court all day! 그들이 하루 종일 테니스장 코트를 독차지하고 있을거야!\nDon’t hog the toys! 장난감을 독차지 하지마!\nQuit hogging all my time! 내 시간을 독차지하려 하지마!\n\n\n\n뭘 망설이고 있어?(용기를 북돋는)\nWhat are you waiting for?\n\n진짜 노래가사에서 많이 들을 수 있음\n\n\n\n떠나다\nget off\n\nYou big kids get off that court right now! 너네 코트에서 당장 나와!\nI need to get off this island. 난 이 섬에서 떠나야 한다\nGet off the bridge! 그 다리에서 떨어져!\n\n\n\n어떻게 생각해?\nWhat do you say?\n\nWhat do you say to going to the movies tomorrow? 내일 영화 보러 가는 것에 대해서 어떻게 생각해?\n\n\n어떤 제안을 하고 나서 그 제안에 대한 상대방의 견해를 물어볼 때 사용 / 일상이나 업무 둘다 사용\n\n\n\n지루하게 자세한 걸 다 읽지는 않겠습니다.\nI won’t bore you by reading all of the details.\n\nI won’t bore you by telling all of the stories. I’ll make a point. 모든 이야기를 다해서 지루하게 하진 않을게요. 요점을 말하겠습니다.\n\n\n\n나 ~ 못하는 거 알잖아.\nYou know I have trouble (with) ~\n\nYou know I have trouble with the can opener! 나 캔 따개 못 따는 거 알잖아!\nYou know I have trouble speaking out loud in front of a big crowd. 제가 많은 관중들 앞에서 이야기 못하는 거 알잖아요."
  },
  {
    "objectID": "posts/note/2023-04-05-daily-english-008.html",
    "href": "posts/note/2023-04-05-daily-english-008.html",
    "title": "🌎Casual English Phrases 008",
    "section": "",
    "text": "내가 그럴 줄 알았어!\nI knew it!\n\n어쩐지..이상하다 그랬어. 라는 뉘앙스\n\n\nI knew I had the wrong code. (어쩐지..) 내가 잘못된 코드를 가지고 있었던 걸 알고있었어.\nI knew it! It was the wrong size! 역시! 사이즈가 잘못된 거였어!\n\n\n\n생각해보니\nUpon reflection\n\nUpon reflection, I think Charlie Brown was wrong. 생각해보니 찰리가 틀린 것 같아.\nUpon reflection, I think we have to change the main color of this banner. 다시 생각해보니 내 생각에 이 배너의 메인 색을 바꿔야 할 것 같아.\nUpon reflection, I prefer this option. 다시 생각해보니 이 옵션이 좋은 것 같아요.\n\n\n\n배짱이 좋다./뻔뻔하다.\nhave(got) the(a/some/a lot) nerve\n\nnerve: 용기, 대담성, 뻔뻔스러움\n\n\nhave the nerve + to 동사원형\n\n\nHe had the nerve to say that I’m not perfect! 내가 완벽하지 않다고 하다니 배짱도 좋지!\nWow, you have the nerve to criticize my report. 와, 당신이 내 리포트를 비평하다니 배짱이 있으시네요.\nYou really have a nerve, arriving an hour late for the event. 넌 그 행사에 1시간이나 늦게 도착하다니 참 뻔뻔하다.\n\n\n\n~을 정리하다, 정돈하다\nstraighten up\n\nInstead of watching TV, you could be straighten up your room. TV 보는 대신에 방 청소를 할 수도 있지.\nI’m going to straighten up my room. 내 방 정리정돈 할거야.\nStraigthen up your back! 너 허리 똑바로 펴!\n\n\n\n진심이야?\nDo you mean it?\n\nYes, I mean it. 응 진심이야.\n\n\n\n넌 절대 마음 바꿀 생각 없나 보네?\nI suppose you’re not going to change your mind.\n\n\n평소와 달리/기분 전환으로\nfor a change\n\nYou look pretty clean today for a change. 오늘은 평소와 달리 깨끗해 보여.\nWe always go to the movies. Let’s go shopping for a change! 우리 맨날 영화만 보러가니까 오늘은 평소와 다르게 쇼핑가자!\nI think I’m going to do an intense workout for a change. 나 오랜만에 강도 높은 운동을 해야 할 것 같아.\nI want to go my hometown and visit my parents for a change. 기분 전환하게 오랜만에 고향에 가서 부모님을 보고 싶어."
  },
  {
    "objectID": "posts/note/2022-10-29-geultto-7th-end.html",
    "href": "posts/note/2022-10-29-geultto-7th-end.html",
    "title": "📘Gueltto 7th End",
    "section": "",
    "text": "이번 post는 지난 2022년 5월 15일부터 10월 16일까지 활동했던 글쓰는 또라이 7기 활동을 마치고 회고한 글입니다. 처음 시작을 다짐으로 시작하여 9월 14일에 중간 점검글도 잠깐 쓰고 이제 마지막 이글로 회고를 하면서 마무리 지어보려고 합니다."
  },
  {
    "objectID": "posts/note/2022-10-29-geultto-7th-end.html#the-bottom-line",
    "href": "posts/note/2022-10-29-geultto-7th-end.html#the-bottom-line",
    "title": "📘Gueltto 7th End",
    "section": "The bottom line …",
    "text": "The bottom line …\n글또 활동 보증금으로 10만원을 내고 시작했고 모든 권장 활동을 충실히 지킨 결과, 굿즈 구입비 1만원도 커피챗 2회 보상(5000원x2)으로 보충되어 그대로 10만원을 받을 수 있었습니다."
  },
  {
    "objectID": "posts/note/2022-10-29-geultto-7th-end.html#본질",
    "href": "posts/note/2022-10-29-geultto-7th-end.html#본질",
    "title": "📘Gueltto 7th End",
    "section": "본질",
    "text": "본질\n처음에 글또에 참여하겠다고 설문지를 작성하던 그때를 떠올리고 오리엔테이션을 마치고 처음 다짐글을 다시 읽으면서 되새겨보면, 제게 가장 강력했던 동기는 글쓰기 힘 기르기였습니다. 글은 장르와 목적에 따라 어떻게 작성하는게 명확하고 전달력이 좋은지 차이가 크고 콘텐츠를 보는 사람의 즉각적인 피드백을 얻기 힘들기 때문에 머릿속을 빙빙도는 스토리들을 잘 만드는 사람이 되고 싶었습니다. 처음 다짐글에 감상적인 글보다는 IT, 개발관련 글을 쓰고 싶었다고 생각한 이유는 생각이나 감정을 배제하고 싶다는 생각보다는 일기와 같은 글에서는 내 이야기 중심이다보니 나 이외의 독자를 배려하거나 고려할 수 있는 힘을 기를 수 없기 때문이었습니다. 같은 맥락으로 사실 블로그에 내가 공부한 것들을 정리한 정리노트 서랍장이 되지 않기를 바랐던 것도 있었습니다. 나 이외의 독자(들)도 이해를 하고 도움을 받을 수 있을까? 나는 내 글로 다른 사람들이 이 글을 읽을 가치가 있다고 설득할 수 있을까? 고민들을 했었습니다. 물론 지금도 이 질문들에 완벽한 답변이 될 만한 포스트들을 작성하고 있지 못하지만 글또에서 조금은 나아졌다고 생각합니다. 단순히 느낌만 그런것이 아니라, 같은 팀내에서 피드백들을 받고 저보다 글을 더 논리정연하게 쓰고 설득할 줄 아는 분들의 어깨너머로 확실이 배울 수 있었습니다.\n활동 보증금이 있어서 정말 완벽한 의지로 글을 쓰는 습관을 들인 것은 아니지만 그래도 활동 보증금을 다 받을 수 있을 정도 만큼은 성장했다는 것에 만족합니다. 일단 1차 목표였고 한 단계를 밟았기 때문입니다. 사실 이 작은 성장을 시작으로 본질 밑바닥에 깔려있던 흑심은 블로그 포스팅으로 서술하는 능력을 길러서 좋은 논문을 내는 연구자가 되고 싶었던 마음이 있었습니다. 연구가 아직 내 적성에 맞는지는 확신이 서지 않지만 일단 석사과정을 시작한 한 미약한 연구자로써 글을 잘쓰고 싶었기 때문입니다. 큰 학술지는 아니었지만 학부과정에서 2번정도 논문들을 쓸 기회가 있었는데 솔직히 CV에는 한줄이라도 쓰기 위해 논문들을 적어놓지만 남보여주기에는 부끄러웠습니다. 저 이외의 연구자들을 전혀 고려하지 않은 서술들이었기 때문이었습니다. 또한 여러 논문들을 읽다보면 어떤 논문은 정말 감탄이 나올정도로 설계도 깔끔하고 서술도 완벽에 가까운데 반해 어떤 논문은 뭘 말하고자 하는지 왜 이렇게 써야했는지 이해하기 어려운데 적어도 저의 석사논문 만큼은 같은 동료 연구자들에게 고개를 끄덕일 수 있도록 퀄리티가 좋았으면..하는 희망이 있습니다. 이 마음 저 생각이 모여 글또로 시작을 열 생각을 하게된 것 같고 그 본질과 흑심을 표면 밖으로 끄집에 내게 해준 글또 활동들이 소중했습니다.\n본질과는 조금 동떨어지지만 글쓰는 작은 변화가 근래에 있었습니다. 영어공부도 할겸 영어 실력을 향상시키기 위해 daily english 시리즈로 글을 되도록 자주 올리도록 노력하게 되었습니다. 이야말로 위에서 말했던 정리노트 서랍장을 만들 수도 있을 위험이 크지만 그래도 영어표현이나 팁들을 다른 사람들도 보기 쉽게 소비하고 도움을 받을 수 있도록 정리하려고 합니다. 실제로 제가 다른분들의 짧은 영어 포스팅들로 도움을 많이 받기 때문에 이와 같은 일환으로 시작하게 되었습니다."
  },
  {
    "objectID": "posts/note/2022-10-29-geultto-7th-end.html#처음-다짐",
    "href": "posts/note/2022-10-29-geultto-7th-end.html#처음-다짐",
    "title": "📘Gueltto 7th End",
    "section": "처음 다짐",
    "text": "처음 다짐\n\n글의 주제\n\n달성정도: 2/5\n처음 다짐에 로봇 제어(이론이나 시뮬레이션 툴 등), 로봇 연구에서의 강화학습, Julia 언어로 하는 Robotics로 글을 쓰고 싶다고 적었었습니다. 하지만 거의 논문 리뷰 위주로 글을 쓰다보니 해당 주제들에서 많이 벗어난 글들도 있었고 로봇 연구에서의 강화학습에 해당하는 글들만 조금 쓸 수 있었던 것 같습니다.\n\n활동 목표\n\n달성정도: 5/5\n활동 목표로 처음 활동 보증금을 모두 회수할 수 있는 미니멈으로 목표를 정했었습니다. 그리고 중간중간 위험하게 마감을 못지킬 것 같은 순간들도 있었지만 결과적으로 100% 회수할 수 있는 활동기록으로 마무리했습니다. 사실 이게 가장 중요하게 생각한 목표였기 때문에 정말 뿌듯했습니다.\n\n생활 목표\n\n달성정도: 2/5\n알고리즘 문제 풀기와 운동 꾸준히 하기를 생활목표로 정했었는데 우선 알고리즘 문제 풀기는 중간에 연구장학생 준비를 위해 조금 했기 때문에 1점 + 운동은 꾸준히는 아니지만 간헐적으로 시간있을 때 챙길려고 노력하기 때문에 1점 해서 총 2점입니다."
  },
  {
    "objectID": "posts/note/2022-10-29-geultto-7th-end.html#중간-점검",
    "href": "posts/note/2022-10-29-geultto-7th-end.html#중간-점검",
    "title": "📘Gueltto 7th End",
    "section": "중간 점검",
    "text": "중간 점검\n\n매일 커밋하기\n\n달성정도: 3/5\n커뮤니티에서 1001일 매일 커밋하신 분을 보고 모티베이션을 얻어서 정한 목표였는데 역시 매일 커밋은 정말 어려운 일이었습니다. 솔직히 하루 무너지니 다짐이 도미노처럼 우르르 무너져 3-4일간 하기 싫은 적도 있었는데 글또에서 피드백 주셨던 것처럼 의미없는 커밋에 마음 뺏기지 말고 의미있는 커밋에 좀 더 중점을 두고 꾸준히 노력해봐야겠습니다. 본질을 생각해야겠죠.\n\n코드가 들어가는 포스팅 작성하기\n\n달성정도: 0/5\n지금까지 작성해보지 않았던 코드와 함께하는 포스팅을 도전하겠다 해놓고 아직까지도 도전 준비중 상태입니다. 코드를 글에 적기 위해서는 실습을 해보는 시간 + 적절한 편집 실력도 필요해서 더 망설여지는 것 같습니다. 그렇지만 아직 하지 않았을 뿐이지 하지 않을 것은 아니기 때문에 조만간 학교 수업에서 듣는 코드들도 조금씩 작성해보려고 합니다.\n\n멘탈관리를 위해 딴짓하기\n\n달성정도: 3/5\n조만간 특별한 취미생활이라며 중간 다짐글에 적어놓지 않았었는데 그 취미는 클라리넷이었습니다. 사실 어렸을 때부터 악기를 매우매우 좋아해서 이것저것 많이 했었는데 그 중 고1때까지 했던 클라리넷 상자가 어느날 제 눈을 사로잡았습니다. 입시만 끝나면 다시 끄낼 줄 알았던 그 상자가 어느새 대학 졸업하고 대학원에 들어왔는데도 열리지 않았는데 멘탈을 위해, 딴짓을 위해 이것저것 생각하다보니 떠올라서 다시 열게되었습니다. 다행히도 학교 근처에서 렛슨을 하는데가 있어서 다시 시작할 수 있었고 2011.00.00 렛슨 날짜가 적혀있는 교본을 다시 꺼내서 2022.00.00날짜를 적으니 감회가 새로웠습니다. 이에 더해 몸이 기억하고 있는 운지법은 정말 신기했던 것 같습니다.\n\n\n마지막 한 줄로 글또 7기를 정리하자면,\n\n좋은 시도, 경험, 마무리였다."
  },
  {
    "objectID": "posts/note/2022-09-04-mid-retrospective.html",
    "href": "posts/note/2022-09-04-mid-retrospective.html",
    "title": "📘2022 상반기 회고",
    "section": "",
    "text": "이번 post는 2022년도 상반기 회고록에 관한 내용을 담고 있습니다. 지난 8월 25일에 있었던 글또콘 후기와 글또를 처음 시작했을 때의 다짐을 기반으로 회고를 해보았습니다."
  },
  {
    "objectID": "posts/note/2022-09-04-mid-retrospective.html#section",
    "href": "posts/note/2022-09-04-mid-retrospective.html#section",
    "title": "📘2022 상반기 회고",
    "section": "1",
    "text": "1\n글또에서 활동하기 시작하면서 개인적으로 Github의 커밋도 되도록이면 매일하는 목표를 이루고 싶었는데 상반기에 열심히 노력했으나 벌써 몇몇 빈구석이 있습니다. 최근에 1001일 매일 커밋하신 분을 봤었는데 하반기에는 더욱 더 노력을 해보려고 합니다. 더욱더 부지런하게..!"
  },
  {
    "objectID": "posts/note/2022-09-04-mid-retrospective.html#section-1",
    "href": "posts/note/2022-09-04-mid-retrospective.html#section-1",
    "title": "📘2022 상반기 회고",
    "section": "2",
    "text": "2\n글의 퀄리티는 항상 신경쓰지만 쉽게 올리기 힘든 것 같습니다. 개인적으로 글의 퀄리티라는 것은 내가 타겟으로 하는 독자들의 만족감이라고 생각하는데 우선 스스로 독자가 되어 되돌아봐도 아직 모자른 내용들이기에 아직 많이 모자른 것 같습니다. 개인 기록용으로 처음에 목표를 낮게 잡긴했지만 그래도 좀 더 욕심을 내보아서 다른 누군가에게 도움이 되고 이후에는 입소문도 날 수 있는 블로그가 되기를 기대해봅니다. 또한 글의 콘텐츠는 사실 하고 싶은 것이 많지만 하반기에는 특별히 지금까지 작성해보지 않았던 코드와 함께하는 포스팅을 도전해보려고 합니다."
  },
  {
    "objectID": "posts/note/2022-09-04-mid-retrospective.html#section-2",
    "href": "posts/note/2022-09-04-mid-retrospective.html#section-2",
    "title": "📘2022 상반기 회고",
    "section": "3",
    "text": "3\n삶의 풍요로움과 멘탈관리를 위해 조만간 특별한 취미생활을 시작할 것 같은데 글또 커뮤니티에 용기내서 공유할 수 있었으면 좋겠습니다. 사실 지금 글을 작성하면서도 이 취미생활을 할 수 있을까 많이 고민되기 떄문에 구체적으로 적진 않고 Slack에서 인사드리겠습니다.\n\n\n\n\n\n\n이번 포스팅에서는 글의 서두에 적은 것처럼 일기처럼 2022년도의 상반기를 한가닥 묶어보았습니다. 연말에 이글을 또 다시 읽어보며 조금 더 성장해있기를 바랍니다. :)"
  },
  {
    "objectID": "posts/note/2023-03-29-daily-english-005.html",
    "href": "posts/note/2023-03-29-daily-english-005.html",
    "title": "🌎Casual English Phrases 005",
    "section": "",
    "text": "그거 참 위로가 되네\nWell, that’s very comforting\n\nThat’s very comforting↘️. Thanks a lot. 위안이 되네. 진짜 고마워\nThat’s very comforting↗️. 하나도 위로가 되지 않네요.\n\n\n문맥과 강약을 어떻게 주느냐에 따라 같은 말도 비꼬는 말이 될 수 있다.\n\n\n\n자는 것도 점점 더 어려워지네.\nEven sleeping is getting too complicated.\n\nEven meeting people is getting too complicated. 심지어 사람들 만나는 것도 점점 어려워져\nEven expressing my own feeling literally is getting too difficult. 내 감정을 말 그대로 표현하는 것조차 어려워지고 있어\n\n\n\n실수로\nby mistake\n\nThis filter paper I bought by mistake has only two holes. 실수로 리필 용지는 2구짜리로 샀거든요\nI made a reservation for tomorrow by mistake. 실수로 예약을 내일로 잡았어\n\n\n\n복수하겠어\nI’ll get you for this\n\nAs soon as I find out what’s going on, I’ll get you for this. 무슨 일인지 알아내기만 하면 반드시 복수하겠어\nHow dare you! I’ll get you for this. 어떻게 니가 그럴 수 있어! 내가 갚아줄거야"
  },
  {
    "objectID": "posts/note/2023-07-05-daily-english-009.html",
    "href": "posts/note/2023-07-05-daily-english-009.html",
    "title": "🌎Casual English Phrases 009",
    "section": "",
    "text": "지지하다/응원하다\nroot for sm/sth\n\nto support or encourage somebody in a sports competition or when they are in a difficult situation\n\n\nMost of the crowd were rooting for the home team. 대부분의 관중들이 홈팀을 응원하고 있었다.\nGood luck, I’m rooting for you! You’ll be amazing. 행운을 빌어, 난 널 믿어! 넌 최고야.\nHis whole hometown was rooting for him as he made his professional boxing debut on live television. 그의 고향 사람들은 그가 생방송으로 전문 복싱 데뷔를 하는 것을 응원했습니다.\nI’ve always rooted for the company to succeed, since they made some of my most cherished games growing up. 나는 어릴 적부터 그 회사가 성공하기를 항상 바래왔습니다. 그들은 내가 가장 소중히 여기는 게임들을 만들어왔기 때문이죠.\n\n\n\n너 T야?(2nd meaning)\ntone-deaf\n\n\nunable to perceive public attitudes or preferences\nlacking emotional insight; insensitive or unsympathetic to others\n\n\n\nThey’re tone-deaf to the opinions of local people; they’re telling them where the new hospital will be without listening to them. 그들은 현지 주민들의 의견을 무시하고 있습니다; 현지 주민들의 의견을 듣지 않고 새로운 병원이 어디에 위치할 것인지 말하고 있습니다.\nThe council’s politically tone-deaf plan would cost lower income residents an extra 100 dollars a year. They simply can’t afford it. 시의 여론을 반영하지 않는(정치적으로 귀담아 듣지 못하는) 계획은 저소득 주민들에게 매년 추가로 100달러의 비용이 들게 할 것입니다. 그들은 당연히 그 비용을 감당할 수 없습니다.\nShe is often tone-deaf to her daughter’s needs. 그녀는 가끔 딸의 도움(정서적 교류의 필요)에 귀기울이지 못한다.\n\n\n\n내 손바닥 안이야\nhave someone in one’s pocket\n\nto have complete control over someone\n\n\nDon’t worry about the mayor. She’ll cooperate. I’ve got her in my pocket.\nJohn will do just what I tell him. I’ve got him and his brother in my pocket.\nI hear that the boss has half the police force in his pocket."
  },
  {
    "objectID": "posts/note/2022-10-03-daily-english-001.html",
    "href": "posts/note/2022-10-03-daily-english-001.html",
    "title": "🌎Casual English Phrases 001",
    "section": "",
    "text": "영어 회화 표현이나 논문에서 쓸 수 있는 유용한 표현들을 조금씩 익히기 위해 시작한 1000일 시리즈 포스트 입니다. 공부 리소스는 유튜브, 리얼클래스, PN님의 블로그, cake 어플, 책, 논문, 기사 등 다양한 출처에서 보고 기록합니다.\n\n\n갑자기\nout of the blue\n\nA cockroach appeared out of the blue. And then I really wanted to get out of there. 갑자기 바퀴벌레가 나타나서 진짜 거기서 나오고 싶었어.\nI just thought about that out of the blue. 그냥 갑자기 생각나서.\nIt happened out of the blue, so I could not handle that situation. 그 일이 너무 갑자기 일어나서 어떻게 할 수가 없었어.\n\n\n\n듣고있어\nI’m all ears\n\nJust keep talking about it. I’m all ears. 계속 말해. 나 듣고 있어.\nWhen you have a conversation with someone, you should be all ears. 누군가와 대화를 하면 귀기울여서 들어야해.\nDon’t pretend to be all ears. 듣고 있는 척 하지마.\n\n\n\n들킬것 같이 위태롭고 불안한\nprecarious\n\nI hate precarious situations. They make me very vexed. 난 진짜 조마조마한 상황들이 싫어. 진짜 나를 초조하게 만들거든.\n\nvexed : 초조한\n\nReal liars don’t have a precarious feelings. Instead, They enjoy the situations and make more fakes. 진짜 거짓말 쟁이들은 불안한 감정을 느끼지 않아. 오히려 그들은 그 상황을 즐기고 더 많은 거짓들을 만들어내지.\nJust tell the truth to her, not make yourself more precarious. 그냥 그녀에게 진실을 털어놔 더 너 자신을 위태롭게 만들지 말고."
  },
  {
    "objectID": "posts/note/2023-08-31-daily-english-010.html",
    "href": "posts/note/2023-08-31-daily-english-010.html",
    "title": "🌎Casual English Phrases 010",
    "section": "",
    "text": "최고야\nNothing beats ~\n\n가장 좋아하는 것을 말할 때 유용한 표현\n\n\nNothing beats “About Time” \"About Time\" 영화가 최고야.\nNothing beats a good nap. 좋은 낮잠이 최고야.\nNothing beats pizza. 피자보다 좋은 건 없어.\n\n\n\n완전 동의/공감해!\nI couldn’t agree more!\nIt’s hard to go wrong with that choice!\n\n\n꼭 명심해야 할 점은\nRule number one,\n\nRule number one, don’t get yourself into this situation again. 첫 번째 규칙, 다시는 이런 상황을 만들지 마.\nRule number one. Don’t be late. Our boss hates being late. 꼭 명심해야 될 게 있어. 절대 늦으면 안돼. 우리 상사가 늦는 거 진짜 싫어해.\nRule number one. The customer is always right. 첫번째 규칙, 가장 중요한 원칙: 손님은 언제나 옳다.\n\n\n\n정신 차리고 제대로 살기 시작하다, 제대로 돌아가기 시작하다\nget oneself together\n\nGot myself together. So now we’ll see. 정신 차렸다고요. 이젠 어떻게 될지 두고봐야죠.\nYou’ve got to stop drinking every day. Get yourself together! 너 매일 술 마시는 것 좀 그만하고 정신 좀 차려!\nThe company got itself together and turned a profit. 그 회사는 재정비를 마치고 제대로 돌아가기 시작하면서 이윤을 내기 시작했다.\n\n\n\n감정을 추스리다, 감정을 컨트롤하다\npull oneself together\n\nPull yourself together. There’s no point getting angry about it. 진정하고 감정 추스려. 화내 봤자 소용없어.\nPull yourself together. You can’t just sit here and cry. 감정 추스려. 여기 앉아서 울고만 있을 순 없잖아."
  },
  {
    "objectID": "posts/code/2022-12-26-class-dict-yaml.html",
    "href": "posts/code/2022-12-26-class-dict-yaml.html",
    "title": "👩‍💻class ⟷ dict ⟷ yaml",
    "section": "",
    "text": "Config Class\n기준이 되는 부모 class AConfig\n\n자식 class가 같은 하위 class에 override 할 때는 이름을 바꾸지 않음\n부모 class에 없었던 하위 class를 만들 경우 자식Class의 이름(e.g. B, C) 활용\n대문자는 Config 객체의 ID\n소문자는 한개의 Config 클래스 객체 내에서의 하위 class 순서\n\n\nclass AConfig():\n    class A_a_Config:\n        element01 = \"element01\"\n        element02 = True\n    class A_b_Config:\n        element03 = 0\n        element04 = [2,4,6]\n\nBConfig는\n\nAConfig의 A_a_Config를 override\nAConfig의 A_b_Config를 override 하지 않음\nB_a_Config 추가\n\n\nclass BConfig(AConfig):\n    class A_a_Config:\n        element02 = False\n    # class A_b_Config:\n    #     pass\n    class B_a_Config:\n        element05 = {\"key\":\"value\"}\n\nCConfig는\n\nAConfig의 A_a_Config를 override\nAConfig의 A_b_Config를 pass로 override\nC_a_Config 추가\n\n\nclass CConfig(AConfig):\n    class A_a_Config:\n        element02 = False\n    class A_b_Config:\n        pass\n    class C_a_Config:\n        element05 = {\"key\":\"value\"}\n\nDConfig는\n\nAConfig의 A_a_Config를 인자로 받아 override\nAConfig의 A_b_Config를 인자로 받아 override\n\n\nclass DConfig(AConfig):\n    class A_a_Config(AConfig.A_a_Config):\n        element02 = False\n    class A_b_Config(AConfig.A_b_Config):\n        pass\n\nEConfig는\n\n__init__(super)로 Aconfig 상속\n\n\nclass EConfig(AConfig):\n    def __init__(super): pass\n    class E_a_Config:\n        element06 = (1,3,5)\n\n\n\nClass to Dict\nclass 객체를 dictionary로 만들기\n\ndef class_to_dict(obj) -&gt; dict:\n    if not hasattr(obj, \"__dict__\"):                # __dict__ : 클래스 객체의 속성 정보를 확인하기 위해 사용. 객체가 가진 여러가지 속성들을 딕셔너리 형태로 편하게 확인\n        return obj\n    result = {}                                     # 빈 dict 객체 만들기\n    for key in dir(obj):                            # dir(): 어떤 객체를 인자로 넣어주면 해당 객체가 어떤 변수와 메소드(method)를 가지고 있는지 나열\n        if key.startswith(\"_\"):                     # startswith: str.startswith(str or tuple) 형식으로 사용하면 되고, 반환 값으로는 True, False를 반환\n            continue\n        element = []                                # 리스트 요소에 대비\n        val = getattr(obj, key)                     # getattr: object에 존재하는 속성의 값을 가져옴\n        if isinstance(val, list):                   # isinstance로 list 객체인지 확인 e.g.) isinstance(object, type)\n            for item in val:\n                element.append(class_to_dict(item))\n        else:                                       # 리스트가 아닌 모든 요소들 처리\n            element = class_to_dict(val)\n        result[key] = element                       # 결과 dict에 저장\n    return result\n\n\nAConfigDict = class_to_dict(AConfig)\nAConfigDict\n\n{'A_a_Config': {'element01': 'element01', 'element02': True},\n 'A_b_Config': {'element03': 0, 'element04': [2, 4, 6]}}\n\n\n\nBConfigDict = class_to_dict(BConfig)\nBConfigDict\n\n{'A_a_Config': {'element02': False},\n 'A_b_Config': {'element03': 0, 'element04': [2, 4, 6]},\n 'B_a_Config': {'element05': {'key': 'value'}}}\n\n\n\nCConfigDict = class_to_dict(CConfig)\nCConfigDict\n\n{'A_a_Config': {'element02': False},\n 'A_b_Config': {},\n 'C_a_Config': {'element05': {'key': 'value'}}}\n\n\n\nDConfigDict = class_to_dict(DConfig)\nDConfigDict\n\n{'A_a_Config': {'element01': 'element01', 'element02': False},\n 'A_b_Config': {'element03': 0, 'element04': [2, 4, 6]}}\n\n\n\nEConfigDict = class_to_dict(EConfig)\nEConfigDict\n\n{'A_a_Config': {'element01': 'element01', 'element02': True},\n 'A_b_Config': {'element03': 0, 'element04': [2, 4, 6]},\n 'E_a_Config': {'element06': (1, 3, 5)}}\n\n\n\n\n\nDict to Yaml\n\nimport yaml\nfrom pprint import pprint\n\n\nAConfigDictYaml = yaml.dump(AConfigDict)\nprint(AConfigDictYaml)\nprint(type(AConfigDictYaml))\n\nA_a_Config:\n  element01: element01\n  element02: true\nA_b_Config:\n  element03: 0\n  element04:\n  - 2\n  - 4\n  - 6\n\n&lt;class 'str'&gt;\n\n\n\n\nYaml to Dict\n\nAYamlDict = yaml.safe_load(AConfigDictYaml)\npprint(AYamlDict)\n\n{'A_a_Config': {'element01': 'element01', 'element02': True},\n 'A_b_Config': {'element03': 0, 'element04': [2, 4, 6]}}\n\n\n\n\nDict to Class\n\n속성값으로 만들어주는 것이기 때문에 내부 메소드나 내부 클래스가 아닌, 내부 변수와 같음\n\n\nclass Dict2Class(object):\n    def __init__(self, input_dict):\n        for key in input_dict:\n            print(\"Set arribute: \", key)\n            setattr(self, key, input_dict[key]) # object에 존재하는 속성의 값을 바꾸거나, 새로운 속성을 생성하여 값을 부여\n\n\nAYamlDictClass = Dict2Class(AYamlDict)\n\nSet arribute:  A_a_Config\nSet arribute:  A_b_Config\n\n\n\nAYamlDictClass.A_a_Config\n\n{'element01': 'element01', 'element02': True}\n\n\n\nAYamlDictClass.A_b_Config\n\n{'element03': 0, 'element04': [2, 4, 6]}"
  },
  {
    "objectID": "posts/code/2025-01-10-realsense-ros2.html",
    "href": "posts/code/2025-01-10-realsense-ros2.html",
    "title": "👩‍💻ROS2 RealSense C++ Node",
    "section": "",
    "text": "In this post, we’ll walk through how to create two ROS 2 nodes in C++ to stream video from an Intel RealSense camera and display it with OpenCV. We’ll also address common build issues, including linking librealsense2.\n\n\n\nPublisher Node\n\nOpens a RealSense camera using librealsense2.\nStreams color frames at 640×480 resolution, 30 FPS.\nPublishes the frames to a ROS 2 topic (image_raw).\n\nSubscriber Node\n\nSubscribes to the image_raw topic.\nConverts the incoming frames to an OpenCV Mat using cv_bridge.\nDisplays the frames in an OpenCV window.\n\n\n\n\n\n\nROS 2 (Galactic, Humble, Iron, etc. — just ensure your environment is set up).\nIntel RealSense SDK (librealsense2) installed system-wide.\n\nOn Ubuntu, for instance:\nsudo apt-get install librealsense2-dev\n\nOpenCV (e.g., sudo apt-get install libopencv-dev on Ubuntu).\ncv_bridge (from vision_opencv) which is typically installed via ROS 2 packages:\nsudo apt-get install ros-${ROS_DISTRO}-vision-opencv\n(Replace ${ROS_DISTRO} with your actual ROS distribution name, like galactic or humble.)\n\n\n\n\nWe’ll create a new ROS 2 package called my_realsense_example:\ncd ~/ros2_ws/src\nros2 pkg create my_realsense_example \\\n  --build-type ament_cmake \\\n  --dependencies rclcpp sensor_msgs cv_bridge\nThen, place the following files in the my_realsense_example/src/ directory: - realsense_publisher.cpp - image_subscriber.cpp\n\n\n\n#include &lt;memory&gt;\n#include &lt;chrono&gt;\n#include &lt;librealsense2/rs.hpp&gt;           // Intel RealSense SDK\n#include \"rclcpp/rclcpp.hpp\"\n#include \"sensor_msgs/msg/image.hpp\"     // ROS 2 Image message\n#include \"cv_bridge/cv_bridge.h\"         // cv_bridge for ROS &lt;-&gt; OpenCV\n#include \"opencv2/opencv.hpp\"            // OpenCV\n\nclass RealSensePublisher : public rclcpp::Node\n{\npublic:\n  RealSensePublisher()\n  : Node(\"realsense_publisher\")\n  {\n    pipeline_ = std::make_shared&lt;rs2::pipeline&gt;();\n\n    // Configure and start the pipeline to stream color images\n    rs2::config cfg;\n    cfg.enable_stream(RS2_STREAM_COLOR, 640, 480, RS2_FORMAT_BGR8, 30);\n    pipeline_-&gt;start(cfg);\n\n    publisher_ = this-&gt;create_publisher&lt;sensor_msgs::msg::Image&gt;(\"image_raw\", 10);\n\n    timer_ = this-&gt;create_wall_timer(\n      std::chrono::milliseconds(100),  // ~10 Hz\n      std::bind(&RealSensePublisher::publishFrame, this)\n    );\n\n    RCLCPP_INFO(this-&gt;get_logger(), \"RealSense Publisher Node started.\");\n  }\n\nprivate:\n  void publishFrame()\n  {\n    // Wait for the next set of frames\n    rs2::frameset frameset = pipeline_-&gt;wait_for_frames();\n    rs2::video_frame color_frame = frameset.get_color_frame();\n    if (!color_frame)\n    {\n      RCLCPP_WARN(this-&gt;get_logger(), \"No color frame received.\");\n      return;\n    }\n\n    // Convert RealSense frame to OpenCV Mat\n    cv::Mat color_image(\n      cv::Size(color_frame.get_width(), color_frame.get_height()),\n      CV_8UC3,\n      (void*)color_frame.get_data(),\n      cv::Mat::AUTO_STEP\n    );\n\n    // Convert to ROS Image message via cv_bridge\n    auto image_msg = cv_bridge::CvImage(std_msgs::msg::Header(), \"bgr8\", color_image).toImageMsg();\n    image_msg-&gt;header.stamp = this-&gt;get_clock()-&gt;now();\n    image_msg-&gt;header.frame_id = \"realsense_color_frame\";\n\n    publisher_-&gt;publish(*image_msg);\n  }\n\n  std::shared_ptr&lt;rs2::pipeline&gt; pipeline_;\n  rclcpp::Publisher&lt;sensor_msgs::msg::Image&gt;::SharedPtr publisher_;\n  rclcpp::TimerBase::SharedPtr timer_;\n};\n\nint main(int argc, char *argv[])\n{\n  rclcpp::init(argc, argv);\n  rclcpp::spin(std::make_shared&lt;RealSensePublisher&gt;());\n  rclcpp::shutdown();\n  return 0;\n}\n\n\n\n#include &lt;memory&gt;\n#include \"rclcpp/rclcpp.hpp\"\n#include \"sensor_msgs/msg/image.hpp\"\n#include \"cv_bridge/cv_bridge.h\"\n#include \"opencv2/highgui.hpp\"\n#include \"opencv2/imgproc.hpp\"\n\nclass ImageSubscriber : public rclcpp::Node\n{\npublic:\n  ImageSubscriber()\n  : Node(\"image_subscriber\")\n  {\n    subscription_ = this-&gt;create_subscription&lt;sensor_msgs::msg::Image&gt;(\n      \"image_raw\",\n      10,\n      std::bind(&ImageSubscriber::imageCallback, this, std::placeholders::_1)\n    );\n\n    // Create an OpenCV window once\n    cv::namedWindow(\"RealSense Camera\", cv::WINDOW_AUTOSIZE);\n\n    RCLCPP_INFO(this-&gt;get_logger(), \"Image Subscriber Node started.\");\n  }\n\nprivate:\n  void imageCallback(const sensor_msgs::msg::Image::SharedPtr msg)\n  {\n    cv::Mat frame;\n    try\n    {\n      frame = cv_bridge::toCvCopy(msg, \"bgr8\")-&gt;image;\n    }\n    catch (cv_bridge::Exception &e)\n    {\n      RCLCPP_ERROR(this-&gt;get_logger(), \"cv_bridge exception: %s\", e.what());\n      return;\n    }\n\n    if (!frame.empty())\n    {\n      cv::imshow(\"RealSense Camera\", frame);\n      cv::waitKey(1);\n    }\n  }\n\n  rclcpp::Subscription&lt;sensor_msgs::msg::Image&gt;::SharedPtr subscription_;\n};\n\nint main(int argc, char *argv[])\n{\n  rclcpp::init(argc, argv);\n  auto node = std::make_shared&lt;ImageSubscriber&gt;();\n\n  rclcpp::Rate rate(30);\n  while (rclcpp::ok())\n  {\n    rclcpp::spin_some(node);\n    rate.sleep();\n  }\n\n  rclcpp::shutdown();\n  return 0;\n}\n\n\n\nYour CMakeLists.txt (in the root of my_realsense_example) should enable building both nodes and link librealsense2. Below is a working example:\ncmake_minimum_required(VERSION 3.8)\nproject(my_realsense_example)\n\nfind_package(ament_cmake REQUIRED)\nfind_package(rclcpp REQUIRED)\nfind_package(sensor_msgs REQUIRED)\nfind_package(cv_bridge REQUIRED)\nfind_package(OpenCV REQUIRED)\nfind_package(realsense2 REQUIRED)  # Important!\n\ninclude_directories(\n  include\n  ${OpenCV_INCLUDE_DIRS}\n)\n\n# Publisher executable\nadd_executable(realsense_publisher src/realsense_publisher.cpp)\nament_target_dependencies(realsense_publisher\n  rclcpp\n  sensor_msgs\n  cv_bridge\n)\ntarget_link_libraries(realsense_publisher\n  ${OpenCV_LIBS}\n  realsense2\n)\n\n# Subscriber executable\nadd_executable(image_subscriber src/image_subscriber.cpp)\nament_target_dependencies(image_subscriber\n  rclcpp\n  sensor_msgs\n  cv_bridge\n)\ntarget_link_libraries(image_subscriber\n  ${OpenCV_LIBS}\n  realsense2\n)\n\ninstall(TARGETS\n  realsense_publisher\n  image_subscriber\n  DESTINATION lib/${PROJECT_NAME}\n)\n\nament_package()\n\n\n\nUndefined reference to rs2_…: This means you haven’t linked the realsense2 library. Check for find_package(realsense2 REQUIRED) and target_link_libraries(... realsense2).\nCannot find image_subscriber.cpp: Ensure the file name and path in CMakeLists.txt match exactly what’s in your src/ folder.\ncv_bridge or OpenCV not found: Check your dependencies in package.xml and ensure you’ve installed cv_bridge and OpenCV on your system.\n\n\n\n\n\nFrom your workspace (e.g., ~/ros2_ws):\ncolcon build --packages-select my_realsense_example\nsource install/setup.bash\n\nTerminal 1:\nros2 run my_realsense_example realsense_publisher\nTerminal 2:\nros2 run my_realsense_example image_subscriber\n\nA window labeled “RealSense Camera” should appear, displaying your camera feed.\n\n\n\nYou now have a minimal ROS 2 system that publishes color frames from a RealSense camera and displays them in an OpenCV window. This can be further extended to handle depth frames, point clouds, or any other RealSense-supported streams."
  },
  {
    "objectID": "posts/code/2025-01-10-realsense-ros2.html#overview",
    "href": "posts/code/2025-01-10-realsense-ros2.html#overview",
    "title": "👩‍💻ROS2 RealSense C++ Node",
    "section": "",
    "text": "Publisher Node\n\nOpens a RealSense camera using librealsense2.\nStreams color frames at 640×480 resolution, 30 FPS.\nPublishes the frames to a ROS 2 topic (image_raw).\n\nSubscriber Node\n\nSubscribes to the image_raw topic.\nConverts the incoming frames to an OpenCV Mat using cv_bridge.\nDisplays the frames in an OpenCV window."
  },
  {
    "objectID": "posts/code/2025-01-10-realsense-ros2.html#prerequisites",
    "href": "posts/code/2025-01-10-realsense-ros2.html#prerequisites",
    "title": "👩‍💻ROS2 RealSense C++ Node",
    "section": "",
    "text": "ROS 2 (Galactic, Humble, Iron, etc. — just ensure your environment is set up).\nIntel RealSense SDK (librealsense2) installed system-wide.\n\nOn Ubuntu, for instance:\nsudo apt-get install librealsense2-dev\n\nOpenCV (e.g., sudo apt-get install libopencv-dev on Ubuntu).\ncv_bridge (from vision_opencv) which is typically installed via ROS 2 packages:\nsudo apt-get install ros-${ROS_DISTRO}-vision-opencv\n(Replace ${ROS_DISTRO} with your actual ROS distribution name, like galactic or humble.)"
  },
  {
    "objectID": "posts/code/2025-01-10-realsense-ros2.html#creating-the-ros-2-package",
    "href": "posts/code/2025-01-10-realsense-ros2.html#creating-the-ros-2-package",
    "title": "👩‍💻ROS2 RealSense C++ Node",
    "section": "",
    "text": "We’ll create a new ROS 2 package called my_realsense_example:\ncd ~/ros2_ws/src\nros2 pkg create my_realsense_example \\\n  --build-type ament_cmake \\\n  --dependencies rclcpp sensor_msgs cv_bridge\nThen, place the following files in the my_realsense_example/src/ directory: - realsense_publisher.cpp - image_subscriber.cpp"
  },
  {
    "objectID": "posts/code/2025-01-10-realsense-ros2.html#publisher-node-realsense_publisher.cpp",
    "href": "posts/code/2025-01-10-realsense-ros2.html#publisher-node-realsense_publisher.cpp",
    "title": "👩‍💻ROS2 RealSense C++ Node",
    "section": "",
    "text": "#include &lt;memory&gt;\n#include &lt;chrono&gt;\n#include &lt;librealsense2/rs.hpp&gt;           // Intel RealSense SDK\n#include \"rclcpp/rclcpp.hpp\"\n#include \"sensor_msgs/msg/image.hpp\"     // ROS 2 Image message\n#include \"cv_bridge/cv_bridge.h\"         // cv_bridge for ROS &lt;-&gt; OpenCV\n#include \"opencv2/opencv.hpp\"            // OpenCV\n\nclass RealSensePublisher : public rclcpp::Node\n{\npublic:\n  RealSensePublisher()\n  : Node(\"realsense_publisher\")\n  {\n    pipeline_ = std::make_shared&lt;rs2::pipeline&gt;();\n\n    // Configure and start the pipeline to stream color images\n    rs2::config cfg;\n    cfg.enable_stream(RS2_STREAM_COLOR, 640, 480, RS2_FORMAT_BGR8, 30);\n    pipeline_-&gt;start(cfg);\n\n    publisher_ = this-&gt;create_publisher&lt;sensor_msgs::msg::Image&gt;(\"image_raw\", 10);\n\n    timer_ = this-&gt;create_wall_timer(\n      std::chrono::milliseconds(100),  // ~10 Hz\n      std::bind(&RealSensePublisher::publishFrame, this)\n    );\n\n    RCLCPP_INFO(this-&gt;get_logger(), \"RealSense Publisher Node started.\");\n  }\n\nprivate:\n  void publishFrame()\n  {\n    // Wait for the next set of frames\n    rs2::frameset frameset = pipeline_-&gt;wait_for_frames();\n    rs2::video_frame color_frame = frameset.get_color_frame();\n    if (!color_frame)\n    {\n      RCLCPP_WARN(this-&gt;get_logger(), \"No color frame received.\");\n      return;\n    }\n\n    // Convert RealSense frame to OpenCV Mat\n    cv::Mat color_image(\n      cv::Size(color_frame.get_width(), color_frame.get_height()),\n      CV_8UC3,\n      (void*)color_frame.get_data(),\n      cv::Mat::AUTO_STEP\n    );\n\n    // Convert to ROS Image message via cv_bridge\n    auto image_msg = cv_bridge::CvImage(std_msgs::msg::Header(), \"bgr8\", color_image).toImageMsg();\n    image_msg-&gt;header.stamp = this-&gt;get_clock()-&gt;now();\n    image_msg-&gt;header.frame_id = \"realsense_color_frame\";\n\n    publisher_-&gt;publish(*image_msg);\n  }\n\n  std::shared_ptr&lt;rs2::pipeline&gt; pipeline_;\n  rclcpp::Publisher&lt;sensor_msgs::msg::Image&gt;::SharedPtr publisher_;\n  rclcpp::TimerBase::SharedPtr timer_;\n};\n\nint main(int argc, char *argv[])\n{\n  rclcpp::init(argc, argv);\n  rclcpp::spin(std::make_shared&lt;RealSensePublisher&gt;());\n  rclcpp::shutdown();\n  return 0;\n}"
  },
  {
    "objectID": "posts/code/2025-01-10-realsense-ros2.html#subscriber-node-image_subscriber.cpp",
    "href": "posts/code/2025-01-10-realsense-ros2.html#subscriber-node-image_subscriber.cpp",
    "title": "👩‍💻ROS2 RealSense C++ Node",
    "section": "",
    "text": "#include &lt;memory&gt;\n#include \"rclcpp/rclcpp.hpp\"\n#include \"sensor_msgs/msg/image.hpp\"\n#include \"cv_bridge/cv_bridge.h\"\n#include \"opencv2/highgui.hpp\"\n#include \"opencv2/imgproc.hpp\"\n\nclass ImageSubscriber : public rclcpp::Node\n{\npublic:\n  ImageSubscriber()\n  : Node(\"image_subscriber\")\n  {\n    subscription_ = this-&gt;create_subscription&lt;sensor_msgs::msg::Image&gt;(\n      \"image_raw\",\n      10,\n      std::bind(&ImageSubscriber::imageCallback, this, std::placeholders::_1)\n    );\n\n    // Create an OpenCV window once\n    cv::namedWindow(\"RealSense Camera\", cv::WINDOW_AUTOSIZE);\n\n    RCLCPP_INFO(this-&gt;get_logger(), \"Image Subscriber Node started.\");\n  }\n\nprivate:\n  void imageCallback(const sensor_msgs::msg::Image::SharedPtr msg)\n  {\n    cv::Mat frame;\n    try\n    {\n      frame = cv_bridge::toCvCopy(msg, \"bgr8\")-&gt;image;\n    }\n    catch (cv_bridge::Exception &e)\n    {\n      RCLCPP_ERROR(this-&gt;get_logger(), \"cv_bridge exception: %s\", e.what());\n      return;\n    }\n\n    if (!frame.empty())\n    {\n      cv::imshow(\"RealSense Camera\", frame);\n      cv::waitKey(1);\n    }\n  }\n\n  rclcpp::Subscription&lt;sensor_msgs::msg::Image&gt;::SharedPtr subscription_;\n};\n\nint main(int argc, char *argv[])\n{\n  rclcpp::init(argc, argv);\n  auto node = std::make_shared&lt;ImageSubscriber&gt;();\n\n  rclcpp::Rate rate(30);\n  while (rclcpp::ok())\n  {\n    rclcpp::spin_some(node);\n    rate.sleep();\n  }\n\n  rclcpp::shutdown();\n  return 0;\n}"
  },
  {
    "objectID": "posts/code/2025-01-10-realsense-ros2.html#cmakelists.txt-configuration",
    "href": "posts/code/2025-01-10-realsense-ros2.html#cmakelists.txt-configuration",
    "title": "👩‍💻ROS2 RealSense C++ Node",
    "section": "",
    "text": "Your CMakeLists.txt (in the root of my_realsense_example) should enable building both nodes and link librealsense2. Below is a working example:\ncmake_minimum_required(VERSION 3.8)\nproject(my_realsense_example)\n\nfind_package(ament_cmake REQUIRED)\nfind_package(rclcpp REQUIRED)\nfind_package(sensor_msgs REQUIRED)\nfind_package(cv_bridge REQUIRED)\nfind_package(OpenCV REQUIRED)\nfind_package(realsense2 REQUIRED)  # Important!\n\ninclude_directories(\n  include\n  ${OpenCV_INCLUDE_DIRS}\n)\n\n# Publisher executable\nadd_executable(realsense_publisher src/realsense_publisher.cpp)\nament_target_dependencies(realsense_publisher\n  rclcpp\n  sensor_msgs\n  cv_bridge\n)\ntarget_link_libraries(realsense_publisher\n  ${OpenCV_LIBS}\n  realsense2\n)\n\n# Subscriber executable\nadd_executable(image_subscriber src/image_subscriber.cpp)\nament_target_dependencies(image_subscriber\n  rclcpp\n  sensor_msgs\n  cv_bridge\n)\ntarget_link_libraries(image_subscriber\n  ${OpenCV_LIBS}\n  realsense2\n)\n\ninstall(TARGETS\n  realsense_publisher\n  image_subscriber\n  DESTINATION lib/${PROJECT_NAME}\n)\n\nament_package()\n\n\n\nUndefined reference to rs2_…: This means you haven’t linked the realsense2 library. Check for find_package(realsense2 REQUIRED) and target_link_libraries(... realsense2).\nCannot find image_subscriber.cpp: Ensure the file name and path in CMakeLists.txt match exactly what’s in your src/ folder.\ncv_bridge or OpenCV not found: Check your dependencies in package.xml and ensure you’ve installed cv_bridge and OpenCV on your system."
  },
  {
    "objectID": "posts/code/2025-01-10-realsense-ros2.html#building-and-running",
    "href": "posts/code/2025-01-10-realsense-ros2.html#building-and-running",
    "title": "👩‍💻ROS2 RealSense C++ Node",
    "section": "",
    "text": "From your workspace (e.g., ~/ros2_ws):\ncolcon build --packages-select my_realsense_example\nsource install/setup.bash\n\nTerminal 1:\nros2 run my_realsense_example realsense_publisher\nTerminal 2:\nros2 run my_realsense_example image_subscriber\n\nA window labeled “RealSense Camera” should appear, displaying your camera feed."
  },
  {
    "objectID": "posts/code/2025-01-10-realsense-ros2.html#conclusion",
    "href": "posts/code/2025-01-10-realsense-ros2.html#conclusion",
    "title": "👩‍💻ROS2 RealSense C++ Node",
    "section": "",
    "text": "You now have a minimal ROS 2 system that publishes color frames from a RealSense camera and displays them in an OpenCV window. This can be further extended to handle depth frames, point clouds, or any other RealSense-supported streams."
  },
  {
    "objectID": "posts/code/2023-04-04-install-orbit.html",
    "href": "posts/code/2023-04-04-install-orbit.html",
    "title": "👩‍💻Orbit 설치하기",
    "section": "",
    "text": "Orbit은 IsaacLab으로 변경되었습니다.\n\n\n\n\n\n\n이번 포스팅은 Nvidia의 Omniverse Isaac Orbit에 대해 알아보고, Orbit Docs - Installation Guide를 따라 설치과정을 기록해보았습니다. 공식 문서를 참고하여 설치부터 예제 코드들을 살펴보며 시리즈로 포스팅할 예정입니다.\n\n1 Orbit?\n그럼, Orbit 이란 무엇을 말하는 걸까요? 공식 Docs에서는 다음과 같이 설명하고 있습니다.\n\n\n\n\n\n\nOverview\n\n\n\nIsaac Orbit (or orbit in short) is a unified and modular framework, built on top of NVIDIA Omniverse and Isaac Sim, for robot learning. It offers a modular design to easily and efficiently create robot learning environments with photo-realistic scenes, and fast and efficient simulation.\n\n\n\n\nOrbit Github Main Image\n\n\n\n소개글을 보면, 크게 2가지 특징을 살펴볼 수 있습니다. 첫번째로는 Robot learning을 위한 모듈화된 프레임워크라는 점을 알 수 있고 두번째로는 Omniverse와 Isaac Sim이라는 것에 기반한 프로그램이라는 것 입니다. Robot learning을 위한 이라는 말에서 Github 메인 사진에서도 볼 수 있듯이 로봇 작동을 위한 모든 learning 과정을 지원하기 위한 프레임 워크라는 것을 알 수 있습니다. 매니퓰레이터부터 핸드 로봇, 사족보행 로봇에 이르기까지 다양하고 폭넓은 로봇의 작동 시나리오를 지원해주는 프로그램이라는 것을 볼 수 있습니다.\n하지만 Orbit이라는 프레임워크가 Omniverse와 Isaac Sim에 기반한다는 설명은 이전에 Nvidia의 시뮬레이터에 대해 알고 있지 못하신 분들이라면 생소하고 헷갈릴 수 있습니다. Nvidia의 시뮬레이터 개발과정이나 제품에 대해 깊이 알아보고자 하는 것이 이번 포스팅의 목적은 아니므로 간단하게 각 프로그램들의 역학관계를 살펴보자면, Omniverse &gt; Isaac Sim &gt; Orbit의 관계라고 파악해볼 수 있습니다. Omniverse가 가장 큰 범위의 가상세계를 위한 플랫폼이고 그 아래에 로봇틱스 분야를 위한 하위 플랫폼 Isaac Sim이 있고 마지막으로 그 안에 오늘 포스팅의 주인공인 Orbit이 있는 것 입니다.\n여기까지 설명을 들었을 때 명확히 이해가 되지 않는 부분이 있을 수 있습니다. Isaac Sim도 로봇틱스 분야를 위한 하위 플랫폼인데 그 안에 Orbit으로 또 따로 Robot learning이라는 모듈 프레임워크가 더 필요할까 의문이 들 수 있습니다. 심지어 이전에 Nvidia의 시뮬레이터를 조금 아셨던 분들이라면, Isaac Gym이라는 강화학습, 즉 robot learning을 위한 (Issac Sim보다 좀 더 기능이 제한된) 프로그램이 있는데 Orbit은 어떤 위치인지 더 애매모호하게 느껴질 수 있습니다. 실제로 저도 기존에 Isaac Gym으로 강화학습을 진행해왔던 사람으로써 Orbit의 등장은 의아한 점이 있었습니다. 이런 의문을 많은 유저들도 느꼈던 것 같습니다. 포럼에 올라온 Q&A에서 공식 Orbit Maintainer의 답변으로는, Orbit은 Isaac Sim의 진입장벽을 낮추기 위한 내장 프레임워크로 2가지의 목적이 있어서 만들어졌다고 합니다. (Q&A 원본은 링크와 함께 아래에 적어두었으니 참고하시길 바랍니다.)\n\n보다 간소화된 인터페이스를 통해서 로봇 학습을 위한 환경 설계 및 강화학습, 모션 플래닝 등과 같은 로봇틱스 워크 플로우를 지원하는 것\n미리 구축된 환경을 벤치마크 예제로 사용하는 프론트 엔드 프레임워크로서 준비된 에셋과 환경 예제를 통해 warm-start를 할 수 있도록 하는 것\n\n\n\n\n\n\n\nIsaacSim과 Orbit의 차이에 대한 Q&A\n\n\n\n\n\nQ: I have two questions about the Orbit.\nMay I know what is the purpose of orbit? It seems like it is a modular framework for especially robot learning. However, reinforcement learning can be realized by Issac Sim (Issac Gym). So, what is so special about Orbit?\nIn the Orbit tutorial, the example always uses the built-in API, e.g. robots and controllers. Is the Orbit an appropriate platform if we want to use custom robots and controllers?\nThank you.\n\nA: In the following extract from the Orbit maintainer’s public post published in the Omniverse Discord channel dedicated to Isaac Sim\n(…) Isaac Orbit- Batteries included framework to reduce barrier to entry. It serves a dual purpose:\n\nsimplified interface for env design and support for many robotics workflows - RL, Motion planning, teleoperation, imitation learning/behavior cloning, and real robot operation. This unification is the USP of orbit as compared to other interfaces.\nFront end framework for prebuilt environments as benchmark examples. (…) We note that many folks in the community are users of IsaacSim, creating new environments rather than physics solvers. Hence, we hope to provide Orbit to warmstart it with prebuilt assets, and environment examples as benchmarks.\n\nImportantly Orbit is designed such that it can accept community contributions with open licensing. We hope that Orbit will be the environment zoo for IsaacSim with contributions from the community as well as internal development.\nOrbit also has modified env interfaces. However, Orbit is open source, and users can modify & suggest change to these interfaces, as needed. (…)\n\n\n\n정리하자면, Orbit은 Robot Learning을 위한 Nvidia의 오픈 소스 프레임워크인데 가장 사용자에게 친근한(쉽게 시작할 수 있는) 프로그램 이라고 정리해볼 수 있을 것 같습니다. 비유를 들어서 Nvidia 플랫폼의 역학을 정리해보자면, Omniverse는 MS Office, Issac Sim은 Power Point, Orbit은 PPT 디자인 추천 프로그램 정도로 정리하고 넘어갈 수 있을 것 같습니다.\n\n\n2 Orbit을 설치하기 전에\n앞에서 정리한 대로 Orbit은 Omniverse 안에 Isaac Sim을 기반으로 돌아가는 모듈이기 때문에 IsaacSim을 먼저 설치해야 합니다. 엄밀히 말하자면 아직 Orbit을 설치한 것은 아니고, Orbit이라는 기능을 사용하기 위해 준비하는 과정이라고 생각하시면 될 것 같습니다. Isaac Sim은 크게 (1)Workstation Installation과 (2)Container Installation 2가지 방법으로 설치할 수 있습니다. 각자 개인 컴퓨터를 사용하여 이용하고자 한다면 첫번째 Workstation Installation을 따라가면 되므로 이번 포스팅에서는 Workstation Installation 방법을 따라서 설치를 진행하겠습니다.\n\nWorkstation Installation: 공식 홈페이지의 Isaac Sim 설치방법(Workstation Installation)을 따라 Omniverse App을 통해 Isaac Sim 2022.2을 설치합니다. 아래는 제가 설치를 진행했던 스펙입니다. (만약 해당 컴퓨터 자원이 없다면 클라우드 자원을 이용하여 Isaac Sim을 사용할 수 있습니다.)\n\n\n\n\n\n\n\n설치 진행 환경\n\n\n\n\nUbuntu 20.04 LTS\nNvidia RTX 4080\nDriver Version: 525.60.13\n\n\n\nIsaac Sim의 설치과정은 Documentation에 step by step으로 잘 나와있으므로 자세하게 설명하진 않겠습니다. 아래와 같이 Omniverse App에서 Isaac Sim이 보이고 LANCH라는 버튼을 눌러 실행할 수 있으면 설치에 성공한 것 입니다. (23.04.05 기준) 최신 버젼 2022.2.1이 맞는지 꼭 확인해주시고 만약 최신 버젼이 아니라면 LANCH 옆의 list 버튼을 눌러서 최신 버젼을 다운받고 버젼을 선택할 수 있습니다. 아래 사진에서 처럼 2022.2.1 버젼으로 선택되어 있는 것을 확인할 수 있습니다.\n\n\nOmniverse App에서 Isaac Sim 버젼 확인하기\n\n\n\n\nPython Environment Setting: Isaac Sim은 내부(built-in) Python 3.7이 있고 이 default Python 환경을 사용하는 것이 좋습니다. IsaacSim Default Python Environment을 사용하기 위해서는 아래의 예시 command와 같이 Isaac Sim의 Root folder(Isaac Sim 설치시 별도의 경로 설정이 없이 설치했을 경우 ${HOME}/.local/share/ov/pkg/isaac_sim-*, *은 해당 Isaac Sim의 버젼)에 있는 Python 실행파일인 python.sh를 가지고 실행해야 합니다.\n# ${HOME}/.local/share/ov/pkg/isaac_sim-2022.2.1\n./python.sh path/to/script.py\n\n\nIsaac Sim의 Default Folder\n\n\n\n하지만 이와 같이 매번 Isaac Sim의 내부 python 실행파일 경로를 입력하여 python 스크립트를 돌리는 것은 매우 번거로우므로 시스템 환경 변수 설정를 통해 간단히 내부 python을 불러올 수 있습니다. Linux Ubuntu에서는 시스템 환경 변수를 설정하는 파일은 ~/.bashrc 나 ~/.zshrc를 사용합니다. 아래와 같이 리눅스 GUI 환경에서 지원하는 텍스트 에디터인 gedit을 이용하여 시스템 환경 변수를 설정해보겠습니다. Terminal 창을 열어서 command를 입력합니다.\ngedit ~/.bashrc\n팝업되는 .bashrc 파일의 맨 아래에 다음과 같이 입력합니다. #으로 훗날 여러 시스템 환경 변수들을 왜 설정했는지 메모하기 위해서 주석을 달아놓는 것을 권장합니다.(Orbit을 사용하지 않고 Isaac Sim만 설치하는 공식 문서에서는 같은 파이썬 실행파일을 다른 환경 변수로 설치하기 때문에 중복되어 이름을 설치 하지 않도록 주의합니다.) 추가한 후 저장하고 창을 닫습니다.\n# Isaac Sim root directory\nexport ISAACSIM_PATH=\"${HOME}/.local/share/ov/pkg/isaac_sim-2022.2.1\"\n# Isaac Sim python executable\nexport ISAACSIM_PYTHON_EXE=\"${ISAACSIM_PATH}/python.sh\"\n파일을 편집했다고 바로 추가된 환경 변수가 바로 적용되는 것이 아니라 아래의 command까지 터미널에서 실행시키고 나서 적용이 됩니다.\nsource ~/.bashrc\n실행 확인 : 프로그램 설치와 환경변수까지 제대로 설정되었는지 확인하기 위해 Running the simulator에 안내되어 있는 몇 가지 점검 command를 실행시켜 보겠습니다.\n\n시뮬레이터 실행 파일(isaac-sim.sh) 확인\n# note: you can pass the argument `--help` to see all arguments possible.\n$ISAACSIM_PATH/isaac-sim.sh --help\n\n시뮬레이터 내부 파이썬 실행 파일(python.sh) 확인\n# checks that python path is set correctly\n$ISAACSIM_PYTHON_EXE -c \"print('Isaac Sim configuration is now complete.')\"\n\n시뮬레이터 standalone 예제(add_cubes.py)로 실행 확인\n# checks that Isaac Sim can be launched from python\n$ISAACSIM_PYTHON_EXE $ISAACSIM_PATH/standalone_examples/api/omni.isaac.core/add_cubes.py\n\n\nadd_cubes.py 실행 결과\n\n\n\n\n\n\n\n3 Orbit 설치하기\n이제 Orbit을 본격적으로 설치해보려고 합니다. 앞서 Isaac Sim이 제대로 설치가 되고 예제까지 돌아가는지 꼭 확인을 한 후 진행해주시기 바랍니다.\n\nOrbit repository를 git clone 해오기\n# Option 1: With SSH\ngit clone git@github.com:NVIDIA-Omniverse/orbit.git\n# Option 2: With HTTPS\ngit clone https://github.com/NVIDIA-Omniverse/Orbit.git\nSymbolic link 생성하기: 심볼릭 링크란 원본 파일을 가리키도록 하는 윈도우에서의 바로가기 아이콘과 같은 의미로 이해하시면 됩니다. 위에서 git clone한 repo의 위치로 들어가서 다음과 같은 명령어를 통해 심볼릭 링크를 만들어 줍니다.\n# enter the cloned repository\ncd Orbit\n# create a symbolic link\nln -s $ISAACSIM_PATH _isaac_sim\nOrbit 실행파일 명령어 등록하기: orbit을 실행하기 위해서는 매번 git clone 한 repository에 있는 orbit.sh 파일을 써주어야 합니다. 만약 orbit.sh이 있는 위치에서 실행시킨다면 ./orbit.sh 로 간단하겠지만 다른 위치의 파일을 Orbit에서 실행시키고 싶을 때 단축키가 있다면 훨씬 간편하게 실행할 수 있어 편리할 것 입니다. Orbit을 편리하게 실행시킬 수 있는 명령어로는 orbit으로 정해서 즉, ./orbit.sh 와 orbit 이 같도록 하는 작업을 진행해보겠습니다. 앞서 리눅스의 시스템 환경변수를 설정하는 방법에서 .bashrc 파일을 이용하여 명령어 별칭을 등록하는 방법을 이용합니다.\necho -e \"alias orbit=$(pwd)/orbit.sh\" &gt;&gt; ${HOME}/.bashrc\n별칭 등록을 적용하기 위해 source ~/.bashrc도 잊지 않고 실행해주세요.(명령어가 제대로 등록됐는지 확인해보려면 gedit ~/.bashrc를 통해 파일에서 맨 아랫줄에 alias orbit=$(Orbit 레포지토리 위치)/orbit.sh이 써있는지 확인해보면 됩니다.) 터미널에서 아래의 명령어를 입력했을때 다음과 같은 창이 나온다면 명령어 별칭 등록이 잘 된 것 입니다.\norbit --help\n\n\norbit 명령어 별칭 확인\n\n\n\n\n\n\n4 Orbit에서 가상환경 설정\nOrbit에서는 orbit -p라는 명령어를 통해 Isaac Sim의 파이썬 실행파일을 자동으로 가져와서 사용합니다. (혹은 위에서 명령어 별칭을 등록안했다면 Orbit repo 위치에서 ./orbit.sh -p라고 입력해도 됩니다.) 하지만 가상환경을 사용하고 싶다면 Conda를 이용하면 됩니다.\norbit -c (또는 --conda)\n위와 같이 입력하면 자동으로 orbit이라는 이름의 가상환경이 만들어지고 앞으로 이 가상환경을 사용하고 싶다면 conda activate orbit을 통해 활성화 시킬 수 있습니다.\n\n\nConda를 이용하여 가상환경 설치 및 확인\n\n\n\n\n\n5 Extensions 설치\n\napt를 이용해서 우분투에서 디펜던시를 설치합니다.\nsudo apt install cmake build-essential\nOrbit repo에 있는 source/extensions 폴더의 확장프로그램들을 설치합니다. 이때 --editable flag로 편집 가능한 모드로 설치하게 되어 개발자가 확장 기능을 수정하게 되면 변경사항이 즉시 적용됩니다.\norbit --install # or orbit -i\n(rsl-rl 같은)Learning framework 등의 여러 디펜던시 프로그램들을 설치합니다.\norbit --extra # or orbit -e\n\n\n\n6 Closing\n마지막까지 설치과정을 잘 마치셨다면 아래의 명령어를 입력 했을 때 사족보행 로봇들이 나오면서 Orbit이 잘 실행되는 것을 확인하실 수 있습니다!👏👏👏(처음 실행시 시간이 조금 걸릴 수 있으니 기다려주세요.)\norbit -p source/standalone/demo/play_quadrupeds.py\n\n\nplay_quadrupeds.py 실행 결과\n\n\n\n이번 포스팅에서는 Nvidia의 Orbit에 대해 간단히 알아보고 설치까지 진행해보았습니다. 아직 공식 Documentation도 업데이트 중이고 프로그램 자체도 업데이트가 활발히 되고 있어서 이후에 이번 포스팅이 버젼에 따라 도움이 안될 수도 있겠지만 많은 분들께 참고가 되었으면 좋겠습니다. 설치과정에 대해 다른 옵션들이나 자세하게 알고 싶으신 분들은 공식 설치 Documentation을 참고하시면 업데이트 되는 소식들과 함께 더 자세하고 많은 정보를 얻으실 수 있을 것 같습니다. 이어지는 포스팅에서는 각 예제들을 통해서 Orbit에 대해 더 알아보도록 하겠습니다."
  },
  {
    "objectID": "posts/code/2020-07-20-import-custom-module.html",
    "href": "posts/code/2020-07-20-import-custom-module.html",
    "title": "👩‍💻Import custom module",
    "section": "",
    "text": "실제 폴더와 파일들은 아래와 같다.\n\n\n\ngym_foo folder에 반드시 __init__.py를 만들어야 한다.\n\n\n\n파일 구조"
  },
  {
    "objectID": "posts/code/2020-07-20-import-custom-module.html#structure",
    "href": "posts/code/2020-07-20-import-custom-module.html#structure",
    "title": "👩‍💻Import custom module",
    "section": "",
    "text": "실제 폴더와 파일들은 아래와 같다.\n\n\n\ngym_foo folder에 반드시 __init__.py를 만들어야 한다.\n\n\n\n파일 구조"
  },
  {
    "objectID": "posts/code/2020-07-20-import-custom-module.html#code",
    "href": "posts/code/2020-07-20-import-custom-module.html#code",
    "title": "👩‍💻Import custom module",
    "section": "Code",
    "text": "Code\n\nutils_foo.py\n\ndef utils_test():\n    print(\"utils_foo\")\n\nprint(\"HERE: utils_foo\")\n\nHERE: utils_foo\n\n\nenv_foo.py\nfrom gym_foo import utils_foo\n\nutils_foo.utils_test()\n\nprint(\"HERE: env_foo\")\n\ndef env_test():\n    print(\"env_foo\") \nmain_foo.py\nfrom gym_foo import utils_foo\nfrom gym_foo import env_foo\n\nutils_foo.utils_test()\nenv_foo.env_test()"
  },
  {
    "objectID": "posts/code/2020-07-20-import-custom-module.html#goal",
    "href": "posts/code/2020-07-20-import-custom-module.html#goal",
    "title": "👩‍💻Import custom module",
    "section": "Goal",
    "text": "Goal\n\n실행 파일: main_foo.py\nimport하는 파일: env_foo.py\nimport하는 파일 내에서(=env_foo.py) import하는 파일: utils_foo.py"
  },
  {
    "objectID": "posts/code/2020-07-20-import-custom-module.html#how",
    "href": "posts/code/2020-07-20-import-custom-module.html#how",
    "title": "👩‍💻Import custom module",
    "section": "How",
    "text": "How\n\nmain_foo.py에서 env_foo.py를 import한다.\nenv_foo.py에서 utils_foo.py를 import 한다.\n이때 env_foo.py에서 import utils_foo로 utils_foo를 불러오면,\n\npython env_foo.py 실행시 잘 작동되지만(같은 위치)\npython main_foo.py 실행에서는 from gym_foo import *코드를 읽을 때 env_foo.py내의 from gym_foo import utils_foo를 불러올 수 없다고 error가 난다.(상위 위치)\n\n$ python main_foo.py \nTraceback (most recent call last):\nFile \"main_foo.py\", line 4, in &lt;module&gt;\n    env_foo.env_test()\nNameError: name 'env_foo' is not defined\nenv_foo.py에서 utils_foo module을 불러올 때, from gym_foo import utils_foo로 불러온다. 상위 위치인 gym_foo를 거쳐서 import해야한다는 뜻이다. 그러면 python main_foo.py 실행시 잘 작동한다.\n$ python main_foo.py \nHERE: utils_foo\nutils_foo\nHERE: env_foo\nutils_foo\nenv_foo\n한 가지 더 주의해야 할 점이 있다. main_foo.py에서 utils_foo와 env_foo를 import 할 때이다.\nfrom gym_foo import * 코드로 utils_foo와 env_foo가 모두 불러와질 것이라고 생각했으나, main_foo.py를 실행했을 때 import 하지 못한다. 따라서 위에 main_foo.py에서 볼 수 있듯이 from gym_foo import utils_foo, from gym_foo import env_foo각각 따로 import 해줘야 한다."
  },
  {
    "objectID": "posts/code/2024-01-07-quarto-blog.html",
    "href": "posts/code/2024-01-07-quarto-blog.html",
    "title": "👩‍💻Quarto Blog 기본 셋팅",
    "section": "",
    "text": "이번 포스팅은 Github Blog를 만들어 갔던 경험과 그 경험들을 통해 Quarto라는 좋은 툴을 소개하기 위한 글로 준비를 했습니다. 본격적인 Quarto로 속편한 Github Blog를 구축하는 방법을 소개하기 전에 어떻게 Quarto를 알게 되었고 적극적으로 소개하게 되었는지 이야기를 드릴려고 합니다.\n이전에 글또 7기를 시작하는 다짐글에서 잠깐 기술 블로그 방황기에 대해 언급을 한 적이 있었습니다. 하지만 기술 블로그를 작성해보고 싶은 계기부터 잠깐 이야기해보려고 합니다. 학부 전공은 기계공학과인지라 컴퓨터, IT 관련글들을 볼 기회가 많았던 것은 아니지만 어쩌다보니 AI, 인공지능의 매력에 빠져서 공부를 하다보니 책보다는 구글링과 유튜브로 많은 지식들을 배우게 되는 경험들이 자연스럽게 기술 블로그를 작성하는 문화에 빠져들게 되었습니다. 저와 같이 아무것도 모르는 사람들을 위해 친절한 설명들과 단계별 캡쳐 사진들을 따라가다보면 원하는 프로그램을 실행시킬 수 있었고, 때론 내가 막혀있던 점들을 푼 이전의 누군가의 간단히라도 적어둔 팁을 확인하고 도움을 얻을 수 있었기 때문에 잘 키워가는 기술 블로그 하나 열 책 안 부러울 정도로 기술 블로그에 대한 막연한 동경이 있었습니다.\n그러다가 어느 정도 공부를 하다보니 \"나도 한번 내 기술 블로그를 운영해볼까?\"라는 호기가 생겼습니다.\n시작도 하기 전에 (살짝은) 비장했던 마음도 있었고 설렜던 마음도 있었습니다. 아직 어떻게 블로그를 만들지 생각도 없었지만 나도 무언가를 아카이빙하고 그 글들이 다른 많은 사람들에게 도움이 되면 좋지 않을까라고 생각하며 유명해지면 어떨까..?라는 행복회로를 돌리며 다들 어떻게 기술 블로그를 만드는지 찾아보기 시작했습니다. 나름 AI 공부를 하며 코딩을 할 줄 아니까 Markdown까지는 문서를 작성하는 규칙이니 쉽게 적응을 했지만, 웹 개발은 정말 잘 모르는 분야에다가 웹 개발 공부에 시간을 투자하면서 까지 Github 블로그를 만들 수는 없었기에 눈 앞이 깜깜해지는 기분이었습니다.\n물론..! 이전에 Hugo, Notion 개인 도메인, Velog 등등 여러 블로그 구축 방법들도 있었지만 플랫폼마다 커스텀하기가 어려운 점도 있었고 새로운 언어를 새롭게 공부해야 하는 경우들도 있었기 때문에, 저는 최대한 블로그의 내가 쓸 내용 Contents에 집중할 수 있도록 내용이 Compile/Rendering 되는 건 알아서 해주면 좋겠다는 바람이 있었습니다. (왜 Github Blog를 선택했는지는 여러 조건과 개인적 선호를 반영한 내용이 있으니 이전글을 참고해주세요!) 그러던 중에 FastAI에서 딥러닝 기술 블로그를 가장 심플하고 빠르게 작성할 수 있는 FastPages를 만들었다는 걸 알게 되었고 이 FastPages를 활용해서 잘 작성하고 있었습니다. FastPages의 최대 장점은 AI에서 많이 사용하는 Jupyter Notebook 형식의 파일을 그대로 포스팅을 할 수 있는 기능 등을 통해 원래 제가 원했던 점인 정말 Contents에 집중할 수 있는 환경을 만들어주는 Build 프로그램이었기에 정말 마음에 쏙 들었고 그대로 기술 블로그를 만들었습니다.\n그렇게 이제는 더이상 블로그 이사를 안할 줄 알았지만..(블로그 이사는 정말 품이 많이 드는 것 같아요🥲) fastpages repository에 청천벽력과 같은 공지가 붙게 되었습니다.\n아예 Fastpages 서비스를 닫아서 블로그가 아예 안열리는 것은 아니었지만, 블로그 지속성을 위해 마지막 블로그 이사를 Quarto로 하게 되었고 결과적으로는 Fastpages보다 Quarto가 훨씬 사용하기도 편하고 적당한 자유도가 있는 플랫폼이라는 생각이 들어서 오늘 이 포스팅을 통해 많은 분들께 알리고 싶어서 글을 작성하게 되었습니다. 생각보다 많은 분들이 알고 계시지 않은 것 같아 앞으로 주변에 더더욱 열심히 홍보하려고 합니다. Quarto를 중심으로 VSCode(+Copliot), Typora를 사용하면 더욱 쾌적한 블로그를 만들 수 있기 때문에 Quarto를 메인으로 다른 프로그램들은 가볍게 서포트 프로그램이라고 생각해주시면 될 것 같습니다.\n그럼 프로그램들을 하나씩 살펴본 후에 Github repository를 하나 만들어서 블로그(Website)를 하나 만들어보는 실습까지 가보도록 하겠습니다!"
  },
  {
    "objectID": "posts/code/2024-01-07-quarto-blog.html#repository-하나-만들기",
    "href": "posts/code/2024-01-07-quarto-blog.html#repository-하나-만들기",
    "title": "👩‍💻Quarto Blog 기본 셋팅",
    "section": "1. Repository 하나 만들기",
    "text": "1. Repository 하나 만들기\n블로그를 만들 Repository를 Github에 하나 만들어 줍니다. Repository는 어떤 이름을 가지고 있어도 상관없지만 보통 Github에서 개인 블로그 주소를 본인의 nickname.github.io로 만들기 때문에 저는 아래와 같이 블로그를 위한 Repository를 만들었습니다.\n\n\n\n다음으로 해당 Repository의 Settings에 들어가서 블로그의 웹페이지들이 랜더링될 폴더를 /docs로 설정하는 과정을 아래와 같이 설정해줍니다.\n\n\n\n다음으로 해당 repository를 원하는 경로에 clone 해서 블로그를 작성할 폴더를 준비합니다."
  },
  {
    "objectID": "posts/code/2024-01-07-quarto-blog.html#quarto-project-시작",
    "href": "posts/code/2024-01-07-quarto-blog.html#quarto-project-시작",
    "title": "👩‍💻Quarto Blog 기본 셋팅",
    "section": "2. Quarto Project 시작",
    "text": "2. Quarto Project 시작\n이제 VSCode를 열어서 확장 프로그램에서 Quarto를 검색한 후 해당 확장 프로그램을 VSCode에 깔아줍니다.\n\n\n\nVSCode에서 Ctrl+Shift+P 단축키로 명령어 팔레트를 열어서 Qaurto:Create Project를 실행합니다.\n\n\n\n출처: Quarto 공식 홈페이지\n\n\nBlog Porject를 선택합니다. 이때 프로젝트 이름은 아무거나 입력헤도 됩니다. 프로젝트를 생성한 후, 프로젝트 하위에 있는 모든 파일들을 Github repository 이름과 동일한 폴더로 옮겨줍니다.\n\n\n\n출처: Quarto 공식 홈페이지\n\n\n그러면 Github Repository의 이름이 test_blog라고 했을 때 아래와 같이 폴더와 파일들의 구성이 되는 것을 확인할 수 있습니다.\n\n\n\nBlog project 시작"
  },
  {
    "objectID": "posts/code/2024-01-07-quarto-blog.html#default-blog-localhost로-확인",
    "href": "posts/code/2024-01-07-quarto-blog.html#default-blog-localhost로-확인",
    "title": "👩‍💻Quarto Blog 기본 셋팅",
    "section": "3. Default Blog localhost로 확인",
    "text": "3. Default Blog localhost로 확인\n이제 VSCode에서 Blog Repository(test_blog)가 열린 프로젝트 창에서 터미널을 열어서 아래의 명령어를 입력해봅니다.\nquarto preview\n\n\n\n그러면 Default로 생성된 블로그 페이지가 localhost로 열리는 것을 확인할 수 있습니다.\n\n\n\nDefault Blog"
  },
  {
    "objectID": "posts/code/2024-01-07-quarto-blog.html#blog-publish",
    "href": "posts/code/2024-01-07-quarto-blog.html#blog-publish",
    "title": "👩‍💻Quarto Blog 기본 셋팅",
    "section": "Blog Publish",
    "text": "Blog Publish\n이전 단계에서는 localhost로 페이지를 랜더링해서 확인한 것이기 때문에 publish가 되지 않았습니다. quarto에서 프로젝트를 rendering하기 위해서는 publish를 하는 명령어를 실행해야 합니다.\n하지만 그전에!\npage들을 publish할 directory를 /docs로 따로 설정을 해주었기 때문에 해당 폴더에 랜더링된 html 파일들을 만들 수 있도록 아래와 같이 ._quarto.yml 설정파일을 아래와 같이 작성해줍니다.\nproject:\n  type: website\n  output-dir: docs\n\n\n\n위와 같이 render이 잘 완료되었다는 메세지가 확인이 되면 아래의 명령어를 실행하여 publish 합니다.\nquarto render\n마지막으로 Github에 Push를 하게 되면 이제 website를 확인할 수 있습니다!(build되는데 약 2-3분 걸릴 수 있습니다.)\n이번에는 Quarto Engine을 이용해서 간단하게 Blog Website를 올리는 부분을 진행했습니다. 좀 더 본격적으로 조정할 수 있는 옵션들에 대해서는 다음 포스팅에서 살펴볼 예정입니다.\n\nReference\n\nQuarto Homepage\nTypora Homepage"
  },
  {
    "objectID": "posts/code/2023-06-18-chord.html",
    "href": "posts/code/2023-06-18-chord.html",
    "title": "👩‍💻Chord Graph",
    "section": "",
    "text": "이전 Accessibility-Based Clustering for Efficient Learning of Locomotion Skills 논문을 리뷰하면서 로봇의 static pose들을 가지고 K-Acc Clustering하는 과정 이후에 Clustering Analysis에서 Inter-cluster accessibility를 Visulization을 하는 부분이 있었습니다.\n오른쪽에 보이는 그래프가 Chord Graph인데 각 Top-20 cluster에 속한 sample pose들을 하나의 node로 표현하고 각 sample pose들이 다른 pose로 transition되는 시간을 기반으로 계산된 accessiblity 값이 높은 부분은 진한 edge로 accessibility가, 낮은 부분은 옅은 edge로 시각화하여 포즈들 간의 관계성을 보여줍니다. 따라서 이런 시각화를 통해 cluster 간의 inter-cluster accessibility를 파악할 수 있는 것 입니다.(자세한 내용은 이전 논문 리뷰 포스팅을 참고 바랍니다.) 이번 포스팅은 바로 이 Chord graph를 Holoviews라는 파이썬 패키지를 이용해서 시각화 하는 방법에 대해 다룰 것 입니다."
  },
  {
    "objectID": "posts/code/2023-06-18-chord.html#accesstimetable",
    "href": "posts/code/2023-06-18-chord.html#accesstimetable",
    "title": "👩‍💻Chord Graph",
    "section": "AccessTimeTable",
    "text": "AccessTimeTable\n샘플링한 Static poses들을 2000개를 poses.pickle 데이터로 저장해놨습니다. 각 pose-to-pose를 PD tracking을 하며 걸리는 시간을 측정하게 되는데 pose-to-pose로 transition되는 시간은 1초가될 수 있도록 joint trajectory를 만들어주고 PD제어를 하면서 0.0025초 마다 destination pose로 도달했는지(시뮬레이터의 dt)를 체크합니다. 이때 무한정 시간을 잴 수는 없기 때문에 10초로 시간을 제한하여 최대 10초까지만 걸리는 시간을 기록하게 됩니다.\n\n이 정보가 총 2000개 샘플 포즈에 대해서 1:1로 모두 구해야 하기때문에 병렬계산을 해서 저장하여 총 75개의 npy데이터로 나누어 계산하였고 이를 2000 by 2000 매트릭스로 만들어서 AccessTimeTable을 시각화하면 아래와 같이 그려집니다.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport pickle5 \nwith open('../poses.pickle', 'rb') as handle: poses = pickle5.load(handle)\n# 불러올 npy 파일 이름들을 리스트로 만들어줍니다.\naccessTimeFiles = ['accessTimeTableAidin%d.npy'%x for x in range(75)]\n\n\ntimeMat = np.sum([np.load(f) for f in accessTimeFiles], axis=0)\n# 자기자신 pose로부터 자기자신의 pose로의 시간은 0초로 처리하기 위해 대각선의 매트릭스 값은 0으로 일괄 처리합니다.\ntimeMat[ range(len(timeMat)), range(len(timeMat)) ]= 0 \n\n\ntimeMat # shape: (2000, 2000)\n\narray([[ 0.   ,  1.245, 10.   , ...,  1.145, 10.   , 10.   ],\n       [10.   ,  0.   ,  1.555, ...,  0.955,  1.31 , 10.   ],\n       [10.   ,  3.09 ,  0.   , ..., 10.   , 10.   , 10.   ],\n       ...,\n       [10.   ,  0.965,  2.035, ...,  0.   , 10.   , 10.   ],\n       [ 1.345,  1.015, 10.   , ...,  1.145,  0.   , 10.   ],\n       [10.   , 10.   ,  1.4  , ..., 10.   , 10.   ,  0.   ]])\n\n\n\nfig = plt.figure(figsize=(8,8))\nplt.imshow(timeMat)\nplt.title(\"Time Matrix\")\nplt.colorbar()\nplt.show()\nplt.close()\n\n\n\n\n\n\n\n\n10초 이하로 측정되었던 time data와 10초 이상으로 측정된 time data의 수를 살펴보면 아래와 같습니다.\n\nnp.sum(timeMat &lt; 9.999)\n\n907572\n\n\n\nnp.sum(timeMat &gt;= 9.999)\n\n3092428"
  },
  {
    "objectID": "posts/code/2023-06-18-chord.html#accessibility-histogram",
    "href": "posts/code/2023-06-18-chord.html#accessibility-histogram",
    "title": "👩‍💻Chord Graph",
    "section": "Accessibility Histogram",
    "text": "Accessibility Histogram\n이러한 timeMat를 Accessiblity 공식에 맞게 다시 계산하게 됩니다. 이때 10초 이상이 되는 데이터는 1e-8으로 만들어서 가장 낮은 accessiblity 점수를 얻도록 처리합니다.\n\nacc_matrix = (timeMat &lt; 9.999) * np.exp(-timeMat/10) + (timeMat &gt;= 9.999) * 1e-8\n\n히스토그램으로 Accessibility를 시각화하면 다음과 같습니다.\n\nvalues = np.reshape(acc_matrix,(-1,)) # 2000 x 2000 = 4000k\nax = plt.hist(values, bins=50, range=(-1e-4,1.1))\nplt.show()\n\n\n\n\n\n\n\n\n무한대 시간이 걸렸던 부분을 제외하고 히스토그램을 그려보면 아래와 같습니다.\n\nax = plt.hist(values, bins=50, range=(+1e-4,1.1))\nplt.show()"
  },
  {
    "objectID": "posts/code/2023-06-18-chord.html#k-acc-algorithm",
    "href": "posts/code/2023-06-18-chord.html#k-acc-algorithm",
    "title": "👩‍💻Chord Graph",
    "section": "K-Acc algorithm",
    "text": "K-Acc algorithm\n계산한 Accessibility 값을 기준으로 K-Acc 알고리즘으로 centroid pose와 적절한 centroid 수를 결정하게 됩니다. 논문에서 소개된 K-Acc 알고리즘은 원저자가 공개한 코드를 그대로 사용하여 AiDIN-VIII 데이터에 적용했습니다.\n\n\n\n\n\n\nK-Acc 알고리즘을 수행하는 K_access 클래스 살펴보기\n\n\n\n\n\nclass K_access:\n    def __init__(self, access, k=2, seed=123):\n        self.seed = seed\n        np.random.seed(self.seed)\n        self.k = k # 클래스 수 \n        self.access = access # weight_matrix\n        self.node_num = len(access) \n        self.access[range(self.node_num),range(self.node_num)] = 1 # 대각 성분을 1로\n        self.core_index = np.zeros((k,), dtype=int) \n        self.core_index[0] = np.random.randint(0, self.node_num, (1,)) # 클래스 범위 내에 랜덤한 정수를 코어 인덱스로 설정\n\n        for i in range(1, k):\n            ready_core_access = np.sum(self.access[self.core_index[:i],:], axis=0) \\\n                                + np.sum(self.access[:,self.core_index[:i]], axis=1)\n            ready_core_access[self.core_index[:i]] += 999999 # accessible to self\n            self.core_index[i] = np.argmin(ready_core_access) # the one that is the farthest from \n            \n        self.assignment = np.zeros((self.node_num,),dtype=int)\n        self.labels = np.zeros((self.node_num,),dtype=int)\n        self.cores_sorted = np.zeros((self.node_num,),dtype=int)\n        self.max_iter = 10000\n        \n        self.assign()        \n            \n    def assign(self):\n        # from core to nodes\n        core_access = self.access[self.core_index] \n        # argmax access(core,node)\n        self.assignment = self.core_index[np.argmax(core_access, axis = 0)] \n        for c in self.core_index:\n            self.assignment[c] = c  # self belongs to self\n        return\n    \n    def update(self):\n        for i in range(self.k):\n            Si = np.where(self.assignment == self.core_index[i])[0]\n            access_Si = self.access[Si,:][:,Si]\n            minaccess_Si = np.min(access_Si,axis=1)\n            self.core_index[i] = Si[np.argmax(minaccess_Si)]\n        return        \n    \n    def fit(self):\n        pre_assignment = np.zeros((self.node_num,),dtype=int) - 1\n        iter_ = 0\n        while np.sum(np.abs(self.assignment - pre_assignment)) != 0 and iter_ &lt; self.max_iter :\n            pre_assignment = self.assignment\n            iter_ += 1\n            self.update()\n            self.assign()\n        return iter_\n            \n    def predict(self):\n        map_ = {}\n        cnt_Si = []\n        for i in range(self.k):\n            cnt_Si.append(len(np.where(self.assignment == self.core_index[i])[0]))\n        sorted_index = (9999 - np.array(cnt_Si)).argsort()  # descending by # samples in clu\n        for i in range(self.k):\n            map_[self.core_index[sorted_index[i]]] = i+1 # from 1 to # clu\n        self.labels = [map_[c] for c in self.assignment]\n        cores_sorted = self.core_index[sorted_index]\n        return self.labels, cores_sorted \n    \n    def inter_access(self):\n        cnt_Si = []\n        for i in range(self.k):\n            cnt_Si.append(len(np.where(self.assignment == self.core_index[i])[0]))\n        sorted_index = (9999 - np.array(cnt_Si)).argsort()  # descending by # samples in clu\n        all_to_C = self.access[:,:][:,self.core_index[sorted_index]]\n        inter_ = np.zeros((self.k,self.k))\n        for i in range(self.k):\n            Si = np.where(self.assignment == self.core_index[sorted_index[i]])[0]\n            Si_to_C = all_to_C[Si,:]\n            inter_[i,:] = np.mean(Si_to_C,axis = 0)\n            inter_[i,i] = 1\n        return inter_\n    \n    def intra_access(self):\n        intra_ = np.zeros((self.k,))\n        cnt_Si = []\n        for i in range(self.k):\n            cnt_Si.append(len(np.where(self.assignment == self.core_index[i])[0]))\n        sorted_index = (9999 - np.array(cnt_Si)).argsort()  # descending by # samples in clu\n        for i in range(self.k):\n            Si = np.where(self.assignment == self.core_index[sorted_index[i]])[0]\n            intra_[i] = np.min(self.access[self.core_index[sorted_index[i]],Si],axis=0)\n        return intra_\n    \n    def evaluate(self):\n        intra_ = np.mean(np.log(self.intra_access()))\n        inter_ = np.mean(np.log(self.inter_access()))\n        cnt_Si = []\n        for i in range(self.k):\n            cnt_Si.append(len(np.where(self.assignment == self.core_index[i])[0]))\n        num_one_sample_cluster = len(np.where(np.array(cnt_Si)==1)[0])\n        alpha = 1\n        # index I\n        # larger is better\n        score_ = intra_ - inter_ - alpha * num_one_sample_cluster \n        return score_\n\n\n\nK_access 클래스를 가지고 최적의 클래스 수를 선정하기 위해 fit을 클래스 수를 늘려가며 수행합니다. 그랬을 때 155개의 centroid cluster를 가졌을 때 가장 index 점수가 높아 최적의 클래스 수를 선정할 수 있었습니다.\n\nscores = []\nn_cls = range(1,201) # 클래스의 수 1 ~ 200까지 조사\nfor num_class in n_cls:\n    k = K_access(acc_matrix, num_class)\n    k.fit()\n    scores.append(k.evaluate())\n    \nfig,ax = plt.subplots(figsize=(10, 10*9/16))\nmax_ind = np.argmax(scores)\nprint(max_ind+1, scores[max_ind]) # 최적의 클래스 수, 그때의 인데스 점수\n\nplt.plot(n_cls, scores, marker='o', markersize=1)\nplt.plot([max_ind+1],[scores[max_ind]],marker='o',c='r',markersize=2)\nplt.xlabel('Number of Clusters')\nplt.ylabel('Index Value')\nplt.xlim([0,170])\nplt.show()\n\n155 14.17499592680445\n\n\n\n\n\n\n\n\n\n선정된 클래스 수를 가지고 다시한번 클러스터링 작업을 거져 각 centroid pose인 cores에 대한 정보와 inter_access, intra_access 점수를 가져올 수 있습니다.\n\nnum_class = max_ind+1\nk = K_access(acc_matrix, num_class)\nk.fit()\nclusters, cores = k.predict()\ninter_access = k.inter_access()\nintra_access = k.intra_access()\n\n\ncores # centroid pose의 ID\n\narray([1213, 1043, 1150, 1555,  112,  100, 1032,  121,  140, 1097, 1116,\n       1554, 1377, 1623,   33,   41,  250,   29, 1272, 1926, 1401,   22,\n       1888,  809, 1262,  165, 1513,  108, 1248,  333, 1008,  330, 1081,\n        157,  419, 1227, 1231, 1244,  564,  392,  543,   18, 1785,  227,\n       1288, 1644, 1715,  398, 1527, 1017,  169, 1056,  139,  323, 1848,\n       1121, 1067,  225,  476,  450,  898,    8, 1492, 1223,  467, 1844,\n       1608, 1803, 1839, 1913, 1015,   96, 1020,  627, 1526,  691,  268,\n       1899, 1521, 1787,    3, 1597,  210,  643,  502, 1358,  209,  798,\n       1887, 1216,  332,  972, 1122,  404,  343, 1423,  363,  173, 1544,\n          5, 1817,  960,   72, 1832,  853,  446,  479,  395,  650,  313,\n       1587,  677,  239, 1089,  464,  891, 1029, 1491, 1477,  761,  860,\n       1352, 1564,  938,  645, 1254,   50, 1149,  453, 1791,  777, 1990,\n       1737,   47, 1220, 1537,  315, 1180,  162, 1277, 1568,  910,  528,\n        523, 1361,  709, 1718, 1045,  618,   86, 1889, 1250,  455,  814,\n        172])\n\n\n각 클러스터마다 포함하고 있는 샘플들의 수는 어떻게 분포하고 있을까요? 히스토그램으로 시각화를 해보았습니다. 각 샘플들의 수는 이후에 chord diagram의 노드가 될 것 입니다.\n\n# figure 설정\nplt.rcParams['lines.linewidth']=0.7\nplt.rcParams['xtick.direction']='in'\nplt.rcParams['ytick.direction']='in'\nplt.rcParams['xtick.major.width']=0.4\nplt.rcParams['ytick.major.width']=0.4\nplt.rcParams['xtick.major.size']=2\nplt.rcParams['ytick.major.size']=2\nfig,ax = plt.subplots(figsize=(20, 5))\nax.spines['right'].set_visible(False)\nax.spines['top'].set_visible(False)\n\nax_ = plt.hist(np.array(clusters)-0.5,\n               range = [0.5, np.max(clusters)+0.5],\n               bins = np.max(clusters),\n               edgecolor='dimgray',\n               color='deepskyblue',\n               rwidth=1,\n               alpha=0.5)\n\nax = plt.xticks(range(1,1+np.max(clusters),5))\nplt.xlabel('Cluster ID',fontsize=15)\nplt.ylabel('Number of Samples',fontsize=15)\nplt.show()"
  },
  {
    "objectID": "posts/code/2023-06-18-chord.html#get-kacc-poses",
    "href": "posts/code/2023-06-18-chord.html#get-kacc-poses",
    "title": "👩‍💻Chord Graph",
    "section": "Get Kacc poses",
    "text": "Get Kacc poses\n알고리즘을 통해 선정된 centroid pose들에 대한 정보를 한번 확인해보겠습니다. 각 pose에 대한 정보는 아래와 같은 형식으로 정리해서 가장 많은 샘플 수를 포함하고 있는 top-3 centroid pose를 확인해보겠습니다.\n[\n([0,0,height], [ 12 joints], [roll, pitch, 0] ),\n]\n\nkaccess_config = []\nfor i in range(np.max(clusters)):\n    centroid_pose = poses[cores[i]]\n    ele1 = [0,0, centroid_pose['store_height']]\n    ele2 = centroid_pose['store_joints']\n    ele3 = list(centroid_pose['store_roll_pitch'])+[0]\n    ele = (ele1,ele2,ele3)\n    kaccess_config.append(ele)\n\n\n\n\n\n\n\nTop-3 Centroid pose 정보 확인하기\n\n\n\n\n\n[([0, 0, 0.30607859912101715],\n  [0.6053835299386103,\n   1.4824030607413299,\n   -0.0809348638895901,\n   0.3995084366561768,\n   1.2970169955948037,\n   -0.7264673491159409,\n   0.05388458488859648,\n   0.33537195041378626,\n   -0.24038702044129037,\n   -0.501582056954722,\n   -1.5199349541933391,\n   -0.44053023354145876],\n  [3.141592551130548, 0.37124208223469835, 0]),\n ([0, 0, 0.12894579304472167],\n  [0.05857655877237639,\n   -1.4077772309827037,\n   -0.7778225275558831,\n   0.1038327197157696,\n   -0.7770055977003666,\n   -0.9684446036207767,\n   0.06583156564999947,\n   -0.903642551883172,\n   0.3522403885013049,\n   -0.6108687315594281,\n   0.7353928319105865,\n   -0.21053293619245944],\n  [-0.412046597574257, 0.043837823368155976, 0]),\n ([0, 0, 0.2968457946843744],\n  [0.06337843283646641,\n   2.7942809454055157,\n   -0.9737491947994557,\n   -0.2517691066931858,\n   -2.624845950050974,\n   -0.18089990597943467,\n   -0.17501576141383207,\n   0.5877695591500649,\n   -0.06264699708493802,\n   -0.2039006032557608,\n   -1.193152752838448,\n   -0.4222694151347171],\n  [-1.3562253990942323, 0.0010176328562240375, 0])]"
  },
  {
    "objectID": "posts/code/2023-06-18-chord.html#chord-graph",
    "href": "posts/code/2023-06-18-chord.html#chord-graph",
    "title": "👩‍💻Chord Graph",
    "section": "Chord Graph",
    "text": "Chord Graph\n이제 본격적으로 pose 데이터를 가지고 chord graph를 그려보겠습니다. pickle 데이터로 저장되어 있는 pose 데이터들 중 2000개의 데이터를 가지고 dataframe 객체로 만들어줍니다. 마지막으로 clustering 과정에서 구한 각 포즈 데이터가 속해있는 cluster의 ID를 데이터프레임의 열을 추가하여 정보를 추가해줍니다.\n\ndf_poses = pd.DataFrame(poses)\ndf_poses_2000 = df_poses[:2000]\ndf_poses_2000[\"cluster\"] = clusters\n\n\ndf_poses_2000.head(3)\n\n\n\n\n\n\n\n\nstore_height\nstore_roll_pitch\nstore_joints\nstore_links\ncluster\n\n\n\n\n0\n0.300289\n(2.150537560664197, -0.004430010187345012)\n[-0.5810368941034365, -1.206576076316329, -0.0...\n[(-0.17671623826026917, -0.23643925786018372, ...\n22\n\n\n1\n0.067273\n(-0.00028379763432160346, 2.8617441250464685e-05)\n[0.15361879275773346, -2.083050326944349, -1.1...\n[(0.14308467507362366, -0.12229745090007782, 0...\n77\n\n\n2\n0.091594\n(-0.21762631665841028, -1.16469456094937e-05)\n[0.1518267329947646, -1.165616939584498, -0.07...\n[(-0.08804446458816528, -0.04484516754746437, ...\n129\n\n\n\n\n\n\n\n항공편 예제에서도 살펴보았듯이 모든 cluster를 시각화하는 것은 의미가 없기 때문에 Top20 cluster에 속해있는 데이터들만 처리하기 위해서 데이터를 전처리하는 과정이 필요합니다. 우선 각 pose 데이터가 source(출발노드) 가 될수도 있고 target(도착노드) 이 될 수도 있기 때문에 data_id라는 변수를 통해 기준 데이터(pose A) 와 페어 데이터(pose B) 를 튜플로 묶어준 리스트를 생성합니다.\n\ndata_id = [(x+1, y+1) for x in range(2000) for y in range(2000)]\n\n각 기준 데이터와 페어 데이터에 대해서 각 데이터가 속해있는 클러스터 아이디를 확장해서 저장해줍니다. 데이터들을 확인하기 위해 index 998:1004범위에 있는 값들을 확인합니다.\n\n# 기준 데이터(pose A)\nid_list = [x[0] for x in data_id]\nprint(id_list[998:1004])\n\n# 페어 데이터(pose B)\npairs = [x[1] for x in data_id]\nprint(pairs[998:1004])\n\n# 기준 데이터의 클러스터 아이디를 확장 \nclusters_expand = [clusters[x//2000] for x in range(2000*2000)]\nprint(clusters_expand[998:1004])\n\n# 페어 데이터의 클러스터 아이디를 확장\npair_cluster = clusters * 2000\nprint(pair_cluster[998:1004])\n\nassert len(id_list)==len(clusters_expand) == len(pairs) == len(values) == len(pair_cluster) # values는 acc 값\n\n[1, 1, 1, 1, 1, 1]\n[999, 1000, 1001, 1002, 1003, 1004]\n[22, 22, 22, 22, 22, 22]\n[1, 103, 21, 40, 59, 61]\n\n\n다음으로 chord 그래프를 위한 데이터 프레임 객체 df_chord를 만들고 칼럼을 재정렬해줍니다.\n\ndf_chord = pd.DataFrame({\"id\":id_list,\n                         \"cluster\":clusters_expand,\n                         \"pair\":pairs,\n                         \"acc\":values,\n                         \"pair_cluster\":pair_cluster})\n\n# column 재정렬\ndf_chord = df_chord[[\"id\", \"cluster\", \"pair\", \"pair_cluster\", \"acc\"]]\n\n\ndf_chord[998:1004]\n\n\n\n\n\n\n\n\nid\ncluster\npair\npair_cluster\nacc\n\n\n\n\n998\n1\n22\n999\n1\n1.000000e-08\n\n\n999\n1\n22\n1000\n103\n1.000000e-08\n\n\n1000\n1\n22\n1001\n21\n1.000000e-08\n\n\n1001\n1\n22\n1002\n40\n1.000000e-08\n\n\n1002\n1\n22\n1003\n59\n1.000000e-08\n\n\n1003\n1\n22\n1004\n61\n8.589883e-01\n\n\n\n\n\n\n\n기준 데이터의 클러스터 기준으로 상위 20개의 클러스터에 속해 있는 데이터들만 남기기는 과정을 진행합니다. 클러스터의 아이디는 크기순 정렬이기 때문에 1~20까지의 클러스터 아이디만 남기면 상위 20개의 클러스터에 속한 포즈 데이터들만 남게 됩니다.\n\ndf_chord.drop(df_chord[(df_chord['cluster']&gt;20)].index, inplace=True)\n\n페어 데이터의 클러스터 기준으로도 상위 20개의 클러스터에 속해 있는 데이터들만 남기는 과정을 똑같이 진행합니다.\n\ndf_chord.drop(df_chord[(df_chord['pair_cluster']&gt;20)].index, inplace=True)\n\n\ndf_chord.head(5)\n\n\n\n\n\n\n\n\nid\ncluster\npair\npair_cluster\nacc\n\n\n\n\n8004\n5\n12\n5\n12\n1.000000e+00\n\n\n8006\n5\n12\n7\n18\n1.000000e-08\n\n\n8007\n5\n12\n8\n8\n1.000000e-08\n\n\n8011\n5\n12\n12\n13\n1.000000e-08\n\n\n8025\n5\n12\n26\n12\n9.080099e-01\n\n\n\n\n\n\n\nholoviews 패키지를 불러와서 chord graph를 그리기 위한 준비를 합니다. holoviews는 시각화 라이브러리 백엔드를 선택할 수 있는데 interaction이 가능한 bokeh 백엔드를 선택했습니다.\n\nimport holoviews as hv\nfrom holoviews import dim\nhv.extension('bokeh') # backend engine 선택\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n\n\n\n\n\n\n\n항공편 예제에서도 groupby를 이용해서 edge 수를 집계했듯이 pose 데이터셋에 대해서도 acc수를 cluster와 pair_cluster를 기준으로 집계해서 edge 정보를 정리해줍니다.\n\nedges = df_chord.groupby([\"cluster\", \"pair_cluster\"]).acc.count().reset_index()\n\n\nedges\n\n\n\n\n\n\n\n\ncluster\npair_cluster\nacc\n\n\n\n\n0\n1\n1\n1764\n\n\n1\n1\n2\n1764\n\n\n2\n1\n3\n1596\n\n\n3\n1\n4\n1302\n\n\n4\n1\n5\n1218\n\n\n...\n...\n...\n...\n\n\n395\n20\n16\n675\n\n\n396\n20\n17\n650\n\n\n397\n20\n18\n650\n\n\n398\n20\n19\n650\n\n\n399\n20\n20\n625\n\n\n\n\n400 rows × 3 columns\n\n\n\n다음으로 공항정보를 저장했듯이 pose데이터가 속해있는 cluster의 아이디를 각 centroid pose(cp_N) 이름으로 매칭해서 저장해줍니다. 각 노드(pose 데이터)가 속해있는 cluster_id를 기준으로 edge 데이터셋에서 cluster와 매칭되는 것을 알 수 있습니다.\n\n# node\nnodes = pd.DataFrame({\"cluster_id\":[x+1 for x in range(20)],\n                     \"name\":[\"cp_{}\".format(i+1) for i in range(20)]})\nnode_dataset = hv.Dataset(nodes, \"cluster_id\", \"name\" )\n\n\nnodes.head(3)\n\n\n\n\n\n\n\n\ncluster_id\nname\n\n\n\n\n0\n1\ncp_1\n\n\n1\n2\ncp_2\n\n\n2\n3\ncp_3\n\n\n\n\n\n\n\n\nchord = hv.Chord((edges, node_dataset),\n                 [\"cluster\", \"pair_cluster\"],\n                 ['acc'])\n\n\nchord\n\n\n\n\n\n  \n\n\n\n\n마찬가지로 옵션을 추가하여 chord diagram을 좀 더 보기 좋게 만들어서 export 해보겠습니다.\n\n%%opts Chord [height=600 width=600]\n\nchord.opts(\n    cmap='Category20',\n    labels='name',\n    edge_color=dim(\"cluster\").astype(str),\n    edge_alpha=0.7,\n    node_color=dim(\"cluster_id\").astype(str))\n\n\n\n\n\n  \n\n\n\n\n이렇게 해서 완성한 K-Acc Chord Diagram을 각 centroid pose와 함께 시각화를 하면 아래 그림과 같이 됩니다!\n\nConclusion\n그동안 데이터 시각화를 위한 코딩작업을 자주하지 않아서 논문에 있는 그림을 하나 따라하기까지 정말 오랜시간이 걸렸던 것 같습니다. 적절한 패키지를 서칭하는 것부터 시작해서 해당 패키지를 어떻게 사용해야 원하는 그림을 뽑을 수 있는지까지 한 과정마다 많은 고민과 연습이 필요했지만 마지막에 원하는 시각화 자료를 뽑을 수 있어서 뿌듯했던 것 같습니다. 데이터 시각화 과정이 연구를 하는 입장에서는 가장 마지막에 설득과 확인의 과정에 필요한 자료라서 소홀히 하기 쉬운데 논문의 결과를 더 빛낼 수 있는 중요한 과정이라는 것을 이번 기회에 또 한번 느낄 수 있었던 것 같습니다. 생소한 그래프 형식과 적용이 만만치는 않았지만 정말 의미있던 과정이었고 Chord diagram이 필요한 그 누군가에게 도움이 되었기를 바라며 이번 포스팅을 마치겠습니다.\nReference\n\nCHORD DIAGRAM\nChord diagram (information visualization)\nHoloviews - Chord\nHoloviews - Route Chord"
  },
  {
    "objectID": "posts/project/active_learning.html",
    "href": "posts/project/active_learning.html",
    "title": "Active Learning Algorithm for Object Detection and Segmentation",
    "section": "",
    "text": "tbd"
  },
  {
    "objectID": "posts/project/pulse_rl.html",
    "href": "posts/project/pulse_rl.html",
    "title": "Deep Reinforcement learning for DME Pulse Design",
    "section": "",
    "text": "tbd"
  },
  {
    "objectID": "posts/project/pick-gpt.html",
    "href": "posts/project/pick-gpt.html",
    "title": "Pick-GPT",
    "section": "",
    "text": "tbd"
  },
  {
    "objectID": "posts/project/get_off_prediction.html",
    "href": "posts/project/get_off_prediction.html",
    "title": "Self-driving Public Mobility Get-off Safety System",
    "section": "",
    "text": "https://github.com/curieuxjy/Safe_Goodbye"
  },
  {
    "objectID": "posts/storage/2021-01-02-GNN-materials.html",
    "href": "posts/storage/2021-01-02-GNN-materials.html",
    "title": "🧩GNN Materials",
    "section": "",
    "text": "GNN에 관심을 가지게 된 계기는 RoboGrammar라는 paper였다. 예전부터 하고 싶었던 Robot design 아이디어를 GNN을 가지고 실현시킨 것이 너무 신기해서 공부해보고 싶었다. 이번 포스팅에서는 GNN과 첫만남인 만큼 공부할 자료들을 정리해보려 한다."
  },
  {
    "objectID": "posts/storage/2021-01-02-GNN-materials.html#materials",
    "href": "posts/storage/2021-01-02-GNN-materials.html#materials",
    "title": "🧩GNN Materials",
    "section": "Materials",
    "text": "Materials\n\nTobigs Graph Study\nCS224W: Machine Learning with Graphs / Videos\nGraph Neural Networks - Penn Engineering\nTF Graph Neural Network Samples\nGraph Neural Networks in TF2\nGraph Representation Learning(Pytorch)\nA Gentle Introduction to Graph Neural Networks (Basics, DeepWalk, and GraphSage)\nInvariant Graph Networks : invariance, equivariance, k-WL GNN 관련 주제\nEnd-to-End, Transferable Deep RL for Graph Optimization : RL + GNN\n\n\nTutorials & Workshops\n\nWWW 18 Tutorial : Representation Learning on Networks\nCIKM 19 Tutorial : Recent Developments of Deep Heterogeneous Information Network Analysis\nWSDM 19 Tutorial : Learning and Reasoning on Graph for Recommendation\nKDD 19 Tutorial : Learning From Networks\nAAAI 20 Tutorial : Graph Neural Networks: Models and Applications\nICML2020 GNN Workshop GRL+\nWWW 20 Hands on Tutorial - Videos\nGraph Neural Networks for Natural Language Processing / PPT\nTutorial on Spectral and Graph ConvNets\n\n\n\nPapers & Survey\n\nGraph Neural Networks: Taxonomy, Advances and Trends\nA Comprehensive Survey on Graph Neural Networks\nDirectional Graph Networks\nGNN KR Paper List\nGraph Meta Learning via Local Sub-graphs\n\nMeta-GNN: On Few-shot Node Classification in Graph Meta-learning\nFew-shot Learning with Graph Neural Networks\nLearning to Propagate for Graph Meta-Learning\n\nSelf-supervised Training of Graph Convolutional Networks\nXGNN: Towards Model-Level Explanations of Graph Neural Networks\nL2-GCN: Layer-Wise and Learned Efficient Training of Graph Convolutional Networks, 2020 CVPR\n\n\n\nVideos\n\nAn Introduction to Graph Neural Networks: Models and Applications\n\n\n\nGraph Convolutional Networks using only NumPy\n\n\n\nGeometric Deep Learning on Graphs and Manifolds\n\n\n\nGraph Nets: The Next Generation\n\nLink\n\n\n\n\nRecent Developments of Graph Network Architectures\n\nSlide\n2019-2020에 발표된 GNN 방법론들을 정리\nGNN의 expressiveness, 그중에서도 invariance and equvariance\n\n\n\n\nDeep learning on graphs: successes, challenges, and next steps\n\n\n\nGraph Representation Learning for Algorithmic Reasoning\n\nSlide\n\n\n\n\nHow Uber uses Graph Neural Networks to recommend you food\n\nPost"
  },
  {
    "objectID": "posts/paper/2024-03-17-vcgs.html",
    "href": "posts/paper/2024-03-17-vcgs.html",
    "title": "📃VCGS 리뷰",
    "section": "",
    "text": "이번 포스팅은 Variational Constrained Grasp Sample 논문을 읽고 정리한 내용입니다. 해당 논문은 IROS 2023 학회에 Accept된 논문으로, 특정 대상 영역에 대한 제약을 가진 6자유도(DoF) Grasp을 샘플링하기 위한 새로운 생성적 그리핑 샘플링 네트워크, VCGS를 소개합니다. 뿐만 아니라 1,400만 개 이상의 훈련 샘플을 포함하는 새로운 데이터셋 CONG를 구축한 내용을 발표했습니다. 제안된 VCGS가 시뮬레이션 및 실제 테스트에서 비교 모델인 GraspNet보다 10-15% 높은 그리핑 성공률을 보이며, 2-3배 더 효율적인 것을 보여준 논문입니다."
  },
  {
    "objectID": "posts/paper/2024-03-17-vcgs.html#grasp-sampler",
    "href": "posts/paper/2024-03-17-vcgs.html#grasp-sampler",
    "title": "📃VCGS 리뷰",
    "section": "3.1 Grasp Sampler",
    "text": "3.1 Grasp Sampler\nVCGS의 Grasp Sampler의 기본적인 틀은 Conditional Variational Autoencoder (CVAE) 구조를 차용해서 아래와 같이 만들었습니다. Grasp pose를 다양하게 Sampling하기 위해 Encoder와 Decoder를 VAE 구조를 차용하여 Gaussian Prior Distribution을 이용해서 가능한 다양한 Grasp pose를 생성할 수 있도록 설계했습니다.\n\n\n\nC-VAE 구조\n\n\n\n\n\nLoss Function"
  },
  {
    "objectID": "posts/paper/2024-03-17-vcgs.html#grasp-evaluator",
    "href": "posts/paper/2024-03-17-vcgs.html#grasp-evaluator",
    "title": "📃VCGS 리뷰",
    "section": "3.2 Grasp Evaluator",
    "text": "3.2 Grasp Evaluator\n학습동안에 좋은 Grasp data만 학습하는 Encoder가 더 다양한 Grasp data를 경험할 수 있도록 Evaluator Network를 추가하여 Bad Grasp에 대한 경험도 할 수 있도록 만들었습니다.\n\n\n\nInput data 형태와 Evaluator Network"
  },
  {
    "objectID": "posts/paper/2024-03-17-vcgs.html#cong-dataset",
    "href": "posts/paper/2024-03-17-vcgs.html#cong-dataset",
    "title": "📃VCGS 리뷰",
    "section": "3.3 CONG Dataset",
    "text": "3.3 CONG Dataset\n\n\n\nCONG Dataset 구축과정\n\n\n구성 요소\n\nO: object point cloud\nG*: target area A에서 랜덤하게 샘플링된 successful grasp\n\n데이터셋 구축 과정\n\nobject를 원점에 랜덤한 orientation으로 놓고 O[N x 3] rendering\nO에서 query point I[K x 3]를 샘플링(K &lt;&lt; N) - Farthest Point Sampling 사용\n각 query point xi(∈I)에 대해서 반경 ri(~U[0, R]) 이웃한 point Ai들을 모두 찾음\n\n이때 R은 mesh bounding box의 대각선 길이\n\n[grasp center point]와 [Ai의 어떤 점]이라도 최대 d인 모든 G를 찾아냄\n\n\n\n\nmesh 데이터에서 grasp data를 추출하는 과정"
  },
  {
    "objectID": "posts/paper/2024-03-17-vcgs.html#a.-simulated-robotic-grasping",
    "href": "posts/paper/2024-03-17-vcgs.html#a.-simulated-robotic-grasping",
    "title": "📃VCGS 리뷰",
    "section": "4.1 A. Simulated Robotic Grasping",
    "text": "4.1 A. Simulated Robotic Grasping\n\nbest grasp, NOT the best reachable\ngripper와 object 둘 다 free-floating 상황\nIsaacGym simulator 사용\n\nAcronym dataset에서 123개의 random object\n물체의 observation data로는 depth sensor 사용\n\n시뮬레이터에서 2개 실험 진행\n\nUnconstrained sampling: target area 없이 그냥 grasp을 샘플링. A=O\nConstrained sampling: target area에서만 grasp 생성\n\n비교군\n\nGraspNet: SOTA\nGraspNetTaI: Target as Input. target area만 grasp sampling network에 넣어준 모델\n\n\n\n\n\n13\n\n\n\nVCGS는 GraspNet보다 3배 이상의 Ratio of grasps kept %를 보여줌\n\n네트워크 입력으로 Constrained grasp sampling을 넣어주는 것의 이점에 대한 증거\n\nGraspNetTaI는 GraspNet보다 Success Rate가 낮음\n\n물체의 전체 정보(global)를 사용하는 것이 특정한 target area에 대한 정보(local)를 사용하는 것보다 좋음을 알 수 있음\n\nGraspNet은 Success Rate가 # of grasps sampled에 영향을 받음\n\n만약 Unconstrained 경우라면 더 많은 sampling이 필요하다고 볼 수 있음\nRGK가 #GS에 영향을 받지 않은 결과를 보고도 확인할 수 있는 가설임\n\n\n\n\n\n14\n\n\n\n\n\n15\n\n\n마지막으로 해당 논문의 발표영상을 마지막으로 이번 포스팅을 마무리하도록 하겠습니다."
  },
  {
    "objectID": "posts/paper/2022-10-16-recovery.html",
    "href": "posts/paper/2022-10-16-recovery.html",
    "title": "📃Robust Recovery Controller 리뷰",
    "section": "",
    "text": "이번 포스트는 4족보행 로봇이 전복되었을 때 다시 정상적으로 보행하기 위해 자세를 회복하는 모션 제어(이하 Recovery 혹은 Reset task라고 지칭)을 강화학습 방법으로 해결하고자한 Robust Recovery Controller for a Quadrupedal Robot using Deep Reinforcement Learning라는 논문에 대한 리뷰입니다."
  },
  {
    "objectID": "posts/paper/2022-10-16-recovery.html#mpc-vs-rl",
    "href": "posts/paper/2022-10-16-recovery.html#mpc-vs-rl",
    "title": "📃Robust Recovery Controller 리뷰",
    "section": "MPC vs RL",
    "text": "MPC vs RL\n제어(Control) 분야에서 현재 크게 양대 방법론으로 MPC(Model Predictive Control)과 RL(Reinforcement Learning) 방법론이 주목을 받고 있습니다. 이분법적으로 두 방법이 확연하게 나누어지거나 우열이 가려지는 것이 아니기 때문에 두 방법론 간의 공통점, 차이점등을 살펴보며 시작해보려고 합니다.\n아래 그림의 Optimal Control(최적제어), Operations Research(경영과학), Reinforcement Learning(강화학습)을 각각 하나의 집합으로 표시하고 각 관계를 살펴보면 기존의 제어 방식의 발전 흐름을 이어온 MPC는 Optimal Control에 포함되어 있다고 볼 수 있습니다. Operations Research은 좀 더 넓은 범위의 의사 결정 모델들을 연구하는 분야로 주로 산업공학에서 배우는 부분이며 의사결정에 도움을 받기위해 수학적 모델, 통계학, 알고리즘들을 이용하는 연구분야 입니다. 따라서 Optimal Control, Operations Research와 오늘의 주제 방법론인 Reinforcement Learning과의 관계를 살펴보자면 수학적 모델링을 기반으로 한 Model-based Methods가 Optimal Control과 Reinforcement Learning의 교집합 부분으로 볼 수 있고 최근 강화학습 연구들도 Model-based RL으로 연구들이 많이 이루어지고 있습니다. Operations Research와 Reinforcement Learning의 교집합으로는 Large action-space Method로 볼 수 있는데, 기존의 아타리 게임과 같이 간단한 방향 조작키와 같은 action의 선택지가 적은 경우에 비해 의사결정을 내려야하는 상황이 복잡할 수록 즉, 문제가 더 어렵고 복잡할 수록 action의 선택지가 많아지기 때문에 action space가 더 커지는 방향으로 연구가 많이 이루어지고 있다고 볼 수 있습니다.\n그럼 좀 더 자세히 MPC과 RL을 비교해보면, MPC는 그림과 같이 마치 어둠속에서 손전등을 키고 빛을 따라 하나의 길을 보듯이, 하나의 최적의 trajectory를 찾아 그 trajectory의 1 step을 실행시킵니다. 수학적으로 모델링한 최적식의 해를 구해서 계산과 실행을 반복하며 제어를 하는 것 입니다. 반면 RL은 search space를 MDP(Markov Decision Process)를 이용하여 정의하고 학습을 하며 시점 t에서의 최적의 action을 할 수 있도록 합니다. 두가지 방법 모두 Advanced 시킬 부분이 많이 남아있고 MPC는 non-linear 방향으로 나아가고 있으며 RL은 value-based나 policy-based 기반의 방법들과 함께 최근에는 인공신경망의 gradient기법을 이용하여 발전해나가고 있습니다. MPC와 RL의 비교는 최근 ICRA와 같은 로봇 제어 관련 학회들에서는 관심이 많은 주제이며 Michiel van de Panne (UBC) 교수님의 발표에서 좀 더 자세히 확인하실 수 있습니다."
  },
  {
    "objectID": "posts/paper/2022-10-16-recovery.html#locomotion-vs-recovery",
    "href": "posts/paper/2022-10-16-recovery.html#locomotion-vs-recovery",
    "title": "📃Robust Recovery Controller 리뷰",
    "section": "Locomotion VS Recovery",
    "text": "Locomotion VS Recovery\n이런 강화학습 방법론으로 4족보행의 Recovery Task를 풀기 위해서는 Recovery task에 대한 특성을 좀 더 살펴볼 필요가 있습니다. 기본적으로 족형(Legged) 로봇들은 안정적인 보행(Locomotion)을 기반으로 여러가지 task를 하는데에 초점이 맞춰져 있습니다. 예를 들면 보행로봇의 위에 Manipulator를 달아서 원하는 위치로 보행을 한 뒤 Manipulator로 컵을 집는 일과 같은 task를 수행하는 것입니다. 하지만 4족보행 로봇을 사용하기 위해 전제된 안정적인 보행이 실제로 로봇이 운용되는 환경에서는 여러가지 예측할 수 없는 변수들로 인해 만족될 수 없는 조건일 수 있으며 따라서 로봇이 넘어질 수 밖에 없다면 다시 보행을 할 수 있는 정상적인 상태로 돌아오기 위한 능력도 갖추고 있어야 합니다. 바로 이를 위한 로봇의 기능을 Recovery라고 할 수 있고 Recovery를 Locomotion task와 비교하여 특징 몇가지를 살펴보겠습니다.\n\n우선 Recovery는 앞서 이야기한 것과 같이 Locomotion이 메인이 되는 task인데 반해, 로봇의 운용 life cycle을 위한 support task라고 할 수 있습니다. 로봇이 적용되는 현장에서 메인 task들을 끊임없이 수행하기 위해 흐름을 유지시켜주는 장치라고 볼 수 있습니다. 다음으로 Locomotion은 보행을 할 때 로봇 다리의 움직임이 주기(period)를 가지는 Cyclic한 모션을 하는 것에 반해 Recovery는 넘어진(Fall) 자세로부터 다시 걸을 수 있는 서있는(Stand) 자세로 가기 위한 모션을 취해야 하기 때문에 Non-cyclic하고 case-by-case로 다양한 회복 모션들을 할 수 있어야 합니다. 따라서 Locomotion은 환경에 따라(지면의 마찰력이나 외력 등) 보행의 난이도가 달라지긴 하지만 로봇의 joint의 동작 범위, 즉 action의 search space로 생각했을 때 Recovery에 비해 좀 더 좁은 search space를 가지고 있다고 할 수 있습니다. 마지막으로 Locomotion을 수행할 때 참고할 수 있는 동물이나 사람의 모션 데이터들이 있지만 Recovery 같은 경우 참고할 수 있는 모션 데이터들이 많이 없고 일어나는 방법에 대해 메뉴얼 같이 각각의 단계를 명시하기 까다롭다는 점이 있습니다."
  },
  {
    "objectID": "posts/paper/2022-10-16-recovery.html#overview",
    "href": "posts/paper/2022-10-16-recovery.html#overview",
    "title": "📃Robust Recovery Controller 리뷰",
    "section": "Overview",
    "text": "Overview\n논문에서 제시한 전체적인 Control System의 모습은 아래 그림과 같습니다. 강화학습 알고리즘으로는 TRPO(Trust Region Policy Optimization algorithm)와 GAE(Generalized Advantage Estimator)를 사용했으며 제시된 방법에서는 크게 3가지 특징들이 있습니다.\n\nControl Task Decomposed : Recovery하는 task를 3개로 Behavior들로 나누어서 각각의 Behavior를 수행하는 Control Policy를 학습\nHierarchical Structure : 여러개의 Behavior policies를 조율할 수 있는 상위계층의 Behavior Selector를 만들어서 계층적인 구조를 사용\nHeight Estimator : 로봇을 실제 운용할 때 필요한 상태 정보인 Height 값의 부정확한 정보를 보완하기 위해 Neural Network 사용(Regression model)"
  },
  {
    "objectID": "posts/paper/2022-10-16-recovery.html#behavior-policies",
    "href": "posts/paper/2022-10-16-recovery.html#behavior-policies",
    "title": "📃Robust Recovery Controller 리뷰",
    "section": "Behavior Policies",
    "text": "Behavior Policies\nRecovery task를 여러개의 Behavior들로 나누어서 총 3개의 각각의 task, Self-Righting, Standing up, Locomotion가 있고 각 behavior을 담당하는 policy가 있습니다.\n\nSelf-Righting은 임의의 로봇의 넘어진 자세에서 로봇의 몸체(base)가 똑바로(뒤집어져 있거나 옆으로 돌아가 있지 않은 상태) 되어 있고 4개의 발이 지면에 닿아 있는 자세로 움직이는 것을 말합니다. Standing up은 Self-righting에서 만든 자세에서 4개의 다리들을 이용하여 일어선 자세로 만드는 것을 말합니다. 마지막으로 Locmotion은 Controller에서 주는 command를 기반으로 보행을 하는 것을 말하며 이때 command로는 forward velocity, lateral velocity, turing rate(yaw 방향)을 주어 움직이게 됩니다.\n\n(1)Self-righting behavior\nSelf-righting을 학습하기 위해 policy에 들어가는 data(state 정보)로는 아래의 표에서 볼 수 있듯이 총 6개의 data가 있습니다. 그 중 e_g는 몸체 base의 z 방향으로의 단위 벡터로 몸체의 upright를 판단하기 위한 정보로 볼 수 있습니다.\n\n보통 강화학습에서는 reward를 최대화 하는 방향으로 학습(최적화)이 일어나지만 reward의 반대 개념인 cost의 값이 최소화하는 방향으로 학습을 시켰습니다. 아래의 식처럼 여러개의 cost term들이 있지만 그 중 self-righting에서 특징적인 cost term은 orientation cost term 입니다. 이 cost term은 c_o=\\left\\|[0,0,-1]^T-e_g\\right\\|로 계산되는데 이는 위에서 설명한 대로 policy network가 로봇 몸체의 base의 방향을 uprighting 하도록 학습되게 하기 위한 term이라고 볼 수 있습니다.\n\n(2)Standing up behavior\nStanding up은 이전의 Self-righting의 state 정보를 포함하고 더하여 Base linear velocity 정보까지 포함하여 policy의 input으로 들어가게 됩니다. Self-righting과 Standing up policy 간의 state 정보 포함 관계 뿐만 아니라 이후 소개할 Locomotion policy, Behavor Selector의 state 정보의 집합관계를 보면 다른 policy에 들어가는 정보를 포함하고 추가적인 data를 더하여서 Self-righting → Standing up → Locomotion → Behavior Selector 순으로 state space가 점점 더 커져가는 것을 확인할 수 있습니다.\n\nStaning up behavior policy가 학습해야할 cost 최소화 식은 아래와 같이 여러 cost term들이 있지만 그중 height cost term이 특징적인 cost라고 볼 수 있으며 ANYmal 로봇이 서있을 때의 지면으로부터 몸체(base frame)까지의 거리, height이 0.35m보다 작을시에는 1, 아니면 0으로 계산합니다.\n\n(3)Locomotion behavior\nLocomotion task policy에는 input으로 command까지 들어가게 되면서 cost식에는 command를 잘 수행하는지 판단하도록 하는 angular velocity, linear velocity, foot clearance, foot slippage cost가 들어가는 것을 확인할 수 있습니다.\n\n지금까지 policy network들의 input와 objective(=cost 최소화)에 대해서 이야기 했지만 결과적으로 network가 어떤 값들을 output 하는지, 그리고 실제로 그 output으로 어떻게 로봇을 움직이는지에 대해서는 아직 설명하지 않았습니다. 로봇을 움직이기 위해서는 timestep마다 로봇의 각 모터가 움직여야 하는 desired joint position을 구해서 해당 position으로 모터를 돌려주면 됩니다.\n\n각 Behavior policy에서 나오는 output은 o_t이며 이는 로봇을 구동시키는 12개의 joint motor에 대응하는 실수 벡터입니다. 대응이라고 표현한 이유는 해당 실수값을 바로 joint의 desired position으로 사용하는 것이 아니라 좀 더 빠른 학습 수렴을 위해 desired joint position을 구하는 식을 거쳐 계산된 값을 사용하기 때문입니다. 우선 Self-righting과 Standing up task에서는 네트워크에서 나온 값 o_t를 현재 각 모터의 position인 \\phi_t에 더해주어서 최종 desired joint position인 \\phi_d 값으로 제어합니다. Locomotion에서는 현재의 joint position 대신, 로봇이 서있는 자세인 nominal joint configuration \\phi_n에 o_t을 더해주어 최종 desired joint position인 \\phi_d을 사용합니다.(o_t에 곱해지는 k는 하이퍼파라미터처럼 찾아야 하는 상수값입니다.)"
  },
  {
    "objectID": "posts/paper/2022-10-16-recovery.html#height-estimator",
    "href": "posts/paper/2022-10-16-recovery.html#height-estimator",
    "title": "📃Robust Recovery Controller 리뷰",
    "section": "Height Estimator",
    "text": "Height Estimator\ndata-driven RL에서 policy에 들어가는 data의 quality는 매우 중요합니다. 따라서 실제 작동하는 로봇의 state를 추정하는 State estimation 또한 로봇에 RL을 적용하기 위해서 뗄레야 뗄수 없는 중요한 부분이라고 할 수 있습니다. 로봇에 모션 캡쳐와 같은 센서를 부착하지 않는한 실제 로봇의 base height값을 잘 알 수 없고 로봇이 정상적으로 보행할 때는 TSIF(Two State Implicit Filter)와 같은 State Estimation 기법을 통해 어느정도 추정할 수 있지만 넘어져서 base가 거의 바닥과 가까울 경우 추정값이 매우 불안정하게 됩니다. 따라서 해당 논문에서는 Regression Neural Network를 통해 height를 넘어진 상태에서도 잘 추정할 수 있도록 했습니다.\n\nHeight Estimator Network가 input으로 받는 data는 아래의 표와 같습니다. Output으로는 body의 IMU 값과 12개 joint들의 position을 출력하여 해당 값들을 가지고 forward kinematics를 이용하여 구한 height 값을 네트워크에서 추정한 값으로 사용합니다. 이 신경망은 강화학습으로 학습을 하는 것이 아니라 지도학습 방법으로 true 값을 맞춰가는 과정을 통해 학습하게 되는데 이떄 true data는 시뮬레이션 상에서는 쉽게 구할 수 있고 Regression model의 loss 값은 \\sum_{j=0}^K\\left\\|h_j-h_\\psi\\left(s_j\\right)\\right\\|^2으로 계산됩니다. (j: joint index, s_j: joint state, \\psi: network parameter)"
  },
  {
    "objectID": "posts/paper/2022-10-16-recovery.html#behavior-selector",
    "href": "posts/paper/2022-10-16-recovery.html#behavior-selector",
    "title": "📃Robust Recovery Controller 리뷰",
    "section": "Behavior Selector",
    "text": "Behavior Selector\n여러개의 behavior policy들을 어떻게 조율할 것인지는 상위 계층에 Behavior Selector를 NN을 이용하여 만들어서 학습시킵니다.\n\n로봇의 상황에 맞추어서 적절한 behavoir를 하도록 behavior selector는 categorical distribution을 학습하게 됩니다. Behavior selector와 앞서 설명한 Height estimator는 3개의 behavior policy들이 다 학습이 된 후에 아래 그림의 오른쪽에 보이는 Algorithm1의 흐름에 따라 학습되게 됩니다. Behavior selelctor의 state 나cost는 locomotion과 매우 유사하고 해당 포스트의 appendix에 표로 정리되어 있으며 cost식 같은 경우에도 모터의 power efficiency를 위한 term 정도 추가된 것이기 때문에 해당 cost 식이 궁금하신분은 원문 논문에서 확인하실 수 있습니다."
  },
  {
    "objectID": "posts/paper/2022-10-16-recovery.html#simulating-anymal",
    "href": "posts/paper/2022-10-16-recovery.html#simulating-anymal",
    "title": "📃Robust Recovery Controller 리뷰",
    "section": "Simulating ANYmal",
    "text": "Simulating ANYmal\n로봇에 강화학습을 적용할 때 큰 이슈들 중 하나는 Sim-to-Real입니다. 시뮬레이션으로 실제 환경을 단순화하고 모사한 것이기 떄문에 시뮬레이션에서 잘 학습이되고 잘 동작하더라도 실제 로봇을 deploy했을때 잘 작동하지 않는 문제가 생깁니다. 따라서 실제 환경에서도 로봇이 경험하게 되는 noise들을 시뮬레이션에도 random하게 적용시켜 최대한 실제 상황과도 유사하게 만든 환경에서 학습을 하게 됩니다.\n해당 논문에서도 Link length, Intertial property, Link mass, CoM(Center of Mass), Collision geometry, Coefficient of friction 등과 같이 물리적인 값들을 아래 표에서 볼 수 있듯이 일정 범위에서 random하게 값을 넣어주었고 policy의 input data가 되는 observation 값들에도 noise 값을 추가하여 Sim-to-Real 문제를 해결하였습니다. 이외에도 ANYmal 로봇 플랫폼에서 사용하는 SEA motor에 스프링과 같은 기계적 요소들의 변수로 인한 제어 이슈도 해결하기 위해 Actuator Network를 학습시켜 이를 해결하였습니다.\n\n(위 요약에서 언급했던 대로 사실상 Actuator Network나 Height Estimator까지 NN이 추가되므로 4개가 아닌 총 6개의 NN이 사용되었음을 알 수 있습니다.)"
  },
  {
    "objectID": "posts/paper/2022-10-16-recovery.html#the-success-of-recovery",
    "href": "posts/paper/2022-10-16-recovery.html#the-success-of-recovery",
    "title": "📃Robust Recovery Controller 리뷰",
    "section": "The success of recovery",
    "text": "The success of recovery\nRecovery 실험은 총 2가지로 진행되었는데 첫번째는 지면에 로봇을 임의의 position으로 넘어진 상황을 연출한 후 5초 이내로 일어날 수 있는지를 확인했고 두번째로는 로봇이 걷고 있을 때 발로 쳐서 넘어뜨린후 로봇이 다시 자연스럽게(각 behavior들 간의 switching이 자연스럽게) 일어나는지를 확인했습니다. 각각의 실험 모두 50번 이상씩 진행했으며이때 약 100번중 97번을 성공하여 97% 성공률을 보였습니다. (실패한 케이스들의 경우에는 joint의 position이 2\\pi를 넘어가는 값으로 나올 때 잘 작동하지 않았다고 합니다.)"
  },
  {
    "objectID": "posts/paper/2022-10-16-recovery.html#the-effectiveness-of-height-estimator",
    "href": "posts/paper/2022-10-16-recovery.html#the-effectiveness-of-height-estimator",
    "title": "📃Robust Recovery Controller 리뷰",
    "section": "The effectiveness of Height Estimator",
    "text": "The effectiveness of Height Estimator\n기존의 State Estimator TSIF(Two State Implicit Filter)만을 사용했을 때는 로봇이 바닥에 넘어져 있을 경우 error 값이 매우컸지마나 Neural Network를 통해 보정했을 때 오차가 1cm 미만으로 떨어지는 것을 확인할 수 있었습니다. 오른쪽의 height 그래프는 맨 아래 캡쳐되어있는 로봇의 일련의 모션과정 중에 height를 그래프로 plotting한 것인데 simulation(초록색)이 true값이며, TSIF만을 사용했을 때(파란색)는 로봇이 넘어져있을 때 오차가 큰데 반해 neural network(주황색)은 simulation data와 거의 같음을 확인할 수 있습니다."
  },
  {
    "objectID": "posts/paper/2022-10-16-recovery.html#the-competitiveness-of-beahavior-selector",
    "href": "posts/paper/2022-10-16-recovery.html#the-competitiveness-of-beahavior-selector",
    "title": "📃Robust Recovery Controller 리뷰",
    "section": "The competitiveness of Beahavior Selector",
    "text": "The competitiveness of Beahavior Selector\n기존의 제어방법들에서는 여러가지 mode를 조율하기 위해서 FSM(Finite State Machine)을 많이 사용합니다. 특정 조건을 if에 넣어주어서 각 mode를 transition하는 기법인데 논문에서도 강화학습으로 학습한 Behavior Selector의 비교를 위해 기존의 FSM 방식을 활용하여 State Machine을 만들어서 비교했습니다. FSM 방식은 여전히 corner case들이 Behavior Selector에 비해 많이 존재했고 각 behavior들 간의 전환도 자연스럽지 않았습니다.\n\nReview\n\n아직 minor 한 주제인 Recovery task에 대해 집중적으로 잘 분석하고 성과도 확실히 보여준 논문이라고 생각합니다. Control system을 working하게 하기 위해 각 파트들을 어떻게 설계하고 학습해야할 지 많은 고민을 했다는 것을 느낄 수 있었습니다. 여전히 flat ground에서만 진행된 연구이기에 slope가 있는 환경이나 다른 object가 있는 좀 더 실제 상황과 비슷한 상황에 대한 recovery가 해결되기 위해서는 연구되어야 할 부분이 충분히 많은 것 같습니다.\n\n\n\nAppendix\n\n\n\nReference\n\norginal paper : https://arxiv.org/abs/1901.07517\nhttps://youtu.be/veXcohbFxKQ"
  },
  {
    "objectID": "posts/paper/2022-08-07-gn-block.html",
    "href": "posts/paper/2022-08-07-gn-block.html",
    "title": "📃GN-Block 리뷰",
    "section": "",
    "text": "이번 post는 Graph Networks as Learnable Physics Engines for Inference and Control 라는 논문을 읽고 리뷰한 내용입니다."
  },
  {
    "objectID": "posts/paper/2022-08-07-gn-block.html#graph-representation-of-a-physical-system",
    "href": "posts/paper/2022-08-07-gn-block.html#graph-representation-of-a-physical-system",
    "title": "📃GN-Block 리뷰",
    "section": "Graph representation of a physical system",
    "text": "Graph representation of a physical system\n물리시스템을 어떻게 그래프로 나타낼 수 있는지 몇가지 용어와 수식들을 정리해보겠습니다.\n\n물리 시스템의 body는 그래프의 node로 표현합니다.\n물리 시스템의 joint는 그래프의 edge로 표현합니다.\n물리 시스템의 global한 속성은 global feature로 표현합니다.\n\n아래 사진에서 보이는 half-cheetah에서 직관적으로 어떻게 그래프가 그려질 수 있는지 알 수 있고 이 그래프를 G로 나타낼 수 있습니다.\n\n\n\n\n\n앞서 설명한 부분을 수식으로 나타내면 다음과 같습니다.\n\nG=\\left(\\mathbf{g},\\left\\{\\mathbf{n}_{i}\\right\\}_{i=1 \\cdots N_{n}},\\left\\{\\mathbf{e}_{j}, s_{j}, r_{j}\\right\\}_{j=1 \\cdots N_{e}}\\right)\n\n\ng : global features 시스템의 중력이나 time step과 같은 속성을 나타내는 벡터입니다.\n\\mathbf{n}_{i} : node features를 나타내는 벡터입니다.\n\\mathbf{e}_{j} : edge features를 나타내는 벡터입니다.\ns_{j} : 이 edge를 통해서 message를 보내는 sender nodes의 인덱스입니다.\nr_{j} : 이 edge를 통해서 message를 받는 receiver nodes의 인덱스입니다."
  },
  {
    "objectID": "posts/paper/2022-08-07-gn-block.html#static-dynamic-properties",
    "href": "posts/paper/2022-08-07-gn-block.html#static-dynamic-properties",
    "title": "📃GN-Block 리뷰",
    "section": "Static & Dynamic properties",
    "text": "Static & Dynamic properties\n여기서 static graph G_s와 dynamic graph G_d 라는 그래프는 2가지 종류가 있습니다. 이 2개의 그래프는 각각 시스템의 속성이 시간에 따라 변화하는지(dynamic/time-variant) 안하는지(static/time-invaritant)에 따라 그래프를 구성하는 정보의 종류가 다릅니다.(자세한 정보는 Appendix G section에서 Mujoco 기반의 어떤 정보로 각 그래프를 구성했는지 나와있습니다.)\n\nA static graph G_s: 시스템의 static한 정보를 가지고 있는 그래프\n\nglobal parameters: the time step, viscosity, gravity, etc\nbody/node parameters: mass, inertia tensor, etc.\njoint/edge parameters: joint type과 properties, motor type and properties, etc\n\nA dynamic graph G_d: 시스템의 일시적인 state 정보를 가지고 있는 그래프\n\nbody/node: 3D Cartesian position, 4D quaternion orientation, 3D linear velocity, 3D angular velocity\njoint/edge: joint에 적용된 action들의 크기"
  },
  {
    "objectID": "posts/paper/2022-08-07-gn-block.html#graph-networks",
    "href": "posts/paper/2022-08-07-gn-block.html#graph-networks",
    "title": "📃GN-Block 리뷰",
    "section": "Graph networks",
    "text": "Graph networks\n\ngraph2graph 모듈을 활용하여 인풋을 그래프로 받고 아웃풋도 그래프로 받는 모델입니다. 따라서 아웃풋의 그래프는 인풋 그래프와 다른 edge, node, global features를 가지게 됩니다.\n\n본 논문의 핵심 아이디어인 GN 블록의 구조에 대해 알아보겠습니다. - A core GN block\n\n\n\n\n\n- 3개의 sub function, MLP로 이루어져 있습니다.\n    - edge-wise $f_e$ : 모든 edge들에 대한 update를 진행합니다.\n    - node-wise $f_n$ : 모든 node들에 대한 update를 진행합니다.\n    - global $f_g$ : 마지막으로 global feature들을 update 합니다.\n하나의 feedforward GN pass는 그래프 상에서 message-passing 단계의 한 스텝으로 간주할 수 있습니다. 이러한 GN-block 내에서의 알고리즘은 아래와 같습니다."
  },
  {
    "objectID": "posts/paper/2022-08-07-gn-block.html#forward-models",
    "href": "posts/paper/2022-08-07-gn-block.html#forward-models",
    "title": "📃GN-Block 리뷰",
    "section": "Forward models",
    "text": "Forward models\nForward model의 목적은 현재 정보를 기반으로 다음 step의 상태를 예측(prediction)하는 것입니다. (이는 영어 단어의 비슷한 의미때문에 다음에 나오는 inference model의 목적과 많이 혼동될 수 있으니 잘 정의하고 넘어가는 것이 좋습니다.) forward model은 RNN(GRU)를 도입했는지 여부에 따라 2가지 타입이 있습니다.\nType1. GNN feed-forward\n\n\n\n\n\n가장 간단한 GNN feed-forward 모델입니다. 그래프는 처음에 GN_1을 거쳐 latent graph인 G'이 됩니다. 그리고 다음 GN_2의 인풋으로는 GN_1을 거치긴 전의 그래프였던 G와 G'를 concatenate를 해서 넣어주게 됩니다. 저자들은 이렇게 디자인한 이유로, 그래프의 모든 노드들과 엣지들이 모두 communicate하게 하기 위함이라고 이야기합니다. 이렇게 GN_1, GN_2를 거쳐 최종적으로 나오는 G^*의 node feature들이 각 body의 상태 prediction 값이 되는 것 입니다.\nType2. RNN+GNN\n\n\n\n\n\n다음으로 앞서 기본이 되는 모델에 G-GRU를 추가한 타입니다. Type 1과 비슷하게 skip connection, latent graph를 모두 사용하는데 GN block의 GRU 버젼인 G-GRU가 들어가면서 G_h라는 RNN에서 hidden vector와 같은 개념의 hidden graph가 추가된 것입니다. 모든 edge, node, global feature들에 대해 각각 RNN이 적용되어 총 3개의 RNN sub-modules이 있습니다.\n두가지 타입의 GNN forward 모델에 공통적인 사항\n\nstate differences를 예측하는 것을 학습해서 state prediction의 절댓값(absolute)을 계산합니다. 이 계산된 absolute state prediction을 가지고 state를 update하게 되는 것입니다.\nlong-range rollout trajectory를 만들어내기 위해서 state prediction 값과 control input을 반복적으로 model에 넣어주어서 여러 스텝의 trajectory를 생성하게 됩니다.\nGN model의 인풋과 아웃풋들은 normalize 됩니다.\n\n사실 리뷰를 하면서 forward model과 inference model 사이의 구분이나 모델의 구체적인 프로세스 이해가 pseudo algorithm을 보기 전까지 잘되지 않았습니다. Appendix에 나와있어서 잘 보지 않을 확률이 높지만 논문의 개념을 대략적으로 이해하고 난 후에는 꼭 line by line으로 보시길 추천합니다.\n먼저 forward model의 학습과정을 보여주는 pseudo algorithm 입니다. 다시한번 이 모델의 목적을 상기시켜보자면, 현재 상태 x^{t_0} 를 기반으로 a^{t_0}와 함께 주어졌을 때, x^{t_0+1}을 예측하는 것입니다. 앞서 설명한 부분들인, state의 잔차를 학습하는 부분이나 normalization 등이 알고리즘내에 잘 나와있습니다.\n\n\n\n\n\n다음은 학습된 forward model을 가지고 다음 상태인 x^{t_0+1}을 어떻게 예측하는지 보여주는 알고리즘입니다.\n\n\n\n\n\n마지막으로 바로 위 알고리즘과 동일하게 학습된 forward model을 가지고 다음 상태인 x^{t_0+1}을 어떻게 예측하는지 보여주는 알고리즘이지만 inference model에서 학습된 GN_p를 가지고 system identification이 추가된 상태에서 어떻게 알고리즘이 흘러가는지 보여줍니다.(이전에 알고리즘에서는 system parameter p라고 표시되었던 부분이 대체된 것입니다.)"
  },
  {
    "objectID": "posts/paper/2022-08-07-gn-block.html#inference-models",
    "href": "posts/paper/2022-08-07-gn-block.html#inference-models",
    "title": "📃GN-Block 리뷰",
    "section": "Inference Models",
    "text": "Inference Models\nInference model의 목적을 한 마디로 표현하자면 System identification이라고 할 수 있습니다. System identification이란 관찰할 수 없는(unobserved) dynamic system의 속성들을 관찰되는(observed) behavior(또는 어떤 양상)를 가지고 추론하는 것을 말합니다. 즉 암시적으로 system을 구성하는 요소들을 (명시적이지 않아) 측정하거나 관찰할 수 없지만 latent representations을 통해 추론할 수 있습니다.\n\n\n\n\n\nInference model도 Recurrent GN-based model 입니다. forward 모델과 다른 점으로는 오직 trajectory의 dynamic states들만 input으로 받습니다. 따라서 dynamic state graph인 G_d와 control input을 받습니다. 아웃풋으로는 일정 time step T이후의 G^*(T)이 되며, 본 논문에서 이후 실험파트에서 20 step을 사용했습니다.\ninference model 학습과정의 pseudo 알고리즘은 아래와 같습니다."
  },
  {
    "objectID": "posts/paper/2022-08-07-gn-block.html#control-algorithm",
    "href": "posts/paper/2022-08-07-gn-block.html#control-algorithm",
    "title": "📃GN-Block 리뷰",
    "section": "Control algorithm",
    "text": "Control algorithm\ncontrol algorithm에서는 그래프 기반이 아니고 앞서 설명한 그래프 기반의 forward model과 inference model을 잘 활용해서 어떻게 control할 수 있을지를 보여줍니다. 본 논문에서는 크게 2가지 control algorithm을 사용했습니다. 강화학습을 주로 연구하는 입장에서 리뷰해보면, 대부분 강화학습은 model-free 기반의 알고리즘이 많이 발전했는데 GN기반의 다음 상태를 예측할 수 있는 model을 만듦으로써 model-based 기반의 강화학습 알고리즘을 적용할 수 있다는 것이 매우 흥미로웠습니다.\n\nMPC(Model Predictive Control)\nGN은 미분 가능하기 때문에 MPC같은 gradient-based trajectory optimization 방법으로 model-based planning을 할 수 있습니다. 대표적으로 MPC가 있고 학습기반이 아니라 최적화 알고리즘이며 알고리즘의 흐름은 아래와 같습니다.\n\n\n\n\n\nSVG(Stochastic Value Gradients)\n강화학습 알고리즘 중 하나이며, GN-based model과 SVG의 policy function을 동시에 학습하는 agent로 control을 하는 방법입니다. SVG(1)은 한 스텝을 예측하는 GN model을 가지고 강화학습 알고리즘으로 control을 한 것이며(model-based) SVG(0)은 예측하는 GN model 없이 model-free 기반으로 control한 것으로 이해하시면 됩니다.\n\n\n사실 MPC와 SVG는 매우 비슷한 측면이 있습니다. MPC에서는 control inputs들이 한 에피소드에서 초기 조건들이 주어졌을 때 최적화 되는 것이라면, SVG에서는 state와 control을 매칭시키는 policy function이 학습과정에서 경험한 states에 대해서 최적화 되는 것입니다."
  },
  {
    "objectID": "posts/paper/2022-08-07-gn-block.html#prediction",
    "href": "posts/paper/2022-08-07-gn-block.html#prediction",
    "title": "📃GN-Block 리뷰",
    "section": "Prediction",
    "text": "Prediction\nLearning a forward model for a single system\n하나의 시스템을 가지고 학습한 forward model의 Prediction 성능 살펴보기\n\nrandom control로 만들어진 데이터들을 가지고 학습된 GN-based model\n\n[Visually] Swimmer6에서 그림에서 처럼 ground truth와 예측 결과가 구분이 안 갈 정도로 흡사하다.(영상에서도 거의 구분이 안 갈 정도로 잘 예측하고 있음을 알 수 있다.)\n\n\n\n\n\n[Quantitatively] 100 step에서 3축 방향으로의 위치, 선속도, 각속도, 쿼터니안 방향 비교\n\n\n\n\n\n\nconstant prediction baseline은 아웃풋으로 인풋을 그대로 복사해서 사용했기 때문에 애러 최대치로 normalization 하기 위해 검은색 점선으로 표기\n우선 검은 점선과 막대기들을 뭉뚱그려서 보면,\n1 step과 100 step의 rollout 결과를 비교했을 때 검은 점선에 비해 파란색 막대기들의 error 값이 낮음을 알 수 있다.\n\n\n\n\n\nGN 모델이 MLP-based 보다 더 낮은 애러를 가지는 것을 알 수 있다. 이는 특별히 Swimmer6처럼 에이전트의 구조가 반복적인 경우에 더욱 눈에 띄게 낮음을 알 수 있었다. 이를 통해 GN-based forward 모델이 다양한 물리 시스템들에서 dynamics를 잘 예측함을 알 수 있다.\n\n\n\n\n\n\n\nGN이 MLP보다 더 generalization이 잘 됨을 확인할 수 있었는데, Swimmer6를 집중적으로 train, valid, test 데이터에 대해 1-step, rollout error를 각각 확인해봤을 때, Best GN의 error 값이 Best MLP보다 낮음을 알 수 있다. 뿐만 아니라 test data의 error 증가율을 봤을 때에도 GN 모델의 test data의 error가 더 적게 증가함을 관찰할 수 있었고 이는 agent의 bodies와 joints들에 대한 inductive bias가 GN을 통해 잘 학습되었음을 증명할 수 있다.\n\n\n\n\n\n\nLearning a forward model for multiple systems\n한 개의 시스템에서의 forward model을 살펴보았으니 이제 여러 시스템에서의 forward model의 성능을 살펴보자. GN을 사용하면 여러 시스템들의 다양한 변수들도 잘 다룰 수 있다는 가정이 있었다. 이를 확인하기 위해 연속적으로 static parameter들(질량, body의 길이, joint의 각도 등)을 바꿔가면서 forward dynamics를 어떻게 학습해가는지 확인했다."
  },
  {
    "objectID": "posts/paper/2022-08-07-gn-block.html#inference",
    "href": "posts/paper/2022-08-07-gn-block.html#inference",
    "title": "📃GN-Block 리뷰",
    "section": "Inference",
    "text": "Inference"
  },
  {
    "objectID": "posts/paper/2022-08-07-gn-block.html#control",
    "href": "posts/paper/2022-08-07-gn-block.html#control",
    "title": "📃GN-Block 리뷰",
    "section": "Control",
    "text": "Control"
  },
  {
    "objectID": "posts/paper/2023-05-07-accessibility.html",
    "href": "posts/paper/2023-05-07-accessibility.html",
    "title": "📃K-Accessibility 리뷰",
    "section": "",
    "text": "이번 포스팅은 최근 ICRA(International Conference on Robotics and Automation) 2022에서도 발표된 Accessibility-Based Clustering for Efficient Learning of Locomotion Skills 논문을 읽고 정리한 내용입니다. 강화학습으로 로봇 제어를 학습할 때 어떻게 효율적으로 initial state distribution을 탐색하도록 만들어 줄 수 있을까?라는 질문을 K-means++ 알고리즘과 유사한 K-Access라는 알고리즘을 고안하여 해결한 논문입니다. 해당 논문에서는 quadruped robot의 Recovery와 Backflip 모션 학습을 보여주었습니다."
  },
  {
    "objectID": "posts/paper/2023-05-07-accessibility.html#initial-state-distrubutions",
    "href": "posts/paper/2023-05-07-accessibility.html#initial-state-distrubutions",
    "title": "📃K-Accessibility 리뷰",
    "section": "1.1 Initial state distrubutions",
    "text": "1.1 Initial state distrubutions\n앞서 Locomotion과 Recovery를 비교하며 살펴보았는데 강화학습으로 하는 로봇 제어의 관점에서 매우 큰 차이점 하나가 더 있습니다. 바로 Initial State Distribution, 강화학습의 Robot Agent가 학습 Episode를 시작하는 맨 처음의 State들의 분포입니다. Locomotion에서는 command(컨트롤러로 조작하는 로봇의 desired velocity 혹은 간단하게 방향키 조작으로 생각할 수 있음)를 따라 움직이는 것이기 때문에 Initial State로 로봇의 standing 자세를 가지고 학습 Episode를 시작하게 됩니다. 반면 Recovery는 로봇이 넘어진 상황(자세)가 각 Episode의 Initial State가 됩니다. 넘어진 자세는 매우 다양하기 때문에 어떤 넘어진 자세는 정상상태로 회복하기가 상대적으로 쉬운 반면, 어떤 자세는 정상상태로 회복하기가 어렵기 때문에 Recovery task에서는 RL(Reinforcement Learning) agent가 Initial State Distribution을 잘 탐색하고 학습할 수 있도록 만들어주는 것이 매우 중요합니다.\n\n위 사진에서 처럼 RL Agent가 탐색해야하는 전체 State Space와 어떤 한 Initial state(혹은 Initial pose, orange dot)와 유사한 state들의 집합 영역 Effective Exploration Region(EER)을 주황색 원 영역으로 표시할 수 있습니다. 여기서 주황색 원 안의 영역의 State들은 원 중심의 하나의 Initial State를 탐색하고 학습하고 나면 어렵지 않게 강화학습 Policy가 잘 학습할 수 있는 State들이라고 볼 수 있습니다. Case 1은 전체 탐색해야 하는 State Space를 빈틈의 최소화하도록 많은 Initial state를 학습하지만 각 EER들이 많이 중복되어 학습되기 때문에 학습 효율이 매우 떨어지는 것을 알 수 있습니다. Case 2에서는 적은 Initial state로 학습해서 State Space가 잘 커버되지 않았을 뿐만 아니라 목표로하는 Target State도 잘 학습되지 않아 학습 Policy의 성능이 매우 떨어지는 것을 알 수 있습니다. Case 3는 목표로하는 Target State는 EER에 들어가서 Policy가 학습한 state라고 할 수 있지만 전체 State Space에서 커버되지 못한 state들이 있기 때문에 Corner case들(Policy가 잘 작동되지 않는 경우)이 있어 Policy의 robustness가 떨어진다고 볼 수 있습니다. 따라서 가장 이상적인 상황은 Case 4에서처럼 Target State도 EER의 범주에 들어가 있고 전체 State Space도 적절한 수의 Initial State들로 탐색되어 Policy의 Robust한 상황이라고 할 수 있습니다."
  },
  {
    "objectID": "posts/paper/2023-05-07-accessibility.html#pose-of-quadruped-robots",
    "href": "posts/paper/2023-05-07-accessibility.html#pose-of-quadruped-robots",
    "title": "📃K-Accessibility 리뷰",
    "section": "1.2 Pose of Quadruped Robots",
    "text": "1.2 Pose of Quadruped Robots\n그렇다면 Initial State, 즉 4족 보행 로봇의 자세(pose)는 어떻게 표현할 수 있을까요? 전복된 상황은 넘어져 있는 로봇의 자세로 표현할 수 있을 것 입니다.\n\n전복된 상황은 움직임이 없는 넘어진 정적(Static) 상황이라 가정하고 로봇의 상황을 다음과 같이 2가지 정보로 표현할 수 있습니다. 첫번째로는 몸체의 기울어짐을 표현하는 Projected gravity vector로 지구 중력 방향의 벡터를 (0, 0, -1)이라고 했을 때, 로봇 몸체의 프레임에 gravity vector를 projection하고 normalized한 3차원의 벡터 정보는 몸체의 기울어짐을 표현할 수 있습니다. 두번째 요소는 로봇의 각 다리에 3개씩 배치되어 관절이 되는 12개의 revolute joint(motor) angle 입니다."
  },
  {
    "objectID": "posts/paper/2023-05-07-accessibility.html#distance-between-poses",
    "href": "posts/paper/2023-05-07-accessibility.html#distance-between-poses",
    "title": "📃K-Accessibility 리뷰",
    "section": "1.3 Distance between poses",
    "text": "1.3 Distance between poses\nPose를 정의한 다음으로 살펴볼 부분은 여러 pose들 간의 관계를 어떻게 정의할 수 있을까에 대한 부분을 고민해볼 수 있습니다. 여러 pose들 간에 가깝다(비슷하다), 멀다를 파악하기 위해서는 거리(Distance)를 정의할 수 있어야 합니다. 가장 직관적으로 pose를 이루고 있는 요소들 간의 유클리디안 거리를 생각해볼 수 있습니다. 앞서 정적인 자세를 구성하는 Projected gravity vector 와 Joint angles의 유클리디안 거리를 계산해서 나온 수치를 기반으로 pose가 서로 비슷하다, 많이 다르다를 판단할 수 있을 것 입니다.\n\n하지만 그림에서의 예시를 통해 유클리디안 거리가 Non-sense하다는 것을 볼 수 있습니다. 3가지 자세, Backward Leaning(B), Forward Leaning(F), Lying(L)를 가지고 유클리디안 거리를 계산해보면 B-F의 거리가 F-L의 거리보다 큰 수치인 것을 확인해볼 수 있습니다. 하지만 로봇을 직접 제어해서 자세를 transition한다고 생각했을 때, F에서 B로의 transition이 F에서 L로의 transition이 훨씬 어렵기 때문에 단순하게 구성 요소들의 유클리디안 거리로 pose들 간의 거리를 정의하는 것은 제어적인 측면에서 말이 된다고 볼 수 없습니다."
  },
  {
    "objectID": "posts/paper/2023-05-07-accessibility.html#sampling-static-poses",
    "href": "posts/paper/2023-05-07-accessibility.html#sampling-static-poses",
    "title": "📃K-Accessibility 리뷰",
    "section": "3.1 Sampling Static Poses",
    "text": "3.1 Sampling Static Poses\n전복된 다양한 자세들을 샘플링하기 위해서 로봇의 base frame의 roll, pitch 각도를 일정 범위에서 랜덤하게 샘플링하고 12개의 joint position도 로봇의 configuration을 고려하여 upper/lower limit range에 있는 각도로 자세를 set해서 전복된 자세를 만듭니다. (이때 yaw 방향은 flat terrain에선 의미가 없기 때문에 0으로 셋팅합니다.) 샘플링된 자세로 pose를 set 했을 때 self-collision을 확인한 뒤 self-collision이 되지 않은 자세 2.4k개를 sampling 합니다.\n\n\n\n예시 사진은 해당 논문의 코드를 연구실에서 개발된 AiDIN-VIII 로봇에 적용한 모습입니다"
  },
  {
    "objectID": "posts/paper/2023-05-07-accessibility.html#estimating-accessibility-values",
    "href": "posts/paper/2023-05-07-accessibility.html#estimating-accessibility-values",
    "title": "📃K-Accessibility 리뷰",
    "section": "3.2 Estimating Accessibility Values",
    "text": "3.2 Estimating Accessibility Values\n앞 단계에서 샘플링한 2.4k개의 pose들 중 1000개만 가지고 Accessibility를 측정하게 됩니다. 우선 2.4k개 중 1000개만 가지고 진행하는 이유는 학습 이후 Policy를 테스트하기 위한 Initial state로 사용하기 위해서 1.4개의 pose는 남겨놓는 것 입니다. 앞서 유클리디안 거리가 metric으로써 좋지 않은 점을 예시를 보며 확인할 수 있었기에 논문에서는 이를 대체할 metric으로 Accessibility라는 개념을 제안합니다.\n\n위의 예시는 어떤 pose A에서 pose B로의 Accessibility를 계산하는 과정을 보여줍니다. 특정 pose에서 다른 특정 pose로 transition하는 과정을 progress라는 작은 timestep들로 쪼개고 각 timestep에 해당하는 transition angle을 PD controller로 제어하게 됩니다. pose를 구성하는 12개의 joint position(angle)은 continuous value이기 때문에 처음과 끝 pose의 angle을 안다면 linear interpolation을 할 수 있습니다. progress를 scaled timeline(0~1로 normalized)이라고 하고 쪼갠 timestep 하나를 변수 t로 본다면 매 순간의 desired transition angle 은 t \\cdot \\text{[joint angle of B]} + (1-t) \\cdot \\text{[joint angle of A]}으로 계산될 수 있습니다. 이렇게 계산된 desired transition angle을 따라가도록 PD제어를 하면서 충분히 pose B에 가까워졌는가?를 판단하게 되는데 이때의 기준은 유클리디안 거리로 계산된 joint position distance, base의 height distance, gravity vector distance이 매우 작은 오차 범위내에 들어갔는지가 됩니다. pose A에서 pose B로 충분히 가까워진 해당 시간 t를 기록하게 되는데, 3초 내에 pose B에 가까워진 상태로 평형상태에 도달하는지 체크하게 됩니다. 저자가 공개한 코드에서 확인해봤을 때 20초를 상한선으로 설정하고 1000 pose \\times 1000 pose Time 매트릭스로 평형상태에 도달한 시간을 기록합니다.\n\n앞서 유클리디안 거리로 판단하는 것이 좋지 않다고 주장할 때는 pose들이 충분히 달랐을 때 pose들 간의 관계 정의로 사용하기에 부적절함을 들어 타당하지 않다고 주장한 것이었고, 현재 pose가 transition이 되었는가를 판단하기 위한 기준으로 유클리디안 거리가 매우 작은지로 판단하는 것은 similarness를 판단하는 것이기에 motivation을 해치지 않는다고 볼 수 있습니다.\n\n\n이렇게 측정한 transition time을 가지고 State Space를 해석해본다면 pose A(s_0)에서 pose B(s_1)으로의 시간 t(s_0, s_1)이 어떤 특정 시간 t_0이하라면 두 pose 사이 관계는 High Accessibility를 가지고 있다고 볼 수 있습니다. 반면, 만약 t(s_0, s_1)이 어떤 특정 시간 t_0 초과라면 Low Accessibility 라고 할 수 있고 이때의 기준이 되는 특정 시간 t_0가 EER R의 경계를 결정합니다. 따라서 이러한 Radial Boundary를 만들기 위해 앞서 계산한 Time 매트릭스(t(s_i, s_j))를 가지고 e^{-t(s_i, s_j)}을 계산한 것을 바로 Accessibility라고 정의하게 됩니다."
  },
  {
    "objectID": "posts/paper/2023-05-07-accessibility.html#clustering",
    "href": "posts/paper/2023-05-07-accessibility.html#clustering",
    "title": "📃K-Accessibility 리뷰",
    "section": "3.3 Clustering",
    "text": "3.3 Clustering\nK-Access Algorithm\n이제 State Space 상의 pose들간의 거리를 정의하는 Accessibility 값을 구한 다음에 어떻게 하면 클러스터링을 잘할 수 있을 것인가?에 대한 고민으로 넘어가게 됩니다. 각 cluster의 centroid가 되는 pose를 정할 수 있어야 하고 몇개의 cluster 갯수가 적절할 지 판단하는 알고리즘으로 K-Access알고리즘을 제안합니다.\n\n우선 결론적으로 cluster의 갯수의 적절성을 Index 지수가 최대가 되는 값로 판단하게 되는데, 이 Index 지수는 Intra-cluster Accessibility와 Inter-cluster Accessibility, 마지막으로 Regularization Term까지 합산하여 결정하게 됩니다.\n\nIntra-cluster Accessibility: 이름에서도 볼 수 있듯이 특정 클러스터에 속해있는(=내부에 있는) sample들(각 pose를 지칭)과 centroid sample간의 Accessibility 값들 중 최소값입니다. 이 값은 Index 지수에 positive sum이 되기 때문에 의미를 해석해본다면 한 클러스터에 속해있는 sample들의 centroid로 향하는 응집력이라고 볼 수 있습니다. Intra-cluster accessibility의 차원은 1000개 샘플이 자신이 속한 클러스터 centroid와의 값을 계산하므로 1000 dimension을 가지고 있습니다.\nInter-cluster Accessibility: 클러스터들 간에 overlapping이 되지 않고 적절히 거리를 유지하며 각 EER이 전체 State Space를 커버할 수 있도록 하기 위해서 centroid sample 간의 Accessibility의 평균을 구한 값\nRegularization Term: 클러스터의 개수가 너무 커지지 않도록 하는 부분으로 Index에 negative sum이 되는 부분입니다. \\alpha 값으로 Regularization의 비중을 높일 수 있는데 논문에서는 1을 사용했습니다.\n\n\nK-means++ VS. K-Access\nK-Access 알고리즘은 기존에 ML에서 자주 사용되는 클러스터링 알고리즘인 K-means++ 알고리즘을 기반으로 만들어진 알고리즘입니다. K-means++ 알고리즘처럼 (1) Initialize the centroids (2) Assignment step (3) Update step 단계를 거치는 것은 비슷하지만 K-means++ 알고리즘에서는 (3)단계에서 평균값을 기반으로 클러스터링이 진행되는 반면 K-Access 알고리즘에서는 robustness를 보장하기 위해 Maximal neighborhood accessibility를 사용합니다.\n좀 더 자세한 알고리즘 과정을 알아보고 싶으신 분들은 아래 Pseudo Code를 확인해주세요.\n\n\nPseudo Code of K-Access\n\n\n\n\nClustering Analysis\n논문에서 사용한 Bittle 로봇 플랫폼으로 clustering을 진행했을 때 43개의 cluster가 최적의 갯수로 정해집니다. 각 클러스터에 속하는 샘플 수를 히스토그램으로 확인해보면 아래 왼쪽 그래프같이 그려지며 이중 해당 클러스터에 속한 샘플 수가 많은 순서대로 top 20개의 클러스터들 간의 inter-cluster accessibility를 Chord graph를 가지고 시각화를 해보면 오른쪽 그래프와 같이 그려집니다. Chord graph에서 강조된 부분들은 0.15 이상의 Accessibility(약 1.9초 이내의 transition time)를 가진 부분들이며 옅게 표시된 부분들은 0.05 이하의 Accessibility(약 3초 이상 transition time)를 가지는 부분들입니다.\n\n\nChord graph 시각화 방법에 대해서는 해당 논문을 기반으로 실제 제가 연구하고 있는 로봇 플랫폼을 이용하여 적용한 코드 실습은 다음 포스팅 에서 확인할 수 있습니다."
  },
  {
    "objectID": "posts/paper/2023-05-07-accessibility.html#reinforcement-learning-process",
    "href": "posts/paper/2023-05-07-accessibility.html#reinforcement-learning-process",
    "title": "📃K-Accessibility 리뷰",
    "section": "3.4 Reinforcement Learning Process",
    "text": "3.4 Reinforcement Learning Process\n마치 Machine Learning에서 Feature Engineering이 많은 주의를 요하는 작업이듯이 앞서 Initial State를 정하는 작업을 진행하고 드디어 강화학습 과정에 들어오게 되었습니다. 유명한 강화학습 알고리즘인 SAC(Soft-Actor-Critic)을 단순한 MLP 레이어로 만들어서 사용했고 Policy Network의 Input과 Ouput 설계도 관련 연구들의 convention과 크게 다르지 않기 때문에 자세한 설명은 생략하고 특징적인 부분을 살펴보겠습니다.\nReward Functions w/ RBF\n\n해당 논문에서 다른 논문들의 강화학습 MDP 설계와 다른 특징적인 부분은 보상함수 설계 부분이었습니다. 일반적으로 Reward function은 각 Reward Term들을 Linear Weigthed Sum형식을 가집니다. 하지만 해당 논문에서는 RBF(Radial Basis Function)를 사용하여 각 Reward를 weighted sum한 값으로 최종 reward를 계산한 것을 알 수 있습니다. 사실 Reward Function을 설계하는 부분은 강화학습 연구에서 Reward Engineering 이슈가 큰 것처럼, 다분히 설계자의 의도와 설명이 필요한 부분이지만 논문에서 자세히 설명이 되어 있지 않고 Main Contribution이 아니라고 생각해서 그런지 Linear sum과 비교한 실험값도 있지 않아서 RBF를 사용한 이유를 파악하기 어려웠습니다.\n\n \n\nLecture 16 - Radial Basis Functions Slides(Caltech)\n\n\n따라서 이 부분은 RBF 커널에 대해 공부하고 나서 제가 생각한 이유를 덧붙이겠습니다. RBF 커널은 기본적으로 Gaussian Distribution 모습으로 target value와 data 간의 radial한 거리 가중치를 주게 되는데, linear sum과 비교했을 때 무한 차원 영역에서 매우 멀리 떨어져 있는 data로부터 영향을 덜 받을 수 있는 장점을 가지고 있습니다. 따라서 Reward를 계산하는 데에 RBF 커널을 통해 계산한 의도는 Maximization해야 하는 Reward term들을 단순히 Linear sum하는 것보다 여러 카테고리의 Reward target 값들에 민감하게 반응할 수 있는 정도를 \\alpha값(Slide에서는 \\gamma로 표현)을 이용하여 학습의 좋은 지표가 될 수 있는 Reward space를 설계한 것으로 보입니다.\n\nReward Term에서 사용된 Symbol의 의미가 궁금하신 분들은 아래 table을 확인해주세요.\n\n\n\nSymbols of Reward Terms for DRL\n\n\n\n\nOther Tasks - Backflip\n해당 논문에서는 Recovery 뿐만 아니라 Locomotion 보다 더 다이나믹한 모션도 학습하는 것을 보여주기 위해 Backflip 학습도 K-Accessibility 알고리즘을 이용하여 학습을 진행하였습니다. (이전에 리뷰했던 WASABI 논문에서도 다이나믹한 모션 4가지 중 하나를 Backflip으로 학습 결과를 보여주었던 것과 같은 맥락으로 해당 모션 Task를 설정했다고 보시면 됩니다.)"
  },
  {
    "objectID": "posts/paper/2024-11-10-ipo.html",
    "href": "posts/paper/2024-11-10-ipo.html",
    "title": "📃IPO 리뷰",
    "section": "",
    "text": "오늘은 “IPO: Interior-point Policy Optimization under Constraints”라는 논문에 대해서 리뷰해보려고 합니다. 흔히 강화학습(Reinforcement Learning)을 처음 개념을 공부하고 나면, 강화학습의 문제를 MDP(Markov Decision Process)로 정의한다는 것을 떠올릴 수 있습니다. 이때 강화학습의 핵심인 Reward, 즉 보상을 잘 설정해주어야 Agent가 원하는 방향대로 학습을 하게 됩니다. 보상은 Agent가 해야하는 행동 양식의 (+)가 되는 방향을 나타내는 지 표이며 우리가 원하는 행동을 Encourage(장려)하는 역할을 하게 됩니다.\n이번 논문에서는 기본적인 강화학습의 MDP가 아닌 Constraint라는 개념을 넣어서 생각을 해보려고 합니다. Constraint(제약)은 가장 단순하게는 -Reward 라고 생각해볼 수 도 있습니다. 우리가 Agent가 하지 않았으면 하는 행동을 정의함으로써 negative reward를 준다고 볼 수 있는 것이죠. (마치 Gradient Ascent가 Gradient Discent의 반대로 생각해볼 수 있듯이요.) 따라서 Reward와 Constraint는 서로 (+)/(-) 부호적인 성격이 다르지만 Agent에게 학습의 방향을 제시하는 신호라는 측면에서는 공통점을 가지고 있습니다.\n조금 더 Constraint에 대해서 자세히 살펴보겠습니다. Constraint는 제약이 발생되는 시점에 따라 2가지로 나누어서 생각해 볼 수 있습니다.\n\n\n\nConstraints\n\n\n우선, instantaneous constraint는 뜻에서도 알 수 있듯이 일시적으로 constraint를 주는 것을 말합니다. 강화학습에서 Agent가 action을 하게 되는 timestep 마다 제약 상황인지를 판단하여 constraint를 주는 것을 말합니다. 이는 기본적인 강화학습 개념에서 매 timestep마다 reward를 주는 상황과 같습니다. 예를 들어 로봇팔(Manipulator)을 제어하는 상황을 생각해보면, Agent는 적절한 움직임을 위해 로봇팔을 구성하는 모터들을 잘 구동하여 원하는 모션을 만들어야 합니다. 이때 로봇이 움직이는 모든 매 순간마다 각 모터들(joint)이 가동범위에 있어야 하고 과한 토크가 가해지지 않도록 해야 합니다. 이러한 제약 상황들은 매 순간 판단해서 해당 범위들을 넘지 않는 action을 선택하도록 학습해야 하므로 instantaneous constraint의 예로 볼 수 있습니다.\n다음으로 cumulative constraint는 Agent가 학습하는 하나의 Episode 내에서 누적해서 나온 값으로 판단하여 제약상황을 판단하는 것을 말합니다. 이때 누적되는 시간은 하나의 Episode가 시작해서 끝날 때까지일 수도 있고 아니면 5 timesteps 동안이라는 특정 timestep 수를 지정하여 계산할 수 있습니다. 로봇팔의 예시로 살펴보자면, 로봇이 펜을 잡는 모션을 할 때까지 100 timestep이 걸렸는데 매 timestep 마다 지연(latency)가 발생하여 이를 제약하고자 합니다. 이러한 상황에서 100 timestep동안의 average latency를 구해서 특정 latency를 넘지 못하도록 constraint를 줄 수 있습니다. 이러한 예시처럼 특정 구간 동안의 값을 통해서 constraint를 주는 것을 cumulative constraint라고 합니다. 이번 IPO 논문에서는 두번째로 소개드린 cumulative constraint에 초점을 맞춰 개발된 알고리즘을 소개하고 있습니다.\n\n\n앞서 설명드린 Constraint가 MDP에 추가된 것을 Constrained Markov Decision Process(CMDP)라고 합니다. CMDP에서는 Reward와 같이 현재 State에서 Action을 취하고 다음 State에 도달했을 때 얻게 되므로 아래 사진에서와 같이 Space가 정의되게 됩니다.\nConstraint는 (s_n, a_n, s_{n+1})과 같은 transition tuple로 계산되게 되며, cumulative constraint는 일정 timestep, 즉 transition이 n(서수:t)개 모여서 계산되게 됩니다. 이때 Constraint도 여러 종류가 있을 수 있으므로 constraint의 가짓 수는 m(서수:i)으로 나타낼 수 있습니다. Constraint는 more than better인 reward와 다르게 제약되는 상황을 정의하게 되는 constraint limit이 있게 되고 이를 \\epsilon_i로 나타내게 됩니다.\n\n\n\nConstraint Space and Constraint Limit\n\n\nConstraint의 Expectation은 다음과 같이 정의가 되며 2가지의 constraint 계산방법이 있습니다. 첫번째로는 discounted cumulative constraint로 할인율 \\gamma를 고려한 constraint들을 하나의 policy가 동작하는 동안 누적합한 값을 말합니다. 두번째로는 일정 timestep T동안 계산한 constraint들의 평균을 말하는 것으로 mean values constraint가 있습니다. 이 2가지 종류의 지표에 대해서 후에 실험에서 다룰 예정이며 CMDP의 목표를 정리해보면, 기존에 J_R만을 Maximization했던 강화학습 문제가 J_{C_i}를 고려해야 한다는 것이 추가 되었다는 것을 알 수 있습니다.\n\n\n\nConstraint Expectation\n\n\n기존의 Constraint가 있는 최적화 문제는 Lagrangian Relaxation Method를 통해서 해결했었습니다. 라그랑지안 승수법이라고도 불리는 해당 방법은 기존의 최적화 식 f(x)에 constraint g_i(x)가 추가된 최적화 문제를 Lagrange Multipilers를 곱하여 기존 최적화 함수 목적식에 더하여서 제약 조건을 푸는 방법입니다.\n\n\n\nLagrangian relaxation method\n\n\n\n\n\n라그랑지안 승수법은 가장 심플하게 제약 조건들을 메인 최적화식에 녹여내어 풀어내는 방식으로, CMDP 문제들도 해당 방법을 통해 해결하는 것이 통상적인 방법이었지만 라그랑지안 승수법은 정책이 수렴할 때 제약 조건이 만족되지만, 이 접근법은 Lagrange multiplier의 초기값과 학습률에 민감하고 학습 과정에서 얻은 정책이 항상 제약 조건을 일관되게 만족시키지는 않는다는 한계점이 있습니다.\n\n\n\n앞 부분에서 살펴본 것과 같이 CMDP Goal은 Reward 값을 최대화하면서 제약식을 만족하는 최적의 policy를 찾는 것이라고 할 수 있습니다.\n\n\n\nCMDP Goal\n\n\n먼저 제약조건을 잠시 뒤로 두고, 본래 기본적인 강화학습의 목적식인 Reward Maximization은 어떻게 할까요? Policy Gradient는 강화학습의 한 계열로 최적의 policy, 즉 가장 Reward를 많이 받을 수 있는 policy를 찾기 위해 아래와 같은 목적식의 gradient를 계산하게 됩니다. 이때 최적의 policy를 찾기 위해서 \\theta는 위에서 구한 gradient 값을 기반으로 아래와 같이 업데이트하게 됩니다.\n\n\n\nPolicy Gradient Methods\n\n\nTrust Region Policy Optimization(TRPO)라는 알고리즘이 PG계열에서 대표적으로 사용되는 알고리즘이며, 최적이 policy를 찾기 위해 surrogate function을 이용하고 policy가 업데이트 되는 step size를 제한하기 위해 KL divergence를 사용합니다. TRPO의 최적화 식은 아래와 같이 표현할 수 있습니다.\n\n\n\nTRPO VS PPO\n\n\n하지만 TRPO는 conjugate gradient optimization으로 풀리는 2차 미분 최적화를 사용하기 때문에 계산 cost가 큽니다. 따라서 TRPO를 실용적으로 사용할 수 있게한 Proximal Policy Optimization (PPO) 알고리즘이 제안되었습니다. PPO의 최적화 식은 TRPO에서 문제였던 2차미분을 1차 미분 surrogate function으로 대체할 수 있었으며 계산복잡성을 줄일 수 있었습니다.\nIPO는 이러한 흐름대로 발전해온 PPO 알고리즘의 최적화 식에서 제약식을 추가하면서 발전하게 됩니다."
  },
  {
    "objectID": "posts/paper/2024-11-10-ipo.html#constrained-markov-decision-processcmdp",
    "href": "posts/paper/2024-11-10-ipo.html#constrained-markov-decision-processcmdp",
    "title": "📃IPO 리뷰",
    "section": "",
    "text": "앞서 설명드린 Constraint가 MDP에 추가된 것을 Constrained Markov Decision Process(CMDP)라고 합니다. CMDP에서는 Reward와 같이 현재 State에서 Action을 취하고 다음 State에 도달했을 때 얻게 되므로 아래 사진에서와 같이 Space가 정의되게 됩니다.\nConstraint는 (s_n, a_n, s_{n+1})과 같은 transition tuple로 계산되게 되며, cumulative constraint는 일정 timestep, 즉 transition이 n(서수:t)개 모여서 계산되게 됩니다. 이때 Constraint도 여러 종류가 있을 수 있으므로 constraint의 가짓 수는 m(서수:i)으로 나타낼 수 있습니다. Constraint는 more than better인 reward와 다르게 제약되는 상황을 정의하게 되는 constraint limit이 있게 되고 이를 \\epsilon_i로 나타내게 됩니다.\n\n\n\nConstraint Space and Constraint Limit\n\n\nConstraint의 Expectation은 다음과 같이 정의가 되며 2가지의 constraint 계산방법이 있습니다. 첫번째로는 discounted cumulative constraint로 할인율 \\gamma를 고려한 constraint들을 하나의 policy가 동작하는 동안 누적합한 값을 말합니다. 두번째로는 일정 timestep T동안 계산한 constraint들의 평균을 말하는 것으로 mean values constraint가 있습니다. 이 2가지 종류의 지표에 대해서 후에 실험에서 다룰 예정이며 CMDP의 목표를 정리해보면, 기존에 J_R만을 Maximization했던 강화학습 문제가 J_{C_i}를 고려해야 한다는 것이 추가 되었다는 것을 알 수 있습니다.\n\n\n\nConstraint Expectation\n\n\n기존의 Constraint가 있는 최적화 문제는 Lagrangian Relaxation Method를 통해서 해결했었습니다. 라그랑지안 승수법이라고도 불리는 해당 방법은 기존의 최적화 식 f(x)에 constraint g_i(x)가 추가된 최적화 문제를 Lagrange Multipilers를 곱하여 기존 최적화 함수 목적식에 더하여서 제약 조건을 푸는 방법입니다.\n\n\n\nLagrangian relaxation method\n\n\n\n\n\n라그랑지안 승수법은 가장 심플하게 제약 조건들을 메인 최적화식에 녹여내어 풀어내는 방식으로, CMDP 문제들도 해당 방법을 통해 해결하는 것이 통상적인 방법이었지만 라그랑지안 승수법은 정책이 수렴할 때 제약 조건이 만족되지만, 이 접근법은 Lagrange multiplier의 초기값과 학습률에 민감하고 학습 과정에서 얻은 정책이 항상 제약 조건을 일관되게 만족시키지는 않는다는 한계점이 있습니다."
  },
  {
    "objectID": "posts/paper/2024-11-10-ipo.html#policy-gradient-methods",
    "href": "posts/paper/2024-11-10-ipo.html#policy-gradient-methods",
    "title": "📃IPO 리뷰",
    "section": "",
    "text": "앞 부분에서 살펴본 것과 같이 CMDP Goal은 Reward 값을 최대화하면서 제약식을 만족하는 최적의 policy를 찾는 것이라고 할 수 있습니다.\n\n\n\nCMDP Goal\n\n\n먼저 제약조건을 잠시 뒤로 두고, 본래 기본적인 강화학습의 목적식인 Reward Maximization은 어떻게 할까요? Policy Gradient는 강화학습의 한 계열로 최적의 policy, 즉 가장 Reward를 많이 받을 수 있는 policy를 찾기 위해 아래와 같은 목적식의 gradient를 계산하게 됩니다. 이때 최적의 policy를 찾기 위해서 \\theta는 위에서 구한 gradient 값을 기반으로 아래와 같이 업데이트하게 됩니다.\n\n\n\nPolicy Gradient Methods\n\n\nTrust Region Policy Optimization(TRPO)라는 알고리즘이 PG계열에서 대표적으로 사용되는 알고리즘이며, 최적이 policy를 찾기 위해 surrogate function을 이용하고 policy가 업데이트 되는 step size를 제한하기 위해 KL divergence를 사용합니다. TRPO의 최적화 식은 아래와 같이 표현할 수 있습니다.\n\n\n\nTRPO VS PPO\n\n\n하지만 TRPO는 conjugate gradient optimization으로 풀리는 2차 미분 최적화를 사용하기 때문에 계산 cost가 큽니다. 따라서 TRPO를 실용적으로 사용할 수 있게한 Proximal Policy Optimization (PPO) 알고리즘이 제안되었습니다. PPO의 최적화 식은 TRPO에서 문제였던 2차미분을 1차 미분 surrogate function으로 대체할 수 있었으며 계산복잡성을 줄일 수 있었습니다.\nIPO는 이러한 흐름대로 발전해온 PPO 알고리즘의 최적화 식에서 제약식을 추가하면서 발전하게 됩니다."
  },
  {
    "objectID": "posts/paper/2024-11-10-ipo.html#interior-point-policy-optimization",
    "href": "posts/paper/2024-11-10-ipo.html#interior-point-policy-optimization",
    "title": "📃IPO 리뷰",
    "section": "2.1 Interior-point Policy Optimization",
    "text": "2.1 Interior-point Policy Optimization\nIPO이전에 CPO(Constrained policy optimization)라는 알고리즘이 제안되었었습니다. IPO는 CPO의 단점을 보완하여 제안된 알고리즘으로 볼 수 있으며 아래와 같이 2개 알고리즘을 비교해볼 수 있습니다.\n\n\n\nCPO VS IPO\n\n\n우선, CPO는 TRPO에서 제약조건을 추가한 목적식을 사용하여 TRPO의 문제이기도 했던 2차 미분 계산이 필요하다는 특성이 있습니다. 따라서 제약조건들을 추가하거나 mean valued constraint와 같은 누적 제약식을 계산하기 까다롭거나 할 수 없다는 문제점을 가지고 있었습니다. 이에 반해, IPO는 PPO에 제약조건을 추가한 목적식을 기반으로 하여 1차 미분만을 하면 된다는 장점을 가지고 있으며, 다양한 제약조건들을 이후에 설명할 핵심 아이디어인 logarithmic barrier function을 이용하여 쉽게 추가할 수 있습니다."
  },
  {
    "objectID": "posts/paper/2024-11-10-ipo.html#logarithmic-barrier-function",
    "href": "posts/paper/2024-11-10-ipo.html#logarithmic-barrier-function",
    "title": "📃IPO 리뷰",
    "section": "2.2 Logarithmic Barrier Function",
    "text": "2.2 Logarithmic Barrier Function\n우선 IPO의 문제 정의는 아래와 같이 PPO의 목적식에다가 Constraint를 추가한 것으로 정의할 수 있습니다.\n\n\n\nIPO Problem Definition\n\n\nConstraint는 Limit을 고려하여 부등호로 나타낼 수 있으며 이는 Indicatior Function에 넣었을때, Constraint를 넘었을 경우 -\\infin로 나타내고 Constraint를 만족했을 경우 0으로 나타낼 수 있습니다. 하지만 Indicator Function은 불연속적이며 미분 불가능하기 때문에 gradient를 구할 수 없어서 Logarithmic Barrier Function을 통해 근사하게 됩니다.\n\n\n\nLogarithmic Barrier Function\n\n\nLogarithmic Barrier Function(\\phi)은 그래프에서와 같이 하이퍼 파라미터인 t의 값이 클수록 Indicator Function과 유사하다는 것을 알 수 있습니다. 그래프에서 초록색 t=50일 때의 그래프가 점선의 Indicator Function과 유사한 것 처럼요. 또한 \\phi는 이분이 가능하기 때문에 gradient를 통해 최적화할 수 있습니다.\n\n\n\nIPO 결론(목적식과 수도코드)\n\n\n따라서 IPO의 최적화식은 PPO의 목적식 (L^{C L I P}(\\theta))에 Logarithmic Barrier Function(\\phi)을 이용하여 제약조건을 합치게 된(\\sum_{i=1}^m \\phi\\left(\\widehat{J}_{C_i}^{\\pi_i}\\right)) 모습이 됩니다."
  },
  {
    "objectID": "posts/paper/2024-11-10-ipo.html#performance-guarantee-bound",
    "href": "posts/paper/2024-11-10-ipo.html#performance-guarantee-bound",
    "title": "📃IPO 리뷰",
    "section": "2.3 Performance Guarantee Bound",
    "text": "2.3 Performance Guarantee Bound\n그렇다면 IPO의 성능 보장을 이론적으로 검증해보겠습니다.\n\n \n\n이러한 수식적인 검증 과정을 거쳐 IPO의 목적식은 일정 한계 내에 있다는 것(Bounded) 되어있다는 결론을 내릴 수 있습니다.\n\n\n\n수식적으로 Performance Guarantee Bound를 확인하여 t(logarithmic barrier function의 하이퍼파라미터)가 클수록 Indicator function에 대한 더 좋은 근사값을 제공하게 되고 더 높은 reward와 cost를 얻을 수 있다는 것을 확인할 수 있습니다. 하지만 t가 클수록 최적화 식이 수렴하는 속도는 느려진다는 단점이 있습니다. 또한 수식으로 확인한 단조성(monotonicity)을 이용하여, 수렴 속도와 최적화 성능 사이의 균형을 맞출 수 있는 적절한 t 값을 찾기 위해 이진 탐색 알고리즘(binary search)을 사용할 수 있다는 사실도 확인할 수 있습니다."
  },
  {
    "objectID": "posts/paper/2024-11-10-ipo.html#discounted-cumulative-constraints",
    "href": "posts/paper/2024-11-10-ipo.html#discounted-cumulative-constraints",
    "title": "📃IPO 리뷰",
    "section": "3.1 Discounted Cumulative Constraints",
    "text": "3.1 Discounted Cumulative Constraints\n\n\n\nDiscounted Cumulative Constraints 실험 결과\n\n\n\nIPO VS. CPO\n\nIPO\n\n최고 성능을 보여줍니다.\n제약 조건이 충족된 이후에도 더 나은 정책을 찾기 위해 탐색을 계속합니다.\n이로 인해 더 높은 보상과 더 낮은 비용으로 수렴합니다.\n수렴 속도는 느리지만, 최종 성능은 CPO보다 우수합니다.\n\nCPO\n\n수렴 속도가 IPO보다 빠릅니다.\n제약 조건이 충족되면 개선 작업을 중단합니다.\n제약 조건을 빠르게 만족시키지만, 그 이후에는 성능 개선이 멈춥니다.\n따라서 보상이나 비용 측면에서 IPO만큼의 최적화를 이루지 못할 가능성이 있습니다.\n\n\n\n\n\n\n\n\n\n\n\n특징\nIPO\nCPO\n\n\n\n\n수렴 속도\n느림\n빠름\n\n\n제약 충족 후 개선\n계속 탐색 (더 나은 정책을 찾음)\n개선 중단 (제약 조건 충족 시)\n\n\n최종 성능\n더 높은 보상과 낮은 비용\n제약 조건 만족 후 개선 없음\n\n\n\n\nIPO VS. PDO\n\nIPO\n\n최고 성능을 보여줍니다.\n제약 조건이 충족된 이후에도 더 나은 정책을 찾기 위해 탐색을 계속합니다.\n안정적인 학습 과정을 가지며, 성능의 변동이 적습니다.\n초기화나 학습률에 덜 민감합니다.\n\nPDO\n\nIPO만큼 좋은 정책으로 수렴 가능하지만, 훈련 중 성능의 분산(variance)이 높습니다.\n제약 조건 값을 한계 이하로 낮추는 정책을 찾을 수 있으나, 그 결과 보상(reward)이 가장 낮아질 수 있습니다.\nLagrange multiplier의 초기값과 학습률(learning rate)에 민감하게 반응합니다.\n초기 설정이 잘못되면, 학습 과정이 불안정해질 수 있습니다.\n\n\n\n\n\n\n\n\n\n\n\n특징\nIPO\nPDO\n\n\n\n\n수렴 성능\n최고 성능에 수렴\nIPO 수준으로 수렴 가능\n\n\n훈련 중 성능 변동\n낮음 (안정적)\n높음 (변동이 큼)\n\n\n제약 조건 만족도\n제약 조건을 충족하며 탐색 지속\n제약 조건 값을 한계 이하로 낮춤\n\n\n보상 (Reward)\n높은 보상\n가장 낮은 보상 가능성\n\n\n초기화/학습률 민감도\n낮음\n높음\n\n\n\n\n(optional)CPO vs. PPO / TRPO\n\n\n\n\n\n\n\n\n\n\n특징\nCPO\nPPO\nTRPO\n\n\n\n\n제약 조건 처리 여부\n제약 조건을 고려함\n제약 조건 없음\n제약 조건 없음\n\n\n보상 (Reward)\n높음 (제약 조건 내에서)\n가장 높음 (제약 조건 위반 가능성 있음)\n높음 (제약 조건을 간접적으로 완화)\n\n\n제약 조건 위반 가능성\n낮음\n높음\n중간 (신뢰 영역으로 일부 완화)\n\n\n학습 안정성\n높음\n높음\n매우 높음\n\n\n계산 복잡도\n중간\n낮음\n높음"
  },
  {
    "objectID": "posts/paper/2024-11-10-ipo.html#mean-valued-constraints",
    "href": "posts/paper/2024-11-10-ipo.html#mean-valued-constraints",
    "title": "📃IPO 리뷰",
    "section": "3.2 Mean Valued Constraints",
    "text": "3.2 Mean Valued Constraints\n\n\n\nMean Valued Constraints 실험 결과\n\n\n\nIPO VS. PDO\n\nIPO\n\n일관된 수렴: 모든 작업(task)에서 할인 누적 보상(discounted cumulative reward)이 높은 정책으로 안정적으로 수렴합니다.\n제약 조건 만족: 모든 작업에서 평균 값 제약(mean valued constraints)을 지속적으로 만족시킵니다.\n안정적인 학습: 훈련 중 성능의 변동이 적으며, 낮은 분산(variance)을 보입니다.\n\nPDO\n\n제약 조건 위반 가능성: 간혹 제약 조건을 위반하는 정책으로 수렴할 수 있습니다. (참조: Figure 3b)\n훈련 중 높은 분산: 훈련 과정에서 성능의 변동이 크며, 높은 분산을 보입니다. (참조: Figure 3d 및 Figure 3f)\n높은 보상 가능성: 때때로 높은 보상을 달성할 수 있지만, 제약 조건을 지키지 못할 위험이 있습니다.\n\n\n\n\n\n\n\n\n\n\n\n특징\nIPO\nPDO\n\n\n\n\n할인 누적 보상\n안정적으로 높은 보상에 수렴\n높은 보상 가능성이 있으나 불안정\n\n\n제약 조건 만족도\n항상 제약 조건을 만족함\n간혹 제약 조건을 위반\n\n\n훈련 중 성능 변동 (분산)\n낮음 (안정적)\n높음 (변동이 큼)\n\n\n안정성\n매우 안정적\n초기화와 학습률에 민감"
  },
  {
    "objectID": "posts/paper/2024-11-10-ipo.html#constraint-effects",
    "href": "posts/paper/2024-11-10-ipo.html#constraint-effects",
    "title": "📃IPO 리뷰",
    "section": "3.3 Constraint Effects",
    "text": "3.3 Constraint Effects\nPoint Gather 환경에서 제약 조건을 완화하여 임계값을 1로 설정한 경우, 각 에이전트는 평균적으로 최대 1개의 폭탄(bomb)을 수집할 수 있습니다. Constraint 값을 내려서 완화하게 되면 제약 조건이 매우 느슨해져서, 제약 조건이 있는 최적화 문제의 성능이 제약 조건이 없는 경우와 동일한 수준으로 나타납니다.\n\nCPO\n\nCPO는 여전히 비용을 증가시켜 제약 임계값(1)에 도달하려고 합니다.\n이는 때때로 랜덤 초기화된 정책보다도 성능이 떨어질 수 있습니다.\nCPO는 항상 비용을 제약 임계값(1)까지 밀어 올리려는 경향을 보입니다.\n\nIPO\n\nIPO는 제약 조건이 충족된 이후에도 비용을 계속 줄여나갑니다.\n이로 인해 더 낮은 비용을 달성하며, 더 나은 최종 성능을 보여줍니다.\n\n\n\n\n\n\n\n\n\n\n특징\nCPO\nIPO\n\n\n\n\n제약 조건 만족도\n제약 임계값(1)까지 비용 증가\n제약 충족 후에도 비용 감소 지속\n\n\n최종 비용 수준\n약 1\n약 0.25\n\n\n성능\n제약 충족을 우선시하며 성능 저하 가능\n제약을 충족하면서도 더 나은 성능\n\n\n\n\n\n\n따라서 실험을 통해 다음과 같은 결론을 내릴 수 있습니다.\n\nCPO는 제약을 맞추기 위해 비용을 적극적으로 증가시키지만, 그 결과 성능이 떨어질 가능성이 있습니다.\nIPO는 제약을 만족한 이후에도 비용을 줄이며, 더 높은 성능을 달성할 수 있습니다."
  },
  {
    "objectID": "posts/paper/2024-11-10-ipo.html#hyperparameter-tuning",
    "href": "posts/paper/2024-11-10-ipo.html#hyperparameter-tuning",
    "title": "📃IPO 리뷰",
    "section": "3.4 Hyperparameter Tuning",
    "text": "3.4 Hyperparameter Tuning\n\nIPO vs. PDO\n\nIPO\n\n하이퍼파라미터 t의 튜닝이 용이합니다.\n보상(reward)과 비용(cost)은 하이퍼파라미터 t와 양의 상관 관계를 가집니다.\nt 값이 커질수록, 보상과 비용이 동시에 증가합니다.\n이진 탐색(binary search)이 가능:\nt 값을 조정하며 성능을 확인할 수 있으며, 이진 탐색을 통해 빠르게 최적의 값을 찾을 수 있습니다.\n\nPDO\n\n초기 Lagrange multiplier (\\lambda)와 학습률(learning rate)의 설정이 까다롭습니다.\n초기 \\lambda 값이 0.01에서 0.1 사이일 때 매우 민감하게 반응합니다.\n잘못된 초기화는 학습 과정의 불안정을 초래할 수 있습니다.\n학습률(learning rate)의 변화에도 민감합니다.\n학습률이 0.01에서 0.001로 작아지면, 정책의 수렴 속도가 느려집니다.\n하이퍼파라미터 설정에 많은 시간과 노력이 필요합니다.\n\n\n\n\n\n\n\n\n\n\n\n특징\nIPO\nPDO\n\n\n\n\n하이퍼파라미터 튜닝 용이성\n쉬움\n어렵고 복잡함\n\n\n보상과 비용의 관계\nt와 양의 상관 관계\n초기 \\lambda와 학습률에 민감\n\n\n초기 설정 민감도\n낮음\n높음\n\n\n튜닝 방법\n이진 탐색 가능\n초기화와 학습률 설정에 많은 노력 필요\n\n\n\n\n\n\n따라서 실험을 통해 다음과 같은 결론을 내릴 수 있습니다.\n\nIPO는 하이퍼파라미터 t의 튜닝이 쉽고, 보상과 비용이 t 값에 따라 예측 가능하게 변화하기 때문에 안정적인 최적화가 가능합니다.\nPDO는 초기화와 학습률에 민감하여 튜닝이 까다롭고 학습 과정이 불안정할 수 있습니다. 특히 초기 \\lambda와 학습률 설정이 중요한 역할을 합니다."
  },
  {
    "objectID": "posts/paper/2024-11-10-ipo.html#multiple-constraints",
    "href": "posts/paper/2024-11-10-ipo.html#multiple-constraints",
    "title": "📃IPO 리뷰",
    "section": "3.5 Multiple Constraints",
    "text": "3.5 Multiple Constraints\nIPO (Interior Point Optimization)는 제약 조건을 다룰 때 유연하고 확장 가능한 방식으로 설계되어 있습니다. 특히, logarithmic barrier function을 사용하여 제약 조건을 쉽게 추가할 수 있습니다. IPO에서는 새로운 제약 조건이 필요할 때, 기존 최적화 함수에 로그 배리어 항을 추가하기만 하면 됩니다. 이 방식은 CPO보다 간단하게 제약 조건을 추가할 수 있는 이점이 있습니다. IPO는 logarithmic barrier function을 사용하여 제약 조건을 쉽게 추가할 수 있어, 확장성과 유연성 측면에서 CPO보다 유리합니다.\n\nCPO와의 비교\n\nCPO (Constrained Policy Optimization)는 제약 조건을 직접적으로 다루지만, 새로운 제약 조건이 추가될 때마다 문제의 복잡도가 증가하고, 튜닝이 어려워질 수 있습니다.\n반면, IPO는 logarithmic barrier function을 사용하기 때문에, 제약 조건을 쉽게 확장할 수 있으며 구현과 튜닝이 더 간단합니다.\n\nPoint Gather 실험에서의 제약 조건 확장\n\nPoint Gather 환경에서는 에이전트가 보상을 얻는 과정에서 다양한 제약 조건을 추가할 수 있습니다.\n실험에서 다양한 제약 조건을 추가하기 위해, 새로운 타입의 ball (제약 조건에 해당하는 오브젝트)을 도입할 수 있습니다.\n예를 들어, 기존의 bomb 외에 새로운 제약 조건을 나타내는 여러 종류의 ball을 추가하여, 에이전트가 이들을 피하면서도 최대한 많은 보상을 얻는 정책을 학습할 수 있습니다.\n이를 통해 다중 제약 조건 환경에서도 IPO의 성능을 평가할 수 있습니다.\n\n\n\n\n\n\n\n\n\n\n\n\n\n특징\nIPO\nCPO\n\n\n\n\n제약 조건 추가 용이성\n로그 배리어 항 추가만으로 가능\n복잡한 추가 작업과 튜닝 필요\n\n\n확장성\n간단하게 여러 제약 조건 확장 가능\n제약 조건 추가 시 복잡도 증가\n\n\nPoint Gather 실험 적용\n다양한 제약 조건 ball 추가 가능\n제약 조건 추가 시 성능 저하 위험"
  },
  {
    "objectID": "posts/paper/2024-11-10-ipo.html#stochastic-environment-effects",
    "href": "posts/paper/2024-11-10-ipo.html#stochastic-environment-effects",
    "title": "📃IPO 리뷰",
    "section": "3.6 Stochastic Environment Effects",
    "text": "3.6 Stochastic Environment Effects\n실세계 환경에서의 불확실성 및 랜덤 노이즈 추가 실험 실제 환경에서는 항상 불확실성(uncertainty)이 존재합니다. 에이전트의 행동 결과는 종종 랜덤 노이즈(random noise)에 의해 영향을 받습니다. 예를 들어, 바람, 센서 오류, 마찰 등의 예기치 못한 요인들이 시스템에 영향을 줄 수 있습니다. 해당 실험에서 행동(action)은 속도(velocity)와 진행 방향(heading)의 벡터로 정의되며, 값의 범위는 -1에서 1 사이입니다. (-1, 1) 범위의 벡터는 에이전트가 움직일 방향과 속도를 나타냅니다.\n실험에서는 평균 0의 랜덤 노이즈를 행동(action)에 추가하여 환경의 불확실성을 모사했습니다.\n\n노이즈의 분산(variance)은 세 가지 값으로 설정되었습니다:\n\n\\sigma^2 = 0.2\n\\sigma^2 = 0.5\n\\sigma^2 = 1.0\n\n\n\n\n\n\n\\sigma^2 = 0.5일 때도 학습이 성공적으로 수렴하는 것을 확인할 수 있었습니다.\n\n이는 에이전트가 일정 수준의 환경 불확실성에서도 안정적으로 정책을 학습할 수 있음을 보여줍니다.\n\n\\sigma^2 = 1.0의 경우, 노이즈가 커져 학습이 불안정해질 가능성이 있으며, 이는 추가 실험에서 확인할 필요가 있습니다.\n실제 환경의 불확실성을 반영하기 위해 랜덤 노이즈를 추가하는 것은 강화 학습의 강건성(robustness) 평가에 중요한 역할을 합니다.\n적절한 수준의 노이즈(\\sigma^2 = 0.5)에서는 학습이 안정적으로 진행되었으며, 에이전트가 다양한 환경 변동에도 잘 적응할 수 있음을 확인했습니다."
  },
  {
    "objectID": "note.html",
    "href": "note.html",
    "title": "Note",
    "section": "",
    "text": "📘 Diary | 🌎 Language | 📝 Study\n\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n\n\n\n\n\nDate\n\n\nTitle\n\n\n\n\n\n\nJan 7, 2025\n\n\n📝Python과 C++ 노드를 모두 포함하는 ROS2 패키지 생성\n\n\n\n\nJan 6, 2025\n\n\n📝Linx Cheet Sheet\n\n\n\n\nJan 5, 2025\n\n\n📘Goodbye 2024\n\n\n\n\nDec 19, 2024\n\n\n📝__init__ VS. __call__\n\n\n\n\nOct 9, 2024\n\n\n📘Geultto 10th Start\n\n\n\n\nJan 15, 2024\n\n\n📝Operating System 001\n\n\n\n\nDec 1, 2023\n\n\n📘Geultto 9th Start\n\n\n\n\nAug 31, 2023\n\n\n🌎Casual English Phrases 010\n\n\n\n\nJul 7, 2023\n\n\n📘Geultto 8th End\n\n\n\n\nJul 5, 2023\n\n\n🌎Casual English Phrases 009\n\n\n\n\nMay 27, 2023\n\n\n📘Github Starstruck 128\n\n\n\n\nApr 5, 2023\n\n\n🌎Casual English Phrases 008\n\n\n\n\nMar 31, 2023\n\n\n🌎Casual English Phrases 007\n\n\n\n\nMar 30, 2023\n\n\n🌎Casual English Phrases 006\n\n\n\n\nMar 29, 2023\n\n\n🌎Casual English Phrases 005\n\n\n\n\nFeb 2, 2023\n\n\n📘Geultto 8th Start\n\n\n\n\nOct 29, 2022\n\n\n📘Gueltto 7th End\n\n\n\n\nOct 24, 2022\n\n\n🌎IT English Experssions 004\n\n\n\n\nOct 17, 2022\n\n\n🌎Casual English Phrases 003\n\n\n\n\nOct 11, 2022\n\n\n🌎Casual English Phrases 002\n\n\n\n\nOct 3, 2022\n\n\n🌎Casual English Phrases 001\n\n\n\n\nSep 4, 2022\n\n\n📘2022 상반기 회고\n\n\n\n\nMay 6, 2022\n\n\n📘Geultto 7th Start\n\n\n\n\nJan 31, 2021\n\n\n📘2021 TU Berlin Winter Course 수강 후기\n\n\n\n\nJan 3, 2021\n\n\n📘Goodbye 2020\n\n\n\n\nJan 3, 2021\n\n\n📘Hello 2021\n\n\n\n\n\nNo matching items"
  }
]