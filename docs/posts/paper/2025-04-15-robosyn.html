<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.24">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-04-15">
<meta name="description" content="In-Hand Manipulation with Visuotactile Sensing">

<title>📃Robot Synesthesia 리뷰 – Curieux.JY</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../profile.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-1e438c382a17f6d88d3993662a872df6.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-a37c72dd2dbac68997fcdc15a3622e78.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-84849963b3550dada6ad14ca0722a3fc.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-1da3f21b942a7af985a78b7d88029ecb.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-84849963b3550dada6ad14ca0722a3fc.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-2NVZN2MJZT"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-2NVZN2MJZT', { 'anonymize_ip': true});
</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    window.setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      window.setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    window.hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(darkModeDefault) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const darkModeDefault = false;
    document.querySelector('link.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !window.hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
    };
    // Switch to dark mode if need be
    if (window.hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Curieux.JY</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../post.html"> 
<span class="menu-text">Post</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../note.html"> 
<span class="menu-text">Note</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">Jung Yeon Lee</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/curieuxjy"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#brief-review" id="toc-brief-review" class="nav-link active" data-scroll-target="#brief-review"><span class="header-section-number">1</span> Brief Review</a></li>
  <li><a href="#detail-review" id="toc-detail-review" class="nav-link" data-scroll-target="#detail-review"><span class="header-section-number">2</span> Detail Review</a></li>
  <li><a href="#논문-리뷰-robot-synesthesia-in-hand-manipulation-with-visuotactile-sensing" id="toc-논문-리뷰-robot-synesthesia-in-hand-manipulation-with-visuotactile-sensing" class="nav-link" data-scroll-target="#논문-리뷰-robot-synesthesia-in-hand-manipulation-with-visuotactile-sensing"><span class="header-section-number">3</span> [논문 리뷰] Robot Synesthesia: In-Hand Manipulation with Visuotactile Sensing</a>
  <ul class="collapse">
  <li><a href="#소개-및-배경" id="toc-소개-및-배경" class="nav-link" data-scroll-target="#소개-및-배경"><span class="header-section-number">3.1</span> 소개 및 배경</a></li>
  <li><a href="#방법론-methodology" id="toc-방법론-methodology" class="nav-link" data-scroll-target="#방법론-methodology"><span class="header-section-number">3.2</span> 방법론 (Methodology)</a>
  <ul class="collapse">
  <li><a href="#시각-촉각-공감각-표현-visuotactile-synesthesia-representation" id="toc-시각-촉각-공감각-표현-visuotactile-synesthesia-representation" class="nav-link" data-scroll-target="#시각-촉각-공감각-표현-visuotactile-synesthesia-representation"><span class="header-section-number">3.2.1</span> 1. <strong>시각-촉각 공감각 표현</strong> – Visuotactile Synesthesia Representation</a></li>
  <li><a href="#학습-구조-및-네트워크-구성" id="toc-학습-구조-및-네트워크-구성" class="nav-link" data-scroll-target="#학습-구조-및-네트워크-구성"><span class="header-section-number">3.2.2</span> 2. <strong>학습 구조 및 네트워크 구성</strong></a></li>
  </ul></li>
  <li><a href="#네트워크-입출력-구조-상세-분석" id="toc-네트워크-입출력-구조-상세-분석" class="nav-link" data-scroll-target="#네트워크-입출력-구조-상세-분석"><span class="header-section-number">3.3</span> 네트워크 입출력 구조 상세 분석</a></li>
  <li><a href="#observation의-구성-및-차원-입력" id="toc-observation의-구성-및-차원-입력" class="nav-link" data-scroll-target="#observation의-구성-및-차원-입력"><span class="header-section-number">3.4</span> 1. Observation의 구성 및 차원 (입력)</a>
  <ul class="collapse">
  <li><a href="#시각-점군-visual-point-cloud" id="toc-시각-점군-visual-point-cloud" class="nav-link" data-scroll-target="#시각-점군-visual-point-cloud"><span class="header-section-number">3.4.1</span> (1) 시각 점군 (Visual Point Cloud)</a></li>
  <li><a href="#촉각-점군-tactile-point-cloud" id="toc-촉각-점군-tactile-point-cloud" class="nav-link" data-scroll-target="#촉각-점군-tactile-point-cloud"><span class="header-section-number">3.4.2</span> (2) 촉각 점군 (Tactile Point Cloud)</a></li>
  <li><a href="#로봇-손-증강-점군-augmented-robot-hand-point-cloud" id="toc-로봇-손-증강-점군-augmented-robot-hand-point-cloud" class="nav-link" data-scroll-target="#로봇-손-증강-점군-augmented-robot-hand-point-cloud"><span class="header-section-number">3.4.3</span> (3) 로봇 손 증강 점군 (Augmented Robot Hand Point Cloud)</a></li>
  <li><a href="#추가-상태-정보-additional-state-vector" id="toc-추가-상태-정보-additional-state-vector" class="nav-link" data-scroll-target="#추가-상태-정보-additional-state-vector"><span class="header-section-number">3.4.4</span> (4) 추가 상태 정보 (Additional State Vector)</a></li>
  <li><a href="#observation-최종-정리" id="toc-observation-최종-정리" class="nav-link" data-scroll-target="#observation-최종-정리"><span class="header-section-number">3.4.5</span> Observation 최종 정리:</a></li>
  </ul></li>
  <li><a href="#action의-구성-및-차원-출력" id="toc-action의-구성-및-차원-출력" class="nav-link" data-scroll-target="#action의-구성-및-차원-출력"><span class="header-section-number">3.5</span> 2. Action의 구성 및 차원 (출력)</a>
  <ul class="collapse">
  <li><a href="#action-최종-정리" id="toc-action-최종-정리" class="nav-link" data-scroll-target="#action-최종-정리"><span class="header-section-number">3.5.1</span> Action 최종 정리:</a></li>
  </ul></li>
  <li><a href="#요약-정리" id="toc-요약-정리" class="nav-link" data-scroll-target="#요약-정리"><span class="header-section-number">3.6</span> 요약 정리 ✅</a></li>
  <li><a href="#실험-설정-및-결과" id="toc-실험-설정-및-결과" class="nav-link" data-scroll-target="#실험-설정-및-결과"><span class="header-section-number">3.7</span> 실험 설정 및 결과</a>
  <ul class="collapse">
  <li><a href="#실험-환경-및-작업-구성" id="toc-실험-환경-및-작업-구성" class="nav-link" data-scroll-target="#실험-환경-및-작업-구성"><span class="header-section-number">3.7.1</span> 1. 실험 환경 및 작업 구성</a></li>
  <li><a href="#주요-결과-및-성능-비교" id="toc-주요-결과-및-성능-비교" class="nav-link" data-scroll-target="#주요-결과-및-성능-비교"><span class="header-section-number">3.7.2</span> 2. 주요 결과 및 성능 비교</a></li>
  </ul></li>
  <li><a href="#논의-및-분석" id="toc-논의-및-분석" class="nav-link" data-scroll-target="#논의-및-분석"><span class="header-section-number">3.8</span> 논의 및 분석</a></li>
  </ul></li>
  <li><a href="#reference" id="toc-reference" class="nav-link" data-scroll-target="#reference"><span class="header-section-number">4</span> Reference</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-page-left" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">📃Robot Synesthesia 리뷰</h1>
  <div class="quarto-categories">
    <div class="quarto-category">paper</div>
    <div class="quarto-category">manipulation</div>
    <div class="quarto-category">visuotactile</div>
  </div>
  </div>

<div>
  <div class="description">
    In-Hand Manipulation with Visuotactile Sensing
  </div>
</div>


<div class="quarto-title-meta column-page-left">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">April 15, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<blockquote class="blockquote">
<p>NeurIPS 2023 Workshop on Touch Processing: a new Sensing Modality for AI</p>
</blockquote>
<ul>
<li><a href=""></a></li>
</ul>
<ol type="1">
<li>🤖 이 논문에서는 시각 및 촉각 입력을 활용하여 로봇이 능숙한 손 안에서의 조작을 가능하게 하는 새로운 시스템인 Robot Synesthesia를 소개합니다.</li>
<li>🎨 인간의 촉각-시각 공감각에서 영감을 받아, Force-Sensing Resistor (FSR)의 촉각 데이터를 카메라의 포인트 클라우드와 결합하여 통합된 3D 공간에 표현하는 새로운 포인트 클라우드 기반 촉각 표현을 제안합니다.</li>
<li>🦾 시뮬레이터에서 학습된 정책이 실제 로봇 손으로 효과적으로 전달되어 복잡한 두 개의 공 회전과 새로운 물체에 대한 일반화가 가능함을 보여주며, 이는 시뮬레이션 환경 및 훈련 파이프라인에 대한 코드 공개로 이어질 것입니다.</li>
</ol>
<section id="brief-review" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Brief Review</h1>
<p>본 논문은 로봇 손의 정교한 조작을 위해 시각 및 촉각 센서 정보를 융합하는 새로운 접근 방식인 “Robot Synesthesia”를 제안합니다. 인간의 공감각에서 영감을 받아, 시각과 촉각 데이터를 통합된 3D 공간에 표현하여 로봇이 촉각적 상호 작용을 “볼” 수 있도록 합니다.</p>
<p><strong>핵심 방법론:</strong></p>
<ol type="1">
<li><p><strong>촉각-시각 공감각 (Tactile-Visual Synesthesia):</strong></p>
<ul>
<li>기존의 방식처럼 시각과 촉각 정보를 개별적으로 처리하고 특징을 추출한 후 결합하는 대신, FSR(Force-Sensing Resistor) 센서의 촉각 데이터를 카메라의 point cloud와 결합하여 3D 공간에 표현합니다. 이를 통해 로봇 링크, FSR 센서, 조작 대상 객체 간의 공간적 관계를 유지합니다.</li>
<li>구체적으로, 각 촉각 센서에서 신호(<span class="math inline">o_{t,i} = 1</span>)가 감지되면 센서 mesh에서 점들을 샘플링하여 촉각 point cloud <span class="math inline">P_{touch}^t</span> 를 생성합니다. 이 point cloud는 카메라 point cloud <span class="math inline">P_c^t</span> 및 로봇의 자기 수용 정보를 바탕으로 생성된 augmented point cloud <span class="math inline">P_a^t</span>와 결합됩니다.</li>
<li>각 point cloud의 유형을 구별하기 위해 one-hot vector가 각 점에 추가됩니다. point cloud의 크기는 <span class="math inline">N_c = 512</span>, <span class="math inline">N_a = 8n_{link}</span>, <span class="math inline">N_t = 8n_{touch}</span>로 설정됩니다 (<span class="math inline">n_{link}</span>: 로봇 링크 수, <span class="math inline">n_{touch}</span>: 활성화된 촉각 센서 수).</li>
<li>모든 point cloud는 로봇 손바닥 frame으로 변환되어 신경망에 입력됩니다.</li>
</ul></li>
<li><p><strong>교사-학생 (Teacher-Student) 학습 파이프라인:</strong></p>
<ul>
<li>고차원 입력 (point cloud)을 사용하는 강화 학습 (RL)의 데이터 비효율성 문제를 해결하기 위해 교사-학생 학습 방식을 사용합니다.</li>
<li><strong>교사 정책 (Teacher Policy):</strong> 낮은 차원의 상태 정보를 사용하여 PPO (Proximal Policy Optimization) 알고리즘으로 학습됩니다. 입력은 로봇 joint 위치 <span class="math inline">q_t</span>, 이진 촉각 신호 <span class="math inline">o_t</span>, 회전축 <span class="math inline">k</span>, 이전 목표 위치 <span class="math inline">\hat{q}_t</span>, 객체의 위치 <span class="math inline">x_t</span>, 속도 <span class="math inline">v_t</span>, 각속도 <span class="math inline">w_t</span>, 그리고 객체 모양 특징 임베딩 <span class="math inline">f</span>입니다.</li>
<li><strong>학생 정책 (Student Policy):</strong> 교사 정책의 행동을 모방하도록 학습됩니다. 입력은 로봇 joint 위치 <span class="math inline">q_t</span>, 이진 촉각 신호 <span class="math inline">o_t</span>, 회전축 <span class="math inline">k</span>, 이전 목표 위치 <span class="math inline">\hat{q}_t</span>, 카메라 point cloud <span class="math inline">P_c^t</span>, augmented point cloud <span class="math inline">P_a^t</span>, 그리고 제안된 촉각 point cloud <span class="math inline">P_{touch}^t</span> 입니다. PointNet [3]을 사용하여 point cloud를 인코딩하고, latent vector를 다른 입력과 함께 MLP에 공급합니다.</li>
<li>2단계 distillation 파이프라인을 사용합니다. 먼저 교사 데이터셋 <span class="math inline">D</span>를 수집하고 BC (Behavior Cloning)를 사용하여 학생 정책 네트워크를 사전 학습합니다. 그 후 DAgger (Dataset Aggregation)를 사용하여 네트워크를 fine-tuning합니다.</li>
</ul></li>
<li><p><strong>보상 함수 (Reward Function):</strong></p>
<ul>
<li>견고하고 전이 가능한 in-hand 회전을 위해 여러 요소의 가중 조합으로 구성된 보상 함수를 사용합니다.</li>
<li><span class="math inline">r_t = c_1r_{rot} + c_2r_{vel} + c_3r_{dist} + c_4r_{torq} + c_5r_{work} + c_6r_{ctrl}</span></li>
<li><span class="math inline">r_{rot}</span>: 객체의 회전 각도를 보상합니다.</li>
<li><span class="math inline">r_{vel}</span>: 객체의 선형 속도를 penalize하여 객체를 이동시키는 움직임을 억제합니다.</li>
<li><span class="math inline">r_{dist}</span>: 객체와 손가락 끝 사이의 거리에 대한 감소 함수입니다.</li>
<li><span class="math inline">r_{torq}</span>: 큰 torque를 penalize합니다.</li>
<li><span class="math inline">r_{work}</span>: 컨트롤러의 work를 penalize합니다.</li>
<li><span class="math inline">r_{ctrl}</span>: 명령 목표와 실제 로봇 모션 간의 제어 오류를 penalize합니다.</li>
<li>객체가 손에서 떨어질 경우 큰 penalty를 부여합니다.</li>
</ul></li>
</ol>
<p>본 논문에서는 다양한 실험을 통해 Robot Synesthesia의 효과를 검증하고, 시뮬레이션에서 학습된 정책이 실제 로봇에 성공적으로 전이될 수 있음을 보여줍니다. 특히, 복잡한 double-ball 회전 작업과 새로운 객체에 대한 일반화 능력을 입증했습니다. 또한, PointNet의 중간 레이어를 시각화하여 제안된 촉각 표현이 PointNet이 action 예측에 중요한 fingertip, 객체 표면, 촉각 점과 같은 중요 지점을 식별하는 데 도움이 된다는 것을 보여줍니다.</p>
</section>
<section id="detail-review" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Detail Review</h1>
</section>
<section id="논문-리뷰-robot-synesthesia-in-hand-manipulation-with-visuotactile-sensing" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> [논문 리뷰] Robot Synesthesia: In-Hand Manipulation with Visuotactile Sensing</h1>
<section id="소개-및-배경" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="소개-및-배경"><span class="header-section-number">3.1</span> 소개 및 배경</h2>
<p>현대 로봇 손으로 물체를 조작하는 <strong>손내 조작(in-hand manipulation)</strong> 과제에서는 <strong>시각</strong>과 <strong>촉각</strong>의 결합이 필수적입니다. 예를 들어 사람이 바늘에 실을 끼울 때, 먼저 <strong>시각 정보</strong>로 바늘 구멍 위치를 찾고 실의 방향을 맞추지만, 막상 실을 끼우는 순간에는 <strong>촉각 정보</strong>로 보이지 않는 실 끝 위치를 감지하여 안내합니다. 이처럼 사람은 시각과 촉각을 자연스럽게 통합하여 복잡한 작업을 수행하지만, 로봇에게 동일한 수준의 <strong>시너지(synergy)</strong>를 구현하는 것은 큰 도전입니다. 기존 로봇 연구에서는 시각 센서(예: 카메라)와 촉각 센서(예: 압력 센서)의 <strong>데이터 형태 차이</strong> 때문에 두 감각을 효과적으로 통합하기 어렵다는 한계가 있었습니다. 시각 데이터는 고해상도의 <strong>풍부한 환경정보</strong>를 제공하는 반면, 촉각 데이터는 국소 부위의 <strong>희소한 접촉정보</strong>만을 제공하기 때문에 하나의 <strong>neural network</strong>에 두 종류의 입력을 함께 학습시키기가 매우 까다롭습니다. 또한 복잡한 손내 조작 정책을 학습하려면 방대한 데이터가 필요한데, 이를 시뮬레이션으로부터 얻어 학습한 후 실제 로봇에 <strong>이식(sim-to-real)</strong> 하는 과정에서도 시각과 촉각 각각의 <strong>도메인 차이(domain gap)</strong>를 동시에 극복해야 하는 어려움이 존재합니다. 이러한 이유로 과거에는 시각과 촉각을 따로 처리한 후 나중에 결합하거나, 아예 한 가지 감각에 의존하는 등 제한적인 접근이 많았습니다.</p>
<p>이 논문 <strong>“Robot Synesthesia: In-Hand Manipulation with Visuotactile Sensing”</strong>은 이러한 배경에서 제안된 연구로, <strong>시각-촉각 동시 통합</strong>을 통해 로봇 손내 조작 능력을 향상시키고자 합니다. 특히 인간의 <strong>공감각(synesthesia)</strong> 개념에 착안하여, 촉각 신호를 시각적으로 <strong>마치 하나의 감각처럼</strong> 표현하는 새로운 표현 방법을 선보입니다. 저자들은 이를 <strong>로봇 공감각(Robot Synesthesia)</strong>이라고 부르며, 로봇이 자신의 <strong>촉각을 눈으로 보듯이</strong> 인식하게 만드는 것이 핵심 아이디어입니다. 이 방법을 통해 촉각과 시각 정보를 본질적으로 <strong>하나의 통합된 형태</strong>로 취급함으로써, 두 감각 간의 관계 학습을 용이하게 하고 시뮬레이션에서 실제로 지식 이전 시 발생하는 오류를 줄이는 효과를 얻습니다. 또한 별도의 실제 데이터 수집 없이 <strong>시뮬레이션 학습만으로</strong> 실제 로봇 손내 조작을 성공적으로 수행했다는 점에서 실용적인 의의도 있습니다. 본 포스트에서는 해당 논문의 동기와 배경, 제안된 방법론, 실험 설정과 결과, 그리고 그에 대한 논의 및 시사점을 석사 수준의 기술적 깊이로 정리합니다.</p>
<p><em>Figure 1: <strong>Robot Synesthesia</strong> 개념 데모. (좌) 시뮬레이션 학습 단계: Allegro 로봇 손이 인공 물체들을 다양한 축으로 회전시키며 훈련되고 있으며, 붉은 점으로 촉각 센서 접촉 지점을 나타낸다. (우) 학습된 정책을 실제 로봇에 적용한 결과: 휠-렌치(위 열), 동일한 두 공(가운데 열), 복잡한 형상의 물체들(아래 열)을 손 내에서 회전시키는 모습을 보여준다. 시뮬레이션에서 학습한 정책을 실제 로봇 손에 별도 추가학습 없이 이식했음을 강조한다.</em></p>
</section>
<section id="방법론-methodology" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="방법론-methodology"><span class="header-section-number">3.2</span> 방법론 (Methodology)</h2>
<section id="시각-촉각-공감각-표현-visuotactile-synesthesia-representation" class="level3" data-number="3.2.1">
<h3 data-number="3.2.1" class="anchored" data-anchor-id="시각-촉각-공감각-표현-visuotactile-synesthesia-representation"><span class="header-section-number">3.2.1</span> 1. <strong>시각-촉각 공감각 표현</strong> – Visuotactile Synesthesia Representation</h3>
<p>이 논문의 가장 큰 기여 중 하나는 시각과 촉각 데이터를 하나의 <strong>공통 표현(common representation)</strong>으로 합치는 <strong>Visuotactile Synesthesia</strong> 기법입니다. 일반적으로 로봇의 촉각 센서는 여러 지점에 부착되어 각 지점의 <strong>이진 접촉 여부</strong> 또는 압력을 출력합니다. 저자들은 이러한 촉각 정보를 로봇 손의 3차원 공간 상에 <strong>점군(point cloud)</strong>으로 투영함으로써 마치 <strong>시각적인 형태</strong>로 표현했습니다. 구체적으로, 로봇 손바닥과 손가락 마디 등에 장착된 <strong>FSR(Force Sensing Resistor) 압력 센서</strong> 16개로부터 접촉을 측정하고, <strong>접촉이 발생한 센서 위치의 로봇 손 메쉬 표면 위에 점들을 샘플링</strong>하여 <strong>촉각 점군</strong>을 생성합니다. 이렇게 생성된 촉각 점군은 해당 순간 로봇 손이 어디서 물체를 만지고 있는지 3D 좌표상에 표시해주는 역할을 합니다. 동시에 로봇 손 옆에 설치된 <strong>깊이 카메라(depth camera)</strong> (Microsoft Azure Kinect)를 통해 물체와 손 주변의 <strong>환경 점군</strong>을 얻습니다. 이 카메라로부터 얻은 <strong>시각 점군</strong>은 물체의 겉모습과 위치를 알려주며, 추가로 로봇의 현재 관절각 등 <strong>자기센서(proprioception)</strong> 정보를 이용해 로봇 손 자체의 3D 메쉬 표면에서도 일부 점들을 샘플링한 <strong>증강 점군(augmented point cloud)</strong>도 생성합니다. 이렇게 하면 로봇 손의 현재 형태와 물체의 상대적 위치 관계까지 공간적으로 표현할 수 있습니다. 최종적으로 <strong>카메라 점군</strong>, <strong>로봇 손 증강 점군</strong>, <strong>촉각 점군</strong>을 모두 하나의 <strong>합쳐진 점군</strong>으로 결합하여 네트워크에 입력합니다. 이때 각 점이 어떤 출처(카메라, 로봇, 촉각)에서 온 것인지 구분할 수 있도록 점마다 <strong>원-핫(one-hot) 벡터 특징</strong>을 추가하여 구별되게 표시합니다. 이러한 3차원 공간 통합 표현 덕분에, 로봇은 마치 <strong>자신의 촉각 정보를 눈으로 보듯이</strong> 전체적인 상태를 인식할 수 있게 됩니다. 저자들은 이처럼 촉각을 시각화하는 통합 감각 방식을 <strong>“로봇 공감각(Robot Synesthesia)”</strong>이라고 명명하였습니다.</p>
<p>이 접근법의 장점은 두 가지입니다. 첫째, 초기에 두 감각을 <strong>자연스럽게 융합</strong>된 형태로 입력받기 때문에, 별도의 late-fusion (나중 단계에서 특징 결합) 없이 <strong>학습 단계부터</strong> 네트워크가 시각-촉각 간 상호관계를 쉽게 학습합니다. 이는 단순히 “이미지 특징 + 촉각 값” 식으로 벡터를 이을 때 발생하는 학습상의 모호성을 줄여 줍니다. 둘째, 시뮬레이션에서 학습한 모델을 실제로 옮길 때 <strong>모달리티 간 도메인 차이</strong>가 누적되어 오류가 커지는 문제를 완화합니다. 즉, 촉각과 시각을 별개로 다루면 시뮬레이터의 촉각 모델 오차와 시뮬레이터의 렌더링 오차가 각각 존재하여 실제와 괴리가 생기는데, 두 감각을 하나로 <strong>일체화된 형태</strong>로 다루면 이러한 오차들이 어느 정도 상쇄되어 <strong>sim-to-real 간격</strong>이 줄어드는 효과가 있습니다. 실제로 저자들은 <strong>RGB 카메라 영상</strong> 대신 <strong>깊이 카메라의 점군 데이터</strong>를 시각 입력으로 선택했는데, 이는 시뮬레이션에서의 점군과 실제에서의 점군 모양이 매우 유사하여 도메인 갭이 작기 때문입니다. 아래 그림에서 볼 수 있듯이, 시뮬레이터 상의 로봇-물체 장면을 깊이 카메라로 찍어 얻은 점군과 실제 로봇에서 동일한 동작을 했을 때 얻은 점군은 형태적으로 거의 일치하지만, RGB 이미지의 경우 시뮬레이터와 실제 간 격차가 큽니다. 이러한 이유로 <strong>point cloud 기반 시각 정보</strong>를 선택함으로써 더욱 원활한 지식 이전을 도모했습니다.</p>
<p><em>Figure 2: 시뮬레이션 vs 실제의 관측 비교. (좌) 시뮬레이터 내부에서 본 장면(RGB 영상)과 그로부터 얻은 물체+손 점군. (우) 실제 로봇에서 동일한 동작을 실행할 때의 RGB 카메라 영상과 깊이 카메라 점군. RGB 이미지의 경우 시뮬레이터(왼쪽 사진)와 실제(오른쪽 사진) 배경 등이 확연히 다르지만, 점군 형태는 시뮬레이션과 실제가 거의 일치함을 알 수 있다. 점군 기반 관측을 활용하면 시뮬레이션 학습 결과를 실제에 옮길 때 이러한 차이를 최소화할 수 있다.</em></p>
</section>
<section id="학습-구조-및-네트워크-구성" class="level3" data-number="3.2.2">
<h3 data-number="3.2.2" class="anchored" data-anchor-id="학습-구조-및-네트워크-구성"><span class="header-section-number">3.2.2</span> 2. <strong>학습 구조 및 네트워크 구성</strong></h3>
<p>로봇 공감각 표현을 통해 시각-촉각 통합 데이터를 얻었다면, 이를 활용해 <strong>강화학습(Reinforcement Learning, RL)</strong>으로 손내 조작 정책을 학습해야 합니다. 그러나 점군 형태의 고차원 관측을 직접 사용하여 RL을 수행하면 학습이 매우 어려울 수 있습니다. 이를 해결하기 위해 저자들은 <strong>교사-학생 학습(teacher-student training)</strong> 구조를 도입했습니다. 이는 <strong>“크로스 앰바디먼트 프리트레이닝(Cross-Embodiment Pretraining)”</strong>의 일종으로 볼 수 있는데, 초기에는 <strong>교사 정책(teacher policy)</strong>이 <strong>보다 쉬운 표현 공간(easier embodiment)</strong>에서 학습되고, 이후 이를 <strong>학생 정책(student policy)</strong>이 <strong>실제 사용할 센서 구성(actual embodiment)</strong>으로 모사하도록 함으로써 두 다른 환경 간 지식을 이전합니다.</p>
<p>먼저 <strong>교사 정책</strong>은 시뮬레이터 상에서 비교적 접근하기 쉬운 <strong>상태 표현</strong>을 사용하여 학습됩니다. 구체적으로 교사 정책은 로봇의 <strong>관절 상태(proprioception)</strong>, 각 FSR 촉각 센서의 <strong>이진 접촉 여부</strong>, 물체의 <strong>정확한 포즈(pose)</strong>와 <strong>형상 임베딩(shape feature)</strong> 정보를 모두 관측으로 사용합니다. 즉, 시뮬레이터이기에 가능한 <strong>물체의 위치와 모양에 대한 완전한 정보</strong>까지 포함하여 RL로 최적 정책을 학습하는 것입니다. 이렇게 하면 높은 차원의 이미지나 점군을 직접 다루지 않아도 되므로 학습 난이도가 낮아지고, 비교적 적은 샘플로도 좋은 정책을 얻을 수 있습니다. 논문에서는 교사 정책을 <strong>PPO 알고리즘</strong>을 통해 훈련하였고, actor-critic 구조의 <strong>MLP (Multi-Layer Perceptron)</strong> 네트워크로 정책함수를 표현했습니다. 그 결과 교사 정책은 주어진 작업들에 대해 안정적인 성능을 얻었으며, 이 교사가 생성하는 시연 데이터를 다음 단계에 활용합니다.</p>
<p><em>Figure 3: <strong>교사-학생 정책 학습 파이프라인</strong>. (상단) <strong>교사(Teacher)</strong> 정책은 로봇의 관절 상태, 이진 촉각 신호, 물체의 포즈와 사전 추출한 형상 특징 등을 입력으로 받는 actor-critic 신경망이다. 교사 정책은 시뮬레이터에서 강화학습(PPO)으로 훈련되며, 비교적 쉬운 입력을 사용하기 때문에 학습이 용이하다. (하단) 훈련된 교사 정책으로부터 <strong>학생(Student)</strong> 정책을 학습시킨다. 학생 정책은 실제와 동일한 센서 입력(프로프리오셉션, 이진 촉각, 카메라 점군+증강 점군+촉각 점군 통합)을 사용하며, 통합 점군 입력은 <strong>PointNet</strong> 기반 encoder로 임베딩 벡터를 추출한 후 다른 상태 입력과 결합하여 actor MLP에 전달된다. 교사 정책의 행동을 모방학습으로 전달받아 초기 학습을 하고, 이후 DAgger 알고리즘으로 지속적으로 교정한다. 붉은 색 실선 화살표는 교사 정책의 결정(action)을 학생이 모사하는 과정을 나타낸다.</em></p>
<p>다음으로 <strong>학생 정책</strong>은 실제 로봇이 사용할 <strong>시각-촉각 점군 관측</strong>을 입력으로 하여 학습됩니다. 학생 정책의 입력은 위에서 설명한 <strong>로봇 공감각 점군(시각+촉각 통합)</strong>과 로봇 관절 상태, 이전 스텝의 명령 등의 정보로 구성됩니다. 학생 정책 네트워크는 <strong>PointNet 기반의 점군 encoder</strong>를 사용하여 수천 개의 점들을 저차원 임베딩 벡터로 변환하고, 여기에 로봇의 관절각, 접촉 센서 이진값 등의 추가 상태를 concatenate하여 <strong>Actor MLP</strong>에 입력합니다. (학생 정책 단계에서는 순수한 RL로 학습하는 것이 아니라, 이미 학습된 교사 정책을 모방하여 학습하므로 별도의 critic 네트워크는 두지 않았습니다.)</p>
<p>학생 정책의 학습은 <strong>모방 학습(Imitation Learning)</strong> 기법을 통해 이루어집니다. 먼저 시뮬레이터에서 학습된 교사 정책을 이용하여 대량의 <strong>상태-행동 데이터셋</strong> D를 수집합니다 (논문에서는 총 512만 <strong>transition</strong>을 교사로부터 모았습니다). 그런 다음 이 데이터로 학생 정책을 <strong>Behavior Cloning (BC)</strong>, 즉 <strong>행동 복제 학습</strong>으로 사전 학습시킵니다. 이렇게 1단계로 초기 학생 정책을 얻은 후, 2단계로는 <strong>DAgger (Dataset Aggregation) 알고리즘</strong>을 적용하여 학생 정책을 미세 조정합니다. DAgger 단계에서는 현재 학생 정책이 수행하다가 실패할 때 교사 정책으로부터 정답 행동을 다시 받아서 데이터에 추가하고 학생을 업데이트함으로써, 학생 정책이 자신의 실수에 대해 교정학습을 할 수 있게 합니다. 이러한 2단계 distillation(지식 증류) 과정으로 최종 <strong>시각-촉각 기반 정책</strong>이 완성됩니다. 정리하면, <strong>교사-학생 파이프라인</strong>을 통해 고차원 <strong>visuotactile</strong> 입력 공간에서의 학습을 간접적으로 수행함으로써 학습 효율과 성능을 높인 것입니다. 이 방식은 서로 다른 센서 구성(embodiment)을 연결하는 일종의 <strong>교차 몸체 사전학습(cross-embodiment pretraining)</strong>이라고 볼 수 있으며, 실제 로봇에서 필요한 <strong>시각+촉각 정책을 효과적으로 얻는 핵심 기술</strong>입니다.</p>
</section>
</section>
<section id="네트워크-입출력-구조-상세-분석" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="네트워크-입출력-구조-상세-분석"><span class="header-section-number">3.3</span> 네트워크 입출력 구조 상세 분석</h2>
<p>이 논문에서는 강화학습(RL)을 통해 로봇이 물체를 손 안에서 조작하는 작업(in-hand manipulation)을 수행합니다. RL에서는 입력 관측 정보(observation)와 출력 행동(action)의 차원을 정확하게 설계하는 것이 중요합니다.</p>
<hr>
</section>
<section id="observation의-구성-및-차원-입력" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="observation의-구성-및-차원-입력"><span class="header-section-number">3.4</span> 1. Observation의 구성 및 차원 (입력)</h2>
<p>이 논문에서 제안하는 로봇 정책의 observation은 크게 다음의 4가지로 구성됩니다 :</p>
<section id="시각-점군-visual-point-cloud" class="level3" data-number="3.4.1">
<h3 data-number="3.4.1" class="anchored" data-anchor-id="시각-점군-visual-point-cloud"><span class="header-section-number">3.4.1</span> (1) 시각 점군 (Visual Point Cloud)</h3>
<ul>
<li><strong>센서</strong>: Azure Kinect RGB-D 카메라의 깊이 데이터로부터 추출된 점군.</li>
<li><strong>데이터 형태</strong>: 환경(물체+배경)을 나타내는 3D 점군 데이터.</li>
<li><strong>차원</strong>: 시각 점군의 경우 일반적으로 <strong>약 300개의 점들</strong>로 샘플링되어 사용됩니다 .</li>
<li>각 점의 특징 벡터는 총 <strong>6차원</strong>으로 구성됩니다:
<ul>
<li><strong>3D 좌표값</strong>: (x, y, z)</li>
<li><strong>원핫 인코딩</strong> (출처를 구분하기 위해): <code>[1, 0, 0]</code> → 시각 카메라 점군임을 나타냄.</li>
</ul></li>
</ul>
<p>따라서 시각 점군 데이터의 전체 차원은 약 <strong>(300, 6)</strong> 정도로 표현됩니다.</p>
<hr>
</section>
<section id="촉각-점군-tactile-point-cloud" class="level3" data-number="3.4.2">
<h3 data-number="3.4.2" class="anchored" data-anchor-id="촉각-점군-tactile-point-cloud"><span class="header-section-number">3.4.2</span> (2) 촉각 점군 (Tactile Point Cloud)</h3>
<ul>
<li><strong>센서</strong>: Allegro 로봇 핸드의 16개의 FSR (Force Sensitive Resistor) 촉각 센서.</li>
<li><strong>데이터 형태</strong>: 이진 촉각 값 (접촉 여부)에서 생성한 3D 점군.<br>
로봇 손 메쉬 표면에서 접촉이 발생한 센서 위치를 기준으로 점들을 샘플링하여 생성합니다.</li>
<li><strong>차원</strong>: 촉각 점군은 보통 <strong>약 80개의 점들</strong>로 샘플링됩니다 .</li>
<li>각 점의 특징 벡터 역시 <strong>6차원</strong>:
<ul>
<li><strong>3D 좌표값</strong>: (x, y, z)</li>
<li><strong>원핫 인코딩</strong>: <code>[0, 1, 0]</code> → 촉각 점군임을 나타냄.</li>
</ul></li>
</ul>
<p>따라서 촉각 점군의 전체 차원은 약 <strong>(80, 6)</strong>이 됩니다.</p>
<hr>
</section>
<section id="로봇-손-증강-점군-augmented-robot-hand-point-cloud" class="level3" data-number="3.4.3">
<h3 data-number="3.4.3" class="anchored" data-anchor-id="로봇-손-증강-점군-augmented-robot-hand-point-cloud"><span class="header-section-number">3.4.3</span> (3) 로봇 손 증강 점군 (Augmented Robot Hand Point Cloud)</h3>
<ul>
<li><strong>센서</strong>: 로봇의 현재 관절 상태로부터 생성된 로봇 손의 자체 형태를 나타내는 점군.</li>
<li><strong>데이터 형태</strong>: 로봇 손 자체의 메쉬 모델에서 샘플링된 점군.</li>
<li><strong>차원</strong>: 이 역시 약 <strong>80개의 점들</strong>로 샘플링 .</li>
<li>각 점의 특징 벡터는 역시 <strong>6차원</strong>으로 구성:
<ul>
<li><strong>3D 좌표값</strong>: (x, y, z)</li>
<li><strong>원핫 인코딩</strong>: <code>[0, 0, 1]</code> → 로봇 손 점군임을 나타냄.</li>
</ul></li>
</ul>
<p>따라서 로봇 손 증강 점군의 전체 차원은 <strong>(80, 6)</strong>입니다.</p>
<hr>
</section>
<section id="추가-상태-정보-additional-state-vector" class="level3" data-number="3.4.4">
<h3 data-number="3.4.4" class="anchored" data-anchor-id="추가-상태-정보-additional-state-vector"><span class="header-section-number">3.4.4</span> (4) 추가 상태 정보 (Additional State Vector)</h3>
<p>점군 외에 별도로 제공되는 로봇 관절과 이전 액션에 대한 정보를 담은 추가 상태 벡터:</p>
<ul>
<li><strong>Proprioception (로봇 자기 감각)</strong>:
<ul>
<li>로봇 손의 관절각: Allegro 핸드는 <strong>16개의 자유도</strong>를 가집니다. 각 관절의 위치를 나타내는 <strong>16차원 벡터</strong> 사용 .</li>
<li>손의 관절 각속도는 포함되지 않고 위치만 포함되었습니다 .</li>
</ul></li>
<li><strong>Binary Tactile Vector</strong>:
<ul>
<li>16개의 촉각 센서에서 오는 이진 접촉 여부: <strong>16차원 벡터</strong> .</li>
</ul></li>
<li><strong>이전 행동(previous action)</strong>:
<ul>
<li>이전 행동 정보를 네트워크가 참조할 수 있도록 제공되며, 이는 역시 Allegro 핸드의 16개 관절 각도의 목표 위치로 표현된 <strong>16차원 벡터</strong> .</li>
</ul></li>
</ul>
<p>결과적으로, 추가 상태 정보는:</p>
<ul>
<li>관절 위치 (<strong>16</strong>) + 이진 촉각 신호 (<strong>16</strong>) + 이전 행동 (<strong>16</strong>)<br>
→ 총 <strong>48차원</strong>의 벡터로 구성됩니다.</li>
</ul>
<hr>
</section>
<section id="observation-최종-정리" class="level3" data-number="3.4.5">
<h3 data-number="3.4.5" class="anchored" data-anchor-id="observation-최종-정리"><span class="header-section-number">3.4.5</span> Observation 최종 정리:</h3>
<p>종합하면, 네트워크에 입력되는 Observation은 다음과 같은 형태로 최종 정리됩니다:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 52%">
<col style="width: 10%">
<col style="width: 19%">
<col style="width: 17%">
</colgroup>
<thead>
<tr class="header">
<th>입력 정보 (Observation)</th>
<th>점 개수</th>
<th>각 점의 특징</th>
<th>전체 차원</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>시각 점군 (Visual)</td>
<td>300</td>
<td>6차원</td>
<td>(300, 6)</td>
</tr>
<tr class="even">
<td>촉각 점군 (Tactile)</td>
<td>80</td>
<td>6차원</td>
<td>(80, 6)</td>
</tr>
<tr class="odd">
<td>로봇 손 증강 점군 (Robot Augmented)</td>
<td>80</td>
<td>6차원</td>
<td>(80, 6)</td>
</tr>
<tr class="even">
<td>추가 상태 정보 (Additional state)</td>
<td>-</td>
<td>-</td>
<td>(48,)</td>
</tr>
</tbody>
</table>
<p><strong>최종 Observation Dimension</strong>: <code>(460개의 점, 각 6차원) + 48차원 상태 벡터</code></p>
<hr>
</section>
</section>
<section id="action의-구성-및-차원-출력" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="action의-구성-및-차원-출력"><span class="header-section-number">3.5</span> 2. Action의 구성 및 차원 (출력)</h2>
<p>본 논문의 행동(action)은 Allegro 로봇 핸드의 <strong>16개 관절의 목표 각도</strong>입니다 .</p>
<ul>
<li><strong>차원</strong>: 행동의 차원은 정확히 <strong>16차원</strong>입니다.</li>
<li>각 액션 값은 다음 스텝에서 로봇의 각 관절이 이동해야 할 목표 관절 각도를 지정합니다.</li>
<li>실제 로봇에서는 목표 관절 각도로 이동하는 방식으로 모터 컨트롤이 이루어지며, 매 step마다 약 10Hz로 새로운 액션을 출력합니다 .</li>
</ul>
<section id="action-최종-정리" class="level3" data-number="3.5.1">
<h3 data-number="3.5.1" class="anchored" data-anchor-id="action-최종-정리"><span class="header-section-number">3.5.1</span> Action 최종 정리:</h3>
<table class="caption-top table">
<thead>
<tr class="header">
<th>출력 행동 (Action)</th>
<th>차원</th>
<th>값의 의미</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>로봇 관절 목표 각도</td>
<td>16</td>
<td>로봇 핸드의 각 관절의 목표 위치</td>
</tr>
</tbody>
</table>
<hr>
</section>
</section>
<section id="요약-정리" class="level2" data-number="3.6">
<h2 data-number="3.6" class="anchored" data-anchor-id="요약-정리"><span class="header-section-number">3.6</span> 요약 정리 ✅</h2>
<ul>
<li><strong>Observation</strong>:
<ul>
<li><strong>점군 입력</strong>: (총 460점, 각 6차원)</li>
<li><strong>추가 상태 입력</strong>: 48차원 벡터</li>
<li>점군은 PointNet 기반 Encoder를 거쳐 하나의 저차원 임베딩 벡터로 변환된 후, 상태 벡터와 결합됩니다.</li>
</ul></li>
<li><strong>Action</strong>:
<ul>
<li><strong>16차원 벡터</strong>: 각 값은 로봇의 16개 관절 목표 위치를 의미.</li>
</ul></li>
</ul>
<p>위와 같은 정확한 Observation 및 Action 차원을 이용해, 네트워크는 복잡한 in-hand manipulation 작업을 효율적으로 학습할 수 있습니다. 이를 통해 높은 Sim-to-Real 성능을 달성한 것이 이 논문의 핵심 기여입니다.</p>
</section>
<section id="실험-설정-및-결과" class="level2" data-number="3.7">
<h2 data-number="3.7" class="anchored" data-anchor-id="실험-설정-및-결과"><span class="header-section-number">3.7</span> 실험 설정 및 결과</h2>
<section id="실험-환경-및-작업-구성" class="level3" data-number="3.7.1">
<h3 data-number="3.7.1" class="anchored" data-anchor-id="실험-환경-및-작업-구성"><span class="header-section-number">3.7.1</span> 1. 실험 환경 및 작업 구성</h3>
<p><strong>환경:</strong> 실험에는 <strong>유니버설로봇 XArm6 로봇 팔</strong> 끝에 장착된 <strong>알레그로(Allegro) 로봇 핸드</strong>(4개의 손가락, 16자유도)가 사용되었습니다. 로봇 손바닥과 마디마디에는 16개의 FSR 촉각 센서가 부착되어 있으며, 이는 접촉시 아날로그 압력 값을 내지만 일정 임계값 이상이면 <strong>이진 접촉</strong>으로 간주합니다. 시각 센서는 로봇 손 앞을 향하도록 배치된 <strong>Azure Kinect 깊이 카메라</strong>로, RGB-D 데이터를 수집하되 알고리즘에는 <strong>Depth로 얻은 점군만 사용</strong>합니다. 시뮬레이션 환경은 NVIDIA IsaacGym으로 구현되어 실제와 동일한 로봇 모델과 물체 모델, 그리고 가상 깊이 카메라/촉각 센서를 갖춥니다. 시뮬레이터에서 접촉 센서는 실제와 동일하게 동작하도록 이진 신호로 모사되며, 제어 주기도 실제와 동일하게 10 Hz로 설정되었습니다. 학습된 정책은 <strong>추가 파인튜닝 없이</strong> 그대로 실제 로봇에 이식하여 검증되었습니다.</p>
<p><strong>작업(Task):</strong> 논문에서는 손내 물체 회전과 관련된 세 가지 벤치마크 작업에 초점을 맞춥니다:</p>
<ul>
<li><p><em>(i) Wheel-Wrench Rotation</em> – <strong>십자 렌치 회전:</strong> 자동차 바퀴 렌치처럼 십자형으로 교차된 막대를 손으로 쥐고, 한쪽 끝을 다 돌리면 다음 손잡이로 <strong>재파지(re-grasp)</strong>하여 연속 회전하는 과제입니다. 로봇은 현재 잡은 손잡이를 다 돌렸다면 <strong>시각적으로 새 손잡이 위치를 찾아</strong> 옮겨 잡아야 하며, 동시에 <strong>촉각으로 놓치지 않고</strong> 회전 힘을 가해야 합니다 (<a href="https://www.touchprocessing.org/2023/camera_ready/camera_ready_7.pdf#:~:text=successfully%20complete%20this%20task%2C%20the,Axis%20Rotation%3A%20This%20task"></a>). 이 작업은 한 손으로 연속 회전하기에 난이도가 높으며, 시각과 촉각 둘 다가 필수적으로 요구됩니다.</p></li>
<li><p><em>(ii) Double-Ball Rotation</em> – <strong>이중 공 회전:</strong> 동일한 크기의 공 두 개를 손으로 동시에 잡고 서로의 주위를 돌도록 회전시키는 작업입니다. 두 공은 똑같이 생겼으므로 시각적으로 구분이 어려울 수 있지만, 촉각만으로는 어느 공이 어디에 있는지 <strong>식별 불가</strong>합니다. 따라서 <strong>두 공의 상대적 위치를 파악하는 시각정보</strong>와 <strong>미끄러지지 않게 잡는 촉각정보</strong>의 결합이 핵심입니다. 이 작업은 손으로 두 개의 물체를 한꺼번에 다루어야 하므로 난이도가 매우 높습니다 (로봇 손가락의 작은 움직임으로는 두 공을 회전시키기에 부족하고, 크게 움직이면 공을 떨어뜨릴 위험이 있습니다).</p></li>
<li><p><em>(iii) Three-Axis Rotation</em> – <strong>3축 회전:</strong> 물체를 z축뿐 아니라 고정된 x축 또는 y축을 중심으로도 회전시키는 일반적인 회전 조작 작업입니다. 이 작업을 통해 로봇 손이 특정 축으로 물체를 돌리는 능력을 평가하며, 특히 학습 단계에서 보지 못한 <strong>다양한 모양의 물체들</strong>에 대해서도 일반화할 수 있는지 테스트합니다. 학습 시에는 단순 기하학 형태의 물체들을 사용하고, 테스트 시에는 실제 일상 물체들(예: 마커펜, 토마토 등)을 줘서 얼마나 <strong>모양 변화에 견고</strong>한지 확인했습니다.</p></li>
</ul>
<p>각 작업에 대해 시뮬레이터에서 교사 정책으로 충분히 학습한 후, 앞서 설명한 모방 학습을 거쳐 학생 정책을 얻었습니다. 최종 정책들을 아무 수정 없이 실제 로봇에 이식하여 평가를 진행했으며, <strong>성공적으로 물체를 떨어뜨리지 않고 회전시키는 시간</strong>(Time-to-Fall, TTF)과 <strong>누적 회전 각도</strong> 또는 <strong>회전 횟수</strong>(Cumulative Rotation) 등을 주요 성능 지표로 사용했습니다.</p>
</section>
<section id="주요-결과-및-성능-비교" class="level3" data-number="3.7.2">
<h3 data-number="3.7.2" class="anchored" data-anchor-id="주요-결과-및-성능-비교"><span class="header-section-number">3.7.2</span> 2. 주요 결과 및 성능 비교</h3>
<p><strong>시뮬레이션 단계 성능:</strong> 먼저 시뮬레이터 상에서 교사/학생 정책들의 성능을 비교한 결과, 제안된 시각-촉각 공감각 정책이 <strong>기존 방법들 대비 일관되게 우수한 성능</strong>을 보였습니다. 예를 들어 학습 곡선을 보면, <strong>Ours (제안 방식)</strong>가 같은 시간 내 <strong>Visual RL</strong>(시각점군+촉각을 직접 RL로 학습)보다 훨씬 빠르게 높은 보상에 도달했고, <strong>PS (Partial State)</strong>라고 불리는 이전 상태기반 방법보다도 최종 성능이 높았습니다. 이는 물체의 정밀한 조작에는 <strong>정확한 물체 상태 정보</strong>(교사가 가진 위치/모양 정보에 대응)가 매우 중요하며, 우리 방법이 비록 직접 물체 pose를 알지는 못하지만 공감각 점군을 통해 이를 효과적으로 추론하고 있기 때문이라고 분석됩니다. 또한 동일한 조건에서 단일 단계로 RL을 한 경우(Visual RL 등)보다 교사-학생 2단계 학습이 <strong>표현 학습 및 탐색 측면에서 유리</strong>함을 보여주었습니다.</p>
<p><strong>실제 로봇 평가:</strong> 가장 주목할 만한 결과는 <strong>한번도 실제 데이터로 학습하지 않은 정책을 실제 로봇에서 바로 사용</strong>했음에도 불구하고 높은 성능을 발휘했다는 점입니다. Table 3에 요약된 실제 실험 결과를 살펴보면, 제안된 <strong>Touch+Cam+Aug+Syn (시각+촉각 공감각)</strong> 정책이 모든 작업에서 다른 대조군들을 능가했습니다. 반면 카메라를 사용하지 않는 <strong>Non-visual RL</strong>이나 <strong>Touch-only</strong>와 같은 방식은 일부 간단한 경우 외에는 제대로 물체를 회전시키지 못했습니다. 대표적으로, <strong>휠-렌치 회전 작업</strong>의 경우 공감각 정책은 평균 <strong>1.54회전</strong>을 수행한 반면, 시각을 사용하지 않는 정책들은 <strong>0.25회전</strong>에 그쳤습니다. <strong>이중 공 회전</strong>에서는 공감각 정책이 약 <strong>11.9회전</strong>을 성공시켜 다른 방법들(<sub>7</sub>8회전)에 비해 확연히 더 많이 회전시켰으며, 물체를 놓치기까지 버틴 시간도 약 <strong>17초</strong>로 타 방법들(10~11초)에 비해 길었습니다. <strong>3축 회전 작업</strong>의 경우에도 z축 기준 회전에서 공감각 정책이 <strong>평균 10.2회전</strong>을 달성해 가장 우수했습니다. 일부 x축, y축 결과에서는 시각이 없는 방법들이 물체를 거의 돌리지 못하지만 오래 붙잡고 있는 반면(거의 움직이지 않으므로 떨어뜨릴 위험도 낮음), 시각이 있는 정책들은 <strong>능동적으로 물체를 회전</strong>시키려다 보니 때때로 더 빨리 놓치는 경우도 있었습니다. 그럼에도 불구하고 전반적으로 시각+촉각 결합 정책이 <strong>회전 각도를 크게 향상</strong>시켰으며, <strong>실제 환경에서 그 우수성이 더욱 두드러졌다</strong>는 것이 저자들의 분석입니다.</p>
<p>특히 재미있는 관찰은, <strong>시각 정보가 있는 정책</strong>은 실행 중에 물체가 손 바닥 중심에서 벗어나면 이를 <strong>감지하고 교정 동작</strong>을 취하는 반면, <strong>시각이 없는 정책</strong>은 물체가 어디로 가는지 모르기 때문에 학습된 반복 패턴대로만 움직여버려 물체가 이상한 위치에 걸려도 수정하지 못한다는 것입니다. 이러한 차이는 시각과 촉각의 통합이 왜 중요한지를 단적으로 보여줍니다. 시각이 있으면 로봇은 매 순간 물체의 <strong>글로벌 위치</strong>를 파악하여 전략을 미세하게 조정할 수 있고, 촉각이 있으면 <strong>로컬 접촉 상태</strong>를 느끼면서 힘 조절을 할 수 있기 때문에, 두 감각을 모두 가진 정책이 훨씬 <strong>유연하고 견고</strong>하게 동작합니다.</p>
<p>또한 <strong>No-Synesthesia</strong> 대조군과의 비교를 통해, 제안된 <strong>공감각 점군 표현의 효과</strong>를 검증했습니다. No-Synesthesia 정책은 시각 점군과 로봇 증강 점군은 사용하되 촉각 정보를 단순 이진값으로만 입력한 경우인데, 이 경우 공감각 정책 대비 성능이 떨어졌습니다. 이는 <strong>촉각을 공간적으로 표시한 점군 표현(Synesthesia)</strong>이 단순한 촉각 신호의 Concatenation보다 실제 조작에 유의미한 기여를 함을 시사합니다. 요컨대, <strong>우리 정책(Ours)</strong>은 모든 작업에서 다른 방식을 능가했고, 특히 <strong>시뮬레이션에서 무리 없이 동작하던 시각 전용 정책이 실제로 오면 성능이 저하되는 반면</strong>, 공감각 정책은 <strong>시뮬레이션과 실제 간 격차가 매우 작아</strong> 실제에서도 성능을 유지하거나 오히려 격차가 벌어지는 모습을 보였습니다. 이는 본 논문의 목표였던 <strong>시각-촉각 융합을 통한 강인한 Sim2Real 성능</strong>이 입증된 결과라고 볼 수 있습니다.</p>
</section>
</section>
<section id="논의-및-분석" class="level2" data-number="3.8">
<h2 data-number="3.8" class="anchored" data-anchor-id="논의-및-분석"><span class="header-section-number">3.8</span> 논의 및 분석</h2>
<p><strong>의의:</strong> <em>Robot Synesthesia</em>는 로봇이 복잡한 접촉-rich 작업에서 <strong>시각과 촉각을 자연스럽게 통합</strong>할 수 있는 새로운 방법론을 제시했습니다. 기존에는 두 감각의 데이터 형태 차이로 인해 병합이 쉽지 않았던 문제를, <strong>점군이라는 공통 포맷</strong>으로 변환함으로써 해결한 점이 특히 돋보입니다. 이를 통해 <strong>학습 효율</strong>과 <strong>성능</strong> 두 마리 토끼를 잡았는데, 교사-학생 구조를 적용하여 시뮬레이션에서 충분히 학습한 후 실제로 투입함으로써 <strong>실제 데이터 없이도</strong> 높은 성능을 달성했습니다. 이러한 접근은 향후 다양한 로봇 manipulation 분야에 응용될 수 있는 포괄적인 아이디어로서, 예를 들어 다른 형태의 촉각 센서나 로봇 플랫폼에도 적용하여 <strong>멀티모달 정책학습</strong>을 쉽게 할 수 있는 길을 열었습니다.</p>
<p><strong>강인성:</strong> 본 연구의 정책은 훈련에 사용하지 않은 새로운 물체들에도 어느 정도 일반화하여 동작할 수 있음을 보여주었습니다. 실제 실험에서 동그란 공으로 학습한 정책이 토마토나 감자 같은 전혀 다른 물체 쌍에도 적용되어 회전을 시도하는 등, <strong>모양 변화에 대한 강인성</strong>을 확인했습니다. 이는 점군 표현 덕분에 물체의 형태적 차이를 임베딩 공간에서 학습하도록 한 효과로 해석됩니다. 또한 시각 정보의 도입으로 정책이 보다 <strong>상황 적응적</strong>으로 변해, 물체가 미끄러질 때 재조정하는 등의 행동이 관찰된 점도 유의미합니다. 다만 일부 경우에 시각 기반 정책이 지나치게 적극적으로 움직이다 보니 물체를 빨리 떨어뜨리는 현상도 있었는데, 이는 <strong>안정성 vs.&nbsp;민첩성</strong> 사이의 트레이드오프로 볼 수 있습니다. 실제 응용에서는 작업 요구사항에 따라 시각/촉각 신호의 가중치를 조절하는 등이 필요할 수 있습니다.</p>
<p><strong>한계:</strong> 현재 사용된 촉각 센서는 16개의 FSR로, 접촉 위치를 이진적으로 제공하기 때문에 <strong>촉각 해상도</strong>가 높지 않습니다. 복잡한 물체의 미세한 표면 질감이나 미끄러짐 방향까지 인지하려면 이보다 풍부한 촉각 정보가 필요할 수 있습니다. 논문에서도 후속 작업으로 <strong>광학식 촉각 센서(optical tactile sensors)</strong> 통합을 제시하고 있는데, 예를 들어 GelSight와 같은 비전 기반 촉각센서를 사용하면 <strong>고해상도 촉각 이미지</strong>를 점군 등과 함께 활용하는 방향도 가능할 것입니다. 또한 본 연구는 주로 <strong>연속 회전</strong> 작업에 집중하였는데, 향후에는 특정 목표 각도로 돌리는 <strong>goal-conditioned 회전</strong>이나, 회전 이외에 <strong>이동/정렬 등의 복합 조작</strong>으로 확장해 볼 여지도 있습니다. 마지막으로, 시각-촉각 이외의 모달리티(예: 힘-토크 센서나 음향 센서 등)도 함께 융합한다면 로봇의 환경에 대한 <strong>멀티모달 이해</strong>를 한층 높일 수 있을 것입니다.</p>
<p><strong>결론:</strong> <em>Robot Synesthesia</em>는 <strong>시각과 촉각의 경계</strong>를 허물어 로봇이 마치 <strong>“만져서 보는”</strong> 새로운 방식으로 세상을 인식하게 함으로써, 난이도 높은 손내 물체 조작을 가능하게 한 흥미로운 연구입니다. 사람의 감각 통합에서 영감을 얻은 아이디어를 공학적으로 구현하여 실질적인 성능 향상을 입증했다는 점에서 의의가 큽니다. 시각-촉각 통합은 향후 인간과 상호작용하는 로봇, 서비스 로봇 등에서 필수적인 능력이 될 것이므로, 본 논문의 접근법은 그 중요한 한 걸음을 내딛은 것으로 평가됩니다. 앞으로 더 다양한 환경과 과제에 이 기법이 적용되고 발전되어, 로봇이 더욱 사람처럼 <strong>다양한 감각을 자유자재로 활용</strong>하는 미래를 기대해 봅니다.</p>
</section>
</section>
<section id="reference" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Reference</h1>
<ul>
<li><a href="https://www.touchprocessing.org/2023/camera_ready/camera_ready_7.pdf">https://www.touchprocessing.org/2023/camera_ready/camera_ready_7.pdf</a></li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    window.setColorSchemeToggle(window.hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<script src="https://utteranc.es/client.js" repo="curieuxjy/blog_comments" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Copyright 2024, Jung Yeon Lee</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/curieuxjy">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>