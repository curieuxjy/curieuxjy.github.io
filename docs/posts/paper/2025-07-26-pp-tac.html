<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-07-25">
<meta name="description" content="Paper Picking Using Tactile Feedback in Dexterous Robotic Hands">

<title>📃PP-Tac 리뷰 – Curieux.JY</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../profile.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-8ef56b68f8fa1e9d2ba328e99e439f80.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-ecf89aac047581c664da7ae53d704519.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-b009f778f5cec7f34f624408a2b5b543.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-ecf89aac047581c664da7ae53d704519.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-2NVZN2MJZT"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-2NVZN2MJZT', { 'anonymize_ip': true});
</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Curieux.JY</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../post.html"> 
<span class="menu-text">Post</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../note.html"> 
<span class="menu-text">Note</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">Jung Yeon Lee</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/curieuxjy"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#brief-review" id="toc-brief-review" class="nav-link active" data-scroll-target="#brief-review">Brief Review</a></li>
  <li><a href="#detail-review" id="toc-detail-review" class="nav-link" data-scroll-target="#detail-review">Detail Review</a>
  <ul class="collapse">
  <li><a href="#서론-introduction" id="toc-서론-introduction" class="nav-link" data-scroll-target="#서론-introduction">1. 서론 (Introduction)</a></li>
  <li><a href="#알고리즘-구조-algorithmic-architecture" id="toc-알고리즘-구조-algorithmic-architecture" class="nav-link" data-scroll-target="#알고리즘-구조-algorithmic-architecture">2. 알고리즘 구조 (Algorithmic Architecture)</a>
  <ul class="collapse">
  <li><a href="#촉각-피드백-제어와-미끄럼-감지-tactile-feedback-control-and-slip-detection" id="toc-촉각-피드백-제어와-미끄럼-감지-tactile-feedback-control-and-slip-detection" class="nav-link" data-scroll-target="#촉각-피드백-제어와-미끄럼-감지-tactile-feedback-control-and-slip-detection">2.1 촉각 피드백 제어와 미끄럼 감지 (Tactile Feedback Control and Slip Detection)</a></li>
  <li><a href="#궤적-데이터-생성-및-확산-기반-정책-학습-trajectory-synthesis-and-diffusion-based-policy-learning" id="toc-궤적-데이터-생성-및-확산-기반-정책-학습-trajectory-synthesis-and-diffusion-based-policy-learning" class="nav-link" data-scroll-target="#궤적-데이터-생성-및-확산-기반-정책-학습-trajectory-synthesis-and-diffusion-based-policy-learning">2.2 궤적 데이터 생성 및 확산 기반 정책 학습 (Trajectory Synthesis and Diffusion-based Policy Learning)</a></li>
  <li><a href="#알고리즘-측면에서의-digit-360과의-비교-comparison-with-digit-360-in-algorithmic-context" id="toc-알고리즘-측면에서의-digit-360과의-비교-comparison-with-digit-360-in-algorithmic-context" class="nav-link" data-scroll-target="#알고리즘-측면에서의-digit-360과의-비교-comparison-with-digit-360-in-algorithmic-context">2.3 알고리즘 측면에서의 DIGIT 360과의 비교 (Comparison with DIGIT 360 in Algorithmic Context)</a></li>
  </ul></li>
  <li><a href="#센서-및-하드웨어-설계-sensor-and-hardware-design" id="toc-센서-및-하드웨어-설계-sensor-and-hardware-design" class="nav-link" data-scroll-target="#센서-및-하드웨어-설계-sensor-and-hardware-design">3. 센서 및 하드웨어 설계 (Sensor and Hardware Design)</a>
  <ul class="collapse">
  <li><a href="#원형-촉각-센서-r-tac의-구조와-특징-design-and-characteristics-of-r-tac" id="toc-원형-촉각-센서-r-tac의-구조와-특징-design-and-characteristics-of-r-tac" class="nav-link" data-scroll-target="#원형-촉각-센서-r-tac의-구조와-특징-design-and-characteristics-of-r-tac">3.1 원형 촉각 센서 R-Tac의 구조와 특징 (Design and Characteristics of R-Tac)</a></li>
  <li><a href="#allegro-로봇-손과의-통합-integration-with-allegro-hand" id="toc-allegro-로봇-손과의-통합-integration-with-allegro-hand" class="nav-link" data-scroll-target="#allegro-로봇-손과의-통합-integration-with-allegro-hand">3.2 Allegro 로봇 손과의 통합 (Integration with Allegro Hand)</a></li>
  <li><a href="#digit-360과의-비교-분석-comparison-with-digit-360-sensor-system" id="toc-digit-360과의-비교-분석-comparison-with-digit-360-sensor-system" class="nav-link" data-scroll-target="#digit-360과의-비교-분석-comparison-with-digit-360-sensor-system">3.3 DIGIT 360과의 비교 분석 (Comparison with DIGIT 360 Sensor System)</a></li>
  </ul></li>
  <li><a href="#결론-conclusion" id="toc-결론-conclusion" class="nav-link" data-scroll-target="#결론-conclusion">4. 결론 (Conclusion)</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-page-left" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">📃PP-Tac 리뷰</h1>
  <div class="quarto-categories">
    <div class="quarto-category">gmm</div>
    <div class="quarto-category">catching</div>
  </div>
  </div>

<div>
  <div class="description">
    Paper Picking Using Tactile Feedback in Dexterous Robotic Hands
  </div>
</div>


<div class="quarto-title-meta column-page-left">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">July 25, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<ul>
<li><a href="https://arxiv.org/abs/2504.16649">Paper Link</a></li>
<li><a href="https://peilin-666.github.io/projects/PP-Tac/">Project Link</a></li>
</ul>
<ol type="1">
<li>PP-Tac은 얇고 변형 가능한 종이류 물체를 집기 어려운 로봇의 한계를 극복하기 위해 촉각 피드백을 활용하는 협응형 로봇 시스템입니다.</li>
<li>이 시스템은 전방향 촉각 감지 및 실시간 슬립 감지를 제공하는 새로운 원형 촉각 센서(R-Tac)가 장착된 다지 로봇 핸드를 특징으로 합니다.</li>
<li>사람의 집기 동작에서 영감을 받은 확산 기반 정책은 합성된 데이터와 도메인 무작위화를 통해 훈련되어 다양한 재료와 지형에서 87.5%의 성공률을 달성하며 견고한 성능을 입증했습니다.</li>
</ol>
<center>
<img src="../../images/2025-07-26-pp-tac/2.gif" width="80%">
</center>
<hr>
<section id="brief-review" class="level1">
<h1>Brief Review</h1>
<p>이 논문은 베이징 일반 인공지능 연구소(Beijing Institute for General Artificial Intelligence), 상하이테크 대학교(ShanghaiTech University), 베이항 대학교(Beihang University)의 연구자들이 제안한 “PP-Tac: Paper Picking Using Tactile Feedback in Dexterous Robotic Hands”에 대한 내용이다.</p>
<p><strong>연구 목적 및 배경:</strong></p>
<p>로봇이 일상생활에서 인간을 돕는 동반자로 활용되는 비전이 증가함에 따라, 종이 또는 직물과 같은 얇고 변형 가능한 물체를 조작하는 능력이 중요해지고 있다. 하지만 기존 시스템은 다양한 외형의 물체에 대한 견고한 상태 추정 및 적절한 파지(grasp) 동작 생성을 위한 지각(perception) 및 계획(planning) 기술의 부족으로 인해 이러한 물체를 다루는 데 어려움을 겪는다. 특히, 시각 시스템은 제한된 감지 양상(sensing modalities)과 가려짐(occlusion)으로 인해 변형 가능한 물체와의 상호작용 중 접촉 정보를 정확하게 인식하기 어렵고, 물체의 평평한 형태는 안정적인 파지를 위한 특징점 부족을 야기하며, 조작 중 예측 불가능한 변형은 시각 기반 방법의 일반화(generalizability)를 저해한다. 이에 반해 인간은 다지(multi-fingered) 동작과 촉각 감지(tactile sensing)를 활용하여 종이와 같은 물체를 능숙하게 집어 올린다. 이러한 인간의 전략에서 영감을 받아, 본 논문은 촉각 피드백을 활용하여 종이와 같은 물체를 집는 로봇 시스템인 PP-Tac을 제안한다.</p>
<p><strong>제안하는 시스템: PP-Tac:</strong></p>
<p>PP-Tac은 크게 두 가지 핵심 요소를 포함한다: 정교한(dexterous) 로봇 핸드와 이와 통합된 구형(hemispherical) 고해상도 시각 기반 촉각 센서(Vision-Based Tactile Sensor, VBTS)인 R-Tac, 그리고 인간의 파지 기술을 모방한 확산 기반(diffusion-based) 동작 생성 정책(policy)인 PP-Tac Policy이다.</p>
<ul>
<li><p><strong>하드웨어: R-Tac 촉각 센서:</strong> R-Tac은 Allegro Hand에 커스터마이징되어 통합된 구형 VBTS이다.</p>
<ul>
<li><strong>설계 원칙:</strong> R-Tac은 효과적인 조작을 위해 다음 다섯 가지 핵심 원칙을 따른다:
<ol type="1">
<li><strong>구형 형태(Round shape):</strong> 전방위적인 촉각 인식을 가능하게 한다.</li>
<li><strong>고해상도(High resolution):</strong> 정확한 깊이 재구성 및 슬립 감지에 필수적이다.</li>
<li><strong>편리한 제작 및 저비용(Convenient to fabricate &amp; low-cost):</strong> 저렴한 상용 부품과 쉬운 제작 공정으로 약 60달러의 비용으로 제작 가능하다.</li>
<li><strong>효율적인 캘리브레이션(Efficient calibration):</strong> 단색(monochrome) 감지 원리를 통해 조명 제어가 단순하고 수동 캘리브레이션 노력이 적어 다지 로봇 핸드에 대규모 배포에 적합하다.</li>
<li><strong>효율적인 데이터 전송(Efficient data transmission):</strong> 경량의 프레임 데이터를 생성하여 고속 데이터 전송을 용이하게 한다.</li>
</ol></li>
<li><strong>구성 요소:</strong>
<ul>
<li><strong>접촉 및 조명 모듈(Contact and Illumination Module):</strong> 균일하게 조명되는 변형 가능한 민감한 표면을 가진 엘라스토머(elastomer)가 핵심이다. 백색 LED 링, 투명 내부 골격, 반투명 감지층(perception layer), 불투명 보호층으로 구성된다. PDMS와 반투명 실리콘, 그리고 실리콘 코팅을 사용하여 제작된다.</li>
<li><strong>카메라 모듈(Camera Module):</strong> OV9281 마이크로 흑백 CMOS 카메라(640x480 해상도, 120Hz, 160° 광각 렌즈)가 사용된다.</li>
</ul></li>
<li><strong>캘리브레이션 및 3D 깊이 재구성:</strong> R-Tac은 엘라스토머와 조명 모듈의 균일한 광학 특성 덕분에 CNC 머신 없이 단 30장의 이미지로 2단계 만에 3D 형상 정보를 단일 채널 픽셀 강도(intensity)로부터 계산할 수 있다. 먼저, 3D 프린팅된 들여쓰기(indentation) 기반 설정에서 29장의 이미지를 사용하여 카메라의 고유 매개변수(intrinsic parameters) <span class="math inline">K</span>와 외부 매개변수(extrinsic parameters)인 회전 행렬 <span class="math inline">A</span> 및 변환 벡터 <span class="math inline">b</span>를 추정한다. 다음으로, 알려진 크기의 구를 센서에 눌러 단일 이미지를 캡처하여 깊이 매핑 함수 <span class="math inline">M</span>을 캘리브레이션한다. 픽셀 좌표 <span class="math inline">(u, v)</span>를 센서 좌표 <span class="math inline">(x, y, z)</span>로 변환하는 완전한 매핑 함수는 다음과 같이 표현된다: <span class="math display"> \begin{bmatrix} x \\ y \\ z \end{bmatrix} = A^{-1} \left( (D(u, v) - M(I_{\Delta}(u, v)))K^{-1} \begin{bmatrix} u \\ v \\ 1 \end{bmatrix} - b \right) </span> 여기서 <span class="math inline">D(u, v)</span>는 센서 표면 참조 투영(reference projection)을 나타낸다.</li>
<li><strong>접촉력 추정 및 슬립 감지:</strong> 센서는 접촉력과 슬립(slip) 이벤트를 모두 감지할 수 있다. 접촉력은 변형 깊이에 비례하는 선형 함수로 모델링된다. 슬립 감지는 센서 이미지에서 나타나는 주름(wrinkles)을 활용하며, CNN과 MLP로 구성된 경량 신경망 아키텍처를 통해 수행된다. 이 네트워크는 이전 5개 프레임과 비접촉 프레임을 입력으로 받아 슬립 확률 <span class="math inline">P_{slip}</span>을 추정하며, 86%의 슬립 감지 정확도를 달성한다(임계값 0.75).</li>
<li><strong>로봇 핸드 시스템:</strong> Allegro Hand (16 DoF)의 각 손가락 끝에 R-Tac 센서가 장착되어 있으며, Dynamixel XC330-M288-T 모터로 구동된다. 전체 시스템은 Franka Research 3 (7 DoF) 로봇 팔에 장착된다.</li>
</ul></li>
<li><p><strong>정책 학습: PP-Tac Policy:</strong> PP-Tac 정책은 촉각 피드백을 활용하여 접촉 조건을 유지하고 성공적인 파지를 위한 좌굴(buckling) 영역을 생성한다. 이는 두 단계로 개발된다:</p>
<ol type="1">
<li>파지 동작 데이터셋 생성 및</li>
<li>확산 정책 훈련.</li>
</ol>
<ul>
<li><p><strong>파지 동작 데이터셋 합성:</strong> 데이터셋은 시뮬레이션에서 궤적 최적화(trajectory optimization)를 통해 생성된다. 복잡한 텔레오퍼레이션(teleoperation) 장치나 연성체(soft-body) 시뮬레이션이 필요 없으며, 경성체(rigid-body) 역학을 사용하여 실제 로봇으로 직접 전이(sim-to-real transfer)된다. 파지 과정은 손가락 끝과 물체의 표면이 접촉하는 것으로 시작하며, 손가락들은 물체를 집기 위해 점차적으로 오므려진다. 각 손가락은 물체 표면에서 독립적인 궤적을 따르며 동시에 목표 수직력(normal force)을 가한다. 다양한 손끝 궤적을 생성하기 위해 무작위 지형 프로파일과 미리 기록된 파지 동작 시퀀스가 사용된다. 미리 기록된 동작에서 손끝 궤적의 (x,y) 좌표를 추출하고, 해당 z-좌표는 이러한 (x,y) 점을 지형 표면에 투영하여 샘플링한다. 접촉력 <span class="math inline">F</span>는 촉각 센서 엘라스토머 층의 변형 깊이 <span class="math inline">dddtac</span>를 조절하여 조절된다. <span class="math inline">dddtac</span>와 <span class="math inline">F</span> 사이의 정확한 관계는 명시적으로 모델링되지 않으며, 위치 제어(position control)를 통해 접촉력을 효율적으로 제어한다. 최종적으로 4개의 손끝에 대한 목표 궤적 <span class="math inline">e_{target}</span>이 얻어진다. 주어진 <span class="math inline">e_{target}</span>에 대해, 모든 손가락 관절 각도와 팔 자세는 다음 최적화 문제(optimization problem)를 통해 해결된다: <span class="math display"> \hat{\gamma} = \arg \min_{\gamma} (L_e + L_{\Delta} + L_{R,p_{wrist}}) </span> <span class="math display"> L_e = w_e \text{MSE}(f_k(\gamma), e_{target}) </span> <span class="math display"> L_{\Delta} = w_{\Delta} \text{MSE}(\bar{\gamma}, \gamma) </span> <span class="math display"> L_{R,p_{wrist}} = w_{R,p_{wrist}} \text{MSE}((\bar{R}, \bar{p}_{wrist}),(R, p_{wrist})) </span> 여기서 <span class="math inline">\gamma</span>는 <span class="math inline">N_{data}</span> 프레임의 핸드 관절 각도 <span class="math inline">q</span>, 손목(팔의 end-effector) 회전 <span class="math inline">R</span> 및 손목의 세계 좌표계 z축 변환 <span class="math inline">p_{wrist}</span>로 구성된 최적화 변수이다. <span class="math inline">f_k</span>는 전방 운동학(forward kinematics)을 계산하고, <span class="math inline">\text{MSE}</span>는 평균 제곱 오차(mean squared error)를 나타낸다. <span class="math inline">L_e</span>는 손끝 위치와 목표 간의 오차를 최소화하고, <span class="math inline">L_{\Delta}</span>는 초기 자세에 가깝게 움직임을 정규화(regularize)하며, <span class="math inline">L_{R,p_{wrist}}</span>는 팔이 작업 공간 내에 있도록 손목 움직임을 최소화한다. 충돌 시퀀스를 제외한 50만 개의 파지 샘플 데이터셋이 생성되며, 각 샘플은 <span class="math inline">N_{data}=100</span> 프레임으로 구성된다.</p></li>
<li><p><strong>확산 기반 정책 훈련:</strong> 데이터셋이 준비되면, 확산 정책(Diffusion Policy)을 사용하여 핸드와 팔을 공동으로 제어하고 다양한 지형 형태와 접촉력 조건에 적응한다. Denoising Diffusion Probabilistic Model (DDPM) 프레임워크를 채택하여 과거 상태 <span class="math inline">x_{prefix}</span>에 조건부로 미래 행동 <span class="math inline">x_{pred}</span>를 예측한다. 각 프레임의 상태 변수는 다음과 같다: <span class="math display"> (p, \dot{p}, q, \dot{q}, R, \Omega, p_{wrist}, \dot{p}_{wrist}, d_{tac}) </span> 여기서 <span class="math inline">p \in \mathbb{R}^{17 \times 3}</span>은 세계 좌표계의 핸드 관절 위치, <span class="math inline">\dot{p} \in \mathbb{R}^{17 \times 3}</span>는 선형 속도, <span class="math inline">q \in \mathbb{R}^{16}</span>는 제어 가능한 핸드 관절의 회전 각도, <span class="math inline">\dot{q} \in \mathbb{R}^{16}</span>는 각속도, <span class="math inline">R \in \mathbb{R}^{6}</span>는 손목의 6D 회전(두 행 벡터로 표현), <span class="math inline">\Omega \in \mathbb{R}^{6}</span>는 손목 회전의 각속도, <span class="math inline">p_{wrist} \in \mathbb{R}</span>는 팔의 z축을 따른 손목 높이, <span class="math inline">\dot{p}_{wrist} \in \mathbb{R}</span>는 <span class="math inline">p_{wrist}</span>의 선형 속도, <span class="math inline">d_{tac} \in \mathbb{R}^{4}</span>는 4개 손끝 촉각 센서의 변형 깊이 값이다. 총 상태 차원은 <span class="math inline">D = 152</span>이다. 인코더 전용 트랜스포머(encoder-only transformer)를 사용하여 <span class="math inline">x_{prefix}</span>, 확산된 미래 동작 <span class="math inline">x_{pred_t}</span>, 확산 단계 <span class="math inline">t</span>, 현재 프레임 인덱스 <span class="math inline">i</span>, 및 목표 변형 깊이 <span class="math inline">\bar{d}_{tac}</span>가 주어졌을 때 미래 로봇 동작 <span class="math inline">\hat{x}^0_{pred}</span>를 예측한다. 예측 오차를 줄이기 위해 다음 손실 함수를 사용한다: <span class="math display"> L = \|\hat{x}^0_{pred} - x^0_{pred}\|^2_2 + \lambda_{consist}L_{consist} </span> <span class="math display"> L_{consist} = \|f_k(q^0_{pred}) - p^0_{pred}\|^2_2 </span> 여기서 <span class="math inline">L_{consist}</span>는 관절 각도와 위치 간의 일관성을 강제하고, <span class="math inline">\lambda_{consist}</span>는 가중치 하이퍼파라미터이다. 추론 시 실시간 성능을 위해 노이즈 제거 단계(denoising step)를 10단계로 줄이고, <span class="math inline">N_{pred}=N_{prefix}=5</span>로 설정하여 RTX4090 GPU에서 11ms 내에 동작 생성이 가능하다. 파지 중 슬립 방지를 위해 손끝 촉각 센서가 슬립을 감지하면 원하는 변형 깊이 <span class="math inline">d_{tac}</span>를 작은 증분 <span class="math inline">\Delta d_{tac}</span>만큼 증가시킨다. 시뮬레이션과 실제 세계 간의 도메인 갭(domain gap)을 해결하기 위해 학습 중 <span class="math inline">x_{prefix}</span>에 네 가지 방식의 교란(disturbance)을 도입한다: 1) <span class="math inline">\gamma</span>에 랜덤 가우시안 노이즈 추가, 2) 첫 프레임에 가우시안 노이즈를 추가하고 후속 프레임에서 점진적으로 증폭, 3) 2에서 <span class="math inline">N_{prefix}</span> 사이의 시간적으로 일관된 프레임을 무작위로 고정(static)시키고 <span class="math inline">d_{tac}</span>를 최대 임계값으로 설정한다.</p></li>
</ul></li>
</ul>
<p><strong>실험 및 결과:</strong></p>
<p>PP-Tac 알고리즘은 4계층 트랜스포머 인코더(latent dimension 512, 4 attention heads)로 구현되었으며, 단일 RTX 4090에서 약 60만 회 반복 학습되었다. 지형은 큐빅 스플라인(cubic spline)으로 모델링되었으며, 제어점의 높이는 [0, 3]cm 범위 내에서 무작위로 샘플링되었다.</p>
<ul>
<li><p><strong>R-Tac 깊이 재구성:</strong> R-Tac은 “RSS”, “⋆”, “2025” 텍스트 콘텐츠를 가진 인덴터(indenter)로 압착 테스트를 거쳐 미세한 표면 디테일을 완전히 재구성할 수 있음을 보여주었다. 평균 절대 오차(L1 error)는 0.35mm, 중앙값은 0.28mm였으며, 깊이 매핑 프로세스는 10ms 미만으로 로봇 애플리케이션의 실시간 성능을 보장한다.</p></li>
<li><p><strong>PP-Tac 정책 평가:</strong> 다양한 재료(종이, 비닐봉투, 천, 크라프트지 봉투)와 지형(평면, 경사, 책 위, 무작위 곡선)에서 시스템의 파지 능력을 평가했다. 각 조합에 대해 20회 시도했다.</p>
<ul>
<li><strong>재료 및 지형별 성능:</strong> 천과 비닐봉투는 낮은 강성으로 인해 쉽게 좌굴되어 파지하기 상대적으로 쉬웠다. 반면 종이와 크라프트지 봉투는 더 뻣뻣하여 좌굴에 저항하므로 성공률이 낮았다. 지형도 파지 성공에 큰 영향을 미쳤다. 평평한 지형(평면, 경사)에서는 종이, 비닐봉투, 천의 성공률이 높았다. 책이 깔린 경우, 책의 모서리와 재료 아래 생성된 부분적인 빈 공간 덕분에 재료가 좌굴되고 지형과 분리되기 쉬워 모든 물체에서 높은 성공률을 유지했다. 그러나 매우 불규칙한 지형에서는 손가락이 미끄러질 가능성이 높아 성공률이 떨어졌다.</li>
<li><strong>다른 시스템 구성과의 비교:</strong> 세 가지 기준선(baseline)과 비교했다:
<ol type="1">
<li><strong>Bi-finger grippers controlled via human teleoperation with a camera:</strong> 인간의 시각 기반 텔레오퍼레이션으로 제어되는 바이-핑거 그리퍼. 천과 비닐봉투는 어느 정도 파지 성공률을 보였으나 PP-Tac보다 낮았고, 뻣뻣한 재료(종이, 크라프트지)는 완전히 실패했다. 이는 PP-Tac의 하드웨어 설계의 효율성을 보여준다.</li>
<li><strong>Open-loop control without tactile feedback:</strong> 지형의 지상 진실(ground truth) 형태를 사용하여 미리 생성된 궤적을 재생하는 방식. 제어 오류로 인해 PP-Tac보다 낮은 성공률을 보였다. 특히 Allegro Hand의 관절 각도 오차(0.1 radian 초과)가 누적되어 정밀 작업에서 성능이 저하됨을 확인하며, 이는 촉각 피드백의 필요성을 강조한다. 불확실한 지형에서는 불가능했다.</li>
<li><strong>“Model based force tracking”:</strong> PP-Tac에서 생성된 손목 궤적을 사용하고 실시간 촉각 피드백을 통해 손끝만 제어하는 방식. 구조화된 지형에서는 만족스러운 성능을 보였으나, 불규칙하거나 복잡한 지형에서는 효과가 제한적이었다. 결과적으로 PP-Tac 파이프라인이 모든 기준선보다 우수한 성능을 보였다.</li>
</ol></li>
</ul></li>
<li><p><strong>어블레이션 연구(Ablation Studies):</strong></p>
<ul>
<li><strong>재료 강성의 영향:</strong> 종이 페이지 수를 늘려 강성을 달리한 실험에서, 페이지 수가 증가할수록 파지 성공률이 유의미하게 감소하고 슬립 감지 횟수가 증가했다. 이는 재료 강성이 작업 성공에 큰 영향을 미침을 보여준다.</li>
<li><strong>데이터 교란의 영향:</strong> 도메인 무작위화(Domain Randomization)의 중요성을 확인했다. 데이터 교란을 제거한 “Non-disturbance” 기준선은 모든 실험에서 성능이 현저히 떨어졌고, 뻣뻣한 물체 파지 시 종종 완전히 실패했다. 이는 도메인 무작위화가 일반화 능력과 파지 성공률을 향상시킴을 강조한다. 단, 이 기법은 훈련 시간을 증가시킨다.</li>
</ul></li>
</ul>
<p><strong>제한 사항:</strong></p>
<p>초기 힘(센서의 목표 변형 깊이) 설정이 수동으로 이루어지는 경험적 매개변수 튜닝 과정이라는 한계가 있다. 초기값이 너무 작으면 적응 시간이 길어져 실패할 가능성이 높고, 너무 크면 핸드 모터의 부하 용량을 초과할 수 있다. 또한, 힘을 조절하는 적응형 알고리즘도 개선의 여지가 있으며, 특히 비평면 지형의 매우 뻣뻣한 재료에 대한 개선이 필요하다.</p>
<p><strong>결론:</strong></p>
<p>본 논문은 종이 및 직물과 같은 얇고 평평한 물체를 조작하기 위한 협응형 핸드-암 시스템인 PP-Tac을 제시한다. 이 시스템은 제작 및 배포가 용이한 다지 시각 기반 촉각 센서(R-Tac)를 갖추고 있으며, 이 센서는 곡면에서 접촉을 감지하고 접촉 중 힘과 마찰을 측정하여 슬립을 최소화하고 재료 변형 가능성을 높인다. 핸드 설계에 기반하여, 파지 동작은 데이터 기반 접근 방식으로 계획된다. 다양한 지형 형태 및 센서 변형 조건에서 슬라이딩 궤적을 생성하는 효율적인 합성 알고리즘을 개발하여 50만 개의 궤적 샘플 데이터셋을 구축했다. 이 데이터셋과 도메인 무작위화 기법을 사용하여 확산 정책을 훈련했으며, 이를 통해 실제 환경에서 다양한 지형에 적응할 수 있었다. 실험 결과, 제안된 시스템은 다양한 두께와 강성을 가진 평평한 물체를 성공적으로 파지하여 87.5%의 성공률을 달성했으며, 외부 교란에 대한 견고성과 다양한 지지 지형 표면에 대한 적응성을 입증했다.</p>
<hr>
</section>
<section id="detail-review" class="level1">
<h1>Detail Review</h1>
<blockquote class="blockquote">
<p>PP-Tac: 촉각 피드백을 활용한 종이 집기</p>
</blockquote>
<section id="서론-introduction" class="level2">
<h2 class="anchored" data-anchor-id="서론-introduction">1. 서론 (Introduction)</h2>
<p><strong>PP-Tac 개요:</strong> <em>PP-Tac</em>는 얇고 변형 가능한 물체(예: 종이, 천 등)를 집어올리는 문제를 해결하기 위해 제안된 로봇 시스템으로, 2025년 Robotics: Science and Systems (RSS)에 발표되었다. 이 시스템은 <strong>다수의 손가락을 갖춘 덱스터러스 로봇 손</strong>에 <strong>고해상도 전방위 촉각 센서</strong>(R-Tac)를 장착하여, 촉각 피드백을 통해 종이와 같은 납작한 객체를 효과적으로 집을 수 있게 한다. 기존의 시각 기반 시스템들이 얇은 물체의 두께나 변형을 인식하기 어려워 작업에 실패하는 반면, PP-Tac는 인간의 전략에서 영감을 받아 <strong>여러 손가락의 협응 동작</strong>과 <strong>촉각 센싱</strong>을 적극 활용한다. 사람은 종이를 집을 때 손가락으로 미끄러짐을 감지하고 힘을 조절하며, 손목과 손가락을 유기적으로 움직여 종이를 말아 올린다. PP-Tac는 이러한 인간의 <strong>슬라이딩 및 집기 동작</strong>을 모방하며, 촉각 정보로 물체의 미세한 움직임까지 감지하여 제어에 반영하는 것을 목표로 한다.</p>
<p><strong>주요 구성요소:</strong> PP-Tac 시스템은 크게 두 축으로 구성된다. 첫째, <strong>알고리즘 구조</strong> 측면에서 <strong>촉각 피드백 제어 시스템</strong>과 <strong>학습 기반 제어 알고리즘</strong>을 포함한다. 구체적으로, 촉각 센서로부터 얻은 정보를 이용한 <strong>실시간 미끄럼(slip) 감지 및 마찰력 제어</strong>, 그리고 <strong>확산 기반 정책(diffusion-based policy)</strong>으로 구현된 <strong>학습형 제어기</strong>가 핵심이다. 이 알고리즘은 인간의 종이 집기 동작 데이터를 모방하여 <strong>궤적(trajectory) 생성</strong>을 수행하고, 이를 통해 학습된 정책이 로봇 손-팔 시스템을 제어한다. 둘째, <strong>센서 및 하드웨어 설계</strong> 측면에서 새로운 <strong>원형 촉각 센서 R-Tac</strong>의 구조와 특징, <strong>센서 보정 방법</strong>, 그리고 이 센서를 <strong>Allegro 로봇 핸드</strong>에 통합한 하드웨어 구성이 소개된다. R-Tac는 반구형의 <strong>시각 기반 촉각 센서</strong>로서, 빠른 응답과 간편한 보정을 위해 <strong>모노크롬 카메라 기반 설계</strong>를 채택한 것이 특징이다.</p>
<p><strong>의의:</strong> 논문 저자들은 PP-Tac가 다양한 재질, 두께, 강성을 가진 종이類 물체들을 성공적으로 집어올리며 <strong>87.5%의 성공률</strong>을 달성했음을 보고하였다. 이는 촉각 센싱과 학습 제어의 결합이 얇은 변형체 조작에 효과적임을 최초로 입증한 사례로 평가된다. 본 리뷰에서는 <strong>(1) 알고리즘 구조</strong>와 <strong>(2) 센서/하드웨어 설계</strong>의 두 축에서 <em>PP-Tac</em>의 기여를 심층 분석하고, 각 측면마다 <strong>기존 촉각 센서 시스템인 DIGIT 360</strong>과의 비교를 통해 구조적 특성, 해상도, 설치 및 제어상의 차이를 논의한다. 이후 섹션에서는 해당 논문의 핵심 내용을 학술적인 문체로 정리하며, 필요한 경우 원문에서 발췌한 인용을 포함한다.</p>
<p><br></p>
</section>
<section id="알고리즘-구조-algorithmic-architecture" class="level2">
<h2 class="anchored" data-anchor-id="알고리즘-구조-algorithmic-architecture">2. 알고리즘 구조 (Algorithmic Architecture)</h2>
<section id="촉각-피드백-제어와-미끄럼-감지-tactile-feedback-control-and-slip-detection" class="level3">
<h3 class="anchored" data-anchor-id="촉각-피드백-제어와-미끄럼-감지-tactile-feedback-control-and-slip-detection">2.1 촉각 피드백 제어와 미끄럼 감지 (Tactile Feedback Control and Slip Detection)</h3>
<p>PP-Tac의 제어 시스템은 <strong>실시간 촉각 피드백</strong>을 활용하여 손가락 힘을 조절함으로써, 종이를 집는 동안 발생할 수 있는 <strong>미끄럼(slip)</strong>을 감지하고 방지한다. 네 개의 손가락 끝에 장착된 R-Tac 촉각 센서는 접촉면의 압력 변화와 움직임을 감지하여, 물체가 손가락 사이에서 미끄러지려는 조짐을 포착한다. 구체적으로, 센서 표면이 물체와 접촉 중에 <strong>마찰력이 부족해질 때 나타나는 주름(wrinkle) 패턴</strong>을 영상으로 포착하여 미끄럼 여부를 판단한다. 이를 위해 저자들은 <strong>경량 CNN+MLP 기반의 슬립 감지 모듈</strong>을 설계하였다. 이 모듈은 <strong>최근 5 프레임의 촉각 이미지 시퀀스</strong>와 <strong>비접촉 시 기준 프레임</strong>을 입력으로 받아, 각 프레임에서 추출한 특징맵들을 통합해 미끄럼 발생 확률을 출력한다. 약 20분간의 촉각 데이터(슬립 사례 40%, 비슬립 60%)를 수집하여 이 네트워크를 학습시켰으며, 최종적으로 <strong>86%의 슬립 감지 정확도</strong>를 달성했다고 보고된다.</p>
<p>미끄럼 감지 신뢰도가 충분히 높아지도록 임계값을 조정한 후(예: 0.75에서 최적 성능 확인), 이 정보를 <strong>온라인 마찰력 제어</strong>에 활용한다. PP-Tac는 매 시각 <strong>촉각 센서로부터 얻은 변형(depth) 값</strong>을 이용해 손가락이 가해야 할 목표 힘(손가락 눌림 정도)을 설정하고, 만약 미끄럼 징후가 감지되면 해당 손가락의 <strong>목표 변형 깊이</strong>를 소량 증가시켜(normal force를 높여) 즉각적인 마찰력 증대를 꾀한다. 이러한 <strong>피드백 제어 루프</strong>를 통해, 로봇 손가락은 <strong>실시간으로 접촉 상태를 모니터링</strong>하며 필요한 경우 미끄러짐을 방지하도록 잡는 힘을 조절한다. 이는 인간이 촉각을 통해 미끄럼을 느끼면 손가락에 힘을 더 주는 무의식적 반사 동작과 유사한 메커니즘으로, 얇은 종이를 들어올릴 때 <strong>재료와 손가락 사이의 정지 마찰을 최대화</strong>하여 종이의 변형(버클링)을 유도하고 견고한 집기를 가능케 한다.</p>
<p>특히 PP-Tac에서는 손가락 힘 제어를 <strong>컴플라이언스 제어</strong> 형태로 구현하여, 손가락이 종이를 누르는 힘은 증가시키되 <strong>손목과 다른 손가락의 위치 제어와 충돌되지 않도록</strong> 유연하게 적용한다. 이러한 촉각 피드백 기반의 제어는 <em>open-loop</em> 시나리오나 단순 그리퍼와 대비될 때 현격한 성능 향상을 보여주었다고 한다. 실험에서 촉각 피드백을 사용하지 않고 사전에 정의된 궤적만 따른 경우(open-loop)에는 불확실한 지형에서 거의 작업이 불가능하였지만, PP-Tac의 폐루프 제어는 다양한 평면 및 경사/복잡 지형에서 높은 성공률을 보였다. 이는 <strong>실시간 촉각 정보의 통합이 얇은 물체 조작의 안정성에 필수적</strong>임을 뒷받침한다.</p>
</section>
<section id="궤적-데이터-생성-및-확산-기반-정책-학습-trajectory-synthesis-and-diffusion-based-policy-learning" class="level3">
<h3 class="anchored" data-anchor-id="궤적-데이터-생성-및-확산-기반-정책-학습-trajectory-synthesis-and-diffusion-based-policy-learning">2.2 궤적 데이터 생성 및 확산 기반 정책 학습 (Trajectory Synthesis and Diffusion-based Policy Learning)</h3>
<p>PP-Tac의 두번째 알고리즘 핵심은 <strong>학습 기반의 제어 정책</strong>으로, 특히 <strong>확산 모델(diffusion model)</strong>을 활용한 생성적 정책을 통해 로봇의 다관절 움직임을 제어한다. 단순한 모델기반 제어로는 다중 손가락과 손목이 얽힌 동역학 문제(시리얼-패럴렐 결합)나 손목 고정 시 손가락 자유도 부족 문제 등을 해소하기 어렵기 때문에, 저자들은 <strong>모델프리(model-free) 방식의 학습 기법</strong>을 채택하였다. 특히 강화학습(RL)을 직접 적용하는 대신, <strong>인간 시연을 모방한 궤적 최적화(trajectory optimization)</strong>로 데이터를 생성하고 이를 학습하는 <strong>확산 정책 (PP-Tac policy)</strong>을 개발하였다.</p>
<p><strong>① 궤적 데이터 생성:</strong> 우선 시뮬레이션 환경에서 <strong>궤적 최적화 기법</strong>을 통해 종이 집기 동작의 <strong>전문가 데이터셋</strong>을 구축한다. 사람의 동작을 흉내내기 위해, 인간이 손가락으로 종이를 밀어 올리는 <strong>슬라이딩+집기 동작</strong>을 원격조작(텔레오퍼레이션)으로 한 차례 녹화하고, 해당 궤적의 손가락 끝 경로를 추출하여 기본 형태로 사용하였다. 이 경로를 다양한 조건에 맞게 변형함으로써 데이터셋을 증강하는데, 예를 들어 <strong>무작위 지형(profile) 변화</strong>를 생성하고 손가락 말단 경로의 <span class="math inline">x,y</span> 좌표를 지형에 투영하여 굴곡진 표면에서도 접촉을 유지하도록 한다. 또한 <strong>접촉력 조건</strong>의 다양화를 위해, 손가락이 누르는 <strong>변형 깊이 값(<span class="math inline">d</span>)</strong>을 다르게 하는 궤적들을 생성한다. 이는 촉각 센서의 변형이 곧 접촉력을 반영한다는 점을 이용한 것으로, 손가락 관절과 지면 사이 거리를 조정하여 센서가 더 눌리거나 덜 눌리게끔 궤적을 만들어 <strong>경우에 따라 강하게 누르는 동작과 약하게 누르는 동작</strong>을 포함시킨다. 이러한 방식은 연성체(종이)의 정확한 물리모델을 사용하지 않고도 <strong>강체 시뮬레이션</strong> 내에서 다양한 힘 조건을 흉내낼 수 있는 장점이 있다. 최종적으로 네 손가락 각각에 대해 다양한 접촉 조건과 지형 조건을 반영한 <strong>50만 개 이상의 grasp 궤적 샘플</strong>이 생성되었으며, 이들 각각은 시계열 프레임들의 시퀀스로 구성되었다고 한다.</p>
<p><strong>② 확산 기반 정책 학습:</strong> 생성된 궤적 데이터셋을 바탕으로, <strong>Denoising Diffusion Probabilistic Model (DDPM)</strong> 프레임워크를 활용한 확산 정책을 학습시킨다. 이 정책 모델은 <strong>트랜스포머 인코더(4층)</strong> 구조로 구현되었으며, 과거 <span class="math inline">H</span> 스텝의 상태(로봇 센서 및 관절 상태)를 입력받아 미래 <span class="math inline">K</span> 스텝의 로봇 동작 시퀀스를 예측하도록 훈련된다. 상태 벡터에는 로봇 손의 <strong>프로프리오셉션</strong> (손가락 관절각 및 속도, 손목 자세(6D) 및 속도)와 <strong>촉각 센서로부터 얻은 실시간 변형 깊이</strong>가 모두 포함된다. 구체적으로, 4개 손가락의 촉각 센서 변형값들이 상태에 포함되어 있어, 정책이 <strong>현재 접촉력 수준</strong>을 인지한 채 다음 움직임을 예측하게 된다. 학습 시에는 확산모델의 표준 절차에 따라 시계열 데이터에 점진적 노이즈를 더하고 제거하는 과정을 통해 모델이 <strong>미래 궤적의 분포</strong>를 학습하도록 한다. 이 때 기존 연구들에서 제안된 방식과 달리, PP-Tac는 직접 상태 시퀀스 <span class="math inline">x\_{0\:T}</span> 자체를 예측(output)하도록 설계하여(속도 대신 상태값 예측) 매 스텝별 <strong>지오메트리 손실(목표 위치 오차)</strong>를 명시적으로 줄이는 학습 목표를 채택하였다. 이 접근은 특히 로봇 모션 데이터에서는 상태 그 자체를 예측하는 편이 성능이 더 안정적이며, 추가로 <em>target loss</em>를 활용해 매 denoising 단계마다 궤적 정확도를 높일 수 있음을 보였다.</p>
<p><strong>③ 실시간 추론 및 제어:</strong> 학습된 확산 정책은 추론 시 과거 상태들을 입력받으면 <strong>다음 다가올 <span class="math inline">K</span> 스텝의 로봇 손/팔 제어 명령</strong>을 생성한다. 구체적으로 손가락 관절 제어명령 <span class="math inline">\tau\_{\text{hand}}</span>와 팔(손목) 제어명령 <span class="math inline">(\tau\_{\text{wrist-pos}}, \tau\_{\text{wrist-rot}})</span>를 예측하며, 이중 손가락 제어명령은 촉각 센서의 목표 변형 깊이(즉, 접촉력 목표)도 함께 고려하여 산출된다. 만약 직전 시점에 <strong>미끄럼 감지 신호</strong>가 발생했다면, 정책은 해당 손가락의 목표 변형을 증가시키도록 보정된 상태로 다음 움직임을 결정한다. 이를 통해 학습된 정책은 슬립 발생 시 자동으로 더 강한 힘으로 누르거나, 필요한 경우 궤적을 미세 조정하여 <strong>종이가 안정적으로 집힐 수 있도록 적응</strong>한다. 중요한 것은 이 모든 과정이 실시간으로 가능하도록 <strong>추론 속도 최적화</strong>가 이루어졌다는 점이다. 확산 모델의 단계 수를 1000→10단계로 줄이고, 노이즈 첨가 및 제거 전략을 조정함으로써 RTX 4090 GPU 상에서 <strong>매 시퀀스 11ms 이내</strong>에 행동결정이 완료되었다고 보고된다. 이는 100Hz 이상의 제어 주기로, 센서의 업데이트 속도(120Hz)와 유사한 수준이어서 충분히 실시간 제어에 활용될 수 있다.</p>
<p><strong>④ 도메인 랜덤화 &amp; sim-to-real:</strong> 시뮬레이터로 생성한 데이터로 학습한 정책을 실제 로봇에 이식할 때의 차이를 줄이기 위해, 학습 중 <strong>도메인 랜덤화</strong> 기법들이 적용되었다. 예를 들어, 제어 명령에 가우시안 노이즈를 추가하여 실제 하드웨어 모터의 오차를 모방하고, 손가락이 움직일 지형을 가상의 경사로 변화시켜보거나, 일부 프레임에서는 손가락이 붙잡혀 움직이지 못하는 상황(과도한 압력으로 마찰 정지)도 모사하였다. 이러한 다양한 교란(disturbance)을 학습에 포함시킴으로써, 정책은 현실 세계의 잡음과 불확실성에 <strong>견고한 제어 행동</strong>을 보이도록訓練되었다.</p>
<p><strong>성과:</strong> 학습된 PP-Tac 정책은 실험적으로 <strong>다양한 평면 및 요철 지형</strong> 위에 놓인 <strong>여러 종류의 얇은 물체(종이, 비닐봉지, 천, 종이가방 등)</strong>에 대해 높은 성공률로 집기 동작을 수행했다. 특히 완전한 폐루프(PP-Tac 전체) 시스템은 <strong>87.5%</strong>의 평균 성공률을 기록한 반면, 촉각 피드백이 없는 개방형 제어나 일반 그리퍼 사용 등의 베이스라인은 현저히 낮은 성공률을 보였다. 이는 제안된 <strong>확산 기반 학습 제어기</strong>가 다양한 상황에 <strong>일반화 능력</strong>을 갖추고 있고, <strong>촉각 피드백</strong>과 통합되어 얇은 변형체 조작에 유효함을 뒷받침한다.</p>
</section>
<section id="알고리즘-측면에서의-digit-360과의-비교-comparison-with-digit-360-in-algorithmic-context" class="level3">
<h3 class="anchored" data-anchor-id="알고리즘-측면에서의-digit-360과의-비교-comparison-with-digit-360-in-algorithmic-context">2.3 알고리즘 측면에서의 DIGIT 360과의 비교 (Comparison with DIGIT 360 in Algorithmic Context)</h3>
<p>DIGIT 360은 Meta AI와 GelSight가 개발한 최신 <strong>인공 촉각 손끝 센서</strong>로, PP-Tac의 R-Tac와 마찬가지로 <strong>반구형 손가락 형태</strong>를 가진다. 그러나 DIGIT 360은 단순한 카메라 기반 촉각센서가 아니라, <strong>18가지 이상의 멀티모달 센싱 기능</strong>(예: 시각 촉각, 힘/전단 감지, 진동, 온도, 화학적 감지 등)을 통합하고 <strong>센서 내 임베디드 AI 프로세서</strong>를 장착한 매우 진보된 플랫폼이다. 이러한 하드웨어적 능력 차이는 알고리즘 구조에도 영향을 미칠 수 있다.</p>
<p>우선, PP-Tac에서는 R-Tac 센서로부터 얻는 정보가 <strong>그레이스케일 영상</strong> 형태이므로, 미끄럼 감지 등의 고차원 신호를 추출하기 위해 별도의 <strong>학습 기반 신호처리 모듈(CNN)</strong>을 사용하였다. 반면 DIGIT 360의 경우, <strong>센서 자체가 전단력과 진동을 직접 측정</strong>할 수 있어 미끄럼 발생을 보다 <strong>물리적인 수준</strong>에서 감지할 수 있다. 예컨대 DIGIT 360은 1kHz 이상의 진동까지 포착 가능하여 물체가 미끄러질 때 발생하는 미세 진동이나 가속도를 바로 인식할 수 있고, 내부의 신경망 가속기(NPU)를 통해 <strong>센서 수준에서 즉각적인 미끄럼 판단과 반응</strong>(일종의 <em>reflex arc</em>)을 수행할 수 있다. 이는 PP-Tac처럼 센서 데이터를 PC로 보내 딥러닝 모델로 처리하는 방식보다 <strong>응답 지연을 크게 단축</strong>시킬 잠재력이 있다. 실제로 DIGIT 360은 인간보다 최대 30배 빠른 속도로 촉각 정보를 처리 가능하다고 소개되고 있어, 이를 활용하면 로봇 제어 시스템에서 <strong>더 빠른 피드백 루프</strong>를 구현할 수 있을 것으로 기대된다.</p>
<p>또한 DIGIT 360은 <strong>정밀한 힘 측정 (정확도 ~1 mN)</strong>과 <strong>다축(force/torque) 정보</strong>를 제공하므로, PP-Tac에서와 같은 학습 기반 정책 없이도 <strong>보다 직접적인 힘 제어 알고리즘</strong>을 설계할 수 있는 가능성이 있다. 예를 들어, 미끄럼 감지를 위해 PP-Tac은 촉각 영상을 학습시켜 분류하였으나, DIGIT 360이라면 내장된 힘/전단 센서로부터 임계값 비교만으로 실시간 미끄럼 검출이 가능할 수 있다. 그럼에도 불구하고, <em>PP-Tac 연구가 가지는 의의</em>는 이러한 복잡한 하드웨어 없이도 <strong>상대적으로 저비용의 단일모달 센서와 고차원 학습 알고리즘의 조합으로 높은 성능을 달성</strong>했다는 점이다. DIGIT 360과 같은 센서가 향후 보급되면, PP-Tac의 확산 정책 역시 더욱 풍부한 감각 정보를 활용해 성능을 높이거나, 혹은 센서 내 처리로 단순화된 제어(예: 센서에서 전처리된 피드백 신호만으로 제어)로 대체되는 등 <strong>여러 연구 방향의 확장</strong>이 가능할 것으로 보인다. 요약하면, 현재 PP-Tac의 알고리즘은 R-Tac 센서의 특성에 맞춰 설계되었지만, 차세대 촉각센서인 DIGIT 360의 등장으로 <strong>센서-알고리즘 공동설계</strong>의 중요성이 더욱 커졌다고 할 수 있다.</p>
<p><br></p>
</section>
</section>
<section id="센서-및-하드웨어-설계-sensor-and-hardware-design" class="level2">
<h2 class="anchored" data-anchor-id="센서-및-하드웨어-설계-sensor-and-hardware-design">3. 센서 및 하드웨어 설계 (Sensor and Hardware Design)</h2>
<section id="원형-촉각-센서-r-tac의-구조와-특징-design-and-characteristics-of-r-tac" class="level3">
<h3 class="anchored" data-anchor-id="원형-촉각-센서-r-tac의-구조와-특징-design-and-characteristics-of-r-tac">3.1 원형 촉각 센서 R-Tac의 구조와 특징 (Design and Characteristics of R-Tac)</h3>
<p>PP-Tac 시스템의 핵심 하드웨어인 <strong>R-Tac 센서</strong>는 원형(둥근) 형태의 <strong>시각 기반 촉각 센서</strong>(Vision-Based Tactile Sensor, VBTS)이다. 설계의 출발점은 기존 평면형 촉각센서로는 얇은 물체의 다방향 접촉을 포착하기 어려우므로, <strong>손가락 끝 모양에 맞는 반구형 센서</strong>를 만들자는 것이다. 저자들은 효과적인 조작을 위한 다섯 가지 설계 원칙을 제시했는데, <strong>(1) 전방위 감지:</strong> 반구형 돔 구조로 <strong>360도 방향</strong>의 접촉을 모두 느낄 수 있을 것, <strong>(2) 고해상도:</strong> 미세한 변형까지 포착해 3차원 깊이 재구성과 미끄럼 감지에 충분한 해상도를 갖출 것, <strong>(3) 제조 용이성과 저비용:</strong> 센서 구성품은 기성품이거나 간단히 제작 가능해야 하며 개당 약 $60 수준의 낮은 부품비용을 가질 것, <strong>(4) 보정(calibration)의 효율성:</strong> 조명 제어를 단순화하여 다채널(RGB) 센서들이 요구하는 복잡한 교정 작업 없이도 신속히 보정 가능할 것, <strong>(5) 고속 데이터 전송:</strong> 센서 출력 데이터량이 가벼워 여러 개 센서의 정보를 실시간 송신해도 병목이 없을 것 등이다. 이러한 원칙을 만족하기 위해 <strong>R-Tac는 단일 채널(모노크롬) 비전 촉각 방식</strong>을 채택하였다.</p>
<p><strong>구조:</strong> R-Tac는 손가락 끝에 장착되는 <strong>반구형 엘라스토머 돔</strong>과 그 내부에 삽입된 <strong>광원 및 카메라 모듈</strong>로 이루어진다. *접촉 모듈(Contact module)*이라 불리는 센서의 외피는, 사용자가 손가락으로 누르는 <strong>변형 가능 표면층(perception layer)</strong>과 내부 지지를 위한 <strong>투명한 골격(skeleton)</strong>, 그리고 주변에 배치된 <strong>원형 LED 조명 링</strong> 및 <strong>광 확산판(diffuser)</strong>으로 구성된다. 센서 표면층은 부드러운 반투명 실리콘(Ecoflex, Shore 00-10 경도)으로 두께 약 2 mm로 제작되어, 접촉 시 적절한 변형을 일으키면서도 <strong>내부의 빛 반사 특성 변화</strong>를 통해 변형 깊이를 영상 신호(밝기 변화)로 전환해준다. 표면 아래의 골격은 더 단단한 PDMS 실리콘(Shore 50A)으로 만들어져 센서의 기본 형상을 유지하고 과도한 변형을 방지한다. 최상층에는 <strong>얇은 불투명 코팅층</strong>(Smooth-On사의 검은색 Psycho Paint 도료)을 에어브러시로 도포하여, 외부 광원의 간섭을 막고 내부 조명에 의해서만 밝기 패턴이 나타나도록 했다. 이러한 <em>모노크롬 촉각 감지 원리</em>는 <strong>“어두워지는 정도로 변형 깊이를 측정”</strong>하는 아이디어로, 3채널 RGB 조명 방식에 비해 <strong>구현이 단순하고 신뢰성 높다</strong>고 알려져 있다. 실제로 R-Tac는 내부 LED가 하얀 빛을 균일히 비추는 가운데, 손가락이 누르면 해당 부위가 어두워지는 패턴으로 변형을 인식하며, 이는 기존 연구 <em>Dtact</em>의 “darkness→depth” 원리에 영감을 받아 적용되었다고 한다.</p>
<p><strong>카메라 및 응답속도:</strong> 반구 내부에는 <strong>전용 초소형 카메라 모듈</strong>이 장착되어 변형에 따른 밝기 이미지를 실시간 촬영한다. 사용된 센서는 <em>OmniVision OV9281</em> 흑백 CMOS 카메라로, 글로벌 셔터를 지원하여 빠르게 움직이는 접촉도 블러 없이 포착 가능하다. 이 카메라는 최대 <strong>120 FPS</strong>로 동작하며 해상도는 <strong>1280×800 픽셀 (약 1메가픽셀)</strong>이다. R-Tac는 모노크롬 영상이기 때문에 프레임 당 데이터량이 컬러 대비 1/3 수준으로 적으며, USB를 통한 전송 시 <strong>약 100ms 정도의 낮은 지연(latency)</strong>을 보인다. 이는 4개의 센서를 동시에 운영할 때도 큰 무리 없는 수준으로, 저자들은 모노크롬 방식 덕분에 <strong>대역폭 한계를 최소화</strong>하여 다지(多指) 시스템 구현이 수월했다고 언급한다.</p>
<p><strong>보정 (Calibration):</strong> R-Tac 센서는 구조를 단순화한 덕분에 <strong>보정 과정도 효율적</strong>이다. 일반적인 GelSight 계열 RGB 촉각센서는 곡면의 경우 삼색 조명으로 인한 불균일 조도로 보정이 매우 번거롭고 CNC 머신 등을 이용한 수천 장의 데이터 수집이 필요했지만, R-Tac는 <strong>단 30장의 이미지 촬영</strong>만으로 깊이 재구성 모델을 구축할 수 있었다고 보고된다. 방법론적으로는, 먼저 3D 프린팅한 압입 장치로 센서를 누르면서 29장의 이미지를 촬영하여 카메라의 <strong>내·외부 파라미터</strong> 및 <strong>곡면 투영 모델</strong>을 계산한다. 이후 알려진 크기의 구형 볼을 센서에 한번 눌러 얻은 영상 한 장으로 <strong>픽셀 강도→깊이</strong> 변환 함수를 보정한다. 이렇게 2단계로 얻은 보정함수를 통해 임의의 촉각 이미지에 대해 그레이스케일 값 분포를 <strong>높이 맵(depth map)</strong>으로 실시간 변환할 수 있게 된다. 내부 엘라스토머와 조명 모듈의 품질을 균일하게 유지한 덕분에, <strong>픽셀 세기의 표준편차가 6 이하</strong>로 매우 균질한 반응을 얻었고, 이는 보정 정확도를 높여준다. 논문에서는 보정된 R-Tac로 얻은 깊이 재구성 결과가 실제 압입 깊이와 평균 0.1 mm 오차 이내로 일치함을 확인하였다.</p>
<p><strong>접촉력 추정:</strong> R-Tac는 본질적으로 영상 기반 센서이지만, <strong>접촉력(압력) 추정</strong>도 가능하다. 저자들은 탄성체 접촉 이론에 따라 <strong>센서 변형 깊이</strong>가 곧 누르는 힘에 비례한다고 가정하였다. 실제 정밀한 힘 값을 역산하지는 않지만, 예컨대 변형값 <span class="math inline">d</span>가 0이면 접촉력이 0, <span class="math inline">d</span>가 커질수록 더 큰 힘으로 누르는 것으로 간주할 수 있다. PP-Tac 알고리즘은 이 상대적인 힘 정보를 이용하여 앞서 설명한 바와 같이 <strong>손가락 간 접촉력 균형을 맞추고 미끄럼을 방지</strong>한다. 한편 Digit 360과 달리 R-Tac에는 별도의 힘센서가 없으므로, 정밀한 뉴턴 단위의 힘 추정보다는 <strong>변형 정도를 통한 간접적인 힘 제어 피드백</strong>에 중점을 두고 있다.</p>
</section>
<section id="allegro-로봇-손과의-통합-integration-with-allegro-hand" class="level3">
<h3 class="anchored" data-anchor-id="allegro-로봇-손과의-통합-integration-with-allegro-hand">3.2 Allegro 로봇 손과의 통합 (Integration with Allegro Hand)</h3>
<p>R-Tac 센서는 <strong>Wonik Robotics사의 Allegro Hand</strong>에 커스터마이징되어 통합되었다. Allegro Hand는 사람 손과 유사한 4개의 손가락(엄지 포함)과 총 16개의 자유도를 가진 덱스터러스 로봇 핸드로, 각 손가락의 세 마디 관절(DIP, PIP, MCP)들과 엄지의 복합 관절(CMC 등)이 모터 구동된다. 연구진은 이 손가락들의 <strong>말단부</strong>를 개조하여 R-Tac 촉각센서를 장착하였다. Fig. 2의 하드웨어 설계도에서 확인할 수 있듯이, 센서 결합을 위해 손가락 끝 부분에 맞게 <strong>센서 하우징과 배선</strong>을 설계하였고, (b)의 exploded view에 각 부품이 상세히 나와 있다. 센서로부터 출력되는 영상 데이터는 각각 별도로 USB를 통해 PC로 전송되며, 모터 제어 신호는 Dynamixel XM 시리즈 서보모터 16개를 구동하기 위해 U2D2 허브를 통해 전달된다. 전체 손-팔 시스템은 <strong>Franka Emika 7-자유도 로봇 팔</strong> 끝에 Allegro Hand를 장착한 형태이며, 팔 쪽 제어는 Ethernet으로 연결된 PC에서 이루어진다. 이와 같이 <strong>고속 통신망</strong>과 <strong>병렬 처리</strong>를 활용하여, 4개의 촉각센서와 16개의 모터, 그리고 팔 제어까지 모두 <strong>실시간 동기</strong>를 맞춰 구동할 수 있었다. 하드웨어 통합의 난이도를 낮추기 위해 R-Tac 센서는 최대한 <strong>컴팩트하고 가벼운 구조</strong>로 만들어졌는데, 부품비 $60, 제조 3일 이내 가능이라는 지표가 보여주듯 소형 로봇 손에 <strong>부담 없이 여러 개 부착</strong>할 수 있는 점이 큰 장점이다. 실제 본 시스템은 <strong>4개 손가락 모두에 촉각센서를 장착</strong>한 드문 사례로서, 이를 통해 손가락 간 <strong>협응적인 촉각 탐지</strong>(예: 한 손가락이 미끄럼을 감지하면 전체 제어에 반영)를 시연하였다.</p>
</section>
<section id="digit-360과의-비교-분석-comparison-with-digit-360-sensor-system" class="level3">
<h3 class="anchored" data-anchor-id="digit-360과의-비교-분석-comparison-with-digit-360-sensor-system">3.3 DIGIT 360과의 비교 분석 (Comparison with DIGIT 360 Sensor System)</h3>
<p>세계적인 추세로 곡면형 촉각 센서에 대한 연구가 활발하며, Meta AI의 <strong>DIGIT 360</strong>은 그 중에서도 가장 첨단을 달리는 시스템이다. 이하에서는 R-Tac와 DIGIT 360을 <strong>구조, 해상도, 설치 유연성, 다지 활용, 실시간성, 재현성</strong> 측면에서 비교한다.</p>
<ul>
<li><p><strong>구조적 특성:</strong> R-Tac는 <strong>반구형 센서 팁</strong>으로 Allegro 손가락 끝에 딱 들어맞도록 소형 설계되었다. 단일 카메라와 LED 조명, 투명/불투명 실리콘 층들로 구성된 <strong>구조적으로 단순한 디자인</strong>이며, 손가락당 하나의 센서로 <strong>국지적인 촉각</strong>을 제공한다. 반면 DIGIT 360은 <strong>손가락 전체 모양에 가까운 인공 지문/피부</strong>를 구현한 센서로, 내부에 다수의 센싱 소자가 집적된 <strong>복합 구조</strong>이다. 약 <strong>830만 개의 taxel</strong>(촉각 화소)이 존재하는 고해상도 촉각 피부, 그리고 힘, 전단, 온도, 진동, 심지어 냄새까지 검출하는 다양한 센서들이 하나의 손가락 팁에 통합돼 있다. 이는 R-Tac의 단일 모달(변형 시각화) 센서와 달리 <strong>멀티모달 센싱 아키텍처</strong>를 지닌다. 구조가 복잡한 만큼 DIGIT 360에는 처리용 회로와 NPU까지 내장되어 사실상 <strong>“작은 로봇 손가락”</strong>처럼 동작하는데, 이러한 설계는 사람 손가락의 다양한 수용기를 전자적으로 모사한 것이다. 요약하면 R-Tac는 <strong>필요한 요소만 담은 최소주의적 설계</strong>라면, DIGIT 360은 <strong>가능한 모든 촉각 능력을 통합한 총합적 설계</strong>라고 볼 수 있다.</p></li>
<li><p><strong>데이터 해상도:</strong> 두 센서 모두 고해상도를 지향하지만, 접근 방식이 다르다. R-Tac의 해상도는 주로 <strong>카메라 픽셀 해상도</strong>로 결정되며 1280×800 (약 1 MP 수준)이다. 이는 접촉 면적 대비 충분히 세밀하여 작은 주름이나 모서리도 감지 가능하다고 논문에서 언급되며, 실제 실험에서 수 mm 두께의 종이, 비닐까지 확실히 인식해낸다. 반면 DIGIT 360은 <strong>약 830만 개</strong>의 taxel을 갖추고 있다고 하며, 이는 촉각 표면의 <strong>공간 분해능이 7 μm</strong>에 달할 정도로 미세함을 의미한다. 다시 말해 수십 μm의 촉각 특징까지 구분 가능하여, 예를 들어 지문이나 매우 고운 질감도 해상할 수 있다. 또한 힘 해상도 면에서도 DIGIT 360은 <strong>1 mN 수준의 정밀도</strong>로 정상력 및 전단력을 측정할 수 있어, R-Tac처럼 간접 추정이 아닌 <strong>정량적인 힘 피드백</strong>을 바로 얻을 수 있다. 요약하면, <strong>공간적 해상도와 힘 해상도 모두 DIGIT 360이 훨씬 우수</strong>하지만, 그만큼 데이터량도 방대하다. 한 프레임당 수 메가픽셀 데이터를 다뤄야 하고 다중 모달 신호까지 포함되므로, R-Tac의 한 프레임(약 1MB 미만의 grayscale 이미지)보다 훨씬 큰 정보를 생성한다. 이를 <strong>실시간 처리</strong>하기 위해 DIGIT 360은 센서 내 전용 하드웨어를 쓴 반면, R-Tac는 PC 기반 처리로도 감당할 수 있을 가벼운 데이터 구조를 택했다.</p></li>
<li><p><strong>설치 유연성:</strong> R-Tac는 설계 목표부터 <strong>멀티 핑거 스케일 업</strong>을 염두에 두고 있었다. 3D 프린터와 몰딩 기법으로 쉽게 복제 가능하고, 부품도 쉽게 조달하여 여러 개를 동시에 제작할 수 있다. 실제 본 논문에서도 4개의 센서를 제작해 한 손에 장착했고, 필요하다면 두 손, 혹은 그 이상의 손가락에도 확장 가능할 것이다. 센서 자체가 가벼워서(구체적인 무게는 미제시지만 작은 카메라와 실리콘으로 구성) 로봇 손의 동작 성능을 크게 해치지 않는 것도 장점이다. DIGIT 360은 인간 손가락 크기에 가깝게 만들어졌다고는 하나, 그 복잡도 때문에 <strong>현재로선 단가가 높고 제조가 까다로울 것</strong>으로 보인다. 다만 Meta AI는 DIGIT 360을 <strong>모듈형 플랫폼으로 공개</strong>하여 연구자들이 사용할 수 있도록 지원하고 있어, 향후 여러 손가락에 DIGIT 360을 달아 사용하는 사례도 충분히 가능할 것이다. 현실적인 설치 시 고려해야 할 것은 <strong>전원 및 통신</strong>인데, R-Tac는 각 센서마다 USB로 PC에 연결하면 되나 DIGIT 360은 여러 센서를 다룰 경우 <strong>버스 구조의 통합 보드(Digit Plexus 등)와 전용 SDK</strong> 등이 필요할 수 있다. 유연성 면에서 R-Tac가 <strong>단순 플러그앤플레이</strong>에 가까운 반면, DIGIT 360은 <strong>고기능 시스템에 맞는 체계적인 인프라</strong>를 요한다. 따라서 <strong>연구 개발 초기 단계에서는 R-Tac 같은 접근</strong>이 빠르게 멀티센서 실험을 하기 수월하며, <strong>대규모 정교한 시스템 구축 단계에서는 DIGIT 360의 모듈 플랫폼</strong>이 힘을 발휘할 것으로 전망된다.</p></li>
<li><p><strong>다지(多指) 제어 적합성:</strong> 복수의 손가락에 촉각 센서를 장착하여 <strong>동시에 촉각제어</strong>를 하는 능력은 두 시스템 모두 지향하지만, 현재까지 구현 수준에는 차이가 있다. R-Tac는 앞서 언급했듯 <strong>4개 손가락 전부에 센서 부착</strong>을 실현하여, <strong>손가락간 협조 제어</strong>를 성공적으로 선보였다. 이는 각 손가락 센서로부터 독립적으로 미끄럼을 감지하되, 최종 제어 정책에서는 이를 통합적으로 반영하는 구조다. 반면 DIGIT 360은 아직 한 손가락 단위의 성능 검증 위주로 발표되었으며, 동시에 여러 개를 한 손에 달고 협응 제어한 사례는 (2025년 시점까지는) 공개되지 않았다. 그러나 DIGIT 360의 <strong>설계 철학</strong>이 애초에 로봇 손 전체의 표준 촉각 플랫폼을 제공하려는 것이므로, 장기적으로는 한 손의 모든 손가락에 DIGIT 360 센서 팁을 붙여 <strong>완전 촉각 피드백 손</strong>을 구현할 가능성이 높다. 그 경우 각 손가락 센서가 개별적으로 <strong>풍부한 정보를 제공</strong>하므로, R-Tac처럼 중앙처리에서 영상으로 미끄럼을 추정하기보다 <strong>센서 자체의 분산처리</strong>를 통해 각 손가락이 자율적으로 미끄럼을 억제하고, 상위 레벨에서 손가락 사이 협조만 조율하는 형태의 <strong>분산 제어</strong>도 구상해볼 수 있다. 요컨대, 현 시점에선 R-Tac가 <strong>실증된 다지 통합 사례</strong>를 보여주었다는 의의가 있고, DIGIT 360은 <strong>그보다 훨씬 풍부한 기능으로 다지 활용을 지원할 플랫폼</strong>으로서 기대된다.</p></li>
<li><p><strong>실시간성:</strong> 실시간 성능은 앞서 알고리즘 부분에서도 일부 비교되었지만, 센서 관점에서 정리하면 다음과 같다. R-Tac는 <strong>120 Hz 프레임레이트, 약 100 ms 지연시간</strong>으로 동작하여 일반적인 로봇 제어 루프(&gt;10 Hz)에 충분히 대응한다. 그러나 매우 빠른 동작(예: 수 ms 내 변화)까지 포착하기엔 한계가 있다. 반면 DIGIT 360은 <strong>10 kHz까지의 진동</strong>도 감지할 수 있고, 센서 내 신경망 가속기로 <strong>로컬 피드백을 즉각 계산</strong>할 수 있어, 이론적으로는 수백 Hz ~ kHz 대의 제어 피드백도 가능하다. 이는 산업용 제어나 미세한 촉각탐색 등 초고속 응답이 필요한 응용에 DIGIT 360이 유리함을 시사한다. 예를 들어, 미끄럼 발생 전 수백 Hz의 미세 진동을 감지해 바로 그립을 조정하는 것이 가능해진다. 다만 그러한 고주파 정보는 표면 재질에 따라 노이즈도 많으므로 활용을 위해선 정교한 필터링과 해석이 필요할 것이다. 한편, <strong>데이터 전송</strong> 면에서는 R-Tac의 모노크롬 설계 덕에 4개 합산 480 fps 데이터도 무리 없이 PC에서 처리할 수 있었던 반면, DIGIT 360은 방대한 데이터 처리를 센서 자체에서 해 주므로 <strong>호스트 PC에는 요약된 정보</strong>만 보내 효율을 높인다. 예컨대 미끄럼 여부, 접촉 지점의 힘 등 <strong>의미 있는 피처만 출력</strong>하는 식이다. 이러한 설계 차이는 <strong>로봇 시스템의 전체 구조</strong>에도 영향을 주는데, R-Tac는 비교적 <strong>중앙집중식 처리</strong>이고 DIGIT 360은 <strong>에지(edge) 컴퓨팅 분산처리</strong>라고 볼 수 있다.</p></li>
<li><p><strong>재현성 및 확장성:</strong> R-Tac는 연구팀이 내부에서 설계·제작한 프로토타입이지만, 논문에서 상세한 제조 방법(몰드 제작, 실리콘 경화, 페인팅 과정)을 공개하고 있고, 부품 역시 특수한 것이 아니어서 <strong>다른 연구자들이 비교적 쉽게 재현</strong>할 수 있다. 부품비 $60은 기존 상용 촉각센서들과 견줘 매우 저렴한 편으로, 예산이 한정된 연구실에서도 <strong>여러 개 제작하여 실험</strong>하기에 적합하다. 또한 보정 절차가 간단하여 새로운 센서를 만들 때마다 복잡한 교정 작업을 반복할 필요도 적다. 반면 DIGIT 360은 Meta에서 <strong>오픈소스로 플랫폼을 제공</strong>한다고는 하나, 그 구성품(고해상도 카메라, 광학계, 마이크로컨트롤러, IMU, 온도/화학 센서 등)이 다소 전문적이고 조립 공정도 복잡할 수 있다. 현재까지는 Meta와 협업한 일부 기관에서만 사용 예시가 있으며, 일반 연구자들이 직접 제작하기엔 진입장벽이 있다. 다만 GelSight사를 통해 상용화될 가능성도 있어, 향후 <strong>표준 제품</strong>으로 구매가 가능해지면 상황이 달라질 수 있다. 결국 재현성 측면에서는 R-Tac 같은 <strong>간단한 DIY 접근</strong>이 단기간에는 유리하나, 장기적으로 DIGIT 360처럼 <strong>표준화된 플랫폼</strong>이 나오면 여러 연구 간 데이터 호환과 비교평가가 쉬워지는 장점이 있을 것이다.</p></li>
</ul>
<p><br></p>
</section>
</section>
<section id="결론-conclusion" class="level2">
<h2 class="anchored" data-anchor-id="결론-conclusion">4. 결론 (Conclusion)</h2>
<p><em>PP-Tac: Paper Picking using Tactile feedback</em> 논문은 <strong>촉각 센싱과 학습 제어의 밀접한 결합</strong>을 통해 로봇에게 인간에 가까운 종이 집기 능력을 부여한 획기적인 연구이다. 알고리즘적으로는 <strong>확산 모델 기반 정책 학습</strong>을 활용하여 복잡한 손-팔 협조 동작을 효과적으로 생성하고, <strong>실시간 촉각 피드백</strong> (특히 미끄럼 감지)을 통합함으로써 얇은 변형체를 안정적으로 다룰 수 있음을 보여주었다. 하드웨어적으로는 <strong>원형 촉각센서 R-Tac</strong>를 개발하여 기존 촉각 센싱의 한계를 극복하고 다수 손가락에의 적용 가능성을 입증하였다. R-Tac의 단순하고 저렴한 설계는 향후 촉각 센서의 <strong>대량 배치와 복합 시스템 구축</strong>에 하나의 방향성을 제시하며, 이미 상용화된 Meta의 <strong>DIGIT 360</strong> 등의 최신 센서와 대비되는 접근법으로서 의미가 크다. DIGIT 360이 <em>“슈퍼휴먼”</em> 성능을 지향하는 고사양 센서라면, PP-Tac의 R-Tac는 *“실용적 문제 해결”*을 위해 필요한 최소한의 센싱과 알고리즘 지능을 결합한 사례로 볼 수 있다. 궁극적으로 이 두 흐름은 상호 보완적이며, 앞으로 고성능 촉각센서를 PP-Tac와 같은 학습 제어기와 결합하는 연구가 진행된다면 더욱 향상된 결과가 기대된다. 예를 들어, DIGIT 360의 풍부한 신호를 확산 정책에 통합하거나, 센서 내 AI를 이용해 저지연 피드백 제어를 구현하는 방향으로 발전할 수 있다.</p>
<p>본 리뷰에서는 논문의 주요 기여를 <strong>학술 리뷰</strong> 형식으로 분석하여 3년차 로보틱스 연구자 수준의 독자가 이해할 수 있도록 정리하였다. <em>PP-Tac</em> 연구는 로봇 촉각 분야에 새로운 지평을 연 사례로 평가되며, 이는 인간 수준의 섬세한 조작능력을 로봇에 부여하기 위한 향후 연구들에 귀중한 밑거름이 될 것이다.</p>
<p><strong>참고 문헌:</strong> PP-Tac 논문 원문 및 DIGIT 360 논문/자료 등. (각주 인용 번호는 본문에 표시)</p>
<!--
> 촉각으로 느끼고 움직이는 손: R-Tac과 PP-Tac의 내부 설계와 영상 파이프라인

최근 몇 년간 로봇공학 분야에서 ‘촉각(Tactile)’은 또 한 번의 혁신을 이끌 핵심 키워드로 부상하고 있습니다. 특히 얇고 변형 가능한 종이, 봉투, 천 등은 기존의 비전(vision) 중심 로봇 핸드로는 잡기가 매우 까다로운 대상이었으나, 2025년 RSS에 발표된 **PP-Tac: Paper Picking Using Tactile Feedback in Dexterous Robotic Hands** 연구는 **촉각 센서 R-Tac**과 **확산 모델 기반 동작 제어**를 결합해 이 한계를 극복하는 새로운 지평을 열었습니다.

본 글에서는 논문의 심장부라 할 수 있는 **R-Tac 센서의 구조**와 **촉각 영상 처리 파이프라인**을 중심으로, 실제 시스템이 어떻게 얇은 종이를 “느끼고” 안정적으로 집어 올리는지를 깊이 있게 다룹니다.

---

## 1. 얇은 물체, 왜 이렇게 어려운가?

로봇이 물체를 집는(Grasping) 작업은 겉보기보다 훨씬 복잡합니다. 두껍고 단단한 사물(컵, 상자 등)은 비교적 쉽지만, 종이나 봉투처럼 **얇고, 휘고, 변형 가능한 대상**은

* “잡는 위치”를 찾기도 힘들고
* 잡는 순간 변형/구김이 발생해
* 접촉 신호(접촉력, 미끄러짐 등)가 약해지기 때문입니다.

비전(카메라) 정보만으로는 실제 접촉 상태를 알기 어렵기 때문에, ‘손끝에서 직접’ 느끼는 **촉각 정보**가 필수적입니다. 이 핵심 감각을 담당하는 것이 바로 R-Tac 센서입니다.

---

## 2. R-Tac 센서 내부: 혁신적 설계

### (1) 구조적 특징

* **반구형(돔형) 디자인**
  R-Tac은 손가락 끝에 장착되는 반구형 돔(hemispherical dome) 구조로, 일반적인 평면형 촉각 센서와 달리 어느 방향에서든지 접촉을 감지할 수 있습니다. 이는 종이나 천 등, ‘정해진’ 방향이 없는 물체를 다룰 때 특히 유리합니다.

* **고해상도 모노크롬 카메라**
  내부에는 소형 모노크롬 CMOS 카메라가 내장되어 있습니다. 이 카메라는 센서의 투명/반투명한 엘라스토머(젤 형태의 실리콘 등) 표면의 변형(눌림, 구겨짐, 미끄러짐 등)을 초당 수십 프레임의 속도로 촬영합니다.

* **균일한 LED 조명**
  카메라와 함께 배열된 LED 조명이 엘라스토머 표면을 밝게 비춥니다. 촬영되는 영상에는 접촉부에서 생긴 미세한 주름, 점, 패턴 변화가 선명하게 나타나며, 바로 이 정보가 ‘손끝 감각’의 디지털 신호가 됩니다.

### (2) 저비용, 쉬운 대량 적용

논문에 따르면 R-Tac의 구성 요소(모노크롬 카메라, LED, 엘라스토머, 간단한 외장 케이싱)는 한 개당 약 60달러 수준으로, 기존 고가의 젤사이트(GelSight) 등 상용 촉각 센서 대비 제작 비용이 대폭 낮습니다.
또한, 센서 크기가 작고 손가락별로 별도의 캘리브레이션(보정) 과정이 단순해 **16자유도(16 DoF)의 Allegro Hand**와 같은 다자유도 로봇 핸드에 대량 장착도 용이합니다.

### (3) Omnidirectional 감지

돔형 센서는 평면 촉각 센서와 달리 여러 방향에서 물체가 접근해도 접촉 패턴이 왜곡 없이 촬영됩니다. 예를 들어, 얇은 종이가 손가락 사이로 비스듬히 들어와도 slip(미끄러짐)이나 force(힘)의 변화를 **실시간**으로 감지할 수 있죠.

---

## 3. 촉각 영상 처리 파이프라인

센서가 ‘느낀’ 정보를 어떻게 디지털 신호로 바꿔 로봇 제어에 사용하는지, 그 핵심 프로세스를 아래와 같이 정리할 수 있습니다.

### (1) 데이터 수집: Non-contact + 연속 프레임

R-Tac는 **무접촉 프레임(non-contact frame)** 한 장과 그 후 **5개 연속 접촉 프레임**을 고속으로 수집합니다.
이 프레임 시퀀스는

* 접촉 이전과 이후의 표면 변형 차이
* 미끄러짐(slip) 발생 유무
* 눌림 강도/방향 변화
  등을 분석하는 데 사용됩니다.

### (2) Slip Detection 신경망

수집된 촉각 영상 시퀀스는 다음과 같은 경로로 slip(미끄러짐) 여부를 예측하게 됩니다.

1. **CNN(합성곱 신경망)**
   각 프레임별로 CNN이 적용되어 표면 변형, 주름, 패턴 등의 특징(feature map)을 추출합니다.

2. **특징 벡터 연결(Concatenation)**
   6장의 프레임(feature map)을 하나의 벡터로 연결(concatenate)합니다.

3. **MLP(다층 퍼셉트론)**
   최종적으로 연결된 벡터를 MLP(fully connected 신경망)에 입력, slip이 발생했을 확률(P\_slip)을 산출합니다.

이 방식은 약 20분 분량의 tactile 로그를 기반으로 학습됩니다. 이 중 약 40%는 slip, 60%는 non-slip 상태로 레이블링 되었습니다.
추론 시, P\_slip이 0.75 이상이면 slip이 발생했다고 판단하고, 즉시 로봇 핸드의 grip force(집는 힘)를 조정합니다.

### (3) 실시간 피드백 루프

이 모든 촉각 정보(슬립, 접촉력 변화 등)는

* **프로프리오셉션(proprioception: 관절 위치, 각도 등 내부 감각) 정보**와 함께
* **Diffusion 기반 policy**(동작 생성 정책)에 실시간 입력됩니다.

로봇 핸드는 촉각 신호에 따라

* 미끄러짐이 감지되면 즉시 힘을 높이고
* 너무 강하게 집었을 때는 힘을 줄여
* 항상 적절한 그립을 유지하도록 동작을 미세 조정합니다.

이 피드백 루프 구조 덕분에, 얇고 미끄러운 종이나 봉투도 손쉽게 ‘미끄러지지 않게’ 집을 수 있게 되었습니다.

---

## 4. 전체 시스템: 촉각, 비전, 강화학습의 통합

이전까지의 로봇 핸드 연구는 비전 기반(카메라+3D 센싱)이 주류였지만, 촉각 정보 없이 얇은 물체의 ‘실제’ 접촉을 알 수 없었습니다.
PP-Tac은 **촉각(R-Tac) + proprioception + diffusion policy**의 조합으로 아래와 같은 완전 새로운 프레임워크를 제안합니다.

* **휴먼 수준의 ‘슬라이드 → 핀치’ 동작**을 imitation learning 기반으로 데이터화
* 수집된 데이터로 diffusion policy를 학습, 노이즈에서 시작해 점진적으로 실제 동작을 복원
* 촉각/관절 정보 기반 피드백 루프로 실시간 미끄러짐 대응 및 force control
* 다양한 두께·재질·표면의 얇은 물체(플라스틱, 실크, 종이, 봉투 등)에서 일관된 grasp 성공률(87.5%) 달성

---

## 5. 실험 결과와 ablation study

논문에서는 R-Tac과 diffusion policy 각각의 기여도를 ablation study로 평가합니다.

* **촉각 센서 없이**: slip 감지가 불가능, grasp 실패율 급증
* **diffusion policy 없이**: 단순 동작 replay에서는 일반화 능력 부족, 다양한 환경에서 실패율 증가
* **둘 다 결합**: 약 87.5% 성공률로 SOTA 달성

특히 R-Tac의 slip detection은 약 86% 정확도로, 실시간 피드백 제어에서 핵심 역할을 담당합니다.

---

## 6. 한계와 개선 방향

아직 R-Tac의 내구성(엘라스토머의 장기 변형, 센서 마모 등)과 조명 조건(강한 주변광 등)에 따른 한계가 남아있습니다.
추후 연구에서는

* 더 빠르고 경량화된 영상 처리 네트워크(CNN 경량화, Hyperdimensional computing 등)
* 더 다양한 tactile 센서(DIGIT, GelSight 등)와의 융합
* 다양한 형태/재질/두께의 물체에 대한 실험 확장
  등이 이루어질 것으로 기대됩니다.

---

## 7. 결론: tactile vision 기반 grasping의 미래

PP-Tac 시스템은 “손끝 감각”을 실시간 비전+신경망+정책학습으로 해석, 종이나 봉투처럼 취급이 어려웠던 대상도 손쉽게 집을 수 있는, 휴먼-레벨 dexterity(기민함)의 새로운 기준을 제시했습니다.

**촉각 기반 로봇 제어**는 이제 막 시작점에 섰지만, R-Tac과 같은 저비용 고성능 센서의 등장, 그리고 촉각-강화학습-비전의 통합 파이프라인은 향후

* 자동 포장/배송,
* 의료/수술용 로봇,
* 미세 작업(섬유, 전자부품 등)
  분야에 큰 파급 효과를 불러올 것으로 전망됩니다.

-->


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<script src="https://utteranc.es/client.js" repo="curieuxjy/blog_comments" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Copyright 2024, Jung Yeon Lee</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/curieuxjy">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>