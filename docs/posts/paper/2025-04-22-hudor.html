<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.24">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-04-21">
<meta name="description" content="Bridging the Human to Robot Dexterity Gap through Object-Oriented Rewards">

<title>📃HuDOR 리뷰 – Curieux.JY</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../profile.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-1e438c382a17f6d88d3993662a872df6.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-a37c72dd2dbac68997fcdc15a3622e78.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-ecf89aac047581c664da7ae53d704519.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-b009f778f5cec7f34f624408a2b5b543.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-ecf89aac047581c664da7ae53d704519.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-2NVZN2MJZT"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-2NVZN2MJZT', { 'anonymize_ip': true});
</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    window.setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      window.setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    window.hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(darkModeDefault) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const darkModeDefault = false;
    document.querySelector('link.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !window.hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
    };
    // Switch to dark mode if need be
    if (window.hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Curieux.JY</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../post.html"> 
<span class="menu-text">Post</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../note.html"> 
<span class="menu-text">Note</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">Jung Yeon Lee</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/curieuxjy"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#brief-review" id="toc-brief-review" class="nav-link active" data-scroll-target="#brief-review"><span class="header-section-number">1</span> Brief Review</a></li>
  <li><a href="#detail-review" id="toc-detail-review" class="nav-link" data-scroll-target="#detail-review"><span class="header-section-number">2</span> Detail Review</a></li>
  <li><a href="#hudor-객체-중심-보상으로-인간-로봇-조작-능력-격차-해소" id="toc-hudor-객체-중심-보상으로-인간-로봇-조작-능력-격차-해소" class="nav-link" data-scroll-target="#hudor-객체-중심-보상으로-인간-로봇-조작-능력-격차-해소"><span class="header-section-number">3</span> HuDOR: 객체 중심 보상으로 인간-로봇 조작 능력 격차 해소</a>
  <ul class="collapse">
  <li><a href="#배경-인간-동작-모방과-형상-격차-문제" id="toc-배경-인간-동작-모방과-형상-격차-문제" class="nav-link" data-scroll-target="#배경-인간-동작-모방과-형상-격차-문제"><span class="header-section-number">3.1</span> 배경: 인간 동작 모방과 형상 격차 문제</a></li>
  <li><a href="#hudor-방법론-객체-지향-보상을-통한-온라인-학습" id="toc-hudor-방법론-객체-지향-보상을-통한-온라인-학습" class="nav-link" data-scroll-target="#hudor-방법론-객체-지향-보상을-통한-온라인-학습"><span class="header-section-number">3.2</span> HuDOR 방법론: 객체 지향 보상을 통한 온라인 학습</a>
  <ul class="collapse">
  <li><a href="#시스템-구성-및-인간-시연-재현" id="toc-시스템-구성-및-인간-시연-재현" class="nav-link" data-scroll-target="#시스템-구성-및-인간-시연-재현"><span class="header-section-number">3.2.1</span> 1. 시스템 구성 및 인간 시연 재현</a></li>
  <li><a href="#객체-중심-포인트-추적과-보상-함수-설계" id="toc-객체-중심-포인트-추적과-보상-함수-설계" class="nav-link" data-scroll-target="#객체-중심-포인트-추적과-보상-함수-설계"><span class="header-section-number">3.2.2</span> 2. 객체 중심 포인트 추적과 보상 함수 설계</a></li>
  <li><a href="#residual-강화학습을-통한-정책-미세조정" id="toc-residual-강화학습을-통한-정책-미세조정" class="nav-link" data-scroll-target="#residual-강화학습을-통한-정책-미세조정"><span class="header-section-number">3.2.3</span> 3. Residual 강화학습을 통한 정책 미세조정</a></li>
  </ul></li>
  <li><a href="#실험-결과-인간-수준-동작-달성과-성능-비교" id="toc-실험-결과-인간-수준-동작-달성과-성능-비교" class="nav-link" data-scroll-target="#실험-결과-인간-수준-동작-달성과-성능-비교"><span class="header-section-number">3.3</span> 실험 결과: 인간 수준 동작 달성과 성능 비교</a></li>
  <li><a href="#결론-및-비평-의의와-한계" id="toc-결론-및-비평-의의와-한계" class="nav-link" data-scroll-target="#결론-및-비평-의의와-한계"><span class="header-section-number">3.4</span> 결론 및 비평: 의의와 한계</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-page-left" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">📃HuDOR 리뷰</h1>
  <div class="quarto-categories">
    <div class="quarto-category">paper</div>
    <div class="quarto-category">rl</div>
    <div class="quarto-category">online-imitation-learning</div>
  </div>
  </div>

<div>
  <div class="description">
    Bridging the Human to Robot Dexterity Gap through Object-Oriented Rewards
  </div>
</div>


<div class="quarto-title-meta column-page-left">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">April 21, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<ul>
<li><a href="https://arxiv.org/abs/2410.23289">논문 링크</a></li>
<li><a href="https://object-rewards.github.io/">프로젝트 Homepage</a></li>
<li><a href="">Code Repository</a></li>
</ul>
<ol type="1">
<li>🤖 HUDOR은 인간 비디오로부터 객체 중심 보상을 계산하여 다지 로봇 손의 온라인 fine-tuning을 가능하게 하는 새로운 프레임워크를 제시합니다.</li>
<li>🖐️ 이 방법은 시판되는 포인트 트래커에서 파생된 객체 지향 궤적을 사용하여 인간과 로봇 손의 형태학적 차이 및 시각적 차이에도 불구하고 의미 있는 학습 신호를 제공합니다.</li>
<li>🎯 HUDOR은 다양한 실험에서 기존의 imitation learning 방법보다 평균 2.64배 향상된 성능을 보여 온라인 보정이 필수적임을 강조합니다.</li>
</ol>
<blockquote class="blockquote">
<p>사람의 행동을 찍은 비디오로부터 직접 로봇을 훈련하는 것은 로보틱스와 컴퓨터 비전 분야에서 떠오르는 영역입니다. 두 손가락 그리퍼에서는 주목할 만한 진전이 있었지만, 이러한 방식으로 다지 로봇 손에 대한 자율적인 작업을 학습하는 것은 여전히 어렵습니다. 이러한 어려움의 주된 이유는 인간 손으로 훈련된 정책이 형태학적 차이로 인해 로봇 손으로 직접 전이되지 않을 수 있기 때문입니다. 본 연구에서는 인간 비디오로부터 직접 보상을 계산하여 정책의 온라인 미세 조정을 가능하게 하는 기술인 HUDOR를 제시합니다. 중요하게도, 이 보상 함수는 시판되는 포인트 트래커에서 파생된 객체 지향 궤적을 사용하여 구축되어 인간과 로봇 손 사이의 형태적 격차 및 시각적 차이에도 불구하고 의미 있는 학습 신호를 제공합니다. 뮤직 박스를 부드럽게 여는 것과 같이 사람이 작업을 해결하는 단일 비디오가 주어지면 HUDOR는 4개의 손가락이 달린 Allegro hand가 한 시간의 온라인 상호 작용만으로 작업을 학습할 수 있도록 합니다. 4가지 작업에 걸친 실험에서 HUDOR는 베이스라인보다 4배 향상된 성능을 보여줍니다</p>
</blockquote>
<hr>
<section id="brief-review" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Brief Review</h1>
<p>이 논문은 인간의 비디오로부터 다지 로봇 손의 섬세한 조작 정책을 학습하는 새로운 방법인 HUDOR를 제시합니다. 기존 방법들은 로봇 조작 데이터를 필요로 하거나, 인간의 개입이 필요한 반면, HUDOR는 단일 인간 비디오만으로 로봇 정책을 학습할 수 있도록 합니다.</p>
<p><strong>핵심 방법론</strong></p>
<p>HUDOR는 다음과 같은 세 단계를 거칩니다.</p>
<ol type="1">
<li><p><strong>데이터 수집</strong>: VR 헤드셋과 RGB 카메라를 사용하여 인간의 비디오와 손 포즈 궤적을 기록합니다. ArUco 마커를 이용하여 VR 프레임과 로봇 프레임 간의 상대적 변환을 계산하고, 인간의 손가락 끝 위치를 로봇 프레임으로 변환합니다.</p>
<ul>
<li><span class="math inline">a_{t,i}^w = H_{OW} a_{t,i}^o</span>: VR 프레임에서의 <span class="math inline">i</span>번째 손가락 끝 위치 <span class="math inline">a_{t,i}^o</span>를 월드 프레임으로 변환합니다. 여기서 <span class="math inline">H_{OW}</span>는 VR 프레임에서 월드 프레임으로의 동차 변환 행렬입니다.</li>
<li><span class="math inline">a_{t,i}^r = H_{RW}^{-1} a_{t,i}^w</span>: 월드 프레임에서의 손가락 끝 위치 <span class="math inline">a_{t,i}^w</span>를 로봇 프레임으로 변환합니다. 여기서 <span class="math inline">H_{RW}</span>는 로봇 프레임에서 월드 프레임으로의 동차 변환 행렬입니다.</li>
</ul></li>
<li><p><strong>초기 로봇 재생성</strong>: 인간의 손 포즈 궤적을 포즈 변환과 전체 로봇 역기구학(IK)을 사용하여 로봇에 전달합니다. IK 모듈은 로봇 팔과 손의 관절 각도를 조정하여 손가락 끝이 원하는 위치에 도달하도록 합니다. 손의 움직임을 우선시하기 위해 IK 최적화 과정에서 손 관절에 더 높은 학습률을 적용합니다.</p>
<ul>
<li><span class="math inline">j_{t+1}^* = I(a_t^r, j_t)</span>: IK 모듈은 원하는 손가락 끝 위치 <span class="math inline">a_t^r</span>과 현재 관절 위치 <span class="math inline">j_t</span>를 입력받아 다음 관절 위치 <span class="math inline">j_{t+1}^*</span>를 출력합니다.</li>
</ul></li>
<li><p><strong>잔여 정책 학습</strong>: 인간과 로봇 손의 형태적 차이와 VR 손 포즈 추정의 오류를 보정하기 위해 강화 학습(RL)을 사용하여 온라인 잔여 정책을 학습합니다. 특히, HUDOR는 객체 중심 궤적 매칭 보상 함수를 사용합니다.</p>
<ul>
<li><p><strong>객체 포인트 추적 및 궤적 매칭</strong>:</p>
<ul>
<li><strong>객체 상태 추출</strong>: langSAM 모델을 사용하여 첫 번째 프레임에서 객체의 분할 마스크 <span class="math inline">P_1</span>을 추출합니다.</li>
<li><strong>포인트 추적</strong>: Co-Tracker 알고리즘을 사용하여 RGB 이미지 궤적 <span class="math inline">\tau</span>에서 객체 포인트 <span class="math inline">p_t^i</span>를 추적합니다.</li>
<li><strong>궤적 매칭</strong>:
<ul>
<li><span class="math inline">\hat{P}_t = \frac{1}{N} \sum_{i=1}^{N} p_t^i</span>: 검출된 포인트의 중심 <span class="math inline">\hat{P}_t</span>를 계산합니다.</li>
<li><span class="math inline">\delta_t^{trans} = \hat{P}_t - \hat{P}_1</span>: 평균 변환 <span class="math inline">\delta_t^{trans}</span>를 계산합니다.</li>
<li><span class="math inline">T_t = \delta_t^{trans}</span>: 시간 <span class="math inline">t</span>에서의 객체 움직임 <span class="math inline">T_t</span>를 정의합니다.</li>
<li><span class="math inline">r_t^{H2R} = -\sqrt{\|T_t^R - T_t^H\|^2}</span>: 인간의 객체 움직임 <span class="math inline">T_t^H</span>과 로봇의 객체 움직임 <span class="math inline">T_t^R</span> 간의 음의 평균 제곱근 오차를 계산하여 보상을 제공합니다.</li>
</ul></li>
</ul></li>
<li><p><strong>탐색 전략</strong>: 일부 동작 차원(예: 카드 슬라이딩 작업에서 엄지손가락의 X 및 Y 축)에 대한 탐색을 집중합니다. Ornstein-Uhlenbeck (OU) 노이즈를 스케줄링하여 로봇 동작을 부드럽게 합니다.</p></li>
<li><p><strong>잔여 정책</strong>: 잔여 정책 <span class="math inline">\pi_r</span>는 인간의 재타겟팅된 손가락 끝 위치 <span class="math inline">a_t^r</span>, 현재 로봇 손가락 끝 위치의 변화 <span class="math inline">\Delta s_t</span>, 로봇 궤적에서 추적된 포인트 세트의 중심 <span class="math inline">\hat{P}_t^R</span>, 객체 움직임 <span class="math inline">T_t^R</span>을 입력으로 받아 추가 동작 <span class="math inline">a_t^+</span>를 생성합니다.</p>
<ul>
<li><span class="math inline">a_t = a_t^r + a_t^+ = a_t^r + \pi_r(a_t^r, \Delta s_t, \hat{P}_t^R, T_t^R)</span>: 최종 실행된 동작 <span class="math inline">a_t</span>를 계산합니다.</li>
</ul></li>
</ul></li>
</ol>
<p><strong>실험 결과</strong></p>
<p>HUDOR는 4가지 섬세한 조작 작업(빵 줍기, 카드 슬라이딩, 뮤직 박스 열기, 종이 슬라이딩)에서 다양한 베이스라인과 비교하여 우수한 성능을 보였습니다. 특히, 온라인 보정의 중요성을 입증했으며, 객체 중심 보상 함수가 일반적인 보상 함수보다 효과적임을 보여주었습니다. 또한, 새로운 객체와 더 넓은 영역으로의 일반화 가능성도 확인했습니다.</p>
</section>
<section id="detail-review" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Detail Review</h1>
<p>좋아요. Object-oriented reward의 설계 방식과 학습 모델링 구성에 초점을 맞춰, 배경 설명을 포함한 석사 수준의 기술 리뷰 포스팅을 준비할게요. 논문에서 제공된 그림도 함께 사용하고 어떤 그림인지 명시하겠습니다. 준비되면 바로 공유드릴게요!</p>
</section>
<section id="hudor-객체-중심-보상으로-인간-로봇-조작-능력-격차-해소" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> HuDOR: 객체 중심 보상으로 인간-로봇 조작 능력 격차 해소</h1>
<p>현대 로봇 공학에서 <strong>다중 손가락 로봇 손</strong>(multi-fingered robot hand)이 인간처럼 섬세하고 기민한 조작(dexterous manipulation)을 해내도록 학습시키는 일은 큰 도전입니다. 인간은 일상에서 복잡한 손동작을 자연스럽게 수행하지만, 로봇에게 이를 가르치려면 막대한 양의 시연 데이터나 특수한 원격조작 장치가 필요했습니다. 특히 <strong>원격 조작을 통한 모방 학습</strong>(teleoperation imitation learning)의 경우, 다자유도 로봇 손의 실시간 제어 한계와 <strong>형상 차이</strong>(morphology gap) 때문에 수천 건의 시연 데이터가 있어도 일반화에 어려움을 겪었습니다. 이를 극복하기 위한 대안으로 <strong>인간 시연 동영상으로부터 직접 로봇 정책을 학습</strong>하려는 접근들이 등장했지만, 기존 방법들은 여전히 추가적인 로봇 데모 데이터나 사람의 개입이 필요하였습니다. 다시 말해, 인간 손과 로봇 손의 모양 및 동작 차이 때문에 동영상만으로는 학습 신호가 부족했기 때문입니다.</p>
<p>이러한 배경에서 <strong>뉴욕대학교(NYU)</strong> 연구팀은 <em>Bridging the Human to Robot Dexterity Gap through Object-Oriented Rewards</em> 논문을 통해, 인간과 로봇 손 사이의 격차를 좁혀주는 새로운 학습 기법 <strong>HuDOR</strong>(Human-to-Dexterous Object-oriented Reward)을 제안했습니다. 핵심 아이디어는 <strong>객체 중심의 보상 함수</strong>를 설계하여, 인간 시연 영상과 로봇 동작 간의 유사도를 <strong>객체의 움직임</strong>으로 측정함으로써 형상 차이를 극복하는 것입니다. 단 하나의 인간 시연 영상과 약 1시간의 온라인 학습만으로, 데이터가 거의 없는 상황에서도 다중 손가락 로봇 손이 새로운 작업을 배우도록 합니다. 이 포스트에서는 HuDOR의 배경과 방법론을 중점적으로 살펴보고, 제안된 <strong>객체-지향 보상 함수</strong> 설계와 학습 모델 구조를 기술적으로 분석합니다. 마지막으로 실험 결과를 간략히 언급하고 본 접근의 의의와 한계를 비평적인 시각에서 정리합니다.</p>
<section id="배경-인간-동작-모방과-형상-격차-문제" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="배경-인간-동작-모방과-형상-격차-문제"><span class="header-section-number">3.1</span> 배경: 인간 동작 모방과 형상 격차 문제</h2>
<p>사람의 손 움직임을 로봇에게 학습시키는 대표적인 방법은 <strong>모방 학습</strong>(imitation learning)입니다. 전통적으로는 사람이나 전문가가 <strong>원격 조작기</strong>를 사용하여 로봇 손을 직접 움직이는 <strong>텔레오퍼레이션 시연</strong> 데이터를 다량 수집하고, 이를 모방학습시키는 방식이 사용되어 왔습니다. 그러나 다섯 손가락에 가까운 <strong>다중 관절 로봇 손</strong>의 경우, 원격 조작으로 자연스러운 시연을 얻기도 어렵고 수천 개 이상의 데모가 필요해 데이터 준비 비용이 매우 큽니다. 또한 이렇게 학습한 정책(policy)은 훈련 조건을 약간만 벗어나도 실패하기 쉬워, 실제 환경에서 <strong>강인성</strong>을 확보하기 힘듭니다.</p>
<p>이를 보완하고자 <strong>인간 행동 영상</strong>에서 로봇 행동을 학습하려는 시도가 최근 주목받고 있습니다. 예컨대 사람 손이 어떤 물체를 조작하는 유튜브 영상을 활용해 로봇에 고차원 행동을 가르치는 연구들이 보고되었습니다. 하지만 기존 접근들은 여전히 <strong>추가적인 로봇 데이터</strong>(예: 로봇으로 같은 작업을 시연한 영상)나 <strong>사람의 중간 개입</strong>이 필요했습니다. 인간 손과 로봇 손은 관절 구성, 동작 범위, 촉각 피드백 등 여러 면에서 다르기 때문에(<strong>형상적 차이</strong>), 단순히 인간 영상을 흉내 내는 것만으로는 로봇이 성공적으로 작업을 수행하지 못합니다. 결국 이전의 방법들은 사람 영상으로 대략적인 행동 윤곽을 학습시킨 뒤, 로봇 환경에서 추가 미세조정(fine-tuning)을 거쳐야 했습니다.</p>
<p>HuDOR는 바로 이 지점에서 <strong>객체(Object)</strong>에 주목합니다. 사람과 로봇의 손 모양은 다르지만, <strong>작업 대상인 객체의 움직임 궤적</strong>은 결국 동일한 목표를 향합니다. 예를 들어 사람 손가락으로 상자를 여는 동작과 로봇 손가락으로 상자를 여는 동작은 다르게 보이지만, “상자가 열렸다”는 객체의 상태 변화는 동일합니다. HuDOR의 핵심은 <strong>객체의 움직임을 매개로</strong> 인간 시연과 로봇 행동을 연결함으로써, 사람-로봇 간 형태상의 차이를 극복하는 것입니다. 이를 위해 논문에서는 <strong>객체 지향적인 보상 함수</strong>를 정의하고, <strong>온라인 강화학습</strong>을 통해 로봇 정책을 사람 영상에 맞추어 세밀하게 조정합니다. 이제 이러한 HuDOR의 방법론을 단계별로 자세히 살펴보겠습니다.</p>
</section>
<section id="hudor-방법론-객체-지향-보상을-통한-온라인-학습" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="hudor-방법론-객체-지향-보상을-통한-온라인-학습"><span class="header-section-number">3.2</span> HuDOR 방법론: 객체 지향 보상을 통한 온라인 학습</h2>
<p>HuDOR 알고리즘은 인간 시연 영상 한 건을 입력으로 받아, 로봇 손이 그 작업을 수행하도록 <strong>온라인 모방 강화학습</strong>을 진행합니다. 전체 과정은 다음 <strong>세 단계</strong>로 요약될 수 있습니다:</p>
<ol type="1">
<li><strong>인간 손동작을 로봇 손으로 재현</strong>: VR 기기를 활용해 사람 손가락 궤적을 추출하고 로봇 손에 <strong>좌표 이식 및 역기구학</strong>(inverse kinematics)으로 실행합니다. 이때 로봇은 아직 과제 수행에 성공하지 못하지만, 인간 동작을 흉내 낸 <strong>초기 정책</strong>을 얻습니다.</li>
<li><strong>객체 움직임 포인트 추적 및 보상 계산</strong>: 인간 영상과 로봇 실행 영상에서 <strong>객체 상의 특징 점(point)</strong>들을 추적하여 두 객체 궤적의 <strong>유사도</strong>를 측정합니다. 인간과 로봇에서 객체가 움직인 경로가 비슷할수록 높은 <strong>보상(reward)</strong>을 부여합니다.</li>
<li><strong>Residual 강화학습으로 정책 미세조정</strong>: 위에서 계산한 객체 중심 보상을 실시간 피드백으로 사용하는 <strong>온라인 강화학습</strong>을 진행합니다. 초기 정책에 <strong>잔차(residual)</strong> 형태로 작은 수정 동작을 학습시켜, 로봇 행동이 점차 인간 시연의 목표와 일치하도록 만듭니다.</li>
</ol>
<p>이러한 과정을 통해 로봇은 반복적인 시도를 거치며 자신의 행동을 인간 데모에 맞춰 개선해나갑니다. 아래에서 각 단계를 구현한 구체적인 기술 요소들을 살펴보겠습니다.</p>
<section id="시스템-구성-및-인간-시연-재현" class="level3" data-number="3.2.1">
<h3 data-number="3.2.1" class="anchored" data-anchor-id="시스템-구성-및-인간-시연-재현"><span class="header-section-number">3.2.1</span> 1. 시스템 구성 및 인간 시연 재현</h3>
<p>HuDOR의 첫 단계는 <strong>인간 시연 동작을 로봇으로 그대로 따라 해보는 것</strong>입니다. 이를 위해 연구진은 <strong>VR 장치와 RGB-D 카메라, ArUco 마커</strong> 등을 활용하여 사람 손의 움직임을 측정하고 로봇에게 전달하는 특수한 시스템을 구축했습니다. <strong>그림 2</strong>는 해당 실험 환경과 절차를 보여줍니다.</p>
<p><strong>Figure 2:</strong> HuDOR 시스템의 로봇 setup과 인간 손동작 <strong>재현</strong> 과정. 왼쪽은 사람이 VR 헤드셋(Quest 3)을 쓰고 실제 로봇이 있는 책상 앞에서 물체(예: 작은 상자)를 조작하는 장면입니다. VR 헤드셋의 손 추적 기능을 이용하여 사람 손가락의 3차원 위치(색 점으로 표시)를 실시간으로 추출합니다. 책상 위와 로봇 손목 위에 부착된 <strong>ArUco 마커</strong>들은 좌표계 보정에 쓰입니다. 오른쪽은 로봇 팔(Kinova JACO)과 로봇 손(Allegro hand)이 사람 손의 궤적을 따라 움직이는 모습입니다. 두 개의 마커를 통해 <strong>세계 좌표계</strong>(W)를 정하고, VR 좌표계에서의 손가락 좌표를 로봇 기준 좌표계로 변환함으로써, 인간 손가락 궤적을 동일한 공간 상에서 로봇 손가락 목표 위치로 사용합니다. 그 결과 사람 손의 포즈 시퀀스를 로봇이 모방하여 재현할 수 있게 됩니다.</p>
<p>구체적으로, VR 헤드셋으로 얻은 <strong>오른손 다섯 손가락의 위치</strong>를 책상의 <strong>월드 좌표계</strong>로 변환한 뒤, 다시 로봇 베이스 좌표계로 변환합니다. 변환된 손가락 목표 위치들은 로봇 팔-손 시스템에 <strong>역기구학(IK)</strong> 방식으로 전달되어, 로봇의 16개 관절이 사람 손가락 위치를 따라가도록 제어됩니다. 예를 들어 사람 검지 손가락이 상자를 집기 위해 이동한 경로가 있다면, 로봇의 검지에 해당하는 손가락 끝이 그 경로를 따르도록 IK 모듈이 각 관절 각도를 계산해 줍니다. 또한 사람 데모 시작 시의 손목 자세를 기록해 두었다가, 로봇 팔을 동일한 초기 자세로 설정함으로써 시작 조건을 맞춰주었습니다. 이처럼 <strong>좌표계 보정</strong>과 <strong>역기구학 기반 제어</strong>를 거치면, 로봇은 인간 시연 동작을 공간적으로 그대로 모방하게 됩니다.</p>
<p>그러나 이렇게 얻은 <strong>초기 정책</strong>(인간 포즈 트레이싱)은 한계가 있습니다. 인간 손과 로봇 손의 형태 차이, 센서 오차 등으로 인해 <strong>그냥 따라 하기만 해서는 작업에 성공하지 못하는 경우가 많습니다</strong>. 예를 들어 사람 손가락 위치를 그대로 흉내 냈지만 미세한 위치 오차로 물체를 제대로 쥐지 못하거나, 인간은 성공적으로 상자를 열었지만 로봇 손가락은 살짝 빗나가 실패하는 식입니다. HuDOR의 novelty는 여기에 <strong>온라인 강화학습을 통한 보정 단계</strong>를 추가한다는 점입니다. 로봇이 반복 시도를 통해 조금씩 실패를 만회하고 성공에 가까워지도록, <strong>실시간 성과 측정</strong>을 해가며 정책을 개선시키는 것이 다음 단계입니다.</p>
</section>
<section id="객체-중심-포인트-추적과-보상-함수-설계" class="level3" data-number="3.2.2">
<h3 data-number="3.2.2" class="anchored" data-anchor-id="객체-중심-포인트-추적과-보상-함수-설계"><span class="header-section-number">3.2.2</span> 2. 객체 중심 포인트 추적과 보상 함수 설계</h3>
<p>로봇이 자신의 수행이 인간 데모와 얼마나 가까운지 알 수 있으려면, <strong>평가 기준</strong>, 즉 보상 함수를 정의해야 합니다. HuDOR는 이 보상 신호를 설계할 때 <strong>로봇 손이나 인간 손의 자세 차이</strong>를 직접 비교하지 않습니다. 대신, <strong>작업 대상인 객체(Object)의 움직임</strong>을 중심에 둡니다. 구체적으로, 사람 시연 영상 속에서 물체가 움직인 궤적과 로봇이 시도할 때 물체가 움직인 궤적을 서로 비교하여 <strong>유사도 점수</strong>를 산정합니다. 두 궤적이 비슷하면 높은 보상, 다르면 낮은 보상을 주는 방식입니다. 이렇게 하면 인간과 로봇의 손 모양이나 동작 방식이 달라도 <strong>물체를 다루는 결과</strong>만 가지고 학습을 유도할 수 있습니다.</p>
<p>하지만 영상을 보고 물체의 “궤적”을 얻는 일 자체가 만만치 않습니다. 이를 위해 논문에서는 <strong>최신 컴퓨터 비전 기법</strong>들을 조합하여 자동으로 물체의 특징 지점들을 추적했습니다. 과정은 크게 <strong>물체 영역 분할 -&gt; 특징 점 추적 -&gt; 궤적 유사도 계산</strong> 세 부분으로 이루어집니다.</p>
<p>먼저, 입력으로 주어진 인간 시연 영상에서 <strong>관심 물체</strong>(예: 음악 상자, 카드, 빵 조각 등)가 어디 있는지 알아내야 합니다. HuDOR는 한 장면에 여러 물체가 있을 수 있으므로, <strong>자연어 텍스트 프롬프트</strong>로 특정 대상 물체를 지정합니다. 예컨대 “orange bread”라는 텍스트로 “주황색 빵” 물체를 가리키는 식입니다. 그런 다음 <strong>Segment Anything Model (SAM)</strong>과 <strong>GroundingDINO</strong>로 구성된 <strong>langSAM</strong> 모델을 이용해 프레임 내 목표 물체의 <strong>초기 마스크</strong>(분할 영역)를 얻습니다. SAM은 어떤 물체든 분할해주는 모델이고, GroundingDINO는 텍스트로 특정 물체의 위치(BB)를 찾아주는 모델입니다. 두 모델을 결합하면 첫 번째 프레임에서 목표 물체의 픽셀 영역을 정확히 지정할 수 있습니다. 이 마스크 영역 안에서 <strong>N개의 점</strong>을 골라 해당 물체의 대표 <strong>특징 점 세트</strong> <span class="math inline">P_0</span>를 정의합니다. 점의 개수 <span class="math inline">N</span>는 물체 크기에 따라 조절하는 하이퍼파라미터입니다.</p>
<p>이렇게 초기 프레임에서 얻은 물체 위의 점들을, 영상의 이후 모든 프레임에 걸쳐 <strong>추적(track)</strong>합니다. HuDOR는 이를 위해 <strong>Transformer 기반의 포인트 추적 알고리즘인 CoTracker</strong>【46】를 사용했습니다. CoTracker는 첫 프레임의 segmentation 마스크와 일련의 RGB 이미지 프레임들을 입력받아, <strong>각 프레임에서 물체 위 점들의 위치를 추적</strong>해줍니다. 이를 통해 인간 시연 영상에서는 <span class="math inline">P_0, P_1, ..., P_T</span>로 표현되는 <strong>물체 점들의 궤적</strong> <span class="math inline">T_H</span> (Human trajectory set)를 얻습니다. 마찬가지로, 로봇이 동일 작업을 시도하는 동안 촬영된 영상으로부터 물체 점 궤적 <span class="math inline">T_R</span> (Robot trajectory set)를 계산합니다. <strong>그림 4</strong>는 이렇게 얻은 인간/로봇 영상 속 물체 점 추적 결과를 시각화한 예시입니다.</p>
<p><strong>Figure 4:</strong> HuDOR의 <strong>객체 포인트 추적</strong> 예시. 각 작업(음악 상자 열기, 카드 슬라이딩, 종이 밀기, 빵 집기)에 대해, 위쪽 행은 <strong>인간 시연 영상</strong>, 아래쪽 행은 <strong>로봇 실행 영상</strong>입니다. 무지개 색상의 점들이 물체 표면 위에 표시되어 있는데, 이 점들이 바로 CoTracker에 의해 추적된 <strong>특징 점들</strong>입니다. 시간에 따라 동일한 점은 같은 색으로 연결되며 움직입니다. 사람 손이나 로봇 손에 가려져 카메라에서 일시적으로 보이지 않는 점들은 빈 원(hollow circles)으로 표시되어 <strong>가려짐(occlusion)</strong>을 나타냅니다. 이 그림을 통해, 비록 인간과 로봇의 손 모양과 움직임은 다르지만 양쪽에서 <strong>물체의 움직임 패턴</strong>(예: 상자의 뚜껑이 열리는 경로, 카드가 옆으로 밀리는 경로 등)이 유사하게 추적될 수 있다는 것을 알 수 있습니다. HuDOR는 바로 이 <strong>객체 움직임 궤적</strong>을 본격적인 학습 신호로 활용합니다.</p>
<p>두 영상에서 얻은 물체 점 궤적 <span class="math inline">T_H</span>와 <span class="math inline">T_R</span>를 어떻게 비교할까요? 가장 간단한 방법은 같은 색상의 점들 각각의 위치 차이를 모두 계산해 평균내는 것입니다. 그러나 서로 다른 시도에서는 같은 인덱스의 점이 물체 상에서 정확히 대응되지 않을 수도 있습니다. 예컨대 CoTracker가 추적한 50개의 점 중 1번 점이 인간 영상에서는 물체 모서리에 있었지만 로봇 영상에서는 약간 다른 지점에 대응될 수 있습니다. 이러한 <strong>점 대 점 불일치</strong>는 직접 위치 오차를 대응시키는 방식을 불안정하게 만들 수 있습니다. HuDOR는 이를 보완하기 위해 <strong>객체의 전체적인 이동 양상</strong>을 비교하는 방향으로 보상을 정의했습니다. 구체적으로, 각 시간 단계 <span class="math inline">t</span>마다 물체 점 집합 <span class="math inline">P_t</span>의 <strong>중심점</strong>(centroid)과 <strong>평균 이동 벡터</strong>를 계산합니다. 예를 들면 <span class="math inline">t</span> 시점에 물체 표면의 모든 점들이 이전 시점에 비해 평균적으로 얼마나 어디로 이동했는지를 나타내는 벡터를 구합니다. 이를 해당 시점의 <strong>객체 모션(object motion)</strong> <span class="math inline">\Delta_t</span>라고 정의합니다. 인간 시연에서의 <span class="math inline">\Delta^H_t</span>와 로봇 실행에서의 <span class="math inline">\Delta^R_t</span>를 비교함으로써, 두 케이스에서 물체가 움직인 방향과 크기가 얼마나 유사한지 측정할 수 있습니다.</p>
<p>HuDOR의 <strong>보상 함수</strong> <span class="math inline">R_t</span>는 바로 이 두 객체 모션 간의 유사도로 정의됩니다. 논문에서는 보상을 식으로 다음과 같이 표현했는데, <strong>두 모션 벡터 간의 RMSD(root mean squared deviation)를 부호 반대로 취한 값</strong>입니다:</p>
<p>[ R_t ;=; -,, ]</p>
<p>여기서 <span class="math inline">d</span>는 차원 (영상 평면에서 2D 이동이면 <span class="math inline">d=2</span>)입니다. 즉 로봇이 물체를 움직인 변위가 인간이 물체를 움직인 변위와 완전히 같다면 <span class="math inline">\Delta^R_t \approx \Delta^H_t</span>가 되어 보상 <span class="math inline">R_t</span>가 0에 가까운 높은 값이 되고, 차이가 크면 큰 음수가 되어 낮은 보상이 됩니다. 요약하면 <strong>“로봇이 물체를 사람처럼 움직이면 높게 점수를 준다”</strong>는 것입니다. 이러한 보상은 매 시각 스텝마다 계산되어 강화학습에 입력됩니다. 중요한 것은, 이 보상 신호가 <strong>로봇 손가락의 모양이나 위치를 직접 참조하지 않고 오직 물체의 상태 변화만을 고려</strong>하므로, 인간-로봇 손의 격차로 인한 오류를 크게 줄여준다는 점입니다.</p>
</section>
<section id="residual-강화학습을-통한-정책-미세조정" class="level3" data-number="3.2.3">
<h3 data-number="3.2.3" class="anchored" data-anchor-id="residual-강화학습을-통한-정책-미세조정"><span class="header-section-number">3.2.3</span> 3. Residual 강화학습을 통한 정책 미세조정</h3>
<p>이제 로봇은 “어떻게 하면 보상을 높일 수 있는지”, 다시 말해 “어떻게 해야 물체를 인간처럼 움직일 수 있는지”를 학습해야 합니다. HuDOR는 이를 위해 <strong>모델 자유 강화학습</strong>(model-free RL)의 한 종류인 <strong>DrQ-v2 알고리즘</strong>【49】을 사용했습니다. DrQ-v2는 픽셀 입력에서도 안정적으로 학습할 수 있도록 정규화한 Q러너로, 여기서는 <strong>상태 공간</strong>을 잘 설계해 활용했습니다. HuDOR의 정책 네트워크(배치너)는 다음과 같은 정보를 관찰로 받아들입니다:</p>
<ul>
<li><ol type="a">
<li><strong>인간 시연에서 해당 시각의 손가락 목표 위치</strong>: 앞서 1단계에서 얻은 인간 손가락 궤적 중 현재 시간 <span class="math inline">t</span>에 해당하는 4개 손끝의 위치 (로봇 기준 좌표계)</li>
</ol></li>
<li><ol start="2" type="a">
<li><strong>로봇 손가락의 현재 움직임 변화량</strong>: 직전 시각에 비해 로봇 손가락 위치가 어떻게 변했는지 (4개 손끝의 위치 증분)</li>
</ol></li>
<li><ol start="3" type="a">
<li><strong>로봇 측에서 추적된 물체 점들의 중심점</strong>: 현재 프레임에서 로봇 영상 상 물체 특징 점들의 평균 위치</li>
</ol></li>
<li><ol start="4" type="a">
<li><strong>현재 시각의 객체 모션 벡터</strong> <span class="math inline">\Delta^R_t</span>: 로봇이 관측한 물체의 이동 변위 (예: 이전 프레임 대비 이동량)</li>
</ol></li>
</ul>
<p>이 네 가지를 합친 상태 표현을 입력으로 받아, 정책 네트워크는 로봇의 <strong>잔여 액션(residual action)</strong>을 출력합니다. Residual action이란 앞서 인간 궤적을 따라가는 <strong>기본 행동</strong>에 더해지는 보정값입니다. HuDOR에서는 로봇의 관절 명령이 아니라, <strong>손가락 끝 목표 위치</strong> 자체를 약간 수정하는 형태로 residual이 적용됩니다. 즉, 정책이 출력한 액션 <span class="math inline">a_t</span>는 “현재 손가락 목표 위치를 이만큼 이동시켜라”는 지령으로서, 원래 인간 궤적에 해당하는 위치에 더해져 최종 목표 손가락 위치가 결정됩니다. 그렇게 생성된 목표 손가락 위치들은 다시 IK 모듈을 통해 로봇 관절 명령으로 변환되어 실행됩니다. 요컨대, <strong>기본 모방 행동 + 학습된 수정</strong>의 합성으로 로봇을 제어하는 것입니다.</p>
<p>Residual 정책을 학습하는 강화학습 과정에서, 액션 공간을 지나치게 크게 두면 학습이 어려울 수 있습니다. 논문에서는 각 작업마다 특히 중요한 손가락의 특정 축만 <strong>탐색 액션 차원</strong>으로 선택하여 학습 효율을 높였습니다. 예를 들어 “카드 슬라이딩” 작업에서는 엄지손가락의 X, Y 축 움직임만 주로 조절하도록 제한하고, 나머지 축은 기본 궤적 그대로 따르게 했습니다. 이렇게 하면 불필요한 탐색을 줄여 <strong>샘플 효율(sample-efficiency)</strong>을 개선할 수 있습니다. 또, 연속 제어 탐색에 흔히 쓰이는 Ornstein-Uhlenbeck 프로세스 기반의 노이즈를 일정 스케줄로 추가하여, 정책이 부드럽게 탐색하도록 했습니다.</p>
<p>강화학습은 에피소드 단위로 이루어지며, 각 에피소드에서 로봇은 초기 상태에서 시작해 한 차례 작업을 수행합니다. 작업이 끝나면 (성공 여부와 무관하게) 에피소드가 종료되고, 누적 보상을 높이는 방향으로 정책이 업데이트됩니다. HuDOR의 경우 한 과제당 <strong>1시간 이내의 온라인 상호작용</strong>(약 수십 에피소드 반복)으로 정책이 수렴되었다고 합니다. 시간은 짧지만 매 시간 로봇이 실제로 시도하며 얻는 객체 중심 보상 신호가 명확하기 때문에, 학습이 안정적으로 진행됩니다. 특히 HuDOR는 <strong>오프라인 데이터 없이 온라인으로만</strong> 학습하기 때문에, 점진적으로 자기 개선(self-improvement)을 하는 과정에서 오히려 <strong>강인성</strong>을 얻는 장점이 있습니다 . 아래 <strong>그림 6</strong>은 이러한 온라인 보정 학습이 가져오는 효과를 잘 보여줍니다.</p>
<p><strong>Figure 6:</strong> 온라인 residual 학습을 통한 <strong>로봇 정책 향상</strong> 과정. 이 예시는 “Paper Sliding (종이 밀기)” 작업에 대한 것으로, 위 그림은 책상 위 종이가 인간 시연(주황색 점선 경로)과 로봇 시도들에서 어떻게 움직였는지를 겹쳐 보여줍니다. 처음 에피소드(빨간 실선)는 종이가 제대로 움직이지 못했지만, 학습이 진행됨에 따라 42번째 에피소드(녹색 실선)에는 종이가 인간 궤적에 가깝게 멀리 밀려난 것을 볼 수 있습니다. 아래 그래프 왼쪽은 시간에 따른 종이의 X축 위치 변화를 인간 vs 로봇(에피소드 1과 42)으로 나타낸 것으로, 에피소드 42의 녹색 곡선이 주황색 인간 곡선을 거의 따라잡은 모습입니다. 오른쪽 그래프는 매 시각 스텝의 보상값으로, 초기에는 큰 음수로 출발했으나 학습 후반에는 보상 분포(녹색)가 현저히 상승하여 0에 근접하고 있음을 보여줍니다. 즉, <strong>객체 이동 궤적의 오차가 점차 줄어들어</strong> 보상이 높아지고, 로봇 정책이 인간 전문가 수준에 가까워지고 있음을 확인할 수 있습니다.</p>
</section>
</section>
<section id="실험-결과-인간-수준-동작-달성과-성능-비교" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="실험-결과-인간-수준-동작-달성과-성능-비교"><span class="header-section-number">3.3</span> 실험 결과: 인간 수준 동작 달성과 성능 비교</h2>
<p>제안된 HuDOR의 성능은 <strong>네 가지 복잡한 조작 작업</strong>에 대한 실험으로 검증되었습니다. 실험에 사용된 과제들은 (1) <strong>Bread Picking</strong>: 빵 조각 집어 들기, (2) <strong>Card Sliding</strong>: 책상 위 얇은 카드를 밀어 잡기, (3) <strong>Music Box Opening</strong>: 뚜껑 달린 작은 음악 상자 한 손으로 열기, (4) <strong>Paper Sliding</strong>: 종이를 밀어서 옆으로 보내기 입니다. 각 작업마다 인간이 한 손으로 시연한 1개의 데모 비디오만을 제공하고, 그 외에 로봇 데이터나 추가 보조정보는 없이 HuDOR 알고리즘을 수행했습니다. 학습이 끝난 후 로봇 정책을 평가하기 위해, 물체의 초기 위치나 형태를 변화시킨 다양한 시나리오에서 여러 차례 시도를 해보았습니다. 예컨대 Bread Picking의 경우 빵 조각을 매 시도마다 임의의 위치와 방향에 놓고 10번 시도하여 성공률을 측정했습니다.</p>
<p>실험 결과, HuDOR는 <strong>모든 작업에서 안정적으로 임무를 성공</strong>할 수 있음을 보였습니다. 특히 이전의 오프라인 모방학습 기반 방법들에 비해 월등한 성능 향상을 달성했습니다. <strong>표 1</strong>의 일부 결과를 인용하면, HuDOR는 Bread Picking에서 10회 중 8회 성공, Card Sliding 7/10, Music Box Opening 6/10의 높은 성공률을 기록했습니다. 반면 동일 조건에서 최신 오프라인 <strong>행동클로닝(BC)</strong> 알고리즘으로 학습한 정책은 Bread 3/10, Card 0/10, Music 0/10 에 그쳤고, <strong>포인트클라우드 기반 BC</strong>와 같은 강화된 기준선조차도 Bread 3/10, Card 0/10, Music 0/10에 머물렀습니다 . <strong>그림 5</strong>는 학습된 HuDOR 정책의 롤아웃 장면을 각 작업별로 보여주며, 우측에 해당 작업의 최종 성능 지표를 표시합니다. 예를 들어 Bread Picking 작업의 경우 빵을 집어 들어 올려 안정적으로 들고 있는 장면이 포착되었고, 성공률 8/10으로 표기되어 있습니다.</p>
<p><strong>Figure 5:</strong> HuDOR로 학습된 정책의 <strong>작업 수행 장면</strong>들. 네 가지 과제 (Bread Picking, Card Sliding, Music Box Opening, Paper Sliding)에 대해, 좌측 이미지는 평가 시 다양한 초기 위치 범위(보라색 박스) 내에서 실험했음을 나타냅니다. 우측 이미지는 각 작업의 최종 성공 결과로, 세 가지 집게손가락 작업은 10회 중 성공 횟수를, Paper Sliding은 밀어낸 거리(cm)로 성과를 표시했습니다. HuDOR 정책은 Bread, Card, Music 세 작업 모두 높은 성공률을 보였으며, Paper Sliding의 경우 평균 약 17cm 이상 종이를 밀어 움직여 기존 대비 월등한 성능을 달성했습니다. 전체적으로 HuDOR는 최소의 인간 데모 데이터로 다양한 섬세한 조작을 성공시키며, <strong>4배 이상의 성능 향상</strong>을 보여준 것으로 요약됩니다.</p>
<p>흥미로운 추가 실험으로, HuDOR의 <strong>일반화 능력</strong>도 검증되었습니다. 학습 시 사용한 물체와 <strong>다른 새로운 물체</strong>에도 적용해보는 실험인데, 예를 들어 Bread Picking 작업을 학습한 정책을 가지고 <strong>빵 대신 작은 병, 장난감 인형</strong> 등을 집도록 시도했습니다. 이를 위해 langSAM에 입력으로 줄 텍스트 프롬프트만 해당 새로운 물체에 맞게 바꾸어주고, 동일한 절차를 수행했습니다. 그 결과 Bread Picking 작업은 새로운 물체들에 대해서도 절반 이상의 성공률을 보이는 등 <strong>상당한 공간/물체 일반화 성능</strong>을 확인했습니다. 다만 Music Box Opening처럼 섬세한 동작이 요구되는 경우 새로운 상자나 위치에 대해서는 일반화에 실패하기도 했습니다. 이는 깊이 카메라 노이즈나 residual 정책의 한계 때문으로 분석되며, 추후 개선 여지가 있다고 합니다.</p>
</section>
<section id="결론-및-비평-의의와-한계" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="결론-및-비평-의의와-한계"><span class="header-section-number">3.4</span> 결론 및 비평: 의의와 한계</h2>
<p>이 논문은 <strong>객체 지향 보상</strong>이라는 개념을 통해 인간 영상과 로봇 제어 간의 격차를 혁신적으로 좁힌 점에서 큰 의의가 있습니다. 불과 한 개의 인간 시연 데이터만으로 다중 손가락 로봇 손에 복잡한 조작을 가르치고, 오프라인 데이터에 의존하던 기존 방법들을 능가하는 성능을 보여주었습니다. 특히 사람 손동작 그대로 흉내 내는 데 그치지 않고, <strong>강화학습을 활용해 온라인으로 오차를 수정</strong>해나가는 접근은 현실 로봇 학습의 새로운 가능성을 제시합니다. 인간 손과 로봇 손의 구조적 차이를 <strong>작업 결과의 유사성</strong>으로 메꾸는 객체 중심 설계는 향후 다른 로봇 학습 문제에도 응용될 수 있는 통찰을 제공합니다.</p>
<p>그럼에도 불구하고 HuDOR에는 몇 가지 <strong>한계점</strong>이 존재합니다.</p>
<ul>
<li>첫째, <strong>데모가 로봇과 같은 장면에서 촬영된 경우에만 동작</strong>합니다. 이번 연구에서는 사람 시연자가 로봇과 같은 테이블에서 같은 물체로 데모를 했기 때문에 비교적 수월했지만, 일반적인 “야생의” 인간 영상(in-the-wild video)으로 확장하려면 추가 연구가 필요합니다.</li>
<li>둘째, <strong>탐색 액션 차원 선택에 사전 지식이 필요</strong>합니다. 저자들은 각 작업마다 어느 손가락 관절을 활성화할지 수동으로 정했는데, 이 부분을 자동화하거나 더욱 일반적으로 만들 여지가 있습니다.</li>
<li>셋째, 현재 정책은 <strong>에피소드 내 재시도(retry)가 불가</strong>합니다. 한번 실수하면 그 에피소드는 그대로 끝나고 다음 번 처음부터 다시 시도하는 구조여서, 여러 단계를 차례로 수행해야 하는 장기 과제에는 약점이 있습니다 . 예를 들어 상자 열기 후 뚜껑을 잡는 동작에서 한 번 놓치면 즉시 실패로 간주되므로, 중간에 다시 잡도록 하는 로직이 없습니다. 이러한 부분은 <strong>다단계 학습 프레임워크</strong>를 도입함으로써 개선될 수 있을 것입니다.</li>
</ul>
<p>요약하면, HuDOR는 <strong>객체 중심 관점의 보상 설계와 온라인 강화학습</strong>을 결합하여 인간-로봇 사이의 <strong>손재주 격차</strong>를 효과적으로 줄인 획기적인 연구입니다. 최소한의 데이터로 최대의 일반화 성능을 끌어냈다는 점에서 실용적 가치가 크며, 향후 다양한 로봇 학습 시나리오에 영감을 줄 수 있습니다. 반면 실제 응용을 위해서는 보다 <strong>비전문가가 찍은 일반 영상으로도 학습 가능하게</strong> 하거나, <strong>장시간 연속 작업</strong>에 견딜 수 있도록 정책을 고도화하는 등의 추가 연구가 필요합니다. 그럼에도 본 연구가 보여준 결과들은, 로봇이 인간의 풍부한 시연에서 학습하도록 하는 길에 한 걸음 더 다가섰음을 분명히 보여줍니다. 앞으로 HuDOR가 발전하고 확장되어, 다양한 로봇들이 사람처럼 능숙하게 물체를 다룰 수 있게 되기를 기대합니다.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    window.setColorSchemeToggle(window.hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<script src="https://utteranc.es/client.js" repo="curieuxjy/blog_comments" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Copyright 2024, Jung Yeon Lee</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/curieuxjy">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>