<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.33">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-07-28">
<meta name="description" content="Comprehensive Overview of Reward Engineering and Shaping in Advancing Reinforcement Learning Applications">

<title>📃Reward Engineering 리뷰 – Curieux.JY</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../profile.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-ea385d0e468b0dd5ea5bf0780b1290d9.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-bc185b5c5bdbcb35c2eb49d8a876ef70.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-ea385d0e468b0dd5ea5bf0780b1290d9.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-23aef1c2a45953e85f3378e7ccfb1407.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-5a614c35f1f90bfd0a5b2992298a8538.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-23aef1c2a45953e85f3378e7ccfb1407.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-2NVZN2MJZT"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-2NVZN2MJZT', { 'anonymize_ip': true});
</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Curieux.JY</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../post.html"> 
<span class="menu-text">Post</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../note.html"> 
<span class="menu-text">Note</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">Jung Yeon Lee</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/curieuxjy"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#brief-review" id="toc-brief-review" class="nav-link active" data-scroll-target="#brief-review">Brief Review</a>
  <ul class="collapse">
  <li><a href="#기본-개념" id="toc-기본-개념" class="nav-link" data-scroll-target="#기본-개념">기본 개념</a></li>
  <li><a href="#reward-shapingengineering-기법-분류" id="toc-reward-shapingengineering-기법-분류" class="nav-link" data-scroll-target="#reward-shapingengineering-기법-분류">Reward Shaping/Engineering 기법 분류</a></li>
  </ul></li>
  <li><a href="#detail-review" id="toc-detail-review" class="nav-link" data-scroll-target="#detail-review">Detail Review</a>
  <ul class="collapse">
  <li><a href="#서론" id="toc-서론" class="nav-link" data-scroll-target="#서론">서론</a></li>
  <li><a href="#연구-방법-문헌-조사-및-선정-과정" id="toc-연구-방법-문헌-조사-및-선정-과정" class="nav-link" data-scroll-target="#연구-방법-문헌-조사-및-선정-과정">연구 방법 (문헌 조사 및 선정 과정)</a></li>
  <li><a href="#배경-및-기본-개념" id="toc-배경-및-기본-개념" class="nav-link" data-scroll-target="#배경-및-기본-개념">배경 및 기본 개념</a></li>
  <li><a href="#reward-shapingengineering-기법의-분류" id="toc-reward-shapingengineering-기법의-분류" class="nav-link" data-scroll-target="#reward-shapingengineering-기법의-분류">Reward Shaping/Engineering 기법의 분류</a>
  <ul class="collapse">
  <li><a href="#정책-경사-기반-방법-policy-gradient-기반-보상-최적화" id="toc-정책-경사-기반-방법-policy-gradient-기반-보상-최적화" class="nav-link" data-scroll-target="#정책-경사-기반-방법-policy-gradient-기반-보상-최적화">정책 경사 기반 방법 (Policy Gradient 기반 보상 최적화)</a></li>
  <li><a href="#강건성-및-적응성-향상-방법" id="toc-강건성-및-적응성-향상-방법" class="nav-link" data-scroll-target="#강건성-및-적응성-향상-방법">강건성 및 적응성 향상 방법</a></li>
  <li><a href="#탐험-전략-기반-방법-exploration-driven-shaping" id="toc-탐험-전략-기반-방법-exploration-driven-shaping" class="nav-link" data-scroll-target="#탐험-전략-기반-방법-exploration-driven-shaping">탐험 전략 기반 방법 (Exploration-Driven Shaping)</a></li>
  <li><a href="#정책-파라미터화-단순-정책-모델의-활용" id="toc-정책-파라미터화-단순-정책-모델의-활용" class="nav-link" data-scroll-target="#정책-파라미터화-단순-정책-모델의-활용">정책 파라미터화 (단순 정책 모델의 활용)</a></li>
  <li><a href="#역보상-설계-inverse-reward-design-ird" id="toc-역보상-설계-inverse-reward-design-ird" class="nav-link" data-scroll-target="#역보상-설계-inverse-reward-design-ird">역보상 설계 (Inverse Reward Design, IRD)</a></li>
  <li><a href="#보상-지평-단축-reward-horizon-shaping" id="toc-보상-지평-단축-reward-horizon-shaping" class="nav-link" data-scroll-target="#보상-지평-단축-reward-horizon-shaping">보상 지평 단축 (Reward Horizon Shaping)</a></li>
  <li><a href="#잠재-기반-방법-potential-based-reward-shaping" id="toc-잠재-기반-방법-potential-based-reward-shaping" class="nav-link" data-scroll-target="#잠재-기반-방법-potential-based-reward-shaping">잠재 기반 방법 (Potential-Based Reward Shaping)</a></li>
  <li><a href="#동적-잠재-기반-reward-shaping-dpbrs" id="toc-동적-잠재-기반-reward-shaping-dpbrs" class="nav-link" data-scroll-target="#동적-잠재-기반-reward-shaping-dpbrs">동적 잠재 기반 Reward Shaping (DPBRS)</a></li>
  <li><a href="#상한-신뢰-기반-가치-반복-ucbvi-기반-보상-설계" id="toc-상한-신뢰-기반-가치-반복-ucbvi-기반-보상-설계" class="nav-link" data-scroll-target="#상한-신뢰-기반-가치-반복-ucbvi-기반-보상-설계">상한 신뢰 기반 가치 반복 (UCBVI 기반 보상 설계)</a></li>
  <li><a href="#차이-보상-difference-rewards" id="toc-차이-보상-difference-rewards" class="nav-link" data-scroll-target="#차이-보상-difference-rewards">차이 보상 (Difference Rewards)</a></li>
  <li><a href="#지식-기반-다목적-다중에이전트-rl-momarl" id="toc-지식-기반-다목적-다중에이전트-rl-momarl" class="nav-link" data-scroll-target="#지식-기반-다목적-다중에이전트-rl-momarl">지식 기반 다목적 다중에이전트 RL (MOMARL)</a></li>
  <li><a href="#계획-기반-방법-plan-based-reward-shaping" id="toc-계획-기반-방법-plan-based-reward-shaping" class="nav-link" data-scroll-target="#계획-기반-방법-plan-based-reward-shaping">계획 기반 방법 (Plan-Based Reward Shaping)</a></li>
  <li><a href="#신념-기반-reward-shaping-belief-reward-shaping-brs" id="toc-신념-기반-reward-shaping-belief-reward-shaping-brs" class="nav-link" data-scroll-target="#신념-기반-reward-shaping-belief-reward-shaping-brs">신념 기반 Reward Shaping (Belief Reward Shaping, BRS)</a></li>
  <li><a href="#이층-최적화-기반-reward-shaping-bipars" id="toc-이층-최적화-기반-reward-shaping-bipars" class="nav-link" data-scroll-target="#이층-최적화-기반-reward-shaping-bipars">이층 최적화 기반 Reward Shaping (BiPaRS)</a></li>
  <li><a href="#메타-보상-네트워크-mrn" id="toc-메타-보상-네트워크-mrn" class="nav-link" data-scroll-target="#메타-보상-네트워크-mrn">메타-보상 네트워크 (MRN)</a></li>
  <li><a href="#기타-기법-및-동향" id="toc-기타-기법-및-동향" class="nav-link" data-scroll-target="#기타-기법-및-동향">기타 기법 및 동향</a></li>
  </ul></li>
  <li><a href="#로봇공학-및-기타-분야에서의-응용" id="toc-로봇공학-및-기타-분야에서의-응용" class="nav-link" data-scroll-target="#로봇공학-및-기타-분야에서의-응용">로봇공학 및 기타 분야에서의 응용</a>
  <ul class="collapse">
  <li><a href="#인간-로봇-협업을-위한-안전-효율형-보상-설계" id="toc-인간-로봇-협업을-위한-안전-효율형-보상-설계" class="nav-link" data-scroll-target="#인간-로봇-협업을-위한-안전-효율형-보상-설계">인간-로봇 협업을 위한 안전 효율형 보상 설계</a></li>
  <li><a href="#대화형-피드백을-활용한-가정용-로봇-학습" id="toc-대화형-피드백을-활용한-가정용-로봇-학습" class="nav-link" data-scroll-target="#대화형-피드백을-활용한-가정용-로봇-학습">대화형 피드백을 활용한 가정용 로봇 학습</a></li>
  <li><a href="#현실-도메인-차이를-극복하는-보상-및-도메인-적응" id="toc-현실-도메인-차이를-극복하는-보상-및-도메인-적응" class="nav-link" data-scroll-target="#현실-도메인-차이를-극복하는-보상-및-도메인-적응">현실 도메인 차이를 극복하는 보상 및 도메인 적응</a></li>
  </ul></li>
  <li><a href="#reward-engineering의-장점과-한계-미래의-핵심인가" id="toc-reward-engineering의-장점과-한계-미래의-핵심인가" class="nav-link" data-scroll-target="#reward-engineering의-장점과-한계-미래의-핵심인가">Reward Engineering의 장점과 한계: 미래의 핵심인가?</a></li>
  <li><a href="#개방된-과제-및-향후-연구-방향" id="toc-개방된-과제-및-향후-연구-방향" class="nav-link" data-scroll-target="#개방된-과제-및-향후-연구-방향">개방된 과제 및 향후 연구 방향</a></li>
  <li><a href="#결론" id="toc-결론" class="nav-link" data-scroll-target="#결론">결론</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-page-left" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">📃Reward Engineering 리뷰</h1>
  <div class="quarto-categories">
    <div class="quarto-category">rl</div>
    <div class="quarto-category">reward</div>
  </div>
  </div>

<div>
  <div class="description">
    Comprehensive Overview of Reward Engineering and Shaping in Advancing Reinforcement Learning Applications
  </div>
</div>


<div class="quarto-title-meta column-page-left">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">July 28, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<ul>
<li><a href="https://arxiv.org/abs/2408.10215">Paper Link</a></li>
</ul>
<ol type="1">
<li>💡 본 논문은 Reinforcement Learning (RL)에서 보상 설계(reward design)의 중요성을 강조하며, Reward Engineering 및 보상 쉐이핑(reward shaping) 방법론과 기술을 포괄적으로 검토합니다.</li>
<li>🛠️ 주요 내용은 보상 설계의 도전 과제(예: sparse/delayed rewards, reward hacking), 스칼라/벡터 보상 논쟁, 그리고 정책 경사(Policy Gradient), 잠재 기반(Potential-Based), 인간 피드백(Human Feedback) 등 다양한 보상 쉐이핑 기법의 상세 분류를 포함합니다.</li>
<li>🚀 이 연구는 보상 설계가 로봇 공학, 자율 주행 등 실제 응용 분야에서 학습 효율성과 견고성을 향상시키며, Sim-to-Real 간극 해소에 필수적임을 보여주고 향후 자동화된 튜닝과 인간-로봇 협업 연구 방향을 제시합니다.</li>
</ol>
<hr>
<section id="brief-review" class="level1">
<h1>Brief Review</h1>
<p>강화 학습(RL)은 자율적 의사결정 시스템 개발에 필수적이며, Reward Engineering과 Reward Shaping은 RL 알고리즘의 효율성과 효과를 높이는 데 중추적인 역할을 한다. 이 기법들은 희소하고 지연된 보상과 같은 문제들을 해결하여 에이전트가 목표 행동을 학습하고, 학습 안정성을 향상시키며, 수렴 속도를 가속화하도록 돕는다. 본 논문은 RL의 보상 설계에 대한 포괄적인 첫 검토를 제공하며, Reward Engineering 및 형성의 방법론과 기법에 초점을 맞춘다. 상세한 분류 체계(taxonomy)를 제시하고, 현재 접근 방식들을 비판적으로 분석하며, 그 한계를 강조함으로써 문헌의 중요한 공백을 메운다.</p>
<p>보상 설계는 RL 에이전트의 원하는 행동과 목표에 부합하도록 보상 함수를 정의하는 미묘하고 복잡한 과정이다.</p>
<ul>
<li><code>Reward Engineering</code>은 보상 함수 <span class="math inline">R(s, a, s')</span>를 초기 생성하는 것을 포함하며, 이는 상태 <span class="math inline">s</span>, 행동 <span class="math inline">a</span>, 다음 상태 <span class="math inline">s'</span>를 수치적 보상 값에 매핑한다.</li>
<li><code>Reward Shaping</code>(<span class="math inline">R'(s, a, s') = R(s, a, s') + \gamma \Phi(s') - \Phi(s)</span>)은 최적 정책을 변경하지 않으면서 보상 신호를 미세 조정하여 학습 과정을 개선하는 데 사용된다. (여기서 <span class="math inline">\gamma</span>는 할인 계수(discount factor)이며, <span class="math inline">\Phi(s)</span>는 잠재 함수(potential function))</li>
</ul>
<p>RL 문제는 MDP(Markov Decision Process)로 정의되며, 에이전트의 목표는 <span class="math inline">G_t = \sum_{k=0}^{\infty} \gamma^k R(s_{t+k}, a_{t+k})</span>로 주어지는 기대 누적 보상(expected cumulative reward)을 최대화하는 정책 <span class="math inline">\pi(a|s)</span>를 학습하는 것이다. 본 논문은 보상 설계의 주요 개념, 동향, 도전 과제 및 기회를 탐구하며, RL 및 AI 분야의 혁신을 이끌 잠재력을 강조한다.</p>
<p><strong>연구 방법:</strong> PRISMA 2020 가이드라인을 준수하여 체계적인 문헌 검토를 수행했다. “reward shaping”, “reward engineering”, “reinforcement learning”, “reward design”, “machine learning”, “control systems” 등의 조합어를 사용하여 1999년부터 2024년까지 발표된 55개의 관련 논문을 선별했다.</p>
<section id="기본-개념" class="level3">
<h3 class="anchored" data-anchor-id="기본-개념">기본 개념</h3>
<ul>
<li><strong>RL 핵심 개념:</strong> 에이전트, 환경, 정책, 보상, 가치 함수, 탐색 대 활용(Exploration vs.&nbsp;Exploitation).</li>
<li><strong>보상 충분성 논쟁:</strong>
<ul>
<li>“보상만으로 충분하다(Reward is Enough)” 가설([18])은 단일 스칼라 보상 최대화가 복잡한 인지 능력의 출현으로 이어진다고 주장하는 반면,</li>
<li>“스칼라 보상만으로는 충분하지 않다(Scalar Reward is Not Enough)” 관점([19])은 윤리적 고려 사항이나 복잡한 목표를 포착하기에는 단일 값이 불충분하며, 벡터 값 보상(vector-valued rewards)이 필요하다고 주장한다.</li>
</ul></li>
<li><strong>보상 설계의 일반적인 함정:</strong> 보상 희소성(sparsity), 기만적인 보상(deceptive rewards), 보상 해킹(reward hacking), 의도치 않은 결과(unintended consequences), 진정한 목표와의 불일치(misaligned reward), 보상 함수 복잡성, 보상 설계 평가의 어려움 등이 있다.</li>
<li><strong>스칼라 대 벡터 보상:</strong>
<ul>
<li>스칼라 보상은 단순하고 계산 효율적이지만, 복잡한 작업의 미묘한 차이를 포착하지 못할 수 있다.</li>
<li>벡터 보상은 여러 값을 사용하여 작업의 다양한 측면을 나타내어 더 풍부한 피드백을 제공하지만, 효과적인 보상 함수 설계와 다중 목표 균형 조절에 어려움이 따른다.</li>
</ul></li>
</ul>
</section>
<section id="reward-shapingengineering-기법-분류" class="level3">
<h3 class="anchored" data-anchor-id="reward-shapingengineering-기법-분류">Reward Shaping/Engineering 기법 분류</h3>
<p><strong>A. 정책 기울기(Policy Gradient) 방법들:</strong></p>
<ul>
<li><strong>정책 기울기 기반 보상 설계(PGRD)</strong>: 온라인 기울기 상승(online gradient ascent)을 통해 보상 매개변수 <span class="math inline">\theta</span>를 반복적으로 조정하여 <span class="math inline">R_O</span>를 최대화한다. <span class="math inline">\theta^* = \arg \max_{\theta \in \Theta} \lim_{N \to \infty} E[\frac{1}{N}\sum_{t=0}^N R_O(s_t)|R(\cdot, \theta)]</span>. 기존 방법에 비해 에이전트 성능을 향상시킨다.</li>
<li><strong>정책 기울기 학습 내재적 보상(LIRPG)</strong>: 에이전트가 내재적 보상(intrinsic rewards)을 동적으로 학습하도록 하며, 정책 매개변수 <span class="math inline">\theta</span>를 <span class="math inline">\theta' \approx \theta + \alpha G_{ex+in}(s_t, a_t) \nabla_\theta \log \pi_\theta(a_t|s_t)</span>를 통해 업데이트한다. 학습 효율성과 샘플 복잡도를 개선한다.</li>
<li><strong>시연 기반 DDPG(DDPGfD)</strong>: Deep Deterministic Policy Gradient(DDPG)의 확장으로, 인간 시연 궤적을 리플레이 버퍼(replay buffer)에 통합하고 우선순위 리플레이(prioritized replay)를 활용하여 희소 보상 문제를 해결한다. <span class="math inline">L_1(\theta_Q)</span> 및 <span class="math inline">L_n(\theta_Q)</span> 손실을 혼합 사용하고, <span class="math inline">L_2</span> 페널티로 정규화한다. 복잡한 Reward Shaping 없이 로봇 작업 학습을 간소화한다.</li>
</ul>
<p><strong>B. 강건성(Robustness) 및 적응성(Adaptability)을 갖춘 방법들:</strong></p>
<ul>
<li><strong>리더-팔로워 프레임워크:</strong> 리더가 팔로워의 보상 함수를 수정하여 원하는 행동을 유도, 시스템의 강건성을 높인다.</li>
<li><strong>강건한 RL:</strong> 노이즈가 있는 보상 신호를 추정하고 수정하기 위해 혼동 행렬(confusion matrix)을 사용한다. <span class="math inline">Q_{t+a}(s_t, a_t) = (1 - \alpha_t)Q(s_t, a_t) + \alpha_t[\hat{r}_t + \gamma \max_{b \in A} Q(s_{t+1}, b)]</span>와 같이 편향 없는 대리 보상(surrogate rewards) <span class="math inline">\hat{r}</span>을 Q-학습에 적용하여 노이즈 조건에서 높은 보상을 달성한다.</li>
</ul>
<p><strong>C. 탐색(Exploration) 전략들:</strong></p>
<ul>
<li><strong>해시 기반 Reward Shaping:</strong> 고차원 및 연속 상태 공간에서 카운트 기반 탐색(count-based exploration)을 확장하기 위해 해시 코드(hash codes)를 활용한다. <span class="math inline">\phi(s) = \text{sgn}(Ag(s)) \in \{-1, 1\}^k</span>와 같이 정적 해싱 기법(locality-sensitive hashing SimHash)을 사용한다.</li>
<li><strong>변분 정보 최대화 탐색(VIME)</strong>: 환경 역학에 대한 에이전트의 이해를 최대화하는 데 중점을 둔다. 베이즈 신경망(Bayesian neural networks) 내에서 변분 추론(variational inference)을 사용하여 정보 획득을 내재적 보상으로 활용한다.</li>
<li><strong>온라인 Reward Shaping(EXPLORES)</strong>: 희소하거나 노이즈가 있는 보상 신호를 처리하는 RL 에이전트의 학습 효율성을 높이기 위해 내재적 보상 학습과 탐색 기반 보너스를 결합한다. 알고리즘은 <span class="math inline">\pi_k \leftarrow L(\pi_{k-1}, \hat{R}_{k-1})</span> 및 <span class="math inline">\hat{R}_k</span> 업데이트를 반복한다.</li>
<li><strong>탐색을 위한 보상 불확실성(RUNE)</strong>: 선호도 기반 RL 알고리즘 내에서 학습된 보상 함수의 불확실성을 탐색 보너스로 통합한다. 앙상블(ensemble) 예측의 분산을 사용하여 탐색을 향상시킨다.</li>
</ul>
<p><strong>D. 정책 매개변수화(Policy Parameterization):</strong></p>
<ul>
<li>선형(linear) 및 방사형 기저 함수(RBF) 정책과 같은 단순화된 정책 매개변수화를 탐색하며, 관측치의 무작위 푸리에 특징(random Fourier features) <span class="math inline">y_t^{(i)} = \sin(\sum_j P_{ij}s_t^{(j)}v + \phi^{(i)})</span>를 사용하여 표현 능력을 강화한다. 경쟁력 있는 성능과 빠른 학습 속도를 제공한다.</li>
</ul>
<p><strong>E. 역 보상 설계(Inverse Reward Design, IRD):</strong></p>
<ul>
<li>전문가 시연을 통해 참 목표를 추론한다. 역 강화 학습(IRL)은 관찰된 전문가 행동을 기반으로 전문가 에이전트가 최대화하는 것으로 추정되는 보상 함수를 추론한다. 주요 방법론은 모델 기반 IRL, 모델 프리 IRL, 딥 역 강화 학습이 있다. 딥 IRL에서는 <span class="math inline">\hat{R}(s, a) = f_\theta(s, a)</span>와 같이 신경망 <span class="math inline">f_\theta</span>로 보상 함수를 학습한다. IRD는 프록시 보상(proxy reward)을 전문가 시연으로 처리하여 보상 불일치 및 보상 조작과 같은 문제를 해결하고 위험 회피 행동을 유도한다.</li>
</ul>
<p><strong>F. 보상 지평(Reward Horizon):</strong></p>
<ul>
<li>보상 지평을 단축함으로써 학습 시간을 단축할 수 있다. 이는 RL 알고리즘이 최적 정책을 학습하는 데 필요한 시간을 극적으로 줄일 수 있음을 보여준다.</li>
</ul>
<p><strong>G. 잠재 함수 기반(Potential-Based) 방법들:</strong></p>
<ul>
<li>보상 함수를 <span class="math inline">R'(s, a) = R(s, a) + \gamma [\Phi(s') - \Phi(s)]</span>로 수정하여 에이전트의 행동을 유도하고 탐색 및 수렴을 개선한다.</li>
<li><strong>잠재 함수 기반 Reward Shaping(PBRS)</strong>: 에피소드(episode) 보상을 기반으로 보상 신호를 강화하여 학습 효율성을 높인다. 잠재 함수 <span class="math inline">\Phi(s, a, t) = 0 \text{ if } R(s,a)=0 \text{ else } (1 + \frac{Rep - Rep_u(t)}{Rep_u(t) - Rep_l(t)})</span>를 사용한다.</li>
<li><strong>다중 에이전트 시스템(MAS)을 위한 이론적 기초:</strong> 잠재 함수 기반 Reward Shaping이 Q-테이블 초기화와 동등하며 기초 확률 게임의 내쉬 균형(Nash Equilibria)에 영향을 미치지 않음을 보여준다.</li>
<li><strong>PBRS-MAXQ-0 방법:</strong> 계층적 강화 학습(HRL) 프레임워크 내에서 PBRS와 MAXQ를 결합한다. 특정 조건에서 이론적 수렴 보장을 제공하며, 적절한 휴리스틱(heuristics)을 사용할 경우 수렴을 가속화한다.</li>
<li><strong>잠재 함수 기반 조언(Potential-Based Advice):</strong> 임의의 보상 함수를 잠재 함수 기반 조언으로 표현하는 프레임워크를 제시한다. 보조 가치 함수를 학습함으로써 정책 불변성을 보장한다.</li>
<li><strong>에피소드 RL에서의 PBRS:</strong> 모델 프리, 모델 기반, 다중 에이전트 RL에서의 PBRS 적용을 탐구한다.</li>
<li><strong>잠재 함수 기반 보상 함수(PBRF)를 사용한 제약 조건부 RL:</strong> 검증된 하이브리드 시스템 모델로부터 안전 지향적 보상 함수를 생성하는 접근 방식을 제안하여 훈련 중 더 빠른 수렴을 보인다.</li>
<li><strong>잠재 함수 기반 Reward Shaping(DRIP) 및 잠재 함수로서의 반사실(CaP)을 통합한 차이 보상:</strong> DRIP은 다중 에이전트 RL에서 차이 보상과 잠재 함수 기반 Reward Shaping을 결합한다. CaP는 동적 잠재 함수를 자동으로 생성하여 수동 설계의 필요성을 없앤다.</li>
</ul>
<p><strong>H. 동적 잠재 함수 기반 Reward Shaping(Dynamic Potential-Based Reward Shaping, DPBRS):</strong></p>
<ul>
<li>단일 및 다중 에이전트 시스템에서 최적 정책에 영향을 미치지 않고 에이전트를 안내하기 위해 동적 잠재 함수를 사용한다. <span class="math inline">F(s, t, s', t') = \gamma \Phi(s', t') - \Phi(s, t)</span>와 같이 시간 요소를 포함하여 잠재 함수를 확장한다.</li>
</ul>
<p><strong>I. UCBVI(Upper Confidence Bound Value Iteration):</strong></p>
<ul>
<li><strong>UCBVI:</strong> 최적 가치 함수에 대한 상한 신뢰 구간(upper confidence bound, UCB)으로 가치 함수가 기능하도록 보장한다.</li>
<li><strong>UCBVI-Shaped:</strong> UCBVI 알고리즘의 수정된 버전으로, Reward Shaping을 통합하여 보너스와 가치 함수 투영(projection)을 수정한다. 상태 공간의 관련 없는 부분을 효과적으로 가지치기하여 샘플 복잡도를 개선한다.</li>
</ul>
<p><strong>J. 차이 보상(Difference Rewards, D):</strong></p>
<ul>
<li><strong>단일 에이전트 시스템:</strong> 에이전트의 현재 상태와 지정된 참조 상태 간의 불일치를 측정하는 차이 항을 통합하여 원래 보상 함수를 강화한다: <span class="math inline">R'(s, a) = R(s, a) + \gamma [D(s', r) - D(s, r)]</span>.</li>
<li><strong>다중 에이전트 시스템(MAS):</strong> <span class="math inline">D_i(s_i, a_i) = G(s, a) - G(s_{-i} \cup s_{ci}, a_{-i} \cup a_{ci})</span>와 같이 에이전트의 기여 유무에 따른 시스템 성능 차이를 반영하는 보상을 제공하여 에이전트가 전체 시스템 유틸리티에 기여하도록 장려한다.</li>
<li><strong>개별 및 차이 보상:</strong> 도로 네트워크의 경로 선택 문제에서 개별 보상(이기적인 접근)과 차이 보상(시스템 최적화)을 비교한다. 차이 보상을 사용하는 DQ-learning이 IQ-learning보다 우수한 성능을 보인다.</li>
</ul>
<p><strong>K. 지식 기반 다중 목표 다중 에이전트 강화 학습(Knowledge-Based Multi-Objective Multi-Agent Reinforcement Learning, MOMARL):</strong></p>
<ul>
<li>차이 보상(D)과 잠재 함수 기반 Reward Shaping(PBRS)을 비교하며, 두 기법 모두 MOMARL 영역에서 파레토 최적(Pareto optimal) 솔루션으로 에이전트를 효과적으로 유도할 수 있음을 입증한다. D는 일반적으로 PBRS보다 성능이 우수하다.</li>
</ul>
<p><strong>L. 계획 기반(Plan Based) 방법들:</strong></p>
<ul>
<li><strong>STRIPS 활용:</strong> STRIPS(Stanford Research Institute Problem Solver) 기반 Reward Shaping 기법을 도입하여 도메인 지식을 활용하여 RL 방법의 수렴을 가속화한다.</li>
<li><strong>계획 기반과 추상 MDP 비교:</strong> 계획 기반 방법은 사전 정의된 계획을 기반으로 추가 보상을 제공하는 반면, 추상 MDP(Abstract MDP) 접근 방식은 더 높은 수준의 MDP를 해결하여 행동을 형성한다. 복잡한 환경에서 계획 기반 방법이 더 효과적이며, 다중 에이전트 시나리오에서는 추상 MDP가 더 우수하다.</li>
</ul>
<p><strong>M. 신념 Reward Shaping(Belief Reward Shaping, BRS):</strong></p>
<ul>
<li>환경의 보상 구조에 대한 사전 지식(prior knowledge)을 통합하여 강화 학습을 향상시킨다. 베이즈 프레임워크(Bayesian framework)에서 파생된 “신념 보상(belief rewards)”으로 표준 보상 신호를 보강한다. <span class="math inline">\Phi(s)</span>만 고려하는 잠재 기반 Reward Shaping의 한계를 넘어 상태와 행동 모두에 직접 형성 보상을 제공한다.</li>
</ul>
<p><strong>N. 매개변수화된 Reward Shaping의 이중 수준 최적화(Bi-Level Optimization of Parameterized Reward Shaping, BiPaRS):</strong></p>
<ul>
<li>인간이 설계한 보상 함수의 불완전성을 해결하기 위해 이중 수준 최적화 프레임워크를 사용한다. 상위 수준에서는 형성 보상에 대한 매개변수화된 가중 함수 <span class="math inline">z_\phi(s,a)</span>를 최적화하고, 하위 수준에서는 형성된 보상 <span class="math inline">\tilde{r}(s, a) = r(s, a) + z_\phi(s, a)f(s, a)</span>를 사용하여 정책을 최적화한다. 목표는 <span class="math inline">\max_\phi E_{s \sim \rho^\pi, a \sim \pi_\theta} [r(s, a)]</span>를 최대화하는 정책 <span class="math inline">\pi_\theta</span>를 <span class="math inline">\theta = \arg \max_{\theta_0} E_{s \sim \rho^\pi, a \sim \pi_{\theta_0}} [\tilde{r}(s, a)]</span> 제약 하에 찾는 것이다.</li>
</ul>
<p><strong>O. 인간 피드백을 통한 Reward Shaping:</strong></p>
<ul>
<li>인간 선호도(human preferences)를 통합하여 보상 구조를 효과적으로 형성하고 학습 과정을 가속화한다. PEBBLE, SURF, RUNE, Text2Reward, Meta-Reward-Net (MRN) 등이 있다. 보상 함수 <span class="math inline">R</span>에 대한 인간 피드백 <span class="math inline">H</span>의 사후 확률은 <span class="math inline">P(R|H) = \frac{P(H|R)P(R)}{P(H)}</span>로 모델링될 수 있다. Text2Reward는 대규모 언어 모델(LLMs)을 사용하여 희소 보상 문제에 대한 밀집 보상(dense reward) 함수를 자동으로 생성한다. MRN은 이중 수준 최적화를 사용하여 보상 함수와 정책을 동시에 학습한다.</li>
</ul>
<p><strong>P. 소수 대표 Reward Shaping 기법들:</strong></p>
<ul>
<li><strong>전력망 사이버 공격 방어를 위한 딥 Q-학습의 Reward Shaping:</strong> 딥 Q-학습을 사용하여 전력망을 사이버 공격으로부터 보호하며, 공격자가 네트워크에 가장 큰 피해를 입힐 수 있는 시나리오를 찾기 위해 Reward Shaping을 사용한다.</li>
</ul>
<p><strong>Q. 기타 방법들:</strong></p>
<ul>
<li>안전한 RL 에이전트 행동을 보장하기 위한 <strong>장벽 함수(Barrier Functions)</strong>, 자연어 명령을 사용하여 밀집 보상을 생성하는 방법, 평균 보상 RL을 위한 <strong>시간 논리 기반(Temporal Logic-based) Reward Shaping</strong>, 불확실성에 기반한 보상에 페널티를 부과하는 프레임워크, 불확실한 RL 환경에서 수렴을 안정화하고 가속화하기 위한 <strong>보상 기대(Reward Expectations)</strong> 사용, 상태 공간의 테셀레이션(tessellation)과 서브 태스크(sub-tasks) 분할에 의존하는 <strong>보상 계획(Reward Planning)</strong> 방법인 “Greedy Divide and Conquer” 등이 있다.</li>
<li><strong>자동화된 매개변수 튜닝(Automated Parameter Tuning):</strong> DEHB(Differential Evolution for HyperBand)와 같은 첨단 하이퍼파라미터 최적화(hyperparameter optimization) 알고리즘을 사용하여 하이퍼파라미터와 보상 함수를 공동으로 최적화한다. AutoRL(Automated Reinforcement Learning)은 MDP 모델링, 알고리즘 선택, 하이퍼파라미터 최적화를 자동화하는 것을 목표로 한다.</li>
</ul>
<p><strong>실제 적용:</strong></p>
<ul>
<li><strong>안전하고 효율적인 인간-로봇 협업(HRC)을 위한 Reward Shaping:</strong> 산업 환경에서 인간과 로봇 간의 안전한 상호작용을 향상시키기 위해 DRL 접근 방식을 사용한다. IRDDPG(Intrinsic Reward-Deep Deterministic Policy Gradient) 알고리즘은 내재적 및 외재적 보상을 결합한 최적화된 보상 함수와 DPG(Deterministic Policy Gradient) 방법을 통합한다. 로봇은 충돌 회피 정책을 효과적으로 학습한다.</li>
<li><strong>자율 주행 차량 및 교통 흐름 최적화를 위한 Reward Shaping:</strong> 오프-정책(off-policy) 강화 학습 방법을 교통 내비게이션에 적용한다. Episodic-Guided Prioritized Experience Replay (EPER) 방법은 에피소드 메모리와 시간차(TD) 오차 기반 우선순위 지정을 사용하여 샘플 비효율성을 해결하고 학습 속도를 높인다.</li>
</ul>
<p><strong>시뮬레이션-실세계 전이 (Sim-to-Real):</strong></p>
<ul>
<li>DRL 및 RL 정책을 시뮬레이션에서 실제 세계로 효과적으로 전이시키는 데 필수적이다. 시뮬레이션과 실제 환경 간의 불일치, 안전, 비용, 효율성 문제 등으로 인해 도전적이다.</li>
<li><strong>주요 기법:</strong> 도메인 적응(domain adaptation), 도메인 무작위화(domain randomization), 점진적 신경망(progressive neural networks, PNNs), 메타 강화 학습(meta-reinforcement learning).</li>
<li><strong>CSAR(Consensus-based Sim-And-Real DRL):</strong> 시뮬레이션과 실제 환경 모두에 적합한 정책을 최적화하여 일관된 보상 구조를 보장한다.</li>
<li><strong>보상 해킹(Reward Hacking) 방지:</strong> 에이전트가 의도된 작업을 실제로 수행하지 않고 보상 함수의 허점을 악용하는 것을 방지하기 위해 효과적인 Reward Shaping이 중요하다.</li>
</ul>
<p><strong>Reward Engineering의 장단점:</strong></p>
<ul>
<li><strong>장점:</strong> 학습 가속화, 탐색 개선, 높은 성능 달성, 정책 강건성 향상.</li>
<li><strong>단점:</strong> 도메인 지식에 대한 높은 의존성, 일부 방법의 계산 복잡성 증가, 매개변수 튜닝의 어려움, 보상 함수 설계 및 형성의 시간 소모.</li>
</ul>
<p><strong>남은 과제 및 미래 방향:</strong></p>
<ul>
<li>로봇 조작에서 샘플 효율성 향상, 알고리즘 강건성 강화, 인간-로봇 협업 촉진, 고급 신경망 아키텍처 연구.</li>
<li>이미지 처리나 복잡한 센서 데이터와 같이 보상 설계가 어려운 작업에 대한 end-to-end RL 접근 방식.</li>
<li>환경의 불확실성에 대한 강건성 부족 및 사용자 질의 의존성 문제 해결.</li>
</ul>
<blockquote class="blockquote">
<p>본 연구는 RL 분야에서 Reward Shaping 기법에 대한 포괄적인 검토를 제공하며, 상세한 분류 체계, 장점, 단점, 적용 영역 및 관련 메트릭스를 제시한다. Reward Shaping이 학습 가속화, 불확실성 관리, 강건성 강화, 시스템 결과 향상 및 RL 에이전트의 성공률 증가에 상당한 이점을 제공함을 강조한다. 그러나 구현은 복잡하고 시간이 소모될 수 있으며, 자동화된 매개변수 튜닝 방법이 이를 일부 완화한다. 미래 연구는 실제 적용에서의 평가와 인간 피드백 통합 가능성을 탐구해야 한다.</p>
</blockquote>
<hr>
</section>
</section>
<section id="detail-review" class="level1">
<h1>Detail Review</h1>
<blockquote class="blockquote">
<p>강화학습에서의 Reward Engineering 및 Reward Shaping: 종합 개관 및 심층 분석</p>
</blockquote>
<section id="서론" class="level2">
<h2 class="anchored" data-anchor-id="서론">서론</h2>
<p>강화학습(Reinforcement Learning, RL)은 에이전트(Agent)가 환경(Environment)과 상호작용하며 시도를 통해 최적의 행동 전략을 학습하는 기계학습의 한 분야입니다. RL 에이전트는 누적 보상(cumulative reward)을 최대화하도록 학습하며, <strong>보상</strong>(reward)은 에이전트가 받는 피드백 신호로서 행동의 성과를 나타냅니다. 따라서 <em>보상 함수</em>의 설계는 RL 알고리즘의 효율성과 성능에 지대한 영향을 미치는 핵심 요소입니다. 잘 설계된 보상 함수는 에이전트에게 명확한 행동 지침을 주어 원하는 목표에 보다 빠르게 도달하도록 만들지만, 부적절한 보상 설계는 의도치 않은 행동이나 학습 지연을 초래할 수 있습니다.</p>
<p><strong>보상 설계</strong>(reward design)는 원하는 에이전트 행동과 목표에 부합하도록 보상 함수를 정의하는 섬세하고도 복잡한 과정입니다. 보상 설계의 중요성은 아무리 강조해도 지나치지 않으며, 실제 환경이 복잡해질수록 그 중요성은 더욱 커집니다. 일반적으로 보상 설계의 접근법은 두 가지 범주로 나뉩니다. 첫째, <strong>Reward Engineering</strong>(Reward Engineering)은 말 그대로 보상 함수를 만들어내는 작업으로, 상태와 행동 (및 결과 상태)을 특정 수치 보상으로 매핑하는 함수를 처음부터 설계하는 것입니다. 잘 설계된 보상 함수는 에이전트의 행동에 대해 <strong>정보성 있는 피드백</strong>을 제공하여 바람직한 행동을 강화해야 합니다. 동시에 <strong>너무 빈번한 보상</strong>은 에이전트가 쉬운 편법을 찾아내는 <em>(reward hacking)</em> 부작용을 낳을 수 있으므로, 적절한 희소성도 필요합니다. 둘째, <strong>Reward Shaping</strong>(Reward Shaping)은 기본 보상 함수를 설정한 후 학습 과정을 개선하기 위해 보상 신호를 세밀 조정하는 기법입니다. Reward Shaping은 <em>기존의 최적 정책(optimal policy)을 바꾸지 않으면서</em> 학습을 가속하기 위해 추가적인 보상 피드백을 주는 것을 목표로 합니다. 예를 들어, 로봇 팔 제어 과제에서 목표물까지의 <strong>음의 거리</strong>를 잠재적 보상(potential reward)으로 설정하면, 에이전트는 최종 성공 여부와 관계없이 목표물에 가까워질 때마다 추가 보상을 받아 목표에 점진적으로 다가가도록 유도할 수 있습니다.</p>
<p>이처럼 Reward Engineering과 Reward Shaping은 RL 에이전트가 <strong>효율적으로 원하는 행동</strong>을 학습하도록 하는 강력한 도구입니다. 그러나 현실 세계의 RL에는 해결해야 할 여러 어려움이 남아 있습니다. <strong>희소하거나 지연된 보상</strong>은 대표적인 문제로, 보상이 드물게 주어지면 에이전트의 학습 진행이 매우 더뎌질 수 있습니다. 또한 실제 환경의 동역학을 정확히 모델링하기 어렵고, 고차원 상태·행동 공간에서 RL 알고리즘을 학습시키는 데 <strong>막대한 계산 비용</strong>이 든다는 한계도 지적됩니다. 다행히 근래 <strong>딥러닝</strong>의 발전으로 복잡한 상태공간을 다루는 능력이 향상되어 로보틱스, 자율주행, 게임 등 다양한 복잡한 과제에도 RL이 응용되고 있지만, 이러한 분야에서 RL을 성공적으로 적용하려면 여전히 효과적인 보상 설계/형성 기법이 필요합니다. 본 종설 논문은 이러한 배경하에, <strong>강화학습의 성능을 극대화하기 위한 Reward Engineering 및 Reward Shaping 기법들</strong>의 현재까지 연구 동향을 포괄적으로 정리하고 각 접근법의 장단점을 분석합니다. 이를 통해 RL 분야 연구자들에게 향후 연구 방향과 실제 응용 시 고려할 통찰을 제공하는 것이 목적입니다.</p>
</section>
<section id="연구-방법-문헌-조사-및-선정-과정" class="level2">
<h2 class="anchored" data-anchor-id="연구-방법-문헌-조사-및-선정-과정">연구 방법 (문헌 조사 및 선정 과정)</h2>
<p>이 논문은 Reward Shaping 기법에 관한 기존 연구들을 <strong>체계적 문헌조사</strong>(systematic literature review) 방식으로 종합했습니다. 저자들은 주요 학술 데이터베이스에서 “reward shaping”, “reward engineering”, “reinforcement learning”, “reward design”, “control systems” 등의 키워드를 조합하여 1999년부터 2024년까지 출판된 관련 문헌을 검색했습니다. 검색된 논문들은 PRISMA 2020 지침에 따라 단계적으로 선별되었으며, 최종적으로 <strong>55편의 핵심 연구</strong>가 리뷰에 포함되었습니다. 이들 논문은 주로 인공지능 기반 제어 시스템에서의 Reward Shaping 기법을 다루며, 실험적 결과를 보고한 연구들로 제한하였습니다. 다만 이론적 프레임워크만 제시하거나 실증 데이터가 없는 논문들은 배제되었으며, 예외적으로 영향력이 크다고 판단된 몇몇 이론 논문은 검토 대상에 포함되었습니다. 또한 포함된 각 연구의 품질을 평가하기 위해 사전에 정의한 기준에 따라 <strong>품질 평가</strong>도 수행하였습니다. 이러한 체계적인 문헌 조사 과정을 통해, 본 리뷰는 강화학습에서의 보상 설계 및 형성에 관한 신뢰도 높은 최신 지식들을 종합적으로 정리하였습니다.</p>
</section>
<section id="배경-및-기본-개념" class="level2">
<h2 class="anchored" data-anchor-id="배경-및-기본-개념">배경 및 기본 개념</h2>
<p><strong>강화학습의 구성 요소:</strong> RL을 이해하기 위해 기본 개념을 짚고 넘어가면 다음과 같습니다:</p>
<ul>
<li><strong>에이전트(Agent)</strong>: 환경과 상호작용하며 학습하는 주체입니다.</li>
<li><strong>환경(Environment)</strong>: 에이전트가 행동을 수행하는 세계로, 정해진 규칙과 동적 특성을 가지고 있습니다.</li>
<li><strong>정책(Policy)</strong>: 주어진 상태에서 에이전트가 취할 행동을 결정하는 전략 또는 함수입니다.</li>
<li><strong>보상(Reward)</strong>: 에이전트의 행동 결과로 주어지는 신호로, 특정 상태나 행동의 바람직함을 나타냅니다.</li>
<li><strong>가치 함수(Value Function)</strong>: 특정 상태(또는 상태-행동 쌍)에 대해 미래에 얻을 누적 보상의 기대값을 추정하는 함수입니다.</li>
<li><strong>탐험 vs 활용(Exploration vs Exploitation)</strong>: 새로운 행동을 시도하여 정보를 얻는 탐험과, 현재 학습한 최적 행동을 사용하는 활용 간의 균형을 말합니다.</li>
</ul>
<p><strong>Reward Shaping과 Reward Engineering의 정의:</strong> Reward Shaping과 Reward Engineering은 앞서 서론에서 설명한 대로 보상 설계의 두 축입니다. <strong>Reward Shaping(reward shaping)</strong>은 동물 훈련에서 영감을 얻은 개념으로, <strong>추가적인 보상</strong>을 제공하여 학습 문제를 쉽게 만드는 기법입니다. 즉, 에이전트의 학습을 가이드하기 위해 원래의 보상 함수에 인센티브나 패널티를 덧붙여 <strong>보상 신호를 조정</strong>하는 것을 의미합니다. 이에 반해 <strong>Reward Engineering(reward engineering)</strong>은 보다 폭넓은 개념으로, <strong>기존 알고리즘이나 기법을 활용해 보상 함수를 설계하거나</strong> 아예 처음부터 보상 함수를 만들어내는 모든 접근을 포괄합니다.</p>
<p><strong>“보상으로 충분하다” vs “보상만으로는 부족하다” 논쟁:</strong> 인공지능 연구자들 사이에서는 단일 스칼라 보상 신호만으로도 충분히 지능적 행동을 끌어낼 수 있는지에 대한 논쟁이 있습니다. 첫 번째 관점인 <strong>“보상이 충분하다(Reward is Enough)”</strong> 가설에 따르면, <strong>하나의 스칼라 보상</strong> 값만 최대화하도록 학습시켜도 복잡한 인지능력이 저절로 발현될 수 있다고 주장합니다. 이 입장에서는 <strong>단일 보상 목표</strong>를 향해 나아가는 과정에서, 환경의 복잡성이 자연스럽게 언어 이해나 사회성 같은 고차원 능력을 끌어낸다고 봅니다. 반면 <strong>“스칼라 보상만으로는 부족하다(Scalar Reward is Not Enough)”</strong>라는 반론은 인간 지능의 다면적 특성을 단일 수치로 표현하기 어렵다고 지적하며, <strong>벡터 보상(vector-valued reward)</strong> 등 <strong>다중 목표</strong>를 도입해야 안전하고 인간과 조화된 AI를 만들 수 있다고 주장합니다. 특히 윤리적 판단이나 주관적 목표가 필요한 영역에서는 여러 지표를 함께 고려해야 올바른 의사결정을 할 수 있다는 것입니다. 양측 모두 보상의 중요성은 인정하지만, <strong>단일 보상의 충분성</strong>에 대해서는 이처럼 견해 차이가 존재합니다. 결국 실제 적용에서는 문제 특성에 따라 단일 보상의 단순함과 다중 보상의 풍부함 사이에서 적절한 선택을 해야 하며, 복잡한 인간 지능을 모사하려면 <strong>더 섬세한 보상 표현</strong>이 필요할 수 있음을 시사합니다.</p>
<p><strong>보상 설계 시 일반적인 함정들:</strong> 효과적인 보상 함수를 만들기란 쉽지 않습니다. 잘못된 보상 설계는 에이전트의 엉뚱한 행동을 유발하거나 학습 실패로 이어질 수 있는데, 이런 위험요소들을 미리 파악하는 것이 중요합니다. 보상 설계의 대표적인 <strong>함정(pitfall)</strong>들은 다음과 같습니다:</p>
<ul>
<li><strong>보상의 희소성(Sparsity)</strong>: 보상 신호가 너무 드물거나 지연되어 제공되면 에이전트는 어떤 행동이 좋은지 학습하기 어려워져 학습 속도가 매우 느려집니다.</li>
<li><strong>기만적 보상(Deceptive Reward)</strong>: 설정한 보상이 의도와 달리 에이전트에게 <em>쉬운 편법</em>을 장려할 수 있습니다. 즉, 표면적인 보상만 높이고 실제 목표는 달성하지 못하는 행동을 취하게 만들 수 있습니다.</li>
<li><strong>보상 해킹(Reward Hacking)</strong>: 에이전트가 보상 함수를 악용하는 현상으로, 원하는 목표는 이루지 않은 채 보상만 최대화하는 <em>치트</em> 행동을 찾는 경우입니다. 예를 들어 청소 로봇에게 “깨끗한 방 유지”에 보상을 주었더니, 로봇이 실제 청소는 하지 않고 센서를 가려 <strong>더러운 것을 인식하지 못하게</strong> 만드는 식입니다.</li>
<li><strong>의도치 않은 부작용(Unintended Consequences)</strong>: 복잡한 환경에서는 보상 설계로 인해 예상치 못한 부작용이 나타날 수 있습니다. 에이전트 행동과 환경의 상호작용이 예측 불가능하여, 사소한 보상 항목이 큰 문제를 일으키기도 합니다.</li>
<li><strong>목표와 보상의 불일치(Misalignment)</strong>: 보상 함수가 개발자의 진짜 의도를 완벽히 대변하지 못하는 경우입니다. 이 경우 에이전트는 높은 보상을 받아도 정작 우리가 원한 목표는 달성하지 못할 수 있습니다.</li>
<li><strong>복잡한 보상 함수(Complexity)</strong>: 보상에 너무 많은 요소를 넣으면 보상 함수를 설계하고 해석하기가 어려워집니다. 다목적 보상은 그 가중치를 정하기 힘들고, 잘못하면 어느 한 쪽도 만족시키지 못할 우려가 있습니다.</li>
<li><strong>평가의 어려움(Evaluation Difficulty)</strong>: 복잡한 환경일수록 한 번 정한 보상 구조가 제대로 작동하는지 객관적으로 평가하기 힘듭니다. 결국 시행착오를 거쳐야 하는데, 이 과정 또한 비용이 큽니다.</li>
</ul>
<p>위와 같은 이유들 때문에, 기존의 단순 보상 설계만으로는 한계가 있을 때 <strong>Reward Engineering적 기법</strong>들이 대안으로 활용됩니다. 즉, 보다 체계적이고 발전된 방법으로 보상 함수를 개선함으로써 이러한 함정을 피하려는 것입니다. 또한 한 가지 기억해야 할 점은, 보상 함정은 완전히 없앨 수는 없더라도 <strong>반복적인 테스트와 검증(iterative testing &amp; validation)</strong>을 통해 발견하고 수정해야 한다는 것입니다. 에이전트가 보상을 잘못 최대화하고 있지는 않은지 지속적으로 관찰하여, 의도대로 행동하도록 보상 구조를 다듬어나가야 합니다.</p>
<p><strong>스칼라 보상 vs 벡터 보상:</strong> 앞서 언급한 논쟁과 관련하여, 보상을 <strong>단일 스칼라 값</strong>으로 줄 것이냐 아니면 <strong>다차원 벡터</strong>로 구성할 것이냐도 중요한 설계 선택입니다. 스칼라 보상은 한 가지 수치로 목표 진척도를 나타내므로 <strong>단순하고 명확</strong>하며 계산도 용이합니다. 예를 들어 게임에서 이겼으면 +1, 졌으면 0과 같이 하나의 값만 주면 되니 해석도 쉽습니다. 그러나 복잡한 과제에서는 스칼라 보상 하나로는 과제의 여러 측면을 모두 반영하기 어렵습니다. 그 결과 에이전트가 중요한 세부 요소를 놓치거나 <strong>국소 최적해</strong>에 빠질 위험이 있습니다. 반면 벡터 보상은 여러 개의 값을 통해 과제의 다양한 측면(예: 속도, 정확도, 자원 소모 등)을 평가하므로 더 <strong>풍부한 피드백</strong>을 제공합니다. 에이전트는 어떤 측면에 더 주력해야 하는지 학습할 수 있어 다목적 문제에서 유용합니다. 다만 벡터 보상을 사용하면 여러 보상 요소 간 <strong>트레이드오프</strong>를 다뤄야 하고, 보상 공간이 다차원이므로 학습이 복잡해지며 계산비용도 증가합니다. 따라서 <strong>과제의 특성</strong>, <strong>요구 성능 수준</strong>, <strong>계산 자원</strong> 등을 고려하여 스칼라 vs 벡터 보상을 결정하게 됩니다. 일반적으로 단순하고 명확한 목표에는 스칼라 보상이 좋지만, 안전성 등 다양한 목표를 동시에 달성해야 하는 문제라면 벡터 보상이나 다중 목표 접근이 필요할 수 있습니다.</p>
</section>
<section id="reward-shapingengineering-기법의-분류" class="level2">
<h2 class="anchored" data-anchor-id="reward-shapingengineering-기법의-분류">Reward Shaping/Engineering 기법의 분류</h2>
<p>이 섹션에서는 다양한 <strong>Reward Shaping 및 Reward Engineering 기법</strong>들을 그 원리에 따라 분류하고, 각 범주별 주요 알고리즘과 특징, 장단점을 상세히 살펴봅니다. 인간의 동기부여 이론에 빗대어 심층강화학습의 보상도 <strong>외재적 보상</strong>(extrinsic reward)과 <strong>내재적 보상</strong>(intrinsic reward)으로 구분할 수 있습니다. 외재적 보상은 환경으로부터 주어지는 명시적인 보상(예: 게임에서의 점수)이고, 내재적 보상은 에이전트 스스로 부여하는 보상(예: 새로운 상태를 발견했을 때의 만족감)으로 <strong>탐험 동기</strong>를 부여합니다. 실제로 RL 연구에서는 이러한 <strong>내재적 동기(intrinsic motivation)</strong>를 활용하여 에이전트가 희소한 외재적 보상 상황에서도 학습을 지속하도록 하는 기법들이 발전해왔습니다. 이처럼 보상에는 여러 종류와 활용법이 존재하며, 아래에서는 최신 연구들을 중심으로 다양한 <strong>Reward Eningeering/Shaping 전략</strong>을 기술합니다. 필요에 따라 각 접근법의 <strong>구체적 메커니즘과 사례</strong>, 그리고 <strong>장점과 한계</strong>를 함께 설명하겠습니다.</p>
<section id="정책-경사-기반-방법-policy-gradient-기반-보상-최적화" class="level3">
<h3 class="anchored" data-anchor-id="정책-경사-기반-방법-policy-gradient-기반-보상-최적화">정책 경사 기반 방법 (Policy Gradient 기반 보상 최적화)</h3>
<p><strong>정책 경사 기법</strong>은 보상 구조를 <strong>동적으로 학습</strong>하거나 최적화하는 접근입니다. 기본적으로 정책 경사(policy gradient) 방법은 가치 함수를 추정하지 않고 에이전트의 정책을 직접 최적화하는 RL 알고리즘군을 말합니다. 이러한 정책 경사 방법을 <em>보상 설계에 응용</em>한 대표 연구로 <strong>PGRD (Policy Gradient for Reward Design)</strong>가 있습니다. PGRD는 에이전트의 학습 중에 <strong>보상 함수의 파라미터를 온라인으로 조정</strong>하기 위해 그래디언트 상승(gradient ascent)을 적용합니다. 즉, 초기의 보상 함수를 고정된 것으로 두지 않고, <strong>설계자가 원하는 목표에 근접하도록 보상 파라미터를 계속 업데이트</strong>하는 것입니다. 이는 보상 자체를 최적화 문제로 취급하여, 에이전트가 환경에서 겪는 <strong>에피소드 결과</strong>에 따라 보상 함수를 조금씩 개선해나갑니다. 이 방법의 핵심 아이디어는 <em>“최적의 보상 파라미터는 결국 최적의 에이전트 성능을 이끈다”</em>는 것이며, 보상 설계 문제를 해결하기 위해 또 다른 학습(proximal gradient ascent)을 돌리는 형태입니다. PGRD는 이론적으로 수렴이 보장되며, 부분 관측 환경 등에서도 효과를 보였습니다. 실제 실험에서 PGRD는 기존에 고정된 보상으로 학습한 에이전트보다 <strong>더 우수한 성능</strong>을 발휘하는 것으로 나타났는데, 이는 에이전트의 능력과 환경 불확실성에 맞게 보상 구조가 적응적으로 변한 덕분입니다. 예를 들어 PGRD를 적용한 결과, 일부 환경에서는 에이전트가 <strong>더 높은 평균 보상</strong>을 얻으며 학습 속도도 향상되는 것이 확인되었습니다.</p>
<p>PGRD의 연장선상에서, <strong>LIRPG (Learning Intrinsic Reward for Policy Gradient)</strong> 알고리즘도 주목할 만합니다. LIRPG는 외재적 보상 외에 <strong>내재적 보상</strong>을 병렬로 학습함으로써 에이전트가 스스로 보상 신호를 구성하도록 합니다. 구체적으로, 정책 파라미터를 업데이트할 때 내재적 보상의 파라미터도 함께 업데이트하여, <strong>정책 학습과 보상 학습을 동시</strong>에 이룹니다. 이를 통해 에이전트는 환경이 주는 보상뿐만 아니라 자체적으로 정의한 보상을 기준으로도 학습 방향을 잡습니다. Atari 게임이나 MuJoCo 로보틱스 환경에서의 실험 결과, LIRPG는 <strong>표준 PPO 에이전트보다 일관되게 높은 성능</strong>을 보였습니다. 특히 내재적 보상만으로 정책을 학습시킨 경우에도 일정 수준 이상의 성능을 내는 등, <strong>표본 효율(sample efficiency)</strong>과 학습 속도를 개선하는 데 효과적이었습니다. 다만 LIRPG를 적용하려면 내재적 보상 함수의 형태와 파라미터를 잘 설정해야 하며, 내재적 보상과 정책 업데이트 간의 상호작용에 따라 민감도가 있을 수 있습니다. 즉, 새로운 보상함수를 함께 학습하는 만큼 <strong>하이퍼파라미터 튜닝의 부담</strong>이 있고, 부적절한 내재 보상은 오히려 학습을 교란할 위험도 존재합니다.</p>
<p>또 다른 사례로, <strong>DDPGfD (DDPG from Demonstrations)</strong> 알고리즘은 정책 경사 방법(DDPG)에 <strong>시범 데이터</strong>를 활용하여 보상 신호를 보강한 방법입니다. 이 접근은 특히 <strong>희소 보상 환경</strong>에서 효과적인데, 인간 시범(전문가 데모)을 <strong>리플레이 버퍼</strong>에 함께 저장하여 초기 학습을 돕습니다. DDPGfD는 오프폴리시(off-policy) Actor-Critic 알고리즘인 DDPG에 다음과 같은 변형을 가했습니다: (1) 인간 시범 데이터를 버퍼에 추가하고, (2) 샘플링 시 우선순위 샘플링을 사용하여 시범과 자가 플레이 데이터를 적절히 혼합하며, (3) 1-step 보상뿐만 아니라 n-step 리턴 보상으로 학습 신호를 강화하고, (4) 한 번 환경 상호작용마다 다중 업데이트를 수행하여 <strong>학습 효율</strong>을 높입니다. 또한 과적합을 막기 위해 가치망(크리틱)과 정책망(액터)에 L2 정규화를 적용했습니다. 이러한 변경을 통해, 예를 들어 7자유도 로봇팔로 <em>플라스틱 부품 끼워 맞추기</em> 작업을 학습하는 실험에서, DDPGfD는 시범 데이터 양에 비례하여 <strong>학습 곡선이 개선</strong>되고 보다 안정적으로 과제를 성공시켰습니다. 이 결과는 인간이 알려준 경로(시범)와 보상 구조를 활용하면, 순수 강화학습보다 훨씬 빠르게 복잡한 과제를 익힐 수 있음을 보여줍니다. DDPGfD의 강점은 <strong>탐험 문제</strong>를 완화하고 학습 초기의 시행착오를 줄여준다는 점이지만, 인간 시범 데이터의 질과 양에 성능이 좌우되며, 추가된 시범 처리 단계로 알고리즘의 구현 복잡도가 증가하는 단점이 있습니다.</p>
</section>
<section id="강건성-및-적응성-향상-방법" class="level3">
<h3 class="anchored" data-anchor-id="강건성-및-적응성-향상-방법">강건성 및 적응성 향상 방법</h3>
<p><strong>강건성(Robustness)</strong>과 <strong>적응성(Adaptability)</strong>을 높이기 위한 보상 설계 기법들은, 환경의 불확실성이나 변화를 견디고 새로운 상황에 빠르게 적응할 수 있도록 보상 함수를 조정하는 데 초점을 둡니다. 강건성은 에이전트가 <strong>잡음(disturbance)</strong>이나 모델 불확실성이 있어도 안정적 성능을 내는 능력이고, 적응성은 <strong>환경이나 임무가 바뀌어도</strong> 정책을 유연하게 조정하는 능력입니다.</p>
<p>한 가지 접근으로, <strong>리더-팔로워 프레임워크</strong>를 활용한 방법이 있습니다. 리더-팔로워 구조에서는 리더 에이전트가 팔로워의 보상 함수를 수정함으로써 팔로워의 행동을 원하는 방향으로 유도합니다. 예컨대, 리더가 보상에 페널티나 보너스를 살짝 가미해 팔로워가 안전한 행동을 우선하게 만드는 식입니다. 이를 통해 시스템 전체의 강건성을 향상시킬 수 있습니다. 이러한 아이디어는 멀티에이전트 시스템이나 계층형 제어에서 활용될 수 있으며, 본질적으로 <strong>한 에이전트가 다른 에이전트의 보상 구조를 정책적으로 관리</strong>하는 형태의 Reward Engineering이라 할 수 있습니다.</p>
<p>또 다른 중요한 문제는 <strong>센서 노이즈</strong>로 인한 보상 신호 왜곡입니다. 현실에서 에이전트의 보상은 센서 측정에 의존하는데, 센서에 노이즈가 끼면 잘못된 보상이 입력되어 RL 성능이 크게 떨어질 수 있습니다. 이를 해결하기 위해, 한 연구에서는 <strong>혼동 행렬(confusion matrix)</strong>을 이용해 노이즈 보상을 보정하는 <strong>강건 RL 프레임워크</strong>를 제안했습니다. 이 방법은 센서 보상 신호의 오류 분포를 가정하지 않고, 관찰된 결과로부터 <strong>혼동 행렬을 추정해 편향을 제거한 대리 보상(surrogate reward)</strong>을 계산합니다. 구체적으로, 혼동 행렬을 기반으로 한 <em>비편향 보상 추정 알고리즘</em>을 도입하여 <strong>진짜 환경 보상의 추정치</strong>를 얻습니다. 그런 다음 이 추정치로 Q러닝을 수행하는데, 이론적으로 추정 보상을 쓸 때도 학습이 최적 Q함수로 수렴함을 증명하였습니다. OpenAI Gym과 Atari 게임들에 이 기법을 적용한 결과, <strong>노이즈가 있는 환경에서도 에이전트의 기대 보상이 크게 향상</strong>되었고, PPO 알고리즘과 결합하면 <strong>노이즈에 강인한 정책</strong>을 학습할 수 있음을 확인했습니다. 심지어 어느 정도의 노이즈는 <strong>탐험을 도와주는 역할</strong>까지 하여 누적 보상이 증가하는 흥미로운 현상도 관찰되었습니다. 다만 이 방법은 혼동 행렬 추정 및 추가 연산으로 <strong>계산량이 늘어나</strong>며, 보정 효과가 다양한 노이즈 패턴에서 항상 잘 들어맞는지는 여전히 정확한 노이즈 모델링과 파라미터 튜닝에 달려 있습니다. 그럼에도 불구하고, 센서 노이즈 하에서 <strong>보상 함수를 재설계</strong>함으로써 RL 학습의 신뢰성을 높인 좋은 사례라 할 수 있습니다.</p>
<p>이밖에, <strong>특정 제어 성능 요구사항</strong>을 만족시키도록 보상을 형성하는 연구도 있습니다. 예를 들어 어떤 로봇 제어 문제에서 <strong>정착 시간</strong>이나 <strong>정상상태 오차</strong> 같은 기준을 충족시키고 싶다면, 모델 없이도 보상을 통해 이러한 목표를 달성하도록 shaping할 수 있습니다. 한 연구에서는 시스템 동역학 모델을 몰라도 RL 정책이 <strong>제어 목표(예: 빠른 안정화)</strong>에 맞는 궤적을 따르도록 <strong>체계적인 Reward Shaping 기법</strong>을 제시했습니다. 이는 보상에 제어 성능 조항을 넣어주거나, 에이전트가 이러한 성능 지표를 만족할 때 추가 보상을 주는 방식으로 이뤄집니다. 이 접근의 효과는 해당 제어 목표를 직접 최적화하도록 유도함으로써, RL 정책이 <strong>전통적인 제어기</strong>처럼 작동하도록 할 수 있다는 점입니다. 다만 이 역시 사전에 만족시켜야 할 성능 기준과 그를 보상으로 <strong>어떻게 정량화할지</strong> 도메인 지식이 필요합니다.</p>
</section>
<section id="탐험-전략-기반-방법-exploration-driven-shaping" class="level3">
<h3 class="anchored" data-anchor-id="탐험-전략-기반-방법-exploration-driven-shaping">탐험 전략 기반 방법 (Exploration-Driven Shaping)</h3>
<p>강화학습의 난제 중 하나는 <strong>탐험-활용 문제</strong>로, 새로운 정보를 얻기 위한 탐험과 이미 알고 있는 좋은 행동을 취하는 활용 사이의 균형을 잡는 것입니다. Reward Shaping 기법 중에는 에이전트의 <strong>탐험을 장려</strong>하여 학습을 촉진하는 것들이 다수 있습니다. 특히 <strong>희소 보상 환경</strong>에서는 에이전트가 보상을 받을 만한 상태를 찾아가야 하므로, <em>탐험 보너스</em> 형태의 내재적 보상을 주는 전략이 효과적입니다.</p>
<p>한 가지 전통적인 접근은 <strong>카운트 기반(count-based) 탐험</strong>으로, 에이전트가 방문하지 않은 상태에 보너스를 주는 방식입니다. 하지만 연속적이거나 고차원 상태 공간에서는 똑같은 상태를 반복해서 만날 가능성이 낮아 count 기반 방법이 힘을 쓰기 어렵습니다. 이를 해결하기 위해, <strong>해시 함수를 이용한 상태 인코딩</strong> 기법이 제안되었습니다. 예를 들어 상태를 저차원 이진 코드로 해싱(hash)하여 비슷한 상태들을 동일하게 취급함으로써, <strong>사실상 유사한 상태 재방문을 카운팅</strong>할 수 있습니다. 한 연구에서는 <strong>지역민감 해시(Locality-Sensitive Hashing)</strong> 기법의 일종인 SimHash를 사용해 연속 상태를 이진코드로 변환한 후, 그 해시 코드를 기반으로 방문 횟수를 세어 <strong>탐험 보너스</strong>를 주었습니다. 또한 자동인코더(AE)를 활용한 <strong>학습된 해싱</strong> 방법도 함께 사용하여, 중요한 상태의 특징을 잡아내는 해시 함수를 학습했습니다. 이러한 해시 기반 탐험 보너스를 TRPO(Trust Region Policy Optimization) 알고리즘에 적용한 결과, MountainCar, HalfCheetah 등의 환경에서 <strong>기존 TRPO로는 불가능했던 목표 도달</strong>을 해시 보너스 덕분에 달성할 수 있었습니다. 특히 상태 공간이 큰 HalfCheetah 환경에서, 해시 보너스를 준 TRPO는 매우 희소한 보상도 포착해내어 일부 성과는 <strong>다른 최신 탐험 기법(VIME 등)과 견줄 만한 수준</strong>을 보였습니다. 이 방법의 강점은 <strong>고차원 공간에도 적용 가능</strong>하다는 점과, 비교적 일반적인 방법으로 다양한 RL 도메인에 활용될 수 있다는 것입니다. 단점으로는 어떤 해시 함수를 쓰느냐에 따라 성능 편차가 생기므로 <strong>해시 함수 설정 및 학습의 난이도</strong>가 있고, 상태 표현의 질에 민감하여 환경마다 튜닝이 필요하다는 것입니다.</p>
<p>다른 접근으로, <strong>탐험 유도 Reward Shaping(EXPLORS)</strong> 프레임워크가 제안되었습니다. EXPLORS는 <strong>내재적 보상 학습</strong>과 <strong>탐험 보너스</strong>를 완전 자율방식으로 결합하여, 에이전트가 <strong>스스로 탐험을 극대화하도록</strong> 설계되었습니다. 이 기법에서는 알고리즘 1과 같은 절차로 매 스텝 보상과 정책을 번갈아 업데이트합니다. 간략히 말해, 에이전트가 행동할 때 <strong>환경 보상(외재적)</strong>뿐만 아니라 <strong>추가적인 내재 보상</strong>을 병렬로 학습하여, 두 보상의 합으로 정책을 개선합니다. 이를 통해 전통적 Reward Shaping이 어려운 상황에서도 학습을 가속할 수 있음을 여러 환경에서 검증하였습니다. 예컨대, 매우 희소하거나 노이즈가 큰 보상 환경에서 EXPLORS를 쓴 에이전트는 <strong>일반 REINFORCE 알고리즘보다 더 빠르게 수렴</strong>하였고, 기존의 수작업 보상 shaping 방법들을 능가하는 성능을 보였습니다. 다만 이 연구에서도 언급하듯이, 보다 복잡한 환경에서 EXPLORS의 효과를 검증할 필요가 있고, <strong>내재 보상과 탐험 보너스를 효과적으로 결합</strong>하는 최적의 방법론에 대한 추가 연구가 필요합니다. 즉, 내재적 탐험 신호를 너무 강하게 주면 본래 목표에서 벗어날 수 있고, 너무 약하면 의미가 없기에 그 균형을 찾는 것이 과제로 남습니다.</p>
<p>또 하나 흥미로운 접근은 <strong>RUNE (Reward Uncertainty for Exploration)</strong>이라는 방법으로, <strong>선호기반 RL</strong>에서 <strong>보상 함수의 불확실성</strong>을 탐험 보너스로 활용한 것입니다. 사람의 피드백을 통해 학습되는 보상 모델(ensemble 여러 개)을 만들고, 그 예측 값들의 <strong>표준편차(불확실성)</strong>를 내재적 보상으로 주어봅니다. 이렇게 하면 에이전트는 <strong>여러 보상 모델 간 예측이 불확실한 상태</strong>를 더 탐험하게 되고, 이는 곧 인간 선호에 대해 명확한 정보를 주는 상태를 찾아다니는 것과 같습니다. RUNE의 장점은 <strong>탐험과 활용을 체계적으로 조율</strong>해준다는 점입니다. 즉, 불확실성이 큰 행동을 우선 시도함으로써, 장기적으로 더 많은 보상을 얻을 가능성을 높입니다. 실제 로봇 조작 과제에서 RUNE을 기존 탐험 기법들과 비교한 결과, <strong>학습 샘플 효율 및 최종 성공률을 향상</strong>시키는 것으로 나타났습니다. 특히 RUNE 적용 시 <strong>성공률 상승 폭이 다른 기법보다 크고</strong> 적은 피드백으로도 높은 성능을 달성했습니다. RUNE의 도전 과제는 보상 모델의 앙상블을 유지해야 하므로 <strong>계산 비용</strong>이 증가하고, 다양한 수준의 보상 불확실성을 다룰 수 있는 <strong>안정적인 기법</strong>이 더 연구되어야 한다는 점입니다. 그럼에도, 인간의 선호 학습 분야에서 <strong>보상 불확실성을 탐험에 통합</strong>한 첫 사례로서 RL 탐색 전략의 새로운 가능성을 열었다는 의의가 있습니다.</p>
</section>
<section id="정책-파라미터화-단순-정책-모델의-활용" class="level3">
<h3 class="anchored" data-anchor-id="정책-파라미터화-단순-정책-모델의-활용">정책 파라미터화 (단순 정책 모델의 활용)</h3>
<p><strong>정책 파라미터화</strong>는 에이전트의 정책 구조를 어떻게 설계하느냐에 관한 기법으로, 간접적으로는 보상에 의존하는 학습 성능과 탐색 행태에도 영향을 미칩니다. 일반적으로 RL에서 정책은 뉴럴네트워크 등 복잡한 함수로 표현되지만, 최근 한 연구에서는 <strong>단순한 선형 또는 RBF(Radial Basis Function) 정책</strong>도 충분한 성능을 낼 수 있음을 보여주었습니다. 핵심은 입력 상태에 대해 <strong>랜덤 Fourier 피쳐</strong>를 사용하여 표현력을 높이고 선형 정책에 적용한 것입니다. 예를 들어, 상태 관측에 대해 랜덤 주파수로 사인/코사인 변환한 피쳐 벡터를 만들고, 그에 가중치를 곱한 선형 정책을 학습하는 식입니다. 이렇게 간소화된 정책은 OpenAI Gym 연속 제어 벤치마크에서 <strong>뉴럴네트워크 정책에 필적하는 성능</strong>을 내면서도 학습 속도는 더 빠른 것으로 나타났습니다. 특히 신경망 없이도 TRPO로 훈련한 선형/RBF 정책이 일부 과제에서는 높은 보상을 얻었으며, 학습 시간도 단축되었습니다. 하지만 단순 정책은 환경 변화나 <strong>교란에 취약</strong>할 수 있어, 연구에서는 초기 상태 분포를 다양화하는 기법(도메인 랜덤화와 유사)을 써서 <strong>정책의 강건성</strong>을 높였습니다. 이렇게 학습하면 앙상블 모델이나 도메인 랜덤화처럼 정책이 다양한 상황에 노출되므로, 훨씬 안정적인 정책이 나온다고 합니다.</p>
<p>이 접근의 장점은 <strong>모델 단순화</strong>로 인한 <strong>학습 안정성</strong>과 <strong>속도 향상</strong>입니다. 복잡한 신경망 구조 대신 선형 또는 RBF 기반 정책을 쓰면 수렴이 빠르고 이해도 쉽습니다. 그러나 단점도 분명합니다. 우선 고차원 상태·행동 공간으로 확장하면 단순 정책으로는 충분한 표현력을 얻기 어려울 수 있습니다. 또한 초기 상태 분포에 민감하여, 학습 시 다양한 상황을 충분히 겪지 않으면 실전에서 성능 저하가 생길 수 있습니다. 복잡한 행동이 필요한 과제에서는 신경망 정책만큼 섬세한 대응이 어려워 <strong>성능 한계</strong>에 부딪힐 가능성도 있습니다. 결국 이 연구는 “가능한 한 정책을 단순화하되, 환경적 다양성을 주어 보완한다”는 철학으로 볼 수 있으며, 이는 보상 설계 측면에서 볼 때 <strong>단순 정책 + 특별한 보상 구성(랜덤화)</strong> 조합을 통해 학습을 안정화하는 하나의 전략이라 할 수 있습니다.</p>
</section>
<section id="역보상-설계-inverse-reward-design-ird" class="level3">
<h3 class="anchored" data-anchor-id="역보상-설계-inverse-reward-design-ird">역보상 설계 (Inverse Reward Design, IRD)</h3>
<p><strong>역보상 설계(IRD)</strong>는 직접 보상 함수를 공학적으로 설계하는 대신, <strong>에이전트의 시연이나 행동을 관찰하여 보상 함수(목적)를 역추정</strong>하는 접근입니다. 이는 강화학습의 역문제에 해당하는 개념으로, 일종의 <strong>역강화학습(IRL)</strong> 기법이라 볼 수 있습니다. 일반적인 RL에서는 에이전트가 상태 <span class="math inline">s</span>에서 행동 <span class="math inline">a</span>를 취하면 미리 정의된 보상 함수 <span class="math inline">R(s,a)</span>에 따라 보상을 받습니다. IRL에서는 반대로 <strong>전문가의 상태-행동 궤적</strong>이 주어졌을 때, 어떤 보상 함수를 가정하면 그 행동이 최적화될지를 학습하는 것입니다. IRD는 이 IRL의 아이디어를 보상 설계에 응용한 것으로, 기존에 <strong>인간이 설계한 보상 함수(프락시 보상)</strong>를 관찰 가능한 신호로 보고 <strong>진짜 의도한 보상 함수</strong>를 추정하려 합니다.</p>
<p>Hadfield-Menell 등의 연구에서 제안된 IRD 방법은, 설계자가 정한 보상 함수를 <em>전문가의 행동을 야기한 신호</em>로 취급합니다. 그리고 에이전트에게는 “이 보상은 사실 완벽하지 않을 수 있으니, 위험한 행동을 피하도록 보상 함수를 재해석하라”는 식으로 학습시킵니다. 예를 들어, 로봇이 용암 지대가 있는 환경에서 이동한다고 할 때, 설계자는 “목적지에 가면 +100”이라는 보상만 주었을 수 있습니다. 이 경우 로봇은 용암을 가로질러 빠르게 가려다가 파괴될 위험이 있습니다. IRD를 적용하면 로봇은 <strong>주어진 보상(목적지 도달)</strong>이 진짜 목표의 완전한 표현이 아니라고 간주하고, 전문가(인간)가 그런 상황에서 어떻게 행동했는지를 바탕으로 <strong>암묵적인 위험 회피 보상</strong>을 추론합니다. 실제 IRD 실험에서, 로봇은 용암 지역을 <strong>회피하는 경로</strong>를 선택하여 목표를 달성함으로써, 주어진 보상을 그대로 최적화했을 때 발생하는 위험 행동을 피했습니다. 이는 IRD가 보상 설계자가 미처 고려하지 못한 <strong>잠재적 위험 요소</strong>를 에이전트 스스로 인지하게 만들 수 있음을 보여줍니다. 또 하나 IRD의 효과는 <strong>보상 조작(reward manipulation)</strong> 문제를 줄이는 것입니다. 에이전트가 보상 함수를 절대적인 목표로 받아들이지 않고, 그것을 발생시킨 의도를 추론하기 때문에, 보상만 높이고 실제 목표는 성취하지 못하는 행동(보상 해킹)을 억제할 수 있습니다.</p>
<p>IRD의 한계로는 계산적 복잡성과 가정의 단순함이 있습니다. IRD는 결국 IRL 문제를 푸는 것이므로 <strong>추론 단계에서 복잡한 최적화</strong>가 필요하고, 환경에 대한 단순한 가정을 놓는 경우가 많습니다. 예컨대 보상 함수가 선형 조합으로 구성된다고 가정하거나, 환경 모델이 알려져 있다고 가정하는 등입니다. 현실의 복잡한 상황에서는 이러한 가정이 어긋나 IRD가 엉뚱한 보상을 추론할 위험도 있습니다. 따라서 IRD를 적용할 땐 환경과 보상에 대한 적절한 모델링이 필요하며, 잘못된 추론을 막기 위한 안전장치도 고려해야 합니다. 그럼에도 IRD는 <strong>보상 설계의 새로운 패러다임</strong>으로, 에이전트가 디자이너의 의도를 역으로 생각하게 함으로써 <strong>보상의 불완전성에 대응</strong>했다는 의의를 가집니다.</p>
</section>
<section id="보상-지평-단축-reward-horizon-shaping" class="level3">
<h3 class="anchored" data-anchor-id="보상-지평-단축-reward-horizon-shaping">보상 지평 단축 (Reward Horizon Shaping)</h3>
<p><strong>보상 지평(horizon)</strong>이란 에이전트가 현재 행동의 결과로 보상을 받기까지 걸리는 단계 수를 의미합니다. 만약 최종 보상까지 거쳐야 할 단계가 너무 많으면, 에이전트는 어떤 행동이 기여했는지를 알기 어려워 학습이 어려워집니다. <strong>Reward Shaping은 이러한 보상 지평을 줄여주는 역할</strong>도 합니다. 즉, 중간 단계마다 적절한 보상을 주어 <em>장기 목표를 여러 개의 단기 목표로 분해</em>하는 것입니다.</p>
<p>한 연구에서는 Reward Shaping을 통해 학습 시간에 대한 <strong>이론적 보장</strong>을 제공하기도 했습니다. 해당 연구의 알고리즘 (Horizon_Learn 알고리즘)은 보상 지평을 <span class="math inline">H</span>로 제한한 환경에서 에이전트가 학습할 때, 학습 시간이 <em>MDP의 전체 상태 수와 관계없이</em> <strong>임계 영역(critical region)</strong>의 크기에 다항식(poly)으로 비례함을 증명했습니다. 이는 Reward Shaping을 잘 하면, 에이전트는 커다란 상태공간을 전부 탐색하지 않고서도 중요한 부분만 배워도 된다는 의미입니다. 직관적으로, <strong>중간 목표에 보상</strong>을 주면 에이전트는 방대한 상태 공간 중에서 <strong>필요한 경로만 집중</strong>하게 되고, 결과적으로 학습 효율이 비약적으로 증가합니다. 예를 들어 미로 찾기에서 최종 출구에만 보상 주는 대신, 중간 체크포인트 몇 군데에도 보상을 주면 에이전트는 쓸데없는 경로를 적게 탐색하게 됩니다.</p>
<p>이런 개념을 공식화한 알고리즘에서는 에이전트가 아직 모르는 상태로 진입하면 임의 정책으로 탐험하되, <strong>이미 안다고 판정된 구간은 최적 정책을 따르는</strong> 식으로 진행합니다. 그리고 새로운 상태를 알게 될 때마다 그 구간의 최적 정책을 <strong>잠재적 보상</strong>으로 강화해줍니다. 그 결과, 에이전트는 <strong>MDP 전체를 탐색할 필요 없이</strong> 중요한 구간만 탐험해도 충분히 최적 정책에 도달하고, 학습 시간은 전체 상태 수가 커져도 크게 악화되지 않습니다. 이러한 이론은 Reward Shaping이 얼마나 학습 문제를 단순화할 수 있는지 보여주는 한 예이며, 실제 문제에도 <strong>적절한 형상(reward shaping)을 통해 효과적인 학습</strong>을 기대할 수 있음을 시사합니다.</p>
<p>보다 일반적으로, 보상 지평 단축은 우리가 직관적으로 흔히 사용하는 기법입니다. 먼 훗날 받을 보상보다는 <strong>가까운 시점의 성과</strong>에 보상을 주면, 에이전트는 목표까지 가는 여정에서 헤매는 일이 줄어듭니다. 다만 주의할 점은 <em>최적 정책의 불변성</em>입니다. Reward Shaping은 어디까지나 <strong>최종 최적해(optimal policy)를 바꾸지 않는 범위에서</strong> 이루어져야 합니다. 너무 강한 중간 보상을 주면 에이전트가 애초 목표를 망각하고 중간 목표에 집착해버릴 수 있습니다. 이를 방지하는 대표적인 이론적 도구가 <strong>잠재 기반 Reward Shaping(potential-based shaping)</strong>으로, 아래에서 자세히 설명합니다.</p>
</section>
<section id="잠재-기반-방법-potential-based-reward-shaping" class="level3">
<h3 class="anchored" data-anchor-id="잠재-기반-방법-potential-based-reward-shaping">잠재 기반 방법 (Potential-Based Reward Shaping)</h3>
<center>
<img src="../../images/2025-07-28-reward-engineering/1.png" width="70%">
</center>
<blockquote class="blockquote">
<p><em>강화학습의 외재적 보상과 내재적 보상 개념. 좌: 에이전트는 환경으로부터 보상을 받는다. 우: 에이전트 내부에 “내부 환경” 또는 비유적으로 <strong>생물체</strong>와 같은 모듈이 있어, 여기서 <strong>내재적 보상</strong>이 발생하여 함께 활용된다.</em></p>
</blockquote>
<p><strong>잠재 기반 Reward Shaping</strong>은 이론적으로 가장 잘 정립된 Reward Shaping 기법 중 하나로, Ng 등의 고전적인 연구에서 처음 제안되었습니다. 잠재 함수 <span class="math inline">\Phi(s)</span>란 상태 <span class="math inline">s</span>에 대해 정의되는 실수 값 함수로, 현재 상태와 다음 상태의 <strong>잠재값 차이</strong>를 추가 보상으로 주는 방식입니다. 수식으로 표현하면 shaped 보상 <span class="math inline">R'(s,a,s') = R(s,a) + \gamma \Phi(s') - \Phi(s)</span> 로, 여기서 <span class="math inline">\gamma</span>는 할인율입니다. 이 방식으로 보상을 변경하면 <em>잠재 함수의 그라디언트 부분이 일종의 가이드 역할을 하지만, 결과적으로 최적 정책은 변하지 않는</em>다는 <strong>정책 불변성 이론</strong>이 증명되었습니다. 따라서 잠재 기반 shaping은 이론적으로 안전하게 보상을 추가할 수 있는 방법입니다.</p>
<p>잠재 기반 방법의 직관적인 이해를 위해, <span class="math inline">\Phi(s)</span>를 <strong>휴리스틱 함수</strong>라고 볼 수 있습니다. 예를 들어 미로 문제에서 <span class="math inline">\Phi(s)</span>를 현재 상태에서 목표까지의 예상 거리의 <em>음수</em>로 설정하면, 에이전트는 한 걸음 움직여 목표에 가까워질 때마다 잠재값 차이만큼 보상을 얻게 됩니다. 이 보너스는 최종 목표 도달이라는 <strong>본래 보상과 방향은 일치</strong>하지만, 중간 단계에서 방향성을 제시해주므로 탐색을 유도합니다. 실제로 로봇 팔 작업에서도, 목표물과의 음수 거리 잠재함수를 쓰면 로봇이 <strong>목표물에 일단 가까이 가려고</strong> 노력하게 만들 수 있습니다.</p>
<p>이 리뷰 논문에서는 다양한 잠재 기반 방법의 발전형들을 소개합니다. 우선 <strong>기본 개념</strong>으로, 잠재 기반 shaping은 <strong>에피소드 전체 보상</strong>을 분석해 <strong>누적 보상을 강화하거나 벌점을 주는</strong> 형태로도 쓰일 수 있습니다. 한 연구에서는 게임 환경에서 에피소드 당 받은 총 보상을 추적하여, <strong>현재 에피소드 성과가 최저치보다 높으면 보상 증가, 최고치에 가까워지면 강화</strong>하는 동적 잠재 함수를 제안했습니다. 예컨대 식 (13)에서, <span class="math inline">F_{\text{min}}, F_{\text{max}}</span>는 지금까지 달성한 최악/최고 에피소드 점수인데, 현 에피소드 보상이 과거 최고치에 가까워질수록 추가 보상을 줌으로써 <strong>긍정적 행동을 강화</strong>합니다. 이 방법은 Pong, Breakout 게임에서 학습 속도를 높이고 여러 과제를 동시에 학습할 때도 성능을 높여준 것으로 보고되었습니다.</p>
<p>잠재 기반 방법은 멀티에이전트 시스템(MAS)에도 적용되어 <strong>이론적 성질</strong>이 연구되었습니다. 잠재 보상을 다중 에이전트 상황에 도입하면, 본래 MDP가 아니라 <strong>스토캐스틱 게임</strong>(stochastic game)이 되는데, 이때도 <em>모든 에이전트에 같은 잠재 보상함수를 적용</em>하면 결과적으로 <strong>Q함수 초기화</strong>를 조정한 것과 동일하며, 게임의 <strong>내시 균형(Nash equilibrium)을 바꾸지 않는다</strong>는 것이 증명되었습니다. 이는 멀티에이전트에서도 정책 불변성이 유지됨을 보장해주는 중요한 이론입니다. 다만 잠재 보상을 주면 <strong>탐색 경로가 달라질 수 있어</strong>, 동일한 균형점이라도 다른 경로로 도달하거나 새로운 joint policy에 수렴할 수 있음이 보고되었습니다. 흥미롭게도, 적절한 잠재 보상을 주었을 때 에이전트들이 <strong>더 높은 글로벌 효용</strong>을 내는 균형에 도달할 확률이 높아지고, 수렴 시간은 줄어드는 경향이 관찰되었습니다. 이는 멀티에이전트에서도 잠재 기반 shaping이 <strong>탐색을 건전하게 유도</strong>하여 보다 좋은 협력 결과를 끌어낼 수 있음을 시사합니다.</p>
<p>잠재 기반 shaping은 <strong>계층형 강화학습(HRL)</strong>에도 활용되었습니다. 예를 들어 <strong>PBRS-MAXQ-0</strong>라는 알고리즘은, 기존 HRL 알고리즘인 MAXQ에 잠재 보상(PBRS)을 통합한 방법입니다. 계층 학습에서는 상위 목표와 하위 목표로 문제가 나뉘는데, 여기에 잠재 보상을 추가하여 <strong>서브태스크 간 연계</strong>를 부드럽게 만들었습니다. 이 알고리즘은 일부 이론 조건 하에서 <strong>추가 보상이 있어도 수렴 보장</strong>을 제공하며, 실험적으로도 적절한 휴리스틱(잠재함수)을 넣으면 표준 MAXQ-0보다 <strong>학습 속도가 빨라지고 성능이 향상</strong>됨을 보였습니다. 흥미로운 점은, 심지어 휴리스틱이 잘못되어 <em>(misleading heuristic)</em> 에이전트를 오도하더라도 최종적으로는 표준 알고리즘 수준의 성능까지 따라온다는 것입니다. 이는 PBRS-MAXQ-0이 어느 정도 <strong>잘못된 잠재 보상에도 강인</strong>하여, 결국 충분히 학습 시간을 주면 극복한다는 의미입니다. 물론 현실적으로는 잘못된 잠재 보상을 쓰면 중간 학습 과정에서 시간 낭비가 발생하므로, 휴리스틱의 품질을 높이는 것이 중요합니다. PBRS-MAXQ-0의 과제로는 <strong>휴리스틱 값을 어떻게 최적화할 것인지</strong>와, 다양한 환경에서 일관되게 성능을 내는지 등이 꼽힙니다.</p>
<p><strong>Potential-Based Advice</strong>라는 이론 프레임워크도 소개되었는데, 이는 임의의 복잡한 보상 함수도 <strong>잠재 함수의 형태로 에이전트에게 주입</strong>할 수 있다는 개념입니다. 즉, 환경이 주는 본래 보상을 바꾸지 않더라도, 추가로 설계자가 원하는 임의의 보상 함수를 잠재적 추가 보상으로 제공하여 <strong>정책에는 영향 주지 않으면서 학습을 가속</strong>할 수 있다는 것입니다. 이 방법은 <strong>보조 가치함수(auxiliary value function)</strong>를 학습하여, 추가 보상이 마치 잠재 보상처럼 기능하도록 합니다. 이를 통해 설계자가 가진 도메인 지식을 행동 강화를 위해 녹여낼 수 있고, 이론적으로 시간과 메모리 복잡도도 선형으로만 증가하여 큰 부담 없이 구현 가능함을 보였습니다. 다만 현재까지는 이 개념이 주로 <strong>이론적 정식화</strong>로 머물러 있고, 이를 실제 알고리즘으로 구현한 예는 많지 않습니다. 저자들은 여러 Reward Shaping 접근법의 어려움을 비교하는데, 잠재 기반 방법은 <strong>이론적 보장</strong>은 있지만 효과적 적용을 위해 <strong>도메인 지식</strong>이 많이 필요하고, 보조 과제(auxiliary task) 기반 방법은 유연하지만 <strong>설계 복잡성</strong>이 증가하며, 내재적 동기 부여 방법은 <strong>파라미터 튜닝</strong>에 민감하다고 지적합니다. 따라서 실용적인 관점에서는 이론과 현실의 균형을 맞춰, 문제에 가장 적합한 shaping 접근을 택하는 것이 중요하다고 결론짓습니다. 향후에는 이러한 Reward Shaping 기법들을 <strong>보다 자동화</strong>하고 강인하게 만드는 연구가 필요하다고 제언합니다.</p>
<p>잠재 기반 shaping은 <strong>에피소드형 강화학습(episodic RL)</strong> 맥락에서도 재검토되었습니다. 한 연구는 에피소드 초·말의 잠재값이 균형에 미치는 영향 등을 분석하여, <strong>잠재 보상이 스토캐스틱 게임의 균형을 변경하거나 새로운 균형을 도입할 수 있음</strong>을 이론적으로 보였습니다. 특히 비-제로(non-zero) 터미널 잠재값은 새로운 균형을 만들 수 있어 주의해야 함을 지적했습니다. 또한 PAC-MDP 관점에서 잠재 보상이 <strong>탐험을 촉진</strong>함을 증명하여, 에피소드 시작·종료 상태가 구분되는 경우에도 잠재 기반 shaping이 유용함을 보여주었습니다. 요약하면, 잠재 함수는 에피소드 기반 문제에서도 학습 효율 향상에 기여하지만, 이론적으로 설계할 때 각별한 신경을 써야 할 요소들이 있음을 시사합니다.</p>
<p>또한 <strong>안전 제약이 있는 강화학습</strong>에도 잠재 보상 개념이 활용되었습니다. 예를 들어 논리 검증된 하이브리드 시스템 모델을 사용하여 <strong>안전 속성을 만족하는 보상 함수</strong>를 자동으로 생성하는 연구가 있습니다. 이 접근에서는 잠재 기반 보상 함수를 논리 제약과 함께 사용하여, <strong>제약을 어기지 않으면서</strong> 에이전트가 학습하도록 했습니다. 실제 차량 종방향 제어 환경에서, 논리 제약 기반 잠재 보상(PBRF)을 사용하니 학습 초기 수렴 속도가 빨라졌고, 안전한 동작을 하도록 유도되었습니다. 완전히 사람이 만든 보상과 비교해도 안전성 측면에서 거의 동등한 정책을 얻을 수 있었으며, 보상 스케일링을 조정하여 어떤 안전 측면을 더 강조할지도 통제할 수 있었습니다. 이는 <strong>형식 기법(formal methods)</strong>과 RL을 결합한 사례로, 잠재 보상을 통해 <strong>안전 영역으로의 유도</strong>를 효과적으로 한 것입니다.</p>
<p>마지막으로, 잠재 기반 방법의 파생으로 <strong>DRiP (Difference Reward incorporating Potential-based reward)과 CaP (Counterfactual as Potential)</strong>이라는 기법도 등장했습니다. 이는 뒤에서 설명할 <strong>차이 보상(Difference reward)</strong> 개념을 잠재 보상과 결합한 것으로, 멀티에이전트에서 각 에이전트가 시스템 성능에 기여한 바를 <strong>잠재 함수 형태</strong>로 보상해주는 방법입니다. DRiP는 도메인 지식이 담긴 잠재 함수를 설계해야 하지만, <strong>학습 가속과 균형 유지</strong> 측면에서 효과적이고, CaP는 <strong>동적으로 잠재 함수를 생성</strong>하여 사람의 개입을 줄인 방법입니다. 두 방법 모두 유사한 지식을 활용하지만, 합쳐 쓸 때 시너지는 없고 오히려 복잡도만 증가하므로 별개로 쓰는 것이 좋다고 합니다. <strong>이론 보장</strong>이 필요하면 CaP를, <strong>최대한 빠른 성능 향상</strong>이 목표면 DRiP을 권장하고 있으며, DRiP은 특히 <strong>도메인 지식이 충분할 때</strong> 강력한 성능을 보였습니다. 다양한 실험 도메인에서 DRiP은 일관되게 기존 방법보다 빠른 학습과 높은 정책 성능을 달성해, 멀티에이전트 shaping 기법의 발전 가능성을 보여줍니다.</p>
</section>
<section id="동적-잠재-기반-reward-shaping-dpbrs" class="level3">
<h3 class="anchored" data-anchor-id="동적-잠재-기반-reward-shaping-dpbrs">동적 잠재 기반 Reward Shaping (DPBRS)</h3>
<p><strong>DPBRS (Dynamic Potential-Based Reward Shaping)</strong>는 잠재 함수를 <strong>시간에 따라 변화</strong>시키는 발전된 기법입니다. 기본 잠재 기반 방법에서는 <span class="math inline">\Phi(s)</span>가 고정된 함수이지만, DPBRS는 현재까지의 학습 상황이나 에이전트의 상태에 따라 잠재 값을 동적으로 조정합니다. 논문에서는 Q-러닝의 업데이트 식에 시간을 나타내는 매개변수를 추가하여 개념을 정의했습니다. 아이디어는 식 (16)처럼, 잠재 함수에 상태 도달 시각 <span class="math inline">t</span>를 반영하여 <span class="math inline">\Phi(s, t)</span>로 확장하는 것입니다. 예컨대, 어떤 상태를 빨리 도달하면 높은 잠재 보상을 주고 늦게 도달하면 줄이는 식으로, <strong>시간 요소를 고려한 보상</strong>을 줄 수 있습니다. 이렇게 하면 에이전트가 <strong>더 신속히 목표에 도달</strong>하도록 유도하거나, 학습 진행에 따라 보상 전략을 조정할 수 있습니다.</p>
<p>DPBRS의 장점은 기존 잠재 방법의 <strong>이론적 보장</strong>(정책 불변성 등)을 유지하면서도, 상황에 맞게 shaping을 조절할 수 있다는 점입니다. 연구에서는 DPBRS를 싱글 에이전트 2D 미로와 멀티에이전트 게임에 테스트하여, <strong>더 빠른 수렴과 협동 향상</strong>을 확인했습니다. 예를 들어 다중 에이전트의 경우, 시간에 따라 잠재 보상을 변동시켜 <strong>초기에 협동을 유도</strong>하고 나중에는 스스로 학습하도록 하는 식의 적용도 가능할 것입니다.</p>
<p>하지만 이 방법은 아직 널리 시험되지 못했습니다. 논문에서도 <strong>다른 알고리즘과의 직접 비교 부족</strong>, <strong>로보틱스나 복잡한 비선형 시스템에의 응용 미비</strong> 등을 한계로 들었습니다. 제시된 예시는 2D 미로와 간단한 게임 정도였고, 실제 로봇 제어나 고차원 문제에서는 효과가 검증되지 않았습니다. 따라서 DPBRS는 <strong>개념 증명 단계</strong>로 볼 수 있으며, 향후 다양한 환경에서 성능을 확인하고 다른 shaping 방법과도 비교 연구가 필요합니다. 그럼에도, 시간 가변적 보상이라는 개념은 중요하여, 향후 <strong>적응형 보상 설계</strong>의 한 방향으로 발전할 것으로 기대됩니다.</p>
</section>
<section id="상한-신뢰-기반-가치-반복-ucbvi-기반-보상-설계" class="level3">
<h3 class="anchored" data-anchor-id="상한-신뢰-기반-가치-반복-ucbvi-기반-보상-설계">상한 신뢰 기반 가치 반복 (UCBVI 기반 보상 설계)</h3>
<p><strong>UCBVI (Upper Confidence Bound Value Iteration)</strong>는 샘플 효율 향상을 위해 이론적으로 제안된 알고리즘으로, <strong>Reward Shaping의 효과를 분석적으로 연구</strong>한 사례입니다. UCBVI 자체는 모델기반 탐색 방법으로, 탐색 보너스를 통해 가치 함수를 상향 편향(upper bound)시켜가며 최적 정책을 찾습니다. 이는 MBIE-EB 등 기존 탐색 알고리즘과 유사하지만 이론 분석에 편리한 형태로 설계되었습니다. 원 논문에서는 이 UCBVI에 <strong>Reward Shaping 개념을 도입</strong>하여, 탐색 보너스와 가치 함수 투영 등을 수정한 <em>UCBVI-shaped</em> 버전을 만들었습니다.</p>
<p>UCBVI-shaped의 분석에 따르면, Reward Shaping을 하면 에이전트가 <strong>탐색할 상태 공간을 효과적으로 줄일 수 있어</strong> <strong>후회(regret)</strong> 및 <strong>샘플 복잡도</strong> 면에서 유리해집니다. 이는 Reward Shaping이 <strong>문제와 관련 없는 부분을 가지치기(pruning)</strong> 하고, 에이전트가 <strong>정책 수립에 중요한 부분에 집중</strong>하게 해준다는 의미입니다. 결과적으로, UCBVI-shaped는 동일한 환경에서 기본 UCBVI보다 <strong>낮은 후회 한계와 적은 샘플로</strong> 비슷한 성능을 달성할 수 있음을 보였습니다. 특히 에이전트가 쓸데없이 환경의 불필요한 영역을 돌아다니는 일이 줄어들어, 최적 정책 발견 시간이 단축됩니다.</p>
<p>실험으로는 미로 환경에서 UCBVI-shaped와 일반 UCBVI를 비교했는데, <strong>irrelevant한 구역이 많은 환경일수록 shaping의 효과가 두드러졌다</strong>고 합니다. 이는 직관적이며, Reward Shaping이 없었다면 에이전트가 헤맸을 부분을 Reward Shaping으로 탐색하지 않도록 유도했기 때문입니다. 이러한 연구는 주로 이론 위주이지만, 중요한 점은 <strong>Reward Shaping을 공식적으로 샘플 복잡도 분석에 통합</strong>했다는 것입니다. 이는 이전까지 대부분 실험적으로 좋다는 Reward Shaping 기법들을, <strong>수학적 성능 보장 측면에서 조명</strong>한 것으로, 향후 Reward Shaping이 RL의 샘플 효율을 높이는 정당한 도구로 더욱 인정받을 수 있게 합니다.</p>
<p>요약하면, UCBVI 연구는 “Reward Shaping을 하면 이론적으로도 학습 효율이 좋아진다”는 것을 증명함으로써, <strong>향후 RL 분석에 Reward Shaping을 적극 고려</strong>해야 함을 시사했습니다. 실제로 RL 연구에서는 종종 보상은 문제 정의의 일부로 고정하고 탐색 알고리즘만 분석하곤 했지만, 이 결과는 <strong>보상 설계 자체도 알고리즘의 일부로 취급</strong>해야 함을 알려줍니다.</p>
</section>
<section id="차이-보상-difference-rewards" class="level3">
<h3 class="anchored" data-anchor-id="차이-보상-difference-rewards">차이 보상 (Difference Rewards)</h3>
<p><strong>차이 보상(Difference Reward)</strong>은 멀티에이전트 강화학습이나 집단 최적화 문제에서 고안된 기법으로, <strong>에이전트의 기여도</strong>에 따른 보상을 주는 방법입니다. 기본 아이디어는, <em>“개별 에이전트의 행동이 전체 시스템에 얼마나 이익을 가져왔는가”</em>를 보상으로 삼는 것입니다. 수식으로, 에이전트 <span class="math inline">i</span>의 차이 보상 <span class="math inline">D_i</span>는 보통 <strong>전체 시스템 보상에서 <span class="math inline">i</span>의 공헌을 뺀 나머지 부분을 뺀 값</strong>으로 정의됩니다. 예를 들어 멀티에이전트 시스템의 전역 보상 <span class="math inline">G(z)</span>가 상태 <span class="math inline">z</span>에서 주어진다면, <span class="math inline">i</span> 에이전트를 제외한 경우의 보상 <span class="math inline">G_{-i}(z)</span>를 가정하고 <span class="math inline">D_i = G(z) - G_{-i}(z)</span>로 보상을 구성하는 식입니다. 이렇게 하면 에이전트 <span class="math inline">i</span>는 자기 행동이 시스템 전체 성능에 미친 <strong>순 효과</strong>를 보상으로 받게 되어, <strong>개인 최적화가 곧 전체 최적화</strong>로 이어지도록 유도됩니다.</p>
<p>단일 에이전트의 경우에도 비슷한 개념을 적용할 수 있는데, 이때는 특정 기준 상태(또는 이상적 상태)와 현재 상태의 차이를 보상으로 삼습니다. 예컨대 <span class="math inline">r</span>이 원래 보상이고 <span class="math inline">\Phi(s)</span>가 현재 상태와 기준 상태의 차이를 재는 함수라면, 수정된 보상을 <span class="math inline">r' = r + \gamma (\Phi(s_{\text{ref}}) - \Phi(s))</span> 형태로 줄 수 있습니다. 이렇게 하면 에이전트는 기준 상태(목표 상태)에 가까워질수록 추가 보상을 받으므로 그 방향으로 행동합니다. 이때의 차이 함수 <span class="math inline">\Phi</span>는 잘 선택해야 효과가 있으며, 너무 단순하면 충분한 정보를 주지 못하고 너무 복잡하면 보상 함정이 될 수 있습니다.</p>
<p>차이 보상의 <strong>큰 장점</strong>은 개별 에이전트의 관점에서 <strong>“무엇을 하면 시스템에 이득이 되는가”</strong>를 명확히 제시해준다는 것입니다. 예를 들어 교통 경로 선택 문제에서 개별 운전자에게 전체 교통 흐름을 개선하는 방향으로 보상을 주면, 각 운전자는 이기적으로 행동하는 대신 <strong>교통 체증 완화</strong>에 기여하는 쪽을 선택하게 됩니다. 실제 연구에서 IQ-learning (개인 보상만 사용하는 Q-learning)과 DQ-learning (차이 보상 사용하는 Q-learning)을 비교한 결과, DQ-learning은 <strong>전체 여행시간을 크게 감소</strong>시켜 혼잡을 완화했고, 전통적인 교통할당 기법보다도 나은 결과를 얻었습니다. 통계적으로도 유의미한 개선을 보였으며, 이는 차이 보상이 <strong>사회적 최적해</strong>에 가까운 결과를 낳았음을 의미합니다. 물론 이 결과는 해당 교통 시뮬레이션 모델에 한정된 것이어서, 다른 도메인에도 일반화될지는 추가 연구가 필요합니다. 특히 운전자 같은 인간이 보상을 따른다고 가정하는 등 몇 가지 이상화된 조건이 있어 현실적 한계도 있지만, 차이 보상의 개념 자체는 여러 협력적 MAS 문제에 적용되고 있습니다.</p>
<p>멀티에이전트 상황에서의 차이 보상 정의는 앞의 식처럼 <em>counterfactual</em> 개념이 핵심입니다. 즉, <strong>“너 없었으면 어땠을지”</strong>를 계산하여 그 차이를 주는 것이죠. 이를 구현하려면 모든 에이전트의 공동 행동에 대한 전역 보상을 계산할 수 있어야 하고, 에이전트 <span class="math inline">i</span>를 제외한 가상의 상황 <span class="math inline">G_{-i}</span>를 추정할 수 있어야 합니다. 보통 이 <span class="math inline">G_{-i}</span>는 <span class="math inline">i</span>의 행동만 다른 동일한 상황이거나, <span class="math inline">i</span>가 기여하지 않은 상태로 가정한 모델 등을 사용합니다. 이러한 <strong>counterfactual 보상</strong>은 에이전트 간 <strong>신호 상호작용을 제거</strong>해주므로, 불필요한 행동 신호의 잡음을 줄이고 크고 작은 협력 문제 모두 잘 해결하도록 도와줍니다.</p>
<p>차이 보상의 단점은 <strong>전역 시스템 보상에 대한 가정</strong>이 필요하다는 점입니다. 시스템 전체의 상태나 보상을 에이전트가 알 수 있어야 하므로, <strong>완전 정보</strong> 또는 <strong>중앙집중식 평가</strong>가 가능한 경우에 적합합니다. 또한 시스템 효용 함수를 수식으로 알고 있어야 하고, 에이전트 제외 상황 <span class="math inline">G_{-i}</span>도 명시적으로 계산하거나 근사해야 하는데, 이 수식이 복잡할 경우 적용이 어려워질 수 있습니다. 반면 잠재 기반 보상(PBRS)은 도메인 전문가가 설계해야 하지만 개별 에이전트 수준에서 적용하기는 쉽다는 장단점 비교가 있습니다.</p>
<p>결국 차이 보상은 <strong>협력적 멀티에이전트 RL</strong>에서 유용한 도구이며, 시스템 전체 최적화를 촉진한다는 점에서 의미가 큽니다. 실제 연구에서는 차이 보상과 잠재 보상을 동시에 활용하거나 비교하여, 다중 목표 환경에서 <strong>Pareto 최적해</strong>로 유도하는 실험이 이루어졌습니다. 그 결과, 차이 보상과 잠재 보상 모두 Pareto해 달성에는 효과적이었지만 각각 요구조건이 달랐습니다. 차이 보상은 시스템 전역 정보를 알아야 하고, 잠재 보상은 수작업 설계가 필요하다는 제약이 있으므로, <strong>문제 상황에 따라 적합한 기법을 선택</strong>해야 한다고 결론내렸습니다. 일반적으로 시스템 모델을 잘 알고 통신이 원활하면 차이 보상이 유리하고, 그렇지 않으면 잠재 보상을 쓰되 적절한 휴리스틱을 마련하는 쪽이 나을 수 있습니다.</p>
</section>
<section id="지식-기반-다목적-다중에이전트-rl-momarl" class="level3">
<h3 class="anchored" data-anchor-id="지식-기반-다목적-다중에이전트-rl-momarl">지식 기반 다목적 다중에이전트 RL (MOMARL)</h3>
<p><strong>MOMARL (Knowledge-Based Multi-Objective Multi-Agent RL)</strong>은 말 그대로 <strong>다중 목표</strong>를 가진 <strong>다중 에이전트</strong> 환경에서의 Reward Shaping 기법들을 비교한 연구입니다. 이 연구에서는 하나의 벤치마크 문제(Multi-Objective Beach Problem Domain)와 전력망 경제 배치 문제에 차이 보상(D)과 잠재 보상(PBRS)을 적용해 보았습니다. 실험 결과, 두 방법 모두 에이전트를 <strong>Pareto 최적 영역</strong>으로 안내하는 데 도움을 주었지만, 각자의 한계도 드러났습니다.</p>
<p>차이 보상(D)은 앞서 언급했듯이 <strong>전역 지식</strong>이 필요합니다. 시스템 전체의 효용 함수를 알아야 하고 각 에이전트의 영향력을 구분할 수 있어야 합니다. 또한 목표 함수(평가 함수)가 수학적 형태로 명확히 주어져야 유효합니다. 만약 시스템 지식이 제한적이거나 계산 자원이 부족하면 차이 보상을 쓰기 어렵습니다.</p>
<p>잠재 보상(PBRS)은 <strong>잠재 함수 설계의 어려움</strong>이 있습니다. 사람의 전문 지식으로 잠재 함수를 만들어야 하는데, 다목적 문제에서는 어떤 잠재 함수를 써야 여러 목표를 균형 있게 달성할지 판단하기가 까다롭습니다. 또한 잘못 설계된 잠재 함수는 특정 목표만 치중하게 하거나 오히려 방해가 될 수 있습니다. 이처럼 PBRS는 <strong>시간과 노력이 많이 드는 수작업 설계</strong>가 필요하며, 모범적인 설계를 못 하면 효과가 제한적입니다.</p>
<p>연구에서는 일반적으로 <strong>차이 보상 D가 잠재 보상 PBRS보다 성능이 우수</strong>한 경향을 보인다고 보고했습니다. 특히 시스템 전체를 아우르는 지식이 사용 가능하고 통신 대역폭 문제가 없는 경우, 차이 보상이 더 직접적으로 목표 달성을 이끌어주기 때문입니다. 반면 에이전트 간 <strong>통신이 제한</strong>되거나, <strong>중앙 집중 평가가 어려운 경우</strong>에는 차이 보상을 적용하기 어려우므로, 이럴 땐 잠재 보상이 현실적인 대안이 됩니다.</p>
<p>이 연구의 결론은 결국 <strong>“주어진 다목적 다중에이전트 문제에 가장 적합한 Reward Shaping 기법을 선택해야 한다”</strong>는 것입니다. 시스템에 대한 지식 수준, 에이전트 간 정보 공유 가능성, 그리고 디자이너의 전문성 등을 종합적으로 고려해, 차이 보상이나 잠재 보상 중 하나 또는 둘 다를 선택적으로 사용하는 것이 바람직합니다. 또한 이들이 잘 동작하는지 비교할 수 있는 공개 <strong>벤치마크(MOBPD 등)</strong>를 제시하여, 향후 연구들이 쉽게 따라 실험하고 개선점을 모색할 수 있도록 했습니다. 요컨대, 멀티에이전트에서의 보상 설계는 단일 에이전트보다 복잡하지만, 적절한 접근법을 활용하면 <strong>개인 이익과 전체 이익을 조화</strong>시키는 방향으로 학습을 유도할 수 있습니다.</p>
</section>
<section id="계획-기반-방법-plan-based-reward-shaping" class="level3">
<h3 class="anchored" data-anchor-id="계획-기반-방법-plan-based-reward-shaping">계획 기반 방법 (Plan-Based Reward Shaping)</h3>
<p><strong>계획 기반 방법</strong>은 모델 기반 계획(planning) 기법과 강화학습을 결합하여 보상을 형성하는 접근입니다. 이 아이디어는 에이전트가 실제 행동을 하기 전에 <strong>“가상의 시나리오(what-if)”</strong>를 정신적으로 시뮬레이션해보고, 거기서 얻은 통찰을 보상에 반영하는 것과 유사합니다. 다시 말해, <strong>계획 알고리즘</strong>이 제시하는 최적 경로 정보를 이용해 RL의 탐색을 돕는 것입니다.</p>
<p>한 가지 방법은 <em>STRIPS</em>와 같은 전통적 AI 계획 표현을 이용한 Reward Shaping입니다. STRIPS는 상태 변화에 대한 전제조건과 효과를 정의하는 논리적 틀인데, 이를 사용해 목표 달성까지 <strong>필요한 행동 순서</strong>를 구합니다. 그런 다음 RL의 보상으로 <strong>이 계획 경로에 따른 행동에는 추가 보상</strong>을 주는 것입니다. 예를 들어, 일반 MDP 기반으로 학습할 때는 전체 상태공간을 탐색해야 하지만, STRIPS 기반 shaping을 도입하면 <strong>계획된 최적 경로 주변으로만 탐색을 집중</strong>하게 되어 학습이 효율화됩니다. 실제 연구에서, STRIPS 계획에서 얻은 경로의 상태들을 따라가면 보상을 주고, 그 외에는 주지 않는 shaping을 했더니 <strong>정책의 수렴 속도가 빨라지고 최종 성능도 향상</strong>되었습니다. 특히 계획 지식에 오류가 있어도, 에이전트는 RL을 통해 이를 보완하며 여전히 MDP 전체를 사용하는 shaping보다 나은 성능을 보였는데, 이는 <strong>계획 정보가 주는 가이드가 유용</strong>함을 나타냅니다.</p>
<p>또 다른 연구에서는 <strong>계획 기반 shaping vs 추상 MDP 기반 shaping</strong>을 비교했습니다. 추상 MDP 방법은 문제를 상위 레벨 MDP (간소화된 모델)로 풀고, 그 가치함수를 실제 학습에 shaping으로 쓰는 방식입니다. 계획 기반은 위와 같이 명시적 계획의 결과를 쓰고요. 비교 결과, <strong>대규모 복잡한 환경</strong>에서는 세부 단계까지 알려주는 <strong>계획 기반 보상</strong>이 더 효율적이어서 <strong>총 보상과 수렴 속도 모두 우수</strong>했습니다. 반면 <strong>다중 에이전트나 충돌하는 목표</strong>가 있는 상황에서는, <em>추상 MDP</em> 접근이 <strong>협조 문제</strong>를 더 잘 다루어 성능이 좋았습니다. 예컨대 여러 에이전트가 각자 계획을 따르면 충돌할 때, 추상 MDP는 상위 수준에서 조율된 목표를 주므로 충돌을 피하게 하는 식입니다. 이 결과는, <strong>환경 특성에 따라 어떤 shaping이 적합한지 다르다</strong>는 것을 보여줍니다. 계획 기반 보상은 각 단계에 세밀한 지도를 주기 때문에, 단일 에이전트의 복잡한 과제에 유리합니다. 그러나 다중 에이전트에서는 계획들이 상충할 수 있고, 이러한 조율은 추상적인 상위 목표를 통해 하는 편이 나을 수 있습니다.</p>
<p>요약하면, 계획 기반 방법은 <strong>모델 기반 AI의 지식</strong>을 활용하여 RL을 가이드한다는 점에서 의미가 큽니다. 실제로도 로보틱스 등에서, 먼저 <strong>시뮬레이터나 플래너로 행동 시퀀스를 짠 후</strong> RL이 미세 조정을 하는 방식이 많이 연구되고 있습니다. 이러한 방법은 보상 함수만으로 에이전트를 끌고 가기 힘든 복잡한 문제에서 <strong>인간 지식을 효과적으로 주입</strong>하는 수단이 됩니다. 물론 플래너가 필요하고, 계획 결과가 항상 최적이 아닐 수 있다는 한계는 있지만, RL과 planning의 <strong>하이브리드</strong>는 앞으로도 유망한 방향입니다.</p>
</section>
<section id="신념-기반-reward-shaping-belief-reward-shaping-brs" class="level3">
<h3 class="anchored" data-anchor-id="신념-기반-reward-shaping-belief-reward-shaping-brs">신념 기반 Reward Shaping (Belief Reward Shaping, BRS)</h3>
<p><strong>신념 기반 Reward Shaping(BRS)</strong>은 <strong>환경 보상 구조에 대한 사전 지식(prior)</strong>을 확률적 <em>신념(belief)</em> 형태로 활용하여 보상을 형성하는 접근입니다. 전통적인 RL은 환경과의 상호작용으로만 보상 구조를 파악하지만, BRS는 애초에 <strong>에이전트 내부에 보상 분포에 대한 가설</strong>을 넣어둡니다. 그리고 학습 중에 이 가설(내부 비평가, internal critic)이 환경의 실제 관측과 비교되면서 업데이트되고, 그 결과를 토대로 에이전트에게 <strong>형성 보상(shaping reward)</strong>을 추가로 제공합니다.</p>
<p>쉽게 말해, 에이전트는 <strong>“내가 생각한 보상 구조”</strong>를 하나 가지고 있고, 실제 받는 보상과의 차이를 보며 그 생각을 조정해나갑니다. 그리고 행동할 때는 <strong>외부 환경 보상 + 내 신념이 준 보상</strong>을 함께 고려합니다. 이 접근은 잠재 기반 보상과 달리 <strong>상태 뿐만 아니라 행동까지</strong> 신념 보상의 함수로 활용할 수 있습니다. 잠재 보상은 일반적으로 상태에 대해서만 정의되지만, BRS는 <strong>상태-행동 쌍</strong>에 대해 신념에 기반한 shaping 보상을 줍니다. 예를 들어, 어떤 행동은 위험할 것이라고 사전에 믿고 있다면, 그 행동에 페널티를 주는 식입니다 (이 페널티는 경험이 쌓이며 업데이트될 수 있음).</p>
<p>BRS에서 <strong>신념</strong>은 베이지안 방법으로 표현됩니다. 보상 분포에 대한 사전 분포를 두고, 환경 경험이 쌓일수록 posterior를 업데이트합니다. 가령 초기엔 “이 행동은 아마 -5 보상이 될 것이다”라고 믿었다가, 여러 번 해보니 -1 정도만 받는다면 내부 모델을 그에 맞게 수정합니다. 이처럼 <strong>에이전트 내부의 비평가(critic)</strong>가 지속적으로 자신의 보상 예측 모델을 개선하고, 에이전트는 그 비평가의 출력을 추가 보상으로 활용합니다.</p>
<p>BRS의 효과는 <strong>정확한 사전 지식</strong>이 있을수록 극대화됩니다. 논문에서는 복잡한 prior일수록 에이전트 성능이 향상됨을 보였습니다. 이는 당연히, 에이전트가 미리 많은 것을 알고 시작하면 시행착오를 줄이기 때문입니다. 또한 이론적으로 BRS를 Q-러닝과 결합했을 때 <strong>추론한 Q값이 일관성(consistency)을 갖는다</strong>는 보장도 제시되었습니다. 단, 이 보장은 <strong>진짜 환경의 보상 분포가 에이전트의 가정 모델 안에 있을 때</strong>만 성립합니다. 만약 현실이 에이전트의 모든 가정을 벗어난 형태라면, 잘못된 신념으로 인해 오히려 학습이 꼬일 수 있습니다.</p>
<p>BRS는 <strong>“환경 외부의 지식”</strong>을 활용한다는 점에서 흥미롭습니다. 개발자가 보상 구조에 대해 알고 있는 통계나 추세를 priors로 녹여낼 수 있습니다. 예를 들어 게임 설계를 하는 경우, 어떤 행동은 평균적으로 나쁘다고 가정하고 시작할 수 있습니다. 이렇게 하면 에이전트가 그걸 직접 학습하는 시간을 줄일 수 있죠. BRS는 또 <strong>잠재 기반 방식의 한계</strong>를 극복하려 했는데, 잠재 <span class="math inline">\Phi(s)</span>만으로는 표현 못 하는 요소 (행동에 따른 보상 차이 등)를 다룰 수 있게 했습니다. 즉, state뿐 아니라 action에도 의존하는 shaping이 가능해진 것입니다.</p>
<p>BRS의 한계는 <strong>복잡한 사전 분포</strong>를 다뤄야 하므로 계산량이 늘 수 있고, prior 설정이 잘못되면 오히려 편향을 심어줄 위험이 있습니다. 또한 BRS를 실제 큰 환경에 적용한 예시는 아직 많지 않습니다. 그러나 개념적으로, BRS는 <strong>내부 모델을 통한 보상 평가</strong>라는 측면에서, 인간의 가치 판단을 모방하려는 시도로 볼 수도 있습니다. 인간도 어떤 행동이 나쁠 것이라고 사회적/도덕적 신념을 가지고 피하듯이, 에이전트도 미리 배운 신념으로 탐색을 가이드받는 것입니다. 이 접근은 향후 <strong>인간 지식과 RL의 통합</strong>에 있어 중요한 아이디어가 될 수 있습니다.</p>
</section>
<section id="이층-최적화-기반-reward-shaping-bipars" class="level3">
<h3 class="anchored" data-anchor-id="이층-최적화-기반-reward-shaping-bipars">이층 최적화 기반 Reward Shaping (BiPaRS)</h3>
<p><strong>BiPaRS (bi-level optimization of parameterized reward shaping)</strong>는 <strong>Reward Shaping의 가중치</strong>를 학습하는 새로운 프레임워크입니다. 지금까지 Reward Shaping은 사람이 추가 보상의 형태와 세기를 결정하는 경우가 많았습니다. 하지만 BiPaRS는 <em>“어떤 shaping 보상이 이롭고 어떤 것은 해로운가”</em>를 스스로 학습하여, 좋은 shaping 신호만 활용하도록 합니다.</p>
<p>구체적으로, BiPaRS는 상위 단계와 하위 단계의 <strong>2계층 최적화</strong>로 구성됩니다. <strong>하위(level 2)</strong>에서는 주어진 shaping 보상 함수를 포함한 환경에서 정책을 학습합니다. <strong>상위(level 1)</strong>에서는 shaping 보상의 <strong>가중치 함수 <span class="math inline">\omega_\theta(s,a)</span></strong>를 학습합니다. 이 가중치 함수는 상태-행동별로 shaping 보상을 얼마나 반영할지 결정하며, <span class="math inline">\theta</span>라는 파라미터를 가집니다. 상위 단계는 목표가 에이전트의 <em>진짜 성능(원래 환경에서의 성과)</em>을 높이도록 <span class="math inline">\theta</span>를 조정하는 것입니다. 쉽게 말해, 상위 단계는 “어떤 shaping은 이득이니까 더 주고, 어떤 건 해로우니 빼자”를 결정합니다.</p>
<p>BiPaRS의 작동 방식을 방정식으로 나타내면 다음과 같습니다: 원래 보상 <span class="math inline">R(s,a)</span>에 shaping 보상 <span class="math inline">F(s,a)</span>를 합친 <span class="math inline">R'(s,a) = R(s,a) + \omega_\theta(s,a) F(s,a)</span>를 에이전트에게 줍니다. 여기서 <span class="math inline">\omega_\theta(s,a)</span>는 [0,1] 범위의 가중치 함수로, <span class="math inline">\omega</span>가 0이면 해당 shaping은 무시되고 1이면 온전히 반영됩니다. 상위 최적화는 <span class="math inline">\theta</span>를 조정하여, 하위 단계에서 학습된 정책이 <strong>원래 환경</strong>에서 누적 보상이 최대화되도록 합니다. 즉 상위 단계의 손실은 “원래 보상에 대한 성과”이고, 하위 단계는 “shaped 보상으로 정책 학습”입니다.</p>
<p>이렇게 함으로써 BiPaRS는 <strong>인간이 준 shaping 보상 중 정말 도움 되는 부분만 뽑아쓰는</strong> 능력을 갖게 됩니다. 실험 결과, Cartpole 등의 고전 제어와 MuJoCo 연속 제어에서 BiPaRS를 적용한 경우, <strong>일반 shaping을 사용한 것보다 안정적이고 좋은 성능</strong>을 보였습니다. 특히 Cartpole에서는 금방 성공하고, HalfCheetah 등에서도 shaping 덕을 보되, 잘못된 shaping 신호는 억제하는 모습을 확인했습니다. 다만 MuJoCo 등의 복잡한 영역에서는 아직 최첨단 성능에 미치진 못해, BiPaRS도 개선 여지가 있다고 언급됩니다.</p>
<p>BiPaRS의 의의는, <strong>Reward Shaping의 자동화된 튜닝</strong>이라는 점입니다. 기존에는 shaping 신호의 세기나 형태를 일일이 사람이 조정했지만, BiPaRS는 그것을 학습하는 또 다른 최적화 문제로 격상시켰습니다. 이를 통해 <strong>인간의 편향이나 시행착오를 줄이고</strong>, 에이전트 스스로 적합한 shaping 전략을 발견하도록 했습니다. 물론 이 접근은 계산 비용이 두 배로 들고(상위 최적화 포함), 안정적인 수렴을 위해 여러 기술적 조건이 필요하지만, 앞으로 <strong>AutoRL</strong>의 한 분야로서 주목할 가치가 있습니다. 궁극적으로 BiPaRS 같은 방법이 발전하면, 사용자는 shaping 보상 후보들만 정의해주고 <strong>에이전트가 최적 조합과 세기를 알아서 설정</strong>하는 시대가 올 수 있습니다.</p>
</section>
<section id="메타-보상-네트워크-mrn" class="level3">
<h3 class="anchored" data-anchor-id="메타-보상-네트워크-mrn">메타-보상 네트워크 (MRN)</h3>
<p><strong>MRN (Meta-Reward-Net)</strong>은 <strong>인간의 피드백</strong>을 제한적으로 이용하여 보상 함수와 정책을 함께 학습하는 프레임워크로, BiPaRS와 유사하게 <strong>이층 최적화</strong> 개념을 활용하지만 인간 선호에 중점을 둔 방법입니다. MRN은 <strong>선호기반 강화학습(preference-based RL)</strong>에서 발전한 아이디어로, 인간이 에이전트의 행동쌍을 비교하여 어느 쪽이 더 나은지 알려주는 피드백을 조금씩 제공하면, 이를 토대로 <strong>보상 모델</strong>과 <strong>정책</strong>을 동시에 학습합니다. 이때 보상 모델 학습과 정책 학습이 이층 구조로 묶여 있어, 보상 모델이 개선되면 정책이 바뀌고, 정책 경험이 늘면 보상 모델도 정교해지는 상호 메커니즘을 가집니다.</p>
<p>MRN의 독특한 점은, <strong>bi-level optimization을 통해 보상 함수 설계와 정책 최적화를 메타 학습</strong>처럼 취급했다는 것입니다. 이는 BiPaRS의 철학과 유사하지만, 인간 피드백이 추가된 것이 차이입니다. MRN에서는 에이전트가 받는 보상이 전적으로 환경으로부터 주어지는 것이 아니고, <strong>인간이 선호하는 방향으로 보상 함수 자체를 보정</strong>해 나갑니다. 이렇게 하면 희소/지연/노이즈 보상 문제도 인간의 직관으로 일부 해소할 수 있고, 에이전트가 중요한 것에 집중하도록 유도할 수 있습니다.</p>
<p>실험에서 MRN은 몇 가지 로봇 시뮬레이션 작업에 적용되어, <strong>한정된 인간 입력만으로도 기존 방법보다 우수한 성능</strong>을 보였습니다. 예를 들면, 로봇 팔 동작 시나리오에서 사람이 “이 동작이 저 동작보다 좋다”라는 비교 피드백을 조금 주면, MRN이 그 정보를 반영한 보상모델을 형성하고 곧바로 정책에 반영하여 <strong>샘플 효율적으로 학습</strong>했습니다. 최종적으로 MRN 기반 에이전트는 이전의 수작업 보상이나 단순 선호학습 알고리즘보다 높은 성공률을 보였습니다.</p>
<p>MRN의 장점을 요약하면: <strong>적은 인간 개입으로도 학습 효율을 높였고</strong>, Reward Shaping의 방향성을 사람에게서 얻음으로써 <strong>유연성과 안정성</strong>을 확보했다는 점입니다. 특히 논문에서는 기존 보상 shaping 기법들이 강건성과 효율 면에서 한계가 있다고 지적하며, MRN처럼 <strong>인간 가이드형 Reward Engineering</strong>이 필요하다고 강조합니다. 물론 MRN에도 단점은 있는데, 가장 큰 것은 <strong>인간의 피드백 품질에 의존</strong>한다는 것입니다. 사람이 잘못 선호를 표시하면 엉뚱한 보상 모델을 배울 수 있고, 또한 사람의 시간과 노력을 요구하기 때문에 대규모 학습에는 한계가 있습니다. 그럼에도 불구하고 MRN은 <strong>인간-에이전트 협력 학습</strong>의 좋은 사례로, 희소하거나 명시적 보상을 설계하기 어려운 문제에서 유용한 프레임워크가 될 수 있습니다.</p>
<p>추가로 MRN의 구현 코드도 공개되어 있어, 다른 연구자들이 이를 시도해볼 수 있게 했다고 합니다. 이는 앞으로 선호학습과 보상 설계 연구를 가속할 것으로 기대됩니다.</p>
</section>
<section id="기타-기법-및-동향" class="level3">
<h3 class="anchored" data-anchor-id="기타-기법-및-동향">기타 기법 및 동향</h3>
<p>이 밖에도 다양한 혁신적 Reward Shaping 프레임워크들이 등장하고 있습니다. 몇 가지 눈에 띄는 동향을 소개하면 다음과 같습니다:</p>
<ul>
<li><strong>배리어 함수(Barrier Function)를 이용한 안전형 RL</strong>: 보상 함수에 배리어 함수를 포함하여, 에이전트가 금지 구역이나 위험 행동을 하지 않도록 하는 방법입니다. 실제 로봇 팔 제어에서 배리어 함수를 보상에 넣었더니 <strong>수렴이 빨라지고 작동 에너지 소모가 줄어드는</strong> 등 효과가 있었습니다. 배리어 함수는 상태가 안전 영역 밖으로 나가려 하면 보상이 급감하도록 설계된 함수입니다.</li>
<li><strong>자연어 지시를 통한 보상 생성</strong>: 인간이 자연어로 설명한 목표나 규칙을 파싱하여, 그에 따라 <strong>dense reward</strong>를 만들어내는 기법도 연구되고 있습니다. 예를 들어 “붉은 물체를 녹색 지점으로 옮겨라”라는 문장을 읽고, 그 의미를 해석해 중간단계 보상을 생성하는 것입니다. 이를 통해 sparse reward 문제를 완화했으나, <strong>자연어 처리와 RL의 통합</strong>이라는 난제가 따릅니다.</li>
<li><strong>시간 논리(Temporal Logic)를 활용한 Reward Shaping</strong>: 형식 언어로 정의한 목표(예: LTL, CTL 등의 논리식)를 만족하면 보상을 주는 프레임워크가 시도되고 있습니다. 특히 평균-보상 RL(episodic이 아닌 지속환경)에서 시간 논리 제약을 보상으로 녹여 학습 속도를 높이는 연구가 있었습니다. 논리식을 만족하는 경로에는 보너스를 주어, 에이전트가 일정한 패턴의 행동을 익히도록 하는 방식입니다.</li>
<li><strong>과적합 방지형 보상 (Uncertainty-Penalized Reward)</strong>: 에이전트의 정책이 특정 상황에 과도하게 특화되는 것을 막기 위해, <strong>보상에 불확실성 페널티</strong>를 추가하는 접근도 있습니다. 모델 앙상블을 사용해 정책의 불확실성이 큰 상태에서는 보상을 깎음으로써, <strong>너무 위험하거나 확신 없는 전략</strong>을 피하게 합니다. 이는 <strong>RL의 오버피팅</strong> 문제를 완화하는 효과가 있습니다.</li>
<li><strong>기대 보상 기반 안정화</strong>: 에이전트가 미래의 기대 보상 분포까지 고려해 학습하는 방법으로, <strong>보상의 기댓값</strong>을 사용해 정책을 안정화하고 수렴을 빠르게 하는 시도도 있습니다. 이는 불확실한 환경에서 보상이 들쭉날쭉할 때, 평균적인 성과를 높이도록 shaping하는 것입니다.</li>
</ul>
<p>또한 <strong>보상 계획(Reward Planning)</strong>이라는 개념도 등장했는데, 상태공간을 쪼개어 서브태스크로 나눈 다음 <strong>탐욕적인 분할-정복</strong> 방식으로 보상을 설정하는 접근입니다. 한 연구에서는 Pendubot(이중 진자 로봇) 제어에서 이 방법을 썼는데, 진자의 <strong>스윙업과 밸런싱</strong>이라는 두 개의 서브태스크로 쪼개고, 각각에 대한 보상 함수를 체계적으로 설계했습니다. 그리고 전체 보상은 이 두 단계의 성취에 기반하여 주는 식으로 했더니, <strong>파라미터 불확실성이 200~300%나 되는 상황</strong>에서도 에이전트가 빠르게 적응하고 약 95%의 높은 성공률로 임무를 달성했습니다. 이 방법의 장점은, <strong>큰 문제를 쪼개서 각 부분에 최적 보상을 설계</strong>할 수 있으므로 불확실성이 크고 복잡한 시스템에도 적용 가능하다는 것입니다. 또한 특정 행동(예: 지나치게 에너지를 쓰는 행동)은 아예 피하도록 보상에서 제외하는 등 <strong>미세한 제어</strong>도 가능합니다. 단점으로는, 이렇게 보상 함수를 체계적으로 쪼개고 만드는 작업 자체가 <strong>시간이 많이 드는 수작업</strong>이라는 점입니다. 결국 설계자가 시스템에 대한 깊은 이해가 있어야 하고, 서브태스크로 나누는 과정도 문제에 특화되어 있어 일반화가 어렵습니다.</p>
<p>요약하면, 최신 Reward Shaping 기법들은 <strong>안전</strong>, <strong>자연어</strong>, <strong>논리</strong>, <strong>불확실성</strong>, <strong>태스크 분할</strong> 등 다양한 키워드를 중심으로 발전하고 있습니다. 이는 보상 설계가 더 이상 단순히 수식 하나 정하는 문제가 아니라, <strong>멀티모달 정보와 고차원 지식, 그리고 인간의 언어/논리적 지식까지 포괄하는 영역</strong>으로 확장되고 있음을 보여줍니다.</p>
</section>
</section>
<section id="로봇공학-및-기타-분야에서의-응용" class="level2">
<h2 class="anchored" data-anchor-id="로봇공학-및-기타-분야에서의-응용">로봇공학 및 기타 분야에서의 응용</h2>
<p>강화학습의 Reward Engineering/형성 기법들은 특히 <strong>로봇공학(Robotics)</strong> 분야에서 활발히 응용되고 있습니다. 로봇은 물리적 환경에서 움직이므로 잘못 학습하면 안전 문제가 생길 수 있고, 센서 노이즈 등 현실적인 어려움도 많습니다. 이에 따라 로봇 분야 연구자들은 <strong>보상 구조를 창의적으로 설계</strong>하여 로봇이 안전하고 효율적으로 학습하도록 시도하고 있습니다.</p>
<section id="인간-로봇-협업을-위한-안전-효율형-보상-설계" class="level3">
<h3 class="anchored" data-anchor-id="인간-로봇-협업을-위한-안전-효율형-보상-설계">인간-로봇 협업을 위한 안전 효율형 보상 설계</h3>
<p>산업 현장에서 사람과 로봇이 같이 일하려면, 로봇이 사람을 다치게 하지 않으면서도 작업 효율은 높여야 합니다. 한 연구에서는 이러한 <strong>인간-로봇 협업(HRC)</strong> 시나리오에서, 로봇이 <strong>충돌을 회피</strong>하면서도 업무를 잘 수행하도록 보상 함수를 설계했습니다. 구체적으로 <strong>IRDDPG (Intrinsic Reward DDPG)</strong> 알고리즘을 제안했는데, DDPG라는 Actor-Critic 강화학습 방법에 <strong>내재적 보상</strong>을 추가한 것입니다. 로봇 팔이 사람과 공동 작업할 때, <strong>외재적 보상</strong>은 작업 완료나 충돌 여부 등으로 주고, <strong>내재적 보상</strong>으로는 로봇 스스로 설정한 안전 거리 유지나 부드러운 움직임 등을 넣었습니다. 이렇게 복합 보상 함수를 사용하니, 실험 결과 로봇은 <strong>사람과 충돌하지 않으면서도 작업을 마치는 정책</strong>을 효과적으로 학습했습니다. 기존에 사람이 손수 만든 보상 함수보다도 학습이 빨랐고, 동적 상황에서도 적응력이 높았습니다. 즉, 학습 중에 보상을 조정함으로써 로봇이 <strong>안전과 효율 두 마리 토끼</strong>를 잡을 수 있었던 것입니다. 이 접근의 성과는, 일반적인 보상으로는 달성하기 어려운 안전 같은 목표를 내재적 보상으로 잘 녹여낸 점입니다. 다만 <strong>복잡한 보상 구조 최적화</strong>와 <strong>높은 연산량</strong>이 단점으로 지적되는데, 이는 내재+외재 보상 모두를 다뤄야 하고 DDPG 자체도 자원 소모적이기 때문입니다.</p>
</section>
<section id="대화형-피드백을-활용한-가정용-로봇-학습" class="level3">
<h3 class="anchored" data-anchor-id="대화형-피드백을-활용한-가정용-로봇-학습">대화형 피드백을 활용한 가정용 로봇 학습</h3>
<p>다른 시나리오로, 가정 내 서비스 로봇이 물건 정리 같은 작업을 배울 때 <strong>사람의 피드백</strong>을 활용하는 연구가 있었습니다. 로봇이 완전히 자율적으로 학습하면 느리고 시행착오가 많을 수 있으므로, <strong>사람 또는 보조 에이전트가 중간에 피드백</strong>을 주어 학습을 돕는 방법입니다. 이 연구에서는 세 가지 학습 모드를 비교했습니다:</p>
<ol type="1">
<li><strong>자율 RL</strong> (오로지 환경 보상으로 학습),</li>
<li><strong>에이전트-보조 대화형 RL</strong> (다른 AI 에이전트가 피드백 제공),</li>
<li><strong>인간-보조 대화형 RL</strong> (사람이 피드백 제공).</li>
</ol>
<p>실험적으로, 동일한 작업에 대해 대화형 피드백을 받은 경우가 <strong>학습 속도가 더 빠르고 오류가 적었으며, 누적 보상도 높았습니다</strong>. 특히 인간이 피드백을 준 경우 성능이 가장 좋았는데, 이는 사람의 전문 지식이나 직관이 유용했기 때문입니다. 예를 들어 로봇이 물건을 엉뚱한 자리에 둘 때 사람은 즉각 “그건 잘못됐다”라고 신호를 줄 수 있어, 로봇이 긴 탐색 끝에 실패하지 않고도 바로 교정할 수 있습니다. 이러한 <strong>인터랙티브 shaping</strong>의 장점은, 에이전트가 <strong>짧은 에피소드 안에 더 많은 학습</strong>을 한다는 것입니다. 즉, 탐험 공간을 사람의 도움으로 좁혀주거나, 보상 신호를 세밀하게 만들어주는 효과입니다. 연구에서는 인간 피드백이 약간 더 효과적이었지만, AI 에이전트의 피드백도 상당한 향상을 주어, <strong>자동화된 코치(agent coach)</strong> 개념도 가능성을 보였습니다. 물론 이러한 접근의 과제는, 사람이나 보조 에이전트의 피드백 전략을 <strong>어떻게 최적화</strong>할 것인가 하는 점입니다. 잘못된 조언을 주면 학습이 망가질 수 있으므로, 언제 개입하고 어느 정도 신호를 줄지 등을 체계화해야 합니다.</p>
</section>
<section id="현실-도메인-차이를-극복하는-보상-및-도메인-적응" class="level3">
<h3 class="anchored" data-anchor-id="현실-도메인-차이를-극복하는-보상-및-도메인-적응">현실 도메인 차이를 극복하는 보상 및 도메인 적응</h3>
<p>로봇 분야에서 흔히 겪는 문제로, <strong>시뮬레이션과 현실 환경의 차이(sim-to-real gap)</strong>가 있습니다. 시뮬레이터로 학습한 정책을 실제 로봇에 이식하면 성능이 급락하는 경우가 많죠. 이를 극복하기 위해 <strong>도메인 적응(domain adaptation)</strong>, <strong>도메인 랜덤화(domain randomization)</strong>, <strong>메타학습(meta-learning)</strong> 등의 기법이 사용되는데, 보상 설계도 이 과정에서 중요한 역할을 합니다.</p>
<center>
<img src="../../images/2025-07-28-reward-engineering/2.png" width="100%">
</center>
<blockquote class="blockquote">
<p>앞서 그림의 상단에서 묘사된 <strong>도메인 적응</strong>은, 시뮬레이터와 현실 간 차이를 줄이기 위해 <strong>공통 특성 공간</strong>을 학습하는 방법입니다. 예를 들어 시뮬레이터 영상과 실제 카메라 영상을 <strong>동일한 표현 공간</strong>으로 임베딩하여, 시뮬레이터에서 학습한 정책이 현실 입력에도 잘 적용되도록 합니다. 그림 하단 좌측의 <strong>도메인 랜덤화</strong>는, 시뮬레이터의 물리 파라미터나 렌더링을 무작위로 변화시켜 학습함으로써, <strong>현실의 다양한 조건에 견디는</strong> 정책을 얻는 기법입니다. 하단 우측의 <strong>메타 RL</strong>은, 다양한 환경들에 빠르게 적응할 수 있는 <strong>메타 정책</strong>을 학습하는 것으로, 이후 새로운 현실 환경에도 짧은 재학습(fine-tuning)으로 적응하도록 하는 방법입니다.</p>
</blockquote>
<p>보상 설계는 이러한 sim-to-real 전략들과 함께 쓰여, <strong>시뮬레이션 단계에서부터 현실 적용을 염두에 둔 보상</strong>을 형성하는 데 활용됩니다. 예를 들어, 한 연구에서는 <strong>CSAR (Consensus-based Sim-And-Real)</strong> 알고리즘을 제안하여 시뮬레이터와 현실에서 <strong>동일한 보상 구조</strong>로 병렬 학습을 진행했습니다. 여러 개의 시뮬레이터 속 가상 로봇과 하나의 실제 로봇을 동시에 학습시키면서, <strong>보상에 대한 합의(consensus)를 이루는 정책</strong>을 찾는 것입니다. 이렇게 하면 시뮬레이터의 빠른 학습과 현실 환경의 검증이 동시에 일어나, 현실 전이 시 성능 저하가 적고 훈련 시간도 줄어듭니다. 실험에서 시뮬레이터 로봇의 수를 늘릴수록 실제 로봇의 작업 성공률이 높아졌는데, 이는 <strong>시뮬레이터로 다양한 상황을 커버</strong>하면서도 보상 구조가 일관되게 유지되었기 때문입니다.</p>
<p>또 다른 연구에서는 <strong>반가상(semi-virtual) 환경</strong>을 중간 단계로 도입하여 2단계 sim-to-real 전이를 수행했습니다. 1단계에서 시뮬레이터+약간의 현실 요소 (예: 실제 로봇의 동역학, 그러나 센서는 가상)로 학습하고, 2단계에서 그 정책을 실제 환경(실제 센서+로봇)에서 fine-tuning 하는 방식입니다. 이때도 <strong>보상 함수는 두 단계에 걸쳐 일관되게</strong> 유지하여, 에이전트가 학습 목표를 바꾸지 않고 적응만 하도록 했습니다. 이러한 체계로 <strong>복잡성을 점진적으로 증가</strong>시키면, 한 번에 현실로 뛰어드는 것보다 안전하고 안정적으로 학습할 수 있습니다.</p>
<p>sim-to-real 전이에서 중요한 것은, <strong>보상 해킹</strong>이 현실에서도 통할 수 있다는 점입니다. 앞서 예로 든 청소 로봇이 센서를 가리는 경우처럼, 시뮬레이션에서 보상 해킹을 배웠다면 실제 로봇도 유사한 꼼수를 부릴 수 있습니다. 따라서 <strong>시뮬레이터 단계에서부터 올바른 보상 구조를 설계</strong>하여 해킹 가능성을 줄이고, 에이전트가 <strong>진짜 목표에 집중</strong>하도록 해야 합니다. 예를 들어 청소 로봇에게는 “방이 깨끗해져라” 보상만 주는 게 아니라, <strong>청소 행동 자체</strong>에도 약간의 보상을 줘서 적어도 행동은 하도록 유도할 수 있습니다. 또는 사람의 피드백을 시뮬레이터 단계에서 활용하여, 에이전트가 편법을 하면 즉시 교정받도록 할 수도 있습니다.</p>
<p>결국 <strong>Reward Shaping + 도메인 적응 + 메타러닝 + 인간피드백</strong> 등이 종합적으로 활용되어야, 시뮬레이션에서 현실로의 성공적인 지식 이전이 이루어집니다. 이들 접근은 각각 정책의 <strong>강건성</strong>을 높이고, <strong>효율성</strong>을 개선하며, <strong>안전성</strong>을 확보하는 역할을 합니다. 앞으로 시뮬레이터의 사실성이 높아지고, 자동화된 보상 설계 기법이 발전한다면, 현실에서 시행착오를 거의 겪지 않고도 시뮬레이션만으로 고성능 정책을 학습하는 것도 꿈이 아닙니다. 다만 그 과정에서 항상 <strong>올바른 보상 설정</strong>을 유지해야 하며, 에이전트가 잘못된 성공 기준을 추구하지 않도록 지속적인 모니터링이 필요합니다.</p>
</section>
</section>
<section id="reward-engineering의-장점과-한계-미래의-핵심인가" class="level2">
<h2 class="anchored" data-anchor-id="reward-engineering의-장점과-한계-미래의-핵심인가">Reward Engineering의 장점과 한계: 미래의 핵심인가?</h2>
<p>지금까지 살펴본 사례들이 보여주듯, <strong>Reward Shaping/Engineering</strong>은 RL 에이전트의 학습을 크게 향상시킬 수 있는 강력한 도구입니다. 정리해보면, 제대로 된 Reward Shaping은 다음과 같은 <strong>장점</strong>을 제공합니다:</p>
<ul>
<li><strong>학습 가속화</strong>: 추가 정보를 제공함으로써 에이전트가 최적 정책을 훨씬 빠르게 찾도록 도와줍니다. 복잡한 환경에서 수십만 단계가 걸리던 학습이 Reward Shaping으로 크게 단축될 수 있습니다. 예를 들어 STRIPS 기반 shaping이나 UCBVI-shaped, DRiP 등의 방법들은 기존보다 <strong>빠른 수렴</strong>을 보여주었습니다.</li>
<li><strong>탐험 향상</strong>: 특히 희소 보상 환경에서, shaping 보상은 에이전트에게 탐험 동기를 부여하여 더 활발하게 환경을 탐색하게 합니다. 이는 에이전트가 놓쳤을 가치 있는 상태들을 발견하게 해주며, 결과적으로 더 나은 정책을 찾게 됩니다. 여러 사례들(해시 탐색, RUNE 등)에서 shaping을 통해 <strong>baseline이 전혀 성공 못 하던 과제를 해결</strong>하기도 했습니다.</li>
<li><strong>최종 성능 개선</strong>: Reward Shaping을 적절히 하면 에이전트가 애초 shaping 없이는 도달 못했을 고성능 정책에 도달하기도 합니다. 최적 정책의 공간에 여러 후보가 있을 때, shaping은 그 중 <strong>더 우수한 정책</strong>을 선택하도록 유도할 수 있습니다. Section IV의 여러 방법들이 <strong>기존 방법 대비 성능 향상</strong>을 입증했습니다.</li>
<li><strong>정책 강건성 증가</strong>: 잘 설계된 shaping은 학습된 정책을 <strong>잡음, 환경 변화 등에 덜 취약</strong>하게 만들 수 있습니다. 예를 들어 혼동행렬로 노이즈를 보정한 보상이나, 도메인 랜덤화와 결합한 shaping 등은 에이전트가 다양한 상황에서도 무너지지 않도록 합니다. 또한 잠재 보상 등으로 특정 위험 행동을 피하도록 학습하면, 환경이 약간 달라져도 기본적인 안전은 지켜집니다.</li>
</ul>
<p>이러한 장점들 때문에, 많은 전문가들이 Reward Engineering을 <strong>미래 강화학습 성과의 열쇠</strong> 중 하나로 봅니다. 그러나 한편으로는 몇 가지 <strong>중대한 도전과 한계</strong>도 존재합니다:</p>
<ul>
<li><strong>도메인 지식 의존성</strong>: 많은 Reward Shaping 기법들이 해당 문제에 대한 전문가 지식에 의존합니다. 잠재 함수나 휴리스틱 보상, 계획 기반 보상 등은 사람이 일일이 설계해야 하고, 이는 복잡한 도메인일수록 어렵습니다. 처음 문제를 접하거나 전문 지식이 부족한 경우엔 어떤 shaping을 줘야 할지조차 모를 수 있습니다.</li>
<li><strong>추가 설계 시간과 노력</strong>: 보상 함수를 기본 환경 보상 외에 또 설계하는 것은 <strong>추가 작업 부담</strong>을 뜻합니다. 보상 함정에 빠지지 않도록 여러 번 시험하며 조정해야 할 수도 있고, 최적의 shaping 파라미터를 찾기 위해 많은 실험을 해야 할 수도 있습니다. 이는 프로젝트 일정이나 비용 측면에서 부담입니다.</li>
<li><strong>계산 복잡도 증가</strong>: 일부 shaping 방법은 알고리즘 구조를 복잡하게 만들어 <strong>계산 비용</strong>을 늘립니다. 예컨대 잠재 기반 방법에서 잠재값을 계산·업데이트하는 비용, BiPaRS처럼 이층 최적화를 수행하는 비용 등이 추가됩니다. 실시간 제어나 제한된 자원 환경에서는 이러한 오버헤드가 문제될 수 있습니다. (물론 반대로, shaping으로 필요 단계 수가 줄어들어 결과적 총비용은 감소하는 경우도 있습니다).</li>
<li><strong>파라미터 튜닝 문제</strong>: 많은 shaping에는 새로운 하이퍼파라미터들이 생깁니다 (보상 스케일, 혼합 비율 등). 이것들을 <strong>적절히 조율</strong>해야 효과를 보는데, 잘못 설정하면 학습이 망가질 수 있습니다. 다행히 BiPaRS같이 파라미터를 자율 튜닝하려는 시도도 있지만, 여전히 일반적이지는 않습니다. 사용자는 새로운 shaping 기법을 도입할 때 또다른 튜닝 과제를 떠안게 됩니다.</li>
<li><strong>설계 오류 가능성</strong>: 잘못된 Reward Shaping은 없느니만 못할 수 있습니다. 의도와 다르게 최적 정책을 바꿔버리거나, 보상 해킹을 유발하거나, 학습을 느리게 만들 수도 있습니다. 따라서 <strong>신중한 검증과 모니터링</strong>이 필요하며, 이는 실험적 노력을 요합니다.</li>
</ul>
<p>이러한 한계 때문에, Reward Shaping을 도입할 때는 <strong>문제 상황에 대한 면밀한 고려와 검토</strong>가 필요합니다. 그럼에도 불구하고, 잘만 활용하면 얻는 이익이 매우 크므로, 많은 RL 연구와 응용 분야에서 Reward Engineering은 포기할 수 없는 도구가 되고 있습니다.</p>
<p><strong>결론적으로</strong>, Reward Shaping은 강화학습 성능을 한 단계 끌어올리는 <strong>귀중한 기법</strong>입니다. 각 방법마다 장단이 있으므로, <strong>과제의 특성에 맞는 적절한 방법을 선택</strong>하는 것이 성공의 열쇠입니다. 또한 Reward Shaping 기법들을 서로 조합하거나 (예: 잠재 보상 + 차이 보상, 인간 피드백 + 도메인 랜덤화 등), 다른 AI 기술과 통합하여 (자연어, 논리 등) 활용하는 방향도 유망합니다. 앞으로 RL을 실제 복잡한 시스템에 적용하려면, 단순한 보상만으로 학습하는 것보다는 이런 <strong>정교한 보상 설계 전략</strong>들을 능숙하게 구사하는 능력이 요구될 것입니다.</p>
</section>
<section id="개방된-과제-및-향후-연구-방향" class="level2">
<h2 class="anchored" data-anchor-id="개방된-과제-및-향후-연구-방향">개방된 과제 및 향후 연구 방향</h2>
<p>Reward Engineering 및 Reward Shaping의 발전을 위해 앞으로 해결해야 할 열린 과제들도 많이 남아 있습니다. 우선, <strong>샘플 효율 향상</strong>은 지속적인 목표입니다. 현실에서 로봇이나 자율주행차를 학습시킬 때, 수많은 시도를 해볼 수 없으므로 <strong>적은 데이터로 학습하는 기법</strong>이 필수입니다. Reward Shaping은 그 해결책 중 하나로 거론되지만, 이를 더욱 개선하여 <strong>필요 최소한의 시도로도 최적 정책에 근접</strong>할 수 있게 하는 연구가 필요합니다. 예를 들어 BiPaRS나 MRN처럼 Reward Shaping 자체를 학습하는 방법, 혹은 메타러닝과 결합한 shaping으로 <strong>few-shot 학습</strong>을 가능케 하는 방향이 유망합니다.</p>
<p>또한 <strong>강건성(Robustness)</strong>은 여전히 중요한 이슈입니다. 앞에서 예를 든 노이즈 보상 처리나 안전형 보상 등이 그 방향이지만, 더욱 일반적인 의미에서, 환경 변화나 예측 불가능한 상황에서도 <strong>안정적 성능을 보장</strong>하는 알고리즘이 요구됩니다. 이를 위해서는 <strong>이론적 보장</strong>과 <strong>실험적 검증</strong>이 균형을 이뤄야 합니다. 예컨대 Osinenko 등의 연구처럼, 강화학습에 대한 안정성 보장을 제공하면서도 Reward Shaping으로 실제 성능을 높이는 접근이 필요합니다.</p>
<p><strong>인간-로봇 협업</strong>이나 <strong>인간 피드백 통합</strong> 역시 미래 핵심 주제입니다. Industry 4.0 시대에 인간과 AI/로봇이 함께 일하는 경우가 많아지고 있으므로, <strong>인간의 의도</strong>를 RL에 반영하는 방법이 중요합니다. 보상 설계 측면에선, <strong>휴먼 인 더 루프(human-in-the-loop)</strong> 학습을 자연스럽게 만들 기술이 요구됩니다. MRN이나 선호 학습 방식은 한 예지만, 더 나아가 <strong>실시간으로 사람과 상호작용</strong>하며 배우는 RL (예: 음성이나 제스처로 보상 신호를 주고받는 시스템)도 생각해볼 수 있습니다. 이는 기술적으로 난이도가 높지만, 성공한다면 RL의 활용 폭을 크게 넓힐 것입니다.</p>
<p><strong>고차원 감각 데이터</strong>(이미지, LiDAR 등)로 작동하는 RL에서의 보상 설계도 과제입니다. 이러한 경우 명시적 보상을 설계하기 매우 어려우며, 종종 <strong>end-to-end RL</strong>로 접근합니다. 예를 들어 Sergey Levine 등의 연구에서는 <strong>보상 설계 없이</strong> 로봇에 성공 데모만 몇 개 보여주고 SAC(Soft Actor-Critic) 알고리즘으로 학습시키는 방식을 취했습니다. 이를 통해 이미지 입력 기반 물체 조작에서 <strong>100%에 가까운 성공률</strong>을 달성했다고 보고합니다. 이러한 <strong>보상 없는(or 최소 보상) 학습</strong>은, 보상 설계의 수고를 덜어준다는 장점이 있습니다. 그러나 완전히 보상을 없앤 방식에도 <strong>한계</strong>가 있습니다. 예컨대 데모를 통해 학습한 정책이 항상 최적은 아니어서, 비효율적인 동작이 나올 수 있고, 환경 변화(물체 위치나 조명 변화 등)에 약하며, 사용자의 질의 또는 교정 과정이 필요하면 그 양이 많아지면 힘들다는 점이 지적됩니다. 다시 말해, <strong>명시적 보상을 제거하면 다른 형태의 어려움이 생길 수 있다</strong>는 것입니다.</p>
<p>따라서 이상적인 방향은, <strong>명시적 보상 설계와 암시적 시범/피드백 학습을 적절히 통합</strong>하는 것입니다. 예컨데, 기본적인 보상 구조는 간단히 설계하되, 세부적인 튜닝이나 예외 상황 처리는 데모나 인간 피드백으로 보완하는 식입니다. 혹은 반대로, 일단 데모로 학습한 후에 추가 Reward Shaping으로 성능을 끌어올리는 방법도 가능할 것입니다.</p>
<p>또 하나의 열린 과제는 <strong>자동화된 보상 설계(Automated Reward Design)</strong>입니다. 이는 앞서 다룬 BiPaRS나 meta reward learning 등이 포함되는 주제로서, <strong>강화학습 자체가 최적의 보상 함수를 찾도록 하는 것</strong>입니다. 이상적으로는, 사용자가 목표만 명시하면 AI가 스스로 적합한 보상 구조를 구성하고 학습까지 해주는 것이죠. 이를 위해선 메타러닝, 진화 알고리즘, 혹은 고차원 최적화 기법들이 활용될 수 있습니다. 현재는 이러한 자동화 기법들이 초기 단계이지만, 점차 발전하여 <strong>보상 설계의 부담을 크게 줄여줄 것</strong>으로 기대됩니다.</p>
<p><strong>실세계 검증</strong>도 중요한 미래 방향입니다. Reward Shaping 기법들은 주로 시뮬레이션이나 제한된 실험에서 효용을 보였지만, <strong>현실 세계의 복잡한 문제</strong>들(예: 자율주행 전체 시스템, 공장 자동화 라인, 의료 처치 전략 등)에 적용된 사례는 적습니다. 따라서 이러한 영역에 대한 <strong>파일럿 적용 연구</strong>를 통해, 실제로도 Reward Engineering이 성과를 내는지 평가해야 합니다. 현실 적용에서는 예기치 못한 문제(보상 해킹이 사회적으로 위험한 결과를 낳는다든지, 윤리적 문제 등)가 나올 수 있으므로, <strong>다학제적</strong> 관점에서 보상 설계의 영향을 살피는 것도 필요합니다.</p>
<p>마지막으로, <strong>인간 가치와의 정렬(Alignment)</strong> 문제를 들 수 있습니다. 강화학습 에이전트가 높은 보상을 쫓다 보면 인간이 원하지 않는 방향으로 행동할 수 있다는 우려가 있습니다. 이걸 방지하려면 애초에 <strong>인간의 가치관을 반영한 보상 함수</strong>가 중요합니다. 향후 연구에서는 윤리적 AI, AI 안전 분야와 연계하여, 보상 설계를 통해 AI의 행동을 <strong>사회적, 윤리적 기준과 부합</strong>하게 만드는 방법을 모색할 것입니다.</p>
<p>정리하면, Reward Engineering은 앞으로 <strong>더 똑똑한 RL</strong>을 위해 반드시 풀어야 할 숙제들과 가능성을 함께 지니고 있습니다. <strong>샘플 효율, 강건성, 인간 통합, 자동화, 실전 적용, 가치 정렬</strong> 등이 키워드이며, 각각 많은 연구 여지가 있습니다. 현 시점까지의 연구들을 기반으로, 이러한 방향으로 혁신이 이어진다면, Reward Engineering은 강화학습의 <strong>미래 핵심 기술</strong>로 자리매김할 것입니다.</p>
</section>
<section id="결론" class="level2">
<h2 class="anchored" data-anchor-id="결론">결론</h2>
<p>본 리뷰에서는 <strong>강화학습에서의 Reward Engineering과 Reward Shaping 기법</strong>들을 종합적으로 고찰하고 세부 내용을 분석하였습니다. 문헌조사 결과를 바탕으로, 다양한 방법론들을 체계적으로 분류하고 각 접근법의 메커니즘, 예시, 장단점을 살펴보았습니다. 또한 실제 로봇 공학 등 분야별 응용 사례를 통해 이러한 기법들의 효과와 도전과제를 논의하였고, 향후 연구 방향과 남은 과제에 대해서도 통찰을 제시했습니다.</p>
<p>중요한 결론 중 하나는, <strong>적절한 Reward Shaping은 강화학습의 학습 속도와 결과를 비약적으로 향상시킬 수 있다는 것</strong>입니다. 보상 shaping을 통해 학습을 가이드하면 에이전트는 더 빠르게 목표를 배우고, 불확실성이나 잡음이 있어도 견디며, 궁극적으로 더 나은 성능을 발휘할 수 있음을 다양한 연구들이 보여주었습니다. 특히 복잡한 현대 RL 과제 (예: 고차원 로봇 제어, 인간과의 상호작용 등)일수록 이러한 <strong>보상 설계 전략이 성공의 열쇠</strong>임을 확인했습니다.</p>
<p>동시에, <strong>Reward Engineering의 구현에는 신중함과 노력이 필요</strong>합니다. 잘못된 설계는 원치 않은 행동을 낳거나 학습을 망칠 수 있으므로, 도메인 지식과 실험적 튜닝을 적절히 활용해야 합니다. 최근에는 BiPaRS 같은 자동화 기법들이 나오고 있으나, 여전히 사람의 판단과 창의성이 중요한 부분입니다. 결국 <strong>문제에 가장 맞는 방법을 골라 쓰는 안목</strong>이 요구됩니다.</p>
<p>본 리뷰의 내용을 통해 얻을 수 있는 실용적 시사점은, 강화학습 연구자나 현업 엔지니어들이 새로운 과제를 접할 때 <strong>다양한 Reward Shaping 기법을 고려 목록에 넣어야 한다</strong>는 것입니다. 어떤 문제에는 잠재 보상이, 어떤 문제에는 차이 보상이, 또 다른 문제에는 인간 피드백이 어울릴 수 있습니다. 이 리뷰가 제시한 분류와 사례들은 그러한 판단에 가이드가 될 것입니다.</p>
<p>마지막으로, <strong>강화학습의 실제 적용에서 보상 설계는 이제 선택이 아닌 필수에 가까워지고 있습니다</strong>. 단순한 목표 함수를 정해두고 방치하기보다는, 학습이 잘 되도록 보상을 정교히 다듬어주는 것이 성패를 좌우합니다. 따라서 향후 연구에서는 Reward Shaping을 <strong>더 자동화하고 체계화하여</strong>, 많은 문제들에 쉽게 적용할 수 있게 하는 것이 중요합니다. 또한 인간의 협력을 받아들이는 방향으로 RL을 발전시켜, 안전하고 신뢰할 수 있는 AI를 만드는 데 기여해야 할 것입니다.</p>
<p>요약하면, <strong>Reward Engineering 및 Reward Shaping은 강화학습의 발전과 실제 응용에 있어 핵심적인 역할</strong>을 하고 있으며, 본 종합 검토를 통해 그 현황과 전망을 살펴보았습니다. 이 지식이 연구자와 실무자들에게 유용한 지침이 되어, 더욱 성공적인 RL 시스템 개발로 이어지길 기대합니다.</p>
<p><strong>+</strong></p>
<ul>
<li><a href="https://doi.org/10.1016/j.ifacol.2022.07.619">Reinforcement learning with guarantees: a review</a></li>
</ul>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<script src="https://utteranc.es/client.js" repo="curieuxjy/blog_comments" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Copyright 2024, Jung Yeon Lee</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/curieuxjy">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>