<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-07-16">
<meta name="description" content="Towards Sim-to-Real Dexterity with Adaptive Controller Learning">

<title>ğŸ“ƒDexCtrl ë¦¬ë·° â€“ Curieux.JY</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../profile.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-8ef56b68f8fa1e9d2ba328e99e439f80.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-ecf89aac047581c664da7ae53d704519.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-b009f778f5cec7f34f624408a2b5b543.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-ecf89aac047581c664da7ae53d704519.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-2NVZN2MJZT"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-2NVZN2MJZT', { 'anonymize_ip': true});
</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Curieux.JY</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../post.html"> 
<span class="menu-text">Post</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../note.html"> 
<span class="menu-text">Note</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">Jung Yeon Lee</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/curieuxjy"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#brief-review" id="toc-brief-review" class="nav-link active" data-scroll-target="#brief-review"><span class="header-section-number">1</span> Brief Review</a></li>
  <li><a href="#detail-review" id="toc-detail-review" class="nav-link" data-scroll-target="#detail-review"><span class="header-section-number">2</span> Detail Review</a>
  <ul class="collapse">
  <li><a href="#introduction-and-the-sim-to-real-challenge-in-dexterous-manipulation" id="toc-introduction-and-the-sim-to-real-challenge-in-dexterous-manipulation" class="nav-link" data-scroll-target="#introduction-and-the-sim-to-real-challenge-in-dexterous-manipulation"><span class="header-section-number">2.1</span> Introduction and the Sim-to-Real Challenge in Dexterous Manipulation</a></li>
  <li><a href="#key-idea-adaptive-controller-learning-with-dexctrl" id="toc-key-idea-adaptive-controller-learning-with-dexctrl" class="nav-link" data-scroll-target="#key-idea-adaptive-controller-learning-with-dexctrl"><span class="header-section-number">2.2</span> Key Idea: Adaptive Controller Learning with DexCtrl</a></li>
  <li><a href="#dexctrl-framework-and-methodology" id="toc-dexctrl-framework-and-methodology" class="nav-link" data-scroll-target="#dexctrl-framework-and-methodology"><span class="header-section-number">2.3</span> DexCtrl Framework and Methodology</a>
  <ul class="collapse">
  <li><a href="#how-dexctrl-works-overview-of-the-architecture" id="toc-how-dexctrl-works-overview-of-the-architecture" class="nav-link" data-scroll-target="#how-dexctrl-works-overview-of-the-architecture"><span class="header-section-number">2.3.1</span> How DexCtrl Works: Overview of the Architecture</a></li>
  <li><a href="#training-and-deployment-strategy" id="toc-training-and-deployment-strategy" class="nav-link" data-scroll-target="#training-and-deployment-strategy"><span class="header-section-number">2.3.2</span> Training and Deployment Strategy</a></li>
  </ul></li>
  <li><a href="#experimental-evaluation-and-results" id="toc-experimental-evaluation-and-results" class="nav-link" data-scroll-target="#experimental-evaluation-and-results"><span class="header-section-number">2.4</span> Experimental Evaluation and Results</a>
  <ul class="collapse">
  <li><a href="#baselines-for-comparison" id="toc-baselines-for-comparison" class="nav-link" data-scroll-target="#baselines-for-comparison"><span class="header-section-number">2.4.1</span> Baselines for Comparison</a></li>
  <li><a href="#simulation-results-performance-boost-even-without-a-gap-q1" id="toc-simulation-results-performance-boost-even-without-a-gap-q1" class="nav-link" data-scroll-target="#simulation-results-performance-boost-even-without-a-gap-q1"><span class="header-section-number">2.4.2</span> Simulation Results: Performance Boost Even Without a Gap (Q1)</a></li>
  <li><a href="#zero-shot-transfer-to-real-robots-narrowing-the-gap-q2" id="toc-zero-shot-transfer-to-real-robots-narrowing-the-gap-q2" class="nav-link" data-scroll-target="#zero-shot-transfer-to-real-robots-narrowing-the-gap-q2"><span class="header-section-number">2.4.3</span> Zero-Shot Transfer to Real Robots: Narrowing the Gap (Q2)</a></li>
  <li><a href="#robustness-to-object-variations-q3" id="toc-robustness-to-object-variations-q3" class="nav-link" data-scroll-target="#robustness-to-object-variations-q3"><span class="header-section-number">2.4.4</span> Robustness to Object Variations (Q3)</a></li>
  <li><a href="#understanding-the-learned-adaptive-strategies-q4" id="toc-understanding-the-learned-adaptive-strategies-q4" class="nav-link" data-scroll-target="#understanding-the-learned-adaptive-strategies-q4"><span class="header-section-number">2.4.5</span> Understanding the Learned Adaptive Strategies (Q4)</a></li>
  </ul></li>
  <li><a href="#effectiveness-limitations-and-outlook" id="toc-effectiveness-limitations-and-outlook" class="nav-link" data-scroll-target="#effectiveness-limitations-and-outlook"><span class="header-section-number">2.5</span> Effectiveness, Limitations, and Outlook</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">2.6</span> Conclusion</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-page-left" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">ğŸ“ƒDexCtrl ë¦¬ë·°</h1>
  <div class="quarto-categories">
    <div class="quarto-category">sim2real</div>
    <div class="quarto-category">adaptive</div>
    <div class="quarto-category">rl</div>
    <div class="quarto-category">hand</div>
  </div>
  </div>

<div>
  <div class="description">
    Towards Sim-to-Real Dexterity with Adaptive Controller Learning
  </div>
</div>


<div class="quarto-title-meta column-page-left">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">July 16, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<ul>
<li><a href="https://arxiv.org/pdf/2505.00991">Paper Link</a></li>
</ul>
<ol type="1">
<li>ğŸ¤– Dexterous manipulationì˜ sim-to-real transfer ë¬¸ì œëŠ” ì €ìˆ˜ì¤€ controller dynamic ë¶ˆì¼ì¹˜ë¡œ ì¸í•´ ë°œìƒí•˜ë©°, ê¸°ì¡´ ë°©ë²•ì€ manual tuningì´ë‚˜ randomizationì— ì˜ì¡´í–ˆìŠµë‹ˆë‹¤.</li>
<li>ğŸ§  ë³¸ ë…¼ë¬¸ì€ historical informationì„ í™œìš©í•˜ì—¬ actionê³¼ controller parametersë¥¼ ë™ì‹œì— í•™ìŠµí•˜ëŠ” adaptive controller í•™ìŠµ í”„ë ˆì„ì›Œí¬ì¸ DexCtrlì„ ì œì•ˆí•©ë‹ˆë‹¤.</li>
<li>ğŸš€ DexCtrlì€ ì‹¤í–‰ ì¤‘ ìë™ìœ¼ë¡œ control parametersë¥¼ ì¡°ì •í•˜ì—¬ sim-to-real gapì„ í¬ê²Œ ì¤„ì´ê³  contact-rich dexterous taskì—ì„œ ìš°ìˆ˜í•œ real-world ì„±ëŠ¥ì„ ë‹¬ì„±í•©ë‹ˆë‹¤.</li>
</ol>
<center>
<img src="../../images/2025-07-16-dexctrl/1.png" alt="Overview" width="100%">
<figcaption>
Overview
</figcaption>
</center>
<hr>
<section id="brief-review" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Brief Review</h1>
<p>DexCtrlì€ ì‹œë®¬ë ˆì´ì…˜(sim)ì—ì„œ í›ˆë ¨ëœ ë¡œë´‡ ì œì–´ ì •ì±…ì„ ì‹¤ì œ(real) í™˜ê²½ìœ¼ë¡œ ì´ì „í•  ë•Œ ë°œìƒí•˜ëŠ” ë‚œì œ, íŠ¹íˆ ì €ìˆ˜ì¤€(low-level) ì œì–´ê¸° ë™ì—­í•™ ë¶ˆì¼ì¹˜ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë° ì¤‘ì ì„ ë‘” ì—°êµ¬ì…ë‹ˆë‹¤. ê¸°ì¡´ ì ‘ê·¼ ë°©ì‹ë“¤ì€ ìˆ˜ë™ íŠœë‹(manual tuning)ì´ë‚˜ ì œì–´ê¸° ë¬´ì‘ìœ„í™”(controller randomization)ì— ì˜ì¡´í–ˆëŠ”ë°, ì´ëŠ” ë…¸ë™ ì§‘ì•½ì ì´ê³  íŠ¹ì • ì‘ì—…ì—ë§Œ ìœ íš¨í•˜ë©° í•™ìŠµ ë‚œì´ë„ë¥¼ ë†’ì´ëŠ” ë‹¨ì ì´ ìˆì—ˆìŠµë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì€ ì´ëŸ¬í•œ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ ë™ì‘(action)ê³¼ ì œì–´ ë§¤ê°œë³€ìˆ˜(controller parameter)ë¥¼ ë™ì‹œì— í•™ìŠµí•˜ëŠ” ìƒˆë¡œìš´ í”„ë ˆì„ì›Œí¬ì¸ DexCtrlì„ ì œì•ˆí•©ë‹ˆë‹¤.</p>
<p><strong>í•µì‹¬ ë¬¸ì œ ì‹ë³„:</strong></p>
<p>ì—°êµ¬ëŠ” ì‹œë®¬ë ˆì´ì…˜-ì‹¤ì œ ë¶ˆì¼ì¹˜(sim-to-real gap)ì˜ ì¤‘ìš”í•œ ì›ì¸ ì¤‘ í•˜ë‚˜ë¡œ ë¡œë´‡ ì œì–´ê¸° ê°„ì˜ ì°¨ì´ë¥¼ ì§€ëª©í•©ë‹ˆë‹¤. <strong>ë™ì¼í•œ ê¶¤ì (trajectory)ì´ë¼ë„ ì œì–´ ë§¤ê°œë³€ìˆ˜ê°€ ë‹¤ë¥´ë©´ ì‹¤ì œ ì ‘ì´‰ë ¥(contact force)ê³¼ ë™ì‘ì´ í¬ê²Œ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.</strong> ê¸°ì¡´ì˜ ë°©ì‹ì€ ì œì–´ ë§¤ê°œë³€ìˆ˜ë¥¼ ê³ ì •í•˜ê±°ë‚˜ í›ˆë ¨ ì‹œ ë¬´ì‘ìœ„í™”í•˜ì—¬ robustnessë¥¼ ë†’ì´ë ¤ í–ˆì§€ë§Œ, ì´ëŠ” ì‹¤ì§ˆì ì¸ ë¬¸ì œ í•´ê²°ì— í•œê³„ê°€ ìˆì—ˆìŠµë‹ˆë‹¤.</p>
<p><strong>DexCtrlì˜ ë°©ë²•ë¡ :</strong></p>
<p>DexCtrlì€ ê³¼ê±°ì˜ ê¶¤ì  ì •ë³´ì™€ ì œì–´ê¸° ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë™ì‘ê³¼ ì œì–´ ë§¤ê°œë³€ìˆ˜ë¥¼ ë™ì‹œì— ì˜ˆì¸¡í•˜ê³  ì ì‘ì ìœ¼ë¡œ ì¡°ì •í•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì •ì±…ì´ ì‹¤í–‰ ì¤‘ì— ì œì–´ ë§¤ê°œë³€ìˆ˜ë¥¼ ìë™ìœ¼ë¡œ íŠœë‹í•˜ì—¬ ì‹œë®¬ë ˆì´ì…˜-ì‹¤ì œ ê°„ê·¹ì„ ì™„í™”í•˜ê³ , ì ‘ì´‰ë ¥ ìƒí˜¸ì‘ìš©ì— ëŒ€í•œ ì¶”ë¡ ì„ ê°œì„ í•˜ì—¬ ì‹¤ì œ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ë¡œë²„ìŠ¤íŠ¸ë‹ˆìŠ¤ë¥¼ í–¥ìƒì‹œí‚µë‹ˆë‹¤.</p>
<p><strong>í•µì‹¬ ë°©ë²•ë¡  ì„¸ë¶€ ì‚¬í•­:</strong></p>
<ol type="1">
<li><strong>ë°ì´í„° ìˆ˜ì§‘ì„ ìœ„í•œ Oracle Policy:</strong>
<ul>
<li>DexCtrlì€ ë¨¼ì € ì‹œë®¬ë ˆì´ì…˜ í™˜ê²½ì—ì„œ ë‹¤ì–‘í•œ ë¬¼ì²´ ë¬¼ë¦¬ ë§¤ê°œë³€ìˆ˜(object physical parameters)ë¥¼ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨ëœ â€œì˜¤ë¼í´ ì •ì±…(oracle policy)â€ìœ¼ë¡œë¶€í„° ì¶©ë¶„í•œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.</li>
<li>ì˜¤ë¼í´ ì •ì±…ì€ ëª¨ë¸-í”„ë¦¬(model-free) ê°•í™” í•™ìŠµ(Reinforcement Learning) ê¸°ë²•ì¸ <strong>PPO(Proximal Policy Optimization)</strong>ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµë©ë‹ˆë‹¤.</li>
<li>ê° ì‹œê°„ ë‹¨ê³„ <span class="math inline">t</span>ì—ì„œ ì˜¤ë¼í´ ì •ì±… <span class="math inline">\pi(a_t, K_t | s_t)</span>ëŠ” í˜„ì¬ ìƒíƒœ <span class="math inline">s_t</span>ë¥¼ ì…ë ¥ë°›ì•„ ì¡°ì¸íŠ¸ ë™ì‘(joint action) <span class="math inline">a_t</span>ì™€ ì œì–´ ë§¤ê°œë³€ìˆ˜ <span class="math inline">K_t</span>ë¥¼ ë™ì‹œì— ì¶œë ¥í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œ <span class="math inline">a_t</span>ëŠ” <span class="math inline">t</span> ì‹œì ì˜ ì¡°ì¸íŠ¸ ìœ„ì¹˜ ë³€í™”ëŸ‰ì„ ë‚˜íƒ€ë‚´ë©°, ì›í•˜ëŠ” ì¡°ì¸íŠ¸ ê¶¤ì ì€ ì´ì „ desired joint positionì— <span class="math inline">a_t</span>ë¥¼ ë”í•˜ì—¬ <span class="math inline">q^d_t = q^d_{t-1} + a_t</span>ë¡œ ì–»ì–´ì§‘ë‹ˆë‹¤. desired joint velocityëŠ” 0ìœ¼ë¡œ ì„¤ì •ë©ë‹ˆë‹¤.</li>
<li>ë¡œë´‡ ì œì–´ëŠ” ì¡°ì¸íŠ¸ í† í¬ ì œì–´(joint torque control) ë°©ì‹ì„ ë”°ë¥´ë©°, í† í¬ <span class="math inline">\tau</span>ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°ë©ë‹ˆë‹¤: <span class="math display">\tau = K_P (q_d - q_c) + K_D (\dot{q}_d - \dot{q}_c)</span> ì—¬ê¸°ì„œ <span class="math inline">q_d</span>ì™€ <span class="math inline">q_c</span>ëŠ” ê°ê° ì›í•˜ëŠ”(desired) ì¡°ì¸íŠ¸ ìœ„ì¹˜ì™€ í˜„ì¬(current) ì¡°ì¸íŠ¸ ìœ„ì¹˜ë¥¼ ë‚˜íƒ€ë‚´ë©°, <span class="math inline">\dot{q}_d</span>ì™€ <span class="math inline">\dot{q}_c</span>ëŠ” ê°ê° ì›í•˜ëŠ” ì¡°ì¸íŠ¸ ì†ë„ì™€ í˜„ì¬ ì¡°ì¸íŠ¸ ì†ë„ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. <span class="math inline">K_P</span>ì™€ <span class="math inline">K_D</span>ëŠ” ê°•ì„±(stiffness) ë° ê°ì‡ (damping) í–‰ë ¬ì„ ë‚˜íƒ€ë‚´ëŠ” ì œì–´ ë§¤ê°œë³€ìˆ˜ì´ë©°, ê°„ë‹¨í•¨ì„ ìœ„í•´ ëŒ€ê° í–‰ë ¬(diagonal matrices)ë¡œ ê°€ì •í•©ë‹ˆë‹¤. <span class="math inline">K = \{K_P, K_D\}</span>ëŠ” ì „ì²´ ì œì–´ ë§¤ê°œë³€ìˆ˜ ì§‘í•©ì…ë‹ˆë‹¤.</li>
<li><strong>ìƒíƒœ(State) êµ¬ì„±:</strong> ìƒíƒœ <span class="math inline">s_t \in \mathbb{R}^{219}</span>ëŠ” ì§€ë‚œ ì„¸ ë‹¨ê³„ì˜ ë¬¼ì²´ ë° ë¡œë´‡ ê´€ì¸¡ ì •ë³´ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.
<ul>
<li>ë¡œë´‡ ì •ë³´ <span class="math inline">s^r_t \in \mathbb{R}^{64}</span>: í˜„ì¬ ì¡°ì¸íŠ¸ ìœ„ì¹˜ <span class="math inline">q^c_t</span>, ì›í•˜ëŠ” ì¡°ì¸íŠ¸ ìœ„ì¹˜ <span class="math inline">q^d_t</span>, ì œì–´ ë§¤ê°œë³€ìˆ˜ <span class="math inline">K_t</span>.</li>
<li>ë¬¼ì²´ ì •ë³´ <span class="math inline">s^{obj}_t \in \mathbb{R}^9</span>: ë¬¼ì²´ ìì„¸(pose) <span class="math inline">p^{obj}_t \in \mathbb{R}^6</span>, ë¬¼ì²´ ì†ì„± ë²¡í„° <span class="math inline">\mu \in \mathbb{R}^3</span> (ìŠ¤ì¼€ì¼, ì§ˆëŸ‰, ë§ˆì°°). <span class="math display">s_t \triangleq (s^r_{t-2:t}, s^{obj}_{t-2:t})</span> <span class="math display">s^r_t \triangleq (q^c_t, q^d_t, K_t)</span> <span class="math display">s^{obj}_t \triangleq (p^{obj}_t, \mu)</span></li>
</ul></li>
<li><strong>ë³´ìƒ(Reward) í•¨ìˆ˜:</strong> ë³´ìƒ <span class="math inline">r_t</span>ëŠ” ì£¼ë¡œ ë„¤ ê°€ì§€ ë¶€ë¶„ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤: <span class="math display">r_t = r_{rotation} + r_{contact} + r_{smoothness} + r_{terminate}</span></li>
</ul></li>
<li><strong>ë™ì‘ ì˜ˆì¸¡ ë° ì œì–´ ë§¤ê°œë³€ìˆ˜ ì˜ˆì¸¡ ëª¨ë“ˆ:</strong>
<ul>
<li>ì˜¤ë¼í´ ì •ì±…ì´ ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬, DexCtrlì€ í•™ìƒ ì •ì±…(student policy)ì„ ë‘ ê°œì˜ ë¶„ë¦¬ëœ ëª¨ë“ˆë¡œ í›ˆë ¨í•©ë‹ˆë‹¤: ë™ì‘ ì˜ˆì¸¡ ëª¨ë“ˆ(Action Prediction Module)ê³¼ ì œì–´ ë§¤ê°œë³€ìˆ˜ ì˜ˆì¸¡ ëª¨ë“ˆ(Control Parameters Prediction Module). ì´ëŠ” ê° ëª¨ë“ˆì´ íƒœìŠ¤í¬ì˜ ê·¼ë³¸ì ìœ¼ë¡œ ë‹¤ë¥¸ ì¸¡ë©´ì„ ì¸ì½”ë”©í•˜ë©°, ì œì–´ ë§¤ê°œë³€ìˆ˜ ì˜ˆì¸¡ì´ ë™ì‘ ì˜ˆì¸¡ì— ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•Šë„ë¡ í•˜ê¸° ìœ„í•¨ì…ë‹ˆë‹¤.</li>
<li><strong>Historical Information ì‚¬ìš©:</strong> ì˜¤ë¼í´ ì •ì±…ì€ ì‹¤ì œ í™˜ê²½ì—ì„œ ì§ì ‘ ì ‘ê·¼í•˜ê¸° ì–´ë ¤ìš´ ë¬¼ì²´ ì†ì„± ê°™ì€ ì›ì‹œ ì •ë³´(primitive information)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ DexCtrlì€ ë¡œë´‡ì˜ ê³ ìœ ìˆ˜ìš©ì„±(proprioception) ì´ë ¥ ìƒíƒœ(last ten stepsì˜ í˜„ì¬ ë° ì›í•˜ëŠ” ì¡°ì¸íŠ¸ ê¶¤ì , í•´ë‹¹ ì œì–´ ë§¤ê°œë³€ìˆ˜)ë¥¼ í•™ìƒ ì •ì±…ì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.</li>
<li><strong>ëª¨ë“ˆ ì„¤ê³„:</strong>
<ul>
<li><strong>ë™ì‘ ì˜ˆì¸¡ ëª¨ë“ˆ:</strong> temporal historical inputì„ ëª¨ë¸ë§í•˜ê¸° ìœ„í•´ self-attention ë©”ì»¤ë‹ˆì¦˜ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ëŠ” ì¡°ì¸íŠ¸ ê¶¤ì  ë³€í™”ì˜ ê²½í–¥(trend)ì„ íŒŒì•…í•©ë‹ˆë‹¤.</li>
<li><strong>ì œì–´ ë§¤ê°œë³€ìˆ˜ ì˜ˆì¸¡ ëª¨ë“ˆ:</strong> cross-attentionì„ ì‚¬ìš©í•˜ë©°, í˜„ì¬ ë™ì‘(current action)ì´ ì¿¼ë¦¬(query) ì—­í• ì„ í•˜ê³  historical inputê°€ í‚¤(key)ì™€ ê°’(value) ì—­í• ì„ í•©ë‹ˆë‹¤. ì´ëŠ” í˜„ì¬ ì¡°ì¸íŠ¸ ë™ì‘ê³¼ ì´ë ¥ ì •ë³´ ê°„ì˜ ê´€ê³„ë¥¼ ëª¨ë¸ë§í•˜ì—¬ ì œì–´ ë§¤ê°œë³€ìˆ˜ë¥¼ ì¶”ë¡ í•˜ëŠ” ë° í™œìš©ë©ë‹ˆë‹¤.</li>
</ul></li>
<li><strong>í›ˆë ¨ ë° ì¶”ë¡ :</strong>
<ul>
<li><strong>í›ˆë ¨:</strong> ë‘ ëª¨ë“ˆì€ ê°œë°© ë£¨í”„(open-loop) ë°©ì‹ìœ¼ë¡œ í›ˆë ¨ë©ë‹ˆë‹¤. ì¦‰, ëª¨ë“  ì…ë ¥ ë°ì´í„°ëŠ” ìˆ˜ì§‘ëœ ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°ì…‹ì—ì„œ ì§ì ‘ ê°€ì ¸ì˜µë‹ˆë‹¤.</li>
<li><strong>ì¶”ë¡ :</strong> ì‹œë®¬ë ˆì´ì…˜ê³¼ ì‹¤ì œ í™˜ê²½ ëª¨ë‘ì—ì„œ íì‡„ ë£¨í”„(closed-loop) ë°©ì‹ìœ¼ë¡œ ìˆ˜í–‰ë©ë‹ˆë‹¤. ì¦‰, í˜„ì¬ ê¶¤ì  ê°’ì€ ì‹¤ì œ ë¡œë´‡ ì„¼ì„œë¡œë¶€í„° ì–»ì–´ì§‘ë‹ˆë‹¤.</li>
<li>ì‹œë®¬ë ˆì´ì…˜ì—ì„œ ì‹¤ì œ ì‹œìŠ¤í…œìœ¼ë¡œ ì œì–´ ë§¤ê°œë³€ìˆ˜ë¥¼ ì„ í˜•ì ìœ¼ë¡œ ë§¤í•‘í•˜ë©°, ì´ëŠ” ëŒ€ëµì ì¸ ìƒí•œ ë° í•˜í•œ ì¶”ì •ì¹˜ë§Œìœ¼ë¡œë„ ì¶©ë¶„í•©ë‹ˆë‹¤.</li>
<li>í•™ìƒ ì •ì±… í›ˆë ¨ ì¤‘ í˜„ì¬ ê¶¤ì  ê°’ì— ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ(Gaussian noise)ë¥¼ ì¶”ê°€í•˜ëŠ” ê²ƒì´ ì‹œë®¬ë ˆì´ì…˜-ì‹¤ì œ ì „ì´(sim-to-real transfer)ì— ì¶©ë¶„í•˜ë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.</li>
</ul></li>
</ul></li>
</ol>
<p><strong>ì‹¤í—˜ ë° ê²°ê³¼:</strong> ë‘ ê°€ì§€ ì ‘ì´‰ ê¸°ë°˜ì˜ ë³µì¡í•œ ì¡°ì‘ ì‘ì—…(in-hand object rotation, flipping)ì—ì„œ DexCtrlì˜ ì„±ëŠ¥ì„ í‰ê°€í–ˆìŠµë‹ˆë‹¤.</p>
<ol type="1">
<li><strong>ì‹œë®¬ë ˆì´ì…˜ ì„±ëŠ¥ í–¥ìƒ:</strong>
<ul>
<li>êµë€(disturbance) ìœ ë¬´ì— ê´€ê³„ì—†ì´ ì‹œë®¬ë ˆì´ì…˜ í™˜ê²½ì—ì„œ DexCtrlì€ ê¸°ì¡´ì˜ â€œManual Tuningâ€ ë° â€œOurs w/o PDâ€ (ì œì–´ ë§¤ê°œë³€ìˆ˜ ì˜ˆì¸¡ ëª¨ë“ˆì´ ì—†ëŠ” ë²„ì „) ë² ì´ìŠ¤ë¼ì¸ ëŒ€ë¹„ í˜„ì €íˆ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. íŠ¹íˆ, íšŒì „ ì†ë„(RotR) ë° ì‹¤íŒ¨ê¹Œì§€ì˜ ì‹œê°„(TTF)ì´ í¬ê²Œ í–¥ìƒë˜ì—ˆê³ , í† í¬ íŒ¨ë„í‹°(Torque) ë° ë¬¼ì²´ ì„ í˜• ì†ë„(ObjVel)ëŠ” ê°ì†Œí–ˆìŠµë‹ˆë‹¤.</li>
<li>ì´ëŠ” DexCtrlì´ ì œì–´ê¸° ë¶ˆì¼ì¹˜ê°€ ì—†ëŠ” ìƒí™©ì—ì„œë„ ë™ì‘ê³¼ ì œì–´ ë§¤ê°œë³€ìˆ˜ë¥¼ ë™ì‹œì— ì¡°ì •í•¨ìœ¼ë¡œì¨ ì‘ì—… í”„ë¡œì„¸ìŠ¤ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì•ˆì •í™”í•˜ê³  ê°€ì†í™”í•  ìˆ˜ ìˆìŒì„ ì…ì¦í•©ë‹ˆë‹¤.</li>
</ul></li>
<li><strong>Sim-to-Real Gap ì™„í™”:</strong>
<ul>
<li>ì‹¤ì œ í™˜ê²½ì—ì„œ DexCtrlì€ ì œë¡œìƒ·(zero-shot) ì‹œë®¬ë ˆì´ì…˜-ì‹¤ì œ ì „ì´ë¥¼ í†µí•´ ë² ì´ìŠ¤ë¼ì¸ë“¤ì„ ì••ë„ì ìœ¼ë¡œ ëŠ¥ê°€í–ˆìŠµë‹ˆë‹¤. íŠ¹íˆ, ì‹œë®¬ë ˆì´ì…˜ì—ì„œë³´ë‹¤ ì‹¤ì œ í™˜ê²½ì—ì„œ DexCtrlê³¼ â€œOurs w/o PDâ€ ê°„ì˜ ì„±ëŠ¥ ê²©ì°¨ê°€ í›¨ì”¬ ë” í¬ê²Œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤.</li>
<li>ì´ ê²°ê³¼ëŠ” ì‹¤ì œ ë¡œë´‡ì—ì„œ ë§¤ ë‹¨ê³„ë§ˆë‹¤ ì œì–´ ë§¤ê°œë³€ìˆ˜ë¥¼ ì ì‘ì ìœ¼ë¡œ ì¡°ì •í•˜ëŠ” ê²ƒì˜ ì¤‘ìš”ì„±ì„ ê°•ì¡°í•˜ë©°, DexCtrlì´ ì‹œë®¬ë ˆì´ì…˜-ì‹¤ì œ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë° í•µì‹¬ì ì¸ ì—­í• ì„ í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.</li>
</ul></li>
<li><strong>ë‹¤ì–‘í•œ ë¬¼ë¦¬ ë§¤ê°œë³€ìˆ˜ë¥¼ ê°€ì§„ ë¬¼ì²´ì— ëŒ€í•œ ì„±ëŠ¥:</strong>
<ul>
<li>ì§ˆëŸ‰(mass)ê³¼ ë§ˆì°°(friction)ì´ ë‹¤ë¥¸ ë¬¼ì²´ì— ëŒ€í•œ ì¶”ê°€ í…ŒìŠ¤íŠ¸ì—ì„œ DexCtrlì€ íŠ¹íˆ ë¬´ê±°ìš´ ë¬¼ì²´ì—ì„œ ë² ì´ìŠ¤ë¼ì¸ë³´ë‹¤ í›¨ì”¬ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ì´ëŠ” DexCtrlì´ ë‹¤ì–‘í•œ ë¬¼ë¦¬ì  íŠ¹ì„±ì„ ê°€ì§„ ë¬¼ì²´ì— ë” ì˜ ì ì‘í•  ìˆ˜ ìˆìŒì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.</li>
</ul></li>
<li><strong>ì œì–´ ë§¤ê°œë³€ìˆ˜(íŠ¹íˆ ê°•ì„±)ì˜ ì˜í–¥ ë¶„ì„:</strong>
<ul>
<li>í•™ìŠµëœ ê°•ì„±(<span class="math inline">K_P</span>)ì´ ë¬¼ì²´ ì§ˆëŸ‰ ë° ë§ˆì°°ê³¼ ì–´ë–»ê²Œ ê´€ë ¨ë˜ëŠ”ì§€ ë¶„ì„í–ˆìŠµë‹ˆë‹¤. ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ë¶„ì„ ê²°ê³¼, ê°•ì„±ì€ ì§ˆëŸ‰ê³¼ ë‹¨ì¡° ì¦ê°€(monotonically increasing) ê´€ê³„ë¥¼ ë³´ì˜€ìŠµë‹ˆë‹¤ (ë¬´ê±°ìš´ ë¬¼ì²´ëŠ” ë” í° í˜ í•„ìš”).</li>
<li>ë§ˆì°°ê³¼ì˜ ê´€ê³„ëŠ” ë” ë¯¸ë¬˜í•˜ì—¬, íŠ¹ì • ê²½ìš°ì—ëŠ” ì¦ê°€í•˜ê³  ë‹¤ë¥¸ ê²½ìš°ì—ëŠ” ê°ì†Œí•˜ëŠ” ë“± ì‘ì—… ì˜ì¡´ì ì¸ ë™ì—­í•™ì„ ë‚˜íƒ€ëƒˆìŠµë‹ˆë‹¤.</li>
<li>ì‹¤ì œ í™˜ê²½ ë¶„ì„ì—ì„œëŠ” ë¬´ê±°ìš´ ë¬¼ì²´ì— ëŒ€í•´ ê°•ì„±ì´ íŠ¹ì • ì‹œê°„ ë‹¨ê³„ì—ì„œ ì¦ê°€í•˜ê±°ë‚˜ ìµœëŒ€ ê°’ìœ¼ë¡œ ë” ì˜¤ë˜ ìœ ì§€ë˜ëŠ” ê²½í–¥ì„, ë¶€ë“œëŸ¬ìš´(smoother) ë¬¼ì²´ì— ëŒ€í•´ì„œëŠ” ì¼ë¶€ ì¡°ì¸íŠ¸ì—ì„œ ìœ ì‚¬í•œ íŒ¨í„´ì„, ë‹¤ë¥¸ ì¡°ì¸íŠ¸ì—ì„œëŠ” ìƒë°˜ëœ ê²½í–¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.</li>
<li>ì´ëŠ” í•™ìŠµëœ ì œì–´ ë§¤ê°œë³€ìˆ˜ê°€ í•„ìš”í•œ ì ‘ì´‰ë ¥ì˜ ë³€í™”ë¥¼ ì¸ì½”ë”©í•˜ë©°, ë¬¼ì²´ ì¡°ì‘ ì„±ëŠ¥ í–¥ìƒì— ê¸°ì—¬í•œë‹¤ëŠ” ê°€ì„¤ì„ ê²€ì¦í•©ë‹ˆë‹¤.</li>
</ul></li>
</ol>
<p><strong>ê²°ë¡  ë° í•œê³„:</strong></p>
<ul>
<li>DexCtrlì€ ì´ë ¥ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë™ì‘ê³¼ ì œì–´ ë§¤ê°œë³€ìˆ˜ë¥¼ ë™ì‹œì— ì¶œë ¥í•˜ì—¬ ë¯¼ì²©í•œ ì¡°ì‘(dexterous manipulation)ì˜ ì‹œë®¬ë ˆì´ì…˜-ì‹¤ì œ ê°„ê·¹ì„ íš¨ê³¼ì ìœ¼ë¡œ ì¤„ì˜€ìŠµë‹ˆë‹¤.</li>
<li>í–¥í›„ ì—°êµ¬ì—ì„œëŠ” ì—¬ëŸ¬ ë¯¼ì²©í•œ ì‘ì—…ì´ ë‹¨ì¼ ì œì–´ ë§¤ê°œë³€ìˆ˜ ì˜ˆì¸¡ ëª¨ë“ˆì„ ê³µìœ í•˜ê±°ë‚˜, í•˜ë“œì›¨ì–´ ì§€ì› ì‹œ ì‹¤ì‹œê°„ í˜ í”¼ë“œë°±(force feedback) ê¸°ë°˜ì˜ ì˜¨ë¼ì¸ ë¯¸ì„¸ ì¡°ì •(fine-tuning)ì„ ìˆ˜í–‰í•  ê³„íšì…ë‹ˆë‹¤.</li>
<li>í˜„ì¬ ë°©ë²•ì˜ í•œê³„ëŠ” í•˜ë“œì›¨ì–´ ì œì•½ìœ¼ë¡œ ì¸í•´ ì‹¤ì œ í˜ ë˜ëŠ” ì´‰ê° ì„¼ì„œ(tactile sensing)ë¥¼ í†µí•©í•˜ì§€ ëª»í•˜ê³ , LeapHand í”Œë«í¼ì— í•œì •ëœ ì‹¤ì œ í™˜ê²½ í‰ê°€ì— ìˆìŠµë‹ˆë‹¤.</li>
</ul>
<hr>
</section>
<section id="detail-review" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Detail Review</h1>
<blockquote class="blockquote">
<p>DexCtrl: Towards Sim-to-Real Dexterity with Adaptive Controller Learning â€“ A Deep Dive</p>
</blockquote>
<section id="introduction-and-the-sim-to-real-challenge-in-dexterous-manipulation" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="introduction-and-the-sim-to-real-challenge-in-dexterous-manipulation"><span class="header-section-number">2.1</span> Introduction and the Sim-to-Real Challenge in Dexterous Manipulation</h2>
<p>Dexterous robotic manipulation has made remarkable strides in simulation, achieving complex feats like in-hand object rotation, tool use, and even piano playing. However, <strong>transferring</strong> these learned manipulation skills <strong>from simulation to real robots</strong> remains notoriously difficult. One critical but often under-addressed culprit is the <strong>low-level controller mismatch</strong> between simulation and reality. In other words, even if a policy learns a perfect motion trajectory in sim, the way a real robotâ€™s motors execute that trajectory (governed by controller parameters like stiffness and damping) can differ, leading to <strong>vastly different forces and outcomes</strong>. Identical motions can succeed in simulation but slip or fail on the real hardware due to these dynamics discrepancies.</p>
<p>Traditionally, researchers have tackled the sim-to-real gap through tedious <strong>manual tuning</strong> of controller gains (e.g.&nbsp;adjusting PD controller stiffness/damping so the real robotâ€™s behavior matches sim) or by massive <strong>domain randomization</strong> of physics parameters during training. Unfortunately, <strong>both approaches have drawbacks</strong>: manual tuning is labor-intensive and often imprecise, while randomizing controller parameters can <strong>significantly increase training difficulty</strong> and still may not guarantee success in the real world. These methods rely on trial-and-error and human intuition rather than a principled solution, and they <strong>demand extensive human effort</strong> in choosing parameter schedules or noise ranges. Clearly, there is a need for a better way to bridge this â€œcontroller gapâ€ in a <strong><em>robust, automatic</em></strong> manner.</p>
<p><strong>Enter DexCtrl</strong> â€“ a new framework proposed to address this very challenge. DexCtrl explicitly targets the controller mismatch by allowing the policy to <strong>adapt its own low-level controller parameters on the fly</strong>, rather than treating them as fixed or randomly perturbed constants. In essence, DexCtrl lets the robot policy not only decide <em>what</em> actions to take, but also <em>how</em> to execute those actions in terms of controller stiffness/damping. By doing so, it aims to <strong>mitigate the sim-to-real gap at the source</strong>: the torque-generation level. This deep-dive review will explore <strong>how DexCtrl works</strong>, its key contributions and novelty, and examine in detail the experimental evidence of its effectiveness in both simulation and real-world trials.</p>
</section>
<section id="key-idea-adaptive-controller-learning-with-dexctrl" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="key-idea-adaptive-controller-learning-with-dexctrl"><span class="header-section-number">2.2</span> Key Idea: Adaptive Controller Learning with DexCtrl</h2>
<p>At its core, <strong>DexCtrl is about jointly learning the control actions <em>and</em> the controller parameters</strong> needed to carry out those actions. The key insight is that the policy can be trained to automatically tune its controller gains (like a robot adjusting its â€œmuscle stiffnessâ€) based on what it has felt and done in the recent past. This is achieved by giving the policy <strong>historical context</strong> and letting it output two things at each time step: (1) the next high-level action (e.g.&nbsp;desired joint positions), and (2) the appropriate controller parameters (e.g.&nbsp;stiffness <span class="math inline">K</span> and damping <span class="math inline">D</span> for a PD controller) to apply that action. By continuously adjusting these parameters in a closed loop, DexCtrl can compensate for modeling errors or unexpected force interactions <em>as they occur</em>, rather than hoping a fixed controller will work for all situations.</p>
<p>Crucially, DexCtrlâ€™s policy <strong>observes the controller state itself</strong>. The current stiffness/damping values are fed into the observation space along with recent trajectory data, so the policy is aware of â€œhow hardâ€ itâ€™s driving the motors at any given time. This enables better reasoning about contact forces â€“ if a task isnâ€™t going as expected, the policy can sense if maybe the controller was too stiff or too compliant, and adjust accordingly. By explicitly including controller parameters in the observation, the agent can directly capture force-related feedback that would otherwise be hidden.</p>
<p><strong>Figure 1</strong> below illustrates the concept. The top shows a conventional approach where the policy outputs only the action <span class="math inline">a_t</span> (like desired joint positions) and uses a fixed controller gain <span class="math inline">K_{\text{const}}</span> for the whole task. This often requires <strong>extensive manual gain tuning</strong> and still might be inaccurate if conditions change. The bottom shows DexCtrlâ€™s approach: the policy outputs both <span class="math inline">a_t</span> and an adaptive gain <span class="math inline">K_t</span> at each step, allowing it to tweak the controller behavior continually. This adaptive scheme promises <strong>reduced labor (no manual tuning)</strong> and <strong>better accuracy</strong> in execution.</p>
<center>
<img src="../../images/2025-07-16-dexctrl/2.png" width="100%">
</center>
<b>
<figcaption>
<em>Conceptual comparison. <strong>Top:</strong> Standard policy outputs action <span class="math inline">a_t</span> with fixed controller parameters (<span class="math inline">K_{\text{const}}</span>), requiring manual tuning and risking performance mismatch. <strong>Bottom:</strong> DexCtrl outputs both action and adaptive controller parameters (<span class="math inline">K_t</span>) each step, automatically adjusting the â€œtorque controllerâ€ to current needs (green check). The right shows dexterous manipulation tasks (in-hand rotation and flipping) where DexCtrl was evaluated.</em>
</figcaption>
</b><p><b></b></p>
<p>The <strong>novelty</strong> of DexCtrlâ€™s adaptive controller learning lies in this <strong>automatic, time-varying adjustment of control gains as part of the policy</strong>. Previous works in dexterous manipulation sim-to-real mostly ignored this lever â€“ they would train with a fixed or randomly perturbed controller and hope for the best. While adaptive force control has been explored in simpler robotic tasks (like adjusting stiffness for wiping or pivoting tasks), it had <strong>rarely been applied to complex dexterous hand manipulation</strong>. DexCtrl is among the first to show that <em>learning to adjust control parameters</em> in tandem with the action policy can yield significant gains for dexterous hands, providing a more <strong>principled approach to closing the sim-to-real gap</strong> at the controller level. In summary, the key contributions of DexCtrl can be highlighted as follows:</p>
<ul>
<li><strong>Identifying Controller Mismatch:</strong> It pinpoints the often-overlooked <strong>mismatch in robot low-level controllers</strong> as a critical factor in sim-to-real failure, and directly tackles it by making control parameters adaptable. This shifts sim-to-real transfer from being a manual tuning art to an automated learning problem.</li>
<li><strong>Joint Action-Control Learning:</strong> It introduces a <strong>simple yet effective framework</strong> that jointly learns the high-level actions and the low-level controller gains from historical data. By leveraging recent trajectories and past controller settings, the policy gains adaptivity to variations in force requirements and object dynamics.</li>
<li><strong>Improved Performance Across Domains:</strong> Through extensive experiments, DexCtrl demonstrates <strong>significantly better performance than baseline approaches</strong> (manual tuning and non-adaptive policies) both in simulation and in zero-shot real-world tests, across multiple contact-rich tasks. The results include not just task success metrics but also stability and efficiency measures, along with insightful analysis of <em>why</em> it works.</li>
</ul>
<p>Next, we dive into <strong>how DexCtrl is designed and trained</strong>, before examining the experimental evidence in detail.</p>
</section>
<section id="dexctrl-framework-and-methodology" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="dexctrl-framework-and-methodology"><span class="header-section-number">2.3</span> DexCtrl Framework and Methodology</h2>
<section id="how-dexctrl-works-overview-of-the-architecture" class="level3" data-number="2.3.1">
<h3 data-number="2.3.1" class="anchored" data-anchor-id="how-dexctrl-works-overview-of-the-architecture"><span class="header-section-number">2.3.1</span> How DexCtrl Works: Overview of the Architecture</h3>
<p>The DexCtrl framework consists of a two-stage learning process: first training an <em>oracle policy</em> in simulation via reinforcement learning, and then <strong>distilling that experience into two learned modules</strong> â€“ one for action prediction and one for controller parameter prediction. <strong>Figure 2</strong> provides an overview of this pipeline. During an offline training phase (Fig. 2a), a high-performing oracle policy (trained with RL in diverse simulated settings) is used to generate a dataset of trajectories. This dataset contains sequences of states, actions, and the oracleâ€™s chosen controller parameters at each step. Then, DexCtrl trains two separate neural networks (students) on this data via supervised learning: one network learns to predict the next action, and the other learns to predict the next controller parameters, both based on the history. At runtime (Fig. 2b), these two learned modules work together in a closed loop: the action module predicts the desired next joint motion, and the controller module immediately predicts the appropriate stiffness/damping to execute that motion, given what has happened in the recent past.</p>
<center>
<img src="../../images/2025-07-16-dexctrl/3.png" width="100%">
</center>
<b>
<figcaption>
<em><strong>DexCtrl framework.</strong> (a) </em>Open-Loop Training:* An oracle policy (trained with PPO in simulation) generates rollouts of states <span class="math inline">s_t</span> with actions <span class="math inline">a_t</span> and controller gains <span class="math inline">K_t</span>. A dataset of sequences <span class="math inline">(q^c, q^d, a, K)</span> is collected (where <span class="math inline">q^c, q^d</span> are current and desired joint angles). Two student networks are distilled on this data: an <strong>Action Prediction Module</strong> (blue) that learns to output <span class="math inline">\hat{a}_t</span> (desired joint positions) from historical trajectories, and a <strong>Control Parameters Prediction Module</strong> (purple) that learns to output <span class="math inline">\hat{K}*t</span> (controller gains) from the history <em>and</em> the current intended action. (Losses are mean-squared errors to the oracleâ€™s outputs.) (b) <em>Closed-Loop Inference:</em> On the real robot or new simulation runs, the policy observes the actual robot state <span class="math inline">q^c_t</span> (and a window of past states <span class="math inline">q^c*{t-1},â€¦</span>), feeds the recent history into the action module to get <span class="math inline">\hat{a}_t</span>, then feeds the history plus the proposed action into the control module to get <span class="math inline">\hat{K}_t</span>. The robotâ€™s low-level torque controller then executes, applying torque <span class="math inline">\tau_t</span> based on the chosen <span class="math inline">K_t</span> and the error between <span class="math inline">q^d_t</span> and <span class="math inline">q^c_t</span>. This loop repeats every time step, continually adapting actions and controller parameters.*
</figcaption>
</b><p><b></b></p>
<p>Under the hood, DexCtrlâ€™s observation at each decision step contains a <strong>window of historical information</strong>. In the implementation, the authors use the past 10 time steps of robot proprioceptive data â€“ specifically, the sequences of <strong>actual joint positions <span class="math inline">q^c</span></strong>, the <strong>previous desired joint positions <span class="math inline">q^d</span></strong>, and the <strong>controller parameters <span class="math inline">K</span> (stiffness, damping)</strong> applied at those steps. This rich history serves as the input for both student networks.</p>
<p>To effectively extract insights from this temporal data, DexCtrl employs an <strong>attention-based architecture</strong>. The action prediction module uses a <strong>self-attention mechanism</strong> over the time window of past states, allowing it to learn temporal patterns and trends in the trajectory (for example, to estimate object motion and infer what the next goal should be). In contrast, the controller prediction module uses a <strong>cross-attention mechanism</strong>: it takes the current planned action as a query and attends to the historical states as keys/values. Intuitively, this helps the controller module learn the relationship between <em>what action youâ€™re about to do</em> and <em>what controller settings worked well in the past in similar situations</em>. The authors note that while both modules see the same form of input (the last 10 steps of <span class="math inline">q^c, q^d, K</span>), their roles are different â€“ the action module is understanding the motion trajectory context, whereas the controller module is learning how past actions and gains led to outcomes, analogous to how a human might adjust their grip stiffness based on recent success or slip events.</p>
<p>Itâ€™s worth emphasizing that <strong>DexCtrl decouples the learning of actions and controller parameters</strong> into two separate prediction modules on purpose. This design ensures that the task of choosing the best motor commands isnâ€™t entangled with the task of tuning gains â€“ which could otherwise make learning unstable. By training them separately (but on the same experiences), DexCtrl can achieve adaptive control without sacrificing the quality of the action policy.</p>
</section>
<section id="training-and-deployment-strategy" class="level3" data-number="2.3.2">
<h3 data-number="2.3.2" class="anchored" data-anchor-id="training-and-deployment-strategy"><span class="header-section-number">2.3.2</span> Training and Deployment Strategy</h3>
<p>The training of DexCtrlâ€™s neural modules is done in an <strong>open-loop, supervised fashion</strong> using the logged simulation data. That is, during training they directly feed in historical sequences from the oracleâ€™s trajectories and train the networks to predict the oracleâ€™s next action and gain, step by step. This avoids any compounding error during training and is effectively a form of <strong>behavioral cloning/distillation</strong> from the oracle RL policy. Because the oracle was trained with access to privileged simulation information (like object pose and properties) that may not be available on real robots, DexCtrlâ€™s student networks rely purely on the observable history of joint states. Impressively, the authors show that the networks can infer necessary information (like how the object is moving or how heavy it might be) just from proprioceptive history, sidestepping the need for direct object state input. This is analogous to a robot feeling the weight of an object through the effort it needed over the last few moments.</p>
<p>When deploying to the real world (or for closed-loop testing), DexCtrl runs in <strong>real-time closed-loop</strong>: it uses the robotâ€™s actual sensor readings at each step as the latest â€œcurrent stateâ€ in the history, then produces an action and controller adjustment which are immediately applied. Notably, <strong>no further training or fine-tuning was done on the real robot</strong> â€“ the policy is deployed zero-shot after training in simulation. To account for units differences between sim and real, they apply a simple linear scaling of the controller parameters (based on approximate upper/lower bounds) rather than any painstaking system identification. Beyond that, no manual calibration was needed.</p>
<p>An interesting detail is how DexCtrl manages to transfer well without heavy randomization. The oracle policy was trained on a range of object physical properties (mass, friction) for generality, but the authors did <strong>not</strong> perform extreme randomization of sensor noise or controller gains when training the oracle. Instead, they added a small amount of Gaussian noise to the student networksâ€™ inputs during training, which turned out to be <strong>sufficient for sim-to-real transfer</strong>. This suggests that because DexCtrlâ€™s policy can adjust to discrepancies on the fly, it doesnâ€™t require as broad a training distribution to cover every possible gap â€“ a little bit of noise was enough to make the student robust, simplifying the training process for the teacher (oracle) policy. In other words, DexCtrl inherently handles some variability through adaptation, whereas a fixed-gain policy would have needed aggressive domain randomization to cope with the same variability.</p>
</section>
</section>
<section id="experimental-evaluation-and-results" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="experimental-evaluation-and-results"><span class="header-section-number">2.4</span> Experimental Evaluation and Results</h2>
<p>The authors evaluated DexCtrl on <strong>two challenging dexterous manipulation tasks</strong> using a 16-degree-of-freedom <em>LEAP hand</em> (a sophisticated robotic hand platform). The tasks were designed to be <strong>contact-rich</strong> and sensitive to force execution, thus ideal for testing sim-to-real performance:</p>
<ul>
<li><strong>In-Hand Object Rotation:</strong> The robot hand must rotate a grasped object (e.g.&nbsp;a cube, apple, etc.) around a specified axis using finger dexterity, without dropping it. This task stresses object-hand contacts and requires controlled forces to keep the object stable while spinning it.</li>
<li><strong>Object Flipping on a Table:</strong> The hand, starting with an object resting on a surface (table), must flip the object 180Â° (like flipping a block or a Rubikâ€™s cube to its other face) using a coordinated push-lift motion. This involves contact between the object, the hand, <em>and</em> the table â€“ increasing sensitivity to dynamics (e.g.&nbsp;friction with the table and impact forces).</li>
</ul>
<p>Both tasks were first learned in simulation (using PyBullet or a similar physics simulator), then the policies were deployed on a <strong>real-world setup</strong> with the LEAP hand performing the same tasks on real objects. To quantify performance, the authors defined several metrics:</p>
<ul>
<li><strong>Rotation Rate (RotR):</strong> How fast the object rotates around the target axis. In simulation this was measured via a shaped reward; in real experiments itâ€™s measured in radians of rotation achieved per trial. Higher RotR means better task success (faster rotation).</li>
<li><strong>Time to Fail (TTF):</strong> How long the hand can continue the task before failing (dropping the object in rotation, or the object falling off course in flipping). This reflects stability â€“ longer is better.</li>
<li><strong>Object Linear Velocity (ObjVel):</strong> The average translational speed of the object during the task (only measured in sim). Lower velocity implies the object isnâ€™t rattling or moving erratically, hence indicating a <em>stable manipulation</em>.</li>
<li><strong>Torque Penalty:</strong> A measure of energy or effort, computed from the torques applied (sim only). Lower torque penalty means the controller is efficient and smooth, not applying excessive forces.</li>
</ul>
<section id="baselines-for-comparison" class="level3" data-number="2.4.1">
<h3 data-number="2.4.1" class="anchored" data-anchor-id="baselines-for-comparison"><span class="header-section-number">2.4.1</span> Baselines for Comparison</h3>
<p>DexCtrl was compared against two main baselines to isolate the benefits of adaptive controller tuning:</p>
<ul>
<li><strong>Manual Tuning Baseline:</strong> This represents the <em>conventional approach</em>: First, manually adjust the simulatorâ€™s controller parameters (PD gains) and add slight randomness so that a policy trained in sim might transfer to real. In practice, the authors carefully tuned the stiffness/damping by comparing simulated vs real trajectory responses, and trained a new RL â€œoracleâ€ policy with those tuned gains (plus minor gain randomization for robustness). This oracle and its student policy <strong>do not adapt gains at runtime</strong> â€“ they output only fixed-gain actions. Essentially, this baseline is a well-tuned fixed controller approach, requiring significant human effort to set up.</li>
<li><strong>Ours w/o PD (No Adaptive Gains):</strong> This ablation is <em>DexCtrl without the core idea</em> â€“ the adaptive gain module is turned off. The policy is trained using the same oracle data as DexCtrl (which had varying gains in simulation), but during student training and deployment, the controller gains are fixed to the manually tuned values. The action module is still trained on the oracleâ€™s trajectories, but it ignores the adaptive aspect. This baseline tests whether the advantage comes simply from our data collection and better action trajectories, versus the <em>addition of adaptive control</em>.</li>
</ul>
<p>Both baselines and DexCtrl share the same training dataset size and neural network architecture for the action module, ensuring a fair comparison.</p>
</section>
<section id="simulation-results-performance-boost-even-without-a-gap-q1" class="level3" data-number="2.4.2">
<h3 data-number="2.4.2" class="anchored" data-anchor-id="simulation-results-performance-boost-even-without-a-gap-q1"><span class="header-section-number">2.4.2</span> Simulation Results: Performance Boost Even Without a Gap (Q1)</h3>
<p>Before tackling sim-to-real transfer, the authors first asked: <em>Even if there is no sim-to-real discrepancy (i.e., using the same controllers in sim as in training), does jointly adapting controller parameters improve performance?</em> To test this, they evaluated the policies in simulation under ideal conditions (no controller mismatch) and also with some external disturbances (random forces applied to the object) to simulate a more challenging scenario.</p>
<p>The results were striking: <strong>DexCtrl outperformed both baselines in simulation, even when all methods used the same simulator dynamics</strong>. For the in-hand rotation task, DexCtrl achieved higher rotation speeds and longer stability than the manually tuned baseline, <em>and</em> used significantly less torque on average. For example, with random disturbances applied, DexCtrlâ€™s rotation speed (RotR) was about 24% higher than the manual baseline, and it sustained the rotation for longer (TTF ~256 vs 239 time steps). It also expended only ~25% of the torque effort that the manual baseline did, indicating much more efficient control. Even the â€œOurs w/o PDâ€ ablation (no adaptive gains) performed better than the manual baseline in rotation, thanks to the improved trajectories generated by our training approach, but <strong>full DexCtrl still had the edge</strong>.</p>
<p>On the flipping task, the difference was even more pronounced. DexCtrl nearly <strong>doubled the rotation rate</strong> of the flipped object compared to the manual-tuning baseline in simulation (RotR ~172 vs 91 with disturbances). It essentially solved the flipping with minimal failures (TTF almost maxed out at 297+ steps) and very low torque usage, whereas the fixed baseline struggled to get the object around as quickly. Interestingly, the ablation without adaptive control <em>did not outperform</em> the manual baseline on flipping at all â€“ it had lower RotR (~82 vs 91 with disturbance). This highlights that flipping (which involves impacts with a table and complex contact) is <strong>highly sensitive to controller parameters</strong>, and a fixed PD policy couldnâ€™t find a one-size-fits-all gain. <strong>DexCtrlâ€™s adaptive gains were key to its success in flipping</strong>, adjusting stiffness on-the-fly to handle the transitions. In summary, even in simulation without any real-world discrepancies, letting the policy co-optimize <em>how</em> to control yields better stability and faster task completion than using a fixed controller. The authors also noted DexCtrlâ€™s training was faster to converge than massive randomization approaches, since the policy doesnâ€™t have to brute-force learn robustness â€“ it gets to sense and correct for variations directly.</p>
</section>
<section id="zero-shot-transfer-to-real-robots-narrowing-the-gap-q2" class="level3" data-number="2.4.3">
<h3 data-number="2.4.3" class="anchored" data-anchor-id="zero-shot-transfer-to-real-robots-narrowing-the-gap-q2"><span class="header-section-number">2.4.3</span> Zero-Shot Transfer to Real Robots: Narrowing the Gap (Q2)</h3>
<p>The true test of DexCtrl is zero-shot transfer from simulation to a <strong>real dexterous hand</strong>. The team deployed the learned policies directly onto a real LEAP hand robot, with <strong>no additional fine-tuning</strong>, to see how well the skills carry over and whether DexCtrl indeed narrows the sim-to-real gap. They evaluated the in-hand rotation task on <strong>12 different real objects</strong> spanning a range of shapes, masses, and surface frictions (including a cube, bottle, apple, yogurt cup, baseball, Rubikâ€™s cube, etc.), to ensure generality. Each objectâ€™s performance was measured over 10 test trials with different initial poses, and averages were compared.</p>
<p>The <strong>real-world results</strong> strongly favored DexCtrl. In fact, the performance gaps that were somewhat modest in simulation became <strong>much larger in the real world</strong>, underscoring how important adaptive control is outside the simulator. DexCtrl significantly outperformed the manually tuned baseline on all objects, achieving higher rotation speeds and similar or longer times before failure. For instance, across objects, the baseline might achieve only ~2â€“3 radians of rotation on average, whereas DexCtrl was often achieving <strong>5â€“15+ radians</strong>, essentially an order-of-magnitude improvement in some cases. It also maintained near-max stability (many objects never dropped within the 300-step trial limit, TTF = 300) whereas the baseline dropped some objects sooner. Even the non-adaptive variant (Ours w/o PD) did reasonably well in real â€“ better than the manual baseline, showing that the learned trajectories were inherently more robust â€“ but <strong>DexCtrlâ€™s adaptive module gave it a clear extra boost</strong> in real conditions. The authors point out that the gap between DexCtrl and its no-PD ablation was <strong>much larger in real-world tests than it had been in sim</strong>, emphasizing that real hardware has many small unpredictabilities (friction, slight delays, minor calibration errors, etc.) that <strong>require continual controller adjustment</strong>. DexCtrl was uniquely capable of handling those, whereas a fixed-gain policy, even a strong one, was inevitably less optimal for some objects or moments.</p>
<p>They also demonstrated the flipping task on the real hand (with a Rubikâ€™s cube being flipped on a table) to show generalization. Visual results (as seen in Fig. 1 and the paperâ€™s videos) indicate that DexCtrl could perform the flip in reality as well. While detailed real-world numbers for flipping werenâ€™t given for each baseline in the text, the success of DexCtrl in flipping further validates the approachâ€™s generalizability to tasks involving external contacts (hand-object-table interactions). The overall takeaway is that <strong>DexCtrl dramatically narrowed the sim-to-real gap</strong> â€“ policies that would normally degrade when going to real instead retained high performance, thanks to adaptive control. The manual baseline, despite careful tuning, could not match this, highlighting the limitations of one-time tuning.</p>
</section>
<section id="robustness-to-object-variations-q3" class="level3" data-number="2.4.4">
<h3 data-number="2.4.4" class="anchored" data-anchor-id="robustness-to-object-variations-q3"><span class="header-section-number">2.4.4</span> Robustness to Object Variations (Q3)</h3>
<p>One of the promises of adaptive controller learning is improved robustness to variations in object properties, since the policy can <em>tune its behavior on the fly</em> to suit the object being manipulated. The authors designed a set of experiments to explicitly test how DexCtrl handles changes in <strong>mass</strong> and <strong>friction</strong> of the object, in a controlled way. They took a <strong>single object shape (a cube)</strong> and created multiple versions: for mass variation, they used a hollow cube and inserted different weights to make it light, medium, or heavy; for friction variation, they used cubes of the same weight but with different surface textures (smooth, medium, rough) to alter the friction coefficient.</p>
<p>The results, summarized in Table 4 of the paper, show that <strong>DexCtrl consistently outperformed the baselines across all mass and friction conditions</strong>. Notably, the advantage of DexCtrl was <strong>most pronounced for the most challenging cases</strong> â€“ e.g.&nbsp;the heavy object and the very slippery object. For the heaviest cube, DexCtrl achieved a high rotation speed and didnâ€™t drop the object (TTF ~300), whereas the fixed-gain baseline could barely rotate it (RotR near 0.6) and tended to fail much sooner. This indicates that DexCtrlâ€™s policy learned to <strong>increase stiffness/force output for heavier objects</strong> to overcome their inertia, something a fixed controller tuned for lighter objects couldnâ€™t do effectively. Similarly, for the low-friction (smooth) cube, DexCtrl spun it the fastest (since low friction makes rotation easier) but also managed stability adequately, whereas the baseline had lower speed and still occasional drops.</p>
<p>Interestingly, the pattern of performance across these variations aligned with intuitive <strong>force-based reasoning</strong>: lighter objects (or lower friction) can be flipped or rotated more easily (so they achieve higher speeds) but are also less stable (easier to slip), whereas heavier or rougher objects move more slowly but are inherently more stable once gripped. DexCtrl was able to handle both ends of these spectrums by adapting its control gains appropriately, confirming that its adaptive strategy generalizes to new physical properties. In contrast, the manual baseline had to pick one compromise setting of stiffness/damping â€“ it could neither apply enough force for heavy objects, nor gentle enough control for very light/smooth objects, leading to either weak performance or instability in those extremes.</p>
</section>
<section id="understanding-the-learned-adaptive-strategies-q4" class="level3" data-number="2.4.5">
<h3 data-number="2.4.5" class="anchored" data-anchor-id="understanding-the-learned-adaptive-strategies-q4"><span class="header-section-number">2.4.5</span> Understanding the Learned Adaptive Strategies (Q4)</h3>
<p>A natural question arises: <em>What exactly is DexCtrlâ€™s controller module learning to do?</em> To demystify this, the authors analyzed the <strong>learned stiffness profiles</strong> under different object conditions. Since the controller outputs a stiffness and damping for each joint at each time step, we can look at how those values change for, say, a heavy object vs a light object, or a high-friction vs low-friction scenario. The paper focuses on <strong>stiffness (K)</strong> since it had a more pronounced effect on performance than damping in their tasks.</p>
<p>Two notable analyses were presented:</p>
<ul>
<li><p><strong>Average Stiffness vs Object Properties:</strong> They computed the average stiffness used (across time and joints) for different object masses and frictions. In simulation, they observed a <strong>monotonic increase of stiffness with object mass</strong> â€“ heavier objects induced the policy to choose higher stiffness values on average. This matches intuition: a heavier object requires more force (higher gain) to manipulate effectively, so DexCtrl â€œcranks up the strengthâ€ accordingly. For friction, the relationship was not one-directional: in one axis of rotation task, stiffness increased with more friction (perhaps because more friction means more resistance to sliding, needing stronger control), while in another case stiffness decreased with more friction (possibly because friction helps hold the object, so less stiffness was needed). The takeaway is that <strong>DexCtrl learned a nuanced strategy</strong> where stiffness tuning depended on the specific context of how friction was affecting the task dynamics. This reflects the complexity of dexterous manipulation â€“ friction can both help and hinder movement depending on the situation, and the policy adjusted different joints differently to compensate.</p></li>
<li><p><strong>Temporal Stiffness Patterns:</strong> They also looked at how stiffness evolved over the course of a trial for different objects (Figure 7 in the paper). To isolate the effect of the controller module, they fed the <em>ground-truth actions</em> (from a successful trajectory) into the controller network and observed what gains it outputs over time for, say, a heavy vs light object. The trends showed that for <strong>heavier objects, DexCtrl often raised stiffness at critical moments or maintained a higher stiffness for longer durations</strong> (essentially keeping the â€œmuscles flexedâ€). For <strong>smoother (low-friction) objects</strong>, certain joints showed <em>opposite stiffness trends</em> compared to rougher objects â€“ in some joints the stiffness would spike for smooth objects (to prevent slip), while in others it might reduce (perhaps to avoid pushing too hard and causing loss of contact). These joint-specific adjustments illustrate that the policy isnâ€™t simply using a single scalar gain â€“ itâ€™s tuning a <em>vector of gains</em> across the handâ€™s joints, tailoring the force distribution to the task. This level of adaptive control is very hard to achieve with manual tuning.</p></li>
</ul>
<p>Overall, this analysis confirmed the hypothesis that <strong>DexCtrlâ€™s controller parameters encode meaningful adjustments to contact forces</strong>, adapting systematically to the objectâ€™s mass and friction to improve performance. In essence, the policy learned to answer questions like â€œIs this object heavy? Then use a stiffer controller to grab it firmly,â€ or â€œIs this surface slippery? Maybe tighten grip in some fingers but loosen in others to keep balance.â€ This provides an insightful peek into <em>why</em> DexCtrl outperforms the fixed strategies â€“ itâ€™s because itâ€™s actively modulating the interaction forces to suit the situation, rather than hoping one fixed setting will work everywhere.</p>
</section>
</section>
<section id="effectiveness-limitations-and-outlook" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="effectiveness-limitations-and-outlook"><span class="header-section-number">2.5</span> Effectiveness, Limitations, and Outlook</h2>
<p><strong>Effectiveness:</strong> The combination of results from simulation, real-world tasks, and controlled variation tests builds a strong case that DexCtrl is an effective solution to narrow the sim-to-real gap in dexterous manipulation. By <strong>adaptively tuning the robotâ€™s controller each time step</strong>, DexCtrl achieved higher task success rates, better stability, and more energy-efficient control compared to state-of-the-art non-adaptive approaches. It essentially offloads some of the burden of sim-to-real transfer from human engineers (who previously had to fine-tune gains or exhaustively randomize environments) to the learning algorithm itself. The approach proved its worth especially in real-world evaluations: tasks that normally required careful gain tuning were accomplished <strong>zero-shot with minimal manual effort</strong>, and even challenging objects were manipulated reliably. In short, DexCtrl demonstrates that <strong>learning â€œhow to controlâ€ is as important as learning â€œwhat to doâ€</strong> for robust robot dexterity.</p>
<p><strong>Limitations:</strong> Despite its promising results, DexCtrl is not without limitations, as the authors openly discuss:</p>
<ul>
<li><p><strong>Lack of Explicit Force/Tactile Sensing:</strong> Currently, DexCtrl relies solely on <strong>proprioceptive data</strong> (joint angles, etc.) and inferred object information from history. The real robot experiments did not use any tactile or force sensors on the hand. This was partly due to hardware limitations (the LEAP hand might not have rich tactile sensors). As a result, the policy might not observe some subtle aspects of contact (like the onset of slip or precise contact forces) directly. Integrating <strong>real-time force feedback or tactile sensing</strong> could further improve DexCtrlâ€™s ability to adapt, essentially giving it an even clearer signal for how the interaction is going. The authors suggest that incorporating high-fidelity force/tactile data is a promising direction for future work. We can imagine a future DexCtrl that <em>feels</em> an object starting to slip and immediately boosts stiffness, even faster than it could infer from joint motions alone.</p></li>
<li><p><strong>Hardware Generalization:</strong> The studyâ€™s real-world tests were confined to the <strong>LEAP hand platform</strong> â€“ a specific 16-DOF dexterous hand at UC Berkeley. While the tasks and objects were varied, it remains to be proven that the DexCtrl approach generalizes to other robot hands (e.g., anthropomorphic hands with different kinematics or pneumatic actuators with different dynamics). The framework itself is general (it could, in principle, be applied to any torque-controlled system with adjustable gains), but practical differences in hardware could pose new challenges. The authors plan to extend evaluations to other dexterous hands to validate generalizability.</p></li>
<li><p><strong>Task Generality and Shared Controllers:</strong> DexCtrl was demonstrated on two tasks. Scaling to a wider range of tasks or more unstructured scenarios might require further development. One idea the authors mention is training a <strong>single controller adaptation module that could be shared across multiple skills</strong>. This could be useful if you want one system to perform many tasks but still benefit from adaptive control â€“ perhaps by fine-tuning gains appropriate for each task context.</p></li>
<li><p><strong>Simulation Reliance:</strong> Like most learning-based robotics approaches, DexCtrl still relies on a high-quality simulation (with diverse randomization of object properties) to learn the policy. If the simulatorâ€™s physics are very far off from reality, adaptive gains alone might not rescue performance. However, the need for less extreme domain randomization is a plus. Future work might explore if a small amount of <strong>online learning or fine-tuning on real hardware</strong> (e.g., a brief adaptation phase with real data) could further improve performance, especially as suggested if real force sensors are available.</p></li>
</ul>
<p>In summary, <strong>DexCtrlâ€™s adaptive controller learning brings a new level of robustness to dexterous manipulation</strong>, but incorporating richer sensing and testing its generality on other platforms are important next steps to fully realize its potential.</p>
</section>
<section id="conclusion" class="level2" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">2.6</span> Conclusion</h2>
<p>DexCtrl represents an exciting development at the intersection of reinforcement learning, adaptive control, and robotics. By giving a dexterous robot hand the ability to <strong>tune its own controller parameters on the fly</strong>, the approach addresses a long-standing barrier in sim-to-real transfer. The method is conceptually elegant â€“ use past experience to inform both what action to take and how aggressively to take it â€“ and it proved remarkably effective in practice. Key contributions include highlighting the often-neglected role of low-level controller differences in sim-to-real gap and providing a principled learning-based solution to tackle it. The experiments showed improved performance over manual tuning even in simulation, and a dramatic reduction of performance loss when moving to real hardware, across various objects and conditions.</p>
<p>For a technical audience, DexCtrl offers insight into how <strong>policy and controller design can be interwoven</strong>: rather than treating the controller as a fixed black box, it becomes part of the learning problem. This blurs the line between high-level planning and low-level control, potentially leading to more integrated and resilient robotic systems. It also suggests that some challenges previously addressed by brute-force domain randomization might be better solved by model adaptation mechanisms like this, which can be more sample-efficient and targeted.</p>
<p>Looking forward, the idea of <strong>adaptive controller learning</strong> could be extended in many ways. As noted, adding direct force sensing could allow even finer-grained adaptation (imagine a robot that adjusts stiffness based on real-time tactile feedback, achieving human-like delicacy). Additionally, applying DexCtrl to more complex multi-step tasks or to legged robots and other domains could test the limits of its generality. If successful, these advances could bring us closer to robots that can reliably perform intricate tasks in unpredictable real-world environments â€“ a step change toward truly <strong>robust, real-world dexterous manipulation</strong>.</p>
<p>Overall, <em>DexCtrl: Towards Sim-to-Real Dexterity with Adaptive Controller Learning</em> demonstrates that bridging the sim-to-real gap is not only about better simulators or more data, but also about smarter control strategies. By <strong>learning to adapt</strong> at the control level, robots can achieve a new level of dexterity and reliability, which is an exciting prospect for the field of robotics and its real-world applications.</p>
<p><strong>Sources:</strong> The analysis and results discussed are based on the paper by Zhao <em>et al.</em> (2025) and the references therein.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "î§‹";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<script src="https://utteranc.es/client.js" repo="curieuxjy/blog_comments" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Copyright 2024, Jung Yeon Lee</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/curieuxjy">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>