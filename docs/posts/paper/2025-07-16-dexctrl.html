<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-07-16">
<meta name="description" content="Towards Sim-to-Real Dexterity with Adaptive Controller Learning">

<title>📃DexCtrl 리뷰 – Curieux.JY</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../profile.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-8ef56b68f8fa1e9d2ba328e99e439f80.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-ecf89aac047581c664da7ae53d704519.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-b009f778f5cec7f34f624408a2b5b543.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-ecf89aac047581c664da7ae53d704519.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-2NVZN2MJZT"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-2NVZN2MJZT', { 'anonymize_ip': true});
</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Curieux.JY</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../post.html"> 
<span class="menu-text">Post</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../note.html"> 
<span class="menu-text">Note</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">Jung Yeon Lee</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/curieuxjy"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#brief-review" id="toc-brief-review" class="nav-link active" data-scroll-target="#brief-review"><span class="header-section-number">1</span> Brief Review</a></li>
  <li><a href="#detail-review" id="toc-detail-review" class="nav-link" data-scroll-target="#detail-review"><span class="header-section-number">2</span> Detail Review</a></li>
  <li><a href="#개요" id="toc-개요" class="nav-link" data-scroll-target="#개요"><span class="header-section-number">3</span> 개요</a>
  <ul class="collapse">
  <li><a href="#방법론" id="toc-방법론" class="nav-link" data-scroll-target="#방법론"><span class="header-section-number">3.1</span> 방법론</a></li>
  <li><a href="#실험-결과" id="toc-실험-결과" class="nav-link" data-scroll-target="#실험-결과"><span class="header-section-number">3.2</span> 실험 결과</a></li>
  <li><a href="#결론" id="toc-결론" class="nav-link" data-scroll-target="#결론"><span class="header-section-number">3.3</span> 결론</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-page-left" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">📃DexCtrl 리뷰</h1>
  <div class="quarto-categories">
    <div class="quarto-category">sim2real</div>
    <div class="quarto-category">adaptive</div>
    <div class="quarto-category">rl</div>
    <div class="quarto-category">hand</div>
  </div>
  </div>

<div>
  <div class="description">
    Towards Sim-to-Real Dexterity with Adaptive Controller Learning
  </div>
</div>


<div class="quarto-title-meta column-page-left">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">July 16, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<ul>
<li><a href="https://arxiv.org/pdf/2505.00991">Paper Link</a></li>
</ul>
<ol type="1">
<li>🤖 Dexterous manipulation의 sim-to-real transfer 문제는 저수준 controller dynamic 불일치로 인해 발생하며, 기존 방법은 manual tuning이나 randomization에 의존했습니다.</li>
<li>🧠 본 논문은 historical information을 활용하여 action과 controller parameters를 동시에 학습하는 adaptive controller 학습 프레임워크인 DexCtrl을 제안합니다.</li>
<li>🚀 DexCtrl은 실행 중 자동으로 control parameters를 조정하여 sim-to-real gap을 크게 줄이고 contact-rich dexterous task에서 우수한 real-world 성능을 달성합니다.</li>
</ol>
<center>
<img src="../../images/2025-07-16-dexctrl/1.png" alt="Overview" width="100%">
<figcaption>
Overview
</figcaption>
</center>
<hr>
<section id="brief-review" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Brief Review</h1>
<p>DexCtrl은 시뮬레이션(sim)에서 훈련된 로봇 제어 정책을 실제(real) 환경으로 이전할 때 발생하는 난제, 특히 저수준(low-level) 제어기 동역학 불일치 문제를 해결하는 데 중점을 둔 연구입니다. 기존 접근 방식들은 수동 튜닝(manual tuning)이나 제어기 무작위화(controller randomization)에 의존했는데, 이는 노동 집약적이고 특정 작업에만 유효하며 학습 난이도를 높이는 단점이 있었습니다. 본 논문은 이러한 한계를 극복하기 위해 동작(action)과 제어 매개변수(controller parameter)를 동시에 학습하는 새로운 프레임워크인 DexCtrl을 제안합니다.</p>
<p><strong>핵심 문제 식별:</strong></p>
<p>연구는 시뮬레이션-실제 불일치(sim-to-real gap)의 중요한 원인 중 하나로 로봇 제어기 간의 차이를 지목합니다. <strong>동일한 궤적(trajectory)이라도 제어 매개변수가 다르면 실제 접촉력(contact force)과 동작이 크게 달라질 수 있기 때문입니다.</strong> 기존의 방식은 제어 매개변수를 고정하거나 훈련 시 무작위화하여 robustness를 높이려 했지만, 이는 실질적인 문제 해결에 한계가 있었습니다.</p>
<p><strong>DexCtrl의 방법론:</strong></p>
<p>DexCtrl은 과거의 궤적 정보와 제어기 정보를 기반으로 동작과 제어 매개변수를 동시에 예측하고 적응적으로 조정하는 메커니즘을 사용합니다. 이를 통해 정책이 실행 중에 제어 매개변수를 자동으로 튜닝하여 시뮬레이션-실제 간극을 완화하고, 접촉력 상호작용에 대한 추론을 개선하여 실제 시나리오에서 로버스트니스를 향상시킵니다.</p>
<p><strong>핵심 방법론 세부 사항:</strong></p>
<ol type="1">
<li><strong>데이터 수집을 위한 Oracle Policy:</strong>
<ul>
<li>DexCtrl은 먼저 시뮬레이션 환경에서 다양한 물체 물리 매개변수(object physical parameters)를 사용하여 훈련된 “오라클 정책(oracle policy)”으로부터 충분한 데이터를 수집합니다.</li>
<li>오라클 정책은 모델-프리(model-free) 강화 학습(Reinforcement Learning) 기법인 <strong>PPO(Proximal Policy Optimization)</strong>를 사용하여 학습됩니다.</li>
<li>각 시간 단계 <span class="math inline">t</span>에서 오라클 정책 <span class="math inline">\pi(a_t, K_t | s_t)</span>는 현재 상태 <span class="math inline">s_t</span>를 입력받아 조인트 동작(joint action) <span class="math inline">a_t</span>와 제어 매개변수 <span class="math inline">K_t</span>를 동시에 출력합니다. 여기서 <span class="math inline">a_t</span>는 <span class="math inline">t</span> 시점의 조인트 위치 변화량을 나타내며, 원하는 조인트 궤적은 이전 desired joint position에 <span class="math inline">a_t</span>를 더하여 <span class="math inline">q^d_t = q^d_{t-1} + a_t</span>로 얻어집니다. desired joint velocity는 0으로 설정됩니다.</li>
<li>로봇 제어는 조인트 토크 제어(joint torque control) 방식을 따르며, 토크 <span class="math inline">\tau</span>는 다음과 같이 계산됩니다: <span class="math display">\tau = K_P (q_d - q_c) + K_D (\dot{q}_d - \dot{q}_c)</span> 여기서 <span class="math inline">q_d</span>와 <span class="math inline">q_c</span>는 각각 원하는(desired) 조인트 위치와 현재(current) 조인트 위치를 나타내며, <span class="math inline">\dot{q}_d</span>와 <span class="math inline">\dot{q}_c</span>는 각각 원하는 조인트 속도와 현재 조인트 속도를 나타냅니다. <span class="math inline">K_P</span>와 <span class="math inline">K_D</span>는 강성(stiffness) 및 감쇠(damping) 행렬을 나타내는 제어 매개변수이며, 간단함을 위해 대각 행렬(diagonal matrices)로 가정합니다. <span class="math inline">K = \{K_P, K_D\}</span>는 전체 제어 매개변수 집합입니다.</li>
<li><strong>상태(State) 구성:</strong> 상태 <span class="math inline">s_t \in \mathbb{R}^{219}</span>는 지난 세 단계의 물체 및 로봇 관측 정보를 포함합니다.
<ul>
<li>로봇 정보 <span class="math inline">s^r_t \in \mathbb{R}^{64}</span>: 현재 조인트 위치 <span class="math inline">q^c_t</span>, 원하는 조인트 위치 <span class="math inline">q^d_t</span>, 제어 매개변수 <span class="math inline">K_t</span>.</li>
<li>물체 정보 <span class="math inline">s^{obj}_t \in \mathbb{R}^9</span>: 물체 자세(pose) <span class="math inline">p^{obj}_t \in \mathbb{R}^6</span>, 물체 속성 벡터 <span class="math inline">\mu \in \mathbb{R}^3</span> (스케일, 질량, 마찰). <span class="math display">s_t \triangleq (s^r_{t-2:t}, s^{obj}_{t-2:t})</span> <span class="math display">s^r_t \triangleq (q^c_t, q^d_t, K_t)</span> <span class="math display">s^{obj}_t \triangleq (p^{obj}_t, \mu)</span></li>
</ul></li>
<li><strong>보상(Reward) 함수:</strong> 보상 <span class="math inline">r_t</span>는 주로 네 가지 부분으로 구성됩니다: <span class="math display">r_t = r_{rotation} + r_{contact} + r_{smoothness} + r_{terminate}</span></li>
</ul></li>
<li><strong>동작 예측 및 제어 매개변수 예측 모듈:</strong>
<ul>
<li>오라클 정책이 수집한 데이터를 사용하여, DexCtrl은 학생 정책(student policy)을 두 개의 분리된 모듈로 훈련합니다: 동작 예측 모듈(Action Prediction Module)과 제어 매개변수 예측 모듈(Control Parameters Prediction Module). 이는 각 모듈이 태스크의 근본적으로 다른 측면을 인코딩하며, 제어 매개변수 예측이 동작 예측에 영향을 미치지 않도록 하기 위함입니다.</li>
<li><strong>Historical Information 사용:</strong> 오라클 정책은 실제 환경에서 직접 접근하기 어려운 물체 속성 같은 원시 정보(primitive information)를 사용합니다. 이를 해결하기 위해 DexCtrl은 로봇의 고유수용성(proprioception) 이력 상태(last ten steps의 현재 및 원하는 조인트 궤적, 해당 제어 매개변수)를 학생 정책의 입력으로 사용합니다.</li>
<li><strong>모듈 설계:</strong>
<ul>
<li><strong>동작 예측 모듈:</strong> temporal historical input을 모델링하기 위해 self-attention 메커니즘을 사용합니다. 이는 조인트 궤적 변화의 경향(trend)을 파악합니다.</li>
<li><strong>제어 매개변수 예측 모듈:</strong> cross-attention을 사용하며, 현재 동작(current action)이 쿼리(query) 역할을 하고 historical input가 키(key)와 값(value) 역할을 합니다. 이는 현재 조인트 동작과 이력 정보 간의 관계를 모델링하여 제어 매개변수를 추론하는 데 활용됩니다.</li>
</ul></li>
<li><strong>훈련 및 추론:</strong>
<ul>
<li><strong>훈련:</strong> 두 모듈은 개방 루프(open-loop) 방식으로 훈련됩니다. 즉, 모든 입력 데이터는 수집된 시뮬레이션 데이터셋에서 직접 가져옵니다.</li>
<li><strong>추론:</strong> 시뮬레이션과 실제 환경 모두에서 폐쇄 루프(closed-loop) 방식으로 수행됩니다. 즉, 현재 궤적 값은 실제 로봇 센서로부터 얻어집니다.</li>
<li>시뮬레이션에서 실제 시스템으로 제어 매개변수를 선형적으로 매핑하며, 이는 대략적인 상한 및 하한 추정치만으로도 충분합니다.</li>
<li>학생 정책 훈련 중 현재 궤적 값에 가우시안 노이즈(Gaussian noise)를 추가하는 것이 시뮬레이션-실제 전이(sim-to-real transfer)에 충분하다는 것을 발견했습니다.</li>
</ul></li>
</ul></li>
</ol>
<p><strong>실험 및 결과:</strong> 두 가지 접촉 기반의 복잡한 조작 작업(in-hand object rotation, flipping)에서 DexCtrl의 성능을 평가했습니다.</p>
<ol type="1">
<li><strong>시뮬레이션 성능 향상:</strong>
<ul>
<li>교란(disturbance) 유무에 관계없이 시뮬레이션 환경에서 DexCtrl은 기존의 “Manual Tuning” 및 “Ours w/o PD” (제어 매개변수 예측 모듈이 없는 버전) 베이스라인 대비 현저히 우수한 성능을 보였습니다. 특히, 회전 속도(RotR) 및 실패까지의 시간(TTF)이 크게 향상되었고, 토크 패널티(Torque) 및 물체 선형 속도(ObjVel)는 감소했습니다.</li>
<li>이는 DexCtrl이 제어기 불일치가 없는 상황에서도 동작과 제어 매개변수를 동시에 조정함으로써 작업 프로세스를 효과적으로 안정화하고 가속화할 수 있음을 입증합니다.</li>
</ul></li>
<li><strong>Sim-to-Real Gap 완화:</strong>
<ul>
<li>실제 환경에서 DexCtrl은 제로샷(zero-shot) 시뮬레이션-실제 전이를 통해 베이스라인들을 압도적으로 능가했습니다. 특히, 시뮬레이션에서보다 실제 환경에서 DexCtrl과 “Ours w/o PD” 간의 성능 격차가 훨씬 더 크게 나타났습니다.</li>
<li>이 결과는 실제 로봇에서 매 단계마다 제어 매개변수를 적응적으로 조정하는 것의 중요성을 강조하며, DexCtrl이 시뮬레이션-실제 문제를 해결하는 데 핵심적인 역할을 함을 보여줍니다.</li>
</ul></li>
<li><strong>다양한 물리 매개변수를 가진 물체에 대한 성능:</strong>
<ul>
<li>질량(mass)과 마찰(friction)이 다른 물체에 대한 추가 테스트에서 DexCtrl은 특히 무거운 물체에서 베이스라인보다 훨씬 뛰어난 성능을 보였습니다. 이는 DexCtrl이 다양한 물리적 특성을 가진 물체에 더 잘 적응할 수 있음을 나타냅니다.</li>
</ul></li>
<li><strong>제어 매개변수(특히 강성)의 영향 분석:</strong>
<ul>
<li>학습된 강성(<span class="math inline">K_P</span>)이 물체 질량 및 마찰과 어떻게 관련되는지 분석했습니다. 시뮬레이션 데이터 분석 결과, 강성은 질량과 단조 증가(monotonically increasing) 관계를 보였습니다 (무거운 물체는 더 큰 힘 필요).</li>
<li>마찰과의 관계는 더 미묘하여, 특정 경우에는 증가하고 다른 경우에는 감소하는 등 작업 의존적인 동역학을 나타냈습니다.</li>
<li>실제 환경 분석에서는 무거운 물체에 대해 강성이 특정 시간 단계에서 증가하거나 최대 값으로 더 오래 유지되는 경향을, 부드러운(smoother) 물체에 대해서는 일부 조인트에서 유사한 패턴을, 다른 조인트에서는 상반된 경향을 보였습니다.</li>
<li>이는 학습된 제어 매개변수가 필요한 접촉력의 변화를 인코딩하며, 물체 조작 성능 향상에 기여한다는 가설을 검증합니다.</li>
</ul></li>
</ol>
<p><strong>결론 및 한계:</strong></p>
<ul>
<li>DexCtrl은 이력 정보를 기반으로 동작과 제어 매개변수를 동시에 출력하여 민첩한 조작(dexterous manipulation)의 시뮬레이션-실제 간극을 효과적으로 줄였습니다.</li>
<li>향후 연구에서는 여러 민첩한 작업이 단일 제어 매개변수 예측 모듈을 공유하거나, 하드웨어 지원 시 실시간 힘 피드백(force feedback) 기반의 온라인 미세 조정(fine-tuning)을 수행할 계획입니다.</li>
<li>현재 방법의 한계는 하드웨어 제약으로 인해 실제 힘 또는 촉각 센서(tactile sensing)를 통합하지 못하고, LeapHand 플랫폼에 한정된 실제 환경 평가에 있습니다.</li>
</ul>
<hr>
</section>
<section id="detail-review" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Detail Review</h1>
</section>
<section id="개요" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> 개요</h1>
<p>로봇의 <strong>섬세한 조작(dexterous manipulation)</strong> 기술은 최근 시뮬레이션 환경에서 큰 진전을 보였지만, 이를 실제 로봇에 옮길 때 <strong>시뮬레이션-현실 간의 차이(sim-to-real gap)</strong>로 인해 성능 저하가 발생하는 문제가 남아 있습니다. 특히 저수준 제어기(dynamics controller)의 차이가 한 원인인데, 시뮬레이터와 현실 로봇에서 <strong>PD 제어기</strong>(비례-미분 제어) 파라미터가 불일치하면 동일한 모션이라도 접촉 시 힘의 상호작용이 크게 달라져 예상 밖의 거동을 초래합니다. 기존에는 이 문제를 해결하기 위해 사람이 직접 <strong>PD 게인을 조정(수동 튜닝)</strong>하거나 학습 시 <strong>무작위화(domain randomization)</strong>를 통해 다양한 제어 동특성에 정책을 노출시키는 방법이 흔했습니다. 하지만 이러한 접근은 작업별로 번거로운 튜닝이 필요하고, 지나친 랜덤화는 학습 난이도를 높여도 성능 향상에 한계가 있었습니다. 한편, <strong>DexPilot</strong>과 같은 원격 조작(teleoperation) 기법은 숙련된 인간 조종사가 로봇 손을 직접 제어함으로써 높은 성공률을 보일 수 있음을 보여주었지만, 이는 어디까지나 사람의 개입에 의존하므로 자율적인 해결책이라 보기 어렵습니다.</p>
<p><strong>DexCtrl</strong>은 이러한 배경에서 제안된 새로운 프레임워크로, <strong>적응형 제어기 학습(adaptive controller learning)</strong>을 통해 시뮬레이션에서 학습한 정책을 현실 세계의 로봇 손에 효과적으로 이전하는 방법을 제시합니다. 핵심 아이디어는 정책이 매 시각 <strong>행동(action)</strong>과 <strong>제어 파라미터(controller parameters)</strong>를 동시에 결정하여, 실행 중에 실시간으로 로봇의 PD 제어 게인을 자동 조절하도록 하는 것입니다. 관측 상태에 현재 사용 중인 제어 파라미터 값을 명시적으로 포함시켜 정책이 접촉 과정의 힘 정보를 추론하게 함으로써, 추가적인 센서 없이도 현실에서의 견고함을 높였습니다. 저자들은 이를 통해 별도의 인간 개입이나 과도한 랜덤화 없이도 시뮬레이터-현실 간 동작 편차를 줄일 수 있음을 보여주었습니다.</p>
<p>이 연구의 주요 기여는 다음과 같습니다: (1) 시뮬레이터와 현실 로봇 <strong>제어기 불일치 문제</strong>를 dexterous manipulation 분야의 핵심 sim-to-real 갭으로 지목하고, 이를 <strong>적응적으로 보정</strong>하는 방법을 처음으로 제시하였으며, (2) <strong>과거 이력 기반</strong>으로 정책이 행동과 제어 파라미터를 동시에 산출하는 단순하고 우아한 프레임워크를 설계하여 다양한 힘 상호작용 변화에 대응할 수 있게 하였고, (3) <strong>접촉이 많은 두 가지 작업</strong>에 대한 광범위한 실험을 통해 시뮬레이션과 실제 모두에서 본 방법이 기존 방법들보다 성능이 크게 향상됨을 정량적으로 입증하였습니다. 아래에서는 DexCtrl의 방법론, 두 가지 baseline 접근법과의 비교, 실험 결과 및 결론을 차례로 살펴보겠습니다.</p>
<section id="방법론" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="방법론"><span class="header-section-number">3.1</span> 방법론</h2>
<p><strong>문제 정의 및 작업</strong> – DexCtrl은 다자유도 로봇 손의 학습된 정책을 현실로 이전할 때 발생하는 <strong>제어기 갭(controller gap)</strong> 문제를 해결하고자 합니다. 검증을 위해 두 가지 대표 작업을 다룹니다. 첫째는 손가락 끝으로 손바닥 위의 물체를 떨어뜨리지 않고 <strong>회전시키는 작업</strong>이고, 둘째는 책상 위에 놓인 물체를 집어 들어 <strong>뒤집는 작업</strong>입니다. 모두 물체-손-환경 간의 복잡한 접촉을 포함하며, 시뮬레이터와 실제 간 제어 파라미터 차이가 성능에 큰 영향을 주는 과제들입니다. 로봇 손은 16자유도의 UC Berkeley <strong>LEAP Hand</strong>를 사용했고, <strong>관절 토크 제어</strong> 방식으로 구동됩니다. 저수준 제어기는 PD 형태로, 원하는 관절 위치 <span class="math inline">q_d</span>와 현재 관절 위치 <span class="math inline">q</span> 간 오차에 대해 <strong>강성 행렬</strong> <span class="math inline">K</span> (스프링 상수에 해당)과 <strong>감쇠 행렬</strong> <span class="math inline">D</span> (댐핑 계수)에 비례하는 토크를 출력합니다. 즉, 각 관절에 <span class="math inline">\tau = K (q_d - q) + D (\dot{q}_d - \dot{q})</span>의 토크를 가합니다. 이때 제어 <strong>파라미터</strong> <span class="math inline">c={K,D}</span>의 선택이 로봇 거동을 크게 좌우하므로 세심한 튜닝이 필요합니다. 예를 들어 <strong>강성</strong> <span class="math inline">K</span>를 높이면 정상 상태 오차는 줄지만 진동이 유발될 수 있고, <strong>감쇠</strong> <span class="math inline">D</span>를 높이면 오버슈트(overshoot)를 억제하나 고주파 진동을 증폭시킬 수 있습니다. 따라서 기존 정책들은 일반적으로 고정된 <span class="math inline">K,D</span>로만 동작하고 (적응 제어가 없는 경우), 이 값 세트를 시뮬레이터와 실제에서 맞추기 위해 많은 노력이나 운에 맡긴 랜덤화가 필요했습니다. <strong>DexCtrl</strong>은 바로 이 부분을 돌파하기 위해 <strong>정책이 자체적으로 제어 파라미터를 조정</strong>하도록 합니다. 한마디로, <strong>“정책이 행동만이 아니라 제어기 설정까지 함께 결정하면 어떨까?”</strong>라는 물음에 대한 해결책입니다.</p>
<p><strong>DexCtrl의 학습 과정</strong> – DexCtrl의 학습은 크게 <strong>교사(oracle) 정책</strong> 단계와 <strong>학생(student) 정책</strong> 단계의 두 부분으로 이루어집니다. 먼저 시뮬레이터 상에서 충분한 데이터를 수집하기 위해, <strong>모델 자유 강화학습 (PPO)</strong> 알고리즘으로 교사 정책을 훈련합니다. 이 교사 정책은 매 시간-step마다 상태를 받아 <strong>관절 행동 <span class="math inline">a_t</span></strong> (예: 다음 목표 관절 위치)와 <strong>제어 파라미터 <span class="math inline">c_t</span></strong> (예: 해당 step에서 사용할 <span class="math inline">K,D</span> 값)을 <strong>동시에 출력</strong>합니다. 곧바로 <span class="math inline">a_t</span>는 <span class="math inline">c_t</span>를 파라미터로 하는 PD 제어기에 입력되어 로봇 손가락들의 토크로 변환되고, 로봇을 움직이게 합니다. (이때 <span class="math inline">a_t</span>는 목표 joint 각도나 상대 위치 등의 형식이며, DexCtrl과 기존 정책 모두 관절 <strong>목표 속도</strong>는 0으로 설정합니다.) 이렇게 하면 교사 정책은 시뮬레이션 내에서 <strong>동시에 최적의 동작궤적과 제어기 게인 조합</strong>을 찾아내도록 학습됩니다. 다만 교사 정책은 시뮬레이터의 이점을 활용해 <strong>물체의 정확한 상태와 물리 속성(질량, 마찰 등)</strong>까지 관측으로 사용합니다. 예컨대 상태 <span class="math inline">s_t</span>에는 최근 3 스텝에 걸친 로봇 관절 정보 (현재 위치 <span class="math inline">q</span>, 목표 위치 <span class="math inline">q_d</span>, 사용한 <span class="math inline">K,D</span>)와 물체의 포즈 및 <strong>물체 속성 벡터</strong> (크기, 무게, 마찰 계수) 등이 포함됩니다. 이러한 <strong>특권 정보(privileged info)</strong>는 현실에서는 알기 어려우므로, 다음 단계에서 이를 제거하는 작업이 필요합니다.</p>
<p>학생 정책 학습을 위해, 우선 앞서 훈련된 교사 정책으로 <strong>여러 물체에 대한 시뮬레이션 데이터를 충분히 수집</strong>합니다. 그런 다음 이 데이터를 활용해 학생 정책을 <strong>모사 학습(distillation)</strong> 방식으로 훈련하는데, 학생 정책은 <strong>두 개의 서브-모델</strong>로 구성됩니다. 하나는 <strong>행동 예측 모듈</strong>로서 로봇의 다음 목표 joint 움직임을 생성하고, 다른 하나는 <strong>제어 파라미터 예측 모듈</strong>로서 해당 step에 사용할 적절한 <span class="math inline">K,D</span> 값을 출력합니다. 두 모듈 모두 입력으로 <strong>과거의 이력 정보</strong>를 사용합니다. 구체적으로, 최근 <strong>10 스텝분의 로봇 관절 상태 기록</strong>을 활용하는데, 각 스텝마다 로봇의 실제 joint 각도, 그때의 목표 joint 각도, 그리고 그때 사용된 제어 파라미터(<span class="math inline">K,D</span>)를 묶어 시계열로 제공합니다. 물체의 속성이나 주변 환경 정보는 직접 주어지지 않지만, 이러한 <strong>프로프리오셉션 이력</strong> 속에 간접적으로 녹아들어 있다는 가정입니다. 예를 들어 무거운 물체를 들고 있었다면 과거 10 스텝 동안 모터 토크 사용량과 운동 패턴에 그 정보가 반영될 것이고, 미끄러운 물체라면 작은 마찰로 인해 나타난 미세한 움직임 차이가 이력에 남을 것입니다. DexCtrl 학생 정책은 이렇게 <strong>오로지 로봇 자체 센서 데이터의 시간 이력만으로</strong> 물체의 특성까지 추측하며 행동을 결정하도록 설계되었습니다. 이는 현실 환경으로 정책을 옮길 때 추가 센서 없이도 동작 가능하게 하는 중요한 설계입니다. 실제로 저자들은 이전 연구의 결과를 인용하여, 회전 작업 등의 경우 과거 관절 정보만으로도 회전된 물체의 방향 등을 추정할 수 있음을 언급합니다.</p>
<p>두 모듈의 <strong>구조</strong>는 시계열 정보를 효과적으로 처리하기 위해 <strong>어텐션 메커니즘</strong>을 활용합니다. 행동 예측 모듈은 과거 10개의 상태 시퀀스를 입력으로 <strong>자기-어텐션(self-attention)</strong>을 수행하여, 관절 움직임의 시간적 추이를 파악하고 다음 목표 joint 위치를 출력합니다. 반면 제어 파라미터 예측 모듈은 <strong>교차-어텐션(cross-attention)</strong> 구조를 갖는데, 이때 <strong>현재 생성된 행동</strong> <span class="math inline">a_t</span>를 쿼리(query)로 삼고, 과거 이력(최근 10 스텝 로봇 상태)을 키/값 (key/value)으로 삼아, 현 행동에 적합한 <span class="math inline">K,D</span> 게인을 산출합니다. 이렇게 함으로써, 현재 로봇이 수행하려는 동작과 과거의 움직임-제어 맥락을 연관지어 “이번 동작을 안정적으로 수행하려면 제어기를 얼마나 단단하게 혹은 부드럽게 설정해야 하는가?”를 학습하는 것입니다. 저자들은 두 모듈의 입력 이력은 같지만 의미가 다르다고 설명합니다: 행동 모듈은 <strong>이력으로부터 궤적의 추세</strong>를 읽어내고 (이전 연구들과 유사), 제어 모듈은 <strong>이력으로부터 이전 동작-제어기 간 관계</strong>를 학습하여 인간이 과거 운전 감으로 현재 차량 제어를 조절하듯이 현재 필요한 제어기 설정을 유추한다고 비유합니다. 또한 모듈을 분리함으로써 <strong>제어 파라미터 학습이 행동 생성에 간섭하는 것을 방지</strong>하여 학습을 안정화할 수 있었다고 합니다. (만약 하나의 거대한 네트워크가 두 출력을 모두 학습한다면, 제어 파라미터 출력이 제대로 예측되지 못하면 행동 출력까지 혼란을 줄 수 있는데, 이를 구조적으로 차단한 것이죠.)</p>
<p>학생 정책의 <strong>훈련 과정</strong>은 시뮬레이터 데이터셋을 활용한 <strong>오프라인 지도학습(behavior cloning)</strong> 형태로 진행됩니다. 수집한 데이터의 각 시간-step에서 교사 정책이 산출한 <span class="math inline">a_t, c_t</span>를 목표로, 학생 정책의 두 모듈을 학습시킵니다. 이때 입력으로 사용되는 과거 이력 값들은 <strong>교사 정책 데이터셋의 값들을 그대로 사용</strong>하므로, 훈련 자체는 오픈-루프(open-loop)로 수행됩니다. (즉, 학생이 내놓은 출력으로 다음 상태를 시뮬레이션하지 않고, 모두 기록된 데이터로만 학습). 대신 학습 후 <strong>실제 실행(추론)</strong> 시에는 매 시각 실제 로봇의 센서에서 받아온 현재 상태를 이력에 추가하여 <strong>폐루프(closed-loop)</strong>로 동작합니다. 시뮬레이터와 현실 로봇 사이의 제어 파라미터 단위 차이가 있을 경우를 대비해, 미리 실험적으로 <strong>제어기 파라미터 값의 상하한을 비교하여 선형 스케일링</strong>해주는 정도의 이식 작업만 거쳤다고 합니다. 별도로 현실에서 미세 튜닝을 하지 않고도 잘 동작하였는데, 이는 교사 정책 학습 시에 무리한 랜덤노이즈 주입 없이도, 학생 정책 훈련 시 현재 관절값에 약간의 <strong>가우시안 노이즈</strong>만 더하는 것으로 충분했기 때문이라고 저자는 설명합니다. 이러한 결과는, <strong>과도한 도메인 랜덤화 없이도 sim-to-real 이전이 가능함</strong>을 보여주는 동시에, <strong>제어기 파라미터 관측</strong>을 제공하는 DexCtrl 접근의 이점을 부각합니다. 정리하면, DexCtrl은 <strong>시뮬레이터에서 학습한 전문가 정책</strong>을 데이터로 활용하여 <strong>이력 기반 학생 정책</strong>을 만들고, 이 학생 정책이 <strong>실행 시 매 순간 행동과 제어기를 모두 조절</strong>하도록 함으로써, 시뮬레이션과 현실 환경 사이의 미묘한 차이를 자동으로 메워주는 것입니다.</p>
<p><strong>Baseline 접근법과의 비교</strong> – DexCtrl과 대비되는 두 가지 주요 baseline은 다음과 같습니다. 첫째, <strong>수동 튜닝(Manual Tuning)</strong> 방식입니다. 이는 사람이 많은 시행착오를 거쳐 시뮬레이터와 실제 로봇의 동작 궤적을 비교해가며 <strong>PD 제어 게인을 조정</strong>하고, 추가로 학습 시 제어 파라미터에 약간의 랜덤 변동을 줘서 얻은 정책입니다. 이렇게 교사 정책부터 현실에 맞춰 새로 학습한 후 학생 정책을 만들어 사용하지만, <strong>정책 자체는 적응형이 아니어서</strong> 실행 중에는 고정된 제어 파라미터만을 사용합니다. 이 접근은 특정 작업에 대해 사람이 <strong>노하우로 게인을 맞추는</strong> 것이기에 일반화가 어렵고, 접촉 조건이 조금만 바뀌어도 성능 보장이 안 되는 문제가 있습니다. 둘째 baseline은 <strong>“Ours w/o PD”</strong>, 즉 <strong>비적응형 DexCtrl</strong>입니다. 이는 DexCtrl 학생 정책에서 <strong>제어 파라미터 예측 모듈을 제거</strong>하고, 제어 파라미터를 위 수동 튜닝과 동일한 값으로 <strong>고정</strong>시킨 버전입니다. 대신 행동 예측 모듈만을 DexCtrl 방식으로 학습하여, <strong>강인한 행동 궤적</strong>은 얻되 제어 파라미터는 조정하지 않는 구조입니다. 이 baseline은 “적응형 PD” 기능이 없는 DexCtrl로 볼 수 있어, 제어기 자동 조정의 효과를 검증하는 용도로 사용됩니다.</p>
<p>DexCtrl의 방법론은 위 baseline들과 근본적으로 접근이 다릅니다. <strong>수동 튜닝</strong>의 경우 사람 전문가의 경험에 의존해 게인을 맞추다 보니 반복 작업과 시행착오가 많고, 한 번 튜닝한 값을 실행 중 동적으로 바꿀 순 없습니다. <strong>학습 시 랜덤화</strong> 기법은 정책이 다양한 상황을 버티도록 해주지만, 정작 중요한 특정 힘 조건에서는 최적의 대응을 배우지 못하고 평균적인 (소극적인) 전략에 머물기 쉽습니다. 반면 <strong>DexCtrl</strong>은 정책이 <strong>스스로 필요한 순간에 제어기 특성을 변화</strong>시키도록 학습되었기 때문에, 처음 접하는 상황에서도 즉각적으로 대응할 수 있습니다. 또한 관측에 제어 파라미터를 포함시켜 <strong>정책이 현재 힘 전달 특성을 인지</strong>하도록 했다는 점도 차별점입니다. 한편, DexCtrl과 문제 접근방식은 다르지만 관련된 맥락으로 <strong>DexPilot</strong>을 언급할 수 있습니다. DexPilot은 사람의 손 동작을 비전 기반으로 추적하여 로봇 손을 원격 조작함으로써 여러 정교한 작업을 수행해낸 체계로, <strong>인간 조종사의 직관과 적응력</strong>을 로봇 제어에 활용한 사례입니다. DexPilot 시스템을 통해 얻은 시연 데이터로 향후 자율 정책 학습에 활용하는 방안도 제시되었지만, 실제 조작 시에는 어디까지나 숙련된 사람의 판단과 시각 피드백에 의존합니다. <strong>DexCtrl은 이러한 인간의 역할을 정책이 대체하도록 함으로써</strong>, 별도의 촉각 센서 없이도 힘 조절을 배우게 했다는 점에서 자율성과 실용성을 높였습니다. 마지막으로, <strong>적응형 힘 제어(adaptive force control)</strong> 자체는 로봇 제어 분야에서 익숙한 개념으로, 간단한 접촉 작업(예: 테이블 닦기, 물체 젓히기, 조립 작업 등)에는 기존에도 <strong>상황에 따라 제어 게인을 바꾸는 기법</strong>이 성과를 보인 바 있습니다. 다만 이런 기법이 <strong>다지 손(dexterous hand)</strong>의 고난도 조작에 적용된 적은 거의 없었고, DexCtrl은 <strong>복잡한 다지 조작에서 적응 제어의 효과를 처음으로 입증</strong>했다는 의의를 갖습니다.</p>
</section>
<section id="실험-결과" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="실험-결과"><span class="header-section-number">3.2</span> 실험 결과</h2>
<p><strong>실험 설정</strong> – 저자들은 DexCtrl의 성능을 앞서 언급한 <strong>두 가지 작업(손바닥 위 물체 회전, 테이블 위 물체 뒤집기)</strong>에서 평가하였습니다. 평가 환경은 시뮬레이터와 실제 로봇 모두 사용되었는데, 시뮬레이터에서는 학습과 동일하게 Mujoco 기반 LEAP Hand 모델을 활용했고, 실제로는 동일한 형태의 로봇 손(실제 LEAP Hand)에 학습된 정책을 <strong>zero-shot</strong>으로 이식하여 시험했습니다. 특히 <strong>여러 가지 물체에 대한 일반화 성능</strong>을 보기 위해, 무게와 마찰 계수가 서로 다른 다양한 물체들을 사용했습니다. 또한 <strong>제어 파라미터 조정의 영향</strong>을 세밀히 분석하기 위해, 같은 모양이지만 속을 채워 무게만 바꾼 물체, 표면 재질만 바꿔 마찰계수만 다르게 한 물체 등을 실험에 포함시켰습니다.</p>
<p>비교를 위해 앞서 설명한 두 baseline (Manual Tuning, Ours w/o PD)을 모두 시뮬레이터에서 훈련하고 현실에 이식하여 테스트했습니다. <strong>Manual Tuning</strong>의 경우, 시뮬레이터와 실제 로봇의 <strong>출력 궤적을 맞추기 위해 세밀히 튜닝된 제어기 파라미터</strong>를 사용하고 학습 시에도 소량의 파라미터 랜덤화를 추가한 교사 정책을 새로 훈련한 후, 그로부터 학생 정책을 얻었습니다. 이러한 정책들은 <strong>실행 시 적응형 제어기 출력을 내지 않으므로</strong> DexCtrl과 구조만 다르고 일반적인 방식의 정책이라 할 수 있습니다. <strong>Ours w/o PD</strong>는 DexCtrl의 <strong>제어기 예측 부분만 제외</strong>한 것으로, 교사 정책은 본래 DexCtrl 교사와 동일하지만 학생 정책이 행동 생성 모듈만 학습된 케이스입니다. 따라서 <strong>PD 게인은 고정</strong>값이고 (Manual Tuning과 동일한 값 사용), adaptiveness가 제거된 DexCtrl로 볼 수 있습니다.</p>
<p>평가지표(metrics)로는 네 가지를 사용했습니다. <strong>회전 보상 (RotR)</strong>은 물체를 원하는 축으로 얼마나 빠르게 회전시키는지를 나타내며, 시뮬레이션에선 에피소드 동안의 평균 회전 속도 보상으로, 실제에선 에피소드 종료 시 물체가 회전한 라디안 각도로 측정했습니다. <strong>실패까지 걸린 시간 (TTF, Time to Fail)</strong>은 물체를 손에서 떨어뜨리거나 (회전 작업), 처음 위치에서 너무 벗어나 버리는 (뒤집기 작업) 시점까지 지속된 시간 스텝 수의 평균입니다. 값이 클수록 물체를 오래 안정적으로 다루었음을 의미합니다. 이 외에 <strong>물체의 불규칙 속도 (ObjVel)</strong>나 <strong>토크 페널티</strong> 등도 측정했는데, ObjVel은 물체가 불필요하게 흔들리는 정도(행동 스텝당 평균 속도)로 안정성을 나타내며 시뮬레이터에서만 계산했고, 토크 페널티는 에너지 소모량 관련 지표입니다. 실험은 <strong>시뮬레이션 평가</strong>와 <strong>실제 로봇 평가</strong>로 나뉘며, 시뮬레이션에서는 추가로 물체에 <strong>무작위 외란 힘(disturbance)</strong>을 가하는 상황과 가하지 않는 상황을 비교하여, 예기치 않은 힘 교란에 대한 견고성도 확인했습니다.</p>
<p><strong>시뮬레이션 성능</strong> – 먼저 <strong>시뮬레이터 상</strong>에서 DexCtrl과 baseline들을 비교한 결과, <strong>DexCtrl가 모든 지표에서 우수한 성능</strong>을 보였습니다. Table 1 (회전 작업)과 Table 2 (뒤집기 작업)은 시뮬레이션 환경에서의 정량적 성능을 보여주는데, <strong>DexCtrl (Ours)</strong> 정책은 <strong>수동 튜닝 baseline</strong>보다 <strong>일관되게 높은 회전 속도</strong>(RotR)와 <strong>긴 실패 시간(TTF)</strong>을 기록했습니다. 예를 들어 손바닥 위 물체 회전의 경우, <strong>외란 힘이 주어졌을 때</strong> DexCtrl의 평균 회전 보상은 baseline보다 <strong>약 24%</strong> 높았고(35.05→43.51), 물체를 떨어뜨리기까지 버틴 시간도 늘어났습니다(239.4→255.9). <strong>외란이 없을 때</strong>는 차이가 더 커져, DexCtrl의 회전 속도가 baseline보다 약 <strong>39% 향상</strong>되었습니다(37.64→52.33). 이는 <strong>제어 파라미터가 동일한 조건</strong>에서도 <strong>행동+제어 동시 학습</strong>이 단순 행동 학습보다 효과적으로 작업을 가속화하고 안정화시켰음을 의미합니다. 흥미롭게도 DexCtrl에서 <strong>PD 적응만 뺀 버전(Ours w/o PD)</strong>도 수동 튜닝보다는 높은 성능을 보였는데, 특히 회전 작업에서 baseline보다 빠른 회전(52.33 대비 47.87)을 달성했습니다. 저자들은 <strong>DexCtrl 방식으로 생성된 궤적 자체가 견고</strong>하기 때문이라고 분석합니다. 다만 <strong>접촉이 격렬한 작업인 뒤집기</strong>의 경우, <strong>비적응형</strong> DexCtrl(Ours w/o PD)은 <strong>baseline보다 나은 성능을 내지 못했습니다</strong>. Table 2를 보면, <strong>외란이 있을 때</strong> 뒤집기 작업의 RotR은 수동 튜닝이 <strong>91.07</strong>, Ours w/o PD가 <strong>82.23</strong>로 오히려 낮았습니다. 반면 <strong>DexCtrl은 172.5</strong>로 <strong>두 배 가까이</strong> 높았습니다. 외란이 없을 때도 비슷한 경향이어서, 결과적으로 <strong>뒤집기 같이 난이도 높은 접촉 작업에서는 실시간으로 제어기를 조정해주는 DexCtrl만이 확실한 성능 개선을 이루었다</strong>고 볼 수 있습니다. 이는 <strong>바닥-손의 복합 접촉</strong> 상황에서, 고정 제어기로는 한계가 있지만 adaptive 제어를 통해 힘 분배를 조절하면 훨씬 유리함을 보여줍니다. 추가로, DexCtrl은 <strong>학습 속도</strong> 측면에서도 우위를 보여주었는데, 동일 조건에서 학생 정책을 학습시킬 때 baseline보다 빠르게 수렴하여 효율적이었다고 합니다. 요컨대 시뮬레이터 결과만 놓고 봐도 <strong>DexCtrl은 기존 방법 대비 작업 성공률과 안정성을 크게 향상</strong>시켰습니다.</p>
<p><strong>현실 세계 성능</strong> – 더 중요하게, <strong>학습된 정책들을 실제 로봇 손에 이식</strong>하여 테스트한 결과, <strong>DexCtrl의 우수성은 현실에서 더욱 두드러졌습니다</strong>. 손바닥 위 물체 회전 작업의 경우, 질량과 마찰이 서로 다른 <strong>12종의 물체</strong>에 대해 각 정책을 <strong>한 번도 추가 훈련하지 않고</strong> 바로 적용해 비교했습니다. 그 결과 DexCtrl은 <strong>어떤 물체에 대해서도 일관되게 가장 높은 회전 성능</strong>을 보여주었고, baseline들과의 격차가 시뮬레이터에서보다 <strong>더 크게 벌어졌습니다</strong>. Table 3은 예시로 몇 가지 물체에 대한 수치를 보여주는데, 가벼운 물체(예: 94g 큐브)에서는 수동 튜닝의 RotR이 1.963인 반면 DexCtrl은 9.386으로 <strong>약 4.8배</strong> 높았습니다. 무거운 물체(221g 사과 장난감)에서는 baseline이 1.914인 데 반해 DexCtrl은 9.676으로 <strong>5배 이상</strong> 차이가 났습니다. 거의 모든 물체에서 DexCtrl이 <strong>몇 배씩 빠르게</strong> 회전시켰으며, 이는 <strong>zero-shot sim-to-real</strong> 설정임을 감안할 때 놀라운 일반화 성능입니다. <strong>안정성 측면</strong>에서도 DexCtrl이 뛰어났는데, 실패까지의 시간(TTF)을 보면 DexCtrl은 여러 물체에서 <strong>평균적으로 에피소드 최대 시간(300 스텝)에 육박</strong>했습니다. 이는 DexCtrl이 대부분 물체를 <strong>끝까지 떨어뜨리지 않고</strong> 잘 잡고 있었다는 뜻입니다. 반면 baseline들의 TTF는 조금 낮아서, 예를 들어 무거운 물체의 경우 수동 튜닝은 평균 <strong>243 스텝</strong> 정도에 물체를 놓쳤지만 DexCtrl은 <strong>300 스텝 내내 성공적으로 잡고 회전</strong>시켰습니다. 전체적으로 <strong>현실 환경에서 DexCtrl과 다른 방법들의 격차는 시뮬레이터보다 컸는데</strong>, 이는 시뮬레이터와 실제 사이의 미세한 차이를 DexCtrl만이 적응적으로 보완한 결과입니다. 특히 <strong>DexCtrl과 비적응형 DexCtrl(Ours w/o PD)의 차이</strong>가 현실에서 크게 벌어졌는데, 이는 <strong>매 시각 제어 파라미터를 조정해주는 기능이 실제 로봇에서 매우 중요함</strong>을 강조해줍니다. 즉, 시뮬레이터에서는 둘 다 일정 성능을 냈어도, 실제에서는 Ours w/o PD가 감당 못 하는 힘 변동을 DexCtrl은 실시간 대응하며 극복한 것입니다. 한편, <strong>테이블 위 물체 뒤집기 작업</strong>도 실제로 시험되었고, Figure 3에 그 결과가 시각적으로 제시되었습니다. DexCtrl 정책은 실제에서도 물체를 들어올려 뒤집는 동작을 성공적으로 수행했고, 반복 실험에서 <strong>높은 성공률</strong>을 보였습니다. 바닥과의 충돌이 있는 이 복잡한 상황에서도 adaptive 제어를 통해 손가락의 <strong>충격 흡수와 힘 분배</strong>가 잘 이루어져, baseline에 비해 <strong>부드럽고 안정적인 뒤집기 동작</strong>을 구현했습니다. 이로써 DexCtrl의 접근이 한 가지 작업에 특화된 게 아니라 <strong>다양한 조작 작업에 일반화 가능함</strong>도 확인되었습니다. Figure 4는 실제 로봇으로 여러 가지 물체를 회전시키는 장면들을 보여주는데, DexCtrl 정책이 <strong>물체마다 알맞은 힘으로 잡고 회전</strong>시키는 모습을 볼 수 있습니다. 반면 고정 제어기의 baseline은 어떤 물체는 힘이 부족해 떨어뜨리고, 어떤 경우는 지나치게 세게 조여 불안정한 움직임을 보이는 등 <strong>일관성이 떨어지는 모습</strong>이 관찰되었습니다.</p>
<p><strong>물체 물성 변화에 대한 적응력 분석</strong> – DexCtrl의 뛰어난 성능이 <strong>어떤 원리로 나오는가</strong>를 알아보기 위해, 저자들은 특히 <strong>질량</strong>과 <strong>마찰</strong>이라는 물체 물성 변수에 대한 정책의 반응을 상세히 분석했습니다. 앞서 언급했듯, 동일한 모양의 빈 플라스틱 상자에 내부 내용물을 추가해 <strong>무게를 가볍게/무겁게</strong> 바꾼 경우와, 동일한 무게 상자에 표면 마찰 코팅만 달리한 <strong>마찰 계수 변화</strong> 경우를 실험했습니다.</p>
<p><strong>그림 – 물체의 질량(왼쪽) 및 마찰 계수(오른쪽) 변화에 따른 회전 성능 비교.</strong> 그래프의 세 곡선은 파란색 <strong>수동 튜닝</strong> baseline, 녹색 <strong>비적응형(Ours w/o PD)</strong>, 자주색 <strong>DexCtrl</strong>을 나타냅니다. <strong>왼쪽 그림</strong>은 가벼운 물체부터 무거운 물체로 질량이 증가할 때 세 방법의 회전 속도(RotR)를 보여주고, <strong>오른쪽</strong>은 마찰 계수가 낮은 물체부터 높은 물체로 바뀔 때의 성능 변화를 나타냅니다. 결과를 보면 <strong>DexCtrl (자주색)</strong>은 <strong>모든 조건에서 가장 높은 회전 속도</strong>를 유지하며, <strong>물체가 무거워지거나 마찰이 변화해도 성능 저하 폭이 매우 작다</strong>는 것을 알 수 있습니다. 예를 들어 질량이 가벼울 때는 세 방법 모두 비교적 높은 속도를 내지만, <strong>무거운 물체</strong>로 갈수록 <strong>baseline들의 성능은 급격히 떨어지는 반면 DexCtrl은 상당한 속도를 유지</strong>하고 있습니다. 가장 무거운 경우를 보면, 수동 튜닝(파란선)은 회전 속도가 거의 0에 수렴할 정도로 실패하지만 DexCtrl(자주선)은 여전히 높은 속도로 물체를 돌릴 수 있습니다. <strong>마찰 계수 변화</strong>에 대해서는, DexCtrl은 마찰이 작든 크든 <strong>안정적으로 높은 성능</strong>을 보이는 반면, baseline들은 <strong>중간 마찰 영역</strong> 등에서 성능이 뚝 떨어지는 등 <strong>들쑥날쑥한 양상</strong>을 보입니다. 수동 튜닝(파란선)은 마찰계수가 중간일 때 최저 성능을 내고, 비적응형(녹색)은 마찰이 작을 때 약간 성능이 높았다가 큰 마찰에서 다시 조금 낮아지는 등, 한 가지 추세로 설명하기 어려운 변화를 보입니다. 이는 고정 제어기에서는 <strong>마찰 조건에 따라 상당히 다른 거동을 보여 정책이 일관성 있게 대응하지 못함</strong>을 뜻합니다. 반면 DexCtrl은 <strong>전체적으로 높은 성능을 유지</strong>하며 약간의 변화만 보일 뿐입니다. 종합하면 <strong>DexCtrl 정책은 물체의 물리적 특성이 바뀌어도 알아서 제어 전략을 조정하여 성능을 유지</strong>하는 반면, baseline들은 특정 조건에서 크게 무너지는 모습을 보였습니다. 실제 데이터에서도 DexCtrl이 <strong>무거운 물체일수록 실패 없이 오래 버티고 (TTF 상승)</strong> <strong>가벼운 물체일수록 빠르게 돌리지만 불안정성은 조금 있는</strong>(회전 속도 높지만 ObjVel 약간 상승) 경향 등을 보여, <strong>힘 조절을 통해 트레이드오프를 잘 관리</strong>하고 있음을 알 수 있었습니다. 이는 사람이 무거운 물체를 들 때 힘을 더 주고, 미끄러운 물체를 다룰 때 주의를 기울이는 것과 유사한 적응력이라 할 수 있습니다.</p>
<p><strong>학습된 제어 파라미터의 패턴</strong> – 마지막으로, DexCtrl이 구체적으로 <strong>어떻게 제어기 파라미터를 조절하고 있는지</strong>를 분석한 결과를 소개합니다. 저자들은 특히 제어 파라미터 중 <strong>강성(stiffness, K)</strong> 값에 주목했는데, 이유는 강성이 감쇠보다 작업 성능에 더 큰 영향을 주었기 때문이라고 합니다. 강성을 높이는 것은 곧 각 관절에 더 큰 토크를 가하는 효과를 내어 <strong>접촉 force 프로파일</strong>을 바꾸므로, 물체의 무게나 마찰에 따라 강성 조절 패턴이 달라질 것으로 예측했습니다. 이를 검증하기 위해 시뮬레이터에서 다양한 물체에 DexCtrl 정책을 적용한 trajactory 데이터를 모아 강성 값 변화를 분석했습니다. <strong>Figure 6</strong>의 좌측 그래프는 물체 <strong>질량에 따른 평균 강성 변화</strong>를 보여주는데, 결과는 <strong>물체가 무거워질수록 강성을 높이는 방향</strong>으로 정책이 학습되었음을 명확히 보여줍니다. Light (가벼움) → Heavy (무거움)으로 갈수록 평균 강성이 단조 증가하였는데, 이는 <strong>무거운 물체를 들고 조작하려면 관절에 더 큰 힘(토크)이 필요하므로 강성을 키운다</strong>는 직관적인 원리에 부합합니다. 한편, Figure 6의 중간 및 우측 그래프는 <strong>마찰 계수에 따른 강성 변화</strong>를 나타냅니다. 여기서는 흥미롭게도 <strong>일부 관절은 마찰이 증가할수록 강성이 증가</strong>하지만, <strong>다른 관절은 마찰이 증가할수록 오히려 강성이 감소</strong>하는 양상이 관찰되었습니다. 저자들은 이를 <strong>작업 및 관절별 역학(dynamic)의 차이</strong>로 해석했습니다. 예를 들어 회전 작업에서 <strong>마찰이 큰 경우</strong> 어떤 관절들(예: 물체를 직접 비틀어 돌리는 역할의 관절)에는 움직임에 저항이 커지므로 더 큰 토크, 즉 높은 강성이 필요할 수 있습니다. 반면 <strong>다른 관절들</strong>(예: 물체를 지지하는 역할의 관절)에서는 마찰이 클 때 오히려 물체가 잘 미끄러지지 않아 조금 힘을 빼도 제어가 가능하고, 너무 강하게 잡으면 불필요한 힘 전달로 불안정해질 수 있습니다. 따라서 <strong>관절마다 마찰이 미치는 역할이 달라</strong>서, 어떤 관절은 마찰↑ → 강성↑, 다른 관절은 마찰↑ → 강성↓ 패턴을 보인다는 것입니다. 이러한 <strong>비일관적인 강성 조정 양상</strong>은 처음엔 이상해 보일 수 있으나, <strong>DexCtrl 정책이 관절별로 최적의 힘 분배를 학습한 결과</strong>라고 이해할 수 있습니다.</p>
<p>위와 같은 경향은 <strong>실제 로봇 실험 데이터</strong>를 통해서도 확인되었습니다. Figure 7에서는 서로 다른 물체들에 대해 시간에 따른 강성 변화를 시각화하였는데, 두 가지 뚜렷한 패턴이 발견됩니다. (1) <strong>무거운 물체의 경우</strong> 몇몇 중요한 관절에서 <strong>초기에 강성을 확 올리거나 최대값으로 유지하는 구간이 길어졌다</strong>는 점, (2) <strong>표면이 매끄러운(낮은 마찰) 물체의 경우</strong> 어떤 관절들은 강성을 크게 주었다가 곧 낮추는 반면, 다른 관절들은 반대로 천천히 높이는 등 <strong>엇갈린 패턴</strong>이 보였다는 점입니다. 이 역시 <strong>관절/작업별로 필요한 힘 조절 전략이 다름</strong>을 반영하며, 종합하면 DexCtrl의 정책이 <strong>물체의 무게와 마찰에 따라 필요 접촉력을 판단하고 그에 맞게 강성을 체계적으로 조절</strong>하고 있음을 증명합니다. 이러한 <strong>학습된 강성 조절</strong>은 결과적으로 각 상황에서 <strong>최적의 힘으로 물체를 다루게 해 주어 조작 성능 향상으로 이어진다</strong>고 저자들은 결론짓고 있습니다. 요컨대 DexCtrl의 내부를 들여다본 결과, <strong>“무거운 물체는 세게 잡아라, 마찰은 상황에 맞게 힘을 재분배하라”</strong>라는 인간 전문가의 암묵적 지식을 스스로 터득한 셈입니다.</p>
</section>
<section id="결론" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="결론"><span class="header-section-number">3.3</span> 결론</h2>
<p>시뮬레이션에서 학습한 로봇 손의 정교한 조작 기술을 실제 환경에 성공적으로 이식하기 위해서는 넘어야 할 장벽이 많습니다. <strong>DexCtrl</strong>은 그 중에서도 특히 <strong>로봇 제어기의 적절한 파라미터 선택</strong>이라는 문제에 주목하여, 이를 <strong>학습을 통해 자동화한 최초의 시도</strong>입니다. 정책이 <strong>매 순간 제어기 게인을 함께 출력하도록 설계</strong>함으로써, 시뮬레이터와 실제 로봇 사이의 미묘한 동작 차이를 <strong>정책 자체가 메꾸도록</strong> 했습니다. 이 방법은 사람이 일일이 파라미터를 맞추는 번거로움 없이도, <strong>접촉력에 대한 추론과 적응적 힘 조절</strong>을 가능하게 했고, 그 결과 복잡한 물체 조작 작업에서 <strong>획기적인 성능 향상</strong>을 이루었습니다. 시뮬레이션과 현실 실험 모두에서 DexCtrl은 기존 방법들을 크게 능가하는 성공률과 안정성을 보여주었으며, 다양한 물체 물성 변화에도 높은 <strong>범용성(generalizability)</strong>을 입증했습니다. 특히 <strong>zero-shot 현실 적용</strong> 시에도 여러 물체를 문제없이 다루는 모습은, 향후 로봇이 <strong>학습된 기술을 실세계에 직접 활용</strong>하는 데 한 걸음 더 다가선 성과로 평가됩니다.</p>
<p>물론 한계와 향후 과제도 존재합니다. 현재 DexCtrl은 <strong>로봇의 고유 감각(proprioception)</strong> 정보만 활용하고, <strong>촉각/힘 센서</strong>는 사용하지 않았습니다. 하드웨어 한계로 실험에 포함하지 못했지만, 접촉이 많은 실제 시나리오에선 이러한 <strong>실시간 힘 센싱</strong>이 있다면 더욱 정교한 제어가 가능할 것입니다. 미래에는 고성능 촉각 센서를 통합하고, 나아가 하드웨어가 지원된다면 <strong>실시간으로 정책을 미세 조정하는 온라인 학습</strong>까지 도입하여 (예컨대 실제 수행 중 약간의 추가 PPO 파인튜닝 등) <strong>적응 제어를 한층 고도화</strong>할 수 있을 것으로 기대됩니다. 또한 본 연구는 <strong>하나의 작업에 특화된 두 모듈</strong>로 정책을 훈련했는데, 이는 각각의 작업에 별도 학습이 필요함을 의미합니다. 이를 개선하기 위해 여러 가지 조작 작업에 대해 <strong>공용의 제어 파라미터 예측 모듈</strong>을 갖도록 정책을 확장하는 연구도 고려되고 있습니다. 궁극적으로는 <strong>다양한 로봇 손 플랫폼</strong>에도 본 방법을 적용해보는 것이 목표인데, Leap Hand 이외의 다른 손에 일반화되는지도 향후 검증할 계획입니다.</p>
<p><strong>DexCtrl</strong>이 보여준 <strong>적응형 제어 학습</strong> 개념은, 향후 인간 수준의 섬세한 로봇 조작을 실현하는 데 중요한 단서를 제공합니다. 사람은 물체를 다룰 때 힘 조절을 무의식적으로 잘 하는데, 이제 로봇도 학습을 통해 그러한 능력을 갖출 수 있음을 증명한 것입니다. 복잡한 접촉이 존재하는 환경에서도 <strong>안정적이면서 민첩한(dexterous yet robust)</strong> 로봇 제어가 가능함을 보인 DexCtrl의 성과는, 향후 sim-to-real 문제를 넘어 실제 산업 및 일상에서 <strong>다재다능한 로봇 손</strong>을 구현하는 데 기여할 것으로 기대됩니다. 이번 연구는 <strong>강화학습 정책과 고전 제어의 만남</strong>을 창의적으로 이루어낸 사례로서, 로보틱스 연구자들에게 시사하는 바가 크며, 추후 다양한 확장과 활용이 뒤따를 것으로 보입니다.</p>
<p><strong>참고 문헌</strong></p>
<ul>
<li>Shuqi Zhao et al., “DexCtrl: Towards Sim-to-Real Dexterity with Adaptive Controller Learning,” <em>arXiv preprint arXiv:2505.00991</em>, 2025.</li>
<li>Ankur Handa et al., “DexPilot: Vision Based Teleoperation of Dexterous Robotic Hand-Arm System,” <em>ICRA 2020</em>, arXiv:1910.03135.</li>
</ul>
<!--

> DexCtrl: Towards Sim-to-Real Dexterity with Adaptive Controller Learning – A Deep Dive

## Introduction and the Sim-to-Real Challenge in Dexterous Manipulation


Dexterous robotic manipulation has made remarkable strides in simulation, achieving complex feats like in-hand object rotation, tool use, and even piano playing. However, **transferring** these learned manipulation skills **from simulation to real robots** remains notoriously difficult. One critical but often under-addressed culprit is the **low-level controller mismatch** between simulation and reality. In other words, even if a policy learns a perfect motion trajectory in sim, the way a real robot’s motors execute that trajectory (governed by controller parameters like stiffness and damping) can differ, leading to **vastly different forces and outcomes**. Identical motions can succeed in simulation but slip or fail on the real hardware due to these dynamics discrepancies.


Traditionally, researchers have tackled the sim-to-real gap through tedious **manual tuning** of controller gains (e.g. adjusting PD controller stiffness/damping so the real robot’s behavior matches sim) or by massive **domain randomization** of physics parameters during training. Unfortunately, **both approaches have drawbacks**: manual tuning is labor-intensive and often imprecise, while randomizing controller parameters can **significantly increase training difficulty** and still may not guarantee success in the real world. These methods rely on trial-and-error and human intuition rather than a principled solution, and they **demand extensive human effort** in choosing parameter schedules or noise ranges. Clearly, there is a need for a better way to bridge this “controller gap” in a ***robust, automatic*** manner.

**Enter DexCtrl** – a new framework proposed to address this very challenge. DexCtrl explicitly targets the controller mismatch by allowing the policy to **adapt its own low-level controller parameters on the fly**, rather than treating them as fixed or randomly perturbed constants. In essence, DexCtrl lets the robot policy not only decide *what* actions to take, but also *how* to execute those actions in terms of controller stiffness/damping. By doing so, it aims to **mitigate the sim-to-real gap at the source**: the torque-generation level. This deep-dive review will explore **how DexCtrl works**, its key contributions and novelty, and examine in detail the experimental evidence of its effectiveness in both simulation and real-world trials.

## Key Idea: Adaptive Controller Learning with DexCtrl

At its core, **DexCtrl is about jointly learning the control actions *and* the controller parameters** needed to carry out those actions. The key insight is that the policy can be trained to automatically tune its controller gains (like a robot adjusting its “muscle stiffness”) based on what it has felt and done in the recent past. This is achieved by giving the policy **historical context** and letting it output two things at each time step: (1) the next high-level action (e.g. desired joint positions), and (2) the appropriate controller parameters (e.g. stiffness $K$ and damping $D$ for a PD controller) to apply that action. By continuously adjusting these parameters in a closed loop, DexCtrl can compensate for modeling errors or unexpected force interactions *as they occur*, rather than hoping a fixed controller will work for all situations.

Crucially, DexCtrl’s policy **observes the controller state itself**. The current stiffness/damping values are fed into the observation space along with recent trajectory data, so the policy is aware of “how hard” it’s driving the motors at any given time. This enables better reasoning about contact forces – if a task isn’t going as expected, the policy can sense if maybe the controller was too stiff or too compliant, and adjust accordingly. By explicitly including controller parameters in the observation, the agent can directly capture force-related feedback that would otherwise be hidden.

**Figure 1** below illustrates the concept. The top shows a conventional approach where the policy outputs only the action $a_t$ (like desired joint positions) and uses a fixed controller gain $K_{\text{const}}$ for the whole task. This often requires **extensive manual gain tuning** and still might be inaccurate if conditions change. The bottom shows DexCtrl’s approach: the policy outputs both $a_t$ and an adaptive gain $K_t$ at each step, allowing it to tweak the controller behavior continually. This adaptive scheme promises **reduced labor (no manual tuning)** and **better accuracy** in execution.

<center>
<img src="../../images/2025-07-16-dexctrl/2.png" width="100%" />
</center>
<b>
<figcaption>*Conceptual comparison. **Top:** Standard policy outputs action $a_t$ with fixed controller parameters ($K_{\text{const}}$), requiring manual tuning and risking performance mismatch. **Bottom:** DexCtrl outputs both action and adaptive controller parameters ($K_t$) each step, automatically adjusting the “torque controller” to current needs (green check). The right shows dexterous manipulation tasks (in-hand rotation and flipping) where DexCtrl was evaluated.*</figcaption>
</b>

The **novelty** of DexCtrl’s adaptive controller learning lies in this **automatic, time-varying adjustment of control gains as part of the policy**. Previous works in dexterous manipulation sim-to-real mostly ignored this lever – they would train with a fixed or randomly perturbed controller and hope for the best. While adaptive force control has been explored in simpler robotic tasks (like adjusting stiffness for wiping or pivoting tasks), it had **rarely been applied to complex dexterous hand manipulation**. DexCtrl is among the first to show that *learning to adjust control parameters* in tandem with the action policy can yield significant gains for dexterous hands, providing a more **principled approach to closing the sim-to-real gap** at the controller level. In summary, the key contributions of DexCtrl can be highlighted as follows:

* **Identifying Controller Mismatch:** It pinpoints the often-overlooked **mismatch in robot low-level controllers** as a critical factor in sim-to-real failure, and directly tackles it by making control parameters adaptable. This shifts sim-to-real transfer from being a manual tuning art to an automated learning problem.
* **Joint Action-Control Learning:** It introduces a **simple yet effective framework** that jointly learns the high-level actions and the low-level controller gains from historical data. By leveraging recent trajectories and past controller settings, the policy gains adaptivity to variations in force requirements and object dynamics.
* **Improved Performance Across Domains:** Through extensive experiments, DexCtrl demonstrates **significantly better performance than baseline approaches** (manual tuning and non-adaptive policies) both in simulation and in zero-shot real-world tests, across multiple contact-rich tasks. The results include not just task success metrics but also stability and efficiency measures, along with insightful analysis of *why* it works.

Next, we dive into **how DexCtrl is designed and trained**, before examining the experimental evidence in detail.

## DexCtrl Framework and Methodology

### How DexCtrl Works: Overview of the Architecture

The DexCtrl framework consists of a two-stage learning process: first training an *oracle policy* in simulation via reinforcement learning, and then **distilling that experience into two learned modules** – one for action prediction and one for controller parameter prediction. **Figure 2** provides an overview of this pipeline. During an offline training phase (Fig. 2a), a high-performing oracle policy (trained with RL in diverse simulated settings) is used to generate a dataset of trajectories. This dataset contains sequences of states, actions, and the oracle’s chosen controller parameters at each step. Then, DexCtrl trains two separate neural networks (students) on this data via supervised learning: one network learns to predict the next action, and the other learns to predict the next controller parameters, both based on the history. At runtime (Fig. 2b), these two learned modules work together in a closed loop: the action module predicts the desired next joint motion, and the controller module immediately predicts the appropriate stiffness/damping to execute that motion, given what has happened in the recent past.


<center>
<img src="../../images/2025-07-16-dexctrl/3.png" width="100%" />
</center>
<b>
<figcaption>***DexCtrl framework.** (a) *Open-Loop Training:* An oracle policy (trained with PPO in simulation) generates rollouts of states $s_t$ with actions $a_t$ and controller gains $K_t$. A dataset of sequences $(q^c, q^d, a, K)$ is collected (where $q^c, q^d$ are current and desired joint angles). Two student networks are distilled on this data: an **Action Prediction Module** (blue) that learns to output $\hat{a}_t$ (desired joint positions) from historical trajectories, and a **Control Parameters Prediction Module** (purple) that learns to output $\hat{K}*t$ (controller gains) from the history *and* the current intended action. (Losses are mean-squared errors to the oracle’s outputs.) (b) *Closed-Loop Inference:* On the real robot or new simulation runs, the policy observes the actual robot state $q^c_t$ (and a window of past states $q^c*{t-1},…$), feeds the recent history into the action module to get $\hat{a}_t$, then feeds the history plus the proposed action into the control module to get $\hat{K}_t$. The robot’s low-level torque controller then executes, applying torque $\tau_t$ based on the chosen $K_t$ and the error between $q^d_t$ and $q^c_t$. This loop repeats every time step, continually adapting actions and controller parameters.*</figcaption>
</b>

Under the hood, DexCtrl’s observation at each decision step contains a **window of historical information**. In the implementation, the authors use the past 10 time steps of robot proprioceptive data – specifically, the sequences of **actual joint positions $q^c$**, the **previous desired joint positions $q^d$**, and the **controller parameters $K$ (stiffness, damping)** applied at those steps. This rich history serves as the input for both student networks.

To effectively extract insights from this temporal data, DexCtrl employs an **attention-based architecture**. The action prediction module uses a **self-attention mechanism** over the time window of past states, allowing it to learn temporal patterns and trends in the trajectory (for example, to estimate object motion and infer what the next goal should be). In contrast, the controller prediction module uses a **cross-attention mechanism**: it takes the current planned action as a query and attends to the historical states as keys/values. Intuitively, this helps the controller module learn the relationship between *what action you’re about to do* and *what controller settings worked well in the past in similar situations*. The authors note that while both modules see the same form of input (the last 10 steps of $q^c, q^d, K$), their roles are different – the action module is understanding the motion trajectory context, whereas the controller module is learning how past actions and gains led to outcomes, analogous to how a human might adjust their grip stiffness based on recent success or slip events.

It’s worth emphasizing that **DexCtrl decouples the learning of actions and controller parameters** into two separate prediction modules on purpose. This design ensures that the task of choosing the best motor commands isn’t entangled with the task of tuning gains – which could otherwise make learning unstable. By training them separately (but on the same experiences), DexCtrl can achieve adaptive control without sacrificing the quality of the action policy.

### Training and Deployment Strategy

The training of DexCtrl’s neural modules is done in an **open-loop, supervised fashion** using the logged simulation data. That is, during training they directly feed in historical sequences from the oracle’s trajectories and train the networks to predict the oracle’s next action and gain, step by step. This avoids any compounding error during training and is effectively a form of **behavioral cloning/distillation** from the oracle RL policy. Because the oracle was trained with access to privileged simulation information (like object pose and properties) that may not be available on real robots, DexCtrl’s student networks rely purely on the observable history of joint states. Impressively, the authors show that the networks can infer necessary information (like how the object is moving or how heavy it might be) just from proprioceptive history, sidestepping the need for direct object state input. This is analogous to a robot feeling the weight of an object through the effort it needed over the last few moments.

When deploying to the real world (or for closed-loop testing), DexCtrl runs in **real-time closed-loop**: it uses the robot’s actual sensor readings at each step as the latest “current state” in the history, then produces an action and controller adjustment which are immediately applied. Notably, **no further training or fine-tuning was done on the real robot** – the policy is deployed zero-shot after training in simulation. To account for units differences between sim and real, they apply a simple linear scaling of the controller parameters (based on approximate upper/lower bounds) rather than any painstaking system identification. Beyond that, no manual calibration was needed.

An interesting detail is how DexCtrl manages to transfer well without heavy randomization. The oracle policy was trained on a range of object physical properties (mass, friction) for generality, but the authors did **not** perform extreme randomization of sensor noise or controller gains when training the oracle. Instead, they added a small amount of Gaussian noise to the student networks’ inputs during training, which turned out to be **sufficient for sim-to-real transfer**. This suggests that because DexCtrl’s policy can adjust to discrepancies on the fly, it doesn’t require as broad a training distribution to cover every possible gap – a little bit of noise was enough to make the student robust, simplifying the training process for the teacher (oracle) policy. In other words, DexCtrl inherently handles some variability through adaptation, whereas a fixed-gain policy would have needed aggressive domain randomization to cope with the same variability.

## Experimental Evaluation and Results

The authors evaluated DexCtrl on **two challenging dexterous manipulation tasks** using a 16-degree-of-freedom *LEAP hand* (a sophisticated robotic hand platform). The tasks were designed to be **contact-rich** and sensitive to force execution, thus ideal for testing sim-to-real performance:

* **In-Hand Object Rotation:** The robot hand must rotate a grasped object (e.g. a cube, apple, etc.) around a specified axis using finger dexterity, without dropping it. This task stresses object-hand contacts and requires controlled forces to keep the object stable while spinning it.
* **Object Flipping on a Table:** The hand, starting with an object resting on a surface (table), must flip the object 180° (like flipping a block or a Rubik’s cube to its other face) using a coordinated push-lift motion. This involves contact between the object, the hand, *and* the table – increasing sensitivity to dynamics (e.g. friction with the table and impact forces).

Both tasks were first learned in simulation (using PyBullet or a similar physics simulator), then the policies were deployed on a **real-world setup** with the LEAP hand performing the same tasks on real objects. To quantify performance, the authors defined several metrics:

* **Rotation Rate (RotR):** How fast the object rotates around the target axis. In simulation this was measured via a shaped reward; in real experiments it’s measured in radians of rotation achieved per trial. Higher RotR means better task success (faster rotation).
* **Time to Fail (TTF):** How long the hand can continue the task before failing (dropping the object in rotation, or the object falling off course in flipping). This reflects stability – longer is better.
* **Object Linear Velocity (ObjVel):** The average translational speed of the object during the task (only measured in sim). Lower velocity implies the object isn’t rattling or moving erratically, hence indicating a *stable manipulation*.
* **Torque Penalty:** A measure of energy or effort, computed from the torques applied (sim only). Lower torque penalty means the controller is efficient and smooth, not applying excessive forces.

### Baselines for Comparison

DexCtrl was compared against two main baselines to isolate the benefits of adaptive controller tuning:

* **Manual Tuning Baseline:** This represents the *conventional approach*: First, manually adjust the simulator’s controller parameters (PD gains) and add slight randomness so that a policy trained in sim might transfer to real. In practice, the authors carefully tuned the stiffness/damping by comparing simulated vs real trajectory responses, and trained a new RL “oracle” policy with those tuned gains (plus minor gain randomization for robustness). This oracle and its student policy **do not adapt gains at runtime** – they output only fixed-gain actions. Essentially, this baseline is a well-tuned fixed controller approach, requiring significant human effort to set up.
* **Ours w/o PD (No Adaptive Gains):** This ablation is *DexCtrl without the core idea* – the adaptive gain module is turned off. The policy is trained using the same oracle data as DexCtrl (which had varying gains in simulation), but during student training and deployment, the controller gains are fixed to the manually tuned values. The action module is still trained on the oracle’s trajectories, but it ignores the adaptive aspect. This baseline tests whether the advantage comes simply from our data collection and better action trajectories, versus the *addition of adaptive control*.

Both baselines and DexCtrl share the same training dataset size and neural network architecture for the action module, ensuring a fair comparison.

### Simulation Results: Performance Boost Even Without a Gap (Q1)

Before tackling sim-to-real transfer, the authors first asked: *Even if there is no sim-to-real discrepancy (i.e., using the same controllers in sim as in training), does jointly adapting controller parameters improve performance?* To test this, they evaluated the policies in simulation under ideal conditions (no controller mismatch) and also with some external disturbances (random forces applied to the object) to simulate a more challenging scenario.

The results were striking: **DexCtrl outperformed both baselines in simulation, even when all methods used the same simulator dynamics**. For the in-hand rotation task, DexCtrl achieved higher rotation speeds and longer stability than the manually tuned baseline, *and* used significantly less torque on average. For example, with random disturbances applied, DexCtrl’s rotation speed (RotR) was about 24% higher than the manual baseline, and it sustained the rotation for longer (TTF \~256 vs 239 time steps). It also expended only \~25% of the torque effort that the manual baseline did, indicating much more efficient control. Even the “Ours w/o PD” ablation (no adaptive gains) performed better than the manual baseline in rotation, thanks to the improved trajectories generated by our training approach, but **full DexCtrl still had the edge**.

On the flipping task, the difference was even more pronounced. DexCtrl nearly **doubled the rotation rate** of the flipped object compared to the manual-tuning baseline in simulation (RotR \~172 vs 91 with disturbances). It essentially solved the flipping with minimal failures (TTF almost maxed out at 297+ steps) and very low torque usage, whereas the fixed baseline struggled to get the object around as quickly. Interestingly, the ablation without adaptive control *did not outperform* the manual baseline on flipping at all – it had lower RotR (\~82 vs 91 with disturbance). This highlights that flipping (which involves impacts with a table and complex contact) is **highly sensitive to controller parameters**, and a fixed PD policy couldn’t find a one-size-fits-all gain. **DexCtrl’s adaptive gains were key to its success in flipping**, adjusting stiffness on-the-fly to handle the transitions. In summary, even in simulation without any real-world discrepancies, letting the policy co-optimize *how* to control yields better stability and faster task completion than using a fixed controller. The authors also noted DexCtrl’s training was faster to converge than massive randomization approaches, since the policy doesn’t have to brute-force learn robustness – it gets to sense and correct for variations directly.

### Zero-Shot Transfer to Real Robots: Narrowing the Gap (Q2)

The true test of DexCtrl is zero-shot transfer from simulation to a **real dexterous hand**. The team deployed the learned policies directly onto a real LEAP hand robot, with **no additional fine-tuning**, to see how well the skills carry over and whether DexCtrl indeed narrows the sim-to-real gap. They evaluated the in-hand rotation task on **12 different real objects** spanning a range of shapes, masses, and surface frictions (including a cube, bottle, apple, yogurt cup, baseball, Rubik’s cube, etc.), to ensure generality. Each object’s performance was measured over 10 test trials with different initial poses, and averages were compared.

The **real-world results** strongly favored DexCtrl. In fact, the performance gaps that were somewhat modest in simulation became **much larger in the real world**, underscoring how important adaptive control is outside the simulator. DexCtrl significantly outperformed the manually tuned baseline on all objects, achieving higher rotation speeds and similar or longer times before failure. For instance, across objects, the baseline might achieve only \~2–3 radians of rotation on average, whereas DexCtrl was often achieving **5–15+ radians**, essentially an order-of-magnitude improvement in some cases. It also maintained near-max stability (many objects never dropped within the 300-step trial limit, TTF = 300) whereas the baseline dropped some objects sooner. Even the non-adaptive variant (Ours w/o PD) did reasonably well in real – better than the manual baseline, showing that the learned trajectories were inherently more robust – but **DexCtrl’s adaptive module gave it a clear extra boost** in real conditions. The authors point out that the gap between DexCtrl and its no-PD ablation was **much larger in real-world tests than it had been in sim**, emphasizing that real hardware has many small unpredictabilities (friction, slight delays, minor calibration errors, etc.) that **require continual controller adjustment**. DexCtrl was uniquely capable of handling those, whereas a fixed-gain policy, even a strong one, was inevitably less optimal for some objects or moments.

They also demonstrated the flipping task on the real hand (with a Rubik’s cube being flipped on a table) to show generalization. Visual results (as seen in Fig. 1 and the paper’s videos) indicate that DexCtrl could perform the flip in reality as well. While detailed real-world numbers for flipping weren’t given for each baseline in the text, the success of DexCtrl in flipping further validates the approach’s generalizability to tasks involving external contacts (hand-object-table interactions). The overall takeaway is that **DexCtrl dramatically narrowed the sim-to-real gap** – policies that would normally degrade when going to real instead retained high performance, thanks to adaptive control. The manual baseline, despite careful tuning, could not match this, highlighting the limitations of one-time tuning.

### Robustness to Object Variations (Q3)

One of the promises of adaptive controller learning is improved robustness to variations in object properties, since the policy can *tune its behavior on the fly* to suit the object being manipulated. The authors designed a set of experiments to explicitly test how DexCtrl handles changes in **mass** and **friction** of the object, in a controlled way. They took a **single object shape (a cube)** and created multiple versions: for mass variation, they used a hollow cube and inserted different weights to make it light, medium, or heavy; for friction variation, they used cubes of the same weight but with different surface textures (smooth, medium, rough) to alter the friction coefficient.

The results, summarized in Table 4 of the paper, show that **DexCtrl consistently outperformed the baselines across all mass and friction conditions**. Notably, the advantage of DexCtrl was **most pronounced for the most challenging cases** – e.g. the heavy object and the very slippery object. For the heaviest cube, DexCtrl achieved a high rotation speed and didn’t drop the object (TTF \~300), whereas the fixed-gain baseline could barely rotate it (RotR near 0.6) and tended to fail much sooner. This indicates that DexCtrl’s policy learned to **increase stiffness/force output for heavier objects** to overcome their inertia, something a fixed controller tuned for lighter objects couldn’t do effectively. Similarly, for the low-friction (smooth) cube, DexCtrl spun it the fastest (since low friction makes rotation easier) but also managed stability adequately, whereas the baseline had lower speed and still occasional drops.

Interestingly, the pattern of performance across these variations aligned with intuitive **force-based reasoning**: lighter objects (or lower friction) can be flipped or rotated more easily (so they achieve higher speeds) but are also less stable (easier to slip), whereas heavier or rougher objects move more slowly but are inherently more stable once gripped. DexCtrl was able to handle both ends of these spectrums by adapting its control gains appropriately, confirming that its adaptive strategy generalizes to new physical properties. In contrast, the manual baseline had to pick one compromise setting of stiffness/damping – it could neither apply enough force for heavy objects, nor gentle enough control for very light/smooth objects, leading to either weak performance or instability in those extremes.

### Understanding the Learned Adaptive Strategies (Q4)

A natural question arises: *What exactly is DexCtrl’s controller module learning to do?* To demystify this, the authors analyzed the **learned stiffness profiles** under different object conditions. Since the controller outputs a stiffness and damping for each joint at each time step, we can look at how those values change for, say, a heavy object vs a light object, or a high-friction vs low-friction scenario. The paper focuses on **stiffness (K)** since it had a more pronounced effect on performance than damping in their tasks.

Two notable analyses were presented:

* **Average Stiffness vs Object Properties:** They computed the average stiffness used (across time and joints) for different object masses and frictions. In simulation, they observed a **monotonic increase of stiffness with object mass** – heavier objects induced the policy to choose higher stiffness values on average. This matches intuition: a heavier object requires more force (higher gain) to manipulate effectively, so DexCtrl “cranks up the strength” accordingly. For friction, the relationship was not one-directional: in one axis of rotation task, stiffness increased with more friction (perhaps because more friction means more resistance to sliding, needing stronger control), while in another case stiffness decreased with more friction (possibly because friction helps hold the object, so less stiffness was needed). The takeaway is that **DexCtrl learned a nuanced strategy** where stiffness tuning depended on the specific context of how friction was affecting the task dynamics. This reflects the complexity of dexterous manipulation – friction can both help and hinder movement depending on the situation, and the policy adjusted different joints differently to compensate.

* **Temporal Stiffness Patterns:** They also looked at how stiffness evolved over the course of a trial for different objects (Figure 7 in the paper). To isolate the effect of the controller module, they fed the *ground-truth actions* (from a successful trajectory) into the controller network and observed what gains it outputs over time for, say, a heavy vs light object. The trends showed that for **heavier objects, DexCtrl often raised stiffness at critical moments or maintained a higher stiffness for longer durations** (essentially keeping the “muscles flexed”). For **smoother (low-friction) objects**, certain joints showed *opposite stiffness trends* compared to rougher objects – in some joints the stiffness would spike for smooth objects (to prevent slip), while in others it might reduce (perhaps to avoid pushing too hard and causing loss of contact). These joint-specific adjustments illustrate that the policy isn’t simply using a single scalar gain – it’s tuning a *vector of gains* across the hand’s joints, tailoring the force distribution to the task. This level of adaptive control is very hard to achieve with manual tuning.

Overall, this analysis confirmed the hypothesis that **DexCtrl’s controller parameters encode meaningful adjustments to contact forces**, adapting systematically to the object’s mass and friction to improve performance. In essence, the policy learned to answer questions like “Is this object heavy? Then use a stiffer controller to grab it firmly,” or “Is this surface slippery? Maybe tighten grip in some fingers but loosen in others to keep balance.” This provides an insightful peek into *why* DexCtrl outperforms the fixed strategies – it’s because it’s actively modulating the interaction forces to suit the situation, rather than hoping one fixed setting will work everywhere.

## Effectiveness, Limitations, and Outlook

**Effectiveness:** The combination of results from simulation, real-world tasks, and controlled variation tests builds a strong case that DexCtrl is an effective solution to narrow the sim-to-real gap in dexterous manipulation. By **adaptively tuning the robot’s controller each time step**, DexCtrl achieved higher task success rates, better stability, and more energy-efficient control compared to state-of-the-art non-adaptive approaches. It essentially offloads some of the burden of sim-to-real transfer from human engineers (who previously had to fine-tune gains or exhaustively randomize environments) to the learning algorithm itself. The approach proved its worth especially in real-world evaluations: tasks that normally required careful gain tuning were accomplished **zero-shot with minimal manual effort**, and even challenging objects were manipulated reliably. In short, DexCtrl demonstrates that **learning “how to control” is as important as learning “what to do”** for robust robot dexterity.

**Limitations:** Despite its promising results, DexCtrl is not without limitations, as the authors openly discuss:

* **Lack of Explicit Force/Tactile Sensing:** Currently, DexCtrl relies solely on **proprioceptive data** (joint angles, etc.) and inferred object information from history. The real robot experiments did not use any tactile or force sensors on the hand. This was partly due to hardware limitations (the LEAP hand might not have rich tactile sensors). As a result, the policy might not observe some subtle aspects of contact (like the onset of slip or precise contact forces) directly. Integrating **real-time force feedback or tactile sensing** could further improve DexCtrl’s ability to adapt, essentially giving it an even clearer signal for how the interaction is going. The authors suggest that incorporating high-fidelity force/tactile data is a promising direction for future work. We can imagine a future DexCtrl that *feels* an object starting to slip and immediately boosts stiffness, even faster than it could infer from joint motions alone.

* **Hardware Generalization:** The study’s real-world tests were confined to the **LEAP hand platform** – a specific 16-DOF dexterous hand at UC Berkeley. While the tasks and objects were varied, it remains to be proven that the DexCtrl approach generalizes to other robot hands (e.g., anthropomorphic hands with different kinematics or pneumatic actuators with different dynamics). The framework itself is general (it could, in principle, be applied to any torque-controlled system with adjustable gains), but practical differences in hardware could pose new challenges. The authors plan to extend evaluations to other dexterous hands to validate generalizability.

* **Task Generality and Shared Controllers:** DexCtrl was demonstrated on two tasks. Scaling to a wider range of tasks or more unstructured scenarios might require further development. One idea the authors mention is training a **single controller adaptation module that could be shared across multiple skills**. This could be useful if you want one system to perform many tasks but still benefit from adaptive control – perhaps by fine-tuning gains appropriate for each task context.

* **Simulation Reliance:** Like most learning-based robotics approaches, DexCtrl still relies on a high-quality simulation (with diverse randomization of object properties) to learn the policy. If the simulator’s physics are very far off from reality, adaptive gains alone might not rescue performance. However, the need for less extreme domain randomization is a plus. Future work might explore if a small amount of **online learning or fine-tuning on real hardware** (e.g., a brief adaptation phase with real data) could further improve performance, especially as suggested if real force sensors are available.

In summary, **DexCtrl’s adaptive controller learning brings a new level of robustness to dexterous manipulation**, but incorporating richer sensing and testing its generality on other platforms are important next steps to fully realize its potential.

## Conclusion

DexCtrl represents an exciting development at the intersection of reinforcement learning, adaptive control, and robotics. By giving a dexterous robot hand the ability to **tune its own controller parameters on the fly**, the approach addresses a long-standing barrier in sim-to-real transfer. The method is conceptually elegant – use past experience to inform both what action to take and how aggressively to take it – and it proved remarkably effective in practice. Key contributions include highlighting the often-neglected role of low-level controller differences in sim-to-real gap and providing a principled learning-based solution to tackle it. The experiments showed improved performance over manual tuning even in simulation, and a dramatic reduction of performance loss when moving to real hardware, across various objects and conditions.

For a technical audience, DexCtrl offers insight into how **policy and controller design can be interwoven**: rather than treating the controller as a fixed black box, it becomes part of the learning problem. This blurs the line between high-level planning and low-level control, potentially leading to more integrated and resilient robotic systems. It also suggests that some challenges previously addressed by brute-force domain randomization might be better solved by model adaptation mechanisms like this, which can be more sample-efficient and targeted.

Looking forward, the idea of **adaptive controller learning** could be extended in many ways. As noted, adding direct force sensing could allow even finer-grained adaptation (imagine a robot that adjusts stiffness based on real-time tactile feedback, achieving human-like delicacy). Additionally, applying DexCtrl to more complex multi-step tasks or to legged robots and other domains could test the limits of its generality. If successful, these advances could bring us closer to robots that can reliably perform intricate tasks in unpredictable real-world environments – a step change toward truly **robust, real-world dexterous manipulation**.

Overall, *DexCtrl: Towards Sim-to-Real Dexterity with Adaptive Controller Learning* demonstrates that bridging the sim-to-real gap is not only about better simulators or more data, but also about smarter control strategies. By **learning to adapt** at the control level, robots can achieve a new level of dexterity and reliability, which is an exciting prospect for the field of robotics and its real-world applications.

**Sources:** The analysis and results discussed are based on the paper by Zhao *et al.* (2025) and the references therein.

-->


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<script src="https://utteranc.es/client.js" repo="curieuxjy/blog_comments" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Copyright 2024, Jung Yeon Lee</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/curieuxjy">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>