<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-06-03">
<meta name="description" content="Digitizing Touch with an Artificial Multimodal Fingertip">

<title>📃Digit360 리뷰 – Curieux.JY</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../profile.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-2NVZN2MJZT"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-2NVZN2MJZT', { 'anonymize_ip': true});
</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Curieux.JY</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../post.html"> 
<span class="menu-text">Post</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../note.html"> 
<span class="menu-text">Note</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">Jung Yeon Lee</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/curieuxjy"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#brief-review" id="toc-brief-review" class="nav-link active" data-scroll-target="#brief-review"><span class="header-section-number">1</span> Brief Review</a></li>
  <li><a href="#detail-review" id="toc-detail-review" class="nav-link" data-scroll-target="#detail-review"><span class="header-section-number">2</span> Detail Review</a>
  <ul class="collapse">
  <li><a href="#기술-분석-인공-멀티모달-손끝의-구성과-작동-원리" id="toc-기술-분석-인공-멀티모달-손끝의-구성과-작동-원리" class="nav-link" data-scroll-target="#기술-분석-인공-멀티모달-손끝의-구성과-작동-원리"><span class="header-section-number">2.1</span> 기술 분석: 인공 멀티모달 손끝의 구성과 작동 원리</a>
  <ul class="collapse">
  <li><a href="#센서-설계-및-주요-구성-요소" id="toc-센서-설계-및-주요-구성-요소" class="nav-link" data-scroll-target="#센서-설계-및-주요-구성-요소"><span class="header-section-number">2.1.1</span> 센서 설계 및 주요 구성 요소</a></li>
  <li><a href="#센서-데이터-융합-및-학습-모델-구조" id="toc-센서-데이터-융합-및-학습-모델-구조" class="nav-link" data-scroll-target="#센서-데이터-융합-및-학습-모델-구조"><span class="header-section-number">2.1.2</span> 센서 데이터 융합 및 학습 모델 구조</a></li>
  </ul></li>
  <li><a href="#응용-가능성-산업부터-vr까지의-활용-전망" id="toc-응용-가능성-산업부터-vr까지의-활용-전망" class="nav-link" data-scroll-target="#응용-가능성-산업부터-vr까지의-활용-전망"><span class="header-section-number">2.2</span> 응용 가능성: 산업부터 VR까지의 활용 전망</a>
  <ul class="collapse">
  <li><a href="#로보틱스-및-자동화-분야" id="toc-로보틱스-및-자동화-분야" class="nav-link" data-scroll-target="#로보틱스-및-자동화-분야"><span class="header-section-number">2.2.1</span> 로보틱스 및 자동화 분야</a></li>
  <li><a href="#가상현실vr-및-텔레프레즌스-분야" id="toc-가상현실vr-및-텔레프레즌스-분야" class="nav-link" data-scroll-target="#가상현실vr-및-텔레프레즌스-분야"><span class="header-section-number">2.2.2</span> 가상현실(VR) 및 텔레프레즌스 분야</a></li>
  <li><a href="#의료-및-보조공학-분야" id="toc-의료-및-보조공학-분야" class="nav-link" data-scroll-target="#의료-및-보조공학-분야"><span class="header-section-number">2.2.3</span> 의료 및 보조공학 분야</a></li>
  </ul></li>
  <li><a href="#한계점-및-향후-과제-도전-과제와-비판적-고찰" id="toc-한계점-및-향후-과제-도전-과제와-비판적-고찰" class="nav-link" data-scroll-target="#한계점-및-향후-과제-도전-과제와-비판적-고찰"><span class="header-section-number">2.3</span> 한계점 및 향후 과제: 도전 과제와 비판적 고찰</a></li>
  </ul></li>
  <li><a href="#reference" id="toc-reference" class="nav-link" data-scroll-target="#reference"><span class="header-section-number">3</span> Reference</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-page-left" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">📃Digit360 리뷰</h1>
  <div class="quarto-categories">
    <div class="quarto-category">paper</div>
    <div class="quarto-category">tactile</div>
  </div>
  </div>

<div>
  <div class="description">
    Digitizing Touch with an Artificial Multimodal Fingertip
  </div>
</div>


<div class="quarto-title-meta column-page-left">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">June 3, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<ol type="1">
<li>✨ 이 논문은 시각, 오디오, 진동, 힘, 열, 냄새 등 다양한 감각을 동시에 측정하는 고해상도 인공 촉각 센서인 ’Digit 360’을 제시합니다.</li>
<li>🔬 이 손끝 센서는 7 마이크로미터의 공간 해상도와 1 밀리뉴턴 수준의 힘 감지 능력을 포함하여 인간의 촉각 성능을 능가하는 결과를 보였습니다.</li>
<li>🤖 장치 내 AI 프로세싱을 통해 빠른 반응 속도를 구현했으며, 로봇 공학, 가상 현실, 원격 조작 등 광범위한 응용 분야를 위한 유망한 플랫폼입니다.</li>
</ol>
<hr>
<section id="brief-review" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Brief Review</h1>
<p>본 논문은 인공 촉각 센서인 “Digit 360”을 통해 촉감을 디지털화하는 기술의 혁신적인 발전을 제시합니다. 기존 시스템들이 둥근 형태의 유연한 구조를 가지면서도 풍부하고 다중 모드(multimodal)의 디지털 촉각 감지 기능을 제공하지 못했다는 한계를 극복하기 위해 개발되었습니다. Digit 360은 인간의 손가락과 유사한 형태의 센서로, 옴니디렉셔널(omnidirectional) 터치에 반응하고 다중 모드 신호를 캡처하며 온-디바이스(on-device) AI를 사용하여 데이터를 실시간으로 처리하는 약 830만 개의 taxels에 해당하는 고해상도 센서를 포함합니다. 평가 결과, Digit 360은 7 μm의 미세한 공간 특징을 구분할 수 있으며, 수직 힘(normal force)은 1.01 mN, 전단 힘(shear force)은 1.27 mN의 분해능으로 감지합니다. 또한, 최대 10 kHz의 진동, 열, 심지어 냄새까지 감지할 수 있습니다. 특히, 인간의 반사 궁(reflex arc)을 모방하여 로봇의 말초 신경계 역할을 하는 온-디바이스 AI 신경망 가속기(neural network accelerator)를 내장하여 실시간 로컬 프로세싱을 통한 반응 지연 최소화 및 통신 대역폭 감소를 실현합니다. 이러한 성능은 인간 이상의 촉각 디지털화 가능성을 보여주며, 로봇 공학, 가상 현실 및 원격 현장감, 의수/의족, 전자상거래 등 다양한 분야에 응용될 잠재력을 가집니다. 연구 발전을 위해 Digit 360의 모듈식 플랫폼 디자인은 오픈 소스로 공개되었습니다.</p>
<p>핵심 방법론은 다음 다섯 가지 하위 시스템에서의 혁신을 통해 높은 공간적, 시간적, 다중 모드 성능을 달성했습니다.</p>
<ol type="1">
<li><p><strong>Elastomer Interface (엘라스토머 인터페이스):</strong> 촉각 상호작용을 위한 유연한 표면입니다. 센서의 민감도는 겔 손가락 반경(<span class="math inline">R_{gel}</span>), 코팅층 두께(<span class="math inline">T_c</span>), 겔층 두께(<span class="math inline">T_g</span>), 높이(<span class="math inline">h</span>), 코팅 영률(<span class="math inline">E_c</span>), 겔 영률(<span class="math inline">E_g</span>) 등 6가지 재료 매개변수에 따라 달라집니다. 재료의 컴플라이언스(compliance)와 두께가 너무 크면 미세한 특징 감지 성능이 저하됩니다. 이를 개선하기 위해 기존의 수작업 방식 대신 은(silver) 박막을 엘라스토머 표면에 화학적으로 증착하는 새로운 기술을 개발하여 이전 방법보다 훨씬 얇고 균일한 코팅 두께(최대 6 μm)를 구현하여 민감도를 향상시켰습니다. 유한 요소법(FEM) 모델링과 동적 기계 열 분석(DMTA)을 통해 최적의 재료 특성(예: Young’s modulus <span class="math inline">E = 2.6 \pm 0.74</span> MPa) 및 두께를 도출했습니다.</p></li>
<li><p><strong>Optical System (광학 시스템):</strong> 터치 표면의 변형을 캡처하는 시스템입니다. 일반적인 상용 카메라 대신 반구형 표면 캡처에 최적화된 맞춤형 고체 몰입 하이퍼 피시아이 렌즈(custom solid immersion hyper fisheye lens)를 개발했습니다. 1.1 μm CMOS 픽셀 크기에 맞춰 공간 해상도 증가를 위한 스팟 크기 수렴, 의도적인 색수차 허용, 객체 압입 깊이에 비례하는 디포커스 허용을 위한 얕은 심도 도입, 내부 반사 및 산란 포착을 위한 반사 방지 코팅 제거 등의 매개변수를 설정하여 촉각 감지에 특화된 성능을 달성했습니다.</p></li>
<li><p><strong>Illumination System (조명 시스템):</strong> 센서 내부를 비추는 시스템입니다. 배경 균일성(background uniformity)과 이미지-배경 균일성 대비(image-to-background uniformity contrast)라는 두 가지 지표를 최적화했습니다. 기존 센서의 내부 구조 라이트 파이프나 Lambertian 산란 표면은 글린트(glint)나 핫스팟(hotspot)과 같은 아티팩트를 유발하거나 대비를 감소시키는 문제가 있었습니다. Digit 360은 내부 지지 구조 대신 단단한 고체 볼륨(rigid solid volume)을 사용하고, Lambertian 산란이 아닌 1°에서 25° 사이의 반치폭 반각(half-width-half-max angles)을 갖는 제어된 수준의 산란(controlled degree of scattering) 표면을 적용했습니다. 이를 통해 이미지 센서 포화 없이 높은 대비와 균일한 배경 조명을 동시에 달성하여 압입에 의한 그림자와 반사를 효과적으로 캡처합니다. Contrast-to-Noise Ratio (CNR) 분석을 통해 최적의 산란 각도 범위(20° ~ 25°)를 확인했습니다. 8개의 완전 제어 가능한 RGB LED를 사용하여 동적 조명 시스템을 구현했습니다.</p></li>
<li><p><strong>Multi-modal Sensing (다중 모드 감지):</strong> 시각 정보 외에 다양한 촉각 정보를 감지합니다. 카메라 기반 시각 센싱(최대 240Hz) 외에, 접촉 강도 및 형상, 정적/동적 힘, 표면 오디오 텍스처 및 진동(최대 10 kHz), 열 변화, 손가락 속도/가속도/방향, 공기 중 화학 물질 감지(냄새) 기능을 포함합니다. 표면 오디오 센싱은 내부 마이크와 압력 MEMS 센서를 사용하며, 불투명 용기 내부의 액체량 감지나 표면 텍스처 구별에 활용될 수 있습니다. 열 감지는 객체 상태(온도)를 파악하고, 가스 감지는 객체의 미끄러움/습함 여부나 특정 물질(커피, 치즈 등) 식별(91% 정확도)에 사용됩니다. 이러한 다중 모달리티는 시각 정보만으로는 얻기 힘든 풍부한 정보를 제공하여 촉각 이해를 심화시킵니다.</p></li>
<li><p><strong>On-device AI Processing (온-디바이스 AI 처리):</strong> 인간의 반사 궁을 모방하여 센서 데이터를 로컬에서 빠르게 처리합니다. Greenwaves Technologies GAP9 신경망 가속기(9-core RISC-V)를 손가락 끝 형태에 통합하여 감지 데이터 처리 및 로봇 엔드 이펙터(end effector) 직접 제어를 가능하게 합니다. 이를 통해 기존의 외부 호스트 컴퓨터 처리 방식(6 ms 이상) 대비 이벤트-액션 지연 시간(event-to-action latency)을 1.2 ms로 대폭 단축하고 지터(jitter)를 감소시켰습니다. 데이터 파이프라인 지연 시간 분석(USB 3.0 전송, 서브샘플링, 추론, 액션 전송) 결과, 온-디바이스 가속기가 MLP 모델의 레이어 깊이를 크게 확장시키며(60층까지), MobileNetV2와 같은 CNN 모델에서도 낮은 지연 시간(64x64 입력 시)을 보여줍니다. 이는 로컬에서 저수준 촉각 신호를 추상화하거나(예: 힘 추정) 미끄러짐 감지 후 즉각적인 그립 재구성 액션을 제공하는 등 빠른 반응이 필요한 작업에 유리합니다.</p></li>
</ol>
<p>평가 과정에서는 정밀한 6 DoF 로봇 인덴터(robot indenter)를 사용하여 공간 해상도, 수직/전단 힘 감도를 측정했습니다. 수직 및 전단 힘 예측을 위해 ResNet50을 기반으로 한 딥러닝 모델을 훈련시켰으며, 특히 표면에 인위적인 마커 없이도 엘라스토머 내부 텍스처의 옵티컬 플로우(optical flow)를 활용하여 전단 힘을 정확하게 예측할 수 있음을 확인했습니다. 스펙큘러(specular) 표면 마감은 전통적인 Lambertian보다 우수한 힘 해상도를 제공했습니다. 다중 모드 분류 연구에서는 Franka Emika 로봇 팔과 Wonik Allegro 로봇 손을 사용하여 다양한 행동(슬라이드, 탭, 저어 섞기)과 재료(나무, 플라스틱, 실리콘)에 대한 데이터를 수집했습니다(약 624k 샘플). 수집된 다중 모드 데이터를 각 모드별 인코더(RGB/오디오는 ResNet-18, 시간 신호는 MLP)를 거쳐 결합하는 딥 신경망 모델을 구축하고, 손가락 독립적(finger-independent) 및 손가락 의존적(finger-dependent) 설정에서 행동 및 재료 분류 성능을 평가했습니다. 손가락 의존적 설정에서 모든 모달리티를 함께 사용할 때 가장 높은 분류 정확도를 보였으며, 특히 관성 측정값(inertial measurement)은 행동 분류에, 시각 및 오디오 신호는 재료 분류에 중요함을 확인했습니다.</p>
<p>Digit 360은 기존 촉각 센서 대비 공간 해상도, 수직 힘 분해능, 전단 힘 분해능 등 여러 면에서 크게 향상된 성능을 제공합니다(테이블 5 참조: 공간 4X, 수직 6X, 전단 9X 개선). 이러한 발전은 촉각 감지 연구의 새로운 방향을 제시하며, 촉각 데이터를 이해하고 처리하는 새로운 계산 모델 개발을 촉진할 것으로 기대됩니다.</p>
</section>
<section id="detail-review" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Detail Review</h1>
<blockquote class="blockquote">
<p>인공 멀티모달 손끝 기술: “Digitizing Touch” 논문의 심층 기술 리뷰</p>
</blockquote>
<p>인간 손끝의 풍부한 촉각 능력을 디지털화하기 위한 혁신적인 인공 손끝 센서를 제시합니다. 이 센서는 하나의 손가락 크기 장치에 <strong>초고해상도 시각 촉각(sensor)</strong>과 다종의 센서를 융합하고, <strong>온보드 AI 프로세서</strong>를 탑재하여 실시간 데이터 처리를 수행함으로써 <em>“촉각의 디지털화”</em>를 실현하고자 합니다.</p>
<section id="기술-분석-인공-멀티모달-손끝의-구성과-작동-원리" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="기술-분석-인공-멀티모달-손끝의-구성과-작동-원리"><span class="header-section-number">2.1</span> 기술 분석: 인공 멀티모달 손끝의 구성과 작동 원리</h2>
<section id="센서-설계-및-주요-구성-요소" class="level3" data-number="2.1.1">
<h3 data-number="2.1.1" class="anchored" data-anchor-id="센서-설계-및-주요-구성-요소"><span class="header-section-number">2.1.1</span> 센서 설계 및 주요 구성 요소</h3>
<p>이 인공 손끝(<code>Digit 360</code>이라 명명)은 인간 손가락과 유사한 <strong>반구형의 유연한 촉각 표면</strong>을 갖추고 있으며, 그 내부에 복합 센서들이 층상 구조로 통합되어 있습니다. 가장 핵심이 되는 것은 <strong>시각 촉각 센서(vision-based tactile sensor)</strong>로, 표면의 미세한 변형을 고해상도로 포착하는 역할을 합니다. 반구형 <strong>엘라스토머 재질의 젤</strong>이 손가락 표면을 이루고, 그 표면 아래에는 <strong>매우 얇은 은(Ag) 반사막 코팅층</strong>이 입혀져 있어 접촉 시 움푹 패이는 변형을 시각적으로 표현합니다. 센서 내부에는 <strong>초광각 어안 렌즈(hyper-fisheye lens)</strong>를 장착한 <strong>고해상도 카메라</strong>가 배치되어 있는데, 이 특수 렌즈 덕분에 하나의 카메라로 손가락 끝 부분부터 옆면까지 <strong>전방위(360°) 촉각 이미지</strong>를 수집할 수 있습니다. 이는 기존에 다수의 카메라를 사용하거나 평면 부위만 감지하던 접근과 달리, <strong>하나의 카메라로 전방위 접촉을 감지</strong>하는 혁신으로 센서 통합과 데이터 처리가 한층 단순화됩니다.</p>
<p>이 <strong>시각 촉각 모듈</strong>은 동작 시 <strong>구조화 조명(structured light)</strong> 기법을 활용하여 표면 변형 정보를 획득합니다. 구체적으로, 내부에 배치된 다색 LED 조명(예: 360도 방향에 고르게 배열된 RGB LED들)을 통해 젤 내부를 조명하고, <strong>반사막 표면의 압흔(depression)</strong>이 만들어내는 빛 반사 변화를 카메라가 포착합니다. 연구진은 조명의 <strong>세기와 색상 스펙트럼을 동적으로 제어</strong>함으로써, 조명 불균일로 인한 영상 왜곡(glare, hotspot)을 최소화하고 표면 눌린 자국과 배경을 높은 대비로 분리해내는 최적의 환경을 구축했습니다. 또한 내부 구조물을 두지 않고 <strong>고체형 실리콘 볼륨</strong>으로 일체화된 젤을採用하여, 빈 공간에서 발생하는 빛 산란 노이즈를 줄이고 깨끗한 촉각 영상을 얻었습니다. 이러한 설계 덕분에 시각 촉각 센서는 <strong>약 8백만 개 이상의 촉각 화소(taxel)</strong>로 이루어진 고해상도 영상을 제공하며, 7μm에 불과한 미세 공간 구조까지 구분할 수 있는 <strong>초정밀 공간 해상도</strong>를 달성했습니다. 이는 인간 손끝이 인지하는 수십 μm 수준의 촉감 해상도를 뛰어넘는, 말 그대로 <em>“초인적(superhuman) 성능”</em>입니다. 더불어, 촉각 영상의 <strong>프레임률을 240Hz</strong>까지 높여 빠르게 변하는 접촉 사건도 포착할 수 있도록 하였는데, 이는 일반 카메라 기반 촉각센서(보통 30~60Hz)의 한계를 크게 확장한 것입니다. 시각 정보만으로도 접촉 지형(contact geometry)과 <strong>힘의 방향/세기 추정</strong>이 가능하지만, 이 논문의 센서는 여기서 그치지 않고 <strong>다양한 물리 신호를 추가로 감지</strong>하도록 설계되었습니다.</p>
<p><strong>멀티모달 감각 통합</strong>을 위해 손끝 내부에는 카메라 외에도 여러 센서들이 융합되어 있습니다. 우선, <strong>소형 마이크로폰 및 압력 기반 진동 센서</strong>가 내장되어 손가락이 표면을 스칠 때 나는 <strong>마찰음</strong>이나 미세 <strong>진동</strong> 정보를 포착합니다. 덕분에 이 손끝은 240Hz 카메라로는 포착하기 어려운 수 kHz 대역의 빠른 진동까지도 감지할 수 있으며, 실제 <strong>10kHz에 이르는 고주파 촉각 진동</strong>까지 인지할 수 있음이 확인되었습니다. 이는 인간의 진동 감각 수용기(Pacinian corpuscle)가 약 수백 Hz 정도까지 반응하는 것과 견줘볼 때 월등히 높은 수치입니다. 높은 대역의 진동 및 음향 정보는 재질 특성 파악에 유용한데, 예를 들어 이 센서는 손가락으로 가볍게 두드리거나 문지르는 동작만으로도 나무와 플라스틱, 직물과 거친 면과 같은 <strong>다양한 표면의 질감 차이를 음향/진동 스펙트럼으로 구별</strong>해낼 수 있었습니다. 또한 연구진은 이 진동 센서를 활용하여 <strong>불투명 용기의 내용물량을 파악</strong>하는 데 응용했는데, 손끝으로 병을 톡톡 두드릴 때 발생하는 공명 소리의 특징으로부터 병에 든 액체의 높이를 가늠하는 데 성공하였습니다. 이러한 사례들은 <strong>시각+청각 촉각의 융합</strong>이 제공하는 정보를 잘 보여줍니다.</p>
<p>다음으로, 이 손끝에는 <strong>온도 센서</strong>가 포함되어 <strong>접촉 물체의 온도 변화</strong>를 감지할 수 있습니다. 이를 통해 단순한 촉감 이상으로, 물체가 차가운지 따뜻한지, 혹은 온도 상승/하강 추세에 있는지까지 파악이 가능하며, 뜨거운 표면이나 냉각 중인 물체 등 <strong>안전과 관련된 정보</strong>를 얻을 수 있습니다.</p>
<p>나아가 <strong>화학 물질 센서(가스 센서)</strong>도 내장되어 있어, 접촉 중이거나 주변 공기 중에 <strong>휘발성 화합물</strong>이 있는지를 감지합니다. 일종의 <em>“후각”</em>에 해당하는 이 능력은 일반적인 촉각 센서에서는 찾아보기 힘든 독특한 모달리티로서, 예를 들어 손가락이 젖었을 때 비누 냄새나 기름기 냄새를 감지하여 표면에 <strong>비누나 윤활유가 묻어 미끄럽게 할 요인이 있는지</strong> 사전에 인지하는 활용이 가능합니다. 이처럼 <strong>힘/형상(시각)</strong>, <strong>진동/음향</strong>, <strong>온도</strong>, <strong>화학</strong>에 이르는 다채로운 센싱을 하나의 손가락 팁에 통합한 것은 해당 장치의 가장 큰 강점입니다. 인간 피부 역시 촉각 수용기가 여러 종류(기계적, 열, 통증 등)로 이루어져 다양한 자극을 느끼는데, 기존의 인공 촉각들은 그 중 일부 신호만 개별적으로 모방했을 뿐 <strong>이처럼 통합적인 멀티모달 센싱을 구현한 예는 전무</strong>했습니다. Digit 360 인공 손끝은 <strong>사람 손끝의 감각 스펙트럼을 거의 전부 (또는 그 이상으로) 디지털화</strong>했다는 점에서 의미가 큽니다.</p>
<p>한편 이러한 방대한 센서 데이터를 효과적으로 활용하기 위해, 손끝 내부에는 <strong>온장치 인공지능 가속기(on-device neural network accelerator)</strong>를 내장하고 있습니다. 이 작은 AI 전용 프로세서는 RISC-V 기반 9코어 NPU로, 일종의 <em>“로봇 말초신경계”</em> 역할을 수행합니다. 즉, 손끝에서 취득한 멀티모달 데이터를 로컬에서 즉각적으로 신경망으로 처리하여 <strong>유의미한 고레벨 정보</strong>(접촉 여부, 힘 추정치, 미끄럼 발생 등)를 뽑아내고, 필요한 경우 로봇의 구동기로 바로 <strong>피드백 제어명령</strong>을 내리는 것입니다. 마치 인간이 뜨거운 물체를 손끝으로 짚었을 때 뇌까지 생각하지 않고 즉각 손을 떼는 <strong>척수 반사(reflex arc)</strong>처럼, 이 손끝은 중요한 촉각 이벤트를 자체적으로 판별해 로봇 팔에 즉각 신호를 줄 수 있습니다. 연구 결과에 따르면, 기존 방식처럼 모든 센서 데이터를 중앙 컴퓨터로 보내 처리하는 경우 약 5~6ms 소요되던 <strong>이벤트 감지부터 반응 출력까지의 지연</strong>이, <strong>온보드 처리 시 약 1.2ms</strong>로 대폭 단축되었습니다. 이는 처리 지연을 5배 이상 줄인 것으로, 로봇 제어의 민첩성을 크게 높여줍니다. 요컨대 Digit 360 플랫폼은 <strong>센서 하드웨어</strong>부터 <strong>즉각적 AI 처리</strong>까지 한 몸체에 넣음으로써, 방대한 촉각 데이터를 다루는 복잡성을 줄이고 실시간성을 극대화한 설계라 할 수 있습니다.</p>
</section>
<section id="센서-데이터-융합-및-학습-모델-구조" class="level3" data-number="2.1.2">
<h3 data-number="2.1.2" class="anchored" data-anchor-id="센서-데이터-융합-및-학습-모델-구조"><span class="header-section-number">2.1.2</span> 센서 데이터 융합 및 학습 모델 구조</h3>
<p>이 인공 손끝이 생성하는 <strong>멀티모달 데이터 스트림</strong>(초당 240장의 촉각 이미지와 여러 실시간 센서신호)을 어떻게 유의미한 정보로 바꿀 것인가도 중요한 기술 요소입니다. 연구팀은 전통적 공학 모델 대신 <strong>딥러닝 기반의 학습 모델</strong>을 활용하여, 복잡한 센서 패턴을 힘이나 물체 특성으로 변환했습니다. 먼저 <strong>촉각 이미지로부터 접촉 힘을 추정</strong>하기 위해 <strong>심층 합성곱 신경망(CNN)</strong>을 학습시켰습니다. 구체적으로, 시각 촉각 이미지(3채널 RGB)를 입력 받아 해당 접촉의 <strong>법선력(normal force)</strong>과 <strong>전단력(shear force)</strong> 값을 회귀 출력하는 CNN 모델을 만들었는데, 이는 ResNet-50 백본을 변형하여 다중 분류 대신 2개의 연속값(힘)을 내도록 구성했습니다. 이 모델을 수천 건의 교정 데이터를 통해 훈련한 결과, 손끝 이미지만 가지고도 법선 및 전단 힘을 각각 <strong>오차 약 1.0~1.3 mN 이내</strong>로 예측할 수 있었습니다. 이는 앞서 언급한 센서의 힘 분해능(≈1 mN) 수준에 근접하는 정확도로서, 광학식 촉각센서로 힘을 측정하는 <em>정량적 캘리브레이션</em>의 새로운 기록이라 할 수 있습니다. 특히 전단력 추정의 경우, 기존 시각 촉각센서들은 표면에 <strong>마커(marker)</strong>점을 찍어 변형 추적을 도와주지 않으면 어려움이 있었으나, 본 센서는 고해상도 영상 자체에 풍부한 표면 특징이 담겨있어 마커 없이도 정밀한 전단 추정이 가능했다고 보고됩니다. 그만큼 <strong>고해상도의 이점</strong>이 학습 모델에도 기여한 것입니다.</p>
<p>또한 연구진은 이 손끝이 산출하는 <strong>다양한 모달 데이터를 종합적으로 해석</strong>하기 위해 <strong>멀티모달 딥러닝 모델</strong>을 구성하였습니다. 하나의 예로, 촉각 센서를 이용한 <strong>행동 분류(예: 밀기, 두드리기, 저어섞기)</strong>와 <strong>재질 분류(예: 물체가 나무인지 플라스틱인지)</strong> 문제를 학습시켰습니다. 여기에는 이미지, 진동(음향), 온도, 가스, 관성 등 <strong>여러 종류의 센서 값들이 입력</strong>으로 사용되었으며, 각 모달리티에 따라 다른 신경망 구조를 결합하는 방식을 취했습니다. 예를 들어 <strong>시각 및 음향 정보</strong>는 시계열 데이터를 주파수-시간 스펙트럼 이미지로 변환하여 <strong>ResNet-18 기반 CNN 인코더</strong>로 처리하고, <strong>관성(IMU), 온도, 가스와 같은 스칼라 신호</strong>는 별도의 다층퍼셉트론(MLP) 인코더로 처리한 후, 최종적으로 이들 특징을 <strong>하나의 잠재 벡터로 융합</strong>하여 행동 및 재질을 예측하도록 했습니다. 이러한 멀티모달 신경망을 통해 <strong>각 모달리티의 기여도</strong>와 상호 작용도 분석할 수 있었는데, 연구 결과 <strong>여러 모달의 결합이 단일 모달 대비 분류 정확도를 향상</strong>시켰으며 특히 동작 종류에 따라 핵심 단서가 되는 센서가 다름을 관찰했습니다. 예를 들어 슬라이딩하는 동작에서는 <strong>진동/음향 신호가</strong> 중요하고, 물체의 재질 식별에는 <strong>시각적인 접촉 패턴</strong>이 주요하게 작용하는 식입니다. 이처럼 딥러닝 모델을 통해 <strong>센서 융합의 유효성</strong>을 정량적으로 확인하였지만, 동시에 복잡한 모달 조합에서 최적의 특징을 학습하는 것은 난이도가 높습니다. 여기서 자연히 <strong>“센서 융합이 학습에 어떤 영향을 주는가?”</strong> 라는 질문이 따라옵니다. 여러 이질적인 센서 데이터를 함께 사용할 때 발생하는 <strong>모델 복잡도 증가</strong>, <strong>학습 데이터 요구량 증가</strong>, 그리고 <strong>모달 간 동기화와 상관관계 파악</strong> 등의 이슈가 존재합니다. 논문의 접근처럼 각 모달별 특징 추출 후 융합하는 구조는 한 해결책이지만, <em>어떤 모달이 얼마나 기여하도록 모델을 설계해야 최적인지</em>는 향후 추가 연구가 필요한 영역입니다. 다행히 저자들은 멀티모달 모델의 <strong>cross-modal significance</strong>를 분석하여 촉각 행동 인식에 기여하는 센서들의 상관관계를 탐구하였고, 이는 향후 <strong>센서 선택 및 모델 경량화</strong>에 참고되는 통찰을 제공합니다.</p>
<p>이 인공 손끝 플랫폼의 또 다른 기술적 특징은 <strong>모듈식 설계</strong>입니다. 센서와 프로세서 전체를 하나로 묶었음에도 불구하고, 내부 구성은 카메라 모듈, 마이크/IMU 모듈, AI 가속기 모듈 등 <strong>기능별로 분리된 소형 회로 블록</strong>들로 구성되어 있습니다. 이러한 모듈식 구조의 장점은 연구 개발 시 <strong>구성요소의 교체나 확장이 용이</strong>하다는 것입니다. 실제로 저자들은 필요에 따라 특정 센서를 빼서 비용을 낮추거나, 새로운 센서로 교체하여 성능을 높이는 식의 <strong>유연한 플랫폼</strong>을 지향하고 있습니다. 예를 들어 <strong>마커가 있는 손끝 젤</strong>로 교체하여 전단력 추정을 더욱 용이하게 하거나, 젤 재질의 <strong>경도나 두께를 변경</strong>해 감도를 조절하는 것도 쉽게 시도해볼 수 있습니다. 이러한 유연성은 학계와 산업계 연구자들이 자신들의 용도에 맞게 손끝 센서를 <strong>재구성(reconfigure)</strong>하여 실험해볼 수 있게 하며, 신기술 도입 시 전체를 새로 설계하지 않고 해당 모듈만 교체하면 되므로 <strong>개발 사이클을 단축</strong>시켜 줍니다. 실제 논문 저자들은 촉각 연구 활성화를 위해 이 손끝의 <strong>설계 도면과 소프트웨어를 오픈소스로 공개</strong>해 두었고, 이를 통해 더 많은 연구자들이 해당 플랫폼을 기반으로 <strong>촉각의 본질</strong>과 <strong>멀티모달 처리 기법</strong>을 탐구하기를 기대하고 있습니다.</p>
</section>
</section>
<section id="응용-가능성-산업부터-vr까지의-활용-전망" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="응용-가능성-산업부터-vr까지의-활용-전망"><span class="header-section-number">2.2</span> 응용 가능성: 산업부터 VR까지의 활용 전망</h2>
<p>이처럼 <strong>다중 센서융합 인공 손끝</strong>이 제공하는 풍부한 촉각 데이터와 정밀 제어 능력은 다양한 분야에서 혁신적인 응용을 가능케 합니다. 저자들은 논문에서 본 기술의 잠재적 적용 분야로 <strong>로보틱스(산업, 의료, 농업, 소비자 영역)</strong>, <strong>가상현실/텔레프레즌스</strong>, <strong>의수(보철)</strong>, <strong>전자상거래</strong> 등을 직접 언급하고 있습니다. 이하에서는 주요 분야별로 기대되는 활용 사례와 기존 기술 대비 이점 및 한계를 살펴봅니다.</p>
<section id="로보틱스-및-자동화-분야" class="level3" data-number="2.2.1">
<h3 data-number="2.2.1" class="anchored" data-anchor-id="로보틱스-및-자동화-분야"><span class="header-section-number">2.2.1</span> 로보틱스 및 자동화 분야</h3>
<p><strong>산업용 로봇</strong>에서 정밀 촉각 센싱은 매우 큰 가치를 갖습니다. 예를 들어 제조 조립 공정에서 로봇팔이 작은 부품을 끼우거나 나사를 조이는 작업을 생각해보면, 육안으로는 보이지 않는 <strong>미세한 맞촉감(fit)</strong>이나 <strong>나사선의 걸림 여부</strong> 등을 사람이 손끝으로 느끼며 작업하는 경우가 많습니다. 인공 멀티모달 손끝을 장착한 로봇은 이러한 작업에서 사람 못지않은 (어떤 면에서는 사람 이상의) 섬세한 촉각 피드백을 얻을 수 있습니다. 7μm 수준의 공간 분해능이라면 <strong>머리카락 굵기의 흠집이나 표면 거칠기 변화도 감지</strong>할 수 있어, 부품 간 결합의 미세한 오차나 표면 결함 검사에도 활용될 수 있습니다. 또한 <strong>1mN 단위의 힘 제어</strong>가 가능하다는 것은, 로봇이 매우 부드럽게 힘을 조절하여 <strong>쉽게 깨지는 부품이나 연성 재료를 다루는 정밀 조작</strong>에 강점을 지닐 것임을 시사합니다. 기존 로봇은 주로 관절 토크센서나 간단한 그립센서로 힘을 제어해왔는데, 이러한 저해상도 정보로는 구현하기 어려웠던 <strong>미끄러짐 감지</strong>, <strong>접촉면 마찰 측정</strong>, <strong>물체 무게 파악</strong> 등을 이제는 하나의 손끝 센서로 모두 수행할 수 있습니다. 실제 연구 결과, 본 센서는 물체가 손에서 미끄러지는 <strong>초기 징후를 고속 진동 센싱으로 탐지</strong>하여 즉각적으로 그립을 조정할 수 있음을 보였는데, 이는 산업 로봇의 <strong>자동 그립 조절(reflex)</strong> 기능 구현에 응용될 수 있습니다. 또한 제조 라인에서 제품 표면을 이 센서로 <em>스캔</em>하면, <strong>시각 검사로 놓치기 쉬운 미세 흠집이나 표면 텍스처 이상</strong>까지도 촉각으로 검출하여 품질 관리에 활용할 수 있습니다.</p>
<p><strong>물류 창고나 서비스 로봇</strong>에서도 응용 가능성은 큽니다. 예를 들어 <strong>소비자용 가정 로봇</strong>이 있다면, 섬세한 촉각이 있으면 물건을 집거나 정리할 때 <strong>물체의 종류와 상태를 파악</strong>하여 알맞게 다룰 수 있습니다. 옷가지나 식기, 전자제품 등 각각 다른 취급이 필요한 물건들을 손끝의 촉감으로 구분하고, 힘 조절을 다르게 할 수 있습니다. 특히 <strong>농업 로봇</strong>의 경우 과일 수확이나 식물 다루는 작업에서, 과일의 숙성도를 촉감으로 느끼거나 (단단함, 표면 질감), 식물 줄기의 두께와 강도를 파악하여 적절한 힘으로 잡는 등의 활용이 가능합니다. 토마토처럼 살짝 눌러봐야 익은 정도를 아는 작물도 이 센서로는 <strong>익은 정도에 따른 미세한 탄성 변화</strong>를 정량화할 수 있을 것으로 기대됩니다. 나아가 젖은 흙과 마른 흙의 감촉 차이나, 병해 충격 받은 잎의 질감 변화 등을 감지하는 등 <strong>농업 환경에서의 촉각 데이터 수집</strong>도 가능할 것으로 보입니다. 산업 및 서비스 로봇 분야에서 이 기술의 <strong>기술적 이점</strong>은 결국, <strong>사람 수준의 촉각 피드백을 로봇이 얻음으로써 자율작업의 신뢰성과 정밀도가 높아진다</strong>는 점입니다. 기존에는 힘을 세게 가해 실험적으로 실패를 겪으며 수행하던 작업도, 섬세한 촉감으로 사전에 상황을 인지하고 적응함으로써 <strong>보다 안정적인 자동화</strong>가 가능해질 것입니다.</p>
<p>물론 이러한 응용을 위해서는 센서의 <strong>견고성</strong>과 <strong>실시간 처리능력</strong>이 뒷받침되어야 합니다. 산업 현장은 온도, 먼지, 전자기 노이즈 등이 존재하므로, 광학식 촉각센서가 그런 환경에서도 유지보수가 용이할지 검증이 필요합니다. 이는 뒤의 한계점 부분에서 논의하겠지만, 간단히 말해 <strong>“이 방식이 실제 환경에서 견딜 수 있는가?”</strong>라는 실용적 질문이 남아 있습니다. 그럼에도 불구하고, Digit 360 센서가 보여준 성능 지표(공간/힘 해상도, 다중감각)는 현재까지 보고된 어떤 촉각 센서보다 뛰어나 인간 수준에 근접하거나 초과하는 것으로 평가되므로, 로보틱스 응용에 있어서 <strong>게임 체인저</strong>가 될 잠재력이 충분하다고 하겠습니다.</p>
<p>한편 <strong>의료 로봇틱스</strong> 분야에서도 정밀 촉각은 매우 중요합니다. 예를 들어 외과 수술 로봇의 수술 도구 끝에 이러한 센서를 장착하면, <strong>조직의 경도나 질감, 맥박 진동</strong>까지 느끼면서 수술을 진행할 수 있습니다. 종양과 정상 조직의 미세한 경도 차이나, 동맥의 박동, 미세 calcification 등을 촉각으로 구분하여 더 정교한 수술이 가능해질 수 있습니다. 또한 로봇 간호 및 재활 기기에서도 환자와 접촉하는 부분에 정밀 촉각 센서가 있다면, <strong>환자의 피부상태, 근육 긴장도, 반응</strong> 등을 감지하여 안전하고 개인화된 케어를 제공할 수 있습니다. 예를 들어 재활 로봇 손이 환자의 관절을 굽힐 때, 환자가 과도한 통증으로 미세한 저항을 보이면 (근육 경직 등 촉각 피드백) 이를 감지해 동작을 완화한다든지 하는 <strong>피드백 제어</strong>가 가능해집니다. 이러한 의료/복지 로봇 응용에서도 중요한 것은 <strong>신뢰성과 안전성</strong>이며, 멀티모달 손끝 센서는 <em>촉각을 통한 이중 안전장치</em> 역할을 하여 인간에게 위해가 가는 상황을 미연에 방지하는 데 도움을 줄 것입니다.</p>
</section>
<section id="가상현실vr-및-텔레프레즌스-분야" class="level3" data-number="2.2.2">
<h3 data-number="2.2.2" class="anchored" data-anchor-id="가상현실vr-및-텔레프레즌스-분야"><span class="header-section-number">2.2.2</span> 가상현실(VR) 및 텔레프레즌스 분야</h3>
<p><strong>가상현실(VR)</strong>과 <strong>증강현실(AR)</strong>에서는 시각, 청각 기술은 크게 발전했지만 <strong>촉각 경험의 부재</strong>가 한계로 지적되어 왔습니다. 본 기술은 현실 세계의 촉감 데이터를 <strong>디지털 콘텐츠로 녹여낼 수 있는 수단</strong>을 제공함으로써, VR/AR의 리얼리티를 한층 높일 수 있습니다. 예를 들어 이 손끝 센서를 이용해 다양한 물체의 질감, 표면 구조, 단단함을 데이터화하여 <strong>가상 객체에 부여</strong>하면, 사용자가 VR 장비를 통해 인터랙션할 때 사실감 있는 촉각 피드백을 줄 수 있을 것입니다. 현재도 VR용 촉각장갑 등이 연구되고 있으나 주로 진동 정도의 피드백만 제공하는데, <strong>본 센서로 측정된 정교한 촉각 프로파일</strong>을 기반으로 하면 사용자가 실제 나뭇결을 손으로 문지르는 듯한 미세한 질감까지도 재현하는 것이 궁극적으로 가능해집니다. 또한 AR 환경에서 로봇이나 장치가 현실 물체를 만졌을 때 그 촉감 정보를 실시간으로 사용자에게 전달한다면, <strong>원격 현실감(remote presence)</strong>이 크게 향상될 것입니다. 이를테면, 사용자가 원격 로봇을 통해 박물관의 유물을 만져보는 AR 투어를 한다면 로봇 손끝의 촉각이 사용자의 장갑에 피드백되어 <strong>“만지는 경험”</strong>을 전달할 수 있을 것입니다.</p>
<p><strong>텔레프레즌스(원격현존)</strong>와 <strong>원격 조작(teleoperation)</strong> 분야는 이 기술과 특히 직접적인 관련이 있습니다. 원격조작 로봇 (예컨대 해저 로봇 팔, 우주 로봇 팔, 원격 의료수술 로봇 등)에서 조종자는 카메라 화면과 약간의 힘 피드백만으로 작업하는 경우가 많은데, 만약 로봇 손끝에 Digit 360 센서를 달아두면 원격지의 미세한 촉각 정보를 실시간 전송받아 느낄 수 있습니다. 예를 들어 원격 수술에서 <strong>조직의 질감이나 경계를 손끝 감각으로 느끼면서 수술</strong>할 수 있게 되며, 위험물 처리 로봇이 <strong>폭발물의 표면 상태나 흔들림</strong>을 손끝으로 감지하여 더 섬세하게 다룰 수 있습니다. 이러한 고품질 촉각 정보는 <strong>양방향(haptic feedback)</strong> 장치와 결합되어 비로소 인간 조종자가 느낄 수 있게 되므로, 향후 촉각 피드백 장치 기술과 함께 발전이 필요합니다. 하지만 일단 <strong>정보를 획득하는 측면에서의 돌파구</strong>로서, 본 센서가 <strong>원격 촉각의 눈</strong> 역할을 할 수 있다는 의의가 있습니다. 특히 인터넷을 통한 원격작업에는 지연 문제가 큰데, 앞서 언급했듯 온보드 AI가 1ms 수준으로 로컬 반사를 수행한다면, 원격 조작에서도 <strong>로봇 자체가 위험을 감지해 즉각 대처</strong>함으로써 네트워크 지연으로 인한 사고를 줄일 수 있습니다. 예컨대 원격 로봇 손이 미끄러운 물체를 잡았을 때, 사람이 느끼기도 전에 센서가 미끄러짐을 감지해 그립을 조정해준다면 원격 조작의 안정성이 크게 향상될 것입니다. 이처럼 <strong>VR/텔레프레즌스</strong> 분야에서 인공 멀티모달 손끝은 <em>현실의 촉감을 디지털로 복제하고 전송하는 역할</em>을 하며, 이는 궁극적으로 <strong>현실-가상의 경계를 허무는 몰입형 경험</strong>으로 이어질 것입니다.</p>
</section>
<section id="의료-및-보조공학-분야" class="level3" data-number="2.2.3">
<h3 data-number="2.2.3" class="anchored" data-anchor-id="의료-및-보조공학-분야"><span class="header-section-number">2.2.3</span> 의료 및 보조공학 분야</h3>
<p>본 기술은 <strong>의수나 의족 등의 보조공학</strong> 분야에도 직접적인 파급 효과가 있을 것으로 기대됩니다. 현존하는 의수의 손가락에는 간단한 압력센서나 가속도센서 등이 달려 있을 뿐, 인간 고유의 풍부한 촉각을 제공하지 못합니다. 사용자(절단 장애인)는 물체를 잡아도 어느 정도 힘으로 잡는지, 미끄럽게 생겼는지 등의 정보를 거의 받지 못해 섬세한 작업이 어렵고, 심지어 뜨거운 물체를 만져도 느끼지 못해 위험하기도 합니다. <strong>인공 멀티모달 손끝을 의수에 적용</strong>한다면 이러한 제약을 크게 줄일 수 있습니다. 우선 의수가 <strong>물체를 쥐는 힘을 1mN 단위로 제어</strong>할 수 있게 되어 달걀처럼 약한 물체도 깨뜨리지 않고 집을 수 있고, 반대로 무거운 물건도 미끄러지지 않을 적절한 힘을 자동 조절할 수 있습니다. 또, 잡은 물체의 <strong>질감과 온도, 맥동</strong>까지 감지하여 사용자에게 피드백해줄 수 있다면, 사용자는 <strong>거의 실제 손과 유사한 촉감 경험</strong>을 얻을 수 있을 것입니다. 예를 들어 따뜻한 커피잔을 쥐었을 때 온기가 느껴지고, 사과를 쥐었을 때 표면의 매끈함과 단단함이 느껴진다면 사용자는 훨씬 자연스럽게 의수를 자신의 일부처럼 여길 수 있을 것입니다. 실시간 촉각 피드백을 사용자에게 전달하는 기술(신경 인터페이스나 촉각 자극 디스플레이)은 별도의 연구과제지만, 본 센서 기술의 존재로 인해 <strong>이식형 촉각 피드백 시스템</strong>도 구체적으로 논의할 수 있게 됩니다. 최소한의 단기 응용으로는, 의수에 멀티모달 손끝을 달아 <strong>물체가 미끄러질 때 자동으로 꽉 쥐거나</strong> 너무 뜨거울 때 즉각 놓아버리는 <strong>자동 반사 동작</strong>을 구현해줄 수 있습니다. 이는 감각이 없는 의수를 사용하는 사람에게 안전망이 되어 줄 것입니다. 더 나아가 의수 사용자에게 손끝 센서 데이터를 진동이나 전기 자극으로 환원(haptic feedback)해 주면, 사용자 스스로 힘 조절을 학습하여 더욱 섬세한 작업 (예: 계란 깨지 않게 들기, 옷감의 두께 느끼기 등)을 수행할 수도 있습니다. 이러한 보조공학 응용에서 가장 큰 <strong>기술적 이점</strong>은, <strong>사용자가 잃어버린 촉각 감각을 기계가 대폭 복원 또는 증강해줄 수 있다</strong>는 점입니다. 기존 의수 개발은 주로 모터 제어에 집중되어 촉각은 등한시되었는데, 이제는 센서 측면에서 인간 촉각에 필적하는 장치를 활용할 수 있게 된 것입니다.</p>
<p>이 밖에도 <strong>전자상거래(e-commerce)</strong> 분야에서의 잠재적 활용도 흥미롭습니다. 온라인 쇼핑에서는 소비자가 <strong>물품을 직접 만져볼 수 없다는 한계</strong>가 있는데, 장차 촉각 센싱 데이터베이스를 구축하여 이를 부분적으로나마 해소할 수 있습니다. 예를 들어 원단이나 의류 상품의 경우, Digit 360 센서로 <strong>천의 질감, 두께, 뻣뻣함, 부드러운 정도</strong> 등을 측정한 데이터를 제공함으로써 소비자가 촉감 특성을 유추하도록 할 수 있습니다. 또 전자제품 버튼의 조작감이나, 가구 표면의 마감 촉감 등을 정량화해 보여주는 것이 가능합니다. 나아가 매장에 <strong>원격 로봇</strong>을 배치하고 소비자가 집에서 VR 장비를 통해 로봇을 조작하면서 상품을 만져볼 수 있게 하는 시나리오도 생각해볼 수 있습니다. 이때 본 손끝 센서가 부착된 로봇이 상품의 촉감을 실시간 전송해주면, 온라인과 오프라인의 체험 격차를 크게 좁힐 수 있을 것입니다. 또한 물류 분야에서 로봇이 상품을 집을 때 얻는 촉감 정보를 바탕으로 <strong>상품의 파손 여부나 상태 이상</strong>을 자동으로 판별하는 등의 응용도 가능합니다. 예컨대 택배 상자를 로봇팔이 쥐었을 때 살짝 눌러본 촉감으로 내부 내용물이 깨졌는지(딸그락 거리는 진동? 균일하지 않은 단단함?)를 감지한다면 <strong>비대면 자동 검사</strong>가 될 수 있습니다. 이러한 전자상거래 및 물류 응용은 아직은 개념적인 단계이지만, <strong>촉각 데이터의 축적과 활용</strong>이라는 새로운 영역을 개척할 수 있다는 점에서 주목할 만합니다.</p>
</section>
</section>
<section id="한계점-및-향후-과제-도전-과제와-비판적-고찰" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="한계점-및-향후-과제-도전-과제와-비판적-고찰"><span class="header-section-number">2.3</span> 한계점 및 향후 과제: 도전 과제와 비판적 고찰</h2>
<p>혁신적인 인공 멀티모달 손끝 기술이지만, 실제 활용을 위해서는 몇 가지 <strong>극복해야 할 한계와 고려사항</strong>이 존재합니다. 여기서는 <strong>실험적 제약</strong>, <strong>내구성 및 환경적 요인</strong>, <strong>비용/복잡성</strong>, <strong>데이터 처리 및 확장성</strong> 측면에서 주요 한계를 지적하고, 동시에 연구 개발자들이 관심을 가질 만한 <strong>후속 연구 방향과 질문</strong>을 제시합니다.</p>
<p><strong>내구성 및 환경 적응성</strong>:</p>
<p>현재 공개된 결과는 실험실 환경에서 얻어진 성능 수치입니다. <strong>“이 방식이 실제 환경에서 견딜 수 있는가?”</strong> 라는 의문은 여전히 남습니다. 예컨대 산업 현장의 먼지, 기름기, 온도 변화, 충격 등에 노출되었을 때 <strong>광학식 + 젤 구조</strong>의 센서가 얼마나 유지될지 알 수 없습니다. 젤 표면의 얇은 은 코팅은 반복 접촉으로 마모되거나 긁힐 우려가 있고, 젤 자체도 날카로운 모서리에 여러 번 눌리면 찢어질 가능성이 있습니다. 논문에서는 일정 힘 이상 가해지면 손끝이 본체에서 분리되는 한계(force threshold)도 실험으로 보였는데, 이는 최대 하중 이상에서는 센서가 물리적으로 파손될 수 있음을 의미합니다. 따라서 거친 산업 작업이나 돌발적인 큰 충격이 가해지는 상황에서 센서를 보호할 수 있는 <strong>외장 커버나 오버레이</strong> 등에 대한 추가 설계가 필요할 것입니다. 그러나 보호층을 추가하면 감도가 낮아질 수 있으므로, <strong>내구성과 성능의 트레이드오프</strong>를 최적화해야 합니다. 또, 실외 환경 (예: 농업 로봇)에서는 온습도 변화로 젤의 탄성이 바뀌거나, 광학 부품에 습기가 차는 등의 변수가 있습니다. 이러한 환경 변수에 <strong>자동 보정(calibration)</strong>할 수 있는 기법이나 자가진단 기능도 후속 연구 과제입니다. <strong>장시간 사용에 따른 노화</strong>도 고려해야 하는데, 센서 재료(실리콘 젤, 코팅 등)가 수백 시간의 사용 후에도 초기 성능을 유지하는지, 혹은 일정 주기마다 <strong>젤 팁 교체</strong>가 필요한 소모품으로 봐야 할지 평가가 필요합니다. 다행히 모듈식 설계로 손쉽게 팁을 교체 가능하므로 현장에서의 유지보수는 큰 어려움이 없을 수 있으나, <strong>잦은 교체가 필요하다면 비용과 다운타임</strong> 문제가 발생할 것입니다.</p>
<p><strong>복잡성 및 비용 문제</strong>:</p>
<p>Digit 360 센서는 최첨단 소자로 구성된 <em>고급 연구 플랫폼</em>입니다. 830만 화소의 카메라, 커스텀 광각 렌즈, 다종의 MEMS 센서, 전용 AI 칩 등 <strong>부품 구성이 복잡하며 원가도 높을 것</strong>으로 예상됩니다. 논문에서도 이전 세대인 Lambeta 등(2020)의 저가형 촉각센서와 대비되는 <strong>하이엔드 디자인</strong>임을 밝히고 있습니다. <strong>일반 상용화</strong>를 위해서는 이러한 복잡성을 줄이고 비용을 낮추는 작업이 필수입니다. 예를 들어 활용 분야별로 정말 필요한 모달리티만 남기고 제거한다든지, 카메라 해상도를 약간 낮춰도 용인되는 작업에는 저해상도 이미지 센서를 써서 가격을 줄이는 식의 <strong>라인업 다양화</strong>가 고려될 수 있습니다. 실제로 모듈식 구조이기에 이러한 <strong>스케일 다운</strong>도 비교적 수월할 것으로 보입니다. 하지만 성능과 비용은 반비례하기 쉽기 때문에, 어느 정도 성능을 포기하고 상용화할지에 대한 <strong>기준점 설정</strong>이 필요합니다. 복잡성이 높다는 것은 <strong>조립과 제조의 어려움</strong>도 의미합니다. 예컨대 논문에서 사용된 <strong>맞춤형 하이퍼어안 렌즈</strong>는 일반 제품이 아니어서 제조사가 한정적이고 가격도 높을 것입니다. 은 코팅 같은 공정도 일반 전자제품 제조라인에서는 특수한 공정입니다. 따라서 대량 생산 단계에서 <strong>공정 표준화와 비용 절감</strong>을 위한 공학적 연구가 필요합니다. 또한 부품이 많아지면 고장 가능성도 높아지므로 신뢰성 확보도 과제입니다. 향후 연구에서는 센서 구조 단순화를 위해 <strong>한 가지 소자로 다중 물리량을 감지</strong>하는 방법 (예: 광센서만으로 온도도 추정하거나, 압전소자로 진동+힘 동시 측정 등)도 모색될 수 있습니다. 궁극적으로는 성능을 크게 해치지 않으면서도 <strong>부품수를 줄이고 가격을 낮추는 엔지니어링</strong>이 이루어져야 산업계에서 폭넓게採用될 것입니다. 이와 관련하여 <strong>“이 정도로 복잡한 센서가 상용 로봇에 현실적으로 부착될 수 있을까?”</strong> 하는 질문에 답하기 위해서는, 실제 현장에서 요구되는 적정 성능과 허용 가능한 비용의 균형점을 연구 커뮤니티와 산업계가 함께 찾아나가야 할 것입니다.</p>
<p><strong>데이터 처리 및 학습 측면 한계</strong>:</p>
<p>멀티모달 손끝이 만들어내는 방대한 데이터 스트림을 <strong>실시간으로 처리</strong>하는 것은 또 다른 도전입니다. 비록 온보드 NPU로 1차 처리를 한다고 해도, 8백만 픽셀 이미지(240fps)와 다수 센서신호를 다루는 일은 여전히 <strong>연산 부담과 통신 부담</strong>이 큽니다. 현재 프로토타입에서는 간단한 힘 추정이나 분류 작업을 데모했지만, 향후 더 복잡한 작업(예: 동시에 여러 손가락 데이터를 통합해 물체 모양을 재구성하거나, 연속적인 촉각 탐색을 통한 물체 인식 등)을 수행하려면 더 고성능의 프로세싱이나 효율적인 알고리즘이 필요합니다. 딥러닝을 활용하는 경우, <strong>학습 데이터셋 확보와 학습 시간</strong>도 고려해야 합니다. 논문에서는 비교적 제한된 환경(일정한 물체들, 정해진 동작들)에서 모델을 학습시켰는데, 이를 실제로 다양하고 예측 불가능한 상황에 대응하려면 훨씬 광범위한 데이터가 필요할 수 있습니다. 예컨대 수천 종류의 물체 재질을 구분하려면 그만큼 데이터 수집이 필요한데, 촉각 데이터 수집은 시각 데이터 대비 시간이 오래 걸리고 자동화하기 어려운 측면이 있습니다. 따라서 <strong>데이터 효율적인 학습 방법</strong> (예: 자율적 탐색을 통한 자기지도 학습, 도메인랜덤화나 시뮬레이션 활용)이 중요한 연구 과제가 될 것입니다. 또한 여러 모달의 데이터는 <strong>시간 동기화(synchronization)</strong>와 <strong>샘플링 레이트 차이</strong> 문제도 존재합니다. 카메라는 240Hz이지만 마이크는 20kHz 수준으로 샘플링되므로, 이를 한 신경망에 넣을 때 <strong>시계열 정렬</strong>을 어떻게 할지 등이 난제입니다. 논문에서는 융합 전에 각각 적절한 창(window)으로 묶어 입력하였지만, 일반적인 임의 동작에서는 최적의 윈도우 설정이나 멀티모달 특징 추출 방법론 연구가 더 필요합니다. <em>센서 융합이 학습과 추론에 미치는 영향</em>을 규명하기 위해서는, <strong>각 모드별 신호의 상호보완적 정보량</strong>을 이론적으로 분석하거나 다양한 모델 아키텍처(예: transformer 기반 멀티모달 인식 등)를 시험해볼 필요가 있습니다. 요약하면, <strong>“방대한 촉각 데이터를 얼마나 효율적으로 처리하고 학습시킬 것인가?”</strong>가 중요한 질문이며, 이는 센서 하드웨어뿐만 아니라 <strong>소프트웨어 알고리즘 측면의 혁신</strong>을 지속적으로 요구하는 분야입니다. 향후 연구에서는 경량화된 학습 모델 설계, 중요 촉각 특징만 추출하는 <strong>특징 공학 또는 압축 기술</strong>, 그리고 로봇의 전략적인 <strong>능동적 탐색(active sensing)</strong>과의 연계 등으로 이 문제에 도전해야 할 것입니다.</p>
<p><strong>다수 센서의 확장성과 표준화</strong>:</p>
<p>현재 프로토타입은 손가락 하나에 대한 것이지만, 사람 손은 다섯 손가락이 모두 촉각을 협업합니다. 로봇 핸드에 5개의 Digit 360 센서를 모두 장착하여 <strong>동시에 구동</strong>할 경우, 데이터량과 시스템 복잡성은 더욱 증가합니다. 손가락마다 온보드 처리를 하더라도, 최종적으로 로봇 컨트롤러는 여러 손끝에서 오는 정보를 통합해야 합니다. 이때 여러 손끝 간 <strong>데이터 동기화와 통신 지연</strong> 문제가 발생할 수 있고, 손가락들 사이의 <strong>상호 간섭(예: 한 손가락의 진동이 다른 손가락의 마이크에 들어가는 등)</strong>도 고려해야 합니다. 이러한 <strong>멀티-핑거 센서 네트워크</strong>를 효과적으로 구성하는 방법 역시 향후 과제입니다. 한 가지 방향은, 인간의 말초신경계를 모방하여 <strong>분산처리 + 중앙요약</strong> 구조를 취하는 것입니다. 이미 손끝 개별 처리는 구현되었으니, 향후에는 여러 손끝의 고레벨 정보를 다시 통합해 맥락을 해석하는 상위 레벨 AI가 필요합니다. 예를 들어 물체를 양 손가락으로 집었다면 양쪽 센서의 힘/변형 데이터를 결합해 물체의 물성이나 잡힘 상태를 추론하는 식입니다. 이러한 멀티-센서 융합은 아직 개척되지 않은 영역으로, <strong>새로운 모델과 제어 프레임워크</strong> 연구가 필요합니다.</p>
<p>또한 촉각센서 분야 전반의 <strong>재현성(reproducibility)</strong>과 <strong>표준화</strong> 이슈도 존재합니다. 논문은 센서 설계를 공개했지만, 이를 <strong>다른 연구자들이 쉽게 제작</strong>하여 실험할 수 있는지에는 의문이 있습니다. 앞서 언급한 특수 부품 수급이나 제작 공정 문제로 누구나 만들 수 있는 수준은 아닐 수 있습니다. 만약 이 센서가 상용 제품으로 출시되지 않는다면, 연구 커뮤니티에서 널리 활용되기까지는 시간이 걸릴 수 있습니다. 따라서 후속 연구로, 이와 유사한 성능을 내면서도 <strong>제작 용이성이 높은 대안 센서</strong> 개발도 고려될 수 있습니다. 또는 주요 부품 (예: 렌즈, 코팅된 젤)을 모듈 형태로 판매하여 조립을 쉽게 하는 방안도 있을 것입니다. 더불어 촉각 연구의 발전을 위해서는 <strong>평가 기준의 표준화</strong>도 중요합니다. 이 논문에서는 7μm, 1mN 등의 수치를 썼지만, 다른 연구에서는 각기 다른 조건으로 성능을 재기 때문에 직접 비교가 어렵습니다. 향후에는 촉각 센서의 <strong>공인된 벤치마크 테스트</strong>(예: 표준화된 패턴으로 공간해상도 측정, 규격화된 힘센서로 최소 감지력 측정 등)가 마련되어야, 여러 접근법 간 객관적 비교와 선택이 가능해질 것입니다. Digit 360의 등장은 이러한 논의를 촉발시킬 것으로 보이며, 이를 계기로 <strong>촉각 센서 분야의 협력적 발전</strong>이 가속되길 기대합니다.</p>
<blockquote class="blockquote">
<p>미래를 향한 질문들</p>
<p>마지막으로, 본 리뷰를 통해 도출된 핵심 질문들을 정리하면 다음과 같습니다:</p>
</blockquote>
<ul>
<li><p><strong>이 인공 손끝 센서가 인간 수준을 뛰어넘는 성능을 달성했지만, 실제 복잡한 물리 환경과 장기간 사용에서도 그 </strong>정밀도와 신뢰성<strong>을 유지할 수 있을까요?</strong> 이는 센서의 내구성, 환경 적응성에 관한 질문으로, 향후 혹독한 조건에서의 장기 테스트와 보완 설계 연구가 필요합니다.</p></li>
<li><p><strong>다양한 모달리티를 통합한 촉각 데이터는 풍부하지만, 그 </strong>방대한 정보를 실시간으로 처리<strong>하여 로봇 행동에 활용하는 데에는 어떤 제약이 있을까요?</strong> 여러 센서의 데이터 융합이 가져오는 학습상의 이득과 비용을 정량화하고, 효율적인 처리 알고리즘을 개발하는 것이 중요합니다.</p></li>
<li><p><strong>복잡하고 값비싼 이 연구 시제품을 </strong>현실에서 경제적으로 구현<strong>하려면 무엇을 타협하고 개선해야 할까요?</strong> 예컨대 불필요한 기능을 줄이는 모듈 구성, 저가 대체 소재 활용 등의 엔지니어링 과제가 있습니다. 이는 기술적 성능과 상용화 가능성 사이의 균형점을 묻는 질문입니다.</p></li>
</ul>
<p>이러한 질문들은 아직 완전히 답변되지 않았지만, <em>Digitizing Touch</em> 논문은 <strong>촉각센서 연구의 지평을 크게 확장</strong>하며 동시에 새로운 도전들을 부각시켰습니다. 앞으로 소재공학, 광학설계, 신호처리, 인공지능 등 다양한 분야의 협력을 통해 이 질문들에 대한 해답이 모색될 것입니다. 결론적으로, 인공 멀티모달 손끝 기술은 인간 촉각의 풍부함을 기계에 불어넣음으로써 <strong>로봇과 인간의 상호작용 방식에 근본적 변화를 가져올 잠재력</strong>을 보여주었습니다.</p>
</section>
</section>
<section id="reference" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Reference</h1>
<ul>
<li><a href="https://arxiv.org/pdf/2411.02479">Digitizing Touch with an Artificial Multimodal Fingertip</a></li>
<li><a href="https://arxiv.org/pdf/2410.24090">Sparsh: Self-supervised touch representations for vision-based tactile sensing</a></li>
<li><a href="https://github.com/facebookresearch/digit360">https://github.com/facebookresearch/digit360</a></li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="curieuxjy/blog_comments" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Copyright 2024, Jung Yeon Lee</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/curieuxjy">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>