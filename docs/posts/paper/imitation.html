<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.313">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="description" content="Reproducing the diverse and agile locomotion skills of animals">

<title>Curieux.JY - ğŸ“ƒLearning Agile Robotic Locomotion Skills by Imitating Animals</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../profile.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Curieux.JY</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">
 <span class="menu-text">Jung Yeon Lee</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../note.html">
 <span class="menu-text">Note</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/curieuxjy"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#abstract" id="toc-abstract" class="nav-link active" data-scroll-target="#abstract">0. Abstract</a></li>
  <li><a href="#i.-introduction" id="toc-i.-introduction" class="nav-link" data-scroll-target="#i.-introduction">I. Introduction</a></li>
  <li><a href="#ii.-related-work" id="toc-ii.-related-work" class="nav-link" data-scroll-target="#ii.-related-work">II. RELATED WORK</a>
  <ul class="collapse">
  <li><a href="#motion-imitation" id="toc-motion-imitation" class="nav-link" data-scroll-target="#motion-imitation">Motion imitation</a></li>
  <li><a href="#sim-to-real-transfer" id="toc-sim-to-real-transfer" class="nav-link" data-scroll-target="#sim-to-real-transfer">Sim-to-real transfer</a>
  <ul class="collapse">
  <li><a href="#ours" id="toc-ours" class="nav-link" data-scroll-target="#ours">Ours</a></li>
  </ul></li>
  <li><a href="#rl-for-legged-locomotion" id="toc-rl-for-legged-locomotion" class="nav-link" data-scroll-target="#rl-for-legged-locomotion">RL for legged locomotion</a>
  <ul class="collapse">
  <li><a href="#ours-1" id="toc-ours-1" class="nav-link" data-scroll-target="#ours-1">Ours</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#iii.-overview" id="toc-iii.-overview" class="nav-link" data-scroll-target="#iii.-overview">III. OVERVIEW</a>
  <ul class="collapse">
  <li><a href="#motion-retargeting" id="toc-motion-retargeting" class="nav-link" data-scroll-target="#motion-retargeting">1. Motion Retargeting</a></li>
  <li><a href="#motion-imitation-1" id="toc-motion-imitation-1" class="nav-link" data-scroll-target="#motion-imitation-1">2. Motion Imitation</a>
  <ul class="collapse">
  <li><a href="#reward-function" id="toc-reward-function" class="nav-link" data-scroll-target="#reward-function">Reward Function</a></li>
  </ul></li>
  <li><a href="#domain-adaptation" id="toc-domain-adaptation" class="nav-link" data-scroll-target="#domain-adaptation">Domain Adaptation</a></li>
  <li><a href="#a.-domain-randomization" id="toc-a.-domain-randomization" class="nav-link" data-scroll-target="#a.-domain-randomization">A. Domain Randomization</a></li>
  <li><a href="#b.-domain-adaptation" id="toc-b.-domain-adaptation" class="nav-link" data-scroll-target="#b.-domain-adaptation">B. Domain Adaptation</a></li>
  <li><a href="#c.-real-world-transfer" id="toc-c.-real-world-transfer" class="nav-link" data-scroll-target="#c.-real-world-transfer">C. Real World Transfer</a></li>
  </ul></li>
  <li><a href="#vii.-experimental-evaluation" id="toc-vii.-experimental-evaluation" class="nav-link" data-scroll-target="#vii.-experimental-evaluation">VII. EXPERIMENTAL EVALUATION</a>
  <ul class="collapse">
  <li><a href="#a.-experimental-setup" id="toc-a.-experimental-setup" class="nav-link" data-scroll-target="#a.-experimental-setup">A. Experimental Setup</a></li>
  <li><a href="#model-representation" id="toc-model-representation" class="nav-link" data-scroll-target="#model-representation">Model Representation</a></li>
  <li><a href="#b.-learned-skills" id="toc-b.-learned-skills" class="nav-link" data-scroll-target="#b.-learned-skills">B. Learned Skills</a></li>
  </ul></li>
  <li><a href="#viii.-discussion-and-future-work" id="toc-viii.-discussion-and-future-work" class="nav-link" data-scroll-target="#viii.-discussion-and-future-work">VIII. DISCUSSION AND FUTURE WORK</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-page-left" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">ğŸ“ƒLearning Agile Robotic Locomotion Skills by Imitating Animals</h1>
  <div class="quarto-categories">
    <div class="quarto-category">quadruped</div>
    <div class="quarto-category">rl</div>
    <div class="quarto-category">imitation</div>
    <div class="quarto-category">paper</div>
  </div>
  </div>

<div>
  <div class="description">
    Reproducing the diverse and agile locomotion skills of animals
  </div>
</div>


<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p><img src="https://i.imgur.com/1Fh5MaD.png?1" class="img-fluid"></p>
<section id="abstract" class="level1">
<h1>0. Abstract</h1>
<blockquote class="blockquote">
<p>Abstractâ€”<code>Reproducing the diverse and agile locomotion skills of animals</code> has been a longstanding challenge in robotics. While manually-designed controllers have been able to emulate many complex behaviors, building such controllers involves a time-consuming and difficult development process, often requiring substantial expertise in the nuances of each skill. Reinforcement learning provides an appealing alternative for automating the manual effort involved in the development of controllers. However, <code>designing learning objectives that elicit the desired behaviors from an agent</code> can also require a great deal of skill-specific expertise. In this work, we present an <code>imitation learning system</code> that enables legged robots to <code>learn agile locomotion skills by imitating real-world animals</code>. We show that by <code>leveraging reference motion data</code>, a single learning-based approach is able to automatically synthesize controllers for a diverse repertoire of behaviors for legged robots. By <code>incorporating sample efficient domain adaptation techniques</code> into the training process, our system is able to learn adaptive policies in simulation that can then be quickly adapted for real-world deployment. To demonstrate the effectiveness of our system, we train an <code>18DoF</code> quadruped robot to perform a variety of agile behaviors ranging from different locomotion gaits to dynamic hops and turns.</p>
</blockquote>
<ul>
<li>ë™ë¬¼ë“¤ì˜ locomotion skillë“¤ì„ ë¡œë´‡ìœ¼ë¡œ ì¬í˜„í•˜ëŠ” ê²ƒì€ ì •ë§ challengingí•˜ë‹¤</li>
<li>agentì—ê²Œ ì›í•˜ëŠ” í–‰ë™ì„ ëª…í™•í•˜ê²Œ objectiveë¡œ ì œì‹œí•˜ëŠ” ê²ƒë„ ë§¤ìš° ì–´ë µë‹¤.</li>
<li>imitation learning ê¸°ë²•ì„ ì´ìš©í•˜ì—¬ ë™ë¬¼ë“¤ì˜ locomotion skillë“¤ì„ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ í–ˆë‹¤.</li>
<li>reference motion dataë“¤ì„ ì´ìš©í•´ì„œ controllerì— í•™ìŠµê¸°ë°˜ì˜ ì ‘ê·¼ë²•ì„ ë°”ë¡œ ë„£ì„ ìˆ˜ ìˆì—ˆê³  domain adaptation ê¸°ë²•ì„ training ê³¼ì •ì— ë„£ì–´ì£¼ì–´ì„œ ì‹¤ì œ ë¡œë´‡ì— deploymentë¥¼ í–ˆì„ ë•Œ ë°”ë¡œ ì˜ ë™ì‘í•  ìˆ˜ ìˆì—ˆë‹¤.</li>
<li>18 ììœ ë„ë¥¼ ê°€ì§€ëŠ” 4ì¡± ë³´í–‰ ë¡œë´‡ì„ ê°€ì§€ê³  ë‹¤ì–‘í•œ locomotion gaitë“¤ê³¼ dynamic hopì´ë‚˜ turnì´ ê°€ëŠ¥í–ˆë‹¤.</li>
</ul>
</section>
<section id="i.-introduction" class="level1">
<h1>I. Introduction</h1>
<ul>
<li>While these methods have demonstrated promising results in simulation, agents trained through RL are prone to adopting unnatural behaviors that are dangerous or infeasible when deployed in the real world</li>
<li>one to wonder: can we build more agile robotic controllers with less effort by directly imitating animal motions?</li>
<li>The use of reference motions alleviates the need to design skill-specific reward functions</li>
<li>In order to transfer policies learned in simulation to the real world, we propose a sample efficient adaptation technique, which fine-tunes the behavior of a policy</li>
<li>Laikago quadruped robot</li>
<li>different locomotion gaits, as well as dynamic hops and turns</li>
</ul>
<hr>
<ul>
<li>RLë¡œ í•™ìŠµí•œ agentë“¤ì€ ì‹œë®¬ë ˆì´ì…˜ì—ì„œ ì¢‹ì€ ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ì§€ë§Œ ë§‰ìƒ ì‹¤ì œ ë¡œë´‡ì— ì ìš©ë˜ì—ˆì„ ë•Œ ë¶€ìì—°ìŠ¤ëŸ½ê³  ìœ„í—˜í•œ ë™ì‘ë“¤ì„ ë³´ì—¬ì¤„ ë•Œê°€ ìˆë‹¤.</li>
<li>ê·¸ë˜ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ë°”ë¡œ ë™ë¬¼ë“¤ì˜ ëª¨ì…˜ì„ ë”°ë¼ì„œ ë¡œë´‡ë“¤ì´ ì›€ì§ì´ë„ë¡ í•˜ë©´ ë˜ì§€ ì•Šì„ê¹Œ? ë¼ëŠ” ìƒê°ì„ í•˜ê²Œ ëœë‹¤.</li>
<li>referenceê°€ ë  ìˆ˜ ìˆëŠ” motion ë°ì´í„°ë“¤ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì€ íŠ¹ì • skillë“¤ì„ ìœ„í•œ reward functionì„ ë§Œë“œëŠ” ì–´ë ¤ì›€ì„ ì¤„ì—¬ì¤„ ìˆ˜ ìˆë‹¤.</li>
<li>ì‹œë®¬ë ˆì´ì…˜ì—ì„œ í•™ìŠµí•œ policyë¥¼ ì‹¤ì œ ë¡œë´‡ì— ì ìš©í•˜ê¸° ìœ„í•´ sample efficientí•œ adaptation techniqueì„ ì œì•ˆí–ˆëŠ”ë°, ì´ëŠ” policyë¡œ ë§Œë“¤ì–´ì§€ëŠ” behaviorë¥¼ fine-tuningí•˜ëŠ” ê²ƒì´ë‹¤.</li>
<li>Laikago 4ì¡± ë³´í–‰ ë¡œë´‡ìœ¼ë¡œ ë‹¤ì–‘í•œ locomotion gait(ì´ë™ ê±¸ìŒìƒˆ)ë“¤ì„ í•™ìŠµí•  ìˆ˜ ìˆì—ˆê³  dynaamic hopsì™€ turnë“¤ë„ ê°€ëŠ¥í–ˆë‹¤.</li>
</ul>
</section>
<section id="ii.-related-work" class="level1">
<h1>II. RELATED WORK</h1>
<ul>
<li>Trajectory optimization and model predictive control can mitigate some of the manual effort involved in the design process, but due to the high-dimensional and complex dynamics of legged systems, reduced-order models are often needed to formulate tractable optimization problems</li>
<li>Trajectory optimizationì´ë‚˜ model predictive control ê°™ì€ ë°©ë²•ë“¤ì€ controller ë””ìì¸ ê³¼ì •ì—ì„œ ë§ì€ manual í•œ ë¶€ë¶„ë“¤ì„ ì—†ì• ì£¼ê¸´ í–ˆì§€ë§Œ ë³´í–‰ ë¡œë´‡ë“¤ì˜ ê³ ì°¨ì›ì ì´ê³  ë³µì¡í•œ ë™ì—­í•™ ê³¼ì • ë•Œë¬¸ì— ì¶•ì•½ëœ ëª¨ë¸ë“¤ì„ ì‚¬ìš©í–ˆì—ˆë‹¤.</li>
</ul>
<section id="motion-imitation" class="level2">
<h2 class="anchored" data-anchor-id="motion-imitation">Motion imitation</h2>
<ul>
<li>But applications of motion imitation to legged robots have predominantly been limited to behaviors that emphasize upper-body motions, with fairly static lower-body movements, where balance control can be delegated to separate control strategies</li>
<li>Recently, motion imitation with reinforcement learning has been effective for learning a large repertoire of highly acrobatic skills in simulation [44, 34, 45, 32]</li>
<li>ë³´í–‰ë¡œë´‡ë“¤ì˜ applicationì— motion imitationì€ ìƒë‹¹íˆ ì •ì ì¸ lower-bodyì˜ ì›€ì§ì„ìœ¼ë¡œ upper-bodyì˜ ëª¨ì…˜ì„ ë„ˆë¬´ ì‹ ê²½ì“°ëŠ” í–‰ë™ë“¤ì— ì œí•œë¬ì—ˆë‹¤. ê·¸ë˜ì„œ balance controlì´ ì œì–´ ì „ëµìœ¼ë¡œ ë”°ë¡œ ë¶„ë¦¬ë¬ì—ˆë‹¤.</li>
<li>ìµœê·¼ ì—°êµ¬ë“¤ë¡œ RLë¡œ í•˜ëŠ” motion imitationì€ ì‹œë®¬ë ˆì´ì…˜ì—ì„œ ë§¤ìš° acrobaticí•œ skillë“¤ì„ ì˜ í•™ìŠµí•˜ëŠ” ê²ƒìœ¼ë¡œ ì•Œë ¤ì¡Œë‹¤.</li>
</ul>
</section>
<section id="sim-to-real-transfer" class="level2">
<h2 class="anchored" data-anchor-id="sim-to-real-transfer">Sim-to-real transfer</h2>
<ul>
<li>by constructing more accurate simulations [58, 62], or adapting the simulator with realworld data [57, 23, 26, 36, 5].</li>
<li>Domain randomization can be incorporated into the training process to encourage policies to be robust to variations in the dynamics [52, 60, 47, 42, 41].</li>
<li>Sample efficient adaptation techniques, such as finetuning [51] and meta-learning [13, 16, 6] can also be applied to further improve the performance of pre-trained policies in new domains.</li>
</ul>
<hr>
<p>ê°•í™”í•™ìŠµì€ ìš°ì„  ì‹œë®¬ë ˆì´ì…˜(source domain)ì—ì„œ ë¨¼ì € í•™ìŠµì„ ì‹œí‚¨ í›„ , real world(target domain)ì— í•™ìŠµëœ agentê°€ ë™ì‘í•˜ê²Œ ëœë‹¤. ì´ ê³¼ì •ì—ì„œ ì‹œë®¬ë ˆì´ì…˜ê³¼ real world ì‚¬ì´ì˜ ì°¨ì´(gap)ì´ ìƒê¸°ê²Œ ë˜ê³  ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ë§ì€ ë°©ë²•ë“¤ì´ ì œì‹œëœë‹¤.</p>
<ul>
<li>ë§¤ìš° ì •í™•í•œ ì‹œë®¬ë ˆì´í„°ë¥¼ ë§Œë“¤ê±°ë‚˜ real world ë°ì´í„°ë¥¼ ê°€ì§€ê³  ì‹œë®¬ë ˆì´í„°ë¥¼ ë§ì¶°ì„œ sim-to-real transferringì„ í•´ê²°í•˜ê³ ì í–ˆì—ˆë‹¤.</li>
<li>Domain randomizationì€ trainingê³¼ì •ì—ì„œ policyê°€ ë‹¤ì–‘í•œ ë‹¤ì´ë‚˜ë¯¹ìŠ¤ë¥¼ ì ‘í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ë°©ë²•ì´ë‹¤.
<ul>
<li>ì—¬ê¸°ì„œ domainì´ë¼ í•¨ì€ ë¬¼ë¦¬ì ì¸ ì¡°ê±´, ë¡œë´‡ì˜ ì§ˆëŸ‰, ë§ˆì°°ë ¥ë“±ê³¼ ê°™ì€ ë¡œë´‡ì´ ë™ì‘í•˜ëŠ” í™˜ê²½ì˜ ë³€ìˆ˜ ì¡°ê±´ë“¤ì˜ ì§‘í•©ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤.</li>
<li>ì´ëŸ° ë¬¼ë¦¬ì ì¸ ê°’ë“¤ì„ ì‹œë®¬ë ˆì´ì…˜ì—ì„œ ì‹¤ì œ ë¡œë´‡ì´ ë™ì‘í•  í™˜ê²½ê³¼ ë˜‘ê°™ì´ ë§ì¶°ì£¼ê¸° í˜ë“¤ë‹¤.</li>
<li>ì´ëŸ° ê°’ë“¤ì´ ì¡°ê¸ˆì´ë¼ë„ ë‹¬ë¼ì ¸ì„œ ë¡œë´‡ì´ ì œëŒ€ë¡œ ë™ì‘í•˜ì§€ ëª»í•˜ê³  ë‹¤ì‹œ ì²˜ìŒë¶€í„° í•™ìŠµì„ í•´ì•¼ í•œë‹¤ë©´ ë§¤ìš° ë‚œê°í•œ ìƒí™©ì´ë˜ê³  ì‚¬ì‹¤ìƒ ë¡œë´‡ì„ ì“¸ ìˆ˜ ì—†ëŠ” ìƒí™©ì¼ ê²ƒì´ë‹¤.</li>
</ul></li>
<li>Fine-tuningì´ë‚˜ meta-learningê³¼ ê°™ì€ sample efficient adaptation ê¸°ë²•ë“¤ì„ ì‚¬ìš©í•´ì„œ ìƒˆë¡œìš´ domainì— ë†“ì¸ pre-trained policyë“¤ì´ í¼í¬ë¨¼ìŠ¤ë¥¼ ì˜ ë‚˜íƒ€ë‚´ê²Œ í•˜ëŠ” ë°©ë²•ë„ ìˆë‹¤.</li>
</ul>
<section id="ours" class="level3">
<h3 class="anchored" data-anchor-id="ours">Ours</h3>
<ul>
<li>latent space methods [24, 65, 67], to transfer locomotion policies from simulation to the real world</li>
<li>During pre-training, these methods learn a latent representation of different behaviors that are effective under various scenarios.</li>
<li>When transferring to a new domain, a search can be conducted in the latent space to find behaviors that successfully execute a desired task in the new domain.</li>
<li>by combining motion imitation and latent space adaptation, our system is able to learn a diverse corpus of dynamic locomotion skills that can be transferred to legged robots in the real world.</li>
</ul>
<hr>
<p>ë³¸ ë…¼ë¬¸ì˜ ë°©ë²•ì€</p>
<ul>
<li>latent space ë°©ë²•ì„ ì´ìš©í•´ì„œ ì‹œë®¬ë ˆì´ì…˜ì—ì„œ real worldì— ë°°í¬í•  ë•Œ locomotion policyë“¤ì´ ì„±ê³µì ìœ¼ë¡œ transferringë˜ë„ë¡ í–ˆë‹¤.
<ul>
<li>pre-trainingê³¼ì •ì—ì„œ ì—¬ëŸ¬ behaviorë“¤ì˜ latent representationì„ í•™ìŠµí•˜ëŠ” ê²ƒì€ ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ë§¤ìš° íš¨ìœ¨ì ì´ë‹¤.</li>
<li>ìƒˆë¡œìš´ ë„ë©”ì¸ì— transferringë  ë•Œ, ìƒˆë¡œìš´ ë„ë©”ì¸ì—ì„œ desired taskë¥¼ í•˜ê¸° ìœ„í•œ behaviorê°€ latent spaceì—ì„œ ê²€ìƒ‰ëœë‹¤.</li>
</ul></li>
<li>motion imitationê³¼ latent space adaptationì„ í•©ì¹˜ë©´ì„œ ë‹¤ì–‘í•œ locomotion skillë“¤ì„ í•™ìŠµí•  ìˆ˜ ìˆì—ˆì„ ë¿ë§Œ ì•„ë‹ˆë¼ ì‹¤ì œ ë¡œë´‡ì—ë„ ì ìš©í•  ìˆ˜ ìˆì—ˆë‹¤.</li>
</ul>
</section>
</section>
<section id="rl-for-legged-locomotion" class="level2">
<h2 class="anchored" data-anchor-id="rl-for-legged-locomotion">RL for legged locomotion</h2>
<ul>
<li>Hwangbo et al.&nbsp;[26] proposed learning a motor dynamics model using real-world data, which enabled direct transfer of a variety of locomotion skills to the ANYmal robot
<ul>
<li>manually-designed reward functions for each skill</li>
</ul></li>
<li>Xie et al.&nbsp;[62] trained bipedal walking policies for the Cassie robot by imitating reference motions recorded from existing controllers and keyframe animations.
<ul>
<li>transferred from simulation to the real world with the aid of careful system identification</li>
</ul></li>
<li>Yu et al.&nbsp;[65] transferred bipedal locomotion policies from simulation to a physical Darwin OP2 robot using a latent space adaptation method, which mitigates the dependency on accurate simulators</li>
</ul>
<section id="ours-1" class="level3">
<h3 class="anchored" data-anchor-id="ours-1">Ours</h3>
<ul>
<li>latent space method, but by combining it with motion imitation, our system enables real robots to perform more diverse and agile behaviors than have been demonstrated by these previous methods.</li>
</ul>
<hr>
<ul>
<li>ë§¤ìš° ì„¸ì„¸í•œ reward function ë””ìì¸ì´ë‚˜ system identificationì„ ì¼ë˜ ì´ì „ ë°©ë²•ë“¤ê³¼ ë‹¬ë¦¬ latent space methodë¥¼ motion imitationê³¼ í•©ì¹˜ë©´ì„œ ë” ë‹¤ì–‘í•˜ê³  agileí•œ behaviorë“¤ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆì—ˆë‹¤.</li>
</ul>
</section>
</section>
</section>
<section id="iii.-overview" class="level1">
<h1>III. OVERVIEW</h1>
<p><img src="https://i.imgur.com/1Fh5MaD.png?1" class="img-fluid"></p>
<ul>
<li>Our framework receives as input a reference motion that demonstrates a desired skill for the robot, which may be recorded using motion capture (mocap) of real animals (e.g.&nbsp;a dog).</li>
<li>Given a reference motion, it then uses reinforcement learning to synthesize a policy that enables a robot to reproduce that skill in the real world</li>
<li>3 stages
<ul>
<li><ol type="1">
<li>The reference motion is first processed by the motion retargeting stage, where the motion clip is mapped from the original subjectâ€™s morphology to the robotâ€™s morphology via inverse-kinematics.</li>
</ol></li>
<li><ol start="2" type="1">
<li>Next, the retargeted reference motion is used in the motion imitation stage to train a policy to reproduce the motion with a simulated model of the robot. To facilitate transfer to the real world, domain randomization is applied in simulation to train policies that can adapt to different dynamics.</li>
</ol></li>
<li><ol start="3" type="1">
<li>Finally, the policy is transferred to a real robot via a sample efficient domain adaptation process, which adapts the policyâ€™s behavior using a learned latent dynamics representation.</li>
</ol></li>
</ul></li>
</ul>
<hr>
<ul>
<li>ì „ì²´ì ì¸ ê³¼ì •ì„ ë³´ë©´ ì›í•˜ëŠ” skillì˜ reference motionì„ inputì„ ë°›ì•„ì„œ ê°•í™”í•™ìŠµìœ¼ë¡œ ê·¸ ëª¨ì…˜ì„ ë”°ë¼í•˜ë„ë¡ í•™ìŠµí•˜ê²Œ í•œë‹¤.</li>
<li>í¬ê²Œ 3ê°€ì§€ ê³¼ì •ìœ¼ë¡œ ì§„í–‰ë˜ë©°,</li>
</ul>
<ol type="1">
<li>reference motionì´ ë§Œë“¤ì–´ì§€ê³  ì›ë˜ ê·¸ í–‰ë™ì„ í•œ ëŒ€ìƒì˜ í˜•íƒœì™€ ë¡œë´‡ì˜ í˜•íƒœë¥¼ inverse-kinematicsë¥¼ ì´ìš©í•´ì„œ matchingí•´ì¤€ë‹¤.(ê°•ì•„ì§€ì˜ ì˜¤ë¥¸ìª½ ì•ë°œ ëê³¼ ë¡œë´‡ì˜ ì˜¤ë¥¸ìª½ ì• ë°œ ëì„ ë§ì¶°ì£¼ëŠ” ê³¼ì •)</li>
<li>ì‹œë®¬ë ˆì´ì…˜ ìƒì—ì„œ retargeted reference motionì„ ì¬í˜„í•˜ë„ë¡ í•™ìŠµí•œë‹¤. ì´ë•Œ domain randomizationì„ í†µí•´ real world transferringì„ ë” ë¹¨ë¦¬ í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤.</li>
<li>ì‹¤ì œ ë¡œë´‡ì— policyê°€ ë°°í¬ë˜ê³  latent dynamics representationì„ í†µí•´ policyì˜ behaviorê°€ ì˜ ì ìš©ë  ìˆ˜ ìˆë„ë¡ í•œë‹¤.</li>
</ol>
<section id="motion-retargeting" class="level2">
<h2 class="anchored" data-anchor-id="motion-retargeting">1. Motion Retargeting</h2>
<p><img src="https://i.imgur.com/mb8oQEE.png?1" class="img-fluid"></p>
<ul>
<li>To address this discrepancy, the source motions are retargeted to the robotâ€™s morphology using inverse-kinematics [19]</li>
<li>The keypoints include the positions of the feet and hips</li>
<li>At each timestep, the source motion specifies the 3D location <span class="math inline">\hat{\mathbf{X}}_{i}(t)</span> of each keypointi. The corresponding target keypoint <span class="math inline">\mathbf{x}_{i}\left(\mathbf{q}_{t}\right)</span> is determined by the robotâ€™s pose <span class="math inline">\mathbf{q}_{t}</span>, represented in generalized coordinates</li>
<li>IK is then applied to construct a sequence of poses <span class="math inline">\mathbf{q}_{0: T}</span> that track the keypoints at each frame</li>
<li><span class="math inline">\underset{\mathbf{q}_{0: T}}{\arg \min } \sum_{t} \sum_{i}\left\|\hat{\mathbf{x}}_{i}(t)-\mathbf{x}_{i}\left(\mathbf{q}_{t}\right)\right\|^{2}+\left(\overline{\mathbf{q}}-\mathbf{q}_{t}\right)^{T} \mathbf{W}\left(\overline{\mathbf{q}}-\mathbf{q}_{t}\right)</span></li>
<li>additional regularization term is included to encourage the poses to remain similar to a default pose <span class="math inline">\overline{\mathbf{q}}</span></li>
<li><span class="math inline">\mathbf{W}=\operatorname{diag}\left(w_{1}, w_{2}, \ldots\right)</span> a diagonal matrix specifying regularization coefficients for each joint</li>
</ul>
<hr>
<ul>
<li>ë¡œë´‡ê³¼ ëª¨ì…˜ ë°ì´í„°ë¥¼ ì–»ì€ ë™ë¬¼ì˜ í˜•íƒœëŠ” ì¼ì¹˜í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ì´ ëª¨ì…˜ ë°ì´í„°ë¥¼ inverse-kinematicsë¥¼ ì´ìš©í•´ì„œ retargettingí•˜ëŠ” ì‘ì—…ì´ í•„ìš”í•˜ë‹¤.</li>
<li>í‚¤í¬ì¸íŠ¸ëŠ” hips, feetì„ ì‚¬ìš©í–ˆë‹¤.</li>
<li>source motionì€ í‚¤í¬ì¸íŠ¸ ië²ˆì§¸ì˜ 3d ì¢Œí‘œ xi(t)</li>
<li>The corresponding target keypoint xi (qt ) is determined by the robotâ€™s pose qt</li>
<li>IKì´ a sequence of poses q0:Tì— ëŒ€í•´ ê° í”„ë ˆì„ì˜ í‚¤í¬ì¸íŠ¸ë“¤ì„ ë”°ë¼ê°€ê¸° ìœ„í•´ ì ìš©ëë‹¤.</li>
<li>default pose Ì„qì™€ ë¹„ìŠ·í•œ ìƒíƒœë¥¼ ìœ ì§€í•˜ê¸° ìœ„í•´ì„œ regularization termì¸ <code>W = diag(w1 , w2 , ...)</code> ì„ ì¶”ê°€í–ˆë‹¤.
<ul>
<li>ê° ì¡°ì¸íŠ¸ì— ëŒ€í•œ regularization coefficientë¥¼ ëª…ì‹œí•œ ëŒ€ê°í–‰ë ¬ì´ë‹¤.</li>
</ul></li>
</ul>
<p><span class="math display">
\underset{\mathbf{q}_{0: T}}{\arg \min } \sum_{t} \sum_{i}\left\|\hat{\mathbf{x}}_{i}(t)-\mathbf{x}_{i}\left(\mathbf{q}_{t}\right)\right\|^{2}+\left(\overline{\mathbf{q}}-\mathbf{q}_{t}\right)^{T} \mathbf{W}\left(\overline{\mathbf{q}}-\mathbf{q}_{t}\right)
</span></p>
<ul>
<li><span class="math inline">\underset{\mathbf{q}_{0: T}}{\arg \min } \sum_{t} \sum_{i}\left\|\hat{\mathbf{x}}_{i}(t)-\mathbf{x}_{i}\left(\mathbf{q}_{t}\right)\right\|^{2}+\left(\overline{\mathbf{q}}-\mathbf{q}_{t}\right)^{T} \mathbf{W}\left(\overline{\mathbf{q}}-\mathbf{q}_{t}\right)</span></li>
</ul>
</section>
<section id="motion-imitation-1" class="level2">
<h2 class="anchored" data-anchor-id="motion-imitation-1">2. Motion Imitation</h2>
<ul>
<li><span class="math inline">J(\pi)=\mathbb{E}_{\tau \sim p(\tau \mid \pi)}\left[\sum_{t=0}^{T-1} \gamma^{t} r_{t}\right]</span></li>
<li><span class="math inline">p(\tau \mid \pi)=p\left(\mathbf{s}_{0}\right) \prod_{t=0}^{T-1} p\left(\mathbf{s}_{t+1} \mid \mathbf{s}_{t}, \mathbf{a}_{t}\right) \pi\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)</span></li>
<li>To imitate a given reference motion, The inputs to the policy is augmented with an additional goal <span class="math inline">g_t</span> , which specifies the motion that the robot should imitate.</li>
<li><span class="math inline">\pi\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}, \mathbf{g}_{t}\right)</span></li>
<li>policy is queried at 30Hz for a new action at each timestep</li>
<li>The state <span class="math inline">\mathbf{s}_{t}=\left(\mathbf{q}_{t-2: t}, \mathbf{a}_{t-3: t-1}\right)</span> is represented by the poses <span class="math inline">\mathbf{q}_{t-2: t}</span> of the robot in the three previous timesteps, and the three previous actions <span class="math inline">\mathbf{a}_{t-3: t-1}</span></li>
<li>The pose features qt consist of IMU readings of the root orientation (row, pitch, yaw) and the local rotations of every joint.</li>
<li>The root position is not included among the pose features to avoid the need to estimate the root position during real-world deployment.</li>
<li>The goal <span class="math inline">\mathbf{g}_{t}=\left(\hat{\mathbf{q}}_{t+1}, \hat{\mathbf{q}}_{t+2}, \hat{\mathbf{q}}_{t+10}, \hat{\mathbf{q}}_{t+30}\right)</span> specifies target poses from the reference motion at four future timesteps, spanning approximately 1 second</li>
<li>The action at specifies target rotations for PD controllers at each joint</li>
<li>To ensure smoother motions, the PD targets are first processed by a low-pass filter</li>
</ul>
<hr>
<ul>
<li>The inputs to the policy is augmented with an additional goal gt ,
<ul>
<li>specifies the motion that the robot should imitate</li>
</ul></li>
<li>Ï€(at |st , gt )</li>
<li>The policy is queried at <code>30Hz</code> for a new action at each timestep</li>
<li>The state <code>st = (qtâˆ’2:t , atâˆ’3:tâˆ’1 )</code>
<ul>
<li>the poses qtâˆ’2:t of the robot in <code>the three previous timesteps</code></li>
<li><code>the three previous actions</code> atâˆ’3:tâˆ’1</li>
</ul></li>
<li>The pose features <code>qt</code> :
<ul>
<li>IMU readings of the root orientation (row, pitch, yaw)</li>
<li>the local rotations of every joint</li>
<li>ì‹¤ì œ ë¡œë´‡ì— ì ìš©í•  ë•Œ root positionì„ ì¶”ì •í•˜ì§€ ì•Šì•„ë„ ë˜ë„ë¡ qtì— root positionì„ í¬í•¨í•˜ì§€ ì•Šì•˜ë‹¤.</li>
</ul></li>
<li>The goal <span class="math inline">\mathbf{g}_{t}=\left(\hat{\mathbf{q}}_{t+1}, \hat{\mathbf{q}}_{t+2}, \hat{\mathbf{q}}_{t+10}, \hat{\mathbf{q}}_{t+30}\right)</span>
<ul>
<li>specifies target poses from the reference motion at <code>four future timesteps</code>,</li>
<li>ì•½ 1 second</li>
</ul></li>
<li>The action at: specifies target rotations for PD controllers at each joint.</li>
<li>ëª¨ì…˜ì´ ë¶€ë“œëŸ¬ì›Œì§€ê¸° ìœ„í•´ì„œ low-pass filterë¥¼ ì‚¬ìš©</li>
</ul>
<section id="reward-function" class="level3">
<h3 class="anchored" data-anchor-id="reward-function">Reward Function</h3>
<p><span class="math display">
\begin{gathered}
r_{t}=w^{\mathrm{p}} r_{t}^{\mathrm{p}}+w^{\mathrm{v}} r_{t}^{\mathrm{v}}+w^{\mathrm{e}} r_{t}^{\mathrm{e}}+w^{\mathrm{rp}} r_{t}^{\mathrm{rp}}+w^{\mathrm{rv}} r_{t}^{\mathrm{rv}} \\
w^{\mathrm{p}}=0.5, w^{\mathrm{v}}=0.05, w^{\mathrm{e}}=0.2, w^{\mathrm{rp}}=0.15, w^{\mathrm{rv}}=0.1
\end{gathered}
</span></p>
<hr>
<ul>
<li>encourages the policy to track the sequence of target poses <span class="math inline">\left(\hat{\mathbf{q}}_{0}, \hat{\mathbf{q}}_{1}, \ldots, \hat{\mathbf{q}}_{T}\right)</span></li>
<li><span class="math inline">\begin{gathered}r_{t}=w^{\mathrm{p}} r_{t}^{\mathrm{p}}+w^{\mathrm{v}} r_{t}^{\mathrm{v}}+w^{\mathrm{e}} r_{t}^{\mathrm{e}}+w^{\mathrm{rp}} r_{t}^{\mathrm{rp}}+w^{\mathrm{rv}} r_{t}^{\mathrm{rv}} \\w^{\mathrm{p}}=0.5, w^{\mathrm{v}}=0.05, w^{\mathrm{e}}=0.2, w^{\mathrm{rp}}=0.15, w^{\mathrm{rv}}=0.1\end{gathered}</span></li>
</ul>
<p>ê° ë¦¬ì›Œë“œ í…€ì„ ì‚´í´ë³´ë©´,</p>
<ul>
<li>The pose rewardëŠ” ë ˆí¼ëŸ°ìŠ¤ ëª¨ì…˜ê³¼ì˜ ì°¨ì´ë¥¼ ì¤„ì´ê¸° ìœ„í•œ ë¦¬ì›Œë“œì´ë‹¤. qëŠ” 1ì°¨ì› rotation ê°’ì´ë‹¤.
<ul>
<li><span class="math inline">r_{t}^{\mathrm{p}}=\exp \left[-5 \sum_{j}\left\|\hat{\mathbf{q}}_{t}^{j}-\mathbf{q}_{t}^{j}\right\|^{2}\right]</span></li>
</ul></li>
<li>the velocity rewardëŠ” pose rewardì™€ ë¹„ìŠ·í•˜ê²Œ 1ì°¨ì›ì˜ ê°’ìœ¼ë¡œ angular velocity ê°’ì´ë‹¤.
<ul>
<li><span class="math inline">r_{t}^{\mathrm{v}}=\exp \left[-0.1 \sum_{j}\left\|\hat{\dot{\mathbf{q}}}_{t}^{j}-\dot{\mathbf{q}}_{t}^{j}\right\|^{2}\right]</span>ì†Œ</li>
</ul></li>
<li>the ee rewardëŠ” 3ì°¨ì› ê°’ìœ¼ë¡œ ë¡œë´‡ì˜ end-effectorë“¤ì˜ ì¢Œí‘œë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.
<ul>
<li><span class="math inline">r_{t}^{\mathrm{e}}=\exp \left[-40 \sum_{e}\left\|\hat{\mathbf{x}}_{t}^{e}-\mathbf{x}_{t}^{e}\right\|^{2}\right]</span></li>
</ul></li>
<li>root pose rewardëŠ” ëª¸ì²´ì˜ global positionê³¼ rotationì„ ê°€ì§€ê³  êµ¬í•˜ë©°, root velocity rewardëŠ” ëª¸ì²´ì˜ ì„ ì†ë„ì™€ ê°ì†ë„ë¥¼ ê°€ì§€ê³  êµ¬í•œë‹¤.
<ul>
<li><span class="math inline">\begin{aligned}&amp;r_{t}^{\mathrm{rp}}=\exp \left[-20\left\|\hat{\mathbf{x}}_{t}^{\mathrm{root}}-\mathbf{x}_{t}^{\mathrm{root}}\right\|^{2}-10\left\|\hat{\mathbf{q}}_{t}^{\mathrm{root}}-\mathbf{q}_{t}^{\mathrm{root}}\right\|^{2}\right] \\&amp;r_{t}^{\mathrm{rv}}=\exp \left[-2\left\|\hat{\mathbf{x}}_{t}^{\mathrm{root}}-\dot{\mathbf{x}}_{t}^{\mathrm{root}}\right\|^{2}-0.2\left\|\hat{\mathbf{q}}_{t}^{\mathrm{root}}-\dot{\mathbf{q}}_{t}^{\text {root }}\right\|^{2}\right]\end{aligned}</span></li>
</ul></li>
</ul>
</section>
</section>
<section id="domain-adaptation" class="level2">
<h2 class="anchored" data-anchor-id="domain-adaptation">Domain Adaptation</h2>
</section>
<section id="a.-domain-randomization" class="level2">
<h2 class="anchored" data-anchor-id="a.-domain-randomization">A. Domain Randomization</h2>
<ul>
<li>simple strategy for improving a policyâ€™s robustness to dynamics variations [52, 60, 42]</li>
<li>varies the dynamics during training</li>
<li>no single strategy that is effective across all environments</li>
</ul>
<hr>
<ul>
<li>domain randomization varies the dynamics during training, thereby encouraging the policy to learn strategies that are functional across different dynamics.</li>
<li>training ì¤‘ì— ì—­í•™ì„ ë³€í™”ì‹œì¼œ, ë‹¤ë¥¸ ì—­í•™ì— ê±¸ì³ ê¸°ëŠ¥ì ì¸ ì „ëµì„ í•™ìŠµí•˜ë„ë¡ ì •ì±…ì„ ì¥ë ¤í•œë‹¤</li>
</ul>
</section>
<section id="b.-domain-adaptation" class="level2">
<h2 class="anchored" data-anchor-id="b.-domain-adaptation">B. Domain Adaptation</h2>
<p><img src="https://i.imgur.com/JLTwjaq.png?1" class="img-fluid"></p>
<ul>
<li><code>Âµ</code> represent the values of the dynamics parameters that are randomized during training in simulation</li>
<li>a random set of parameters are sampled according to <code>Âµ âˆ¼ p(Âµ)</code></li>
<li>The dynamics parameters are then encoded into a latent embedding z âˆ¼ E(z|Âµ) by a stochastic encoder E</li>
<li>z is provided as an additional input to the policy <code>Ï€(a|s, z)</code>
<ul>
<li>gëŠ” ì¸í’‹ìœ¼ë¡œ ë„£ì–´ì£¼ì§€ ì•Šì•˜ë‹¤.</li>
</ul></li>
<li><strong>incorporate an information bottleneck into the encoder</strong></li>
<li>The information bottleneck enforces an upper bound Ic on the mutual information I(M, Z) between the dynamics parameters M and the encoding Z</li>
</ul>
<p><span class="math display">
\begin{array}{ll}
\underset{\pi, E}{\arg \max } &amp; \mathbb{E}_{\boldsymbol{\mu} \sim p(\boldsymbol{\mu})} \mathbb{E}_{\mathbf{z} \sim E(\mathbf{z} \mid \boldsymbol{\mu})} \mathbb{E}_{\tau \sim p(\tau \mid \pi, \boldsymbol{\mu}, \mathbf{z})}\left[\sum_{t=0}^{T-1} \gamma^{t} r_{t}\right] \\
\text { s.t. } &amp; I(\mathbf{M}, \mathbf{Z}) \leq I_{c} .
\end{array}
</span></p>
<ul>
<li>noteì— ì •ë¦¬</li>
</ul>
</section>
<section id="c.-real-world-transfer" class="level2">
<h2 class="anchored" data-anchor-id="c.-real-world-transfer">C. Real World Transfer</h2>
<ul>
<li><span class="math inline">\mathbf{z}^{*}=\underset{\mathbf{z}}{\arg \max } \quad \mathbb{E}_{\tau \sim p^{*}(\tau \mid \pi, \mathbf{z})}\left[\sum_{t=0}^{T-1} \gamma^{t} r_{t}\right]</span></li>
</ul>
</section>
</section>
<section id="vii.-experimental-evaluation" class="level1">
<h1>VII. EXPERIMENTAL EVALUATION</h1>
<p><img src="https://i.imgur.com/O1ZRjHb.png?1" class="img-fluid"></p>
<ul>
<li><code>18</code> degrees-of-freedom quadruped
<ul>
<li>3 actuated degrees-of-freedom per leg</li>
<li>6 under-actuateddegrees of freedom for the root (torso)</li>
</ul></li>
<li>We further study the effects of regularizing the latent dynamics encoding with an information bottleneck, and</li>
<li>show that this provides a mechanism to trade off between the robustness and adaptability of the learned policies.</li>
</ul>
<section id="a.-experimental-setup" class="level2">
<h2 class="anchored" data-anchor-id="a.-experimental-setup">A. Experimental Setup</h2>
<p><img src="https://i.imgur.com/yo5bpza.png?1" class="img-fluid"></p>
<ul>
<li>mocap clips are collected from a public dataset</li>
<li>Performance is recorded as the average normalized return,
<ul>
<li>0 corresponding to the minimum possible return per episode</li>
<li>1 being the maximum return</li>
</ul></li>
<li>Each policy is trained with proximal policy optimization using about 200 million samples in simulation</li>
<li>end-to-end using the reparameterization trick</li>
<li>Domain adaptation is performed on the physical system with AWR in the latent dynamics space,
<ul>
<li>using approximately 50 real-world trials to adapt each policy</li>
<li>Trials vary between 5s and 10s in length depending on the space requirements of each skill.</li>
<li>Hyperparameter settings are available in Appendix A</li>
</ul></li>
</ul>
</section>
<section id="model-representation" class="level2">
<h2 class="anchored" data-anchor-id="model-representation">Model Representation</h2>
<p><img src="https://i.imgur.com/tSMcnZl.png?1" class="img-fluid"></p>
<ul>
<li>The encoder E(z|Âµ) is represented by a fully-connected network that maps the dynamics parameters Âµ to the mean mE (Âµ) and standard deviation Î£E (Âµ) of the encoder distribution</li>
<li>The policy network Ï€(a|s, g, z)
<ul>
<li>input : the state s, goal g, and dynamics encoding z</li>
<li>output : the mean mÏ€ (s, g, z) of a Gaussian action distribution</li>
</ul></li>
<li>The standard deviation <span class="math inline">\Sigma_{\pi}=\operatorname{diag}\left(\sigma_{\pi}^{1}, \sigma_{\pi}^{2}, \ldots\right)</span> of the action distribution is represented by a fixed matrix.</li>
<li>The value function V(s,g,Î¼) : input the state, goal, and dynamics parameters</li>
</ul>
</section>
<section id="b.-learned-skills" class="level2">
<h2 class="anchored" data-anchor-id="b.-learned-skills">B. Learned Skills</h2>
<ul>
<li>pacing and trotting, as well as agile turning and spinning motions
<ul>
<li>Pacing is typically used for walking at slower speeds, and is characterized by each pair of legs on the same side of the body moving in unison</li>
<li>Trotting is a faster gait, where diagonal pairs of legs move together</li>
</ul></li>
<li>train policies for these different gaits â† providing the system with different reference motions</li>
</ul>
</section>
</section>
<section id="viii.-discussion-and-future-work" class="level1">
<h1>VIII. DISCUSSION AND FUTURE WORK</h1>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "î§‹";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="curieuxjy/curieuxjy.github.io" data-repo-id="R_kgDOIa8jzg" data-category="General" data-category-id="DIC_kwDOIa8jzs4CShZ1" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light_high_contrast" data-lang="en" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->



</body></html>