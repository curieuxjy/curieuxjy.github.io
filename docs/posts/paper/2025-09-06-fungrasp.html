<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.33">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-09-06">
<meta name="description" content="Functional Grasping for Diverse Dexterous Hands">

<title>📃FunGrasp 리뷰 – Curieux.JY</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../profile.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-ea385d0e468b0dd5ea5bf0780b1290d9.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-bc185b5c5bdbcb35c2eb49d8a876ef70.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-ea385d0e468b0dd5ea5bf0780b1290d9.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-23aef1c2a45953e85f3378e7ccfb1407.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-5a614c35f1f90bfd0a5b2992298a8538.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-23aef1c2a45953e85f3378e7ccfb1407.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-2NVZN2MJZT"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-2NVZN2MJZT', { 'anonymize_ip': true});
</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Curieux.JY</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../post.html"> 
<span class="menu-text">Post</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../note.html"> 
<span class="menu-text">Note</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">Jung Yeon Lee</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/curieuxjy"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#brief-review" id="toc-brief-review" class="nav-link active" data-scroll-target="#brief-review"><span class="header-section-number">1</span> Brief Review</a></li>
  <li><a href="#detail-review" id="toc-detail-review" class="nav-link" data-scroll-target="#detail-review"><span class="header-section-number">2</span> Detail Review</a>
  <ul class="collapse">
  <li><a href="#논문의-핵심-기여점-요약" id="toc-논문의-핵심-기여점-요약" class="nav-link" data-scroll-target="#논문의-핵심-기여점-요약"><span class="header-section-number">2.1</span> 1. 논문의 핵심 기여점 요약</a></li>
  <li><a href="#주요-구성-요소-및-기술적-접근-방식" id="toc-주요-구성-요소-및-기술적-접근-방식" class="nav-link" data-scroll-target="#주요-구성-요소-및-기술적-접근-방식"><span class="header-section-number">2.2</span> 2. 주요 구성 요소 및 기술적 접근 방식</a>
  <ul class="collapse">
  <li><a href="#h2r-그립-자세-재타게팅" id="toc-h2r-그립-자세-재타게팅" class="nav-link" data-scroll-target="#h2r-그립-자세-재타게팅"><span class="header-section-number">2.2.1</span> H2R 그립 자세 재타게팅</a></li>
  <li><a href="#강화학습-기반-동적-파지-제어" id="toc-강화학습-기반-동적-파지-제어" class="nav-link" data-scroll-target="#강화학습-기반-동적-파지-제어"><span class="header-section-number">2.2.2</span> 강화학습 기반 동적 파지 제어</a></li>
  <li><a href="#시뮬레이션-실환경-전이-기법" id="toc-시뮬레이션-실환경-전이-기법" class="nav-link" data-scroll-target="#시뮬레이션-실환경-전이-기법"><span class="header-section-number">2.2.3</span> 시뮬레이션-실환경 전이 기법</a></li>
  </ul></li>
  <li><a href="#사용된-모델-및-학습-기법-실험-설정-분석" id="toc-사용된-모델-및-학습-기법-실험-설정-분석" class="nav-link" data-scroll-target="#사용된-모델-및-학습-기법-실험-설정-분석"><span class="header-section-number">2.3</span> 3. 사용된 모델 및 학습 기법, 실험 설정 분석</a></li>
  <li><a href="#실험-결과-및-성능-비교-분석" id="toc-실험-결과-및-성능-비교-분석" class="nav-link" data-scroll-target="#실험-결과-및-성능-비교-분석"><span class="header-section-number">2.4</span> 4. 실험 결과 및 성능 비교 분석</a></li>
  <li><a href="#장점-및-제한점-평가" id="toc-장점-및-제한점-평가" class="nav-link" data-scroll-target="#장점-및-제한점-평가"><span class="header-section-number">2.5</span> 5. 장점 및 제한점 평가</a></li>
  <li><a href="#향후-연구-방향-및-응용-가능성" id="toc-향후-연구-방향-및-응용-가능성" class="nav-link" data-scroll-target="#향후-연구-방향-및-응용-가능성"><span class="header-section-number">2.6</span> 6. 향후 연구 방향 및 응용 가능성</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-page-left" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">📃FunGrasp 리뷰</h1>
  <div class="quarto-categories">
    <div class="quarto-category">rl</div>
    <div class="quarto-category">raisim</div>
    <div class="quarto-category">functional</div>
  </div>
  </div>

<div>
  <div class="description">
    Functional Grasping for Diverse Dexterous Hands
  </div>
</div>


<div class="quarto-title-meta column-page-left">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">September 6, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<ul>
<li><a href="https://arxiv.org/abs/2411.16755">Paper Link</a></li>
<li><a href="https://hly-123.github.io/FunGrasp/">Homepage</a></li>
</ul>
<ol type="1">
<li>작업별 기능적 그랩과 미지의 객체에 대한 일반화 능력이 부족한 덱스터러스 로봇 핸드의 한계를 해결하기 위해, 단일 RGBD 인간 그랩 이미지를 활용하는 FunGrasp 시스템을 제안합니다.</li>
<li>FunGrasp는 인간의 기능적 그랩 포즈를 다양한 로봇 핸드에 정밀하게 재조정하는 H2R Grasp Retargeting 모듈, 강화 학습 기반의 동적 그랩 제어, 그리고 견고한 sim-to-real 전이를 위한 특권 학습 및 시스템 식별 기술을 포함합니다.</li>
<li>이 시스템은 미등록 객체에 대해 평균 74%의 성공률을 달성하고 다양한 로봇 핸드에 성공적으로 일반화됨을 실제 실험을 통해 입증하며, 사람의 그랩 이미지를 통해 동적인 기능적 로봇 그랩을 가능하게 합니다.</li>
</ol>
<center>
<img src="../../images/2025-09-06-fungrasp/Overview_Final.jpg" width="100%">
</center>
<hr>
<section id="brief-review" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Brief Review</h1>
<p>FunGrasp는 단일 RGBD 이미지로부터 이전에 본 적 없는(unseen) 객체에 대한 원샷(one-shot) 기능적(functional) 정밀(dexterous) 로봇 그립(grasp)을 구현하는 시스템을 제안합니다. 이 시스템은 다양한 로봇 핸드에 적용 가능하며, 인간의 태스크별(task-specific) 그립 자세를 로봇으로 효과적으로 전달하고, 시뮬레이션에서 학습된 제어 정책을 실제 환경에 견고하게 적용하는 데 중점을 둡니다.</p>
<p>핵심 방법론은 세 가지 주요 단계로 구성됩니다.</p>
<p><strong>1. Static Functional Grasp Retargeting (정적 기능적 그립 리타겟팅)</strong></p>
<p>이 모듈은 단일 RGBD 이미지에서 얻은 인간의 기능적 그립 자세(<span class="math inline">G_h = (q_h, T_h, T_o, c)</span>)를 로봇 핸드에 맞게 변환합니다. 여기서 <span class="math inline">q_h</span>는 목표 손가락 관절 각도, <span class="math inline">T_h</span>와 <span class="math inline">T_o</span>는 각각 손과 객체의 6D 전역 자세, <span class="math inline">c</span>는 각 손가락 링크와 객체 간의 이진 접촉 상태를 나타냅니다.</p>
<p>로봇 핸드는 DoF, 손가락 수, 너클 크기 등 형태학적(morphological) 차이가 크기 때문에, <span class="math inline">G_h</span>를 로봇 핸드에 맞게 리타겟팅된 자세(<span class="math inline">G_r</span>)로 변환하는 과정이 필요합니다. 초기화 단계에서는 객체 프레임(object frame) 내에서 인간 손의 손가락 끝(fingertip) 위치와 손가락 링크(finger link) 방향을 로봇 핸드에 일치시킵니다. 손가락이 적거나 관절이 적은 로봇 핸드의 경우, 해당 손가락이나 관절을 단순히 제거합니다. 이후, 다음과 같은 손실 함수를 최소화하여 리타겟팅된 자세를 최적화합니다.</p>
<ul>
<li><strong>Penetration Energy Loss (<span class="math inline">L_{pen}</span>):</strong> 객체와 로봇 핸드 간의 관통(penetration)을 방지합니다 (참고 문헌 [37]에서 차용).</li>
<li><strong>Force Closure Loss (<span class="math inline">L_{fc}</span>):</strong> 안정적인 그립을 유도합니다 (참고 문헌 [38]에서 차용).</li>
<li><strong>Contact Position Loss (<span class="math inline">L_{pos}</span>):</strong> 로봇 핸드가 객체와 정확한 위치에서 접촉을 유지하도록 장려합니다. <span class="math display">L_{pos} = \sum_{j=1, c_j=1}^{P} \|p_h^j - p_r^j\|^2</span> 여기서 <span class="math inline">p_h^j</span>와 <span class="math inline">p_r^j</span>는 각각 <span class="math inline">j</span>번째 인간 손 관절과 해당 로봇 핸드 관절의 위치이며, <span class="math inline">c_j=1</span>은 <span class="math inline">j</span>번째 인간 손 관절이 객체와 접촉 중임을 나타냅니다.</li>
<li><strong>Joint Limit Loss (<span class="math inline">L_{joints}</span>):</strong> 로봇 관절 각도가 제한 범위를 벗어나지 않도록 합니다. <span class="math display">L_{joints} = \sum_{i=1}^{M} \left( \max(0, \theta_i - \theta_{upper_i}) + \max(0, \theta_{lower_i} - \theta_i) \right)</span> 여기서 <span class="math inline">M</span>은 로봇 핸드 관절의 수, <span class="math inline">\theta_i</span>는 <span class="math inline">i</span>번째 관절의 현재 각도, <span class="math inline">\theta_{lower_i}</span>와 <span class="math inline">\theta_{upper_i}</span>는 각각 하한 및 상한입니다.</li>
<li><strong>Collision Loss (<span class="math inline">L_{col}</span>):</strong> 로봇 핸드 자체 및 테이블과의 충돌을 방지합니다. <span class="math display">L_{col} = \sum_{i=1}^{M} \left( \sum_{j=1, j \neq i}^{M} \max(\tau - d(i, j), 0) + \max(h_i - table, 0) \right)</span> 여기서 <span class="math inline">d(i, j)</span>는 <span class="math inline">i</span>번째와 <span class="math inline">j</span>번째 관절 사이의 거리, <span class="math inline">\tau</span>는 임계값, <span class="math inline">h_i - table</span>은 <span class="math inline">i</span>번째 관절과 테이블 표면 사이의 부호 있는(signed) 거리입니다.</li>
</ul>
<p><strong>2. Dynamic Grasp Control (동적 그립 제어)</strong></p>
<p>리타겟팅된 자세 <span class="math inline">G_r</span>에 의해 안내되는 동적 그립 제어는 강화 학습(Reinforcement Learning, RL) 문제로 정식화됩니다 (참고 문헌 [4]에서 영감). 정책 네트워크는 로봇의 관절 각도 <span class="math inline">q_r</span>, 손목의 6D 전역 자세 <span class="math inline">T_r</span> 및 속도 <span class="math inline">\dot{T_r}</span>, 객체의 6D 자세 <span class="math inline">T_o</span> 및 속도 <span class="math inline">\dot{T_o}</span>, 손가락 링크별 이진 접촉 상태 <span class="math inline">c</span> 및 접촉 힘 <span class="math inline">f</span>, 그리고 참조 <span class="math inline">G_r</span>을 포함하는 상태 공간 <span class="math inline">s</span>를 입력받습니다.</p>
<p>특징 추출 계층 <span class="math inline">\phi</span>를 통해 <span class="math inline">s</span>는 학습에 적합한 표현으로 변환됩니다: <span class="math inline">\phi(s) = (q_r, \tilde{T_r}, \dot{\tilde{T_r}}, \tilde{T_o}, \dot{\tilde{T_o}}, \tilde{p_o}, \tilde{p_{z_r}}, f, \tilde{g_p}, \tilde{g_r}, g_c)</span>. 여기서 <span class="math inline">\tilde{\cdot}</span>는 손목 상대 프레임(wrist-relative frame)에서의 변수, <span class="math inline">\tilde{p_o}</span>는 객체 변위, <span class="math inline">\tilde{p_{z_r}}</span>는 손목-테이블 거리, <span class="math inline">\tilde{g_p}</span>는 각 관절의 현재와 목표 3D 위치 간 거리, <span class="math inline">\tilde{g_r}</span>는 현재와 목표 손목 회전 간 차이, <span class="math inline">g_c = [c | c - c]</span>는 이진 목표 접촉과 목표-현재 접촉 차이를 나타냅니다. 정책은 이 특징들을 입력받아 다음 프레임의 손가락 관절 각도와 손목 6D 자세로 정의되는 액션 <span class="math inline">a</span>를 출력합니다. 예측된 손목 자세는 역운동학(inverse kinematics)을 통해 팔 관절 각도로 변환되며, 이 모든 관절 각도는 PD 컨트롤러에 입력되어 토크를 계산합니다.</p>
<p>보상 함수는 <span class="math inline">r = \omega_p r_p + \omega_c r_c + \omega_s r_s + \omega_q r_q</span>로 구성됩니다.</p>
<ul>
<li><strong>Joint Position Reward (<span class="math inline">r_p</span>):</strong> 관절 위치에 대한 보상입니다 (참고 문헌 [4]와 동일).</li>
<li><strong>Contact Reward (<span class="math inline">r_c</span>):</strong> 정확한 접촉 위치를 촉진합니다. 동적 가중치 <span class="math inline">\omega_c = \frac{\sum_{j=1, c_j=1}^{P} \|p_r^j\|^2}{\sum_{j=1, c_j=1}^{P} \|\bar{p_r}^j\|^2}</span>와 함께 사용되며, <span class="math inline">p_r^j</span>는 현재 접촉 위치, <span class="math inline">\bar{p_r}^j</span>는 목표 접촉 위치입니다.</li>
<li><strong>Safety Reward (<span class="math inline">r_s</span>):</strong> 핸드와 테이블 또는 핸드 자신 간의 원치 않는 접촉 힘에 대해 패널티를 부여합니다. <span class="math inline">r_s = \sum_{i=1}^{L} |f_{coll_i}^i|</span>이며, <span class="math inline">L</span>은 링크 수, <span class="math inline">f_{coll_i}^i</span>는 <span class="math inline">i</span>번째 링크의 원치 않는 충돌 힘입니다.</li>
<li><strong>Pose Reward (<span class="math inline">r_q</span>):</strong> 로봇 핸드가 인간과 유사한 자세를 유지하도록 장려합니다. <span class="math display">r_q = \frac{1}{F \cdot K} \sum_{i=1}^{F} \sum_{j=1}^{K} \left( \frac{v_{ij} \cdot \bar{v}_{ij}}{\|v_{ij}\| \|\bar{v}_{ij}\|} - 1 \right)</span> 여기서 <span class="math inline">F</span>는 손가락 수, <span class="math inline">K</span>는 손가락당 링크 수, <span class="math inline">v_{ij}</span>와 <span class="math inline">\bar{v}_{ij}</span>는 객체 프레임에서 <span class="math inline">i</span>번째 손가락의 <span class="math inline">j</span>번째 링크의 현재 및 목표 방향입니다.</li>
</ul>
<p><strong>3. Sim-to-Real Transfer (시뮬레이션-실제 전이)</strong></p>
<p>강화 학습 정책을 실제 로봇에 견고하게 적용하기 위해 여러 기술을 활용합니다.</p>
<ul>
<li><strong>Privileged Learning (특권 학습):</strong> 시뮬레이션에서만 접근 가능한 접촉 정보(<span class="math inline">c</span>와 <span class="math inline">f</span>)를 사용하여 교사 정책(teacher policy)을 학습한 다음, 이 정책을 실제 환경에서 사용 가능한 정보(proprioceptive data)만을 사용하는 학생 정책(student policy)으로 증류(distill)합니다. 학생 정책은 과거 10 프레임의 상태-액션 쌍을 입력받는 LSTM 기반 인코더를 사용하여 접촉을 재구성하고, 동시에 교사 정책의 액션을 모방하도록 학습됩니다. 접촉 재구성 손실은 <span class="math inline">L_{re} = \|\hat{c}_t - c_t\|^2 + \|\hat{f}_t - f_t\|^2</span>이며, 액션 모방 손실은 <span class="math inline">L_{act} = \|\hat{a}_t - a_t\|^2</span>입니다.</li>
<li><strong>System Identification (시스템 식별):</strong> 로봇 핸드의 관절 동역학(joint dynamics)을 정확하게 모델링하기 위해 관절 강성(stiffness) 및 감쇠(damping) 계수와 같은 액추에이터(actuator) 파라미터를 식별합니다. 초기에는 대략적인 파라미터 값으로 정책을 사전 학습하고, 실제 로봇에서 개루프(open-loop) 방식으로 명령-상태 궤적(trajectory)을 수집합니다. 그 후, 시뮬레이션 파라미터를 최적화하여 시뮬레이션과 실제 로봇의 상태 궤적 간의 불일치를 최소화합니다. 최적화에는 CMA-ES [39]를 사용하며, 손실 함수는 <span class="math inline">L_{Sim-Real} = \sum_{t=1}^{N} \|q_t^s - q_t^r\|^2</span>입니다. 여기서 <span class="math inline">q_t^s</span>와 <span class="math inline">q_t^r</span>는 각각 시뮬레이션과 실제 로봇의 관절 각도입니다.</li>
<li><strong>Domain Randomization (도메인 무작위화):</strong> 정책 학습 중 관절 감쇠, PD 컨트롤러 게인, 마찰 계수, 객체 질량, 테이블 높이, 핸드 상태 관측값 등 다양한 파라미터를 무작위화하여 모델의 견고성을 향상시킵니다.</li>
<li><strong>Gravity Compensation (중력 보상):</strong> 각 손가락 관절에 작용하는 핸드 중력의 영향을 보상하기 위해, Kinematics and Dynamics Library [41]를 사용하여 각 로봇 핸드 링크의 질량 분포 및 무게 중심 위치를 계산하고, 현재 핸드 상태를 기반으로 중력에 의해 유발되는 토크를 실시간으로 계산하여 액추에이터에 공급합니다.</li>
</ul>
<p>실험을 통해 FunGrasp는 이전에 본 적 없는 객체에 대해 74%의 평균 성공률로 기능적 그립을 달성하며, Shadow Hand, Faive Hand, Allegro Hand 등 다양한 로봇 핸드에서 75% 이상의 성공률을 보였습니다. Ablation study를 통해 H2R Grasp Retargeting 모듈, Privileged Learning 프레임워크, System Identification 및 Gravity Compensation 기술의 효과가 검증되었습니다. 현재 시스템의 한계는 객체 자세 추정 모델이 알려진 객체 메쉬에 의존한다는 점이며, 향후 연구에서는 이미지 입력만을 직접 활용하는 통합 모델 개발을 통해 일반화 능력을 더욱 향상시킬 수 있습니다.</p>
<hr>
</section>
<section id="detail-review" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Detail Review</h1>
<blockquote class="blockquote">
<p>FunGrasp: 다양한 다관절 로봇 손을 위한 기능적 파지 – 심층 리뷰 및 분석</p>
</blockquote>
<section id="논문의-핵심-기여점-요약" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="논문의-핵심-기여점-요약"><span class="header-section-number">2.1</span> 1. 논문의 핵심 기여점 요약</h2>
<p><strong>FunGrasp</strong>는 다양한 로봇 손(dexterous hands)에 <strong>기능적 파지</strong>(functional grasping)를 구현하기 위한 새로운 시스템을 제안한 연구입니다. 기존의 다지 로봇 손 연구들은 주로 물체를 강하게 쥐는 <strong>힘 파지</strong>(power grasp)에 집중하여 작업 특유의 그립 자세는 간과되어 왔습니다. 이에 반해 FunGrasp는 사람의 작업 맥락에 맞춘 그립(pose)을 로봇 손에 모사함으로써, 예를 들어 가위를 <strong>자르기 위해 손잡이를 쥐거나, 안전하게 건네주기 위해 날부분을 잡는</strong> 등 작업에 최적인 자세를 로봇이 취하도록 합니다. 이 논문의 주요 기여는 다음과 같습니다:</p>
<ol type="1">
<li><p><strong>FunGrasp 시스템 구현:</strong> 단 하나의 RGB-D 이미지에 포착된 <strong>인간의 작업별 그립 자세</strong>로부터, 미지의 새로운 물체에 대해서도 <strong>원샷(one-shot) 일반화</strong>를 달성하는 기능적 파지 로봇 시스템을 제시하였습니다. 이 시스템은 시뮬레이션을 넘어 실제 환경에서도 동작하며, 인간 그립 영상 하나만으로 다양한 새로운 물체를 잡을 수 있습니다.</p></li>
<li><p><strong>H2R 그립 재타게팅 모듈:</strong> <strong>Human-to-Robot (H2R)</strong> 그립 <strong>재타게팅</strong> 모듈을 개발하여, 인간의 작업 특화 그립 자세를 다양한 형태의 로봇 손 모델로 효과적으로 전이합니다. 이때 <strong>인간과 로봇 손의 형태적 차이</strong>(손가락 개수, 관절 구성 등)에도 불구하고 <strong>인간 유사한 손가락 자세</strong>와 <strong>정확한 접촉 지점</strong>을 최대한 보존하는 것이 특징입니다.</p></li>
<li><p><strong>시스템 식별 기반의 동역학 모델 보정:</strong> 로봇 손의 정확한 <strong>관절 동역학 모델</strong>을 확보하기 위해 <strong>시스템 식별(module)</strong> 기법을 도입하고, 이를 통해 시뮬레이션에서 학습한 정책을 현실 로봇에 강건하게 이식(sim-to-real)할 수 있도록 하였습니다. 구체적으로, 로봇 손 관절의 강성(stiffness) 및 감쇠(damping) 계수를 실제 하드웨어에 맞게 최적화하여 시뮬레이션과 현실의 차이를 줄였습니다.</p></li>
<li><p><strong>다양한 로봇 손 및 환경에 대한 일반화 실험:</strong> 서로 다른 구조를 가진 여러 로봇 손에 <strong>단일 시스템</strong>을 적용하여도 성능이 유지됨을 시뮬레이션 및 실제 로봇 실험을 통해 입증했습니다. 또한 다양한 새로운 물체에 대한 기능적 파지 성공률을 측정하고, 각 모듈의 효과를 <strong>종합적인 소절(ablation) 실험</strong>으로 검증함으로써 시스템의 <strong>구성 요소별 기여도</strong>를 밝혔다고 보고하였습니다.</p></li>
</ol>
<p>요약하면, FunGrasp는 <strong>기능적</strong>이고 <strong>다양한 작업별 그립 자세</strong>를 실제 다지 로봇 손에 구현한 첫 사례로서, <strong>단일 인간 시연</strong>만으로 새로운 물체와 다양한 로봇 손에 빠르게 적응하는 점에서 큰 의의가 있습니다. 이러한 접근은 가정, 의료 등 사람의 일상 활동을 보조하는 로봇 손에 직접적으로 응용될 수 있는 잠재력을 보여줍니다. </p>
</section>
<section id="주요-구성-요소-및-기술적-접근-방식" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="주요-구성-요소-및-기술적-접근-방식"><span class="header-section-number">2.2</span> 2. 주요 구성 요소 및 기술적 접근 방식</h2>
<p>FunGrasp 시스템은 <strong>세 단계 모듈</strong>로 구성되며, <strong>(A)</strong> 인간 그립 자세의 <strong>로봇 손 재타게팅</strong>, <strong>(B)</strong> 재타게팅된 자세를 활용한 <strong>동적 그립 제어(강화학습)</strong>, <strong>(C)</strong> 학습 결과를 실제 로봇에 이전하는 <strong>시뮬레이션-실환경 전이</strong>로 나뉩니다. 그림으로 나타내면, 사람 손의 입력 자세를 받아 로봇 손 목표 자세로 변환하고, 이를 기준으로 로봇 손가락을 움직이는 제어 정책을 학습한 뒤, 여러 현실적인 보정 기법을 통해 실제 로봇에서 동작시키는 파이프라인입니다. 각 구성 요소의 기술적 접근은 다음과 같습니다. </p>
<section id="h2r-그립-자세-재타게팅" class="level3" data-number="2.2.1">
<h3 data-number="2.2.1" class="anchored" data-anchor-id="h2r-그립-자세-재타게팅"><span class="header-section-number">2.2.1</span> H2R 그립 자세 재타게팅</h3>
<p>FunGrasp는 <strong>단일 RGB-D 이미지</strong>에서 <strong>인간 손과 물체의 자세</strong>를 추정하여 입력으로 사용합니다. 연구진은 <strong>오프라인 학습된 손-물체 포즈 추정 모델</strong>(예: <strong>FoundationPose</strong> 등)을 활용하여, 컬러/깊이 이미지로부터 사람의 손 관절 각도, 물체의 6-자유도 자세(3D 위치 및 방향) 및 손-물체 <strong>접촉 지점</strong>을 인식합니다. 이렇게 얻은 <strong>인간의 기능적 파지 참고 값</strong>(hand grasp reference)을 로봇 손으로 <strong>재타게팅(retargeting)</strong>하는 것이 1단계의 목표입니다.</p>
<p>재타게팅 모듈에서는 우선 사람 손과 로봇 손의 <strong>관절 대응 관계</strong>를 정의하지 않고도, <strong>물체 좌표계</strong>에서 <strong>각 손가락 마디의 방향</strong>을 정렬시키는 방식으로 로봇 손의 초기자세를 결정합니다. 이후 <strong>최적화 알고리즘</strong>을 통해 이 초기 로봇 손 자세를 세밀하게 조정하여, 다음의 조건들을 만족하도록 합니다:</p>
<ul>
<li><strong>정밀한 접촉 위치 보존:</strong> 인간 손가락이 물체를 접촉한 위치에 로봇 손가락도 최대한 근접하도록 손가락 위치를 미세 조정합니다. 이를 위해 <strong>접촉점 위치 오차</strong>를 줄이는 항목(<span class="math inline">L_{pos}</span>)의 손실 함수를 사용합니다.</li>
<li><strong>관통 및 충돌 최소화:</strong> 로봇 손가락 모델이 물체 모델을 <strong>관통(penetration)</strong>하거나 책상/자기 자신과 <strong>충돌(collision)</strong>하지 않도록 제약을 겁니다. 이를 위해 물체와의 <strong>겹침을 벌주는 손실</strong>(<span class="math inline">L_{pen}</span>) 및 <strong>자기충돌/환경충돌 손실</strong>(<span class="math inline">L_{col}</span>)을 포함시켜 최적화합니다.</li>
<li><strong>힘닫힘(Force Closure) 확보:</strong> 로봇 손가락들이 물체를 놓치지 않고 안정적으로 잡도록 <strong>힘닫힘 조건</strong>을 평가하는 손실(<span class="math inline">L_{fc}</span>)을 도입하여, 접촉된 여러 면에서 물체를 고정할 수 있는 <strong>안정적 그립</strong>을 찾습니다.</li>
<li><strong>관절 가동범위 및 자세 유지:</strong> 각 로봇 손가락 관절은 <strong>물리적 가동 한계</strong> 내에서 움직이게 하고, 사람 손의 <strong>형태에 가까운 손가락 자세</strong>를 유지하도록 유도합니다. 이를 위해 <strong>인간 유사 자세</strong>에 대한 보상(term)을 추가하여 최적화 중 <strong>불필요하게 어색한 손 모양</strong>이 되지 않도록 합니다.</li>
</ul>
<p>이러한 <strong>다목적 최적화</strong>를 통해 FunGrasp의 재타게팅 모듈은 <strong>사람의 그립 의도</strong>를 로봇 손에 최대한 충실히 반영합니다. 기존의 단순한 관절각 매핑이나 손가락 끝점 맞춤 기법들은 각각 <strong>접촉 정확도 부족</strong>, <strong>자세 불안정</strong>의 문제를 가졌는데, FunGrasp는 <strong>접촉 지점</strong>과 <strong>자세</strong>를 동시에 고려함으로써 보다 <strong>정밀하고 인간다운 그립姿勢</strong>를 얻어냅니다. 이 점은 이후 단계에서 로봇이 <strong>얇거나 작은 물체</strong>를 다룰 때 <strong>책상과의 충돌을 피하며</strong> 정확히 집는 능력으로 이어집니다. </p>
</section>
<section id="강화학습-기반-동적-파지-제어" class="level3" data-number="2.2.2">
<h3 data-number="2.2.2" class="anchored" data-anchor-id="강화학습-기반-동적-파지-제어"><span class="header-section-number">2.2.2</span> 강화학습 기반 동적 파지 제어</h3>
<p>2단계에서는, 앞서 결정된 <strong>정적(static) 로봇 손 목표 자세</strong>를 실제로 구현하기 위한 <strong>동적인 폐루프 제어 정책</strong>을 학습합니다. 이는 단순히 그 자세로 손가락을 쳐들어 고정하는 것이 아니라, <strong>팔과 손가락을 움직여 물체를 잡아 들어올리는 동작 전체</strong>를 수행하는 문제입니다. 연구진은 이를 <strong>강화학습(RL)</strong> 문제로 구성하여, 시뮬레이션 환경에서 로봇 손이 <strong>물체를 잡고 들어올리는 제스처</strong>를 배우도록 했습니다.</p>
<p>강화학습 <strong>상태(state) 관측값</strong>에는 로봇 손의 <strong>손목 위치/자세</strong>와 <strong>속도</strong>, 물체의 <strong>6D 자세와 속도</strong>, 로봇 손의 <strong>모든 관절 각도</strong>가 포함됩니다. 여기에 더해, 핵심적으로 <strong>목표로 하는 그립 자세</strong>에 관한 정보가 제공되는데, 이는 이전 단계의 재타게팅 모듈이 출력한 <strong>로봇 손 목표 관절각도 설정</strong>과 <strong>목표 접촉 지점들</strong>입니다. 이러한 <strong>목표 그립 정보</strong>는 로봇에게 물체의 <strong>국소 형상</strong>에 대한 사전지식을 제공하는 역할을 합니다. 즉, 어떤 손가락이 어디를 잡아야 하는지에 대한 <strong>암시적 힌트</strong>로 작용하여, 한 가지 정책으로도 다양한 모양의 물체들을 다룰 수 있도록 돕습니다.</p>
<p>FunGrasp의 정책은 <strong>실시간 센서 정보</strong>를 활용하는 <strong>폐루프(closed-loop)</strong> 제어입니다. 시뮬레이션에서는 접촉 여부나 힘 등의 <strong>접촉 상태 정보(c, f)</strong>도 알고리즘이 직접 관측할 수 있지만, 실제 환경에서는 이러한 <strong>특권 정보</strong>를 얻기 어렵습니다. 이를 해결하기 위해 FunGrasp는 <strong>교사-학생(</strong>teacher-student<strong>) 강화학습</strong> 전략을 채택하였습니다. 먼저 <strong>접촉 상태를 모두 알고 있는</strong> 시뮬레이션 환경에서 <strong>교사 정책</strong>(Teacher)을 충분히 학습시킨 후, 이 정책의 행동을 모방하면서 <strong>접촉 정보를 추론</strong>하도록 설계된 <strong>학생 정책</strong>(Student)을 추가 학습시킵니다. 학생 정책은 과거 몇 시각의 <strong>관절각도 변화, 목표 대비 손목 위치 오차, 그리고 이전 행동 값</strong> 등을 입력으로 받아 <strong>LSTM 기반 인코더</strong>를 통해 접촉 여부를 예측하고, 이를 이용해 제어를 수행합니다. 이 과정에서 <strong>교사 정책의 행동</strong>과의 차이를 최소화하는 <strong>행동 모방 손실</strong>과, <strong>접촉 예측 정확도</strong>를 높이는 <strong>접촉 재구성 손실</strong>을 함께 최적화하여, 결과적으로 <strong>실제 센서로 얻을 수 있는 정보만으로도</strong> 교사와 유사한 성능을 내는 폐루프 제어기가 완성됩니다. 그 결과, 로봇 손은 <strong>자신의 관절 움직임과 물체 움직임을 감지하여 접촉을 추론</strong>하고, 물체가 미끄러지거나 외부 방해가 발생해도 <strong>실시간으로 손가락 힘과 자세를 조정</strong>하는 적응적 행동을 학습하게 됩니다. </p>
<p>강화학습 <strong>보상 함수</strong>는 로봇이 <strong>목표 자세로 정확히 파지하고 안정적으로 물체를 다루도록</strong> 설계되었습니다. 구체적으로, 보상 <span class="math inline">R</span>는 <strong>(1)</strong> 목표 잡기 자세에 대한 <strong>관절 위치 오차 보상</strong> <span class="math inline">R_{jp}</span>, <strong>(2)</strong> 지정된 접촉 점들을 달성하는 <strong>접촉 보상</strong> <span class="math inline">R_{c}</span>, <strong>(3)</strong> 불필요한 충돌이나 과도한 힘을 피하는 <strong>안전 보상</strong> <span class="math inline">R_{safety}</span>, <strong>(4)</strong> 인간과 유사한 손가락 <strong>자세 유지를 장려하는 보상</strong> <span class="math inline">R_{pose}</span>의 합으로 구성됩니다. 이 중 <span class="math inline">R_{c}</span>에는 <strong>동적 가중치</strong>를 도입하여 <strong>접촉 지점 부근에서 정밀하게 손가락을 조정</strong>하도록 하였고, <span class="math inline">R_{safety}</span>는 로봇 손가락이 책상이나 자기 손에 강하게 부딪히는 힘이 감지되면 패널티를 주어 <strong>충돌을 피하도록</strong> 유도합니다. <span class="math inline">R_{pose}</span>는 각 손가락 마디의 <strong>공간적 방향</strong>이 인간 손의 해당 마디 방향과 얼마나 일치하는지를 비교하여 계산되며, 이를 통해 로봇 손이 <strong>인간다운 자세를 유지</strong>하면서 파지하도록 유도됩니다. 이러한 다항목 보상 설정 하에서, 강화학습 에이전트는 <strong>목표 접촉을 이루고 안정적으로 물체를 잡는 동작</strong>을 배우게 되며, 동시에 <strong>테이블과의 충돌을 회피</strong>하고 <strong>사람과 유사한 손 모양</strong>을 유지하는 해결책을 찾게 됩니다. </p>
<p>정책 학습은 <strong>물리 시뮬레이터 RaiSim</strong> 상에서 진행되었으며, <strong>Proximal Policy Optimization (PPO)</strong> 알고리즘을 사용하여 다중 병렬 환경에서 훈련이 이루어졌습니다. 훈련에는 NVIDIA RTX 3090 GPU 1장과 128개의 CPU 코어가 동원되어 약 <strong>2일간</strong> 실행되었으며, 시뮬레이션 초기 단계에서 로봇 손의 <strong>손가락 관절은 인간 목표 각도의 일부까지만 구부린 상태</strong>로 세팅하여 학습을 용이하게 하였습니다. 이처럼 <strong>초기 자세를 인간 시연과 유사하게 설정</strong>함으로써 학습 초기에 무작위 탐색으로 인한 불필요한 실패를 줄이고 빠른 수렴을 유도하였습니다. 또한 에피소드 시작 시 로봇 <strong>팔</strong>(UR5 로봇팔)의 위치도 적절히 조정하여, 손이 물체 중심에서 일정 거리 떨어진 곳에서 <strong>물체를 향하도록</strong> IK(역기구학) 기반으로 배치하였고, 속도/가속도 제한 등을 걸어 안전한 탐색을 보장했습니다. </p>
</section>
<section id="시뮬레이션-실환경-전이-기법" class="level3" data-number="2.2.3">
<h3 data-number="2.2.3" class="anchored" data-anchor-id="시뮬레이션-실환경-전이-기법"><span class="header-section-number">2.2.3</span> 시뮬레이션-실환경 전이 기법</h3>
<p>시뮬레이션으로 학습된 정책을 실제 로봇 손에 이식할 때 발생하는 모델 차이, 센서 잡음 등의 문제를 극복하기 위해, FunGrasp는 여러 가지 <strong>Sim-to-Real 전이</strong> 기술을 병합하여 활용하였습니다. <strong>(1) 특권 정보 학습(Privileged Learning)</strong>, <strong>(2) 시스템 식별(System Identification)</strong>, <strong>(3) 도메인 랜덤화(Domain Randomization)</strong>, <strong>(4) 중력 보상(Gravity Compensation)</strong>의 네 가지가 주요 기법입니다. 앞서 설명한 교사-학생 정책 지식증류 과정은 특권 정보 학습의 일환이며, 이로써 <strong>시뮬레이션 전용 정보</strong>에 의존하지 않는 정책을 확보했습니다.</p>
<p><strong>시스템 식별</strong>은 실제 로봇의 동작 데이터를 활용해 시뮬레이터의 <strong>모델 파라미터를 보정</strong>하는 과정입니다. FunGrasp에서는 우선 시뮬레이션 상에서 근사 파라미터로 학습한 정책을 <strong>실제 로봇(알레그로 손)</strong>에 <strong>오픈루프</strong>로 실행하여, 로봇의 관절 각도 변화 등의 <strong>실측 궤적 데이터</strong>를 수집했습니다. 그런 다음 동일한 입력 행동 sequence를 시뮬레이터에 적용하여 <strong>시뮬레이션의 관절 궤적</strong>과 <strong>실제 로봇의 관절 궤적</strong> 간의 차이를 측정하고, 이 차이가 최소화되도록 시뮬레이터의 <strong>관절 강성, 감쇠 계수</strong> 등을 <strong>CMA-ES</strong> 최적화 방법으로 조정하였습니다. 즉, 실제 로봇의 움직임을 가장 잘 모사하는 시뮬레이터 파라미터를 식별한 후, 그 값으로 시뮬레이터를 업데이트하고 정책을 미세 재학습(fine-tuning)함으로써 현실 오차를 줄였습니다. 이러한 시스템 식별 과정을 거친 정책은 <strong>관절 움직임의 관성, 마찰 등 현실적인 특성</strong>을 반영하므로, 보정 전보다 <strong>현실에서의 성공률을 크게 향상</strong>시켰습니다.</p>
<p><strong>도메인 랜덤화</strong>는 학습 시 시뮬레이션의 다양한 물리 속성을 무작위로 변화시키는 기법으로, FunGrasp에서는 <strong>관절 마찰계수, PID 제어 게인, 물체의 질량, 테이블 높이, 센서 잡음 등</strong>을 무작위로 변동시키며 정책을 훈련했습니다. 이렇게 함으로써 정책이 <strong>환경 변화</strong>나 <strong>모델 불확실성</strong>에 둔감하게 되고, 현실에서도 강인하게 동작할 확률이 높아집니다.</p>
<p>마지막으로 <strong>중력 보상</strong>은 로봇 손가락의 무게로 인한 <strong>처짐 현상</strong>을 상쇄하기 위한 것입니다. 각 로봇 손 링크의 질량과 질량중심을 고려하여 <strong>관절별 중력에 의한 토크</strong>를 실시간 계산하고, 제어 명령에 이 <strong>중력 보정 토크</strong>를 추가로 더해주는 피드포워드 방식을 적용하였습니다. 이를 통해 로봇 손이 어떤 자세를 취하든 <strong>자기 무게에 의해 손가락이 벌어지거나 힘을 못 주는 문제</strong>를 완화하여, <strong>의도한 접촉력과 자세를 정확히 유지</strong>하도록 하였습니다. </p>
</section>
</section>
<section id="사용된-모델-및-학습-기법-실험-설정-분석" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="사용된-모델-및-학습-기법-실험-설정-분석"><span class="header-section-number">2.3</span> 3. 사용된 모델 및 학습 기법, 실험 설정 분석</h2>
<p>FunGrasp의 핵심 모델은 <strong>강화학습 정책 신경망</strong>입니다. 초기 <strong>교사 정책</strong>은 비교적 단순한 MLP(Multi-Layer Perceptron) 구조로, <strong>관찰 상태</strong>를 입력받아 각 손가락 관절 및 팔 관절에 대한 목표 움직임(속도/토크)을 출력합니다. 이 교사 정책은 시뮬레이션 상에서 <strong>접촉 여부(c)와 접촉 힘(f)</strong>까지 모두 포함된 완전한 상태정보로 훈련되므로 이상적인 조건에서 성능을 극대화할 수 있습니다. 이후 <strong>학생 정책</strong>에는 <strong>LSTM(Long Short-Term Memory) 기반 인코더</strong>가 추가되어, 실제 환경에서 얻을 수 있는 <strong>고유감각 정보(관절 각도, 속도 등)</strong>와 과거 몇 스텝의 상태-액션 기록만으로 <strong>접촉 여부와 힘을 추정</strong>하도록 설계되었습니다. 학생 정책의 MLP 부분은 교사 정책의 가중치를 초기화 값으로 사용하고, LSTM 인코더는 <strong>접촉 재구성 손실</strong>과 <strong>행동 모방 손실</strong>에 의해 학습됩니다. 이 <strong>지식 증류(knowledge distillation)</strong> 방식의 학습 기법은 강화학습과 지도학습(모방학습)을 결합한 형태로, 시뮬레이터에서만 가능한 부가 정보를 교사로부터 간접적으로 전수받아 현실 적용성을 확보합니다. </p>
<p><strong>시뮬레이션 환경</strong>으로는 <strong>RaiSim</strong> 물리 엔진이 사용되었으며, 시간당 1000Hz 이상의 고속 시뮬레이션을 통해 미세한 접촉 동작까지 모델링하였습니다. 학습 알고리즘은 PPO로, 약 2일간의 훈련을 통해 정책이 수렴하였습니다. 학습 초기에는 에이전트가 물체를 제대로 잡지 못하고 자주 실패하지만, 설정된 <strong>복합 보상 함수</strong>의 지표들(접촉, 자세, 안전 등)을 점차 향상시키면서 에피소드 당 성공률이 높아집니다. 최종적으로 시뮬레이터 상에서 <strong>성공 에피소드 비율</strong>이 충분히 높아지면(예: 80% 이상), 해당 정책을 기반으로 앞서 기술한 학생 정책으로의 이식 및 추가 훈련을 진행합니다. </p>
<p><strong>실험 설정</strong> 측면에서, 저자들은 <strong>DexYCB 데이터셋</strong>의 우손(right-handed) 그립 시퀀스를 활용했다고 명시하고 있습니다. YCB Object Set은 다양한 가정용 물체들의 3D 모델과 실제 물체를 제공하는 벤치마크로, FunGrasp에서는 이 <strong>YCB 물체들</strong> 중 다수를 선택하여 실험에 사용했습니다. 학습 시에는 DexYCB의 전체 그립 레퍼런스 중 <strong>75%를 훈련용으로</strong>, <strong>25%를 테스트용</strong>으로 분할하여 사용하였고, 훈련에 사용되지 않은 <strong>새로운 물체</strong>에 대해서 평가하였습니다. 특히 <strong>한 번도 본 적 없는 범주의 물체</strong>라도, 해당 물체를 사람이 잡고 있는 RGB-D 사진 <strong>한 장만 제공</strong>하면 그에 맞게 로봇 손이 파지를 시도하도록 설정하였습니다. 예를 들어 학습 데이터에 없던 <strong>망치</strong>나 <strong>인형</strong>도, 사람이 쥐고 있는 모습을 한 번 관측하면 그 기능적 잡기 방식을 로봇이 모방하도록 한 것입니다. </p>
<p><strong>하드웨어 구성</strong>으로는, 기본적으로 6자유도 UR5 로봇 팔 끝에 <strong>알레그로 로봇 손(Allegro Hand)</strong>을 장착한 형태를 사용했습니다. 알레그로 손은 사람 손과 유사한 4개의 손가락(검지~소지 3개 + 엄지)과 총 16개의 관절 자유도를 지닌 다지 로봇 손입니다. 추가로, 형태적 일반화 실험을 위해 <strong>인스파이어(Inspire) 손</strong>이라는 상이한 구조의 로봇 손도 사용되었는데, 이 손은 엄지에 4개 관절, 나머지 손가락들에 2개 관절씩을 가진 보다 간소화된 형태입니다. 이러한 서로 다른 로봇 손들에 대해, FunGrasp 시스템이 <strong>별도 구조 변경 없이 적용</strong>될 수 있음을 확인하고자 했습니다. <strong>카메라</strong>는 Intel RealSense D435i 깊이 카메라를 고정 배치하여 사용하였으며, 물체의 6D 자세 추적에는 사전에 알려진 물체 CAD 모델을 이용하는 <strong>퍼스펙티브-n-포인트 기반 알고리즘</strong>(FoundationPose 등)을 활용했습니다. 실시간 제어는 10Hz로 수행되었는데, 매 시각 정책이 출력한 손가락 목표와 IK로 계산된 팔 움직임이 PD 제어기를 통해 로봇에 명령되는 방식입니다. 실험 중 안전을 위해 <strong>손끝 속도 0.25m/s, 가속도 0.3m/s^2 이하</strong>로 제한을 두었으며, 충돌 감지 시 즉시 종료하도록 하였습니다. </p>
<p>FunGrasp의 <strong>성능 측정 지표</strong>로는 <strong>그립 성공률(Success Rate)</strong>, <strong>시뮬레이션 상에서 물체 흔들림 정도(Simulated Displacement, SimD)</strong>, <strong>목표 접촉 달성률(Contact Ratio)</strong> 등이 사용되었습니다. 그립 성공은 로봇이 물체를 들어올려 <strong>일정 시간 이상 떨어뜨리지 않고 유지</strong>하면 1회 성공으로 간주하였고, SimD는 잡은 후 물체의 미세한 움직임(흔들림 속도)을 나타내며 값이 작을수록 안정적 파지를 뜻합니다. Contact Ratio는 로봇 손이 달성한 실제 접촉 지점 수를 인간 시연시 목표로 한 접촉 지점 수로 나눈 비율로 정의되어, 1.0이면 <strong>모든 의도된 접촉을 이뤘음</strong>을 의미합니다. 추가로, 다양한 로봇 손 사이 <strong>일반화 성능</strong> 평가, 앞서 언급한 <strong>어블레이션(ablation) 실험</strong> 등도 실시하여 세부 모듈의 효과를 정량화했습니다. 전반적인 실험 구성은 시뮬레이션 결과를 우선 확인한 뒤 실제 로봇에서 최종 검증하는 순서로 이루어졌으며, <strong>모든 실험은 다섯 번 이상의 반복 수행</strong>으로 신뢰도를 높였습니다. </p>
</section>
<section id="실험-결과-및-성능-비교-분석" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="실험-결과-및-성능-비교-분석"><span class="header-section-number">2.4</span> 4. 실험 결과 및 성능 비교 분석</h2>
<p><strong>FunGrasp의 실험 결과</strong>, 다양한 측면에서 시스템의 우수성과 한계를 보여줍니다. 먼저, <strong>새로운 물체에 대한 원샷 기능적 파지</strong> 실험에서, 사람 시연 이미지만 주어졌을 때 로봇이 해당 물체를 제대로 잡아드는지를 평가하였습니다. 18가지의 가정용 물체들에 대해 시도한 결과, <strong>평균 74%의 높은 성공률</strong>을 달성하였습니다. 여기에는 무겁고 기다란 <strong>망치</strong>부터 크지만 가벼운 <strong>바구니</strong>, 심지어 휘어지는 재질의 <strong>봉제 인형(loopy doll)</strong>까지 다양한 형태와 질량의 물체들이 포함되었습니다. FunGrasp는 훈련 때 전혀 접하지 못했던 이러한 물체들도 대부분 성공적으로 파지하였으며, 특히 인형처럼 <strong>훈련 물체들과 물리적 성질이 완전히 다른</strong> 경우에도 적절히 대응하는 모습을 보였습니다. 이는 본 시스템의 <strong>범용적인 일반화 능력</strong>을 잘 입증합니다. 성공 사례들을 보면, 예를 들어 <strong>양동이는 손잡이를 걸어 쥐고</strong>, <strong>스프레이 병은 손잡이 목부분을 움켜쥐며</strong>, <strong>장난감 자동차는 위에서 집어 들어올리는</strong> 등, <strong>인간이 실제 사용하려고 의도한 방식대로</strong> 로봇이 물체를 파지하고 있음을 알 수 있습니다. 이러한 <strong>사람과 유사한 그립 형태</strong>들은 FunGrasp가 단순한 힘 파지가 아닌 <strong>과제 지향적 파지</strong>를 구현했음을 보여주는 정성적인 증거입니다. </p>
<p>한편, <strong>일부 한계 사례</strong>도 관찰되었습니다. 예를 들어 <strong>글루 건(공업용 풀건)</strong>의 경우, 모양이 특이하고 한쪽으로 치우쳐져 있어 로봇 손이 잡는 데 어려움을 겪어 <strong>성공률이 낮았습니다(2/6회 성공)</strong>. 저자들은 글루 건의 <strong>비정형적 형상</strong> 때문에 이상적인 접촉 지점을 찾기 어려웠던 점을 한계로 지적하였습니다. 이 외에도 <strong>노트북 충전기 케이블</strong>과 같이 잡기가 애매한 형태의 일부 물체에서도 간혹 실패가 발생하였으나, 전반적으로 대부분의 물체에 대해 <strong>4회 이상/6회 시도 중 성공</strong>이라는 준수한 성적을 거두었습니다. 더욱이 FunGrasp의 폐루프 제어는 <strong>외부에서 일부러 물체를 잡아당기는 방해</strong>를 주었을 때도 즉각적인 적응 동작으로 그립을 유지하는 <strong>견고함</strong>을 보였다고 보고되었는데, 이는 공개된 보조 영상을 통해 확인할 수 있습니다. 이러한 적응 능력은 <strong>강화학습 기반 폐루프 제어의 이점</strong>으로서, 사전에 정해둔 정적 자세만 실행하는 <strong>개방형 제어</strong>보다 훨씬 유연하고 안정적인 파지 동작을 가능케 함을 의미합니다. </p>
<p><strong>다양한 로봇 손에 대한 일반화</strong> 실험에서는, FunGrasp가 로봇 손의 형태가 달라져도 일관된 성능을 내는지 평가했습니다. 시뮬레이션에서 인간 손과 형태가 상이한 <strong>Shadow Hand(쉐도우 핸드)</strong>, <strong>Faive Hand</strong>, <strong>Allegro Hand</strong> 세 종류에 대해 각각 정책을 학습 및 테스트한 결과, <strong>모든 손에서 성공률 75% 이상</strong>을 달성하였습니다. 구체적으로 Shadow Hand 75%, Faive Hand 81%, Allegro Hand 85%의 성공률을 보였는데, <strong>Allegro Hand의 성능이 가장 높았다</strong>고 보고되었습니다. 이는 알레그로 손이 상대적으로 <strong>손가락 마디가 굵어 잡기가 쉬운 구조</strong>인 덕분으로 분석되었습니다. 즉, 손가락 관절의 물리적 형태에 따라 약간의 난이도 차는 있지만, FunGrasp의 알고리즘 자체는 <strong>손가락 개수(Shadow는 5지, Allegro는 4지 등)</strong>나 <strong>관절 구성의 차이</strong>에 견고하게 대응함을 알 수 있습니다. 실제 로봇으로도 두 종류(Allegro, Inspire)의 손을 테스트한 결과, <strong>두 로봇 손 모두 동일한 인간 시연 입력에 대해</strong> 얇고 작은 물체를 테이블 위에서 집어올리는 동작까지 성공적으로 수행하여 <strong>정량적 결과와 일치하는 일반화 능력</strong>을 확인했습니다. 특히 이는 <strong>책상과 손가락 간 잠재적 충돌 위험</strong>이 있음에도 불구하고 사람 시연의 <strong>정밀한 접촉 자세를 보존</strong>한 덕분에 가능했던 것으로 분석됩니다. </p>
<p><strong>성능 비교</strong>를 위해 저자들은 관련 선행 연구들과 FunGrasp를 정성적으로 대비하는 표를 제시하였습니다. 예컨대, D-Grasp(CVPR 2022)나 DexTransfer(2022) 등의 방법은 <strong>실제 로봇 적용</strong>과 <strong>다양한 객체 범주 일반화</strong> 측면에서는 성과를 보였으나, <strong>기능적 그립</strong>에는 초점을 두지 않았습니다. 반면 Agarwal 등(CoRL 2023)의 연구는 <strong>기능적 파지</strong>를 다루었지만 <strong>단일 로봇 손</strong>에 국한되었고, UniDexGrasp 계열 연구들은 <strong>여러 손 모형</strong>을 고려했으나 <strong>동적 폐루프 제어</strong>가 아니었습니다. <strong>FunGrasp (본 연구)</strong>만이 <strong>실제 하드웨어에서</strong> <strong>다양한 자세의 기능적 파지</strong>를 <strong>다종의 로봇 손</strong>에 걸쳐 구현함으로써, 이 모든 측면을 아우르는 <strong>포괄적인 해법</strong>을 제시한 것으로 평가됩니다. 요컨대 FunGrasp는 이전 기법들의 장점을 모으면서도 각자의 한계를 넘어서, <strong>광범위한 범용성</strong>을 달성했다는 의의가 있습니다. (<a href="https://openreview.net/forum?id=93qz1k6_6h&amp;utm_source=chatgpt.com" title="Dexterous Functional Grasping">OpenReview</a>, <a href="https://proceedings.mlr.press/v229/agarwal23a/agarwal23a.pdf?utm_source=chatgpt.com" title="Dexterous Functional Grasping">Proceedings of Machine Learning Research</a>)</p>
<p>마지막으로, FunGrasp의 각 모듈이 성능에 얼마나 기여하는지 확인하기 위한 <strong>어블레이션 실험</strong> 결과를 살펴보겠습니다. 첫째, <strong>H2R 재타게팅 모듈의 효과</strong>를 검증하기 위해, 이를 대체하는 두 가지 방안을 비교했습니다: (i) 기존의 <strong>DexGraspNet</strong> 모델을 이용해 로봇 그립 자세를 생성하는 경우와, (ii) <strong>Angle Reset</strong>이라 하여 <strong>인간 손가락 관절 각도를 로봇 손에 그대로 복사</strong>만 하는 경우입니다. 이 둘은 각각 “접촉점은 고려하나 자세 제약이 부족한 방법”과 “자세는 흉내내나 접촉 조정이 전혀 없는 방법”이라 할 수 있습니다. 시뮬레이션으로 동일한 강화학습을 수행한 결과, DexGraspNet 대안은 <strong>65% 성공률</strong>, Angle Reset은 <strong>62% 성공률</strong>을 보인 반면, FunGrasp의 재타게팅을 사용한 경우 <strong>85% 성공률</strong>로 <strong>월등히 높은 성능</strong>을 냈습니다. 또한 접촉 정확도를 나타내는 Contact Ratio도 각각 0.68, 0.65에 그친 대안들에 비해 FunGrasp는 0.79로 크게 향상되었습니다. 실패 양상을 분석하면, DexGraspNet 기반은 <strong>파지의 안정성(힘닫힘)을 높이기 위해 물체를 감싸쥐는 파워그립 자세만 생성</strong>하려는 경향이 있어 <strong>얇은 물체를 책상 위에서 집을 때 테이블과 충돌</strong>하는 사례가 많았습니다. Angle Reset 방식은 <strong>접촉점 정밀 조정이 전혀 없다보니</strong> 작은 물체에서 손가락이 빗나가거나 헛집는 경우가 빈번했습니다. 반면 FunGrasp의 재타게팅은 <strong>인간의 세밀한 접촉 위치와 손 모양을 유지</strong>해주므로, 이러한 상황에서도 <strong>보다 견고한 파지</strong>가 가능했고 결과적으로 성능 우위로 나타났습니다. </p>
<p>둘째, <strong>Privileged Learning(교사-학생 학습)</strong>의 효과를 확인한 실험에서, 접촉 정보 사용 유무와 학습 방식에 따른 네 가지 조건을 비교했습니다. <strong>접촉 정보를 아예 제공하지 않고</strong> RL만 수행한 경우(w/o Priv. Info.) 성공률은 <strong>61%</strong>에 머물렀고, <strong>접촉 정보가 없는 상태에서 LSTM 인코더를 바로 RL로 학습시킨 경우</strong>(w/o Priv. Learn.)는 <strong>40%</strong>로 성능이 크게 저하되었습니다. 후자의 경우 학생 정책을 한 단계로 학습시킨 것인데, 접촉 단서를 전혀 모르는 상태에서 랜덤하게 탐색해야 하므로 학습 난도가 높아진 것입니다. 반면 <strong>교사 정책</strong>(시뮬레이션 접촉 GT 사용)은 <strong>85%</strong>의 성공률을 보였고, FunGrasp의 <strong>학생 정책</strong>도 <strong>85%</strong>로 <strong>교사와 거의 동등한 성능</strong>을 달성하였습니다. 이는 <strong>LSTM 인코더를 활용한 2단계 학습 프레임워크</strong>가 효과적으로 작동하여, <strong>시뮬레이터 상의 접촉 정보가 없어도 유사한 정보를 추론</strong>해냈음을 의미합니다. 특히 학생 정책의 성능이 교사 대비 손색이 없다는 것은, FunGrasp 접근법이 <strong>시뮬레이션-현실 간 정보 불일치 문제</strong>를 성공적으로 풀어낸 중요한 성과로 볼 수 있습니다. </p>
<p>마지막으로, <strong>시스템 식별과 중력 보상</strong> 기법의 중요성을 검증한 실험에서는, 이 두 가지를 제거한 경우의 실환경 성공률을 측정했습니다. 그 결과, <strong>시스템 식별 없이</strong> 초기 추정 파라미터로만 정책을 쓴 경우 성공률이 가장 크게 떨어져 <strong>평균 38%</strong>에 불과했고, <strong>중력 보상 없이</strong>인 경우도 <strong>59%</strong>로 성능 저하가 뚜렷했습니다. 반면 두 기법을 모두 적용한 본래 설정에서는 <strong>75%</strong>로 최고 성능을 보였습니다. 이는 <strong>정확한 관절 모델링</strong>의 중요성을 방증하는데, 특히 시스템 식별이 부족하면 거의 모든 테스트 물체에서 실패 확률이 크게 늘어남을 표에서 확인할 수 있습니다. 중력 보상 역시 무거운 손가락을 가진 로봇 손의 경우 파지 유지에 결정적인 역할을 함을 알 수 있습니다. 종합하면, FunGrasp에 도입된 여러 모듈들은 각각 의미있는 성능 향상에 기여하고 있으며, 이들의 조합이 최종 시스템의 강건함을 만들어냈음을 알 수 있습니다. </p>
</section>
<section id="장점-및-제한점-평가" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="장점-및-제한점-평가"><span class="header-section-number">2.5</span> 5. 장점 및 제한점 평가</h2>
<p>FunGrasp는 <strong>다관절 로봇 손 조작 분야에 여러 가지 혁신적 장점</strong>을 제공합니다. 가장 큰 강점은 <strong>범용성</strong>과 <strong>유연성</strong>입니다. 하나의 통합된 시스템으로 <strong>새로운 객체</strong>와 <strong>새로운 로봇 손 구조</strong> 모두에 빠르게 적응할 수 있다는 점은 매우 고무적입니다. 예를 들어, 이전의 많은 연구들은 특정 범주의 물체나 단일 로봇 손에 특화되어 일반화 범위가 제한적이었는데, FunGrasp는 <strong>범주 불문 다양한 물체</strong>에 대해, 그것도 <strong>작업에 맞는 방식으로 잡는</strong> 고차원적인 목표까지 달성하였습니다. 또한 새로운 물체마다 <strong>인간의 데모를 한 번씩만</strong> 제공하면 되므로, <strong>데이터 효율성</strong> 측면에서도 현실적인 장점을 갖습니다. 이는 복잡한 <strong>텔레조작 데이터 수집</strong>이나 대규모 <strong>3D 스캔 데이터</strong> 없이도, <strong>사람의 시연 한 번</strong>으로 로봇이 기능적 동작을 학습한다는 뜻이어서 실제 현장에서 로봇을 가르치는 비용을 크게 줄일 수 있습니다.</p>
<p>두 번째로, FunGrasp는 <strong>인간의 지식을 효과적으로 활용</strong>했다는 점에서 의의가 있습니다. 인간은 물체를 어떻게 잡으면 되는지 직관적으로 알지만, 로봇에게 이를 학습시키는 것은 어려운 문제입니다. 본 연구는 <strong>인간→로봇 그립 모방</strong>이라는 직관적인 접근을 취하면서도, 단순 모방의 한계를 넘어 <strong>정밀 접촉과 안정성</strong>까지 고려하였습니다. 그 결과 로봇의 그립이 단순히 물체를 놓치지 않는 것을 넘어, <strong>사람처럼 물체를 유용하게 활용할 수 있는 형태</strong>가 되었습니다. 이는 향후 로봇이 사람과 같은 환경에서 협업하고 도구를 사용하는 데 필수적인 능력이며, FunGrasp는 그 실마리를 제시했다고 볼 수 있습니다.</p>
<p>세 번째 강점은 <strong>실시간 동적 제어의 견고함</strong>입니다. 기존의 많은 방법들이 <strong>정적 파지 자세</strong>를 계획한 후 그대로 실행(Open-loop)하는 방식이어서, 실행 중 예기치 않은 물체 미끄러짐이나 외력에 취약했습니다. 반면 FunGrasp는 <strong>강화학습으로 폐루프 제어 정책</strong>을 학습하여, <strong>센서 피드백에 따라 그립을 지속적으로 보정</strong>할 수 있습니다. 실험에서 보인대로, 물체를 잡은 뒤 일부 방해를 줘도 로봇이 자동으로 움켜쥐는 힘과 각도를 조절하여 떨어뜨리지 않으려는 <strong>적응 행위</strong>를 보여주었습니다. 이러한 능력은 로봇 손을 <strong>실제 세계의 불확실성 속에서 신뢰성 있게 운영</strong>하는 데 핵심적인 요소입니다.</p>
<p>네 번째로, FunGrasp는 <strong>실제 로봇 구현까지 검증</strong>되었다는 점에서 중요합니다. 유사한 이전 연구들 중에는 시뮬레이션상 수십만 가지 물체에 대한 놀라운 성능을 보이지만 정작 현실 로봇에는 검증되지 않은 경우도 있었습니다. FunGrasp는 설계 단계부터 <strong>Sim-to-Real 전이</strong>를 고려하여, 비교적 간단한 하드웨어(UR5 + 로봇 손, RGB-D 카메라) 조합으로도 실험을 성공시켰습니다. 이는 해당 알고리즘이 현실적 한계까지 감안하여 개발되었음을 의미하며, 결과적으로 논문 발표와 동시에 <strong>즉시 실용적 응용이 가능한 수준</strong>의 성능을 입증했다는 점에서 높이 평가할 만합니다.</p>
<p>그럼에도 불구하고, 이 연구에는 <strong>몇 가지 제한점</strong>도 존재합니다. 우선, <strong>물체의 3D 모델 정보를 사전에 알고 있어야 한다는 점</strong>입니다. FunGrasp 시스템은 <strong>이미 알려진 물체</strong>의 경우에 그 물체의 CAD 모델을 이용해 정확한 자세 추정을 하고 접촉 정보를 얻을 수 있었지만, <strong>완전히 처음 보는 물체</strong>에 대해선 동일한 방식으로 접근하기 어렵습니다. 실제로 실험에서도 YCB 벤치마크처럼 <strong>표준 객체 세트</strong> 내에서 평가를 진행한 것으로 보이며, 임의의 일상 물체에 대해 <strong>모델 없이 동작</strong>할 수 있는지는 추가 연구가 필요한 부분입니다. 이와 관련하여 저자들도 <strong>현재는 물체 메쉬가 알고 있다는 가정이 한계</strong>이며, 장차 <strong>이미지 입력만으로 이를 대체</strong>할 통합 모델 개발이 필요하다고 언급하였습니다.</p>
<p>또 다른 제한으로는, <strong>각 새로운 물체마다 사람의 데모가 필요</strong>하다는 점을 들 수 있습니다. 본 연구는 one-shot 상황에 초점을 맞추었지만, <strong>zero-shot</strong> 일반화 즉 <strong>시연 없이도 로봇이 스스로 알맞은 기능적 파지를 찾는 것</strong>은 지원하지 않습니다. 물론 한 번의 시연이면 충분하다는 것은 큰 장점이지만, 만약 사람의 시연을 구하기 어려운 상황(예: 위험한 환경의 물체)에서는 이 접근을 바로 적용하기 어렵습니다. 궁극적으로는 <strong>사람 시연조차 필요 없이</strong>, 로봇이 <strong>학습된 경험과 사전 지식</strong>만으로 물체를 어떻게 잡을지 판단하게 하는 방향으로 나아가야 할 것입니다.</p>
<p>이외에도 기술적인 한계로, <strong>시각 모듈과 제어 모듈 간의 분리</strong>로 인한 잠재적 오류 전파 문제가 있습니다. 예를 들어 카메라로 측정된 손-물체 자세에 오차가 있을 경우 재타게팅 결과도 부정확해지고, 이는 강화학습 정책에도 영향을 미칠 수 있습니다. FunGrasp에서는 pose estimator의 잡음을 <strong>저역통과 필터</strong>로 안정화하는 등 대처를 했지만, 완전히 근본적인 해결은 아닌 것으로 보입니다. 또한 현재 시스템은 <strong>단일 손</strong>의 파지에 초점을 맞추고 있어, 두 손으로 물체를 협동으로 잡는 <strong>양손 파지</strong>나 파지 이후의 <strong>도구 사용 동작</strong> 등까지는 다루지 못합니다. 마지막으로, 성공률 74%라는 수치도 아직 <strong>상용화 관점</strong>에서 절대적인 신뢰성을 확보했다고 보긴 어렵습니다. 특히 일부 복잡한 형상의 물체에서는 실패가 누적될 가능성이 있으므로, 이러한 케이스를 더 줄이는 보완이 필요합니다.</p>
<p>요약하면 FunGrasp는 <strong>혁신적인 기능성</strong>과 <strong>범용성</strong>을 지닌 시스템이지만, <strong>사전 물체 모델 가정</strong>과 <strong>인간 시연 필요성</strong> 등의 한계가 존재하며, 이 부분은 향후 연구를 통해 개선될 여지가 있습니다. </p>
</section>
<section id="향후-연구-방향-및-응용-가능성" class="level2" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="향후-연구-방향-및-응용-가능성"><span class="header-section-number">2.6</span> 6. 향후 연구 방향 및 응용 가능성</h2>
<p>FunGrasp 연구는 <strong>기능적 로봇 파지</strong>에 대한 새로운 장을 열었으며, 이를 바탕으로 여러 흥미로운 향후 연구 방향과 폭넓은 응용 가능성이 예상됩니다. 우선 학계적으로는, <strong>보다 통합된 인지-제어 모델</strong>로의 발전이 자연스레 제기됩니다. 앞서 한계에서 지적한 바와 같이, <strong>물체 모델에 의존하지 않고도</strong> 동작하기 위해서는 로봇이 <strong>카메라 이미지 자체에서 물체의 잡기 포인트와 용도를 이해</strong>할 수 있어야 합니다. 이를 위해 딥러닝 기반의 <strong>엔드투엔드(visuo-motor) 정책 학습</strong>이나, 대규모 <strong>시각-언어 사전지식</strong>을 활용한 <strong>작업 의미 추론</strong> 등을 접목하는 방향이考案될 수 있습니다. 예를 들어, 미래에는 로봇이 카메라로 처음 본 가위를 보고도 “이것은 자르는 도구이니 손잡이를 잡아야겠다”고 <strong>추론</strong>하여 파지하는, 더 높은 수준의 인지 능력까지 포함한 시스템으로 발전할 수 있을 것입니다.</p>
<p>또한 <strong>진정한 의미의 원샷 학습</strong>으로 나아가기 위해, 단순히 한 번 시연을 본 물체만이 아니라 <strong>유사한 물체 전체에 일반화</strong>하는 연구도 필요합니다. FunGrasp가 one-shot generalization을 달성하였다고는 하지만, 이는 <strong>특정 개체의 시연</strong>을 바로 그 <strong>같은 개체에 적용</strong>한 형태입니다. 향후에는 예컨대 <strong>한 종류의 가위 시연 한 번</strong>으로 <strong>모든 종류의 가위</strong>를 쥘 수 있다거나, <strong>사람 시연 없이 VR 등을 통한 간접 시연</strong>으로도 학습하는 형태로 확장될 수 있습니다. 이를 위해서는 <strong>메타 러닝</strong>이나 <strong>도메인 적응</strong> 기법을 접목하여 <strong>시연 경험의 범용화</strong>를 도모하는 연구가 이어질 것으로 보입니다.</p>
<p>FunGrasp의 접근은 <strong>다지 로봇 손의 활용 범위</strong>를 크게 넓혔기 때문에, 응용 측면에서도 많은 기회가 있습니다. 예를 들어 <strong>서비스 로봇</strong>이나 <strong>케어 로봇</strong> 분야에서, 사람이 일일이 로봇에게 동작을 프로그래밍하지 않고 <strong>직접 시범을 보이며 가르칠 수 있는</strong> 직관적 인터페이스로 활용될 수 있습니다. 가령 간병인이 로봇에게 <strong>환자에게 컵을 쥐여주는 방법</strong>을 한 번 보여주면, 로봇은 그 컵을 같은 방식으로 잡아 환자에게 전달할 수 있을 것입니다. <strong>산업 현장</strong>에서도 새로운 공구나 부품을 로봇에게 가르칠 때 유용할 것입니다. 작업자는 보호장갑을 낀 자신의 손으로 공구를 어떻게 잡는지 한번 보여주고, 로봇은 이를 즉각 학습하여 동일한 자세로 공구를 잡은 후 필요한 작업을 수행하게 할 수 있습니다. 이러한 <strong>라이브 데모 기반 학습</strong>은 기존의 프로그래밍이나 오프라인 학습보다 훨씬 빠르고 유연하게 <strong>로봇 투입을 가능</strong>하게 만들어줄 것으로 기대됩니다.</p>
<p>더 나아가, FunGrasp의 기술을 발전시키면 로봇이 <strong>단순 파지를 넘어 물체를 능숙하게 조작(manipulate)</strong>하는 단계로도 나아갈 수 있습니다. 예컨대 가위를 손잡이에 쥐는 것에서 끝나지 않고, 실제로 <strong>가위질을 해서 물체를 자르는 동작</strong>까지 연계한다면 <strong>기능적 파지→기능 실행</strong>으로 임무를 확장할 수 있을 것입니다. 이를 위해서는 파지 이후의 <strong>세부 동작 계획</strong>과 <strong>력 제어</strong> 등이 추가로 필요하지만, <strong>기능적 파지</strong>가 선행되어야 가능한 작업이므로 FunGrasp의 성과는 이러한 후속 연구의 토대가 됩니다. 실제로 FunGrasp의 저자들도 이후 연구로 <strong>양손을 이용한 물체 조작</strong>(예: 문고리를 한 손으로 잡고 다른 손으로 돌리기)이나 <strong>물체의 상태 변화(관절 움직임 등)까지 고려한 파지</strong> 등의 문제를 다루고자 하고 있으며, 관련된 선행 연구(ArtiGrasp 등)와의 접목도 충분히 가능해 보입니다.</p>
<p>마지막으로, <strong>센서 기술의 발전과의 통합</strong>도 기대되는 방향입니다. 현재 FunGrasp는 시뮬레이션 접촉 정보를 LSTM으로 추론하는 방식을 썼지만, 향후 <strong>로봇 손가락에 실제 고해상도 촉각 센서</strong>가 내장된다면 훨씬 풍부한 피드백으로 제어를 정교화할 수 있습니다. 예컨대 인간은 손끝의 촉각으로 미끄러짐을 바로 감지해 힘을 조절하는데, 로봇도 촉각 센서와 FunGrasp의 학습 정책을 결합하면 더욱 인간에 가까운 <strong>세밀 조작</strong>이 가능해질 것입니다. 요컨대, <strong>FunGrasp는 기능적 파지의 중요성을 환기시키고 실현 가능한 솔루션을 제시했다</strong>는 점에서 이후 많은 연구의 출발점이 될 것이며, <strong>사람과 로봇이 함께 생활하고 작업하는 미래</strong>를 앞당기는 데 기여할 것으로 전망됩니다. </p>
<hr>
<p><strong>주요 출처</strong>: FunGrasp 원문(초록·방법·결론·표/그림)과 구현·실험 상세(하드웨어, 학습 설정, 74% 성공률, 소절 결과 등)은 다음을 참고했습니다. </p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<script src="https://utteranc.es/client.js" repo="curieuxjy/blog_comments" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Copyright 2024, Jung Yeon Lee</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/curieuxjy">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>