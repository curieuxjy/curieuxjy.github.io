<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.33">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-11-02">
<meta name="description" content="Gravity-Invariant In-Hand Object Rotation with Sim-to-Real Touch">

<title>📃AnyRotate 리뷰 – Curieux.JY</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../profile.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-ea385d0e468b0dd5ea5bf0780b1290d9.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-bc185b5c5bdbcb35c2eb49d8a876ef70.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-ea385d0e468b0dd5ea5bf0780b1290d9.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-23aef1c2a45953e85f3378e7ccfb1407.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-5a614c35f1f90bfd0a5b2992298a8538.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-23aef1c2a45953e85f3378e7ccfb1407.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-2NVZN2MJZT"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-2NVZN2MJZT', { 'anonymize_ip': true});
</script>
<script src="../../site_libs/quarto-diagram/mermaid.min.js"></script>
<script src="../../site_libs/quarto-diagram/mermaid-init.js"></script>
<link href="../../site_libs/quarto-diagram/mermaid.css" rel="stylesheet">

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Curieux.JY</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../post.html"> 
<span class="menu-text">Post</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../note.html"> 
<span class="menu-text">Note</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">Jung Yeon Lee</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/curieuxjy"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#ping-review" id="toc-ping-review" class="nav-link active" data-scroll-target="#ping-review">🔍 Ping Review</a></li>
  <li><a href="#ring-review" id="toc-ring-review" class="nav-link" data-scroll-target="#ring-review">🔔 Ring Review</a>
  <ul class="collapse">
  <li><a href="#왜-이-연구가-중요할까" id="toc-왜-이-연구가-중요할까" class="nav-link" data-scroll-target="#왜-이-연구가-중요할까">1. 왜 이 연구가 중요할까?</a>
  <ul class="collapse">
  <li><a href="#로봇의-영원한-숙제-in-hand-manipulation" id="toc-로봇의-영원한-숙제-in-hand-manipulation" class="nav-link" data-scroll-target="#로봇의-영원한-숙제-in-hand-manipulation">로봇의 영원한 숙제: In-Hand Manipulation</a></li>
  <li><a href="#기존-연구들의-한계" id="toc-기존-연구들의-한계" class="nav-link" data-scroll-target="#기존-연구들의-한계">기존 연구들의 한계</a></li>
  </ul></li>
  <li><a href="#anyrotate는-어떻게-작동할까" id="toc-anyrotate는-어떻게-작동할까" class="nav-link" data-scroll-target="#anyrotate는-어떻게-작동할까">2. AnyRotate는 어떻게 작동할까?</a>
  <ul class="collapse">
  <li><a href="#시스템-구성-로봇-손-촉각-센서" id="toc-시스템-구성-로봇-손-촉각-센서" class="nav-link" data-scroll-target="#시스템-구성-로봇-손-촉각-센서">시스템 구성: 로봇 손 + 촉각 센서</a></li>
  <li><a href="#학습-전략-2단계-접근법" id="toc-학습-전략-2단계-접근법" class="nav-link" data-scroll-target="#학습-전략-2단계-접근법">학습 전략: 2단계 접근법</a></li>
  <li><a href="#핵심-아이디어-보조-목표auxiliary-goal" id="toc-핵심-아이디어-보조-목표auxiliary-goal" class="nav-link" data-scroll-target="#핵심-아이디어-보조-목표auxiliary-goal">핵심 아이디어: 보조 목표(Auxiliary Goal)</a></li>
  <li><a href="#적응형-커리큘럼-단계적-학습" id="toc-적응형-커리큘럼-단계적-학습" class="nav-link" data-scroll-target="#적응형-커리큘럼-단계적-학습">적응형 커리큘럼: 단계적 학습</a></li>
  </ul></li>
  <li><a href="#고밀도-촉각-표현-게임-체인저" id="toc-고밀도-촉각-표현-게임-체인저" class="nav-link" data-scroll-target="#고밀도-촉각-표현-게임-체인저">3. 고밀도 촉각 표현: 게임 체인저</a>
  <ul class="collapse">
  <li><a href="#기존-촉각-표현들" id="toc-기존-촉각-표현들" class="nav-link" data-scroll-target="#기존-촉각-표현들">기존 촉각 표현들</a></li>
  <li><a href="#anyrotate의-고밀도-촉각-dense-touch" id="toc-anyrotate의-고밀도-촉각-dense-touch" class="nav-link" data-scroll-target="#anyrotate의-고밀도-촉각-dense-touch">AnyRotate의 고밀도 촉각 (Dense Touch)</a></li>
  <li><a href="#sim-to-real-전이-시뮬레이션에서-현실로" id="toc-sim-to-real-전이-시뮬레이션에서-현실로" class="nav-link" data-scroll-target="#sim-to-real-전이-시뮬레이션에서-현실로">Sim-to-Real 전이: 시뮬레이션에서 현실로</a></li>
  </ul></li>
  <li><a href="#실험-결과-숫자가-말해주는-성능" id="toc-실험-결과-숫자가-말해주는-성능" class="nav-link" data-scroll-target="#실험-결과-숫자가-말해주는-성능">4. 실험 결과: 숫자가 말해주는 성능</a>
  <ul class="collapse">
  <li><a href="#시뮬레이션-실험-촉각-정보의-중요성" id="toc-시뮬레이션-실험-촉각-정보의-중요성" class="nav-link" data-scroll-target="#시뮬레이션-실험-촉각-정보의-중요성">시뮬레이션 실험: 촉각 정보의 중요성</a></li>
  <li><a href="#보조-목표의-효과" id="toc-보조-목표의-효과" class="nav-link" data-scroll-target="#보조-목표의-효과">보조 목표의 효과</a></li>
  <li><a href="#실제-로봇-실험-진짜-세계에서의-도전" id="toc-실제-로봇-실험-진짜-세계에서의-도전" class="nav-link" data-scroll-target="#실제-로봇-실험-진짜-세계에서의-도전">실제 로봇 실험: 진짜 세계에서의 도전</a></li>
  <li><a href="#촉각-센서-분석-로봇이-느끼는-것" id="toc-촉각-센서-분석-로봇이-느끼는-것" class="nav-link" data-scroll-target="#촉각-센서-분석-로봇이-느끼는-것">촉각 센서 분석: 로봇이 ‘느끼는’ 것</a></li>
  <li><a href="#궁극의-테스트-회전하는-손" id="toc-궁극의-테스트-회전하는-손" class="nav-link" data-scroll-target="#궁극의-테스트-회전하는-손">궁극의 테스트: 회전하는 손</a></li>
  </ul></li>
  <li><a href="#기술적-깊이-파고들기" id="toc-기술적-깊이-파고들기" class="nav-link" data-scroll-target="#기술적-깊이-파고들기">5. 기술적 깊이 파고들기</a>
  <ul class="collapse">
  <li><a href="#보상-함수-해부" id="toc-보상-함수-해부" class="nav-link" data-scroll-target="#보상-함수-해부">보상 함수 해부</a></li>
  <li><a href="#네트워크-아키텍처-상세" id="toc-네트워크-아키텍처-상세" class="nav-link" data-scroll-target="#네트워크-아키텍처-상세">네트워크 아키텍처 상세</a></li>
  <li><a href="#시스템-식별-시뮬레이션을-현실과-맞추기" id="toc-시스템-식별-시뮬레이션을-현실과-맞추기" class="nav-link" data-scroll-target="#시스템-식별-시뮬레이션을-현실과-맞추기">시스템 식별: 시뮬레이션을 현실과 맞추기</a></li>
  <li><a href="#도메인-무작위화-다양성이-답이다" id="toc-도메인-무작위화-다양성이-답이다" class="nav-link" data-scroll-target="#도메인-무작위화-다양성이-답이다">도메인 무작위화: 다양성이 답이다</a></li>
  <li><a href="#실시간-제어-파이프라인" id="toc-실시간-제어-파이프라인" class="nav-link" data-scroll-target="#실시간-제어-파이프라인">실시간 제어 파이프라인</a></li>
  </ul></li>
  <li><a href="#비교-분석-기존-연구와-어떻게-다른가" id="toc-비교-분석-기존-연구와-어떻게-다른가" class="nav-link" data-scroll-target="#비교-분석-기존-연구와-어떻게-다른가">6. 비교 분석: 기존 연구와 어떻게 다른가?</a>
  <ul class="collapse">
  <li><a href="#openai의-solving-rubiks-cube-with-a-robot-hand-2019와-비교" id="toc-openai의-solving-rubiks-cube-with-a-robot-hand-2019와-비교" class="nav-link" data-scroll-target="#openai의-solving-rubiks-cube-with-a-robot-hand-2019와-비교">OpenAI의 “Solving Rubik’s Cube with a Robot Hand” (2019)와 비교</a></li>
  <li><a href="#최근-촉각-기반-연구들과-비교" id="toc-최근-촉각-기반-연구들과-비교" class="nav-link" data-scroll-target="#최근-촉각-기반-연구들과-비교">최근 촉각 기반 연구들과 비교</a></li>
  <li><a href="#sievers-et-al.-2022---토크-제어-순수-촉각" id="toc-sievers-et-al.-2022---토크-제어-순수-촉각" class="nav-link" data-scroll-target="#sievers-et-al.-2022---토크-제어-순수-촉각">Sievers et al.&nbsp;(2022) - 토크 제어 순수 촉각</a></li>
  </ul></li>
  <li><a href="#이론적-관점-왜-이게-작동하는가" id="toc-이론적-관점-왜-이게-작동하는가" class="nav-link" data-scroll-target="#이론적-관점-왜-이게-작동하는가">7. 이론적 관점: 왜 이게 작동하는가?</a>
  <ul class="collapse">
  <li><a href="#정보-이론으로-바라보기" id="toc-정보-이론으로-바라보기" class="nav-link" data-scroll-target="#정보-이론으로-바라보기">정보 이론으로 바라보기</a></li>
  <li><a href="#pomdp-관점" id="toc-pomdp-관점" class="nav-link" data-scroll-target="#pomdp-관점">POMDP 관점</a></li>
  <li><a href="#접촉-역학의-관점" id="toc-접촉-역학의-관점" class="nav-link" data-scroll-target="#접촉-역학의-관점">접촉 역학의 관점</a></li>
  <li><a href="#강화학습-관점-탐색의-어려움" id="toc-강화학습-관점-탐색의-어려움" class="nav-link" data-scroll-target="#강화학습-관점-탐색의-어려움">강화학습 관점: 탐색의 어려움</a></li>
  <li><a href="#창발적-행동-미끄럼-감지" id="toc-창발적-행동-미끄럼-감지" class="nav-link" data-scroll-target="#창발적-행동-미끄럼-감지">창발적 행동: 미끄럼 감지</a></li>
  </ul></li>
  <li><a href="#한계점과-개선-방향" id="toc-한계점과-개선-방향" class="nav-link" data-scroll-target="#한계점과-개선-방향">8. 한계점과 개선 방향</a>
  <ul class="collapse">
  <li><a href="#물체-형상의-까다로움" id="toc-물체-형상의-까다로움" class="nav-link" data-scroll-target="#물체-형상의-까다로움">물체 형상의 까다로움</a></li>
  <li><a href="#하드웨어의-물리적-한계" id="toc-하드웨어의-물리적-한계" class="nav-link" data-scroll-target="#하드웨어의-물리적-한계">하드웨어의 물리적 한계</a></li>
  <li><a href="#회전을-넘어서-더-복잡한-작업들" id="toc-회전을-넘어서-더-복잡한-작업들" class="nav-link" data-scroll-target="#회전을-넘어서-더-복잡한-작업들">회전을 넘어서: 더 복잡한 작업들</a></li>
  <li><a href="#변형되는-물체들" id="toc-변형되는-물체들" class="nav-link" data-scroll-target="#변형되는-물체들">변형되는 물체들</a></li>
  <li><a href="#시뮬레이션과-현실의-간극" id="toc-시뮬레이션과-현실의-간극" class="nav-link" data-scroll-target="#시뮬레이션과-현실의-간극">시뮬레이션과 현실의 간극</a></li>
  <li><a href="#학습-비용의-현실" id="toc-학습-비용의-현실" class="nav-link" data-scroll-target="#학습-비용의-현실">학습 비용의 현실</a></li>
  <li><a href="#평가의-어려움" id="toc-평가의-어려움" class="nav-link" data-scroll-target="#평가의-어려움">평가의 어려움</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#dig-review" id="toc-dig-review" class="nav-link" data-scroll-target="#dig-review">⛏️ Dig Review</a>
  <ul class="collapse">
  <li><a href="#anyrotate-논문-개요-및-시스템-구조" id="toc-anyrotate-논문-개요-및-시스템-구조" class="nav-link" data-scroll-target="#anyrotate-논문-개요-및-시스템-구조">AnyRotate 논문 개요 및 시스템 구조</a></li>
  <li><a href="#촉각-센서-데이터의-표현과-정책에서의-활용" id="toc-촉각-센서-데이터의-표현과-정책에서의-활용" class="nav-link" data-scroll-target="#촉각-센서-데이터의-표현과-정책에서의-활용">촉각 센서 데이터의 표현과 정책에서의 활용</a></li>
  <li><a href="#촉각-피드백의-시뮬레이션-구현과-도메인-적응" id="toc-촉각-피드백의-시뮬레이션-구현과-도메인-적응" class="nav-link" data-scroll-target="#촉각-피드백의-시뮬레이션-구현과-도메인-적응">촉각 피드백의 시뮬레이션 구현과 도메인 적응</a></li>
  <li><a href="#observation-model을-통한-시뮬-실제-전이" id="toc-observation-model을-통한-시뮬-실제-전이" class="nav-link" data-scroll-target="#observation-model을-통한-시뮬-실제-전이">Observation Model을 통한 시뮬-실제 전이</a></li>
  <li><a href="#교사-학생-정책-학습-파이프라인" id="toc-교사-학생-정책-학습-파이프라인" class="nav-link" data-scroll-target="#교사-학생-정책-학습-파이프라인">교사-학생 정책 학습 파이프라인</a></li>
  <li><a href="#촉각-감지-모델링의-혁신성과-도전-과제" id="toc-촉각-감지-모델링의-혁신성과-도전-과제" class="nav-link" data-scroll-target="#촉각-감지-모델링의-혁신성과-도전-과제">촉각 감지 모델링의 혁신성과 도전 과제</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-page-left" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">📃AnyRotate 리뷰</h1>
  <div class="quarto-categories">
    <div class="quarto-category">touch</div>
    <div class="quarto-category">in-hand</div>
    <div class="quarto-category">rotation</div>
  </div>
  </div>

<div>
  <div class="description">
    Gravity-Invariant In-Hand Object Rotation with Sim-to-Real Touch
  </div>
</div>


<div class="quarto-title-meta column-page-left">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">November 2, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<blockquote class="blockquote">
<p>🔍 Ping. 🔔 Ring. ⛏️ Dig. A tiered review series: quick look, key ideas, deep dive.</p>
</blockquote>
<ul>
<li><a href="https://arxiv.org/abs/2405.07391">Paper Link</a></li>
<li><a href="https://maxyang27896.github.io/anyrotate/">Project LInk</a></li>
</ul>
<ol type="1">
<li>✋ 다자유도 로봇 손의 중력 불변 다축 물체 회전(gravity-invariant multi-axis in-hand object rotation)은 어려운 과제이며, 본 논문은 시뮬레이션-실제 환경 제로샷 전환이 가능한 고밀도 촉각 정보(dense featured sim-to-real touch)를 활용하는 AnyRotate 시스템을 제안합니다.</li>
<li>🧠 이 시스템은 목표 조건 강화 학습과 접촉 자세 및 힘을 포함하는 고밀도 촉각 표현(dense tactile representation)을 사용하여, 중력 영향을 받지 않는 임의의 회전 축과 다양한 손 방향에서 동작하는 단일 정책을 성공적으로 학습했습니다.</li>
<li>🚀 실제 실험에서 AnyRotate는 다양한 손 방향과 회전 축에서 미지의 객체에 대한 강력한 강건성(robustness)을 입증했으며, 풍부한 촉각 정보로 불안정한 파지(unstable grasp)를 감지하고 이를 복구하는 자율적 행동(emergent behavior)을 보여주었습니다.</li>
</ol>
<center>
<img src="../../images/2025-11-02-anyrotate/0.png" width="80%">
</center>
<hr>
<section id="ping-review" class="level1">
<h1>🔍 Ping Review</h1>
<blockquote class="blockquote">
<p>🔍 Ping — A light tap on the surface. Get the gist in seconds.</p>
</blockquote>
<p>본 논문 “AnyRotate: Gravity-Invariant In-Hand Object Rotation with Sim-to-Real Touch”는 로봇 손이 다양한 손의 움직임 속에서도 물체를 조작하는 데 있어 중력 불변(gravity-invariant)의 in-hand object rotation을 달성하기 위한 시스템 AnyRotate를 제안합니다. 기존 비전 기반 접근 방식은 self-occlusion과 같은 문제로 인해 in-hand manipulation에 한계가 있었으며, 촉각 센서(tactile sensor)는 상세한 접촉 정보를 제공할 수 있음에도 불구하고 sim-to-real gap으로 인해 고해상도 촉각 데이터가 충분히 활용되지 못했습니다. 본 연구는 이러한 문제를 해결하기 위해 dense featured sim-to-real touch를 활용하여 시뮬레이션에서 훈련된 policy를 실제 로봇에 zero-shot transfer하는 방법을 제시합니다.</p>
<p><strong>핵심 방법론:</strong></p>
<p>AnyRotate는 multi-axis gravity-invariant in-hand object rotation을 위해 다음과 같은 핵심 요소들을 통합합니다.</p>
<ol type="1">
<li><p><strong>Goal-Conditioned Reinforcement Learning (RL) Formulation:</strong> 물체 회전 문제를 물체 재방향(object reorientation)으로 정형화하며, 회전 속도(angular velocity) 기반의 목표 설정이 비효율적임을 지적하고, 보조 목표(auxiliary goals) 방식을 도입합니다. 이 보조 목표는 현재 물체 자세(object orientation)를 원하는 회전 축(rotation axis)을 중심으로 일정한 간격으로 회전시켜 새로운 목표 자세를 생성합니다. Policy는 이러한 동적인 목표 자세를 추종하도록 훈련됩니다. 보상 함수 <span class="math inline">r</span>는 물체 회전(rotation), 접촉(contact), 안정성(stability), 종료(termination)의 네 가지 요소로 구성됩니다. <span class="math display">r = r_{\text{rotation}} + r_{\text{contact}} + r_{\text{stable}} + r_{\text{terminate}}</span> <span class="math inline">r_{\text{rotation}}</span>은 키포인트(keypoint) 거리 <span class="math inline">K(||k_o^i - k_g^i||)</span>와 회전 보너스(goal bonus)를 통해 목표 자세 도달 및 연속적인 회전을 장려합니다. <span class="math inline">r_{\text{contact}}</span>는 손끝(fingertip) 접촉을 최대화하고 다른 부위의 접촉을 패널티화하여 정밀한 파지(precision grasp)를 유도합니다. <span class="math inline">r_{\text{stable}}</span>은 물체의 각속도(angular velocity), 손 자세(hand-pose), 제어기(controller)의 작업량 및 토크(torque)를 패널티화하여 안정적인 회전을 가능하게 합니다. 마지막으로 <span class="math inline">r_{\text{terminate}}</span>는 물체가 파지에서 벗어나거나 회전 축이 목표에서 크게 벗어날 경우 패널티를 부여합니다. 학습 과정의 효율성을 위해, episode당 달성된 평균 회전 횟수에 따라 <span class="math inline">r_{\text{contact}}</span>와 <span class="math inline">r_{\text{stable}}</span>에 가중치 계수 <span class="math inline">\lambda_{\text{rew}}</span>를 선형적으로 증가시키는 적응형 커리큘럼(adaptive curriculum)을 적용합니다.</p></li>
<li><p><strong>Dense Tactile Representation:</strong> 물체와의 상호작용에 대한 풍부한 정보를 제공하기 위해, 국부적인 접촉 자세(local contact pose)와 접촉력(contact force)으로 구성된 dense tactile representation을 사용합니다. Contact pose는 극각(polar angle) <span class="math inline">R_x</span>와 방위각(azimuthal angle) <span class="math inline">R_y</span>로 정의되며, contact force는 3D 접촉력의 크기 <span class="math inline">||F||</span>를 의미합니다. 시뮬레이션에서는 센서를 강체(rigid body)로 근사하여 접촉 정보를 직접 가져오고, 실제 센서의 지연과 노이즈를 시뮬레이션하기 위해 접촉력에 exponential moving average를 적용하고 값을 실제 센서의 범위에 맞춰 포화(saturate) 및 재조정(rescale)합니다.</p></li>
<li><p><strong>Sim-to-Real Policy Transfer:</strong> Policy 학습은 두 단계의 policy distillation 방식을 따릅니다.</p>
<ul>
<li><strong>Teacher Training:</strong> 특권 정보(privileged information, 예: 물체 위치, 자세, 질량, 중력 벡터 등)가 제공된 시뮬레이션 환경에서 RL (Proximal Policy Optimization, PPO)을 통해 teacher policy를 훈련합니다.</li>
<li><strong>Student Training:</strong> 실제 환경에서 관측 가능한 정보(proprioception, 촉각 피드백)만을 사용하는 student policy를 훈련합니다. Student policy는 teacher policy와 동일한 actor-critic architecture를 가지며, 과거 <span class="math inline">N</span>개의 관측 시퀀스(sequence of observations)로부터 TCN (Temporal Convolutional Network) encoder를 통해 저차원 잠재 벡터(latent vector) <span class="math inline">z_t</span>를 예측합니다. Student는 teacher의 행동을 모방하도록 지도 학습(supervised learning) 방식으로 훈련되며, 이때 잠재 벡터와 행동 분포에 대한 MSE (Mean Squared Error) 및 NLL (Negative Log-Likelihood) 손실을 최소화합니다.</li>
<li><strong>Tactile Perception Model:</strong> Zero-shot sim-to-real transfer를 위해, 실제 촉각 이미지에서 dense contact feature를 추출하는 tactile perception model을 훈련합니다. 이 모델은 UR5 로봇 팔에 tactile sensor를 부착하고 force/torque sensor가 부착된 작업대 표면에서 센서를 무작위로 움직이며 접촉 깊이(contact depth), 접촉 자세, 접촉력을 레이블로 하여 데이터를 수집하여 CNN 기반으로 훈련됩니다. 실제 배포 시에는 SSIM (Structured Similarity Index)을 사용하여 이진 접촉(binary contact)을 계산하고, 이를 기반으로 contact pose 및 contact force 예측값을 마스킹(masking)합니다.</li>
</ul></li>
</ol>
<p><strong>실험 및 분석:</strong></p>
<p>실험은 Allegro Hand에 vision-based tactile sensor를 장착한 로봇 시스템에서 수행되었습니다.</p>
<ul>
<li><strong>훈련 성능:</strong> 보조 목표(auxiliary goal)를 사용하는 제안된 formulation이 angular rotation objective보다 multi-axis rotation 작업에서 훨씬 높은 정확도와 수렴 성공률을 보였습니다. 이는 특히 물체가 본질적으로 불안정한 configuration에서 작은 랜덤 액션이 회복 불가능한 상태로 이어질 수 있는 상황에서, 목표 지향적인 보상이 agent를 더 효과적으로 안내했음을 시사합니다. 적응형 커리큘럼 또한 학습에 긍정적인 영향을 주었습니다.</li>
<li><strong>시뮬레이션 결과:</strong> Dense touch policy(contact pose 및 contact force)는 proprioception, binary touch, discrete touch 등 더 단순하고 덜 상세한 촉각 정보를 사용하는 policy들보다 우수한 성능을 보였습니다. 이는 상세한 촉각 정보가 다양한 mass 및 shape를 가진 unseen objects를 다루는 데 중요함을 입증합니다.</li>
<li><strong>실제 환경 결과:</strong> Dense touch policy는 실제 환경에서 10가지 다양한 물체에 대해 성공적인 zero-shot transfer를 달성했으며, 다양한 hand orientation(손 방향) 및 rotation axis(회전 축)에서 가장 강력한 robustness를 보여주었습니다. 특히, 손이 중력에 대해 수평으로 위치할 때 성능 저하가 있었으나, dense touch policy는 이러한 noisy system에서도 안정적인 성능을 유지했습니다.</li>
<li><strong>Emergent Behavior:</strong> Rich tactile sensing이 불안정한 파지(unstable grasp)를 감지하고, 물체 미끄러짐을 방지하는 반응적인 finger-gaiting 움직임을 유발하는 emergent behavior를 보여주었습니다. 이는 proprioception이나 binary touch만으로는 관찰되지 않았습니다.</li>
<li><strong>Gravity Invariance 및 Rotating Hand:</strong> 훈련된 policy는 중력 벡터가 지속적으로 변하는 회전하는 손(rotating hand)에서도 효과적으로 물체 조작에 적응할 수 있음을 입증했습니다. 이는 물체의 6D 재방향과 동시에 파지 위치를 변경할 수 있는 새로운 수준의 dexterity를 제공합니다.</li>
</ul>
<p><strong>결론 및 한계:</strong></p>
<p>본 연구는 rich tactile sensing을 활용하여 어떤 hand direction에서도 어떤 rotation axis로든 in-hand object rotation을 수행하는 일반적인 policy의 가능성을 보여주었습니다. 이는 multi-fingered robot hand의 촉각 dexterity 발전에 중요한 진전을 의미합니다. 하지만, 날카로운 기하학적 특징(sharp geometric features)을 가진 물체(예: 모서리)를 다루는 데 어려움이 있었으며, 이는 더 풍부한 촉각 표현(tactile representation)이나 시각 정보(visual information)를 통합하여 개선될 수 있습니다. 또한, Allegro Hand의 actuation이 특정 손 방향에서 약화되는 한계를 보여, 향후 low-cost면서도 더 강력한 하드웨어의 개발이 필요함을 시사합니다.</p>
</section>
<section id="ring-review" class="level1">
<h1>🔔 Ring Review</h1>
<blockquote class="blockquote">
<p>🔔 Ring — An idea that echoes. Grasp the core and its value.</p>
</blockquote>
<blockquote class="blockquote">
<p>“거꾸로 들어도, 옆으로 들어도, 로봇 손이 물체를 자유자재로 회전시킨다”</p>
</blockquote>
<p>안녕하세요! 오늘은 로봇 조작 분야에서 정말 흥미로운 논문을 소개해드리려고 합니다. Bristol 대학교의 Nathan Lepora 교수 연구팀이 발표한 <strong>AnyRotate</strong>라는 시스템인데요, 제목에서부터 야심차게도 “Gravity Invariant”(중력 불변)라는 단어를 사용하고 있습니다.</p>
<p>여러분, 손바닥을 위로 향한 채로 공을 돌리는 건 쉽죠? 그런데 손바닥을 아래로 뒤집은 채로 공을 떨어뜨리지 않고 계속 돌릴 수 있나요? 심지어 손을 옆으로 돌리거나 계속 움직이면서요? 사람도 어려운 이 동작을, 로봇이 해낸다는 겁니다. 그것도 <strong>처음 보는 물체</strong>로 말이죠!</p>
<hr>
<section id="왜-이-연구가-중요할까" class="level2">
<h2 class="anchored" data-anchor-id="왜-이-연구가-중요할까">1. 왜 이 연구가 중요할까?</h2>
<section id="로봇의-영원한-숙제-in-hand-manipulation" class="level3">
<h3 class="anchored" data-anchor-id="로봇의-영원한-숙제-in-hand-manipulation">로봇의 영원한 숙제: In-Hand Manipulation</h3>
<p>로봇 팔이 물체를 집는 건 이제 어느 정도 해결됐습니다. 하지만 <strong>손 안에서 물체를 자유롭게 조작하는 것</strong>(in-hand manipulation)은 여전히 어려운 문제입니다. 우리는 손 안에서 연필을 돌리고, 동전을 손가락으로 굴리고, 루빅스 큐브를 맞추죠. 하지만 로봇에게 이런 일은 정말 어렵습니다.</p>
<p><strong>왜 어려울까요?</strong></p>
<ol type="1">
<li><p><strong>높은 자유도</strong>: 사람 손은 27개의 뼈와 수많은 관절로 이루어져 있습니다. 로봇 손도 16개 이상의 관절을 제어해야 합니다.</p></li>
<li><p><strong>복잡한 물리</strong>: 손가락과 물체 사이의 마찰, 미끄러짐, 접촉 등을 정확히 이해해야 합니다.</p></li>
<li><p><strong>부분 관측</strong>: 손이 물체를 가리기 때문에 카메라로는 정확한 상태를 보기 어렵습니다.</p></li>
<li><p><strong>중력의 방해</strong>: 손을 뒤집으면 물체가 떨어지려고 합니다.</p></li>
</ol>
</section>
<section id="기존-연구들의-한계" class="level3">
<h3 class="anchored" data-anchor-id="기존-연구들의-한계">기존 연구들의 한계</h3>
<p><strong>OpenAI의 루빅스 큐브</strong> (2019)</p>
<ul>
<li>정말 인상적한 성과였지만…</li>
<li>엄청난 수의 카메라가 필요했습니다 (자기 폐색 문제)</li>
<li>손은 항상 위를 향했습니다</li>
<li>특정 물체(루빅스 큐브)에만 작동했습니다</li>
</ul>
<p><strong>최근 촉각 기반 연구들</strong></p>
<ul>
<li>이진 접촉(닿았다/안 닿았다)만 사용</li>
<li>손바닥 위 방향에서만 작동</li>
<li>x, y, z축 중 하나의 축만 회전 가능</li>
<li>각 축마다 별도의 정책 필요</li>
</ul>
<p><strong>이 논문이 해결하려는 것:</strong></p>
<ul>
<li>✅ 풍부한 촉각 정보 활용</li>
<li>✅ 어떤 방향에서든 작동</li>
<li>✅ 임의의 축으로 회전</li>
<li>✅ 하나의 통합된 정책</li>
<li>✅ 다양한 미지의 물체</li>
</ul>
<hr>
</section>
</section>
<section id="anyrotate는-어떻게-작동할까" class="level2">
<h2 class="anchored" data-anchor-id="anyrotate는-어떻게-작동할까">2. AnyRotate는 어떻게 작동할까?</h2>
<section id="시스템-구성-로봇-손-촉각-센서" class="level3">
<h3 class="anchored" data-anchor-id="시스템-구성-로봇-손-촉각-센서">시스템 구성: 로봇 손 + 촉각 센서</h3>
<p>AnyRotate는 <strong>Allegro Hand</strong>라는 4개 손가락 로봇 손을 사용합니다. 그리고 핵심은 각 손가락 끝에 부착된 <strong>DigiTac 촉각 센서</strong>입니다.</p>
<p><strong>DigiTac 센서가 특별한 이유:</strong></p>
<p>이 센서는 사람의 지문처럼 작은 돌기들이 있는 부드러운 스킨으로 덮여 있습니다. 스킨 아래에는 작은 카메라가 있어서, 물체가 닿으면 돌기들이 움직이는 모습을 촬영합니다. 이를 통해:</p>
<ul>
<li><strong>접촉 위치</strong>: 물체가 손가락의 어디에 닿았는지 (각도로 표현)</li>
<li><strong>접촉 힘</strong>: 얼마나 세게 누르는지 (힘의 크기)</li>
</ul>
<p>이 두 가지 정보를 동시에 알 수 있습니다!</p>
<pre><code>기존 방식: "물체가 닿았다" (1 bit 정보)
AnyRotate: "물체가 15도 각도로, 2.3N의 힘으로 닿았다" (연속적인 정보)</code></pre>
<p>이 차이가 얼마나 중요한지는 뒤에서 실험 결과로 보여드리겠습니다.</p>
</section>
<section id="학습-전략-2단계-접근법" class="level3">
<h3 class="anchored" data-anchor-id="학습-전략-2단계-접근법">학습 전략: 2단계 접근법</h3>
<p>로봇을 학습시키는 과정이 정말 영리합니다. 마치 학생이 선생님께 배우는 것처럼 2단계로 나뉩니다.</p>
<section id="단계-teacher-정책-학습-시뮬레이션" class="level4">
<h4 class="anchored" data-anchor-id="단계-teacher-정책-학습-시뮬레이션">1단계: Teacher 정책 학습 (시뮬레이션)</h4>
<p><strong>시뮬레이션의 장점:</strong></p>
<ul>
<li>로봇을 실제로 수천 번 돌릴 필요 없음</li>
<li>물체를 떨어뜨려도 괜찮음</li>
<li>8,192개의 로봇을 동시에 학습시킬 수 있음!</li>
</ul>
<p><strong>Teacher가 가진 특권:</strong></p>
<p>선생님(Teacher)은 물체의 정확한 위치, 방향, 무게, 중력 방향 등 모든 정보를 알고 있습니다. 이는 실제 세계에서는 불가능한 “치팅”이지만, 시뮬레이션에서는 가능하죠.</p>
<p><strong>강화학습으로 학습:</strong></p>
<p>Teacher는 PPO(Proximal Policy Optimization)라는 강화학습 알고리즘으로 학습됩니다. 보상 함수를 잘 설계하는 게 핵심인데, 이 논문의 보상 함수는 정말 정교합니다:</p>
<pre><code>총 보상 = 회전 보상 + 목표 달성 보너스 + 접촉 보상
        - 나쁜 접촉 페널티 - 과도한 속도 페널티
        - 자세 페널티 - 에너지 소비 페널티</code></pre>
</section>
<section id="단계-student-정책-증류-실전-준비" class="level4">
<h4 class="anchored" data-anchor-id="단계-student-정책-증류-실전-준비">2단계: Student 정책 증류 (실전 준비)</h4>
<p>학생(Student)은 선생님이 하는 행동을 따라하면서 배웁니다. 하지만 중요한 차이가 있습니다:</p>
<p><strong>Student는 특권 정보가 없습니다!</strong></p>
<ul>
<li>물체의 정확한 위치? 모름</li>
<li>목표 자세? 모름</li>
<li>오직 손가락의 관절 각도와 촉각 정보만 사용</li>
</ul>
<p><strong>어떻게 가능할까?</strong></p>
<p>TCN(Temporal Convolutional Network)이라는 네트워크가 <strong>과거 30 타임스텝의 관찰</strong>을 받아서 압축된 표현(latent vector)으로 만듭니다. 이 표현에는 물체의 상태에 대한 암묵적인 정보가 담겨 있습니다.</p>
<pre><code>[30개의 과거 관찰] → TCN → [8차원 잠재 벡터] → 정책 → [행동]</code></pre>
</section>
</section>
<section id="핵심-아이디어-보조-목표auxiliary-goal" class="level3">
<h3 class="anchored" data-anchor-id="핵심-아이디어-보조-목표auxiliary-goal">핵심 아이디어: 보조 목표(Auxiliary Goal)</h3>
<p>이 부분이 정말 영리합니다! 기존 연구들은 “초당 30도씩 회전해!”라고 각속도를 직접 목표로 했습니다. 하지만 이건 학습이 정말 어렵습니다.</p>
<p><strong>AnyRotate의 접근:</strong></p>
<p>대신 “지금부터 1.5초 뒤에 이 자세가 되어야 해”라는 <strong>중간 목표</strong>를 계속 생성합니다. 목표에 도달하면 새로운 목표를 생성하고, 또 도달하면 또 생성하고… 이렇게 계속하면 자연스럽게 회전이 됩니다!</p>
<p>이게 왜 좋을까요?</p>
<ul>
<li>각 목표는 달성 가능한 수준</li>
<li>연속적인 회전이 자연스럽게 발생</li>
<li>학습이 훨씬 안정적</li>
</ul>
<p><strong>비유하자면:</strong></p>
<ul>
<li>기존 방식: “100m를 10초에 뛰어!” (너무 어려워…)</li>
<li>AnyRotate: “10m 앞으로 가! (달성) 또 10m 앞으로! (달성) 또 10m…!” (할 수 있어!)</li>
</ul>
</section>
<section id="적응형-커리큘럼-단계적-학습" class="level3">
<h3 class="anchored" data-anchor-id="적응형-커리큘럼-단계적-학습">적응형 커리큘럼: 단계적 학습</h3>
<p>사람도 걷기 전에 기기를 배우듯이, 로봇도 단계적으로 배워야 합니다.</p>
<p><strong>초기 단계:</strong></p>
<ul>
<li>보상: “일단 물체를 안정적으로 잡아!”</li>
<li>행동: 조심스럽게 물체를 파지</li>
<li>결과: 물체를 잘 잡지만 회전은 안 함</li>
</ul>
<p><strong>중간 단계:</strong></p>
<ul>
<li>보상의 가중치가 서서히 변화</li>
<li>회전 보상의 비중이 점점 증가</li>
<li>행동: 안정성을 유지하면서 조금씩 회전 시도</li>
</ul>
<p><strong>후기 단계:</strong></p>
<ul>
<li>보상: “이제 회전이 중요해!”</li>
<li>행동: 적극적으로 손가락을 움직여 회전</li>
<li>결과: 안정적이면서도 빠른 회전</li>
</ul>
<p>이 과정은 자동으로 진행됩니다. 로봇이 평균적으로 얼마나 회전을 달성했는지에 따라 커리큘럼 계수 α가 0에서 1로 증가합니다.</p>
<hr>
</section>
</section>
<section id="고밀도-촉각-표현-게임-체인저" class="level2">
<h2 class="anchored" data-anchor-id="고밀도-촉각-표현-게임-체인저">3. 고밀도 촉각 표현: 게임 체인저</h2>
<section id="기존-촉각-표현들" class="level3">
<h3 class="anchored" data-anchor-id="기존-촉각-표현들">기존 촉각 표현들</h3>
<p><strong>1. 이진 접촉 (Binary Touch)</strong></p>
<pre><code>손가락 1: 접촉 O
손가락 2: 접촉 X
손가락 3: 접촉 O
손가락 4: 접촉 O</code></pre>
<p>정보량: 4 bits</p>
<p><strong>2. 이산 촉각 (Discrete Touch)</strong></p>
<pre><code>손가락 1: 영역 5번에서 접촉
손가락 2: 접촉 없음
손가락 3: 영역 12번에서 접촉
손가락 4: 영역 3번에서 접촉</code></pre>
<p>정보량: 16 bits (16개 영역 중 하나)</p>
</section>
<section id="anyrotate의-고밀도-촉각-dense-touch" class="level3">
<h3 class="anchored" data-anchor-id="anyrotate의-고밀도-촉각-dense-touch">AnyRotate의 고밀도 촉각 (Dense Touch)</h3>
<pre><code>손가락 1:
  - 접촉 자세: θ=15.2°, φ=23.7°
  - 접촉 힘: 2.34 N
손가락 2: 접촉 없음
손가락 3:
  - 접촉 자세: θ=-8.1°, φ=45.3°
  - 접촉 힘: 1.87 N
...
</code></pre>
<p>정보량: <strong>연속적인 실수 값들</strong> (비교할 수 없을 만큼 풍부!)</p>
</section>
<section id="sim-to-real-전이-시뮬레이션에서-현실로" class="level3">
<h3 class="anchored" data-anchor-id="sim-to-real-전이-시뮬레이션에서-현실로">Sim-to-Real 전이: 시뮬레이션에서 현실로</h3>
<p>여기서 큰 문제가 있습니다. 시뮬레이션에서는 접촉 정보를 직접 알 수 있지만, 실제 로봇은 촉각 <strong>이미지</strong>를 받습니다.</p>
<p><strong>해결책: 관찰 모델(Observation Model)</strong></p>
<ol type="1">
<li><strong>데이터 수집</strong>: 실제 센서를 평평한 표면에 여러 각도와 힘으로 누르면서 3,000장의 이미지 수집</li>
<li><strong>CNN 학습</strong>: 촉각 이미지 → (접촉 자세, 접촉 힘) 예측하도록 학습</li>
<li><strong>배포</strong>: 학습된 모델로 실시간으로 촉각 특징 추출</li>
</ol>
<pre><code>[240×135 촉각 이미지]
    ↓ (CNN)
[θ, φ, Fx, Fy, Fz]
    ↓ (계산)
[접촉 자세, 접촉 힘]</code></pre>
<p>놀라운 점은, 이렇게 학습한 정책을 <strong>추가 학습 없이</strong> 실제 로봇에 바로 적용할 수 있다는 겁니다! (제로샷 전이)</p>
<hr>
</section>
</section>
<section id="실험-결과-숫자가-말해주는-성능" class="level2">
<h2 class="anchored" data-anchor-id="실험-결과-숫자가-말해주는-성능">4. 실험 결과: 숫자가 말해주는 성능</h2>
<section id="시뮬레이션-실험-촉각-정보의-중요성" class="level3">
<h3 class="anchored" data-anchor-id="시뮬레이션-실험-촉각-정보의-중요성">시뮬레이션 실험: 촉각 정보의 중요성</h3>
<p>첫 번째 실험은 “촉각 정보가 정말 중요한가?”를 확인합니다.</p>
<p><strong>테스트 환경:</strong></p>
<ul>
<li>학습 때 보지 못한 물체들</li>
<li>OOD Mass: 더 무거운 물체</li>
<li>OOD Shape: 다른 형상의 물체</li>
</ul>
<p><strong>결과 - OOD Mass (무거운 물체):</strong></p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>관찰 유형</th>
<th>회전 수</th>
<th>시간(초)</th>
<th>성능</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>고유수용감각만</td>
<td>0.56</td>
<td>6.9</td>
<td>⭐</td>
</tr>
<tr class="even">
<td>이진 촉각</td>
<td>1.03</td>
<td>11.1</td>
<td>⭐⭐</td>
</tr>
<tr class="odd">
<td>이산 촉각</td>
<td>1.26</td>
<td>13.1</td>
<td>⭐⭐⭐</td>
</tr>
<tr class="even">
<td>고밀도 촉각 (힘 제외)</td>
<td>1.55</td>
<td>15.4</td>
<td>⭐⭐⭐⭐</td>
</tr>
<tr class="odd">
<td>고밀도 촉각 (자세 제외)</td>
<td>1.35</td>
<td>13.8</td>
<td>⭐⭐⭐</td>
</tr>
<tr class="even">
<td><strong>고밀도 촉각 (전체)</strong></td>
<td><strong>1.77</strong></td>
<td><strong>17.4</strong></td>
<td>⭐⭐⭐⭐⭐</td>
</tr>
</tbody>
</table>
<p><strong>결과 - OOD Shape (다른 형상):</strong></p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>관찰 유형</th>
<th>회전 수</th>
<th>시간(초)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>고유수용감각만</td>
<td>0.84</td>
<td>10.4</td>
</tr>
<tr class="even">
<td>이진 촉각</td>
<td>1.35</td>
<td>14.9</td>
</tr>
<tr class="odd">
<td>이산 촉각</td>
<td>1.66</td>
<td>17.7</td>
</tr>
<tr class="even">
<td><strong>고밀도 촉각 (전체)</strong></td>
<td><strong>2.35</strong></td>
<td><strong>23.3</strong></td>
</tr>
</tbody>
</table>
<p><strong>분석:</strong></p>
<ol type="1">
<li><strong>촉각이 있으면 훨씬 좋다</strong>: 고유수용감각만 쓰는 것보다 이진 촉각도 큰 도움</li>
<li><strong>상세할수록 좋다</strong>: 촉각 정보가 상세할수록 성능이 계속 향상</li>
<li><strong>자세와 힘 모두 중요</strong>: 둘 중 하나를 빼면 성능 하락
<ul>
<li>힘 정보: 무거운 물체를 다룰 때 특히 중요</li>
<li>자세 정보: 정밀한 손가락 움직임에 중요</li>
</ul></li>
<li><strong>일반화 능력</strong>: 학습 때 못 본 물체에도 잘 작동</li>
</ol>
</section>
<section id="보조-목표의-효과" class="level3">
<h3 class="anchored" data-anchor-id="보조-목표의-효과">보조 목표의 효과</h3>
<p>“보조 목표가 정말 필요한가?” 테스트:</p>
<p><strong>비교 대상:</strong></p>
<ul>
<li>제안 방법: 보조 목표 + 적응형 커리큘럼</li>
<li>w/o 보조 목표: 각속도 직접 제어</li>
<li>w/o 커리큘럼: 보조 목표는 있지만 고정된 보상</li>
</ul>
<p><strong>결과:</strong></p>
<pre><code>제안 방법:       ████████████████████ (20회 연속 목표 달성)
w/o 커리큘럼:    ██                    (물체만 잡고 회전 안 함)
w/o 보조 목표:   ████                  (단일 축만 가능, 다축 실패)</code></pre>
<p><strong>결론:</strong> 보조 목표와 적응형 커리큘럼 <strong>둘 다 필수</strong>입니다!</p>
</section>
<section id="실제-로봇-실험-진짜-세계에서의-도전" class="level3">
<h3 class="anchored" data-anchor-id="실제-로봇-실험-진짜-세계에서의-도전">실제 로봇 실험: 진짜 세계에서의 도전</h3>
<p>이제 진짜 로봇으로 실험합니다. 10가지 다양한 물체를 사용했는데, 플라스틱 과일부터 금속 실린더, 고무 장난감까지 정말 다양합니다.</p>
<section id="손-방향에-따른-성능" class="level4">
<h4 class="anchored" data-anchor-id="손-방향에-따른-성능">손 방향에 따른 성능</h4>
<p>손을 여러 방향으로 돌려가며 테스트했습니다:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>방향</th>
<th>설명</th>
<th>난이도</th>
<th>고밀도 촉각 성능</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Palm Up</td>
<td>손바닥 위</td>
<td>⭐ 쉬움</td>
<td>6.2회/24.7초</td>
</tr>
<tr class="even">
<td>Palm Down</td>
<td>손바닥 아래</td>
<td>⭐⭐ 중간</td>
<td>2.8회/18.3초</td>
</tr>
<tr class="odd">
<td>Base Up</td>
<td>손목 위</td>
<td>⭐⭐ 중간</td>
<td>3.5회/20.7초</td>
</tr>
<tr class="even">
<td>Base Down</td>
<td>손목 아래</td>
<td>⭐⭐⭐ 어려움</td>
<td>2.0회/16.3초</td>
</tr>
<tr class="odd">
<td>Thumb Up</td>
<td>엄지 위</td>
<td>⭐⭐⭐⭐ 매우 어려움</td>
<td>1.5회/14.7초</td>
</tr>
<tr class="even">
<td>Thumb Down</td>
<td>엄지 아래</td>
<td>⭐⭐⭐⭐⭐ 극악</td>
<td>1.2회/13.3초</td>
</tr>
</tbody>
</table>
<p><strong>왜 이렇게 차이가 날까?</strong></p>
<ul>
<li><strong>Palm Up/Down</strong>: 중력이 파지를 도와줌 (또는 적당히 방해)</li>
<li><strong>Thumb Up/Down</strong>: 손가락이 수평이라 중력이 구동력에 정면으로 반대
<ul>
<li>손가락이 물체 무게를 온전히 버텨야 함</li>
<li>Allegro Hand의 구동력이 이 방향에서 약해짐</li>
</ul></li>
</ul>
<p><strong>놀라운 점:</strong></p>
<p>어려운 방향에서도 고밀도 촉각을 사용한 정책은 작동합니다! 고유수용감각이나 이진 촉각으로는 거의 불가능한 수준입니다.</p>
</section>
<section id="회전축에-따른-성능" class="level4">
<h4 class="anchored" data-anchor-id="회전축에-따른-성능">회전축에 따른 성능</h4>
<p>x, y, z 세 축으로 회전을 시도했습니다:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>회전축</th>
<th>특징</th>
<th>고밀도 촉각 성능</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>z축</td>
<td>물체의 주축, 가장 자연스러움</td>
<td>4.2회/22.3초</td>
</tr>
<tr class="even">
<td>x축</td>
<td>손가락 2개는 고정, 2개는 회전</td>
<td>5.5회/25.3초</td>
</tr>
<tr class="odd">
<td>y축</td>
<td>가장 복잡한 손가락 협응 필요</td>
<td>3.8회/21.7초</td>
</tr>
</tbody>
</table>
<p><strong>흥미로운 발견:</strong></p>
<p>y축과 x축 회전은 <strong>정교한 손가락 게이팅</strong>이 필요합니다: - 중지와 엄지(또는 검지와 새끼)가 물체를 고정 - 나머지 두 손가락이 회전력 제공 - 이런 복잡한 협응은 촉각 정보 없이는 거의 불가능</p>
<p>이진 촉각도 z축에서는 고유수용감각과 비슷한 성능이지만, x축과 y축에서는 확실히 더 좋습니다.</p>
</section>
</section>
<section id="촉각-센서-분석-로봇이-느끼는-것" class="level3">
<h3 class="anchored" data-anchor-id="촉각-센서-분석-로봇이-느끼는-것">촉각 센서 분석: 로봇이 ‘느끼는’ 것</h3>
<p>실험 중 촉각 센서가 측정하는 값들을 분석했습니다. 그래프를 보면 두 가지 핵심 패턴이 보입니다:</p>
<p><strong>1. 미끄럼 감지</strong></p>
<pre><code>시간 0초: 접촉 힘 = 2.5N, 접촉 자세 = 15°
시간 1초: 접촉 힘 = 2.3N, 접촉 자세 = 15° ← 안정
시간 2초: 접촉 힘 = 1.8N, 접촉 자세 = 18° ← 미끄러짐!</code></pre>
<p>힘이 줄고 각도가 변하면 물체가 미끄러지는 중입니다.</p>
<p><strong>2. 반응적 게이팅</strong></p>
<p>미끄럼을 감지하면 정책이 즉시 반응합니다: - 미끄러지는 손가락: 힘을 증가 - 다른 손가락들: 보상 동작 수행 - 결과: 물체를 다시 안정적으로 잡음</p>
<p><strong>놀라운 점:</strong></p>
<p>명시적인 “미끄럼 감지 모듈”이 없습니다! 고밀도 촉각 정보만으로도 정책이 <strong>암묵적으로</strong> 미끄럼을 감지하고 대응하는 법을 학습했습니다. 이는 이진 촉각이나 고유수용감각으로는 관찰되지 않은 창발적 행동입니다.</p>
</section>
<section id="궁극의-테스트-회전하는-손" class="level3">
<h3 class="anchored" data-anchor-id="궁극의-테스트-회전하는-손">궁극의 테스트: 회전하는 손</h3>
<p>가장 인상적인 실험입니다. 손 자체를 계속 회전시키면서 물체를 조작합니다!</p>
<ul>
<li>손이 앞뒤로 흔들리면서</li>
<li>동시에 좌우로 회전하면서</li>
<li>그 와중에 손안의 공을 계속 돌림</li>
</ul>
<p>사람도 어려운 이 동작을, 로봇이 해냅니다. 중력 방향이 손의 좌표계에서 계속 변하는데도 말이죠!</p>
<p><strong>세 가지 궤적 테스트:</strong></p>
<ol type="1">
<li><strong>단순 회전</strong>: 손이 한 축으로 계속 회전 → 성공!</li>
<li><strong>복잡한 3D 궤적</strong>: 손이 여러 축으로 복합 회전 → 성공!</li>
<li><strong>서보잉</strong>: 손은 계속 움직이지만 물체는 공중에 거의 정지
<ul>
<li>마치 저글링처럼 손이 물체 주위를 이동</li>
<li>6DoF 재배향 + 파지 위치 재배치 동시 수행</li>
<li>픽앤플레이스 같은 작업에 유용할 듯!</li>
</ul></li>
</ol>
<hr>
</section>
</section>
<section id="기술적-깊이-파고들기" class="level2">
<h2 class="anchored" data-anchor-id="기술적-깊이-파고들기">5. 기술적 깊이 파고들기</h2>
<p>여기서는 좀 더 기술적인 세부사항을 살펴보겠습니다. 엔지니어링 측면에서 정말 잘 설계되었거든요.</p>
<section id="보상-함수-해부" class="level3">
<h3 class="anchored" data-anchor-id="보상-함수-해부">보상 함수 해부</h3>
<p>보상 함수는 로봇이 “무엇을 배워야 하는지” 정의합니다. AnyRotate의 보상 함수는 10개의 항으로 구성됩니다:</p>
<section id="긍정적-보상-로봇이-이렇게-하면-좋아" class="level4">
<h4 class="anchored" data-anchor-id="긍정적-보상-로봇이-이렇게-하면-좋아">긍정적 보상 (로봇이 이렇게 하면 좋아!)</h4>
<p><strong>1. 회전 보상 (r_rot)</strong></p>
<div class="sourceCode" id="cb10"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1"></a>r_rot <span class="op">=</span> exp(<span class="op">-</span>β <span class="op">*</span> d_keypoint)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li>물체의 키포인트가 목표에 가까울수록 높은 보상</li>
<li>키포인트: 물체에서 5cm 떨어진 6개 점</li>
<li>β = 2.0 (가중치)</li>
</ul>
<p><strong>2. 목표 달성 보너스 (r_bonus)</strong></p>
<div class="sourceCode" id="cb11"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1"></a>r_bonus <span class="op">=</span> <span class="fl">10.0</span>  <span class="co"># 목표 도달 시</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li>키포인트 거리가 임계값(0.15) 미만이면 발동</li>
<li>희소 보상으로 중요한 이벤트 강조</li>
</ul>
<p><strong>3. 델타 회전 보상 (r_delta)</strong></p>
<div class="sourceCode" id="cb12"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1"></a>r_delta <span class="op">=</span> Δθ <span class="op">/</span> Δt  <span class="co"># 실제 회전량</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li>목표 축에 대한 실제 각도 변화</li>
<li>연속적인 회전 장려</li>
</ul>
<p><strong>4. 좋은 접촉 보상 (r_contact)</strong></p>
<div class="sourceCode" id="cb13"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1"></a>r_contact <span class="op">=</span> <span class="fl">1.0</span> <span class="cf">if</span> n_tip_contacts <span class="op">&gt;=</span> <span class="dv">2</span> <span class="cf">else</span> <span class="dv">0</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li>손가락 끝 접촉이 2개 이상이면 보상</li>
<li>안정적인 파지 유도</li>
</ul>
</section>
<section id="부정적-보상-로봇이-이러면-안-돼" class="level4">
<h4 class="anchored" data-anchor-id="부정적-보상-로봇이-이러면-안-돼">부정적 보상 (로봇이 이러면 안 돼!)</h4>
<p><strong>5. 나쁜 접촉 페널티 (p_bad_contact)</strong></p>
<ul>
<li>손바닥이나 손가락 옆면 접촉 시 페널티</li>
<li>손가락 끝만 사용하도록 유도</li>
</ul>
<p><strong>6. 각속도 페널티 (p_ang_vel)</strong></p>
<div class="sourceCode" id="cb14"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1"></a>p_ang_vel <span class="op">=</span> <span class="op">-</span><span class="fl">1.0</span> <span class="cf">if</span> <span class="op">|</span>ω<span class="op">|</span> <span class="op">&gt;</span> <span class="dv">30</span> rad<span class="op">/</span>s <span class="cf">else</span> <span class="dv">0</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li>너무 빠른 회전 방지</li>
<li>제어 가능한 속도 유지</li>
</ul>
<p><strong>7. 자세 페널티 (p_pose)</strong></p>
<ul>
<li>표준 파지 자세에서 너무 멀어지면 페널티</li>
<li>극단적인 관절 각도 방지</li>
</ul>
<p><strong>8. 일 페널티 (p_work)</strong></p>
<ul>
<li>에너지 소비 최소화</li>
<li>효율적인 움직임 유도</li>
</ul>
<p><strong>9. 토크 페널티 (p_torque)</strong></p>
<ul>
<li>높은 토크 사용 억제</li>
<li>하드웨어 부담 감소</li>
</ul>
<p><strong>10. 종료 페널티 (p_term)</strong></p>
<div class="sourceCode" id="cb15"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1"></a>p_term <span class="op">=</span> <span class="op">-</span><span class="dv">100</span>  <span class="co"># 물체 낙하 또는 축 이탈 시</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li>실패에 대한 강력한 불이익</li>
<li>안정성 최우선</li>
</ul>
<p><strong>최종 보상 함수:</strong></p>
<pre><code>r_total = r_rot + r_bonus + r_delta + r_contact
        - p_bad_contact - p_ang_vel - p_pose
        - p_work - p_torque - p_term</code></pre>
<p>각 항의 가중치는 실험을 통해 세심하게 조정되었습니다.</p>
</section>
</section>
<section id="네트워크-아키텍처-상세" class="level3">
<h3 class="anchored" data-anchor-id="네트워크-아키텍처-상세">네트워크 아키텍처 상세</h3>
<section id="teacher-policy" class="level4">
<h4 class="anchored" data-anchor-id="teacher-policy">Teacher Policy</h4>
<pre><code>[특권 정보 18차원]
    ↓
MLP Encoder [256 → 128 → 8]
    ↓
[잠재 벡터 8차원] + [고유수용+촉각 정보]
    ↓
Policy Network [512 → 256 → 128]
    ↓
[평균 μ, 표준편차 σ] (Gaussian policy)
    ↓
샘플링 → [행동 16차원]</code></pre>
<p><strong>활성화 함수:</strong> - MLP: ReLU - Policy: ELU (Exponential Linear Unit) - 음수 영역에서도 부드러운 그래디언트 - 죽은 뉴런(dead neuron) 문제 해결</p>
</section>
<section id="student-policy" class="level4">
<h4 class="anchored" data-anchor-id="student-policy">Student Policy</h4>
<pre><code>[30 타임스텝 × 관찰 차원]
    ↓
TCN Layer 1: Conv1D (kernel=9, stride=2) + ReLU
    ↓
TCN Layer 2: Conv1D (kernel=5, stride=1) + ReLU
    ↓
TCN Layer 3: Conv1D (kernel=5, stride=1) + ReLU
    ↓
[잠재 벡터 8차원]
    ↓
Policy Network [512 → 256 → 128] (Teacher와 동일)
    ↓
[행동]</code></pre>
<p><strong>TCN의 장점:</strong></p>
<ul>
<li>시간적 패턴 포착 (과거 30 프레임 = 1.5초)</li>
<li>RNN보다 병렬화 효율적</li>
<li>긴 시퀀스에서도 안정적</li>
</ul>
<p><strong>관찰 차원:</strong></p>
<ul>
<li>고유수용감각만: 79차원</li>
<li>이진 촉각: 83차원 (+4)</li>
<li>고밀도 촉각: 91차원 (+12)</li>
</ul>
</section>
</section>
<section id="시스템-식별-시뮬레이션을-현실과-맞추기" class="level3">
<h3 class="anchored" data-anchor-id="시스템-식별-시뮬레이션을-현실과-맞추기">시스템 식별: 시뮬레이션을 현실과 맞추기</h3>
<p>시뮬레이션이 아무리 좋아도 현실과 차이가 있습니다. 이를 줄이기 위해 <strong>시스템 식별</strong>을 수행합니다.</p>
<p><strong>최적화 대상:</strong> - 16개 관절 × 5개 파라미터 = <strong>80개 파라미터</strong> - 강성 (stiffness) - 감쇠 (damping) - 질량 (mass) - 마찰 (friction) - armature</p>
<p><strong>방법:</strong></p>
<ol type="1">
<li>실제 로봇으로 여러 방향에서 궤적 기록</li>
<li>같은 명령을 시뮬레이션에서 실행</li>
<li>CMA-ES 알고리즘으로 MSE 최소화</li>
<li>최적 파라미터 발견</li>
</ol>
<p><strong>결과:</strong></p>
<p>시뮬레이션과 실제 로봇의 움직임이 훨씬 유사해져서 sim-to-real 격차 감소!</p>
</section>
<section id="도메인-무작위화-다양성이-답이다" class="level3">
<h3 class="anchored" data-anchor-id="도메인-무작위화-다양성이-답이다">도메인 무작위화: 다양성이 답이다</h3>
<p>시뮬레이션 학습 중에 파라미터를 계속 변경합니다:</p>
<p><strong>물체 무작위화:</strong></p>
<div class="sourceCode" id="cb19"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1"></a>mass <span class="op">=</span> random.uniform(<span class="fl">0.025</span>, <span class="fl">0.20</span>)  <span class="co"># kg</span></span>
<span id="cb19-2"><a href="#cb19-2"></a>friction <span class="op">=</span> <span class="fl">10.0</span>  <span class="co"># 고정</span></span>
<span id="cb19-3"><a href="#cb19-3"></a>radius <span class="op">=</span> random.uniform(<span class="fl">0.025</span>, <span class="fl">0.034</span>)  <span class="co"># m</span></span>
<span id="cb19-4"><a href="#cb19-4"></a>center_of_mass <span class="op">=</span> random.uniform(<span class="op">-</span><span class="fl">0.01</span>, <span class="fl">0.01</span>)  <span class="co"># m</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>손 무작위화:</strong></p>
<div class="sourceCode" id="cb20"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1"></a>stiffness <span class="op">=</span> random.uniform(<span class="dv">35</span>, <span class="dv">45</span>)</span>
<span id="cb20-2"><a href="#cb20-2"></a>damping <span class="op">=</span> random.uniform(<span class="fl">0.5</span>, <span class="fl">2.5</span>)</span>
<span id="cb20-3"><a href="#cb20-3"></a>joint_noise <span class="op">=</span> gaussian(<span class="dv">0</span>, <span class="fl">0.03</span>)</span>
<span id="cb20-4"><a href="#cb20-4"></a>position_noise <span class="op">=</span> gaussian(<span class="dv">0</span>, <span class="fl">0.005</span>)  <span class="co"># m</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>촉각 무작위화:</strong></p>
<div class="sourceCode" id="cb21"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1"></a>pose_noise <span class="op">=</span> gaussian(<span class="dv">0</span>, <span class="fl">0.0174</span>)  <span class="co"># rad (약 1도)</span></span>
<span id="cb21-2"><a href="#cb21-2"></a>force_noise <span class="op">=</span> gaussian(<span class="dv">0</span>, <span class="fl">0.1</span>)  <span class="co"># N</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>외란:</strong></p>
<ul>
<li>25% 확률로 랜덤 힘 적용</li>
<li>외란 스케일: 2.0</li>
<li>지수 감쇠: 0.99</li>
</ul>
<p>이렇게 다양한 조건에서 학습하면 실제 세계의 예측 불가능한 상황에 강건해집니다!</p>
</section>
<section id="실시간-제어-파이프라인" class="level3">
<h3 class="anchored" data-anchor-id="실시간-제어-파이프라인">실시간 제어 파이프라인</h3>
<p>실제 로봇에서의 제어는 20Hz로 작동합니다:</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart TD
    Start(["20 Hz 메인 루프 시작"]) --&gt; Step1["1. 촉각 이미지 수집&lt;br/&gt;비동기, 최대 30 FPS"]
    Step1 --&gt; Step2["2. 그레이스케일 변환 + 전처리"]
    Step2 --&gt; Step3["3. CNN으로 특징 추출&lt;br/&gt;4개 센서 병렬"]
    Step3 --&gt; Step3a["접촉 자세: θ, φ&lt;br/&gt;접촉 힘: |F|"]
    Step3a --&gt; Step4["4. 관절 위치 읽기"]
    Step4 --&gt; Step5["5. Forward Kinematics"]
    Step5 --&gt; Step5a["손가락 끝 위치&lt;br/&gt;손가락 끝 방향"]
    Step5a --&gt; Step6["6. 관찰 벡터 구성"]
    Step6 --&gt; Step7["7. 정책 추론"]
    Step7 --&gt; Step7a["TCN → 잠재 벡터&lt;br/&gt;Policy → 행동 출력"]
    Step7a --&gt; Step8["8. 지수 이동 평균&lt;br/&gt;q_target = 0.9 × q_old + 0.1 × q_new"]
    Step8 --&gt; Step9["9. 목표 관절 위치 전송"]
    Step9 --&gt; PD["PD 제어기&lt;br/&gt;300 Hz"]
    PD --&gt; Motor["모터 구동"]
    Motor --&gt; Start

    style Start fill:#e1f5ff
    style Step3 fill:#fff4e1
    style Step7 fill:#ffe1f5
    style PD fill:#e1ffe1
    style Motor fill:#ffe1e1

</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p><strong>병목 지점:</strong></p>
<ul>
<li>CNN 추론: ~5ms (GPU 사용)</li>
<li>정책 추론: ~3ms</li>
<li>나머지: ~42ms (여유 있음)</li>
</ul>
<p><strong>최적화:</strong></p>
<ul>
<li>촉각 센서들이 비동기로 스트리밍 (대기 시간 감소)</li>
<li>가장 최근 이미지 사용 (지연 최소화)</li>
<li>SSIM으로 빠른 접촉 감지</li>
</ul>
<hr>
</section>
</section>
<section id="비교-분석-기존-연구와-어떻게-다른가" class="level2">
<h2 class="anchored" data-anchor-id="비교-분석-기존-연구와-어떻게-다른가">6. 비교 분석: 기존 연구와 어떻게 다른가?</h2>
<section id="openai의-solving-rubiks-cube-with-a-robot-hand-2019와-비교" class="level3">
<h3 class="anchored" data-anchor-id="openai의-solving-rubiks-cube-with-a-robot-hand-2019와-비교">OpenAI의 “Solving Rubik’s Cube with a Robot Hand” (2019)와 비교</h3>
<p>2019년 OpenAI가 발표한 “Solving Rubik’s Cube with a Robot Hand” (Akkaya et al., 2019)와 후속 연구 “Learning Dexterous In-Hand Manipulation” (Andrychowicz et al., 2020)은 로봇 조작 분야에 큰 반향을 일으켰습니다.</p>
<p><strong>OpenAI가 보여준 것:</strong></p>
<p>이 프로젝트는 Shadow Hand라는 24자유도 로봇 손으로 루빅스 큐브를 푸는 놀라운 성과를 달성했습니다:</p>
<ul>
<li><strong>하드웨어</strong>: Shadow Dexterous Hand (24 DoF, 20개 구동 관절)</li>
<li><strong>센싱</strong>: RGB 카메라 3개 (손을 둘러싼 위치에 배치)</li>
<li><strong>학습 방법</strong>:
<ul>
<li>ADR (Automatic Domain Randomization) - 자동으로 시뮬레이션 난이도 증가</li>
<li>Vision-based state estimation</li>
<li>대규모 분산 학습 (6,144 CPU 코어 + 8 V100 GPU)</li>
</ul></li>
<li><strong>성과</strong>: 루빅스 큐브를 60% 성공률로 해결 (심지어 고무 장갑 착용 상태에서도)</li>
</ul>
<p><strong>하지만 한계가 명확했습니다:</strong></p>
<ol type="1">
<li><strong>과도한 비전 의존성</strong>
<ul>
<li>16-30개의 카메라 포인트 필요</li>
<li>자기 폐색(self-occlusion) 문제 해결을 위한 복잡한 설정</li>
<li>실험실 환경 밖에서는 적용 어려움</li>
</ul></li>
<li><strong>고정된 손 방향</strong>
<ul>
<li>손은 항상 팔 위 (palm up)</li>
<li>중력이 파지를 도와주는 상황</li>
<li>다른 방향에서의 조작은 시도되지 않음</li>
</ul></li>
<li><strong>작업 특화</strong>
<ul>
<li>루빅스 큐브라는 특정 물체에 최적화</li>
<li>다른 물체로 일반화 검증 안 됨</li>
<li>큐브의 특수한 기하학적 특성 활용</li>
</ul></li>
<li><strong>막대한 컴퓨팅 자원</strong>
<ul>
<li>총 학습 시간: 100년 상당의 시뮬레이션</li>
<li>13,000시간의 실제 시간</li>
<li>일반 연구실에서 재현 어려움</li>
</ul></li>
</ol>
<p><strong>AnyRotate의 근본적 차이점:</strong></p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>측면</th>
<th>OpenAI (2019-2020)</th>
<th>AnyRotate (2024)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>주요 센싱</strong></td>
<td>비전 (다중 카메라)</td>
<td>촉각 (4개 센서)</td>
</tr>
<tr class="even">
<td><strong>자기 폐색</strong></td>
<td>문제가 됨</td>
<td>문제 없음</td>
</tr>
<tr class="odd">
<td><strong>손 방향</strong></td>
<td>고정 (palm up)</td>
<td>임의 (6+ 방향)</td>
</tr>
<tr class="even">
<td><strong>중력 대응</strong></td>
<td>중력이 도움</td>
<td>중력에 불변</td>
</tr>
<tr class="odd">
<td><strong>회전축</strong></td>
<td>작업 특화</td>
<td>임의 축 통합 정책</td>
</tr>
<tr class="even">
<td><strong>물체 일반화</strong></td>
<td>루빅스 큐브</td>
<td>10+ 다양한 물체</td>
</tr>
<tr class="odd">
<td><strong>설치 복잡도</strong></td>
<td>매우 높음</td>
<td>상대적으로 낮음</td>
</tr>
<tr class="even">
<td><strong>컴퓨팅 자원</strong></td>
<td>6,144 CPU + 8 GPU</td>
<td>GPU 시뮬레이터 (표준)</td>
</tr>
<tr class="odd">
<td><strong>배포 환경</strong></td>
<td>실험실 전용</td>
<td>실용적 배포 가능</td>
</tr>
</tbody>
</table>
<p><strong>철학적 차이:</strong></p>
<p>OpenAI의 접근은 “특정 작업에서 초인적 성능”을 목표로 했습니다. 루빅스 큐브를 푸는 것은 인상적이지만, 그 시스템을 다른 작업에 적용하기는 어렵습니다.</p>
<p>반면 AnyRotate는 “일반화 가능한 조작 능력”을 추구합니다. 처음 보는 물체도 다룰 수 있고, 어떤 손 방향에서도 작동하며, 복잡한 외부 센서 없이도 가능합니다.</p>
<p><strong>둘 중 무엇이 더 나은가?</strong></p>
<p>사실 이것은 “무엇이 더 나은가?”보다는 “무엇을 목표로 하는가?”의 문제입니다:</p>
<ul>
<li><strong>OpenAI</strong>: “이것 봐! 로봇이 루빅스 큐브를 풀 수 있어!” → 기술의 한계를 보여주는 데모</li>
<li><strong>AnyRotate</strong>: “로봇이 다양한 물체를 실용적으로 다룰 수 있어” → 실제 응용을 향한 발걸음</li>
</ul>
<p>두 접근 모두 중요하고, 서로 보완적입니다.</p>
</section>
<section id="최근-촉각-기반-연구들과-비교" class="level3">
<h3 class="anchored" data-anchor-id="최근-촉각-기반-연구들과-비교">최근 촉각 기반 연구들과 비교</h3>
<p><strong>Qi et al.&nbsp;(2023) - “General In-Hand Object Rotation with Vision and Touch” (RotateIt)</strong></p>
<p>UC Berkeley의 Jitendra Malik 교수 연구팀과 Meta AI가 공동으로 개발한 시스템으로, CoRL 2023에 발표되었습니다.</p>
<p><strong>연구 배경과 동기:</strong></p>
<p>Haozhi Qi를 비롯한 연구팀은 “시각과 촉각을 함께 사용하면 더 나을까?”라는 질문에서 출발했습니다. 인간도 눈과 손을 함께 사용하니까요. 그들의 답은 RotateIt 시스템이었습니다.</p>
<p><strong>시스템 구성:</strong></p>
<ol type="1">
<li><p><strong>하드웨어</strong></p>
<ul>
<li>Allegro Hand (AnyRotate와 동일한 4-finger hand)</li>
<li>DIGIT 촉각 센서 (Meta에서 개발, 각 손가락에 부착)</li>
<li>외부 RGB-D 카메라 (손 위에 설치)</li>
</ul></li>
<li><p><strong>센싱 방식</strong></p>
<ul>
<li><strong>비전</strong>: 외부 카메라로 물체의 전체 형상과 깊이 정보 관찰</li>
<li><strong>촉각</strong>: DIGIT 센서로 고해상도 접촉 정보 수집
<ul>
<li>촉각 이미지를 16개 이산 영역으로 표현</li>
<li>주로 접촉 위치에 집중</li>
</ul></li>
<li><strong>고유수용감각</strong>: 관절 각도와 속도</li>
</ul></li>
<li><p><strong>핵심 기술: Visuotactile Transformer</strong></p>
<pre><code>[RGB-D 이미지] → Vision Encoder (ResNet)
      ↓
[Transformer Fusion]  ← [4개 DIGIT 이미지] → Tactile Encoder
      ↓
[융합된 표현] → 물체 형상 + 물리 속성 추론
      ↓
[정책 네트워크] → 회전 행동 출력</code></pre>
<p>Transformer가 cross-attention으로 시각과 촉각을 융합합니다:</p>
<ul>
<li>카메라가 물체의 전체 형상을 보면</li>
<li>촉각이 접촉 지점의 세밀한 정보를 제공하고</li>
<li>“아, 이 물체는 이런 모양이고 이 무게라서 이렇게 잡아야겠다”</li>
</ul></li>
</ol>
<p><strong>학습 과정:</strong></p>
<p>AnyRotate와 유사하게 Teacher-Student 구조 사용:</p>
<ul>
<li><strong>Oracle Teacher</strong>: GT 형상과 물리 속성 알고 있음</li>
<li><strong>Visuotactile Student</strong>: 노이즈가 있는 시각-촉각만으로 작동</li>
<li><strong>특별한 점</strong>: latent space에서 물체의 3D 형상을 재구성 가능!</li>
</ul>
<p><strong>성능:</strong></p>
<p>시뮬레이션:</p>
<ul>
<li>X, Y, Z축 각각 80% 이상 성공률</li>
<li>비전만 vs 촉각만 vs 둘 다 → 둘 다가 최고</li>
</ul>
<p>실제 로봇:</p>
<ul>
<li>다양한 일상 물체(머그컵, 망치, 플라스틱 병 등)</li>
<li>처음 보는 물체에도 잘 일반화</li>
</ul>
<p><strong>AnyRotate와의 상세 비교:</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 12%">
<col style="width: 52%">
<col style="width: 36%">
</colgroup>
<thead>
<tr class="header">
<th>차원</th>
<th>RotateIt (Qi et al.&nbsp;2023)</th>
<th>AnyRotate (2024)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>촉각 표현</strong></td>
<td>16개 이산 영역 + 원본 이미지</td>
<td>연속적 자세(θ,φ) + 힘(|F|)</td>
</tr>
<tr class="even">
<td><strong>정보 밀도</strong></td>
<td>중간 (이산화로 일부 손실)</td>
<td>높음 (연속 실수 값)</td>
</tr>
<tr class="odd">
<td><strong>비전 필요성</strong></td>
<td>필수 (외부 카메라)</td>
<td>불필요 (순수 촉각)</td>
</tr>
<tr class="even">
<td><strong>설치 복잡도</strong></td>
<td>높음 (카메라 캘리브레이션)</td>
<td>낮음 (센서만)</td>
</tr>
<tr class="odd">
<td><strong>중력 불변성</strong></td>
<td>X (손바닥 위 고정)</td>
<td>O (6+ 방향)</td>
</tr>
<tr class="even">
<td><strong>회전축 제어</strong></td>
<td>다축 가능 (개별 학습?)</td>
<td>통합 정책으로 임의 축</td>
</tr>
<tr class="odd">
<td><strong>미끄럼 대응</strong></td>
<td>명시적 모듈 필요</td>
<td>암묵적 학습 (창발)</td>
</tr>
<tr class="even">
<td><strong>물체 이해</strong></td>
<td>3D 형상 재구성 O</td>
<td>직접 재구성 X</td>
</tr>
<tr class="odd">
<td><strong>센서</strong></td>
<td>DIGIT (Meta)</td>
<td>DigiTac (Bristol)</td>
</tr>
<tr class="even">
<td><strong>주요 기여</strong></td>
<td>멀티모달 융합</td>
<td>중력 불변 + 고밀도 촉각</td>
</tr>
</tbody>
</table>
<p><strong>RotateIt의 강점:</strong></p>
<ol type="1">
<li><strong>형상 이해</strong>:
<ul>
<li>RGB-D로 물체 전체 형상 파악</li>
<li>특히 처음 보는 복잡한 물체에 유리</li>
<li>Learned representation으로 3D shape 재구성 가능</li>
</ul></li>
<li><strong>Transformer 융합</strong>:
<ul>
<li>Attention으로 시각-촉각 상호보완</li>
<li>중요한 영역에 자동으로 집중</li>
</ul></li>
<li><strong>멀티모달 프레임워크</strong>:
<ul>
<li>향후 연구를 위한 좋은 방법론</li>
<li>다른 센서 추가도 가능한 확장성</li>
</ul></li>
</ol>
<p><strong>RotateIt의 한계:</strong></p>
<ol type="1">
<li><strong>외부 센서 의존</strong>:
<ul>
<li>카메라 설치와 캘리브레이션 필수</li>
<li>실험실 밖 적용 어려움</li>
<li>조명 변화에 민감</li>
</ul></li>
<li><strong>고정된 손 방향</strong>:
<ul>
<li>여전히 palm up만</li>
<li>중력 불변성 미달성</li>
</ul></li>
<li><strong>일부 자기 폐색</strong>:
<ul>
<li>손이 물체를 가림</li>
<li>완전한 해결은 아님</li>
</ul></li>
</ol>
<p><strong>결론:</strong></p>
<p>RotateIt는 “멀티모달”이 답이라고 주장합니다. 시각+촉각 &gt; 각각의 합.</p>
<p>AnyRotate는 “고밀도 촉각만으로도 충분”하다고 보여줍니다. 그것도 임의 방향에서!</p>
<p>실용적 선택:</p>
<ul>
<li>카메라 사용 가능 → RotateIt (더 많은 정보)</li>
<li>카메라 어려움 → AnyRotate (더 강건하고 단순)</li>
</ul>
<hr>
<p><strong>Khandate et al.&nbsp;(2022-2023) - 손가락 게이팅 학습 시리즈</strong></p>
<p>Columbia University의 Matei Ciocarlie 교수 연구실에서 진행한 연구입니다. Gagan Khandate 박사과정 학생이 주도했습니다.</p>
<p><strong>연구의 핵심 주제: 손가락 게이팅(Finger Gaiting)</strong></p>
<p>손가락 게이팅이란, 물체를 잡고 있는 손가락을 바꿔가며 조작하는 기술입니다. 마치 암벽등반처럼 손과 발을 번갈아 움직이는 것과 비슷하죠.</p>
<p>예를 들어:</p>
<ol type="1">
<li>검지+중지로 물체 고정</li>
<li>엄지+약지가 움직여서 물체 회전</li>
<li>이제 엄지+약지로 고정</li>
<li>검지+중지가 움직여서 더 회전</li>
<li>반복…</li>
</ol>
<hr>
<p><strong>2022년 연구: “On the Feasibility of Learning Finger-gaiting In-hand Manipulation with Intrinsic Sensing” (ICRA 2022)</strong></p>
<p>“정말 강화학습만으로 이런 복잡한 기술을 배울 수 있을까?”</p>
<p><strong>시스템 구성:</strong></p>
<ul>
<li><strong>로봇 손</strong>: 커스텀 제작 5-finger hand
<ul>
<li>각 손가락: 1 roll joint + 2 flexion joints = 3 DoF</li>
<li>총 15 DoF (5 fingers × 3 joints)</li>
<li>모두 Dynamixel XM430 서보모터로 구동</li>
<li>위치 제어 모드</li>
</ul></li>
<li><strong>센싱</strong>:
<ul>
<li>각 손가락 끝에 광학 기반 촉각 센서</li>
<li>유연한 반사막 + LED + 포토다이오드 배열</li>
<li>막이 눌리면 빛 반사 패턴 변화</li>
<li><strong>출력</strong>: 이진 신호 (닿음 1 / 안 닿음 0)</li>
<li>고유수용감각 (관절 각도, 속도)</li>
</ul></li>
</ul>
<p><strong>핵심 도전: 탐색 문제</strong></p>
<p>손가락 게이팅의 가장 큰 어려움:</p>
<pre><code>문제: 무작위 행동으로는 거의 실패
- 손가락을 조금만 잘못 움직여도 → 물체 낙하
- 성공적인 게이팅은 극히 드물게 발생
- 학습 신호가 너무 희소 (sparse reward)</code></pre>
<p><strong>해결책: Initial State Distribution</strong></p>
<ol type="1">
<li>안정적인 파지 자세들을 사전 생성
<ul>
<li>다양한 관절 각도 샘플링</li>
<li>물리 시뮬레이션으로 안정성 검증</li>
<li>안정한 자세만 저장</li>
</ul></li>
<li>학습 시 이 자세들을 초기 상태로 사용
<ul>
<li>매 에피소드 시작을 안정한 자세에서</li>
<li>거기서부터 게이팅 연습</li>
</ul></li>
<li>효과
<ul>
<li>떨어뜨릴 확률 ↓↓</li>
<li>유용한 경험 수집 ↑↑</li>
<li>학습 속도 5-10배 향상</li>
</ul></li>
</ol>
<p><strong>학습 설정:</strong></p>
<ul>
<li>알고리즘: PPO (Proximal Policy Optimization)</li>
<li>목표: Z축 회전 (수직축)</li>
<li>보상: 각속도 + 안정성 페널티</li>
<li>시뮬레이터: PyBullet</li>
</ul>
<p><strong>성과:</strong></p>
<ul>
<li>Z축 손가락 게이팅 학습 성공!</li>
<li>Palm up과 palm down 두 방향 작동</li>
<li>기존 방법보다 5-10배 빠른 학습</li>
<li>단순한 볼록 물체 (구, 실린더)</li>
</ul>
<p><strong>한계:</strong></p>
<ul>
<li>단일 축(Z축)만</li>
<li>시뮬레이션에서만 검증</li>
<li>실제 로봇 전이 안 됨</li>
</ul>
<hr>
<p><strong>2023년 연구: “Sampling-based Exploration for Reinforcement Learning of Dexterous Manipulation” (RSS 2023)</strong></p>
<p>“더 어려운 물체는 어떻게 다룰까? L자나 U자 같은?”</p>
<p><strong>새로운 도전:</strong></p>
<p>볼록한 공은 어디를 잡아도 비슷합니다. 하지만:</p>
<ul>
<li><strong>L자 물체</strong>: 한쪽이 무거우면 균형 깨짐</li>
<li><strong>U자 물체</strong>: 잘못 잡으면 빠져나감</li>
<li><strong>긴 막대기</strong>: 회전 관성 높음</li>
</ul>
<p>이런 물체들은 <strong>좁은 통로(narrow passage)</strong> 문제를 만듭니다:</p>
<pre><code>상태 공간:
[안정 영역 A] ---- 좁은 통로 ---- [안정 영역 B]

문제: 무작위 탐색으로는 좁은 통로를 통과하기 극히 어려움</code></pre>
<p><strong>해결책: Sampling-Based Planning + RL 하이브리드</strong></p>
<p>두 세계의 장점을 결합:</p>
<p><strong>1. Sampling-Based Planning (SBP):</strong></p>
<div class="sourceCode" id="cb25"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1"></a><span class="co"># RRT* 같은 알고리즘 사용</span></span>
<span id="cb25-2"><a href="#cb25-2"></a><span class="kw">def</span> plan_path(start_pose, goal_pose):</span>
<span id="cb25-3"><a href="#cb25-3"></a>    tree <span class="op">=</span> initialize_tree(start_pose)</span>
<span id="cb25-4"><a href="#cb25-4"></a></span>
<span id="cb25-5"><a href="#cb25-5"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(iterations):</span>
<span id="cb25-6"><a href="#cb25-6"></a>        random_pose <span class="op">=</span> sample_random_pose()</span>
<span id="cb25-7"><a href="#cb25-7"></a>        nearest <span class="op">=</span> find_nearest_in_tree(random_pose)</span>
<span id="cb25-8"><a href="#cb25-8"></a>        new_pose <span class="op">=</span> extend_towards(nearest, random_pose)</span>
<span id="cb25-9"><a href="#cb25-9"></a></span>
<span id="cb25-10"><a href="#cb25-10"></a>        <span class="cf">if</span> physics_check(new_pose):  <span class="co"># 떨어지지 않는지 확인</span></span>
<span id="cb25-11"><a href="#cb25-11"></a>            tree.add(new_pose)</span>
<span id="cb25-12"><a href="#cb25-12"></a></span>
<span id="cb25-13"><a href="#cb25-13"></a>        <span class="cf">if</span> close_to_goal(new_pose, goal_pose):</span>
<span id="cb25-14"><a href="#cb25-14"></a>            <span class="cf">return</span> extract_path(tree, new_pose)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>이렇게 찾은 경로는:</p>
<ul>
<li>물리적으로 실현 가능</li>
<li>좁은 통로도 통과</li>
<li>하지만 실시간은 아님 (계획에 시간 소요)</li>
</ul>
<p><strong>2. Reinforcement Learning:</strong></p>
<div class="sourceCode" id="cb26"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1"></a><span class="co"># SBP가 찾은 경로를 활용</span></span>
<span id="cb26-2"><a href="#cb26-2"></a><span class="kw">def</span> train_policy():</span>
<span id="cb26-3"><a href="#cb26-3"></a>    <span class="co"># 1. SBP로 좋은 경로들 수집</span></span>
<span id="cb26-4"><a href="#cb26-4"></a>    paths <span class="op">=</span> sampling_based_planner.get_paths()</span>
<span id="cb26-5"><a href="#cb26-5"></a></span>
<span id="cb26-6"><a href="#cb26-6"></a>    <span class="co"># 2. 경로의 state들을 초기 상태로 활용</span></span>
<span id="cb26-7"><a href="#cb26-7"></a>    <span class="cf">for</span> episode <span class="kw">in</span> training:</span>
<span id="cb26-8"><a href="#cb26-8"></a>        init_state <span class="op">=</span> sample_from(paths)</span>
<span id="cb26-9"><a href="#cb26-9"></a>        <span class="co"># 여기서부터 RL 학습</span></span>
<span id="cb26-10"><a href="#cb26-10"></a></span>
<span id="cb26-11"><a href="#cb26-11"></a>    <span class="co"># 3. 결과: 경로를 넘어서 일반화된 정책</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>하이브리드 접근의 장점:</strong></p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>방법</th>
<th>장점</th>
<th>단점</th>
<th>하이브리드</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>순수 RL</td>
<td>일반화 좋음</td>
<td>탐색 어려움</td>
<td>✓ 일반화</td>
</tr>
<tr class="even">
<td>순수 Planning</td>
<td>특정 목표 달성</td>
<td>실시간 어려움</td>
<td>✓ 효율</td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td>일반화 제한</td>
<td>✓ 실시간</td>
</tr>
</tbody>
</table>
<p><strong>실험 물체:</strong></p>
<ul>
<li><strong>Easy</strong>: 구, 실린더 (기본선)</li>
<li><strong>Moderate</strong>: 타원체, 두꺼운 막대</li>
<li><strong>Hard</strong>: L자, U자, 얇고 긴 막대</li>
</ul>
<p><strong>성과:</strong></p>
<p>시뮬레이션:</p>
<ul>
<li>모든 난이도의 물체에서 게이팅 성공</li>
<li>Hard 물체도 80% 이상 성공률</li>
<li>순수 RL보다 3-5배 빠른 수렴</li>
</ul>
<p>실제 로봇:</p>
<ul>
<li>2023년 논문에서 드디어 실제 검증!</li>
<li>L자, U자 물체 실제로 회전 성공</li>
<li>IEEE Spectrum에 “Robot Hand Manipulates Complex Objects by Touch Alone” 기사화</li>
</ul>
<p><strong>실제 로봇 하드웨어:</strong></p>
<p>“Highly Dexterous Robot Hand Can Operate in the Dark” - 5개 손가락, 15 DoF - 각 손가락 끝에 광학 촉각 센서 - 완전 암흑에서도 작동 (순수 촉각) - 복잡한 형상 다룸</p>
<p><strong>AnyRotate와의 상세 비교:</strong></p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>측면</th>
<th>Khandate et al.&nbsp;(2022-2023)</th>
<th>AnyRotate (2024)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>촉각 해상도</strong></td>
<td>이진 (1 bit/finger)</td>
<td>연속 (θ,φ,F/finger)</td>
</tr>
<tr class="even">
<td><strong>정보량</strong></td>
<td>5 bits (5 fingers)</td>
<td>~15 연속 변수</td>
</tr>
<tr class="odd">
<td><strong>회전 자유도</strong></td>
<td>주로 Z축</td>
<td>임의 축 통합 정책</td>
</tr>
<tr class="even">
<td><strong>손 방향</strong></td>
<td>Palm up/down (2가지)</td>
<td>6+ 방향 자유롭게</td>
</tr>
<tr class="odd">
<td><strong>중력 도전</strong></td>
<td>부분적 해결</td>
<td>완전 해결</td>
</tr>
<tr class="even">
<td><strong>학습 접근</strong></td>
<td>SBP + RL 하이브리드</td>
<td>목표조건 RL + 커리큘럼</td>
</tr>
<tr class="odd">
<td><strong>탐색 전략</strong></td>
<td>Planning으로 경로 제공</td>
<td>Adaptive curriculum</td>
</tr>
<tr class="even">
<td><strong>물체 복잡도</strong></td>
<td>볼록 → 복잡 (L, U)</td>
<td>다양한 일상 물체</td>
</tr>
<tr class="odd">
<td><strong>실제 검증</strong></td>
<td>2023년 일부</td>
<td>광범위 (10개 물체)</td>
</tr>
<tr class="even">
<td><strong>주요 기여</strong></td>
<td>Planning-RL 결합</td>
<td>중력 불변 + 고밀도 촉각</td>
</tr>
</tbody>
</table>
<p><strong>Khandate 연구의 강점:</strong></p>
<ol type="1">
<li><strong>이론적 체계성</strong>:
<ul>
<li>Narrow passage 문제를 원리적으로 해결</li>
<li>SBP와 RL 결합의 이유를 명확히 설명</li>
<li>로보틱스 커뮤니티에 방법론 제시</li>
</ul></li>
<li><strong>복잡한 형상</strong>:
<ul>
<li>L자, U자 등 기존 방법이 실패한 물체</li>
<li>형상 복잡도의 한계를 넓힘</li>
<li>“이런 것도 학습으로 가능하다”</li>
</ul></li>
<li><strong>점진적 발전</strong>:
<ul>
<li>2022: 기본 개념 검증 (시뮬레이션)</li>
<li>2023: 어려운 물체 + 실제 로봇</li>
<li>체계적이고 탄탄한 연구 진행</li>
</ul></li>
</ol>
<p><strong>Khandate 연구의 한계:</strong></p>
<ol type="1">
<li><strong>이진 촉각의 한계</strong>:
<ul>
<li>“닿았다/안 닿았다”만 알 수 있음</li>
<li>얼마나 세게? → 모름</li>
<li>어느 각도로? → 모름</li>
<li>미세한 미끄럼 감지 불가능</li>
</ul></li>
<li><strong>제한된 중력 대응</strong>:
<ul>
<li>Palm up/down은 가능</li>
<li>하지만 Thumb up/down 같은 극한 자세는 여전히 어려움</li>
<li>중력이 어느 정도 도와주는 상황</li>
</ul></li>
<li><strong>회전축 제한</strong>:
<ul>
<li>주로 Z축 (수직축) 중심</li>
<li>X, Y축 회전은 명시적으로 다루지 않음</li>
<li>임의 축은 미해결</li>
</ul></li>
<li><strong>실제 배포 검증</strong>:
<ul>
<li>2022년은 시뮬레이션만</li>
<li>2023년에 일부 실제 검증</li>
<li>장시간 신뢰성, 다양한 조건 검증은 제한적</li>
</ul></li>
</ol>
<p><strong>연구 철학의 차이:</strong></p>
<p><strong>Khandate의 철학:</strong></p>
<pre><code>"Planning(계획)과 Learning(학습)을 결합하면
 어려운 탐색 문제를 해결할 수 있다"

→ 방법론에 집중
→ 어려운 물체로 방법의 우수성 증명
→ 이론과 실전의 균형</code></pre>
<p><strong>AnyRotate의 철학:</strong></p>
<pre><code>"고밀도 센싱이 있으면
 극한 조건에서도 강건한 조작이 가능하다"

→ 센싱의 중요성 강조
→ 중력 불변이라는 새 차원 추가
→ 실용성에 집중</code></pre>
<p><strong>상호 보완성:</strong></p>
<p>흥미롭게도, 두 접근을 결합하면 시너지가 날 것 같습니다:</p>
<pre><code>Khandate의 SBP + AnyRotate의 고밀도 촉각
= 복잡한 형상을 임의 방향에서 다루는 시스템</code></pre>
<p>예를 들어:</p>
<ul>
<li>L자 물체를 거꾸로 들고</li>
<li>임의의 축으로 회전시키면서</li>
<li>손가락 게이팅까지 수행</li>
</ul>
<p><strong>핵심 차별점:</strong></p>
<ol type="1">
<li><strong>정보의 풍부함</strong>:
<ul>
<li>기존: 4 bits ~ 16 bits</li>
<li>AnyRotate: 연속적 실수 값 (무한 해상도)</li>
</ul></li>
<li><strong>통합성</strong>:
<ul>
<li>기존: 조건별 별도 정책</li>
<li>AnyRotate: 하나의 정책으로 모든 조건</li>
</ul></li>
<li><strong>강건성</strong>:
<ul>
<li>기존: 제한된 조건에서만</li>
<li>AnyRotate: 극한 조건(거꾸로, 옆으로)에서도</li>
</ul></li>
</ol>
</section>
<section id="sievers-et-al.-2022---토크-제어-순수-촉각" class="level3">
<h3 class="anchored" data-anchor-id="sievers-et-al.-2022---토크-제어-순수-촉각">Sievers et al.&nbsp;(2022) - 토크 제어 순수 촉각</h3>
<p><strong>그들의 접근:</strong></p>
<ul>
<li>토크 제어 DLR 손 사용</li>
<li>순수 촉각 (비전 없음)</li>
<li>모델 기반 + 강화학습 결합</li>
</ul>
<p><strong>장점:</strong></p>
<ul>
<li>물리 법칙 직접 활용</li>
<li>더 정밀한 힘 제어</li>
</ul>
<p><strong>한계:</strong></p>
<ul>
<li>고정된 손 방향</li>
<li>특정 물체에만 테스트</li>
<li>일반화 능력 제한적</li>
</ul>
<p><strong>AnyRotate가 더 나은 점:</strong></p>
<ul>
<li>다양한 물체로 일반화</li>
<li>중력 불변</li>
<li>완전 학습 기반 (물리 모델 불필요)</li>
</ul>
<p><strong>Sievers가 더 나은 점:</strong></p>
<ul>
<li>더 정밀한 힘 제어</li>
<li>물리적 직관 통합</li>
</ul>
<hr>
</section>
</section>
<section id="이론적-관점-왜-이게-작동하는가" class="level2">
<h2 class="anchored" data-anchor-id="이론적-관점-왜-이게-작동하는가">7. 이론적 관점: 왜 이게 작동하는가?</h2>
<p>기술적 디테일을 넘어서, 이 시스템이 왜 작동하는지 이론적으로 이해해봅시다.</p>
<section id="정보-이론으로-바라보기" class="level3">
<h3 class="anchored" data-anchor-id="정보-이론으로-바라보기">정보 이론으로 바라보기</h3>
<p><strong>채널 용량 관점:</strong></p>
<p>각 센싱 방식이 전달할 수 있는 정보량을 생각해봅시다.</p>
<p><strong>고유수용감각:</strong></p>
<ul>
<li>16개 관절 각도</li>
<li>연속값이지만 <strong>간접적</strong> 정보</li>
<li>접촉 상태를 추론해야 함</li>
</ul>
<p><strong>이진 촉각:</strong></p>
<ul>
<li>4개 손가락 × 1 bit = 4 bits</li>
<li><strong>직접적</strong>이지만 <strong>제한적</strong></li>
<li>“어디에 닿았나”는 모름</li>
</ul>
<p><strong>고밀도 촉각:</strong></p>
<ul>
<li>4개 손가락 × (θ, φ, |F|) = 12개 연속 변수</li>
<li><strong>직접적</strong>이고 <strong>풍부함</strong></li>
<li>위치와 힘을 동시에 앎</li>
</ul>
<p><strong>상호 정보 (Mutual Information):</strong></p>
<p>물체 상태 S와 관찰 O 사이의 상호 정보:</p>
<p><span class="math display">
I(S; O) = H(S) - H(S|O)
</span></p>
<p>즉, “관찰을 통해 상태에 대한 불확실성이 얼마나 줄어드는가?”</p>
<p><span class="math display">
I(S; O_dense) &gt; I(S; O_discrete) &gt; I(S; O_binary) &gt; I(S; O_proprio)
</span></p>
<p>고밀도 촉각이 더 많은 불확실성을 제거하므로, 더 좋은 의사결정이 가능합니다!</p>
</section>
<section id="pomdp-관점" class="level3">
<h3 class="anchored" data-anchor-id="pomdp-관점">POMDP 관점</h3>
<p>In-hand 조작은 본질적으로 <strong>부분 관측 마르코프 결정 과정</strong>(POMDP)입니다.</p>
<p><strong>문제:</strong></p>
<ul>
<li>물체의 정확한 자세는 모름</li>
<li>접촉 지점의 정확한 위치는 모름</li>
<li>마찰 계수는 모름</li>
</ul>
<p><strong>해결책: Belief State</strong></p>
<p>에이전트는 “실제 상태가 무엇일까?”에 대한 확률 분포를 유지합니다:</p>
<p><span class="math display">
b(s) = P(s | \text{history})
</span></p>
<p><strong>TCN의 역할:</strong></p>
<p>TCN은 지난 30 프레임의 관찰로부터 belief state의 <strong>충분 통계량</strong>을 계산합니다:</p>
<p><span class="math display">
z_t = f_TCN(o_{t-29:t})
</span></p>
<p>이 8차원 벡터 z_t는 belief state를 압축한 표현입니다!</p>
<p><strong>고밀도 촉각의 이점:</strong></p>
<p>더 많은 정보 → 더 정확한 belief → 더 좋은 결정</p>
<p><span class="math display">
Uncertainty(b_{dense}) &lt; Uncertainty(b_{binary})
</span></p>
</section>
<section id="접촉-역학의-관점" class="level3">
<h3 class="anchored" data-anchor-id="접촉-역학의-관점">접촉 역학의 관점</h3>
<p><strong>왜 미끄럼 감지가 중요한가?</strong></p>
<p>마찰 제약:</p>
<p><span class="math display">
|F_{tangential}| ≤ μ * |F_{normal}|
</span></p>
<p>미끄럼이 임박하면:</p>
<p><span class="math display">
|F_{tangential}| / |F_{normal}| → μ
</span></p>
<p><strong>고밀도 촉각으로 알 수 있는 것:</strong></p>
<ul>
<li>접촉 자세 (θ, φ) → 법선 방향 추정</li>
<li>접촉 힘 |F| → 전체 힘의 크기</li>
</ul>
<p>이 두 정보를 결합하면:</p>
<ul>
<li>법선 성분 F_normal 추정</li>
<li>접선 성분 F_tangential 추정</li>
<li>마찰 여유도 계산!</li>
</ul>
<p><strong>정책이 배우는 것:</strong></p>
<div class="sourceCode" id="cb30"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1"></a><span class="cf">if</span> friction_margin <span class="op">&lt;</span> threshold:</span>
<span id="cb30-2"><a href="#cb30-2"></a>    <span class="co"># 미끄럼 임박!</span></span>
<span id="cb30-3"><a href="#cb30-3"></a>    increase_normal_force()</span>
<span id="cb30-4"><a href="#cb30-4"></a>    adjust_finger_positions()</span>
<span id="cb30-5"><a href="#cb30-5"></a><span class="cf">else</span>:</span>
<span id="cb30-6"><a href="#cb30-6"></a>    <span class="co"># 안전함</span></span>
<span id="cb30-7"><a href="#cb30-7"></a>    continue_rotation()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>이 모든 것이 명시적 프로그래밍 없이 <strong>학습</strong>으로 획득됩니다!</p>
</section>
<section id="강화학습-관점-탐색의-어려움" class="level3">
<h3 class="anchored" data-anchor-id="강화학습-관점-탐색의-어려움">강화학습 관점: 탐색의 어려움</h3>
<p><strong>왜 보조 목표가 필요한가?</strong></p>
<p>각속도를 직접 목표로 하면:</p>
<p><span class="math display">
r = ω_{desired} - ω_{actual}
</span></p>
<p>문제:</p>
<ul>
<li>초기에 ω_actual ≈ 0 (물체가 안 돌아감)</li>
<li>보상이 계속 음수</li>
<li>어떻게 개선할지 모름 (희소 보상 문제)</li>
</ul>
<p><strong>보조 목표 방식:</strong></p>
<p><span class="math display">
r = exp(-\text{distance to goal})
</span></p>
<p>장점:</p>
<ul>
<li>목표에 가까워질수록 점진적으로 보상 증가</li>
<li>밀집 보상 (dense reward)</li>
<li>학습 신호가 명확</li>
</ul>
<p><strong>적응형 커리큘럼의 역할:</strong></p>
<p>초기 단계:</p>
<p><span class="math display">
α = 0
r_{total} = r_{stability}  # 안정성만 집중
</span></p>
<p>후기 단계:</p>
<p><span class="math display">
α = 1
r_{total} = r_{rotation} + r_{stability}  # 회전도 중요
</span></p>
<p>이는 <strong>automatic curriculum learning</strong>의 일종입니다. 로봇이 기본을 먼저 배우고, 준비되면 자동으로 다음 단계로 넘어갑니다.</p>
</section>
<section id="창발적-행동-미끄럼-감지" class="level3">
<h3 class="anchored" data-anchor-id="창발적-행동-미끄럼-감지">창발적 행동: 미끄럼 감지</h3>
<p>가장 흥미로운 발견은 <strong>명시적으로 가르치지 않은 행동</strong>이 나타났다는 것입니다.</p>
<p><strong>설계:</strong></p>
<ul>
<li>미끄럼 감지 모듈 없음</li>
<li>미끄럼에 대한 명시적 보상 없음</li>
<li>단지 “회전하라, 떨어뜨리지 마라” 만 명시</li>
</ul>
<p><strong>결과:</strong></p>
<ul>
<li>정책이 스스로 미끄럼을 감지</li>
<li>미끄러질 때 반응적으로 대응</li>
<li>안정성 유지</li>
</ul>
<p><strong>왜 이런 일이?</strong></p>
<ol type="1">
<li><strong>풍부한 정보</strong>: 고밀도 촉각으로 미세한 변화 감지</li>
<li><strong>시간적 패턴</strong>: TCN이 “정상 vs 비정상” 패턴 학습</li>
<li><strong>부정적 피드백</strong>: 떨어뜨리면 큰 페널티 → 그 직전 신호 학습</li>
</ol>
<p>이는 <strong>암묵적 학습</strong>(implicit learning)의 강력함을 보여줍니다. 모든 것을 명시적으로 가르칠 필요가 없습니다!</p>
<hr>
</section>
</section>
<section id="한계점과-개선-방향" class="level2">
<h2 class="anchored" data-anchor-id="한계점과-개선-방향">8. 한계점과 개선 방향</h2>
<section id="물체-형상의-까다로움" class="level3">
<h3 class="anchored" data-anchor-id="물체-형상의-까다로움">물체 형상의 까다로움</h3>
<p>실험을 해보니 재미있는 패턴이 나타났습니다. 사과나 공 같은 둥근 물체는 잘 다루는데, 상자나 긴 막대는 유독 어려워하더라는 것이죠.</p>
<p>왜 그럴까요? 상자를 생각해보세요. 로봇이 상자의 모서리를 잡았을 때와 평평한 면을 잡았을 때, 촉각 센서로는 둘을 구별하기 어렵습니다. 둘 다 “평평한 표면”으로 느껴지니까요. 이런 <strong>모호성</strong>이 문제입니다.</p>
<p>긴 막대기도 마찬가지입니다. 막대기의 어느 부분을 잡았는지, 어느 방향으로 기울어져 있는지를 촉각만으로는 정확히 알기 어렵습니다. 특히 접촉 자세와 힘이라는 제한된 정보만으로는 말이죠.</p>
<p><strong>그렇다면 어떻게 개선할 수 있을까요?</strong></p>
<p>현재는 촉각 이미지를 θ, φ, F 같은 숫자들로 압축해서 사용합니다. 하지만 원본 촉각 이미지에는 더 풍부한 정보가 담겨있습니다. 표면의 질감, 접촉 면적의 분포 같은 것들이죠. 이런 정보를 직접 사용하면 상자의 모서리와 면을 구별할 수 있을 것입니다.</p>
<p>또 다른 방법은 여러 센서의 정보를 결합하는 것입니다. 예를 들어, 네 손가락이 느끼는 촉각 패턴을 종합하면 “아, 이건 긴 물체구나” 혹은 “이건 상자의 모서리를 잡은 거구나”를 추론할 수 있겠죠. 마치 우리가 눈을 감고도 물체를 만져서 형태를 파악하는 것처럼요.</p>
</section>
<section id="하드웨어의-물리적-한계" class="level3">
<h3 class="anchored" data-anchor-id="하드웨어의-물리적-한계">하드웨어의 물리적 한계</h3>
<p>Allegro Hand는 훌륭한 로봇 손이지만, 인간의 손과 비교하면 아직 약한 부분이 있습니다. 특히 손을 옆으로 돌렸을 때(Thumb Up/Down 방향) 성능이 떨어집니다.</p>
<p>이유는 간단합니다. 손가락이 수평 방향일 때는 중력이 손가락을 아래로 당기는 힘과 정면으로 맞섭니다. 손가락 모터가 물체의 무게뿐 아니라 자기 자신의 무게까지 버텨야 하는 거죠. 마치 팔을 앞으로 쭉 뻗고 무거운 물건을 들고 있으면 금방 팔이 아프듯이 말입니다.</p>
<p>실험 결과에서 봤듯이, Palm Up 방향에서는 평균 6.2회 회전했지만, Thumb Down에서는 1.2회밖에 못했죠. 거의 5배 차이입니다.</p>
<p><strong>해결책은 무엇일까요?</strong></p>
<p>가장 직접적인 방법은 더 강력한 모터를 사용하는 것입니다. 하지만 단순히 “더 센 모터”만으로는 부족합니다. 무거워지면 오히려 역효과가 날 수 있으니까요.</p>
<p>더 영리한 접근은 <strong>적응형 제어</strong>입니다. 손의 방향에 따라 제어 게인을 자동으로 조절하는 거죠. 예를 들어, Thumb Down 방향일 때는 모터에 더 큰 힘을 주고, Palm Up일 때는 적당히 조절하는 식입니다. 또는 <strong>중력 보상 알고리즘</strong>을 사용해서, 중력이 미치는 영향을 계산하고 그만큼을 미리 보정할 수도 있습니다.</p>
<p>장기적으로는 저비용 고성능 하드웨어 개발이 필요합니다. 로봇공학의 영원한 숙제죠.</p>
</section>
<section id="회전을-넘어서-더-복잡한-작업들" class="level3">
<h3 class="anchored" data-anchor-id="회전을-넘어서-더-복잡한-작업들">회전을 넘어서: 더 복잡한 작업들</h3>
<p>지금 AnyRotate가 잘하는 건 “회전”입니다. 하지만 실제 세계에서 우리가 하는 조작은 훨씬 다양합니다.</p>
<p>생각해보세요. 병뚜껑을 열 때는 어떻게 하나요? 먼저 병을 잡고, 뚜껑을 잡고, 비틀고, 위로 당기죠. 여러 단계가 순차적으로 일어납니다. 레고를 조립할 때는? 블록 A를 잡고, 블록 B를 찾고, 둘을 정렬하고, 꾹 눌러서 끼워야 합니다.</p>
<p>현재 시스템은 이런 <strong>다단계 조작</strong>을 할 수 없습니다. 회전이라는 하나의 기본 동작(primitive)만 가능하죠. 더 복잡한 작업으로 확장하려면 <strong>계층적 강화학습</strong>이 필요합니다. 상위 레벨에서는 “뚜껑 잡기 → 비틀기 → 당기기” 같은 큰 계획을 세우고, 하위 레벨에서는 각 단계를 실행하는 방식이죠.</p>
<p>또 다른 흥미로운 방향은 <strong>도구 사용</strong>입니다. 망치로 못을 박거나, 드라이버로 나사를 조이는 것들이요. 도구를 사용하면 손의 능력이 확장됩니다. 하지만 도구의 동적 특성을 학습하고, 간접적인 접촉을 다루는 건 또 다른 도전 과제입니다.</p>
<p><strong>양손 협업</strong>도 중요한 확장입니다. 우리는 큰 물체를 다룰 때 양손을 씁니다. 한 손으로는 잡고, 다른 손으로는 조작하죠. 로봇도 두 손을 조율해서 사용할 수 있다면 훨씬 더 유용할 것입니다.</p>
</section>
<section id="변형되는-물체들" class="level3">
<h3 class="anchored" data-anchor-id="변형되는-물체들">변형되는 물체들</h3>
<p>지금까지의 모든 실험은 “강체(rigid body)”, 즉 변형되지 않는 단단한 물체들로 했습니다. 하지만 실제 세계는 그렇지 않습니다.</p>
<p>천을 접어본 적 있나요? 천은 잡는 위치에 따라 모양이 계속 바뀝니다. 종이도 마찬가지죠. 고무공은 누르면 찌그러집니다. 반죽은 손가락 사이로 빠져나갑니다.</p>
<p>이런 <strong>변형 가능한 물체(deformable objects)</strong>를 다루는 건 훨씬 어렵습니다. 왜냐하면:</p>
<ol type="1">
<li><p><strong>시뮬레이션이 어렵습니다.</strong> 현재 사용하는 강체 물리 엔진으로는 천이나 반죽의 움직임을 정확히 모사할 수 없습니다. 소프트 바디 시뮬레이션이 필요한데, 이건 계산 비용이 훨씬 높습니다.</p></li>
<li><p><strong>상태 추정이 복잡합니다.</strong> 강체는 위치와 자세만 알면 됩니다. 하지만 천은? 수백 개의 점의 위치를 다 추적해야 합니다.</p></li>
<li><p><strong>접촉 역학이 다릅니다.</strong> 마찰, 미끄럼, 변형이 모두 복합적으로 일어납니다.</p></li>
</ol>
<p>그래도 흥미로운 연구 방향입니다. 옷을 개거나, 음식을 다루거나, 포장을 뜯는 등 많은 실용적 작업이 변형 물체를 다루니까요.</p>
</section>
<section id="시뮬레이션과-현실의-간극" class="level3">
<h3 class="anchored" data-anchor-id="시뮬레이션과-현실의-간극">시뮬레이션과 현실의 간극</h3>
<p>연구팀이 정말 많은 노력을 기울였지만, 여전히 시뮬레이션과 현실 사이에는 차이가 있습니다.</p>
<p><strong>접촉 물리의 복잡성</strong>이 대표적입니다. 시뮬레이션에서는 “마찰 계수 = 0.8”처럼 깔끔한 숫자를 씁니다. 하지만 실제 세계의 마찰은 훨씬 복잡합니다. 미끄럼 속도에 따라 달라지고, 표면 상태에 영향받고, 심지어 습도나 온도에도 민감합니다. 플라스틱 병과 금속 실린더의 마찰은 완전히 다르죠.</p>
<p><strong>센서 노이즈</strong>도 문제입니다. 시뮬레이션에서는 “가우시안 노이즈 추가”로 간단히 처리합니다. 하지만 실제 센서의 노이즈는 구조화되어 있습니다. 조명이 바뀌면 촉각 이미지가 달라지고, 시간이 지나면 센서가 드리프트합니다. 온도가 올라가면 센서 값이 달라지기도 합니다.</p>
<p><strong>시간 지연</strong>도 무시할 수 없습니다. 시뮬레이션에서는 모든 게 즉각적입니다. 하지만 실제로는 센서에서 데이터를 읽는 데 몇 밀리초, 네트워크로 전송하는 데 몇 밀리초, 처리하는 데 또 몇 밀리초가 걸립니다. 20Hz 제어에서 10ms 지연은 큰 문제입니다.</p>
<p><strong>어떻게 개선할 수 있을까요?</strong></p>
<p>가장 좋은 방법은 실제 데이터로 시뮬레이터를 계속 개선하는 것입니다. 실제 로봇에서 데이터를 모으고, 그 데이터가 시뮬레이션과 어떻게 다른지 분석하고, 시뮬레이터를 조정하는 거죠. 이런 <strong>디지털 트윈(Digital Twin)</strong> 접근은 점점 더 중요해지고 있습니다.</p>
<p>또 다른 방향은 <strong>온라인 적응</strong>입니다. 실제 로봇에 배포한 후에도 계속 학습하는 거죠. 새로운 물체를 만나면 빠르게 적응하는 <strong>메타 학습(Meta-learning)</strong> 기법도 유망합니다.</p>
</section>
<section id="학습-비용의-현실" class="level3">
<h3 class="anchored" data-anchor-id="학습-비용의-현실">학습 비용의 현실</h3>
<p>솔직히 말하면, AnyRotate를 재현하려면 꽤 좋은 컴퓨터가 필요합니다.</p>
<p>8,192개의 병렬 환경을 돌리려면 고성능 GPU가 필수입니다. IsaacGym 같은 GPU 기반 시뮬레이터를 쓰면 가능하지만, 모든 연구실이 이런 자원을 갖춘 건 아니죠. 학습에도 수일이 걸립니다.</p>
<p>이건 강화학습의 고질적인 문제입니다. 너무 많은 경험이 필요해요. 인간은 물건 몇 번 만져보면 감을 잡는데, 로봇은 수백만 번의 시도가 필요합니다.</p>
<p><strong>더 효율적인 방법은 없을까요?</strong></p>
<p>여러 대안이 연구되고 있습니다:</p>
<ul>
<li><strong>샘플 효율적 알고리즘</strong>: SAC, TD3 같은 off-policy 알고리즘은 데이터를 더 효율적으로 씁니다. PPO는 on-policy라서 경험을 한 번만 쓰고 버리지만, off-policy는 여러 번 재사용할 수 있죠.</li>
<li><strong>모델 기반 강화학습</strong>: 세계가 어떻게 작동하는지 모델을 학습하면, 머릿속(모델 속)에서 연습할 수 있습니다. 실제로 해보지 않고도 “이렇게 하면 이렇게 될 것 같은데?”를 시뮬레이션하는 거죠.</li>
<li><strong>오프라인 강화학습</strong>: 이미 수집된 데이터셋으로 학습합니다. 새로운 시도를 하지 않고도 과거 경험에서 배우는 거죠. 마치 유튜브 영상 보고 배우는 것처럼요.</li>
<li><strong>인간 시연 활용</strong>: 사람이 먼저 몇 번 시연하고, 거기서부터 학습을 시작합니다. 완전히 무에서 시작하는 것보다 훨씬 빠릅니다.</li>
</ul>
<p>하지만 아직 완벽한 해법은 없습니다. 로봇공학 커뮤니티가 함께 풀어야 할 숙제입니다.</p>
</section>
<section id="평가의-어려움" class="level3">
<h3 class="anchored" data-anchor-id="평가의-어려움">평가의 어려움</h3>
<p>“얼마나 잘하는가?”를 측정하는 것도 생각보다 어렵습니다.</p>
<p>현재는 두 가지 메트릭을 씁니다:</p>
<ul>
<li><strong>회전 수</strong>: 30초 동안 몇 번 회전했나?</li>
<li><strong>TTT (Time to Terminate)</strong>: 떨어뜨리기 전까지 얼마나 버텼나?</li>
</ul>
<p>명확하긴 한데, 뭔가 부족합니다. 예를 들어:</p>
<ul>
<li>5번 회전했는데 엄청 거칠게 돌렸다면?</li>
<li>30초 버텼는데 물체가 상처투성이가 됐다면?</li>
<li>느리지만 부드럽게 돌린 것과 빠르지만 위험하게 돌린 것, 뭐가 더 좋은가?</li>
</ul>
<p><strong>더 좋은 평가 방법은?</strong></p>
<p>여러 가지를 추가로 측정할 수 있습니다:</p>
<ul>
<li><strong>에너지 효율성</strong>: 같은 작업을 더 적은 에너지로 하는 게 좋겠죠. 로봇이 과도하게 힘을 쓰면 배터리도 빨리 닳고 모터도 빨리 닳습니다.</li>
<li><strong>파지 안정성</strong>: 물체와 손가락 사이의 접촉력이 얼마나 일정하게 유지되는가? 들쭉날쭉하면 불안정한 거고, 부드럽게 유지되면 좋은 거죠.</li>
<li><strong>물체 손상</strong>: 표면에 스크래치나 찌그러짐이 있나요? 특히 섬세한 물체를 다룰 때 중요합니다.</li>
<li><strong>일반화 능력</strong>: 처음 보는 물체에 얼마나 잘 적응하나? 이건 정량화하기 정말 어렵습니다.</li>
</ul>
<p>하지만 이런 것들을 자동으로 측정하기는 어렵습니다. 모션 캡처 시스템으로 물체 위치를 정밀하게 추적하고, 힘-토크 센서로 접촉력을 측정하고, 고해상도 카메라로 표면 상태를 확인해야 합니다. 장비가 많이 필요하죠.</p>
<p>이상적으로는 로봇공학 커뮤니티가 합의한 <strong>표준 벤치마크</strong>가 있으면 좋겠습니다. 컴퓨터 비전에 ImageNet이 있고, 자연어 처리에 GLUE가 있듯이, 로봇 조작에도 그런 게 필요합니다. 그래야 서로 다른 방법들을 공정하게 비교할 수 있으니까요.</p>
<p><strong>결국 이 한계들은…</strong></p>
<p>이런 한계점들을 나열하니 뭔가 AnyRotate가 부족해 보일 수도 있습니다. 하지만 전혀 그렇지 않습니다.</p>
<p>오히려 이런 한계들은 <strong>연구가 얼마나 정직한지</strong>를 보여줍니다. 연구팀이 자신들의 시스템을 냉정하게 평가하고, 다음 단계를 명확히 제시한 것이죠. “우리가 이만큼 했고, 여기서 이런 문제가 남았어요”라고 솔직하게 말하는 것은 과학적 정직성의 표시입니다.</p>
<p>그리고 이 한계들이 바로 <strong>다음 세대 연구자들을 위한 로드맵</strong>입니다. “여기까지 왔으니, 이제 저기로 가보자”라는 이정표인 셈이죠. 누군가는 변형 물체를 다루는 법을 연구할 것이고, 누군가는 더 효율적인 학습 방법을 개발할 것이고, 누군가는 저비용 하드웨어를 설계할 것입니다.</p>
<p>과학은 이렇게 발전합니다. 완벽한 시스템을 한 번에 만드는 게 아니라, 한 걸음씩 나아가면서 점점 나아지는 것입니다. AnyRotate는 그 여정에서 중요한 한 걸음이고, 다음 연구자들은 여기서 시작해서 더 멀리 갈 것입니다.</p>
</section>
</section>
</section>
<section id="dig-review" class="level1">
<h1>⛏️ Dig Review</h1>
<blockquote class="blockquote">
<p>⛏️ Dig — Go deep, uncover the layers. Dive into technical detail.</p>
</blockquote>
<section id="anyrotate-논문-개요-및-시스템-구조" class="level2">
<h2 class="anchored" data-anchor-id="anyrotate-논문-개요-및-시스템-구조">AnyRotate 논문 개요 및 시스템 구조</h2>
<p>AnyRotate는 인간 수준의 다축 중력 무관(invariant) 인-핸드 객체 회전을 위해 고해상도 촉각 정보를 활용하는 로봇 시스템이다. 4-지간(Allegro) 로봇 핸드의 각 손가락 끝에 부착된 생체모방형 광학 촉각 센서(TacTip 기반 DigiTac)를 사용하며, 강화학습(RL)을 통해 어떤 축(axis)에 대해서도 임의의 방향으로 물체를 회전시킬 수 있는 단일 정책(policy)을 학습한다. 이 시스템의 핵심은 시뮬레이션에서 연속적 접촉 피처 표현(continuous contact feature representation) 을 학습하고, 실제 환경의 촉각 영상으로부터 이를 예측하여 제로샷(sim-to-real) 정책 이전을 달성하는 것이다. 특히, 촉각 센서의 연속적 접촉 포즈(contact pose)와 접촉력(contact force) 정보를 정책에 입력함으로써, 기존의 이진(binary) 또는 이산(discrete) 촉각 표현보다 풍부한 접촉 정보를 활용할 수 있음을 보였다.</p>
<p><strong>촉각 센서 및 핸드 구조:</strong> Allegro 4-지간 핸드(16자유도)에 TacTip 기반 촉각 센서를 부착하고, UR5 로봇암에 장착하여 다양한 손 방향(palm up/down, thumb up/down 등)에서 객체를 회전시킨다 (그림 1, 2 참조). 촉각 센서는 커브드 표면을 가지며 밀착면 전체의 접촉 정보를 고해상도 영상으로 제공한다.</p>
<p><strong>목표 및 제약:</strong> AnyRotate는 임의의 회전축, 임의의 손 방향에서 연속적이고 안정적인 객체 회전을 수행하도록 학습된다. 이를 위해 목표 지향(goal-conditioned) 강화학습 문제로 정의하고, 보조 서브골(auxiliary goal)과 점진적 커리큘럼(adaptive curriculum) 등을 통해 효율적인 학습을 설계했다. 교사-학생(Teacher-Student) 구조로, 시뮬레이션에서 특권 정보(privileged information)를 사용한 교사 정책을 학습하고, 학생 정책이 촉각-고유 감각만으로 이를 모방하도록 한다.</p>
<p>본 분석에서는 촉각 센서 데이터의 표현(Representation), 인코딩(Encoding), 제어 정책 통합 방법을 중심으로, 촉각 피드백의 시뮬레이션 구현과 시뮬-실제 전이 과정, 그리고 촉각 감지 모델링의 혁신점 및 과제를 상세히 살펴본다.</p>
</section>
<section id="촉각-센서-데이터의-표현과-정책에서의-활용" class="level2">
<h2 class="anchored" data-anchor-id="촉각-센서-데이터의-표현과-정책에서의-활용">촉각 센서 데이터의 표현과 정책에서의 활용</h2>
<p>촉각 센서가 생성하는 고해상도 영상(tactile image) 은 원시적으로 매우 고차원이므로, 이를 제어 정책이 활용 가능한 형태로 변환해야 한다. AnyRotate에서는 촉각 영상을 접촉 포즈와 접촉력이라는 저차원 연속적 피처로 인코딩한다. 구체적으로, 접촉 포즈(contact pose) 는 손가락 끝점(fingertip) 기준의 구형 좌표계에서 극각(polar angle) θ와 방위각(azimuthal angle) φ로 나타낸다. 접촉력(contact force) 은 접촉점에서 발생하는 힘 벡터의 크기(등급크기, magnitude)이다. 추가로, 이진 접촉 신호(binary contact) 는 실제로 센서가 물체에 접촉 중임을 나타내는 0/1 값으로, 깊이(z축) 변위나 힘의 크기가 일정 임계값 이상이면 참(1)으로 설정한다. 모든 손가락에 대하여 이들 값이 계산되어 정책 관측(observation) 벡터에 포함된다.</p>
<p>제어 정책의 관측 공간(observation space) 은 다음과 같은 정보들로 구성된다: 현재 및 목표 관절각, 이전 행동, 각 손가락 끝의 위치와 자세, 그리고 촉각 관련 변수로서 각 손가락의 이진 접촉 여부, 접촉 포즈(θ, φ), 접촉력 크기이다. 실제 환경에서 손가락 끝 위치/자세는 역기구학으로 계산하며, 촉각 영상처리 결과를 통해 접촉 포즈와 접촉력을 추출한다. 예를 들어, 시뮬레이션에서는 각 손가락 끝의 로컬 접촉 위치 벡터로부터 θ, φ를 계산하고, 접촉력의 정합(net force)을 <span class="math inline">|\mathbf{F}|</span>로 취해 이를 관측값으로 사용한다. 이처럼 연속적이고 풍부한 촉각 변수들은 정책이 물체의 미세 움직임과 접촉 상태를 정밀하게 파악하도록 돕는다.</p>
<p>이렇게 추출된 촉각 피처는 정책의 입력으로 사용된다. 교사 정책은 이들 값을 포함한 관측 벡터를 받아 연속 동작(관절 상대 위치명령)을 출력하며, 실제 정책(학생)은 촉각 히스토리와 고유 감각(proprioception) 히스토리를 결합한 잠재벡터를 통해 행동을 선택한다. 학생 정책에서는 TCN(순환합성곱 신경망) 구조를 사용하여 시간축으로 연속된 촉각 관측 시퀀스를 인코딩한다. 즉, 과거 30스텝에 걸친 촉각 피처와 관절각 히스토리를 TCN에 통과시켜 잠재벡터(z)로 만들고, 이를 바탕으로 정책(액터)이 동작을 출력한다. 정책 학습 과정에서는 교사 정책이 생성한 잠재벡터와 학생의 잠재벡터 간 MSE(평균제곱오차) 및 행동 출력 간 NLL(부정로그우도) 손실을 최소화하여 학생을 지도학습 방식으로 훈련한다.</p>
<center>
<img src="../../images/2025-11-02-anyrotate/01.png" width="100%">
</center>
<blockquote class="blockquote">
<p>AnyRotate의 촉각 예측 파이프라인. (a) 각 손가락 끝의 촉각 영상은 전처리(그레이스케일 변환, 리사이즈 등)를 거친다. (b) 각 촉각 영상은 학습된 관찰 모델(observation model, CNN)을 통과하여 접촉 포즈(θ, φ)와 접촉력 |F|을 예측한다. (c) 예측된 접촉 포즈(극각 θ, 방위각 φ)는 구면좌표계에서 표시되며, 접촉력은 색상의 면적으로 시각화된다. 이와 같이 추출된 촉각 피처가 강화학습 정책의 입력으로 사용된다.</p>
</blockquote>
<p>위 그림에서 보듯이, 촉각 영상은 먼저 흐림/노이즈 제거 등의 전처리를 거친 뒤 CNN 관찰 모델에 입력된다. CNN은 학습을 통해 각 손가락 접촉의 극각(θ), 방위각(φ) 그리고 총 접촉력 크기 <span class="math inline">|\mathbf{F}|</span>을 예측한다. 접촉 포즈 <span class="math inline">(\theta,\phi)</span>와 접촉력 크기는 정책에 연속 변수로 제공되며, 손가락별로 <span class="math inline">(</span>이진 접촉, θ, φ, <span class="math inline">|\mathbf{F}|)</span> 값이 매 타임스텝의 관측(state)에 포함된다. 이러한 연속적 촉각 표현은 2D 이미지를 단순히 접촉 있음/없음으로 이진화한 기존 방식에 비해 훨씬 풍부한 정보를 제공한다. 실제로 실험 결과 연속 접촉 포즈와 힘 정보를 활용한 정책은 이산화(discrete) 촉각 표현보다 물체 회전 성능이 현저히 우수하였다.</p>
</section>
<section id="촉각-피드백의-시뮬레이션-구현과-도메인-적응" class="level2">
<h2 class="anchored" data-anchor-id="촉각-피드백의-시뮬레이션-구현과-도메인-적응">촉각 피드백의 시뮬레이션 구현과 도메인 적응</h2>
<p>AnyRotate에서는 촉각 센서를 시뮬레이션 환경에서 가상으로 구현하여 교사 정책을 학습시킨다. IsaacGym 물리엔진을 사용하여 충돌을 처리하며, 각 손가락 끝에서 발생한 실제 접촉 정보를 이용한다. 구체적으로, 시뮬레이션 단계에서 손가락 끝의 로컬 접촉 위치 <span class="math inline">p_{\rm local}</span>를 추출하여 이를 극좌표 <span class="math inline">(\theta,\phi)</span>로 변환한다. 또한, 접촉으로 인한 힘 벡터를 계산하여 그 크기 <span class="math inline">|\mathbf{F}|</span>를 구하고, 이를 접촉력 관측값으로 사용한다. 이 때 실세계 센서의 특성을 모사하기 위해 몇 가지 처리를 추가한다. 먼저 부드러운(tactile) 센서가 가진 탄성 변형 딜레이를 모사하기 위해 힘 값에 지수평활(exponential moving average)을 적용하고, 시뮬레이터에서의 힘/포즈 값을 실제 센서의 측정 범위에 맞춰 포화(saturation) 및 재스케일링한다. 예를 들어, 힘 크기의 최대값을 제한하고 범위를 재조정하여 가상 센서가 실제와 유사한 범위 내에서 동작하도록 한다. 포즈 각도도 물리적 센서가 허용하는 최대 각도로 제한한다. 이러한 조정은 시뮬레이션과 실제 센서 간의 데이터 분포 차이를 줄이기 위한 일종의 도메인 적응(domain adaptation) 기법이다.</p>
<p>또한, 실제 촉각 센서에서는 접촉이 발생하지 않는 상태에서도 미약한 노이즈가 발생할 수 있는데, 이를 처리하기 위해 시뮬레이션에서도 이진 접촉 신호를 만들 때 일정 임계값(<span class="math inline">\theta_F</span>)을 사용한다. 즉, 힘 크기가 작은 경우에는 접촉 없음으로 간주하여 접촉 포즈와 힘을 0으로 마스킹한다. 이러한 절차로 시뮬레이션된 촉각 관측값(θ, φ, <span class="math inline">|\mathbf{F}|</span>, 접촉 유무)이 RL 관측 공간에 공급되며, 교사 정책은 이를 포함한 완전 관측 상태를 사용하여 학습된다. 시뮬레이션 및 로봇 제어 주파수는 각각 60Hz, 20Hz로 실제 시스템과 동일하게 설정되었다. 이처럼 세심하게 조정된 시뮬레이션 촉각은 실제 센서와의 심도 차이(depth), 슈어(shear) 범위를 반영하도록 설계되었다.</p>
<p>도메인 랜덤화(domain randomization)의 관점에서 살펴보면, AnyRotate는 전통적인 이미지 노이즈/텍스처 랜덤화 대신, 물리적 파라미터 포화 및 스케일 매핑으로 시뮬레이션-실제 격차를 줄인다. 예를 들어, 실제 TacTip 센서는 깊이 방향 변위가 수 mm 범위 내에서만 민감하게 반응하므로, 시뮬레이션에서는 접촉 깊이도 약 ~ mm 범위로 제한한다. 이처럼 실험적으로 정해진 범위 내에서 촉각 데이터를 생성하여 관찰 모델을 학습함으로써 시뮬-실제 간의 특성 불일치를 최소화한다.</p>
</section>
<section id="observation-model을-통한-시뮬-실제-전이" class="level2">
<h2 class="anchored" data-anchor-id="observation-model을-통한-시뮬-실제-전이">Observation Model을 통한 시뮬-실제 전이</h2>
<p>실제 로봇 환경에서는 시뮬레이션에서처럼 접촉 위치나 힘을 직접 계산할 수 없으므로, 이미지 기반 관찰 모델을 활용하여 실제 촉각 영상을 촉각 피처로 변환한다. 이를 위해 AnyRotate는 CNN 기반 관찰자 모델(Observation Model)을 학습한다. 학습 데이터는 6-자유도 UR5 로봇암에 탑재된 TacTip 센서와 힘/토크(F/T) 센서를 사용하여 수집된다. 구체적으로, 센서를 평평한 자극 표면에 무작위 방향·위치로 접촉(tap)시켜 얻은 촉각 이미지와 F/T 센서의 힘 정보를 함께 저장한다. 각 데이터 샘플은 로봇의 <span class="math inline">x,y,z</span> 위치와 접촉력 <span class="math inline">(F_x,F_y,F_z)</span>을 포함하며, 이 중 접촉 포즈(θ, φ)는 로봇 암의 위치로부터, 접촉력 크기는 F/T 센서 값의 크기로 정답 레이블(label)로 사용한다. 데이터 수집 시 센서 자세(pose)는 실험적으로 다음 범위 내에서 랜덤 샘플링된다: 깊이 <span class="math inline">\Delta z\in[-4,-1]</span>mm, <span class="math inline">x,y</span>방향 <span class="math inline">\pm2</span>mm, 회전 <span class="math inline">\pm28^\circ</span> 범위. 센서당 약 3000개의 이미지를 모아 CNN을 학습한다.</p>
<p>학습된 관찰 모델은 입력으로 원시 촉각 영상(그레이스케일, 240×135 픽셀)을 받아 6차원 출력을 예측한다. 이 6차원은 접촉 깊이 <span class="math inline">d_z</span>, 접촉 포즈 <span class="math inline">\theta,\phi</span>, 그리고 힘 벡터 성분 <span class="math inline">F_x,F_y,F_z</span>이다. 이후 이 중에서 실제 강화학습 관측값으로는 (<span class="math inline">\theta,\phi,|\mathbf{F}|</span>) 세 값만 사용된다. 즉, 모델이 예측한 <span class="math inline">(F_x,F_y,F_z)</span>로부터 크기 <span class="math inline">|\mathbf{F}|=\sqrt{F_x^2+F_y^2+F_z^2}</span>을 계산하여 접촉력을 얻고, 접촉 여부는 SSIM(구조적 유사성 지표)를 기반으로 결정한다. 구체적으로, 현재 입력 영상과 센서가 접촉하지 않은 기준 영상을 비교하여 SSIM 유사도를 계산하고, 이 값이 0.6 이상이면 접촉으로 간주하여 위에서 예측한 <span class="math inline">(\theta,\phi,|\mathbf{F}|)</span>를 사용한다. 그렇지 않으면 접촉하지 않은 것으로 처리하여 모든 촉각 피처를 0으로 마스킹한다. 이 과정에서 SSIM 임계값은 경험적으로 0.6으로 설정하였고, 그레이스케일 영상에 블러와 어댑티브 임계처리(adaptive threshold) 등 전처리 필터를 적용하여 노이즈를 줄인다.</p>
<p>학습된 CNN 관찰 모델은 제로샷(sim-to-real) 정책 이전에 핵심 역할을 한다. 즉, 시뮬레이션에서 학습된 교사-학생 정책은 시뮬레이션상의 촉각 피처를 이용하지만, 실제 배치 시에는 센서가 제공하는 실시간 영상에서 관찰 모델이 추출한 촉각 피처를 입력으로 사용한다. 이 때 관찰 모델은 이전에 수집한 데이터로 충분히 학습되었기 때문에, 정책은 학습 시와 유사한 형태의 촉각 입력을 받게 되어 추가적인 재학습 없이(제로샷) 바로 실환경에 적용 가능하다.</p>
</section>
<section id="교사-학생-정책-학습-파이프라인" class="level2">
<h2 class="anchored" data-anchor-id="교사-학생-정책-학습-파이프라인">교사-학생 정책 학습 파이프라인</h2>
<p>AnyRotate의 학습은 크게 두 단계로 구성된다. 첫째, 교사 정책(Teacher Policy)은 시뮬레이션에서 특권 정보(privileged information)를 사용하여 PPO 같은 RL 기법으로 학습한다. 교사 정책의 입력 관측에는 앞서 언급한 촉각 피처 외에도 객체의 정확한 위치·자세, 각도 속도, 객체 크기, 중력 작용력 등 시뮬레이션에서만 얻을 수 있는 정보가 포함된다. 이렇게 학습된 교사 정책은 목표 축에 대해 물체를 안정적으로 회전시키는 정책을 만들어낸다.</p>
<p>둘째, 학생 정책(Student Policy)은 실제 상황에서의 실행을 위해 학습된다. 학생은 교사를 모방하기 위해 지도학습(policy distillation)을 사용한다. 학생 정책은 오로지 고유감각(관절각 등)과 앞서 추출된 촉각 히스토리만을 입력으로 받는다. 교사 정책은 매 시점마다 내부 잠재벡터(z)를 생성하는데, 학생 정책은 TCN 인코더로 입력 연속 관측을 잠재벡터로 압축하고, 이 잠재벡터가 교사의 것과 유사해지도록 학습된다. 또한 학생 정책의 출력 행동이 교사 정책의 행동과 가까워지도록 NLL 손실을 추가로 최적화한다. 이때 전체 손실은 잠재벡터 MSE와 행동 NLL의 합이며, 식으로 표현하면 다음과 같다: <span class="math display">\mathcal{L} = \alpha |z_{\rm teacher} - z_{\rm student}|^2 + \beta\,\bigl(-\log p_{\rm student}(a_{\rm teacher}|s)\bigr),</span> 여기서 <span class="math inline">z_{\rm teacher}</span>는 교사의 잠재벡터, <span class="math inline">z_{\rm student}</span>는 학생의 인코더 출력, <span class="math inline">p_{\rm student}(a_{\rm teacher}|s)</span>는 학생 정책이 교사 행동 <span class="math inline">a_{\rm teacher}</span>를 취할 확률이다. 이 과정에서 학습된 관찰 모델을 활용하여 시뮬레이션 학생 학습 단계에서도 실제 촉각 피처를 모사하여 사용할 수 있다.</p>
<center>
<img src="../../images/2025-11-02-anyrotate/00.png" width="100%">
</center>
<blockquote class="blockquote">
<p>AnyRotate의 학습 및 적용 파이프라인 개요. (왼쪽) 목표 회전축에 대한 물체 자세 재설정(auxiliary goal) 방식의 RL 문제 설정. (오른쪽) 교사-학생 정책(distillation) 구조. 교사 정책(Teacher)은 시뮬레이션의 특권 정보와 촉각 관측을 입력으로 RL 학습을 수행하고, 학생 정책(Student)은 촉각·고유감각 관측 히스토리를 입력으로 교사를 모방한다. 실세계 이전 시에는 CNN 관찰 모델이 실제 촉각 영상에서 접촉 피처를 추출하여 학생 정책에 공급한다.</p>
</blockquote>
<p>그림에서 보듯, AnyRotate는 교사-학생 정책 구조와 보조 목표(auxiliary goal), 적응형 커리큘럼을 결합하여 학습을 진행한다. 교사 정책은 객체 자세를 목표로 설정하고, 객체의 6D 키포인트(keypoint) 거리 보상 등을 포함한 목표 지향 보상함수로 훈련된다. 학생 정책은 이렇게 학습된 교사의 안정적인 움직임을 실제 센서 데이터로 재현할 수 있도록, TCN 인코딩과 행동 모방을 통해 학습된다. 이 과정을 통해 AnyRotate는 시뮬레이션에서 학습한 정책을 추가 데이터 수집이나 재학습 없이 실환경에 전달할 수 있게 된다.</p>
</section>
<section id="촉각-감지-모델링의-혁신성과-도전-과제" class="level2">
<h2 class="anchored" data-anchor-id="촉각-감지-모델링의-혁신성과-도전-과제">촉각 감지 모델링의 혁신성과 도전 과제</h2>
<p>AnyRotate가 제시하는 주요한 혁신성 중 하나는 고해상도 촉각 정보를 활용한 시뮬-실제 RL 정책을 구현했다는 점이다. 기존의 촉각 기반 조작 연구에서는 종종 접촉 영상을 단순한 이진 신호나 이산 공간으로 축소하여 사용해 왔다. 반면 본 논문은 촉각 영상을 연속적이고 기하학적인 피처(θ, φ, |F|)로 표현함으로써, 훨씬 더 풍부한 접촉 정보를 정책 학습에 활용하였다. 이를 통해 미지의 물체나 무거운 물체에서도 유연하게 일반화할 수 있는 단일 정책을 달성했으며, 실제 다양한 물체로의 전이 실험에서 복수의 촉각 센서만으로도 성공적으로 회전 작업을 수행하였다. 또한, 복수 촉각 정보를 통해 물체의 미끄러짐(slippage)이나 잡기 실패를 암묵적으로 탐지하여 반응하는 행동을 구현할 수 있었다는 점도 주목할 만하다. 연구진은 명시적 미끄럼 검출기 없이도 “풍부한 다지간 촉각 센서가 객체 움직임을 감지하고 정책의 강인성(robustness)을 향상시키는 반응 행동을 유발”한다는 사실을 보고하였다.</p>
<center>
<img src="../../images/2025-11-02-anyrotate/02.png" width="100%">
</center>
<p>그러나 촉각 센서 모델링에는 여러 도전 과제가 존재한다. 우선, 물리 기반 시뮬레이션의 한계다. TacTip과 같은 광학 촉각 센서는 유연한 겔 표면의 변형을 영상으로 포착하므로, 완전히 정확한 물리 시뮬레이션은 어려우며 매우 계산 집약적일 수 있다. AnyRotate에서는 단순히 충돌 지점을 기반으로 접촉 위치와 힘을 계산하는 방식으로 근사했지만, 실제 센서의 미세한 압력 분포나 센서 내부 렌즈 왜곡 등은 반영하지 못한다. 따라서 시뮬-실제 간의 데이터 분포 차이가 발생할 수 있으며, 이를 보정하기 위해 실험적으로 값의 포화, 시점 변경 등의 도메인 적응 기법을 활용하였다.</p>
<p>또한, 촉각 영상 자체가 매우 높은 차원을 가지므로 이를 유용한 피처로 압축하는 신경망 설계도 중요한 문제다. AnyRotate는 각 손가락마다 개별 CNN을 학습하여 접촉 피처를 추출했지만, 이 과정에서 과적합(overfitting) 방지와 정밀도(accuracy) 확보를 위해 많은 데이터와 적절한 네트워크 규모를 필요로 했다. 뿐만 아니라, 로봇 조작 중에는 센서 이미지에 동적 노이즈(손떨림, 광원 변화 등)가 발생할 수 있기 때문에, 관찰 모델은 다양한 환경 변화에 견고해야 한다. 이 연구에서는 SSIM 기반의 접촉 유무 검출과 영상 전처리로 노이즈를 감소시켰지만, 완전한 일반화를 위해서는 추가적인 도메인 랜덤화(예: 조명 색깔 변화, 노이즈 추가 등)가 필요할 수 있다.</p>
<p>끝으로, AnyRotate는 단일 정책으로 모든 회전축에 일반화한다는 점에서 기술적으로도 과제다. 서로 다른 축 회전은 중력의 영향과 접촉 기구학이 달라지므로, 정책이 충분히 모든 상황을 커버하도록 학습 데이터를 다양화해야 한다. 이를 위해 연구진은 에피소드마다 목표 회전축과 손 방향을 무작위로 설정하고, 적응형 보상 커리큘럼을 통해 학습 초기에는 쉬운 과제부터 시작했다. 이러한 접근은 시뮬레이션에서는 효과적이었으나, 실세계에서는 눈에 띄는 추가 구현 없이도 전이 가능해야 한다. AnyRotate는 관찰 모델을 통한 제로샷 전이로 이를 달성했으나, 아직도 “시뮬레이션에서 본 것과 완전히 다른 형태의 접촉”에는 민감할 수 있다. 예를 들어 이형적인 재질이나 윤곽을 가진 물체에서는 관찰 모델이 예측 오차를 낼 수 있어 정책 성능에 영향이 있을 수 있다. 따라서 고해상도 촉각 신호를 시뮬레이션에 정밀히 반영하는 것은 지속적인 연구 과제로 남아 있다.</p>
<p>이처럼 AnyRotate는 고해상도 촉각 정보의 시뮬-실제 통합이라는 측면에서 중요한 진전을 이뤘다. 연속적인 촉각 피처 표현과 관찰 모델을 결합하여 제로샷 이전을 가능하게 했다는 점은 촉각 기반 조작 연구의 새로운 방향을 제시한다. 동시에, 시뮬레이션에서의 센서 근사화 및 시뮬-실제 차이 극복이라는 어려운 문제를 다루었으며, 다가오는 연구에서는 더욱 정교한 물리 모델링과 데이터 증강 방법이 요구될 것으로 보인다.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<script src="https://utteranc.es/client.js" repo="curieuxjy/blog_comments" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Copyright 2024, Jung Yeon Lee</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/curieuxjy">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>