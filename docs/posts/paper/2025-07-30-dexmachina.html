<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-07-30">
<meta name="description" content="Functional Retargeting for Bimanual Dexterous Manipulation">

<title>📃DexMachina 리뷰 – Curieux.JY</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../profile.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-8ef56b68f8fa1e9d2ba328e99e439f80.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-ecf89aac047581c664da7ae53d704519.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-b009f778f5cec7f34f624408a2b5b543.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-ecf89aac047581c664da7ae53d704519.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-2NVZN2MJZT"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-2NVZN2MJZT', { 'anonymize_ip': true});
</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Curieux.JY</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../post.html"> 
<span class="menu-text">Post</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../note.html"> 
<span class="menu-text">Note</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">Jung Yeon Lee</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/curieuxjy"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#brief-review" id="toc-brief-review" class="nav-link active" data-scroll-target="#brief-review">Brief Review</a></li>
  <li><a href="#detail-review" id="toc-detail-review" class="nav-link" data-scroll-target="#detail-review">Detail Review</a>
  <ul class="collapse">
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#방법론의-독창성-dexmachina만의-리타게팅-접근법" id="toc-방법론의-독창성-dexmachina만의-리타게팅-접근법" class="nav-link" data-scroll-target="#방법론의-독창성-dexmachina만의-리타게팅-접근법">1. 방법론의 독창성: DexMachina만의 리타게팅 접근법</a></li>
  <li><a href="#실험-결과-분석-성능-적응성-일반화-능력" id="toc-실험-결과-분석-성능-적응성-일반화-능력" class="nav-link" data-scroll-target="#실험-결과-분석-성능-적응성-일반화-능력">2. 실험 결과 분석: 성능, 적응성, 일반화 능력</a></li>
  <li><a href="#기존-연구와의-비교-dexmachina의-기여도와-차별화" id="toc-기존-연구와의-비교-dexmachina의-기여도와-차별화" class="nav-link" data-scroll-target="#기존-연구와의-비교-dexmachina의-기여도와-차별화">3. 기존 연구와의 비교: DexMachina의 기여도와 차별화</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-page-left" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">📃DexMachina 리뷰</h1>
  <div class="quarto-categories">
    <div class="quarto-category">retargeting</div>
    <div class="quarto-category">hand</div>
  </div>
  </div>

<div>
  <div class="description">
    Functional Retargeting for Bimanual Dexterous Manipulation
  </div>
</div>


<div class="quarto-title-meta column-page-left">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">July 30, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<ul>
<li><a href="https://arxiv.org/abs/2505.24853">Paper Link</a></li>
<li><a href="https://project-dexmachina.github.io/">Project Link</a></li>
<li><a href="https://github.com/MandiZhao/dexmachina">Github Link</a></li>
</ul>
<ol type="1">
<li>🤖 이 연구는 인간의 손-객체 시연을 로봇의 양손으로 관절형 객체를 조작하는 기능적 리타겟팅(functional retargeting) 정책으로 학습시키는 어려운 문제를 다룹니다.</li>
<li>💡 핵심 아이디어는 가상 객체 컨트롤러(virtual object controllers)의 강도를 점진적으로 약화시키는 커리큘럼 기반 RL(강화 학습) 알고리즘인 DexMachina를 제안하여, 정책이 초기에는 객체를 자동으로 목표 상태로 이동시키면서 점차 조작을 인수하도록 훈련시키는 것입니다.</li>
<li>✅ 이 알고리즘은 기존의 기준 방법론을 훨씬 능가하는 성능을 보였으며, 다양한 덱스터러스 핸드(dexterous hands)와 작업을 포함하는 시뮬레이션 벤치마크를 공개하여 하드웨어 설계의 기능적 비교를 가능하게 합니다.</li>
</ol>
<center>
<img src="../../images/2025-07-30-dexmachina/dexmachina-teaser-website.png" width="100%">
</center>
<hr>
<section id="brief-review" class="level1">
<h1>Brief Review</h1>
<p>이 논문은 인체-객체 데모로부터 객체 상태를 추적하기 위한 능숙한 조작 정책을 학습하는 <code>functional retargeting</code> 문제를 다룬다. 특히, 높은 차원의 액션 공간, 시공간적 불연속성, 그리고 인간 손과 로봇 손 사이의 <code>embodiment gap</code>으로 인해 어려운 장기적이고 양손을 사용하는(bimanual) <code>articulated object</code> 작업에 중점을 둔다.</p>
<p>이러한 문제들을 해결하기 위해, 저자들은 가상 객체 컨트롤러(<code>virtual object controllers</code>, VOCs)를 기반으로 하는 새로운 커리큘럼 기반 <code>RL</code> 알고리즘인 <code>DexMachina</code>를 제안한다. <code>DexMachina</code>의 핵심 아이디어는 가상 객체 컨트롤러를 사용하여 객체를 목표 상태로 자동으로 움직이게 함으로써, 정책이 동작 및 접촉 지침 하에서 점진적으로 제어권을 인계받도록 학습하는 것이다. 초기에는 <code>VOCs</code>가 객체 움직임의 대부분을 담당하며, 정책은 작업을 망치지 않으면서 인간의 동작을 모방하는 것을 학습한다. 시간이 지남에 따라 <code>VOCs</code>의 강도가 약해지면서, 정책은 점차적으로 객체 조작을 스스로 수행하도록 학습한다.</p>
<p><strong>핵심 방법론: DexMachina</strong></p>
<p><code>DexMachina</code>는 <code>RL</code> 환경에서 <code>functional retargeting</code> 작업을 수행하기 위해 정책을 학습시킨다.</p>
<ol type="1">
<li><strong>RL 환경 및 Task Reward</strong>: 각 타임스텝 <span class="math inline">t</span>에서 객체의 실제 달성 상태 <span class="math inline">\hat{G}_t = \{\hat{g}_t^P, \hat{g}_t^R, \hat{g}_t^J\}</span> (위치, 회전, 조인트 각도)와 데모의 목표 상태 <span class="math inline">G_t = \{g_t^P, g_t^R, g_t^J\}</span>를 비교한다. <code>task reward</code> <span class="math inline">r_{\text{task}}</span>는 각 상태 구성 요소의 정확도를 측정하는 세 항의 곱으로 정의되어 균형 잡힌 학습을 장려한다.
<ul>
<li>위치 오차: <span class="math inline">d_{\text{pos}} = || \hat{g}_t^P - g_t^P ||_2</span></li>
<li>회전 오차: <span class="math inline">d_{\text{rot}} = 2 \cos^{-1}(|\langle \hat{g}_t^R, g_t^R \rangle|)</span></li>
<li>관절 오차: <span class="math inline">d_{\text{ang}} = || \hat{g}_t^J - g_t^J ||_2</span></li>
<li><code>Task reward</code>: <span class="math display">r_{\text{task}} = \exp(-\beta_{\text{pos}}d_{\text{pos}}) \exp(-\beta_{\text{rot}}d_{\text{rot}}) \exp(-\beta_{\text{ang}}d_{\text{ang}})</span> 여기서 <span class="math inline">\beta_{\text{pos}}, \beta_{\text{rot}}, \beta_{\text{ang}}</span>는 각 구성 요소에 대한 가중치이다.</li>
</ul></li>
<li><strong>액션 정형화 및 Aux Reward (<code>Auxiliary Rewards</code>)</strong>: <code>task reward</code>만으로는 복잡한 장기 작업에서 정책이 효과적으로 학습하기 어렵다. 이를 보완하기 위해 두 가지 방법을 사용한다.
<ul>
<li><strong>하이브리드 액션 정형화 (<code>Hybrid Action Formulation</code>)</strong>: 인간 데모와 더 잘 일치하도록 손목(<code>wrist</code>) 액션 공간을 제한하고, 나머지 손가락(<code>finger</code>) 관절은 절대 액션을 사용한다. <code>kinematics-only retargeting</code> 알고리즘 (<code>Anyteleop</code> [3] 기반)을 통해 얻은 <code>retargeted joint</code> 값 <span class="math inline">Q \in \mathbb{R}^{T \times J}</span>를 손목 관절의 기본 액션으로 사용하며, 정책은 그 위에 잔차(<code>residual</code>) 액션을 출력한다. 손가락 관절은 관절 한계에 의해 정규화된 절대 액션을 사용한다.</li>
<li><strong>Aux Reward (<code>Auxiliary Rewards</code>)</strong>: 정책이 인간의 손-객체 상호작용 전략을 따르도록 유도한다.
<ul>
<li><strong>데이터 전처리</strong>: 데모 데이터 <span class="math inline">D_\eta</span>에서 <code>collision-aware kinematic retargeted joints</code> <span class="math inline">Q</span>와 참조 키포인트(<code>reference keypoints</code>) <span class="math inline">X \in \mathbb{R}^{T \times K \times 3}</span>를 추출한다. 또한, 손 링크와 객체 부품 사이의 근사 접촉 위치 <span class="math inline">C \in \mathbb{R}^{(T \times N \times K \times 3)}</span>와 유효성 마스크 <span class="math inline">M \in \mathbb{R}^{(T \times N \times K)}</span>를 추출한다.</li>
<li><strong>동작 모방 Reward (<code>Motion Imitation Reward</code>)</strong>: 인간과 유사한 손 동작을 장려하기 위해 키포인트 매칭 기반의 <span class="math inline">r_{\text{imi}}</span>와 관절 각도 거리 기반의 행동 복제(<code>behavior-cloning</code>) Reward <span class="math inline">r_{\text{bc}}</span>를 정의한다. <span class="math display">r_{\text{imi}} = \frac{1}{K} \sum_{i=1}^K \exp(-\beta_{\text{imi}}||\hat{x}_i - x_i||_2)</span> <span class="math display">r_{\text{bc}} = \frac{1}{J} \sum_{i=1}^J \exp(-\beta_{\text{bc}}||\hat{q}_i - q_i||_2)</span></li>
<li><strong>접촉 Reward (<code>Contact Reward</code>)</strong>: 정책의 접촉을 데모의 접촉과 일치시켜 계산한다. 접촉 거리 <span class="math inline">D \in \mathbb{R}^{N \times K}</span>는 유효성 마스크에 의해 가려진 <code>L2</code> 거리를 사용하여 계산된다. <span class="math display">r_{\text{con}} = \frac{1}{2NK} (\sum_{i=1}^N \sum_{j=1}^K \exp(-\beta_{\text{con}}D(i,j)_{\text{left}}) + \sum_{i=1}^N \sum_{j=1}^K \exp(-\beta_{\text{con}}D(i,j)_{\text{right}}))</span> 최종 <code>RL</code> Reward <span class="math inline">r_t</span>는 이 모든 항의 가중 합이다: <span class="math display">r_t = \lambda_{\text{task}}r_{\text{task}} + \lambda_{\text{imi}}r_{\text{imi}} + \lambda_{\text{bc}}r_{\text{bc}} + \lambda_{\text{con}}r_{\text{con}}</span></li>
</ul></li>
</ul></li>
<li><strong>가상 객체 컨트롤러를 이용한 자동 커리큘럼 (<code>Auto-Curriculum with Virtual Object Controllers</code>)</strong>: 정책이 복잡한 작업에서 초기 실패에 빠지는 것을 방지하기 위해 <code>VOCs</code>를 도입한다. <code>VOCs</code>는 데모 상태 <span class="math inline">G</span>를 제어 목표로 삼아 가상 스프링-댐퍼 제약(<code>spring-damper constraints</code>)을 적용하여 객체를 목표 궤적을 따라 움직이게 한다.
<ul>
<li><strong>구현</strong>: 시뮬레이션에서 <code>privileged information</code>을 사용하여 구현된다. 각 객체는 기본 포즈에 대해 6-DoF, 관절 동작에 대해 1-DoF를 가진 가상 관절을 갖추고 있으며, 모든 관절은 <code>PD</code> 컨트롤러에 의해 구동된다.</li>
<li><strong>커리큘럼 스케줄링</strong>: 학습 시작 시에는 높은 <code>VOC</code> 이득(<code>gains</code>) <span class="math inline">(k_p, k_v)</span>을 설정하고, 정책의 학습 진행에 따라 이 이득을 점진적으로 지수적으로 감소시킨다. 정책이 모든 Reward(<code>task</code>, <code>imi</code>, <code>bc</code>, <code>con</code>)에 대해 일정 임계값(<code>threshold</code>)을 초과하면 이득이 감소한다. 이를 통해 정책은 초기에 높은 <code>task reward</code>를 달성하면서 Aux Reward을 개선하는 방법을 학습하고, <code>VOCs</code>가 약해지면 높은 <code>task reward</code>를 유지하기 위해 자체 동작을 조정하는 법을 배운다.</li>
</ul></li>
</ol>
<p><strong>실험 및 결과</strong></p>
<p>저자들은 6개의 능숙한 로봇 손(<code>Inspire</code>, <code>Allegro</code>, <code>Xhand</code>, <code>Schunk</code>, <code>Ability</code>, <code>DexRobot Hand</code>)과 5개의 <code>articulated object</code>로 구성된 시뮬레이션 벤치마크를 구축했다. <code>Genesis</code> 물리 시뮬레이터와 <code>PPO</code> 알고리즘을 사용하여 <code>DexMachina</code>를 평가했다.</p>
<ul>
<li><strong>주요 결과</strong>: <code>DexMachina</code>는 모든 손과 작업에서 일관되게 성능을 향상시켰으며, 특히 복잡한 장기 작업에서 기준선(<code>baseline</code>) 방법들(<code>Kinematics Only</code>, <code>ObjDex</code>, <code>Task + Auxiliary Rewards without curriculum</code>, <code>ManipTrans</code>)을 크게 능가했다. <code>kinematic retargeting</code>만으로는 작업을 완료할 수 없었다.</li>
<li><strong>하드웨어 적응성</strong>: <code>DexMachina</code>는 정책이 하드웨어 제약에 맞춰 작업 전략을 학습하도록 한다. 예를 들어, <code>Notebook-300</code> 작업에서 <code>XHand</code>는 인간 데모를 따랐지만, 더 작고 구동이 적은 <code>Inspire Hand</code>는 객체를 안정화하고 커버를 닫기 위해 양손을 사용하는 다른 전략을 학습했다. 이는 Aux Reward이 엄격한 지침이 아닌 유연한 안내 역할을 하여 정책이 더 나은 <code>task reward</code>를 위해 참조 동작에서 벗어날 수 있음을 보여준다.</li>
<li><strong>Ablation Study</strong>:
<ul>
<li><strong>액션 Ablation</strong>: 제안된 하이브리드 액션 정형화(손목 움직임에 더 제한적인 경계를 사용)가 절대 액션이나 덜 제약적인 잔차 액션보다 전반적으로 더 나은 학습 성능을 보였다.</li>
<li><strong>커리큘럼 Ablation</strong>: <code>ManipTrans</code>의 커리큘럼(오차 임계값, 중력, 마찰 매개변수 감소)은 <code>DexMachina</code>의 <code>VOC</code> 기반 커리큘럼만큼 효과적이지 않았다. 물리 매개변수만 감소시키는 것으로는 장기적 <code>articulated object</code> 작업에 충분하지 않음을 시사한다.</li>
</ul></li>
<li><strong>손 구현체 분석 (<code>Hand Embodiment Analysis</code>)</strong>: <code>DexMachina</code>와 벤치마크를 사용하여 다른 능숙한 손 디자인을 기능적으로 비교했다. 더 크고 완전히 구동되는 손(<code>Allegro Hand</code> 등)이 더 높은 최종 성능과 더 나은 학습 효율을 보였다. 크기보다는 자유도(<code>degrees of freedom</code>, <code>DoF</code>)가 더 중요하며, <code>Schunk Hand</code>는 <code>Inspire</code>, <code>Ability</code>와 비슷한 크기임에도 더 많은 <code>DoF</code>와 유연한 디자인 덕분에 더 나은 성능을 달성했다.</li>
</ul>
<center>
<img src="../../images/2025-07-30-dexmachina/plot.png" width="100%">
</center>
<p><strong>결론 및 한계</strong></p>
<p>이 작업은 <code>functional retargeting</code>을 위한 <code>DexMachina</code> 알고리즘과 포괄적인 시뮬레이션 벤치마크를 제공한다. <code>DexMachina</code>는 기존 방법들을 뛰어넘는 성능을 보이며, 다양한 능숙한 손 디자인에 대한 기능적 비교를 가능하게 한다. 한계점으로는, 정책이 시뮬레이터의 <code>privileged information</code>에 의존하는 상태 기반 입력을 사용한다는 점(실제 세계에서의 취득 어려움), 고품질 인간 손-객체 데모 데이터의 필요성(수집 비용과 큐레이션), 시뮬레이션된 손 모델의 물리적 속성 추정으로 인한 실제 하드웨어와의 동역학 불일치 가능성, 그리고 실제 세계에서의 평가 부족 등이 있다.</p>
<hr>
</section>
<section id="detail-review" class="level1">
<h1>Detail Review</h1>
<blockquote class="blockquote">
<p>DexMachina: 기능적 리타게팅을 통한 양손 섬세 조작</p>
</blockquote>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>인간의 손재주는 로봇 공학에서 오랫동안 궁극적인 목표였지만, <strong>인간 손과 로봇 손의 차이(embodiment gap)</strong>로 인해 동일한 동작을 이식(retarget)하는 데 많은 어려움이 존재합니다. 2025년 5월 Arxiv에 공개된 <strong>“DexMachina: Functional Retargeting for Bimanual Dexterous Manipulation”</strong> 논문은 이러한 문제를 해결하기 위해 제안된 새로운 강화학습 기반 방법입니다. 이 글에서는 해당 논문의 핵심 아이디어와 방법론, 실험 결과, 그리고 기존 관련 연구들과의 비교를 <strong>전문가의 시각</strong>에서 깊이 있게 분석합니다. 특히 <strong>① 방법론의 독창성</strong> (기존 리타게팅/모션 트랜스퍼 기법 대비 차별점), <strong>② 실험 결과 분석</strong> (다양한 환경에서의 성능과 효과), <strong>③ 기존 연구와의 비교</strong> (유사 목적을 가진 프레임워크들과의 기술적 차이와 기여도)에 중점을 두어 살펴보겠습니다.</p>
<ul>
<li><p><strong>기능적 리타게팅의 개념:</strong> DexMachina는 <strong>기능적 리타게팅</strong>(functional retargeting)이라는 개념을 정립합니다. 이는 <strong>인간 시연의 “결과”에 초점을 맞춰</strong> 로봇이 물체를 동일하게 조작하도록 학습하는 것으로, 단순히 인간의 손동작을 흉내내는 <strong>운동학적 리타게팅</strong>과 대비됩니다. 기존의 운동학적 리타게팅은 로봇 손가락 위치를 사람 손과 유사하게 따라하게 할 수는 있어도 물체 조작 성공을 보장하지 못하는 반면, DexMachina는 <strong>물체의 목표 상태를 따라가도록</strong> 정책을 학습시키는 점에서 근본적으로 다릅니다.</p></li>
<li><p><strong>가상 객체 제어와 커리큘럼 학습:</strong> DexMachina의 가장 독창적인 아이디어는 <strong>“가상 객체 제어기(virtual object controller)”</strong>를 활용한 자동 <strong>커리큘럼 학습</strong>입니다. 초기 학습 단계에서는 외부 힘(가상 제어기)이 물체를 자동으로 목표 위치까지 <strong>밀어주면서</strong> 정책 학습을 돕고, 점진적으로 그 도움을 줄여나감으로써 최종적으로 로봇 정책이 <strong>스스로 물체를 조작</strong>하도록 만듭니다. 이러한 <strong>점감적 Aux</strong> 방식은 초반 학습의 난이도를 크게 낮춰주어, 긴 시퀀스 작업에서도 <strong>초기 실패를 방지</strong>하고 안정적으로 탐색할 수 있게 합니다.</p></li>
<li><p><strong>다양한 손과 작업에 대한 범용성:</strong> 저자들은 <strong>6종의 로봇 손</strong>(Inspire Hand, Allegro Hand, X-Hand, Schunk Hand 등)과 <strong>5종의 복잡한 물체</strong>(노트북, 주방기구 등 관절부를 가진 물체들)로 구성된 <strong>시뮬레이션 벤치마크</strong>를 구축하여 DexMachina의 성능을 평가했습니다. 결과적으로 DexMachina는 <strong>모든 손과 작업에 걸쳐 기존 방법들을 능가하는 성공률</strong>을 보였으며, 특히 <strong>복잡한 양손 장기 작업(long-horizon)</strong>에서 두각을 나타냈습니다. 또한 하나의 알고리즘으로 다양한 로봇 손에 별도 튜닝 없이 적용 가능함을 보여주어, 향후 <strong>로봇 손 하드웨어 설계 비교</strong>에도 유용한 표준을 제시합니다.</p></li>
</ul>
<p>이제 위 세 가지 주제에 대해 순서대로 자세히 살펴보겠습니다.</p>
</section>
<section id="방법론의-독창성-dexmachina만의-리타게팅-접근법" class="level2">
<h2 class="anchored" data-anchor-id="방법론의-독창성-dexmachina만의-리타게팅-접근법">1. 방법론의 독창성: DexMachina만의 리타게팅 접근법</h2>
<p><strong>인간 시연의 “기능”을 학습 목표로 삼다.</strong> DexMachina는 <strong>인간 손-물체 상호작용 시연</strong>(예: 사람이 두 손으로 와플 기계를 들어 열었다 닫는 시연)을 입력으로 받아, 로봇의 두 손이 <strong>물체의 동일한 기능적 결과를 재현</strong>하도록 정책을 학습시킵니다. 여기서 <strong>기능적 결과</strong>란 물체의 상태 변화에 초점을 둔 것으로, 사람 시연과 <strong>똑같은 동작 경로를 그리지 않더라도</strong> 물체의 움직임이 같다면 성공으로 간주합니다. 이는 사람 움직임 자체를 따라하려는 기존의 접근과 근본적으로 다릅니다. 예를 들어 사람이 공을 던지는 시연이 주어졌을 때, DexMachina는 로봇이 <strong>공을 같은 목표에 맞히는 것</strong>에 집중하지, 인간의 모든 손가락 각도를 그대로 재현하려 하지는 않습니다. 이러한 발상의 전환 덕분에, 로봇과 인간 손 구조가 달라도 <strong>Task 중심</strong>으로 학습이 이루어질 수 있습니다.</p>
<p><strong>시연 데이터로 자동 Reward 설계:</strong> DexMachina는 한 편의 <strong>인간 데모</strong>(모션 캡쳐된 손/물체 궤적)만으로도 추가 Reward 설계 없이 학습 목표를 정의합니다. 구체적으로, <strong>데모의 물체 상태 궤적</strong>을 추출하여 로봇이 따라가야 할 <strong>Task Reward</strong>(task reward)을 정하고, 인간 손동작을 로봇 손으로 <strong>충돌 없게 변환한 기준 모션</strong>을 계산하여 <strong>모션 모방 Reward</strong>으로 활용합니다. 또한 데모에서 <strong>손-물체 접촉 지점</strong>을 근사추정하여, 로봇 손도 비슷한 지점에 접촉하도록 <strong>접촉 Reward</strong>을 부여합니다. 흥미로운 점은, <strong>로봇 손목(wrist)</strong> 움직임은 데모 궤적을 최대한 따르도록 제한하고 손가락 관절은 절대 제어하게 하는 <strong>하이브리드 방식</strong>을 취한다는 것입니다. 이렇게 하면 큰 팔 동작은 인간과 비슷하게 유지하면서도, 세밀한 손가락 움직임은 로봇이 <strong>자유롭게 조정</strong>하여 자기 구조에 맞게 최적화할 수 있습니다. 요약하면, DexMachina는 <strong>데모 → (Task + 모션 + 접촉) Reward</strong>으로 변환하는 <strong>객체 중심 학습 목표</strong>를 자동 구축하여, 별도의 Reward 함수 설계 없이도 <strong>효과적인 강화학습 환경</strong>을 마련합니다.</p>
<p><strong>“Deus ex machina” – 가상 스프링으로 시작하는 커리큘럼 RL:</strong> DexMachina라는 이름에는 “기계에서 내려온 신”이라는 뜻이 담겨 있는데, 이는 알고리즘이 <strong>초반에 마치 보이지 않는 손처럼 물체를 움직여주는 모습</strong>에 착안한 것입니다. 정책 학습이 특히 어려운 <strong>장기간 양손 조작</strong>의 경우, 두 손의 미세한 협응 실패로 초반에 곧잘 <strong>임무가 좌초</strong>되기 쉽습니다. 이를 해결하기 위해 DexMachina는 <strong>자동 커리큘럼(auto-curriculum)</strong> 전략을 도입했습니다. 초기 학습 단계에서는 <strong>가상 객체 제어기(Virtual Object Controller)</strong>가 일종의 <strong>스프링 힘</strong>으로 물체를 사람이 보여준 방향대로 움직여 줍니다. 정책은 이 때 <strong>실패 위험 없이</strong> 따라하는 법을 배우고, 점차 정책이 성과를 내기 시작하면 <strong>가상 힘의 세기를 줄여</strong> 정책이 <strong>자율적으로</strong> 물체를 조작하도록 만듭니다. 이러한 <strong>점진적 난이도 상승</strong>은 강화학습에서 흔히 쓰이는 커리큘럼 학습 개념을 응용한 것으로, 물체 물리 동역학을 제어하는 외력으로 난이도를 조절하는 점이 특징입니다.</p>
<center>
<img src="../../images/2025-07-30-dexmachina/1.png" width="100%">
</center>
<blockquote class="blockquote">
<p><em>DexMachina 알고리즘의 개요도.</em> 왼쪽은 인간 <strong>양손 데모</strong>에서 얻은 물체 상태 변화(예: 와플 기계 뚜껑의 각도)와 로봇 모션/접촉 <strong>Reward 신호</strong> 추출 과정을 보여준다. 오른쪽은 <strong>자동 커리큘럼 학습</strong> 과정으로, 초기에는 <strong>강한 가상 스프링</strong>(가상 객체 제어기)이 물체를 목표대로 움직여주어 로봇 정책이 실패 없이 모방학습을 하고, 중기에는 <strong>약한 스프링</strong>으로 Aux를 줄이면서, 최종적으로 <strong>스프링 없이</strong> 정책 혼자서 물체를 조작하도록 훈련된다. 이러한 방식으로 DexMachina는 <strong>운동학적 리타게팅의 초기 가이드</strong>와 <strong>강화학습의 자율 탐색</strong>을 자연스럽게 연결한다.</p>
</blockquote>
<p><strong>기존 기법 대비 차별성:</strong> DexMachina의 방법론은 기존 리타게팅 및 모션 트랜스퍼 기술과 몇 가지 중요한 차이를 보입니다. <strong>첫째</strong>, 사람 손동작을 로봇에 <strong>단순 이식</strong>하던 접근과 달리, DexMachina는 사람-로봇 간 <strong>공통의 Task 공간</strong>(object state trajectory)에서 문제를 정의하여 <strong>물리적으로 실행 가능한 전략</strong>을 학습합니다. 예컨대 Park 등(2025)은 인간-로봇-물체 움직임의 <strong>공동 임베딩 공간</strong>을 학습하여 거기서 로봇 동작을 추론하는 방식으로 <strong>데이터 기반 리타게팅</strong>을 수행하였는데, DexMachina는 이를 <strong>강화학습 정책</strong>으로 구현함으로써 새로운 손/환경에도 <strong>온라인 적응 능력</strong>을 부여합니다. <strong>둘째</strong>, 기존 많은 <strong>텔레오퍼레이션 기반 시연</strong> 방법들은 로봇 손마다 별도 시스템 구축이 필요하고 주로 단발적 그립 동작에 그쳤지만, DexMachina는 <strong>단 하나의 시연</strong>만으로 장시간의 복잡한 조작을 가능케 했습니다. 또한 <strong>DeepMimic</strong>이나 <strong>DAPG</strong>(Rajeswaran et al., 2018)처럼 예제 모션을 RL로 따라하는 선행 연구들이 있었지만, 대부분 <strong>단일 로봇 손</strong>에 <strong>단일 작업</strong>을 다루고 Reward 신호도 제한적이었습니다. 반면 DexMachina는 <strong>Reward 구성의 다양화(Task+모션+접촉)</strong>와 <strong>외력 커리큘럼</strong>이라는 새로운 요소로 이러한 모션 트랜스퍼 문제를 확장했고, 이를 통해 <strong>두 손이 협응</strong>해야 하는 복잡한 작업도 자동학습이 가능함을 보였습니다. 종합하면 DexMachina는 <strong>“시연 데이터 + 강화학습 + 커리큘럼”</strong>의 세 박자를 맞추어, 기존 방법들이 부분적으로만 해결했던 문제들을 하나의 프레임워크에 통합한 점에서 독창적입니다.</p>
</section>
<section id="실험-결과-분석-성능-적응성-일반화-능력" class="level2">
<h2 class="anchored" data-anchor-id="실험-결과-분석-성능-적응성-일반화-능력">2. 실험 결과 분석: 성능, 적응성, 일반화 능력</h2>
<p><strong>벤치마크 구성 및 평가 지표:</strong> 저자들은 사람이 양손으로 수행한 긴 조작 시연 7개를 선택하여, 이를 모사해야 하는 시뮬레이션 환경을 만들었습니다. 해당 시연 데이터는 <strong>ARCTIC</strong>이라는 공개 데이터셋의 일부로, 예를 들어 <strong>박스를 집어들어 뚜껑 열기</strong>, <strong>노트북 들어 덮기</strong>, <strong>믹서기 뚜껑 닫기</strong>, <strong>와플 기계 들어 열기</strong> 등 <strong>5가지 물체</strong>에 대한 단기/장기 과제들을 포함합니다. 로봇 손으로는 오픈소스 <strong>6종의 로봇 핸드</strong>(Inspire, Allegro, X-Hand, Schunk, Ability, DexRobot 등 크기와 구조가 다양한 모델)를 활용했습니다. 성능 평가는 <strong>물체 추적 오차</strong> 기반 지표를 사용했는데, 데모의 물체 궤적 대비 로봇이 조작한 물체의 위치/자세가 얼마나 일치하는지를 <strong>AUC-ADD</strong>(평균 거리 오차에 대한 곡선 아래 면적) 형태의 점수로 환산하고, 이를 <strong>성공률(%)</strong>로 표현했습니다. 직관적으로 100%면 물체를 정확히 따라간 것이고, 0%면 전혀 따라가지 못한 것입니다.</p>
<p><strong>주요 비교 방법:</strong> 실험에서는 DexMachina를 다음과 같은 방법들과 비교했습니다:</p>
<ul>
<li><strong>Kinematic Only:</strong> 인간 <strong>운동학적 리타게팅 결과</strong>를 그대로 재생. (정책 학습 없음)</li>
<li><strong>Task Rew Only:</strong> <strong>ObjDex</strong>라 명명된 기준으로, <strong>Task Reward만</strong>으로 RL 정책 학습 (인간 데모의 물체 궤적만 목표, 모션/접촉 Reward 없음).</li>
<li><strong>Task + Aux Reward:</strong> DexMachina에서 제안한 <strong>Task + 모션 + 접촉 Reward</strong>은 쓰되, <strong>커리큘럼 없이</strong> 학습 (즉, 가상 객체 제어 미사용).</li>
<li><strong>ManipTrans (2025)</strong>: Li 등(2025)이 제안한 최신 방법으로, <strong>사전 모방학습 + 잔여(residual) 정책 RL</strong> 2단계로 인간 양손 시연을 이식하는 기법. (저자들이 공개한 코드/데이터를 이용한 결과)</li>
</ul>
<center>
<img src="../../images/2025-07-30-dexmachina/2.png" width="100%">
</center>
<blockquote class="blockquote">
<p><em>여러 방법의 성공률 비교 (높을수록 우수).</em> 가로축은 작업 종류(예: <em>Ketchup-100</em>은 짧은 케첩 통 흔들기 과제, <em>Waffleiron-300</em>은 긴 와플기계 조작 과제)이며, 세로축은 해당 과제 완료 성공률(%)입니다. <strong>노랑</strong>은 학습 없이 <strong>운동학적 리타게팅만</strong>으로 실행한 경우, <strong>갈색(ObjDex)</strong>은 <strong>Task Reward만</strong>으로 RL한 경우, <strong>회색</strong>은 <strong>Task+Aux Reward</strong>으로 RL했으나 <strong>커리큘럼 없는</strong> 경우, <strong>녹색(DexMachina)</strong>은 제안 기법 (Aux Reward+커리큘럼)이며, <strong>진갈색(ManipTrans)</strong>은 최신 2단계 잔여학습 방식입니다. DexMachina(녹색)가 <strong>대부분의 과제에서 최고 성능</strong>을 보이며, 특히 난이도가 높은 <em>-300 장기 과제</em>들에서 두드러지게 앞서는 것을 알 수 있습니다. 또한 <strong>ManipTrans</strong> 대비해서도 DexMachina가 많은 경우 높은 성공률을 보이는데, 이는 제안 기법의 <strong>탁월한 장기 탐색 능력</strong>을 방증합니다. 실제로 ManipTrans 연구에서도 자사 방법이 성공률과 모션 재현 정확도에서 기존 기법들을 능가한다고 보고하였지만, DexMachina의 커리큘럼 전략이 동일 과제에서 한층 높은 성과를 낸 것을 확인할 수 있습니다.</p>
</blockquote>
<p><strong>종합 성능:</strong> 결과 그래프를 보면 DexMachina(녹색 막대)가 <strong>모든 실험 환경에서 가장 높은 성공률</strong>을 기록함을 알 수 있습니다.</p>
<ul>
<li><strong>단순 리타게팅 재생(Kinematic Only)</strong>의 경우 사람과 로봇 손 구조 차이로 인해 물체를 제대로 다루지 못해 성공률이 거의 <strong>0에 수렴</strong>하며, <strong>Reward만 준 RL(ObjDex)</strong>도 <strong>초반 탐색 실패</strong>로 장기 작업을 끝내지 못하는 경우가 많았습니다.</li>
<li>반면 DexMachina는 <strong>커리큘럼이 있는 경우</strong> <strong>없는 경우 대비 크게 향상</strong>되어, <strong>모든 손</strong>과 <strong>모든 작업</strong>에서 일관되게 최고의 성능을 달성했습니다. 특히 각 작업 이름에 <code>-300</code>이 붙은 <strong>장기 시나리오</strong>(예: Notebook-300, Waffleiron-300 등)에서 그 격차가 두드러졌습니다.</li>
</ul>
<p><strong>정량적 수치 및 분석:</strong></p>
<ul>
<li>DexMachina는 전반적인 평균 <strong>약 85%</strong> 수준의 높은 성공률을 기록하였는데, 이는 기존 방법들에 비해 크게 향상된 수치입니다.</li>
<li>특히 <strong>단기 과제</strong>의 경우 모든 로봇 손이 <strong>70~90%</strong>에 달하는 준수한 성과를 거두었고, <strong>장기 과제</strong>에서도 가장 어려운 시나리오(Waffleiron-300 등)조차 성공률 40~80% 범위를 달성하여 <strong>난제 해결의 가능성</strong>을 보여주었습니다.</li>
<li>반면 Aux Reward이나 커리큘럼이 없었던 RL 정책은 장기 과제에서 <strong>0~30%</strong> 수준에 머무는 등 불안정한 모습을 보였으며, 이는 <strong>초기 탐색 실패</strong>와 <strong>접촉 타이밍 학습 미비</strong> 등에 기인합니다.</li>
<li>DexMachina는 <strong>가상 제어기의 초기 개입</strong> 덕분에 이러한 실패 구간을 건너뛰고 효과적으로 학습을 진행, <strong>에피소드 말미까지 임무를 완수</strong>하는 비율을 크게 끌어올린 것입니다.</li>
</ul>
<p><strong>질적 결과: 적응적 전략의 학습</strong></p>
<ul>
<li>흥미로운 것은, DexMachina로 학습된 정책이 <strong>주어진 인간 시연을 맹목적으로 복제하지 않고</strong>, 로봇 자신의 신체에 맞게 <strong>전략을 재구성</strong>했다는 점입니다.</li>
<li>예를 들어 <strong>노트북 덮기</strong> 작업에서, <strong>XHand 로봇 손</strong>은 인간 시연과 동일하게 <strong>왼손으로 노트북을 들고 오른손으로 덮개를 닫는 전략</strong>을 따라한 반면, <strong>더 작고 자유도가 낮은 Inspire Hand</strong>는 <strong>양손 모두로 노트북을 지탱하면서 덮개를 닫는</strong> 방식으로 임무를 완수했습니다.</li>
<li>동일한 인간 시연을 참고했음에도 각 로봇의 <strong>크기와 관절 한계에 최적화된 동작</strong>을 스스로 찾아낸 것입니다. 비슷하게 <strong>믹서기 뚜껑 닫기</strong> 작업에서는, <strong>Allegro Hand</strong>가 사람처럼 <strong>긴 엄지손가락을 활용</strong>해 뚜껑을 눌러 닫은 반면, <strong>구조가 다른 Schunk Hand</strong>는 <strong>손바닥과 손목을 이용</strong>해 뚜껑을 밀어 닫는 등 상이한 접근을 보였습니다.</li>
<li>이러한 사례들은 DexMachina의 정책이 <strong>하드웨어 제약에 적응</strong>하여 <strong>기능적 목표를 달성하는 법</strong>을 학습했음을 보여줍니다. 사람이 시연한 방식을 그대로 흉내내는 것이 아니라, <strong>시연의 의도</strong>를 이해해 로봇 자신의 방식으로 임무를 달성했다는 점에서 의미가 있습니다.</li>
</ul>
<p><strong>로봇 손 설계 간 비교:</strong></p>
<ul>
<li>저자들은 나아가 <strong>제안한 벤치마크를 활용한 로봇 손 설계 비교 실험</strong>도 수행했습니다.</li>
<li>모든 핸드는 동일한 인간 손 모션 참조를 사용하지만, 정책이 사람의 지침에서 벗어나는 정도는 핸드 크기와 운동학적 제약에 따라 달라집니다.</li>
<li>동일한 네 가지 장기 과제에 대해 서로 다른 로봇 손들이 DexMachina로 학습했을 때의 성능을 비교한 결과, <strong>더 큰 크기이면서 모든 손가락이 능동 구동되는 손</strong>일수록 <strong>학습 효율과 최종 성공률이 높게</strong> 나타났습니다.
<ul>
<li>예를 들어 <strong>Schunk Hand</strong>나 <strong>X-Hand</strong>는 작은 Inspire Hand나 Ability Hand보다 <strong>성공률과 학습속도 모두 우수</strong>했는데, 이는 단순한 크기 차이뿐 아니라 <strong>자유도의 차이</strong>에서 기인한 것으로 분석됩니다.</li>
</ul></li>
<li>크기보다 <strong>자유도(degrees of freedom, DOF)가 더 중요한 성능 결정 요인</strong>임을 발견했습니다.</li>
<li><strong>큰 사이즈와 완전히 작동하는 핸드(fully-actuated hands)</strong>는 학습 효율성과 최종 성능 면에서 뛰어나며, <strong>긴 손가락을 가진 Allegro Hand가 특히 우수한 안정성을 제공</strong>합니다.
<ul>
<li>크기가 비슷한 Inspire, Ability, Schunk 핸드 중에서는 Schunk 핸드가 손가락 끝 부분이 작동하고 접히는 손바닥을 가지고 있어 평균적으로 더 나은 성능을 냈습니다.</li>
</ul></li>
<li>적게 작동하는 핸드들은 인간 손과 더 닮아 보이지만 학습된 전략은 더 크고 기능적인 핸드보다 덜 인간적입니다.</li>
<li>결과적으로 Inspire와 Ability 핸드는 주어진 과제를 수행하기 위해 다른 전략을 자주 선택해야 합니다.</li>
<li><strong>구동 가능한 관절 수(DoF)</strong>가 많은 손은 물체를 다루는 대체 동작을 찾기 쉽기 때문에 학습에 유리하며, 반대로 인간 손 크기에 가깝더라도 <strong>제약이 많은 손</strong>은 학습 난이도가 높았습니다.</li>
<li>이러한 정량적 비교는 DexMachina가 제시한 하나의 중요한 활용 예로, <strong>동일한 알고리즘 아래 여러 로봇 손의 기능적 성능을 객관적으로 평가</strong>할 수 있음을 보여줍니다. 이는 향후 새로운 로봇 핸드 설계시 <strong>어떤 구조가 실제 작업에 유리한지</strong> 가늠하는 데에도 큰 도움을 줄 것으로 기대됩니다.</li>
</ul>
<center>
<img src="../../images/2025-07-30-dexmachina/3.png" width="60%">
</center>
</section>
<section id="기존-연구와의-비교-dexmachina의-기여도와-차별화" class="level2">
<h2 class="anchored" data-anchor-id="기존-연구와의-비교-dexmachina의-기여도와-차별화">3. 기존 연구와의 비교: DexMachina의 기여도와 차별화</h2>
<p>마지막으로, DexMachina를 <strong>유사한 목표를 가진 기존 프레임워크/논문들과 비교</strong>하여 그 기술적 위치를 살펴보겠습니다. 크게 <strong>(a) 기존 리타게팅 기법</strong>, <strong>(b) 시연 기반 학습(RL/IL) 기법</strong>, <strong>(c) 최근 발표된 유사 연구</strong> 세 범주로 나누어 논의합니다.</p>
<p><strong>(a) 전통적 리타게팅 vs.&nbsp;DexMachina:</strong> 과거의 손 동작 <strong>리타게팅 기술</strong>은 주로 <strong>인간 손가락 궤적을 로봇 손으로 대응시켜보는</strong> 수준이었습니다. 예를 들어 VR 장갑이나 모캡으로 인간 손 움직임을 읽어 로봇 손가락 관절로 매핑하는데, 이는 <strong>로봇과 인간의 형태 차이</strong> 때문에 충돌을 일으키거나 물체를 제대로 쥐지 못하는 경우가 많았습니다. Park 등(2025)은 이러한 <strong>기존 end-effector 정렬 기반 리타게팅</strong>이 <strong>비현실적인 동작</strong>을 만들기 쉽다고 지적하며, 인간-로봇-물체 사이의 <strong>공동 모션 manifold</strong>를 학습하여 보다 <strong>플라우저블(plausible)한 로봇 동작</strong>을 얻는 방법을 제시하였습니다. 해당 방법은 대량의 인간 시연 데이터로 <strong>인간-물체-로봇 사이의 관계 모델</strong>을 훈련한 후, 주어진 새로운 시연에 대해 <strong>로봇 행동을 직접 추론</strong>하는 접근으로, 실제 로봇에 실험하여 <strong>기존 단순 매핑보다 성공률 향상</strong>을 시연했습니다. DexMachina 역시 <strong>인간 시연→로봇 동작</strong>이라는 큰 흐름은 같지만, 접근법은 사뭇 다릅니다. DexMachina는 <strong>명시적으로 물리 시뮬레이션 환경에서 RL로 정책을 학습</strong>하므로, 중간에 인간 모델을 로봇으로 변환하는 모션 생성기가 필요한 대신, <strong>강화학습 자체가 모션을 만들어내는 역할</strong>을 합니다. 이로써 한편으로는 <strong>시연이 부족한 상황에서도</strong> (정책이 탐색을 통해) 답을 찾아갈 수 있고, 다른 한편으로는 <strong>학습된 정책이 온라인으로 물체 반응에 대응</strong>할 수 있어 강인성을 얻습니다. 다만 DexMachina는 현재 <strong>시뮬레이션 학습에 집중</strong>하고 있어 즉시 실세계 로봇에 적용되지는 않았는데, Park 등의 접근은 애초에 <strong>실물 로봇 대상 데이터</strong>로 학습/검증되었다는 차이가 있습니다. 요약하면, 기존 리타게팅 기법들이 <strong>“모델을 학습시켜 한 번에 매핑”</strong>하는 경향이라면, DexMachina는 <strong>“시뮬레이터 안에서 직접 배우게”</strong> 하는 방식으로 문제를 푼다고 볼 수 있습니다. 이 과정에서 <strong>커리큘럼을 통한 탐색 Aux</strong>라는 혁신을 추가하여, 인간처럼 <strong>양손 협응이 필요한 복잡한 작업도 실패 없이 학습</strong>하게 만들었다는 점이 두드러집니다.</p>
<p><strong>(b) 시연 기반 강화학습(IL/RL) vs.&nbsp;DexMachina:</strong> 인간 시연을 활용하는 기법들은 <strong>모방학습(IL)</strong>과 <strong>Reward 강화학습(RL)</strong>로 크게 나뉩니다. <strong>모방학습</strong>의 경우 시연 데이터만으로 정책을 모방하게 하는데, 로봇 손의 경우 <strong>정확한 로봇행동-결과 페어 데이터 수집</strong>이 어려워 제한적이었습니다. 이를 극복하고자 <strong>텔레오퍼레이션 장비</strong>(VR 장갑 등)를 이용해 <strong>사람이 로봇 손을 직접 원격조작</strong>하며 데이터를 모은 연구들이 다수 있었으나, 특정 로봇에 시스템을 특화해야 하고 주로 <strong>단순 집기(grasp) 등 짧은 작업</strong>에 머무는 한계가 있었습니다. 반면 DexMachina는 <strong>단 한 번의 인간 시연</strong>만으로도 긴 작업을 배울 수 있도록 했고, <strong>인간-로봇 사이의 자세한 매핑 데이터 없이</strong>도 학습이 가능하다는 점에서 <strong>데이터 효율성</strong>을 보여주었습니다. 한편 <strong>강화학습+시연 혼합</strong> 기법으로 2018년 <strong>DAPG</strong> 등이 인간 시演을 초기 정책으로 활용하고 추가 RL 훈련을 통해 성과를 낸 바 있습니다. 그러나 DAPG는 문 손잡이 돌리기 등 <strong>단일 손의 비교적 짧은 작업</strong>을 대상으로 했고, Reward도 <strong>시연 모션 모방</strong>과 <strong>작업 완료 신호</strong>를 수동 설정하는 식이었습니다. DexMachina는 이러한 선행들의 교훈 – 시연이 주는 <strong>탐색 가이드</strong> 효과 – 을 받아들이면서도, <strong>Reward 신호를 자동으로 구성</strong>하고 <strong>외력 지원으로 탐색 효율을 높이는</strong> 등 여러 개선을 통해 <strong>문제 난이도를 한 단계 끌어올린 사례</strong>라 할 수 있습니다. 특히 <strong>장기간의 연속적인 물체 조작</strong>이라는 새로운 영역에서 <strong>시연+RL</strong>의 위력을 입증한 점은 학술적으로 의미가 큽니다.</p>
<p><strong>(c) 최신 유사 연구들과의 비교:</strong> DexMachina와 같은 시기에 발표된 몇몇 연구들도 <strong>양손 조작 학습</strong>에 도전하고 있어 흥미로운 비교가 됩니다. <strong>ManipTrans (Li et al., CVPR 2025)</strong>는 DexMachina와 동일하게 <strong>인간의 양손 기술을 로봇에 전이</strong>하는 목표를 갖되, <strong>“두 단계”</strong>로 접근한 것이 특징입니다. 먼저 인간 시연을 흉내내는 <strong>트래젝토리 모방 모델</strong>을 학습한 뒤, 이를 기반으로 <strong>잔여 정책(residual policy)</strong>을 강화학습으로 파인튜닝하는 구조입니다. 이러한 <strong>사전학습+미세조정</strong> 방식은 데이터 효율을 높이고 학습을 가속하는 효과가 있어, 저자들은 다양한 시연 데이터로 거대한 <strong>DexManipNet</strong>이라는 로봇 조작 데이터셋까지 구축하였습니다. 결과적으로 ManipTrans 역시 높은 성공률과 정확도를 보였지만, <strong>정책 최종 성능</strong> 측면에서는 DexMachina가 앞서는 것으로 DexMachina 논문 실험에서 확인되었습니다 (위 결과 그래프에서 갈색 막대 비교). 이는 <strong>잔여 학습단계의 제한적 탐색</strong>보다 DexMachina의 <strong>초기부터 끝까지 RL로 최적화하는 접근</strong>이 장기적으로 더 나은 솔루션을 찾았기 때문으로 추정됩니다. 다만 ManipTrans는 시연 모방 모델 덕분에 <strong>3천 개 이상의 다양한 작업 에피소드</strong>를 생성하여 <strong>데이터 다양성</strong>을 확보한 반면, DexMachina는 <strong>각 작업별 한 개의 시연</strong>에 집중합니다. 따라서 <strong>일대일 전이 학습 효율</strong>은 DexMachina가 높지만, <strong>대량의 시연을 일반화</strong>하는 측면은 ManipTrans 쪽이 방향성이 다르다고 볼 수 있습니다. 또 하나 주목할 것은 <strong>실제로봇 적용</strong>인데, ManipTrans 쪽은 시뮬레이션에서 학습한 정책을 실제 로봇으로 이식하는 실험을 시도하고 있습니다. DexMachina는 현재 결과가 모두 시뮬레이터 상이지만, <strong>정밀한 상태 입력에 의존</strong>하고 있어 시각센서 기반 정책으로의 확장은 과제로 남아 있습니다. 저자들도 향후 <strong>비전 기반 RL 정책</strong>이나 <strong>고급 센서 데이터 통합</strong>을 통해 실세계 적용을 모색할 수 있다고 언급하였습니다.</p>
<p>마지막으로, <strong>Videodex (Shaw et al., 2022)</strong>나 <strong>XSkill (Xu et al., 2023)</strong>처럼 <strong>사전 녹화 동영상</strong>이나 <strong>다른 로봇의 경험</strong>으로부터 간접적으로 기술을 학습하는 시도들도 있습니다. Videodex는 웹 비디오로부터 인간의 조작 시퀀스를 추출해 로봇에 학습시켰고, XSkill은 <strong>교차 형태 간(skill transfer across embodiments)</strong> 유용한 스킬을 발견하는 방법을 제안했습니다. 이러한 연구들은 <strong>시연 데이터의 형태</strong>가 다르지만, <strong>로봇이 인간 수준의 다양한 조작을 배우는 방법</strong>을 탐구한다는 점에서 DexMachina와 맥을 같이 합니다. DexMachina의 <strong>가치</strong>는 특히 <strong>고품질의 한정된 시연을 최대한 활용</strong>하여 복잡한 임무를 달성하는 쪽에 있는데, 이는 향후 비디오나 저해상도 데이터에도 응용될 수 있는 통찰을 줍니다. 또한 <strong>AnyTeleop (Qin et al., 2023)</strong> 같은 <strong>원격 조작 시스템</strong>들은 사람의 즉각적인 조작을 로봇으로 투영하여 복잡한 임무를 수행했는데, DexMachina는 <strong>한 걸음 더 나아가</strong> 이러한 인간 개입 없이도 <strong>자율 정책으로 임무를 지속 수행</strong>하게 만들었다는 점에서 <strong>완전자율성</strong>에 한층 가까워졌습니다.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>DexMachina는 <strong>인간 시연으로부터 양손 로봇 조작 기술을 배우는 새로운 방법론</strong>을 제시함으로써, 현재 활발한 <strong>섬세 조작(dexterous manipulation)</strong> 연구 분야에 큰 진전을 가져왔습니다. 방법론적으로 보면, <strong>Reward 설계의 자동화</strong>와 <strong>커리큘럼을 통한 탐색 지원</strong>을 결합하여 <strong>강화학습의 취약점</strong>을 효과적으로 보완한 점이 돋보입니다. 실험적으로는 다양한 로봇 손에 걸쳐 <strong>일관된 성능 우위</strong>를 증명함으로써, 제안 기법의 <strong>범용성</strong>과 <strong>실용적 가치</strong>를 입증했습니다. 특히 하나의 프레임워크로 <strong>하드웨어 성능을 비교 평가</strong>할 수 있다는 관점은, 향후 <strong>로봇 손 개발자들이 디자인 선택을 최적화</strong>하는 데에도 기여할 수 있을 것입니다.</p>
<p>물론 해결해야 할 과제도 남아 있습니다. 앞서 언급했듯이 DexMachina는 <strong>시뮬레이션 상태정보에 크게 의존</strong>하고 있어, 이를 <strong>실세계 센서 입력(시각/촉각)</strong>으로 옮기는 작업이 필요합니다. 또한 현실에서는 예기치 못한 교란이나 물체 모델의 불확실성 등이 존재하므로, 정책이 <strong>오류 복구</strong>나 <strong>적응적 재계획</strong>을 할 수 있도록 강화하는 연구도 중요할 것입니다. <strong>데이터 수집</strong> 측면에서는, 현재는 사람 시연을 별도로 캡쳐해야 하지만, 미래에는 <strong>3D 비전</strong>이나 <strong>모션 캡쳐 자동화</strong> 기술의 발전으로 보다 손쉽게 시연 데이터를 확보할 수 있을 것입니다. 이러한 보완이 이루어진다면 DexMachina의 접근법은 산업 현장이나 서비스 로봇에서 <strong>사람처럼 도구를 다루고 협업하는 로봇</strong>을 훈련하는 데 큰 역할을 할 것으로 기대됩니다.</p>
<p><strong>결론적으로</strong>, DexMachina는 <strong>기능적 리타게팅</strong>이라는 개념을 통해 로봇에게 <strong>“동작의 형태”보다 “동작의 목적”</strong>을 가르치는 법을 보여주었습니다. 이는 향후 인간 수준의 다재다능한 로봇 조작을 실현하는 데 중요한 방향을 제시하며, 현재 진행 중인 많은 후속 연구들의 기반이 되고 있습니다. 인간이 시연하고 로봇이 배워서 자기만의 방식으로 임무를 수행하는 모습은, 궁극적으로 <strong>휴먼-로봇 협업</strong>과 <strong>자율 기술 학습</strong>의 접점에서 매우 유망한 패러다임이라 할 수 있습니다.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<script src="https://utteranc.es/client.js" repo="curieuxjy/blog_comments" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Copyright 2024, Jung Yeon Lee</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/curieuxjy">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>