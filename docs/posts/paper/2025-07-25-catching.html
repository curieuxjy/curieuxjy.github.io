<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-07-25">
<meta name="description" content="Catching, Gaussian mixture model">

<title>📃Catching Objects in Flight 리뷰 – Curieux.JY</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../profile.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-2NVZN2MJZT"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-2NVZN2MJZT', { 'anonymize_ip': true});
</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Curieux.JY</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../post.html"> 
<span class="menu-text">Post</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../note.html"> 
<span class="menu-text">Note</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">Jung Yeon Lee</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/curieuxjy"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#brief-review" id="toc-brief-review" class="nav-link active" data-scroll-target="#brief-review"><span class="header-section-number">1</span> Brief Review</a></li>
  <li><a href="#detail-review" id="toc-detail-review" class="nav-link" data-scroll-target="#detail-review"><span class="header-section-number">2</span> Detail Review</a>
  <ul class="collapse">
  <li><a href="#서론-비행-물체-잡기의-어려움과-연구-목표" id="toc-서론-비행-물체-잡기의-어려움과-연구-목표" class="nav-link" data-scroll-target="#서론-비행-물체-잡기의-어려움과-연구-목표"><span class="header-section-number">2.1</span> 서론: 비행 물체 잡기의 어려움과 연구 목표</a></li>
  <li><a href="#기존-연구와의-차별성" id="toc-기존-연구와의-차별성" class="nav-link" data-scroll-target="#기존-연구와의-차별성"><span class="header-section-number">2.2</span> 기존 연구와의 차별성</a></li>
  <li><a href="#물체-궤적-학습과-예측-dynamics-learning" id="toc-물체-궤적-학습과-예측-dynamics-learning" class="nav-link" data-scroll-target="#물체-궤적-학습과-예측-dynamics-learning"><span class="header-section-number">2.3</span> 물체 궤적 학습과 예측 (Dynamics Learning)</a></li>
  <li><a href="#최적-잡기-자세-결정-가용-공간-파지-공간-모델링" id="toc-최적-잡기-자세-결정-가용-공간-파지-공간-모델링" class="nav-link" data-scroll-target="#최적-잡기-자세-결정-가용-공간-파지-공간-모델링"><span class="header-section-number">2.4</span> 최적 잡기 자세 결정: 가용 공간 &amp; 파지 공간 모델링</a></li>
  <li><a href="#로봇-팔-손-운동-계획과-제어-reactive-arm-hand-control" id="toc-로봇-팔-손-운동-계획과-제어-reactive-arm-hand-control" class="nav-link" data-scroll-target="#로봇-팔-손-운동-계획과-제어-reactive-arm-hand-control"><span class="header-section-number">2.5</span> 로봇 팔-손 운동 계획과 제어 (Reactive Arm-Hand Control)</a></li>
  <li><a href="#실험-결과-및-분석" id="toc-실험-결과-및-분석" class="nav-link" data-scroll-target="#실험-결과-및-분석"><span class="header-section-number">2.6</span> 실험 결과 및 분석</a></li>
  <li><a href="#결론" id="toc-결론" class="nav-link" data-scroll-target="#결론"><span class="header-section-number">2.7</span> 결론</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-page-left" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">📃Catching Objects in Flight 리뷰</h1>
  <div class="quarto-categories">
    <div class="quarto-category">gmm</div>
    <div class="quarto-category">catching</div>
  </div>
  </div>

<div>
  <div class="description">
    Catching, Gaussian mixture model
  </div>
</div>


<div class="quarto-title-meta column-page-left">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">July 25, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<ul>
<li><a href="https://infoscience.epfl.ch/server/api/core/bitstreams/9c2bd9d0-16bd-4e01-b6be-7ee5ab2e133b/content">Paper Link</a></li>
<li><a href="https://youtu.be/FxvVJzb61js?si=uVQtWdehFujEAgxd">Youtube Link</a></li>
</ul>
<ol type="1">
<li>이 논문은 불규칙한 형태의 고속 비행 물체의 궤적을 예측하고 성공적으로 포획하는 로봇 시스템을 제안합니다.</li>
<li>시스템은 시연 학습을 통해 물체 역학 모델과 로봇의 포착 가능 및 도달 가능 공간 모델을 구축하고, 결합 동적 시스템(Coupled Dynamical Systems, CDS)으로 손-팔 협응 동작을 실시간으로 생성합니다.</li>
<li>iCub 시뮬레이션 및 KUKA LWR 4+ 실제 실험에서 해머, 라켓, 병 등 다양한 복잡한 물체들을 성공적으로 포획하여, 불확실성 환경에서의 초고속 로봇 제어 가능성을 입증했습니다.</li>
</ol>
<center>
<img src="../../images/2025-07-25-catching/2.png" width="60%">
</center>
<hr>
<section id="brief-review" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Brief Review</h1>
<p>이 논문은 불규칙한 형태의 비행 물체를 잡는 어려운 문제를 다룹니다. 이 문제는 세 가지 복잡한 하위 문제의 해결을 요구합니다: 빠른 속도로 움직이는 물체의 궤적을 정확하게 예측하는 것, 가능한 잡기 자세(catching configuration)를 예측하는 것, 그리고 로봇 팔의 움직임을 밀리초 단위로 빠르게 계획하는 것입니다.</p>
<p><strong>핵심 방법론</strong></p>
<p>본 연구는 “시범을 통한 프로그래밍(programming-by-demonstration)” 접근 방식을 채택하여 물체 동역학 및 팔 움직임 모델을 학습합니다. 특히, 확률론적 방식으로 실행 가능한 잡기 자세를 찾는 새로운 방법론을 제안하며, 여러 시범으로부터 움직임을 인코딩하기 위해 동역학 시스템(Dynamical Systems, DS) 접근 방식을 사용합니다. 이는 센서 불확실성이 존재하는 상황에서도 팔 움직임을 빠르고 반응적으로 적응시키는 것을 가능하게 합니다.</p>
<p><strong>1. 움직이는 물체의 동역학 학습 (Learning the Dynamics of a Moving Object)</strong></p>
<ul>
<li><strong>문제 정의</strong>: 공기 저항, 불규칙한 형태, 질량 중심(COM)이 아닌 지점에서 잡아야 하는 비강체 또는 불규칙한 물체의 비선형적인 병진 및 회전 운동을 예측하는 것은 매우 어렵습니다. 기존의 강체 역학 모델은 물체의 질량, COM 위치, 관성 모멘트와 같은 물리적 속성 정보를 필요로 하지만, 이들은 임의의 물체에 대해 측정하기 어렵거나 비행 중 변할 수 있습니다(예: 부분적으로 채워진 병).</li>
<li><strong>제안하는 방법</strong>: 저자는 시범을 통해 관찰된 물체의 움직임으로부터 직접 물체의 비선형 동역학을 학습합니다. 물체의 관심 지점(point of interest)의 상태 <span class="math inline">\xi \in \mathbb{R}^D</span> (위치 및 쿼터니언을 사용한 자세)와 그 미분 값 <span class="math inline">\dot{\xi}, \ddot{\xi}</span>를 사용하여 물체의 동역학을 2차 자율 동역학 시스템으로 모델링합니다: <span class="math display">\ddot{\xi} = f(\xi, \dot{\xi})</span></li>
<li><strong>학습 과정</strong>: 미지의 함수 <span class="math inline">f(.)</span>는 서포트 벡터 회귀(Support Vector Regression, SVR)를 사용하여 모델링됩니다. 입력 <span class="math inline">\zeta = [\xi; \dot{\xi}] \in \mathbb{R}^{2 \times D}</span>에 대해 각 차원마다 <span class="math inline">D</span>개의 SVR 모델 <span class="math inline">d f_{SVR}</span>가 학습됩니다. 회귀 추정치는 다음과 같습니다: <span class="math display">\ddot{\xi} = f_{SVR}(\zeta) = [\mathrm{d}f_{SVR}(\zeta)]_{d=1...D}</span>
<ul>
<li>여기서 <span class="math inline">\mathrm{d}f_{SVR}(\zeta) = \sum_{m=1}^M \mathrm{d}\alpha_m K (\zeta, \mathrm{d}\zeta_m) + \mathrm{d}b</span> 입니다.</li>
<li>커널 함수 <span class="math inline">K(\zeta, \zeta_m) = \exp(-\gamma\|\zeta - \zeta_m\|^2)</span>는 방사형 기저 함수(RBF) 커널을 사용합니다. 학습된 모델은 칼만 필터(Extended Kalman Filter, EKF)와 결합되어 잡음이 있는 센싱에 대한 견고성을 확보합니다.</li>
</ul></li>
</ul>
<p><strong>2. 잡기 자세 예측 (Predicting the Catching Configuration)</strong></p>
<p>이 단계는 세 가지 하위 문제로 구성됩니다: 물체를 잡을 수 있는 자세 학습, 로봇의 도달 가능한 공간 모델링, 그리고 이 두 모델을 결합하여 최적의 잡기 자세 및 시간을 결정하는 것입니다. 잡기 자세 <span class="math inline">\eta = [\eta_{pos}; \eta_{ori}]</span>는 엔드-이펙터의 위치 <span class="math inline">\eta_{pos} \in \mathbb{R}^3</span>와 자세 <span class="math inline">\eta_{ori} \in \mathbb{R}^6</span> (회전 행렬의 처음 두 열 벡터로 구성)로 정의됩니다.</p>
<ul>
<li><strong>잡을 수 있는 공간 모델 (Graspable-Space Model)</strong>:
<ul>
<li><strong>학습</strong>: 로봇 핸드를 수동으로 물체에 가져다 대는 시범을 통해 다양한 잡기 자세(위치 및 자세)를 기록합니다. 이 데이터는 물체 좌표계에서 저장됩니다.</li>
<li><strong>모델링</strong>: 기록된 잡기 자세의 밀도 분포는 가우시안 혼합 모델(Gaussian Mixture Model, GMM)을 사용하여 모델링됩니다: <span class="math inline">M_{grasp} = \{\pi_k, \mu_k, \Sigma_k\}_{k=1:K}</span>.</li>
<li><strong>확률 밀도</strong>: 주어진 잡기 자세 <span class="math inline">\eta \in \mathbb{R}^9</span>에 대한 확률 밀도는 다음과 같습니다: <span class="math inline">P(\eta|M_{grasp}) = \sum_{k=1}^K \pi_k \mathcal{N}(\eta|\mu_k, \Sigma_k)</span></li>
<li><strong>실행 가능성</strong>: <span class="math inline">P(\eta|M_{grasp})</span>가 특정 임계값 <span class="math inline">\rho_{grasp}</span>를 초과하면 해당 자세는 실행 가능한 것으로 간주됩니다.</li>
<li><strong>좌표계 변환</strong>: 물체가 움직이므로 물체 좌표계의 모델을 로봇 좌표계로 변환해야 합니다: <span class="math display">\text{robot}\mu_k(t) = \Omega(t)\text{obj}\mu_k + P(t)</span> <span class="math display">\text{robot}\Sigma_k(t) = \Omega(t)\text{obj}\Sigma_k\Omega(t)^T</span>
<ul>
<li>여기서 <span class="math inline">P(t) = \begin{pmatrix} p(t) \\ \text{zeros}(6, 1) \end{pmatrix}</span>이고, <span class="math inline">\Omega(t) = \text{diag}(R(t), R(t), R(t))</span>입니다. <span class="math inline">p(t)</span>와 <span class="math inline">R(t)</span>는 물체 참조 프레임의 위치 벡터와 회전 행렬입니다.</li>
</ul></li>
</ul></li>
<li><strong>도달 가능한 공간 모델 (Reachable-Space Model)</strong>:
<ul>
<li><strong>학습</strong>: 로봇 팔의 모든 관절 변위를 체계적으로 샘플링하여 로봇이 달성할 수 있는 엔드-이펙터 자세(위치 및 자세)를 기록합니다. 이 샘플들도 GMM <span class="math inline">M_{reach} = \{\pi_l, \mu_l, \Sigma_l\}_{l=1:L}</span>로 모델링됩니다.</li>
<li><strong>정확성 및 제약</strong>: GMM의 가우시안 개수는 베이지안 정보 기준(BIC)으로 결정됩니다. 모델의 정확도는 무작위 샘플링을 통해 검증됩니다. 또한, 로봇이 테이블과의 충돌을 피하기 위해 <span class="math inline">z &lt; 0.1 \text{m}</span>인 도달 가능한 샘플은 버려집니다. 자체 충돌을 피하기 위해 보수적인 관절 범위가 설정되며, 손바닥 방향이 지면을 향하는 자세도 제외됩니다.</li>
</ul></li>
<li><strong>최적 잡기 자세 예측 (Predicting Catching Configuration)</strong>:
<ul>
<li><strong>결합 모델</strong>: 잡을 수 있는 공간 모델과 도달 가능한 공간 모델은 통계적으로 독립적이므로, 두 모델의 확률 분포를 곱하여 결합 확률 분포 <span class="math inline">M_{joint}(t)</span>를 계산할 수 있습니다. 결합된 GMM의 각 가우시안 파라미터 <span class="math inline">j=(l-1) \times L + k</span>는 다음과 같습니다: <span class="math inline">\Sigma_j(t) = (\Sigma_k^{-1}(t) + \Sigma_l^{-1})^{-1}</span> <span class="math inline">\mu_j(t) = \Sigma_j(t) (\Sigma_k^{-1}(t)\mu_k(t) + \Sigma_l^{-1}\mu_l)</span> <span class="math inline">\pi_j(t) = \pi_k(t)\eta_{grasp} \cdot \pi_l\eta_{reach} \cdot \mathcal{N}(\mu_k(t)|\mu_l, \Sigma_k(t) + \Sigma_l)</span></li>
<li><strong>최적화</strong>: 예측된 물체 자세에서 최적의 잡기 자세 <span class="math inline">\eta(t)</span>를 찾기 위해, 결합 모델 <span class="math inline">M_{joint}(t)</span>에 대해 경사 상승법(gradient ascent)을 사용하여 <span class="math inline">P(\eta(t)|M_{joint}(t))</span>를 최대화합니다. 초기값은 현재 손 자세에 가장 가까운 가우시안의 중심 <span class="math inline">\mu_j(t)</span>로 설정됩니다.</li>
<li><strong>충돌 회피 휴리스틱</strong>: 손이 물체와 충돌할 위험을 줄이기 위해, 잡기 자세에서 로봇 손바닥의 방향이 물체 속도 방향의 반대여야 한다는 제약 조건을 추가합니다: <span class="math inline">\text{dot}(\dot{\eta}_{pos}(t), \eta_{palm}(t)) &lt; d</span>. 이는 실시간 충돌 회피 계산의 부재를 보완합니다.</li>
</ul></li>
</ul>
<p><strong>3. 팔-손 협응 운동 (Hand–Arm Coordinated Motion)</strong></p>
<ul>
<li><strong>접근 방식</strong>: 엔드-이펙터의 위치 <span class="math inline">\xi_h \in \mathbb{R}^3</span>와 자세 <span class="math inline">\xi_o \in \mathbb{R}^3</span> (스케일된 축-각 표현) 궤적은 동역학 시스템 기반 모델을 사용하여 모델링됩니다. 이는 안정적인 동역학 시스템 추정기(Stable Estimator of Dynamical System, SEDS) 기술로 학습됩니다. 손-손가락 연동은 결합 동역학 시스템(Coupled Dynamical Systems, CDS) 모델을 사용하여 모델링됩니다.</li>
<li><strong>CDS 모델</strong>: CDS는 세 가지 GMM으로 학습된 독립적인 동역학 시스템으로 구성됩니다:
<ol type="1">
<li><strong>마스터 서브시스템</strong>: 손 이동의 동역학을 인코딩하는 <span class="math inline">P(\xi_h, \dot{\xi}_h|\theta_h)</span>.</li>
<li><strong>슬레이브 서브시스템</strong>: 손가락 움직임의 동역학을 인코딩하는 <span class="math inline">P(\xi_f, \dot{\xi}_f|\theta_f)</span>.</li>
<li><strong>추론 서브시스템</strong>: 손가락의 추론된 상태와 현재 손 위치의 결합 확률 분포를 인코딩하는 <span class="math inline">P(\Psi(\xi_h), \xi_f|\theta_{inf})</span>. 여기서 <span class="math inline">\Psi(\xi_h)</span>는 손의 현재 상태를 손가락 움직임에 연결하는 결합 함수입니다.</li>
</ol></li>
<li><strong>궤적 생성 및 타이밍 제어</strong>: 잡기 동작을 실행하는 동안, 마스터 DS는 엔드-이펙터 속도 명령을 생성하며, 이는 스칼라 부스트 인자로 지속적으로 크기가 보정되어 로봇이 원하는 시간에 잡기 자세에 도달하도록 합니다. 손가락 관절은 추론된 (원하는) 상태로 움직이는 데 필요한 속도가 가우시안 혼합 회귀(Gaussian mixture regression)를 통해 생성됩니다. Cartesian 공간의 궤적은 역기구학(Inverse Kinematics, IK)을 사용하여 7-자유도 관절 상태로 변환됩니다.</li>
</ul>
<p><strong>실험 검증 (Empirical Validation)</strong></p>
<p>제안된 시스템의 성능을 평가하기 위해 두 가지 실험을 수행했습니다: iCub 시뮬레이터와 실제 KUKA LWR 4+ 로봇(Allegro Hand 장착).</p>
<ul>
<li><strong>iCub 시뮬레이션</strong>: 해머와 테니스 라켓을 사용하여 20번의 던지기 시범으로 동역학 모델을 학습했습니다. 무작위 초기 위치, 속도, 각속도로 50번의 던지기를 수행하여 100%의 성공률을 기록했습니다 (도달 가능한 공간 밖으로 나간 3번의 시도는 제외).</li>
<li><strong>실제 KUKA LWR 4+ 로봇</strong>: 빈 병, 부분적으로 채워진 병, 테니스 라켓, 골판지 상자 등 네 가지 물체를 사용했습니다. 각 물체는 20번 던져져 동역학 모델 학습에 사용되었고, 옵티트랙(Optitrack) 모션 캡처 시스템으로 물체와 로봇 핸드의 자세를 240Hz로 기록했습니다. 물체의 동역학 모델은 SVR-RBF로 학습되었으며, 부분적으로 채워진 병에 대한 강체 역학 모델과의 비교에서 SVR-RBF가 훨씬 높은 예측 정확도를 보였습니다. 손-팔 협응 모델은 키네스틱 시범(kinesthetic demonstrations)과 인간이 물체를 잡는 시범(모션 캡처 및 5DT 데이터 글러브 사용)을 통해 학습되었습니다.
<ul>
<li><strong>성공률</strong>: 총 80번의 시도 중 로봇의 도달 가능한 공간에 들어오지 않은 9번의 시도를 제외한 71번의 시도에서 52번 성공하여 73.2%의 성공률을 보였습니다.</li>
<li><strong>실패 원인</strong>:
<ol type="1">
<li>IK 해법의 실행 불가능성: 목표 관절 자세가 초기 자세와 너무 멀어 로봇이 제시간에 도달할 수 없는 경우 (19번의 실패 중 12번).</li>
<li>손가락이 물체를 쳐서 튕겨내는 경우 (19번의 실패 중 4번).</li>
<li>관절 토크 한계 위반으로 로봇이 자동으로 멈추는 경우 (19번의 실패 중 3번).</li>
</ol></li>
<li><strong>인간과의 비교</strong>: 동일한 제약 조건에서 인간은 100번의 시도 중 38번 성공하여 38%의 성공률을 보였으며, 이는 로봇보다 현저히 낮은 수치입니다.</li>
</ul></li>
</ul>
<p><strong>결론 및 한계</strong></p>
<p>본 논문은 불규칙한 질량 분포와 비강체 질량 분포를 가진 비행 물체를 잡는 로봇 프레임워크를 제시했습니다. 이 프레임워크는 인간 시범을 통한 학습과 탐색을 통해 프로세스의 각 단계를 로봇이 습득하도록 합니다. 이는 두 가지 다른 로봇 플랫폼(iCub 시뮬레이터, KUKA LWR 4+)과 복잡한 동역학을 가진 네 가지 물체에 대해 검증되었습니다. 제안된 방법은 기존의 연구보다 계산 속도가 매우 빠르며, 복잡한 물체에 대한 높은 성공률을 보였습니다.</p>
<p><strong>한계 및 향후 연구</strong>:</p>
<ul>
<li>SVR-RBF 기반 동역학 모델은 학습된 상태 공간에서 멀리 떨어진 영역에서는 예측 정확도가 떨어질 수 있습니다 (국소적인 일반화).</li>
<li>명시적인 실시간 충돌 회피가 없으며, 손바닥 방향 휴리스틱에 의존합니다. 이는 일부 실패의 원인이 됩니다.</li>
<li>잡을 수 있는 모델은 주로 파워 그립에 중점을 두었으며, 정밀 그립은 다루지 않았습니다.</li>
<li>로봇의 동역학(예: 관절 속도, 토크 한계)이 모델링되지 않아, 생성된 궤적이 로봇에게 실행 불가능할 수 있습니다. 향후 연구에서는 최적 제어를 통해 로봇 동역학을 만족하는 실행 가능한 궤적을 생성하여 CDS 모델을 보강하는 방향이 고려될 수 있습니다.</li>
</ul>
<p>이러한 문제점들은 향후 연구에서 다루어질 과제로 남아있습니다.</p>
<hr>
</section>
<section id="detail-review" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Detail Review</h1>
<section id="서론-비행-물체-잡기의-어려움과-연구-목표" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="서론-비행-물체-잡기의-어려움과-연구-목표"><span class="header-section-number">2.1</span> 서론: 비행 물체 잡기의 어려움과 연구 목표</h2>
<p>고속으로 비행하는 물체를 로봇이 공중에서 잡아내는 것은 로봇공학에서 매우 도전적인 과제입니다. 이 작업을 위해서는 <strong>(1)</strong> 빠르게 움직이는 물체의 <strong>궤적</strong>을 정확히 <strong>예측</strong>하고, <strong>(2)</strong> 물체를 잡기 위한 <strong>최적의 로봇 팔-손 자세</strong>(잡기 구성)를 찾아내며, <strong>(3)</strong> 제한된 시간 내에 로봇 <strong>팔의 궤적을 계획</strong>하여 목표 지점에 도달하도록 제어하는 것, 이 세 가지 문제를 모두 풀어야 합니다. 이러한 모든 처리를 <strong>밀리초(ms) 단위의 시간 안에</strong> 수행해야 한다는 점에서 기술적 난도가 매우 높습니다. 특히 공중에서 잡으려는 대상이 <strong>망치, 라켓, 병</strong>과 같이 무게 중심이 한쪽으로 치우치거나 비대칭인 물체인 경우, 단순한 포물선 모델로는 운동을 설명하기 어렵고 물체의 <strong>자세(orientation)</strong>까지 고려해야 하므로 문제가 더욱 복잡해집니다.</p>
<p>이 논문의 저자들은 이러한 문제를 해결하기 위해 <strong>프로그래밍-바이-데몬스트레이션(programming by demonstration)</strong> 접근을 활용하였습니다. 즉, 사람이나 로봇이 물체를 던지고 잡는 <strong>시범 데이터</strong>로부터 학습함으로써, 물체의 동특성과 로봇 팔의 움직임을 모델링하였습니다. 이를 통해 물체 비행 궤적의 <strong>학습 기반 예측 모델</strong>을 얻고, 확률적인 방법으로 로봇의 <strong>잡기 자세</strong>를 결정하며, <strong>동적 시스템(Dynamical System, DS)</strong> 기반의 제어기로 로봇 팔을 빠르고 유연하게 움직이도록 하였습니다. 이러한 통합 프레임워크를 통해 <strong>센서 노이즈나 예기치 않은 변화에도 실시간으로 대응</strong>하면서 로봇 팔이 공중의 물체를 잡을 수 있도록 하였습니다. 본 리뷰에서는 해당 논문의 <strong>기술적 기여</strong>를 중심으로, 제안된 <strong>방법론</strong>(알고리즘, 실험 설정, 하드웨어/소프트웨어 구성, 데이터 흐름 등)을 상세히 분석하고, 기존 연구들과의 <strong>차별점</strong>을 함께 논의합니다.</p>
<center>
<img src="../../images/2025-07-25-catching/1.png" width="50%">
</center>
<blockquote class="blockquote">
<p><em>그림 1: KUKA LWR 4+ 로봇 팔과 Allegro 로봇 손을 이용해 공중에 던져진 우유병 형태의 물체를 잡는 장면. 해당 연구에서 로봇은 테니스 라켓, 망치, 빈 병, 내용물이 일부 든 병, 박스 등 다양한 형태의 물체를 0.5초 이내의 비행 시간 동안 추적하여 잡아낼 수 있음을 입증하였다.</em></p>
</blockquote>
</section>
<section id="기존-연구와의-차별성" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="기존-연구와의-차별성"><span class="header-section-number">2.2</span> 기존 연구와의 차별성</h2>
<p>고속 비행 물체 포획에 관한 선행 연구들은 주로 <strong>이론적 모델</strong>이나 <strong>사전 정의된 궤적</strong>에 기반한 접근을 취해왔습니다. 예를 들어, 공을 잡는 문제에서는 물체의 궤적을 <strong>포물선(탄도)</strong>으로 가정하고 <strong>최소자승</strong> 등의 기법으로 실시간 추정하거나, <strong>공기 저항을 포함한 탄도 모델</strong>에 <strong>확장 칼만 필터(EKF)</strong>를 결합하여 궤적을 예측하는 시도가 있었습니다. 이러한 방법들은 <strong>물체의 동역학 모델이 정확히 알려져 있는 경우</strong>에는 궤적 추정 정확도가 높았지만, <strong>모델에 대한 사전 지식</strong>(예: 물체의 질량, 무게중심 위치, 관성 모멘트 등)이 필요하고, 주로 <strong>구 형태의 물체(공)</strong>를 전제로 <strong>질점(COM) 궤적</strong>만 추적하기 때문에 망치나 라켓처럼 <strong>복잡한 형태</strong>의 물체에는 적용하기 어렵다는 한계가 있습니다. 반면 이 논문에서는 <strong>임의 형태의 물체</strong>에도 적용 가능하도록, <strong>데이터 기반으로 물체의 운동을 학습</strong>하여 모델을 구축함으로써 사전에 물리적 매개변수를 알 필요 없이 궤적을 예측할 수 있게 했습니다. 실제로 저자들의 이전 연구에서는 사람의 던지기 시演으로부터 <strong>물체 운동의 동역학</strong>을 학습하여, 물체의 질량이나 관성 등의 정보 없이도 <strong>6자유도(위치+자세)</strong> 궤적을 예측하는 <strong>자율 동적 시스템 모델</strong>을 구현한 바 있습니다.</p>
<p>또 다른 차별점은 <strong>실시간성</strong>과 <strong>불확실성 대응</strong>입니다. 과거의 여러 로봇 catching 연구들은 사전에 계산된 <strong>시간-의존 궤적</strong>(예: 다항식 보간, 최소 가속도 궤적 등)으로 로봇을 움직이도록 했기 때문에, 일단 궤적 생성 후에 <strong>시간상의 교란</strong>이나 목표 위치의 변화가 생기면 대응하기 어려웠습니다. 즉, 잡기 동작이 시작된 후에 물체의 운동에 변화가 생겨도 로봇 궤적을 <strong>즉각 재계산</strong>하기 어렵다는 문제입니다. 본 연구에서는 이러한 한계를 극복하기 위해 <strong>시간-불변적 동적 시스템</strong>을 기반으로 한 <strong>반응형 제어기</strong>를 사용하고, 여기에 <strong>시간 조율 기법</strong>(fast-forward integration &amp; scaling)을 적용하여 <strong>잡는 순간에 정확히 도달</strong>하도록 궤적을 실시간으로 조절합니다. 그 결과, 예측된 catching 자세가 수시로 변경되어도 로봇 팔의 DS 제어 궤적을 그에 맞게 <strong>고속 재계획(약 10ms 주기)</strong>할 수 있어, 비행 중 발생하는 <strong>센서 지연이나 잡기 시점 변화</strong>에도 대응할 수 있습니다. 특히 논문에서는 제안한 확률적 모델 덕분에 <strong>전체 비행 시간이 0.7초 이하인 경우에도</strong> 로봇이 물체를 잡을 수 있었고, <strong>최적 잡기 자세 계산에는 0.2ms</strong> 정도밖에 걸리지 않았다고 보고합니다. 이는 이전 연구들과 비교해 <strong>압도적으로 빠른 계산 및 적응 속도</strong>를 보여주는 것으로, 과거에는 이 정도 속도의 궤적 재계획을 위해 32코어 병렬컴퓨팅을 활용한 복잡한 최적화 기법을 써야 했던 사례도 있었던 것과 대비됩니다.</p>
<p>마지막으로, <strong>로봇 손가락 제어</strong> 측면에서도 본 연구는 기존 방식과 차별됩니다. 전통적으로 로봇이 물체를 잡을 때는 <strong>엔드이펙터(손바닥)</strong>와 물체 사이 거리가 일정 임계값 이하로 좁혀지는 순간 손가락을 닫는 간단한 <strong>트리거 방식</strong>이 주로 사용되었습니다. 이러한 방식은 손가락 닫는 타이밍, 속도 등의 매개변수를 사람이 튜닝해야 하고 물체의 접근 방향이나 속도 변화에 따라 최적의 타이밍을 보장하기 어렵습니다. 반면 이 논문에서는 <strong>팔과 손가락을 하나의 연합된 동적 시스템(CDS)</strong>으로 모델링하여, <strong>팔의 운동 궤적과 손가락의 닫힘 동작이 자연스럽게 동기화</strong>되도록 만들었습니다. 즉, 팔의 움직임 상태에 따라 손가락 동작이 <strong>자동으로 결정</strong>되므로, 별도의 임계값 트리거 없이도 물체가 손바닥에 닿는 정확한 순간에 손가락이 닫히게 됩니다. 이는 팔 운동이 예측과 달리 <strong>지연되거나 가속</strong>되더라도 손가락 타이밍이 함께 조절되므로, 물체를 <strong>흘리거나 튕겨내지 않고</strong> 안정적으로 잡을 확률을 높여줍니다. 이러한 <strong>팔-손 연합 제어</strong> 기법은 저자들의 선행 연구에서 제시된 것으로, 본 논문에서는 이를 실제 <strong>7자유도 KUKA 팔과 4-손가락 Allegro Hand</strong> 플랫폼에 적용하여 유효성을 검증하였습니다.</p>
</section>
<section id="물체-궤적-학습과-예측-dynamics-learning" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="물체-궤적-학습과-예측-dynamics-learning"><span class="header-section-number">2.3</span> 물체 궤적 학습과 예측 (Dynamics Learning)</h2>
<p>이 논문에서는 비행 물체의 궤적을 모델링하기 위해 <strong>비선형 동역학 모델</strong>을 <strong>학습</strong>하는 접근법을 사용했습니다. 물체의 상태를 <strong><span class="math inline">\xi = [p,; q]</span></strong>로 정의하는데, 여기서 <strong><span class="math inline">p</span></strong>는 물체에 고정된 특정 <strong>관심 지점(point of interest)</strong>의 3차원 위치이고, <strong><span class="math inline">q</span></strong>는 물체의 <strong>자세(orientation)</strong>를 나타내는 사원수(quaternion)입니다. 물체의 운동 방정식은 2차 자율 동적 시스템(이차 미분방정식)으로 가정되며, 형태는 다음과 같습니다:</p>
<ul>
<li><strong><span class="math inline">\ddot{\xi} = f(\xi,; \dot{\xi})</span></strong>, 즉 가속도 <span class="math inline">\ddot{\xi}</span>가 현재의 위치와 속도 <span class="math inline">(\xi, \dot{\xi})</span>의 함수로 표현됩니다. 이때 <span class="math inline">f(\cdot)</span>는 <strong>비선형 함수</strong>로서 명시적 수식을 알 수 없으므로, <strong>머신러닝 기반 회귀</strong>로 근사하게 됩니다.</li>
</ul>
<p>저자들은 <strong>서포트 벡터 회귀(Support Vector Regression, SVR)</strong> 기법을 활용하여 이 함수를 학습시켰습니다. <span class="math inline">N</span>개의 던지기 실험으로부터 얻은 물체 궤적 데이터(각 궤적에 시간순으로 <span class="math inline">T</span>개의 상태 데이터 포함)를 사용하여, 입력 <strong><span class="math inline">\zeta = [\xi; \dot{\xi}]</span></strong> (위치+자세와 그 미분을 이어붙인 벡터)을 받고 출력 <strong><span class="math inline">\ddot{\xi}</span></strong> (가속도)을 예측하는 SVR 모델을 훈련합니다. 각 출력 차원(<span class="math inline">\xi</span>의 각 성분)에 대해 별도의 SVR을 학습하여 총 <span class="math inline">D</span>개의 회귀 모델 <span class="math inline">f_{SVR}^d</span>를 얻으며, <strong>RBF 커널</strong>을 사용하여 비선형 특성을 모델링했습니다. 최적의 커널 매개변수 및 정규화 파라미터 <span class="math inline">C</span>는 교차검증(grid search)을 통해 결정되었고, 학습 완료 후 SVR 모델은 다음과 같은 회귀 함수를 제공합니다:</p>
<p><span class="math display">
\hat{\ddot{\xi}} = f_{SVR}(\zeta) = \sum_{m=1}^{M} \alpha_m K(\zeta, \zeta_m) + b,
</span></p>
<p>여기서 <span class="math inline">M</span>은 <strong>서포트 벡터의 수</strong>, <span class="math inline">\alpha_m</span>은 학습된 계수, <span class="math inline">K</span>는 RBF 커널 함수입니다. 이 회귀식이 곧 물체 운동 방정식 <span class="math inline">f(\cdot)</span>에 대한 근사이며, 임의의 현재 상태 <span class="math inline">(\xi, \dot{\xi})</span>를 넣으면 다음 순간의 가속도를 예측할 수 있습니다.</p>
<p>이렇게 학습된 <strong>물체 동역학 모델</strong>을 이용하면, 로봇은 비행 물체의 현재 <strong>상태 측정값</strong>(위치, 자세, 속도)을 입력하여 미래의 궤적을 <strong>실시간으로 예측</strong>할 수 있습니다. 논문에서는 이 예측 모듈을 <strong>주기적으로 갱신</strong>하도록 설계하였는데, 카메라 등의 센서로부터 새로운 물체 위치/자세 데이터가 들어올 때마다 SVR 모델로 <em>남은 비행 경로</em>를 빠르게 계산합니다. 또한 앞서 언급한 대로 예측 과정에 <strong>EKF 기반의 보정</strong>을 결합하여, 센서 노이즈나 외란으로 인한 불확실성을 줄였습니다. 요컨대, <strong>SVR 회귀</strong>가 이상적인 운동을 내다보는 역할을 하고, <strong>EKF 추정기</strong>가 새로운 관측값으로 그 예측을 미세 조정함으로써, <strong>노이즈에 강인하면서도 빠른 궤적 예측</strong>을 구현한 것입니다.</p>
<p>저자들은 물체 동역학 모델을 학습하기 위해 <strong>다양한 초기 조건으로 물체를 던지는 실험</strong>을 수행하여 데이터셋을 구축했습니다. 예를 들어, 시뮬레이션 상의 iCub 로봇을 이용해 망치와 테니스공을 20회씩 던져 서로 다른 궤적을 만들고, 실제 실험에서도 빈 병, 물이 절반 든 병, 라켓, 상자 등을 각각 여러 차례 던져 데이터로 활용했습니다. 각 던지기 궤적은 <strong>최대 100Hz</strong>로 기록되었고, 사전에 <strong>저역 통과 필터(Butterworth, 25Hz)</strong>로 노이즈를 제거한 후 <strong>속도와 가속도</strong>를 계산하여 학습에 사용했습니다. 이렇게 수집한 약 <strong>20개 내외의 궤적 데이터</strong>로 SVR을 훈련한 결과, 불과 수십 개의 시연만으로도 물체 운동의 비선형 패턴을 학습할 수 있었습니다. 학습된 모델은 <strong>텍스트 파일로 저장</strong>하여 실시간 실행 시 불러오도록 구성하였으며, C++ 기반 제어 소프트웨어에서 해당 모델을 활용해 매 주기마다 물체의 <strong>다음 위치와 자세를 예측</strong>하게 됩니다.</p>
<p>예측 성능 평가를 위해 진행된 시뮬레이션 실험에서, 제안한 SVR-RBF 기반 모델은 <strong>매우 정확한 궤적 예측</strong>을 보여주었습니다. iCub 시뮬레이터 상에서 망치와 라켓을 무작위 초기조건으로 50회 던져본 결과, 그 중 <strong>로봇 작업공간 내로 들어온 시도 47회</strong>에 대해 로봇이 <strong>100%의 성공률</strong>로 물체를 잡았습니다. (나머지 3회는 물체가 로봇이 닿을 수 없는 범위를 벗어나 아예 잡을 수 없는 경우였습니다.) 특히 시뮬레이션에서는 공기저항이나 센서 오차 등의 <strong>불확실성이 없었기 때문에</strong>, 처음 몇 프레임의 관측만으로도 궤적 예측이 실제 궤적에 거의 수렴하였고 예측 오차도 매우 작았습니다. 한편, <strong>부분적으로 물이 든 병</strong>과 같이 <strong>질량 분포가 변화하는</strong> 복잡한 물체의 경우, SVR로 학습한 모델과 전통적인 <strong>강체 동역학 모델</strong>의 예측 성능을 비교한 결과가 흥미롭습니다. 강체 모델을 위해 병의 초기 무게중심과 질량, 관성 등을 계측하여 포물선+회전 운동 방정식을 구성해 보았으나, <strong>SVR 학습 모델은 실제 궤적을 정확히 맞춘 반면</strong>, 강체 모델은 <strong>특히 물체의 회전(자세) 예측에서 큰 오차</strong>를 보였습니다. 논문에 제시된 부분채워진 물병의 궤적 그래프(Fig. 13)를 보면, SVR 기반 추정은 물체의 복잡한 운동(비선형 공중회전까지 포함)을 잘 따라가는 반면, 강체 역학 모델은 시간 경과에 따라 예측 자세가 실제와 동떨어져 결국 <strong>물체를 놓치는 상황</strong>으로 이어짐을 알 수 있습니다. 이 결과는 <strong>사전 모델링 없이 데이터로부터 학습한 접근법의 우수성</strong>을 보여주는 사례로, 특히 <strong>물체의 물리적 특성이 불완전하거나 변화하는 상황</strong>에서 유용함을 시사합니다.</p>
</section>
<section id="최적-잡기-자세-결정-가용-공간-파지-공간-모델링" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="최적-잡기-자세-결정-가용-공간-파지-공간-모델링"><span class="header-section-number">2.4</span> 최적 잡기 자세 결정: 가용 공간 &amp; 파지 공간 모델링</h2>
<p>비행 물체를 잡기 위해서는 로봇 팔이 <strong>어떤 위치</strong>에서 <strong>어떤 자세</strong>로 물체를 포착할 것인지 결정해야 합니다. 일반적인 로봇공학 용어로, 이는 로봇의 <strong>작업 공간</strong>(reachable space)과 물체의 <strong>파지(grasp) 가능한 자세 공간</strong>이 <strong>교집합</strong>을 이루는 지점을 찾는 문제로 볼 수 있습니다. 본 논문에서는 이 문제를 풀기 위해 <strong>로봇의 가용 공간 모델</strong>과 <strong>물체의 파지 공간 모델</strong>을 <strong>확률적 방식으로 학습</strong>하고, 두 모델을 결합하여 <strong>실시간으로 최적의 잡기 자세(인터셉트 지점)</strong>를 결정하는 새로운 방법론을 제안했습니다.</p>
<p>먼저 <strong>로봇의 가용 공간(reachable space)</strong> 모델링부터 살펴보겠습니다. 로봇 팔이 <strong>도달 가능한 모든 손끝 자세(위치+방향)</strong>의 집합은 일반적으로 6차원(SE(3)) 공간에서 매우 복잡한 형태의 영역을 이룹니다. 이를 분석하기 위해 과거 연구들에서는 로봇의 작업공간 경계를 기하학적으로 해석하거나, 격자화된 3D 공간의 점들에 대해 <strong>역기구학(IK)</strong>으로 도달 가능 여부를 일일이 검사하여 <strong>데이터베이스</strong>를 만드는 접근이 시도되었습니다. 그러나 이러한 방법들은 <strong>불연속적(discrete)</strong>인 근사만 제공하거나 로봇 종류마다 개별 설계가 필요하며, 실시간으로 활용하기 어렵다는 단점이 있습니다. 이에 반해 본 논문에서는 <strong>로봇의 6D 가용 공간을 하나의 확률분포로 모델링</strong>하는 아이디어를 취했습니다. 구체적으로, 로봇이 취할 수 있는 <strong>손끝의 위치 및 방향 데이터를 양성 표본</strong>(positive examples)으로 수집한 뒤, 이를 학습시켜 <strong>확률 밀도 함수</strong>로 표현합니다. 저자들은 이러한 <strong>one-class 분류</strong> 접근법을 사용하면, 오로지 도달 가능한 경우의 데이터만으로도 로봇 작업공간을 충분히 묘사할 수 있고, 도달 불가능한 영역에 대해서는 모델의 밀도 값이 자연스럽게 0에 가까워져 <strong>거짓 양성(false positive)</strong>을 거의 내지 않는다고 설명합니다.</p>
<p>가용 공간 모델 학습을 위한 데이터 획득은 두 가지 방식으로 이루어졌습니다. 첫째, <strong>인간 조작을 통한 로봇 팔 시범</strong>입니다. 연구진은 KUKA LWR 4+ 로봇팔을 사람이 직접 이끌어(kinesthetic teaching) 다양한 위치로 움직이는 <strong>시범 조작</strong>을 수행하여, 로봇이 닿을 수 있는 끝부분 자세들을 수집했습니다. 특히 <strong>로봇이 매번 동일한 초기 자세</strong>(대기자세)에서 출발하여, <strong>작업공간 내 여러 지점</strong>을 향해 팔을 뻗는 약 20개의 데모를 진행함으로써, 로봇이 <strong>어디까지 팔을 뻗을 수 있는지</strong> 다양한 방향으로 데이터를 얻었습니다. 각 데모의 말단 자세(손끝 위치와 자세)를 모으면 로봇 reachable space 경계 부근의 표본을 얻을 수 있는데, 연구진은 <strong>20개 정도의 데모로도 충분히 안정적인 작업공간 모델을 학습</strong>할 수 있었다고 합니다. 둘째, 이러한 인간 시범 외에 <strong>알고리즘적 표본 생성</strong>도 활용되었습니다. 예를 들어 로봇의 관절공간을 임의로 샘플링하여 손끝의 6D 자세를 계산하고, 자가 충돌이나 특이점 등을 걸러낸 <strong>유효 자세들</strong>을 모을 수도 있습니다. 논문에서는 자세한 구현을 밝히진 않았으나, 언급된 Section II-B2의 방법으로 LWR 4+ 로봇의 reachable space를 추가로 모델링했다고 되어 있습니다. 결과적으로 이렇게 수집된 <strong>수천 개 이상의 손끝 자세 데이터</strong>에 대해 <strong>가우시안 혼합 모델(GMM)</strong>을 학습함으로써, 로봇 팔이 <strong>어떤 위치</strong>에서 <strong>어떤 방향</strong>으로 손목을 향하게 할 수 있는지에 대한 <strong>확률 모델</strong>을 만들었습니다.</p>
<center>
<img src="../../images/2025-07-25-catching/3.png" width="100%">
</center>
<p>예를 들어, 논문에 제시된 Fig. 7은 7-DOF LWR 로봇의 reachable space를 나타낸 그림으로, 로봇 손끝의 3차원 위치를 둘러싼 등고선(plots)과 대응 가능한 손목 방향의 분포를 시각화하고 있습니다. 이 모델을 이용하면, <strong>어떤 3D 위치가 로봇으로 reachable 한지</strong>를 밀도 값으로 판단할 수 있을 뿐 아니라, <strong>특정 위치에서 로봇이 취할 수 있는 최적 손목 방향</strong>이 무엇인지도 <strong>조건부 확률</strong> 질의로 얻을 수 있습니다. 즉, 학습된 GMM으로부터 “만약 손끝 위치가 (x,y,z)라면, 로봇 손목은 어떤 방향 쪽으로 향할 수 있는가?”를 질의하면 가장 가능도 높은 방향(orientation)을 계산할 수 있고, 반대로 “손목 방향을 특정 각도로 정하면 reachable 위치가 어디인가?”도 추론할 수 있는 것입니다.</p>
<p>한편, <strong>물체의 파지 공간(graspable space)</strong> 모델링은 물체를 <strong>어떤 자세로 쥘 수 있는지</strong>를 확률적으로 표현한 것입니다. 로봇이 공중에서 물체를 잡는 경우, 아무 위치나 잡는 것이 아니라 <strong>정해진 부분</strong>(예: 라켓의 손잡이, 망치의 손잡이 부분 등)을 잡아야 하며, 또 로봇 손가락이 물체를 둘러싸는 <strong>방향</strong>도 제한됩니다. 따라서 물체마다 <strong>잡기 좋게 손을 가져갈 수 있는 위치와 자세의 집합</strong>이 존재하며, 이를 Graspable space라 정의합니다. 이 또한 6차원 공간(손끝의 상대적인 위치+자세)에서 복잡한 분포를 가지므로, <strong>시범 학습</strong>을 통해 모델링하였습니다. 연구진은 각 물체에 대해 <strong>인간이 직접 로봇 팔과 손을 움직여서 물체를 잡는 시범</strong>을 제공했고, 이 과정에서 <strong>물체와 로봇 손의 상대적인 위치/자세</strong>를 캡쳐하여 데이터로 활용했습니다. 예를 들어 Allegro Hand로 병을 잡는 파지 공간을 학습하기 위해, 사람 연구자가 로봇 팔을 수동으로 이끌어 약 15초 동안 여러 각도에서 병을 쥐어보는 시범을 보였습니다. 이 동안 <strong>240Hz</strong>로 모션 캡쳐 시스템이 <strong>물체의 좌표</strong>와 <strong>로봇 손끝의 좌표</strong>를 동기화하여 기록하였고, 총 약 3600개의 순간 자세 표본을 얻었습니다. 이렇게 모인 데이터 중 <strong>대표적인 300개 표본</strong>을 무작위 추출하여 GMM 학습에 사용함으로써, <strong>해당 물체를 잡을 수 있는 손 위치/방향의 확률 모델</strong>을 구축했습니다. Fig. 4에 그 예시가 제시되어 있는데, (a)는 로봇 손, (b)는 시범 과정, (c)는 시범으로 얻은 파지 자세 표본 분포, (d)는 GMM으로 모델링한 후 특정 위치에서의 손목방향 가능도 분포 등으로 구성되어 있습니다. 이를 통해 물체에 대해 “어떤 <strong>상대적 자세</strong>로 로봇 손을 가져가면 잡을 수 있다”를 확률적으로 표현할 수 있게 되었습니다.</p>
<p>가용 공간 모델과 파지 공간 모델이 모두 준비되었다면, 이제 <strong>비행 물체를 언제 어디서 잡을지</strong> 결정하는 일만 남습니다. 저자들은 두 모델을 <strong>통합적으로 활용</strong>하여 <strong>최적의 catching 자세와 시점</strong>을 산출하는 알고리즘을 구현했습니다. 개략적인 과정은 다음과 같습니다. 우선 앞서 설명한 <strong>SVR 기반 물체 궤적 예측 모듈</strong>을 통해, 물체가 앞으로 비행할 <strong>경로(연속적인 위치와 자세의 시간 함수)</strong>를 예측합니다. 이 예측 궤적 상에서 일정한 시간 간격으로 <strong>후보 포인트</strong>(미래 시각 <span class="math inline">t_i</span>에서의 물체 자세)를 샘플링합니다. 그런 다음 각 후보 <span class="math inline">(p_i, q_i)</span>에 대해 <strong>로봇이 그 지점에서 잡을 수 있는지</strong>를 평가하는데, 이것이 바로 reachable-space 모델과 graspable-space 모델의 결합 판단입니다. 구체적으로, 후보 물체 자세 <span class="math inline">(p_i, q_i)</span>가 주어지면 우선 로봇 <strong>가용 공간 모델</strong>을 이용해 그 위치 <span class="math inline">p_i</span>에서 로봇 손끝이 취할 수 있는 방향 분포를 계산할 수 있습니다. 동시에 <strong>파지 공간 모델</strong>로부터는 물체가 자세 <span class="math inline">q_i</span>일 때 로봇 손이 취해야 할 상대 방향을 얻습니다. 이 둘이 <strong>일치</strong>한다면 (즉, 물체의 그 자세에서 로봇이 손을 넣어 잡을 수 있는 경우), 해당 지점은 <strong>유효한 catching 자세</strong>가 됩니다. 논문에서는 이 과정을 더욱 효율화하기 위해, 아예 reachable-space와 graspable-space를 <strong>하나의 결합 확률모델</strong> <span class="math inline">M_{\text{joint}}</span>로 간주하여, 특정 물체 자세에 대한 <strong>공동 확률밀도</strong>를 계산하는 방법을 취했습니다. 예를 들어 “시간 <span class="math inline">t_i</span>에 물체가 자세 <span class="math inline">(p_i, q_i)</span>일 확률”과 “그때 로봇 손이 자세 <span class="math inline">(p_i, o)</span> (어떤 방향 <span class="math inline">o</span>)로 도달할 수 있을 확률”을 함께 고려하는 식입니다. 이 joint 모델의 밀도가 가장 높은 <span class="math inline">(p, q, o, t)</span> 조합이 바로 <strong>최적 잡기 자세 및 시점</strong>으로 선택됩니다. 저자들은 여기에 하나의 추가적인 휴리스틱을 적용했는데, 잡기 직전 로봇 손바닥의 방향이 날아오는 물체의 <strong>운동 방향에 정확히 반대가 되도록</strong> 자세를 정제하였습니다. 이는 물체를 받쳐서 충격을 흡수하고 흘리지 않기 위한 것으로, 예를 들어 공이 날아올 때 손바닥이 공의 진행 방향을 향해 있어야 튕겨내지 않고 품을 수 있는 것과 같은 원리입니다.</p>
<p>위와 같은 절차를 통하면, 매 새로운 센서 관측 시점마다 로봇 시스템은 <strong>현재 추정된 물체 상태로부터 미래의 잡기 지점과 시간</strong>을 지속적으로 업데이트하게 됩니다. 논문에서는 이 과정을 <strong>Thread 1</strong>로 명명하여, 물체 궤적 예측과 최적 자세 계산이 병렬 쓰레드로 상시 실행됨을 설명합니다. 이 쓰레드는 새로운 카메라 측정이 들어올 때마다 물체의 남은 궤적을 다시 예측하고, 최적의 catching 시점/자세를 다시 계산하여 갱신합니다. 이렇게 계산된 목표 catching 자세는 <strong>Thread 2</strong>인 로봇 팔 제어기에 실시간으로 전달되어, 로봇 팔이 그 지점으로 이동하도록 유도됩니다. 두 쓰레드의 상호작용은 논문의 Fig. 2 블록 다이어그램에 잘 나타나 있으며, 이러한 <strong>계속적인 재계획(replanning)</strong> 덕분에 센서 지연이나 예측 오차가 조금씩 누적되더라도 로봇이 잡기 직전까지 궤적을 수정하여 <strong>정밀한 포획</strong>이 가능해집니다. 다만, 실제 시스템에서는 물체가 로봇에 너무 가까이 접근한 <strong>마지막 0.09초 정도 전</strong>부터는 더 이상 계획을 변경하지 않고 최종 동작에 집중하도록 하였습니다. 이는 물체가 눈앞에 근접했을 때 후방 카메라 시야에서 부분적으로 사라져 위치 추정이 흔들릴 수 있고, 임계시간 내에 새로운 계산을 반영하기 어려워질 수 있기 때문입니다. 따라서 <strong>잡기 90ms 전</strong>을 최종 커트오프로 두고, 그 이후에는 현재 계획대로 손가락을 닫는 동작까지 수행하도록 설계되었습니다.</p>
<p>요약하면, 제안된 방법은 <strong>확률적 모델링</strong>을 통해 <strong>로봇의 가능한 자세 공간</strong>과 <strong>물체의 잡기 가능한 자세</strong>를 하나로 통합한 뒤, 실시간 궤적 예측과 결합하여 <strong>밀리초 단위로 최적 인터셉트 지점을 찾아내는 알고리즘</strong>입니다. 이러한 접근은 기존의 기하학적/격자 기반 방법에 비해 훨씬 <strong>매끄럽고 연속적인 해결책</strong>을 주며, 계산 비용도 낮아서 2.7GHz CPU에서 <strong>0.2밀리초</strong> 만에 잡기 자세를 결정할 수 있었던 것으로 보고됩니다. 이는 로봇이 공중에 떠있는 물체를 잡기 위해 쓸 수 있는 시간 여유가 많지 않을 때(보통 &lt;0.5–0.7초)에 큰 강점이 됩니다.</p>
</section>
<section id="로봇-팔-손-운동-계획과-제어-reactive-arm-hand-control" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="로봇-팔-손-운동-계획과-제어-reactive-arm-hand-control"><span class="header-section-number">2.5</span> 로봇 팔-손 운동 계획과 제어 (Reactive Arm-Hand Control)</h2>
<p>잡기 시점과 목표 자세가 정해지면, 이제 로봇은 그에 맞추어 <strong>팔을 움직이고 손가락을 제어</strong>해야 합니다. 이 논문에서는 로봇 팔의 궤적을 생성하기 위해 <strong>안정적인 동적 시스템 기반 제어기</strong>를 사용하였습니다. 이는 저자들이 이전 연구들에서 개발한 기법으로, 여러 시연 궤적을 일반화하여 <strong>종료 안정(equilibrium stable)</strong>한 벡터장 형태로 학습하는 <strong>시간-불변 DS</strong> 기술입니다. 간단히 말해, 사람이 가르쳐준 몇 가지 경로를 <strong>자유롭게 뒤섞어도</strong> 목표점에 수렴하도록 보장되는 연속 함수 형태로 모델링하는 것으로, <strong>SEDS (Stable Estimator of Dynamical Systems)</strong> 등의 알고리즘을 응용한 방법입니다. 이 DS 제어기의 장점은 <strong>현재 상태</strong>를 입력하면 <strong>다음에 어느 방향으로 움직일지 즉각 출력</strong>해주는 형태이므로, 로봇이 목표에 도달할 때까지 <strong>닫힌 형식(closed-form)</strong>으로 쭉 따라가는 <strong>연속 궤적</strong>을 만들어준다는 점입니다. 특히 DS는 <strong>시간 매개변수가 없어</strong> 목표점만 향하도록 설계되므로, 예측되는 목표 위치나 팔의 경로가 실시간으로 바뀌어도 그때그때 새로운 <strong>벡터장</strong>을 따라가면 됩니다.</p>
<p>논문에서는 이 DS 기반 팔 제어기를 학습하기 위해 <strong>20개의 팔 움직임 시범</strong>을 사용했습니다. 앞서 가용 공간 모델을 수집할 때와 동일한 방법으로, 연구자는 로봇 팔을 초기자세에서 시작하여 여러 가지 잡기 목표 자세까지 이끄는 데모들을 수행했습니다. 각 데모는 시작 (대기자세)부터 끝 (잡기자세)까지 팔의 궤적을 제공하며, 이렇게 모인 다수의 경로를 <strong>Gaussian Mixture Model</strong>로 학습한 후 <strong>Dynamical System</strong>으로 변환하여 팔 움직임을 생성합니다. 결과적으로, 로봇 팔은 현재 자신의 손끝 위치가 어디에 있든지 간에 DS가 지시하는 방향으로 움직이면 자연스럽게 목표 잡기 자세로 수렴하게 됩니다. 또한 잡기 목표 자체가 움직이는 경우(예: 예측이 변경된 경우)에도, DS는 새로운 목표를 향해 <strong>연속적으로 방향을 수정</strong>해주므로, 마치 목표를 <strong>추적하는 유도장치</strong>처럼 팔을 몰고 가는 효과를 냅니다.</p>
<p>하지만 물체 잡기 동작에서는 <strong>“언제 도착하는가”</strong> 또한 매우 중요합니다. DS 단독으로는 시간에 대한 개념이 없기 때문에 목표점에는 수렴하지만 <strong>주어진 시간 내 도달</strong>을 보장할 수 없습니다. 이를 위해 논문에서는 <strong>타이밍 조절기(timing controller)</strong>를 도입했습니다. 타이밍 조절기는 현재 남은 시간과 거리를 바탕으로 DS 출력을 <strong>스케일링</strong>하여, 로봇이 정확히 물체 도착 시간에 맞춰 도달하도록 속도를 조절합니다. 예를 들면, 처음 예측으로는 잡기 시점이 0.5초 뒤로 계획되었다가 이후 예측이 0.6초로 수정되면, DS의 진행 속도를 20% 늦추어 팔이 약간 천천히 움직이도록 만드는 식입니다. 반대로 시간이 촉박해지면 DS의 스케일을 키워 최대 속도 한계까지 더 빨리 움직이도록 합니다. 이 타이밍 조절은 상시로 작동하여, <strong>예측 시간이 바뀔 때마다 로봇이 그 새로운 시간에 맞춰 도달</strong>하도록 해줍니다. 단, 로봇의 물리적 한계를 넘어서까지 보상할 수는 없기 때문에, 만약 목표 잡기 자세에 도달하려면 처음 자세에서 <strong>너무 크게 움직여야 하는 경우</strong>(예: 반대편에 던져졌는데 로봇이 돌아가야 한다거나)에는 최대 속도로도 시간 내 도달이 불가능할 수 있습니다. 그런 상황은 <strong>실패 시도</strong>로 간주되며, 논문에서도 실패 사례의 약 12건이 이러한 <strong>동역학적으로 불가능한</strong> 목표 때문이었다고 보고하고 있습니다.</p>
<p>이제 <strong>손가락 제어</strong>를 살펴보겠습니다. 앞서 강조했듯, 본 연구는 <strong>팔(손목)과 손가락의 움직임을 밀접히 연동</strong>시켰습니다. 이를 위해 팔 동작을 담당하는 DS를 <strong>마스터(master) DS</strong>, 손가락 쪽을 담당하는 DS를 <strong>슬레이브(slave) DS</strong>로 설정한 <strong>연합 Dynamical System (CDS)</strong> 구조를 사용했습니다. 팔의 end-effector 상태 (위치 <span class="math inline">\xi_h \in \mathbb{R}^3</span>, 손목 자세는 별도 <span class="math inline">\xi_o \in \mathbb{R}^3</span> 축각(axis-angle) 표현)와 손가락 벌어짐 정도 (<span class="math inline">\xi_f \in \mathbb{R}</span>, 이를 여러 손가락 조인트로 매핑) 등을 모두 상태로 포함하는 결합 시스템을 구성한 것입니다. 마스터 DS는 팔의 궤적을 생성하고, 슬레이브 DS는 손가락의 궤적(벌리고 쥐는 동작)을 생성하되, 슬레이브는 마스터의 진행 상황에 따라 결정되도록 설계되었습니다. 쉽게 말해, 팔이 목표에 가까워짐에 따라 손가락이 닫히는 속도가 빨라지거나, 팔의 속도가 느려지면 손가락 동작도 지연되는 식으로 <strong>상호 결합(coupling)</strong>된 것입니다. 이 모델을 학습시키기 위해, 연구진은 <strong>별도의 데모 실험</strong>도 수행했습니다. 사람에게 센서를 장착하여 직접 물체를 잡게 하고, 이 때 <strong>손의 궤적</strong>과 <strong>손가락 굽힘 각도</strong> 변화를 기록하여 팔-손 연동 동작의 예시로 삼았습니다. 구체적으로, <strong>5DT 데이터 글러브</strong>로 사람 손가락 관절각을 측정하고, 모션캡쳐로 물체와 손의 위치를 추적하면서, 사람이 공중에 던져진 물체를 한 손으로 잡는 동작을 여러 차례 수행했습니다. 이 데이터를 통해 “팔이 이만큼 움직였을 때 손가락은 얼마나 닫혀야 하는가”의 관계를 학습하고, CDS의 마스터-슬레이브 DS 간 <strong>결합 함수</strong>를 구했습니다. 논문에서는 손가락 동작을 팔 동작의 특정 <strong>지표(distance)</strong>에 따라 트리거하는 대신, 이러한 <strong>데이터 기반 결합 모델</strong>을 사용함으로써 별도의 튜닝 없이도 손가락이 <strong>정확한 타이밍</strong>에 닫히도록 만들었다고 강조합니다. Fig. 15에는 Barrett Hand를 사용한 팔-손 CDS 제어 구성요소들이 제시되어 있고, Fig. 16에는 실제 잡기 동작 시 로봇 팔의 궤적이 예측 궤적을 계속 추종하면서 <strong>시간에 맞춰 도달</strong>하는 모습이 그려져 있습니다.</p>
<p>최종적으로, 로봇 제어 시스템은 <strong>Thread 2 (Arm Controller)</strong>로 구현되어 500 Hz 주기로 동작합니다. 이 제어 스레드는 매 주기마다 현재 로봇 팔의 상태와 Thread 1이 업데이트한 최신 목표(잡기 위치, 도달 시각)를 받아서, <strong>CDS 컨트롤러</strong>를 통해 다음 순간의 팔-손속도 명령을 계산합니다. 계산된 로봇 손끝의 목표 변화(선속도, 각속도)는 다시 <strong>역기구학(IK)</strong> 모듈을 거쳐 7개 관절의 목표 회전각으로 변환되며, <strong>임계 감쇠 필터(critically damped filter)</strong>로 신호를 부드럽게 한 뒤 로봇에 전송됩니다. KUKA LWR 4+ 로봇팔은 <strong>자체 내장 제어기</strong>로 500Hz의 위치 제어가 가능하므로, 호스트 컴퓨터에서 보낸 관절각 명령을 실시간으로 추종하게 됩니다. 이처럼 <strong>예측-계획 쓰레드</strong>와 <strong>제어기 쓰레드</strong>가 동시에 돌아가면서, 로봇은 물체를 향해 팔을 뻗고 손가락을 벌린 채 접근하다가, 적절한 순간에 손가락을 오므려 물체를 붙잡게 됩니다.</p>
</section>
<section id="실험-결과-및-분석" class="level2" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="실험-결과-및-분석"><span class="header-section-number">2.6</span> 실험 결과 및 분석</h2>
<p>논문에서는 제안된 시스템을 <strong>시뮬레이터</strong>와 <strong>실제 로봇</strong> 양쪽에서 검증하였습니다. 시뮬레이션 실험은 <strong>iCub 휴머노이드 로봇</strong>의 상반신 모델을 사용했고, 실제 실험은 <strong>KUKA LWR 4+ (7-자유도 로봇팔)</strong>에 <strong>SimLab Allegro Hand (4지 로봇 손)</strong>를 장착한 구성으로 진행되었습니다. 물체 인식 및 추적을 위해 <strong>OptiTrack</strong> 다중 카메라 모션캡쳐 시스템이 이용되었으며, 물체마다 표면에 <strong>반사 마커 3개</strong>를 부착하여 <strong>240 Hz</strong>로 6DOF 자세를 측정했습니다. 주요 실험 시나리오는 사람이 물체를 로봇을 향해 던지면 로봇이 이를 받아잡는 형태로, 던지는 초기 조건(위치, 속도, 각속도)은 실험마다 무작위로 조금씩 달리하여 다양한 상황을 시험했습니다. 특히 물체로는 <strong>망치</strong>, <strong>테니스 라켓</strong>, <strong>빈 플라스틱 병</strong>, <strong>절반가량 물이 든 병</strong>, <strong>작은 박스</strong> 등을 사용하여, 물체의 질량 분포와 공기역학적 특성이 크게 다르도록 구성했습니다. 이들은 로봇에게 <strong>매우 다양한 도전 상황</strong>을 제공하는데, 예컨대 라켓이나 망치는 <strong>잡아야 할 부분(손잡이)</strong>이 무게중심과 떨어져 있고, 특히 <strong>부분 채운 물병</strong>은 비행 중에 내부 유체 이동으로 무게중심이 계속 변하는 등 <strong>난해한 움직임</strong>을 보입니다. 그럼에도 불구하고, 로봇은 이러한 물체들을 <strong>정확히 추적</strong>하여 대부분의 경우 공중에서 붙잡는 데 성공하였습니다.</p>
<p><strong>시뮬레이션 결과</strong>는 앞서 언급한 대로 iCub 로봇을 통해 확인되었으며, 50회의 무작위 던지기 중 reachable space에 들어온 약 47회의 상황에서 <strong>100% 잡기 성공률</strong>을 보였습니다. 시뮬레이터에서는 물체 움직임에 모델링 오차나 외란이 없으므로 예측이 완벽에 가깝게 맞아떨어졌고, DS 제어기의 이상적 동작으로 로봇팔이 신속히 목표에 도달하여 <strong>실패 사례가 전혀 없었습니다</strong>. 이에 반해 <strong>실제 로봇 실험</strong>에서는 소프트웨어적/기계적 한계와 센서 불확실성이 존재하기 때문에 일부 실패가 발생했지만, 그럼에도 불구하고 <strong>매우 높은 성공률</strong>을 달성했습니다. 논문에 따르면, 빈 병, 절반 채운 병, 라켓, 박스의 4가지 물체에 대해 각각 20회씩 (총 80회) 던져 실험한 결과, <strong>로봇 작업공간에 들어오지 않은 9회를 제외한 71회의 시도 중 52회 성공</strong>하여 <strong>73.2%</strong>의 전체 성공률을 기록했습니다. 이때 한 번 던져진 물체의 평균 <strong>비행시간은 약 0.5초</strong>에 불과했으며, 3.5m 거리에서 던진 물체를 그 짧은 시간 안에 잡아낸 것입니다.</p>
<p>성공률 73%라는 수치는 언뜻 완벽해 보이지 않을 수 있으나, 상대적으로 <strong>인간의 성능과 비교</strong>하면 이 로봇 시스템의 우수성이 두드러집니다. 저자들은 동일한 실험 조건에서 <strong>사람 10명</strong>에게 맨손으로 물체 잡기를 시도하도록 하여 결과를 비교했는데, 피험자들은 100회의 빈 병 던지기 중 평균 <strong>38회</strong>밖에 잡지 못했고, 잘 하는 사람도 성공률 <strong>70%</strong> 수준에 그쳤다고 합니다. 특히 <strong>경험이 적은 사람</strong>의 경우 성공률이 겨우 10%대에 불과하여, 공중의 변칙적인 물체를 한 손으로 잡는 일이 얼마나 어려운지 보여주었습니다. 이에 비하면, 로봇은 <strong>일관되고 높은 성공률(73%)</strong>을 보이며 숙련 인간 수준에 가까운 성능을 발휘한 것입니다. 이는 본 논문의 접근법이 실제 현실 상황에서도 효과적으로 작동한다는 강력한 증거라 할 수 있습니다.</p>
<p>물론 실험에서 관찰된 <strong>실패 사례들</strong>은 향후 개선점을 시사합니다. 논문에서는 총 19회의 실패 원인을 분석하였는데, <strong>12건(과반수)</strong>은 잡기 목표가 로봇의 초기자세로부터 너무 멀리 떨어져 <strong>관절속도 한계</strong>를 넘은 경우였습니다. 이때는 로봇이 최대로 팔을 뻗어도 시간 내 도달하지 못해 물체를 놓치게 됩니다. <strong>4건</strong>은 잡는 순간에 <strong>손가락 일부가 물체를 잘못 쳐서</strong> 물체가 튕겨나간 경우였습니다. 이러한 경우는 드물게 발생했으며, 아마도 물체의 접근 각도가 예상보다 미묘하게 달라 손가락이 충돌한 것으로 보입니다. 남은 <strong>3건</strong>은 로봇 관절에 부하가 급격히 걸려 <strong>토크 제한</strong>에 걸리며 로봇이 안전정지한 경우였습니다. 전체적으로 보면 실패는 주로 <strong>하드웨어적 한계 상황</strong>에서 발생했으며, 제어 알고리즘 자체의 문제로 인한 실패는 매우 적었습니다. 특히 <strong>센서의 잡음이나 예측 오차 때문에 엉뚱한 곳으로 팔이 간 경우는 없었다</strong>는 점에서, 제안된 확률적 예측+계획 통합 알고리즘의 <strong>견고함</strong>을 확인할 수 있습니다.</p>
<p>마지막으로, 논문의 실험 결과에는 제안된 방법의 <strong>학습 효율</strong>과 <strong>범용성</strong>에 대한 언급도 있습니다. 물체 동역학 모델 학습에 약 20개 정도의 던지기 궤적을 사용하고, 팔 DS 및 파지 모델 학습에도 10~20개 안팎의 시연을 사용한 정도로, 비교적 <strong>적은 데이터</strong>로 상당히 복잡한 과제를 달성한 것이 인상적입니다. 또한 이 프레임워크를 <strong>서로 다른 두 로봇 플랫폼</strong>(iCub과 KUKA)에서 모두 구현해보았고, <strong>여러 형태의 물체</strong>에 대해서도 유사한 성능을 보였다는 점에서, 제안 기법의 <strong>일반화 가능성</strong>을 보여줍니다. 이는 핵심 아이디어들이 구체적인 로봇 파라미터나 물체 속성에 강하게 의존하지 않고, 학습으로부터 자동적으로 유도되기 때문으로 판단됩니다. 실제로 저자들은 “본 논문이 <strong>불확실성 하에서 초고속 제어</strong>의 한 예를 제시하였으며, 과거 연구들이 주로 단순 공 모양 물체나 천천히 회전하는 물체에 국한되었던 데 비해 우리의 연구는 그 범위를 크게 넓혔다”라고 강조하고 있습니다.</p>
</section>
<section id="결론" class="level2" data-number="2.7">
<h2 data-number="2.7" class="anchored" data-anchor-id="결론"><span class="header-section-number">2.7</span> 결론</h2>
<p>“Catching Objects in Flight” 논문은 로봇이 공중에서 복잡한 물체를 잡는 문제에 대해 <strong>학습 기반의 종합적 해결책</strong>을 제시한 뛰어난 연구입니다. <strong>물체 궤적 예측</strong>부터 <strong>잡기 자세 결정</strong>, <strong>팔-손 제어</strong>에 이르는 전 과정을 통합하였고, 각각의 단계에서 기존 방식의 한계를 극복하는 독창적인 접근을 보여주었습니다. 프로그래밍-바이-데몬스트레이션 기법으로 <strong>물체의 비행 동역학</strong>과 <strong>로봇의 동작 패턴</strong>을 획득하고, 이를 <strong>확률적 모델</strong>과 <strong>동적 시스템 제어기</strong>로 구현함으로써, 로봇이 <strong>밀리초 단위의 반응속도</strong>와 <strong>높은 적응성</strong>을 갖추게 했습니다. 특히 <strong>잡기 대상이 불규칙한 모양</strong>이고 <strong>비행 시간이 매우 짧아</strong> 사람조차 잡기 어려운 상황에서, 제안된 시스템이 <strong>인간에 필적하는 수준의 성능</strong>(성공률 ~73%)을 보인 점은 주목할 만합니다. 이는 로봇이 단순 반복 작업을 넘어 <strong>동적인 실제 환경</strong>에 대응할 수 있음을 보여준 사례로, 향후 <strong>우주 쓰레기 포집</strong>이나 <strong>고속 물체 조작</strong> 등 다양한 응용에 활용될 수 있을 것으로 기대됩니다. 물론 현재 프레임워크는 <strong>고정된 위치의 모션캡쳐 카메라</strong>에 크게 의존하고 있고, 로봇의 물리적 스펙 한계로 인한 제약도 존재합니다. 따라서 <strong>이동 로봇 플랫폼</strong>이나 <strong>온보드 센서</strong>로 확장하고, 보다 빠르고 강인한 로봇 하드웨어와 결합한다면, 공중에서 물체를 잡는 로봇의 활용 범위는 더욱 넓어질 것입니다. 전체적으로 이 논문은 <strong>로봇 학습</strong>과 <strong>실시간 제어</strong>를 접목하여 난제를 해결한 훌륭한 사례로서, 추후 동분야 연구자들에게도 많은 영감을 제공하고 있습니다.</p>
<p><strong>참고 자료:</strong> 본 리뷰에서는 논문의 내용과 실험 결과를 기반으로 핵심적인 사항들을 정리하였습니다. 자세한 기술 구현이나 추가 실험 정보는 원문을 참고하시기 바랍니다. 또한 관련 분야의 발전된 연구로, 물체 포획을 위한 궤적 최적화 기법이나 딥러닝 기반 접근법 등이 있으나, 해당 논문의 범위를 벗어나므로 본 리뷰에서는 다루지 않았습니다. 이 논문의 접근법은 로봇의 <strong>인지-계획-제어</strong> 전 과정을 아우르는 통합 설계를 보여주며, 향후 더욱 복잡한 동작 학습과 고속 제어 연구의 밑거름이 될 것으로 평가됩니다.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="curieuxjy/blog_comments" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Copyright 2024, Jung Yeon Lee</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/curieuxjy">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>