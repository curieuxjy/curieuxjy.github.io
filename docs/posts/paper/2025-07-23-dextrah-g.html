<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-07-23">
<meta name="description" content="Pixels-to-Action Dexterous Arm-Hand Grasping with Geometric Fabrics">

<title>📃DextrAH-G 리뷰 – Curieux.JY</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../profile.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-2NVZN2MJZT"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-2NVZN2MJZT', { 'anonymize_ip': true});
</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Curieux.JY</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../post.html"> 
<span class="menu-text">Post</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../note.html"> 
<span class="menu-text">Note</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">Jung Yeon Lee</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/curieuxjy"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#brief-review" id="toc-brief-review" class="nav-link active" data-scroll-target="#brief-review"><span class="header-section-number">1</span> Brief Review</a></li>
  <li><a href="#detail-review" id="toc-detail-review" class="nav-link" data-scroll-target="#detail-review"><span class="header-section-number">2</span> Detail Review</a>
  <ul class="collapse">
  <li><a href="#논문의-주요-기여점-요약" id="toc-논문의-주요-기여점-요약" class="nav-link" data-scroll-target="#논문의-주요-기여점-요약"><span class="header-section-number">2.1</span> 논문의 주요 기여점 요약</a></li>
  <li><a href="#사용된-기술모델-및-아키텍처-분석" id="toc-사용된-기술모델-및-아키텍처-분석" class="nav-link" data-scroll-target="#사용된-기술모델-및-아키텍처-분석"><span class="header-section-number">2.2</span> 사용된 기술/모델 및 아키텍처 분석</a></li>
  <li><a href="#실험-설정-및-결과-분석" id="toc-실험-설정-및-결과-분석" class="nav-link" data-scroll-target="#실험-설정-및-결과-분석"><span class="header-section-number">2.3</span> 실험 설정 및 결과 분석</a></li>
  <li><a href="#기존-연구와의-비교-및-차별점" id="toc-기존-연구와의-비교-및-차별점" class="nav-link" data-scroll-target="#기존-연구와의-비교-및-차별점"><span class="header-section-number">2.4</span> 기존 연구와의 비교 및 차별점</a></li>
  <li><a href="#장점과-한계점" id="toc-장점과-한계점" class="nav-link" data-scroll-target="#장점과-한계점"><span class="header-section-number">2.5</span> 장점과 한계점</a>
  <ul class="collapse">
  <li><a href="#장점" id="toc-장점" class="nav-link" data-scroll-target="#장점"><span class="header-section-number">2.5.1</span> 장점</a></li>
  <li><a href="#한계점" id="toc-한계점" class="nav-link" data-scroll-target="#한계점"><span class="header-section-number">2.5.2</span> 한계점</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-page-left" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">📃DextrAH-G 리뷰</h1>
  <div class="quarto-categories">
    <div class="quarto-category">rl</div>
    <div class="quarto-category">fabric-guided</div>
    <div class="quarto-category">hand</div>
  </div>
  </div>

<div>
  <div class="description">
    Pixels-to-Action Dexterous Arm-Hand Grasping with Geometric Fabrics
  </div>
</div>


<div class="quarto-title-meta column-page-left">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">July 23, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<ul>
<li><a href="https://arxiv.org/abs/2407.02274">Paper Link</a></li>
<li><a href="https://sites.google.com/view/dextrah-g">Project Link</a></li>
</ul>
<ol type="1">
<li>🤖 DextrAH-G는 Reinforcement Learning(RL)과 geometric fabrics, teacher-student distillation을 결합하여 시뮬레이션에서만 훈련된, 뎁스 이미지 기반의 정교한 로봇 팔-손 그립 정책입니다.</li>
<li>🛠️ 이 시스템은 고차원 공간, sim2real gap, 충돌 회피 등 주요 난제를 해결하며, geometric fabrics로 하드웨어 안전과 행동 유도를 보장하고, 교사-학생 증류(distillation)를 통해 실제 환경으로의 제로-샷 전이(transfer)를 성공시켰습니다.</li>
<li>🚀 DextrAH-G는 실제 환경에서 다양한 신규 물체를 성공적으로 파지 및 운반하며, 87%의 성공률과 분당 5.63회 처리가 가능한 최첨단 성능을 시연했고, 테스트 중 하드웨어 손상이 전혀 없어 높은 안전성을 입증했습니다.</li>
</ol>
<center>
<img src="../../images/2025-07-23-dextrah-g/2.gif" width="80%">
</center>
<hr>
<section id="brief-review" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Brief Review</h1>
<p>DextrAH-G는 강화 학습(RL), Geometric Fabrics, 그리고 교사-학생 증류(teacher-student distillation)를 결합하여 시뮬레이션 환경에서만 학습하고 실제 세계에 제로-샷(zero-shot)으로 전이될 수 있는 깊이(depth) 기반의 능숙한 팔-손 로봇 조작(arm-hand grasping) 정책입니다. 이 연구는 고차원 관측 및 행동 공간, sim2real gap, 충돌 회피, 하드웨어 제약 등 관절형 팔-손 정책 학습의 주요 난제를 해결합니다. DextrAH-G는 23개의 모터를 가진 로봇이 스트리밍 깊이 이미지(streaming depth images)를 사용하여 다양한 물체를 고속으로 안전하게 연속적으로 잡고 운반할 수 있게 합니다.</p>
<p>이 연구의 주요 기여는 다음과 같습니다:</p>
<ol type="1">
<li>정책 학습을 위한 귀납적 편향(inductive bias)을 생성하고, 충돌을 피하며, 관절 제약을 유지하고, 행동을 형성하는 벡터화된 Geometric Fabric controller.</li>
<li>벡터화된 Geometric Fabrics 위에서 여러 다른 물체에 대한 고성능 조작을 가능하게 하는 시뮬레이션 전용 RL 훈련의 privileged FGP(Fabric-Guided Policy).</li>
<li>원래 행동을 복제하고 물체 위치 예측을 가능하게 하는 깊이(depth) 기반, 다중 모달(multi-modal) FGP의 privileged FGP 증류(distillation).</li>
<li>실제 세계의 다양한 새로운 물체에 대한 최신 능숙한 로봇 조작 성능을 달성한 DextrAH-G의 제로-샷 sim2real 전이(transfer).</li>
</ol>
<p><strong>1. Geometric Fabrics 및 Fabric-Guided Policies (FGPs)</strong></p>
<p>Geometric Fabrics는 고전 역학 시스템의 행동을 일반화하여 설계 유연성, 조합성 및 안정성을 갖춘 제어기를 모델링할 수 있게 합니다. Geometric Fabric은 다음 형태의 방정식을 따릅니다: <span class="math display">M_f (q_f , \dot{q}_f )\ddot{q}_f + f_f (q_f , \dot{q}_f ) + f_\pi (a) = 0</span> 여기서 <span class="math inline">M_f \in \mathbb{R}^{n \times n}</span>는 시스템의 우선순위를 포착하는 양의 정부호 시스템 메트릭(mass), <span class="math inline">f_f \in \mathbb{R}^n</span>는 명목 경로 생성 Geometric Force, <span class="math inline">f_\pi (a) \in \mathbb{R}^n</span>는 행동 <span class="math inline">a \in \mathbb{R}^m</span>에 대한 추가 구동력입니다. <span class="math inline">q_f, \dot{q}_f, \ddot{q}_f \in \mathbb{R}^n</span>는 Fabric의 위치, 속도, 가속도입니다. 이 방정식은 Fabric 상태 <span class="math inline">q_f</span>와 <span class="math inline">\dot{q}_f</span>를 시간에 따라 진화시키는 가속도 <span class="math inline">\ddot{q}_f</span>를 생성합니다. <span class="math inline">f_\pi</span>는 <span class="math inline">\ddot{q}_f</span>에 영향을 미치므로 Fabric 상태에 영향을 줍니다.</p>
<p>Geometric Fabric controller는 네 가지 주요 이유로 사용됩니다:</p>
<ol type="1">
<li>원치 않는 충돌 회피,</li>
<li>정책 탐색을 동시에 유도하고 전체 로봇 움직임을 유리하게 형성하는 노출된 행동 공간을 통한 귀납적 편향 생성,</li>
<li>관절 제약 조건 준수,</li>
<li>운동학적 조작성을 촉진하기 위한 로봇 자세 유지.</li>
</ol>
<ul>
<li><p><strong>충돌 회피(Collision Avoidance):</strong> 환경 및 자체 충돌 회피는 Geometric Fabric 항과 forcing Fabric 항을 통해 처리됩니다. 로봇의 기하학적 구조는 구(spheres)의 집합으로 모델링됩니다. <span class="math inline">x = \phi_{fk}(q) \in \mathbb{R}^3</span>는 로봇 구성에서 각 구의 원점으로의 포워드 운동학 매핑입니다. <span class="math inline">\hat{n}_i = \frac{r_i-x}{\|r_i-x\|} \in \mathbb{R}^3</span>는 구 점에서 충돌 물체 <span class="math inline">i</span>의 가장 가까운 점 <span class="math inline">r_i \in \mathbb{R}^3</span>까지의 방향입니다. <span class="math inline">d_i = \max(d_{min}, d_i) \in \mathbb{R}^+</span>는 하한이 있는 거리입니다. Geometric 가속도는 <span class="math inline">\ddot{x} = k_g \|\dot{x}\|^2 \hat{\ddot{x}}_b</span>이며, forcing 가속도는 <span class="math inline">\ddot{x} = k_f \hat{\ddot{x}}_b - b \dot{x}</span>입니다. 여기서 <span class="math inline">\ddot{x}_b = - \sum_i \frac{1}{d_i}\hat{n}_i</span>는 충돌에서 멀어지는 구당 기본 가속도 반응입니다. Fabric의 메트릭은 <span class="math inline">M = \beta e_d^2 \hat{M}_b</span>로 설계되며, 여기서 <span class="math inline">e_d = \min_i\{d_i\}</span>입니다. <span class="math inline">M_b = \sum_i s_i d_i \hat{n}_i \otimes \hat{n}_i</span>는 구당 기본 메트릭 반응이며, <span class="math inline">s_i = \frac{1}{2} \tanh(-\alpha_1(v_i - \alpha_2) + 1)</span>는 구가 충돌 물체 <span class="math inline">i</span>로 향할 때 활성화되는 스무스 속도 게이트입니다(<span class="math inline">v_i = - \dot{x} \cdot \hat{n}_i</span>).</p></li>
<li><p><strong>행동 공간(Action Space):</strong> Allegro hand의 경우, 사람의 조작 데이터(human grasping motion data)를 Allegro hand에 리타겟팅(retargeting)하고 주성분 분석(PCA)을 적용하여 5차원 특성 조작(eigengrasp) 매니폴드를 생성합니다. PCA를 통해 얻은 처음 다섯 개의 주성분 <span class="math inline">A \in \mathbb{R}^{5 \times 16}</span>을 사용하여 <span class="math inline">e_A = [0, A] \in \mathbb{R}^{5 \times 23}</span>로 정의된 태스크 맵(taskmap) <span class="math inline">x = e_A q \in \mathbb{R}^5</span>를 만듭니다. 이 공간에서 인력 Fabric 항을 정의하며, 메트릭 <span class="math inline">M(x) = mI</span>이고 가속도 <span class="math inline">\ddot{x} = -k_a \tanh(\alpha_a\|x-x_{pca,target}\|) \frac{x-x_{pca,target}}{\|x-x_{pca,target}\|} -b \dot{x}</span>입니다. <span class="math inline">x_{pca,target}</span>는 5차원 손 행동 공간으로 사용됩니다. 팔 제어를 위해 팜(palm)에 부착된 7개의 3차원 점을 21차원 공간으로 매핑하는 새로운 태스크 맵을 생성합니다. 팔을 위한 6차원 행동 공간은 목표 팜 위치 <span class="math inline">x_{f,target} \in \mathbb{R}^3</span>와 목표 팜 오리엔테이션(Euler angles) <span class="math inline">r_{f,target} \in \mathbb{R}^3</span>로 구성됩니다. 전체 로봇에 대한 행동 공간은 총 11차원입니다.</p></li>
<li><p><strong>관절 제약 조건(Joint Constraints):</strong> Fabric은 2차 제어기이므로, 관절 가속도 및 저크(jerk) 제한은 폐쇄형으로 처리될 수 있습니다. 다음 이차 계획(quadratic program)을 풀어 처리합니다: <span class="math inline">L = \frac{1}{2} (\ddot{q}_f - \ddot{q})^T M_f (\ddot{q}_f - \ddot{q}) + \alpha^2 \ddot{q}_f^T M_f \ddot{q}_f</span> 여기서 <span class="math inline">\ddot{q}_f = -(M_f + \alpha I)^{-1}f_f</span>이며, <span class="math inline">\alpha \rightarrow \infty</span>일 때 <span class="math inline">||\ddot{q}_f|| \rightarrow 0</span>입니다. 또한, 관절 위치 제한은 Fabric의 관절 반발 항을 통해 적용됩니다.</p></li>
<li><p><strong>자세 제어(Posture Control):</strong> 로봇의 제어되는 관절보다 Fabric의 노출된 행동 공간의 차원이 적기 때문에 중복성 문제(redundancy issues)를 해결해야 합니다. 이는 구성 공간(configuration space)의 기하학적 인력(geometric attractor)을 따름으로써 달성됩니다. Fabric이 로봇 전체 움직임을 구성 공간의 <span class="math inline">x_g</span>로 안내하되, PCA 및 포즈 태스크 맵에서의 <span class="math inline">x_g</span>로의 수렴을 방해하지 않습니다.</p></li>
</ul>
<p><strong>2. Teacher Privileged FGP 훈련 (강화 학습)</strong></p>
<p>능숙한 조작은 강화 학습 문제로 간주되며, 시뮬레이션에서 privileged-state teacher policy가 140가지 다른 물체를 능숙하게 조작하도록 훈련됩니다. Geometric Fabric 행동 공간은 로봇이 안전하고 자연스러운 행동을 수행하도록 보장하므로, 보상 설계는 전적으로 손가락 끝-물체 접촉 및 물체를 목표 위치로 들어 올리는 데 중점을 둡니다.</p>
<ul>
<li><strong>비대칭 액터-크리틱(Asymmetric Actor Critic):</strong> 실제 세계에 배포될 때 제어 정책은 privileged 시뮬레이션 상태 정보에 접근할 수 없지만, privileged 정보는 시뮬레이션 훈련 속도를 높이는 데 사용됩니다. 크리틱 <span class="math inline">V(s)</span>는 모든 privileged 상태 정보 <span class="math inline">s</span>를 얻고, teacher policy <span class="math inline">\pi_{privileged}(o_{privileged})</span>는 이 privileged 상태 정보의 제한된 부분인 관측 <span class="math inline">o_{privileged}</span>를 얻습니다.
<ul>
<li>Teacher policy의 관측 <span class="math inline">o_{privileged}</span>는 로봇의 cspace 위치 <span class="math inline">q</span>, cspace 속도 <span class="math inline">\dot{q}</span> (총 23개), 팜의 세 지점 위치 <span class="math inline">x_{palm}, x_{palm-x}, x_{palm-y}</span>, 4개 손가락 끝의 위치 <span class="math inline">x_{fingertips}</span>, Fabric 상태 <span class="math inline">q_f, \dot{q}_f, \ddot{q}_f</span>, 목표 물체 위치 <span class="math inline">x_{goal}</span>, 그리고 노이즈가 있는 물체 위치 <span class="math inline">ex_{obj}</span> 및 쿼터니언 <span class="math inline">eq_{obj}</span>, 물체 one-hot embedding <span class="math inline">e</span>를 포함합니다.</li>
<li>크리틱의 입력 상태 <span class="math inline">s</span>는 <span class="math inline">o_{privileged}</span>와 함께 로봇 관절력 <span class="math inline">f_{dof}</span>, 손가락 끝 접촉력 <span class="math inline">f_{fingers}</span>, 실제 물체 위치 <span class="math inline">x_{obj}</span>, 실제 물체 쿼터니언 <span class="math inline">q_{obj}</span>, 실제 물체 속도 <span class="math inline">v_{obj}</span>, 실제 각속도 <span class="math inline">w_{obj}</span>를 포함하는 privileged 상태 정보 <span class="math inline">s_{privileged}</span>를 포함합니다.</li>
<li>Teacher policy의 행동 <span class="math inline">a</span>는 Underlying Geometric Fabric에 대한 입력으로, 목표 팜 위치 <span class="math inline">x_{f,target} \in \mathbb{R}^3</span>, 목표 팜 오리엔테이션 <span class="math inline">r_{f,target} \in \mathbb{R}^3</span>, 손가락의 목표 PCA 위치 <span class="math inline">x_{pca,target} \in \mathbb{R}^5</span>로 구성된 11차원 벡터입니다. Fabric은 60Hz로 통합되고 시뮬레이션은 60Hz로 진행되며, Teacher policy는 15Hz로 실행됩니다.</li>
</ul></li>
<li><strong>강건한 조작을 위한 환경 수정(Environment Modifications for Robust Grasping):</strong>
<ul>
<li><strong>랜덤 렌치 교란(Random Wrench Perturbations):</strong> 물체를 예측 불가능하게 움직이고 회전시키는 랜덤 렌치를 적용합니다. <span class="math inline">f_{perturb} = f_{scale} m u_f</span> 및 <span class="math inline">\tau_{perturb} = \tau_{scale} I u_\tau</span> (확률 <span class="math inline">p=0.1</span>로).</li>
<li><strong>포즈 노이즈(Pose Noise):</strong> 물체 포즈 관측에 비상관(uncorrelated) 및 상관(correlated) 노이즈를 추가하여 위치 및 기하학적 불확실성을 설명하고 손이 물체에 접근할 때 더 넓게 열리도록 유도합니다.</li>
<li><strong>마찰 감소(Friction Reduction):</strong> 물체의 기본 마찰 계수를 <span class="math inline">\mu = 0.7</span>로 줄여 마찰에 지나치게 의존하는 조작 행동을 완화합니다.</li>
<li><strong>도메인 무작위화(Domain Randomization):</strong> 시뮬레이션 매개변수에 대한 도메인 무작위화를 사용하여 다양한 동적 스펙트럼에 걸쳐 강건한 정책을 학습합니다.</li>
</ul></li>
<li><strong>보상 함수(Reward Function):</strong> 보상은 개별 보상 항 <span class="math inline">r = \sum_i w_i r_i</span>의 가중 합으로 정의됩니다.
<ul>
<li><span class="math inline">r_{to-obj} = \text{minimize}(\|x_{fingertips} - x_{obj}\|)</span></li>
<li><span class="math inline">r_{lift} = \text{minimize}(z_{lifted} - z(x_{obj})) \times (1 - \text{lifted}(x_{obj}))</span></li>
<li><span class="math inline">r_{lifted} = \text{lifted}(x_{obj})</span> (첫 번째 타임스텝)</li>
<li><span class="math inline">r_{to-goal} = \text{minimize}(\|x_{goal} - x_{obj}\|) \times \text{lifted}(x_{obj})</span></li>
<li><span class="math inline">r_{reached} = \mathbb{1}(\|x_{goal} - x_{obj}\| &lt; d_{success})</span></li>
<li><span class="math inline">r_{success} = \mathbb{1}(r_{reached} = 1 \text{ for } T_{success} \text{ consecutive timesteps}) \times (T_{max} - T)</span> 여기서 <span class="math inline">\text{minimize}(e)</span> 함수는 오차 <span class="math inline">e</span>가 현재까지의 최소 오차 <span class="math inline">e_{smallest}</span>보다 작아질 때만 양의 보상을 제공합니다. <span class="math inline">\mathbb{1}(c)</span>는 조건 <span class="math inline">c</span>가 참이면 1, 아니면 0입니다. 환경은 물체가 테이블 아래로 떨어지거나, <span class="math inline">r_{success}</span> 보상을 받거나, 에피소드 시간 제한에 도달하면 리셋됩니다.</li>
</ul></li>
</ul>
<p><strong>3. Student Depth FGP 훈련 (정책 증류)</strong></p>
<p>교사-학생 프레임워크와 온라인 DAgger[23]를 사용하여 전문가 정책을 실제 세계에 배포할 수 있는 학생 정책으로 증류합니다. 이 증류는 15Hz로 연속적인 이미지 입력을 사용하여 실제 세계에서 반응적이고 동적인 조작을 수행하는 pixels-to-action 정책을 만듭니다.</p>
<ul>
<li><strong>입력 및 출력:</strong> 증류 중 학생 정책 <span class="math inline">\pi_{depth}(o_{depth})</span>는 로봇 상태 <span class="math inline">o_{robot}</span>, 목표 위치 <span class="math inline">x_{goal}</span>, 그리고 원시 깊이 이미지 <span class="math inline">I \in [0.5, 1.5]^{160 \times 120}m</span>를 포함하는 관측 <span class="math inline">o_{depth}</span>를 받습니다. 학생은 행동 <span class="math inline">\hat{a} \in \mathbb{R}^{11}</span>과 물체 위치 예측 <span class="math inline">\hat{x}_{obj} \in \mathbb{R}^3</span>을 출력합니다.</li>
<li><strong>손실 함수(Loss Function):</strong> 학생은 감독 손실 <span class="math inline">L = L_{action} + \beta L_{pos}</span>로 훈련됩니다. 여기서 <span class="math inline">L_{action} = \|\hat{a} - a\|^2</span>이고 <span class="math inline">L_{pos} = \|\hat{x}_{obj} - x_{obj}\|^2</span>입니다. <span class="math inline">a</span>는 teacher policy <span class="math inline">\pi_{privileged}</span>가 예측한 행동이고 <span class="math inline">x_{obj}</span>는 시뮬레이터의 ground-truth 물체 위치입니다.</li>
<li><strong>깊이 이미지 증강(Depth Image Augmentations):</strong> 시뮬레이션에서 렌더링된 깊이 이미지에 픽셀 드롭아웃, 랜덤 값 설정, 선형 세그먼트(robot wires mimic), 비상관/상관 깊이 노이즈 모델 등 다양한 증강이 추가됩니다.</li>
</ul>
<p><strong>4. 실험 및 결과</strong></p>
<ul>
<li><strong>시뮬레이션:</strong> <span class="math inline">\pi_{depth}</span>는 140개의 훈련 물체에 대해 평균 99%의 성공률을 기록하여 <span class="math inline">\pi_{privileged}</span>의 성능과 거의 일치합니다. 물체당 평균 80%의 성공률을 보였습니다.</li>
<li><strong>실제 세계(Real-World):</strong>
<ul>
<li><strong>하드웨어 설정:</strong> Allegro Hand가 Kuka LBR iiwa arm에 장착되어 있고, Intel Realsense D415 카메라가 테이블에 고정되어 있습니다. 로봇은 23개의 독립적인 모터를 가지며, 단일 카메라 스트림을 정책 입력으로 사용합니다. 관절 PD 제어기는 팔에 대해 1kHz, 손에 대해 333Hz로 작동합니다. Geometric Fabric은 60Hz로, <span class="math inline">\pi_{depth}</span>는 15Hz로 작동합니다.</li>
<li><strong>단일 물체 조작 평가(Single Object Grasping Assessment):</strong> 11개의 표준 물체에 대해 시도당 5번의 조작을 수행한 결과, DextrAH-G는 Table 1에 보고된 바와 같이 새로운 최첨단 조작 성공률을 달성했습니다. 예를 들어, Pitcher는 80%, Pringles는 100%, Coffee Container는 100%, Cup은 80% 등의 성공률을 보였습니다.</li>
<li><strong>빈 패킹 평가(Bin Packing Assessment):</strong> 30가지 이상의 다양한 물체를 연속적으로 잡고 옆에 놓인 빈으로 운반하는 테스트입니다.
<ul>
<li><strong>연속 성공(CS):</strong> DextrAH-G는 8번의 시도에서 평균 6.56 ± 2.41개의 물체를 연속적으로 운반했습니다.</li>
<li><strong>사이클 시간(Cycle time):</strong> 평균 10.66 ± 0.84초, 즉 분당 5.63회 집기(PPM)의 속도를 기록했습니다.</li>
<li><strong>성공률:</strong> 총 256번의 시도 중 87%의 성공률로 모든 물체를 성공적으로 잡고 운반했습니다.</li>
</ul></li>
</ul></li>
</ul>
<p>DextrAH-G는 이러한 결과를 통해 능숙한 로봇 조작 분야에서 최첨단 성능을 크게 발전시켰으며, 실제 세계 응용에 더 가까워졌습니다. 수많은 테스트 시간 동안 하드웨어 손상은 발생하지 않았습니다.</p>
<p><strong>5. 한계(Limitations)</strong></p>
<p>DextrAH-G의 한계는 다음과 같습니다:</p>
<ol type="1">
<li>FGP가 손가락 제어를 위해 PCA 태스크 맵에서 목표를 발행하므로 로봇의 운동학적 민첩성을 제한합니다.</li>
<li>모델 기반 행동에 대한 의존도를 줄이기 위해 감각 입력 기반의 장애물 회피 행동이 학습되어야 합니다. Fabric의 장애물 회피는 로봇이 테이블과 심각하게 충돌하는 것을 막지만, 저자세 물체에 대한 효과적인 탐색을 어렵게 하여 성능을 저하시킵니다.</li>
<li>장면에서 한 번에 하나의 물체만 처리할 수 있으며, 복잡한 환경에서 효과적으로 작동하려면 분할(segmentation)과 같은 추가 변경이 필요할 수 있습니다.</li>
</ol>
<hr>
</section>
<section id="detail-review" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Detail Review</h1>
<section id="논문의-주요-기여점-요약" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="논문의-주요-기여점-요약"><span class="header-section-number">2.1</span> 논문의 주요 기여점 요약</h2>
<p>이 논문에서는 <strong>DextrAH-G</strong>라는 새로운 dexterous 로봇 파지(grasping) 방법을 제안하며, 이는 23자유도 로봇 팔-손이 <strong>depth 카메라 입력</strong>만으로 다양한 물체를 빠르고 안전하게 파지할 수 있는 <strong>픽셀-투-액션 정책</strong>이다. 주요 기여사항은 다음과 같다:</p>
<ul>
<li><strong>벡터화된 기하 패브릭 제어기 도입:</strong> RL 정책의 <strong>행동 공간(action space)</strong>으로 <strong>기하 패브릭(geometric fabric)</strong> 기반 제어기를 설계하였다. 이를 통해 정책 학습에 강력한 <strong>inductive bias</strong>을 제공하고, 로봇의 <strong>충돌 회피</strong> 및 <strong>관절 한계 준수</strong>를 보장함으로써 로봇 움직임을 안정적이고 자연스럽게 형성할 수 있었다.</li>
<li><strong>시뮬레이션 기반 교사 정책 학습:</strong> <strong>특권 정보(privileged information)</strong>를 활용하는 <strong>교사 FGP</strong>(fabric-guided policy)를 <strong>시뮬레이션에서만</strong> 강화학습으로 훈련하였다. 이 교사 정책은 앞서 언급한 패브릭 제어기 위에서 동작하여, 다양한 물체들에 대한 <strong>고성능 파지 동작</strong>을 학습하는 데 성공하였다.</li>
<li><strong>학생 정책 증류(distillation):</strong> 심도 카메라 등의 <strong>멀티모달 관찰</strong>에 기반한 <strong>학생 FGP</strong>를 훈련하여, 교사 정책의 행동을 모방함과 동시에 물체의 위치까지 추론(predict)하도록 하였다. 이 <strong>온라인 지식 증류</strong> 과정을 거친 학생 정책은 교사의 원래 성능을 재현하면서도 실세계 센서 입력으로 동작 가능하게 된다.</li>
<li><strong>제로샷 실환경 적용:</strong> 학습된 학생 정책을 <strong>현실 로봇에 별도의 추가 튜닝 없이 바로 적용</strong>함으로써, <strong>다양한 새로운 물체들</strong>에 대해 <strong>최고 수준(state-of-the-art)</strong>의 파지 성공률을 달성하였다. 이는 복잡한 형태의 물체도 사람처럼 잡을 수 있는, 이른바 “<strong>잡을 수 있는 것은 무엇이든 잡는(grasp-anything)</strong>” 능력에 한 걸음 다가선 성과로 평가된다.</li>
</ul>
</section>
<section id="사용된-기술모델-및-아키텍처-분석" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="사용된-기술모델-및-아키텍처-분석"><span class="header-section-number">2.2</span> 사용된 기술/모델 및 아키텍처 분석</h2>
<p><strong>DextrAH-G의 핵심은</strong> 세 단계로 구성된 <strong>교사-학생 학습 프레임워크</strong>와, 이를 뒷받침하는 <strong>기하 패브릭 기반 제어 아키텍처</strong>이다. 먼저, <strong>기하 패브릭 제어기</strong>란 비선형 기하학에 기반한 2차 동역학 제어기로서, 로봇의 움직임에 <strong>안전성과 안정성</strong>을 내재화한 기술이다. 이 제어기는 로봇 관절의 한계를 자동으로 고려하고, <strong>자가 충돌 및 환경 충돌을 회피</strong>하며, <strong>전역적으로 안정적인</strong> 경로로 로봇을 움직이게 한다. 또한 고차원 관절계를 <strong>저차원 행동 공간</strong>으로 노출시켜 제어 문제를 단순화하고, 다중 관절의 <strong>여유 자유도(redundancy)</strong>도 효율적으로 해소한다. 이러한 속성 덕분에, DextrAH-G에서는 학습된 RL 정책의 출력을 곧바로 로봇 모터에 보내지 않고 <strong>패브릭 제어기</strong>를 중간 계층으로 사용한다. 정책은 패브릭 상의 목표 동작(예: 손가락 끝 위치나 힘)을 출력하고, 패브릭 제어기가 이를 받아 <strong>로봇에게 안전한 실제 관절 명령</strong>으로 변환한다. 이는 일반적인 강화학습 정책이 종종 과격한 속도 명령이나 충돌 유발 동작을 내는 문제를 완화하며, 하드웨어 제약을 만족시키도록 명령을 <strong>가로채 변환하는 제어 계층</strong>을 둔다는 로봇 RL의 모범적인 설계와 일치한다. 특히 NVIDIA 연구진은 이러한 <strong>기하 패브릭 제어기</strong>를 대규모 병렬 RL 훈련에 사용 가능하도록 <strong>벡터화(vectorization)</strong>하여, 시뮬레이션 훈련부터 실제 배치까지 동일한 제어 로직을 일관되게 적용할 수 있음을 보였다. 그 결과 RL 정책이 패브릭 제어기와 함께 훈련되므로, 학습 단계에서부터 안전한 동작만 탐색하게 되고 sim2real 차이도 최소화된다.</p>
<center>
<img src="../../images/2025-07-23-dextrah-g/2.png" width="100%">
</center>
<p>DextrAH-G의 <strong>학습 과정</strong>은 다음의 <strong>3단계 파이프라인</strong>으로 이루어진다:</p>
<ol type="1">
<li><p><strong>교사 정책 학습 (Privileged Teacher FGP):</strong> 시뮬레이터 상에서 <strong>강화학습(RL)</strong>으로 <strong>교사 정책</strong>을 훈련한다. 이때 <strong>비대칭 Actor-Critic</strong> 기법을 사용하여, <strong>크리틱(가치망)</strong>에는 물체의 정확한 위치 등 <strong>모든 상태 정보</strong>를 제공하고 <strong>액터(정책)</strong>에는 로봇의 관절 상태나 노이즈가 추가된 센서 정보 등 <strong>제한된 관찰 정보</strong>만을 주는 방식으로 학습시킨다. 이를 통해 교사 정책은 현실에서도 이용 가능한 입력만으로 동작하면서도, 시뮬레이션에서는 충분한 정보로 효율적으로 학습할 수 있다. 학습 알고리즘으로는 Proximal Policy Optimization(PPO) 기반의 대규모 병렬 학습을 활용하였으며, <strong>LSTM 기반의 순환신경망</strong> 정책을 구성하여 부분 관측 상황에서도 메모리를 활용해 의사결정을 할 수 있게 했다. 특히 LSTM 층에는 <strong>skip-connection</strong>을 추가하여 출력값을 잔차 형태로 처리함으로써 순환신경망 학습의 <strong>안정성</strong>을 높였다. 또한 시뮬레이션 환경에서는 현실 적응력을 높이기 위해 다양한 <strong>도메인 랜덤화</strong>가 적용되었다. 예를 들어, 매 학습 에피소드마다 물체에 <strong>임의의 힘과 토크</strong>를 가해 위치를 흩뜨려보고, 물체 상태와 센서 관측에 <strong>노이즈</strong>를 추가하여 부분 관측 하에서도 견실한 파지 동작을 학습하도록 했다. 로봇의 물리 파라미터(질량, 마찰계수 등) 역시 범위를 두고 무작위로 변화시켜서 시뮬레이터와 현실 간 차이를 줄였다. 이렇게 훈련된 교사 FGP 정책은 시뮬레이터 내에서 <strong>다양한 물체</strong>에 대해 높은 성공률로 파지 및 조작을 수행할 수 있게 된다.</p></li>
<li><p><strong>학생 정책 학습 (Depth Student FGP Distillation):</strong> 두 번째 단계에서는, 앞서 얻은 교사 정책을 <strong>시연자(expert)</strong>로 삼아 <strong>학생 정책</strong>을 학습시킨다. 교사 정책과 동일한 환경에서 <strong>온라인 증류(distillation)</strong> 방법인 <strong>DAgger</strong>를 활용하여, 교사가 실행한 행동을 학생이 모방하도록 학습을 진행한다. 학생 정책은 <strong>연속적인 심도 영상</strong>을 주요 입력으로 받도록 설계되었으며, 약 <strong>15Hz 주기</strong>로 이미지를 받아들여 그때그때 상황에 반응하는 <strong>폐루프 정책</strong>을 학습한다. 학생 정책의 관측에는 <strong>로봇의 관절 상태 등의 proprioception</strong>도 포함되어 있으며, 심도 영상으로부터 <strong>물체의 위치를 추정하는 보조 출력</strong>도 내도록 멀티태스크 학습시켰다. 즉 학생 정책은 매 시각 <strong>심도 카메라 영상</strong>과 <strong>자기센서 값</strong>으로 현재 상황을 파악하고, <strong>다음 순간 로봇 팔과 손가락에 줄 명령</strong>을 출력함과 동시에 현재 <strong>물체의 예상 위치</strong>도 추론하여 보고한다. 이러한 설정은 실제 환경에서 물체가 보이지 않게 되는 <strong>가림(occlusion)</strong> 상황에서도 proprioception과 과거 정보를 활용해 물체 위치를 끝까지 추적할 수 있도록 하기 위함이다. 한편 학생 정책 학습시에도 시뮬레이션 심도 영상에 랜덤 노이즈, 잡음 객체 등을 추가하여 <strong>현실 카메라 화질</strong>과 최대한 유사하게 맞추는 기법을 사용하였다. 이 과정을 통해 결과적으로 학생 FGP는 교사 정책에 필적하는 성능을 가지면서도 <strong>픽셀 단위의 센서 입력</strong>만으로 동작하는 정책으로 거듭난다.</p></li>
<li><p><strong>실세계 배치 (Zero-Shot Deployment):</strong> 마지막으로, 이렇게 획득한 <strong>학생 정책</strong>을 <strong>현실 로봇</strong>에 이식한다. 시뮬레이션에서 학습된 모델을 별도 추가 학습이나 미세조정 없이 바로 <strong>제로샷(sim2real)으로 적용</strong>하는 것이 특징이며, 이는 앞서 사용된 <strong>기하 패브릭 제어기</strong> 덕분에 가능했다. 실제 로봇 하드웨어는 <strong>KUKA LBR iiwa 7-자유도 로봇팔</strong>에 <strong>Allegro 다관절 로봇 손(4 finger, 16 DOF)</strong>을 장착한 구성으로, 총 23개의 모터를 가진 플랫폼이다. 테이블 위에 Intel Realsense D415 깊이 카메라 한 대를 고정 설치하여 작업 공간을 내려다보게 했고, 이 <strong>단일 카메라 영상 스트림</strong>이 정책의 주된 시각 입력이다. 로봇 제어는 <strong>ROS 2</strong> 기반으로 구현되었으며, 팔과 손에 각각 <strong>1kHz와 333Hz 주기</strong>의 저수준 <strong>PD 제어기</strong>가 동작하고 있다. 학습된 <strong>FGP 정책 모듈</strong>은 별도 노드로 실행되어 <strong>15Hz 주기</strong>로 카메라 이미지와 로봇 상태를 받아 액션을 출력하며, <strong>기하 패브릭 제어기</strong>는 또 다른 노드에서 <strong>60Hz 주기</strong>로 실행되어 정책이 보낸 액션 명령을 실제 관절 명령으로 변환한다. 이처럼 정책과 패브릭 제어를 모듈화하여 병렬 실행함으로써, 설령 정책 모듈이 일시적으로 지연되거나 비정상 동작하더라도 패브릭 제어기가 <strong>로봇 움직임의 안전성</strong>을 지속적으로 관리할 수 있게 설계되었다. 결과적으로 DextrAH-G는 현실 환경에서 <strong>교사 정책을 모사한 학생 정책 + 패브릭 제어기</strong> 조합으로 구동되며, 시뮬레이션에서 보여준 높은 성능을 현실에서도 이어갈 수 있게 된다.</p></li>
</ol>
</section>
<section id="실험-설정-및-결과-분석" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="실험-설정-및-결과-분석"><span class="header-section-number">2.3</span> 실험 설정 및 결과 분석</h2>
<p><strong>DextrAH-G의 성능</strong>은 시뮬레이션과 실환경 모두에서 면밀히 평가되었다. 먼저 시뮬레이터상에서 학습된 교사 정책은 <strong>140개의 훈련 물체</strong>들에 대해 에피소드 기준 <strong>99%</strong> 이상의 성공률을 보였으며, 개별 물체 기준으로도 평균 <strong>80%</strong> 수준의 성공률을 달성했다. 학생 정책으로 증류한 후에도 시뮬레이터에서 교사 대비 근소한 성능 저하만 나타났을 뿐 대체로 유사한 파지 성공률을 유지했으며, 이는 sim2real 이전 단계에서 이미 정책 성능이 충분히 확보되었음을 의미한다.</p>
<p><strong>실세계 평가</strong>는 <strong>두 가지 프로토콜</strong>로 진행되었다. 첫째, <strong>단일 객체 파지 평가</strong>(single object grasping)로 표준 벤치마크에 준거한 실험이다. 여러 가지 <strong>대표적인 형태의 물체 11종</strong>을 선정한 후, 각 물체를 테이블 위에 다섯 번씩 임의의 자세로 배치하고 로봇이 이를 집어 들게 하여 성공 여부를 측정하였다. 만약 첫 시도에 실패해도 로봇이 <strong>연속적으로 재시도</strong>하도록 하여, 정책의 <strong>지속적인 적응 능력</strong>도 평가했다. 그 결과 DextrAH-G는 대부분의 물체에서 <strong>5회 시도 내 100%에 가까운 성공률</strong>을 기록하며, <strong>새로운 최고 성능</strong>을 달성했다. 구체적으로, 물체별 평균 성공률이 <strong>80%~100%</strong> 사이였고, 예를 들어 <strong>머그컵</strong>이나 <strong>과자 상자(직육면체)</strong>의 경우 기존 방법(Matak 등)은 <strong>0%</strong> 성공에 그쳤던 반면 DextrAH-G는 <strong>100%</strong> 성공을 거두었다는 보고가 있다. 이러한 결과는 DextrAH-G의 <strong>견고한 일반화 능력</strong>을 보여주는 것으로, 한정된 훈련 세트로 학습했음에도 불구하고 <strong>처음 보는 형태의 물체까지 실시간 파지가 가능</strong>함을 입증한다. 다만 이 단일 객체 평가는 속도나 연속 작업 상황을 반영하지 못하기 때문에, 논문 저자들은 추가로 <strong>연속 작업 평가</strong>를 제안하였다.</p>
<p>둘째 평가로 <strong>빈(pack) 채우기 연속 파지 테스트</strong>를 수행하였다. 이는 로봇이 <strong>다양한 물체들을 연달아 집어서 옆 상자에 옮기는 작업</strong>을 지속적으로 수행하도록 하는 시나리오로, <strong>실제 산업용 피킹 작업</strong>을 방불케 하는 응용 맥락을 실험에 도입한 것이다. 로봇 앞 테이블에 <strong>30여 종류</strong>의 다양한 물체를 무작위로 놓고 한 번에 하나씩 집어 들게 한 다음, 잡은 물체를 옆의 빈(bin) 상자에 떨어뜨리면 다시 새로운 물체를 잡는 식으로 <strong>실시간 연속 작업</strong>을 진행하였다. 이러한 한 사이클(집기-이동-놓기)에 걸리는 <strong>평균 시간(cycle time)</strong>과 <strong>연속 성공 횟수</strong>, 그리고 <strong>종합 성공률</strong>이 성능 지표로 사용되었다. 총 8회에 걸친 연속 테스트 결과, DextrAH-G는 <strong>평균 6.56회 연속 성공</strong>(한 번 실패하기 전까지 연속 집어 옮긴 물체 수, 95% 신뢰구간 ±2.41)으로 <strong>여러 물체를 연속 처리</strong>할 수 있음을 보였다. <strong>사이클 타임</strong>은 한 사이클당 <strong>평균 10.66초</strong>(표준편차 ±0.84초)로 측정되었는데, 이는 <strong>분당 약 5.63개의 물체</strong>를 옮길 수 있는 속도에 해당한다. 총 256번의 집기 시도 중 <strong>87%</strong>에서 최종적으로 물체를 성공적으로 옮기는 데에 성공하여, <strong>종합 성공률 87%</strong>를 기록하였다. DextrAH-G는 이처럼 <strong>높은 신뢰도(87% 성공)</strong>와 <strong>빠른 동작 속도(5.6 PPM)</strong>를 동시에 달성함으로써, 현재까지 보고된 덱스터러스 로봇 손 파지 연구 중 가장 뛰어난 <strong>속도-정확도 트레이드오프</strong>를 달성했다는 평가를 받는다. 저자들은 이러한 <strong>신속성과 신뢰성의 조합</strong>이 <strong>실제 활용에 한 걸음 다가선 성능 지표</strong>라고 강조하며, DextrAH-G의 연속 작업 사이클 타임이 이미 <strong>실용적 기준에 근접</strong>했다고述하고 있다. 참고로 산업공정 분석에 따르면 <strong>인간 작업자의 이상적인 피킹 속도</strong>는 분당 약 <strong>16.5회</strong> 정도로 추산되는데, DextrAH-G는 현재 그보다 약간 느리지만(≈5.6회/분) <strong>개선 여지</strong>를 충분히 가지고 있으며 가까운 미래에 속도를 더욱 높일 수 있을 것으로 기대된다. 실제로 논문에서는 <strong>연속 동작</strong> 중에도 로봇에 <strong>어떤 손상도 발생하지 않았으며</strong>, 수 시간 동안 수백 회에 걸친 테스트에도 <strong>하드웨어 고장이나 파손 없이</strong> 원활하게 작동했음을 보고하고 있어, <strong>안전성 측면</strong>에서도 본 기법의 우수함을 보여주었다.</p>
</section>
<section id="기존-연구와의-비교-및-차별점" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="기존-연구와의-비교-및-차별점"><span class="header-section-number">2.4</span> 기존 연구와의 비교 및 차별점</h2>
<p>DextrAH-G는 <strong>기존의 덱스터러스 그리퍼 연구들과 여러 면에서 차별화</strong>된다. 먼저 <strong>접근 방법의 차이</strong>가 두드러진다. 과거의 전통적인 로봇 파지 기법들은 주로 <strong>시각적 인식을 통한 그립 포즈 선정</strong>에 집중하고, 선정된 자세로 로봇을 움직이기 위해 <strong>모델 예측 제어(MPC)</strong>나 <strong>경로 계획 알고리즘</strong>을 사용하는 <strong>계층적 접근</strong>이 많았다. 이러한 방식은 개별 동작 단계(탐지-접근-파지)로 분리되어 있어 어느 정도 성과를 보였지만, <strong>실시간으로 연속적으로 반응하지 못하고</strong> 일회성 실행에 그치는 경우가 대부분이었다. 예를 들어 팔의 <strong>전체 관절 자유도</strong>를 모두 활용하여 파지 지점을 향해 유연하게 접근하거나, <strong>파지 후에 손가락과 팔을 조화롭게 재조정</strong>하는 등의 능력이 부족했다. 반면 <strong>DextrAH-G는 한 번에 끝까지 계획을 세우는 대신</strong>, <strong>센서 데이터가 들어올 때마다 15Hz로 계속 재계획 및 제어</strong>한다는 점에서 <strong>진정한 실시간 폐루프 제어</strong>를 구현하였다. 카메라 영상과 로봇 관절 센서 등 <strong>모든 정보원을 통합(fuse)</strong>하여 <strong>고주파수 제어 명령</strong>을 생성함으로써, 부분 관측 환경에서 발생하는 불확실성에 대응하고 파지 동작의 성공률을 향상시킨 것이다. 이는 기존 방식과 달리 <strong>항상 주변을 감지하고 즉각 피드백을 반영</strong>하므로, 물체가 미끄러지거나 움직이더라도 신속히 보정할 수 있다는 강점이 있다.</p>
<p><strong>학습 기반 기법들의 입력과 모델 의존성 측면</strong>에서도 중요한 차이가 존재한다. 기존의 많은 딥러닝/강화학습 기반 덱스터러스 파지 연구들은 <strong>심층 센서 데이터의 한계</strong> 또는 <strong>사전 지식에 대한 의존</strong>을 가지고 있었다. 예를 들어, 3D 물체 모델로부터 수십만 건의 <strong>합성 파지 데이터셋</strong>을 만들고 학습하거나, 물체의 CAD 모델을 미리 알고 있어야만 사용할 수 있는 <strong>분석적 지표</strong>(예: form closure, force closure 등)를 활용하기도 했다. 일부 최신 연구들은 <strong>전체 물체의 포인트클라우드</strong>를 입력으로 받아 파지 후보를 생성하고 RL 정책으로 집는 시도를 했으나, <strong>실제 적용 시에는 물체의 정확한 3D 모델</strong>을 알아야 하거나 센서 점구름과 그 모델을 <strong>정합(registration)</strong>해야 하는 어려움이 있어 <strong>범용성이 떨어지는</strong> 문제가 있었다. 예를 들어 Liu 등(2023)은 손-물체 상호작용을 표현하는 새로운 피처를 제안했지만, <strong>실제로는 물체의 CAD 모델을 센서 데이터에 맞춰 정렬해야</strong> 했기 때문에 현실에서는 적용이 제한적이었다고 보고한다. 그 외에도 Agarwal 등(2023)은 <strong>이미 학습된 비전 트랜스포머(DINO-ViT)</strong> 특징으로 물체를 분류하여 <strong>사전 정의된 손 자세(eigengrasp)</strong>로 잡는 RL 정책을 제시했으며, Qin 등(2022)은 <strong>특정 범주(category)의 물체들</strong>에 대해서만 동작하는 <strong>포인트클라우드 기반 RL 정책</strong>을 선보였다. 이들은 모두 흥미로운 접근이지만, <strong>특정 상황에 특화</strong>되어 있거나 <strong>시뮬레이션에 한정</strong>되는 경우가 많았다. <strong>DextrAH-G는 이러한 제약을 대폭 완화</strong>하여, <strong>단 한 대의 심도 카메라로부터 얻은 깊이 영상</strong>과 <strong>로봇의 관절각 등의 proprioception</strong>만으로 <strong>팔과 손을 동시에 제어</strong>하는 <strong>일원화된 정책</strong>을 학습시켰다. <strong>물체의 CAD 모델이나 사전 촬영 데이터베이스 없이</strong>도 동작하며, 한 번 학습되면 <strong>카테고리에 상관없이 새로운 물체</strong>에도 바로 적용 가능함을 실제로 시연한 것이 큰 차별점이다. 요약하면, DextrAH-G는 <strong>센서리얼(sim-to-real)</strong> 관점에서 훨씬 간결하고 실용적인 입력만으로 훈련되었음에도 기존 기법들보다 뛰어난 범용 파지 능력을 보여준다.</p>
<p><strong>제어 및 안전성 측면</strong>에서도 DextrAH-G는 기존 연구들과 다른 철학을 취하고 있다. 전통적인 RL 기반 로봇 정책은 주로 <strong>조인트 PD 제어기나 OSC</strong>(작업공간 제어) 같은 <strong>단순 제어기</strong>에 명령을 보내는 형태로 작동했다. 이렇게 낮은 수준의 단순 제어를 쓰면 구현은 쉽지만, <strong>로봇 행동의 모든 세부를 학습된 정책이 담당해야</strong> 하므로 학습 난이도가 크게 증가하고, 자칫 <strong>충돌 회피나 관절 한계 준수</strong>같이 중요한 안전 요소들이 정책에 의해 제대로 학습되지 않을 위험이 있다. 실제로 고차원 행동 우주에서 복잡한 우선순위(목표 달성, 하드웨어 보호, 자연스러운 움직임 등)를 <strong>신경망 하나로 모두 발견</strong>해내는 것은 극히 어렵기 때문에, <strong>정책이 최적화 과정에서 편협한 해법(local optima)</strong>에 빠지거나 <strong>예상치 못한 부자연스러운 동작</strong>을 만들어내기 쉽다. <strong>DextrAH-G는 이러한 문제를 해결하기 위해</strong>, <strong>제어기 쪽에 많은 지능을 미리 넣어두는 방식</strong>을 채택했다. 앞서 설명한 <strong>기하 패브릭 제어기</strong>가 바로 그런 역할을 하며, 이 <strong>고도화된 제어 레이어가 알아서</strong> 충돌을 피하고 관절 제약을 지키며 손가락 말단의 움직임을 <strong>의미있는 방식으로 유도</strong>해준다. 실제 Van Wyk 등(2024)의 선행 연구에서는 이 패브릭 제어기를 활용해 <strong>손가락 끝이 물체를 향하도록 자연스럽게 끌어당기는 힘</strong>을 적용하고 <strong>관절각 제한을 자동 처리</strong>함으로써, 복잡한 <strong>다관절 손 내 물체 재배열</strong> 작업에서 새로운 SOTA 성능을 낸 바 있다. DextrAH-G 역시 이러한 <strong>패브릭 가이드 정책(FGP)</strong> 접근을 계승하여, RL 정책이 <strong>주요 목표 달성</strong>에만 집중해도 될 만큼 다른 부수적인 행동들은 <strong>제어기가 책임지도록</strong> 만들었다. 그 결과 <strong>보상 함수를 단순화</strong>할 수 있었고(예: 충돌 최소화 등의 보조 보상을 크게 신경쓰지 않아도 됨), 정책 최적화도 수월해졌다. 더 중요한 점은, 패브릭 제어기가 정책의 <strong>과격한 출력으로부터 로봇을 보호</strong>해주기 때문에 훈련된 정책을 실제 로봇에 적용할 때 <strong>안전성이 확보</strong>된다는 것이다. 논문에서도 이전 세대 RL 정책들은 실험 중 <strong>모터가 과열되고 연기가 나는 고장</strong>을 겪기도 했으나, DextrAH-G에서는 <strong>패브릭 제어층 덕분에 그런 사태 없이</strong> 자유롭게 실험을 반복할 수 있었다고 언급된다. 요컨대, DextrAH-G는 <strong>학습과 제어의 긴밀한 통합</strong>을 통해 기존 연구들이 직면했던 안전-성능 딜레마를 해결하고자 한 점에서 차별화되며, 이러한 <strong>모델 기반 + 학습 혼합 전략</strong>은 복잡한 로봇 기술 학습에 있어 한 방향성을 제示하고 있다.</p>
<p><strong>성능 비교</strong> 측면에서도 DextrAH-G의 우수성은 돋보인다. 단일 객체 파지 평가에서 제시된 <strong>성공률 수치</strong>만 보더라도, DextrAH-G는 기존의 거의 모든 방법들을 능가하는 결과를 얻었다. 예컨대, <strong>Dex-_diffuser</strong> 기반 생성모델 방법이나 <strong>ISA-Grasp</strong>, Matak 등의 기존 기법들이 각각 제한된 물체군에서 40~80% 정도의 성공률을 보고한 데 비해, DextrAH-G는 <strong>모든 평가 물체에 대해 80% 이상 (대부분 100%)</strong>의 성공률을 시현하였다. <strong>연속 파지 시험</strong>에서도 87%의 종합 성공률과 5.6 PPM의 속도로 <strong>명확한 우위</strong>를 보였으며, 논문 저자들은 이러한 <strong>신뢰도와 속도의 조합이 현 시점 덱스터러스 파지의 새로운 SOTA</strong>임을 강조하고 있다. 요컨대, DextrAH-G는 <strong>학습 기반의 범용 파지 정책</strong>임에도 불구하고 <strong>기존의 특화된 방법들</strong>(예: 특정 물체군 전용 RL, 사전 모델 기반 계획 등)을 능가하는 성능과 범용성을 동시에 달성하여 두각을 나타낸다. 또한 <strong>sim-to-real</strong> 측면에서도, 다른 많은 연구들이 현실 적용을 위해 추가 튜닝이나 도메인 적응 단계를 필요로 한 반면 DextrAH-G는 <strong>한 번의 시뮬레이터 학습으로 곧장 현실 로봇에 투입</strong>하여 성과를 냈다는 점에서 실용적인 우월성이 있다. 이런 차별점들 덕분에 DextrAH-G는 <strong>“픽앤플레이(pick-and-play)”에 가까운 범용 로봇 파지 시스템</strong>의 가능성을 보여준 것으로 평가된다.</p>
</section>
<section id="장점과-한계점" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="장점과-한계점"><span class="header-section-number">2.5</span> 장점과 한계점</h2>
<section id="장점" class="level3" data-number="2.5.1">
<h3 data-number="2.5.1" class="anchored" data-anchor-id="장점"><span class="header-section-number">2.5.1</span> 장점</h3>
<ul>
<li><strong>탁월한 성능과 범용성:</strong> DextrAH-G는 <strong>다양한 형태·크기의 물체 30여 종</strong>에 대해 <strong>87%</strong>의 높은 파지 성공률을 달성하며, 현재까지 보고된 덱스터러스 파지 기술 중 <strong>최고 수준의 성능</strong>을 보여준다. 심도 카메라 한 대만으로 동작하면서도 이전 방법들이 다루기 어려웠던 <strong>불특정 다수의 새로운 물체</strong>들을 제약 없이 파지해 보임으로써, 향후 <strong>범용 로봇 파지(grasp-anything)</strong>에 한 발 다가선 성과를 이뤘다.</li>
<li><strong>빠른 작업 속도:</strong> 본 시스템은 한 사이클(집기-운반-복귀)에 평균 <strong>10.66초</strong>밖에 걸리지 않아 <strong>분당 약 5.6회</strong>의 연속 집게 작업을 수행할 수 있었다. 이러한 <strong>사이클 타임 단축</strong>은 다관절 로봇 손으로 이루어지는 파지 작업에서는 매우 고무적인 결과로, <strong>산업적 활용에도 근접한 속도</strong>이다. (인간 작업자의 이상적 파지 속도가 약 16.5회/분으로 추정되는 것에 비하면 약 1/3 수준이지만, 현재 로봇 손으로 달성한 속도 중에서는 최고 수준이며 추가 개선의 여지가 있다.)</li>
<li><strong>안전한 연속 동작:</strong> DextrAH-G의 정책 출력은 항상 <strong>기하 패브릭 제어기</strong>를 거쳐 로봇을 구동하므로, 로봇을 <strong>무리한 동작으로부터 보호</strong>하는 장치가 내재되어 있다. 실제 여러 시간에 걸친 수백 회의 실험 동안 <strong>모터과열이나 기구 손상 없이</strong> 시스템을 운영할 수 있었으며, 실험자들이 <strong>로봇 파손에 대한 우려 없이</strong> 정책을 자유롭게 테스트할 수 있었던 점은 큰 장점이다. 이전 세대의 RL 정책들이 종종 급격한 관절 움직임으로 하드웨어에 부담을 주었던 것과 달리, DextrAH-G는 <strong>연속적인 반응 속도</strong>를 유지하면서도 <strong>충돌 회피와 관절 제한</strong>을 준수하여 <strong>안정적인 동작</strong>을 보장한다.</li>
<li><strong>학습 효율성과 현실 적용 용이성:</strong> DextrAH-G는 <strong>전체 학습을 시뮬레이션에서 완료</strong>함으로써, <strong>비싼 현실 로봇을 사용한 데이터 수집 없이</strong>도 고성능 정책을 얻어냈다. 도메인 랜덤화, 교사-학생 증류 같은 기법을 통해 <strong>sim2real 격차</strong>를 성공적으로 극복한 덕분에, 추가 실환경 학습 없이도 바로 현실 투입이 가능했다는 점은 실용적인 매력이다. 이는 향후 유사한 로봇 과제들에서도 <strong>시뮬레이터 기반 대량 학습 → 현실 즉시 배치</strong>의 워크플로를 활용할 수 있다는 희망을 준다. 또한 패브릭 제어기 덕분에 복잡한 보상 설계나 제약 조건을 일일이 벌점으로 학습시키지 않아도 되어 RL <strong>학습 난이도</strong> 자체도 완화된 측면이 있다.</li>
<li><strong>자연스럽고 조화로운 동작:</strong> 패브릭 제어기가 로봇의 운동을 <strong>글로벌하게 최적화된 경로</strong>로 유도해주기 때문에, DextrAH-G의 팔과 손 동작은 비교적 <strong>부드럽고 일관된 모션</strong>을 보여준다. 기존의 일부 RL 정책들은 목표 달성에는 성공해도 사람이 보기에는 <strong>부자연스러운 자세</strong>를 취하거나 불안정한 동작을 보이는 경우가 있었는데, DextrAH-G는 <strong>안정적이고 사람 손과 유사한 움직임 패턴</strong>으로 물체를 잡고 옮겨 <strong>로봇 동작의 품질</strong> 측면에서도 진전을 이루었다. 이는 패브릭 제어를 통해 <strong>자연스러운 손가락 접촉 경로</strong>를 형성하고 관절 움직임을 <strong>매끄럽게 보간</strong>해준 결과로 볼 수 있다.</li>
</ul>
</section>
<section id="한계점" class="level3" data-number="2.5.2">
<h3 data-number="2.5.2" class="anchored" data-anchor-id="한계점"><span class="header-section-number">2.5.2</span> 한계점</h3>
<ul>
<li><strong>손가락 운동 범위의 제한:</strong> 현재 DextrAH-G의 상위 FGP 정책은 손가락 동작을 <strong>PCA(taskmap) 좌표계의 주요 축 방향</strong>으로만 목표치를 내도록 제한되어 있다. 이는 <strong>파지 행동 학습에 집중</strong>하기 위한 의도적 설계였으나, 그 대가로 손가락의 <strong>세밀한 조작 능력(kinematic dexterity)</strong> 일부를 포기한 셈이 된다. 다시 말해 손가락이 만들어낼 수 있는 다양한 모션 중 주성분적인 일부만 사용하므로, 잠재적인 복잡한 조작(skill)의 표현력이 떨어질 수 있다. 향후에는 손가락 운동 자유도를 더 활용하면서도 학습 효율을 유지할 방법이 과제로 남아 있다.</li>
<li><strong>학습 기반 충돌 회피의 부재:</strong> DextrAH-G는 <strong>로봇-환경 간 충돌 회피</strong>를 주로 <strong>패브릭 제어기의 내장 기능</strong>에 의존하고 있다. 이상적으로는 정책 자체가 카메라 등 <strong>센서 정보를 통해 장애물을 인지하고 회피 행동을 학습</strong>하는 것이 바람직하지만, 현재 정책은 이러한 능력이 없다. 그 결과 시뮬레이션 학습 시에도 <strong>테이블 면과 가까운 영역</strong> 등 <strong>높은 비용(risk)의 상태</strong>를 적극적으로 탐색하지 못하고 피해가는 경향이 있었다. 실제로 패브릭 제어기가 테이블과의 큰 충돌은 예방해주지만, <strong>너무 보수적으로 회피</strong>하다 보니 <strong>테이블에 납작하게 놓인 낮은 물체</strong>를 잡을 때 어려움을 겪는 현상이 나타났다. 이 부분은 <strong>강화학습의 탐색 전략 개선</strong>이나 <strong>커리큘럼 학습</strong> 등을 통해 정책이 어느 정도 <strong>충돌을 무릅쓴 탐색</strong>도 할 수 있게 유도하는 방향으로 보완이 필요하다.</li>
<li><strong>탐색 알고리즘 및 RL 안정성:</strong> 앞의 문제와 관련하여, 현 정책의 RL 알고리즘이 <strong>고비용 영역을 탐색</strong>하는 데 한계를 보인다는 지적이 있다. 패브릭 제어기가 없었다면 아마 로봇이 테이블에 부딪히는 등의 시도를 통해 낮은 물체 잡는 법을 배울 수도 있었겠지만, 제어기의 보호막 때문에 애초에 해당 영역을 경험하지 못한 것이다. 이를 해결하려면 <strong>더 똑똑한 탐색 기법</strong>이나 <strong>알고리즘적 개선</strong>(예: 안전 탐험, 보상 도메인 개선 등), 또는 <strong>일부러 충돌에 가까이 가보도록 유도하는 커리큘럼</strong>을 도입하는 등 연구가 필요하다.</li>
<li><strong>복잡한 환경에서의 활용 한계:</strong> 현재 DextrAH-G 시스템은 <strong>단일 물체 파지</strong>에 초점을 맞추고 있어, 한 장면(scene)에 <strong>여러 개의 물체가 섞여 있는 경우</strong>에는 대처하기 어렵다. 예를 들어 바닥에 흩어진 여러 물건 중 하나를 집으라고 하면, <strong>어떤 물체를 목표로 할지 인식</strong>하는 능력이나 잡고자 하는 대상 이외의 다른 물건을 무시하는 처리가 필요하다. 논문에서도 <strong>“하나의 장면에 한 물체만 존재한다”</strong>는 가정 하에 정책을 개발했기 때문에, 이를 다수 객체 <strong>난잡한 환경(clutter)</strong>으로 확장하려면 <strong>시각적 세그멘테이션</strong> 모듈을 붙이거나 정책을 여러 단계로 구성하는 등의 추가 설계가 필요함을 언급하고 있다. 실제 응용을 위해서는 로봇이 장면 내 목표 물체를 <strong>스스로 식별</strong>하고 그 물체에만 초점을 맞춰 파지하는 기술과의 통합이 과제로 남는다.</li>
</ul>
<p>이런 한계에도 불구하고, DextrAH-G는 현재까지 보고된 결과만 놓고 보면 <strong>덱스터러스 로봇 파지 분야의 상당한 진일보</strong>를 이뤄냈다. 고속·고성공률의 파지, 안전한 실환경 동작, 그리고 범용적인 물체 대응력 등을 모두 갖춘 사례로서, 향후 이 분야 연구와 실제 산업적용에 귀중한 참고가 될 것으로 기대된다. 동시다발적인 여러 연구의 발전 속에서 DextrAH-G가 보여준 <strong>교사-학생 학습 + 기하패브릭 제어</strong>의 조합은, 향후 <strong>고성능 로봇 스킬 학습</strong>을 위한 하나의 유망한 방향으로 평가할 만하다. 이번 연구를 바탕으로 남은 한계점들 – 예컨대 복잡한 작업 공간, 더욱 높은 속도 향상, 연성 물체나 촉각 활용 등 – 에 대한 후속 연구가 지속된다면, <strong>‘무엇이든 정확히 빨리 집어내는’ 범용 로봇 손</strong>의 실현도 머지않아 보인다.</p>
<!--

> Review of *“DextrAH-G: Pixels-to-Action Dexterous Arm-Hand Grasping with Geometric Fabrics”*

## Introduction

Achieving **fast, safe, and robust dexterous grasping** across diverse objects remains a core challenge in robotics. Traditional grasping pipelines often separate perception, grasp pose planning, and execution, which can limit responsiveness and coordination between a robot’s arm and hand. Many existing methods compute a grasp pose from vision and then rely on a motion planner or predefined controller to reach that pose. This approach is **not continuously reactive** and typically does not jointly plan through all degrees-of-freedom (DOFs) of a dexterous arm-hand system, often ignoring coordinated finger-arm adjustments after grasping. Furthermore, prior solutions frequently suffer from **limited speed, unnatural motions, poor generalization,** or lack safety guarantees (e.g. risk of self-collisions or joint limit violations). These limitations hinder real-world deployment in tasks like warehouse bin picking or home assistance, where a robot must rapidly grasp a variety of unknown objects safely and reliably.

**DextrAH-G** (Dexterous Arm-Hand Grasping with Geometric Fabrics) is a new system that directly addresses these challenges. It introduces a **pixels-to-actions policy** for a 23-DOF robotic arm-hand (7-DOF robot arm + 16-DOF Allegro hand) that continuously **reacts to streaming depth images** to grasp and transport objects in real time. The policy is trained entirely in simulation and deployed **zero-shot** on real hardware, achieving **state-of-the-art dexterous grasping performance** on many novel objects without any on-hardware fine-tuning. A key insight of DextrAH-G is to combine **deep reinforcement learning (RL)** with a **geometric fabric controller** and a **teacher–student distillation** strategy. This integrated approach provides several benefits: (1) the geometric fabric imposes physics-based constraints and bias that ensure safety and natural motion, (2) the RL policy learns on top of this fabric, simplifying learning by focusing on high-level objectives, and (3) a privileged teacher policy (with access to additional state information) is distilled into a **vision-based student policy**, enabling robust perception-to-control mapping.

In summary, **DextrAH-G** offers a **unified arm-hand grasping framework** that is **continuously reactive**, **safety-guaranteed**, and **generalizes** across object geometries. It represents a significant step toward “grasp anything” capability in dexterous robotics. This review will delve into the **proposed methodology** – how DextrAH-G’s architecture works and how **Geometric Fabrics** are integrated – and highlight **key innovations and differences** from prior works. We will also summarize the **experimental results** that validate the approach, discuss **limitations**, and provide a final assessment of the work’s impact.

## Methodology Summary

**System Overview:** *DextrAH-G* comprises a **fabric-guided policy (FGP)** layered on top of a **geometric fabric controller**. The fabric controller serves as a low-level motion generator that respects safety constraints, while the high-level policy issues goals (actions) in an abstract action space defined by the fabric. The overall training and deployment pipeline has three stages:

(1) Train a **privileged teacher FGP** in simulation via reinforcement learning (the teacher has access to certain state information for easier learning),
(2) **Distill** this teacher policy into a **student FGP** that uses only realistic inputs (depth images and proprioception),
(3) **Deploy** the student policy on the real robot zero-shot, using a simple state machine to handle repetitive grasp-and-drop cycles.

Figure 2 illustrates this pipeline: a teacher policy is trained in a vectorized simulation environment, then a student is learned from it, and finally the learned policy is run on a real robot performing continuous bin-picking.

<center>
<img src="../../images/2025-07-23-dextrah-g/2.png" width="80%" />
</center>

> *Figure 2: Overview of the DextrAH-G framework. A teacher policy is trained with reinforcement learning over a geometric fabric controller (top). The teacher’s behavior is then distilled into a depth-image-based student policy (middle). At deployment, the student policy controls a 23-DOF arm-hand robot in real time, coordinated by a state machine for continuous grasp-and-transport tasks (bottom). The geometric fabric runs as an underlying controller throughout, ensuring safety and shaping motions.*

**Geometric Fabric Controller:**

At the core of DextrAH-G is a **geometric fabrics**-based controller that drives the robot’s joint motions. Geometric fabrics are a mathematical framework that generalizes classical mechanics and Riemannian Motion Policies (RMPs) to design stable, composable motion behaviors. In essence, a fabric defines a second-order dynamical system: it specifies a state-dependent metric (analogous to a mass/inertia matrix) and a set of potential fields or “forces” that govern acceleration of the system. Multiple fabric terms can be combined, each encoding a desired behavior (e.g. move toward a target, avoid collision), and the framework guarantees **stable and smooth motions** without sacrificing modeling fidelity. Prior work showed that fabric-based controllers can outperform RMPs and other motion primitives in complex tasks, providing strong theoretical underpinnings for stability and optimality.

For DextrAH-G, the authors design **the most feature-rich geometric fabric to date** for an arm-hand system. This fabric controller explicitly handles:

* **Joint Limit Enforcement:** Joint angle and velocity limits are incorporated into the fabric (similarly to prior work by Allshire et al.) to ensure the robot never commands motions outside its safe joint range. This guarantees hardware safety by construction.
* **Collision Avoidance:** Both **self-collisions** and **environment collisions** are avoided through dedicated fabric terms. The robot’s geometry is approximated by a set of spheres on each link; using forward kinematics, the fabric monitors distances between these spheres and any obstacles (including the robot’s own links or the table). If a sphere approaches a collision, the fabric generates a **repulsive acceleration** pushing the robot away from the obstacle. The avoidance behavior is tuned to be **velocity-aware** (slowing or diverting motion toward collision) and to produce smooth, **speed-invariant evasive paths**. A damping term prevents any residual penetration when near contact, ensuring the robot safely slows as it nears a surface. These design elements let the robot operate quickly while **provably preventing crashes**, even if the high-level policy issues aggressive or unsafe commands.
* **Posture and Redundancy Control:** A fabric term is included to maintain a favorable arm posture for dexterous manipulation. In high-DOF systems, there are typically infinitely many arm configurations (elbow angles, etc.) to reach a target pose. The DextrAH-G fabric biases the arm toward configurations that maximize **kinematic manipulability** and natural movement (details are given in Appendix, but intuitively it means the arm avoids kinematic singularities or overly stretched postures). This **posture-shaping** fabric guides the robot to move in a human-like, balanced way without explicit instruction.
* **Low-Dimensional Action Spaces:** Perhaps most critically, the fabric exposes a **compressed action space** for the RL policy. Directly controlling all 23 joints would be a huge action space for learning. The authors leverage human grasping synergies to reduce this dimensionality. They retargeted a dataset of human grasp motions to the robot hand and performed a **Principal Component Analysis (PCA)** on these motions. The first 5 principal components of finger joint movement (which capture common grasp shapes like power grasps and pinch grasps) form a 5-dimensional **hand action manifold**. Essentially, rather than commanding each finger joint, the policy will command a 5-D vector that corresponds to a coordinated finger posture (a combination of joint movements) along human-derived grasp axes. Likewise, for the arm, instead of commanding 7 joint angles, the policy controls the **end-effector (palm) pose** in 6 dimensions (3 for translation and 3 for orientation). The fabric maps these 6-D end-effector targets to the full arm joint motions by treating the palm as a rigid body with multiple reference points (7 points on the palm, giving a 21-D configuration space that the fabric controller uses internally). In summary, the **policy’s action space is 11-D** (5 dimensions for hand configuration + 6 for arm pose), which greatly simplifies learning while still affording dexterous control. The geometric fabric continuously converts these 11-D high-level actions into smooth 23-D joint commands at 60 Hz, obeying all the above constraints. This acts as a form of **inductive bias** – the policy’s outputs always result in safe, reasonable motions, so the RL agent can focus on *what* grasping motion to perform rather than *how* to execute it safely.

**Reinforcement Learning of a Privileged Teacher Policy:**

With the fabric in place, the authors formulate dexterous grasping as an RL problem. They train a **teacher policy** in simulation using a massive set of 140 diverse objects. The teacher is a *fabric-guided policy* (FGP) – meaning its actions are the 11-D target commands (palm pose + PCA finger coordinates) that get fed into the fabric controller. Training such a policy end-to-end from pixels would be difficult, so they adopt an **asymmetric actor-critic** approach with privileged information. In asymmetric training, the **critic** (value function) has access to full state information (ground-truth object pose, velocities, contact forces, etc.) to better judge the value of states, while the **actor (policy)** is given a limited, more realistic observation. Notably, the teacher’s observation is not raw pixels but rather a curated set of state features: it includes the robot’s proprioceptive state (joint positions/velocities, fingertip positions, etc.), the fabric’s internal state, and a **noisy estimate of the object’s position and orientation**, along with a one-hot object identity vector. By providing the object’s approximate pose (with noise) and an object ID, the teacher policy can learn grasp strategies specific to the object geometry without needing to infer everything from vision. At the same time, the added noise and the limited sensing (no raw image) ensure the teacher doesn’t become too reliant on perfectly accurate state information, which the student won’t have. The critic, on the other hand, gets the true object state and contact forces, enabling more precise reward estimation and helping stabilize training.

Thanks to the geometric fabric’s guarantees, the **reward function** for RL can be kept remarkably simple. The fabric already prevents collisions or unnatural motions, so the reward focuses on **what matters for grasp success**: encouraging the fingers to make contact with the object and lifting the object off the table to a target height. In fact, the authors report that the reward is *entirely centered on fingertip-object contact and lifting the object*, with no terms needed for collision avoidance or arm posture. This significantly reduces reward shaping effort and avoids “reward hacking” behaviors, since the policy doesn’t get extraneous incentives beyond grasp success. The training was done in a *vectorized simulation* (likely using a platform like NVIDIA Isaac Gym, given the involvement of NVIDIA) to collect massive experience. To promote **robustness and generalization**, the authors applied several environment randomizations and perturbations during training: random wrench forces were applied to objects to disturb them (forcing the policy to learn strong, stable grasps), noise was added to the perceived object pose (making the teacher open the hand wider or be tolerant to uncertainty), the friction of objects was artificially reduced (preventing the policy from relying on unrealistically high friction), and standard domain randomization (varying simulation dynamics, textures, etc.) was used to bridge the sim-to-real gap. These measures encourage the learned policy to be **robust to perturbations, sensing errors, and different object properties**, which is crucial for success in the real world.

After training, the teacher FGP is a policy that takes in a partial state observation and outputs an 11-D action (target arm pose and hand PCA coordinates) at **15 Hz**, which is then upsampled by the fabric controller (running at 60 Hz) to smooth joint commands. This teacher achieved high performance in simulation (as we will discuss in the results section). However, the teacher still relies on knowing an approximate object pose and identity – information that would come from a vision system in reality. Instead of building a separate object estimator, the authors opt to **distill this expertise into a single depth-based policy**.

**Student Policy Distillation (Depth-Based FGP):**

The next stage is to obtain a **vision-driven policy** that can replace the teacher. DextrAH-G employs a **teacher–student distillation via DAgger (Dataset Aggregation)**. In the distillation process, the **student** policy is a neural network that takes **raw depth images** (from an onboard depth camera) along with the robot’s proprioceptive readings as input, and outputs the same type of action as the teacher (the 11-D fabric action) *plus an auxiliary prediction of the object’s 3D position*. The student is trained in simulation by having it mimic the teacher’s actions: at each time-step, the student observes the depth image of the scene and the robot state, and the teacher provides the “ground truth” action for that state. The training uses a supervised loss that penalizes the difference between the student’s predicted action and the teacher’s action, as well as the error in the student’s predicted object position (compared to the true object position in simulation). Essentially, the student learns to imitate the teacher’s control decisions and simultaneously perform **object pose estimation** from depth. Importantly, because the student fuses depth vision with proprioceptive inputs, its internal estimate of object position can be more **robust to occlusions** (e.g., fingers partially blocking the object) than an external vision module alone. The authors note that the student’s learned object position prediction is accurate enough to be used by the high-level state machine (to know, for example, when an object has been grasped and lifted). To further aid sim-to-real transfer, the depth images in training are augmented with realistic noise and distractor artifacts (to resemble real sensor data with clutter or imperfect readings). After this imitation learning stage (which can be thought of as *online learning from the teacher* in simulation), the resulting student FGP is a **15 Hz closed-loop vision policy**: it observes depth frames continuously and outputs smooth arm-hand actions through the geometric fabric controller, now **without needing any explicit object pose input**.

**Real-World Deployment:**

The final system, dubbed **DextrAH-G**, consists of the learned depth-based student policy running in tandem with the geometric fabric controller on a real robot. A simple finite-state machine coordinates the overall **grasp-and-transport sequence**. In practice, this means the robot will: look for an object, reach and grasp it under policy control, then once grasped, switch to a transport sub-policy to move the object to a drop location (e.g. a bin) and release it, then reset for the next object. The policy handles the continuous movements during reaching and grasping, while the state machine likely handles discrete events like “object lifted – now transition to drop motion” using the student’s predicted object position and contact feedback. Importantly, the **geometric fabric runs in real time (60 Hz)** on the robot, taking the student’s 15 Hz high-level commands and ensuring all motions remain safe and smooth. The authors emphasize that even if the learned policy were to output a hazardous action (say, moving the arm quickly toward a wall or singular configuration), the fabric controller would automatically modulate that into a safe behavior or refuse to execute motions that violate constraints. In fact, during extensive hardware testing, **no damage occurred to the robot**, attesting to the safety guarantees provided by the fabric-based control layer.

In summary, the DextrAH-G methodology marries **learning and control**: a sophisticated model-based controller (geometric fabric) provides a scaffold that makes training feasible and safe, and in turn a learned policy can operate directly from perceptual inputs to achieve dexterous grasping. This design addresses many challenges of joint arm-hand manipulation, as we will contrast with prior approaches next.

## Key Innovations and Differences from Prior Work

DextrAH-G introduces several innovations and improvements over prior **arm-hand grasping frameworks**:

* **Integrated Arm-Hand *Pixels-to-Actions* Control:** Unlike many previous dexterous grasping methods that decompose the problem (e.g. first select a grasp for the hand, then plan an arm motion), DextrAH-G learns a *unified policy* that jointly controls **all DOFs of the arm and hand** in one closed-loop system. Prior works often focused either on the hand manipulation given a fixed end-effector pose or assumed a separate arm motion planner. For example, Agarwal et al. (2022) first selected a pre-grasp position via vision and then used a “blind” hand policy (eigen-grasp based) to close the fingers. Qin et al. (2023) trained a hand policy conditioned on point clouds for objects in a single category. **In contrast, DextrAH-G’s policy uses depth images to simultaneously decide how to move the arm and fingers**, resulting in more **agile and coordinated** reaches and grasp closures. This end-to-end perception-to-control approach allows reactive adjustment of the arm and hand throughout the grasp – if the object shifts or is in a different pose, the policy can continuously correct the arm-hand trajectory. The result is a more **dynamic and generalizable behavior across novel objects** than methods that rely on a fixed grasp pose followed by open-loop execution.

* **Use of Geometric Fabrics for Safe and Bias-Shaped Control:** A major novelty of DextrAH-G is leveraging a **geometric fabric controller** to underpin learning. Many prior RL-based robotic policies simply output joint torques or desired joint positions to a low-level PD controller. Such simple controllers place the entire burden of achieving sophisticated behavior on the neural policy, which is prone to learning unnatural or unsafe strategies (e.g., jerky motions, visiting dangerous states). DextrAH-G instead embeds expert knowledge of physics and kinematics into the control layer. This **fabric-guided policy (FGP)** approach was previously explored in a much simpler context (in-hand cube reorientation) by Allshire et al., who found it dramatically improved learning outcomes. DextrAH-G extends this idea to *grasping*, building the most extensive fabric to date with features like integrated collision avoidance, joint limit handling, posture optimization, and custom action manifolds. By doing so, it **guarantees hardware safety** (the robot will not self-collision or exceed limits) and produces **natural, human-like motions** by design. This is a key differentiator: earlier works had to accept slow or conservative motions to ensure safety, or they risked damage during fast maneuvers. DextrAH-G can move at high speeds confidently because the geometric fabric enforces safety continuously. Moreover, the fabric **provides an inductive bias** for learning. The policy operates in a *reduced action space* (the 11-D space of meaningful arm-hand movements) which simplifies the learning problem compared to controlling 23 raw joints. Similar dimension reduction existed in some prior work (e.g. Agarwal’s use of an eigengrasp space for the hand), but DextrAH-G’s approach is more holistic – it couples arm and hand synergies and does so within a reactive motion framework (RMP/fabric) that blends multiple objectives. Additionally, geometric fabrics come with rigorous mathematical tools, and have been shown to outperform alternative motion representation frameworks like Dynamic Movement Primitives (DMPs) or Koopman operator approaches in comparable settings. By choosing geometric fabrics, the authors build on a strong theoretical foundation for **composable, stable behavior** synthesis.

* **Teacher-Student Training for Perception and Generalization:** Another differentiator is the **two-stage training** (privileged RL teacher then vision-based student). Prior deep dexterous grasping methods often struggled with generalization when training directly from pixels or needed enormous data. For instance, some works require collecting large grasp datasets or point cloud simulations to pre-train grasp proposals, and others rely on known object models or categories to succeed (e.g. Liu et al. needed object models registered to point clouds). DextrAH-G bypasses the need for any labeled grasp datasets or prior object models by training entirely in simulation with reinforcement learning. The asymmetric teacher policy makes learning more efficient by utilizing additional state information (object pose, etc.) without requiring the student to have it. This **privileged training** paradigm has been used in legged locomotion and manipulation (to leverage simulation state), but in dexterous grasping it is relatively novel and powerful – it allows the teacher to attain a high level of performance with less training time or tricky exploration. Then, via **online distillation (DAgger)**, DextrAH-G successfully transfers that capability to a **vision-based student**. The result is an end-to-end **depth image to action** policy that is both skilled and robust. This addresses the sim-to-real gap more elegantly than some prior approaches. For example, other works have attempted zero-shot sim-to-real by heavy domain randomization alone or by using coarse perception (like classifying object identity then using a specific strategy). Here, the combination of domain randomization, robust teacher training, and supervised distillation yields a policy that directly handles real depth images and varied objects without additional real-world fine-tuning. The authors explicitly demonstrate **generalization to novel objects** outside the training set, which we discuss below (achieving high success on unseen objects) – a feat that many earlier RL-based graspers did not evaluate or achieve to this extent.

* **High Speed and Continuous Reactivity:** DextrAH-G distinguishes itself in terms of **operational speed and closed-loop reactivity**. Traditional grasp planners might take seconds to plan a grasp and arm trajectory, and even learning-based grasping systems often execute slow, cautious motions. In contrast, DextrAH-G’s policy runs at 15 Hz, and the low-level fabric control at 60 Hz, enabling **real-time adjustments**. The system can complete a grasp-and-drop cycle in around 10 seconds on average (≈5-6 picks per minute) in real-world tests. This is significantly faster than previous dexterous hand systems, which often were not benchmarked on pick-per-minute but generally ran much slower or only in simulation. The continuous feedback from depth sensing means if an object moves or the grasp is not perfect, DextrAH-G can correct mid-flight, something static grasp execution can’t do. The paper explicitly notes that this combination of **“celerity and reliability”** is a notable advance in dexterous robot grasping, bringing it closer to real-world utility. For comparison, a recent generative grasp planner like DexDiffuser (Weng et al. 2024) focuses on grasp synthesis but does not incorporate such real-time feedback or arm-hand coordination, and as a result, it may require careful execution and is tested mostly per grasp rather than continuous operation. The empirical results (Table 1 in the paper) indeed show DextrAH-G outperforming DexDiffuser and other baselines on a variety of test objects, indicating both higher success rates and the ability to handle more objects (those baselines often failed or did not even attempt certain objects).

* **Robustness and Safety Guarantees:** DextrAH-G’s robustness comes from both its training regime and its control design. The heavy perturbations (random object pushes, pose noise, low friction) used during RL training force the policy to learn **grasp strategies that work in less-than-ideal conditions**. This is a contrast to some earlier works where a policy might overfit to clean simulation scenarios and then struggle with real-world variability (e.g., a policy that only learned to grasp perfectly placed objects might fail if the object is slightly out of expected position or if the vision sensing has noise). The results demonstrate that DextrAH-G can grasp objects even as they move slightly or when perception is noisy, thanks to this robust training. On the safety side, few (if any) prior dexterous grasping papers provided *formal guarantees* of safety during execution. By embedding the constraints in the geometric fabric, DextrAH-G guarantees that even if the RL policy hasn’t learned to avoid, say, the table or its own arm, the controller will automatically prevent a collision. This not only protects the hardware (no damage was observed over extensive tests) but also enables the policy to train and operate in regimes that might be risky for other methods. For example, the policy can aggressively explore reaching near the table or making tight grasps without fear of a catastrophic collision, whereas a pure learning method might either avoid these regimes (leading to poor performance on low objects) or crash the robot. This integration of analytical safety measures with learning is a clear differentiator from prior art, which typically handled safety via mere caution or not at all.

In summary, DextrAH-G’s novelty lies in **combining advanced control (geometric fabrics) with deep learning (RL and distillation)** in the context of dexterous arm-hand grasping. It achieves a **holistic solution** where others tackled pieces of the problem. By doing so, it **surpasses prior work** in *coordination* (arm + hand together), *input modality* (directly using depth images, no requirement for full point cloud or known models), *performance* (speed & success rates), and *safety*. As we will see next, the experimental results back up these claims, showing new state-of-the-art levels of dexterous grasping performance.

## Evaluation and Results

The authors validate DextrAH-G through both **simulation benchmarks** and extensive **real-world tests** on a physical robot. Key findings from the experiments include:

* **Simulation Performance:** In simulation, the **privileged teacher policy** achieves near-perfect grasp success on the 140 training objects, and the **distilled student policy** (depth-based) almost matches it. Quantitatively, when evaluating over many parallel simulation trials, the teacher FGP had about *85% per-object success rate* on average, and the student FGP achieved about *80%* on those same objects. The difference is small, indicating the student learned to mimic the teacher’s grasp behavior well. The paper reports a 99% success rate per batch of trials (when counting success if an object is grasped in any attempt of the episode), and \~80% success per object when each object is attempted individually. The slight drop for the student is attributed to the fact that the teacher had access to privileged info and an object ID, but importantly the student’s performance was still high enough to enable successful transfer to real. The **average successful grasp execution time in simulation was only \~4 seconds**, highlighting how quickly the policy can achieve a stable grasp and lift. This fast cycle (enabled by the 60 Hz control and aggressive motion possible under the fabric’s safety net) is promising for throughput.

* **Real-World Single-Object Tests:** DextrAH-G was deployed on a real robot (an Allegro hand on a KUKA arm, as per the paper setup) to grasp a variety of objects **one at a time**. Table 1 of the paper summarizes the success rates over 5 trials for several representative objects, including a pitcher, a Pringles can, a coffee canister, a cup, a Cheez-It box, a spray bottle (cleaner), a toy brick, a Spam can, a cooking pot, and a toy airplane. **DextrAH-G achieved success on most of these objects in 4 or 5 out of 5 trials**, i.e., 80% or 100% success rates on each item. Notably, it succeeded in some challenging cases like a small cup (grasped 5/5 times) and a heavy spam can (5/5), and even the hardest item (a flat toy airplane) was grasped 3 out of 5 times. These results are *at or above* the success rates reported for prior methods. For comparison, the table includes **DexDiffuser (2024)**, a diffusion-model-based grasp planner, which only succeeded on a subset of those objects (and wasn’t attempted on others) – for instance DexDiffuser had \~60% on the Pringles can and cup, and only 20% on the airplane. Another baseline, **ISAGrasp (2023)**, and a prior method by Matak et al., likewise show patchy performance (some objects 0% or 40% success). DextrAH-G’s consistently high rates across all objects demonstrate its *general grasping ability*. These objects vary in shape, size, and weight (from boxes to cylinders to objects with handles), indicating the policy did not overfit to a single category. The use of depth input and training on 140 objects likely enabled this broad generalization. It’s important to highlight that **these successes are achieved with no fine-tuning on hardware** – the same student policy trained in sim was run on the robot, attesting to the effectiveness of the sim-to-real transfer techniques (domain randomization, noise, etc.).

* **Continuous **Bin-Packing** Task:** Beyond isolated grasps, the authors devised a **bin packing (or pick-and-place) evaluation** to test DextrAH-G in an application-like scenario. In this setup, the robot faced a pile or set of over 30 different objects and had to continuously pick them up one by one and drop them into a bin. This is a stringent test because it requires sustained performance over time, handling various objects in sequence, and measuring both **speed and consistency**. Three metrics were used: (1) **Consecutive Successes (CS)** – how many objects in a row the robot can pick and place correctly before a failure, (2) **Cycle Time** – how long it takes per object (from starting a grasp to dropping and returning to start), and (3) overall **Success Rate** across attempts. DextrAH-G performed impressively: across 8 continuous runs, it achieved an average **CS of 6.5 objects** (with some runs exceeding 8+ in a row). This means on average it could successfully pick up six or seven objects sequentially before any mistake (like a drop or miss) occurred. Its average **cycle time was \~10.7 seconds**, which translates to **5.6 picks per minute (PPM)**. For a dexterous hand (with complex finger control), this speed is quite high – approaching the realm of industrial pickers, though still about one-third the speed of a human operator who might achieve \~16 PPM on similar tasks. Most importantly, the **overall success rate was 87% across 256 grasp attempts** in these runs. This indicates reliability: the majority of picks succeeded even in an extended, varied sequence. According to the authors, this combination of speed (cycle time) and reliability is a **significant advance in the state of the art** for dexterous grasping. Many prior dexterous hand results were either in simulation or single-grasp demonstrations; achieving nearly 90% success over hundreds of grasps on different objects, at several picks per minute, is a new milestone. It suggests that such a system could be approaching practicality for tasks like automated sorting or bin clearing, especially as hardware and policies improve further.

* **Generalization to Novel Objects:** The paper also tested DextrAH-G on **novel objects not seen in training** (beyond the 140). It is reported that on a set of unseen objects, the system reached about **87% grasp success** as well. The failure modes observed were mostly sensible issues: about 8% of failures were due to the robot accidentally **pushing the object out of a stable grasping region** (e.g., sliding it away before grabbing), and \~3% due to repeated failed grasp attempts on very challenging shapes. A very small percentage (1–2%) were cases of losing the object during transport (either a slightly loose grip or a poor grasp that gave way en route). These analyses show that while not perfect, the policy is quite robust – the dominant failure mode (pushing objects) is something that could potentially be improved with better strategy or recognizing when to stop pushing and re-grasp. It’s encouraging that **no single object type completely defeated the system**; even the low-profile objects (which are generally hard because the hand has to get very low without hitting the table) were sometimes grasped, albeit with lower success rates. This broad generalization stems from the training on many objects and the policy’s ability to see depth – it wasn’t restricted to known shapes or a fixed set of grasps.

* **Safety and Motion Quality:** While harder to quantify, the authors note qualitatively that DextrAH-G’s motions are **natural-looking and smooth**, a benefit of the fabric controller. They also explicitly mention that over many hours of testing various policies (including intentionally *“ill-behaved”* ones for stress-testing), **no hardware damage occurred**. This is a strong testament to the safety framework: even if the RL policy had some bad outputs, the geometric fabric’s built-in safeguards (collision avoidance, limit enforcement) protected the robot. In contrast, pure end-to-end learned policies could easily drive a real robot to self-collision or joint overextension if not carefully constrained.

Overall, the experimental results support the authors’ claims that DextrAH-G achieves **state-of-the-art performance** in dexterous grasping. It not only surpasses prior success rates on standard test objects, but it also introduces the metric of **picks-per-minute with a dexterous hand**, demonstrating a level of speed that wasn’t seen before. The combination of an **87% success rate and \~5.6 PPM throughput** in the continuous task is highlighted as a considerable step forward. For context, most earlier dexterous hand studies didn’t even report such metrics; DextrAH-G is bringing dexterous manipulation closer to the practicality seen in simpler parallel-jaw gripper systems, but with the added versatility of a human-like hand.

## Limitations

Despite its impressive contributions, DextrAH-G has some **limitations** acknowledged by the authors, which also point to avenues for future improvement:

* **Reduced Finger Dexterity:** By using a 5-D PCA manifold for the hand action space, the policy **limits the range of hand motions** to those spanned by the principal components. This was an intentional design choice to focus on grasping (closing around objects) and not in-hand manipulation. However, it means the robot cannot perform more fine-grained finger movements or reorient objects within the grasp that fall outside of those PCA modes. In other words, the **kinematic dexterity is traded off for simplicity**. If a task required complex finger gaiting or precise repositioning of an object in-hand, the current policy might not handle it. Future work could consider increasing the hand action dimensionality or using hierarchical policies to regain some dexterity beyond the PCA subspace.

* **Limited Learning of Obstacle Avoidance:** DextrAH-G relies on the model-based geometric fabric for collision avoidance (with the table and robot itself). The policy thus does not explicitly learn obstacle avoidance from sensory input. In scenarios with dynamic or novel obstacles (e.g., clutter or a moving human), the fabric’s built-in avoidance might not be enough, and the policy currently has no mechanism to alter its plan based on unseen obstacles except via the fabric’s reactive forces. The authors suggest that **ideally some obstacle avoidance behavior should be learned from perception**, to reduce dependency on the known geometric model. For instance, if the policy could visually detect obstacles or foresee collisions, it might perform better in clutter. As is, DextrAH-G is best suited to relatively structured scenes (a table, known robot model, one object at a time).

* **RL Exploration Near Contact Limits:** The use of a strong avoidance fabric has a side effect: it makes it **hard for the RL policy to explore grasps that require approaching very close to obstacles**. For example, picking up a very thin object lying flat on the table requires the fingers to come extremely close to the table surface. The fabric’s collision avoidance will resist motions that get too close to the table (to prevent crashes), which in training could lead to the policy rarely experiencing successful grasps of such low-profile objects. The authors observed reduced performance on objects that lie low on the table due to this issue. This highlights an inherent tension: too strong a safety barrier can impede learning of risky-but-necessary maneuvers. They suggest that improving exploration strategies or using curricula to gradually allow closer approaches could help, as might advances in RL algorithms that handle constraints better. Learning-based collision avoidance (where the policy is penalized for collision but not outright prevented) could also be a future direction, to give the policy more flexibility near contact boundaries.

* **Single Object Assumption (No Clutter Handling):** Currently, DextrAH-G is designed to **grasp one object at a time** in an otherwise clear environment. The depth image input is used to localize and grasp a single target object. In a cluttered scene with multiple objects touching or overlapping, the system would likely face difficulties. The authors note that **extensions like visual segmentation or scene parsing** would be needed to handle multiple objects or clutter piles. In practical terms, the policy doesn’t “know” how to choose among many objects or avoid knocking one object into another because it was never trained on clutter. This is a common limitation – many learning-based graspers assume a single target at a time. Future work could integrate an object detection module or train the policy in cluttered simulations to make it effective in more unstructured piles of items.

In summary, DextrAH-G excels at what it was designed for – single-object grasps in a constrained action space – but **does not yet solve all aspects** of dexterous manipulation. It doesn’t do complex finger tricks (due to PCA action space), it doesn’t inherently understand obstacles beyond its pre-modeled avoidance (limiting adaptation to new surroundings), it struggles with objects that require pushing into “forbidden” zones (very flat objects on surfaces), and it is not directly applicable to cluttered scenes without additional perception help. Recognizing these limitations is important, as it frames the scope of the contribution and points to how the approach might be expanded (e.g., using higher-dimensional action spaces or multi-object vision in future work).

## Final Assessment

**DextrAH-G** represents a significant advancement in the domain of dexterous robotic grasping. By intelligently blending model-based control (geometric fabrics) with model-free learning, the authors achieve a synergy that capitalizes on the strengths of each. The geometric fabric endows the system with **safety, stability, and structured knowledge** of the robot’s dynamics, while reinforcement learning and distillation inject **adaptability and perceptual intelligence**. This combination enabled, for the first time, a 23-DOF dexterous hand-arm robot to grasp a wide array of objects *directly from vision* at high speed **without retraining on the robot**. The methodological contributions – such as the use of a **fabric-guided policy (FGP)**, the design of a reduced **PCA-based hand action space**, and the extensive use of **privileged training with domain randomization** – tackle long-standing challenges like high-dimensional control, sim-to-real transfer, and exploration in contact-rich tasks. These ideas are likely to influence future research, suggesting that complex skills can be better learned when the right inductive biases (like motion fabrics or synergies) are built into the learning process.

From a performance standpoint, DextrAH-G clearly pushes the state of the art. It achieves **higher success rates across diverse objects** than prior methods and does so with **remarkable efficiency (5+ picks per minute)**, all while maintaining safety (zero collisions or hardware issues reported). The real-world demonstrations of continuous picking of novel objects with \~87% success are especially compelling – they indicate that the policy generalizes well and could be deployed in practical settings with minimal fuss. Few works in dexterous manipulation have shown this level of generality and reliability so far.

It should be noted that DextrAH-G’s focus was on **powerful grasp execution** rather than fine manipulation or multi-step tasks. Thus, its constrained finger motion space and single-object assumption are reasonable trade-offs for its target application (fast picking tasks). The limits identified (e.g., difficulty with flat objects or clutter) highlight that there is still room to grow: integrating more perception for clutter, improving exploration near contact limits, and expanding the action space for greater dexterity are all interesting future directions. The concept of **learning within a fabric (or RMP) framework** might also extend to other tasks – for instance, bimanual manipulation or tool use – where safe coordination of many DOFs is needed.

In conclusion, *DextrAH-G* demonstrates how marrying **geometry and learning** can yield robust robot skills. It offers a template for **“grasp anything” policies** that are not only proficient but also hardware-friendly and efficient. This deep integration of a physics-based controller with deep RL is a notable innovation, and the successful results validate the approach. As the field progresses, DextrAH-G paves the way toward dexterous robots that can operate with a human-like combination of caution and agility – fast when needed, careful when required, and effective across a broad range of real-world objects. The work is a **significant step toward deploying dexterous robotic hands in industrial and everyday environments**, bridging the gap between simulation-trained policies and reliable real-world performance.

**Sources:** The analysis above is based on the paper by Lum *et al.* (2024) and associated results in the text, which provide detailed descriptions of the DextrAH-G system, its innovations, and experimental outcomes.

-->


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="curieuxjy/blog_comments" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Copyright 2024, Jung Yeon Lee</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/curieuxjy">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>