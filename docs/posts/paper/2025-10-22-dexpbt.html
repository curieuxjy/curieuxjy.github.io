<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-10-22">
<meta name="description" content="Scaling up Dexterous Manipulation for Hand-Arm Systems with Population Based Training">

<title>📃DexPBT 리뷰 – Curieux.JY</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../profile.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-8ef56b68f8fa1e9d2ba328e99e439f80.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-ecf89aac047581c664da7ae53d704519.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-b009f778f5cec7f34f624408a2b5b543.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-ecf89aac047581c664da7ae53d704519.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-2NVZN2MJZT"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-2NVZN2MJZT', { 'anonymize_ip': true});
</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Curieux.JY</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../post.html"> 
<span class="menu-text">Post</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../note.html"> 
<span class="menu-text">Note</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">Jung Yeon Lee</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/curieuxjy"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#brief-review" id="toc-brief-review" class="nav-link active" data-scroll-target="#brief-review"><span class="header-section-number">1</span> Brief Review</a></li>
  <li><a href="#detail-review" id="toc-detail-review" class="nav-link" data-scroll-target="#detail-review"><span class="header-section-number">2</span> Detail Review</a>
  <ul class="collapse">
  <li><a href="#논문의-핵심-아이디어-요약" id="toc-논문의-핵심-아이디어-요약" class="nav-link" data-scroll-target="#논문의-핵심-아이디어-요약"><span class="header-section-number">2.1</span> 논문의 핵심 아이디어 요약</a></li>
  <li><a href="#기술적-기여-및-dexpbt-프레임워크의-구성" id="toc-기술적-기여-및-dexpbt-프레임워크의-구성" class="nav-link" data-scroll-target="#기술적-기여-및-dexpbt-프레임워크의-구성"><span class="header-section-number">2.2</span> 기술적 기여 및 DexPBT 프레임워크의 구성</a>
  <ul class="collapse">
  <li><a href="#dexpbt-프레임워크-구조와-강화학습-기법" id="toc-dexpbt-프레임워크-구조와-강화학습-기법" class="nav-link" data-scroll-target="#dexpbt-프레임워크-구조와-강화학습-기법"><span class="header-section-number">2.2.1</span> DexPBT 프레임워크 구조와 강화학습 기법</a></li>
  <li><a href="#단계적-보상-설계와-pbt-메타-목표" id="toc-단계적-보상-설계와-pbt-메타-목표" class="nav-link" data-scroll-target="#단계적-보상-설계와-pbt-메타-목표"><span class="header-section-number">2.2.2</span> 단계적 보상 설계와 PBT 메타-목표</a></li>
  <li><a href="#시뮬레이션-및-학습-환경-설정" id="toc-시뮬레이션-및-학습-환경-설정" class="nav-link" data-scroll-target="#시뮬레이션-및-학습-환경-설정"><span class="header-section-number">2.2.3</span> 시뮬레이션 및 학습 환경 설정</a></li>
  <li><a href="#기존-접근법과의-차별성" id="toc-기존-접근법과의-차별성" class="nav-link" data-scroll-target="#기존-접근법과의-차별성"><span class="header-section-number">2.2.4</span> 기존 접근법과의 차별성</a></li>
  </ul></li>
  <li><a href="#실험-결과-분석" id="toc-실험-결과-분석" class="nav-link" data-scroll-target="#실험-결과-분석"><span class="header-section-number">2.3</span> 실험 결과 분석</a>
  <ul class="collapse">
  <li><a href="#실험-환경-및-평가-지표" id="toc-실험-환경-및-평가-지표" class="nav-link" data-scroll-target="#실험-환경-및-평가-지표"><span class="header-section-number">2.3.1</span> 실험 환경 및 평가 지표</a></li>
  <li><a href="#주요-결과-및-그래프-해석" id="toc-주요-결과-및-그래프-해석" class="nav-link" data-scroll-target="#주요-결과-및-그래프-해석"><span class="header-section-number">2.3.2</span> 주요 결과 및 그래프 해석</a></li>
  <li><a href="#성능-비교-및-종합-논의" id="toc-성능-비교-및-종합-논의" class="nav-link" data-scroll-target="#성능-비교-및-종합-논의"><span class="header-section-number">2.3.3</span> 성능 비교 및 종합 논의</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-page-left" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">📃DexPBT 리뷰</h1>
  <div class="quarto-categories">
    <div class="quarto-category">visuo-tactile</div>
    <div class="quarto-category">bimanual</div>
  </div>
  </div>

<div>
  <div class="description">
    Scaling up Dexterous Manipulation for Hand-Arm Systems with Population Based Training
  </div>
</div>


<div class="quarto-title-meta column-page-left">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">October 22, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<ul>
<li><a href="https://arxiv.org/abs/2305.12127">Paper Link</a></li>
<li><a href="https://sites.google.com/view/dexpbt">Homepage</a></li>
<li><a href="https://github.com/isaac-sim/IsaacGymEnvs/blob/main/docs/rl_examples.md#dexpbt-scaling-up-dexterous-manipulation-for-hand-arm-systems-with-population-based-training">Code Link</a> - <a href="https://github.com/isaac-sim/IsaacGymEnvs/blob/main/docs/pbt.md">Docs</a></li>
<li><a href="https://developer.nvidia.com/blog/streamline-robot-learning-with-whole-body-control-and-enhanced-teleoperation-in-nvidia-isaac-lab-2-3/">Streamline Robot Learning with Whole-Body Control and Enhanced Teleoperation in NVIDIA Isaac Lab 2.3</a></li>
</ul>
<ol type="1">
<li>🚀 이 연구는 GPU 가속 물리 시뮬레이터인 Isaac Gym을 활용하여 고자유도(high-DoF) 핸드-암 로봇 시스템의 능숙한 물체 조작 학습을 위한 알고리즘과 방법을 제안합니다.</li>
<li>💡 분산형 PBT(Population-Based Training) 알고리즘을 도입하여 딥 강화 학습의 탐색 능력을 크게 향상시키고, 기존의 종단간 학습보다 뛰어난 성능으로 재파지, 투척, 재배향과 같은 복잡한 태스크에서 견고한 제어 정책을 발견했습니다.</li>
<li>💪 PBT는 보상 함수 튜닝 및 하이퍼파라미터 탐색을 자동화하며, 단일 및 이중 팔 시스템 모두에서 고자유도 조작 정책을 성공적으로 훈련함으로써 확장 가능하고 강인한 로봇 제어 가능성을 입증했습니다.</li>
</ol>
<center>
<img src="../../images/2025-10-22-dexpbt/00.png" width="100%">
</center>
<center>
<img src="../../images/2025-10-22-dexpbt/111.png" width="100%">
</center>
<hr>
<section id="brief-review" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Brief Review</h1>
<p>이 논문은 다지 로봇 손과 팔 시스템을 사용하여 숙련된 물체 조작을 학습하기 위한 새로운 알고리즘과 방법론인 DexPBT를 제안합니다. 특히, 자유도(DoF)가 높은 단일 또는 양팔 로봇 시스템을 대상으로 하며, 시뮬레이션 환경에서 접촉이 많은 복잡한 작업(예: 재파지, 잡고 던지기, 물체 재정렬)을 수행합니다. 기존의 심층 강화 학습(RL) 방법이 높은 결과 분산과 초기 조건에 대한 의존성을 보이는 한계를 극복하기 위해, 본 연구는 탐색 능력을 대규모로 확장할 수 있는 분산형 개체군 기반 훈련(PBT) 알고리즘을 도입합니다.</p>
<p><strong>I. 서론</strong></p>
<p>최근 심층 RL은 드론 비행이나 사족보행 로봇과 같이 연속적인 제어가 필요한 복잡한 영역에서 인상적인 성과를 보였습니다. 로봇 조작 분야에서는 복잡한 접촉 동역학을 모델링하기 어려운 전통적인 방법론 대신 학습 기반 접근 방식이 유망하게 부상하고 있습니다. 특히 “Dactyl”이나 “DeXtreme”과 같은 시스템은 대규모 시뮬레이션 기반 RL이 복잡한 인핸드(in-hand) 물체 조작에 대한 견고한 정책을 얻는 데 사용될 수 있음을 보여주었습니다.</p>
<p>본 논문은 이러한 접근 방식을 확장하여 4개의 손가락과 16자유도를 가진 Allegro 손이 7자유도 Kuka 팔에 장착된 완전 작동식 핸드-암 시스템에 적용합니다. Isaac Gym 시뮬레이션 환경에서 재파지, 던지기, 재정렬과 같은 다양한 조작 작업을 목표로 합니다. 나아가, 하나의 신경망 정책으로 총 46자유도를 가진 한 쌍의 팔과 손을 제어하는 양팔 에이전트(ambidextrous agent)를 훈련하기 위해 학습 방법을 확장합니다. 표준 종단 간 학습이 상당한 결과 편차와 초기 조건 의존성을 보이는 반면, 제안된 PBT 알고리즘은 이러한 문제를 완화하고 종단 간 RL의 탐색 능력을 크게 증폭시킵니다. 중앙 조율자 없이 비동기식 학습을 용이하게 하는 분산형 PBT 구현은 모든 시나리오에서 표준 종단 간 학습보다 향상된 성능을 보이며, 두 핸드-암 시스템을 동시에 제어하는 양팔 에이전트의 성공적인 훈련을 가능하게 합니다.</p>
<p>주요 기여는 다음과 같습니다:</p>
<ul>
<li>고자유도 단일 및 양팔 시스템의 숙련된 물체 조작 정책 훈련을 위해 GPU 가속 로봇 시뮬레이션과 분산형 PBT를 결합한 프레임워크를 개발했습니다.</li>
<li>보상 조정을 단순화하고 자동화하기 위한 단계별 보상 함수 정식화와 PBT 메타-목표(meta-objective)를 소개합니다.</li>
<li>추가 연구를 위해 환경, RL 및 PBT 코드를 공개합니다.</li>
</ul>
<p><strong>II. 방법론</strong></p>
<p><strong>A. 문제 정의</strong></p>
<p>본 연구는 물체를 대상 위치 <span class="math inline">x \in \mathbb{R}^3</span>와 선택적으로 방향 <span class="math inline">R \in SO(3)</span>에 맞추는 단일 물체 재배치 문제를 다룹니다. 이는 일반적인 재배치를 수행하는 데 필요한 필수적인 기본 요소로 간주됩니다. 제어기는 각 단계에서 환경 상태 <span class="math inline">s_t \in \mathbb{R}^{N_{obs}}</span>를 관찰하고 팔과 손가락 관절의 원하는 각도를 지정하는 행동 <span class="math inline">a_t \in \mathbb{R}^{N_{dof}}</span>를 생성합니다. 물체 상태가 특정 허용 오차 내에서 목표와 일치하면 성공으로 간주되며, 목표 상태는 재설정됩니다. 물체가 떨어지거나 특정 시간 <span class="math inline">\tau</span> 내에 목표 상태를 달성하지 못하면 실패로 간주됩니다. 작업 성능은 에피소드 내에서 연속 성공 횟수 <span class="math inline">N_{succ} \le N_{max}</span>로 측정됩니다.</p>
<p>관찰된 물체 자세를 나타내기 위해 <span class="math inline">N_{kp}</span>개의 핵심 점 <span class="math inline">x_{kp} \in \mathbb{R}^{3N_{kp}}</span>을 사용합니다 (그림 3). 재정렬 작업에서는 목표 물체 자세 <span class="math inline">x_{targ} \in \mathbb{R}^{3N_{kp}}</span>도 핵심 점으로 표현됩니다. 모든 <span class="math inline">N_{kp}</span>개의 핵심 점이 해당 목표 위치의 허용 오차 <span class="math inline">\epsilon^*</span> 내에 있으면 작업이 성공적으로 실행된 것으로 간주됩니다: <span class="math inline">\max_{i \in 1..N_{kp}}||x^{(i)}_{targ} - x^{(i)}_{kp}|| \le \epsilon^*</span>. 핵심 점 표현은 회전과 변환에 대한 허용 오차 및 RL 보상 가중치를 별도로 조정할 필요성을 없앱니다. 또한, 물체의 볼록 껍질(convex hull)에 미리 정의된 패턴으로 배치된 핵심 점은 정책이 강체(rigid body)의 모양을 관찰할 수 있도록 합니다.</p>
<p><strong>B. 시나리오</strong></p>
<p>숙련된 조작의 다양한 과제를 강조하는 세 가지 물체 조작 작업 변형을 개발했습니다: 재파지, 잡고 던지기, 재정렬 (그림 1). 에피소드 시작 시 물체는 테이블의 임의 위치에 나타나고 핸드-암 시스템은 임의의 관절 각속도와 자유도 제한 내의 초기 각도로 구성된 임의 상태 <span class="math inline">s_{robot} \in \mathbb{R}^{2N_{dof}}</span>로 재설정됩니다.</p>
<ul>
<li><strong>재파지(Regrasping)</strong>: 에이전트가 물체를 잡고 테이블에서 들어 올린 후 지정된 위치에 일정 시간 동안 유지해야 합니다. 성공적인 수행을 위해 제어 정책은 물체를 떨어뜨릴 확률을 최소화하는 안정적인 파지(grasp)를 개발해야 합니다.</li>
<li><strong>잡고 던지기(Grasp-and-Throw)</strong>: 물체를 집어 매니퓰레이터의 도달 범위를 벗어날 수 있는 컨테이너에 던져 넣는 작업입니다. 이 시나리오는 목표를 조준하고 상당한 거리로 물체를 던지는 것을 요구합니다. 이 작업을 성공적으로 실행하려면 올바른 궤적 지점에서 그립을 해제하고 목표 방향으로 적절한 양의 운동량을 물체에 부여해야 합니다.</li>
<li><strong>재정렬(Reorientation)</strong>: 선택된 시나리오 중 가장 어려운 물체 조작 과제입니다. 물체를 잡고 연속적으로 다른 목표 위치와 방향으로 이동시키는 것이 목표입니다. 이 시나리오는 시뮬레이션 시간으로 수분 동안 안정적인 그립을 유지하고 로봇 팔 관절의 미세 제어를 요구하며, Kuka 팔의 동작만으로는 재정렬이 불가능할 때 인핸드 회전(in-hand rotation)을 필요로 합니다. 재파지 및 잡고 던지기 작업은 목표 위치 일치만을 요구하지만, 재정렬 시나리오는 물체 회전이 목표와 일치하는 것을 추가로 요구합니다. 재파지 및 재정렬 모두 최종 요구 허용 오차는 <span class="math inline">\epsilon^* = 1 \text{ cm}</span>로 매우 정밀한 제어를 요구합니다. 잡고 던지기에서는 물체를 컨테이너에 착륙시키는 데 중점을 두므로 <span class="math inline">\epsilon^* = 7.5 \text{ cm}</span>를 사용합니다.</li>
</ul>
<p><strong>양팔 시나리오(Dual-Arm Scenarios)</strong>: 연속 제어를 위한 종단 간 학습의 한계를 찾기 위해, 재파지 및 재정렬 시나리오의 양팔 버전을 도입했습니다. 총 46자유도를 가진 두 로봇을 시뮬레이션합니다. 초기 및 목표 물체 위치 샘플링을 조정하여 한 로봇만으로는 작업을 해결할 수 없도록 보장합니다. 따라서 완전한 해결책은 물체를 파지하고, 한 손에서 다른 손으로 물체를 넘기고, 인핸드 조작을 포함하여 모든 시나리오의 가장 어려운 요소들을 결합해야 합니다. 단일 정책이 두 매니퓰레이터를 모두 제어할 수 있도록 관측(표 I) 및 행동 공간(<span class="math inline">a_t \in \mathbb{R}^{N_{dof} * N_{arm}}, N_{arm} = 2</span>)을 확장했습니다.</p>
<p><strong>C. 강화 학습</strong></p>
<p>문제는 에이전트가 예상되는 에피소드별 할인된 보상 합계 <span class="math inline">\mathbb{E}[\sum_{t=0}^T \gamma^t r(s_t, a_t)]</span>를 최대화하기 위해 환경과 상호작용하는 마르코프 의사결정 과정(MDP)으로 정식화됩니다. 정책 <span class="math inline">\pi_\theta</span>와 가치 함수 <span class="math inline">V^\pi_\theta(s)</span>를 동시에 학습하기 위해 근접 정책 최적화(PPO) 알고리즘을 사용하며, 둘 다 단일 파라미터 벡터 <span class="math inline">\theta</span>로 파라미터화됩니다. 모델 아키텍처는 LSTM 뒤에 3계층 MLP가 따릅니다.</p>
<p>정책은 고도로 병렬화된 GPU 가속 물리 엔진인 Isaac Gym에서 시뮬레이션된 경험을 사용하여 훈련됩니다. 시뮬레이터에서 생성되는 대량의 데이터를 처리하기 위해 계산 그래프를 전적으로 GPU에 유지하는 효율적인 PPO 구현(rl_games)을 사용합니다. 이는 <span class="math inline">2^{15}</span>개의 트랜지션(transition) 미니배치(minibatch) 크기와 결합되어 하드웨어 활용도와 학습 처리량을 최대화합니다. 관측값, 이점(advantages) 및 TD-반환(TD-returns)의 정규화를 사용하여 알고리즘이 관측값 및 보상의 절대 스케일에 불변하도록 만듭니다. 또한, 현재 정책 <span class="math inline">\pi_\theta</span>와 롤아웃을 수집한 행동 정책 <span class="math inline">\pi_{\theta_{old}}</span> 사이의 KL-발산 <span class="math inline">D_{KL}(\pi|\pi_{old})</span>을 일정하게 유지하는 적응형 학습률 알고리즘을 사용합니다. <span class="math inline">D_{KL}</span>이 훈련 반복(iteration)이 끝날 때 임계값을 초과하면 학습률이 1.5배 감소하고, 임계값 아래로 떨어지면 1.5배 증가합니다.</p>
<p>재파지 및 재정렬 모두 매우 정밀한 제어(<span class="math inline">\epsilon^* = 1 \text{ cm}</span>)를 요구하므로, 훈련 초기에 에이전트가 성공적인 작업 실행을 거의 경험하지 못합니다. 부드러운 학습 커리큘럼을 만들기 위해 더 큰 초기 값 <span class="math inline">\epsilon_0 = 7.5 \text{ cm}</span>에서부터 허용 오차를 적응적으로 점진적 감소(anneal)시킵니다. 정책이 성능 임계값 <span class="math inline">N_{succ} &gt; 3</span>을 넘었는지 주기적으로 확인하고, 이 경우 현재 성공 허용 오차를 최종 값 <span class="math inline">\epsilon^*</span>에 도달할 때까지 감소시킵니다: <span class="math inline">\epsilon \leftarrow \max(0.9\epsilon, \epsilon^*)</span>.</p>
<p><strong>보상 함수(Reward function)</strong>: 성공적인 RL 적용을 위해 보상은 탐색을 용이하게 할 만큼 충분히 조밀해야 하지만, 희소한 최종 목표(연속 성공적인 조작 횟수 최대화)로부터 에이전트의 주의를 분산시키지 않아야 합니다. 본 연구는 에이전트를 물체에 손을 뻗는 것부터 물체를 집어 들고 최종 위치로 옮기는 것까지 작업을 완료하는 데 필요한 일련의 동작을 자연스럽게 안내하는 보상 함수를 제안합니다: <span class="math display">r(s, a) = r_{\text{reach}}(s) + r_{\text{pick}}(s) + r_{\text{targ}}(s) - r_{\text{vel}}(a) \ \ (1)</span></p>
<ul>
<li><span class="math inline">r_{\text{reach}}</span>: 시도 시작 시 물체에 손을 더 가깝게 움직이는 에이전트에게 보상합니다. <span class="math display">r_{\text{reach}} = \alpha_{\text{reach}} * \max(d_{\text{closest}} - d, 0) \ \ (2)</span> 여기서 <span class="math inline">d</span>는 현재 말단-효과기(end-effector)와 물체 사이의 거리이고, <span class="math inline">d_{\text{closest}}</span>는 현재 시도 중 달성된 가장 가까운 거리입니다. 양팔 시나리오에서는 물체에 더 가까운 말단-효과기에 대해 <span class="math inline">d</span>를 계산합니다.</li>
<li><span class="math inline">r_{\text{pick}}</span>: 물체를 집어 들고 테이블에서 들어 올리는 에이전트에게 보상합니다. <span class="math display">r_{\text{pick}} = (1 - 1_{\text{picked}}) * \alpha_{\text{pick}} * h_t + r_{\text{picked}} \ \  (3)</span> 여기서 <span class="math inline">1_{\text{picked}}</span>는 물체의 테이블에 대한 높이 <span class="math inline">h_t</span>가 사전 정의된 임계값(15 cm)을 초과하면 1이 되는 지시 함수입니다. 이 순간 에이전트는 추가적인 희소 보상 <span class="math inline">r_{\text{picked}}</span>를 받습니다.</li>
<li><span class="math inline">r_{\text{targ}}</span>: 물체가 일단 집어 올려지면, 물체를 목표 상태에 더 가깝게 이동시키는 에이전트에게 보상합니다. <span class="math display">r_{\text{targ}} = 1_{\text{picked}} * \alpha_{\text{targ}} * \max(\hat{d}_{\text{closest}} - \hat{d}, 0) + r_{\text{success}} \ \ (4)</span> 재정렬 작업에서 <span class="math inline">\hat{d} = \max_{i \in 1..N_{kp}}||x^{(i)}_{targ} - x^{(i)}_{kp}||</span>는 물체와 목표 핵심 점 쌍 사이의 최대 거리이며, 방향 일치를 요구하지 않는 작업에서는 <span class="math inline">\hat{d}</span>는 단순히 물체 중심과 목표 위치 사이의 거리입니다. 두 경우 모두 <span class="math inline">\hat{d}_{\text{closest}}</span>는 시도 중 달성된 가장 작은 <span class="math inline">\hat{d}</span>입니다. 원하는 위치 및/또는 방향에 도달했을 때(<span class="math inline">\hat{d} = \hat{d}_{\text{closest}} \le \epsilon^*</span>) 큰 희소 보상 <span class="math inline">r_{\text{success}}</span>가 추가됩니다.</li>
<li><span class="math inline">r_{\text{vel}}</span>: 식 (1)의 <span class="math inline">r_{\text{vel}}</span>은 더 부드러운 움직임을 촉진하기 위해 조정할 수 있는 간단한 관절 속도 페널티입니다.</li>
</ul>
<p>전반적으로, 보상 정식화는 순차적인 패턴을 따르며, 보상 구성 요소인 <span class="math inline">r_{\text{reach}}</span>, <span class="math inline">r_{\text{pick}}</span>, <span class="math inline">r_{\text{targ}}</span>는 상호 배타적이므로 서로 간섭하지 않습니다. 이는 보상 튜닝을 용이하게 하고 많은 가능한 지역 최적해(local minima)를 피할 수 있도록 합니다. 또한, 식 (2)와 (4)의 보상은 각각 손과 물체, 물체와 목표 사이의 초기 거리에 따라 미리 정의된 최대 총 값을 가집니다. 이는 에이전트가 근접 보상을 계속 수집하기 위해 목표에 거의 도달하지 않은 상태로 머무는 보상 해킹(reward hacking) 행동을 방지합니다.</p>
<p><strong>관측값 및 행동(Observations and actions)</strong>: 실험에서 액터 <span class="math inline">\pi_\theta</span>와 비평가 <span class="math inline">V^\pi_\theta(s)</span> 모두 관절 각도와 속도, 손가락 끝 위치, 물체 회전, 속도 및 각속도를 포함한 환경 상태를 직접 관찰합니다. 추가적으로, 핵심 점 위치와 물체 크기는 물체 모양에 대한 정보를 제공합니다 (표 I 참조). 정책 <span class="math inline">\pi_\theta</span>는 <span class="math inline">N_{dof} * N_{arm}</span>개의 독립적인 가우시안 확률 분포의 파라미터로 사용되는 두 개의 벡터 <span class="math inline">\mu, \sigma \in \mathbb{R}^{N_{dof} * N_{arm}}</span>를 출력합니다. 행동은 이러한 분포에서 <span class="math inline">a \sim \mathcal{N}(\mu, \sigma)</span>로 샘플링되어 해당 관절 한계로 클리핑(clip)되고 목표 관절 각도로 해석됩니다. 그 다음 PDT 제어기는 정책에 의해 지정된 목표 각도로 관절을 가져오기 위해 관절 토크를 생성합니다.</p>
<p><strong>D. 개체군 기반 훈련 (Population-Based Training)</strong></p>
<p>최대 192개의 관측 차원(표 I)과 최대 46개의 행동 차원을 가진 접촉이 많은 연속 제어 문제는 현대 RL 알고리즘에게도 매우 도전적일 수 있습니다. 주요 과제는 탐색입니다. 또한, 학습 방법을 사용할 때의 또 다른 복잡성은 하이퍼파라미터 튜닝입니다.</p>
<p>이러한 문제를 완화하기 위해 PBT 접근 방식을 사용합니다. 핵심 아이디어는 진화 알고리즘과 유사합니다. 개체군 <span class="math inline">P</span>를 훈련하고, 유망한 하이퍼파라미터 조합을 생성하기 위해 변이(mutation)를 수행하며, 우수한 성능을 가진 에이전트의 우선순위를 정하기 위해 선택(selection)을 사용합니다. 각 에이전트 <span class="math inline">(\theta_i, p_i) \in P</span>는 파라미터 벡터 <span class="math inline">\theta_i</span>와 하이퍼파라미터 집합 <span class="math inline">p_i</span>로 특징지어집니다. 이는 RL 알고리즘 설정과 보상 계수(<span class="math inline">\alpha_{\text{reach}}, \alpha_{\text{pick}}, \alpha_{\text{targ}}, r_{\text{picked}}, r_{\text{success}}</span>)를 포함합니다 (표 II 참조).</p>
<p>주기적으로( <span class="math inline">N_{\text{iter}}</span> 환경 전이(transition)에 대해 훈련한 후), 각 에이전트는 목표 성능 지표 <span class="math inline">r_{\text{meta}}</span>를 얻기 위해 평가되며, 개체군은 이 지표에 따라 <span class="math inline">P_{\text{top}}, P_{\text{mid}}, P_{\text{bottom}} \subset P</span>의 세 부분으로 정렬됩니다 (각각 개체군의 30%, 40%, 30%를 차지). 가장 성능이 좋은 <span class="math inline">P_{\text{top}}</span>에 속하는 에이전트는 중단 없이 훈련을 계속합니다. 개체군 중간 40%인 <span class="math inline">P_{\text{mid}}</span>에 속하는 에이전트는 하이퍼파라미터 변이를 겪고 훈련을 계속합니다. 성능이 낮은 에이전트 <span class="math inline">(\theta, p) \in P_{\text{bottom}}</span>는 폐기되고, 변이된 하이퍼파라미터 <span class="math inline">p^*</span>를 가진 무작위로 샘플링된 고성능 에이전트 <span class="math inline">(\theta^*, p^*)\sim P_{\text{top}}</span>의 복사본으로 대체됩니다. 이 알고리즘은 계산 자원을 <span class="math inline">P_{\text{top}}</span>의 유망한 에이전트에 집중시키고 수많은 하이퍼파라미터 조합을 탐색하여 탐색을 극대화합니다.</p>
<p>하이퍼파라미터 변이 방식은 간단합니다: 각 변이 반복에서 각 부동 소수점 하이퍼파라미터는 <span class="math inline">\beta_{\text{mut}}</span> 확률로 균등 분포 <span class="math inline">\mu \sim U(\mu_{\text{min}}, \mu_{\text{max}})</span>에서 샘플링된 무작위 숫자로 곱해지거나 나눠집니다 (알고리즘 2 및 표 III 참조). 일부 하이퍼파라미터 변이는 일시적으로 성능 감소로 이어질 수 있으므로, 에이전트가 변경된 파라미터에 적응할 시간을 주기 위해 <span class="math inline">N_{\text{adapt}} = 5 \times 10^7</span> 단계 동안 PBT 업데이트를 일시적으로 중단합니다. 또한, 개체군 다양성을 촉진하기 위해 훈련 시작 후 <span class="math inline">N_{\text{start}} = 2 \times 10^8</span> 단계 후에만 PBT를 활성화합니다.</p>
<p><strong>메타-목표(Meta-objective)</strong>: PBT의 한 가지 장점은 다양한 조밀한 보상 구성 요소를 균형 있게 조정하는 내부 RL 루프와 달리, 최종 희소 스칼라 목표를 메타-최적화할 수 있는 외부 최적화 루프를 도입한다는 것입니다. <span class="math inline">N_{\text{succ}}</span>에 대한 메타-최적화는 당연한 선택이지만, 섹션 III-C에서 설명된 적응형 허용 오차 점진적 감소는 합병증을 유발합니다: 개체군의 다른 에이전트들은 현재 성공 허용 오차 <span class="math inline">\epsilon</span> 값이 다를 수 있으므로 직접 비교할 수 없습니다. 이 문제를 해결하기 위해, <span class="math inline">N_{\text{succ}}</span>와 <span class="math inline">\epsilon</span>을 모두 고려하는 메타-최적화 목표를 정의합니다: <span class="math display">r_{\text{meta}} = \begin{cases} \frac{\epsilon_0 - \epsilon}{\epsilon_0 - \epsilon^*} + 0.01 * N_{\text{succ}} &amp; \text{if } \epsilon &gt; \epsilon^* \\ 1 + N_{\text{succ}} &amp; \text{if } \epsilon = \epsilon^* \end{cases} \ \ (5)</span> 목표 허용 오차 <span class="math inline">\epsilon^*</span>에 도달할 때까지 이 목표는 <span class="math inline">\epsilon</span>이 <span class="math inline">\epsilon^*</span>에 접근할 때 최대화되는 항 <span class="math inline">0 \le \frac{\epsilon_0 - \epsilon}{\epsilon_0 - \epsilon^*} \le 1</span>에 의해 지배됩니다. 원하는 허용 오차에 도달한 후에는 <span class="math inline">N_{\text{succ}}</span>의 최대화가 우선시됩니다.</p>
<p><strong>분산형 PBT(Decentralized PBT)</strong>: 본 구현의 특징은 일반적으로 다른 알고리즘에서 발견되는 중앙 조율자가 완전히 없다는 것입니다. 대신, 완전히 분산된 PBT 아키텍처를 제안합니다 (그림 2 참조). 이러한 아키텍처에서 각 에이전트는 알고리즘 1의 자체 부분 실행을 담당하며, 개체군에서의 순위 찾기, 자체 하이퍼파라미터 변이, 또는 다른 에이전트의 복사본으로 자신을 대체하는 것을 포함합니다.</p>
<p>본 구현에서 <span class="math inline">P</span>의 에이전트들은 에이전트 체크포인트 및 각 에이전트의 성능 지표 기록을 포함하는 공유 네트워크 디렉토리에 대한 저대역폭 액세스를 통해 독점적으로 상호작용합니다. 이는 인스턴스 간의 다른 형태의 네트워크 통신이나 메시지 전달의 필요성을 없애고 시스템 배포를 매우 쉽게 만듭니다. 중앙 제어기가 없다는 것은 단일 실패 지점을 제거할 뿐만 아니라, 일부 작업이 오랫동안 대기열에 머무를 수 있는 경합 환경과 같은 불안정한 컴퓨팅 환경에서의 훈련을 허용합니다. 이러한 경우, 훈련을 늦게 시작하는 에이전트는 일찍 시작한 다른 <span class="math inline">P</span> 구성원보다 불리할 수 있습니다. 이러한 에이전트가 개체군에 기여할 수 있도록, 수집된 경험의 동일한 양에 해당하는 (더 고급 에이전트의) 과거 체크포인트와만 성능을 비교합니다.</p>
<p><strong>III. 실험</strong></p>
<p>실험은 8개의 CPU 코어와 16Gb VRAM을 가진 단일 Nvidia V100 GPU 인스턴스를 사용하여 수행되었습니다. Isaac Gym 엔진을 사용하여 각 GPU에서 8192개의 병렬 환경을 시뮬레이션할 수 있었습니다. GPU 기반 벡터화 RL 구현인 rl_games와 결합하여 단일 팔 작업의 경우 초당 <span class="math inline">5 \times 10^4</span> 샘플, 양팔 시나리오의 경우 초당 <span class="math inline">3.5 \times 10^4</span> 샘플의 훈련 처리량에 도달할 수 있었습니다. 이 속도로, <span class="math inline">5 \times 10^9</span> 환경 전이에 대한 성공적인 정책을 훈련하는 데 단일 팔 작업은 30시간, 양팔 작업은 40시간이 소요됩니다.</p>
<p>비PBT 실험의 경우, 여러 무작위 시드에서 시작하여 각 시드를 단일 인스턴스에서 훈련했습니다. PBT 실험에서는 <span class="math inline">|P|</span>개의 개별 1-GPU 인스턴스를 사용했으며, 이들은 공유 디렉토리(NFS/sshfs 공유 폴더)에 대한 저대역폭 액세스를 사용하여 정보를 교환했습니다. 표 II는 실험에 사용된 RL 파라미터 및 보상 조정 계수를 나열합니다.</p>
<p>그림 4와 5는 <span class="math inline">|P|=8</span>인 PBT와 일반 PPO의 성능을 보여줍니다. 두 경우 모두 동일한 양의 컴퓨팅을 사용하고, 개체군 내 최고의 에이전트와 8개의 독립적인 PPO 실행 중 최고의 에이전트를 비교했습니다. PBT가 모든 시나리오에서 훈련을 실질적으로 개선했으며, 그 중 세 가지 시나리오에서는 PBT가 알고리즘이 비자명한(non-trivial) 성능에 도달하는 가능 요인이 됨을 발견했습니다. 예를 들어, 단일 팔 재정렬 작업에서 PBT 없이 8개의 단일 GPU 훈련 세션 중 어느 것도 목표 허용 오차 <span class="math inline">\epsilon^*</span>에 도달하지 못했습니다 (그림 4의 가장 오른쪽 그래프). 세 가지 시나리오 중 재정렬은 탐색에 가장 많이 의존합니다. PBT는 유망한 에이전트에 계산 자원을 집중시키고 RL 하이퍼파라미터 및 보상 조정 계수의 여러 조합을 시도함으로써 RL의 탐색 능력을 크게 증폭시킵니다.</p>
<p>전반적인 복잡성과 탐색 과제가 크게 증가했음에도 불구하고, 학습 접근 방식은 양팔 작업에도 잘 확장됩니다. PBT 양팔 에이전트(그림 5)는 재정렬 작업에서 더 나은 성능을 보였으며, <span class="math inline">N_{max} = 50</span> 성공 중 거의 40회에 도달했습니다. 양팔 작업은 에이전트가 끊임없이 한 손에서 다른 손으로 물체를 넘겨야 하므로, 정책은 물체를 저글링하고 던지는 데 더 자신감을 갖게 되었고, 단일 손 에이전트는 더 보수적인 인핸드 회전에 의존하는 경향을 보였습니다.</p>
<p>알고리즘의 확장 특성을 테스트하기 위해 단일 팔 재정렬 작업에 대해 16개 및 32개 에이전트의 개체군을 훈련했으며, 각 에이전트가 <span class="math inline">10^{10}</span> 환경 전이를 관찰하도록 허용했습니다 (따라서 <span class="math inline">|P|=32</span> 실험에서 수집된 총 경험량은 0.32조 환경 단계입니다). 개체군 크기가 증가할 때마다 수렴 속도와 최종 에이전트 성능 모두에서 상당한 개선을 보였으며, 최고의 에이전트의 <span class="math inline">N_{succ}</span>가 42를 초과했습니다 (그림 6 참조).</p>
<p>그림 6은 PBT가 발견한 일부 하이퍼파라미터 스케줄을 추가적으로 보여줍니다. 분명히, 메타-최적화는 훈련이 진행됨에 따라 더 작은 정책 업데이트를 선호하는 경향이 있으며, 적응형 학습률 KL-발산 임계값을 강화합니다 (섹션 III-C). 고성능 정책은 큰 SGD 단계에 상당히 민감해지며, 이러한 스케줄은 트러스트 영역(trust region)을 강화하고 파괴적인 파라미터 업데이트를 방지하는 데 도움이 된다고 가정합니다. 흥미롭게도, 가장 큰 실험(<span class="math inline">|P|=32</span>)만이 더 높은 <span class="math inline">\gamma</span>를 가진 솔루션을 찾을 수 있었습니다. 이 실험에서 가장 성능이 좋은 에이전트는 대안적인 전략을 사용합니다: 특정 재정렬의 경우 물체를 다시 테이블에 놓고 회전시킨 다음 다시 집어 들어 목표 위치에 놓는 것을 선택합니다. 이 접근 방식은 더 느리지만, 물체를 떨어뜨릴 위험을 최소화하고 장기적으로 더 나은 결과를 초래합니다.</p>
<p><strong>IV. 결론</strong></p>
<p>이 논문에서 종단 간 심층 RL이 정교한 숙련된 매니퓰레이터를 위한 제어 정책을 학습하는 능력을 입증했습니다. PBT를 대규모로 사용하여 접촉이 많은 조건에서 고자유도 시뮬레이션 로봇 시스템을 제어할 수 있는 에이전트를 훈련했습니다. 시나리오는 로봇 숙련도 영역의 작은 부분만을 다루지만, 이러한 물체 재배치 과제를 해결하는 것은 인간 수준의 물체 조작 능력을 가진 로봇 시스템의 실제 환경 배포를 향한 중요한 단계라고 생각합니다.</p>
<p>에이전트들이 시뮬레이션에서 강력한 성능을 보여주지만, 실제 적용이 가능하기 전에 극복해야 할 많은 추가 장애물이 있습니다. 정책은 로봇 능력의 한계에서 공격적인 제어를 보여주며, 이는 실제 환경에서 장비 손상으로 이어질 수 있습니다. 유망한 접근 방식 중 하나는 보수적인 동작 사전 지식(motion priors)을 부과하여 실제 로봇의 안전을 개선하기 위해 Riemannian Motion Policies 또는 Geometric Fabrics를 활용하는 것입니다.</p>
<p>가장 중요한 미래 연구 방향 중 하나는 시뮬레이션-실제 환경 격차를 줄이는 것입니다. 시뮬레이션-실제 환경 전이(transfer)를 개선하는 것으로 나타난 한 가지 접근 방식은 훈련 중에 물리적 파라미터를 무작위화하는 것입니다. Dactyl과 같은 프로젝트는 도메인 무작위화(domain randomization)를 통한 정책 훈련이 특히 어렵고 상당한 계산 예산이 필요할 수 있음을 보여주었습니다. DeXtreme과 유사하게, 병렬 물리 시뮬레이션과 고처리량 GPU 가속 학습을 기반으로 하는 본 연구의 접근 방식은 특히 도전적인 양팔 시스템을 포함하는 실험에 대한 계산 요구 사항을 크게 줄이고 더 넓은 연구 커뮤니티가 접근할 수 있도록 할 잠재력을 가지고 있습니다.</p>
</section>
<section id="detail-review" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Detail Review</h1>
<blockquote class="blockquote">
<p>DexPBT: Population Based Training을 활용한 손-팔로봇 조작 학습 확장 – 논문 심층 리뷰</p>
</blockquote>
<section id="논문의-핵심-아이디어-요약" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="논문의-핵심-아이디어-요약"><span class="header-section-number">2.1</span> 논문의 핵심 아이디어 요약</h2>
<p><strong>배경 및 문제 정의</strong>: 전통적인 로봇 매니퓰레이션 연구에서는 주로 2핑거 그리퍼와 같은 단순 말단장치를 사용하는 로봇팔을 다루어 왔습니다. 이는 다수의 접촉이 발생하는 복잡한 물체 조작에서 고자유도(High-DoF) 로봇 손을 제어하기가 매우 어렵기 때문입니다. 인간 손과 유사한 다관절 로봇 핸드는 잠재적으로 더 뛰어난 조작 능력을 제공하지만, 접촉이 많은 환경에서 수십 개의 자유도를 안정적으로 제어하는 것은 큰 도전 과제입니다. 이에 최근 들어 심층 강화학습 기법을 활용하여 이러한 복잡한 조작 문제를 풀려는 시도가 늘고 있습니다. OpenAI의 Dactyl이나 NVIDIA의 DeXtreme과 같은 사례들은 대규모 시뮬레이션 강화학습을 통해 인간 손 모형의 복잡한 물체 내 손 조작을 성공적으로 학습시킨 바 있습니다. 하지만 이들 작업은 주로 손목 고정된 로봇 손 단독 또는 비교적 제한된 환경에 초점을 맞추었고, 학습 과정에서 막대한 탐색과 튜닝 노력이 요구되었습니다.</p>
<p><strong>연구 목표</strong>: 본 논문의 저자들은 이러한 접근을 한 단계 확장하여, 멀티핑거 로봇 손(예: Allegro Hand, 4손가락 16자유도)을 로봇 팔(Kuka 암, 7자유도)에 부착한 손-팔 통합 시스템으로 다루고자 합니다. 나아가 한쪽 팔이 아닌 양쪽손-팔 시스템(총 46자유도)에 대한 양손 조작(ambidextrous manipulation) 정책도 단일 신경망으로 학습시키는 것을 목표로 삼았습니다. 해결하고자 하는 과제는 가정이나 창고 환경에서 유용한 다양한 물체 조작 작업들로, 구체적으로 재그립(regrasping), 잡아서-던지기(grasp-and-throw), 물체 자세 재정렬(reorientation)의 세 가지 시나리오로 구성됩니다. 이 작업들은 물체를 집어 들고 재배치하거나, 물체를 들어 올렸다가 목표 지점에 던지거나, 오랜 시간 동안 물체를 쥔 채 정밀하게 위치와 자세를 바꾸는 등 각기 다른 난제를 포함하고 있습니다. 예를 들어 재그립 작업에서는 테이블 위 물체를 안정적으로 잡아 들어 목표 위치에 일정 시간 유지해야 하므로 물체를 떨어뜨리지 않는 견고한 그립이 중요합니다. 잡아서-던지기 작업은 팔 길이 밖에 있는 용기에 물체를 던져 넣는 과제로, 정확한 조준 및 타이밍이 핵심이며 물체를 경로상의 정확한 지점에서 놓아줘야 성공합니다. 자세 재정렬 작업은 난이도가 가장 높은 시나리오로, 물체를 잡은 후 연속적인 목표 위치-자세 변화를 따라 이동시켜야 합니다. 이 경우 몇 분에 해당하는 긴 시뮬레이션 시간 동안 물체를 안정적으로 쥐고 있어야 하며, 팔의 모든 관절을 미세 조정하는 정교함이 요구됩니다. 단순 팔 움직임만으로 목표 자세에 도달할 수 없을 때는 손가락을 이용한 내손(in-hand) 회전 조작도 수행해야하므로, 사실상 이전 연구들의 난제를 모두 결합한 형태입니다. 특히 재그립과 재정렬 작업에서는 최종 목표 정확도가 1cm 이내의 오차로 제한되어 있어 매우 정밀한 제어가 필요합니다 (던지기 작업은 목표 용기에 들어가는 것을 중점으로 하여 약 7.5cm의 오차 허용). 저자들은 나아가 양손 협력 시나리오도 정의하는데, 하나의 물체를 두 로봇 손팔이 주고받으며 재배치하는 설정입니다. 이 듀얼-암 시나리오에서는 초기 물체 위치와 목표 위치를 한 팔로는 절대 달성할 수 없도록 설정하여, 반드시 두 팔이 물체를 전달하며 협력해야 임무를 완수하도록 하였습니다. 예컨대 한손으로 물체를 집어 다른 손에 넘겨주고, 다시 물체의 자세를 조정하여 최종 위치에 배치하는 등 모든 단일 팔 작업의 어려움이 합쳐진 형태입니다.</p>
<p><strong>주요 제안 방법</strong>: 이렇게 복잡한 손-팔 조작 문제를 해결하기 위해, 논문에서는 「DexPBT」 프레임워크를 제안합니다. 이는 병렬로 실행되는 심층 강화학습(RL) 알고리즘에 개체군 기반 훈련(Population Based Training, PBT) 기법을 결합한 형태의 학습 체계입니다. 강화학습 측면에서는 대규모 병렬 시뮬레이션 환경에서 온-폴리시(onpolicy) 알고리즘를 사용하여 정책을 학습합니다 (구체적으로 PPO 알고리즘을 LSTM 기반 정책과 함께 활용). 여기에 PBT라는 바깥쪽 최적화 루프를 추가하여, 여러 개의 학습 에이전트가 병렬로 서로 다른 하이퍼파라미터와 초기 조건 하에 탐색을 진행하도록 합니다. 일정 간격마다 개체군 내부의 성능을 비교하여 상위 에이전트의 신경망 파라미터를 하위 에이전트에 복제(exploit)하고, 하위 혹은 중간 성능 에이전트의 하이퍼파라미터를 무작위로 변이시켜(explore) 재시도하게 함으로써, 강화학습 탐색 능력을 증폭시키는 것이 핵심 아이디어입니다. 요약하면, 여러 에이전트가 각자 조금씩 다른 조건에서 학습을 진행하다가 주기적으로 잘 된 전략을 공유하고 탐색 범위를 넓히도록 설정하여, 복잡한 과제에서 개별 에이전트로는 찾기 힘든 우수한 정책을 발견하도록 유도하는 것입니다. 특히 저자들은 이러한 PBT 알고리즘을 분산 환경에서도 동작하도록 중앙 조정자 없이(decentralized) 구현함으로써, 실제 대규모 GPU 클러스터에서 수십 개의 병렬 학습 작업을 손쉽게 관리할 수 있게 하였습니다.</p>
<p><strong>핵심 성과</strong>: 제안된 DexPBT 접근법을 통해, 저자들은 앞서 언급한 난제들에서 기존의 단일 강화학습 대비 월등한 성능 향상을 달성했습니다. 논문에 따르면, 동일한 연산 자원을 투입한 비교 실험에서 PBT를 접목한 경우가 모든 과제에서 일반 PPO 기반 엔드투엔드 학습을 유의미하게 능가했으며, 특히 난도가 높은 과제에서는 정책 학습의 성패를 좌우할 결정적 요소가 되었습니다. 예를 들어, 단일 손으로 수행하는 재정렬 과제의 경우 PBT를 사용하지 않은 8개의 독립 실행 실험 중 어느 것도 최종 목표에 도달하지 못했지만, DexPBT 프레임워크 하에서는 탐색 증폭과 보상 튜닝 덕분에 성공적인 정책을 발견할 수 있었다고 보고됩니다. 또한 하나의 신경망으로 양손 로봇을 제어하는 매우 복잡한 경우도 PBT 덕분에 학습이 가능했으며, 이는 표준 방법으로는 불가능했던 새로운 양손 조작 정책의 학습을 가능하게 한 중요한 성과로 강조됩니다. 정리하면, DexPBT는 고차원 접촉력 학습 문제에서 탐색 효율을 극대화하고 하이퍼파라미터를 자동 최적화함으로써, 기존에 다루기 힘들었던 손-팔 조작 작업들을 단기간에 성공적으로 학습할 수 있음을 입 증하였습니다.</p>
</section>
<section id="기술적-기여-및-dexpbt-프레임워크의-구성" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="기술적-기여-및-dexpbt-프레임워크의-구성"><span class="header-section-number">2.2</span> 기술적 기여 및 DexPBT 프레임워크의 구성</h2>
<section id="dexpbt-프레임워크-구조와-강화학습-기법" class="level3" data-number="2.2.1">
<h3 data-number="2.2.1" class="anchored" data-anchor-id="dexpbt-프레임워크-구조와-강화학습-기법"><span class="header-section-number">2.2.1</span> DexPBT 프레임워크 구조와 강화학습 기법</h3>
<p>DexPBT는 크게 내부의 강화학습 알고리즘과 외부의 PBT 최적화 루프로 구성됩니다. 내부 학습 알고리즘으로는 PPO 기반의 온-폴리시 심층 강화학습이 사용되었습니다. 구체적으로, 관측된 상태로부터 동작을 산출하는 정책 <span class="math inline">\pi_\theta(a|s)</span>와 가치를 평가하는 가치함수 <span class="math inline">V_\theta(s)</span>를 한 신경망(파라미터 <span class="math inline">\theta</span>)으로 표현하며, LSTM 기반의 순환 신경망에 다층 퍼셉트론(MLP) 출력층을 결합한 아키텍처를 채택하였습니다. 순 환신경망을 둔 이유는 현실 로봇에 적용할 때 부분 관측 상황에 대비하고, 시뮬레이션에서도 메모리 기반의 추론 안정성을 높이기 위함입니다. PPO 알고리즘은 수집한 롤아웃 데이터에 대해 정책 갱신을 수행하면서도 업데이트 폭을 KL발산(KL-divergence)으로 제한하여 학습의 안정성을 보장하는 것으로 알려져 있습니다. 저자들은 이 PPO를 대규모 병렬 환경에 효율적으로 적용하기 위해, 수만 개 수준의 배치 샘플을 한 번에 GPU 상에서 처리하는 구현을 활용했습니다. 예를 들어 Isaac Gym 시뮬레이터로 8192개의 환경에서 데이터를 동시에 수집하고, 매 학습 단계마다 <span class="math inline">2^{15} \approx 32768</span> 개의 transition으로 구성된 미니배치를 GPU에 올려 연산함으로써 하드웨어 활용도를 극대화했습니다. 또한 관측값과 리턴의 정규화 기법을 적용하여 스케일 변화에 러버스트하게 만들고, 적응형 학습률 제어를 도입하여 정책의 KL 발산량이 목표치 이상 커지면 학습률을 줄이고 너무 낮으면 다시 높이는 자동 스케줄 조정을 수행했습니다. 이러한 장치들은 고난이도 지속학습에서 흔히 발생하는 학습 불안정성을 완화하고자 적용된 것으로, 실제 실험에서도 학습 초기 폭발적 업데이트로 인한 정책 붕괴를 방지하는 데 기여했습니다.</p>
<p>PBT 외부 루프에서는, <span class="math inline">N</span>개의 에이전트 (정책 복제본)을 병렬로 학습시키며 세대 경쟁 방식의 진화전략을 수행합니다. 각 에이전트는 고유한 하이퍼파라미터 설정(예: 학습률, 할인율, 손실 가중치, 보상 계수 등)을 갖고 독립적으로 PPO 훈련을 진행합니다. 일정 주기마다 모든 에이전트의 현재 정책 성능을 평가하여 메타-성능 지표 <span class="math inline">r_{\text{meta}}</span> 를 산출하고 랭킹을 매깁니다. 보통 상위 20~30%를 우수 집단, 하위 20~30%를 열등 집단으로 구분하며, 중간은 그대로 둡니다. PBT의 exploit 단계에서는 열등 집단의 에이전트들을 우수 집단의 최상위 에이전트로부터 치환합니다. 구체적으로, 성능 최상 에이전트의 신경망 가중치 <span class="math inline">\theta^</span>를 복사하여 하위 에이전트 <span class="math inline">\theta</span>를 대체하고, 해당 최상 에이전트의 하이퍼파라미터 설정 <span class="math inline">p^</span>에 확률적 돌연변이(mutation)를 가한 새로운 하이퍼파라미터 <span class="math inline">p</span> 로 치환합니다. 이렇게 하면 성능이 안 좋던 에이전트가 곧바로 상위 에이전트의 정책 노하우를 이어받아 재도전하되, 학습률이나 보상 계수 등을 살짝 변경함으로써 탐색의 다양성을 확보하게 됩니다. 한편 중간 성능 에이전트들은 자신의 하이퍼파라미터만 소폭 변이시키고(weight는 유지) 계속 학습을 이어갑니다. 이러한 exploration 단계를 거친 후 모든 에이전트들이 다시 개별 PPO 학습을 진행하고, 추후 주기에 다시 평가-교배 과정을 반복하는 식입니다. 이 알고리즘은 기존 PBT 기법(Jaderberg 등[13]의 연구)과 동일한 개념이지만, 본 논문에서는 특히 복잡한 로봇 조작 문제에 맞게 성공률 기반의 메타-성능 지표와 보상 함수 단계화 등을 설계하여 적용한 점이 특징입니다.</p>
<p>DexPBT 프레임워크의 또 다른 기술적 기여는 분산형 PBT 구현입니다. 전형적인 PBT는 중앙에서 개체군을 관리하는 오케스트레이터 프로세스가 필요하고, 개별 학습 프로세스들과 계속 통신해야 합니다. 그러나 이 논문의 저자들은 중앙서버 없이도 동작하는 완전 분산 PBT 아키텍처를 고안하여, 실험 노드 간 공유 디렉토리만 있으면 각 에이전트가 스스로 PBT 절차를 수행하도록 만들었습니다. 구체적으로, 모든 에이전트는 자신의 성능 평가 결과와 최신 정책 체크포인트를 공유 폴더에 주기적으로 기록하고, 다른 에이전트들의 기록을 읽어들여 현재 자신이 개체군에서 어느 순위에 해당하는지 자율적으로 판단합니다. 그리고 자신의 순위가 하위권이면 공유 폴더에서 상위 에이전트의 체크포인트를 불러와 앞서 설명한 exploit/mutate 단계를 수행한 뒤 다시 학습을 이어갑니다. 이 과정에서 별도의 메시지 전달이나 동기화가 없어도 되므로, 구현과 운영이 단순해지고 단일 실패 지점(single point of failure)이 제거되는 이점이 있습니다. 실제로 이러한 구조 덕분에 Slurm과 같은 클러스터 스케줄러 상에서 쉽게 구동할 수 있었으며, 일부 학습 노드가 중도에 장애로 빠져도 나머지 에이전트들이 공유 폴더에 기록된 과거 체크포인트를 기준으로 자체 판단하여 순위를 매기므로 PBT 절차가 계속 진행될 수 있었습니다.</p>
</section>
<section id="단계적-보상-설계와-pbt-메타-목표" class="level3" data-number="2.2.2">
<h3 data-number="2.2.2" class="anchored" data-anchor-id="단계적-보상-설계와-pbt-메타-목표"><span class="header-section-number">2.2.2</span> 단계적 보상 설계와 PBT 메타-목표</h3>
<p>고난도 물체 조작 과제에서 보상 함수를 어떻게 설계하느냐는 학습 성능에 큰 영향을 미칩니다. 보상이 지나치게 희소(sparse)하면 탐색 초기 성공 사례를 전혀 만나지 못해 학습이 진행되지 않고, 반대로 풍부한 밀도형 보상(dense reward)를 주면 에이전트가 최종 목표와 무관한 중간 보상 신호만 최적화하는 현상 유지 행위(예: 물체를 끝까지 들고가지 않고 중간까지만 움직인 후 그 보상만 반복적으로 얻는 경우)에 빠질 수 있습니다. 본 논문에서는 이러한 문제를 해결하기 위해 태스크 완료까지의 동작 단계를 따라 보상함수를 계층적으로 구성하였습니다. 예를 들어 재그립/던지기/재정렬 과제를 위한 총 보상은 다음 세 부분의 합으로 이루어집니다 : 접근 보상 <span class="math inline">r_{reach}</span>: 에피소드 시작 시 로봇 손 끝부분(end-effector)이 물체에 가까이 접근할수록 주는 보상입니다. 초기 거리 <span class="math inline">d</span>가 줄어들 때마다</p>
<p><span class="math display">r_{reach} = \alpha_{reach} \max(d_{\text{closest}} - d,\; 0)</span></p>
<p>의 형태로 양의 보상이 주어지며, 한 번 손이 물체에 충분히 가까워지면(<span class="math inline">d=0</span>) 이 보상은 더 이상 증가하지않습니다. 이를 통해 탐색 초기에 팔을 물체 쪽으로 뻗도록 유도합니다. - 잡기 보상 <span class="math inline">r_{pick}</span>: 물체를 들어올렸을 때 주는 보상입니다. 로봇 손이 물체를 붙잡아 테이블 높이보다 일정 높이(<span class="math inline">15\text{cm}</span>) 이상 들어올이면 1회에 한해 <span class="math inline">\;r_{pick} = \alpha_{pick} h_t + r_{picked}\;</span> 형태의 보상을 줍니다. 여기서 <span class="math inline">h_t</span>는 물체의 높이, <span class="math inline">r_{picked}</span>는 집기 성공 순간에 발생하는 추가 상수 보상입니다. 이 보상은 물체를 들어올린 이후에는 더 이상 증가하지 않으므로, 한 번 성공적으로 픽업한 후에는 더 높은 곳으로 들고 갈 유인이 사라지고 다음 단계 보상으로 넘어갑니다. - 목표 도달 보상 <span class="math inline">r_{targ}</span>: 물체가 목표 위치(및 자세)에 가까워질수록 주는 보상입니다. 물체를 들어올린 이후( <code>1picked=1</code> )부터 활성화되며, 목표와의 거리(및 각도)를 <span class="math inline">\hat{d}</span>라 할 때 <span class="math inline">r_{targ} = \alpha_{targ}\max(\hat{d}{closest} - \hat{d},;0) + r{success}</span>로 정의됩니다. 즉 목표와 점점 가까워질수록 증대되는 신용할당을 주되, 목표 범위 이내에 들어가면 한 번 <span class="math inline">r_{success}</span>의 종합적인 보상을 추가로 지급합니다. 특히 재정렬 과제의 경우 위치뿐 아니라 물체의 회전 각도까지 맞춰야 하므로, <span class="math inline">\hat{d}</span>를 위치 오차와 자세 오차를 모두 고려한 값으로 설정하여 목표 상태와 일치하게 유도합니다. - 속도 패널티 <span class="math inline">r_{vel}</span>: 행동의 급격함으로 인한 불안정을 막기 위해, 로봇 조작의 속도나 힘이 지나치게 크면 그에 비례해 작은 패널티를 부과합니다. 이를 통해 에이전트가 불필요하게 난폭한 움직임을 피하고 안정적 으로 조작하도록 유도합니다.</p>
<p>이렇듯 <span class="math inline">r = r_{reach} + r_{pick} + r_{targ} - r_{vel}</span> 로 구성된 보상 체계에서는, 각 단계의 주요 보상 신호들이 서로 간섭하지 않고 순차적으로 등장하도록 설계된 것이 핵심입니다. 실제로 하나의 에피소드 진행 동안 로봇은 (1)물체에 접근 → (2) 집기 → (3) 이동 및 배치의 단계를 순차적으로 거치는데, 각 단계별로 해당 과업에만 반응하는 보상 성분이 존재하고 이전 단계 보상은 이미 소멸된 상태입니다. 예컨대 로봇 손이 물체에 닿는 순간 <span class="math inline">r_{reach}</span>는 더이상 증가하지 않으므로 이후에는 <span class="math inline">r_{pick}</span>만이 동작에 영향하고, 물체를 집어 들면 <span class="math inline">r_{pick}</span>은 종료되고 <span class="math inline">r_{targ}</span>로 이행되는 식입니다. 이러한 단계적 보상 설계 덕분에 보상값 튜닝이 용이해지고, 에이전트가 중간 보상 하나만 편향적으로 노리는 현상을 방지할 수 있습니다. (만약 <span class="math inline">r_{reach}, r_{pick}, r_{targ}</span>가 동시에 활성화되어 있었다면, 에이전트가 물체를 끝까지 들지는 않고 테이블 위에서 목표 방향으로 미는 식으로 <span class="math inline">r_{targ}</span>만 극대화하려고 하는 등 엉뚱한 정책이 나올 수 있음을 언급합니다.)</p>
<p>한편, PBT에서 에이전트의 성과를 판단하는 메타-목표(혹은 적합도 함수) 역시 이 작업의 특성에 맞게 정의되었습니다. 저자들은 각 에이전트의 정책을 일정 에피소드 동안 실행시켜 얻은 연속 성공 횟수 <span class="math inline">N_{\text{succ}}</span>를 기본 성능 지표로 삼았습니다. <span class="math inline">N_{\text{succ}}</span>란 하나의 에피소드에서 목표 달성에 연속적으로 성공한 횟수를 의미합니다. 예를 들어 재정렬 작업에서 에이전트가 첫 번째 목표 위치에 물체를 정확히 배치하면 1회 성공으로 기록하고 즉시 새로운 목표로 이어집니다. 이렇게 실패할 때까지 연속으로 성공한 횟수를 <span class="math inline">N_{\text{succ}}</span>로 정의하며, 실험에서는 최대 50회까지 카운트하여 <span class="math inline">N_{\text{succ}}=50</span>에 도달하면 더 이상 반복 없이 성공으로 간주합니다. 이 지표는 에이전트가 한 번만 성공하고 끝나는 것이 아니라 얼마나 연속적·안정적으로 작업을 수행할 수 있는지 나타내기 때문에, 조작 정책의 실제 유용성과 견고함을 평가하는 좋은 척도가 됩니다. PBT의 메타-성능으로 단순히 <span class="math inline">N_{\text{succ}}</span>만을 사용할 수도 있지만, 앞서 재그립/재정렬 과제에서는 성공 허용 오차 <span class="math inline">\epsilon</span>을 초기 7.5cm에서 최종 1cm까지 적 응형으로 점진 축소(annealing)하는 교육과정(curriculum)을 도입했다고 했습니다. 이로 인해 개체군 내 에이전트들마다 현재 적용 중인 성공 판정 기준 <span class="math inline">\epsilon</span> 값이 다를 수 있습니다 (일찍 성능 향상을 이룬 에이전트는 이미 <span class="math inline">\epsilon</span>이 작게 조정됨). <span class="math inline">\epsilon</span> 값이 느슨한 에이전트는 더 쉽게 성공 판정을 받아 <span class="math inline">N_{\text{succ}}</span>가 높아질 수 있으므로, 공정한 비교를 위해 메타-목표는 현재 <span class="math inline">\epsilon</span>도 함께 고려하도록 설정되었습니다. 구체적으로, 아직 최종 목표 정확도 <span class="math inline">\epsilon^</span>에 도달하지 못한 경우에는 남은 오차 비율 <span class="math inline">(\epsilon_0 - \epsilon) /(\epsilon_0 - \epsilon^)</span>를 주 성분으로 하고 <span class="math inline">N_{\text{succ}}</span>에 작은 가중치를 더해 점수를 계산하며, 일단 <span class="math inline">\epsilon = \epsilon^</span> (예: 1cm 목표 정확도)에 도달한 이후부터는 <span class="math inline">N_{\text{succ}}</span> 값 자체를 주 성분으로 삼는 형태입니다. 이렇게 하면 학습 초반에는 점점 엄격해지는 성공 기준을 달성하는 것에 보상이 주어지고, 최종 난이도에 도달한 이후에는 순수하게 연속 성공 횟수를 극대화하도록 유도할 수 있습니다. 이 메타-목표 함수를 기준으로 개체군 내 순위가 결정되므로, PBT 과정이 에이전트가 자동으로 난이도를 올리면서도 최종 성능을 높이기 위한 방향으로 작동하게 됩니다. 요약하면, DexPBT에서는 계층형 보상 함수 설계와 성공 커리큘럼 적용, 그리고 이를 반영한 PBT 메타-목표까지 제시함으로써, 복잡한 물체 조작 문제에서 보상 튜닝과 탐색 전략을 자동화*한 점이 중요한 기술적 기여라 할 수 있습니다.</p>
</section>
<section id="시뮬레이션-및-학습-환경-설정" class="level3" data-number="2.2.3">
<h3 data-number="2.2.3" class="anchored" data-anchor-id="시뮬레이션-및-학습-환경-설정"><span class="header-section-number">2.2.3</span> 시뮬레이션 및 학습 환경 설정</h3>
<p>논문의 모든 실험은 NVIDIA의 Isaac Gym 시뮬레이터 상에서 이루어졌습니다. Isaac Gym은 GPU 가속을 활용하여 물리 시뮬레이션을 대량 병렬 수행할 수 있는 환경으로, 강화학습 연구에서 점점 활발히 사용되고 있습니다. 저자들은 이 엔진을 통해 앞서 설명한 재그립, 던지기, 재정렬 작업의 사실적인 시뮬레이션 환경을 구성하였습니다. 구체적으로, Allegro Hand 로봇 손과 Kuka IIWA 7자유도 로봇 팔을 결합한 모델을 사용하고, 물체로는 적당한 크기의 직육면체 블록을 테이블 위에 놓고 다루도록 했습니다. 각 에피소드마다 초기 조건을 무작위화하여, 물체의 시작 위치와 로봇 팔의 초기 관절 상태를 다양하게 변화시켰습니다. 이는 학습된 정책이 특정 초기 조건에 과적합되지 않고 일반적인 상황에 대응할 수 있도록 하기 위함입니다. 앞서 과제 설명에서 언급한 대로, 재그립 작업은 목표 지점 위치만 주어지고 물체의 최종 자세는 상관없게 설정되며, 던지기 작업은 목표로 삼는 용기(container)의 위치를 무작위로 변화시켜 여러 거리와 각도에 대응하도록 했습니다. 재정렬 작업은 목표 위치와 자세 쌍을 여러 단계 제시하며, 로봇이 이를 순차적으로 달성하도록 환경을 구성했습니다. 듀얼-암 환경에서는 동일한 물체를 두 대의 Allegro+Kuka 로봇이 마주보는 형태로 배치하고, 초기/목표 물체 위치를 한쪽 로봇만으로는 도달 불가능한 위치로 정해 반드시 두 로봇의 협력이 필요하도록 시나리오를 설계했습니다. 이 경우 관측 벡터와 행동 벡터의 차원이 두 로봇을 아우르도록 확대되지만, 단일 정책 신경망이 양손 제어까지 출력하도록 설계되어 있습니다.</p>
<p>대규모 병렬 환경을 활용한 덕분에, 학습 효율 측면에서 매우 높은 샘플 처리율(throughput)을 달성했습니다. 논문에따르면 단일 NVIDIA V100 GPU에서 Isaac Gym으로 8192개의 병렬 환경을 구동하고 PPO 알고리즘을 실행한 결과, 초당 약 5만 개의 환경 상호작용 샘플을 처리할 수 있었다고 합니다. 싱글 손-팔 작업의 경우 50억 스텝(5×10^9)의 시뮬레이션 상호작용을 약 30시간 만에 돌파할 정도의 속도이며, 양손 작업의 경우 동시 제어 관절이 늘어나 다소 느리지만 그래도 약 40시간에 50억 스텝을 학습할 수 있었다고 합니다. 이러한 속도는 과거 CPU 기반 시뮬레이터로는 수주일 걸릴 경험량을 불과 하루~이틀 만에 확보하는 수준으로, 대규모 병렬화의 위력을 잘 보여줍니다. 연구진은 8개의 CPU 코어와 1개의 V100 GPU로 구성된 인스턴스를 한 대의 학습 노드로 활용하였고, PBT 실험의 경우 이러한 노드를 개체군 크기만큼 (예: 8개 노드 등) 병렬 활용하였습니다. PBT를 사용하지 않는 베이스라인 실험도 공정한 비교를 위해 동일한 수의 GPU 노드에서 서로 다른 랜덤 시드로 독립 실행한 여러 개의 PPO 학습으로 진행했습니다. 이렇게 하여, PBT를 통한 학습과 동일한 총 환경 상호작용 횟수와 동일한 총 연산 자원을 소비한 설정으로 결과를 비교함으로써, 제안 기법의 효과를 객관적으로 평가했습니다.</p>
</section>
<section id="기존-접근법과의-차별성" class="level3" data-number="2.2.4">
<h3 data-number="2.2.4" class="anchored" data-anchor-id="기존-접근법과의-차별성"><span class="header-section-number">2.2.4</span> 기존 접근법과의 차별성</h3>
<p>DexPBT의 가장 큰 특징은 전통적 방식이나 이전 연구들과 달리, 고난이도 손-팔 조작 문제를 end-to-end 심층 강화학습 + 자동화된 PBT 탐색으로 풀어냈다는 점입니다. 과거에는 로봇 조작에서 복잡한 접촉을 다루기 어렵기 때문에, 로봇손 대신 평행 그리퍼같이 단순한 말단장치를 쓰거나, 물체 조작을 아예 사전에 구조화된 계획 문제로 풀거나, 혹은 사람의 데모나 휴리스틱으로 초기 정책을 주입하는 식으로 난이도를 낮추곤 했습니다. 반면 이 논문은 사람 개입없이 순수 강화학습으로, 그것도 팔+손이 결합된 23자유도 시스템을 다루고 있어 한 단계 복잡성이 높습니다. OpenAI의 Dactyl 연구는 24자유도의 Shadow Hand 로봇손으로 손안에서 물체 돌리기를 성공했지만, 팔 움직임은 없었고 실제 학습에는 도메인 랜덤화 등의 기법으로 막대한 반복 학습이 필요했습니다. 본 연구는 Dactyl 등에서 입증된 대규모 시뮬레이션 학습 방향성을 따르면서도, 팔 움직임까지 포함하여 작업 공간을 확대하고 더욱 현실적인 조작 시나리오로 나아갔습니다. 또한 PBT를 활용해 하이퍼파라미터 튜닝과 보상 설계를 자동화함으로써, 사람이 일일이 보상 가중치나 학습률을 조정해야 했던 부분을 Meta-RL 방식으로 해결했다는 점도 차별화됩니다. 특히 저자들의 접근법은 NVIDIA 연구진의 이전 작업인 DeXtreme [10]과 유사한 맥락에서 출발하지만, 여기에 인구 기반 훈련 기법을 추가함으로써 탐색 능력을 비약적으로 향상시키고 성능 안정성을 높인 점이 핵심적인 개선입니다. 실제로 “우리는 Handa 등[10]의 접근법에 대규모 병렬 시뮬레이션을 활용한다는 점에서 유사하지만, PBT 외부 루프를 추가하여 탐색을 강화하고, 보상 튜닝 및 하이퍼파라미터 검색을 자동화하였다”고 밝히고 있습니다.</p>
<p>또 다른 차별성은 양손 조작 학습 자체입니다. 기존에 양팔 로봇을 학습시킨 선행 연구는 주로 각각의 팔이 별개 작업을 수행하거나, 두 팔을 쓰더라도 병렬 그리퍼나 진공 흡착 패드와 같은 비교적 제어가 쉬운 말단효과기를 사용한 경우가 많았습니다. 두 팔로 하나의 물체를 협력 조작하는 연구도 존재했지만, 이 논문의 경우처럼 모두 다관절 손가락을 가진 팔 두 개를 단일 정책으로 동시에 제어하도록 학습시킨 예는 매우 드뭅니다. 저자들은 이 점을 특히 강조하면서, 인간이 두 손을 사용하는 복잡한 작업을 로봇이 재현하도록 하는 것은 로봇 조작 분야에서 하나의 중요한 이정표라고 언급합니다. 실제 양손 조작을 성공하려면 폭발적으로 늘어나는 상태공간과 행동공간 때문에 학습 난이도가 기하급수적으로 높아지는데, DexPBT는 PBT를 통한 방대한 탐색과 최적화로 이를 정복했다는 점에서 기존 방법론 대비 크게 진일보한 것으로 평가됩니다.</p>
<p>마지막으로, 분산형 PBT 프레임워크의 구현 및 공개도 중요한 기여입니다. 저자들은 자신들의 코드를 공개하여 다른 연구자들이 손쉽게 Slurm 클러스터나 AWS 등의 환경에서 이 방식을 활용할 수 있도록 했습니다. 분산 학습을 위한 중앙 관리 없이도 개체군 기반 탐색을 수행하는 알고리즘 구조는, 향후 대규모 강화학습 실험을 수행하는 데 유용한 엔지니어링 혁신으로 볼 수 있습니다. 이는 단순히 로봇 조작 분야 뿐 아니라, 강화학습의 자동화된 탐색 기법을 필요한 다른 복잡한 문제들에도 적용하는 데 도움을 줄 수 있을 것입니다.</p>
</section>
</section>
<section id="실험-결과-분석" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="실험-결과-분석"><span class="header-section-number">2.3</span> 실험 결과 분석</h2>
<section id="실험-환경-및-평가-지표" class="level3" data-number="2.3.1">
<h3 data-number="2.3.1" class="anchored" data-anchor-id="실험-환경-및-평가-지표"><span class="header-section-number">2.3.1</span> 실험 환경 및 평가 지표</h3>
<p>학습 및 테스트 환경: 앞서 기술된 시뮬레이션 설정에서, 저자들은 제안한 DexPBT 프레임워크의 효과를 정량적으로 평가하기 위해 다양한 실험을 수행했습니다. 각 작업(재그립, 던지기, 재정렬 및 양손 재그립, 양손 재정렬)에 대해 PBT를 사용한 경우와 사용하지 않은 경우를 비교하였고, 특히 PBT를 사용하지 않은 경우에는 동일한 수의 독립적 PPO 학습 시도를 통해 성능 분포를 관찰했습니다. 모든 실험은 Isaac Gym 상에서 진행되었기 때문에 시뮬레이션-현실 간 차이는 존재하지 않지만, 대신 시드(seed)와 초기 조건이 성능에 미치는 영향을 최소화하고자 충분히 많은 에피소드와 병렬 환경을 활용했습니다. 최종 성능 평가는 앞서 정의한 연속 성공 횟수 <span class="math inline">N_{\text{succ}}</span>를 기본 지표로 삼았습니다. 정책이 일정 시간(step) 내에 목표를 달성하면 성공으로 간주하고 새로운 목표를 부여하며, 하나의 에피소드 내에서 실패하지 않고 연속으로 성공한 횟수를 <span class="math inline">N_{\text{succ}}</span>로 기록합니다. 각 작업별로 최대 50회의 연속 성공을 달성할 수 있도록 에피소드 길이를 길게 설정했으며, <span class="math inline">N_{\text{succ}}=50</span>에 도달하면 해당 에이전트는 사실상 과제를 완벽히 익혔다고 평가합니다. 이와 함께 보조적인 성능 척도로 성공률(예: 에피소드 당 평균 성공 횟수 또는 최종 목표 달성 여부)도 참고하였으나, 핵심 척도로는 연속 성공 횟수가 채택되었습니다. 이는 조작 과제의 특성상 한두 번 간 신히 성공하는 것보다 지속적으로 실패 없이 수행하는 능력이 중요하기 때문입니다. 예컨대 1회 성공 후 실패하는 정책 보다 5회 연속 성공하는 정책이 훨씬 안정적이고 실용적입니다.</p>
<p><strong>하이퍼파라미터 및 PBT 설정</strong>: PBT를 통해 탐색된 하이퍼파라미터 목록에는 PPO 학습률, 할인율 <span class="math inline">\gamma</span>, GAE 파라미터, PPO epoch 횟수, 정책 클리핑 계수 등 강화학습 알고리즘 관련 값들 뿐만 아니라, 보상 함수의 <span class="math inline">\alpha_{reach}, \alpha_{pick}, \alpha_{targ}</span> 및 <span class="math inline">r_{picked}, r_{success}</span> 등의 보상 가중치 값들도 포함되었습니다. 즉 PBT는 학습 자체의 파라미터뿐만 아니라 보상 shaping 계수들까지 개체군에서 다양하게 변이시키면서 탐색했습니다. 이는 사람이 직관적으로 튜닝하기 어려운 보상 항목들(예: 접근 vs 집기 보상의 비중)을 자동으로 조절하여 최적의 학습 성능을 끌어내기 위함입니다. PBT 메타-성능 지표는 앞서 언급한 <span class="math inline">r_{\text{meta}}</span>를 사용하였고, 개체군 크기는 기본적으로 8개 에이전트로 설정되었습니다 (추가 실험에서는 16, 32까지 확장). PBT 단계는 일정 학습 iteration 마다 (수십 또는 수백만 스텝 간격) 수행되었으며, 변이 확률과 범위 등은 표 II에 명시된 값을 따랐다고 합니다. 한편, 베이스라인으로 비교된 “No PBT” 실험은 PBT의 외부 루프 없이 동일한 환경에서 PPO만 수행한 것이며, 이때는 초기 하이퍼파라미터를 DexPBT와 동일하게 설정하고 여러 다른 시드로 8회 반복 실험하여 그 중 최고 성능을 별도로 기록했습니다. 이를 통해 동일한 자원으로 PBT를 쓴 경우와 안 쓴 경우의 최고 성능을 공정 비교하고자 했습니다.</p>
</section>
<section id="주요-결과-및-그래프-해석" class="level3" data-number="2.3.2">
<h3 data-number="2.3.2" class="anchored" data-anchor-id="주요-결과-및-그래프-해석"><span class="header-section-number">2.3.2</span> 주요 결과 및 그래프 해석</h3>
<p><strong>단일 손-팔 조작 작업 성능</strong></p>
<p>먼저 한 손-팔로 수행하는 세 가지 작업(재그립, 던지기, 재정렬)에 대한 학습 곡선을 살펴보겠습니다. 논문 Fig.4에 제시된 결과에 따르면, PBT 적용 사례는 학습 초기부터 빠른 성능 향상을 보이며, 일정 시간 이후 꾸준히 높은 성공 횟수를 달성했습니다. 반면 기본 PPO(PBT 미적용) 실험들은 시드에 따라 편차가 컸고, 평균적으로도 PBT보다 낮은 성능 수준에 머물렀습니다. 예를 들어 재그립 작업의 경우, PBT를 사용하면 약 50억 스텝 학습 내에 평균 30회 내외의 연속 성공을 이루는데, 최고 에이전트는 40회 이상까지 도달했습니다. 반면 동일한 환경에서 PBT 없이 8번 학습한 결과는 최고 경우에만 PBT 평균 정도의 성능을 보여줄 뿐, 대다수 시드에서는 그보다 현저히 낮은 성공 횟수에 그쳤습니다. 던지기 작업 역시 PBT의 효과가 뚜렷했습니다. PBT 적용 시도들은 50억 스텝 이내에 20~30회 이상의 연속 성공을 안정적으로 기록한 반면, PBT 미사용 시도들은 최적 사례조차 이 수준에 미치기 어려웠습니다. 가장 난이도가 높은 재정렬 작업(single-arm reorientation)에서는 두 방법 간 격차가 극명했습니다. PBT를 적용한 개체군에서는 여러 에이전트가 학습 후반에 20회 이상의 연속 성공을 달성하며 일부는 30~35회까지 도달했지만, PBT를 쓰지 않은 PPO 실험들에서는 어느 것도 5회 연속 성공을 넘지 못하고 실패로 돌아갔습니다. 사실상 재정렬 작업은 PBT 없이는 학습이 불안정하여 해결되지 못한 것입니다. 저자들도 “PBT가 세 과제 모두에서 학습을 상당히 향상시키며, 이 중 세 가지 시나리오에서는 PBT가 없이는 놀라운 performance에 도달하지 못했던 것을 가능케 했다”고 언급합니다. 특히 단일 손 재정렬의 경우, 8개의 독립 PPO 시도 중 그 어떤 것도 성공 정책을 얻지 못했으나 PBT를 통해 비로소 유의미한 성능을 끌어낼 수 있었다고 구체적으로 보고하고 있습니다.</p>
<p>이러한 그래프들의 음영 영역(shaded area)은 PBT의 경우 개체군 내 최고 vs 최저 에이전트의 성능 차이를, PPO 경우 여러 시드 중 최고 vs 최저 성능 범위를 나타냅니다. 흥미롭게도 PBT의 음영 범위가 비교적 좁게 유지되는 경향을 볼 수 있는데, 이는 개체군이 일관되게 높은 성능군으로 수렴함을 시사합니다. 반대로 PPO 독립 실행들은 일부 시드는 운 좋게 잘 되더라도 다른 시드들은 실패하는 경우가 많아 편차가 큼을 알 수 있습니다. PBT가 탐색 실패 확률을 낮추고 전체 학습의 안정성을 높여준다는 증거라 할 수 있습니다.</p>
<p><strong>양손 조작 작업 성능</strong></p>
<p>Fig.5에는 두 손-팔을 동시에 제어하는 작업에 대한 결과가 나와 있습니다. 여기서는 양손 재그립과 양손 재정렬 두 가지를 시도했는데, 전반적인 경향은 단일 손 실험과 유사하나 절대적인 난이도가 더 높음을 알 수 있습니다. 우선 양손 재그립의 경우, PBT 개체군은 약 50억 스텝 학습 후 20회 이상 연속 성공하는 정책을 만들어냈으며 최고 성능은 30~35회에 달했습니다. 반면 PBT 미사용 PPO들은 대부분 10회 미만에 머무르고 일부 시드만 15회 내외를 기록하여 확연한 성능 차이가 있었습니다. 양손 재정렬 작업에서는 PBT 적용시 최고 에이전트가 거의 40회에 가까운 연속 성공을 달성했는데 (<span class="math inline">{N_{\max}}=50</span> 기준 약 80% 수준), 이는 두 손을 협력해서 물체를 아주 능숙하게 전달하고 재배치한다는 것을 의미합니다. 참고로 단일 손 재정렬에서 PBT 최고 성능이 30대 중반이었다는 점과 비교하면, 오히려 양손을 활용하니 더 높은 성공 횟수에 도달한 것입니다. 논문에서는 이를 흥미로운 관찰로 언급하면서, 양손 정책이 싱글 손 정책보다 능숙하게 물체를 다루는 경향을 보였다고 분석합니다. 구체적으로, 양손 에이전트는 물체를 한 손에서 다른 손으로 공중에 던지듯이 전환(juggling)하거나 이동 중에 빠르게 손바뀜을 하는 등 공격적이고 민첩한 조작 전략을 구사한 반면, 단일 손 정책은 물체를 떨어뜨리지 않기 위해 비교적 보수적인 회전 동작만을 사용했다고 합니다. 이는 두 손을 사용할 수 있을 때 정책이 행동 공간의 여유를 활용하여 더 최적화된(때로는 인간이 보기엔 아슬아슬한) 해결책을 찾아낸 예라고 볼 수 있습니다.</p>
<p><strong>PBT 개체군 규모 확장 실험</strong></p>
<p>추가로 저자들은 PBT의 개체군 크기 <span class="math inline">|P|</span>를 늘리면 성능이 어떻게 향상되는지도 실험했습니다. 단일 손 재정렬 작업을 대상으로, 기본 8개 에이전트 외에 16개, 32개 에이전트로 이루어진 PBT를 각각 실행하여 결과를 비교한 것이 Fig.6에 제시됩니다. 각 에이전트 당 10<sup>10</sup> (100억) 스텝의 경험을 쌓도록 장기간 학습시킨 결과, 개체군 규모가 클수록 수렴 속도와 최종 성능이 모두 향상되는 추세를 보였습니다. 특히 32개 에이전트로 학습한 경우 최종적으로 $N_{} &gt; 42`(42회 초과)의 놀라운 성과를 기록하여, 사실상 50회 만점에 근접한 정책을 발견했습니다. 이는 8개 에이전트 구성의 최고 성능(약 35회)보다 상당히 높은 수치로, 탐색 병렬화를 늘리면 아직 발견되지 않은 더 우수한 정책을 찾아낼 가능성이 높아진다는 점을 시사합니다. 개체군을 키울 때의 자원 대비 효율도 고려해야겠지만, 적어도 시뮬레이션 연구 단계에서는 가능하다면 더 많은 병렬 탐색이 유리함을 보여주는 결과입니다.</p>
<p>흥미로운 부가 분석으로, 저자들은 PBT가 학습 과정에서 자동으로 조정한 하이퍼파라미터들의 시간 추이를 Fig.6에 함께 제시하였습니다. 예를 들어, 32개 에이전트 실험에서 PBT는 학습 막바지에 할인율 <span class="math inline">\gamma</span>를 0.99에서 0.995 이상으로 높이는 경향을 보였는데, 이는 장기적인 보상을 더 중시하도록 하는 변화입니다. 이로 인해 개체군 내 일부 에이전트는 새로운 전략을 터득했는데, 바로 일부 재정렬 목표에서는 물체를 잠시 테이블에 내려놓고 방향을 돌린 후 다시 집어서 옮기는 전략입니다. 이런 전략은 당장은 목표 달성에 시간이 더 걸리지만, 물체를 공중에서 계속 쥐고 회전시키는 것보다 떨어뜨릴 위험이 적어 장기적으로 더 많은 연속 성공을 가져오는 안정적인 방법이었습니다. 높은 할인율은 이처럼 장기적 성공 횟수를 최대화하는 방향으로 정책을 유도한 것으로 해석할 수 있습니다. 또한 PBT는 학습 후반에 PPO의 KL 기준 임계값을 점차 낮추어, 정책 갱신이 매우 보수적으로 이뤄지도록 조절했습니다. 이는 성능이 높아진 최종 단계에서는 큰 업데이트로 인한 정책 붕괴를 방지하기 위해 신중한 학습률 스케줄을 채택한 결과로 보입니다. 이러한 자동 튜닝 과정은 사람 연구자가 미리 예상하기 어려운 부분으로, PBT의 메타 최적화 능력이 어떻게 정책 학습을 세밀하게 조율하는지를 보여주는 흥미로운 대목입니다.</p>
</section>
<section id="성능-비교-및-종합-논의" class="level3" data-number="2.3.3">
<h3 data-number="2.3.3" class="anchored" data-anchor-id="성능-비교-및-종합-논의"><span class="header-section-number">2.3.3</span> 성능 비교 및 종합 논의</h3>
<p>종합하면, DexPBT 프레임워크는 모든 실험 시나리오에서 기존 강화학습 대비 우월한 성과를 입증했습니다. 특히 과제 난이도가 높아질수록 PBT의 장점이 두드러졌는데, 가장 어려운 단일 손/양손 재정렬 작업은 오직 PBT를 사 용해야만 안정적으로 학습이 가능했습니다. 이는 PBT가 없다면 광대한 탐색 공간에서 지역 최적해에 빠지거나 초반 시행착오를 극복하지 못하는 반면, 다중 탐색 경로를 병렬로 시도하고 우수 정책을 확산시키는 메커니즘이 있어야 비로소 올바른 해법을 찾을 수 있음을 의미합니다. 또한 동일한 8개 GPU 자원을 사용할 때, PBT는 개체군 전체 경험은 8배 많아지지만 각 에이전트별로는 독립 실행과 똑같이 50억 스텝을 학습합니다. 그럼에도 개체군 내 최고 정책의 성능이 독립 실행 8회 중 최고 성능보다 항상 높게 나타났습니다. 이는 같은 양의 총 환경 상호작용으로 더 나은 정책을 얻어낸 것이므로, 결과적으로 탐색 효율 측면에서도 PBT가 이득임을 시사합니다. 물론 PBT를 쓰면 병렬로 여러 정책을 학습하므로 벽시계 시간 기준으로는 비슷한 시간에 더 많은 경험을 쌓는 셈이지만, 이는 대규모 병렬 컴퓨팅이 가능하다는 현대 강화학습의 장점을 십분 활용한 것입니다. 특히 물체 조작처럼 탐색 난이도가 큰 문제에서는 계산량을 늘리는 것이 곧 성능 향상으로 직결됨을 보여주었다는 점에서, DexPBT의 결과는 학계와 산업계에 중요한 통찰을 제공합니다.</p>
<p>마지막으로, 저자들은 이 연구의 의의와 한계에 대해서도 언급하고 있습니다. 비록 본 논문에서 개발한 정책들이 시뮬레이션에서 매우 높은 연속 성공률을 보였지만, 실제 로봇에 바로 적용하기 위해서는 추가 과제가 남아있습니다. 예를들어, 학습된 정책들은 종종 로봇의 물리적 한계까지 공격적인 동작을 구사하는데, 이는 현실 로봇에 그대로 적용하면 장비 손상이나 예기치 못한 실패를 초래할 수 있습니다. 이를 완화하기 위해 Riemannian Motion Policy나 Geometric Fabric과 같은 기법으로 로봇의 움직임에 보수적인 모션 프라이어를 걸어주는 방안 등이 제시되었습니다. 또한 가장 큰 과제는 시뮬레이션-현실 격차(sim-to-real gap)를 극복하는 것입니다. 현실 세계의 마찰, 물리 변수의 불확실성 등을 견디도록 정책을 일반화하려면 도메인 랜덤화(domain randomization)나 적응형 로봇 학습 기법이 필요하지만, 이러한 기법들은 학습 난이도를 더욱 높입니다. Dactyl과 같은 사례에서도 도메인 랜덤화를 적용하여 성공하기까지 막대한 비용이 들었는데, 저자들은 자신들의 병렬 시뮬레이션 + PBT 접근법이 그러한 대규모 학습을 수행하는 데 필요한 샘플 수요를 크게 줄여줄 가능성을 보여주었다고 강조합니다. 예컨대 두 손-팔 시스템처럼 복잡한 설정에서도 본 연구의 방법론으로 비교적 짧은 시간 내에 정책을 얻어냈으므로, 향후 현실 실험을 대비한 엄청난 양의 학습이 요구되는 경우에도 보다 실용적인 해법이 될 수 있다는 전망입니다. 결국 DexPBT 연구는 높은 난도의 로봇 조작을 학습을 통해 자동으로 획득하는 가능성을 제시하였고, 복잡한 시스템에 대한 인구 기반 탐색의 유용성을 입증한 의미 있는 사례로 평가됩니다. 이는 향후 인간 수준의 섬세한 조작 능력을 지닌 로봇을 현실화하는 여정에서 한 단계 진전을 이룬 결과로 볼 수 있습니다. 대규모 병렬 강화학습과 PBT의 조합이라는 본 논문의 핵심 아이디어는, 앞으로 다양한 로봇 과제를 풀어나가는 데에도 폭넓게 응용될 수 있을 것으로 기대됩니다.</p>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<script src="https://utteranc.es/client.js" repo="curieuxjy/blog_comments" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Copyright 2024, Jung Yeon Lee</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/curieuxjy">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>