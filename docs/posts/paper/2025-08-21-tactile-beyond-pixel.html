<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-08-19">
<meta name="description" content="Tactile Beyond Pixels - Multisensory Touch Representations for Robot Manipulation">

<title>📃Sparsh-X 리뷰 – Curieux.JY</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../profile.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-2NVZN2MJZT"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-2NVZN2MJZT', { 'anonymize_ip': true});
</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Curieux.JY</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../post.html"> 
<span class="menu-text">Post</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../note.html"> 
<span class="menu-text">Note</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">Jung Yeon Lee</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/curieuxjy"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#brief-review" id="toc-brief-review" class="nav-link active" data-scroll-target="#brief-review">Brief Review</a></li>
  <li><a href="#detail-review" id="toc-detail-review" class="nav-link" data-scroll-target="#detail-review">Detail Review</a>
  <ul class="collapse">
  <li><a href="#abstract" id="toc-abstract" class="nav-link" data-scroll-target="#abstract">Abstract</a></li>
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#related-work" id="toc-related-work" class="nav-link" data-scroll-target="#related-work">Related Work</a></li>
  <li><a href="#proposed-method---멀티모달-촉각-표현-학습" id="toc-proposed-method---멀티모달-촉각-표현-학습" class="nav-link" data-scroll-target="#proposed-method---멀티모달-촉각-표현-학습">Proposed Method - 멀티모달 촉각 표현 학습</a>
  <ul class="collapse">
  <li><a href="#sparsh-x-모델-개요" id="toc-sparsh-x-모델-개요" class="nav-link" data-scroll-target="#sparsh-x-모델-개요">Sparsh-X 모델 개요</a></li>
  <li><a href="#자기-지도-사전학습-ssl-training-pipeline" id="toc-자기-지도-사전학습-ssl-training-pipeline" class="nav-link" data-scroll-target="#자기-지도-사전학습-ssl-training-pipeline">자기 지도 사전학습 (SSL Training Pipeline)</a></li>
  </ul></li>
  <li><a href="#실험-다운스트림-과제에서의-멀티센서-촉각-활용" id="toc-실험-다운스트림-과제에서의-멀티센서-촉각-활용" class="nav-link" data-scroll-target="#실험-다운스트림-과제에서의-멀티센서-촉각-활용">실험: 다운스트림 과제에서의 멀티센서 촉각 활용</a>
  <ul class="collapse">
  <li><a href="#물리적-속성-추론-inferring-physical-properties-with-sparsh-x" id="toc-물리적-속성-추론-inferring-physical-properties-with-sparsh-x" class="nav-link" data-scroll-target="#물리적-속성-추론-inferring-physical-properties-with-sparsh-x">물리적 속성 추론 (Inferring Physical Properties with Sparsh-X)</a></li>
  <li><a href="#정책-학습에서의-sparsh-x-활용-sparsh-x-for-policy-learning" id="toc-정책-학습에서의-sparsh-x-활용-sparsh-x-for-policy-learning" class="nav-link" data-scroll-target="#정책-학습에서의-sparsh-x-활용-sparsh-x-for-policy-learning">정책 학습에서의 Sparsh-X 활용 (Sparsh-X for Policy Learning)</a></li>
  </ul></li>
  <li><a href="#discussion" id="toc-discussion" class="nav-link" data-scroll-target="#discussion">Discussion</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  <li><a href="#limitations-and-future-work" id="toc-limitations-and-future-work" class="nav-link" data-scroll-target="#limitations-and-future-work">Limitations and Future Work</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-page-left" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">📃Sparsh-X 리뷰</h1>
  <div class="quarto-categories">
    <div class="quarto-category">tactile</div>
    <div class="quarto-category">digit360</div>
    <div class="quarto-category">multlimodal</div>
    <div class="quarto-category">sparsh-x</div>
  </div>
  </div>

<div>
  <div class="description">
    Tactile Beyond Pixels - Multisensory Touch Representations for Robot Manipulation
  </div>
</div>


<div class="quarto-title-meta column-page-left">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">August 19, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<ul>
<li><a href="https://arxiv.org/abs/2506.14754">Paper Link</a></li>
<li><a href="https://github.com/facebookresearch/sparsh-multisensory-touch">Code Link</a></li>
</ul>
<ol type="1">
<li>이 논문은 로봇 조작을 위한 최초의 멀티모달 촉각 표현 학습 모델인 Sparsh-X를 제시하며, 이는 Digit 360 센서에서 얻은 이미지, 오디오, 모션, 압력 등 네 가지 촉각 양상을 자체 지도 학습을 통해 하나의 통합된 표현으로 융합합니다.</li>
<li>약 100만 건의 접촉 상호작용 데이터로 학습된 Sparsh-X는 물체의 물리적 특성 및 접촉 역학을 효과적으로 포착하며, 다양한 작업에서 이러한 정보를 활용할 수 있음을 입증합니다.</li>
<li>실험 결과, Sparsh-X는 모방 학습 및 Sim-to-Real 촉각 적응을 통해 로봇 조작 성공률을 63% 향상시키고, 물리적 특성 추정 정확도를 기존 방식 대비 48% 개선하여 견고한 미세 조작 성능을 제공합니다.</li>
</ol>
<hr>
<section id="brief-review" class="level1">
<h1>Brief Review</h1>
<p>이 논문은 로봇 조작을 위한 일반 목적의 다중 감각 촉각 표현(multisensory touch representations) 모델인 Sparsh-X를 제안합니다. Sparsh-X는 Digit 360 센서에서 수집된 이미지, 오디오, 모션(IMU), 압력의 네 가지 촉각 모달리티를 활용하여 약 100만 건의 접촉 기반 상호작용 데이터로 자가 지도 학습(self-supervised learning)되었습니다. 이 모델은 다양한 시간적, 공간적 스케일의 상호 보완적인 촉각 신호들을 단일화된 표현으로 융합하여 물리적 특성을 포착합니다.</p>
<p>Sparsh-X의 핵심 방법론은 transformer 기반의 백본 아키텍처에 있습니다.</p>
<center>
<img src="../../images/2025-08-21-tactile-beyond-pixel/Screenshot 2025-08-21 at 11.27.27 AM.png" width="100%">
</center>
<p>입력 신호는 먼저 <span class="math inline">L_f</span>개의 레이어(<span class="math inline">L_f = 8</span>)에서 각 모달리티별로 독립적인 self-attention을 통해 처리됩니다. 이후 <span class="math inline">L_b</span>개의 블록(<span class="math inline">L_b = 4</span>)에서는 attention bottlenecks [35]를 사용하여 cross-modal 정보 흐름이 이루어집니다. 이를 위해 <span class="math inline">B</span>개의 bottleneck fusion tokens(<span class="math inline">B = 4</span>)이 각 모달리티의 embedding에 연결되며, 각 cross-modal 업데이트 후 fusion tokens는 모달리티 전반에 걸쳐 평균화되어 정보 공유를 촉진합니다. 총 transformer 레이어 수는 <span class="math inline">L = L_f + L_b = 12</span>로 설정되었습니다.</p>
<p>입력 모달리티는 다음과 같이 전처리되고 토큰화됩니다:</p>
<ul>
<li><strong>Image</strong>: 30fps로 샘플링된 촉각 이미지를 5의 temporal stride로 채널 차원에 따라 연결합니다. hyper-fisheye 이미지는 224x224x3 크기로 자르고 리사이즈되며, 16x16 크기의 패치(patch)로 분할된 후 linear projection layer를 통해 768차원의 embedding으로 토큰화됩니다.</li>
<li><strong>Audio</strong>: 48kHz로 샘플링된 두 개의 접촉 마이크에서 얻은 0.55초의 오디오 신호는 5ms Hamming window와 2.5ms의 hop length로 128채널의 log-mel spectogram으로 변환됩니다. 두 마이크의 spectogram이 연결되어 224x256 오디오 입력이 되며, 16 크기의 패치로 토큰화됩니다.</li>
<li><strong>IMU (Accelerometer)</strong>: 400Hz로 샘플링된 3축 가속도계 데이터는 0.55초 창으로 통합되어 224x3 temporal signal로 토큰화됩니다.</li>
<li><strong>Pressure</strong>: 200Hz로 샘플링된 압력 신호는 1.1초 창으로 통합되어 224x1 temporal signal로 토큰화됩니다.</li>
</ul>
<p>Sparsh-X는 자가 지도 학습을 위해 <code>teacher-student self-distillation</code> 접근 방식 [40, 11]을 사용합니다. 인코더와 예측 헤드로 구성된 두 브랜치에서, 학생 입력 토큰에 마스킹을 적용하고 (로컬 마스크의 경우 10-50%, 전역 마스크의 경우 50-100% 신호 유지), 교사 토큰을 pseudo-label로 사용하여 클러스터링 예측 작업을 수행합니다. 최적화 목표는 교사와 학생 네트워크의 softmax 출력 간 cross-entropy입니다.</p>
<p>평가 실험은 세 가지 주요 영역에서 진행되었습니다:</p>
<ol type="1">
<li><strong>물리적 특성 추론 (Inferring physical properties)</strong>: 객체-행동-표면 분류(object-action-surface classification), 재료-양 추정(material-quantity estimation), 법선력 추정(normal force estimation)과 같은 지도 학습(supervised learning) 작업을 통해 Sparsh-X 표현의 품질을 평가했습니다. Sparsh-X의 인코더 가중치는 고정된 상태로 task-specific attentive decoder를 훈련시켜 순수하게 표현의 품질을 측정했습니다. 그 결과, 다중 모달리티를 함께 사용했을 때 촉각 이미지 단독 사용 대비 분류 정확도가 현저히 향상되었으며, end-to-end 모델보다 데이터 효율성과 일반화 성능이 우수함을 보였습니다.</li>
<li><strong>정책 학습을 위한 Sparsh-X 통합 (Sparsh-X for Policy Learning)</strong>:
<ul>
<li><strong>모방 학습(Imitation Learning)을 통한 플러그 삽입</strong>: 로봇이 Allegro hand와 Digit 360 센서를 사용하여 플러그를 소켓에 삽입하는 작업에서, Sparsh-X를 활용한 다중 감각 촉각 피드백이 정책 성공률을 크게 향상시켰습니다. 외부 시각 정보만 사용했을 때보다 500%, 촉각 이미지만 사용한 end-to-end 정책보다 63%의 성공률 향상을 보였습니다.</li>
<li><strong>시뮬레이션에서 실제 세계로의 촉각 적응(Sim-to-Real Tactile Adaptation)을 통한 손안 객체 회전</strong>: Hora [51]와 같은 시뮬레이션 훈련된 기본 정책 위에 ControlNet [52]을 활용하여 촉각 적응 모듈을 학습시켰습니다. 이는 객체의 물리적 특성(질량, 마찰 등) 변화에 대한 정책의 견고성을 높여 수직 표류(vertical drift)를 90% 감소시키고, 객체 슬립을 줄여 회전 안정성을 향상시켰습니다.</li>
</ul></li>
</ol>
<p>Sparsh-X는 다중 감각 촉각 정보를 통합함으로써 로봇 조작의 정밀성과 견고성을 크게 향상시킬 수 있음을 입증하며, 촉각 센싱 분야의 foundation models 개발에 중요한 발걸음을 내디뎠습니다.</p>
<center>
<img src="../../images/2025-08-21-tactile-beyond-pixel/Screenshot 2025-08-21 at 11.27.55 AM.png" width="100%">
</center>
<hr>
</section>
<section id="detail-review" class="level1">
<h1>Detail Review</h1>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<p>이 논문에서는 Sparsh-X (또는 TacX로도 언급됨)라는 다중감각 촉각 표현 학습 모델을 제안한다. Sparsh-X는 하나의 로봇 촉각 센서로부터 얻은 네 가지 상이한 촉각 신호 – 영상(Image), 소리(Audio), 운동(IMU 데이터), 압력(Pressure) – 를 자기 지도 학습을 통해 하나의 통합된 표현 공간에 융합하는 백본 모델이다. 이 모델은 Meta(Facebook) FAIR 연구팀과 대학 연구자들이 함께 개발하였으며, 새로운 멀티모달 촉각 센서 Digit 360으로 약 100만 회 이상의 다양한 접촉 상호작용 데이터를 수집하고 이를 학습에 활용했다. 저자들은 Sparsh-X가 이러한 방대한 비지도 멀티모달 데이터로부터 접촉에 내재된 물리적 속성을 효과적으로 학습하여, 이후 로봇 조작 과제에 활용할 때 데이터 효율성과 성능을 크게 향상시킬 수 있음을 보였다. 실제로 모방 학습 기반 플러그 삽입 과제에서, Sparsh-X 표현을 사용한 정책은 순수 시각 기반 정책 대비 성공률이 500% 향상되었고, 단일 촉각 이미지 기반(end-to-end) 정책 대비 63% 높아졌다. 또한 촉각 정보를 활용한 미끄럼 감소를 통해 시뮬레이션에서 학습된 정책을 현실로 적응시킬 때 물체의 안정적 파지 유지 능력이 90% 개선되는 등, 로봇 조작의 견고성 및 적응성 면에서 현저한 개선을 확인하였다. 나아가 Sparsh-X 표현이 물체-행동-표면 식별, 재질 및 양 추정, 힘 예측 등의 물리적 속성 추론 과제에서도 기존 단일모달 접근보다 평균 48% 높은 정확도를 보임으로써, 멀티센서 사전학습의 이점을 입증하였다. 본 리뷰에서는 논문의 핵심 내용을 각 섹션별로 정리하고, 저자들의 문제의식, 제안 기법의 창의성 및 기존 연구와의 차별성, 실험 설계의 타당성 및 결과 분석, 그리고 한계점과 향후 과제 등에 대해 심층적으로 해설한다.</p>
</section>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p><strong>배경 및 문제의식</strong></p>
<p>촉각은 인간의 섬세한 조작 능력에 필수적인 풍부한 감각이다. 사람은 피부 변형, 진동, 운동, 압력 등 다양한 촉각 신호를 통합함으로써 물체의 물리적 특성을 파악하고 적응적으로 조작한다. 예를 들어 손끝 감각만으로도 플라스틱 컵과 종이컵을 구분하거나, 손가락 사이에 펜을 굴리거나, 시야가 가려진 상태에서도 도구를 다루는 것이 가능하다. 멀티모달 촉각 피드백을 적극 활용하기 때문에 이러한 섬세한 조작이 가능한 것이다. 그러나 로봇 공학 분야에서는 아직까지 멀티센서 촉각의 활용이 크게 뒤처져 있다. 대부분의 기존 연구는 하나의 촉각 모달리티(주로 촉각 이미지)에 의존하는데, 이는 그간 사용 가능한 표준화된 하드웨어(예: GelSight 계열 광학 촉각 센서)에 한정되어 왔기 때문이다. 멀티모달 촉각 센서가 이제 등장하고 있지만, 이질적인 촉각 신호를 통합하여 활용할 수 있는 통합적이고 확장 가능한 방법론이 부족한 실정이었다. 이러한 격차를 해결하기 위해 저자들은 표현 학습(Representation Learning)을 활용하는 접근을 제안한다. 여러 센서 모드로부터 얻은 상호보완적인 접촉 정보를 하나의 공유 잠재 공간에 융합함으로써, 다운스트림 조작 과제에 데이터 효율적이고 견고한 학습을 가능케 하려는 것이다. 특히, 자기 지도학습(self-supervised learning)을 통해 거대한 비라벨 데이터에서 촉각 표현을 사전학습하면, 이후 특정 과제를 학습할 때 소규모의 데이터로도 우수한 성능을 얻거나 잡음 및 분산에 강인한 모델을 만들 수 있음이 시각 기반 촉각 연구들에서 시사되어 왔다.</p>
<p><strong>제안 기법</strong></p>
<p>이러한 동기를 바탕으로, 본 논문은 Sparsh-X라 명명된 첫 번째 범용 멀티센서 촉각 백본 모델을 소개한다. Sparsh-X는 Meta AI가 개발한 Digit 360이라는 신형 촉각 센서를 통해 얻은 네 가지 촉각 정보(이미지, 소리, 운동, 압력)를 Transformer 기반 신경망으로 융합하여 인코딩한다. 데이터는 슬라이딩, 탭(tapping), 회전, 물체 집기 및 놓기 등 다양한 조작 동작 상황에서 수집된 비라벨 접촉 데이터 약 100만 건이며, Sparsh-X는 이 다양한 접촉 경험으로부터 일반적인 촉각 표현을 자기 지도학습으로 학습한다. 저자들은 학습된 Sparsh-X 표현이 물체의 종류나 질량과 같은 객체 특성, 접촉력이나 재질과 같은 정적 접촉 속성, 미끄러짐이나 충격 같은 동적 상호작용 단서 등 광범위한 물리적 속성들을 손끝 수준에서 인코딩하고 있음을 확인하였다. 이러한 손끝 촉각 표현은 로봇으로 하여금 물체 및 접촉 상태 정보를 직접 잠재공간에서 피드백받게 해주므로, 섬세한 조작(dexterous manipulation)에 특히 유용하다고 강조한다.</p>
<p><strong>정책 학습과의 연계</strong></p>
<p>궁극적으로 촉각 신호의 가치는 로봇 학습에 얼마나 효과적으로 쓰이느냐에 달려있다고 저자들은 말한다. 그러나 현실에서는 촉각 정보를 강화학습 등에 활용하기 어려운 점이 많다. 특히 시뮬레이션-현실 간 격차(sim-to-real gap)로 인해, 촉각 센서가 없는 시뮬레이션 환경에서 학습한 정책을 현실 로봇에 이식하기가 까다롭다. 논문에서는 Sparsh-X 표현이 이러한 문제를 완화할 수 있음을 두 가지 시나리오로 실증하였다: 첫째, 모방 학습(imitation learning)을 통해 인간 데모의 촉각 데이터를 정책 학습에 사용하는 경우, 둘째, 시뮬레이션에서 privileged information(접촉력 등 현실에서 직접 얻기 힘든 정보)으로 학습된 정책에 현실 촉각 피드백을 추가하여 적응(sim-to-real tactile adaptation)하는 경우이다. 각각의 경우에 대해, 플러그 삽입(insertion) 과 손바닥 위 물체 회전(in-hand rotation)이라는 정밀 조작 과제를 실험하여 Sparsh-X 활용의 효과를 평가하였다. 결과적 으로, 다중 촉각을 통합한 Sparsh-X 표현을 활용하면 기존의 단일 촉각 이미지 기반 접근보다 현저히 향상된 성능을 얻을 수 있었고, 멀티센서 촉각을 공유 잠재 공간에 통합함으로써 터치의 분야에서도 파운데이션 모델로 향하는 한 걸음을 내딛었다고 저자들은 강조한다. 아래는 저자들이 밝힌 본 연구의 핵심 기여사항이다:</p>
<ul>
<li>첫째, Sparsh-X라는 통합 멀티센서 촉각 백본을 제시하였다. 이 모델은 이미지, 오디오, 관성(IMU), 압력 신호를 하나의 범용 표현 공간에 융합하며, 자기 지도학습으로 대규모(Digit 360으로부터 수집된 M개 이상의) 비라벨 촉각 데이터에서 학습된다. 이를 통해 확장 가능하고 이식 가능한(t transferable) 로봇 촉각 지각 능력을 구현하였다.</li>
<li>둘째, Digit 360 센서로부터 수집된 세계 최초의 멀티모달 촉각 데이터셋을 구축하고 이를 공개하였다. 이 데이터셋은 멀티모달 촉각 표현 연구를 위한 벤치마크로 활용될 수 있으며, 접촉 역학(contact dynamics)이나 물체의 물리적 속성 관점에서 해석 가능한 분석을 수행할 수 있도록 설계되었다. (Sparsh-X의 학습과 모든 평가 실험에 이 데이터셋이 활용되었다.)</li>
<li>셋째, Sparsh-X를 사용함으로써 로봇 정책 학습의 성능과 견고성이 크게 향상됨을 현실 세계 실험으로 실증하였다. 특히 정밀 조작 과제인 플러그 삽입과 손 안에서의 물체 회전 작업에서, Sparsh-X의 멀티센서 촉각 표현을 통합하면 단일 촉각 이미지 기반 모델 대비 성공률이 63% 향상되고, 시뮬레이션 학습 정책의 현실 적응 시 물체 상태 복원 능력이 90% 개선됨을 보여주었다. 이는 촉각 적응 기법을 통해 시뮬레이션에서의 privileged 정보가 현실의 촉각 피드백으로 효과적으로 대체될 수 있음을 나타낸다.</li>
</ul>
</section>
<section id="related-work" class="level2">
<h2 class="anchored" data-anchor-id="related-work">Related Work</h2>
<p><strong>시각 기반 촉각 센서</strong> 기존의 로봇 촉각 연구는 시각 정보를 이용한 촉각 센서(vision-based tactile sensor)에 크게 의존해왔다. GelSight와 유사한 촉각 카메라 센서들이 대표적인 예로, 이들은 손가락 내부에 장착된 카메라로 탄성체 표면의 변형 이미지를 촬영하여 접촉을 감지한다. 이러한 시각 촉각 센서는 재질 분류나 내용물 부피 예측 , 물체 형상 추론 , 접촉 지점 위치 추정 , 삽입 동작 , 윤곽 추적 등 다양한 접촉 과제에 활용되어 왔다. 한편, 오디오(진동)를 촉각에 활용한 연구도 존재하는데, 접촉시 발생하는 소리를 통해 물체의 속성 이나 동적 상호작용을 추정하는 시도들이 있었다. 그러나 오디오만으로는 연속적인 힘이나 변형, 운동량 등을 인지하기 어렵기 때문에, 최근에는 오디오-비전 멀티모달 학습으로 확장하는 연구도 나타났다. 시각과 소리는 서로 보완적(tactile state를 보강)인 관계이지만, 손가락 자체의 움직임(IMU)이나 압력 분포와 같은 추가 촉각 정보까지 활용한다면, 전단 력(shear force) 감지나 물체 특성 인식, 접촉면에서의 미끄럼 및 자세 변화 감지 등에 더욱 풍부한 정보를 줄 수 있다고 논문은 지적한다.</p>
<p><strong>촉각 표현 학습</strong></p>
<p>자기 지도학습을 통한 촉각 표현 학습(SSL for tactile)은 최근 부상한 분야로, 주로 촉각 이미지 기반 표현 학습에 집중되어 왔다. 선행 연구들은 대량의 unlabeled 촉각 이미지 데이터에서 특징 표현(feature embedding)을 학습시켜, 다운스트림 과제의 성능 향상과 데이터 효율성 증대를 입증하였다. 한편, 추가적인 촉각 모달(ex: 오디오)을 함께 활용하려는 시도에서는, 대개 사전학습된 인코더를 특정 과제 데이터로 파인튜닝(finetuning)하거나 , 오디오-비디오 합성 모델을 학습시키는 정도였다. 이러한 접근은 각 모달별 개별적인 처리에 머물렀고, 근본적인 멀티모달 통합 표현을 학습하지는 못했다. 예를 들어, MULSA라는 연구는 촉각 비전 이미지와 접촉 마이크 오디오를 RGB 이미지로 간주하여 입력을 단순 병합한 멀티모달 트랜스포머를 제안했으나, 모달별 토큰을 전부연결(concatenate)하여 쿼드러틱(Quadratic) 복잡도의 Self-Attention을 수행하는 비효율로 한계를 보였다. 또한 MimicTouch라는 연구는 촉각 이미지와 오디오 각각에 개별적인 자기지도학습을 적용했을 뿐 명시적인 교차 모 달 융합은 없었다고 한다. 이와 달리, Sparsh-X는 이미지, 오디오, 운동, 압력의 네 가지 촉각 모달을 한꺼번에 통합하는 새로운 멀티센서 프레임워크이다. 특히 트랜스포머 내에 bottleneck self-attention 메커니즘을 도입하여 모달 간 정보를 주고받도록 설계함으로써, 멀티모달 촉각의 접촉 특성을 효과적으로 포착하면서도 기존의 단순 토큰 연결 방식보다 계산 복잡도를 낮추는 장점을 갖는다. 이렇게 학습된 공용 잠재 표현은 다운스트림 접촉 과제에서 유용한 물리적 특징들을 함축하고 있어, 다양한 작업에 쉽게 적용될 수 있다.</p>
<p><strong>Digit 360 센서</strong></p>
<p>저자들은 본 연구에 사용된 Digit 360 촉각 센서의 특징도 소개한다. Digit 360은 손가락 크기의 반구형 부드러운 엘라스토머 돔 내부에 초광각(fisheye) 카메라, 접촉 마이크, IMU(관성측정장치), 정적 압력 센서 등을 통합한 다중감각 촉각 센서이다. 이처럼 작은 폼팩터의 멀티모달 촉각 센서가 등장함으로써, 로봇 손가락마다 촉각 이미지를 비롯한 다양한 접촉 신호를 획득하는 것이 가능해졌다. 그러나 독특한 반구형 형태 때문에 새로운 과제도 발생 한다. 예를 들어, 돔 형태의 말랑한 센서 표면이 접촉 시 뒤틀리고 변위되므로, 전단력 추정이 매우 어려워지는 문제가 있다. 또한 하이퍼-피쉬아이 렌즈와 LED 조명이 내장되어 있지만, 기존의 광학적인 3D 표면 복원 기법(예: 포아송 적분법)을 바로 적용하기 어렵다. 이러한 한계에도 불구하고, 저자들은 대규모 자기 지도 사전학습을 통해 해당 센서의 복잡한 접촉 양상을 극복하는 방향이 유망하다고 보고 있다. 다시 말해, 센서 하드웨어의 제약으로 인한 데이 터 왜곡이나 불확실성조차도 방대한 경험 데이터를 학습함으로써 모델이 내재적으로 보정할 수 있으리라는 것이다.</p>
</section>
<section id="proposed-method---멀티모달-촉각-표현-학습" class="level2">
<h2 class="anchored" data-anchor-id="proposed-method---멀티모달-촉각-표현-학습">Proposed Method - 멀티모달 촉각 표현 학습</h2>
<section id="sparsh-x-모델-개요" class="level3">
<h3 class="anchored" data-anchor-id="sparsh-x-모델-개요">Sparsh-X 모델 개요</h3>
<p><strong>입력과 아키텍처</strong></p>
<p>Sparsh-X는 Transformer 기반의 표현 학습 백본(backbone)으로서, Digit 360 센서로부터 동시 수집된 네 가지 촉각 신호(이미지, 오디오, 가속도, 압력)를 입력으로 받는다. 모델의 구조적 핵심은 “모달리티별 처리 + bottleneck 토큰을 통한 융합”이라고 요약할 수 있다. 우선 각 모달리티 입력은 개별적인 Transformer 인코더 층을 통과하며, 이 층들은 Self-Attention 기제로 해당 모달의 패치/시계열 토큰들 사이의 패턴 을 학습한다. 그런 다음 일정 깊이의 층을 지나면, 교차-모달 정보 교환 단계가 등장하는데, 여기서 “bottleneck token”이라 불리는 특별한 토큰들을 매개로 모달 간 Attention이 이루어진다. 구체적으로, 각 모달리티 인코더에 공유된 bottleneck 토큰들을 입력으로 추가하여, 이 토큰들이 일종의 요약자(summarizer) 역할을 하며 다른 모달로부터 정보를 추출하도록 한다. 한 번 교차-어텐션이 수행된 후에는, 각 모달리티별로 추가 처리를 하기 전에 각 모달 에 삽입된 bottleneck 토큰들의 값을 평균내어 공유함으로써, 모달들 사이에 정보가 원활히 교환되고 결합되게 설계했다. 이러한 과정이 여러 Transformer 블록에 걸쳐 반복되며, 최종적으로 모든 모달의 정보가 하나의 응집된 잠재표현으로 압축된다. 요컨대, bottleneck 토큰들은 네 가지 촉각 모달리티 사이를 연결해주는 소통 채널로 기능하며, 각 블록마다 다중감각 요약정보를 뽑아내 공유함으로써 효율적인 멀티모달 융합을 가능케 한다. (이는 기존의 토큰 단순 병 합 대비 현저히 낮은 연산 복잡도로 모달 결합을 실현한 아이디어라는 점에서 창의적이다.) Sparsh-X의 전체 Transformer 레이어는 총 L층으로 구성되며, 그 중 처음 U개 층은 모달별 자체 처리만 수행하고, 나머지 L–U개 층은 방금 설명한 bottleneck 기반 교차-모달 융합을 포함한다. 이러한 설계는 저자들이 참고한 선행 연구 를 바탕으로 실험적으로 최적화되었다고 한다.</p>
<center>
<img src="../../images/2025-08-21-tactile-beyond-pixel/Screenshot 2025-08-21 at 11.27.27 AM.png" width="100%">
</center>
<blockquote class="blockquote">
<p>Sparsh-X 멀티센서 촉각 백본 모델의 개략도. 각 입력 모달리티(이미지, 오디오, IMU, 압력)는 우선 별도의 Transformer 인코더 층들을 통과하며, 이후 블록에서는 주황색의 Bottleneck 토큰들을 매개로 서로의 정보를 교환(fuse)한다. 이렇게 얻어진 통합 표현은 상위에 원하는 다운스트림 작업용 디코더/정책망에 연결될 수 있다.*</p>
</blockquote>
<p><strong>입력 데이터 전처리 및 토큰화</strong></p>
<p>네 가지 촉각 모달리티는 수집 주기와 데이터 형태가 서로 상이하기 때문에, 각각 Sparsh-X에 입력되기 전에 적절한 전처리와 토큰화 과정을 거친다. 아래는 각 모달리티별 처리 요약이다:</p>
<ul>
<li>촉각 이미지: Digit 360 센서는 약 30Hz로 촉각 영상을 획득한다. 연속 프레임 간의 변화를 반영하기 위해, 과거 0.17초 구간(5프레임)을 채널 방향으로 연결(concatenate)하여 하나의 입력으로 사용한다. (어떤 연구에서는 짧은 프레임 묶음을 이용해 미세한 시간 변화까지 인식하는 방법을 취한다.) 이렇게 결합된 이미지는 둥근 어안 영상이라 일부 가장자리를 크롭(crop)하여 중심부로 확대하고, 최종적으로 정해진 해상도로 리사이 즈한다. 이후 일반적인 ViT(Vision Transformer)처럼 이미지를 작은 패치들로 분할하고, 각 패치를 선형투영하여 768차원 임베딩 토큰 시퀀스로 변환한다. (예컨대 96×96 크기의 이미지를 16×16 패치로 분할하면 36개 패치가 나오고, 각 패치를 768차원 벡터로 임베딩하는 식이다.)</li>
<li>접촉 오디오: Digit 360에는 두 개의 접촉 마이크가 내장되어 있어, 48kHz의 고속으로 접촉 진동음을 수집한다. 논문에서는 약 0.55초 길이의 오디오 신호 구간을 사용하였다. 이 구간의 신호를 5ms 윈도우, 2.5ms 홉으로 로그-멜 스펙트로그램으로 변환하며, 멜 주파수 채널은 128개를 사용했다. 두 마이크의 스펙트로그램을 상하로 붙여(concatenate) “2 × 128 채널”로 만들고, 이를 다시 패치로 나누어(예: 16×16 패치 등) 임베딩 시퀀스로 변환한다. 결과적으로 오디오로부터도 일정 길이의 토큰 시퀀스가 생성된다.</li>
<li>운동/가속도(IMU): Digit 360의 3축 가속도계 신호는 400Hz로 샘플링된다. 약 0.55초 분량의 IMU 데이터를 모아서 하나의 입력 시퀀스로 사용하며, 이 신호 역시 적절한 시계열 패치로 분할 후 임베딩 벡터로 투영한다. (IMU 데이터는 3개의 축 성분이 있어 3채널 시계열로 볼 수 있다. 논문에 따르면 특정 window 길이로 묶어 하나의 토큰으로 만드는 식으로 처리하였다.)</li>
<li>정적 압력: 압력 센서는 200Hz로 읽히며, 다른 신호보다 느리므로 1.1초 길이의 데이터를 한 번에 사용한다. 이 역시 IMU와 동일하게 시계열 신호를 임베딩 토큰 시퀀스로 만든다. (압력 데이터는 1축의 시간흐름 값으로 볼 수 있다.)</li>
</ul>
<p>요약하면, 이미지는 공간 패치들의 시퀀스, 오디오/IMU/압력은 시간 패치들의 시퀀스로 변환되어 Sparsh-X의 입력으로 주어진다. 여기에 포지셔널 임베딩을 더해 위치 정보를 인코딩하고, 앞서 설명한 대로 모달별 Transformer 인코더와 bottleneck 융합 모듈을 통해 최종 통합 표현을 산출한다.</p>
</section>
<section id="자기-지도-사전학습-ssl-training-pipeline" class="level3">
<h3 class="anchored" data-anchor-id="자기-지도-사전학습-ssl-training-pipeline">자기 지도 사전학습 (SSL Training Pipeline)</h3>
<p>Sparsh-X 모델은 라벨 없는 접촉 데이터로 자기 지도 사전학습(self-supervised pretraining)된다. 자기 지도학습을 택함으로써 범용적인 표현 학습, 잡음/분산에 대한 강인함, 대량의 데이터에서 학습 가능 등의 이점을 얻을 수 있다. 저자들은 최근 성공적인 자기 지도학습 기법들을 참고하여, 교사-학생 네트워크 간 지식 증류(self-distillation) 방식을 활용했다. 이는 DINO 등의 비디오 표현 학습 기법과 유사한 아이디어로, 동일한 모델 구조를 가진 Teacher 네트워 크와 Student 네트워크를 두고, Student가 부분 정보만 가지고 Teacher의 출력을 맞추도록 학습하는 방법이다. 구체적인 절차는 다음과 같다:</p>
<ol type="1">
<li><p>데이터셋 구성: Sparsh-X의 학습에 사용된 자기 지도 학습 데이터셋은 두 가지 경로로 수집되었다. 첫째는 로봇 손(Allegro Hand)에 Digit 360 센서들을 장착하고 무작위로 물체를 만지작거리며(random motions with objects) 접촉을 발생시키는 방식이다. 이때 로봇 손가락이 각종 물체 더미 속을 휘젓거나 누르고 비트는 등 여러 접촉 경험을 쌓는다. 둘째는 수동 조작기(manual picker)라는 장치를 사용한 것으로, Digit 360을 집 게 끝에 달아 물체를 집고(sliding), 끌고 슬라이딩하거나, 탁탁 두드리거나, 들어올렸다 떨어뜨리는 등의 원자적 조작 행동(atomic manipulation actions)을 수행한다. 이때 접촉하는 표면의 거칠기, 경도, 마찰계수, 재질 등을 다양하게 바꾸어가며 데이터를 모았다. 이렇게 수집된 다채로운 촉각 상호작용으로부터 총 M개(약 백만)의 샘플을 얻어 학습에 사용했다. (M의 정확한 값은 논문에서 100만 내외로 언급됨.)</p></li>
<li><p>교사-학생 구조: Teacher 네트워크와 Student 네트워크는 동일한 Sparsh-X 구조를 갖지만, Teacher는 학습 과정에서 지속적으로 업데이트되는 평균 가중치(EMA) 등을 사용하여 비교적 안정된 출력을 만든다 (또는 과거 epoch의 student를 teacher로 사용하는 방식). Student 네트워크는 실제로 학습되는 네트워크로, Teacher의 출력을 예측하도록 훈련된다. 둘 모두 Transformer 인코더와 예측 헤드(projection head)를 갖추 고 있다.</p></li>
<li><p>마스킹 및 예측: 각 학습 iteration에서 멀티모달 입력에 대해, 학생(Student) 입력 토큰에는 마스킹을 적용한다. 한 샘플 내에서도 일부 토큰은 local mask (10~50% 남김), 일부는 global mask (50~100% 남김) 처리를 하여, 부분적인 정보만으로 전체를 추론하도록 만든다. (이는 최근 자기지도 비디오 학습에서 자주 쓰이는 국소/전역 마스킹 전략이다.) Teacher 네트워크에는 원본 입력(마스킹 없음 또는 훨씬 적게)으로 통과시켜 보다 풍부한 정보로부터 나온 표현을 얻는다. 그런 다음 Student의 모든 모달 출력 중 “[CLS] 토큰” 격인 대표 토큰들을 한데 모으고 (마스크 종류별로 구분하여) 각각 예측 헤드를 통과시켜 확률 분포 출력을 생성한다. 한편 Teacher 네트워크 출력 토큰들은 클러스터링된 의사 레이블(pseudo-label)로 사용된다. 구체적으로 Teacher의 특정 레이어 출력에 대해 온라인 k-means 등의 방법으로 클러스터 중심(centroid)들을 정하고, 각 토큰을 해당 중심들에 할당하여 soft label (토큰이 속할 확률분포)로 삼는다. 이 Teacher 토큰 기반의 soft label을 Student 네트워크의 예측과 비교하여 크로스 엔트로피 손실을 계산한다. 즉 Student는 제한된 정보를 보고 Teacher가 만든 군집분포를 예측하도록 학습되는 셈이다. 이러한 지식 증류 기반의 자기 지도학습은, 라벨 없이도 모델이 자기 자신에게 배워나가는(self-distill) 과정으로 볼 수 있으며, 이미지는 물론 다중센 서 입력에서도 효과적임을 보여준다. 저자들은 학습 중 클러스터 중심들이 모델 향상에 따라 변화(adapt over time)하도록 하여 점진적으로 표현이 세련되게 하는 기법을 사용했다.</p></li>
<li><p>학습 설정: Sparsh-X는 200 epoch 동안 학습되었다. 사용한 GPU 자원은 A100 40GB × 16대로, 배치 크기는 128, 최적화 알고리즘은 AdamW를 사용했고, 학습률은 초반에 선형 증가(linear ramp-up) 후 코사인 스케줄로 감소시켰다. 대규모 연산이 요구되었지만 Meta의 연구 인프라를 통해 이를 수행할 수 있었다. 추가적인 학습 세부사항(하이퍼파라미터 등)은 부록에 제시되었다.</p></li>
</ol>
<p>이러한 자기 지도 사전학습 결과, Sparsh-X는 대량의 조작 경험으로부터 접촉의 공통 표현을 습득하게 된다. 다음으로 저자들은 이렇게 학습된 멀티모달 촉각 표현이 실제로 유용한지를 다양한 다운스트림 과제를 통해 검증하였다.</p>
</section>
</section>
<section id="실험-다운스트림-과제에서의-멀티센서-촉각-활용" class="level2">
<h2 class="anchored" data-anchor-id="실험-다운스트림-과제에서의-멀티센서-촉각-활용">실험: 다운스트림 과제에서의 멀티센서 촉각 활용</h2>
<p>저자들은 Sparsh-X 표현의 성능과 일반화 능력을 평가하기 위해 다양한 접촉 중심의 다운스트림 과제를 설계하였다. 연구의 중심 질문은 두 가지이다: (1) Sparsh-X가 학습한 표현은 어떤 물리적 속성들을 담고 있는가? (2) 현실 세계의 로봇 정책 학습에 이 촉각 표현을 어떻게 효과적으로 활용할 수 있는가?. 이를 조사하기 위해 지도학습을 통한 물리 속성 추론 실험(4.1절)과, 강화/모방학습을 통한 정책 향상 실험(4.2절)의 두 흐름으로 나누어 실험이 진행되었다.</p>
<section id="물리적-속성-추론-inferring-physical-properties-with-sparsh-x" class="level3">
<h3 class="anchored" data-anchor-id="물리적-속성-추론-inferring-physical-properties-with-sparsh-x">물리적 속성 추론 (Inferring Physical Properties with Sparsh-X)</h3>
<p>이 부분에서는 Sparsh-X의 고정된(frozen) 표현을 사용하여, 물리적인 특성을 얼마나 잘 추론할 수 있는지 평가한다. Sparsh-X 인코더는 학습된 가중치를 그대로 고정시키고, 별도의 가벼운 디코더(MLP 등)를 훈련시켜 특정 과제를 푸는 방식이다. 이렇게 하면 사전학습된 표현 자체의 품질을 온전히 평가할 수 있다 (만약 인코더까지 fine-tuning한다면 사전학습의 공헌도 파악이 어려워지므로). 저자들은 세 가지 유형의 과제를 준비하였다 :</p>
<ol type="1">
<li>객체-행동-표면 분류: 한 번의 접촉 상호작용에서 어떤 물체를 어떤 행동으로 어떤 표면에서 다루었는지를 식별하는 과제이다. 데이터는 앞서 설명한 manual picker 장치로 수집한 하위셋을 활용했다. 구체적으로, 집게로 쥐는 물체는 골프공, 레고 블럭, 나무토막 (3종) 중 하나이고, 수행한 행동은 평면 슬라이딩, 원형 슬라이딩, 두드리기 (3종) 중 하나이며, 접촉한 외부 표면은 플라스틱, 직물(천), 잔디, 거친 합판 (4종) 중 하나였 다. 이 세 가지 범주의 조합으로 총 3×3×4 = 36가지 클래스가 존재한다. 이 과제를 통해 Sparsh-X 표현이 마찰, 경도, 거칠기 등의 정적 접촉 특성과 슬라이딩 vs 탭핑 같은 동적 상호작용 패턴까지 인코딩하고 있는지를 알아볼 수 있다. 저자들은 Sparsh-X 입력 모달리티를 다양하게 변경/제거(ablations)해가며 멀티센서의 이점을 분석했고, 전통적인 end-to-end 학습 모델(촉각 이미지만 입력 받아 처음부터 학습한 모델)과도 성능을 비교하였다.</li>
<li>재질-양(질량) 추정: Sparsh-X 표현이 물체의 재질(material)과 내용물의 양 또는 질량(quantity)까지도 구분할 수 있는지 평가하는 과제이다. 이는 용기 속에 든 물질의 종류와 그 채워진 정도를 추정하는 것으로 생각할 수 있다. 논문에서는 인간의 직관처럼 흔들어서 소리와 감각으로 내용물 파악하는 시나리오를 실험했다. 예를 들어 8온스짜리 병에 쌀, 옥수수알, 비타민 알약, 렌틸콩, 물, 기름 등 여섯 가지 재료 중 하나를 담고, 양을 달리 채워 넣은 후 뚜껑을 닫고 로봇 팔로 잡아 위아래로 흔드는 데이터를 수집하였다. Sparsh-X 센서에는 이 과정에서 내용물의 흔들리는 소리, 병의 미세한 움직임, 압력 변화 등이 모두 입력된다. 총 18가지 (6 재질 × 3 채움 수준) 조합에 대해, Sparsh-X 표현으로부터 재질 종류 + 양 수준을 동시에 분류하도록 디코더를 학습했다. 이 역시 학습 데이터 양을 달리 해보며 표현의 데이터 효율성을 시험했고, 촉각 이미지 만으로 학습한 E2E 모델과 정확도를 비교하였다.</li>
<li>접촉력(정규력) 추정: Sparsh-X 표현이 접촉 시 가해진 힘(magnitude of force)까지 예측할 수 있는지 확인하는 과제이다. 흔히 접촉력 추정은 시각 촉각센서에서 중요한 문제로, Gelsight류 센서는 이미지 변형으로 힘의 크기를 역추정하기도 한다. 여기서는 Digit 360 센서에 반구형 프로브(둥근 눌러지는 도구)를 갖다 대어 일정한 힘으로 누를 때 Sparsh-X 표현을 보고 그 힘(정규 방향 힘)을 회귀 예측하도록 학습했다. 힘의 크기 는 센서 뒤에 장착된 포스/토크(force-torque) 측정 장비로 정확히 계측하면서 데이터를 모았다. 여러 위치, 여러 크기의 힘으로 누르면서 데이터를 만들고, Sparsh-X 표현을 입력으로 회귀 헤드를 붙여 힘 값을 예측하도록 한 것이다. 이 경우도 다양한 입력 조합별로 성능을 비교하였다.</li>
</ol>
<p>평가 및 결과: 이 세 가지 과제에서 Sparsh-X (동결) 표현 + 얕은 디코더 조합의 성능을 측정하여, 1) 어떤 모달리티 조합이 가장 기여하는지, 2) 사전학습된 표현이 없는 경우 대비 얼마나 향상되는지 확인했다. 그 결과 전반적으로 여러 촉각 신호를 함께 쓸 때 성능이 최고로 높았다.</p>
<ul>
<li>객체-행동-표면 분류의 경우, Sparsh-X의 네 모달 통합 표현을 사용할 때 정확도가 가장 높았으며, 이는 촉각이미지 하나만 사용할 때보다 유의미하게 향상된 것이었다. 또한 동일한 양의 학습 데이터로 end-to-end 방식(처음부터 학습)과 비교하면, Sparsh-X 표현을 활용한 쪽이 더 적은 데이터로도 높은 정확도를 달성하여 데이터 효율성 면에서 이점을 보였다. (저자들은 데이터셋 크기를 1천, 4.8천, 48.3천 등으로 늘려가며 비교했는데, Sparsh-X 기반 모델은 소량의 데이터로도 포화에 가까운 성능을 낸 반면 E2E 모델은 많은 데이터가 필요했다.) 구체적인 수치로, 예를 들어 적은 데이터(1.0k 샘플) 학습 시 Sparsh-X 멀티모달 표현은 약 60~70%대 정확도를 보인 반면, 촉각 이미지 단독 E2E 모델은 20%대에 머물렀다. 충분한 데이터(48k 샘플)를 줬을 때는 E2E도 향상되지만, Sparsh-X 멀티모달이 여전히 최고 정확도(약 90% 근접)를 기록했다. 이는 다중 모달리티의 상호 보완적 효과를 보여준다. 한편, 오디오+IMU만 사용한 경우는 정확도가 다소 낮았지만, 이는 이미지/압력 정보 부재로 물체 식별 등에서 한계가 있었기 때문으로 보인다. (오디오는 접촉 여부와 패턴은 알려주지만 재질 등의 힌트는 부족하므로.)</li>
<li>재질-양 분류 과제에서도 멀티모달 Sparsh-X가 탁월한 정확도를 보였다. 이 과제는 18 클래스 멀티라벨 분류로 꽤 어렵지만, Sparsh-X 표현으로 학습한 디코더는 적은 데이터로도 높은 정확도를 달성했다. 특히 오디오+IMU 모달이 이 과제에 중요하게 작용했는데, 병을 흔들 때 나는 소리나 미세 진동으로 내용물 종류와 양을 구분할 수 있기 때문이다. 촉각 이미지 단독으로 학습한 모델은 학습 데이터 100% 사용해도 정확도가 60~70% 정도였지만, Sparsh-X 멀티모달 표현을 쓰면 더 적은 데이터로도 이를 상회했다. (논문 부록의 혼동행렬을 보면, E2E 모델은 예컨대 물과 기름을 잘 혼동하지만 TacX(Sparsh-X) 기반 모델은 정확히 구분했다고 한다.) 이는 사전학습 표현이 재질/질량에 관한 특징도 잘 포착하고 있음을 시사한다.</li>
<li>정규력 추정 실험에서는, 모든 모달 통합시 평균 오차가 약 35 mN으로 가장 낮았고, 이는 촉각 이미지 단독 대비 17% 향상된 정확도라고 보고되었다. 특히 압력 센서나 이미지는 힘의 크기에 관한 단서가 되고, 오디오도 접촉 강도에 따라 달라지는 소리 진폭 등을 제공하므로, 여러 신호를 함께 쓸 때 힘 추정이 정확해졌다. 이 정도 수준의 오차(수십 mN)는 기존 비전 촉각센서로 힘 추정하는 것과 비슷하거나 더 나은 수준으로, 멀티모달 통합이 힘 추정에도 유용함을 보여준다. 참고로 이미지 단일 모달로도 어느 정도 힘 크기를 예측할 수 있었는데, 이는 Elastomer 변형 크기로부터 힘을 가늠할 수 있기 때문이고, Sparsh-X 사전학습을 거친 표현이 그 상관관계를 일부 학습했음을 의미한다.</li>
</ul>
<p>이러한 결과들을 Figure 4에 요약되어 있다. 전체적으로, Sparsh-X의 멀티센서 표현은 접촉을 통해 알 수 있는 다양한 물리량들을 성공적으로 내포하고 있었다. 또한 사전학습의 효과로 적은 레이블 데이터로도 높은 성능을 보였으며, 이는 멀티모달 사전학습 접근이 유망함을 뒷받침한다. 특히 여러 촉각 모달의 시너지가 단일 모달 대비 크며, 각 모달이 제공하는 보완적 정보(예: 오디오는 접촉 이벤트, 이미지는 접촉면 분포, IMU는 운동 변화, 압력은 힘 변화)를 효율적으로 결 합하는 Sparsh-X의 능력이 드러났다.</p>
</section>
<section id="정책-학습에서의-sparsh-x-활용-sparsh-x-for-policy-learning" class="level3">
<h3 class="anchored" data-anchor-id="정책-학습에서의-sparsh-x-활용-sparsh-x-for-policy-learning">정책 학습에서의 Sparsh-X 활용 (Sparsh-X for Policy Learning)</h3>
<p>두 번째로, Sparsh-X 촉각 표현을 로봇 정책 학습에 통합함으로써 얻을 수 있는 이점을 실험했다. 구체적으로 현실 로봇 조작 과제 두 가지에 Sparsh-X를 적용하였다:</p>
<ol type="1">
<li>플러그 삽입 과제 (Plug Insertion via Imitation Learning): 플러그(전원 커넥터)를 소켓에 꽂는 동작은 로봇 조작에서 대표적인 삽입 작업으로 많이 연구되어 왔다. 이 과제에서 저자들은 촉각 멀티모달 표현이 높은 정밀도 작업의 성공률을 얼마나 향상시키는지 평가했다. 실험 환경은 Allegro 로봇 핸드에 Digit 360 센서를 엄지, 집게, 중지에 장착하고, 로봇이 미리 쥐고 있는 플러그를 고정된 소켓에 끼우는 작업이다. 사람의 kinesthetic teleop(힘 반피드백 장치 등으로 원격 조종)를 통해 100개의 데모 시연을 모아서, 그 데이터를 모방학습으로 정책을 훈련했다. 데모에는 손가락 관절상태, 손목 위치, 외부 카메라 영상, 손가락 촉각 데이터 등이 기록되었다.</li>
<li>정책 구조: 학습된 정책은 ACT (Action Chunking with Transformers)라는 기존 모방학습 모델 구조를 변형해 사용했다. 입력으로는 손목 장착 카메라 영상과 Sparsh-X 촉각 표현을 모두 받는다. 카메라 입력은 별도의 CNN 인코더를 거쳐 임베딩되고, 촉각은 Sparsh-X 인코더(사전학습된)를 통과시켜 각 손가락(3개 손가락)의 표현을 얻는다. 그런 다음 세 손가락의 촉각 표현은 어텐티브 풀링(attentive pooling)으로 하나로 통합된다. 이 시각+촉각 융합 임베딩을 Transformer 정책망에 넣어, 미래 Timestep 동안의 endeffector(손목) 움직임 경로를 예측하도록 했다. (즉 한번에 일정 시간구간의 궤적을 출력하는 정책이다.)</li>
<li>비교 기법: 비교를 위해 여러 가지 입력 조합의 정책을 학습시켰다. (a) Sparsh-X 모든 모달 촉각 + 카메라, (b) Sparsh-X 중 특정 모달만 (예: 오디오+IMU만, 또는 이미지만) + 카메라, (c) 촉각 없는 시각카메라만 입력(baseline), (d) 촉각 이미지만 입력하여 end-to-end로 처음부터 학습한 정책(카메라도 포함) 등이다. 이렇게함으로써 멀티모달 촉각의 기여, 사전학습 유무의 영향 등을 정량적으로 비교했다. 각 정책은 20회의 삽입 시도를 통해 성공률을 측정했으며, 초기 손목 자세는 무작위로 다양하게 주었다.</li>
</ol>
<p>결과: 멀티모달 촉각 표현을 사용한 정책은 현격한 성능 향상을 보였다. 손목 카메라만 사용하는 경우 성공률이 15% 수준에 불과했는데, Sparsh-X 모든 촉각 모달을 사용하면 90%의 높은 성공률을 달성했다. 이는 시각-only 대비 500% 향상된 결과로, 시각정보만으로는 모호했던 삽입 정합 여부를 촉각이 확실히 보완해줌을 보여준다. 또한 촉각 이미지 하나만 사용하는 end-to-end 정책의 성공률 (~55%)과 비교해도 Sparsh-X 멀티모달 정책은 63%p 높았다. 아래 그래프는 다양한 경우의 성공률을 요약한 것이다 (Wrist는 손목 카메라, Touch는 촉각을 의미).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/2025-08-21-tactile-beyond-pixel/fcd5e6f3-0c08-4d63-bfb5-a96a1d167973.png" class="img-fluid figure-img"></p>
<figcaption>fcd5e6f3-0c08-4d63-bfb5-a96a1d167973</figcaption>
</figure>
</div>
<blockquote class="blockquote">
<p>플러그 삽입 과제에서 다양한 입력 조합에 따른 성공률 비교. 보라색이 Sparsh-X 사전학습 표현 사용 정책, 회색이 동일 입력을 end-to-end로 학습한 정책(E2E)를 나타낸다. 왼쪽 첫 번째 그룹은 손목 카메라 + 모든 촉각 모달(Sparsh-X)로, 약 90%의 성공률을 보이며 가장 우수하다. 반면 손목 카메라 단독 (맨 오른쪽)은 5% 수준으로 실패율이 매우 높다. 촉각 이미지 하나만 사용한 경우(Wrist+Touch(Image)) E2E가 Sparsh-X보다 높게 나타났는데, 이는 특정 작업에 과적합된 학습의 효과로 볼 수 있으며, Sparsh-X 표현은 보다 범용적임을 고려해야 한다는 언급이 있다.</p>
</blockquote>
<p>위 결과에서 주목할 점은, 사전학습(Pretraining)의 효과다. Sparsh-X (보라색 바)는 동일한 멀티모달 입력을 처음부터 정책과 함께 학습한 경우(회색바)보다 훨씬 높은 성공률을 보였다. 논문에 따르면, 사전학습된 표현을 쓰지 않고 정책 데이터로만 멀티모달 표현을 학습하면 성공률이 절반 수준으로 떨어지는데, Sparsh-X 사용 시 약 90% 성능 향상이 있었다고 한다. 이는 대규모 사전학습으로 얻은 촉각 표현이 없었다면, 정책 학습 데이터만으로는 복잡한 촉각 특징을 충분히 학습시키기 어려웠을 것임을 보여준다. 즉 멀티모달+사전학습의 조합이 정책 성능에 크게 기여한 것이다. 저자들은 “접촉의 미묘한 단서를 구별하려면 여러 감각이 필요하며, 사전학습은 그런 특징들을 미리 학습해두는 역할을 한다”라고 설명한다. 예를 들어, 오디오는 플러그 핀과 소켓이 처음 닿을 때 “딸깍” 소리를 감지하여 접촉 개시를 알려주고, 촉각 이미지와 압력은 정렬 및 삽입 중의 미세한 힘 변화를 감지하여 잘 끼워지고 있는지 피드백해준 다. 이러한 여러 신호를 종합해야 좁은 공차의 삽입 작업을 성공시킬 수 있는데, Sparsh-X는 그러한 촉각 상호보완 신호들을 잠재공간에서 잘 융합해주었다는 것이다.</p>
<p>한편 흥미로운 관찰로, 촉각 이미지 단일 모달의 경우 end-to-end로 해당 작업에 특화해 학습한 인코더가 사전학습된 표현보다 성능이 높았다. 위 그래프의 세 번째 그룹에서 회색막대(E2E 이미지)가 보라막대(Sparsh-X 이미지)보다 높음을 볼 수 있다. 이는 촉각 이미지 신호는 실험 조건에서 변화가 작기 때문에, 특정 환경에 최적화된 전용 인코더가 미세한 차이를 더 잘 포착할 수 있었기 때문이라고 저자들은 분석했다. 다만 이 경우 분포 내(in-distribution) 평가이므로, 범용성 측면에서는 Sparsh-X 표현이 더 다양한 상황에 견고할 가능성이 높다. 또한 Sparsh-X도 더 방대한 데 이터로 학습되거나 파인튜닝을 거치면 성능이 개선될 여지가 있다. 결국 모든 모달을 조합하는 것이 이러한 단일모달 한계를 상쇄하며 최고 성능을 내었다는 점이 핵심이다. 손목 카메라만 사용하는 경우에는 시각적인 모호성(visual aliasing) 문제로 실패가 잦았는데, 이는 카메라 각도가 제한적이라 플러그 핀이 소켓 구멍 바로 위에 있지 않아도 그렇게 보이는 경우가 있었기 때문이다. 이러한 경우 촉각이 없으면 잘못 정렬된 채로 힘을 주어 밀어 실패하지만, 촉각이 있으면 안 맞는 접촉을 즉시 감지하고 미세하게 조정함으로써 성공 확률이 크게 높아졌다.</p>
<ol type="1">
<li>손안의 물체 회전 – 시뮬레이션 정책의 촉각 적응 (In-hand Rotation with Sim-to-real Tactile Adaptation): 두 번째 실험은, 시뮬레이터에서 학습된 dexterous 조작 정책에 현실의 촉각 정보를 통합하여 성능을 향상시키는 시나리오다. 현실에서 다지 손으로 물체를 돌리는 작업은 어려워서, 보통 시뮬레이터에서 privileged information(물체의 정확한 pose, 질량, 마찰 등)에 접근해 정책을 먼저 학습하고, 이를 현실로 이전하는 방법이 쓰인다. 하지만 시뮬레이션에서 사용한 이러한 정보들은 현실에서는 직접 얻을 수 없는데, 촉각은 그 대체재가 될 수 있다. 저자들은 “현실에서 더 풍부한 센싱(촉각)이 가능해졌다면, 시뮬레이션 정책을 어떻게 개선할 수 있을까?”라는 질문을 던졌다. 이를 검증하기 위해, Hora라는 이름의 기존 시뮬레이션 학습 정책을 활용했다. Hora는 관절 위치 등 고유감각(proprioception)만으로 컵 모양 물체를 손 안 에서 z축 회전시키는 정책으로, 시뮬레이터에서 빠른 모터 적응 기법으로 학습된 것이다. 학습 당시 Hora는 물체의 무게, 마찰계수, 접촉지점 위치 등 다양한 privileged 정보를 활용했기 때문에 현실보다 유리한 조건이었다. 현실로 가져오면 이러한 정보 없이 오직 관절 상태만으로 수행해야 해서 성능이 저하된다. 저자들은 Sparsh-X 촉각 표현을 Hora 정책에 추가로 공급하여, 마치 시뮬레이션 때의 privileged 정보를 일부 보완해주는 촉각 적응(tactile adaptation)을 시도했다.</li>
<li>적응 기법(ControlNet 활용): 베이스 정책인 Hora를 보존하면서 새로운 촉각 입력을 통합하기 위해, ControlNet이라는 방법을 사용했다. ControlNet은 원래 이미지생성 등에서 추가 조건을 주입할 때 쓰이는 방법인데, 핵심은 기존 모델의 가중치는 고정하고 병렬적인 경로를 추가하여 새로운 입력을 학습시키는 것이다. 이때 0으로 초기화된 합성곱 레이어를 통해 연결하여, 학습 초기에 기존 모델 출력에 영향을 주지 않다가 점진적으로 학습이 진행되며 영향을 미치도록 한다. 이를 정책 학습에 응용하여, Hora 정책망은 그대로 두고 별도의 촉각 적응 모듈을 추가한 것이다. 구체적으로, Sparsh-X 인코더는 양손가락(Allegro 손의 4개 손가락)의 촉각 데이터를 입력받아 표현을 출력하고, 최근 1.5초간의 시계열을 일정 길이로 묶어 Conv 기반 temporal encoder를 통과시킨다. 이 출력이 ControlNet을 통해 Hora 정책의 중간층에 합쳐져, 최종적으로 손가락 모터 명령을 생성한다. 학습은 실제 로봇에서 Hora 정책을 실행하며 수집한 데이터를 사용했다. 우선 Hora 기본 정책을 이용해 현실에서 여러 시도를 수행하고, 그 중 30초 이상 물체를 안정적으로 회전시킨 50개의 성공 트라젝토리를 선별했다. 이 데이터에 대해 ControlNet 경로의 가중치만을 학습시켜 (Hora의 원래 경로 가중치는 고정) 실제 관절 상태(joint angles)와 Hora+ControlNet 출력 행동(target action) 간 L2 오차로 학습을 진행했다. 이 과정은 일종의 행동 모방이지만, 중요한 것은 촉각 피드백을 이용해 Hora 정책이 내는 명령을 보정하도록 학습된다는 점이다.</li>
<li>평가 설정: 학습된 Hora+ControlNet(Sparsh-X) 정책을 평가하기 위해, 다양한 상황에서 물체 회전 안정성을 측정했다. 비교 대상은 원본 Hora (베이스라인), Hora에 Sparsh-X 촉각 추가 – end-to-end로 함께 학습(E2E), Hora를 현실 데이터로 파인튜닝한 버전, 관절+시각 모달로 imitation learning한 정책 등이었다. 성능 척도로는, 물체를 회전시키는 동안 물체가 손에서 미끄러져 내려가는 정도(Vertical drift)와 물체 를 떨어뜨리기까지 버틴 시간(Time-to-fall)를 쟀다. 각 정책별로 최대 60초 에피소드를 10회 반복해 평균을 구했다. 또한 물체의 마찰 감소, 물체 질량 증가 등의 물체 물성 변화 상황에서도 동일하게 평가하여 견고성을 살폈다.</li>
</ol>
<p><strong>결과</strong>: Hora+ControlNet(Sparsh-X) 정책은 기존 Hora 대비 월등히 안정적이었다. 기본 물체 물성 조건에서, Sparsh-X 촉각을 넣어준 경우 물체의 수직 변위(미끄러짐 높이)가 90% 감소했고, 이는 Hora 파인튜닝이나 End-toEnd 촉각 통합보다도 뛰어난 성능이었다. 구체적으로, 원본 Hora는 물체가 수초 내에 손바닥 쪽으로 미끄러져 몇 cm 이동했지만, Sparsh-X 촉각피드백을 받으면 거의 미끄러짐이 발생하지 않아 회전 내내 안정적으로 잡고 있었다. Time-to-fall도 크게 향상되어, Sparsh-X 정책은 60초 내내 물체를 떨어뜨리지 않는 비율이 높았다. 반면 imitation learning으로 촉각 없이 학습한 정책(관절 데이터 -&gt; 행동 바로 매핑)은 금방 불안정 상태에 빠져 평균 유지시간이 가장 낮았다. 이는 단순 시도-오차 만으로는 학습이 어려운 복잡한 적응을, 촉각 피드백이 있어야 제대로 해낼 수 있음을 의미한다. 또한 Hora 파인튜닝 역시 Sparsh-X에 못 미쳤는데, 이는 좋은 시연만으로는 센서 정보 부족을 메꾸기 어렵고, 실시간 피드백이 있어야 성능 개선에 한계가 없음을 보여준다.</p>
<p>더욱 흥미로운 부분은 물체 물리 특성이 변했을 때의 성능 비교다. 먼저, 마찰 계수를 낮춘(미끄러운) 물체의 경우, Sparsh-X 촉각을 사용하는 정책만이 거의 성능 저하 없이 안정적으로 회전을 지속할 수 있었다. 반면 Sparsh-X 중 촉각 이미지 모달만 사용한 경우(즉 Sparsh-X(Image))는 어려움을 겪었는데, 이는 마찰 감소로 인한 접촉 패치의 변화가 미묘해서 이미지로만은 감지하기 힘들었던 것으로 보인다. Sparsh-X 전체 모달에서는 오디오/IMU/압력 등이 합쳐져 이런 변화를 감지하고 대응할 수 있었기에 여전히 물체를 놓치지 않았다. 다음으로 물체 질량을 20g 증가시켜 무겁게 만든 경우에는, Sparsh-X 멀티모달과 Sparsh-X 이미지 모두 기본 Hora보다 개선된 성능을 보였다. 무게가 무거워지면 원래 Hora 정책은 이를 모른 채 기존 패턴대로 움직이다가 물체를 놓치지만, Sparsh-X 표현이 물체가 무거워졌음을 촉각압/이미지 등으로 감지하여, 마치 시뮬레이션에서 알려주던 물체질량 정보를 대체해준 셈이 된다. 그 결과 손가락 움직임을 좀 더 넓게 (finger gaiting) 조절하여 무거워진 물체도 떨어뜨리지 않고 회전시킬 수 있었다. 요컨대, Sparsh-X 촉각 표현이 시뮬레이션의 privileged 정보에 상응하는 역할을 하여 정책의 적응력을 높였다는 것이 핵 심 결론이다.</p>
<p>종합하면, 시뮬레이션→현실 정책 이전 상황에서 Sparsh-X는 촉각 적응 모듈을 통해 현실 촉각 신호를 시뮬레이션의 부가정보처럼 활용하게 함으로써, 기존 정책의 성능을 상당히 향상시켰다. 이는 단순히 현실 데이터로 파인튜닝하거나 데모를 모방하는 것보다 효과적이었으며, 촉각 피드백 자체가 주는 정보량이 크기 때문에 가능한 것으로 해석된다. 특히 여러 모달의 촉각을 모두 활용해야 미끄러움, 무게 변화 등 미세한 접촉 변화를 감지하여 적절히 대응할 수 있음을 보여 준 점이 인상적이다.</p>
</section>
</section>
<section id="discussion" class="level2">
<h2 class="anchored" data-anchor-id="discussion">Discussion</h2>
<p>본 연구는 멀티센서 촉각 표현 학습이 로봇 조작 분야에 가져올 수 있는 잠재력을 여러 각도에서 입증하였다. 저자들은 서두에서 제기했던 두 가지 핵심 질문에 대해 다음과 같이 답을 제시한다:</p>
<ul>
<li>Q1: 현실 세계의 촉각 표현을 로봇 정책 학습에 활용하는 방법? – Sparsh-X와 같은 사전학습된 촉각 백본을 사용하면, 정책 학습의 성능과 견고성을 향상시킬 수 있다는 것이 확인되었다. 본 논문에서는 모방 학습과 시뮬레이션 정책의 촉각 적응이라는 두 가지 접근을 탐색했는데, 모방 학습은 데모에 내재된 풍부한 촉각 신호를 활용하기에 자연스러운 방법이고, 촉각 적응은 시뮬레이션에서나 얻을 수 있었던 정보를 현실 촉각으로 대체한 다는 점에서 새로운 시도를 보여줬다. 두 경우 모두 플러그 삽입과 물체 회전이라는 중요 조작 작업에 적용되어, 촉각 멀티모달 표현의 도입으로 정책 성공률 63% 향상, 물체 상태 복원력 90% 향상 등의 구체적인 성과를 달성했다. 이는 로봇 학습에서 촉각 정보가 얼마나 큰 가치를 가질 수 있는지 보여주는 강력한 증거다. 특히 시뮬레이션-현실 이식에서 촉각 적응은, privileged 정보의 격차를 상당 부분 해소하여, 기존의 모터 적응 접근보다도 현실성 있게 정책을 개선할 수 있음을 시사한다.</li>
<li>Q2: Sparsh-X 표현이 학습한 촉각 속성은 무엇인가? – 다양한 벤치마크 과제를 통해 확인한 결과, SparshX의 자기 지도 표현은 물체 식별, 행동 유형, 표면 재질, 내용물 양, 접촉력 등 다방면의 물리적 속성들을 효과적으로 인코딩하고 있었다. 예를 들어, Sparsh-X 표현만으로도 어떤 물체를 잡고 어떤 행동을 했는지 80~90% 정확도로 알아맞힐 수 있었고, 물 속에 든 것과 기름을 구분하거나 병에 절반 채웠는지 가득 채웠는지 파악하는 등 정교한 인식 능력을 보였다. 또한 일반적으로 촉각 연구에서 다루는 힘 예측도 수행하여, 기존 단일 모달 방식보다 오차를 크게 줄였다. 모달리티 별 기여를 분석한 결과, 이미지+오디오+IMU+압력의 모든 조합이 항상 최상의 성능을 주었고, 데이터가 적을수록 사전학습의 이득이 더욱 부각되었다. 사전학습 없는 촉각 이미지 단독 모델 대비 평균 48% 정확도 향상이 있었다는 결과 는 멀티센서 융합과 사전학습의 위력을 단 적으로 보여준다. 이러한 점들을 들어, 저자들은 Sparsh-X가 손끝 수준의 다양한 물리적 특징을 포착하고 있음을 확인하였고, 이를 접촉 상태 피드백을 잠재 공간에서 직접 제공하는 기술적 기반으로서 활용할 수 있음을 논한다.</li>
</ul>
<p>의의 및 차별성: Sparsh-X는 사상 최초로 이미지, 소리, 동작, 압력의 네 가지 촉각감을 하나로 융합한 표현 학습 모델이라는 점에서 큰 의의를 지닌다. 기존 연구들이 하나 또는 두 개 모달에 국한되었던 것과 달리, 본 연구는 인간 촉각의 다면성을 로봇에 구현하려는 포괄적인 시도라 할 수 있다. 특히 Transformer+보틀넥 토큰이라는 세련된 아키텍처로 계산 효율과 표현력을 모두 잡은 점, 그리고 대규모 자기지도 사전학습을 통해 라벨 부족 문제를 극복한 점은 주목할 만하다. 또한 멀티센서 촉각 데이터셋을 구축하여 공개함으로써, 향후 이 분야 연구의 기반을 마련한 것도 중요한 공헌이다. 실험적으로도, 논문은 로봇 제어 실험과 표현 평가 실험을 모두 다루어 표현 학습이 실제 로봇 성능에까지 이어짐을 입증하였다. 이는 단순한 특성 평가에 그치는 여러 표현 학습 연구들과 차별화되는 부분이다. 예컨대, Sparsh-X를 활용한 정책은 시각 정보만으로는 불가능했던 고난도 작업을 성공시켜 보였는데, 이러한 성공 사례는 “촉각이 로봇 조작 에 필수적”이라는 메시지를 강화한다. 더 나아가 foundation model for touch라는 표현에서 볼 수 있듯, 저자들은 이 연구가 다양한 로봇 응용에 일반적으로 쓰일 수 있는 범용 촉각 모델의 시작점이 되길 기대하고 있다.</p>
<p>고찰: 물론, Sparsh-X에도 몇 가지 한계와 향후 개선 지점이 있다. 예를 들어 플러그 삽입 실험에서 본 것처럼, 특정 촉각 모달(이미지)만으로 전문화된 학습을 하면 그 과제에 한해 더 나은 성능을 보일 수도 있었다. 이는 Sparsh-X 표현이 범용성 대가로 특화 성능이 약간 떨어질 가능성을 시사한다. 이러한 격차는 사전학습 데이터의 다양성이나 finetuning 기법으로 메울 수 있을 것이다. 또한 Sparsh-X는 현재 Digit 360 센서에 특화되어 있는데, 다른 촉각 센서나 로봇 플랫폼에서도 일반화되는지는 추가 검증이 필요하다. 그러나 Sparsh-X의 개념은 센서 종류에 무관하게 멀티모달 통합이라는 보편성을 지니므로, 센서-불변적인 촉각 표현(sensor-invariant tactile representation) 방향으로도 발전시킬 수 있을 것이다. 저자들이 참고문헌에서 언급한 동시대 연구들 을 보면, 여러 센서간 transferable한 표현 학습이나 Sparsh-X의 단순화 버전들도 연구되고 있어, 이 분야가 활발히 전개되고 있음을 알 수 있다.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>요약하면, “Tactile Beyond Pixels” 논문은 멀티센서 촉각 데이터의 융합 표현학습에 대한 선구적인 연구로서, Sparsh-X라는 강력한 촉각 백본 모델을 제시하였다. 이 모델은 Digit 360 센서로부터 얻은 이미지, 오디오, IMU, 압력 신호를 Transformer를 통해 효율적으로 통합하여, 손끝 접촉의 핵심 정보를 함축한 임베딩을 만들어낸다. 자기 지도 사전학습 덕분에 라벨 없이도 거대한 접촉 데이터로부터 학습되었고, 이를 기반으로 다양한 다운스트림 과제에서 뛰어 난 성능 향상을 시현했다. 논문은 정교한 로봇 조작(플러그 삽입)과 시뮬레이터 정책의 현실 적응(물체 회전) 시나리오를 통해 멀티모달 촉각의 실제적 가치를 보여주었고, 동시에 물체 식별, 재질/질량 파악, 힘 추정 등 촉각 인지 측면에서도 Sparsh-X의 우수성을 입증하였다. 이러한 결과들은 로봇에게 촉각을 “통합된 감각”으로 가르치는 접근이 현실화되었음을 의미하며, 향후 더 큰 규모의 데이터와 다양한 환경으로 확장된다면 로봇 조작의 범용 지능을 높이는 데 크게 기 여할 것으로 전망된다. Touch 분야의 Foundation Model 개념은 아직 초보적이지만, Sparsh-X는 그 유망성을 증명한 첫 걸음으로 평가할 수 있다. 특히 인간이 촉각을 통해 얻는 다층적 정보를 로봇도 학습할 수 있다는 가능성을 열어주 었으며, 이는 완전 자율적인 섬세한 로봇 손 조작을 향한 중요한 진전이다.</p>
<p>이번 연구로 인해, 멀티모달 촉각 센서의 활용과 대규모 표현 학습의 결합이라는 새로운 패러다임이 주목받게 되었다. 앞으로 다양한 연구자들이 Sparsh-X 모델과 데이터를 활용하거나 확장하여, 더욱 향상된 촉각 지능과 광범위한 로봇 응용을 실현해 나갈 것으로 기대된다. 나아가 시각, 촉각, 청각을 모두 통합한 진정한 멀티모달 로봇 지각 연구로까지 발전할 수 있으며, 이는 인간 수준의 섬세한 작업을 수행하는 로봇 개발에 핵심적인 역할을 할 것이다.</p>
</section>
<section id="limitations-and-future-work" class="level2">
<h2 class="anchored" data-anchor-id="limitations-and-future-work">Limitations and Future Work</h2>
<ul>
<li>데이터 다양성의 한계: Sparsh-X의 사전학습은 단일 유형의 센서(Digit 360)로 수집된 데이터에 기반하기 때문에, 센서 자체의 편향이 있을 수 있다. 예를 들어 촉각 이미지 모달의 경우, 사용된 센서 장치마다 광학적 특성이 조금씩 다를 수 있지만 데이터셋 내에서는 기기 종류가 제한적이라 다양성이 낮다고 한다. 이는 학습된 표현의 범용성에 제약을 걸 수 있다. 향후에는 여러 센서와 다양한 환경에서 더 방대한 멀티촉각 데이터를 수집하여 사전학습 함으로써, 일반화 성능을 높이는 공동 노력이 필요하다.</li>
<li>표현 미세조정(fine-tuning) 부재: 본 연구의 실험들은 Sparsh-X 인코더를 항상 동결(frozen) 상태로 두고 진행되었다. 이는 순수한 사전학습 효과를 보기 위한 의도적 설정이지만, 다운스트림 과제별로 표현을 미세조정했다면 성능이 더 향상될 가능성이 높다. 특히 앞서 논의된 대로, 특정 촉각 모달의 사전학습 표현은 한정된 데이터 상황에서는 특화 학습에 밀릴 수 있으므로, 과제별 추가 학습이나 데이터 증강 등을 통해 보완할 수 있다. 향후 연구에서는 Sparsh-X를 end-to-end로 파인튜닝하면서도 과적합을 방지하는 기법, 또는 모달리티마다 적절한 가중치 조정을 하는 방향 등이 고려될 수 있다.</li>
<li>힘 센싱 범위의 제한: 논문에서는 정적인 수직 방향 힘(normal force) 추정만 다루었으며, 그것도 단순한 접촉 조건(구형 프로브로 수직 누름)에 국한되었다. 실제 현장에서는 다양한 접촉 기하(geometry)나 복수 접촉 지점 동시 발생 등이 흔하지만, 이러한 경우로 Sparsh-X를 평가하지 못했다. 특히 전단력(shear force) 추정은 이번에 제외되었는데, 이는 촉각센서 내부의 변형과 외력이 뒤섞여 모형화가 까다롭기 때문이라고 한다. 향후에는 다양한 형상의 센서 접촉이나 비정형적인 힘 패턴에 대해서도 Sparsh-X 표현의 활용을 확장하고, 필요하다면 물리기반 모델링과 학습을 접목하여 보다 풍부한 힘 인지 능력을 갖추는 방향으로 연구가 진행될 수 있다. 또한 미끄러짐 검출이나 마찰계수 추정 등, 현재 결과에서 가능성을 보인 영역들을 더 체계적으로 파고들 수 있을 것이다.</li>
<li>범용성 및 멀티모달 통합의 확장: Sparsh-X는 촉각에 초점을 맞췄지만, 장기적으로 보면 시각, 촉각, 청각을 모두 아우르는 로봇 감각 통합이 목표가 될 수 있다. 예를 들어 로봇이 물체를 조작할 때, 눈(카메라)으로 전반적 상황을 보고, 손끝(Digit 360)으로 세밀한 접촉을 느끼며, 마이크로 소리도 들을 수 있다. 이러한 전신적(multimodal) 감각 통합을 한꺼번에 학습하는 거대 모델도 구상 가능하다. Sparsh-X의 성공은 우선 촉각 내부의 멀티모달 융합을 보여준 것이지만, 나아가 이질적인 센서 간 통합까지 포괄하려면 추가 연구가 필요하다. 이는 제한이라기보다 방향성으로서 언급할 부분이다. 또한 경량화나 실시간성 측면에서, 현재 모델은 크고 무거운데 이를 효율적으로 압축하거나 distillation하여 임베디드 시스템에 넣는 연구도 향후 실용화를 위해 필요할 것이다.</li>
</ul>
<p>위의 논의와 한계들을 종합하면, Sparsh-X 연구는 분명히 혁신적이지만 그 완성도를 높이기 위해 여러 후속 연구의 여지가 남아 있다. 그러나 이러한 미래 방향들은 이 연구의 가치를 훼손하기보다는 새로운 연구 분야를 개척했다는 긍정적 의미로 해석해야 할 것이다. 본 논문의 성과는 로봇 촉각 연구 커뮤니티에 멀티모달 학습의 중요성을 환기시켰고, 앞으로 더 정교한 촉각 지능을 향한 경쟁과 협력을 가속화할 것으로 기대된다.</p>
<!--
## 1. 논문의 주요 내용 요약


> Sparsh-X의 멀티센서 촉각 표현 융합 구조 예시. 이 구조는 Digit 360 센서로부터 얻은 촉각 이미지, 진동(오디오), 관성모션, 압력 신호를 입력으로 받아, 트랜스포머 기반 백본에서 이들을 융합하여 물체의 물리적 특성 및 접촉 상태를 나타내는 통합 표현을 학습한다. 

본 논문에서는 Meta/FAIR가 개발한 초고해상도 촉각 센서 Digit 360을 활용하여 4가지 촉각 모달리티(이미지, 오디오, 관성, 압력)를 통합하는 Sparsh-X라는 표현 학습 백본을 제안한다. 저자들은 약 100만 건의 접촉-조작 데이터(삽입, 미끄럼, 두드림, 회전 등 다양한 조작 행동)를 수집하여 자기 지도 학습으로 Sparsh-X를 사전학습시켰다. 이렇게 학습된 표현은 물체의 질량, 마찰, 힘 등의 물리적 특성을 추론할 수 있는 정보를 포함한다.

* 제안 방식: Sparsh-X는 각 모달리티를 독립적인 트랜스포머 블록으로 인코딩한 후, 병목(Bottleneck) 토큰을 매개로 크로스-모달 퓨전을 수행하는 구조를 갖는다. 이를 통해 고해상도 촉각 이미지뿐 아니라 초음속 진동음, 가속도/자이로 센서 정보, 압력 신호 등 이질적인 데이터를 하나의 잠재 공간에 효과적으로 융합한다.
* 데이터: 양팔 로봇 손가락과 수동 집게에 장착한 Digit 360으로 다양한 환경에서 수집한 약 1M의 비라벨 접촉 데이터를 사용했다. 이 대규모 무라벨 데이터셋은 향후 유사한 연구를 위한 벤치마크로도 활용될 수 있다.
* 실험 과제: 학습된 표현은 두 가지 주요 실험에 적용되었다. 
    (1) 흉내 학습(imitation learning)을 통한 플러그 삽입 과제(Allegro 로봇 손에 플러그를 꽂는 작업)와, 
    (2) 시뮬레이션-현실 간 촉각 적응을 통한 손안 회전 과제(컵 모양 물체를 손 안에서 회전시키는 작업)이다. 
    * 또한 물리적 속성 추론 과제(물체-행동 분류, 재질·질량·양 추정, 가해진 힘 추정 등)도 병행하여 평가했다.
* 주요 결과: 
    * Sparsh-X를 활용한 정책은 (a) 외부 카메라 정보만 사용한 정책 대비 성공률이 약 500%(즉 5배) 높고, (b) 촉각 이미지만 단독으로 사용한 종단간(end-to-end) 학습 대비 63% 높은 성공률을 보였다. 
    * 특히, 플러그 삽입 실험에서는 Sparsh-X 기반 정책이 20회 시도 중 90%의 성공률을 기록했으며, 기존 비전·촉각 방식 대비 성능이 크게 향상되었다. 
    * 손안 회전 과제에서는 Sparsh-X를 이용해 시뮬레이터에서 훈련된 정책을 적응시킨 결과, 물체의 수직 이동이 약 90% 감소하는 등 안정성이 크게 개선되었다. 
    * 물리 속성 추론에서는 Sparsh-X 표현을 사용해 48% 높은 정확도를 기록해, 기존 종단 학습 방식 대비 물리적 특성 이해 능력이 크게 향상됨을 보였다. 예를 들어, 모든 모달리티를 결합하면 힘 추정 오차가 평균 35mN로 촉각 영상만 사용했을 때보다 17% 감소했다.

## 2. 기술적 기여 및 한계 분석

* 통합 멀티모달 백본: 본 논문의 핵심 기여는 Sparsh-X라는 최초의 멀티센서 촉각 표현 백본이다. 이전까지 촉각 표현 학습은 대부분 단일 모달리티(예: GelSight류 촉각 영상)나 모달리티별 개별 학습에 그쳤다. Sparsh-X는 네 가지 모달리티를 병목 기반 트랜스포머로 융합함으로써, 다양한 촉각 신호 간의 상호 보완적 정보를 포착한다. 특히, 단순 토큰 병합 방식을 사용한 기존 멀티모달 트랜스포머(MULSA)보다 계산 복잡도를 크게 낮추고, 네 가지 입력을 하나의 잠재 공간으로 압축할 수 있다.
* 대규모 자기 지도 학습: 약 100만 건의 실제 접촉 데이터를 활용한 SSL(자기 지도 학습)을 수행했다. 이를 통해 데이터 라벨링 비용 없이 일반화 가능한 촉각 표현을 학습하였다. 사전학습된 Sparsh-X 표현은 downstream 학습 시 데이터 효율을 크게 높여, 적은 레이블 샘플로도 안정적인 정책 학습이 가능하다.
* 정밀한 물리특성 학습: Sparsh-X는 물체의 질량, 마찰계수, 적용 힘 등 다양한 물리 속성을 포착하며, 이러한 속성 예측 성능이 크게 향상되었다. 실제로 본 논문에서는 물체-행동-면 분류, 재질·양 예측, 가해진 힘 추정 실험을 통해 Sparsh-X가 48% 더 높은 분류 정확도를 보임을 확인했다. 또한 정상(normal) 힘 추정 실험에서 모든 모달리티를 결합할 때 평균 오차가 17% 감소하여 힘 추정 정확도가 향상되었다.
* 정책 학습과 시뮬레이터 적응: Sparsh-X 표현은 실제 로봇 조작 정책 학습에도 적용되었다. 예를 들어, 플러그 삽입 과제에서 이미지+촉각의 조합으로 행동을 예측하는 종단간 모델에 Sparsh-X를 추가하자 성공률이 63% 증가했다. 또한, 시뮬레이터에서 훈련된 손안 물체 회전 정책에 Sparsh-X 기반의 촉각 적응 모듈(ControlNet)을 적용하자 물체의 미끄러짐이 현저히 줄어들었으며, 기존 방법 대비 수직 이동량 90% 감소 효과를 보였다. 이처럼 멀티센서 촉각 표현은 시뮬-실전 전이(sim-to-real) 문제 해결에도 기여함을 보였다.
* 한계점: 이 논문은 멀티센서 촉각의 잠재력을 보여주었으나 다음과 같은 한계도 지적한다. 첫째, 현재 사용된 데이터셋은 Digit 360 센서가 포함된 특정 플랫폼(예: Allegro 손, 수동 집게)에서 수집되었기 때문에 촉각 영상 모달리티의 다양성이 제한적일 수 있다. 센서별 광학적 특성 차이로 인해 일반화 성능이 제한될 우려가 있다. 둘째, 모든 실험에서 Sparsh-X 표현을 고정(frozen) 상태로 사용했으며, 다운스트림 과제별로 파인튜닝을 하지 않았다. 실제 적용 시 파인튜닝을 허용하면 개별 모달리티의 데이터 부족 문제를 보완하고 성능을 더욱 높일 수 있다. 셋째, 힘 추정 실험은 정상 방향 힘에 한정되었고, 다양한 접촉 기하나 전단력 추정은 다루지 않았다. 전단력은 Digit 360 구조(탄성돔) 때문에 모델링이 복잡하며, 본 논문에서는 별도 고려되지 않았다. 마지막으로, 대용량 트랜스포머를 학습하는 데 필요한 계산 자원과 데이터 수집 노력이 커서, 실제로 적용하는 데 비용 부담이 있다.

## 3. 관련 연구와의 비교

* 기존 촉각 기반 조작 연구: 전통적으로 로봇 촉각 연구에서는 GelSight, DIGIT 등 비전 기반 촉각 센서가 주로 사용되었다. 이런 센서는 고해상도 촉각 이미지를 제공해 물체 형상, 힘, 마찰 등을 추정할 수 있다. 그러나 대부분 작업은 단일 촉각 이미지에 의존하거나, 외부 카메라와 연계하는 방식이었다. 예를 들어, 다양한 조립, 표면 식별, 경로 추적 과제에서 GelSight류 센서가 활용되었지만, 영상 촉각만으로는 연속적인 접촉 동작의 미세한 변화를 완전히 포착하기 어렵다는 한계가 있다.
* 오디오 및 기타 모달리티 활용: 일부 연구에서는 접촉 시 발생하는 진동음(오디오)이나 외부 카메라 영상을 함께 이용하여 물체 특성을 추정하려 했다. 예컨대, 접촉 마이크를 이용해 재질을 식별하거나, 영상-오디오 합성 학습을 시도한 바 있다. 하지만 오디오 단일 모달로는 접촉과정의 복잡한 힘·변형 정보를 온전히 얻기 어렵고, 멀티모달 학습을 하더라도 주로 시각과 청각에만 국한되었다.
* 기존 멀티모달 접근: MULSA 등 최근 연구는 비전, 촉각 영상, 오디오를 함께 Transformer로 학습하는 방식으로 멀티모달 촉각 표현을 시도했다. 그러나 MULSA는 단순히 모든 토큰을 이어붙여(attention concatenation) 처리하기 때문에 계산 복잡도가 매우 커지는 문제가 있다. 또한 MimicTouch (Yu et al., 2024)와 같은 연구는 영상 촉각과 오디오를 개별적으로 SSL로 학습했지만, 모달리티 간 융합을 수행하지 않아 촉각 간 상호작용을 충분히 활용하지 못했다.
* 본 논문의 차별점: Sparsh-X는 네 가지 모달리티(이미지, 오디오, 관성, 압력)를 통합하는 점에서 기존 연구와 뚜렷한 차별성을 가진다. 특히, 병목 토큰 기반의 멀티모달 트랜스포머를 도입해 모달리티 간 정보를 효율적으로 융합하며, 종래 방식보다 계산 효율성과 표현력 모두 개선했다. 이는 기존 음성 기반 접근이나 단일센서 기반 모델이 다루지 못한 다양한 촉각 신호를 통합하여, 더 풍부한 촉각 표현을 학습할 수 있게 한다. 즉, 종전 연구가 해결하지 못했던 멀티센서 융합 방식을 제안함으로써, 로봇의 촉각 인지가 한층 발전되었다.

## 4. 실제 응용 가능성 평가

* 센서 및 하드웨어 측면: 제안된 방법은 Digit 360 센서를 전제로 한다. Digit 360은 Meta FAIR와 GelSight가 공개한 최첨단 촉각 센서로, 지름 14mm의 인조 손가락 모양이며 18개 이상의 센싱 기능을 통합한다[16]. 물리적 변형과 압력, 진동을 초고해상도로 감지하여 인간 수준의 정밀도로 터치를 디지털화할 수 있다[16]. GelSight 측은 이 센서를 내년부터 본격 공급할 예정이므로, 향후 산업용 로봇에도 장착이 가능해질 것으로 보인다. 그러나 현재는 비교적 실험실용 프로토타입 수준이므로, 실제 공장이나 서비스 환경에 배치하려면 추가적인 내구성 검증과 비용 고려가 필요하다.
* 시스템 요구 사항: Sparsh-X는 대규모 사전학습이 전제되므로 상당한 계산 자원과 데이터 수집이 필요하다. 산업 현장에서 도입하려면 개별 작업에 맞춰 추가 학습 또는 미세 조정(fine-tuning)이 필수적이다. 또한, Digit 360을 로봇 손가락에 부착하고 실시간으로 데이터를 처리하려면 고속 데이터 처리와 연산 하드웨어가 요구된다. 예를 들어 삽입 조립 작업에서는 센서-행동 반응 지연(latency)을 줄여야 하며, 산업용 로봇 암에 정확히 맞도록 센서 장착 방식을 고려해야 한다.
* 적용 가능성: 그럼에도 불구하고, Sparsh-X의 촉각 표현은 고정밀 작업에 유리하다. 플러그 삽입, 나사 체결 등 카메라로는 어려운 정밀 조립 작업이나, 조리 로봇의 섬세한 조작, 복잡 형상의 부품 검사 등에 활용할 수 있다. 또한, 센서 데이터와 학습을 병행하면 비전 정보가 불충분한 어둡거나 부분 가려진 환경에서도 안정적인 조작이 가능하다. 예컨대, 복잡한 회로기판 위 작은 부품을 집거나, 의료용 로봇이 미세한 조직을 다루는 작업 등에 응용될 수 있다. 실제로 본 논문에서도 플러그 삽입과 같은 산업적 의미가 있는 조작에서 큰 성능 향상이 관찰되었다.
* 제약 및 전망: 현재 연구 단계에서는 다양한 접촉 형태(경사진 표면, 전단력 등)에 대한 검증이 부족하며, 대량의 촉각 데이터 구축도 필요한 상태이다. 또한 실제 산업 환경에서는 센서의 내구성, 잡음·오염 문제, 모델의 추론 속도 등이 추가 과제가 될 수 있다. 그럼에도 불구하고 Sparsh-X는 촉각에 기반한 ‘기초 모델(Foundation Model)’ 접근의 가능성을 보여준다. 즉, 다양한 로봇 작업에 재사용 가능한 촉각 표현을 제공함으로써, 추후 도메인별 미세 조정으로 적용 범위를 확장할 수 있는 잠재력이 크다. 실제로 GelSight 측은 Digit 360을 로봇 촉각 연구의 다음 단계로 평가하며, 의료·가상현실·휴머노이드 등 다양한 분야에 응용할 수 있을 것으로 전망한다. 따라서 충분한 데이터와 연산 자원이 확보된다면, Sparsh-X 방식은 산업용·서비스용 로봇에서 섬세한 조작을 필요로 하는 다수 과제에 적용 가능할 것이다.
-->


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="curieuxjy/blog_comments" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Copyright 2024, Jung Yeon Lee</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/curieuxjy">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>