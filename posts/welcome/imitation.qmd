---
draft: true
title: "ğŸ“ƒLearning Agile Robotic Locomotion Skills by Imitating Animals"
description: Reproducing the diverse and agile locomotion skills of animals
date: ""
categories: [legged robots, rl, imitation learning, pybullet, paper review]
toc: true
---

![Imgur](https://i.imgur.com/1Fh5MaD.png?1)

# 0. Abstract

> Abstractâ€”`Reproducing the diverse and agile locomotion skills of animals` has been a longstanding challenge in robotics. While manually-designed controllers have been able to emulate many complex behaviors, building such controllers involves a time-consuming and difficult development process, often requiring substantial expertise in the nuances of each skill. Reinforcement learning provides an appealing alternative for automating the manual effort involved in the development of controllers. However, `designing learning objectives that elicit the desired behaviors from an agent` can also require a great deal of skill-specific expertise. In this work, we present an `imitation learning system` that enables legged robots to `learn agile locomotion skills by imitating real-world animals`. We show that by `leveraging reference motion data`, a single learning-based approach is able to automatically synthesize controllers for a diverse repertoire of behaviors for legged robots. By `incorporating sample efficient domain adaptation techniques` into the training process, our system is able to learn adaptive policies in simulation that can then be quickly adapted for real-world deployment. To demonstrate the effectiveness of our system, we train an `18DoF` quadruped robot to perform a variety of agile behaviors ranging from different locomotion gaits to dynamic hops and turns.


- ë™ë¬¼ë“¤ì˜ locomotion skillë“¤ì„ ë¡œë´‡ìœ¼ë¡œ ì¬í˜„í•˜ëŠ” ê²ƒì€ ì •ë§ challengingí•˜ë‹¤
- agentì—ê²Œ ì›í•˜ëŠ” í–‰ë™ì„ ëª…í™•í•˜ê²Œ objectiveë¡œ ì œì‹œí•˜ëŠ” ê²ƒë„ ë§¤ìš° ì–´ë µë‹¤.
- imitation learning ê¸°ë²•ì„ ì´ìš©í•˜ì—¬ ë™ë¬¼ë“¤ì˜ locomotion skillë“¤ì„ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ í–ˆë‹¤.
- reference motion dataë“¤ì„ ì´ìš©í•´ì„œ controllerì— í•™ìŠµê¸°ë°˜ì˜ ì ‘ê·¼ë²•ì„ ë°”ë¡œ ë„£ì„ ìˆ˜ ìˆì—ˆê³  domain adaptation ê¸°ë²•ì„ training ê³¼ì •ì— ë„£ì–´ì£¼ì–´ì„œ ì‹¤ì œ ë¡œë´‡ì— deploymentë¥¼ í–ˆì„ ë•Œ ë°”ë¡œ ì˜ ë™ì‘í•  ìˆ˜ ìˆì—ˆë‹¤.
- 18 ììœ ë„ë¥¼ ê°€ì§€ëŠ” 4ì¡± ë³´í–‰ ë¡œë´‡ì„ ê°€ì§€ê³  ë‹¤ì–‘í•œ locomotion gaitë“¤ê³¼ dynamic hopì´ë‚˜ turnì´ ê°€ëŠ¥í–ˆë‹¤.

# I. Introduction

- While these methods have demonstrated promising results in simulation, 
agents trained through RL are prone to adopting unnatural behaviors 
that are dangerous or infeasible when deployed in the real world
- one to wonder: 
can we build more agile robotic controllers with less effort by directly imitating animal motions?
- The use of reference motions alleviates the need to design skill-specific reward functions
- In order to transfer policies learned in simulation to the real world, 
we propose a sample efficient adaptation technique, which fine-tunes the behavior of a policy
- Laikago quadruped robot
- different locomotion gaits, as well as dynamic hops and turns

---

- RLë¡œ í•™ìŠµí•œ agentë“¤ì€ ì‹œë®¬ë ˆì´ì…˜ì—ì„œ ì¢‹ì€ ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ì§€ë§Œ ë§‰ìƒ ì‹¤ì œ ë¡œë´‡ì— ì ìš©ë˜ì—ˆì„ ë•Œ ë¶€ìì—°ìŠ¤ëŸ½ê³  ìœ„í—˜í•œ ë™ì‘ë“¤ì„ ë³´ì—¬ì¤„ ë•Œê°€ ìˆë‹¤.
- ê·¸ë˜ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ë°”ë¡œ ë™ë¬¼ë“¤ì˜ ëª¨ì…˜ì„ ë”°ë¼ì„œ ë¡œë´‡ë“¤ì´ ì›€ì§ì´ë„ë¡ í•˜ë©´ ë˜ì§€ ì•Šì„ê¹Œ? ë¼ëŠ” ìƒê°ì„ í•˜ê²Œ ëœë‹¤.
- referenceê°€ ë  ìˆ˜ ìˆëŠ” motion ë°ì´í„°ë“¤ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì€ íŠ¹ì • skillë“¤ì„ ìœ„í•œ reward functionì„ ë§Œë“œëŠ” ì–´ë ¤ì›€ì„ ì¤„ì—¬ì¤„ ìˆ˜ ìˆë‹¤.
- ì‹œë®¬ë ˆì´ì…˜ì—ì„œ í•™ìŠµí•œ policyë¥¼ ì‹¤ì œ ë¡œë´‡ì— ì ìš©í•˜ê¸° ìœ„í•´ sample efficientí•œ adaptation techniqueì„ ì œì•ˆí–ˆëŠ”ë°, ì´ëŠ” policyë¡œ ë§Œë“¤ì–´ì§€ëŠ” behaviorë¥¼ fine-tuningí•˜ëŠ” ê²ƒì´ë‹¤.
- Laikago 4ì¡± ë³´í–‰ ë¡œë´‡ìœ¼ë¡œ ë‹¤ì–‘í•œ locomotion gait(ì´ë™ ê±¸ìŒìƒˆ)ë“¤ì„ í•™ìŠµí•  ìˆ˜ ìˆì—ˆê³  dynaamic hopsì™€ turnë“¤ë„ ê°€ëŠ¥í–ˆë‹¤.

# II. RELATED WORK

- Trajectory optimization and model predictive control can mitigate some of the manual effort involved in the design process, 
but due to the high-dimensional and complex dynamics of legged systems, 
reduced-order models are often needed to formulate tractable optimization problems
- Trajectory optimizationì´ë‚˜ model predictive control ê°™ì€ ë°©ë²•ë“¤ì€ controller ë””ìì¸ ê³¼ì •ì—ì„œ ë§ì€ manual í•œ ë¶€ë¶„ë“¤ì„ ì—†ì• ì£¼ê¸´ í–ˆì§€ë§Œ ë³´í–‰ ë¡œë´‡ë“¤ì˜ ê³ ì°¨ì›ì ì´ê³  ë³µì¡í•œ ë™ì—­í•™ ê³¼ì • ë•Œë¬¸ì— ì¶•ì•½ëœ ëª¨ë¸ë“¤ì„ ì‚¬ìš©í–ˆì—ˆë‹¤.

## Motion imitation

- But applications of motion imitation to legged robots have predominantly been limited to behaviors that emphasize upper-body motions, 
with fairly static lower-body movements, 
where balance control can be delegated to separate control strategies
- Recently, motion imitation with reinforcement learning has been effective 
for learning a large repertoire of highly acrobatic skills in simulation [44, 34, 45, 32]
- ë³´í–‰ë¡œë´‡ë“¤ì˜ applicationì— motion imitationì€ ìƒë‹¹íˆ ì •ì ì¸ lower-bodyì˜ ì›€ì§ì„ìœ¼ë¡œ upper-bodyì˜ ëª¨ì…˜ì„ ë„ˆë¬´ ì‹ ê²½ì“°ëŠ” í–‰ë™ë“¤ì— ì œí•œë¬ì—ˆë‹¤. ê·¸ë˜ì„œ balance controlì´ ì œì–´ ì „ëµìœ¼ë¡œ ë”°ë¡œ ë¶„ë¦¬ë¬ì—ˆë‹¤.
- ìµœê·¼ ì—°êµ¬ë“¤ë¡œ RLë¡œ í•˜ëŠ” motion imitationì€ ì‹œë®¬ë ˆì´ì…˜ì—ì„œ ë§¤ìš° acrobaticí•œ skillë“¤ì„ ì˜ í•™ìŠµí•˜ëŠ” ê²ƒìœ¼ë¡œ ì•Œë ¤ì¡Œë‹¤.

## Sim-to-real transfer

- by constructing more accurate simulations [58, 62], 
or adapting the simulator with realworld data [57, 23, 26, 36, 5].
- Domain randomization can be incorporated into the training process to encourage policies to be robust to variations in the dynamics [52, 60, 47, 42, 41].
- Sample efficient adaptation techniques, 
such as finetuning [51] and meta-learning [13, 16, 6] can also be applied 
to further improve the performance of pre-trained policies in new domains.

---

ê°•í™”í•™ìŠµì€ ìš°ì„  ì‹œë®¬ë ˆì´ì…˜(source domain)ì—ì„œ ë¨¼ì € í•™ìŠµì„ ì‹œí‚¨ í›„ , real world(target domain)ì— í•™ìŠµëœ agentê°€ ë™ì‘í•˜ê²Œ ëœë‹¤. ì´ ê³¼ì •ì—ì„œ ì‹œë®¬ë ˆì´ì…˜ê³¼ real world ì‚¬ì´ì˜ ì°¨ì´(gap)ì´ ìƒê¸°ê²Œ ë˜ê³  ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ë§ì€ ë°©ë²•ë“¤ì´ ì œì‹œëœë‹¤.

- ë§¤ìš° ì •í™•í•œ ì‹œë®¬ë ˆì´í„°ë¥¼ ë§Œë“¤ê±°ë‚˜ real world ë°ì´í„°ë¥¼ ê°€ì§€ê³  ì‹œë®¬ë ˆì´í„°ë¥¼ ë§ì¶°ì„œ sim-to-real  transferringì„ í•´ê²°í•˜ê³ ì í–ˆì—ˆë‹¤.
- Domain randomizationì€ trainingê³¼ì •ì—ì„œ policyê°€ ë‹¤ì–‘í•œ ë‹¤ì´ë‚˜ë¯¹ìŠ¤ë¥¼ ì ‘í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ë°©ë²•ì´ë‹¤.
    - ì—¬ê¸°ì„œ domainì´ë¼ í•¨ì€ ë¬¼ë¦¬ì ì¸ ì¡°ê±´, ë¡œë´‡ì˜ ì§ˆëŸ‰, ë§ˆì°°ë ¥ë“±ê³¼ ê°™ì€ ë¡œë´‡ì´ ë™ì‘í•˜ëŠ” í™˜ê²½ì˜ ë³€ìˆ˜ ì¡°ê±´ë“¤ì˜ ì§‘í•©ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤.
    - ì´ëŸ° ë¬¼ë¦¬ì ì¸ ê°’ë“¤ì„ ì‹œë®¬ë ˆì´ì…˜ì—ì„œ ì‹¤ì œ ë¡œë´‡ì´ ë™ì‘í•  í™˜ê²½ê³¼ ë˜‘ê°™ì´ ë§ì¶°ì£¼ê¸° í˜ë“¤ë‹¤.
    - ì´ëŸ° ê°’ë“¤ì´ ì¡°ê¸ˆì´ë¼ë„ ë‹¬ë¼ì ¸ì„œ ë¡œë´‡ì´ ì œëŒ€ë¡œ ë™ì‘í•˜ì§€ ëª»í•˜ê³  ë‹¤ì‹œ ì²˜ìŒë¶€í„° í•™ìŠµì„ í•´ì•¼ í•œë‹¤ë©´ ë§¤ìš° ë‚œê°í•œ ìƒí™©ì´ë˜ê³  ì‚¬ì‹¤ìƒ ë¡œë´‡ì„ ì“¸ ìˆ˜ ì—†ëŠ” ìƒí™©ì¼ ê²ƒì´ë‹¤.
- Fine-tuningì´ë‚˜ meta-learningê³¼ ê°™ì€ sample efficient adaptation ê¸°ë²•ë“¤ì„ ì‚¬ìš©í•´ì„œ ìƒˆë¡œìš´ domainì— ë†“ì¸ pre-trained policyë“¤ì´ í¼í¬ë¨¼ìŠ¤ë¥¼ ì˜ ë‚˜íƒ€ë‚´ê²Œ í•˜ëŠ” ë°©ë²•ë„ ìˆë‹¤.

### Ours

- latent space methods [24, 65, 67], 
to transfer locomotion policies from simulation to the real world
- During pre-training, 
these methods learn a latent representation of different behaviors 
that are effective under various scenarios.
- When transferring to a new domain, 
a search can be conducted in the latent space 
to find behaviors that successfully execute a desired task in the new domain.
- by combining motion imitation and latent space adaptation, 
our system is able to learn a diverse corpus of dynamic locomotion skills 
that can be transferred to legged robots in the real world.

---

ë³¸ ë…¼ë¬¸ì˜ ë°©ë²•ì€

- latent space ë°©ë²•ì„ ì´ìš©í•´ì„œ ì‹œë®¬ë ˆì´ì…˜ì—ì„œ real worldì— ë°°í¬í•  ë•Œ locomotion policyë“¤ì´ ì„±ê³µì ìœ¼ë¡œ transferringë˜ë„ë¡ í–ˆë‹¤.
    - pre-trainingê³¼ì •ì—ì„œ ì—¬ëŸ¬ behaviorë“¤ì˜ latent representationì„ í•™ìŠµí•˜ëŠ” ê²ƒì€ ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ë§¤ìš° íš¨ìœ¨ì ì´ë‹¤.
    - ìƒˆë¡œìš´ ë„ë©”ì¸ì— transferringë  ë•Œ, ìƒˆë¡œìš´ ë„ë©”ì¸ì—ì„œ desired taskë¥¼ í•˜ê¸° ìœ„í•œ behaviorê°€ latent spaceì—ì„œ ê²€ìƒ‰ëœë‹¤.
- motion imitationê³¼ latent space adaptationì„ í•©ì¹˜ë©´ì„œ ë‹¤ì–‘í•œ locomotion skillë“¤ì„ í•™ìŠµí•  ìˆ˜ ìˆì—ˆì„ ë¿ë§Œ ì•„ë‹ˆë¼ ì‹¤ì œ ë¡œë´‡ì—ë„ ì ìš©í•  ìˆ˜ ìˆì—ˆë‹¤.

## RL for legged locomotion

- Hwangbo et al. [26] proposed learning a motor dynamics model using real-world data, which enabled direct transfer of a variety of locomotion skills to the ANYmal robot
    - manually-designed reward functions for each skill
- Xie et al. [62] trained bipedal walking policies for the Cassie robot 
by imitating reference motions recorded from existing controllers and keyframe animations.
    - transferred from simulation to the real world with the aid of careful system identification
- Yu et al. [65] transferred bipedal locomotion policies from simulation 
to a physical Darwin OP2 robot 
using a latent space adaptation method, 
which mitigates the dependency on accurate simulators

### Ours

- latent space method, but by combining it with motion imitation, 
our system enables real robots to perform more diverse and agile behaviors 
than have been demonstrated by these previous methods.

---

- ë§¤ìš° ì„¸ì„¸í•œ reward function ë””ìì¸ì´ë‚˜ system identificationì„ ì¼ë˜ ì´ì „ ë°©ë²•ë“¤ê³¼ ë‹¬ë¦¬ latent space methodë¥¼ motion imitationê³¼ í•©ì¹˜ë©´ì„œ ë” ë‹¤ì–‘í•˜ê³  agileí•œ behaviorë“¤ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆì—ˆë‹¤.

# III. OVERVIEW

![Imgur](https://i.imgur.com/1Fh5MaD.png?1)

- Our framework receives as input a reference motion 
that demonstrates a desired skill for the robot, 
which may be recorded using motion capture (mocap) of real animals (e.g. a dog).
- Given a reference motion, 
it then uses reinforcement learning 
to synthesize a policy that enables a robot to reproduce that skill in the real world
- 3 stages
    - 1) The reference motion is first processed by the motion retargeting stage, 
    where the motion clip is mapped from the original subjectâ€™s morphology 
    to the robotâ€™s morphology via inverse-kinematics.
    - 2) Next, the retargeted reference motion is used in the motion imitation stage 
    to train a policy to reproduce the motion with a simulated model of the robot. 
    To facilitate transfer to the real world, 
    domain randomization is applied in simulation 
    to train policies that can adapt to different dynamics.
    - 3) Finally, the policy is transferred to a real robot 
    via a sample efficient domain adaptation process, 
    which adapts the policyâ€™s behavior using a learned latent dynamics representation.
    

---

- ì „ì²´ì ì¸ ê³¼ì •ì„ ë³´ë©´ ì›í•˜ëŠ” skillì˜ reference motionì„ inputì„ ë°›ì•„ì„œ ê°•í™”í•™ìŠµìœ¼ë¡œ ê·¸ ëª¨ì…˜ì„ ë”°ë¼í•˜ë„ë¡ í•™ìŠµí•˜ê²Œ í•œë‹¤.
- í¬ê²Œ 3ê°€ì§€ ê³¼ì •ìœ¼ë¡œ ì§„í–‰ë˜ë©°, 
1) reference motionì´ ë§Œë“¤ì–´ì§€ê³  ì›ë˜ ê·¸ í–‰ë™ì„ í•œ ëŒ€ìƒì˜ í˜•íƒœì™€ ë¡œë´‡ì˜ í˜•íƒœë¥¼ inverse-kinematicsë¥¼ ì´ìš©í•´ì„œ matchingí•´ì¤€ë‹¤.(ê°•ì•„ì§€ì˜ ì˜¤ë¥¸ìª½ ì•ë°œ ëê³¼ ë¡œë´‡ì˜ ì˜¤ë¥¸ìª½ ì• ë°œ ëì„ ë§ì¶°ì£¼ëŠ” ê³¼ì •) 
2) ì‹œë®¬ë ˆì´ì…˜ ìƒì—ì„œ retargeted reference motionì„ ì¬í˜„í•˜ë„ë¡ í•™ìŠµí•œë‹¤. ì´ë•Œ domain randomizationì„ í†µí•´ real world transferringì„ ë” ë¹¨ë¦¬ í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤. 
3) ì‹¤ì œ ë¡œë´‡ì— policyê°€ ë°°í¬ë˜ê³  latent dynamics representationì„ í†µí•´ policyì˜ behaviorê°€ ì˜ ì ìš©ë  ìˆ˜ ìˆë„ë¡ í•œë‹¤.

## 1. Motion Retargeting

![Imgur](https://i.imgur.com/mb8oQEE.png?1)

- To address this discrepancy, 
the source motions are retargeted to the robotâ€™s morphology using inverse-kinematics [19]
- The keypoints include the positions of the feet and hips
- At each timestep, 
the source motion specifies the 3D location $\hat{\mathbf{X}}_{i}(t)$ of each keypointi. 
The corresponding target keypoint $\mathbf{x}_{i}\left(\mathbf{q}_{t}\right)$ is determined 
by the robotâ€™s pose $\mathbf{q}_{t}$, represented in generalized coordinates
- IK is then applied 
to construct a sequence of poses $\mathbf{q}_{0: T}$ that track the keypoints at each frame
- $\underset{\mathbf{q}_{0: T}}{\arg \min } \sum_{t} \sum_{i}\left\|\hat{\mathbf{x}}_{i}(t)-\mathbf{x}_{i}\left(\mathbf{q}_{t}\right)\right\|^{2}+\left(\overline{\mathbf{q}}-\mathbf{q}_{t}\right)^{T} \mathbf{W}\left(\overline{\mathbf{q}}-\mathbf{q}_{t}\right)$
- additional regularization term is included 
to encourage the poses to remain similar to a default pose $\overline{\mathbf{q}}$
- $\mathbf{W}=\operatorname{diag}\left(w_{1}, w_{2}, \ldots\right)$ 
a diagonal matrix specifying regularization coefficients for each joint

---

- ë¡œë´‡ê³¼ ëª¨ì…˜ ë°ì´í„°ë¥¼ ì–»ì€ ë™ë¬¼ì˜ í˜•íƒœëŠ” ì¼ì¹˜í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ì´ ëª¨ì…˜ ë°ì´í„°ë¥¼ inverse-kinematicsë¥¼ ì´ìš©í•´ì„œ retargettingí•˜ëŠ” ì‘ì—…ì´ í•„ìš”í•˜ë‹¤.
- í‚¤í¬ì¸íŠ¸ëŠ” hips, feetì„ ì‚¬ìš©í–ˆë‹¤.
- source motionì€ í‚¤í¬ì¸íŠ¸ ië²ˆì§¸ì˜ 3d ì¢Œí‘œ xi(t)
- The corresponding target keypoint xi (qt ) is determined by the robotâ€™s pose qt
- IKì´ a sequence of poses q0:Tì— ëŒ€í•´ ê° í”„ë ˆì„ì˜ í‚¤í¬ì¸íŠ¸ë“¤ì„ ë”°ë¼ê°€ê¸° ìœ„í•´ ì ìš©ëë‹¤.
- default pose Ì„qì™€ ë¹„ìŠ·í•œ ìƒíƒœë¥¼ ìœ ì§€í•˜ê¸° ìœ„í•´ì„œ regularization termì¸ `W = diag(w1 , w2 , ...)` ì„ ì¶”ê°€í–ˆë‹¤.
    - ê° ì¡°ì¸íŠ¸ì— ëŒ€í•œ regularization coefficientë¥¼ ëª…ì‹œí•œ ëŒ€ê°í–‰ë ¬ì´ë‹¤.

$$
\underset{\mathbf{q}_{0: T}}{\arg \min } \sum_{t} \sum_{i}\left\|\hat{\mathbf{x}}_{i}(t)-\mathbf{x}_{i}\left(\mathbf{q}_{t}\right)\right\|^{2}+\left(\overline{\mathbf{q}}-\mathbf{q}_{t}\right)^{T} \mathbf{W}\left(\overline{\mathbf{q}}-\mathbf{q}_{t}\right)
$$

- $\underset{\mathbf{q}_{0: T}}{\arg \min } \sum_{t} \sum_{i}\left\|\hat{\mathbf{x}}_{i}(t)-\mathbf{x}_{i}\left(\mathbf{q}_{t}\right)\right\|^{2}+\left(\overline{\mathbf{q}}-\mathbf{q}_{t}\right)^{T} \mathbf{W}\left(\overline{\mathbf{q}}-\mathbf{q}_{t}\right)$

## 2. Motion Imitation

- $J(\pi)=\mathbb{E}_{\tau \sim p(\tau \mid \pi)}\left[\sum_{t=0}^{T-1} \gamma^{t} r_{t}\right]$
- $p(\tau \mid \pi)=p\left(\mathbf{s}_{0}\right) \prod_{t=0}^{T-1} p\left(\mathbf{s}_{t+1} \mid \mathbf{s}_{t}, \mathbf{a}_{t}\right) \pi\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)$
- To imitate a given reference motion,
The inputs to the policy is augmented with an additional goal $g_t$ , 
which specifies the motion that the robot should imitate.
- $\pi\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}, \mathbf{g}_{t}\right)$
- policy is queried at 30Hz for a new action at each timestep
- The state $\mathbf{s}_{t}=\left(\mathbf{q}_{t-2: t}, \mathbf{a}_{t-3: t-1}\right)$ is represented 
by the poses $\mathbf{q}_{t-2: t}$ of the robot in the three previous timesteps, 
and the three previous actions $\mathbf{a}_{t-3: t-1}$
- The pose features qt consist of 
IMU readings of the root orientation (row, pitch, yaw) 
and the local rotations of every joint.
- The root position is not included among the pose features 
to avoid the need to estimate the root position during real-world deployment.
- The goal $\mathbf{g}_{t}=\left(\hat{\mathbf{q}}_{t+1}, \hat{\mathbf{q}}_{t+2}, \hat{\mathbf{q}}_{t+10}, \hat{\mathbf{q}}_{t+30}\right)$ 
specifies target poses from the reference motion 
at four future timesteps, spanning approximately 1 second
- The action at specifies target rotations for PD controllers at each joint
- To ensure smoother motions, the PD targets are first processed by a low-pass filter

---

- The inputs to the policy is augmented with an additional goal gt ,
    - specifies the motion that the robot should imitate
- Ï€(at |st , gt )
- The policy is queried at `30Hz` for a new action at each timestep
- The state `st = (qtâˆ’2:t , atâˆ’3:tâˆ’1 )`
    - the poses qtâˆ’2:t of the robot in `the three previous timesteps`
    - `the three previous actions` atâˆ’3:tâˆ’1
- The pose features `qt` :
    - IMU readings of the root orientation (row, pitch, yaw)
    - the local rotations of every joint
    - ì‹¤ì œ ë¡œë´‡ì— ì ìš©í•  ë•Œ root positionì„ ì¶”ì •í•˜ì§€ ì•Šì•„ë„ ë˜ë„ë¡ qtì— root positionì„ í¬í•¨í•˜ì§€ ì•Šì•˜ë‹¤.
- The goal $\mathbf{g}_{t}=\left(\hat{\mathbf{q}}_{t+1}, \hat{\mathbf{q}}_{t+2}, \hat{\mathbf{q}}_{t+10}, \hat{\mathbf{q}}_{t+30}\right)$
    - specifies target poses from the reference motion at `four future timesteps`,
    - ì•½ 1 second
- The action at: specifies target rotations for PD controllers at each joint.
- ëª¨ì…˜ì´ ë¶€ë“œëŸ¬ì›Œì§€ê¸° ìœ„í•´ì„œ low-pass filterë¥¼ ì‚¬ìš©

### Reward Function

$$
\begin{gathered}
r_{t}=w^{\mathrm{p}} r_{t}^{\mathrm{p}}+w^{\mathrm{v}} r_{t}^{\mathrm{v}}+w^{\mathrm{e}} r_{t}^{\mathrm{e}}+w^{\mathrm{rp}} r_{t}^{\mathrm{rp}}+w^{\mathrm{rv}} r_{t}^{\mathrm{rv}} \\
w^{\mathrm{p}}=0.5, w^{\mathrm{v}}=0.05, w^{\mathrm{e}}=0.2, w^{\mathrm{rp}}=0.15, w^{\mathrm{rv}}=0.1
\end{gathered}
$$

---

- encourages the policy to track the sequence of target poses $\left(\hat{\mathbf{q}}_{0}, \hat{\mathbf{q}}_{1}, \ldots, \hat{\mathbf{q}}_{T}\right)$
- $\begin{gathered}r_{t}=w^{\mathrm{p}} r_{t}^{\mathrm{p}}+w^{\mathrm{v}} r_{t}^{\mathrm{v}}+w^{\mathrm{e}} r_{t}^{\mathrm{e}}+w^{\mathrm{rp}} r_{t}^{\mathrm{rp}}+w^{\mathrm{rv}} r_{t}^{\mathrm{rv}} \\w^{\mathrm{p}}=0.5, w^{\mathrm{v}}=0.05, w^{\mathrm{e}}=0.2, w^{\mathrm{rp}}=0.15, w^{\mathrm{rv}}=0.1\end{gathered}$

ê° ë¦¬ì›Œë“œ í…€ì„ ì‚´í´ë³´ë©´,

- The pose rewardëŠ” ë ˆí¼ëŸ°ìŠ¤ ëª¨ì…˜ê³¼ì˜ ì°¨ì´ë¥¼ ì¤„ì´ê¸° ìœ„í•œ ë¦¬ì›Œë“œì´ë‹¤. qëŠ” 1ì°¨ì› rotation ê°’ì´ë‹¤.
    - $r_{t}^{\mathrm{p}}=\exp \left[-5 \sum_{j}\left\|\hat{\mathbf{q}}_{t}^{j}-\mathbf{q}_{t}^{j}\right\|^{2}\right]$
- the velocity rewardëŠ” pose rewardì™€ ë¹„ìŠ·í•˜ê²Œ 1ì°¨ì›ì˜ ê°’ìœ¼ë¡œ angular velocity ê°’ì´ë‹¤.
    - $r_{t}^{\mathrm{v}}=\exp \left[-0.1 \sum_{j}\left\|\hat{\dot{\mathbf{q}}}_{t}^{j}-\dot{\mathbf{q}}_{t}^{j}\right\|^{2}\right]$ì†Œ
- the ee rewardëŠ” 3ì°¨ì› ê°’ìœ¼ë¡œ ë¡œë´‡ì˜ end-effectorë“¤ì˜ ì¢Œí‘œë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.
    - $r_{t}^{\mathrm{e}}=\exp \left[-40 \sum_{e}\left\|\hat{\mathbf{x}}_{t}^{e}-\mathbf{x}_{t}^{e}\right\|^{2}\right]$
- root pose rewardëŠ” ëª¸ì²´ì˜ global positionê³¼ rotationì„ ê°€ì§€ê³  êµ¬í•˜ë©°, root velocity rewardëŠ” ëª¸ì²´ì˜ ì„ ì†ë„ì™€ ê°ì†ë„ë¥¼ ê°€ì§€ê³  êµ¬í•œë‹¤.
    - $\begin{aligned}&r_{t}^{\mathrm{rp}}=\exp \left[-20\left\|\hat{\mathbf{x}}_{t}^{\mathrm{root}}-\mathbf{x}_{t}^{\mathrm{root}}\right\|^{2}-10\left\|\hat{\mathbf{q}}_{t}^{\mathrm{root}}-\mathbf{q}_{t}^{\mathrm{root}}\right\|^{2}\right] \\&r_{t}^{\mathrm{rv}}=\exp \left[-2\left\|\hat{\mathbf{x}}_{t}^{\mathrm{root}}-\dot{\mathbf{x}}_{t}^{\mathrm{root}}\right\|^{2}-0.2\left\|\hat{\mathbf{q}}_{t}^{\mathrm{root}}-\dot{\mathbf{q}}_{t}^{\text {root }}\right\|^{2}\right]\end{aligned}$



## Domain Adaptation

## A. Domain Randomization

- simple strategy for improving a policyâ€™s robustness to dynamics variations [52, 60, 42]
- varies the dynamics during training
- no single strategy that is effective across all environments

---

- domain randomization varies the dynamics during training, thereby encouraging the policy to learn strategies that are functional across different dynamics.
- training ì¤‘ì— ì—­í•™ì„ ë³€í™”ì‹œì¼œ, ë‹¤ë¥¸ ì—­í•™ì— ê±¸ì³ ê¸°ëŠ¥ì ì¸ ì „ëµì„ í•™ìŠµí•˜ë„ë¡ ì •ì±…ì„ ì¥ë ¤í•œë‹¤

## B. Domain Adaptation

![Imgur](https://i.imgur.com/JLTwjaq.png?1)

- `Âµ` represent the values of the dynamics parameters that are randomized during training in simulation
- a random set of parameters are sampled according to `Âµ âˆ¼ p(Âµ)`
- The dynamics parameters are then encoded into a latent embedding z âˆ¼ E(z|Âµ) 
by a stochastic encoder E
- z is provided as an additional input to the policy `Ï€(a|s, z)`
    - gëŠ” ì¸í’‹ìœ¼ë¡œ ë„£ì–´ì£¼ì§€ ì•Šì•˜ë‹¤.
- **incorporate an information bottleneck into the encoder**
- The information bottleneck enforces an upper bound Ic on the mutual information I(M, Z) between the dynamics parameters M and the encoding Z

$$
\begin{array}{ll}
\underset{\pi, E}{\arg \max } & \mathbb{E}_{\boldsymbol{\mu} \sim p(\boldsymbol{\mu})} \mathbb{E}_{\mathbf{z} \sim E(\mathbf{z} \mid \boldsymbol{\mu})} \mathbb{E}_{\tau \sim p(\tau \mid \pi, \boldsymbol{\mu}, \mathbf{z})}\left[\sum_{t=0}^{T-1} \gamma^{t} r_{t}\right] \\
\text { s.t. } & I(\mathbf{M}, \mathbf{Z}) \leq I_{c} .
\end{array}
$$

- noteì— ì •ë¦¬

## C. Real World Transfer

- $\mathbf{z}^{*}=\underset{\mathbf{z}}{\arg \max } \quad \mathbb{E}_{\tau \sim p^{*}(\tau \mid \pi, \mathbf{z})}\left[\sum_{t=0}^{T-1} \gamma^{t} r_{t}\right]$

# VII. EXPERIMENTAL EVALUATION

![Imgur](https://i.imgur.com/O1ZRjHb.png?1)

- `18` degrees-of-freedom quadruped
    - 3 actuated degrees-of-freedom per leg
    - 6 under-actuateddegrees of freedom for the root (torso)
- We further study the effects of regularizing the latent dynamics encoding with an information bottleneck, and
- show that this provides a mechanism to trade off between the robustness and adaptability of the learned policies.

## A. Experimental Setup

![Imgur](https://i.imgur.com/yo5bpza.png?1)

- mocap clips are collected from a public dataset
- Performance is recorded as the average normalized return,
    - 0 corresponding to the minimum possible return per episode
    - 1 being the maximum return
- Each policy is trained with proximal policy optimization 
using about 200 million samples in simulation
- end-to-end using the reparameterization trick
- Domain adaptation is performed on the physical system with AWR in the latent dynamics space,
    - using approximately 50 real-world trials to adapt each policy
    - Trials vary between 5s and 10s in length depending on the space requirements of each skill.
    - Hyperparameter settings are available in Appendix A

## Model Representation

![Imgur](https://i.imgur.com/tSMcnZl.png?1)

- The encoder E(z|Âµ) is represented by a fully-connected network 
that maps the dynamics parameters Âµ to the mean mE (Âµ) and standard deviation Î£E (Âµ) of the encoder distribution
- The policy network Ï€(a|s, g, z)
    - input : the state s, goal g, and dynamics encoding z
    - output : the mean mÏ€ (s, g, z) of a Gaussian action distribution
- The standard deviation $\Sigma_{\pi}=\operatorname{diag}\left(\sigma_{\pi}^{1}, \sigma_{\pi}^{2}, \ldots\right)$ of the action distribution is represented by a fixed matrix.
- The value function V(s,g,Î¼) : input the state, goal, and dynamics parameters

## B. Learned Skills

- pacing and trotting, as well as agile turning and spinning motions
    - Pacing is typically used for walking at slower speeds, and is characterized by each pair of legs on the same side of the body moving in unison
    - Trotting is a faster gait, where diagonal pairs of legs move together
- train policies for these different gaits â† providing the system with different reference motions

# VIII. DISCUSSION AND FUTURE WORK