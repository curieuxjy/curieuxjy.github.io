---
draft: false
title: "ğŸ§©CoRL 2025"
description: Plan & Search for meaningful insights
date: "2025-08-26"
categories: [corl, 2025, conference]
toc: true
---

### [Programs](https://www.corl.org/program)

> â€œì†/ì´‰ê°/ëŸ¬ë‹â€ ì¤‘ì‹¬ ì½”ìŠ¤ì— ë§ì¶° ì •ë¦¬í•œ CoRL 2025 Schedule ì •ë¦¬

# ğŸ“… ì¼ì • ìš”ì•½

* **9/27(í† )**: ì›Œí¬ìˆ ì§‘ì¤‘ â€” *2nd Workshop on Dexterous Manipulation* (ì†Â·ì´‰ê° ì—°êµ¬ì ë„¤íŠ¸ì›Œí‚¹/íŠœí† ë¦¬ì–¼ì— ìµœì ) ([Dexterous Manipulation Workshop][2])
* **9/28(ì¼)**: ë©”ì¸ì»¨í¼ëŸ°ìŠ¤ ì˜¤ëŸ´ ìœ„ì£¼ â€” **Oral 6 (Humanoid & Hardware)** â†’ **Oral 3 (Manipulation II)**
* **9/29(ì›”)**: í¬ìŠ¤í„° ìŠ¤í¬íŠ¸ë¼ì´íŠ¸ & í¬ìŠ¤í„° ë€ í™œìš© â€” **Spotlight 5, 6** ì¤‘ì‹¬(ì¤‘ê°„ì¤‘ê°„ ë°ëª¨/ë¶€ìŠ¤)
* **9/30(í™”)**: ë‚¨ì€ í¬ìŠ¤í„°/ë°ëª¨ & í‚¤ë…¸íŠ¸/EC Keynote ì²´í¬(ì‹œê°„ ë§ì¶° ì´ë™) ([CoRL][3])


![](../../images/2025-08-26-corl-2025/corl.png)

### 2025-09-27 (í† ) â€” ì›Œí¬ìˆ ë°ì´

- 09:00 â€“ 09:30 : í–‰ì‚¬ ë“±ë¡ ë° ì˜¤í”„ë‹ (ì˜ˆì •)
- 09:30 â€“ 10:30 : *RemembeRL Workshop â€“ Invited Talk 1* (ì›Œí¬ìˆ ì‹œì‘)  
  (+ ì¤‘ê°„ íœ´ì‹: 10:30 â€“ 11:00)
- 11:00 â€“ 12:30 : ì›Œí¬ìˆ ì—°ì‚¬ ë°œí‘œ ë° ì„¸ì…˜ (TBD)
- 12:30 â€“ 13:30 : ì ì‹¬ì‹œê°„
- 13:30 â€“ 15:00 : ì›Œí¬ìˆ í›„ë°˜ ì„¸ì…˜ ë° Poster Session 2  
  (+ íœ´ì‹: 15:00 â€“ 15:30)
- 15:30 â€“ 16:30 : Poster Spotlights / íŒ¨ë„ í† ë¡   
- 16:30 â€“ 16:40 : ì›Œí¬ìˆ ì¢…ë£Œ ë° í´ë¡œì§• ë¦¬ë§ˆí¬

### 2025-09-28 (ì¼) â€” ë©”ì¸ ì»¨í¼ëŸ°ìŠ¤ Day 1
- 09:00 â€“ 10:00 : ì»¨í¼ëŸ°ìŠ¤ ë“±ë¡ ë° ì˜¤í”„ë‹
- 10:00 â€“ 12:00 : Oral Session 6 (DexUMI, DexSkin ì°¸ì„)
- 12:00 â€“ 13:00 : ì ì‹¬
- 13:00 â€“ 15:00 : Oral Session 3 (KineSoft, Tactile Beyond Pixels, Cross-Sensor Touch Generation ì°¸ì„)
- 15:00 â€“ 15:30 : íœ´ì‹ / ë„¤íŠ¸ì›Œí‚¹
- 15:30 â€“ 18:00 : Spotlight 5 Poster (Self-supervised perception, Sim-to-Real RL, Crossing the Gap)
- 18:00 â€“           : ììœ  ì‹œê°„ / ë°ëª¨ ë¶€ìŠ¤ íƒë°©

### 2025-09-29 (ì›”) â€” ë©”ì¸ ì»¨í¼ëŸ°ìŠ¤ Day 2
- 09:00 â€“ 10:30 : Poster Spotlight 6 (VT-Refine, KineDex, LocoTouch, Text2Touch)
- 10:30 â€“ 11:30 : ì¶”ê°€ ê´€ì‹¬ í¬ìŠ¤í„° íƒë°© / íœ´ì‹
- 11:30 â€“ 13:00 : Early Career Keynotes (Yuan, Fazeli, Pinto)
- 13:00 â€“ 14:00 : ì ì‹¬
- 14:00 â€“ 15:30 : ê´€ì‹¬ ë…¼ë¬¸ ì„¸ì…˜ (ì¬ê²€í†  ë° ë¶€ìŠ¤ ë°©ë¬¸)
- 15:30 â€“ 16:00 : íœ´ì‹
- 16:00 â€“ 18:00 : Poster ìŠ¤ìœ• ë˜ëŠ” ì¶”ê°€ ë„¤íŠ¸ì›Œí‚¹
- 18:00 â€“           : ììœ  ì‹œê°„ ë˜ëŠ” ì €ë… ì„¸ì…˜ ì°¸ì—¬

### 2025-09-30 (í™”) â€” ë©”ì¸ ì»¨í¼ëŸ°ìŠ¤ Day 3 / í‚¤ë…¸íŠ¸ ë°ì´
- 09:00 â€“ 10:00 : ë“±ë¡ / ë§ˆë¬´ë¦¬ ì¤€ë¹„
- 10:00 â€“ 10:30 : **Jun-Ho Oh ê¸°ì¡° ê°•ì—°** ("The Golden Age of Humanoid Robots")
- 10:30 â€“ 11:00 : **Kristen Grauman ê¸°ì¡° ê°•ì—°** ("Skill learning from video")
- 11:00 â€“ 12:00 : ë‚¨ì€ ê´€ì‹¬ Oral/Poster ì„¸ì…˜ ë˜ëŠ” ë°ëª¨ íƒë°©
- 12:00 â€“ 13:00 : ì ì‹¬
- 13:00 â€“ 15:00 : ë¶€ìŠ¤ ë°©ë¬¸ / ë„¤íŠ¸ì›Œí‚¹
- 15:00 â€“ 16:00 : ë‚¨ì€ ë°œí‘œ ì²­ì·¨ ë˜ëŠ” ë°œí‘œì Q&A
- 16:00 â€“ 18:00 : ë§ˆë¬´ë¦¬ ì •ë¦¬ / ë„¤íŠ¸ì›Œí‚¹ ë§ˆê°

---

# ğŸ¤ Oral ì„ íƒ ì„¸ì…˜

## Oral 6 â€” Humanoid & Hardware (í•µì‹¬ í‚¤ì›Œë“œ: ì†Â·ì´‰ê°Â·Real World)

* **DexUMI: Using Human Hand as the Universal Manipulation Interface for Dexterous Manipulation** â€” *arXiv*
  - **í•œì¤„ í•µì‹¬**: ì† ì°©ìš© ì™¸ê³¨ê²©+ë¹„ì „ ì¸í˜ì¸íŒ…ìœ¼ë¡œ ì¸ê°„ ì† ë™ì‘ì„ ë‹¤ì–‘í•œ ë¡œë´‡ í•¸ë“œë¡œ ì „ì´, **Real World í‰ê·  86%** ì„±ê³µ.
  - **ë°œí‘œê°€ì¹˜ ì˜ˆì¸¡**: ğŸ”¥ *Must-see* â€” ë²”ìš© í•¸ë“œ ì „ì´/ë°ì´í„° ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸ì´ ì‹¤ì „ì„± ë†’ìŒ. ([arXiv][4])
* **DexSkin: High-Coverage Conformable Robotic Skin for Learning Contact-Rich Manipulation** â€” *Project page*
  - **í•œì¤„ í•µì‹¬**: ìœ ì—°í•œ ê³ ë°€ë„ ì •ì „ìš©ëŸ‰í˜• e-skinìœ¼ë¡œ ì†ê°€ë½ ì „ë©´/ë°°ë©´ì„ ì´‰ê°ìœ¼ë¡œ ë®ì–´ **ì ‘ì´‰ í’ë¶€ ê³¼ì œì˜ í•™ìŠµ/ì „ë‹¬** ì‹œì—°.
  - **ë°œí‘œê°€ì¹˜ ì˜ˆì¸¡**: ğŸ”¥ *Must-see* â€” ì €ê°€Â·ëŒ€ë©´ì  ì´‰ê° í•˜ë“œì›¨ì–´ì˜ Real World í•™ìŠµ ì ìš©ì´ ë§¤ë ¥ì . (arXiv ì˜ˆì •) ([DexSkin][5])

## Oral 3 â€” Manipulation II (í•µì‹¬ í‚¤ì›Œë“œ: ì†Œí”„íŠ¸í•¸ë“œÂ·ì´‰ê°í‘œí˜„Â·Tactile ìƒì„±)

* **KineSoft: Learning Proprioceptive Manipulation Policies with Soft Robot Hands** â€” *arXiv*
  - **í•œì¤„ í•µì‹¬**: ì†Œí”„íŠ¸í•¸ë“œ ë‚´ë¶€ **ë³€í˜•/ìŠ¤íŠ¸ë ˆì¸ ê¸°ë°˜ ê³ ìœ ê°ê°**ìœ¼ë¡œ **í‚¤ë„¤ìŠ¤í‹± í‹°ì¹­+í˜•ìƒì¡°ê±´ ì œì–´** ê²°í•©í•œ ëª¨ì‚¬í•™ìŠµ í”„ë ˆì„ì›Œí¬.
  - **ë°œí‘œê°€ì¹˜ ì˜ˆì¸¡**: ğŸ‘ *High* â€” ì†Œí”„íŠ¸í•¸ë“œ ì‹¤ì‚¬ìš© ë°ëª¨/ì •í™•ë„ í–¥ìƒ ê·¼ê±° ëª…í™•. ([arXiv][6])
* **Tactile Beyond Pixels: Multisensory Touch Representations for Robot Manipulation** â€” *arXiv*
  - **í•œì¤„ í•µì‹¬**: ì´ë¯¸ì§€Â·ì˜¤ë””ì˜¤Â·ëª¨ì…˜Â·ì••ë ¥ **4ëª¨ë‹¬ ì´‰ê°í‘œí˜„(Sparsh-X)** ì‚¬ì „í•™ìŠµìœ¼ë¡œ ì •ì±… **ì„±ê³µë¥  +63%**, ê°•ê±´ì„± **+90%**.
  - **ë°œí‘œê°€ì¹˜ ì˜ˆì¸¡**: ğŸ”¥ *Must-see* â€” ë©€í‹°ëª¨ë‹¬ ì´‰ê°í‘œí˜„ì˜ ìŠ¤ì¼€ì¼Â·ì¼ë°˜í™” ê·¼ê±° ì œì‹œ. ([arXiv][7])
* **Cross-Sensor Touch Generation** â€” (ì˜ˆì •/ì •ë³´ ì œí•œ)
  - **í•œì¤„ í•µì‹¬**: **ì´ê¸°ì¢… ì´‰ê°ì„¼ì„œ ê°„ ìƒì„±/ë²ˆì—­**ìœ¼ë¡œ ë°ì´í„° ì¦ê°•Â·í‘œí˜„ ì •í•©ì„ ê²¨ëƒ¥í•œ ì‘ì—….
  - **ë°œí‘œê°€ì¹˜ ì˜ˆì¸¡**: ğŸ‘€ *Watch* â€” ì„¸ë¶€ ë©”íŠ¸ë¦­ ê³µê°œ ì—¬ë¶€ì— ë”°ë¼ ê°€ì¹˜ ìƒí–¥ ê°€ëŠ¥. ([arXiv][8])

---

# ğŸ§ª Poster Spotlights / Posters (ìš°ì„ ìˆœìœ„ ì…€ë ‰ì…˜)

## Spotlight 5 & Poster 3 (ì´‰ê°Â·íœ´ë¨¸ë…¸ì´ë“œ Dexterity ì§‘ì¤‘)

* **Self-supervised perception for tactile skin covered dexterous hands** â€” *arXiv*
  - **í•œì¤„ í•µì‹¬**: **Self-supervised Sparsh-skin ì¸ì½”ë”**ë¡œ ì† ì „ì²´ì˜ ìê¸°ìì„ì‹ ìŠ¤í‚¨ ì‹ í˜¸ë¥¼ ì ì¬í‘œí˜„í™”, **ì„±ëŠ¥ +41%**/í‘œë³¸íš¨ìœ¨ ê°œì„ .
  - **ë°œí‘œê°€ì¹˜ ì˜ˆì¸¡**: ğŸ”¥ *Must-see* â€” ì „ë©´ ìŠ¤í‚¨ í™œìš© í¼ì…‰ì…˜Â·ì •ì±… ëª¨ë‘ ê°œì„ . ([arXiv][9])
* **Sim-to-Real Reinforcement Learning for Vision-Based Dexterous Manipulation on Humanoids** â€” *arXiv*
  - **í•œì¤„ í•µì‹¬**: íœ´ë¨¸ë…¸ì´ë“œ ì–‘ì† Dexterityì—ì„œ **ì‹¤-ì‹œë®¬ ìë™íŠœë‹, Reward ì„¤ê³„ ì¼ë°˜í™”, ë¶„í• ì¦ë¥˜**ë¡œ ì‹œì—°Â·ì¼ë°˜í™” í™•ë³´.
  - **ë°œí‘œê°€ì¹˜ ì˜ˆì¸¡**: ğŸ‘ *High* â€” ë°ëª¨ ì˜ì¡´â†“, RL ë‹¨ë…ìœ¼ë¡œ ì ‘ì´‰í’ë¶€ ê³¼ì œ ë‹¬ì„±. ([arXiv][10])
* **Crossing the Human-Robot Embodiment Gap with Sim-to-Real RL using One Human Demonstration** â€” *arXiv*
  - **í•œì¤„ í•µì‹¬**: **ë‹¨ 1ê°œ ì¸ê°„ RGB-D ë°ëª¨**ì—ì„œ **ê°ì²´ê¶¤ì  Reward+í”„ë¦¬-ê·¸ë© í¬ì¦ˆ ì´ˆê¸°í™”**ë¡œ íœ´ë¨¼-ë¡œë´‡ ê²©ì°¨ë¥¼ RLë¡œ ë¸Œë¦¬ì§€.
  - **ë°œí‘œê°€ì¹˜ ì˜ˆì¸¡**: ğŸ”¥ *Must-see* â€” ë°ì´í„° ë¹„ìš© í˜ì‹ Â·íœ´ë¨¼â†’ë¡œë´‡ ì „ì´ ì‹¤íš¨ì„± í¼. ([arXiv][11])

## Spotlight 6 & Poster 3 (TactileÂ·RewardÂ·Teaching íŒŒì´í”„ë¼ì¸)

* **VT-Refine: Learning Bimanual Assembly with Visuo-Tactile Feedback via Simulation Fine-Tuning** â€” *OpenReview*
  - **í•œì¤„ í•µì‹¬**: ì‹¤ë°ëª¨+ê³ ì¶©ì‹¤ Tactile ì‹œë®¬+RL ì¡°í•©ìœ¼ë¡œ **ì •ë°€ ì–‘íŒ” ì¡°ë¦½** í•™ìŠµ(ì‹œì—°/ì–´ë¸”ë ˆì´ì…˜ í¬í•¨).
  - **ë°œí‘œê°€ì¹˜ ì˜ˆì¸¡**: ğŸ‘ *High* â€” ë¹„ì£¼-ì´‰ê° í†µí•©ì˜ ì •ì„ ì‚¬ë¡€. ([OpenReview][12])
* **KineDex: Learning Tactile-Informed Visuomotor Policies via Kinesthetic Teaching for Dexterous Manipulation** â€” *arXiv*
  - **í•œì¤„ í•µì‹¬**: **í•¸ë“œ-ì˜¤ë²„-í•¸ë“œ í‚¤ë„¤ìŠ¤í‹± í‹°ì¹­+ë¹„ì£¼-Tactile ì •ì±…+í˜ì œì–´**ë¡œ ì ‘ì´‰í’ë¶€ ê³¼ì œ **74.4%** ë‹¬ì„±.
  - **ë°œí‘œê°€ì¹˜ ì˜ˆì¸¡**: ğŸ‘ *High* â€” í¬ìŠ¤/ì´‰ê° ê²°í•© ì‹¤ì„±ëŠ¥ ìˆ˜ì¹˜ ì œì‹œ. ([arXiv][13])
* **Text2Touch: Tactile In-Hand Manipulation with LLM-Designed Reward Functions** â€” *Project page*
  - **í•œì¤„ í•µì‹¬**: **LLM ê¸°ë°˜ Reward ì„¤ê³„**ë¥¼ ì‹¤ì œ **ë¹„ì „-ê¸°ë°˜ Tactile ì¸-í•¸ë“œ íšŒì „**ì— ì ìš©í•´ Rewardê³µí•™ ë¹„ìš© ì ˆê°.
  - **ë°œí‘œê°€ì¹˜ ì˜ˆì¸¡**: ğŸ‘€ *Watch* â€” ì‹¤ì œ Tactile í•˜ë“œì›¨ì–´ì™€ LLM Rewardì˜ ì ‘ì  í™•ì¸ ê°€ì¹˜. ([efi robotics][14])

## Spotlight 4 & Poster 2 

> ì´ë™ ì‹œê°„ ì—¬ìœ  ì‹œ ê°•ì¶”

* **LocoTouch: Learning Dexterous Quadrupedal Transport with Tactile Sensing** â€” *arXiv*
  - **í•œì¤„ í•µì‹¬**: ë“±ë©´ Tactile ì–´ë ˆì´+ì‹œë®¬-ì •í•©ìœ¼ë¡œ **ë¬´ê³ ì • ì›í†µë¬¼ ì¥ê±°ë¦¬ ìš´ë°˜**ì„ 4ì¡± ë¡œë´‡ì´ **ì œë¡œìƒ· ì‹¤ì „** ìˆ˜í–‰.
  - **ë°œí‘œê°€ì¹˜ ì˜ˆì¸¡**: ğŸ‘ *High* â€” ì´‰ê°ì„ ì´ë™ì²´(ë¡œì½”ëª¨ì…˜)ì— ì ‘ëª©í•œ ì°¸ì‹  ì‘ìš©. ([arXiv][15])

---

# ğŸ—ï¸ Keynotes / EC Keynotes(ì°¸ê³ )

* ë©”ì¸ í‚¤ë…¸íŠ¸ & EC Keynotes(ì˜ˆ: Wenzhen Yuan, Nima Fazeli, Lerrel Pinto) 
    â€” ì´‰ê°/ì¡°ì‘ ì£¼ì œì™€ ì§ì ‘ ì—°ê´€ëœ ê´€ì  ì—…ë°ì´íŠ¸ì— ìœ ìµ. ì„¸ì…˜ ì‹œê°„ëŒ€ëŠ” ê³µì‹ í˜ì´ì§€ ìˆ˜ì‹œ í™•ì¸. ([CoRL][3])

---

## âœ… ì´ë™/ì²­ì·¨ íŒ

* **ìš°ì„ ìˆœìœ„ ê·œì¹™**: (1) Oral 6Â·3 > (2) Spotlight 5Â·6 > (3) Spotlight 4 (ì¶©ëŒ ì‹œ).
* **í¬ìŠ¤í„° ì „ëµ**: ìœ„ ëª©ë¡ì„ ìš°ì„  ë¼ìš°íŒ…í•˜ê³ , ê°™ì€ ì„¹ì…˜ëŒ€ **ì¸ì ‘ í¬ìŠ¤í„°**(ì†/ì´‰ê° í‚¤ì›Œë“œ)ê¹Œì§€ ë¹ ë¥´ê²Œ ìŠ¤ìœ•.
* **ë²„í¼ í™•ë³´**: ì˜¤ëŸ´â†’í¬ìŠ¤í„° ì´ë™ ì „í›„ ìµœì†Œ 10â€“15ë¶„ ì´ë™Â·ëŒ€í™” ë²„í¼ í™•ë³´(ì§ˆë¬¸/ë„¤íŠ¸ì›Œí‚¹ í¬í•¨).
* **ìë£Œ íšŒìˆ˜**: arXiv/í”„ë¡œì íŠ¸ í˜ì´ì§€ë¥¼ ë¯¸ë¦¬ ì¦ê²¨ì°¾ê¸°(ëª¨ë°”ì¼)í•´ ë¶€ìŠ¤/ë°ëª¨ì—ì„œ ë°”ë¡œ ë ˆí¼ëŸ°ìŠ¤ ê³µìœ .


[1]: https://www.corl.org/?utm_source=chatgpt.com "CoRL 2025"
[2]: https://dex-manipulation.github.io/corl2025/?utm_source=chatgpt.com "2nd Workshop on Dexterous Manipulation: Learning and ..."
[3]: https://www.corl.org/program/keynotes?utm_source=chatgpt.com "Keynotes"
[4]: https://arxiv.org/html/2505.21864v2?utm_source=chatgpt.com "DexUMI: Using Human Hand as the Universal ..."
[5]: https://dex-skin.github.io/ "DexSkin | High-Coverage Conformable Robotic Skin for Learning Contact-Rich Manipulation"
[6]: https://arxiv.org/abs/2503.01078 "[2503.01078] KineSoft: Learning Proprioceptive Manipulation Policies with Soft Robot Hands"
[7]: https://arxiv.org/abs/2506.14754 "[2506.14754] Tactile Beyond Pixels: Multisensory Touch Representations for Robot Manipulation"
[8]: https://arxiv.org/abs/2504.06156?utm_source=chatgpt.com "Learning Contact-Rich Tasks Through Robot-Free Visuo ..."
[9]: https://arxiv.org/abs/2505.11420 "[2505.11420] Self-supervised perception for tactile skin covered dexterous hands"
[10]: https://arxiv.org/abs/2502.20396?utm_source=chatgpt.com "Sim-to-Real Reinforcement Learning for Vision-Based Dexterous Manipulation on Humanoids"
[11]: https://arxiv.org/abs/2504.12609?utm_source=chatgpt.com "Crossing the Human-Robot Embodiment Gap with Sim-to-Real RL using One Human Demonstration"
[12]: https://openreview.net/forum?id=mV3W5givYb&utm_source=chatgpt.com "VT-Refine: Learning Bimanual Assembly with Visuo-Tactile ..."
[13]: https://arxiv.org/abs/2505.01974 "[2505.01974] KineDex: Learning Tactile-Informed Visuomotor Policies via Kinesthetic Teaching for Dexterous Manipulation"
[14]: https://efi-robotics.github.io/?utm_source=chatgpt.com "efi robotics"
[15]: https://arxiv.org/abs/2505.23175 "[2505.23175] LocoTouch: Learning Dexterous Quadrupedal Transport with Tactile Sensing"


---

# âœ‹ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸

| ì„¸ì…˜ / ë°œí‘œì                           | ì§ˆë¬¸ ë‚´ìš© (í•œêµ­ì–´)                                              | ì§ˆë¬¸ ë‚´ìš© (ì˜ì–´)                                                                                                |
| ---------------------------------- | -------------------------------------------------------- | --------------------------------------------------------------------------------------------------------- |
| **DexUMI (Oral 6)**                | ì°©ìš©í˜• ì™¸ê³¨ê²© ì‚¬ìš©ì ë§ì¶¤ì€ ì–´ë–»ê²Œ ì´ë£¨ì–´ì§€ë‚˜ìš”?                              | How is user customization achieved in the wearable exoskeleton?                                           |
|                                    | ë¹„ì „ ì¸í˜ì¸íŒ… ê¸°ìˆ ì´ ë‹¤ì–‘í•œ ë¡œë´‡ í•¸ë“œì— ì–´ë–»ê²Œ ì¼ë°˜í™”ë˜ì—ˆë‚˜ìš”?                      | How does the vision-based inpainting generalize across different robotic hands?                           |
|                                    | ì„œë¡œ ë‹¤ë¥¸ ë¡œë´‡ í•¸ë“œ í”Œë«í¼ ê°„ì˜ ì„±ëŠ¥ ì°¨ì´ê°€ ìˆì—ˆë‚˜ìš”?                          | Were there performance differences among different robotic hand platforms?                                |
| **KineSoft (Oral 3)**              | ë‚´ë¶€ ìŠ¤íŠ¸ë ˆì¸ ì„¼ì„œì˜ í•´ìƒë„ì™€ ì‘ë‹µ ì†ë„ëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?                         | What is the resolution and response time of the internal strain sensor array?                             |
|                                    | í‚¤ë„¤ìŠ¤í‹± í‹°ì¹­ ë°©ì‹ê³¼ ê¸°ì¡´ IL ë°©ì‹ì˜ ì„±ëŠ¥ ì°¨ì´ëŠ” ë¬´ì—‡ì¸ê°€ìš”?                      | What are the performance differences between kinesthetic teaching and traditional imitation learning?     |
|                                    | í‹°ì¹­ ê¸°ë°˜ìœ¼ë¡œ ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ í†µí•œ ì¬í˜„ì„±ì€ ì–´ë–¤ ìˆ˜ì¤€ì¸ê°€ìš”?                       | How reproducible are the demonstrations collected via kinesthetic teaching across different object types? |
| **Tactile Beyond Pixels (Oral 3)** | ë„¤ ê°€ì§€ ì´‰ê° ëª¨ë‹¬ë¦¬í‹° ì¤‘ ê°€ì¥ ê¸°ì—¬ê°€ ì»¸ë˜ downstream taskëŠ” ë¬´ì—‡ì¸ê°€ìš”?         | Which downstream task benefited most from the four tactile modalities?                                    |
|                                    | ê° ëª¨ë‹¬ë¦¬í‹°ì˜ ì¤‘ìš”ë„ëŠ” ì–´ë–»ê²Œ í‰ê°€í•˜ì…¨ë‚˜ìš”?                                 | How were the importance levels of each tactile modality assessed?                                         |
|                                    | Self-supervised ì‚¬ì „í•™ìŠµ í›„, ì‹¤ì œ ì •ì±… í•™ìŠµì—ëŠ” ì–´ë–»ê²Œ í†µí•©ë˜ì—ˆë‚˜ìš”?                      | After self-supervised pretraining, how was it integrated into actual policy learning?                     |
| **Sparsh-skin (Poster)**           | tactile ìŠ¤í‚¨ Self-supervised í•™ìŠµì—ì„œ ì‚¬ìš©í•œ ì£¼ìš” í”„ë¦¬í…ìŠ¤íŠ¸ ì‘ì—…ì€ ë¬´ì—‡ì¸ê°€ìš”?             | What were the main pretext tasks used in self-supervised learning for tactile skin?                       |
|                                    | latent í‘œí˜„ì—ì„œ ì–´ë–¤ íŠ¹ì§•ì´ ê°€ì¥ ì¤‘ìš”í•˜ë‹¤ê³  ë³´ì‹œë‚˜ìš”?                        | Which features in the latent representation do you consider most crucial?                                 |
|                                    | ì´ í‘œí˜„ì„ í™œìš©í•œ downstream manipulation taskì˜ ì„±ëŠ¥ í–¥ìƒì€ ì–´ëŠ ì •ë„ì¸ê°€ìš”? | How much did manipulation task performance improve using these representations?                           |
| **Sim-to-Real RL (Poster)**        | ì‹œë®¬ë ˆì´í„°ë¥¼ í˜„ì‹¤ì— ë§ì¶”ê¸° ìœ„í•´ ì¡°ì •í•œ í•µì‹¬ íŒŒë¼ë¯¸í„°ëŠ” ë¬´ì—‡ì¸ê°€ìš”?                    | What were the key simulation parameters tuned for sim-to-real transfer?                                   |
|                                    | ì¼ë°˜í™” ê³¼ì •ì—ì„œ ê°€ì¥ ì–´ë ¤ì› ë˜ ìƒí™©ì€ ë¬´ì—‡ì´ì—ˆë‚˜ìš”?                             | What was the most challenging situation during the generalization phase?                                  |
|                                    | ì‹¤ì œ í™˜ê²½ì—ì„œ RL ì •ì±…ì˜ ì•ˆì •ì„±ì„ í™•ë³´í•˜ê¸° ìœ„í•œ ì „ëµì€ ë¬´ì—‡ì¸ê°€ìš”?                   | What strategies were used to ensure RL policy stability in real-world settings?                           |
| **Crossing the Gap (Poster)**      | ë‹¨ 1ê°œì˜ RGB-D ë°ëª¨ë§Œ ì‚¬ìš©í•˜ëŠ” ê¸°ë²•ì˜ ì£¼ìš” ì˜ë„ëŠ” ë¬´ì—‡ì¸ê°€ìš”?                   | What is the core intention behind using only one RGB-D demonstration?                                     |
|                                    | ë‹¨ì¼ ë°ëª¨ê°€ ê³¼ì œ ë³µì¡ë„ì— ë”°ë¼ ì–´ë–»ê²Œ ì„±ëŠ¥ì— ì˜í–¥ì„ ë¯¸ì³¤ë‚˜ìš”?                      | How did a single demonstration impact performance depending on task complexity?                           |
|                                    | ì´ ë°©ì‹ì´ ì‹œë®¬ ê¸°ë°˜ í•™ìŠµê³¼ ê²°í•©ë  ê²½ìš° ì–´ë–¤ ì‹œë„ˆì§€ê°€ ìˆë‚˜ìš”?                      | What synergies arise when combining this method with sim-based learning?                                  |
| **VT-Refine (Poster)**             | ì‹œê° ë° ì´‰ê° ì •ë³´ëŠ” ì–´ëŠ ì‹œì ì—ì„œ ìœµí•©ë˜ì—ˆë‚˜ìš”?                              | At what point are visual and tactile inputs fused in the control loop?                                    |
|                                    | ì–‘ì† ì¡°ë¦½ ê³¼ì œì˜ ë‚œì´ë„ëŠ” ì–´ë–»ê²Œ ì •ì˜ë˜ì—ˆë‚˜ìš”?                               | How was the difficulty level of the bimanual assembly task defined?                                       |
|                                    | fine-tuning ì „ëµê³¼ ë°˜ë³µ ì œì–´ êµ¬ì¡°ë¥¼ ì–´ë–»ê²Œ ì„¤ê³„í•˜ì…¨ë‚˜ìš”?                    | How did you design the fine-tuning strategy and iterative control structure?                              |
| **KineDex (Poster)**               | ì‹œë®¬ í™˜ê²½ì˜ ë…¸ì´ì¦ˆ ë° ì˜¤ì°¨ì— ëŒ€í•´ ì–´ë–»ê²Œ ê°•ê±´ì„±ì„ í™•ë³´í•˜ì…¨ë‚˜ìš”?                     | How did you ensure policy robustness against noise/errors in simulation?                                  |
|                                    | í¬ìŠ¤/ì´‰ê° ê¸°ë°˜ ì ‘ì´‰ ì¡°ì‘ì˜ ì‹¤íŒ¨ìœ¨ì€ ì–´ëŠ ì •ë„ì˜€ë‚˜ìš”?                           | What was the failure rate for force/tactile-based contact manipulation?                                   |
|                                    | í‹°ì¹­ ë°ì´í„°ì—ì„œ ì •ì±… í•™ìŠµ ì „ì´ ì‹œ ì˜¤ë²„í—¤ë“œëŠ” ì–´ë–»ê²Œ ê´€ë¦¬í•˜ì…¨ë‚˜ìš”?                    | How did you manage overheads during the transfer from teaching data to policy learning?                   |
| **LocoTouch (Poster)**             | ì´‰ê° ì–´ë ˆì´ì˜ í˜•íƒœì™€ ìœ„ì¹˜ëŠ” ì–´ë–»ê²Œ ì„¤ê³„ë˜ì—ˆë‚˜ìš”?                              | How were the shape and placement of the tactile array designed?                                           |
|                                    | zero-shot ì‹œë®¬â†’ì‹¤ ì „ì´ì— í¬í•¨ëœ ë„ë©”ì¸ ëœë¤í™” ìš”ì†ŒëŠ” ë¬´ì—‡ì¸ê°€ìš”?                | What domain randomization factors were included in the zero-shot sim-to-real transfer?                    |
|                                    | ì›í†µ ìš´ë°˜ ê³¼ì œì—ì„œ ì‹¤íŒ¨ ì‚¬ë¡€ëŠ” ë¬´ì—‡ì´ì—ˆê³  ì–´ë–»ê²Œ ê°œì„ í–ˆë‚˜ìš”?                       | What were the failure cases in the cylinder transport task, and how were they improved?                   |


# +++

- [FFHFlow: A Flow-based Variational Approach for Learning Diverse Dexterous Grasps with Shape-Aware Introspection](https://arxiv.org/abs/2407.15161)
- [Sim-to-Real Reinforcement Learning for Vision-Based Dexterous Manipulation on Humanoids](https://arxiv.org/abs/2502.20396)
- [RobustDexGrasp: Robust Dexterous Grasping of General Objects](https://arxiv.org/abs/2504.05287)
- [LodeStar: Long-horizon Dexterity via Synthetic Data Augmentation from Human Demonstrations](https://lodestar-robot.github.io/)
- [DexTrack: Towards Generalizable Neural Tracking Control for Dexterous Manipulation from Human References](https://arxiv.org/abs/2502.09614)
- [Morphologically Symmetric Reinforcement Learning for Ambidextrous Bimanual Manipulation](https://arxiv.org/abs/2505.05287)
- [Vision in Action: Learning Active Perception from Human Demonstrations](https://arxiv.org/abs/2506.15666)
- [Point Policy: Unifying Observations and Actions with Key Points for Robot Manipulation](https://arxiv.org/abs/2502.20391)
- [CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal Diffusion](https://arxiv.org/abs/2506.14769)
- [D-Cubed: Latent Diffusion Trajectory Optimisation for Dexterous Deformable Manipulation](https://arxiv.org/abs/2403.12861)
- [Humanoid Policy ~ Human Policy](https://arxiv.org/abs/2503.13441)
- [ClutterDexGrasp: A Sim-to-Real System for General Dexterous Grasping in Cluttered Scenes](https://arxiv.org/abs/2506.14317)
- [RoboArena: Distributed Real-World Evaluation of Generalist Robot Policies](https://arxiv.org/abs/2506.18123)
- [Training Strategies for Efficient Embodied Reasoning](https://arxiv.org/abs/2505.08243)
- [Ï€0.5: a Vision-Language-Action Model with Open-World Generalization](https://arxiv.org/abs/2504.16054)
- [DemoSpeedup: Accelerating Visuomotor Policies via Entropy-Guided Demonstration Acceleration](https://arxiv.org/abs/2506.05064)
- [ImMimic: Cross-Domain Imitation from Human Videos via Mapping and Interpolation](https://sites.google.com/view/immimic)
- [X-Sim: Cross-Embodiment Learning via Real-to-Sim-to-Real](https://arxiv.org/abs/2505.07096)
- [SAIL: Faster-than-Demonstration Execution of Imitation Learning Policies](https://arxiv.org/abs/2506.11948)
- [Streaming Flow Policy: Simplifying diffusion/flow-matching policies by treating action trajectories as flow trajectories](https://arxiv.org/abs/2505.21851)
- [Steering Your Diffusion Policy with Latent Space Reinforcement Learning](https://arxiv.org/abs/2506.15799)
- [Real2Render2Real: Scaling Robot Data Without Dynamics Simulation or Robot Hardware](https://arxiv.org/abs/2505.09601)
- [DexSkin: High-Coverage Conformable Robotic Skin for Learning Contact-Rich Manipulation](https://dex-skin.github.io/)
- [Learning Visuotactile Skills with Two Multifingered Hands](https://arxiv.org/abs/2404.16823)
- [3D-ViTac: Learning Fine-Grained Manipulation with Visuo-Tactile Sensing](https://arxiv.org/abs/2410.24091)
- [GELLO: A General, Low-Cost, and Intuitive Teleoperation Framework for Robot Manipulators](https://arxiv.org/abs/2309.13037)