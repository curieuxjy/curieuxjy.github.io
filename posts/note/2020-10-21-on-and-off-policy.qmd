---
title: "📝On policy VS. Off policy"
description: 강화학습 알고리즘에서 On-Policy와 Off-Policy의 차이
date: "2020-10-21"
categories: [rl, 2020]
toc: true
execute:
  freeze: auto 
---


# On-Policy vs Off-Policy in Reinforcement Learning

강화학습에서 **On-Policy**와 **Off-Policy**는 에이전트가 데이터를 수집하고 학습하는 방식에 따라 구분되는 두 가지 접근 방식입니다. 이 문서에서는 두 방식의 차이점, 특징, 장단점 및 주요 개념들을 정리합니다.

---

## 1. On-Policy

### 정의
현재 학습 중인 정책(behavior policy)에서 생성된 데이터를 사용하여 같은 정책을 개선합니다. 즉, 학습에 사용하는 데이터와 정책이 동일합니다.

### 특징
- 에이전트는 현재의 정책을 따라 환경과 상호작용하며 데이터를 수집합니다.
- 점진적으로 정책이 개선되며, 이전 정책과 크게 다르지 않습니다.
- **데이터 효율성**은 낮지만, 안정적이고 수렴 성능이 뛰어난 경우가 많습니다.

### 대표 알고리즘
- PPO (Proximal Policy Optimization)
- A3C (Asynchronous Advantage Actor-Critic)
- TRPO (Trust Region Policy Optimization)

### 장단점
- **장점:**
  - 학습 안정성이 높음
  - 수렴 속도가 빠를 수 있음
- **단점:**
  - 새로운 데이터를 매번 수집해야 하므로 데이터 효율성이 낮고 계산 비용이 높음

---

## 2. Off-Policy

### 정의
현재의 정책(target policy)을 학습하지만, 다른 정책(behavior policy)에서 생성된 데이터를 활용합니다. 즉, 학습에 사용하는 데이터와 정책이 다를 수 있습니다.

### 특징
- 과거의 경험(리플레이 버퍼 등)에 저장된 데이터를 재활용할 수 있습니다.
- 정책이 바뀌어도 이전의 데이터를 계속 활용할 수 있어 데이터 효율성이 높습니다.
- 행동 정책(behavior policy)이 반드시 학습 중인 정책(target policy)와 같을 필요는 없습니다.

### 대표 알고리즘
- DQN (Deep Q-Network)
- DDPG (Deep Deterministic Policy Gradient)
- SAC (Soft Actor-Critic)
- TD3 (Twin Delayed Deep Deterministic Policy Gradient)

### 장단점
- **장점:**
  - 데이터를 재사용할 수 있어 학습이 효율적
  - 다양한 정책으로 생성된 데이터를 활용 가능
- **단점:**
  - 학습이 불안정할 수 있음
  - 복잡한 알고리즘 설계가 필요할 수 있음

---

## 3. 주요 차이점
| **특징**                | **On-Policy**                              | **Off-Policy**                             |
|--------------------------|--------------------------------------------|--------------------------------------------|
| **데이터 수집 방식**      | 현재 학습 중인 정책에서 데이터를 생성      | 이전에 수집한 데이터도 재활용 가능          |
| **정책과 데이터 관계**    | 동일한 정책으로 생성된 데이터만 사용       | 서로 다른 정책으로 생성된 데이터도 사용      |
| **데이터 효율성**         | 낮음                                     | 높음                                       |
| **학습 안정성**          | 상대적으로 안정적                        | 불안정할 수 있음                           |
| **대표 알고리즘**         | PPO, A3C, TRPO                           | DQN, SAC, DDPG, TD3                        |

---

## 4. 추가 설명

### (1) On-policy 학습의 특징
- **현재 정책**(예: 행동 정책)만을 기반으로 데이터를 수집합니다.
- 데이터 수집 후 즉시 정책 업데이트에 활용되며, 과거 데이터를 재사용하지 않습니다.

### (2) Off-policy 학습의 특징
- **목표 정책**(예: 학습 중인 정책)과 다른 행동 정책에서 생성된 데이터를 활용합니다.
- 과거 데이터를 재활용하기 위해 경험 재생(experience replay)을 자주 사용합니다.
- Off-policy 학습자들은 부트스트래핑(현재 Q 값을 추정하기 위해 다음 상태/동작의 Q 값을 사용하는 방식)을 사용하기 때문에 불안정할 수 있으며, 경험 재생으로 이를 보완합니다.

### (3) ε-탐욕적 정책과 학습 방식의 관계
- 탐험 상수 ε이 0으로 설정되면 Off-policy 방식이 On-policy 방식처럼 작동할 수 있습니다. (예: SARSA vs Q-러닝)
- 하지만 ε-탐욕적 정책이 반드시 모든 환경에서 효율적이지는 않습니다.

---

## 5. 요약
- **On-policy:** 현재 정책으로 데이터를 수집하고 학습하며, 데이터 효율성이 낮지만 안정적입니다.
- **Off-policy:** 과거 데이터를 재활용하며 데이터 효율성이 높지만, 학습이 불안정할 수 있습니다.
- **경험 재생:** Off-policy 방식에서 더 흔히 사용되며, 특히 함수 근사기와 함께 안정성을 높이는 데 기여합니다.

---

### 참고: 알고리즘 간의 차이
- SARSA와 Q-러닝의 비교에서 On-policy와 Off-policy의 차이를 이해할 수 있습니다.
  - SARSA: On-policy 방식으로 ε-탐욕적 정책을 사용합니다.
  - Q-러닝: Off-policy 방식으로 최대화 정책(maximising policy)을 사용합니다.

