---
draft: true
title: "ğŸ“ƒRobust Recovery Controller"
description: Robust Recovery Controller for a Quadrupedal Robot using Deep Reinforcement Learning
date: ""
categories: [recovery, eth, paper review]
toc: true
---

# Abstract

The ability to `recover from a fall` is an essential feature for a legged robot to navigate in challenging environments robustly. Until today, there has been very little progress on this topic. Current solutions mostly build upon (heuristically) predefined trajectories, resulting in unnatural behaviors and requiring considerable effort in engineering system-specific components. In this paper, we present an approach based on model-free Deep Reinforcement Learning (RL) to control recovery maneuvers of quadrupedal robots using a `hierarchical behavior-based` controller. The controller consists of `four neural network policies` including `three behaviors` and `one behavior selector` to coordinate them. Each of them is trained individually in simulation and deployed directly on a real system. We experimentally validate our approach on the quadrupedal robot ANYmal, which is a dog-sized quadrupedal system with 12 degrees of freedom. With our method, ANYmal manifests dynamic and reactive recovery behaviors to recover from an arbitrary fall configuration within `less than 5 seconds`. We tested the recovery maneuver more than 100 times, and `the success rate was higher than 97 %`.

# Introduction

ë³´í–‰ë¡œë´‡ì˜ ë§ì€ ì—°êµ¬ë“¤ì€ ë³´í–‰í•˜ëŠ” locomotion taskì— ì§‘ì¤‘ë˜ì–´ ìˆëŠ” ê²½í–¥ì´ ìˆì§€ë§Œ ì´ì— ëª»ì§€ ì•Šê²Œ recovery taskì— ëŒ€í•´ì„œë„ ìƒê°í•´ë´ì•¼ í•©ë‹ˆë‹¤. ë³´í–‰ ë¡œë´‡ì´ ì˜ ê±·ë‹¤ê°€ë„ ì˜ˆìƒì¹˜ ëª»í•œ í™˜ê²½ìš”ì¸ì— ì˜í•´ ë„˜ì–´ì§ˆ ìœ„í—˜ì€ í•­ìƒ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ë”°ë¼ì„œ ë³¸ ë…¼ë¬¸ì—ì„œëŠ” `robust recovery maneuver`ë¥¼ ê°•í™”í•™ìŠµì„ í†µí•´ ë””ìì¸í–ˆìŠµë‹ˆë‹¤. ì˜ ì—°êµ¬ë˜ì§€ ì•ŠëŠ” ë¶€ë¶„ì¸ ë§Œí¼ ìš°ì„  ì´ëŸ¬í•œ maneuverì— ëŒ€í•œ ì •ì˜ë¥¼ í•˜ìë©´, ë¡œë´‡ì´ ë„˜ì–´ì§„ ìƒí™©(fall)ì—ì„œ ë‹¤ì‹œ ì •ìƒ ì‘ë™ ìƒíƒœ(staningì´ë‚˜ walking)ìœ¼ë¡œ ëŒì•„ì˜¬ ìˆ˜ ìˆë„ë¡ í•˜ëŠ” maneuverë¥¼ ë§í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ê¸°ëŠ¥ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ì„œëŠ” (1) **ì§€ë©´ê³¼ì˜ interaction**ì„ ì ì ˆíˆ ì´ìš©í•  ì¤„ ì•Œì•„ì•¼ í•˜ë©°, (2) **ë‹¤ë¦¬ë¥¼ ì´ìš©í•´ì„œ swinging motion**ì„ ì‚¬ìš©í•  ì¤„ ì•Œê³  (3) ë¡œë´‡ì´ ì›€ì§ì´ëŠ” ê³¼ì • ë™ì•ˆì— ëª¨ì…˜ì´ ê¼¬ì—¬ì„œ **í˜¼ìì„œ ì¶©ëŒí•˜ì§€ ì•Šë„ë¡** í•´ì•¼ í•©ë‹ˆë‹¤.  

![Imgur](https://i.imgur.com/kKRwRlP.png?1)

ì´ì „ì˜ ì—°êµ¬ë“¤ì€ ë‹¤ë¦¬ ì´ì™¸ì˜ ê¼¬ë¦¬ë‚˜ ì¶”ê°€ì ì¸ limbë¥¼ ì¥ì°©í•˜ì—¬ í•´ê²°í•˜ê±°ë‚˜ ì˜ˆì¸¡ ê°€ëŠ¥í•œ caseë“¤ì— ëŒ€í•´ì„œë§Œ fall-recoveryë¥¼ í–ˆì—ˆê¸°ì— corner caseë“¤ì—ì„œëŠ” ì·¨ì•½í–ˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ model-free RL ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ agentê°€ ì§ì ‘ í™˜ê²½ê³¼ ìƒí˜¸ì‘ìš©í•˜ë©° ê²½í—˜ìœ¼ë¡œë¶€í„° ë‹¤ì´ë‚˜ë¯¹ìŠ¤ì™€ ì»¨íŠ¸ë¡¤ policyë¥¼ í•™ìŠµí•˜ë„ë¡ í–ˆìŠµë‹ˆë‹¤. ë¡œë´‡ì´ ë„˜ì–´ì¡Œë‹¤ê°€ ì¼ì–´ë‚˜ëŠ” ê³¼ì •ì„ ë‹¨ê³„ë³„ë¡œ ë‚˜ëˆ„ì–´ì„œ ìƒê°í•´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

1. self-righting
2. standing up
3. locomotion(ì •ìƒ ë³´í–‰)

ì´ 3ê°€ì§€ behaviorë¥¼(multi-task) í•œ ê°œì˜ policyì— í•™ìŠµ ì‹œí‚¬ ê²½ìš° ì˜ í•™ìŠµì´ ë˜ì§€ ì•Šì„ ë¿ë§Œ ì•„ë‹ˆë¼ ì˜¤íˆë ¤ ê° behaviorì˜ í¼í¬ë¨¼ìŠ¤ë¥¼ ì„œë¡œ í•´ì¹˜ê²Œ ë˜ê¸° ë•Œë¬¸ì— **ì—¬ëŸ¬ ê°œì˜ policyë“¤ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒ**ì´ ì¢‹ìŠµë‹ˆë‹¤. ê·¸ë˜ì„œ ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ê° behaviorë“¤ì„ ì»¨íŠ¸ë¡¤í•˜ëŠ” **policy 3ê°œ**ì™€ 3ê°œì˜ policyë“¤ ì¤‘ì— í•˜ë‚˜ë¥¼ ì„ íƒí•˜ëŠ” **behavior selection policy 1ê°œ**, ì´ 4ê°œë¥¼ ë”°ë¡œ í•™ìŠµ ì‹œì¼°ìŠµë‹ˆë‹¤. ì´ëŸ° `ê³„ì¸µì (hierarchical)ì¸ êµ¬ì¡°`ëŠ” ëª¨ë“  ë¶€ë¶„ì´ Neural Networkë¡œ ì´ë£¨ì–´ì ¸ ìˆì–´ì„œ ë³µì¡í•œ maneuverë¥¼ ë§Œë“¤ê¸°ì— ì í•©ë‹ˆë‹¤.

ê°•í™”í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œëŠ” **Trust Region Policy Optimization(TRPO)**ë¥¼ ì‚¬ìš©í–ˆìœ¼ë©°, **í•™ìŠµ ê¸°ë°˜ì˜ state estimator**ë¥¼ behavior selectionê³¼ ë™ì‹œì— í•™ìŠµì‹œì¼œì„œ ë„˜ì–´ì¡Œì„ ë•Œ ì•Œ ìˆ˜ ì—†ëŠ” contact conditionê³¼ ê°™ì€ ìƒíƒœë¥¼ íŒŒì•…í•  ìˆ˜ ìˆë„ë¡ í–ˆìŠµë‹ˆë‹¤.

# Method