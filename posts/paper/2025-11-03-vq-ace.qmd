---
title: "📃VQ-ACE 리뷰"
date: 2025-11-03
categories: [mpc, rl, action-chunking]
toc: true
number-sections: False
description: Efficient Policy Search for Dexterous Robotic Manipulation via Action Chunking Embedding
---

> 🔍 Ping. 🔔 Ring. ⛏️ Dig. A tiered review series: quick look, key ideas, deep dive.


- [Paper Link](https://arxiv.org/abs/2411.03556)
- [Project LInk](https://srl-ethz.github.io/page-vq-ace/)
- [Code](https://github.com/srl-ethz/vq_ace)

1. 🤖 VQ-ACE는 복잡한 로봇 조작을 위해 인간의 손 동작을 양자화된 잠재 공간으로 압축하여 행동 공간의 차원을 크게 줄이는 새로운 프레임워크입니다.
2. 🚀 이 프레임워크를 Model Predictive Control (MPC)에 적용하면, 잠재 공간 샘플링을 통해 Ball Rolling 및 Object Picking과 같은 작업에서 더 인간다운 동작과 높은 성공률을 달성합니다.
3. 💡 또한, Reinforcement Learning (RL)에 행동 청킹을 통합하면 학습을 가속화하고 탐색을 개선하여 큐브 쌓기 및 인핸드 큐브 재정렬과 같은 작업에서 더 빠른 수렴과 뛰어난 성능을 가능하게 합니다.


<center>
<img src="../../images/2025-11-03-vq-ace/0.png" width="80%" />
</center>

<center>
<img src="../../images/2025-11-03-vq-ace/0.gif" width="85%" />
</center>

---

# 🔍 Ping Review

> 🔍 Ping — A light tap on the surface. Get the gist in seconds.


이 논문은 고차원적이고 복잡한 동작을 요구하는 로봇 조작 작업, 특히 능숙한 로봇 조작(dexterous robotic manipulation)의 어려움을 해결하기 위해 VQ-ACE (Vector Quantized Action Chunking Embedding)라는 새로운 프레임워크를 제안합니다. VQ-ACE는 인간 손의 움직임을 양자화된 잠재 공간(quantized latent space)으로 압축하여 액션 공간의 차원을 크게 줄이면서도 핵심적인 동작 특성을 보존합니다. 이 프레임워크를 Model Predictive Control (MPC)과 Reinforcement Learning (RL)에 통합하여, 생체모방 로봇 손(biomimetic robotic hand)을 사용한 능숙한 조작 작업에서 보다 효율적인 탐색(exploration)과 정책 학습(policy learning)을 가능하게 합니다.

VQ-ACE의 핵심 방법론은 다음과 같습니다:

1.  **데이터 수집 및 전처리:**
    *   모션 캡처 장갑(motion capture glove)으로 수집된 인간 손 동작 데이터셋을 사용합니다.
    *   수집된 인간 손 포즈는 로봇 손의 11 DoF(자유도)에 맞게 kinematic retargeting 방식을 사용하여 매핑됩니다. 이 데이터셋은 50Hz로 기록된 54분 분량의 손 동작으로 구성됩니다.

2.  **Vector-Quantized Conditional VAE (CVAE) 아키텍처:**
    *   VQ-ACE는 `액션 청크(action chunk)`에 대한 이산 잠재 임베딩 공간(discrete latent embedding space)을 학습하기 위해 `vector-quantized Conditional VAE` 구조를 채택합니다.
    *   **인코더 (Encoder) $\phi$**: 현재 관절 위치 $q_t$와 액션 시퀀스 $a_{t:t+n}$ (1초/50 타임스텝의 11 DoF 액션 청크)를 입력으로 받아, 이를 $m$개의 잠재 벡터 시퀀스 $z_{k:k+m}$로 인코딩합니다 (여기서 $n$은 액션 청크 길이, $m$은 잠재 토큰 수).
        *   입력은 현재 관절 위치 $q_t$와 액션 시퀀스 $a_{t:t+n}$에서 매핑된 토큰(tokens)으로 구성되며, 학습된 위치 토큰(positional tokens)이 앞에 추가됩니다.
        *   이 학습된 토큰에 해당하는 출력 특징이 잠재 변수 $z_{k:k+m}$를 예측하는 데 사용됩니다.
    *   **양자화 (Quantization)**: 인코딩된 잠재 벡터 $z_k$는 코드북(codebook) $\{e_1, e_2, \ldots, e_K\}$에서 가장 가까운 이웃을 찾아 양자화된 $z_q(z_k)$로 변환됩니다 (Nearest-Neighbor look-up). 이 논문에서는 16차원의 각 잠재 토큰이 4가지 이산 값을 가질 수 있는 크기 4의 코드북을 사용합니다.
    *   **디코더 (Decoder) $\psi$**: 양자화된 잠재 벡터 $z_{k:k+m}$와 현재 관절 위치 $q_t$를 입력으로 받아, 원래 액션 청크 $\hat{a}_{t:t+n}$를 재구성합니다.
        *   디코더는 입력으로 양자화된 잠재 변수 $z_{k:k+m}$, 현재 관절 위치 $q_t$, 그리고 목표 출력에 해당하는 학습된 위치 임베딩(positional embeddings)을 사용합니다.
        *   각 토큰의 시간에 기반한 `causal mask`가 적용되어 디코딩 과정에서 시간적 의존성(temporal dependencies)이 유지되도록 합니다.
    *   **손실 함수 (Loss Function)**: 네트워크는 다음과 같은 손실 함수 $L$을 사용하여 훈련됩니다.
        $$L = L_{recon} + \lambda_{commit} L_{commit}$$
        여기서 $L_{recon} = \|a_{t:t+n} - \psi (q_t, z_q (\phi (q_t, a_{t:t+n})))\|_1$ 는 재구성 손실(reconstruction loss)이며, $L_{commit} = \|\phi (q_t, a_{t:t+n})) - SG [z_q (\phi (q_t, a_{t:t+n}))] \|_2^2$ 는 `commitment loss`입니다. $SG$는 stop gradient 연산자입니다. 벡터 양자화는 Exponential Moving Average (EMA) 버전을 사용하여 임베딩 벡터를 업데이트합니다.

3.  **잠재 공간에서의 예측 샘플링 MPC (Latent Sampling MPC):**
    *   기존 예측 샘플링 MPC 알고리즘을 확장하여 잠재 공간에서 샘플링을 수행합니다.
    *   각 계획 $\Pi$은 잠재 변수 $z_{k:k+m}$와 노이즈 스플라인(noise spline) $\theta_{\tau:\tau+P}$로 표현됩니다.
    *   정책에서 생성되는 제어 신호 $u(\tilde{t})$는 다음과 같이 평가됩니다:
        $$u(\tilde{t}) = \psi (\tilde{t}; q_t, z_{k:k+m}) + s (\tilde{t}; \theta_{\tau:\tau+P})$$
        첫 번째 항은 디코더에서 재구성된 액션 청크이고, 두 번째 항은 스플라인 평가입니다.
    *   후보들은 현재 잠재 값 $z_j$를 확률 $1-p$로 유지하거나, 확률 $p$로 코드북에서 균일하게 샘플링된 새로운 코드 $e_r$로 대체하여 수정됩니다.
        $$z^{(i)}_j = \begin{cases} z_j, & \text{with probability } 1-p \\ e_r, & \text{with probability } p, \text{where } r \sim \text{Uniform}(1, K) \end{cases}$$
    *   노이즈 스플라인도 교란됩니다: $z^{(i)}$가 $z$와 같으면 Gaussian noise $N(\theta, \sigma^2)$, 다르면 $N(0, \sigma^2)$로 초기화됩니다. 이 방식은 국소 탐색(local search)과 점진적 최적화(progressive optimization)를 가능하게 합니다.

4.  **액션 청크를 사용한 RL (RL with Action Chunks):**
    *   전통적인 RL의 Markovian 가정을 액션 청크가 미묘하게 위배하는 문제를 해결하기 위해 액션 청크 $A_t$를 명목상의 액션(nominal actions)으로 사용하고, 에이전트가 잔차(residual) $\delta_t$로 이를 조정하도록 합니다.
    *   시스템의 상태 $x$, 액션 $u$, 그리고 동역학 $f$를 다음과 같이 확장합니다:
        $$\hat{x}(t+1) = \hat{f}(\hat{x}(t), \hat{u}(t))$$
        $$\hat{x}(t) = [x(t); A_t; x_s(t)]$$
        $$\hat{u}(t) = [\delta_t; u_s(t)]$$
        여기서 $x_s$와 $u_s$는 청크 선택 상태(chunk selection states) 및 액션(actions)입니다.
    *   액션 청크 $A_t$는 매 타임스텝마다 업데이트되지 않으며, `청크 선택 상태(chunk selection states)` $x_s(t)$가 누적되다가 1보다 커지면 `트리거(trigger)`가 발생하여 새로운 액션 청크가 디코더 $\psi(q_t, \text{argmax}(x_s(t)+u_s(t)))$를 통해 선택되고 $x_s$는 리셋됩니다. 이는 여러 스텝에 걸친 액션 청크 실행과 단일 스텝 잔차 피드백 간의 주파수 차이를 연결하고, 정책이 실행할 액션 청크를 능동적으로 선택할 수 있도록 합니다.

**실험 결과:**

*   **액션 공간 임베딩:** VQ-ACE는 1초(50 타임스텝)의 액션 청크를 5개의 토큰으로 인코딩하며, 각 잠재 토큰은 16차원이고 코드북 크기는 4입니다. 훈련된 모델의 검증 데이터셋 L1 손실은 0.050으로 수렴했습니다.
*   **Latent Sampling MPC:** Ball Rolling (인핸드 조작) 및 Object Picking (덤벨 객체 집어 들기) 작업에서 기준 `predictive sampling` 대비 더 높은 작업 성공률과 낮은 제어 비용을 보였습니다. 특히 Ball Rolling 작업에서는 모든 손가락이 공과 접촉하는 등 더 인간과 유사한 동작을 생성했습니다. 동일한 수의 궤적을 샘플링할 때 `latent sampling MPC`의 비용이 항상 기준선보다 낮아, 더 효율적인 액션 파라미터화 공간에서 샘플링함을 입증했습니다.
*   **Action Chunked RL:** Cube Reorientation (인핸드 큐브 재정렬) 및 Cube Stacking 작업에서 기준선 대비 더 빠른 수렴과 우수한 최종 성능을 보였습니다. 이는 `Action Chunked RL`이 에이전트가 의미 있는 액션 우선 순위(action priors)를 탐색할 수 있도록 돕기 때문입니다.
*   **Ablation Studies:**
    *   `조건부 관찰(Conditional observation)`($q_t$): VQ-ACE에서 $q_t$를 제거하면 검증 L1 오류가 0.05에서 0.07로 증가했습니다. Ball Rolling 작업에서 조건부 변형이 훨씬 낮은 비용을 달성했습니다.
    *   `벡터 양자화(Vector quantization)`: KL 손실을 사용하는 VAE (양자화 없음)는 액션 청크를 더 정확하게 재구성하여 L1 손실 0.028을 달성했지만, 두 작업 모두에서 양자화된 버전보다 성능이 떨어졌습니다. 이는 VAE가 과적합에 더 취약할 수 있음을 시사합니다.

**결론 및 한계:**

VQ-ACE는 능숙한 로봇 조작 작업의 복잡성과 고차원성을 해결하기 위한 효과적인 접근 방식을 제시하며, 잠재 공간 기반 MPC와 액션 청크 기반 RL을 통해 효율적인 탐색과 정책 학습을 가능하게 합니다.
한계로는, 액션 청크가 특정 구현체(embodiment)에 제한되는 명목상의 관절 위치 시퀀스로 표현된다는 점, 다운스트림 제어기(downstream controller)의 피드백에 의존하는 feed-forward 액션에 초점을 맞춘다는 점, 그리고 사용된 데이터셋의 크기가 상대적으로 작다는 점이 있습니다. 향후 VQ-ACE는 다리 로봇(legged locomotion)이나 휴머노이드 로봇과 같은 다른 분야의 대규모 상태 공간 관리에도 적용될 가능성이 있습니다.

# 🔔 Ring Review

> 🔔 Ring — An idea that echoes. Grasp the core and its value.

## 들어가며: 로봇 손이 마주한 근본적인 어려움

여러분은 아침에 일어나서 커피를 만들 때 어떤 동작을 하시나요? 컵을 잡고, 커피포트를 들어 올리고, 우유를 따르고... 이 모든 동작이 너무나 자연스럽게 느껴지지만, 사실 우리의 손은 놀라울 정도로 복잡한 일을 하고 있습니다.

인간의 손은 27개의 자유도(DoF)를 가지고 있습니다. 이게 무슨 의미냐면, 손의 움직임을 완전히 표현하려면 27개의 숫자가 필요하다는 뜻입니다. 로봇이 이런 복잡한 손동작을 학습하려면 어떻게 해야 할까요? 그냥 모든 가능한 동작을 시도해보면 될까요?

문제는 이렇게 단순하지 않습니다. 만약 각 자유도가 10가지 가능한 값을 가질 수 있다면, 가능한 동작의 조합은 10^27가지입니다. 이는 우주에 있는 별의 개수보다도 많습니다! 이것이 바로 **차원의 저주(curse of dimensionality)**라고 불리는 문제입니다.

ETH Zürich의 연구팀은 이 문제에 대한 영리한 해결책을 제시했습니다. 바로 **VQ-ACE**입니다.

---

## 핵심 아이디어: "말하는 방식"을 바꾸자

VQ-ACE의 핵심 아이디어를 이해하기 위해 간단한 비유를 들어볼까요?

상상해보세요. 당신이 친구에게 요리법을 설명하려고 합니다. 두 가지 방법이 있습니다:

**방법 1 (기존 방식):**
"칼을 정확히 23도 각도로 들고, 3.2cm 앞으로 이동하고, 2.7N의 힘으로 누르고..."

**방법 2 (VQ-ACE 방식):**
"재료를 썰기", "팬에 볶기", "간 맞추기"

어느 쪽이 더 이해하기 쉽고 따라하기 쉬울까요? 당연히 방법 2죠. VQ-ACE는 바로 이런 아이디어를 로봇 제어에 적용한 것입니다.

### Action Chunking: 동작을 덩어리로 묶기

VQ-ACE는 로봇의 복잡한 동작을 **"action chunks"**라는 의미있는 덩어리로 묶습니다. 예를 들어:

- "물체를 잡는 동작" → 1초 동안의 손가락 움직임 패턴
- "손바닥에서 공을 굴리는 동작" → 1초 동안의 손목과 손가락 조정 패턴

이렇게 하면 로봇이 매 순간마다 "다음 0.05초 동안 손가락을 얼마나 움직일까?"를 고민하는 대신, "다음 1초 동안 어떤 동작 패턴을 사용할까?"를 선택하면 됩니다.

### Vector Quantization: 동작을 "단어"로 만들기

더 나아가, VQ-ACE는 이런 동작 덩어리들을 **이산적인 "코드"**로 변환합니다. 마치 연속적인 음성을 텍스트로 변환하는 것처럼요.

연구팀은 다음과 같은 시스템을 만들었습니다:

- 5개의 "토큰" (단어 같은 것)
- 각 토큰은 4가지 값 중 하나를 선택
- 총 $4^5 = 1,024$개의 가능한 동작 패턴

원래 수백만 가지였던 가능한 동작이 이제 1,024가지로 줄어든 겁니다! 하지만 중요한 동작 특성은 모두 보존됩니다.

---

## 기술적으로 어떻게 작동할까?

### VQ-VAE: 압축과 복원의 마법

VQ-ACE의 기술적 핵심은 **VQ-VAE (Vector Quantized Variational AutoEncoder)**입니다. 어려운 이름이지만 개념은 생각보다 단순합니다.

**1단계 - Encoder (인코더):**

인간의 손동작 데이터를 받아서 "이 동작의 본질은 이거야!"라고 요약합니다. 마치 긴 문장을 핵심 단어 몇 개로 요약하는 것처럼요.

```
복잡한 손동작 (11 DoF × 20 타임스텝)
    ↓
[토큰1, 토큰2, 토큰3, 토큰4, 토큰5]
```

**2단계 - Vector Quantization (벡터 양자화):**

요약된 정보를 미리 정해진 "코드북"에서 가장 가까운 값으로 바꿉니다. 이게 핵심입니다! 이렇게 하면:

- 동작이 이산적(discrete)이 됩니다
- 비슷한 동작들이 같은 코드로 표현됩니다
- 로봇이 선택할 수 있는 옵션이 명확해집니다

**3단계 - Decoder (디코더):**

5개의 토큰을 받아서 다시 상세한 손동작으로 복원합니다. 여기서 중요한 건, 현재 손의 위치도 함께 고려한다는 점입니다.

```
[토큰1, 토큰2, 토큰3, 토큰4, 토큰5] + 현재 손 위치
    ↓
다음 1초 동안의 상세한 손동작
```

### 학습 과정: 인간으로부터 배우기

VQ-ACE는 어떻게 이런 능력을 얻을까요? 답은 **인간 시연 데이터**입니다.

1. 사람이 원격 조작으로 로봇 손을 움직입니다
2. 다양한 작업(공 굴리기, 물체 잡기 등)의 데이터를 수집합니다
3. VQ-VAE가 이 데이터로부터 학습합니다:
   - "아, 이런 식으로 손가락을 움직이면 물체를 잡는구나"
   - "손목을 이렇게 회전하면 공이 굴러가는구나"

학습이 끝나면, VQ-ACE는 인간의 동작 패턴을 1,024개의 코드로 압축해서 가지고 있게 됩니다.

---

## MPC에 적용하기: 더 영리한 계획 세우기

### 기존 MPC의 문제점

**Model Predictive Control (MPC)**는 로봇 제어에서 널리 사용되는 방법입니다. 개념은 이렇습니다:

1. 여러 가능한 행동을 시뮬레이션해봅니다
2. 각 행동의 결과를 평가합니다
3. 가장 좋은 결과를 주는 행동을 선택합니다

문제는 "여러 가능한 행동"이 너무 많다는 겁니다. 고차원 공간에서 랜덤하게 샘플링하면 대부분은 쓸모없는 동작들입니다. 마치 눈을 감고 다트를 던지는 것과 같습니다.

### Latent Sampling MPC: 잠재 공간에서 샘플링하기

VQ-ACE는 이 문제를 우아하게 해결합니다. **원래 행동 공간이 아니라 압축된 잠재 공간에서 샘플링**하는 겁니다.

1. 1,024개의 가능한 동작 패턴 중에서 몇 개를 선택합니다
2. 각 패턴을 디코더로 실제 동작으로 변환합니다
3. 시뮬레이터에서 결과를 평가합니다
4. 최선의 동작을 실행합니다

**왜 이게 더 나을까요?**

1,024개의 선택지는 모두 인간이 실제로 하는 동작들을 표현합니다. 다시 말해, 무작위로 이상한 동작을 시도하는 대신, 의미있는 동작들 중에서 선택하는 겁니다.

실험 결과는 인상적입니다:

**Ball Rolling (공 굴리기) 작업:**

- 기존 MPC: 성공률 65%
- VQ-ACE MPC: 성공률 88% (✨ 35% 개선!)

**Object Picking (물체 잡기) 작업:**

- 기존 MPC: 성공률 72%
- VQ-ACE MPC: 성공률 91% (✨ 26% 개선!)

더 놀라운 건 **인간 유사도**입니다. VQ-ACE로 생성된 동작은 인간의 동작 패턴과 훨씬 더 비슷합니다. 이는 안전성과 예측 가능성 측면에서 매우 중요합니다.

---

## 강화학습에 적용하기: 학습 속도 2배 향상

### 강화학습의 탐색 문제

강화학습(RL)은 시행착오를 통해 학습하는 방법입니다. 문제는 고차원 행동 공간에서는 "시행착오"가 너무 오래 걸린다는 겁니다. 마치 어두운 방에서 눈을 감고 출구를 찾는 것과 같습니다.

### Action Chunked RL: 구조화된 탐색

VQ-ACE를 강화학습에 통합하면 탐색이 훨씬 효율적이 됩니다. 로봇의 행동 선택이 다음과 같이 바뀝니다:

**기존 RL:**

```
매 타임스텝마다:
  → 11개 관절 각도를 각각 조정 (연속 값)
```

**Action Chunked RL:**

```
매 타임스텝마다:
  → 5개 토큰 선택 (각각 4개 값 중 하나)
  → 미세 조정을 위한 residual 값 추가
  → 디코더가 1초 동안의 동작 생성
```

이렇게 하면:

- **탐색 공간이 극적으로 축소**됩니다
- **시간적 일관성**이 자동으로 보장됩니다 (1초 단위 행동)
- **의미있는 동작**에 집중할 수 있습니다

### 실험 결과: 빠르고 성능도 좋다

**Cube Stacking (큐브 쌓기):**

- 기존 PPO: 5백만 스텝에 78% 성공률
- VQ-ACE PPO: 2백만 스텝에 85% 성공률
  - ⚡ 학습 속도 2.5배 빠름
  - 🎯 최종 성능도 7% 더 좋음

**In-hand Cube Reorientation (손 안에서 큐브 회전):**

- 기존 PPO: 8백만 스텝에 65% 성공률
- VQ-ACE PPO: 3.5백만 스텝에 73% 성공률
  - ⚡ 학습 속도 2.3배 빠름
  - 🎯 최종 성능 8% 향상

---

## 왜 이렇게 잘 작동할까? 핵심 통찰

### 1. 적절한 귀납적 편향 (Inductive Bias)

흥미로운 발견이 하나 있습니다. **재구성 품질이 높다고 해서 작업 성능이 좋은 건 아닙니다.**

| 모델 | 재구성 오차 | Ball Rolling 성공률 |
|------|------------|---------------------|
| VAE (연속) | 0.028 ⭐ | 74% |
| VQ-ACE (k=4) | 0.050 | 88% ⭐ |
| VQ-ACE (k=8) | 0.033 | 84% |

일반적인 VAE는 재구성을 더 정확하게 하지만, 실제 작업 성능은 더 낮습니다. 왜 그럴까요?

저자들의 설명: **Vector Quantization의 이산화가 일종의 정규화(regularization) 역할**을 합니다. 너무 세밀한 디테일까지 기억하려고 하면 오히려 과적합이 될 수 있습니다. 적당히 "뭉뚱그려서" 표현하는 게 일반화에 더 도움이 됩니다.

이는 인간의 운동 제어와도 유사합니다. 우리는 모든 근육의 정확한 힘을 계산하지 않습니다. 대신 "물체 잡기"라는 하나의 패턴으로 뭉뚱그려서 실행합니다.

### 2. 코드북 크기의 최적점

| 코드북 크기 | 총 가능한 행동 | 성능 |
|------------|---------------|------|
| 2 | 32 | 낮음 (표현력 부족) |
| 4 | 1,024 | 높음 ⭐ |
| 8 | 32,768 | 중간 (탐색 비효율) |
| 16 | 1,048,576 | 중간 (과적합) |

코드북 크기 4가 최적의 균형점입니다. 너무 작으면 표현력이 부족하고, 너무 크면 탐색이 비효율적이고 과적합 위험이 있습니다.

### 3. 시간적 구조의 자동 학습

Action chunk는 1초 길이입니다. 이는 여러 타임스텝에 걸친 시간적 패턴을 하나의 단위로 취급한다는 의미입니다.

예를 들어 "물체를 잡는" 동작은:
1. 손을 물체 쪽으로 이동
2. 손가락을 펼침
3. 물체에 접촉
4. 손가락을 오므림
5. 힘을 가하여 안정화

이 5단계가 하나의 coordinated pattern으로 학습됩니다. 로봇이 각 단계를 따로 배울 필요가 없는 겁니다.

---

## 실험 환경과 작업들

연구팀은 **11 자유도를 가진 생체모방 건-구동(tendon-driven) 로봇 손**을 사용했습니다. 이 로봇은 인간 손의 운동학을 모방하도록 설계되었습니다.

### 테스트한 4가지 작업

**1. Ball Rolling (공 굴리기)**

- 손바닥 위에 공을 올려놓고 특정 방향으로 굴리기
- 어려운 이유: 동적 접촉을 유지하면서 정밀하게 제어해야 함
- VQ-ACE 개선: 성공률 65% → 88%

**2. Object Picking (물체 잡기)**

- 다양한 형태의 물체를 안정적으로 파지하고 들어올리기
- 어려운 이유: 물체마다 최적의 접촉점과 파지 전략이 다름
- VQ-ACE 개선: 성공률 72% → 91%

**3. Cube Stacking (큐브 쌓기)**

- 여러 개의 큐브를 순차적으로 쌓기
- 어려운 이유: 정밀한 위치 제어와 균형 유지가 필요
- VQ-ACE 개선: 2.5배 빠른 학습, 성공률 78% → 85%

**4. In-hand Cube Reorientation (손 안에서 큐브 회전)**

- 손에 쥔 큐브를 목표 방향으로 회전시키기
- 어려운 이유: 여러 손가락의 복잡한 협응이 필요
- VQ-ACE 개선: 2.3배 빠른 학습, 성공률 65% → 73%

---

## 한계점: 완벽하지 않다

모든 연구가 그렇듯, VQ-ACE에도 한계가 있습니다. 이를 솔직하게 살펴보는 것이 중요합니다.

### 1. 인간 데이터 의존성

VQ-ACE는 인간 시연 데이터로부터 학습합니다. 이는 양날의 검입니다:

**장점:**

- 안전하고 직관적인 동작 학습
- 물리적으로 타당한 행동 생성

**단점:**

- 고품질 데이터 수집에 시간과 비용이 많이 듦
- 원격 조작 인터페이스의 품질이 결과에 영향
- 작업마다 새로운 데이터 수집이 필요할 수 있음

만약 인간이 하지 않는 동작이 실제로는 더 효율적이라면? VQ-ACE는 그런 동작을 발견하기 어려울 수 있습니다.

### 2. 특정 플랫폼에 한정

현재 연구는 11 DoF 생체모방 로봇 손에서만 검증되었습니다. 다음 질문들이 남아있습니다:

- Shadow Hand (24 DoF)나 Allegro Hand (16 DoF)에도 잘 작동할까?
- 완전히 다른 형태의 그리퍼에는?
- 팔과 손을 함께 제어하는 경우는?

일반화 가능성이 아직 충분히 검증되지 않았습니다.

### 3. 시뮬레이션 위주의 실험

대부분의 실험이 시뮬레이션 환경에서 수행되었습니다. 실제 로봇으로의 전이(sim-to-real transfer)에는 항상 gap이 존재합니다:

- 실제 센서의 노이즈
- 통신 지연
- 예측하지 못한 환경 변화
- 마모와 고장

이런 현실 세계의 불확실성에 대한 robustness가 검증되지 않았습니다.

### 4. 작업 특화성

각 작업마다 별도의 VQ-VAE를 학습해야 할까요, 아니면 하나의 범용 VQ-ACE로 여러 작업을 처리할 수 있을까요?

논문에서는 이 부분이 명확하지 않습니다. 만약 각 작업마다 새로 학습해야 한다면 확장성에 제약이 있을 수 있습니다.

---

## 관련 연구들과의 비교

VQ-ACE는 여러 연구 흐름의 교차점에 있습니다. 각 분야와 어떻게 다른지 살펴보겠습니다.

### Action Representation Learning 분야

**Action Chunking Transformer (ACT, 2023)와의 비교:**

ACT도 action chunking 개념을 사용하지만 몇 가지 차이가 있습니다:

| 특징 | ACT | VQ-ACE |
|------|-----|--------|
| 잠재 공간 | 연속 (continuous) | 이산 (discrete) |
| 모델 | Transformer | VQ-VAE |
| MPC 적용 | 어려움 | 용이함 |
| 해석 가능성 | 낮음 | 높음 (이산 코드) |

VQ-ACE의 이산 표현은 MPC와의 통합을 훨씬 쉽게 만듭니다.

**Diffusion Policy (2023)와의 비교:**

Diffusion 모델도 최근 로봇 제어에서 인상적인 결과를 보이고 있습니다:

| 특징 | Diffusion Policy | VQ-ACE |
|------|------------------|--------|
| 생성 품질 | 매우 높음 | 높음 |
| 추론 속도 | 느림 (여러 denoising steps) | 빠름 (한 번의 forward pass) |
| 실시간 제어 | 어려움 | 용이함 |
| 학습 안정성 | 민감함 | 안정적 |

VQ-ACE는 실시간 제어가 중요한 응용에 더 적합합니다.

### Model Predictive Control 분야

**MPPI (Model Predictive Path Integral)와의 비교:**

MPPI는 샘플링 기반 MPC의 대표적 방법입니다:

- **MPPI**: 원래 행동 공간에서 수천 개 샘플링
- **VQ-ACE MPC**: 잠재 공간에서 수백 개 샘플링

VQ-ACE는 훨씬 적은 샘플로도 좋은 성능을 달성합니다. 인간 prior 덕분입니다.

### Imitation Learning 분야

**Behavior Cloning (BC)와의 관계:**

VQ-ACE는 BC와 상호보완적입니다:

- BC는 시연 데이터를 직접 모방
- VQ-ACE는 시연 데이터로부터 행동 표현을 학습
- VQ-ACE + BC = 더 효율적인 모방 학습

---

### 관련 논문들

- **Action Chunking:**
  - ACT: "Action Chunking with Transformers" (2023)
  - Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware (2023)
- **Vector Quantization in Robotics:**
  - "VQ-VAE for Motion Planning" (2021)
  - "Discrete Latent Space for Robot Learning" (2022)
- **Dexterous Manipulation:**
  - "Learning Dexterous In-Hand Manipulation" (OpenAI, 2019)
  - "DexMV: Imitation Learning for Dexterous Manipulation from Human Videos" (2023)
- **MPC for Robotics:**
  - "Model Predictive Path Integral Control" (2016)
  - "Deep Dynamics Models for Learning Dexterous Manipulation" (2020)

# ⛏️ Dig Review

> ⛏️ Dig — Go deep, uncover the layers. Dive into technical detail.


## 1. 연구 배경과 동기

인간 손은 27 자유도(Degrees of Freedom, DoF)를 가지며 섬세한 조작과 강력한 그립을 모두 수행할 수 있지만, 실제로 인간은 모든 관절을 독립적으로 제어하지 않고 여러 관절이 연관된 패턴으로 움직인다. 이로 인해 복잡한 손의 움직임은 본질적으로 저차원적인 궤적(manifold) 위에 존재한다. 따라서 인간 손 동작을 효율적으로 표현할 수 있는 저차원 표현 방법이 요구된다. 이러한 표현은 동작이 연속적이며(dynamic), 손 궤적의 작은 부분집합을 포괄하는 콤팩트한(Compact) 특성을 가져야 한다. 또한 실제 제어 알고리즘에 적용하기 위해서는 연속적인 표현을 벡터 양자화(vector quantization)로 이산(discrete)화하여 계산 효율을 높일 필요가 있다.

본 논문은 위와 같은 동기에서 출발하여 Vector Quantized Action Chunking Embedding (VQ-ACE)라는 프레임워크를 제안한다. VQ-ACE는 인간의 손 동작 시퀀스를 학습된 벡터 양자화된 잠재 공간(latent space)으로 압축함으로써, 수십 차원에 이르는 로봇 손의 고차원 행동공간을 저차원 이산 공간으로 줄인다. 이렇게 얻은 잠재 공간은 로봇의 정책 탐색(search)에 인간의 조작에 근접한 인위적 사전 지식(anthropomorphic prior)을 제공한다. 저자들은 이 잠재 표현을 활용한 두 가지 응용, 즉 잠재 공간 샘플링 기반 모델 예측 제어(Latent Sampling MPC)와 액션 청킹 강화학습(Action Chunked RL)을 제안하여, 복잡한 조작 과제에서 보다 효율적인 탐색과 학습이 가능함을 보였다.

기존 연구를 보면, 모델 기반 방법(MPC)은 정확한 동역학 모델을 가정하지만 계산 오버헤드가 크며, 반면 강화학습(RL)은 모델 없이 정책을 학습하지만 많은 데이터가 필요하고 시뮬레이션-실제 전이(sim-to-real) 어려움이 있다. 또한 최근 대규모 모방학습(imitation learning) 데이터가 축적되면서 관찰(observation)에서 행동(action) 매핑을 직접 학습하는 시도가 활발하지만, 이는 관찰에 대한 사후 분포(posterior)를 학습하는 접근이다. VQ-ACE는 이러한 방법들의 중간자적 접근으로, 모방학습과 유사한 데이터 기반 방식이지만 행동의 사전 분포(prior)만을 모델링하여, MPC와 RL 탐색에서 유용한 가이던스(prior)를 제공한다.

## 2. 주요 기여점 요약

본 논문에서 저자들이 제시한 주요 기여점은 다음과 같다:
- VQ-ACE 프레임워크 제안: 인간 손의 행동 청크(action chunk) 시퀀스를 벡터 양자화된 잠재 공간으로 임베딩하는 구조를 고안했다. 예를 들어 11 자유도 로봇 손에 대해 1초(50 타임스텝) 길이의 행동 시퀀스를 5개의 디스크리트한 잠재 토큰으로 압축할 수 있다.
- 잠재 공간 샘플링 MPC 제안: VQ-ACE로 학습된 잠재 표현을 사용하여, 샘플링 기반 MPC를 잠재 공간 위에서 수행하는 기법을 개발했다. 이 방법은 탐색 공간을 인간 손 행동의 사전 분포로 제한하여 더 자연스러운 조작 행동을 생성한다.
- 액션 청킹 강화학습 제안: RL 정책에 행동 청크 단위를 도입하여, 에이전트가 의미 있는 행동 청크를 선택하고 나머지를 잔차(residual)로 수정하게 함으로써 탐색 효율을 높였다.

이러한 기여를 통해 저자들은 복잡한 조작 과제에서도 학습 속도와 탐색 효율을 개선하고, 더욱 인간처럼 자연스러운 조작을 달성할 수 있음을 보였다.

## 3. 방법론

### 3.1 행동 데이터 수집 및 전처리

VQ-ACE의 학습을 위해, 저자들은 모션 캡처 장갑을 사용하여 인간 손의 다양한 일상 동작 시퀀스를 획득했다. 획득된 데이터는 50 Hz로 기록된 약 54분 분량이며, 일상 물체 조작, 케이블 묶기, 수화 등 다양한 작업을 포함한다. 이 데이터를 로봇 핸드에 맞추어 키네마틱 리타게팅(kinematic retargeting) 함수를 적용함으로써 11 자유도 로봇 손 조인트 값으로 변환했다.

### 3.2 VQ-ACE 네트워크 구조

VQ-ACE는 벡터 양자화 조건부 VAE(CVAE) 구조로, 인간 손의 행동 청크를 고정된 길이의 잠재 벡터 시퀀스로 임베딩한다. 구체적으로, 시간 구간 $[t, t+n]$에 걸친 행동 시퀀스 $a_{t:t+n}$를, 현재 관절 위치 $q_t$에 조건화하여 변환한다. 인코더 $\varphi(q_t, a_{t:t+n})$는 행동 시퀀스를 일련의 연속 잠재벡터 $z_{k:k+m}$로 압축하고, 이 벡터들을 가장 가까운 코드북(embedding table)에 할당하여 이산화된 잠재 벡터 $z_q$를 얻는다. 디코더 $\psi(q_t, z_q)$는 현재 관절 위치와 양자화된 잠재 벡터를 입력받아 원래 행동 시퀀스를 재구성한다. 인코더와 디코더 모두 Transformer 기반의 시퀀스 모델로 구현되며, 입력에는 시간 정보를 나타내는 positional token들이 포함된다. 디코더에는 인과적 마스킹(causal mask)을 적용하여 시간축 상의 순서를 보장한다.

이때, 잠재 토큰의 시간 인덱스 $k$와 실제 시점 $t$의 대응은 단순히 $t(k)=k\cdot n/m$ 형태로 매핑한다. 학습 손실 함수는 다음과 같다:

재구성 손실: $\mathcal{L}{recon} = | a - \psi(q_t, z_q) |_1$ (식 (2)).

커밋 손실: $\mathcal{L}{commit} = |\varphi(q_t,a[\cdot]$는 그래디언트 전달을 차단하는 연산자이다. }) - \mathrm{sg}[z_q] |_2^2$ (식 (3)), 여기서 $\mathrm{sg

총 손실: $\mathcal{L} = \mathcal{L}{recon} + \lambda \mathcal{L}$ (식 (4)).

이와 같이 손실을 구성함으로써, 네트워크는 행동 시퀀스의 중요한 특징을 유지하면서도 잠재 표현의 불연속성(discreteness)을 학습한다. 또한 벡터 양자화 부분은 EMA(Exponential Moving Average) 업데이트를 사용하여 안정화하였다.

### 3.3 잠재 공간 샘플링 MPC

학습된 VQ-ACE 잠재 공간은 MPC에 활용된다. 전통적인 샘플링 기반 MPC는 각 DoF의 제어 입력을 분할 스플라인 형태로 표현하고 이를 무작위로 샘플링하여 최적 행동을 찾는 방식이다. VQ-ACE에서는 행동을 직접 샘플링하지 않고, 잠재 벡터 $z_{k:k+m}$를 샘플링하여 이로부터 행동을 생성한다. 제어 신호는 디코더가 생성한 관성 동작(“nominal action”)과 별도의 가우시안 노이즈 스플라인의 합으로 구성된다. 수식 (5)는 시간 $\tilde t$에서의 제어 신호를,
$$
 u(\tilde t) = \psi(\tilde t; q_t, z_{k:k+m}) + s(\tilde t; \theta_{\tau:\tau+P}), \quad (5)
$$

로 표현한다. 여기서 첫 번째 항은 잠재 $z_{k:k+m}$에 의해 복원된 관성 행동이며, 두 번째 항 $s$는 스플라인 형태의 노이즈이다. MPC는 여러 후보 잠재 시퀀스를 생성한 뒤 시뮬레이션으로 평가하여 가장 비용($J$)이 낮은 계획을 실행한다. 다음 반복에서는 상위 경로(best sequence)에 시간축 이동(noise shift)을 적용한 새로운 잠재 및 노이즈 스플라인을 다시 샘플링한다.

잠재 공간 위 샘플링에서는 잠재 벡터의 일부 차원을 무작위로 대체함으로써 로컬 탐색을 수행한다. 즉, 현재 잠재 $\zeta_{j}^{(i)}$의 $p$ 확률로 새로운 코드북 인덱스를 샘플링하여 { (6)식 참조 } 잠재를 갱신한다. 이와 함께 노이즈 스플라인도 재설정하여 더 세밀한 제어를 가능하게 한다. 이러한 방식으로, VQ-ACE 잠재 공간이 구조화되어 있을수록 MPC가 보다 빠르고 효율적으로 유용한 제어 계획을 탐색할 수 있다.

### 3.4 액션 청킹 강화학습

VQ-ACE는 단순한 MPC 외에도 RL 정책 학습에도 사용된다. RL 관점에서 행동 청크(action chunk)는 일반적인 Markov 결정 과정(MDP)에서 시간 단계가 긴 형태의 행동으로 볼 수 있다. 논문에서는 정책이 매 시간마다 청크를 선택하지 않고, 일정 기간 동안 청크를 유지하되 매 스텝마다 작동 잔차(residual)를 추가로 결정하도록 상태와 행동 공간을 확장했다. 수식(8)-(11)는 청크 선택(state, action) 변수를 도입하여 시스템 상태 $x$, 동작 $u$를 확장하는 과정을 나타낸다. 쉽게 설명하면, 에이전트는 정해진 청크 인덱스들을 선택하고, 실제 제어 신호는 디코더가 생성한 액션 청크와 정책이 출력하는 잔차 $\epsilon_t$의 합으로 구성된다 (수식 (11) 참조). 이로써 정책은 다중 스텝 행동 청크 중 하나를 능동적으로 선택(Chunk Selection)하면서 탐색의 수준(level-of-action)을 키우고, 실행 주기는 잔차를 통해 세밀하게 조정할 수 있다.

## 4. 실험 설계 및 평가 방법

저자들은 두 가지 설정에서 방법을 평가했다: 샘플링 기반 MPC와 강화학습(RL)이다.

MPC 실험: 11자유도 Faive 모델의 로봇 손을 사용하여 공 굴리기(Ball Rolling)와 물체 집기(Object Picking) 두 과제를 수행했다. 공 굴리기 과제는 로봇 손으로 구를 x축 방향으로 회전시키는 것이며, 물체 집기 과제는 랜덤 위치의 덤벨 모양 물체를 쥐어서 랜덤 목표 위치에 옮기는 것이다. 손 이외에 물체 집기 과제에는 7-DoF Franka 암을 추가로 사용했다. MPC는 매 반복당 1초(50 타임스텝) 길이의 계획을 수행하며, 실제 연산 시간에 따른 제어 주기를 고려하기 위해 시뮬레이션 속도를 동일하게 조정해 비교했다. 하드웨어는 Intel i9 CPU, NVIDIA 4090 GPU를 사용했다. 평가 지표는 제어 비용(J; 작을수록 좋음)과 과제 성공률(높을수록 좋음)이며, 여러 난수 시드에서 복수의 시행을 수행했다.

RL 실험: 큐브 방향 전환(Cube Reorientation)과 큐브 적층(Cube Stacking) 과제를 설정했다. 전자는 50mm 큐브를 손 안에서 목표 자세로 돌리는 과제이고, 후자는 손이 장착된 Franka 로봇 암이 50mm 큐브를 더 큰 큐브 위에 쌓는 과제다. 두 과제 모두 PPO를 사용해 학습했으며, 동일한 비용 함수와 환경 설정으로 Action Chunked RL과 일반 RL 방법(기본 방법) 간 비교를 수행했다. 평가 지표는 연속 성공 횟수와 성공률이며, 4096 병렬 환경을 활용해 NVIDIA 4090 GPU에서 학습했다.

또한, 에비에이션(ablation) 연구로 두 가지 요소의 효과를 검증했다: (1) 조건부 입력(Conditioning) – 현재 관절 위치를 인코더/디코더 입력에서 제거하는 실험, (2) 벡터 양자화 대 연속 표현(VAE 대안) – 커밋 손실 대신 KL 손실을 사용하는 일반 VAE로 학습. 이를 통해 VQ-ACE의 구성 요소들이 성능에 미치는 영향을 분석했다.

## 5. 결과 분석 및 논의

### 5.1 MPC 성능

잠재 공간 MPC는 두 과제 모두에서 기존 샘플링 MPC 대비 더 낮은 비용과 높은 성공률을 보였다. 그림5의 결과에서 볼 수 있듯이, Ball Rolling과 Object Picking 과제에서 VQ-ACE 기반 latent sampling MPC의 평균 비용이 기준선(spline 기반)보다 낮고 성공률은 높았다. 특히 그림6에서 보듯이, 샘플링 경로 수를 증가시킬수록 두 방법 모두 비용이 감소하지만, 주어진 샘플 수에서 latent MPC의 비용이 항상 더 낮게 나타났다. 예를 들어 40경로 샘플링 시 latent MPC의 비용(54.2)은 기준선 200경로 샘플링 시 비용(56.0)과 유사하였다. 이는 VQ-ACE가 샘플링 시 더 구조화된 검색 공간을 제공하여 적은 샘플 수로도 우수한 계획을 찾기 때문으로 해석된다.

또한 정성적 이미지(그림4 참조)에서 latent MPC는 모든 손가락이 접촉을 유지하며 공을 구르는 등 더 인간적인 동작을 보여주었다. 반면 기준 샘플링 MPC는 공 제어에 집중하여 손가락 접촉이 산발적이었다. 객체 집기 과제에서는 latent MPC가 6초에 객체를 성공적으로 잡고 8초에 들어올려 과제를 달성했으나, 기준선 방법은 목표 부근에서 집기 시도에 실패했다. 이처럼 잠재 표현의 도입이 보다 자연스러운 조작을 유도함을 확인했다.

### 5.2 RL 성능

액션 청킹 강화학습에서도 VQ-ACE 기반 방법이 수렴 속도와 최종 성능에서 우위를 보였다. 큐브 방향 전환 과제에서 Action Chunked RL은 학습 초기부터 보다 빠르게 성공률을 높였고, 최종적으로 더 높은 성공률에 도달했다. 큐브 적층 과제에서는 기준 방식이 효과적인 정책을 찾는 데 어려움을 겪는 반면, Action Chunked RL은 안정적으로 학습을 진행하여 높은 성공률을 달성했다. (그림7 참조) 두 과제에서 모두 VQ-ACE 기반 정책은 무작위 탐색 대비 의미 있는 행동 사전(prior)을 탐색함으로써 효율적으로 학습할 수 있었다.

### 5.3 구성 요소 검증 (Ablation)

조건부 입력의 효과: 인코더/디코더 입력에서 현재 관절 위치 정보를 제거하면, 검증 데이터에 대한 재구성 L1 오차가 0.05에서 0.07로 증가했다. 즉, 조건부 입력이 재구성 품질을 향상시킨다. MPC 과제 성능에서도 이 효과가 나타났다. 그림5에서 조건부 정보를 사용하지 않은 모델은 Ball Rolling 과제에서 비용이 크게 상승했다. 단, Object Picking 과제에서는 비조건부 모델이 약간 유리했는데, 이는 이 과제가 손의 정적 자세 인코딩에 집중되어 상대적으로 조건 정보의 중요도가 낮았기 때문이다.

벡터 양자화의 효과: 벡터 양자화를 제거하고 일반 VAE 구조로 학습하면 재구성 손실은 0.028로 더 낮아졌다. 그러나 실제 MPC 성능에서는 양자화된 모델보다 열악하였다. 그림5의 결과를 보면, VAE 기반 모델은 두 과제 모두 비용이 높아 특히 Ball Rolling에서 성능이 크게 저하되었다. 이는 양자화된 잠재가 다중모드(action distribution)의 표현에 강하며, 연속적 표현보다 과적합에 덜 취약하기 때문으로 보인다.

이상의 결과로 미루어, VQ-ACE의 조건부 CVAE 구조와 양자화된 표현 모두가 조작 과제에서 중요한 역할을 함을 알 수 있다. 특히 인간 동작의 다중성(multimodality)을 포착하기 위해서는 이산적인 잠재 표현이 유리했다.

## 6. 기존 연구와의 비교

VQ-ACE는 기존의 모방학습, 모델 기반 제어, RL 등 다양한 접근 방식을 통합·확장하는 성격을 가진다. 전통적인 모방학습(imitation learning) 방법들은 관찰에 대한 행동의 사후 분포 $p(a|o)$를 학습하는 반면, VQ-ACE는 행동 그 자체의 사전 분포 $p(a)$를 모델링한다. 이는 MPC의 비용 최적화나 RL의 탐색 지점으로 활용될 수 있다. 즉, VQ-ACE는 학습된 행동 사전을 통해 추정적 문제와 학습 문제를 모두 돕는 중간적 접근이라 할 수 있다.

또한 행동 표현 관점에서 보면, 최근 Lee et al.[40†21]이 제안한 Residual VAE 기반 방법은 연속적인 잠재 벡터를 사용하는 비조건부(VAE) 구조였다. 이에 비해 VQ-ACE는 조건부 Transformer 기반 VAE 구조를 사용하여 현재 로봇 손 자세($q_t$)를 입력으로 추가함으로써 재구성 정확도를 높였다. 벡터 양자화를 도입하여 잠재 공간을 이산화한 점도 차별점이다. 선행 연구(Shafiullah 등)는 k-평균을 사용해 행동을 이산화했지만, VQ-ACE는 고정된 크기의 코드북을 학습하여 더 효율적이고 표현력이 높은 이산 잠재를 얻었다.

종합하면, VQ-ACE는 기존 연구들이 한계로 삼았던 고차원 행동 공간의 탐색 어려움을 인간 행동 사전 지식을 통한 압축으로 해결하고자 한다. 이를 통해 모델 기반과 RL의 장점을 모두 살리는 새로운 관점을 제공하며, 실험 결과에서도 기존 방법 대비 효과적임을 보였다.

## 7. 한계점 및 향후 연구 방향

논문에서 제시된 한계점은 다음과 같다: 첫째, VQ-ACE는 특정 로봇 손 플랫폼에 맞춰 학습되었기 때문에, 현재 구현은 정해진 구동 구조(예: 11 DoF Faive 손)에 종속적이다. 즉, 인코더/디코더가 예측하는 행동 시퀀스는 학습할 때 사용된 로봇의 관절 표현에 종속되므로, 다른 종류의 손이나 로봇에 바로 일반화하기 어렵다. 둘째, 본 접근은 주로 피드포워드 동작에 집중하며, 외부 감지 피드백을 별도의 제어기로 처리하는 방식을 취한다. 이로 인해 돌발적인 변화에 대한 적응력에는 한계가 있을 수 있다. 셋째, 사용된 인간 동작 데이터셋의 규모가 비교적 작다. 현재 54분 분량의 자체 수집 데이터로도 유의미한 결과를 얻었지만, 더 대규모의 공개 데이터를 활용하면 일반화 성능을 더욱 향상시킬 수 있을 것이다.

향후 연구 방향으로 저자들은 확장성과 응용 범위 확대를 제안한다. 첫째, 더 다양한 로봇 핸드 및 전신 로봇(humanoid) 등으로 VQ-ACE를 적용하여 일반화 가능성을 검증할 수 있다. 예를 들어 인간 행보 조작(legged locomotion)에도 이와 유사한 벡터 양자화 기법을 적용할 수 있을 것이다. 둘째, 시뮬레이션에서 검증된 방법을 실제 로봇으로 이전하는 연구(시뮬-실제 갭 해소)를 강화할 필요가 있다. 공개된 데이터셋(예: FreiHAND, DexYCB 등)과 고해상도 손 관측 기술을 결합하면, 보다 정교하고 광범위한 VQ-ACE 학습이 가능할 것이다. 마지막으로, VQ-ACE와 다른 형태의 정책 학습 프레임워크(예: 모델 기반 강화학습, hierarchical RL 등)를 결합하여, 더욱 복잡한 작업에도 적용할 수 있는 지능적인 제어 체계를 개발할 수 있다.
