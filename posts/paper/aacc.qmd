---
draft: true
title: "ğŸ“ƒAACC"
description: Asymmetric Actor-Critic in Contextual Reinforcement Learning
date: "2023-07-31"
categories: [context, rl, paper]
toc: true
number-sections: true
---

ì´ë²ˆ í¬ìŠ¤íŒ…ì€ DeepMindì—ì„œ ë°œí‘œëœ [AACC: Asymmetric Actor-Critic in Contextual Reinforcement Learning](https://arxiv.org/abs/2208.02376) ë…¼ë¬¸ì„ ì½ê³  ì •ë¦¬í•œ ë‚´ìš©ì…ë‹ˆë‹¤. 

# Abstrct 

Reinforcement Learning (RL) techniques havedrawn great attention in many challenging tasks,but their performance deteriorates dramaticallywhen applied to real-world problems. Variousmethods, such as domain randomization, have beenproposed to deal with such situations by trainingagents under different environmental setups, andtherefore they can be generalized to different envi-ronments during deployment. However, they usu-ally do not incorporate the underlying environmen-tal factor information that the agents interact withproperly and thus can be overly conservative whenfacing changes in the surroundings. In this paper,we first formalize the task of adapting to changingenvironmental dynamics in RL as a generaliza-tion problem using Contextual Markov DecisionProcesses (CMDPs). We then propose the Asym-metric Actor-Critic in Contextual RL (AACC) asan end-to-end actor-critic method to deal with suchgeneralization tasks. We demonstrate the essen-tial improvements in the performance of AACCover existing baselines experimentally in a rangeof simulated environments

# Discussion & Conclusion


# Reference

- [Original Paper: Asymmetric Actor-Critic in Contextual Reinforcement Learning](https://arxiv.org/abs/2208.023762)