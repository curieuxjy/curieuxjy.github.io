---
title: "📃Human2Sim2Robot 리뷰"
date: 2025-09-11
categories: [real2sim, sim2real, rl]
toc: true
number-sections: true
description: Crossing the Human-Robot Embodiment Gap with Sim-to-Real RL using One Human Demonstration
---

- [Paper Link](https://arxiv.org/abs/2504.12609)
- [Homepage](https://human2sim2robot.github.io/)
- [Code Link](https://github.com/tylerlum/human2sim2robot)

1. 로봇의 숙련된 조작 기술 학습을 위해 단 하나의 RGB-D 인간 시연 영상을 활용하는 real-to-sim-to-real RL 프레임워크인 HUMAN2SIM2ROBOT을 제안합니다.
2. 이 프레임워크는 시연 영상에서 객체 포즈 궤적을 추출해 객체 중심적이고 embodiment-agnostic한 보상 함수를 정의하고, 조작 전 손 포즈로 RL 훈련의 효율적인 초기화 및 탐색을 안내하여 human-robot embodiment gap을 극복합니다.
3. HUMAN2SIM2ROBOT은 단일 인간 시연 환경에서 grasping, non-prehensile manipulation, multi-step tasks를 포함한 다양한 실제 작업에서 기존 방법 대비 55%에서 68% 이상 높은 성공률을 달성하며, 별도의 reward tuning 없이 zero-shot sim-to-real 전이를 가능하게 합니다.



---

# Brief Review

이 논문은 로봇이 능숙한 조작 기술(dexterous manipulation skill)을 학습하기 위해 기존 방법론들의 한계를 해결하는 `HUMAN2SIM2ROBOT`라는 새로운 real-to-sim-to-real 프레임워크를 제안합니다. 기존에는 로봇 조작 기술을 가르치기 위해 수백 개의 시연(demonstration) 데이터가 필요했으며, 이는 웨어러블 센서(wearable sensor)나 텔레오퍼레이션(teleoperation)을 통해 수집되어 비용이 많이 들고 확장하기 어려웠습니다. 인간-객체 상호작용(human-object interaction) 비디오는 쉽게 수집할 수 있지만, 명시적인 로봇 액션 라벨(action label)이 부족하고 인간-로봇 간 신체적 구현 차이(embodiment gap)로 인해 직접 로봇 학습에 활용하기 어려웠습니다. 이 논문은 단 하나의 RGB-D 인간 시연 비디오만으로 강건한 로봇 정책(robot policy)을 학습하는 방법을 제시하며, 특히 복잡한 보상 설계(reward engineering)나 대규모 데이터 수집 없이도 인간-로봇 간의 신체적 구현 차이를 극복하는 데 중점을 둡니다.

제안하는 방법론의 핵심은 단일 인간 시연 비디오에서 두 가지 주요 구성 요소를 추출하는 것입니다.

첫째, **객체 6D 자세 궤적(object 6D pose trajectory)**입니다. 이는 `FoundationPose` 모델과 `Segment Anything Model 2 (SAM 2)`를 사용하여 비디오 프레임별로 객체의 자세를 추정하여 얻어집니다. 이 객체 자세 궤적은 로봇의 신체적 특징에 구애받지 않는(embodiment-agnostic) 객체 중심 보상 함수를 정의하는 데 사용됩니다. 보상 함수는 다음과 같이 정의됩니다: $$r_{obj_t} = \exp^{-\alpha d(T^{target}_{\tau + t}, T^{obj}_t)}$$, 여기서 $d(T_1, T_2) = \sum^{N_{anchor}}_{i=1} \|T_1 k_i - T_2 k_i\|$는 두 객체 자세 $T_1$, $T_2$ 사이의 상대적인 자세 거리를 앵커 포인트(anchor point) $k_i$를 이용하여 측정하며, $\alpha=10$은 스케일링 계수입니다. 이 보상은 로봇이 목표 객체 자세 궤적을 정확하게 따라가도록 유도합니다.

둘째, **단일 조작 전(pre-manipulation) 손 자세(hand pose)**입니다. 이 자세는 객체가 움직이기 시작하는 시점($t_0$)에서 약간 앞선 시점($\tau = t_0 - t_{offset}$)의 인간 손 자세를 추출하여 얻습니다. 인간 손 자세는 `HaMeR` 모델로 추정된 MANO 파라미터($\theta_t, \beta_t$)를 기반으로 하며, 깊이 정보(depth information)와 ICP 등록(registration)을 통해 3D 정확도를 높입니다. 이렇게 추출된 인간 손 자세는 `cuRobo`를 이용한 2단계 역운동학(Inverse Kinematics, IK) 절차를 통해 로봇의 손 자세로 리타게팅(retargeting)됩니다. 1단계에서는 로봇 팔의 관절 각도를 조절하여 로봇 손의 중간 너클(middle knuckle) 위치와 방향을 인간 손의 중간 너클에 정렬하고, 2단계에서는 로봇 손의 관절 각도를 조절하여 로봇 손가락 끝(fingertip) 위치를 인간 손가락 끝에 정렬합니다. 이 조작 전 손 자세는 강화 학습(Reinforcement Learning, RL) 훈련 시 이점을 주는 초기 상태(initial state)를 제공하여 탐색(exploration) 효율성을 높이는 데 사용됩니다.

정책 학습은 `IsaacGym` 시뮬레이터에서 `Proximal Policy Optimization (PPO)` 알고리즘을 사용하여 진행됩니다. 로봇은 `KUKA LBR iiwa 14` 팔에 `Allegro Hand`가 장착된 셋업을 사용하며, 제어 액션은 기하학적 패브릭 컨트롤러(geometric fabric controller)와 PCA 기반 손 액션 공간(`$x_{palm-target_t}, r_{palm-target_t}, x_{pca-target_t}$`)을 사용합니다. 시뮬레이션에서 실제 환경으로의 제로샷(zero-shot) 전이를 위해 도메인 무작위화(domain randomization)를 광범위하게 적용하고, LSTM 기반 정책(policy)을 사용하여 부분적 관측성(partial observability)과 노이즈가 있는 데이터를 처리합니다.

실험 결과, `HUMAN2SIM2ROBOT`은 단일 인간 시연만으로도 로봇 조작 과제에서 기존 베이스라인(replay, object-aware replay, behavior cloning)보다 55%에서 68% 이상 높은 성공률을 달성했습니다. 특히, 객체 자세 궤적을 활용한 밀집 보상(dense reward)과 조작 전 손 자세를 활용한 초기화는 RL 정책 학습의 속도와 안정성, 최종 성능에 결정적인 역할을 함이 입증되었습니다. 이 프레임워크는 잡기(grasping), 비잡기 조작(non-prehensile manipulation), 다단계 조작(multi-step tasks) 등 다양한 실제 로봇 조작 과제에서 효과적인 성능을 보였습니다.

---

# Detail Review

> 인간-로봇 체화 간극을 넘어서: 한 번의 인간 시연으로 Sim-to-Real 강화학습

## 배경: 인간 시연을 활용한 로봇 학습의 도전

로봇에게 섬세한 물체 조작(dexterous manipulation)을 가르치는 전통적인 방법 중 하나는 모방 학습(imitation learning, IL)입니다. 그러나 효과적인 모방 학습을 위해서는 사람이 로봇을 직접 조종(텔레오퍼레이션)하거나 특수 장비(예: 센서 장갑, VR 장치 등)를 착용해 다수의 고품질 시연 데이터를 수집해야 하는데, 이러한 과정은 매우 많은 노력과 시간이 필요합니다. 실제 연구 사례들을 보면, 하나의 작업에 수백에서 수천 개에 이르는 시연 데이터가 요구되었으며, 이는 로봇 학습의 확장성 측면에서 큰 걸림돌이었습니다.

한편, 사람이 맨손으로 물체를 다루는 영상은 비교적 쉽게 얻을 수 있고, 일반인도 직관적으로 시연할 수 있다는 장점이 있습니다. 예를 들어 스마트폰 카메라로 일상 동작을 녹화하는 것만으로도 시연 데이터를 모을 수 있습니다. 그러나 이러한 인간-객체 상호작용 비디오를 로봇 학습에 직접 활용하기는 어렵습니다. 가장 큰 이유는, 비디오에는 로봇이 따라 할 명시적인 행동 레이블(어떤 로봇 명령을 수행해야 하는지)이 없으며, 더구나 인간 손과 로봇 손의 형태 차이(embodiment gap) 때문에 사람 손동작을 로봇에 그대로 매핑하기도 힘들기 때문입니다. 사람 손에 비해 로봇 손은 관절 구성과 운동 한계가 다르므로, 사람의 동작을 똑같이 흉내 내는 것은 종종 불가능하거나 비효율적입니다.

기존 접근법 중 하나로, 인간 손 포즈 추정 기술을 사용해 비디오의 매 프레임마다 사람 손의 3D 관절각을 추출하고, 이를 로봇 손의 관절각으로 재타겟팅(retargeting)하여 로봇 행동으로 변환하려는 시도가 있었습니다. 그러나 이 방법은 두 가지 큰 문제에 직면합니다. 첫째, 최근 3D 손 추적 기법들이 발전했음에도 불구하고, 일반 RGB-D 영상에서는 손가락이 물체에 가려지거나 센서 노이즈가 존재하여 완벽한 추정을 보장하기 어렵습니다. 미세한 추정 오류도 누적되면 로봇 손가락 끝의 목표 위치가 어긋나게 되고, 로봇 관절 명령으로 변환할 때 부정확한 결과를 초래합니다. 둘째, 설령 인간 손의 포즈를 정확히 알아내더라도, 사람과 로봇의 형태 차이 때문에 해당 포즈를 로봇 손으로 구현하지 못하거나(역기구학 해가 없음) 그 작업에 부적절한 로봇 자세가 되는 경우가 많습니다. 특히 여러 손가락으로 동시에 힘을 가하는 다지(多指) 조작에서는, 부정확하거나 불안정한 재타겟팅 동작이 접촉 힘 불균형을 일으켜 작업 수행을 어렵게 만듭니다. 요컨대, 인간 시연과 로봇 행동 간에 정확한 1:1 대응을 강요하는 기존 모방 학습 방식으로는 사람-로봇 체화 차이를 극복하기 어렵고, 한두 개의 시연으로는 성공 확률이 매우 낮았습니다.

강화학습(Reinforcement Learning, RL)은 이러한 문제를 해결할 대안으로 주목받아 왔습니다. RL은 로봇이 스스로 시행착오를 겪으며 자기 몸체에 맞는 최적의 행동을 학습하도록 하기 때문에, 인간과 로봇의 차이를 직접 메우는 대신 로봇 고유의 정책을 찾아낼 수 있습니다. 하지만 RL 역시 현실 로봇에 바로 적용하기엔 난관이 있습니다. 원하는 작업마다 적절한 보상 함수를 설계해야 하고, 많은 시행횟수가 필요해 물리 로봇으로 시도하기엔 비현실적이죠. 이를 위해 시뮬레이션 환경에서 학습하고 실제에 옮기는 Sim-to-Real 기법이 발전해 왔지만, 보상 설계의 어려움과 시뮬레이터와 현실 간 차이를 줄이는 문제 등이 남아있습니다.

이러한 배경에서 2025년 발표된 "Crossing the Human-Robot Embodiment Gap with Sim-to-Real RL using One Human Demonstration" 논문은, 인간 시연 한 개만으로 로봇의 다지 조작 정책을 학습시키는 혁신적인 프레임워크 Human2Sim2Robot을 제안합니다. 이 방법은 한 번의 인간 RGB-D 비디오 시연으로부터 핵심 정보를 추출하여 시뮬레이터 상에서 RL로 정책을 학습하고, 이를 제로샷(zero-shot)으로 실환경에 이전함으로써 사람-로봇 체화 간극을 극복합니다. 놀랍게도 이 과정에서 별도의 웨어러블 장치나 텔레오퍼레이션 데이터, 수십 개의 시연 모음이 전혀 필요하지 않으며, 작업별로 보상을 일일이 튜닝할 필요도 없습니다. 본 리뷰에서는 Human2Sim2Robot의 핵심 기술 내용을 깊이 있게 설명하고, 기존 연구들과의 차별점을 비교하며, 이 접근법의 기여와 향후 로봇공학/강화학습 분야에 주는 의미를 분석하겠습니다.

## Human2Sim2Robot 프레임워크: 한 개 시연으로 Sim-to-Real 정책 학습

<center>
<img src="../../images/2025-09-11-human2sim2robot/splash.gif" width="90%" />
</center>

> Human2Sim2Robot 프레임워크 개요. 이 시스템은 현실-시뮬레이션-현실(real-to-sim-to-real)로 이어지는 RL 학습 과정을 통해, 인간 시연 하나만으로도 로봇 다지 조작 정책을 배웁니다. 사람의 RGB-D 시연 영상에서 (1) 물체의 6-자유도(6-DoF) 자세 궤적과 (2) 조작 시작 직전의 손 자세를 추출하여, 이를 시뮬레이션 강화학습에 활용합니다. 시뮬레이터 안에서 로봇은 해당 물체 궤적을 따라가도록 훈련되며(보상 설계), 초반 탐색 단계에서는 인간 시연에서 얻은 손의 초기 자세를 사용해 효율을 높입니다. 이렇게 학습된 정책은 도메인 랜덤화 등을 통해 현실에 바로 적용 가능하도록 일반화되어 있으며, 실제 Kuka 7자유도 로봇팔과 Allegro 다지 로봇손으로 구성된 시스템에서 추가 튜닝 없이 곧바로 성공적으로 동작했습니다. 아래에서는 이 프레임워크의 각 구성 요소를 단계별로 자세히 살펴보겠습니다.

### 인간 시연 처리: 객체 궤적 추출과 손 포즈 재현

Human2Sim2Robot의 첫 단계는 단 하나의 인간 시연 영상을 기계가 이해할 수 있는 형태로 가공하는 것입니다. 논문에서는 RGB-D 카메라(깊이 정보를 포함한 카메라)를 이용해 테이블 위에서 사람이 목표 작업을 수행하는 단일 시연을 녹화하였습니다. 이때 카메라는 내부 파라미터(intrinsics)와 위치가 미리 보정(calibration)되어 있어, 영상 픽셀 좌표를 물리 좌표로 정확히 변환할 수 있는 상태입니다.

(a) 디지털 트윈 환경 구성: 시연 영상을 분석하기에 앞서, 연구팀은 로봇이 학습할 시뮬레이션 환경을 현실과 유사하게 만드는 작업을 수행했습니다. 이를 위해 휴대용 3D 스캐닝 앱(예: Kiri Engine, 3D Scanner App)을 이용하여 작업에 사용되는 물체와 장면(환경)의 정밀한 3D 메쉬(mesh) 모델을 얻었습니다. 단 몇 분 정도의 스캔 노력으로, 테이블 위 정적 환경(박스, 냄비, 건조대 등)과 대상 물체(예: 상자, 물컵, 접시)의 형상을 모두 복제한 디지털 트윈을 구축한 것입니다. 이렇게 해두면 시뮬레이터에서 현실과 동일한 지오메트리로 학습시킬 수 있어, Sim-to-Real 간 차이를 줄이는 데 도움이 됩니다.

(b) 물체 6D 자세 궤적 추출: 시연 영상으로부터 객체의 움직임 경로를 정확히 알아내는 것이 핵심입니다. 논문에서는 이를 위해 FoundationPose라는 사전 학습된 객체 자세 추정 모델을 활용했습니다. 입력으로는 앞서 획득한 물체의 3D 메쉬 모델과, 영상 각 프레임에서의 물체 마스크(물체가 차지하는 픽셀 영역)가 주어집니다. 물체 마스크는 최신 분할 모델인 SAM (Segment Anything Model) 2를 이용해 자동으로 얻었습니다. FoundationPose는 이러한 정보를 바탕으로, 시연 내내 물체의 6-자유도(평면 위치 x-y, 높이 z, 롤-피치-요 각도) 자세를 추적합니다. 그 결과 각 시간 스텝마다 물체의 위치와 방향으로 이루어진 궤적 ${O}_{1:T}$를 얻을 수 있습니다 (T는 시연 길이). 이 객체 자세 궤적은 사람이 이 작업에서 물체를 어떻게 이동시켰는지에 대한 목표 시나리오를 정의해주며, 로봇에게는 무엇을 해야 하는지 알려주는 역할을 하게 됩니다.

(c) 인간 손 조작 초기 자세 추출: 다음으로, 한 번의 시연 중에서 "조작 직전"의 인간 손 모양과 위치를 포착합니다. 연구진은 오픈소스 모델 HaMeR를 사용해 매 프레임의 인간 손 3D 포즈(손가락 관절 각도 및 손바닥 위치)를 추정했습니다. HaMeR는 RGB 영상으로부터 손 관절을 추정하는 모델인데, 더 정확도를 높이기 위해 깊이 영상 정보도 활용했습니다. 구체적으로, 각 프레임의 RGB로 추정한 손 모델을 ICP (Iterative Closest Point) 정합 방법으로 해당 깊이 점들과 정교하게 맞추어, 손가락 위치를 보정하였습니다. 이렇게 얻은 인간 손 포즈 시퀀스 ${H}_{1:T}$ (MANO 손 모델 파라미터 형태로 표현됨)에서, 논문은 특별히 pre-manipulation hand pose, 즉 조작 시작 직전 손 자세를 찾아냅니다. "조작 시작" 시점을 찾기 위해 물체 궤적을 활용하는데, 물체의 속도가 처음으로 일정 임계값을 넘는 시점 $t$을 찾고, 그보다 약간 이전 시간 $t-\Delta$ (예컨대 몇 프레임 전)을 조작 직전 순간으로 정의합니다. 이 시점에는 손이 막 물체를 잡기 시작하기 바로 직전의 상태일 것입니다. 해당 프레임에서의 인간 손 관절 각도들과, 손바닥의 기준점(논문에서는 중지의 첫 마디 관절 부분)을 추출하면 이것이 인간의 조작 시작 자세입니다. 다시 말해, 사람이 물체에 접촉하기 직전 어떻게 손가락을 벌리고 어떤 위치에 손을 놓았는지를 나타냅니다.

(d) 로봇 손으로 초기 자세 재타겟팅: 이제 추출된 인간 손의 pre-manipulation 자세를 로봇 손의 초기 자세로 옮겨야 합니다. 사람과 로봇의 손은 형태가 다르므로 단순 일대일 대응으로는 힘듭니다. 논문에서는 이를 위해 두 단계 IK(역기구학) 알고리즘을 사용했습니다. 우선, 인간 손에서 뽑은 중지(knuckle) 관절의 위치와 방향에 최대한 맞도록 로봇팔과 손목을 이동시킵니다. KUKA 팔의 관절들을 움직여 로봇 손의 중지 관절이 인간 손 중지 관절 위치로 오게 하고, 손바닥 면의 방향도 정렬시키는 것입니다 (아주 약간의 오프셋은 고려했다고 합니다). 다음으로, 로봇 손가락 관절들을 움직여 로봇 손가락 끝이 인간 손가락 끝 포인트와 각각 최대한 겹치도록 만듭니다. 예를 들어 인간의 엄지, 검지, 중지 끝 위치에 로봇의 해당 손가락 끝이 오도록 로봇 손의 각도를 조정합니다. 이러한 2단계 IK를 통해 로봇의 pre-manipulation 손 자세를 구하게 되는데, 이는 로봇팔의 손목 위치/자세와 로봇손의 관절각 전체를 포함하는 구성입니다. 이 방법은 비교적 간단한 절차이지만, 인간 손의 초기 포즈를 로봇에 충분히 근사하게 옮겨주어 로봇 입장에서 자연스러운 초기 조건을 만들어냅니다. 더욱이 이 과정은 한 순간의 자세만 맞추면 되므로, 앞서 말한 프레임별 재타겟팅에서 발생하는 누적 오류 문제를 피할 수 있습니다.

(e) 두 가지 핵심 추상화: 요약하면, Human2Sim2Robot은 한 개의 인간 시연에서 (1) 물체의 목표 이동 경로와 (2) 로봇의 초깃자세라는 두 가지 핵심 정보를 추출한 것입니다. 이들은 사람 시연의 복잡한 시공간 데이터를 로봇 학습에 유용한 방식으로 추상화한 것이라 볼 수 있습니다. 물체 자세 궤적은 작업에 대한 객체 중심의 목표를 정의해 주며, 이는 로봇과 인간 손의 차이에 독립적(embodiment-agnostic)인 목표로서 사용됩니다. 즉, 로봇이 사람과 다른 방식으로 움직이더라도, 결과적으로 이 궤적대로 물체를 옮기기만 하면 작업을 성공한 것으로 볼 수 있다는 뜻입니다. 한편 pre-manipulation 로봇 손 자세는 학습 과정에서 로봇에게 유리한 초기 조건을 제공합니다. 이를 통해 로봇은 탐색 초기부터 인간이 사용한 유사한 물체 파지(grasp) 자세에서 시작할 수 있고, 따라서 학습 정책이 자연스럽게 시연과 비슷한 전략으로 수렴하도록 돕습니다. 중요한 점은 이 두 가지 정보가 지침(guidance) 역할을 할 뿐, 우리가 로봇에게 인간의 매 순간 동작을 강요하지 않는다는 것입니다. 논문의 표현에 따르면, 시연은 "과업 명세와 지도 (task specification and guidance)"로만 사용되고, 로봇은 인간처럼 행동하는 것이 유리할 때는 그렇게 하도록 유도되지만, 로봇 자신의 형상에 맞지 않을 때는 과감히 다른 동작으로 우회할 수 있게 허용됩니다. 이러한 설계 철학 덕분에, 사람-로봇 체화 차이가 큰 경우에도 로봇이 독자적으로 최적 행동을 찾는 자유도가 보장됩니다.

### 시뮬레이션에서의 정책 학습과 보상 설계

이제 준비된 디지털 트윈 시뮬레이션 환경에서 강화학습을 통해 로봇 조작 정책(policy)을 학습합니다. 학습에는 NVIDIA의 IsaacGym 시뮬레이터를 사용했으며, 앞서 스캔한 장면 메쉬(테이블 및 고정 배경)와 물체 메쉬를 불러와 현실과 동일한 물리 환경을 구축했습니다. 로봇 모델 역시 실제와 동일한 7-자유도 KUKA 팔과 16-자유도 Allegro 로봇 손으로 설정되었습니다[30]. 요약하면, 시뮬레이터 속에 현실 실험실의 축소판을 만든 셈입니다 (그림 6 참조).

(a) 강화학습 알고리즘: 정책 학습에는 Proximal Policy Optimization (PPO) 알고리즘을 활용하였습니다. 로봇 정책은 반복적인 에피소드 훈련을 거쳐 업데이트되며, 최종적으로 로봇 관절 제어 명령을 출력하는 신경망으로 얻어집니다. 정책의 관찰 상태로는 로봇 자체의 관절 상태(프로프리오셉션)와 실시간 물체의 6D 자세가 주어집니다. 이는 실제 배치 시와 동일하게, 카메라를 통해 추정된 물체의 위치/자세를 입력으로 사용함을 의미합니다. 초기 학습 시에는 에피소드마다 로봇의 초기 자세를 앞서 구한 pre-manipulation 로봇 손 자세로 리셋(reset)하여 시작합니다. 이렇게 하면 로봇이 초반부터 물체를 잡을 준비가 된 상태에서 탐색을 시작하므로, 맨 처음부터 엉뚱한 곳을 탐색하느라 시간을 허비하지 않습니다. 연구에 따르면 이 초기화 전략이 RL 학습의 안정성과 속도를 높여주며, 후에 비교 실험에서도 더 나은 성능을 보였습니다.
(b) 보상 함수 설계: Human2Sim2Robot의 중요한 특징 중 하나는 작업별 보상 함수를 추가로 설계/튜닝하지 않았다는 것입니다. 대신 하나의 일반적 보상 함수를 모든 작업에 사용했는데, 그것이 바로 "객체 궤적 추적 보상"입니다. 구체적으로, 시뮬레이션에서 매 시점의 보상 $r_t$를 로봇이 물체를 목표 궤적에 얼마나 잘 맞게 따라가고 있는가로 정의합니다. 수식으로는 물체의 현재 자세와 인간 시연에서 그 시점에 해당하는 목표 자세 사이의 거리(distance)를 측정하여, 거리가 작을수록 높은 보상을 주는 형식입니다. 여기서 거리란 단순한 유클리드 거리 개념을 넘어서, 물체의 위치와 방향 오차를 동시에 반영할 수 있도록 고안되었습니다. 논문에서는 물체의 내부 기준 프레임에 여러 개의 기준 점(anchor point)을 두고, 현재 물체와 목표 물체 사이 각 기준점의 상대 위치 차를 계산하는 방식을 사용했습니다. 예를 들어 물체의 표면 모서리나 중심 등에 점을 찍어두고, 로봇이 물체를 들고 움직일 때 이 점들이 목표 위치에 얼마나 가까이 갔는지로 보상을 주는 것입니다. 이렇게 하면 물체의 회전 오차도 점들의 배치에 따라 자연스럽게 거리로 반영됩니다. 만약 어떤 축으로 회전이 중요한 물체라면 그 축 방향으로 거리를 크게 느껴지도록 점을 배치하고, 반대로 회전 대칭적인 물체라면 불필요한 방향 오차는 무시하도록 점을 배치할 수도 있습니다. 이러한 기준점 설정은 물체 유형에 따라 한 번 정해주면 되고, 본 연구에서는 대부분의 작업에서 동일한 파라미터를 사용하여 특별한 보상 튜닝 없이도 잘 동작함을 보였습니다. 그림 4에서는 이 객체 궤적 추적 보상의 개념도를 시각화하고 있는데, 간단히 말해 물체의 현재 자세가 시연 궤적 어디 쯤 와 있는지를 측정하여 그에 따라 보상을 주는 체계입니다.
이 보상 설계의 장점은 RL 연구에서 흔히 겪는 과업별 보상 설계의 어려움을 크게 줄였다는 점입니다. 연구진은 “특정 물체가 회전 대칭인 경우를 제외하고는, 단 하나의 보상 공식과 파라미터로 매우 다양한 작업들을 모두 학습시킬 수 있었다”라고 강조합니다. 이는 본 방법의 범용성을 보여주는 대목으로, 새로운 작업마다 보상을 새로 만들 필요 없이 인간 시연 궤적만 있으면 동일한 방법으로 학습 가능함을 시사합니다. 실제로 논문에서 실험한 작업들(아래 설명될 접시 꽂기, 상자 피벗 등)에 대해 추가적인 보상 조정 없이도 모두 성공적인 정책을 얻을 수 있었습니다.
(c) 인간 시연을 통한 간접 학습: 앞서 언급했듯, 이 RL 학습은 인간의 시연 동작 자체를 모방하는 것이 아니라 시연을 통해 정의된 목표(물체 궤적)를 로봇 방식으로 달성하도록 하는 것입니다. 사람과 로봇의 움직임은 다를 수 있지만, 결과적으로 물체가 동일한 경로를 따라 움직이면 성공으로 간주됩니다. 논문에서도 "우리는 인간 시연 행동을 흉내 내기보다, 물체의 움직임에 관심을 둔다"라고 명시하고 있습니다. 인간의 pre-manipulation 포즈는 초기에 대략적인 가이드로만 쓰이고, 최종 정책은 로봇의 체형에 맞게 자율적으로 최적화됩니다. 예를 들어 사람이 물체를 잡을 때 오른손으로 집었다고 해서 로봇도 꼭 같은 방향, 같은 손가락 순서로 잡아야 하는 것은 아닙니다. 로봇은 자기 관절 구조상 더 편리한 방식이 있다면 그렇게 해도 좋고, 대신 물체 경로만 맞추면 되는 것이죠. 이렇듯 인간 시演은 결과 중심의 스케치 역할만 하고, 구체적인 행동 구현은 RL이 알아서 찾아내도록 하는 것이 Human2Sim2Robot의 철학입니다. 이는 인간-로봇 형태 차이를 극복하는 핵심 아이디어로, 사람 전략을 맹목적으로 따라하기보다 참고만 함으로써, 오히려 성공률 높은 전략을 로봇이 스스로 발견할 수 있게 해줍니다.
(d) 도메인 랜덤화와 강인한 학습: 시뮬레이터에서 학습한 정책을 현실에 바로 적용하려면, 시뮬레이션-현실 간 갭(차이)을 줄여야 합니다. 이를 위해 논문은 학습 중 다양한 도메인 랜덤화(domain randomization) 기법을 사용했습니다. 구체적으로, 시뮬레이션에서 일정 단계마다 무작위로 환경과 로봇의 물리 파라미터를 변화시켰습니다. 예를 들어, 중력 값을 약간씩 바꾸거나, 마찰 계수나 물체 무게를 임의로 조정하고, 로봇 관절의 질량/감쇠/강성 등의 모수를 무작위 스케일링했습니다. 또한 매 시뮬레이션 스텝마다 5% 확률로 물체에 작은 무작위 힘(외부 충격)을 가하여, 예기치 않은 방해가 생기도록 했습니다. 관찰되는 물체 자세와 로봇 상태, 그리고 로봇의 출력 액션에도 센서 노이즈에 해당하는 가우시안 잡음을 추가하여, 카메라 오차나 제어 신호 오차도 견딜 수 있도록 만들었습니다. 이렇게 광범위한 랜덤화를 주면, 정책은 다양한 조건에서 물체를 궤적대로 옮기는 연습을 하게 되므로 현실에서 마주칠 변동에도 강건해집니다. 실제 논문에서도 이러한 랜덤화와 함께, 이미지 대신 6D 포즈를 관찰로 사용하는 낮은 차원의 상태 표현 등이 합쳐져 로봇 정책의 적응성과 강인성이 향상되어 제로샷 실환경 이전이 가능했다고 분석합니다.

### Zero-Shot Sim-to-Real: 학습된 정책의 실환경 수행

시뮬레이션에서 충분히 학습을 마친 정책은 별도 미세조정(fine-tuning) 없이 그대로 실제 로봇에 이식되어 테스트됩니다. 연구진은 학습된 정책을 물리 로봇에 배치하기 위해 다음과 같은 제어 파이프라인을 구성했습니다:

- 우선, 실험 환경에 ZED stereo camera(깊이 카메라)를 설치하여 실시간으로 물체의 6D 자세를 30Hz로 추적합니다. 여기에도 학습 때와 동일한 FoundationPose 모델을 사용하여, 물체의 실제 위치와 방향을 지속적으로 추정했습니다.
- 로봇 정책(신경망)은 이 물체의 현재 자세와 로봇의 관절 상태를 입력으로 받아들여, 15Hz의 주기로 다음 행동을 출력합니다. 정책의 출력 행동은 추상적인 명령(예: 원하는 속도나 가상 목표 위치 등)으로 볼 수 있습니다.
- 이 출력은 바로 로봇 관절로 보내는 대신, Geometric Fabric Controller라는 중간 제어기로 전달됩니다. Geometric Fabrics는 로봇 운동을 매끄럽고 안정적으로 제어하기 위한 알고리즘으로, 정책이 내린 명령을 받아 60Hz로 로봇 관절의 목표 위치/속도 (PD 타겟)를 계산합니다. 이 단계는 로봇의 동역학을 고려하여 부드러운 경로를 생성함으로써, RL 정책이 바로 토크를 내보내는 것보다 안전하고 현실적이게 해줍니다.
- 마지막으로, 로봇 내장 저층 PD 제어기가 200Hz의 고주파수로 이러한 목표 값에 맞게 각 관절 모터에 제어를 가합니다. 요약하면, 정책은 고수준 의사결정을 하고, 중간 컨트롤러가 이를 구체적인 관절 명령으로 바꾸어 실제 로봇을 구동하는 체계입니다.

Human2Sim2Robot의 결과 중 주목할 점은, 이렇게 이식된 정책이 한 번도 로봇 실물로 연습해보지 않고도 (zero-shot) 성공적으로 작업을 수행했다는 것입니다. 이는 앞서 말한 시뮬레이션 도메인 랜덤화 덕분에 정책이 현실 물리의 변동성까지 포괄했기 때문입니다. 또한 시뮬레이터 자체를 실제와 매우 가깝게 (스캔된 환경, 정확한 물체 모델, 실제와 동일한 로봇 매니퓰레이터 모델) 구성한 것도 주효했습니다. 연구진은 KUKA LBR iiwa 14 팔과 Allegro Hand로 이루어진 실제 로봇 시스템에서, 학습된 정책을 바로 실행하여 높은 성공률을 얻었음을 보고했습니다. 이는 로봇공학에서 사람이 한 번 시연한 것을 보고 (추가 보정 없이) 로봇이 따라 해내는 학습된 행동의 실증적인 사례로서 큰 의미를 갖습니다. 아래에서는 구체적으로 어떤 작업들을 실험했고 어떤 성과가 있었는지 살펴보겠습니다.

### 다양한 조작 과제에서의 실험 결과

연구에서는 Human2Sim2Robot의 성능을 검증하기 위해 다양한 유형의 물체 조작 작업에 대해 실험을 진행했습니다. 실험 환경은 테이블 위에 정적인 장애물(상자, 큰 냄비, 접시 건조대 등)을 배치하고, 세 가지 이동 물체(스낵 박스, 주전자/피처, 접시)를 사용했습니다. 이 환경은 가정이나 일상에서 있을 법한 상황들을 축소 모사한 것으로, 테이블 위에서 물체를 집어 들어 올리거나, 밀거나, 젖혀서 세우거나, 다른 용기에 넣는 등의 시나리오를 포함합니다.

**단일 스킬 과제:** 우선 기본 조작 기술 단위의 과제들로 파지/이동(grasping), 비파지 조작(non-prehensile manipulation), 외력 활용 조작(extrinsic manipulation) 등의 사례가 선정되었습니다.

- 예 1: 스낵박스 밀기(push) – 얇은 직사각형 상자를 옆의 고정된 박스 쪽으로 밀어서 접촉시키는 과제 (밀기: non-prehensile).
- 예 2: 스낵박스 피벗(pivot) – 상자를 세워 세우기 힘드니, 고정된 박스 모서리를 지렛대 삼아 스낵박스를 옆으로 기울여 세우는 과제 (extrinsic manipulation, 환경을 활용).
- 예 3: 피처(주전자) 집어 올리기 – 손잡이를 잡고 들어올려 옆 냄비 위로 옮겨 물 따르듯이 위치시키는 과제 (prehensile grasp + 이동).
- 예 4: 접시 들어올리기 – 납작한 접시를 평면에서 잡아 들어올리는 과제 (prehensile, 난이도 높은 파지).
- 그 외에도 접시 밀기, 접시 세우기 등 다양한 단일 스킬 작업이 포함되었습니다.

**다중 단계 과제:** 더 나아가, 논문은 이러한 기본 기술을 연속적으로 두세 개 조합해야 하는 복합 작업에도 도전했습니다.

- 예 5: 스낵박스 푸시-피벗 – 상자를 먼저 밀어 박스에 붙인 후, 이어서 피벗 동작으로 세우는 2단계 작업.
- 예 6: 접시 피벗-리프트-랙 – 테이블에 놓인 납작한 접시를 먼저 벽(박스)에 기대 세운 후, 잡아서 들어올리고, 최종적으로 건조대 틀 사이에 끼워 세워 두는 3단계 작업. 이는 접시 세우기 → 접시 들어올리기 → 접시 꽂기 순서의 복합 과제입니다.
- 예 7: 접시 리프트-랙 – 접시를 바로 들어올려 건조대에 꽂는 2단계 작업 (세우지 않고 바로 들어올려 꽂기).

이러한 복합 시나리오는 인간에게도 여러 단계의 사고가 필요한 비교적 복잡한 작업입니다. Human2Sim2Robot에서는 각 복합 작업 역시 단 하나의 인간 시연 영상으로부터 학습되었습니다. 예컨대 접시 pivot-lift-rack 작업의 경우, 사람이 처음부터 끝까지 접시를 세워 잡아 옮겨 꽂는 일련의 동작을 한번 보여준 것이고, 로봇은 그 시연의 물체 궤적 (접시의 움직임 경로)에 맞춰 3단계 행동을 모두 해내야 하는 것입니다. 특히 접시처럼 넓고 얇은 물체는 바닥에 놓인 상태로는 집기 어려워 먼저 세워야 하고, 건조대에 끼울 때 각도를 잘 유지해야 하는 등 정교한 제어가 필요합니다. 이러한 복잡한 작업을 한 번의 시연으로 배우게 한 것은 본 연구의 난이도 높은 도전이었습니다.

**실험 결과:** 저자들은 Human2Sim2Robot으로 학습한 RL 정책을 위 7가지 실제 작업에 대해 실행하고, 성공률을 측정하였습니다. 각 작업마다 10회의 독립 실행을 시험한 결과, 제안된 방법이 매우 높은 성공률을 보였음을 보고합니다. 구체적인 수치는 작업별로 제시되진 않았지만, 모든 작업을 평균하여 볼 때 Human2Sim2Robot 정책은 대부분의 시도를 성공적으로 수행했습니다.

연구의 핵심 관심은 Human2Sim2Robot이 기존 방식보다 얼마나 우수한가이므로, 세 가지 대표적인 기존 접근법과 비교 실험이 이루어졌습니다. 비교 대상은 다음과 같습니다:

- Replay (오픈루프 재생): 인간 시연 영상의 모든 프레임마다 손 추정을 하고 로봇 포즈로 재타겟팅하여 얻은 로봇의 전체 관절 궤적을 그대로 따라 재생하는 방법입니다. 이는 일종의 open-loop(피드백 없음) 실행으로, 사람이 했던 그대로 로봇이 해보는 것입니다.
- Object-Aware Replay (객체 위치 보정 재생): 상기 Replay와 동일하나, 현실에서 초기 물체의 위치가 시연 때와 약간 다를 수 있으므로, 물체의 초기 변환 차이를 보정하여 로봇 궤적을 약간 이동/회전시키는 개선된 방법입니다. 이는 기존 연구인 PEP이나 OKAMI 등의 아이디어를 참고한 것으로, 시연 대비 물체 위치가 어긋나도 로봇이 궤적을 따라갈 수 있도록 합니다.
- Behavior Cloning (행동 복제): 폐루프 정책이긴 하나, RL이 아닌 지도학습 기반 모방학습으로 정책을 학습하는 방법입니다. 한 개의 시연만으로는 학습이 어려우니, 제안 방식에 맞추어 시연 하나를 증강하여 데이터셋을 구성했습니다. 물체 초기 위치를 여러 방식으로 무작위 샘플링하고, 그 상황에서 Object-Aware Replay를 실행하여 30개의 로봇 시연 데이터를 생성했습니다. 이것을 정책 학습용 데이터로 삼아, Diffusion policy와 같은 IL 알고리즘으로 행동 복제 학습을 한 것이 BC baseline입니다. 쉽게 말해, 한 개 시연을 여러 번 재배치해가며 데이터로 부풀린 후, 전통적 모방학습을 한 것입니다.

비교 결과, Human2Sim2Robot의 RL 정책은 이들 모든 기준선보다 현저히 높은 성공률을 기록했습니다. 평균적으로 볼 때, Human2Sim2Robot은 객체-인식 오픈루프 재생보다 55%포인트 높은 성공률을 보였고, 행동 복제보다 68%포인트 높았습니다. 가장 단순한 Replay와 비교하면 무려 67%포인트나 향상되었다고 합니다. 그림 8의 실험 결과 그래프를 보면, 거의 모든 작업에서 RL 정책이 압도적으로 높은 성공 비율을 달성했음을 알 수 있습니다.

이러한 큰 성능 차이의 이유에 대해 저자들은 상세한 분석을 제공합니다. 우선 Replay의 경우, 오픈루프로 사전에 녹화된 궤적을 그대로 따라가기 때문에 현실에서는 여러 실패 요인이 발생했습니다. 시연 시와 초기 조건이 조금만 달라도 (예: 물체 위치 오차, 마찰 차이 등) 보정이 없으면 로봇 동작은 빗나가게 마련이고, 한번 어긋나면 피드백이 없어 끝까지 실패하게 됩니다. Object-Aware Replay는 초기 위치는 보정했지만 여전히 개선되지 않은 세 가지 문제를 겪었습니다: (1) 앞서 논한 손 추정 및 재타겟팅 오류가 누적되어 정확도가 떨어지고, (2) 근본적인 사람-로봇 형상 차이로 인한 부조화가 있고, (3) 피드백이 없어 중간에 수정되지 못한다는 점입니다. 그나마 이 방법은 물체 위치 정도는 반영했기에, 정밀도가 크게 요구되지 않는 작업(예: 상자를 대략 밀어세우는 정도)에서는 부분 성공을 보이기도 했지만, 조금 복잡한 작업부터는 실패했습니다.

Behavior Cloning(BC)의 경우, 데이터 30개로 폐루프 정책을 학습했음에도 불구하고 결과가 저조했습니다. 이는 사용된 데이터 자체가 재타겟팅을 통해 생성된 것이라 품질이 낮고, 개수도 충분치 않기 때문입니다. 로봇 행동의 학습이 누적 오차에 취약하여, 데이터 분포에서 조금만 벗어나면 금세 잘못된 동작으로 이어지는 과적합 현상이 나타났습니다. 요컨대, 시연 한두 개로 행동을 모방 학습하는 데에는 근본적인 한계가 있음을 보여줍니다. 한편 Human2Sim2Robot의 RL 정책은 시연을 모방하지 않고 과업 목표를 성취하도록 학습되었기 때문에, 중간 상황이 달라져도 목표 지향적으로 적응하며 과업을 완수할 수 있었습니다. 예컨대 접시 세우기-꽂기와 같은 복잡한 작업에서, 모방 학습 기반 방법은 시연 때와 조금만 달라져도 실패했지만, RL 정책은 사람 시연의 전략을 응용하되 로봇 나름대로 세분화하여 접시를 끝까지 꽂아넣는 데 성공했습니다. 그림 9는 이러한 예시로, 접시 pivot-lift-rack 작업에서 로봇이 인간 시연을 참고하면서도 자기 손에 맞게 변형된 전략으로 과업을 수행하는 모습을 보여줍니다. 결국 Human2Sim2Robot은 인간처럼 행동하도록 강요하기보다, 인간의 힌트를 이용해 로봇 스스로 학습하게 함으로써 훨씬 높은 성능을 낼 수 있음을 증명한 것입니다.

또한 저자들은 각 구성 요소의 기여를 검증하기 위한 추가 실험(Ablation)도 수행했습니다. 예를 들어, 객체 궤적 보상 대신 단순히 최종 목표 위치까지만 가도록 하거나, 궤적을 희미하게 만드는 변형들을 비교했는데, 그 결과 본래의 세밀한 궤적 보상이 학습을 가장 빠르고 안정적으로 만들고 성공도 높였다고 보고합니다. 특히 접시와 같이 중간 단계를 필요로 하는 작업에서, 최종 목표만 주었을 경우 로봇은 지름길을 택하려다가 (접시를 바로 들어올리려다) 실패했지만, 인간 시연의 전체 경로를 보상으로 제시했을 때 비로소 피벗 후 집기라는 해법을 찾아냈습니다. 이는 인간 시연이 단순히 결과뿐만 아니라 과정상의 중요한 힌트를 제공하며, RL이 그것을 잘 활용하고 있음을 보여줍니다. 마찬가지로 초기 손 자세 안내가 없을 때와 있을 때도 비교한 결과, 초기 안내가 있으면 학습 초기 성능과 수렴 속도가 크게 향상되고 더 나은 최종 성과를 얻었습니다. 이러한 실험들은 Human2Sim2Robot 프레임워크의 구성 요소들이 각각 유의미한 역할을 하고 있음을 뒷받침합니다.

## 기존 연구와의 비교 및 차별점

Human2Sim2Robot은 "한 개의 인간 시연으로 다관절 로봇손 정책을 학습"했다는 점에서 매우 도전적인 목표를 달성한 최초의 사례로 꼽힙니다. 저자들의 주장에 따르면, 이렇게 적은 시연 데이터로 실제 로봇에 바로 실행 가능한 폐루프 정책을 만든 것은 본 연구가 처음입니다. 개별 구성요소 차원에서는 이전에도 유사한 시도가 없었던 것은 아닙니다. 예를 들어, 객체 중심 보상을 사용한 로봇 학습이나, 인간-로봇 포즈 재매핑을 시도한 연구들이 있습니다. 그러나 이들 기존 연구는 대개 시뮬레이션 안에 국한되었거나, 혹은 정책이 아닌 오픈루프 제어 수준에 머물렀으며, 더 나아가 다수의 시연이 필요하거나 단순 파지 동작에만 국한되는 등 한계가 있었습니다. 반면 본 논문은 이러한 요소들을 통합하면서도 실환경 폐루프 제어까지 구현하고, 비파지 및 다단계 작업으로 범위를 넓혔다는 데 큰 차별점이 있습니다.

**모방학습(IL) vs. Human2Sim2Robot:** 전통적인 IL은 앞서 설명했듯, 인간 시연과 로봇 행동 간 엄격한 매핑이 전제됩니다. 이는 시연 데이터가 많고 조건이 잘 맞으면 효과적이지만, Human2Sim2Robot이 다루는 시연 1개, 인간-로봇 형태 상이 상황에서는 적용이 사실상 불가능했습니다. IL을 억지로 적용하려 한 baseline 실험(Replay, BC 등)은 모두 낮은 성공률로 이를 방증합니다. Human2Sim2Robot은 "시연 따라하기" 대신 "시연 참고하여 RL로 배우기"라는 패러다임 전환을 보여줍니다. 특히 사람이 손가락 하나하나 어떻게 움직였는지까지 베끼려 하지 않고, 결과적인 객체 움직임만 학습 목표로 삼은 점은 사람-로봇 간극을 우회적으로 해소한 뛰어난 아이디어입니다. 그 결과, 사람의 전략을 부분적으로 모방하되 로봇 스스로 최적 행동을 찾게 함으로써, IL 방식이 겪는 오류 누적과 실패를 피할 수 있었습니다. 또한 IL은 일반적으로 많은 시연이 있어야 안정적인데, 본 방법은 하나의 시연으로도 충분하도록 설계되었다는 점에서 현실적인 장점을 갖습니다. 실제 현장에서 로봇에게 새 작업을 가르칠 때, 수십 번 텔레옵으로 반복하는 것보다 한 번 시범을 보여주는 것이 훨씬 수월할 것이기 때문입니다.

**유사 연구들과의 비교:** 최근 인간 비디오를 활용해 로봇 다지 조작을 배우려는 연구들이 몇 가지 등장했습니다. 예를 들어 2024년의 HuDOR (Human-to-Dexterous-Object-Rewards) 방법은 한 개의 인간 영상으로부터 물체 움직임을 추적하여 객체-지향 보상을 만들고, 이를 이용해 실제 로봇에서 온라인 RL 파인튜닝을 수행한 바 있습니다. HuDOR 역시 사람-로봇 형태 차이 문제를 인지하고, 물체 움직임에 집중한 보상으로 RL을 한 점은 유사합니다. 그러나 HuDOR는 실제 로봇을 1시간 가량 직접 학습시켜야 했고, 다루는 작업도 단일 단계 동작 위주였습니다. 반면 Human2Sim2Robot은 시뮬레이션 상에서 충분히 학습하고 현실에 제로샷 투입함으로써, 실제 로봇의 고된 온라인 학습을 피했습니다. 또한 더 다양한 작업 (훨씬 복잡한 다단계 작업 포함)에서 검증되었다는 점에서도 앞선 결과라 할 수 있습니다. 그 밖에 From One Hand to Multiple Hands (2022)라는 연구는 단일 사람의 시연을 여러 로봇 손에 일반화하는 모방학습을 다뤘지만, 이것은 텔레옵으로 수집된 다수의 시연을 전제하였고 Human2Sim2Robot처럼 RL을 활용하지는 않았습니다. X-Sim: Cross-Embodiment Learning via Real-to-Sim-to-Real (2025) 등의 동시대 연구들도 체화 차이 문제를 다루고 있으나, Human2Sim2Robot만큼 시연 데이터의 극단적 축소 (1개)와 실환경 검증을 모두 보여준 사례는 드뭅니다.

**베이스라인 기법과의 차별점:** 앞 절의 비교실험 결과가 잘 보여주었듯, Human2Sim2Robot의 RL 정책은 폐루프 제어로 환경 변화에 대응한다는 점에서 오픈루프 재생(Replay)과 근본적으로 다릅니다. 또한 사람 시연을 학습의 형태로 활용한다는 점에서, 단순히 시연 궤적을 조정해 사용하는 Object-Aware Replay와도 차별화됩니다. Behavior Cloning과 비교하면, 같은 데이터라도 강화학습을 통해 성능을 끌어올리고 오차 누적 문제를 해소했다는 점이 다릅니다. 요컨대, Human2Sim2Robot은 “시연 → 데이터 → 모방”의 기존 틀을 “시연 → 목표 추출 → RL 학습”으로 바꾸어 성능과 범용성을 모두 얻은 것이며, 이는 로봇 학습 커뮤니티에 새로운 방향을 제시합니다.

마지막으로, Human2Sim2Robot이 특별한 장비에 의존하지 않는다는 실용적 강점도 있습니다. 과거에는 모션 캡쳐 시스템이나 특수 센서 장갑 등 정밀한 장비로 사람 시연 데이터를 수집하곤 했지만, 본 논문은 그냥 RGB-D 카메라로 촬영한 영상만으로도 충분히 학습이 가능함을 보였습니다. 이는 데이터 수집 비용을 크게 낮추고, 비전문가도 로봇 학습에 참여할 수 있는 길을 열어줍니다. 또한 시연 영상을 통한 학습이 성공하려면 컴퓨터비전 기술 (물체 추적, 손 추적)이 필수적인데, 최근 몇 년간 해당 분야가 크게 발전했기에 (SAM, FoundationPose, HaMeR 등) 가능해진 것이기도 합니다. Human2Sim2Robot은 최신 CV 기술과 RL을巧妙하게 결합하여 얻은 결과로, 서로 다른 AI 하위 분야의 시너지를 보여주는 사례라고도 할 수 있습니다.

## 기여 및 의의: 로봇공학과 강화학습 분야에서의 가능성

Human2Sim2Robot의 기여는 여러 측면에서 주목할 만합니다. 우선, 로봇 학습 방식의 간소화와 인간 친화적 데이터 수집입니다. 오직 하나의 시연 영상으로 다관절 로봇손의 복잡한 조작을 익힐 수 있다는 것은, 로봇에게 새 기술을 가르치는 진입장벽을 크게 낮춥니다. 이제까지는 전문가가 긴 시간 공들여 데이터를 모아야 했다면, 앞으로는 일반 사람이 휴대폰으로 시범을 하나 보여주고도 로봇을 학습시킬 수 있는 방향으로 나아갈 수 있습니다. 이는 로봇의 학습 비용 절감과 적응력 향상으로 이어져, 가정용 서비스 로봇이나 작업장 협동 로봇 등에서 새로운 작업을 빠르게 가르치는 데 활용될 수 있습니다. 특히 집안일처럼 사람은 쉽게 시연할 수 있지만 로봇에 일일이 프로그래밍하기 어려운 작업들(예: 식기 세척기 적재, 옷 개어넣기 등)에 응용 가능성이 높습니다. 사람이 시연한 영상을 통해 로봇이 자율적으로 그 과업을 터득할 수 있다면, 로봇 활용 범위는 비약적으로 넓어질 것입니다.

둘째로, 이 논문은 강화학습과 모방학습의 장점을 결합한 새로운 프레임워크를 제시했습니다. 기존에는 시연을 이용하면 주로 BC같은 모방학습을 하고, 강화학습은 별도로 방대한 상호작용을 통해 배우는 식이었습니다. Human2Sim2Robot은 시演으로부터 보상과 초기조건을 구성하여 RL에 자연스럽게 녹여냄으로써, 적은 시연데이터 + 강화학습의 강인함이라는 두 마리 토끼를 잡았습니다. 이는 강화학습 분야에서도 흥미로운 성과로, 향후 다른 형태의 인간 힌트(예: 언어설명, 시연 몇 개 등)를 보상 설계나 탐색 가이드로 활용하는 연구들에 영감을 줄 수 있습니다. 또한 보상 엔지니어링 없이 시연으로 과업을 명세하는 기법은, 보상 설계가 난해한 문제들(예: 창의적 동작, 사회적 상호작용 등)을 풀 때 유용한 방향일 수 있습니다.

셋째, 실세계 복잡한 작업에서의 성공적 Sim-to-Real을 시연해 보였다는 점입니다. 특히 다수의 비간단한 조작(비파지, 환경 활용)과 다단계 작업을 한 프레임워크로 모두 해결한 것은 놀라운 결과입니다. 로봇이 단순 잡기뿐 아니라 벽을 활용해 물체를 세우고, 여러 동작을 연달아 수행하는 수준까지 도달한 것은, 범용적인 조작 능력에 한 걸음 다가간 것으로 볼 수 있습니다. 이는 향후 로봇이 사람과 같이 주변 환경을 활용하고, 연속된 업무를 수행하는 복합 스킬 학습에도 본 방법을 확장할 수 있음을 시사합니다. 실제 논문에서도 “주전자로 액체 붓기, 상자를 벽에 기대 기울이기, 접시를 건조대에 꽂기” 등 다양한 실제 과업을 예로 들며, Human2Sim2Robot 정책의 범용적 실행 능력을 강조합니다. 이러한 성과는 로봇공학 커뮤니티에 Sim-to-Real 강화학습의 가능성을 한층 확신시켜줍니다. 과거에는 시뮬레이터에서 배운 복잡한 다지 동작이 실제에서 과연 될까 회의적이었으나, 이 논문은 적절한 방법을 쓰면 가능함을 보여준 것이죠.

넷째, Human2Sim2Robot은 로봇의 체화(embodiment)를 적극 고려한 학습의 중요성을 부각시켰습니다. 사람의 데이터를 쓰면서도 사람처럼 움직이게 하지 않고, 로봇 고유의 해결책을 찾게 한 것이 주효했습니다. 이는 다른 형태의 로봇 (예: 다리가 있는 로봇 vs 사람 걷기 영상) 간에도 적용될 수 있는 일반 개념입니다. 예를 들어 인간의 걷는 영상을 네 발 로봇에 가르친다면, 다리를 두 개만 쓰는 대신 네 개를 다 사용하도록 유도하는 방식으로 응용해볼 수 있습니다. 즉, Cross-Embodiment Learning의 한 사례로서, 이 논문은 시연 행동의 결과(목표)에 집중하고 구체적 구현은 로봇에게 맡기는 접근이 성과가 있음을 보여주었습니다.

물론 한계와 향후 과제도 있습니다. 첫째로, 현재 방법은 단일 로봇 손, 단일 인간 시연, 단일 작업에 초점을 맞추고 있습니다. 논문에서도 향후 연구로 양손 조작(bimanual)이나 여러 로봇이 협력하는 경우, 혹은 여러 작업을 한꺼번에 학습하는 멀티태스킹 확장 등을 언급하고 있습니다. 한 개 시연으로 여러 스킬을 배우게 하거나, 두 손을 동시에 제어하는 문제는 난이도가 더욱 높지만, Human2Sim2Robot의 아이디어를 확장해나갈 수 있는 흥미로운 방향입니다. 둘째, 시연 영상 처리의 신뢰성 문제입니다. 본 연구는 전적으로 비디오에서 추출한 물체/손 포즈에 의존하므로, 만약 비디오 품질이 낮거나 물체 추적이 실패하면 학습에 지장이 생깁니다. 실제로 저자들은 시연 영상에서 물체나 손이 크게 가려져 추적이 실패하면 정책이 엉뚱한 방향으로 수렴할 수 있다고 보고합니다. 이는 입력 데이터의 한계로 인한 실패 모드로, 향후 멀티뷰 카메라 사용이나, 사람이 부분 교정해주는 인터페이스 등으로 개선될 수 있을 것입니다. 셋째, 사전 스캔과 모델 준비에 약간의 수작업이 필요합니다. 이번 연구에서는 물체와 환경을 3D 스캔하고, 또 물체 메쉬를 준비하여 포즈 추적에 사용했습니다. 이러한 과정은 예컨대 가정집 로봇이 일일이 하기에는 번거로울 수 있습니다. 하지만 현재 3D 스캐닝 기술도 자동화가 발전 중이고, 범용 객체 인식 모델이 더 좋아지면 필수 요건이 완화될 가능성이 있습니다. 끝으로, 실험 범위로 보자면 다룬 물체들이 비교적 단단하고 하나의 부피를 가진 것들이었습니다. 옷감이나 액체, 관절 있는 도구 등 변형체나 복합 물체에 대해서도 이 접근이 유효할지는 추가 연구가 필요합니다. 논문은 이러한 향후 과제로 변형 또는 관절 구조를 가진 물체로의 확장을 언급하며, 이는 추후 이 방법의 적용 범위를 넓혀 줄 것이라고 전망합니다.

**요약 및 결론:** "한 번의 인간 시연으로 Sim-to-Real RL"이라는 Human2Sim2Robot의 개념은, 로봇 학습 분야에 새로운 가능성을 제시한 중요한 성과입니다. 이 연구를 통해 우리는 인간의 시연을 로봇 학습에 활용하는 더욱 똑똑한 방법을 보았습니다. 과거처럼 무조건 따라하게 하지 않고, 인간 시연을 분석해 로봇이 이해할 형태의 과제로 정의해주니, 로봇은 자기 몸에 맞춰 그 과제를 수행할 방법을 스스로 터득했습니다. 그 결과 적은 데이터로도 높은 성능을 내고, 복잡한 실제 작업까지 실행에 옮길 수 있었습니다. 이러한 접근은 향후 다양한 로봇 플랫폼과 작업들에 응용될 수 있을 것이며, 로봇이 사람에게서 배우는 자연스러운 상호작용의 토대를 마련해줍니다. Human2Sim2Robot은 인간-로봇 협력의 간극을 한층 좁힌 혁신으로서, 강화학습과 로봇공학 발전의 교차점에서 큰 의미를 가지며, 향후 많은 후속 연구를 촉발할 것으로 기대됩니다.
