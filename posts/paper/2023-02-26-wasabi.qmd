---
title: "📃WASABI"
description: Learning Agile Skills via Adversarial Imitation of Rough Partial Demonstrations
date: "2023-02-26"
categories: [rl, gan, paper]
toc: true
---

![](https://i.imgur.com/JFJxLbR.png)

이번 포스팅은 [WASABI: Learning Agile Skills via Adversarial Imitation of Rough Partial Demonstrations](https://arxiv.org/abs/2206.11693) 논문을 읽고 정리한 내용입니다. 4족 보행 로봇 연구에서 많은 연구 성과들을 발표하는 스위스의 ETH Robotic System Lab과 독일의 Max Plank Institude for Intelligent Systems에서 발표한 논문으로, 강화학습에서 중요한 부분들 중 하나인 `reward design에 대한 고민을 generatvie adversarial method(WGAN, Wasserstein GAN)를 통해 해결`할 수 있음을 보여주었습니다. 

![](https://i.imgur.com/4uX3Ypk.png)

보행 로봇의 모션 제어에서 기본적인 보행뿐만 아니라 다양한 **다이나믹한 모션**을 수행하도록 로봇의 퍼포먼스를 끌어올리는 방향으로 연구가 활발하게 진행되고 있습니다. 여기서 말하는 다이나믹한 모션들로는 로봇이 공중에서 한바퀴 돌아야 하는 backflip과 같은 기존의 전통적인 보행 제어 연구를 기반으로 rule-based로 제어하기에는 매우 어려운 모션들을 말합니다. 로봇이 이런 모션들을 수행하도록 수학적으로 자세히 명시하고 그리고 모든 물리적 환경요소들을 고려하여 제어하기 어려울 때, 강화학습이라는 인공지능 프레임 워크를 이용하여 **reward라는 보상체계를 기준**으로 trial-and-error를 통해 모션을 학습하도록 하는 것이 직관적으로 매우 좋은 해결책으로 보입니다.

하지만 다이나믹한 모션을 각 task로 정의하고 우리가 원하는 방향대로 로봇이 모션들을 학습되기 위해서는 **reward를 잘 정의해주어야** 하는데 이 과정이 만만치 않게 까다롭고 어려우며, 오히려 수학적인 동역학 모델을 기반으로 제어할 때보다 분석적인 접근이 어렵기 때문에 **reward design**이라는 과제를 해결해야만 우리가 원했던 다이나믹 모션들을 강화학습을 이용하여 로봇이 수행할 수 있을 것 입니다. 바로 이 부분을 생성모델로 유명한 GAN 모델들 중 하나인 WGAN을 이용하여 해결하고자 했으며 해당 논문에서 가장 흥미로웠던 접근법은 **강화학습의 policy를 GAN의 generator 관점으로** 바라보고 reward를 추론하도록하는 프레임 워크를 만들었다는 점이었습니다. (이후 관련해서 더 논문들을 찾아보니 생성모델과 강화학습은 닮은 점이 많은 것 같습니다. 관련해서 흥미롭게 읽었던 다른 논문 [Connecting Generative Adversarial Networks and Actor-Critic Methods](https://arxiv.org/abs/1610.01945)도 관심이 있으시다면 가볍게 읽어보시는 것을 추천드립니다.)

# Introduction

강화학습은 정말 매력적인 인공지능 학습법 중 하나라고 생각합니다. 저도 직관적이고, 어떻게 보면 가끔 우리네 인생의 모습을 단순하지만 명료하게 보여주는 것 같아 그런 강화학습의 매력에 빠져 지금까지도 열심히 이해하고 공부하려고 노력하고 있는 것 같습니다. 많은 분들이 인공지능을 처음에 학습할 때 마주하게 되는 것은 "지도학습(Supervised Learning)"인데 이론 공부를 어느정도 마친 후, 관련해서 vision이나 자연어 등의 프로젝트를 시작하면 처음에 마주치는 난관은 *데이터셋 구축*이라고 생각됩니다. 빅데이터 기반으로 동작되는 방법론이다 보니 Garbage In, Garbage Out이 안되도록 조심해야하고 내가 원하는 커스텀 데이터 셋을 구축하는 데만 엄청난 에너지를 쏟아야 합니다. (오픈 데이터셋이나 transfer learning 기법 등을 이용해서 해결하기도 하지만요.)

*하지만 강화학습에서는 데이터 셋이 필요없습니다!* 왜냐하면 강화학습 프레임워크가 동작하면서 trial-and-error를 통해 interaction data를 만들게 되고 이를 기반으로 학습이 되는 것이기 때문입니다. 하지만 안타깝게도 강화학습에도 데이터셋 구축의 어려움 만큼이나(혹은 그 이상으로) 어려운 점이 있습니다. 바로 **환경(Environment) 구축**입니다. 유명한 DeepMind의 시니어 연구자는 *Behind every great agent, there's a great environment*라고 이야기 했을 정도로 강화학습에서는 환경 구축에 학습의 성패가 달렸다고 해도 과언이 아닙니다.

![](https://i.imgur.com/1bAIZR7.png)

강화학습의 현실에 대해 좀 더 살펴보겠습니다.(로봇틱스 분야 강화학습 연구자의 관점이므로 다른 분야에서 강화학습을 도입할 때의 관점과는 차이가 있을 수 있습니다.) 먼저 첫번째로 딥러닝 기반 강화학습 또한 빅데이터 기반으로 학습되는 것이기 때문에 **(1)많은 interaction data가 필요**합니다. 따라서 로봇틱스에 강화학습을 도입하기 위해서는 로봇을 여러번 돌리며 데이터를 얻어야 하는데 (연구 초기에는 실제로 로봇을 연구자가 여러번 다시 셋팅하고 실험을 하며 데이터를 얻었다고는 하지만..) 사실상 불가능에 가깝기 때문에 **시뮬레이터를 활용**하여 데이터를 얻게 됩니다. 하지만 이 점에서 실제 물리적인 세계에서 로봇이 구동되어 얻어지는 데이터와 물리적인 세계를 모사한 시뮬레이터에서 얻게된 데이터는 차이가 존재할 수 밖에 없기 때문에 **Sim-to-real**이라는 또 하나의 연구과제가 만들어지게 됩니다.

다음으로는 앞서 이야기 했던, **(2)강화학습의 환경 구축이 잘 되어야 제대로 학습이 될 수 있다**는 것입니다. 여기서 환경 구축, 혹은 강화학습의 수학적 모델링인 MDP(Markov Decision Process)의 요소들을 잘 정의해주어야 한다는 것은 사진에서 보이는 강화학습 프레임워크에 있는 **State, Reward, Action 등을 풀고자 하는 문제에 맞게 잘 정해주어야 한다**는 것입니다. 저는 강화학습 알고리즘 연구자가 아니고 강화학습을 활용한 로봇제어 연구자이기에 같은 강화학습 방법론을 보더라도 알고리즘 연구자와 어플리케이션 연구자가 보는 환경의 디테일이 많이 다른 것을 느꼈었습니다. 위 사진에서 같은 quadruped walking robot의 locomotion(보행) task를 생각할 때, 강화학습 알고리즘 논문들은 `Ant`와 같은 단순한 rigid model을 생각하고 실험을 하지만 강화학습을 실제 로봇에 적용하려고 보면 로봇의 각 모터의 특성, 센서등을 고려한 State, Reward, Action을 정의해야 하기 때문에 훨씬 복잡합니다. 사실 환경의 요소들 중, State와 Action은 각 도메인 마다 관례적인 정의 방법들이 있고 로봇의 센서들이 한정적이기 때문에 어느정도 정해져있다(limited)고 볼 수 있지만 **Reward는 강화학습에서 학습의 motivation이 되는 가장 중요한 부분이자 수행하고자 하는 task에 영향을 가장 많이 받는 부분이기 때문에 가장 정의하기가 어렵습니다.** 따라서 이런 어려움을 해결하고자 하는 또 하나의 연구 방향을 **Reward Engineering**이라고 지칭하기도 합니다. 이번 논문에서는 바로 이점을 파고든 것이라고 볼 수 있습니다.

---

어떤 Decision Process(의사결정 방법)를 학습한다고 생각했을 때 가장 직관적으로 떠오르는 방법이 무엇인가요? 그냥 *잘하는 사람을 따라하는 것*입니다. 이를 **Imitation Learning 혹은 Behavior Cloning**이라고 하는데(구별을 위해 이하 내용에서 Plain Imitation Learning이라고 칭하기도 함.) 한가지 예시로는 운전을 잘하는 인공지능(Agent)을 만들고 싶다면 운전을 잘하는 사람의 데이터를 그대로 따라하도록 학습하면 될 것 입니다. 이 방법은 Expert의 State-Action pair를 데이터 셋으로 보고 지도학습을 한 Agent를 만드는 것인데 Expert의 데이터만 학습하다보니 error가 들어가게 되고 generalization도 잘 되지 않습니다.

![](https://i.imgur.com/RVyXmCY.png)

이에 대한 보완으로 **GAIL(Generative Adversarial Imitation Learning)**이라는 방법이 제안되었습니다. 이름에서도 볼 수 있듯이 Generative Adversarial Network(적대적 신경망)와 Imitation Learning(모방 학습)이 합쳐진 학습 방법인데, **Expert의 state-action 분포를 True data distribution으로**, 학습하는 Agent의 **Policy를** True data distribution을 따라가고자 하는 **Generator로** 보는 것 입니다. 앞서 이야기한 plain imitation learning과 비교해보면 pair data point에 대해 맞춰가는 학습이 아닌 **data distribution이라는 확률적 스펙트럼을 이용해서 더 generalization을 잘할 수 있는** 해결책을 제안한 것으로 이해할 수 있을 것 같습니다. Data distribution을 학습하는 방법들은 생성 모델 분야에서 활발히 연구되고 있고, 이 중 GAN이라는 적대적 신경망 방법에서 Generator와 Discriminator라는 개념을 Imitation Learning에 적용한 것이라고 볼 수 있습니다.

![](https://i.imgur.com/grSsR8I.png)

GAIL의 방법론들 중 하나로, **AMP(Adversarial Motion Priors)**라는 방법이 있습니다. 이번 포스팅에서 소개되는 알고리즘인 WASABI와 AMP 모두 GAIL이라는 방법론 안에 속해있고, 둘을 비교해서 생각해보면 좋기 때문에 간략하게 짚고 넘어가보려고 합니다. AMP는 Motion data, 예를 들면 동물의 움직임에서 따온 expert data를 가지고 로봇 agent의 모션이 좀 더 자연스럽게 움직임을 학습할 수 있도록 합니다. **Discriminator**가 **Motion data**에서 나온 State-transition($S_t \rightarrow S_{t+1}$)인지 아니면 학습 중인 **Policy(Generator 역할)**에서 나온 State-transition인지를 구별하여 실제 동물의 움직임처럼 *자연스러운 스타일*을 학습 할 수 있도록 **보조적인 Style Reward($r_{style}$)을 기존의 강화학습 프레임워크 안에 추가**해줍니다. State-action pair를 가지고 학습하는 Plain imitation learning과 다르게, State-transition을 보고 Discriminator가 판단하는 것이기 때문에 *expert의 Action에 대한 정보는 필요가 없습니다.*

AMP 방법에서는 `자연스러운 모션`에 초점을 맞추었다는 것을 짚어볼 필요가 있습니다. Walking, Jumping과 같은 다이나믹한 **주요 모션 task에 대한 reward가 아니라** 학습할 때 자연스럽지 못한 모션으로 학습 방향이 튀지 않도록, 말 그대로 보조적인 모션 스타일을 잡아준 것입니다. 따라서 이런 자연스러움을 학습하기 위해서는 Motion data는 로봇의 pose configuration에 대해서 하나하나 명시되어 있어야 합니다. 이를 well-defined된 task이어야 한다는 말로 바꿔 말할 수 있는데, 로봇의 joint(관절) position이 timestep 마다 어떻게 움직여야 하는지 수치적으로 다 명시되어 있는 Motion data가 있어야 한다고 볼 수 있습니다. 이렇게 주요 모션 Task reward 디자인에 고려가 아닌 Style reward 디자인에 GAN 방법을 도입한 AMP 방법에 반해 WASABI는 Task reward에 GAN 방법을 도입했다는 점에서 가장 큰 차이점이 있다고 볼 수 있습니다.

## GAN

적대적 신경망에 대해 기본적인 이론부터 시작해보겠습니다. GAN은 생성 모델을 학습하기 위한 방법론 중 하나로 **Generative**, 어떠한 새로운 데이터 생성을 하는, **Adversarial** 게임과 같이 Discriminator와 Generator라는 2개의 알고리즘 모듈이 경쟁을 하며 학습을 하는 방법론 입니다. 아래 사진에서 보이는 예시로 보면 진짜 모나리자 그림이라는 Real example을 보고 이를 모사한 작품을 파는 화가를 Generator라고 생각해볼 수 있습니다. 그러면 미술 작품 감별사인 Discriminator는 이 작품이 진짜 모나리자 그림인지 아니면 화가가 모사한 가짜 모나리자 인지 판단하게 됩니다. 당연히 Generator 입장에서는 Discriminator가 감별하기 어렵게 점점 더 진짜같은 모나리자를 그리게 되고(new data) Discriminiator 입장에서는 진짜와 가짜 사이에 더 자세하고 민감한 차이를 찾아내어 Generator의 모사품을 찾아내려고 할 것 입니다.

![](https://i.imgur.com/ge1VZdI.png)

이러한 GAN의 학습 과정에는 **지도 학습**과 **비지도 학습**이 모두 들어있습니다. 우선 Discriminator 입장에서는 진짜와 가짜 라벨을 가진, 인풋 데이터가 들어오면 2개의 카테고리들 중 하나를 선택하는 지도학습을 하게 됩니다. Generator는 비지도 학습으로 latent code라는 일종의 trigger 요소인 어떤 벡터를 인풋으로 받으면 진짜 data distribution과 가까운 데이터인 new data를 생성하게 됩니다.

![](https://i.imgur.com/caSRhra.png)

잠깐 data distribution이라는 개념이 GAN에서는 중요한 개념이므로 `Probability Distribution(확률 분포)`을 간단하게 짚고 넘어가겠습니다. 확률 분포란 어떤 사건을 대변하는 랜덤 변수들의 확률 분포라고 볼 수 있습니다. 주사위를 총 6번 던져서 1, 2, 3, 5가 각각 1번씩 그리고 6이 2번 나왔다면 아래와 같은 확률 분포 그래프를 그릴 수 있고, 이때의 `Expectation(기댓값)`을 구해보면 $1 \cdot \frac{1}{6} + 2 \cdot \frac{1}{6} + 3 \cdot \frac{1}{6} + 4 \cdot \frac{0}{6} + 5 \cdot \frac{1}{6} + 6 \cdot \frac{2}{6} = \frac{23}{6} \eqsim 3.8$ 임을 알 수 있습니다.

![](https://i.imgur.com/ednff5I.png)

이미지를 데이터 포인트 $x$라고 하고 우리가 가지고 있는 사람 얼굴 **이미지 데이터 셋의 분포**가 왼쪽의 분포와 같다고 한다면, 여러개의 모드(mode)가 있는데 가장 높은 확률의 mode에서는 금발 여성의 얼굴이 있고 상대적으로 낮은 확률로 흑발의 안경 쓴 남자의 얼굴 이미지가 있음을 알 수 있습니다. 또한 mode가 아닌 매우 낮은 확률을 보이는 분포의 꼬리 부분을 보면 매우 이상한 얼굴 이미지들이 나오는 것을 알 수 있습니다.

![](https://i.imgur.com/PRohEhz.png)

바로 우리가 가지고 있는 이미지 데이터 셋 분포(빨강색)과 유사한 데이터 분포(파란색)를 학습하는 것이 생성 모델의 목표이고 이를 Discriminator와 Generator를 가지고 학습하도록 하는 것이 **GAN**입니다.

![](https://i.imgur.com/nLFrh1I.png)

**Discriminator**의 Objective Function($V$)을 보면, 먼저 첫번째 term은 데이터 $x$는 true dataset distribution인 $p_{data}$에서 샘플링 되었을 때 Discriminator는 이를 진짜라고 판별해야 하고 이는 output 1(true label)을 출력해야하는 방향으로 학습되어야 합니다. 두번째 term은 fake dataset distribution인, 즉 generator가 만든 데이터일 경우에 가짜라고 판별해야 하고 output 0(fake label)을 출력해야 합니다. 따라서 2개의 term을 모두 maxmization하는 것이 Discriminator의 목표이기 때문에 $\text{max}_DV(\cdot)$이 됩니다.

![](https://i.imgur.com/W390bdP.png)

**Generator**의 Objective Function을 보면, 첫번째 true dataset distribution에서 샘플링 되는 부분은 Generator와 상관이 없습니다. 두번째 term에서 Generator에서 나온 ouput new data를 Discriminator에게 넘겨주었을 때 1(true label)로 착각하도록 만들어야 하므로 $\text{min}_GV(\cdot)$이 됩니다.

![](https://i.imgur.com/9zG3oU1.png)

## WGAN

위에서 설명한 기본적인 GAN을 잘 학습했을 때 확률분포를 그려보면 다음과 같이 Discriminator의 판별 분포가 빨간색 그래프처럼 그려지는 것을 알 수 있습니다. 완벽하게 true distribution인 $p_{data}$에 대해서는 1을, generated distribution $p_G$에 대해서는 0을 보여주고 있지만 이런 상황에서는 유의미한 학습이 일어나기 힘듭니다.

![](https://i.imgur.com/8JEzbak.png)

Optimal한 Discriminator를 가정하고 Objective function을 다시보면 $p_{data}$와 $p_G$가 너무 멀리 떨어져 있어서 사실상 계산된 $V(\cdot)$값이 0이기 때문입니다. 따라서 Generator가 두 분포가 가깝도록 만드는 방향으로 학습을 해야 하는데 Classic GAN의 Objective Function에는 이러한 정보를 알려줄 수 있는 부분이 수학적으로 모델링이 되어 있지 않습니다.

![](https://i.imgur.com/T2RcnRe.png)

따라서 분포들간의 먼 정도를 모델링할 수 있는 **WGAN(Wasserstein GAN)**이 제안되었고 이에 대해서는 수학적으로 매우 딥한 내용이 있지만 본 포스팅에서는 간단하게 개념적으로 공사장의 포크레인을 이용하여 이해하고 넘어가겠습니다. Wassertein Distance는 Earth mover's distance라고도 불리는데 이름에서 직관적으로 이해할 수 있듯이, 두 분포를 어떤 흙더미라고 생각하고 우리가 Generated Distribution에 있는 흙들을 Real Distribution의 모양대로 흙들을 옮긴다고 했을 때 드는 cost가 distance로 정의된다고 볼 수 있습니다. (수학적으로 더 궁금하신 분들은 [Implicit DGM 29 | Wasserstein Distance with GAN](https://youtu.be/Zx8ffArd-Bc)을 추천합니다.) 본 연구에서는 이 WGAN을 이용하여 reward 디자인을 했습니다.

![](https://i.imgur.com/LzexuQs.png)

## RL with GAN

GAN 내용을 설명할 때 이미지 생성 분야의 예시가 직관적이고 쉽기 때문에 이를 가지고 설명하다 보니 문득 그래서 강화학습에서 어떻게 GAN을 사용하는데? 라는 의문이 생길 수 있습니다. 다시 강화학습에서의 여러 어려움들 중 Task reward를 

![](https://i.imgur.com/QlP3aFv.png)

# Method

## Problem Definition

![](https://i.imgur.com/msxpT4a.png)

![](https://i.imgur.com/D6FEQEE.png)

## Reward Design

![](https://i.imgur.com/0wg8bM0.png)

![](https://i.imgur.com/z8U8CC6.png)

### Imitation(Task) Reward

![](https://i.imgur.com/8DRZBxx.png)

![](https://i.imgur.com/NXdF03E.png)

![](https://i.imgur.com/v1lt07o.png)

![](https://i.imgur.com/rXawMI8.png)

![](https://i.imgur.com/Oqh2wGI.png)

### Regularization Reward

![](https://i.imgur.com/H5pvaF8.png)

### Termination Reward

![](https://i.imgur.com/UwAzXUZ.png)

### Total Reward

![](https://i.imgur.com/XSJWcDx.png)

# Result

![](https://i.imgur.com/CsTqUZw.png)

![](https://i.imgur.com/PBnpWKR.png)

## Induced Imitation Reward Distributions

![](https://i.imgur.com/7HPHwPA.png)

## Learning to Mimic Rough Demonstrations

### Dynamic Time Warping

![](https://i.imgur.com/xPDivSD.png)

![](https://i.imgur.com/8mpyySq.png)


### Handcrafted Task Reward

![](https://i.imgur.com/NhqRkfs.png)

## Evaluation on Real Robot

![](https://i.imgur.com/hNAIFbK.png)

## Cross-platform Imitation

![](https://i.imgur.com/dtIbRK7.png)

# Conclusion



# Reference

- [Original Paper: Learning Agile Skills via Adversarial Imitation of Rough Partial Demonstrations](https://arxiv.org/abs/2206.11693)
- [Original Project Homepage: CoRL2022-WASABI](https://sites.google.com/view/corl2022-wasabi/home)
- [Learning Quadrupedal Locomotion over Challenging Terrain](https://youtu.be/9j2a1oAHDL8)
- [Joonho Lee: Learning Quadrupedal Locomotion over Challenging Terrain](https://youtu.be/69tqQq1EpEI)
- [Advanced Skills through Multiple Adversarial Motion Priors in Reinforcement Learning](https://youtu.be/kEdr0ARq48A)
- [What Are GANs?](https://freecontent.manning.com/what-are-gans/)
- [1시간만에 GAN(Generative Adversarial Network) 완전 정복하기](https://youtu.be/odpjk7_tGY0)
- [CS 182: Lecture 19: Part 3: GANs](https://youtu.be/RdC4XeExDeY?t=454)
- [GANs for Synthetic Data Generation](https://ydata.ai/resources/gans-for-synthetic-data-generation)
- [An Open Torque-Controlled Modular Robot Architecture for Legged Locomotion Research](https://ieeexplore.ieee.org/document/9015985/)
- [DTW(Dynamic Time Warping)](https://no-never-no.tistory.com/19)
- [파이썬 코딩으로 말하는 데이터 분석 - 10. DTW (Dynamic time wrapping)](https://hamait.tistory.com/862)
- [Dynamic time warping 1: Motivation](https://youtu.be/ERKDHZyZDwA)
