---
title: "📃Text2Touch 리뷰"
date: 2025-09-19
categories: [touch, llm, rl]
toc: true
number-sections: False
description:  Tactile In-Hand Manipulation with LLM-Designed Reward Functions
---

- [Paper Link](https://openreview.net/pdf?id=U9zcbQVDGa)
- [Homepage](https://hpfield.github.io/text2touch-website/)

1. 이 연구는 LLM(Large Language Model)이 촉각 센싱을 활용하여 실제 로봇의 복잡한 손 안 물체 조작을 위한 보상 함수를 설계하는 Text2Touch 프레임워크를 제시합니다.
2. Text2Touch는 70개 이상의 환경 변수를 처리하는 향상된 프롬프트 엔지니어링 전략과 시뮬레이션에서 실제 로봇으로 정책을 성공적으로 이전하는 sim-to-real 증류 파이프라인을 활용합니다.
3. 실제 촉각 Allegro Hand에서 Text2Touch는 사람이 설계한 기존 기준선보다 뛰어난 회전 속도와 안정성을 보였으며, LLM 설계 보상이 로봇 학습 배포 시간을 크게 단축할 수 있음을 입증했습니다.


<center>
<img src="../../images/2025-09-19-text2touch/0.png" width="100%" />
</center>

---

# Brief Review

이 논문은 대규모 언어 모델(LLM)이 햅틱(tactile) 센싱을 활용하는 복잡한 로봇 작업의 Reward Function 설계를 자동화할 수 있음을 입증한 Text2Touch 프레임워크를 제안합니다. 기존 LLM 기반 Reward 설계 연구는 주로 시각 및 고유 수용성 감각(proprioception)에 초점을 맞췄으며, 사람과 같은 정교한 조작(dexterous manipulation)에 필수적인 촉각 센싱은 다루지 않았습니다. Text2Touch는 LLM이 real-world에서 촉각 정보를 활용하여 물체를 다축(multi-axis)으로 회전시키는 In-Hand Manipulation 작업을 성공적으로 수행하도록 Reward Function을 생성하는 방법을 보여줍니다.

이 연구의 핵심 방법론은 다음과 같습니다.

1.  **반복적 LLM Reward Design**: Eureka 프레임워크를 기반으로, LLM이 PPO (Proximal Policy Optimization) 학습 에이전트와 함께 Reward Function을 반복적으로 생성하고 개선합니다.
    *   **Initialization**: LLM은 자연어 작업 설명 $l$과 환경 Context $M$을 기반으로 초기 Reward Function 후보군 $\{R^{(0)}\}$을 생성합니다.
    *   **Iteration**: 각 반복 $k$에서, 생성된 Reward Function $\{R^{(k)}\}$으로 정책(policies) $\{\pi^{(k)}\}$을 학습시키고, Fitness Function $F$를 사용하여 작업 점수 $\{s^{(k)}\}$를 평가합니다. 가장 우수한 Reward Function $R^{(k)}_{best}$가 선택되고, 이 Reward Function과 해당 정책의 피드백을 활용하여 LLM이 다음 반복을 위한 Reward Function $\{R^{(k+1)}\}$을 생성합니다. 이는 다음 공식으로 표현됩니다:
        $\text{Initialization: } \{R^{(0)}\} \sim \text{LLM}(l, M)$
        $\text{For each iteration } k \ge 0 :$
        $\quad \{\pi^{(k)}\} \leftarrow \{\text{RL}(\{R^{(k)}\})\},$
        $\quad \{s^{(k)}\} \leftarrow \{F(\{\pi^{(k)}\})\},$
        $\quad R^{(k)}_{best} = \text{SelectBest}(\{R^{(k)}, s^{(k)}\}),$
        $\quad \{R^{(k+1)}\} \sim \text{LLM}(l, M, \text{Feedback}(s^{(k)}_{best}, R^{(k)}_{best}).$
    *   **Fitness Function**: 물체의 목표 회전이 달성될 때마다 증가하는 "성공 횟수(running successes count)"를 사용합니다. 에피소드는 시간이 만료되거나 에이전트가 크게 이탈하면 재설정됩니다.

2.  **고복잡도 환경을 위한 Prompt Engineering 강화**:
    *   **확장 가능한 Bonus 및 Penalty**: 기존 Eureka 프레임워크는 고정된 Success Bonus $B$를 사용했지만, Text2Touch는 LLM이 최종 Reward 표현 내에서 Success Bonus $B$와 Early-Termination Penalty $P$를 적절하게 조정할 수 있도록 확장 가능한 변수로 LLM Context에 포함시킵니다.
    *   **수정된 Prompt 구조**: 70개 이상의 환경 변수를 다루기 위해, LLM에 모든 변수 이름과 Type이 포함된 명시적인 Reward Function Signature $S_{detailed}$를 제공합니다. 이는 `M_{modified} = {E, S_{detailed}}$와 같이 환경 Context를 확장하여 코드 오류를 크게 줄이고 정책 성능을 향상시킵니다.

3.  **Sim-to-Real Distillation을 통한 Real-World 검증**:
    *   **Reward Discovery Training**: Reward 후보군을 효율적으로 평가하기 위해 각 반복마다 1억 5천만 Step의 짧은 시뮬레이션 훈련을 수행하고, 최적의 Reward $R^*$에 대해서는 80억 Step의 전체 훈련을 진행합니다.
    *   **정책 전이 (Teacher-Student Pipeline)**: 시뮬레이션에서 privileged information (물체 Pose 및 Velocity)을 사용하여 Teacher Policy를 훈련시킨 후, 이 정책을 촉각 및 고유 수용성 관측값만을 사용하는 Student Policy로 Distillation합니다. Student Policy는 Teacher의 Action과 평균 제곱 거리(Mean Squared Distance)를 최소화하도록 학습됩니다. 이렇게 훈련된 Student Policy는 추가 튜닝 없이 real-world Allegro Hand에 배포됩니다.

**주요 성과**:

*   **LLM 기반 촉각 조작 Reward Design**: LLM이 촉각 센싱 데이터를 통합하여 Real-World에서 효과적인 Reward Function을 설계할 수 있음을 처음으로 시연했습니다.
*   **향상된 Prompting 전략**: 70개 이상의 환경 변수를 포함하는 복잡한 환경에서 확장 가능한 Bonus 및 Penalty, 그리고 명시적인 Reward Function Signature를 통해 LLM의 Reward 생성 능력을 크게 개선했습니다.
*   **Sim-to-Real 전이의 Real-World 검증**: LLM이 설계한 정책은 real-world의 tactile Allegro Hand에서 중력 불변(gravity-invariant) 다축 In-Hand Object Rotation 작업을 성공적으로 수행하며, Palm-Up 및 Palm-Down configuration 모두에서 작동합니다.
*   **Human-Engineered Baseline 능가**: Text2Touch는 기존에 신중하게 튜닝된 Human-Engineered Baseline보다 우수한 회전 속도와 안정성을 보여주며, LLM이 생성한 Reward Function은 Baseline에 비해 훨씬 짧고 간결합니다. LLM이 생성한 Reward Function은 Baseline의 약 1/10의 변수, 1/4의 코드 라인(LoC), 1/8의 Halstead Volume (HV)을 사용했습니다.

이 연구 결과는 LLM이 Reward Design에 크게 기여하여 복잡한 multimodal 로봇 학습의 Concept부터 배포 가능한 정교한 촉각 기술까지의 시간을 단축할 수 있음을 입증합니다.

---

# Detail Review

Text2Touch는 대규모 언어 모델(LLM)이 설계한 보상 함수를 활용하여 로봇 손내 물체 조작을 학습시키는 최신 연구로, 2025년 CoRL (Conference on Robot Learning)에 채택된 논문입니다. 이 연구는 사람의 섬세한 손재주처럼 물체를 손 안에서 자유롭게 회전시키는 복잡한 과제를, 촉각 센서 정보와 LLM 기반 보상 설계를 결합하여 해결한 점에서 주목받고 있습니다. 특히 사전에 인간 전문가가 고안한 복잡한 보상 함수 대신, LLM에게 환경 정보를 설명하고 자동으로 보상 함수를 생성하도록 함으로써 학습 효율과 성능을 높였다는 점이 핵심입니다. 본 리뷰에서는 해당 논문의 주요 기여, 방법론과 실험 구성, 기존 연구 대비 차별점, 장점과 한계점을 분석하고, 일반 독자를 위한 쉬운 설명을 제공합니다.

## 논문의 핵심 기여 요약

이 논문의 저자들은 “Text2Touch” 프레임워크를 제시하며, 다음과 같은 핵심 기여를 합니다:

* LLM 기반 촉각 보상 설계의 최초 구현: 기존 LLM 활용 연구들이 시각이나 고유감각(Proprioception)에 한정되었던 데 반해, 이 논문은 촉각 센싱을 통합한 최초의 자동 보상 함수 생성 기법을 선보였습니다. 즉, 로봇이 물체를 “느끼는” 접촉 정보까지 활용하여 보상을 설계함으로써, 인간 수준의 섬세한 조작에 한 발 더 다가섰습니다.
* 복잡한 환경에 대한 프롬프트 엔지니어링 기법: 저자들은 70개가 넘는 환경 변수를 LLM에 효과적으로 전달하기 위한 프롬프트 구조화 전략을 개발했습니다. 이를 통해 LLM이 복잡한 상태 공간에서도 오류 없이 동작하는 보상 코드를 생성하도록 유도하고, 시뮬레이션 학습 성능을 향상시켰습니다.
* 시뮬레이션-현실 이식(Sim-to-Real) 검증: 시뮬레이터에서 LLM이 설계한 보상으로 학습된 정책을 교사-학생 모델 증류를 통해 실제 로봇 손에 성공적으로 이식하였습니다. 그 결과, 로봇 손바닥을 위로 향한 경우뿐 아니라 아래로 향한 어려운 조건에서도 물체를 3축 회전시키는 데 성공했고, 해당 분야 기존 최고 성능(인간 설계 보상)을 뛰어넘는 회전 속도와 안정성을 달성했습니다. 이는 현재까지 보고된 가장 어려운 손내 조작 과제에서 새로운 최고 기록을 세운 것입니다.

이러한 기여를 통해, 이 논문은 LLM으로 보상 함수를 자동 설계하여 복잡한 촉각 멀티모달 로봇 학습을 가속할 수 있음을 증명했습니다.

## 사용된 방법론 및 실험 구성 분석

Text2Touch의 전체 학습 파이프라인 개요. 왼쪽은 LLM에 제공되는 프롬프트 구성 요소들로, 환경의 관측 변수 정의, 과제 목표에 대한 자연어 설명, 그리고 보상 설계 지침 등이 포함된다. 가운데는 LLM이 생성한 보상 함수를 이용해 시뮬레이터에서 강화학습을 수행하는 과정이다. 여러 후보 보상 함수를 시험하여 최적의 정책(노란색 교사 모델)을 얻은 후, 오른쪽의 보라색 학생 모델로 지식 증류를 거쳐 촉각 센서와 관절각 등 실제 센서 입력만으로 동작하는 정책을 확보한다. 마지막으로 이 학생 정책을 실제 로봇 손에 배치하여 물체 회전 과제를 수행한다.

### 과제 정의 및 LLM 보상 함수 생성

연구의 목표 과제는 손바닥 안팎의 어떤 방향에서도 물체를 공중에 든 채로 3축 회전시키는 것입니다. 이는 물체를 놓치지 않으면서 X, Y, Z축으로 연속 회전시키는 고난도 조작으로, 로봇 손에 장착된 시각 기반 촉각 센서(예: TacTip)에서 얻은 접촉힘/미끄럼 정보를 활용해야 달성할 수 있는 과제입니다. 이처럼 고차원 센서 정보(촉각 + 관절각 등 총 70여 개 변수)를 다루는 환경에서, 저자들은 체계적인 프롬프트 엔지니어링을 통해 LLM (주로 GPT-4)을 활용한 보상 설계 방법을 개발했습니다.

프롬프트에는 환경 상태 변수들의 목록과 의미, 과제 목표에 대한 자연어 설명, 그리고 보상 함수에 필요한 보너스/패널티 구조 지침이 포함되었습니다. 예를 들어 “물체의 회전각 속도를 높일수록 보상을 높이고, 물체를 떨어뜨리면 큰 패널티를 부여하라”와 같은 식으로 보상 구조를 서술합니다. 이렇게 하면 LLM이 주어진 변수들을 활용하여 성공 보너스와 실패 패널티를 명시적으로 포함한 파이썬 보상 함수를 생성하도록 유도할 수 있습니다. 저자들은 프롬프트 설계를 반복 실험하며 다듬었는데, 그 결과 LLM이 짧고 실행 가능한 보상 코드를 높은 확률로 산출하게 되었습니다. (향상된 프롬프트 없이 LLM을 직접 사용할 경우, 엉뚱한 변수에 의존하거나 실행 오류가 나는 코드가 생성되기 쉽다는 것도 실험을 통해 확인되었습니다.)

### 강화학습 및 보상 함수 탐색

LLM이 출력한 보상 함수는 시뮬레이터 상의 로봇 손에 적용되어 강화학습(RL)으로 정책을 훈련하는 데 쓰입니다. 이때 교사-학생 2단계 학습이 활용됩니다. 먼저 교사 모델은 특권 정보(privileged info)를 활용하는 정책으로, 시뮬레이터에서 물체의 실제 자세나 접촉 여부 등 완전한 상태정보를 참고하여 학습됩니다. 이러한 교사 정책은 주어진 보상 함수가 이론적으로 낼 수 있는 최상의 성능을 대표합니다. 저자들은 최신 LLM 연구인 Eureka 방식을 따라, 여러 후보 보상 함수들을 자동 탐색하여 가장 성능이 좋은 보상 함수를 선택했습니다. 그 결과 GPT-4를 비롯한 몇 가지 LLM이 제시한 보상 함수들이 시뮬레이터에서 높은 회전 속도 성능을 보였고, 이 중 최고 성능의 보상 함수들을 추려냈습니다. 선택된 보상으로 학습된 교사 정책들은 에피소드당 평균 5회 이상의 360° 회전을 달성했는데, 이는 기존 인간 설계 보상의 약 4.7회보다 높은 수치입니다. 뿐만 아니라 LLM이 만든 보상들은 코드가 훨씬 간결함에도 불구하고, 학습 과정에서 안정적인 그립 유지(물체를 떨어뜨리지 않는 비율) 역시 개선되었습니다.

### 교사 모델의 지식 증류 및 실험 설계

다음으로, 시뮬레이터 상에서 특권 정보로 학습된 교사 정책을 지식 증류(distillation) 기법으로 학생 정책에 이식합니다. 학생 정책은 오직 실제 로봇에서 얻을 수 있는 관측(손가락 관절각, 각 촉각센서 이미지 등)만을 입력으로 받아 동작하도록 학습됩니다. 이를 위해 교사 정책이 생성한 행동 데이터를 모으고, 학생 정책이 이 행동을 모방하거나 추가적인 RL 튜닝을 통해 성능을 맞추도록 합니다. 이렇게 하면 시뮬레이터에서 잘 동작하던 정책을 실제 세계의 촉각 센서 기반 정책으로 변환할 수 있습니다. 저자들은 현실 적응을 돕기 위해 시뮬레이터에서 물체의 마찰 계수, 질량 등 물리 특성을 무작위로 변화시키는 도메인 랜덤화 기법도 적용했습니다. 최종적으로 얻어진 학생 정책들은 영국 브리스톨 로봇랩의 4손가락 로봇 손(Allegro Hand)에 구현되었으며, 이 손가락들에는 TacTip 촉각 센서가 장착되어 있어 실제 접촉 정보를 감지합니다.

### 실험 구성 및 평가

평가를 위해 다양한 형태와 질량의 실제 물체들을 사용했습니다. 예를 들어 플라스틱 사과와 오렌지(둥근 물체), 네모난 작은 상자, 고무 오리 인형(복잡한 형상) 등 약 8~10종의 물체를 로봇 손으로 집어들어 회전시키는 실험을 진행했습니다. 특히 손바닥을 위로 향한 경우(palm-up)와 아래로 향한 경우(palm-down) 모두 테스트하여, 중력 방향과 관계없이 물체를 놓치지 않고 돌릴 수 있는지를 검증했습니다. 성능 지표로는 에피소드당 연속 회전 횟수(한 번 잡아서 떨어뜨리기 전까지 몇 바퀴 돌리는가)와 에피소드 지속 시간(버티컬 안정성 지표로 해석 가능)을 측정했습니다. 또한 학습에 사용하지 않은 새로운 무게나 새로운 형태의 물체에 대해서도 일반화 성능을 시험했습니다 (논문에서는 이를 OOD (out-of-distribution) Mass/Shape 실험으로 보고합니다).

그 결과, LLM-설계 보상으로 학습된 정책들은 기존 인간 보상 정책보다 뛰어난 성능을 보였습니다. 예를 들어, GPT-4 기반 보상 함수로 학습한 정책은 인간 보상 대비 회전 속도가 빠르고(에피소드당 평균 5.2회 vs 4.7회), 물체를 더 오래 떨어뜨리지 않고 유지했습니다. 여러 LLM 중에서도 Gemini-1.5 (구글 Gemini 모델) 기반 보상이 가장 많은 회전을, GPT-4 기반 보상이 가장 높은 성공 확률을, DeepSeek 기반 보상이 가장 긴 에피소드 지속시간을 보여주는 등 모델별 특성이 나타났으나, 모든 LLM 보상 정책이 일관되게 인간 보상 정책을 상회했습니다. 특히 Deepseek-R1 모델로 생성된 보상을 사용할 경우, 현실 실험에서 회전 횟수가 38% 증가하고 실패하기 전 유지 시간도 25% 늘어났다고 보고되었습니다. 한편, 인간이 공들여 설계한 기준 보상(Baseline)은 매우 보수적으로 동작하여 물체를 떨어뜨리는 경우는 적었지만 회전 속도가 느렸고, LLM 보상 기반 정책들은 다소 공격적이지만 빠르게 회전시키면서 미끄럼이 감지되면 즉각 대응하는 민첩한 조정 능력을 보였습니다. 이러한 차이는 손바닥을 위로 향한 상태에서 오리 인형을 돌리는 동작 등에서 두드러졌는데, LLM 보상 정책은 살짝 미끄러지면 빠르게 자세를 고쳐 잡고 계속 회전한 반면, 인간 보상 정책은 아예 미끄럼이 안 생기도록 천천히 신중히 돌리는 전략을 취해 효율이 낮았습니다.

마지막으로, 저자들은 LLM 보상 함수의 복잡도를 정량적으로 비교했습니다. 흥미롭게도, LLM이 출력한 보상 코드는 매우 단순하고 짧았습니다. 예를 들어 GPT-4가 설계한 최적 보상 함수는 코드 본문이 약 30줄에 불과하고 사용한 상태 변수도 8개 정도뿐이었습니다. 반면, 인간 전문가가 만든 기존 보상 함수는 각종 세부 항목(물체의 각도 정렬 항목, 접촉 안정화 항목, 속도 부드러움 항목 등)을 모두 합쳐 수백~수천 줄의 코드와 100개 이상의 변수로 이루어진 매우 복잡한 형태였습니다. 그럼에도 불구하고 LLM 보상은 동일한 핵심 아이디어들을 겹치지 않고 깔끔하게 구현해냈으며, 결과적으로 짧은 코드로 더 나은 성능을 얻어낸 것입니다. 저자들은 이러한 간결성과 가독성이 향후 보상 함수 설계의 유지보수 측면에서도 큰 이점이 될 것으로 언급했습니다.

## 기존 연구와의 차별점 및 관련 연구 비교

본 연구는 강화학습의 보상 설계 문제와 로봇의 섬세한 손동작 학습이라는 두 영역의 최첨단을 교차하는 지점에 위치합니다. 선행 연구들과 비교하여 Text2Touch가 가지는 차별점은 다음과 같습니다.

* LLM을 통한 보상 자동화 연구의 확장: 최근 몇 년간 LLM을 이용해 로봇의 보상 함수를 자동 생성하려는 시도가 등장했습니다. 예를 들어 Eureka라는 연구에서는 GPT-4에게 보상 함수를 작성하고 개선시키게 하여, 시뮬레이션 상의 여러 작업에서 사람보다 나은 보상을 설계한 바 있습니다. 또한 사용자가 서술한 과제 내용을 코드로 변환해 로봇을 제어하거나, LLM이 스스로 진행률 지표나 보상 구성 요소를 설계하도록 한 사례들도 있었습니다. 그러나 이러한 선행 연구들은 주로 시뮬레이터 내부의 단순 센서 정보(로봇의 관절 상태나 위치 정보 등)만 사용하거나, 현실 실험을 하더라도 시각 카메라나 관절 센서 정도에 한정된 환경이었습니다. Text2Touch는 촉각 센서라는 고차원 감각 정보를 보상 설계에 도입함으로써, LLM 보상 자동화 연구의 범위를 새로운 센서 모달리티로 확장했다는 의의가 있습니다. 이는 이전까지 LLM이 다루지 않던 풍부한 접촉 정보까지 포함하도록 프롬프트 구성과 모델 활용을 정교화했다는 점에서 고유한 기여입니다.
* 최첨단 손내 조작 기술과의 결합: 로봇 손으로 물체를 자유롭게 회전시키는 문제는 최근 크게 주목받아 왔습니다. 기존 연구들에서는 단일 축으로 살짝 돌리기나 손바닥 위에 공 균형 잡기처럼 제한된 과제를 다루거나, 물체를 잡았다 놓았다 반복하며 목표 자세로 재배치하는 등 간소화된 문제 설정이 많았습니다. 2023~2024년경에 이르러서야 비로소 AnyRotate나 General In-Hand Rotation과 같은 연구에서 중력의 영향을 받지 않는(hand orientation 무관) 다축 회전 과제가 가능해졌습니다. 예를 들어 Yang 등의 AnyRotate 연구는 시뮬레이션과 실제 로봇 모두에서 첫 번째로 중력-무관 다축 회전을 시현하며, 이를 위해 풍부한 촉각 센싱이 필수적임을 보여주었습니다. Qi 등이 수행한 연구에서는 비전+촉각을 결합해 일반적인 물체 회전을 달성하기도 했습니다. 그러나 이들 선행 성과들은 모두 전문가가 신중히 설계한 보상 함수에 의존하고 있었습니다. 보상 함수를 사람이 일일이 튜닝하는 작업은 매우 느리고, 사람의 직관에 따른 편향이 개입되기 쉽다는 한계가 지적됩니다. 반면 Text2Touch는 기존 최첨단 손내 조작 결과들을 LLM 활용을 통해 한 단계 발전시켰습니다. 자동 설계된 보상으로도 인간 설계 보상 만큼이나 복잡한 목표를 달성할 수 있음을 보였을 뿐 아니라, 오히려 성능 면에서 더 우수함을 입증했습니다. 이는 인간 전문가의 개입을 줄이면서도 최첨단 결과를 경신했다는 점에서 큰 차별화 포인트입니다. 나아가, 본 연구는 시뮬레이션-현실 사이의 격차를 메우는 과정에서도 LLM 보상이 통용될 수 있음을 처음으로 확인하였습니다. 이전까지 자동 생성된 보상이 현실 로봇에서 성공적으로 쓰인 사례는 거의 없었는데, Text2Touch는 촉각 센서까지 포함된 복잡한 현실 상황에서도 LLM 보상이 유효함을 보여준 것입니다.
* 멀티모달 로봇 학습에서의 활용 가능성: 일부 다른 연구들은 LLM을 로봇 제어의 고수준 플래너로 활용하거나, 거대한 비전-언어 모델로 로봇의 다중센서 피드백을 통합하려는 시도를 해왔습니다. 예를 들어, LLM이 주어진 목표를 달성하기 위한 단계별 로봇 행동 시퀀스를 생성한다든지, 사전 학습된 거대 모델을 통해 시각과 텍스트 정보를 결합한다든지 하는 접근입니다. 그러나 이런 방법들은 주로 이미 학습된 하위 제어기를 가정하거나, 대규모 비전 모델을 필요로 하여 계산 비용이 막대하다는 문제가 있었습니다. Text2Touch는 반대로 학습되지 않은 새로운 기술(skill)을 바닥부터 훈련하면서, LLM을 보상 설계라는 한정된 목적에 활용함으로써 보다 경량의 접근을 취했습니다. 또한 자동 커리큘럼 설계나 환경 생성 기법 등도 로봇 학습을 돕는 다른 방안으로 연구되고 있으나, 본 논문에서는 고정된 커리큘럼을 사용하여 보상 함수의 영향만을 엄밀히 비교했습니다. 이러한 실험 설계는 LLM 보상 설계의 효과를 정확히 검증하는데 주력한 것으로, 멀티모달 로봇 학습의 한 요소기술로 LLM을 자리매김시킨다는 의미가 있습니다.

요약하면, Text2Touch는 “촉각까지 아우르는 LLM 기반 보상 자동화”라는 독자적인 아이디어를 통해, 기존 인간 전문가 기반의 방법론을 능가하는 새로운 로봇 학습 패러다임을 제시했다는 점에서 관련 연구들과 차별화됩니다.

## 논문의 장점과 한계점

### 장점

이 논문의 가장 큰 강점은 높은 효율성과 성능을 둘 다 잡았다는 것입니다. 인간 전문가가 며칠씩 고민하며 만들 법한 복잡한 보상 함수를 LLM이 짧은 시간에 자동 생성했고, 그 결과로 나온 정책이 성능까지 더 좋았습니다. 이는 로봇 학습에서 개발 사이클을 획기적으로 단축시킬 잠재력을 보여줍니다. 저자에 따르면, LLM을 활용하면 새로운 과제에 대해 “개념에서 배포까지” 걸리는 시간을 크게 줄일 수 있다고 합니다. 둘째, LLM이 만들어낸 보상 함수는 코드가 단순명료하여 해석 가능성(interpretability)이 높습니다. 복잡한 보상은 종종 의도치 않은 부작용을 일으키지만, Text2Touch의 자동 보상들은 사람이 읽어봐도 이해하기 쉬운 형태였고, 이는 추후 보상 구조를 수정하거나 디버깅하기에도 용이합니다. 셋째, 실제 로봇 실험으로 검증했다는 점도 강점입니다. 시뮬레이션 상의 좋은 결과가 현실에서도 이어질 수 있다는 것을 촉각 센싱이라는 까다로운 조건 하에 증명한 것은 큰 성과입니다. 넷째, 다양한 LLM 모델(예: GPT-4, Gemini, Llama 등)을 시도하고 서로 비교分析함으로써, 어떤 모델이 왜 더 나은 보상을 설계하는지 통찰을 제시하려 한 점도 눈에 띕니다. 마지막으로, 강화학습 커리큘럼과 지식 증류 등 기존 로봇학습 기법들과 LLM 보상 설계를 잘 통합하여 안정적인 학습을 구현한 것도 장점입니다. 예컨대, LLM 보상만 바꾸고 나머지 조건은 동일하게 통제함으로써 공정한 비교를 했고, 결과적으로 LLM 보상의 이점이 명확히 드러났습니다.

### 한계

반면, 몇 가지 제한사항도 존재합니다. 첫째, 프롬프트 엔지니어링에 대한 의존성입니다. LLM이 유용한 보상 함수를 만들어내기까지 저자들은 프롬프트 설계를 여러 번 수정해야 했습니다. 단순한 설명으로는 LLM이 문제를 제대로 이해하지 못했고, 지나치게 상세한 프롬프트는 오히려 편향된 보상을 만들었다고 합니다 (예: 특정 성능 지표만 극대화하려는 편향). 결국 좋은 결과를 얻으려면 여전히 인간의 프롬프트 설계 노하우가 필요하며, 이는 완전한 자동화라고 보기 어렵습니다. 둘째, LLM 자체에 대한 의존성입니다. 실험 결과를 보면 거대 모델(GPT-4 등)은 우수한 보상을 찾았지만, 작은 모델(o3-mini 등)은 성능이 떨어졌습니다. 이는 결국 현 시점에서는 성능 좋은 대형 LLM (종종 유료 서비스)을 필요로 한다는 뜻이며, 모델 접근성이 제한될 수 있습니다. 셋째, 현 방법은 보상 함수 설계만 자동화한 것이지, 정책 학습 자체의 시간/비용은 그대로 듭니다. 시뮬레이터에서 여러 후보 보상을 테스트하려면 그만큼 다수의 RL 훈련을 병렬로 돌려봐야 하고, 최종 선발된 보상으로 다시 충분한 학습과 현실 도메인 적응 단계를 거쳐야 합니다. 논문에서도 5종 LLM * 4종 프롬프트 전략 = 20여 개 보상 함수를 실험한 것으로 나오는데, 이는 상당한 계산 자원을 요합니다. 넷째, 일반화 범위의 한계입니다. 본 연구는 “물체 회전”이라는 비교적 명확한 목표에 집중했습니다. 다른 유형의 과제(예: 조립 작업이나 도구 사용 등)는 보상 구조가 더욱 복잡할 수 있는데, LLM이 이런 복잡한 목표까지 제대로 보상으로 표현할 수 있을지는 미지수입니다. 또한 촉각 이외의 센서(예: 소리, 온도 등)를 다루거나, 로봇 팔과 손의 협조 제어처럼 범위가 넓어지면 현 기법을 그대로 적용하기 어려울 수 있습니다. 저자들도 미래 과제로 새로운 작업과 센서로의 일반화 문제를 언급하며, 복잡한 다단계 작업에 대해서는 추가 연구가 필요함을 인정했습니다. 다섯째, 안전성 및 현실 구현 이슈도 있습니다. LLM이 생성한 보상 함수가 로봇 하드웨어에 무리를 주는 행동(예를 들어 과도한 힘을 가하도록 유도)으로 이어질 가능성도 배제할 수 없습니다. 이를 막으려면 프롬프트나 환경에서 별도의 제약을 걸어야 하는데, 이런 부분은 논문에서 크게 다루지 않았습니다. 끝으로, 이 접근법이 진정 인간 전문가의 완전한 대체가 될 수 있을지는 아직 열린 질문입니다. 보상 설계 이외에도 하이퍼파라미터 튜닝, 알고리즘 선택 등 사람의 개입 지점이 남아 있고, LLM이 항상 최적의 보상을 만들어준다고 보장할 수도 없습니다.

그럼에도 불구하고, Text2Touch는 현재 한계들에도 불구하고 LLM 활용의 실질적 유용성을 로봇 분야에 증명한 사례로 평가할 수 있습니다. 보상 함수 설계처럼 사람의 노력이 많이 드는 문제에 AI 조수를 투입함으로써 성과를 높인 선구적인 예시이며, 향후 이를 개선·확장하는 연구가 다수 뒤따를 것으로 기대됩니다.

## 마무리

사람이 눈을 감고도 손안에서 물체를 이리저리 굴릴 수 있는 건, 촉각으로 느끼는 정보 덕분입니다. 로봇도 카메라로 보는 것만으로는 한계가 있어서, 손가락에 작은 카메라와 젤리 같은 센서를 달아 물체의 미끄러짐이나 힘을 “촉감”으로 느낄 수 있게 만들었습니다. 이제 로봇에게 “이 물체를 손 안에서 돌려봐”라고 가르치려면, 로봇이 어떨 때 잘했고 어떨 때 못했는지 채점을 해줘야 합니다. 이 채점 기준을 강화학습의 보상 함수라고 부릅니다. 예컨대 “물체를 많이 회전하면 점수를 높게 주고, 떨어뜨리면 큰 벌점을 준다” 같은 규칙이 보상 함수에 해당합니다. 문제는, 이렇게 점수를 매기는 규칙을 사람이 일일이 만들어주기가 무척 어렵다는 점입니다. 너무 단순하게 주면 로봇이 엉뚱한 행동을 하게 되고, 세세하게 짜주자니 경우의 수가 너무 많아 복잡해집니다.

여기서 대규모 언어 모델(LLM), 쉽게 말해 똑똑한 인공지능 비서에게 도움을 청해보자는 게 이 논문의 아이디어입니다. 사람 대신 “이러이러하게 점수를 매겨줘” 하고 LLM에게 시키는 것이죠. 연구자들은 로봇 손의 상태를 나타내는 여러 숫자들(센서 값들)을 LLM에게 설명해주고, “목표는 물체를 잘 돌리는 것이다”라고 알려준 다음 알맞은 채점 프로그램(보상 함수)을 코드로 써달라고 부탁했습니다. 그러자 놀랍게도, LLM이 사람 전문가가 짠 것보다 짧고 간단한 채점 프로그램을 만들어냈습니다. 예를 들어 사람은 수십 가지 조건을 고려해 1000줄이 넘는 복잡한 코드를 짰는데, LLM은 불과 30줄 남짓한 코드로 핵심을 표현해낸 것이죠. 물론 그냥 한 번에 성공한 것은 아니고, 연구자들이 “이 부분은 보너스로 줘”, “이 상황엔 패널티를 빼먹지 마” 등 몇 가지 힌트를 주면서 여러 번 시도하여 얻은 결과입니다.

이렇게 AI가 만들어준 보상으로 로봇을 학습시켜 봤더니, 로봇이 물체를 더 빨리 그리고 안정적으로 돌리는 것을 확인했습니다. 원래 사람이 만든 보상으로 학습시킨 로봇은 너무 조심스럽게 움직여서 시간도 오래 걸리고 회전 횟수도 적었는데, AI 보상으로 학습한 로봇은 약간 과감하게 움직이면서도 물체를 떨어뜨릴 것 같으면 재빨리 자세를 고쳐 잡는 영리한 행동을 보였습니다. 그 결과 회전 속도가 이전보다 약 30~40% 빨라졌습니다. 쉽게 말해, AI가 채점 기준을 잘 만들어주니 로봇 학생이 더 훌륭하게 과제를 해낸 셈입니다.

이 연구가 의미하는 바는, 앞으로 로봇공학 분야에서도 복잡한 설정값이나 규칙들을 사람이 일일이 손대기보다 AI의 도움을 받아 자동 생성하는 방향으로 나아갈 수 있다는 것입니다. 특히 다양한 감각(시각, 촉각 등)을 활용하는 어려운 문제에서도 효과가 입증되었기에, 추후 가사로봇이나 제조로봇이 스스로 배워나갈 때 인간의 부담을 덜어줄 기술로 발전할 수 있습니다. 물론 아직은 사람이 프롬프트를 잘 짜줘야 하고, 모든 과제에 다 적용되지는 않겠지만, Text2Touch는 로봇에게 “이렇게 하면 잘했어!”라고 알려주는 방법을 똑똑한 AI에게 맡겨본 흥미로운 시도이자 한 걸음 진보라 할 수 있습니다.
