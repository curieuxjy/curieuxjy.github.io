---
title: "📃IBRL 리뷰"
date: 2025-10-29
categories: [deepseek, llm]
toc: true
number-sections: False
description: Contexts Optical Compression
---

- [Paper Link](https://arxiv.org/abs/2510.18234)
- [Code](https://github.com/deepseek-ai/DeepSeek-OCR)


1. 💡 DeepSeek-OCR은 대규모 언어 모델(LLM)의 긴 컨텍스트 처리 문제를 해결하기 위해 텍스트 정보를 시각적 모달리티를 통해 효율적으로 압축하는 새로운 접근 방식을 제시합니다.
2. ✨ 이 모델은 고해상도 입력에서도 낮은 활성화 메모리와 최소한의 비전 토큰을 유지하며 높은 압축률을 달성하는 DeepEncoder와 DeepSeek3B-MoE 디코더로 구성됩니다.
3. 🚀 DeepSeek-OCR은 10배 압축률에서 97%의 OCR 정확도를 보이고, OmniDocBench에서 적은 비전 토큰으로 기존 SOTA 모델들을 능가하며, long-context 압축 및 LLM의 forgetting mechanism 연구에 큰 가능성을 보여줍니다.


<center>
<img src="../../images/2025-10-29-deepseek-ocr/fig1.png" width="100%" />
</center>

---

# Brief Review

DeepSeek-OCR은 대규모 언어 모델(LLM)이 긴 텍스트를 처리할 때 직면하는 계산 문제를 해결하기 위해 시각적 양식을 통한 "contexts optical compression"의 가능성을 탐구하는 연구입니다. 이 논문은 이미지를 사용하여 텍스트 정보를 효율적으로 압축하는 방법을 제시하며, 이를 위해 DeepEncoder와 DeepSeek3B-MoE-A570M 디코더로 구성된 DeepSeek-OCR 모델을 소개합니다.

**핵심 방법론 (Core Methodology)**

DeepSeek-OCR의 핵심은 다음과 같은 구성 요소와 훈련 파이프라인으로 이루어져 있습니다.

1.  **DeepEncoder**: 이 모델의 핵심 컴포넌트로, 고해상도 입력에서도 낮은 activation memory와 적은 vision token 수를 유지하면서 높은 압축률을 달성하도록 설계되었습니다.
    *   **Architecture of DeepEncoder**: DeepEncoder는 두 가지 주요 컴포넌트를 직렬로 연결합니다.
        *   **Visual Perception Feature Extraction Component**: Window attention 기반으로 SAM-base (80M 파라미터, patch-size 16)를 사용합니다. 이는 대량의 vision token을 처리하는 데 효과적입니다.
        *   **Visual Knowledge Feature Extraction Component**: Dense global attention 기반으로 CLIP-large (300M 파라미터)를 사용합니다. CLIP의 첫 번째 patch embedding layer는 제거됩니다.
        *   **16x Token Compressor**: 이 두 컴포넌트 사이에 위치하며, 2-layer convolutional module로 구성됩니다. 각 convolutional layer는 kernel size 3, stride 2, padding 1을 가지며 채널 수를 256에서 1024로 증가시킵니다. 예를 들어, 1024x1024 이미지는 SAM에 의해 4096개의 patch token으로 분할되고, compressor를 거쳐 256개의 token으로 압축되어 CLIP으로 전달됩니다. 이는 activation memory를 제어하고 token 수를 효과적으로 줄입니다.
    *   **Multiple resolution support**: 다양한 압축률과 실제 적용을 위해 DeepEncoder는 여러 해상도 모드를 지원합니다.
        *   **Native Resolution Modes**: Tiny (512x512, 64 tokens), Small (640x640, 100 tokens), Base (1024x1024, 256 tokens), Large (1280x1280, 400 tokens). Tiny 및 Small 모드는 이미지를 직접 resize하고, Base 및 Large 모드는 원본 이미지의 aspect ratio를 유지하기 위해 padding을 적용합니다. 유효 vision token 수는 다음 공식으로 계산됩니다:
            $N_{valid} = \lceil N_{actual} \times [1 - ((max(w, h) - min(w, h))/(max(w, h)))] \rceil$
            여기서 $w$와 $h$는 원본 이미지의 너비와 높이를 나타냅니다.
        *   **Dynamic Resolution Modes (Gundam, Gundam-Master)**: 여러 native resolution을 조합하여 초고해상도 입력을 처리합니다. 예를 들어, Gundam 모드는 $n \times 640 \times 640$ 크기의 local view 타일과 $1024 \times 1024$ 크기의 global view로 구성되며, 총 vision token 수는 $n \times 100 + 256$입니다. 이는 InternVL2.0의 타일링(tiling) 방식을 따르며, 추가적인 window attention 형태로 activation memory를 줄입니다.

2.  **MoE Decoder**: DeepSeekMoE [19, 20]의 DeepSeek-3B-MoE 아키텍처를 사용합니다. 추론 시 64개의 expert 중 6개와 2개의 shared expert가 활성화되어 약 570M의 활성 파라미터를 가집니다. 디코더는 DeepEncoder에서 압축된 latent vision token $\mathbf{Z} \in \mathbb{R}^{n \times d_{latent}}$으로부터 원본 텍스트 표현 $\hat{\mathbf{X}} \in \mathbb{R}^{N \times d_{text}}$을 재구성합니다:
    $f_{dec} : \mathbb{R}^{n \times d_{latent}} \to \mathbb{R}^{N \times d_{text}}; \hat{\mathbf{X}} = f_{dec}(\mathbf{Z})$
    여기서 $n \le N$이며, $f_{dec}$는 OCR 스타일 훈련을 통해 효과적으로 학습될 수 있는 비선형 매핑을 나타냅니다.

3.  **Data Engine**: DeepSeek-OCR은 복잡하고 다양한 훈련 데이터로 구성됩니다.
    *   **OCR 1.0 data**: 전통적인 OCR (장면 이미지 OCR, 문서 OCR) 데이터로, 30M 페이지의 PDF 데이터 (100개 언어, 중국어/영어 25M, 기타 5M)를 포함합니다. Coarse annotations (fitz로 추출)와 Fine annotations (PP-DocLayout, MinuerU, GOT-OCR2.0으로 라벨링한 2M 중국어/영어 페이지, 600K 소수어 페이지)으로 구성됩니다.
    *   **OCR 2.0 data**: 복잡한 인공 이미지 분석 (차트, 화학식, 평면 기하학) 데이터로, OneChart 방식을 따른 10M 차트 데이터 (HTML 테이블 형식), PubChem SMILES 형식의 5M 화학식 데이터, Slow Perception을 따른 1M 평면 기하학 데이터로 구성됩니다.
    *   **General vision data**: CLIP의 사전 훈련 이점을 활용하여 캡션, 감지, grounding과 같은 일반적인 이미지 이해 능력을 주입합니다. 전체 데이터의 20%를 차지합니다.
    *   **Text-only data**: 모델의 언어 능력을 보장하기 위해 10%의 내부 텍스트 전용 데이터를 포함합니다 (8192 토큰 길이).

4.  **Training Pipelines**: 두 단계로 이루어집니다.
    *   **Training DeepEncoder**: Vary의 방식을 따라 compact language model과 next token prediction 프레임워크를 사용하여 DeepEncoder를 훈련합니다. 모든 OCR 1.0, 2.0 데이터와 LAION에서 샘플링한 100M 일반 데이터를 사용하며, AdamW optimizer와 cosine annealing scheduler로 2 epoch 훈련합니다.
    *   **Training DeepSeek-OCR**: DeepEncoder가 준비된 후, DeepSeek-OCR 전체 모델을 훈련합니다. Pipeline parallelism (4 부분 분할)을 사용하며, SAM과 compressor는 PP0에 고정(freeze)하고, CLIP 부분은 PP1에 배치하여 가중치를 학습합니다. DeepSeek3B-MoE 디코더는 12개 레이어를 PP2와 PP3에 각각 6개씩 배치합니다. 20개의 노드 (각 A100-40G GPU 8개)에서 DP 40, global batch size 640으로 훈련하며, AdamW optimizer를 사용합니다.

**실험 결과 및 중요성 (Experimental Results and Significance)**

*   **Vision-text Compression Study (Fox benchmark)**: DeepSeek-OCR은 텍스트가 풍부한 문서에 대한 압축-비압축 능력을 검증했습니다. 텍스트 토큰 수의 10배 이내의 압축률($<10\times$)에서는 97%에 달하는 OCR 디코딩 정확도를 달성했으며, 20배 압축에서도 약 60%의 정확도를 유지했습니다. 이는 LLM의 historical long-context compression 및 memory forgetting 메커니즘 연구에 상당한 가능성을 제시합니다.
*   **OCR Practical Performance (OmniDocBench)**: DeepSeek-OCR은 실제 문서 분석 작업에서 강력한 성능을 보여주었습니다.
    *   100개의 vision token (640x640 해상도)만으로 GOT-OCR2.0 (256 tokens)을 능가합니다.
    *   400개의 token (1280x1280 해상도, 285개의 유효 토큰)으로 SOTA 모델들과 대등한 성능을 보입니다.
    *   800개 미만의 token (Gundam 모드)을 사용하여 약 7,000개의 vision token이 필요한 MinerU2.0을 능가합니다.
    *   신문과 같은 문서에서는 높은 텍스트 토큰 수(4-5천)로 인해 Gundam 또는 Gundam-Master 모드가 필요했습니다.
*   **Qualitative Study (Deep Parsing, Multilingual Recognition, General Vision Understanding)**:
    *   **Deep parsing**: 차트, 기하학적 도형, 화학식, 자연 이미지 등 문서 내의 다양한 시각 정보를 "deep parsing"할 수 있습니다. 예를 들어, 금융 보고서의 차트에서 구조화된 결과를 추출하고, 화학 문서의 화학식을 SMILES 형식으로 변환합니다.
    *   **Multilingual recognition**: 약 100개 언어의 PDF 문서를 처리할 수 있으며, 레이아웃 및 비-레이아웃 OCR 형식을 모두 지원합니다.
    *   **General vision understanding**: 이미지 설명, 객체 감지, grounding과 같은 일반적인 이미지 이해 능력도 제공합니다.

**결론 (Conclusion)**

DeepSeek-OCR은 시각적 양식을 통한 contexts optical compression의 타당성을 입증하며, 적은 수의 vision token으로부터 텍스트 토큰 수의 10배 이상을 효과적으로 디코딩할 수 있음을 보여줍니다. 이 연구는 미래 VLM 및 LLM 개발을 촉진할 잠재력을 가지고 있으며, 대규모 사전 훈련 데이터 생성을 위한 실용적인 모델로서의 가치도 입증되었습니다. 또한, 인간 기억의 망각 메커니즘을 모방하여 오래된 문맥을 점진적으로 해상도를 낮춰 압축함으로써 token 소비를 줄이는 방법을 제안하며, 이는 장기적인 문맥 처리의 새로운 방향을 제시합니다.

# Detail Review

> DeepSeek-OCR: Contexts Optical Compression 심층 분석 리뷰

## 연구의 배경과 동기

DeepSeek-AI가 발표한 DeepSeek-OCR 논문은 단순한 광학 문자 인식 모델의 개발을 넘어서, 대규모 언어 모델이 직면한 근본적인 계산 복잡도 문제에 대한 창의적 해결책을 모색하는 연구입니다. 현재 LLM들은 긴 텍스트를 처리할 때 시퀀스 길이의 제곱에 비례하는 계산 비용을 감당해야 하는데, 이는 실용적인 애플리케이션에서 심각한 병목 현상을 일으킵니다. 연구진은 이 문제를 해결하기 위해 완전히 다른 각도에서 접근합니다. 텍스트 정보를 시각적 양식으로 변환하여 압축할 수 있다면, 적은 수의 비전 토큰으로 많은 양의 텍스트 정보를 효율적으로 표현할 수 있지 않을까 하는 통찰에서 출발한 것입니다.

이 연구가 제기하는 핵심 질문은 매우 구체적이면서도 근본적입니다. "1000개의 단어가 포함된 문서를 디코딩하려면 최소 몇 개의 비전 토큰이 필요한가?" 이는 "한 장의 그림이 천 마디 말의 가치가 있다"는 오래된 격언을 정량적으로 검증하려는 시도이며, 동시에 비전-언어 모델의 토큰 효율성에 대한 실증적 연구입니다. 연구진은 OCR 작업을 이러한 비전-텍스트 압축 패러다임을 테스트하기 위한 이상적인 실험장으로 선택했습니다. OCR은 시각적 표현과 텍스트 표현 사이의 자연스러운 압축-복원 매핑을 제공하며, 정량적 평가 지표를 통해 명확한 성능 측정이 가능하기 때문입니다.

## 아키텍처의 혁신: DeepEncoder

DeepSeek-OCR의 핵심은 DeepEncoder라는 새로운 비전 인코더 아키텍처에 있습니다. 기존 오픈소스 비전-언어 모델들이 사용하는 인코더들은 각각 고유한 한계를 가지고 있었습니다. Vary와 같은 듀얼 타워 구조는 이중 이미지 전처리가 필요하고 파이프라인 병렬화가 어렵습니다. InternVL2.0과 같은 타일 기반 방식은 낮은 네이티브 해상도로 인해 대형 이미지가 과도하게 조각나고 많은 비전 토큰을 생성합니다. Qwen2-VL과 같은 적응형 해상도 인코더는 대형 이미지에서 막대한 활성화 메모리를 소비하고 극도로 긴 시퀀스 길이를 요구합니다.

DeepEncoder는 이러한 기존 접근법들의 한계를 극복하기 위해 독창적인 구조를 제안합니다. 약 380M 파라미터로 구성된 이 인코더는 두 개의 주요 경로를 직렬로 연결합니다. 첫 번째 경로는 80M 파라미터의 SAM-base 모델로 구성되며, 윈도우 어텐션을 중심으로 시각적 지각 특징을 추출합니다. 두 번째 경로는 300M 파라미터의 CLIP-large 모델로, 밀집 글로벌 어텐션을 통해 시각적 지식 특징을 추출합니다. 이 두 경로 사이에는 16배 압축을 수행하는 합성곱 모듈이 위치합니다.

이 설계의 핵심 통찰은 계산 비용의 분배에 있습니다. 1024×1024 이미지를 예로 들면, 초기 패치 분할 단계에서 $1024/16 \times 1024/16 = 4096$개의 패치 토큰이 생성됩니다. 이 많은 수의 토큰은 먼저 윈도우 어텐션 기반의 SAM을 거치는데, 윈도우 어텐션은 전체 시퀀스가 아닌 지역적 윈도우 내에서만 어텐션을 계산하므로 활성화 메모리가 상대적으로 낮습니다. 그 다음 16배 압축 모듈을 통과하면서 토큰 수가 $4096/16 = 256$개로 대폭 줄어듭니다. 이렇게 압축된 토큰들이 비로소 계산 비용이 높은 글로벌 어텐션 기반의 CLIP을 거치게 됩니다. 결과적으로 전체 활성화 메모리는 제어 가능한 수준으로 유지되면서도, 충분한 표현력을 확보할 수 있습니다.

압축 모듈 자체는 비교적 단순한 구조입니다. 커널 크기 3, 스트라이드 2, 패딩 1을 가진 2층의 합성곱 레이어로 구성되며, 채널 수는 256에서 1024로 증가합니다. 이는 Vary의 접근법을 차용한 것이지만, 전체 아키텍처 맥락에서 전략적 위치에 배치되어 효과를 극대화합니다.

## 다중 해상도 지원: 유연성과 실용성

DeepEncoder의 또 다른 중요한 특징은 다양한 해상도를 지원한다는 점입니다. 이는 단순히 기술적 편의성을 넘어서, 연구 질문 자체에 필수적인 요소입니다. 서로 다른 수의 비전 토큰으로 텍스트를 디코딩하는 성능을 비교하려면, 모델이 가변적인 토큰 수를 처리할 수 있어야 하기 때문입니다.

네이티브 해상도 모드는 네 가지로 구성됩니다. Tiny 모드는 512×512 해상도에서 64개의 비전 토큰을 생성하고, Small 모드는 640×640에서 100개, Base 모드는 1024×1024에서 256개, Large 모드는 1280×1280에서 400개의 토큰을 생성합니다. Tiny와 Small 모드는 상대적으로 낮은 해상도를 가지므로, 비전 토큰의 낭비를 피하기 위해 원본 이미지를 직접 리사이징합니다. 반면 Base와 Large 모드는 원본 이미지의 종횡비를 보존하기 위해 패딩 방식을 사용합니다.

패딩 방식을 사용할 때 흥미로운 문제가 발생합니다. 패딩된 영역은 실제 정보를 담고 있지 않으므로, 유효한 비전 토큰의 수는 실제 생성된 토큰 수보다 적습니다. 연구진은 이를 정확하게 계산하기 위한 공식을 제시합니다:

$$N_{valid} = \lceil N_{actual} \times [1 - \frac{\max(w,h) - \min(w,h)}{\max(w,h)}] \rceil$$

여기서 $w$와 $h$는 원본 입력 이미지의 너비와 높이입니다. 이 공식은 종횡비에 따라 얼마나 많은 패딩이 추가되었는지를 고려하여, 실제로 정보를 담고 있는 토큰의 비율을 계산합니다.

동적 해상도 모드는 실용적 응용을 위해 설계되었습니다. Gundam 모드는 $n \times 640 \times 640$ 타일들(로컬 뷰)과 하나의 $1024 \times 1024$ 글로벌 뷰를 결합합니다. 이는 InternVL2.0의 타일링 방법을 따르지만, DeepEncoder의 상대적으로 큰 네이티브 해상도 덕분에 이미지가 과도하게 조각나지 않습니다. 타일 수 $n$은 2에서 9 사이로 제어되며, Gundam 모드에서 생성되는 총 비전 토큰 수는 $n \times 100 + 256$입니다. 특히 너비와 높이가 모두 640보다 작은 이미지의 경우 $n = 0$으로 설정되어 Base 모드로 자동 퇴화합니다. Gundam-Master 모드는 더 큰 해상도($1024 \times 1024$ 로컬 뷰 + $1280 \times 1280$ 글로벌 뷰)를 사용하지만, 학습 속도 균형을 위해 별도로 훈련됩니다.

## 디코더 설계: 효율성과 표현력의 균형

디코더로는 DeepSeek-3B-MoE가 채택되었습니다. 이는 전략적으로 매우 적절한 선택입니다. 이 모델은 64개의 라우팅 전문가(routed experts) 중 6개만 활성화하고, 2개의 공유 전문가(shared experts)를 추가로 사용하는 Mixture-of-Experts 아키텍처입니다. 결과적으로 추론 시점에 약 570M 파라미터만 활성화됩니다. 이는 3B 규모 모델의 표현 능력을 가지면서도 500M 소형 모델 수준의 추론 효율성을 달성한다는 의미입니다.

디코더의 역할은 DeepEncoder가 생성한 압축된 잠재 비전 토큰들로부터 원본 텍스트 표현을 재구성하는 것입니다. 이는 다음과 같이 수학적으로 표현할 수 있습니다:

$$f_{dec} : \mathbb{R}^{n \times d_{latent}} \to \mathbb{R}^{N \times d_{text}}; \quad \hat{X} = f_{dec}(Z) \text{ where } n \leq N$$

여기서 $Z \in \mathbb{R}^{n \times d_{latent}}$는 DeepEncoder로부터의 압축된 잠재 토큰들이고, $\hat{X} \in \mathbb{R}^{N \times d_{text}}$는 재구성된 텍스트 표현입니다. 함수 $f_{dec}$는 소형 언어 모델도 OCR 스타일의 훈련을 통해 효과적으로 학습할 수 있는 비선형 매핑을 나타냅니다. 연구진은 대규모 LLM이 특화된 사전훈련 최적화를 통해 이러한 능력을 더욱 자연스럽게 통합할 수 있을 것이라고 합리적으로 추론합니다.

## 데이터 엔진: 다양성과 규모의 조화

DeepSeek-OCR의 강력한 성능은 정교하게 설계된 데이터 엔진에 크게 기인합니다. 연구진은 복잡하고 다양한 훈련 데이터를 구성했는데, 이는 크게 세 가지 카테고리로 나뉩니다. OCR 1.0 데이터는 장면 이미지 OCR과 문서 OCR 같은 전통적인 OCR 작업으로 구성됩니다. OCR 2.0 데이터는 차트, 화학 구조식, 평면 기하학 파싱 같은 복잡한 인공 이미지 파싱 작업을 포함합니다. 일반 비전 데이터는 모델에 일반적인 이미지 이해 능력을 주입하고 범용 비전 인터페이스를 보존하기 위해 사용됩니다.

문서 데이터는 DeepSeek-OCR의 최우선 순위입니다. 연구진은 인터넷에서 약 100개 언어를 포괄하는 3천만 페이지의 다양한 PDF 데이터를 수집했습니다. 중국어와 영어가 약 2천5백만 페이지를 차지하고, 나머지 언어들이 5백만 페이지를 구성합니다. 이 데이터에 대해 두 가지 유형의 ground truth가 생성됩니다. 거친 주석(coarse annotations)은 fitz를 사용하여 전체 데이터셋에서 직접 추출되며, 모델이 광학 텍스트를 인식하도록, 특히 소수 언어에서 학습하도록 설계되었습니다. 정밀 주석(fine annotations)은 중국어와 영어 각각 2백만 페이지에 대해 생성되는데, PP-DocLayout 같은 고급 레이아웃 모델과 MinerU 및 GOT-OCR2.0 같은 OCR 모델을 사용하여 감지와 인식이 인터리브된 데이터를 구성합니다.

소수 언어 처리에서 연구진은 창의적인 접근법을 사용합니다. 감지 부분에서는 레이아웃 모델이 일정한 일반화 능력을 가지고 있음을 발견했습니다. 인식 부분에서는 fitz를 사용하여 작은 패치 데이터를 생성하고, 이를 통해 GOT-OCR2.0을 훈련시킨 후, 훈련된 모델로 레이아웃 처리 후의 작은 패치들을 라벨링하는 모델 플라이휠(model flywheel) 방식을 채택합니다. 이를 통해 60만 개의 데이터 샘플을 생성합니다. 이는 데이터 엔지니어링에서 우수한 사례로, 제한된 리소스로 효과적으로 고품질 데이터를 생성하는 방법을 보여줍니다.

Word 데이터 3백만 건도 수집되었는데, 이는 콘텐츠를 직접 추출하여 레이아웃 없이 고품질 이미지-텍스트 쌍을 구성합니다. 이 데이터는 주로 수식과 HTML 형식 표에 유익합니다. 자연 장면 OCR의 경우, 모델은 주로 중국어와 영어를 지원합니다. 이미지 데이터는 LAION과 Wukong에서 가져오고, PaddleOCR을 사용하여 라벨링되며, 중국어와 영어 각각 1천만 개의 데이터 샘플이 구성됩니다.

OCR 2.0 데이터는 GOT-OCR2.0을 따라 차트, 화학 구조식, 평면 기하학 파싱 데이터를 포함합니다. 차트 데이터의 경우, OneChart를 참고하여 pyecharts와 matplotlib를 사용해 1천만 개의 이미지를 렌더링합니다. 주로 선형, 막대, 파이, 복합 차트 등 일반적으로 사용되는 유형들입니다. 중요한 설계 결정은 OneChart의 딕셔너리 형식 대신 HTML 테이블 형식을 라벨로 정의한 것인데, 이는 일정량의 토큰을 절약할 수 있습니다. 화학 구조식의 경우, PubChem의 SMILES 형식 데이터를 소스로 사용하고 RDKit을 사용하여 이미지로 렌더링하여 5백만 개의 이미지-텍스트 쌍을 구성합니다.

평면 기하학 이미지는 Slow Perception을 따라 생성됩니다. 구체적으로 perception-ruler 크기를 4로 사용하여 각 선분을 모델링합니다. 렌더링된 데이터의 다양성을 높이기 위해 기하학적 변환 불변 데이터 증강을 도입하는데, 동일한 기하학적 이미지를 원본 이미지에서 이동시키되, 좌표계의 중앙에 그려진 동일한 ground truth에 대응시킵니다. 이를 기반으로 총 1백만 개의 평면 기하학 파싱 데이터를 구성합니다.

일반 비전 데이터와 관련하여, DeepEncoder는 CLIP의 사전훈련 이득을 활용할 수 있고 일반적인 시각적 지식을 통합할 충분한 파라미터를 가지고 있습니다. DeepSeek-VL2를 따라 캡션, 감지, 그라운딩 같은 작업에 대한 관련 데이터를 생성합니다. 다만 DeepSeek-OCR은 범용 VLM 모델이 아니므로 이 부분의 데이터는 전체의 20%만 차지합니다. 이러한 유형의 데이터를 도입하는 주된 목적은 범용 비전 인터페이스를 보존하여, 향후 모델에 관심 있는 연구자들이 편리하게 작업을 진행할 수 있도록 하기 위함입니다.

모델의 언어 능력을 보장하기 위해 10%의 자체 텍스트 전용 사전훈련 데이터를 도입했으며, 모든 데이터는 8192 토큰 길이로 처리됩니다. 이는 DeepSeek-OCR의 시퀀스 길이이기도 합니다. 요약하면, DeepSeek-OCR을 훈련할 때 OCR 데이터가 70%, 일반 비전 데이터가 20%, 텍스트 전용 데이터가 10%를 차지합니다. 이러한 균형 잡힌 구성은 OCR 전문성을 유지하면서도 언어 능력과 일반적 비전 이해 능력을 확보하는 전략입니다.

## 훈련 파이프라인: 단순함 속의 효율성

DeepSeek-OCR의 훈련 파이프라인은 놀라울 정도로 단순하면서도 효과적입니다. 주로 두 단계로 구성됩니다. 첫 번째 단계는 DeepEncoder를 독립적으로 훈련하는 것이고, 두 번째 단계는 전체 DeepSeek-OCR을 훈련하는 것입니다. Gundam-Master 모드는 사전 훈련된 DeepSeek-OCR 모델에서 6백만 개의 샘플링된 데이터로 지속 훈련하여 얻어지지만, 훈련 프로토콜이 다른 모드와 동일하므로 상세한 설명은 생략됩니다.

DeepEncoder를 훈련할 때, Vary를 따라 소형 언어 모델을 활용하고 next token prediction 프레임워크를 사용합니다. 이 단계에서는 앞서 언급한 모든 OCR 1.0 및 2.0 데이터와 LAION 데이터셋에서 샘플링된 1억 개의 일반 데이터를 사용합니다. 모든 데이터는 배치 크기 1,280으로 2 에폭 동안 훈련되며, AdamW 옵티마이저와 코사인 어닐링 스케줄러를 사용하고 학습률은 5e-5입니다. 훈련 시퀀스 길이는 4,096입니다.

DeepEncoder가 준비되면, 섹션 3.4에서 언급한 데이터를 사용하여 DeepSeek-OCR을 훈련합니다. 전체 훈련 과정은 HAI-LLM 플랫폼에서 수행됩니다. 전체 모델은 파이프라인 병렬화(PP)를 사용하며 4개 부분으로 나뉩니다. DeepEncoder는 두 부분을 차지하고 디코더는 두 부분을 차지합니다. DeepEncoder의 경우, SAM과 압축기를 비전 토크나이저로 취급하여 PP0에 배치하고 파라미터를 동결합니다. CLIP 부분은 입력 임베딩 레이어로 취급하여 PP1에 배치하고 가중치를 동결하지 않고 훈련합니다. 언어 모델 부분의 경우, DeepSeek3B-MoE가 12개 레이어를 가지고 있으므로 PP2와 PP3에 각각 6개 레이어씩 배치합니다.

훈련에는 20개 노드(각각 8개의 A100-40G GPU)가 사용되며, 데이터 병렬화(DP)는 40이고 글로벌 배치 크기는 640입니다. AdamW 옵티마이저를 스텝 기반 스케줄러와 함께 사용하며 초기 학습률은 3e-5입니다. 텍스트 전용 데이터의 경우 훈련 속도는 하루 900억 토큰이고, 멀티모달 데이터의 경우 하루 700억 토큰입니다. 이러한 훈련 효율성은 파이프라인 병렬화와 적절한 하드웨어 활용의 결과입니다.

## 비전-텍스트 압축 연구: 핵심 발견

DeepSeek-OCR의 가장 중요한 기여 중 하나는 비전-텍스트 압축 비율에 대한 체계적이고 정량적인 분석입니다. 연구진은 Fox 벤치마크를 사용하여 모델의 압축-복원 능력을 검증합니다. Fox의 영어 문서 부분에서 ground truth 텍스트를 DeepSeek-OCR의 토크나이저(어휘 크기 약 129k)로 토크나이징하고, 600에서 1,300 토큰 사이의 문서들을 선택하여 테스트합니다. 이는 정확히 100페이지에 해당합니다.

텍스트 토큰 수가 크지 않으므로, Tiny와 Small 모드에서만 성능을 테스트합니다. Tiny 모드는 64개의 비전 토큰에 해당하고 Small 모드는 100개의 비전 토큰에 해당합니다. 프롬프트로는 레이아웃 없는 형식인 `<image>\nFree OCR.`을 사용하여 모델의 출력 형식을 제어합니다. 그럼에도 불구하고 출력 형식이 Fox 벤치마크와 완전히 일치할 수는 없으므로, 실제 성능은 테스트 결과보다 다소 높을 것입니다.

결과는 매우 고무적입니다. 64개의 비전 토큰을 사용하는 Tiny 모드에서, 600-700개의 텍스트 토큰을 가진 문서는 96.5%의 정확도와 10.5배의 압축 비율을 달성합니다. 700-800 토큰 범위에서는 93.8% 정확도와 11.8배 압축, 800-900 토큰에서는 83.8%와 13.2배 압축을 보입니다. 900-1000 토큰 범위에서는 85.9%와 15.1배, 1000-1100에서는 79.3%와 16.5배, 1100-1200에서는 76.4%와 17.7배, 1200-1300에서는 59.1%와 19.7배의 압축을 달성합니다.

100개의 비전 토큰을 사용하는 Small 모드에서는 성능이 크게 향상됩니다. 600-700 토큰 범위에서 98.5% 정확도와 6.7배 압축, 700-800에서 97.3%와 7.5배, 800-900에서 96.8%와 8.5배, 900-1000에서 96.8%와 9.7배, 1000-1100에서 91.5%와 10.6배, 1100-1200에서 89.8%와 11.3배, 1200-1300에서 87.1%와 12.6배의 압축을 보입니다.

이 결과에서 가장 중요한 통찰은 압축 비율이 약 10배 이내일 때 모델의 디코딩 정밀도가 약 97%에 달한다는 것입니다. 이는 매우 유망한 결과로, 향후 텍스트-이미지 접근법을 통해 거의 10배의 무손실 문맥 압축을 달성할 수 있음을 시사합니다. 압축 비율이 10배를 초과하면 성능이 저하되기 시작하는데, 이는 두 가지 이유 때문일 수 있습니다. 하나는 긴 문서의 레이아웃이 더 복잡해진다는 것이고, 다른 하나는 512×512 또는 640×640 해상도에서 긴 텍스트가 흐릿해진다는 것입니다. 첫 번째 문제는 텍스트를 단일 레이아웃 페이지에 렌더링하여 해결할 수 있고, 두 번째 문제는 망각 메커니즘의 특징이 될 것으로 연구진은 믿습니다.

토큰을 거의 20배로 압축할 때도 정밀도가 여전히 약 60%에 접근한다는 사실은 주목할 만합니다. 이러한 결과들은 광학 문맥 압축이 매우 유망하고 가치 있는 연구 방향임을 나타내며, 이 접근법은 멀티모달 시스템이 본질적으로 추가적인 비전 인코더를 필요로 하기 때문에 어떠한 오버헤드도 가져오지 않습니다.

## 실용적 OCR 성능 평가

DeepSeek-OCR은 실험적 모델에 그치지 않고 강력한 실용적 능력을 가지고 있으며, LLM과 VLM 사전훈련을 위한 대규모 데이터를 생성할 수 있습니다. OCR 성능을 정량화하기 위해 연구진은 OmniDocBench에서 DeepSeek-OCR을 테스트합니다. 결과는 매우 인상적입니다. 단 100개의 비전 토큰(640×640 해상도)만으로 256개의 토큰을 사용하는 GOT-OCR2.0을 능가합니다. 400개의 토큰(285개의 유효 토큰, 1280×1280 해상도)으로 이 벤치마크에서 최첨단 수준의 성능을 달성합니다. 800개 미만의 토큰(Gundam 모드)을 사용하여 거의 7,000개의 비전 토큰을 필요로 하는 MinerU2.0을 능가합니다.

편집 거리 측면에서 보면, DeepSeek-OCR의 다양한 모드는 서로 다른 정확도-효율성 트레이드오프를 제공합니다. Tiny 모드는 전체적으로 0.320의 편집 거리를 보이며, Small 모드는 0.205, Base 모드는 0.156, Large 모드는 0.117, Gundam 모드는 0.083, 그리고 200dpi로 보간된 Gundam-Master 모드는 0.077을 달성합니다. 비교하자면, MinerU2.0은 0.133의 편집 거리를 보이는데, 이는 DeepSeek-OCR의 Gundam 모드가 훨씬 적은 토큰으로 더 나은 성능을 달성함을 의미합니다.

문서 유형별로 분석하면 더욱 흥미로운 패턴이 드러납니다. 슬라이드는 단 64개의 비전 토큰으로도 0.116의 편집 거리를 달성할 수 있습니다. 책과 금융 보고서는 100개의 비전 토큰으로 각각 0.085와 0.079의 우수한 성능을 보입니다. 교과서, 시험지, 잡지, 학술 논문, 노트 등은 256개의 토큰(Base 모드)으로 만족스러운 성능을 얻을 수 있습니다. 반면 신문의 경우 Gundam 모드나 심지어 Gundam-Master 모드가 필요한데, 이는 신문의 텍스트 토큰이 4,000에서 5,000개로 다른 모드의 10배 압축을 훨씬 초과하기 때문입니다.

이러한 실험 결과들은 문맥 광학 압축의 경계를 더욱 명확하게 보여주며, VLM의 비전 토큰 최적화와 LLM의 문맥 압축 및 망각 메커니즘 연구에 효과적인 참고 자료를 제공할 수 있습니다. 연구진이 제시한 10배 압축 한계 이론은 다양한 문서 유형에서 실증적으로 검증됩니다.

## 심층 파싱 능력: 단순 OCR을 넘어서

DeepSeek-OCR의 독특한 강점 중 하나는 심층 파싱(deep parsing) 능력입니다. 이는 모델이 레이아웃과 OCR 2.0 능력을 모두 가지고 있어, 2차 모델 호출을 통해 문서 내 이미지를 더욱 파싱할 수 있다는 것을 의미합니다. 금융 연구 보고서 분야에서 DeepSeek-OCR의 심층 파싱 모드는 문서 내 차트의 구조화된 결과를 얻는 데 사용될 수 있습니다. 차트는 금융 및 과학 분야에서 중요한 데이터 표현 형식이며, 차트 구조화 추출은 미래 OCR 모델에 필수적인 능력입니다.

책과 논문의 경우, 심층 파싱 모드는 문서 내 자연 이미지에 대해 밀집 캡션을 출력할 수 있습니다. 단순한 프롬프트만으로 모델은 자동으로 어떤 유형의 이미지인지 식별하고 필요한 결과를 출력할 수 있습니다. 예를 들어 교실 장면 이미지를 제공하면, 모델은 "이미지는 어린이들과 성인이 있는 실내 교실 환경을 묘사합니다. 어린이들은 바닥에 앉아 있고, 서 있는 여성을 향하고 있으며, 여성은 그들에게 책을 읽거나 발표하는 것으로 보입니다"와 같은 상세한 설명을 생성합니다.

화학 문서의 경우, DeepSeek-OCR은 심층 파싱 모드에서 화학 구조식을 인식하고 SMILES 형식으로 변환할 수 있습니다. 이는 STEM 분야에서 VLM과 LLM 개발에 OCR 1.0과 2.0 기술이 중요한 역할을 할 수 있음을 보여줍니다. 평면 기하학의 경우, DeepSeek-OCR은 단순한 평면 기하학적 도형을 복사(구조화)할 수 있는 능력을 가지고 있습니다. 기하학적 형태에서 선분들 간의 복잡한 상호 의존성 때문에, 기하학 파싱 작업은 극도로 도전적이며 갈 길이 멉니다.

다국어 인식 능력도 주목할 만합니다. 인터넷의 PDF 데이터는 중국어와 영어뿐만 아니라 많은 양의 다국어 데이터를 포함하고 있으며, 이는 LLM 훈련 시에도 중요합니다. PDF 문서의 경우, DeepSeek-OCR은 거의 100개 언어를 처리할 수 있습니다. 중국어 및 영어 문서와 마찬가지로, 다국어 데이터도 레이아웃이 있는 OCR 형식과 없는 형식 모두를 지원합니다. 아랍어나 싱할라어 같은 언어로 된 소수 언어 문서도 서로 다른 프롬프트를 통해 레이아웃이 있거나 없는 출력을 지원할 수 있습니다.

DeepSeek-OCR은 또한 일정 수준의 일반적인 이미지 이해 능력을 제공합니다. 이미지 설명, 객체 탐지, 그라운딩 등의 작업을 수행할 수 있습니다. 텍스트 전용 데이터가 포함되어 있기 때문에 DeepSeek-OCR의 언어 능력도 유지됩니다. 다만 SFT 단계가 포함되지 않았기 때문에 모델은 챗봇이 아니며, 일부 능력은 완성 프롬프트로 활성화해야 합니다.

## 망각 메커니즘으로서의 광학 압축

연구진이 제시한 가장 흥미로운 이론적 통찰 중 하나는 광학 문맥 압축을 인간의 기억 망각 메커니즘과 유사한 것으로 해석한 것입니다. 망각 메커니즘은 인간 기억의 가장 근본적인 특성 중 하나입니다. 문맥 광학 압축 접근법은 이전 라운드의 역사적 텍스트를 이미지로 렌더링하여 초기 압축을 수행한 다음, 오래된 이미지를 점진적으로 리사이징하여 다단계 압축을 달성할 수 있습니다. 토큰 수는 점차 감소하고 텍스트는 점점 흐릿해져 텍스트 망각을 달성합니다.

시간에 따른 기억의 변화를 생각해보면, 방금 발생한 일은 매우 선명하게(crystal clear) 기억되지만, 1시간 전의 일은 매우 명확하게(very clear), 1일 전은 명확하게(clear), 1주일 전은 흐릿하게(blurry), 1개월 전은 매우 흐릿하게(very blurry), 1년 전의 일은 거의 사라진(almost gone) 상태로 기억됩니다. 이는 공간적 거리에 따른 시각적 지각의 저하와도 유사한 패턴을 보입니다. 10cm 거리에서는 매우 선명하지만, 50cm에서는 매우 명확, 1m에서는 명확, 3m에서는 흐릿, 10m에서는 매우 흐릿, 20m에서는 거의 보이지 않습니다.

이를 DeepSeek-OCR의 해상도 모드와 연결하면, Crystal Clear는 Gundam 모드, Very Clear는 Large 모드, Clear는 Base 모드, Blurry는 Small 모드, Very Blurry는 Tiny 모드, Almost Gone은 더욱 압축된 상태에 해당합니다. 이러한 메커니즘들을 결합하면, 문맥 광학 압축 방법은 생물학적 망각 곡선을 모방하는 일종의 기억 감쇠를 가능하게 합니다. 최근 정보는 높은 충실도를 유지하면서 먼 기억은 압축 비율 증가를 통해 자연스럽게 흐려집니다.

실용적 제안으로, 오래된 문맥에 대해서는 렌더링된 이미지를 점진적으로 다운사이징하여 토큰 소비를 더욱 줄일 수 있습니다. 이러한 가정은 시간 경과에 따른 인간의 기억 감쇠와 공간적 거리에 따른 시각적 지각 저하 사이의 자연스러운 평행성에서 영감을 받았습니다. 둘 다 유사한 점진적 정보 손실 패턴을 보입니다. 연구진의 초기 탐색은 확장 가능한 초장기 문맥 처리의 잠재력을 보여주는데, 최근 문맥은 높은 해상도를 보존하고 오래된 문맥은 더 적은 리소스를 소비합니다.

이 접근법은 정보 보존과 계산 제약 사이의 균형을 맞추면서 이론적으로 무한한 문맥 아키텍처로 가는 길을 제시하지만, 연구진은 이것이 추가 조사가 필요한 초기 단계 작업임을 인정합니다. 이러한 비전-텍스트 압축 시스템의 실용적 함의와 한계는 향후 연구에서 더 깊이 연구할 가치가 있습니다.

## 한계와 향후 연구 방향

연구진은 이 작업이 문맥 광학 압축의 경계에 대한 초기 탐색을 나타낸다고 명확히 밝힙니다. $N$개의 텍스트 토큰을 디코딩하는 데 몇 개의 비전 토큰이 필요한지 조사하는 것입니다. 예비 결과는 고무적입니다. DeepSeek-OCR은 약 10배 압축 비율에서 거의 무손실 OCR 압축을 달성하고, 20배 압축에서도 여전히 60%의 정확도를 유지합니다. 이러한 발견은 다중 턴 대화에서 $k$ 라운드 이상의 대화 히스토리에 대해 광학 처리를 구현하여 10배 압축 효율성을 달성하는 것과 같은 미래 응용을 위한 유망한 방향을 제시합니다.

그러나 현재의 작업에는 몇 가지 한계가 있습니다. OCR만으로는 진정한 문맥 광학 압축을 완전히 검증하기에 충분하지 않으며, 향후 디지털-광학 텍스트 인터리브 사전훈련, needle-in-a-haystack 테스트 및 기타 평가를 수행할 예정입니다. 다른 관점에서 보면, 광학 문맥 압축은 여전히 상당한 연구와 개선의 여지를 제공하며, 유망한 새로운 방향을 나타냅니다.

SFT 단계가 포함되지 않았기 때문에 모델은 챗봇이 아니며, 일부 기능은 완성 프롬프트로만 활성화됩니다. Fox 벤치마크에서 출력 형식이 ground truth와 완전히 일치하지 않아 실제 성능이 보고된 것보다 높을 수 있다는 점도 고려해야 합니다. 긴 문서의 복잡한 레이아웃이 성능 저하의 한 원인일 수 있으며, 이는 텍스트를 단일 레이아웃 페이지에 렌더링하여 해결할 수 있습니다.

향후 연구 방향으로는 여러 가지가 있습니다. 진정한 문맥 압축을 위해서는 디지털-광학 텍스트 인터리브 사전훈련이 필요하고, 긴 문맥 벤치마크에서의 평가, 에이전트 시스템과의 통합이 필요합니다. 망각 메커니즘을 더 깊이 연구하여 다양한 압축 비율에서의 정보 손실 특성을 규명하고, 중요도 기반 적응형 압축을 개발하며, 검색 증강 생성과 결합하는 것도 가능합니다. 다중 모달 통합 측면에서는 오디오, 비디오 등 다른 모달리티로 확장하고, 크로스 모달 압축 전략을 개발하며, 모달리티 간 정보 전송 효율성을 연구할 수 있습니다.

## 실용적 영향과 산업적 가치

DeepSeek-OCR의 실용적 가치는 연구적 기여만큼이나 중요합니다. 데이터 생성 능력면에서 매우 인상적인 수치를 보여줍니다. 20개 노드(각각 8개의 A100-40G GPU로 구성)를 사용하면 하루에 3천3백만 페이지의 데이터를 생성할 수 있습니다. 단일 A100-40G GPU로도 하루에 20만 페이지 이상을 처리할 수 있다는 것은 대규모 사전훈련 데이터셋 구축에 혁명적인 변화를 가져올 수 있음을 의미합니다. 이는 단순히 속도의 문제가 아니라, LLM과 VLM 커뮤니티가 고품질의 문서 데이터에 접근할 수 있는 가능성을 크게 확장시킵니다.

품질 측면에서도 DeepSeek-OCR은 여러 장점을 제공합니다. 정밀한 레이아웃 정보를 보존하면서 텍스트를 추출할 수 있어, 문서의 구조적 특성이 중요한 작업에 특히 유용합니다. 거의 100개 언어를 지원한다는 점은 다국어 LLM 훈련에 매우 중요한 기여입니다. 많은 언어에서 고품질 텍스트 데이터가 부족한 상황에서, PDF 문서로부터 직접 추출할 수 있다는 것은 상당한 가치가 있습니다. OCR 2.0 기능인 차트, 수식, 기하학 파싱은 STEM 분야의 문서를 처리할 때 필수적인 능력이며, 과학 문헌을 학습 데이터로 활용하려는 연구에 중요한 도구가 됩니다.

문서 처리 산업의 관점에서 보면, DeepSeek-OCR은 비용 절감과 정확도 향상을 동시에 달성합니다. 기존의 파이프라인 모델들은 감지, 인식, 레이아웃 분석 등 여러 단계를 거쳐야 했지만, DeepSeek-OCR은 통합된 엔드-투-엔드 솔루션을 제공합니다. MinerU2.0과 비교하면 약 1/8의 토큰으로 유사하거나 더 나은 성능을 달성하는데, 이는 추론 비용과 지연시간을 대폭 감소시킵니다. 현대의 API 기반 서비스에서 토큰 수는 직접적인 비용 요소이므로, 이러한 효율성 개선은 즉각적인 경제적 이익으로 이어집니다.

복잡한 문서 레이아웃을 정확하게 처리하고, 표, 차트, 수식 등 특수 요소를 정확하게 추출하는 능력은 기업 환경에서 매우 중요합니다. 금융 보고서, 과학 논문, 법률 문서 등은 모두 복잡한 구조를 가지고 있으며, 이러한 구조를 보존하면서 정보를 추출하는 것은 전통적인 OCR 시스템에서 항상 도전적인 과제였습니다. OmniDocBench에서 0.123의 편집 거리를 달성한 것은 실용적 배포에 충분한 수준의 정확도를 보여줍니다.

## 비전-언어 모델 연구에 대한 함의

DeepSeek-OCR이 제시하는 가장 중요한 이론적 기여는 비전-언어 모델을 LLM 중심의 관점에서 재해석했다는 것입니다. 기존의 VLM 연구는 주로 인간이 뛰어난 시각적 질문 답변(VQA), 이미지 캡셔닝 같은 작업에 초점을 맞췄습니다. 하지만 DeepSeek-OCR은 비전 인코더가 LLM의 텍스트 정보 처리 효율성을 어떻게 향상시킬 수 있는지에 주목합니다. 이는 근본적으로 다른 사고방식입니다. 비전 모달리티를 단순히 추가적인 입력 유형으로 보는 것이 아니라, 텍스트 정보를 더 효율적으로 표현하기 위한 압축 매체로 보는 것입니다.

이러한 관점은 VLM 아키텍처 설계에 중요한 시사점을 제공합니다. 전통적으로 VLM 연구자들은 비전 인코더의 성능을 이미지 이해 작업에서의 정확도로 평가했습니다. 하지만 DeepSeek-OCR이 제시하는 프레임워크에서는 비전 인코더의 중요한 평가 지표가 추가됩니다. 얼마나 적은 수의 토큰으로 얼마나 많은 텍스트 정보를 표현할 수 있는가? 이 질문은 특히 긴 문서를 처리하거나, 다중 턴 대화 히스토리를 관리하거나, 대규모 지식 베이스를 다루는 시스템에서 매우 중요합니다.

연구진이 제시한 10배 압축 경계는 향후 VLM 설계에 실용적인 가이드라인을 제공합니다. 거의 무손실 압축을 원한다면 텍스트 토큰 수의 약 1/10에 해당하는 비전 토큰을 할당해야 합니다. 일정 수준의 정보 손실을 허용할 수 있다면 20배까지 압축할 수 있습니다. 이러한 정량적 지침은 멀티모달 시스템에서 토큰 예산을 할당할 때 매우 유용합니다. 예를 들어, 제한된 컨텍스트 윈도우를 가진 LLM에서 얼마나 많은 공간을 비전 토큰에 할당해야 하는지 결정할 때 이 비율을 참고할 수 있습니다.

DeepEncoder의 설계 철학도 향후 비전 인코더 개발에 영향을 미칠 것입니다. 윈도우 어텐션과 글로벌 어텐션을 계층적으로 배치하고, 그 사이에 압축 레이어를 두는 전략은 활성화 메모리와 표현력 사이의 균형을 찾는 효과적인 방법입니다. 특히 고해상도 이미지를 처리해야 하는 경우, 모든 토큰에 글로벌 어텐션을 적용하는 것은 계산적으로 비현실적입니다. DeepEncoder가 보여준 것처럼, 초기에는 지역적 처리로 시작하고 압축 후에 전역적 처리를 수행하는 것이 효율적인 해결책이 될 수 있습니다.

## 데이터 엔지니어링의 교훈

DeepSeek-OCR의 성공은 단순히 아키텍처의 우수성만으로 설명되지 않습니다. 정교한 데이터 엔지니어링이 핵심적인 역할을 했습니다. 연구진이 OCR 1.0 데이터에서 사용한 이중 주석 전략은 매우 영리합니다. 전체 3천만 페이지에 대해서는 자동으로 추출한 거친 주석을 사용하여 기본적인 텍스트 인식 능력을 학습시키고, 중국어와 영어 각 2백만 페이지에 대해서는 고급 모델을 사용한 정밀 주석을 적용하여 레이아웃 이해와 정확한 인식 능력을 강화합니다. 이는 제한된 리소스로 대규모 데이터를 효과적으로 활용하는 방법을 보여줍니다.

소수 언어 처리에서 사용한 모델 플라이휠 접근법도 주목할 만합니다. 레이아웃 모델의 일반화 능력을 활용하고, 작은 패치 데이터로 GOT-OCR2.0을 훈련시킨 후, 그 모델로 더 많은 데이터를 생성하는 순환적 방식은 데이터 부족 문제를 창의적으로 해결합니다. 이는 특히 리소스가 제한된 언어에서 고품질 훈련 데이터를 확보하는 데 적용할 수 있는 일반적인 전략입니다.

OCR 2.0 데이터에서 차트 라벨링에 HTML 테이블 형식을 사용한 결정도 깊이 생각해볼 가치가 있습니다. OneChart의 딕셔너리 형식 대신 HTML 테이블을 선택한 것은 단순히 토큰을 절약하는 것 이상의 의미가 있습니다. HTML 테이블은 구조화된 데이터를 표현하는 널리 사용되는 표준 형식이므로, 모델이 학습한 표현이 다른 작업으로 전이되기 쉽습니다. 또한 웹 기반 애플리케이션에서 직접 사용할 수 있어 실용성이 높습니다.

평면 기하학 데이터에서 기하학적 변환 불변 증강을 도입한 것도 영리한 전략입니다. 동일한 기하학적 도형을 다른 위치에 배치하더라도 좌표계 중심에서 그려진 동일한 표현에 대응시킴으로써, 모델이 절대적 위치가 아닌 상대적 기하학적 관계를 학습하도록 유도합니다. 이는 데이터 효율성을 높이면서도 모델의 일반화 능력을 향상시킵니다.

데이터 구성의 균형(OCR 70%, 일반 비전 20%, 텍스트 10%)도 신중하게 설계된 것으로 보입니다. OCR 능력을 주된 목표로 하면서도 일반적인 시각적 이해와 언어 능력을 완전히 포기하지 않습니다. 이는 전문화된 모델을 만들면서도 향후 확장 가능성을 열어두는 현명한 접근입니다. 일반 비전 데이터를 20% 포함시킨 것은 특히 중요한데, 이는 연구자들이 DeepEncoder를 다른 비전-언어 작업에 파인튜닝할 수 있는 기반을 제공합니다.

## 벤치마크 결과의 심층 해석

Fox 벤치마크와 OmniDocBench에서의 결과를 종합적으로 분석하면 흥미로운 패턴이 드러납니다. Fox 벤치마크에서 압축 비율과 정확도의 관계는 거의 선형적이지 않습니다. 10배 압축까지는 성능이 매우 높게 유지되다가, 그 이후 급격한 저하를 보입니다. 이는 단순히 해상도의 문제만이 아니라, 정보 이론적 관점에서 해석할 수 있는 현상입니다. 일정 압축 비율을 넘어서면 비전 토큰이 담을 수 있는 정보량이 포화 상태에 도달하고, 추가적인 텍스트 정보는 손실될 수밖에 없습니다.

OmniDocBench의 문서 유형별 결과는 더욱 미묘한 통찰을 제공합니다. 슬라이드가 단 64개 토큰으로 좋은 성능을 보이는 이유는 슬라이드가 일반적으로 간결한 텍스트와 명확한 구조를 가지기 때문입니다. 반면 신문은 Gundam 모드가 필요한데, 이는 밀집된 텍스트, 다단 레이아웃, 다양한 폰트 크기 등 복잡한 특성 때문입니다. 이러한 차이는 실용적 응용에서 매우 중요합니다. 문서 유형에 따라 적절한 해상도 모드를 선택함으로써 계산 비용과 정확도 사이의 최적 균형을 찾을 수 있습니다.

편집 거리 지표 자체도 해석에 주의가 필요합니다. 편집 거리는 문자 수준의 차이를 측정하므로, 의미적으로는 동일하지만 형식이 다른 출력(예: 공백이나 줄바꿈의 차이)도 오차로 계산됩니다. 연구진이 언급했듯이, Fox 벤치마크에서 실제 성능은 보고된 것보다 높을 수 있습니다. 이는 OCR 평가의 근본적인 도전과제를 드러냅니다. 완벽한 평가 지표는 존재하지 않으며, 여러 지표와 정성적 분석을 종합해야 합니다.

다른 모델들과의 비교에서 DeepSeek-OCR의 강점은 명확합니다. GOT-OCR2.0보다 적은 토큰으로 더 나은 성능을 보이고, InternVL2-76B나 Qwen2.5-VL-72B 같은 대형 모델들보다 효율적입니다. MinerU2.0과의 비교는 특히 인상적인데, 약 1/8의 토큰으로 더 낮은 편집 거리를 달성합니다. 이는 DeepEncoder의 설계가 얼마나 효과적인지 보여줍니다. 단순히 더 많은 토큰을 사용하는 것이 아니라, 정보를 효율적으로 압축하고 표현하는 것이 중요함을 입증합니다.

## 이론적 깊이와 철학적 함의

DeepSeek-OCR 연구는 표면적으로는 기술적 문제를 다루지만, 깊이 들어가면 인지과학과 정보 이론의 근본적인 질문들과 연결됩니다. 망각 메커니즘과 광학 압축을 연결한 것은 단순한 비유가 아니라 깊은 통찰입니다. 인간의 기억은 시간이 지남에 따라 자연스럽게 압축되고 추상화됩니다. 최근의 경험은 생생한 세부 사항과 함께 기억되지만, 오래된 기억은 핵심적인 요소만 남고 세부 사항은 흐려집니다. 이는 정보 처리 시스템이 제한된 용량으로 무한한 정보 스트림을 다루는 방법에 대한 자연의 해결책입니다.

DeepSeek-OCR이 제안하는 다단계 압축 전략은 이러한 생물학적 메커니즘을 인공 시스템에 구현하려는 시도입니다. 최근 문맥은 고해상도로 유지하고, 시간이 지남에 따라 점진적으로 해상도를 낮추는 것은 중요도 기반 리소스 할당의 한 형태입니다. 이는 어텐션 메커니즘과도 철학적으로 연결됩니다. 어텐션은 현재 작업에 관련된 정보에 더 많은 계산 리소스를 할당하는 방법이고, 광학 압축은 시간적으로 관련성이 떨어지는 정보에 더 적은 표현 리소스를 할당하는 방법입니다.

정보 이론의 관점에서 보면, 압축은 중복성을 제거하고 본질적인 정보만 남기는 과정입니다. 10배 압축에서 거의 무손실이라는 결과는 텍스트 문서에 상당한 중복성이 존재함을 시사합니다. 자연어는 높은 예측 가능성을 가지고 있고, 문서의 구조적 패턴은 더욱 규칙적입니다. 비전 표현은 이러한 패턴을 2차원 공간적 구조로 인코딩함으로써 효율적인 압축을 달성할 수 있습니다.

흥미롭게도, 이는 문자의 발명 이전 인간이 정보를 저장하고 전달하던 방식으로의 회귀라고 볼 수도 있습니다. 상형문자나 그림 문자는 의미를 시각적 형태로 직접 표현했습니다. 알파벳과 같은 음성 문자의 발명은 추상화와 선형화를 가져왔지만, 정보 밀도 측면에서는 손실이 있었을 수 있습니다. DeepSeek-OCR이 하는 것은 어떤 의미에서 디지털 텍스트를 다시 시각적 형태로 변환하여 정보 밀도를 회복하는 것입니다.

## 에이전트 시스템과 장기 기억에 대한 함의

DeepSeek-OCR의 기술은 LLM 기반 에이전트 시스템에 중요한 함의를 가집니다. 현재 에이전트 시스템의 주요 한계 중 하나는 제한된 컨텍스트 윈도우입니다. 복잡한 작업을 수행하면서 축적되는 대화 히스토리, 중간 결과, 도구 호출 기록 등은 빠르게 컨텍스트를 채웁니다. 벡터 데이터베이스와 같은 외부 메모리 시스템이 부분적인 해결책을 제공하지만, 검색 기반 접근법은 관련성 판단이 어렵고 검색 오버헤드가 있습니다.

광학 압축 접근법은 다른 가능성을 열어줍니다. 오래된 대화 라운드를 텍스트로 렌더링하여 이미지로 변환하고, 시간이 지남에 따라 해상도를 줄여나가는 것입니다. 이는 여러 장점을 가집니다. 첫째, 원본 텍스트의 선형적 흐름과 구조가 시각적으로 보존됩니다. 둘째, LLM은 필요할 때 이 압축된 기억을 직접 처리할 수 있어 별도의 검색 단계가 필요 없습니다. 셋째, 압축 수준을 조정함으로써 메모리와 정확도 사이의 트레이드오프를 동적으로 제어할 수 있습니다.

장기 기억(long-term memory) 메커니즘의 관점에서도 중요한 시사점이 있습니다. 인간의 장기 기억은 단순히 모든 정보를 저장하는 것이 아니라, 중요한 정보는 강화하고 덜 중요한 정보는 약화시키는 동적 과정입니다. 광학 압축에서 해상도 조정은 이러한 메커니즘의 유사물이 될 수 있습니다. 중요한 대화나 문서는 높은 해상도로 유지하고, 참고용으로만 필요한 정보는 낮은 해상도로 압축합니다. 심지어 어떤 정보는 완전히 삭제하는 대신 극도로 압축하여 희미한 흔적만 남길 수 있습니다.

다중 에이전트 시스템에서는 공유 메모리 문제가 더욱 복잡해집니다. 여러 에이전트가 협력하여 작업을 수행할 때, 각자의 로컬 컨텍스트와 공유 히스토리를 관리하는 것은 도전적입니다. 광학 압축을 사용하면 공유 히스토리를 효율적으로 표현하여 모든 에이전트가 접근할 수 있습니다. 각 에이전트는 자신에게 관련된 부분에 대해서는 고해상도 버전을 유지하고, 다른 부분은 압축된 버전으로 인식할 수 있습니다.

## 훈련 효율성과 확장성

DeepSeek-OCR의 훈련 파이프라인은 대규모 멀티모달 모델 훈련의 모범 사례를 보여줍니다. 파이프라인 병렬화를 4단계로 나누어 DeepEncoder와 언어 모델을 효율적으로 배치한 것은 하드웨어 리소스를 최대한 활용하는 전략입니다. 특히 SAM과 압축기를 동결하고 CLIP만 훈련시키는 것은 계산 비용을 절약하면서도 모델이 새로운 데이터에 적응할 수 있도록 합니다. 이는 전이 학습의 원칙을 대규모 시스템에 적용한 좋은 예입니다.

HAI-LLM 플랫폼의 사용은 실용적인 선택이지만, 동시에 재현성에 대한 도전을 제시합니다. 특정 플랫폼에 의존하는 것은 다른 연구자들이 정확히 같은 조건에서 실험을 재현하기 어렵게 만들 수 있습니다. 그러나 연구진이 훈련 세부사항을 상세히 기술하고 모델 가중치를 공개한 것은 이러한 한계를 완화합니다. 다른 플랫폼에서 유사한 결과를 얻을 수 있는 충분한 정보를 제공합니다.

훈련 속도 수치(텍스트 전용 90B 토큰/일, 멀티모달 70B 토큰/일)는 멀티모달 데이터 처리의 추가 비용을 정량화합니다. 약 22%의 속도 저하는 비전 인코더 처리와 더 복잡한 데이터 로딩으로 인한 것입니다. 이는 합리적인 오버헤드이며, 얻어지는 능력을 고려하면 충분히 정당화됩니다. 대규모 사전훈련에서 이러한 효율성 고려는 매우 중요합니다. 며칠 또는 몇 주의 훈련 기간에서 20%의 차이는 상당한 시간과 비용의 차이를 의미하기 때문입니다.

확장성 측면에서 DeepSeek-OCR의 아키텍처는 여러 방향으로 확장 가능합니다. 더 큰 디코더를 사용하여 표현력을 높일 수 있고, DeepEncoder에 더 많은 파라미터를 추가하여 시각적 이해를 향상시킬 수 있습니다. 특히 MoE 아키텍처는 효율적인 확장을 가능하게 합니다. 전문가 수를 늘리면 모델 용량은 증가하지만 추론 시 활성화되는 파라미터는 크게 늘지 않습니다. 이는 DeepSeek 시리즈의 일관된 철학입니다.

## 비교 분석: 기존 접근법들과의 차별점

DeepSeek-OCR을 기존의 문서 처리 시스템 및 VLM들과 비교하면 뚜렷한 차별점이 드러납니다. 전통적인 파이프라인 모델들(Marker, Mathpix, MinerU 등)은 여러 전문화된 컴포넌트를 순차적으로 실행합니다. 텍스트 감지, OCR, 레이아웃 분석, 후처리 등의 단계를 거치는데, 각 단계에서 오류가 누적될 수 있고 전체 시스템의 복잡도가 높아집니다. DeepSeek-OCR의 엔드-투-엔드 접근법은 이러한 복잡성을 제거하고 전체 프로세스를 단일 모델로 통합합니다.

다른 엔드-투-엔드 OCR 모델들과 비교하면, DeepSeek-OCR의 주요 혁신은 압축 효율성에 있습니다. Nougat은 학술 논문에 특화되어 있고 2,352개의 토큰을 사용합니다. GOT-OCR2.0은 256개 토큰으로 효율적이지만 DeepSeek-OCR은 100개 토큰으로 더 나은 성능을 보입니다. 범용 VLM들(InternVL, Qwen-VL 등)은 강력한 일반 능력을 가지고 있지만 OCR 작업에서 수천 개의 비전 토큰을 사용하여 비효율적입니다. DeepSeek-OCR은 OCR에 특화되면서도 일반 비전 인터페이스를 유지하는 균형을 찾았습니다.

아키텍처 측면에서 DeepEncoder의 독특한 설계는 기존 인코더들의 장점을 결합합니다. Vary의 압축 모듈 아이디어를 차용하지만 듀얼 타워의 복잡성을 피합니다. InternVL의 타일링 개념을 사용하지만 더 큰 네이티브 해상도로 과도한 조각화를 방지합니다. Qwen-VL의 적응형 해상도를 지원하지만 윈도우 어텐션과 압축을 통해 활성화 메모리를 제어합니다. 이러한 절충적 설계는 각 접근법의 한계를 인식하고 보완한 결과입니다.

데이터 엔지니어링 측면에서도 DeepSeek-OCR은 차별화됩니다. 많은 OCR 모델들이 합성 데이터나 기존 데이터셋에 의존하는 반면, DeepSeek-OCR은 다양한 소스의 실제 문서를 대규모로 수집하고 다단계 주석 전략을 사용합니다. 거친 주석과 정밀 주석의 조합, 모델 플라이휠을 통한 소수 언어 데이터 생성, OCR 2.0 작업을 위한 합성 데이터 생성 등은 종합적인 데이터 전략을 보여줍니다.

## 제한사항과 개선 가능성

연구진이 솔직하게 인정하듯이, DeepSeek-OCR은 완성된 제품이 아니라 개념 증명(proof-of-concept)입니다. 여러 한계점이 있으며, 이들은 동시에 향후 연구의 기회를 나타냅니다. SFT 단계의 부재는 모델이 사용자 친화적인 챗봇이 아니라 연구 도구임을 의미합니다. 실용적 배포를 위해서는 지시 따르기, 대화 맥락 이해, 안전성 정렬 등의 추가 훈련이 필요합니다. 이는 기술적으로 어렵지 않지만 상당한 추가 노력이 필요합니다.

Fox 벤치마크에서 출력 형식 불일치 문제는 OCR 평가의 근본적 도전을 드러냅니다. 의미적으로 동일한 텍스트도 형식 차이(공백, 줄바꿈, 문장 부호 등)로 인해 편집 거리가 증가할 수 있습니다. 더 정교한 평가 지표가 필요하며, 의미적 유사성을 측정할 수 있는 방법(예: 임베딩 기반 유사도)을 함께 사용하는 것이 도움될 것입니다. 또한 작업별 평가도 중요합니다. 일부 응용에서는 완벽한 텍스트 재현이 필요하지만, 다른 경우에는 핵심 정보만 추출하면 충분할 수 있습니다.

10배 압축 경계에 대한 이론적 설명이 부족한 것도 한계입니다. 왜 정확히 10배인가? 이것이 텍스트의 본질적 중복성에서 기인하는가, 아니면 현재 모델의 능력 한계인가? 정보 이론적 분석을 통해 이론적 압축 한계를 도출하고, 현재 모델이 그 한계에 얼마나 근접했는지 평가하는 것이 흥미로울 것입니다. Shannon의 엔트로피 개념을 적용하여 문서 텍스트의 정보량과 비전 표현의 용량을 비교할 수 있습니다.

다른 모달리티로의 확장이 탐구되지 않은 것도 제한사항입니다. 텍스트-이미지 압축만 다루었지만, 유사한 원리가 오디오, 비디오, 3D 데이터 등에도 적용될 수 있을 것입니다. 예를 들어, 긴 오디오 대화를 스펙트로그램으로 변환하여 압축하거나, 비디오의 핵심 프레임을 선택하여 시각적으로 요약하는 것도 가능할 수 있습니다. 멀티모달 압축에 대한 통합된 프레임워크를 개발하는 것이 자연스러운 다음 단계입니다.

실제 LLM의 긴 컨텍스트 작업에서 검증이 이루어지지 않은 것도 한계입니다. 연구진은 OCR을 테스트베드로 사용했지만, 진정한 가치는 LLM이 긴 문서를 처리할 때 드러날 것입니다. Needle-in-a-haystack 테스트, 장문 요약, 다중 문서 질의응답 등의 작업에서 광학 압축의 효과를 검증해야 합니다. 특히 압축된 표현에서 특정 정보를 검색하는 능력, 여러 압축 수준의 정보를 통합하는 능력 등이 중요합니다.

## 재현성과 오픈 사이언스

DeepSeek-AI가 코드와 모델 가중치를 공개한 것은 오픈 사이언스의 정신을 잘 보여줍니다. 이는 연구 커뮤니티가 결과를 검증하고, 모델을 자신의 응용에 적용하며, 후속 연구를 진행할 수 있게 합니다. 특히 비전-언어 모델 분야에서 많은 최첨단 모델들이 폐쇄되어 있는 상황에서, DeepSeek-OCR의 공개는 귀중한 공헌입니다. 연구자들은 DeepEncoder 아키텍처를 다른 작업에 적용하거나, 데이터 엔지니어링 전략을 참고하거나, 압축 메커니즘을 개선할 수 있습니다.

그러나 완전한 재현성에는 여전히 도전이 있습니다. 훈련 데이터의 일부(특히 인터넷에서 수집한 PDF)는 저작권이나 접근성 문제로 공개될 수 없을 수 있습니다. HAI-LLM 플랫폼이 특정 인프라에 의존한다는 점도 다른 연구자들이 정확히 같은 조건을 재현하기 어렵게 만듭니다. 데이터 엔지니어링 파이프라인의 구체적인 구현 세부사항(예: 품질 필터링 기준, 데이터 클리닝 절차)도 더 상세히 문서화되면 도움이 될 것입니다.

벤치마크 결과의 재현성도 고려해야 합니다. Fox와 OmniDocBench는 공개 벤치마크이지만, 평가 스크립트, 전처리 방법, 하이퍼파라미터 등의 세부사항이 결과에 영향을 미칠 수 있습니다. 연구진이 평가 코드도 함께 공개하면, 다른 연구자들이 자신의 모델을 정확히 같은 조건에서 비교할 수 있습니다. 표준화된 평가 프로토콜은 문서 이해 분야의 발전에 중요합니다.

커뮤니티의 기여를 장려하는 것도 중요합니다. DeepSeek-OCR을 기반으로 한 확장, 개선, 응용 사례들이 공유되면, 모델의 가치는 배가됩니다. GitHub 저장소에서 이슈와 풀 리퀘스트를 활발히 관리하고, 사용자 커뮤니티를 육성하는 것이 장기적인 영향력에 기여할 것입니다. 특히 다양한 언어와 문서 유형에 대한 커뮤니티 기여는 모델의 일반화 능력을 향상시킬 수 있습니다.

## 결론: 패러다임의 전환

DeepSeek-OCR은 단순한 OCR 모델 이상의 의미를 가집니다. 이는 멀티모달 AI 시스템에서 정보를 표현하고 처리하는 방식에 대한 근본적인 재고를 촉구합니다. 전통적으로 비전과 언어는 분리된 모달리티로 취급되었고, VLM은 주로 시각적 정보를 언어로 번역하는 도구로 여겨졌습니다. DeepSeek-OCR은 다른 관점을 제시합니다. 시각적 표현이 언어 정보를 더 효율적으로 인코딩하는 매체가 될 수 있다는 것입니다.

10배 압축에서 거의 무손실 재구성이 가능하다는 발견은 실용적으로나 이론적으로나 중요합니다. 실용적으로는 LLM의 컨텍스트 제한을 완화하는 구체적인 방법을 제공합니다. 이론적으로는 정보 표현의 모달리티 간 효율성 차이에 대한 정량적 증거를 제공합니다. 20배 압축에서도 60%의 정확도를 유지한다는 것은 손실 압축의 가능성도 보여주며, 이는 망각 메커니즘과 연결됩니다.

DeepEncoder 아키텍처는 고해상도 비전 처리의 실용적 해결책을 제시합니다. 윈도우 어텐션, 압축, 글로벌 어텐션의 계층적 구성은 활성화 메모리와 표현력 사이의 균형을 찾는 효과적인 패턴입니다. 다중 해상도 지원은 연구와 응용 모두를 위한 유연성을 제공합니다. 이 설계 원칙들은 다른 비전-언어 모델 개발에도 적용될 수 있습니다.

데이터 엔지니어링의 정교함도 간과할 수 없습니다. 다양한 데이터 소스, 이중 주석 전략, 모델 플라이휠, 합성 데이터 생성 등의 조합은 대규모 멀티모달 모델 훈련의 복잡성을 잘 보여줍니다. 단순히 많은 데이터를 모으는 것이 아니라, 데이터의 다양성, 품질, 균형을 신중하게 설계하는 것이 중요함을 입증합니다.

망각 메커니즘으로서의 광학 압축이라는 개념적 프레임워크는 향후 연구의 풍부한 방향을 제시합니다. 이는 단순히 효율성의 문제가 아니라, AI 시스템이 시간적으로 확장되는 정보를 어떻게 관리하는지에 대한 근본적인 질문입니다. 인간의 인지 시스템에서 영감을 받은 이러한 접근법은 더 자연스럽고 효율적인 AI 시스템으로 이어질 수 있습니다.

DeepSeek-OCR의 진정한 가치는 제공하는 답보다 제기하는 질문에 있을 수 있습니다. 정보의 최적 표현이란 무엇인가? 모달리티 간 변환에서 무엇이 보존되고 무엇이 손실되는가? 압축과 추상화의 차이는 무엇인가? AI 시스템은 어떻게 제한된 리소스로 무한한 정보 스트림을 처리할 수 있는가? 이러한 질문들은 AI 연구의 핵심에 있으며, DeepSeek-OCR은 그에 대한 구체적이고 정량적인 탐구의 시작점을 제공합니다.
