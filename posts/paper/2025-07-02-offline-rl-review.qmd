---
title: "ğŸ“ƒOffline RL Survey ë¦¬ë·°"
date: 2025-07-02
categories: [rl, offline-rl, survey]
toc: true
number-sections: false
description: A Survey on Offline Reinforcement Learning - Taxonomy, Review, and Open Problems
---


- [Paper Link](https://arxiv.org/pdf/2203.01387)


1.  ì˜¤í”„ë¼ì¸ RL(Offline RL)ì€ í™˜ê²½ê³¼ì˜ ìƒí˜¸ì‘ìš© ì—†ì´ ì •ì  ë°ì´í„°ì…‹ë§Œì„ ì´ìš©í•´ í•™ìŠµí•˜ëŠ” íŒ¨ëŸ¬ë‹¤ì„ìœ¼ë¡œ, ì‹¤ì œ í™˜ê²½ ì ìš©ì— í•„ìˆ˜ì ì´ì§€ë§Œ ë°ì´í„° ë¶„í¬ ë³€í™”(distributional shift) ë¬¸ì œë¥¼ í•´ê²°í•´ì•¼ í•©ë‹ˆë‹¤.
2.  ë³¸ ë…¼ë¬¸ì€ ì˜¤í”„ë¼ì¸ RL ê¸°ë²•ì„ ë¶„ë¥˜í•˜ëŠ” ìƒˆë¡œìš´ Taxonomyë¥¼ ì œì•ˆí•˜ê³ , ìµœì‹  ì•Œê³ ë¦¬ì¦˜ ë° ë²¤ì¹˜ë§ˆí¬ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ê²€í† í•˜ë©° ë‹¤ì–‘í•œ ë°ì´í„° íŠ¹ì„±ì— ë”°ë¥¸ ê¸°ë²•ë³„ ì„±ëŠ¥ì„ ë¶„ì„í•©ë‹ˆë‹¤.
3.  ë”ë¶ˆì–´ ì˜¤í”„ë¼ì¸ ì •ì±… í‰ê°€(Off-Policy Evaluation, OPE)ë¥¼ í¬í•¨í•œ ë¯¸í•´ê²° ê³¼ì œë“¤ì„ ë…¼ì˜í•˜ê³  ë¶„ì•¼ì˜ í–¥í›„ ì—°êµ¬ ë°©í–¥ì— ëŒ€í•œ í†µì°°ì„ ì œê³µí•©ë‹ˆë‹¤.

---

# Brief Review

ë³¸ ë…¼ë¬¸ì€ ì •ì  ë°ì´í„°ì…‹($\mathcal{D}$)ìœ¼ë¡œë¶€í„° í•™ìŠµí•˜ë©° í™˜ê²½ê³¼ì˜ ì¶”ê°€ ìƒí˜¸ì‘ìš© ì—†ì´ ì •ì±…($\pi_{\text{off}}$)ì„ ë„ì¶œí•˜ëŠ” Offline Reinforcement Learning (ì˜¤í”„ë¼ì¸ ê°•í™”í•™ìŠµ) ë¶„ì•¼ì— ëŒ€í•œ í¬ê´„ì ì¸ ì„œë² ì´ ë…¼ë¬¸ì…ë‹ˆë‹¤. ì˜¨ë¼ì¸ ë˜ëŠ” Off-policy RL (ì˜¤í”„-í´ë¦¬ì‹œ ê°•í™”í•™ìŠµ)ê³¼ ë‹¬ë¦¬ ì˜¤í”„ë¼ì¸ RLì€ ê³ ë¹„ìš© ë˜ëŠ” ìœ„í—˜ì„±ìœ¼ë¡œ ì¸í•´ í™˜ê²½ ìƒí˜¸ì‘ìš©ì´ ì–´ë ¤ìš´ ì‹¤ì œ ì‘ìš© ë¶„ì•¼(ì˜ˆ: êµìœ¡, í—¬ìŠ¤ì¼€ì–´, ë¡œë³´í‹±ìŠ¤)ì— íŠ¹íˆ ìœ ìš©í•©ë‹ˆë‹¤.

ì˜¤í”„ë¼ì¸ RLì˜ í•µì‹¬ ê³¼ì œëŠ” í•™ìŠµëœ ì •ì±…($\pi_{\theta}$)ì´ í›ˆë ¨ ë°ì´í„°ì˜ ë¶„í¬($\pi_{\beta}$ë˜ëŠ”$d^{\pi_\beta}$)ì—ì„œ ë²—ì–´ë‚  ë•Œ ë°œìƒí•˜ëŠ” Distributional Shift (ë¶„í¬ ë³€í™”) ë¬¸ì œì…ë‹ˆë‹¤. íŠ¹íˆ function approximatorì˜ ê³¼ëŒ€ ì¶”ì •(overestimation)ê³¼ ì˜¤ì°¨ ëˆ„ì (compounding error)ì´ ë¬¸ì œê°€ ë©ë‹ˆë‹¤. ê°€ì¹˜ ê¸°ë°˜ ë°©ë²•(value-based method)ì˜ ê²½ìš°, ë²¨ë§Œ ì—ëŸ¬(Bellman error) ìµœì†Œí™” ëª©í‘œ í•¨ìˆ˜

$$J(\phi) = \mathbb{E}_{s, a, s' \sim \mathcal{D}}[(r(s, a) + \gamma \mathbb{E}_{a' \sim \pi_{\text{off}}(\cdot|s')}[Q^{\pi}_{\phi}(s', a')] - Q^{\pi}_{\phi}(s, a))^2]$$

ì—ì„œ$a'$ì´ ë°ì´í„°ì…‹ì˜ í–‰ë™ ë¶„í¬$\pi_{\beta}$ì™€ ë‹¤ë¥¼ ë•Œ ë¬¸ì œê°€ ë°œìƒí•©ë‹ˆë‹¤.

## ì¢…ë¥˜

ë…¼ë¬¸ì€ ì˜¤í”„ë¼ì¸ RL ë°©ë²•ë¡ ì„ ë¶„ë¥˜í•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ Taxonomy (ë¶„ë¥˜ì²´ê³„)ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ìƒìœ„ ìˆ˜ì¤€ì—ì„œëŠ” í•™ìŠµ ëŒ€ìƒì„ ê¸°ì¤€ìœ¼ë¡œ Model-Based (ëª¨ë¸ ê¸°ë°˜), One-step (ì›ìŠ¤í…), Imitation Learning (ëª¨ë°© í•™ìŠµ) ë°©ë²•ìœ¼ë¡œ ë‚˜ë‰©ë‹ˆë‹¤. ë˜í•œ, ì†ì‹¤ í•¨ìˆ˜ë‚˜ í›ˆë ¨ ì ˆì°¨ì— ëŒ€í•œ ë³€í˜•ì¸ Policy Constraints (ì •ì±… ì œì•½), Regularization (ì •ê·œí™”), Uncertainty Estimation (ë¶ˆí™•ì‹¤ì„± ì¶”ì •)ì„ ë¶€ê°€ì ì¸ íŠ¹ì„±ìœ¼ë¡œ ì„¤ëª…í•©ë‹ˆë‹¤.

*   **Policy Constraints:** í•™ìŠµëœ ì •ì±…$\pi_{\theta}$ë¥¼ í–‰ë™ ì •ì±…$\pi_{\beta}$ì— ê°€ê¹ê²Œ ì œì•½í•©ë‹ˆë‹¤.
    *   Direct (ì§ì ‘):$\pi_{\beta}$ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì¶”ì •í•˜ê³ $\mathcal{D}(\pi_{\theta}(\cdot|s), \hat{\pi}_{\beta}(\cdot|s)) \le \epsilon$ì™€ ê°™ì€ ì œì•½ ì¡°ê±´(e.g.,$f$-divergence ì‚¬ìš©)ì„ ë¶€ì—¬í•©ë‹ˆë‹¤ (BCQ, BRAC). ì¶”ì • ì˜¤ë¥˜ì— ë¯¼ê°í•©ë‹ˆë‹¤.
    *   Implicit (ì•”ë¬µì ):$\pi_{\beta}$ ì¶”ì • ì—†ì´ ìˆ˜ì •ëœ ëª©ì  í•¨ìˆ˜ë¥¼ í†µí•´ ì•”ë¬µì ìœ¼ë¡œ ì œì•½í•©ë‹ˆë‹¤. ì•„ë˜ì™€ ê°™ì€ Advantage-weighted regression í˜•íƒœê°€ ëŒ€í‘œì ì…ë‹ˆë‹¤ (BEAR, AWR, AWAC, TD3+BC).
    $$J(\theta) = \mathbb{E}_{s,a \sim \mathcal{D}}[\log \pi_{\theta}(a|s) \exp(\frac{1}{\lambda} \hat{A}^{\pi}(s, a))]$$

*   **Importance Sampling (IS):** Off-policy ì •ì±… í‰ê°€ë¥¼ ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤. íŠ¸ë¼ì í† ë¦¬ í™•ë¥  ë¹„ìœ¨ì˜ ê³±($w_{i:j}$)ìœ¼ë¡œ ì¸í•´ ë¶„ì‚°ì´ ë§¤ìš° ë†’ìŠµë‹ˆë‹¤. Variance Reduction (ë¶„ì‚° ê°ì†Œ) ê¸°ë²•(Per-decision IS, Doubly Robust Estimator, Marginalized IS)ì´ ì œì•ˆë˜ì—ˆìŠµë‹ˆë‹¤. Marginalized ISëŠ” ìƒíƒœ í•œê³„ ë¶„í¬ ë¹„ìœ¨($\rho_{\pi}(s)$) ë˜ëŠ” ìƒíƒœ-í–‰ë™ í•œê³„ ë¶„í¬ ë¹„ìœ¨($\rho_{\pi}(s, a)$)ì˜ ë²¨ë§Œ ë°©ì •ì‹$$d^{\pi_\beta}(s')\rho_\pi(s') = (1-\gamma)d_0(s') + \gamma \sum_{s,a} d^{\pi_\beta}(s)\rho_\pi(s)\pi(a|s)T(s'|s,a)$$ì„ í™œìš©í•˜ì—¬ ë¶„ì‚° ë¬¸ì œë¥¼ ì™„í™”í•©ë‹ˆë‹¤ (GenDICE).

*   **Regularization:** ì •ì±… ë˜ëŠ” ê°€ì¹˜ í•¨ìˆ˜ì— í˜ë„í‹° í•­ì„ ì¶”ê°€í•˜ì—¬ ë°”ëŒì§í•œ ì†ì„±ì„ ë¶€ì—¬í•©ë‹ˆë‹¤.
    *   Policy Regularization: ì •ì±…ì˜ ì—”íŠ¸ë¡œí”¼(entropy)ë¥¼ ìµœëŒ€í™”í•˜ì—¬ í™•ë¥ ì„±(stochasticity)ì„ ë†’ì…ë‹ˆë‹¤ (SAC).
    *   Value Regularization: OOD í–‰ë™ì— ëŒ€í•œ Q-ê°’ ì¶”ì •ì„ ë‚®ê²Œ ê°•ì œí•˜ì—¬ ë³´ìˆ˜ì ì¸ ê°€ì¹˜ ì¶”ì •ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. CQLì€
    $$\max_{\mu} \mathbb{E}_{s \sim \mathcal{D}, a \sim \mu(\cdot|s)}[Q^{\pi}_{\phi}(s, a)] - \mathbb{E}_{s \sim \mathcal{D}, a \sim \hat{\pi}_{\beta}(\cdot|s)}[Q^{\pi}_{\phi}(s, a)] + \mathcal{R}(\mu)$$
    ì™€ ê°™ì€ ì •ê·œí™” í•­ì„ í†µí•´ ë°ì´í„°ì…‹ì˜ ê°€ì¹˜ í•¨ìˆ˜ê°€ ì°¸ ê°’ì˜ í•˜í•œ(lower bound)ì´ ë˜ë„ë¡ í•™ìŠµí•©ë‹ˆë‹¤.

*   **Uncertainty Estimation:** í•™ìŠµëœ ì •ì±…, ê°€ì¹˜ í•¨ìˆ˜ ë˜ëŠ” ëª¨ë¸ì˜ ë¶ˆí™•ì‹¤ì„±ì„ ì¶”ì •í•˜ì—¬ ë³´ìˆ˜ì„±ì˜ ì •ë„ë¥¼ ë™ì ìœ¼ë¡œ ì¡°ì ˆí•©ë‹ˆë‹¤. ë³´í†µ ì•™ìƒë¸”(ensemble)ì„ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡ ë¶„ì‚° ë“±ìœ¼ë¡œ ë¶ˆí™•ì‹¤ì„±ì„ ì¸¡ì •í•©ë‹ˆë‹¤ (REM).

*   **Model-Based Methods:** ë°ì´í„°ì…‹$\mathcal{D}$ë¡œ ì „ì´ ë™ì—­í•™($T$)ê³¼ ë³´ìƒ í•¨ìˆ˜($r$)ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤. í•™ìŠµëœ ëª¨ë¸ì€ ê³„íš(planning)ì— ì‚¬ìš©ë˜ê±°ë‚˜ ëª¨ë¸ ë¡¤ì•„ì›ƒ(model rollout)ì„ í†µí•´ í•©ì„± ë°ì´í„° ìƒì„±ì— ì‚¬ìš©ë©ë‹ˆë‹¤. ëª¨ë¸ ë¶„í¬ ë³€í™” ë¬¸ì œë¥¼ í”¼í•˜ê¸° ìœ„í•´ ë¶ˆí™•ì‹¤ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ ë³´ìƒì— í˜ë„í‹°ë¥¼ ì£¼ëŠ” ë³´ìˆ˜ì ì¸ ëª¨ë¸($\tilde{r}_{\psi_r}(s, a) = r_{\psi_r}(s, a) - \lambda U_r(s, a)$)ì„ í•™ìŠµí•˜ëŠ” ì ‘ê·¼ ë°©ì‹ì´ ìˆìŠµë‹ˆë‹¤ (MOReL, MOPO, COMBO). COMBOëŠ” ëª¨ë¸ ê¸°ë°˜ í™˜ê²½ì—ì„œì˜ ê°€ì¹˜ ì •ê·œí™”(value regularization)ë¥¼ í†µí•´ ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™” ì—†ì´ë„ ë³´ìˆ˜ì„±ì„ í™•ë³´í•©ë‹ˆë‹¤.

*   **One-Step Methods:** ì •ì±… í‰ê°€ ë° ì •ì±… ê°œì„  ë‹¨ê³„ë¥¼ ë°˜ë³µí•˜ì§€ ì•Šê³ , í–‰ë™ ì •ì±…($\pi_{\beta}$)ì˜ ê°€ì¹˜ í•¨ìˆ˜($Q^{\pi_{\beta}}$)ë¥¼ ì •í™•í•˜ê²Œ í•™ìŠµí•œ í›„ ë‹¨ì¼ ì •ì±… ê°œì„  ë‹¨ê³„ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ OOD í–‰ë™ì— ëŒ€í•œ ê°€ì¹˜ í‰ê°€ë¥¼ í”¼í•©ë‹ˆë‹¤. IQL(Implicit Q-Learning)ì€ ê°€ì¹˜ í•¨ìˆ˜($V^{\pi}$) í•™ìŠµì— Expectile Regression (ë¶„ìœ„ íšŒê·€) ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ë¶„í¬ ë‚´ì˜ 'ì¢‹ì€' í–‰ë™ë“¤ì— ëŒ€í•œ Qê°’ì˜ ìƒí•œì— ê·¼ì‚¬í•©ë‹ˆë‹¤.

*   **Imitation Learning:** í–‰ë™ ì •ì±…ì„ ëª¨ë°©(mimic)í•©ë‹ˆë‹¤. ë‹¨ìˆœ Behavior Cloning (í–‰ë™ ë³µì œ, BC)ì€ ì „ì²´ ë°ì´í„°ë¥¼ ë³µì œí•©ë‹ˆë‹¤. ê³ ê¸‰ ê¸°ë²•ì€ ê°€ì¹˜ í•¨ìˆ˜ ë“±ì„ ì‚¬ìš©í•˜ì—¬ ì°¨ì„  í–‰ë™ì„ í•„í„°ë§í•˜ê±°ë‚˜(BAIL, CRR) ì›í•˜ëŠ” ê²°ê³¼(ëª©í‘œ, ë³´ìƒ ë“±)ì— ì¡°ê±´í™”ëœ ì •ì±…ì„ í•™ìŠµí•©ë‹ˆë‹¤(RvS).

*   **Trajectory Optimization (íŠ¸ë¼ì í† ë¦¬ ìµœì í™”):** ì „ì²´ íŠ¸ë¼ì í† ë¦¬($\tau = (s_0, a_0, \dots, s_H)$)ì— ëŒ€í•œ ê²°í•© ìƒíƒœ-í–‰ë™ ë¶„í¬($p_{\pi_{\beta}}(\tau)$)ë¥¼ ì‹œí€€ìŠ¤ ëª¨ë¸(Sequence Model, ì˜ˆ: Transformer)ë¡œ í•™ìŠµí•©ë‹ˆë‹¤. í•™ìŠµëœ ë¶„í¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì›í•˜ëŠ” ìˆ˜ìµ(Return-to-Go) ë“±ì— ì¡°ê±´í™”í•˜ì—¬ ê³„íšì„ ìˆ˜í–‰í•©ë‹ˆë‹¤(TT, DT). í¬ì†Œ ë³´ìƒ ë¬¸ì œì— ê°•ì ì„ ë³´ì…ë‹ˆë‹¤.

## í‰ê°€

Off-policy Evaluation (OPE, ì˜¤í”„-í´ë¦¬ì‹œ í‰ê°€)ëŠ” ì˜¤í”„ë¼ì¸ RLì˜ ì¤‘ìš”í•œ Open Problem ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. í™˜ê²½ê³¼ì˜ ìƒí˜¸ì‘ìš© ì—†ì´ ì˜¤í”„ë¼ì¸ìœ¼ë¡œ ì •ì±…ì˜ ì„±ëŠ¥ì„ ì •í™•íˆ ì¶”ì •í•˜ê³  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ íŠœë‹í•˜ëŠ” ê²ƒì€ ì‹¤ìš©ì ì¸ ì˜¤í”„ë¼ì¸ RLì— í•„ìˆ˜ì ì…ë‹ˆë‹¤. ì£¼ìš” OPE ë°©ë²•ì—ëŠ” Model-Based ì ‘ê·¼ë²•, Importance Sampling, Fit Q Evaluation (FQE)ê°€ ìˆìŠµë‹ˆë‹¤. ê²½í—˜ì  ì—°êµ¬ë“¤ì— ë”°ë¥´ë©´ FQEê°€ ì¢…ì¢… ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ì§€ë§Œ, ëª¨ë“  ì„¤ì •ì—ì„œ ì¼ê´€ì ìœ¼ë¡œ ìš°ìˆ˜í•œ ë°©ë²•ì€ ì•„ì§ ì—†ìŠµë‹ˆë‹¤ (DOPE ë²¤ì¹˜ë§ˆí¬).

> **ì˜¤í”„ë¼ì¸ RL Benchmarkë¡œëŠ” [D4RL](https://paperswithcode.com/paper/datasets-for-data-driven-reinforcement)ê³¼ [RL Unplugged](https://paperswithcode.com/paper/rl-unplugged-a-collection-of-benchmarks-for)ê°€ ë„ë¦¬ ì‚¬ìš©ë©ë‹ˆë‹¤.**

ì´ë“¤ì€ Narrow and Biased Data Distributions (ì¢ê³  í¸í–¥ëœ ë°ì´í„° ë¶„í¬), Undirected and Multitask Data (ì§€í–¥ë˜ì§€ ì•Šì€ ë‹¤ì¤‘ ì‘ì—… ë°ì´í„°), Sparse Rewards (í¬ì†Œ ë³´ìƒ), Suboptimal Data (ì°¨ì„  ë°ì´í„°), Nonrepresentable Behavior Policies (í‘œí˜„ ë¶ˆê°€ëŠ¥í•œ í–‰ë™ ì •ì±…), Non-Markovian Behavior Policies (ë¹„ ë§ˆë¥´ì½”í”„ í–‰ë™ ì •ì±…), Realistic Domains (í˜„ì‹¤ì ì¸ ë„ë©”ì¸) ë“± ì‹¤ì œ ì‘ìš©ì— ì¤‘ìš”í•œ Dataset Design Factors (ë°ì´í„°ì…‹ ì„¤ê³„ ìš”ì†Œ)ë¥¼ í¬í•¨í•˜ëŠ” ë‹¤ì–‘í•œ í™˜ê²½ê³¼ ë°ì´í„°ì…‹ì„ ì œê³µí•©ë‹ˆë‹¤.

í•˜ì§€ë§Œ Stochastic Dynamics (í™•ë¥ ì  ë™ì—­í•™), Nonstationarity (ë¹„ì •ìƒì„±), Risky Biases (ìœ„í—˜í•œ í¸í–¥), Multiagent í™˜ê²½ ë“±ì€ ì—¬ì „íˆ ë¶€ì¡±í•œ ì‹¤ì •ì…ë‹ˆë‹¤. D4RL ë²¤ì¹˜ë§ˆí¬ ì„±ëŠ¥ ë¶„ì„ì— ë”°ë¥´ë©´ ìµœê·¼ ë°©ë²•(TT, IQL)ê³¼ íŠ¸ë¼ì í† ë¦¬ ìµœì í™” ë° ì›ìŠ¤í… ë°©ë²•ì´ í¬ì†Œ ë³´ìƒì´ë‚˜ ë‹¤ì¤‘ ì‘ì—… ë°ì´í„°ì—ì„œ ê°•ì ì„ ë³´ì´ë©° ìœ ë§í•œ ë¶„ë¥˜ë¡œ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.

ë¯¸ë˜ ì—°êµ¬ ë°©í–¥ìœ¼ë¡œëŠ” OPEì˜ ì‹ ë¢°ì„± í–¥ìƒ, Unsupervised RL ê¸°ë²•ì„ í™œìš©í•œ ë ˆì´ë¸” ì—†ëŠ” ë°ì´í„° í™œìš©, Incremental RLì„ í†µí•œ ì˜¨ë¼ì¸ Fine-tuning ì „ëµ ê°œë°œ, Safety-critical RL (ì•ˆì „ í•„ìˆ˜ ê°•í™”í•™ìŠµ, ì˜ˆ: CVaR) ë¶„ì•¼ ì—°êµ¬ ë“±ì´ ì œì•ˆë©ë‹ˆë‹¤. íš¨ê³¼ì ì¸ ë°ì´í„° ìˆ˜ì§‘ ë° curation ë˜í•œ ì•Œê³ ë¦¬ì¦˜ ê°œë°œë§Œí¼ ì¤‘ìš”í•©ë‹ˆë‹¤.
