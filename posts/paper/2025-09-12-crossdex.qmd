---
title: "📃CrossDex 리뷰"
date: 2025-09-12
categories: [isaacgym, grasp, il]
toc: true
number-sections: False
description: Cross-Embodiment Dexterous Grasping with Reinforcement Learning
---

- [Paper Link](https://arxiv.org/abs/2410.02479)
- [Homepage](https://sites.google.com/view/crossdex)
- [Code Link](https://github.com/PKU-RL/CrossDex/)

1. 기존 로봇 dexterous grasping 정책들이 특정 로봇 핸드에만 적용되어 다양한 핸드에 대한 일반화가 어렵다는 문제점을 해결하기 위해, 본 연구는 강화 학습(RL)을 이용한 cross-embodiment dexterous grasping 정책 학습을 제안합니다.
2. 본 연구는 인간 손의 eigengrasps 기반 통합 액션 공간과 손가락 끝 및 손바닥 위치만을 포함하는 통일된 관찰 공간을 설계하여, 단일 정책으로 다양한 로봇 핸드를 제어할 수 있도록 하며, 훈련 효율성을 위해 신경망으로 가속화된 리타겟팅 매핑을 활용합니다.
3. 제안된 CrossDex 정책은 단일 비전 기반 정책으로 네 가지 훈련 핸드와 YCB 데이터셋 객체에 대해 80%의 성공률을 달성했으며, 이전에 본 적 없는 두 가지 핸드에 대한 zero-shot generalization과 효율적인 finetuning 성능 향상을 보여줍니다.

<center>
<img src="../../images/2025-09-12-crossdex/crossdex.png" width="100%" />
</center>

---

# Brief Review

본 연구는 다양한 로봇 손(dexterous hand)에 적용 가능한 범용적인 dexterity grasping 정책 학습에 대한 난제를 다룬다. 기존 연구들이 특정 로봇 손에 특화된 정책을 학습하는 반면, 본 논문은 CrossDex라는 방법론을 제안하여 다양한 로봇 손을 제어하는 단일 정책을 학습한다.

핵심 방법론은 두 가지 주요 아이디어에서 출발한다. 첫째, 사람의 손이 여러 로봇 손을 원격 조작(teleoperation)할 수 있다는 점에 영감을 받아, 사람 손의 'eigengrasps'를 기반으로 하는 보편적인 액션 공간을 제안한다. Eigengrasps는 MANO(Romero et al., 2022) 손 모델의 45차원 손 포즈를 압축한 저차원 고유벡터들($e_i$)로 구성된다. 정책은 $k$-차원 손 액션 $w = (w_1, \ldots, w_k)$를 출력하며, 이는 고유벡터들의 가중합으로 사람 손 포즈 $\theta = \sum_{i=1}^{k} w_i e_i$를 생성한다. 이 $\theta$는 MANO 모델을 통해 3D keypoint로 변환된 후, retargeting 알고리즘을 거쳐 각 로봇 손의 특정 관절 액션($\hat{J_h}$)으로 전환된다. 본 연구에서는 이 retargeting 프로세스의 속도를 향상시키기 위해 신경망 $P_{\xi}^h$를 훈련시켜 사람 손 포즈 $\theta_i$에서 로봇 손 관절 위치 $J_h^i$로의 매핑을 학습한다. 원래의 retargeting은 각 time step $t$마다 로봇 손 관절 위치 $J_h^t$를 최적화하는 과정을 포함하며, 이는 다음과 같은 quadratic programming 문제로 표현될 수 있다:
$$
\min_{J_h^t} S(f_h(J_h^t), x_M^t) + \|J_h^t - J_h^{t-1}\|_2^2
$$

$$
\text{s.t. } J_h^{\text{lower}} \le J_h^t \le J_h^{\text{upper}}
$$
여기서 $f_h$는 로봇 손의 forward kinematics 함수이고, $x_M^t$는 사람 손 keypoint 위치이며, $S$는 로봇 손과 사람 손 포즈 간의 유사도를 측정하는 함수이다.

둘째, 로봇 손의 고유 수용성(proprioception)을 간소화하여 통일된 관찰 공간을 제공한다. 로봇 손의 관절 위치($J_h$)와 같이 손마다 다른 고유한 정보 대신, 모든 로봇 손에 공통적으로 중요한 정보인 손끝과 손바닥의 3D 위치($x_h$)만을 관찰 공간에 포함시킨다. 이는 손과 객체 간의 공간적 관계를 효과적으로 추론하는 데 필수적이며, 다양한 embodiment에 걸쳐 일관성을 유지한다.

정책 학습은 IsaacGym 시뮬레이션 환경에서 teacher-student 프레임워크를 따른다. 먼저 PPO(Schulman et al., 2017)를 사용하여 각 객체에 대한 state-based teacher 정책($\pi_S$)을 훈련시킨다. 이 과정에서 여러 로봇 손 환경을 병렬로 활용하여 cross-embodiment co-training을 수행한다. 이후, DAgger(Ross et al., 2011)를 통해 학습된 teacher 정책들을 단일 vision-based student 정책($\pi_V$)으로 증류한다. 학습 효율을 높이기 위해, 손과 팔 연결 지점의 위치에 가우시안 노이즈를 추가하는 embodiment randomization을 적용하여 정책의 강건성과 전이 가능성을 향상시킨다.

실험 결과, CrossDex는 YCB 데이터셋의 객체들에 대해 4가지 훈련용 로봇 손과 2가지 미훈련 로봇 손(LEAP Hand, Inspire Hand) 모두에서 80% 이상의 높은 성공률을 달성하며, baseline 방법들을 능가했다. 특히, 미훈련 로봇 손에 대한 zero-shot generalization 능력이 뛰어나며, finetuning 시 학습 효율이 크게 향상됨을 보였다. Cross-embodiment co-training이 개별 손 훈련보다 안정성과 효율성 면에서 이점을 제공함도 확인되었다. Ablation 연구를 통해 eigengrasp 액션 공간의 견고성과 embodiment randomization의 긍정적 효과, 그리고 embodiment-specific 관찰 정보가 오히려 일반화에 방해가 됨을 입증했다.

---

# Detail Review

# 다양한 로봇 손을 위한 범용 파지 정책: CrossDex 논문 심층 리뷰

## 1. 배경과 문제 정의

다지 로봇 손을 이용한 물체 파지(grasping)는 로봇이 현실 세계의 물체와 상호작용하기 위한 핵심 기술로 오래전부터 연구되어 왔습니다. 기존 연구들은 주로 특정 로봇 손에 한정된 파지 정책 학습에 집중해 왔지만, **서로 다른 형태의 로봇 손들에 공통적으로 적용될 수 있는 범용 정책**에 대한 연구는 거의 이루어지지 않았습니다. 예를 들어, 5손가락에 22자유도를 가진 ShadowHand와 4손가락에 16자유도인 LEAP Hand를 생각해 보면, 두 손의 형태 차이로 인해 한 손의 동작 제어 명령을 다른 손에 직접 적용하기가 매우 어렵습니다. 손가락 수와 관절구조, 가동 범위가 다르기 때문에 **행동 공간**(제어 명령의 형태)을 통일하기가 어렵고, 물체와 접촉하는 방식도 달라 단일 정책으로 다양한 손을 제어하는 것은 큰 도전 과제입니다.

이러한 문제의식 아래, **Cross-Embodiment Dexterous Grasping**(이하 CrossDex) 논문은 **다양한 로봇 손 체형에 통용되는 단일 강화학습 정책**을 개발하고자 합니다. 각 로봇 손마다 별도의 정책을 학습하는 대신, **공유되는 구조적 특징**을 활용해 여러 손에 걸쳐 일반화할 수 있는 파지 기술을 학습시키는 것이 목표입니다. 이는 새로운 로봇 손이 투입될 때 일일이 초기부터 학습해야 하는 비용을 줄이고, 이미 학습된 정책을 빠르게 이전하거나 `제로샷(Zero-shot)`으로 적용하는 기반을 마련한다는 점에서 실용적인 의미가 큽니다. 요약하면, 이 논문은 *“다양한 로봇 손에 대해 하나의 뇌(정책)로 물체 잡기를 잘 해보자”*라는 문제를 정의하고 있습니다.

## 2. 제안 방법의 핵심 기술 내용

CrossDex의 핵심 아이디어는 **인간 손 동작 공간을 매개체로 로봇 손들의 행동과 관측 공간을 통일**하는 것입니다. 저자들은 사람이 여러 로봇 손을 원격 조작(teleoperation)할 때, **자신의 손 동작**을 통해 직관적으로 로봇 손을 제어할 수 있다는 점에 주목했습니다. 이를 본뜬 `“휴먼-라이크 (human-like) 정책”`을 도입하여, 정책의 행동 출력이 곧 **인간 손의 자세**로 표현되도록 만들었습니다. 구체적으로, 인간 손의 다양한 자세를 주성분분석(PCA)을 통해 얻은 **eigengrasp**(고유 그립 모드)를 **통합 행동 공간**으로 채택합니다. MANO라는 인간 손 모델의 45차원 관절 각도 표현을 저차원(e.g. 수 개에서 수십 개 차원) eigengrasp 벡터로 압축하여 사용하며, 하나의 정책이 출력하는 이 **eigengrasp 벡터**가 어떤 로봇 손을 제어할 때든 공통된 의미를 갖도록 합니다.

다음 단계는 이렇게 나온 인간 손 형태의 동작 명령을 **각 로봇 손의 개별 관절 명령으로 변환**하는 것입니다. 이를 `리타게팅(retargeting)`이라고 하며, 본 연구에서는 주로 DexPilot 알고리즘을 기반으로 하되, 초기에는 최적화 기반의 리타게팅을 활용하고 이후엔 이를 대신할 **신경망 매핑**을 학습시켰습니다. 리타게팅은 인간 손 모드의 움직임을 개별 로봇 손의 관절값(각 손의 PD 제어기에 넘겨줄 목표 각도)으로 변환하는 역할을 합니다. 예를 들어, 정책이 “손을 쥐는” 방향의 eigengrasp 값을 출력하면, 5손가락 ShadowHand든 4손가락 Allegro Hand든 해당 손가락들이 모두 오므려져 물체를 잡는 방향으로 관절이 움직이도록 매핑됩니다. 저자들은 이 매핑을 빠르게 수행하기 위해 각 로봇 손별로 **4층 MLP 신경망**을 학습시켜 최적화 과정을 대체하였고, 이를 통해 대량의 병렬 강화학습 환경에서도 속도 병목 없이 동작 변환을 할 수 있게 했습니다.

한편, **관측 공간의 통합**도 중요한 요소입니다. 서로 다른 로봇 손의 **자기 수용감각(proprioception)** 정보, 즉 관절 각도들은 종류마다 차원이 다르고 의미도 달라 직접 비교하거나 하나의 정책 입력으로 합치는 데 무리가 있습니다. CrossDex에서는 **손바닥과 손가락 끝 포인트의 3차원 위치 정보만** 관측에 사용함으로써 이 문제를 풀었습니다. 로봇 손의 구체적인 관절각 대신, 각 손의 손바닥 중심과 다섯 손가락(또는 네 손가락) 끝점들의 위치를 취하면, 비록 손마다 구조가 달라도 “손가락 끝이 어디 있는가”라는 공통 표현으로 정규화할 수 있다는 것입니다. 이는 해당 위치들이 파지에서 물체와의 접촉 및 배치에 핵심적인 역할을 하기 때문이며, Handa 등(2020)의 연구에서도 인간 손가락 끝 위치의 중요성이 강조된 바 있습니다. 요약하면, **행동 공간**은 “인간 손 고유 동작 모드”로, **관측 공간**은 “손바닥과 손가락 끝의 위치”로 통합하여 **로봇 손 종류에 구애받지 않는 정책 입력/출력 구조**를 설계한 것이 CrossDex 방법의 핵심입니다.

이렇게 통합된 관측·행동 표현을 가지고, **강화학습 정책 학습**은 크게 두 단계로 진행되었습니다. (i) 우선 *teacher-student* 전략을 활용하여, **상태기반(state-based) 교사 정책**들을 개별 물체 대상 파지에 대해 학습시킵니다. 여기서 상태기반이란 물체의 정확한 위치와 형상, 로봇 손의 상태 등을 완전 관측한 조건에서 학습한다는 뜻입니다. 각 물체마다 PPO 알고리즘으로 최적 정책을 찾고, 이를 각 물체의 파지 전문가(교사)로 간주합니다. (ii) 이후 이러한 다수의 교사들을 이용해 **비전기반(vision-based) 학생 정책**을 학습시키는데, 여기에는 *DAgger*(Dataset Aggregation) 알고리즘을 사용했습니다. 비전 정책은 물체의 3D 포인트클라우드(시뮬레이션 상의 depth sensor 데이터)를 입력으로 받아 동작을 결정하며, 초기에는 교사 정책들의 시연을 모방하고 점진적으로 자기 정책으로 데이터를 확장해가는 방식으로 학습됩니다. 이 비전 정책이 최종적으로 **모든 물체**와 **모든 로봇 손**을 한꺼번에 다루는 **범용 파지 정책**이 됩니다. 학습에는 NVIDIA Isaac Gym 시뮬레이터를 활용하여 총 8192개의 병렬 환경에서 대량의 경험을 모았고, 이후 시뮬레이션에 16,384개 환경을 활용하여 비전 정책으로 지식을 이행(distill)했습니다. 이러한 대규모 병렬 학습 세팅 덕분에 수십 가지 물체와 여러 손에 대한 복잡한 정책을 현실적인 시간 안에 얻을 수 있었습니다. (참고로 정책 신경망 구조는 상태기반의 경우 5개 레이어 MLP, 비전 정책의 경우 PointNet 기반의 간소화된 구조로 물체 점군을 처리한 후 MLP로 액터/크리틱을 구성하였습니다.)

마지막으로, 저자들은 이렇게 학습된 정책을 새로운 손이나 새로운 객체에 빠르게 적응시키는 **파인튜닝(fine-tuning)** 기법도 제안합니다. 사전학습된 CrossDex 정책을 초기화 값으로 활용하여 PPO 재학습을 할 때, 기존 정책과의 **KL 발산 페널티**를 추가로 줘서 급격한 변화(포겟팅)를 막는 형태로 미세조정을 수행하였습니다. 이를 통해 **학습되지 않은 새로운 손이나 새로운 물체 세트에 대한 추가 학습 효율을 높일 수 있음**을 보였습니다.

## 3. 기존 연구 대비 주요 기여점

CrossDex는 앞서의 기술적 설계를 통해 **기존 연구 대비 몇 가지 두드러진 기여**를 제공합니다. 첫째, **범용 다지 손 파지 정책의 구현**입니다. 기존에는 로봇 손마다 별도의 정책을 만들어야 했고, 심지어 유사한 형태의 손 사이에서도 정책을 재학습해야 했습니다. Patel & Song (2024)의 GET-Zero 연구가 동일한 LEAP Hand의 변형들 사이에서 정책 일반화를 시도했지만, **서로 다른 종류의 로봇 손 간 일반화에는 실패**했다고 보고합니다. 반면 CrossDex는 ShadowHand, Allegro Hand, Schunk SVH, Ability Hand처럼 **구조가 제각각인 손 네 종류**를 한 번에 학습시키고, 학습에 포함되지 않은 LEAP Hand와 Inspire Hand까지 **단일 정책으로 제어 가능함**을 처음으로 보여주었습니다. 이는 **로봇 손 분야 최초의 범용 정책** 시도로서 의의가 있습니다.

둘째, **인간 손 기반의 행동 공간 통합과 손가락 끝 관측을 통한 일반화 전략** 자체가 기여점입니다. 앞서 설명한 인간 **eigengrasp** 공간을 활용한 행동 통합은 인간 조작의 **공통 분모**를 활용하는 창의적인 접근으로서, **서로 다른 손의 움직임을 일관된 방식으로 표현**할 수 있게 해주었습니다. 이를 통해 저자들은 기존에 복잡한 그래프 신경망이나 트랜스포머 등을 사용해 각 로봇 형태를 인코딩해야 했던 접근들과 달리, 훨씬 명시적이고 해석적인 **공용 인터페이스**로 문제를 단순화했습니다. 특히 **teleoperation (원격 조작)** 분야의 지식을 정책 학습에 접목하여, 인간-로봇 사이의 제어 맵핑을 로봇-로봇 사이에도 적용한 점이 흥미로운 혁신입니다. 또한 손가락 끝 위치만으로 관측을 구성한 것은 단순히 차원을 줄이는 효과뿐만 아니라, **손 크기나 관절 구성 차이에도 불구하고 파지의 핵심인 손-물체 관계를 공통되게 파악**할 수 있다는 장점이 있습니다.

셋째, **실험적으로 높은 성능과 일반화 능력**을 입증했습니다. CrossDex 정책은 **YCB 벤치마크 객체**들을 대상으로 4가지 훈련 손에서 평균 약 **80%의 그립 성공률**을 기록했으며, 학습에 전혀 포함되지 않았던 두 가지 새로운 손에 대해서도 **제로샷으로 35% 이상의 성공률**을 보였습니다. 이는 단순히 하나의 손에 특화된 정책을 새로운 손에 적용했을 때 기대되는 성능 (거의 0에 수렴)과 비교하면 크게 뛰어난 일반화 결과입니다. 특히 **CrossDex는 동시 학습한 손들에 대해서도 각 손별 전용 정책과 비슷한 성능을 유지**하면서, **새로운 손에 대해서는 월등히 우수한 성공률**을 나타냈습니다. 이러한 성능 우위는 저자들이 고안한 통합 관측/동작 공간의 효과와, 교사-학생 학습 프레임워크의 효용을 함께 보여주는 증거입니다.

마지막으로, **학습 효율 및 전이 학습 측면의 개선**도 중요한 기여입니다. 다중 손을 한꺼번에 학습하는 크로스-임바디먼트 훈련은 **개별 손별로 따로 학습할 때에 비해 약간 더 안정적이고 효율적인 학습곡선**을 보였습니다. 즉, 공통 정책으로 묶어서 학습하면 오히려 훈련이 수렴하는 데 도움이 되고 각 손에 대한 성능 저하도 없었다는 것입니다. 뿐만 아니라, 학습된 정책을 활용한 **파인튜닝** 실험에서는, 사전 학습 없이 처음부터 새 손에 대해 학습하는 경우보다 **훨씬 빠르게 성능을 끌어올릴 수 있음**이 확인되었습니다. 예컨대 LEAP Hand에 대해 처음부터 학습한 정책의 다중 물체 성공률이 43.6%였다면, CrossDex 사전학습 정책을 미세조정한 경우 64.3%까지 달성하였습니다. 새로운 객체들에 대한 학습에서도 사전학습의 이점이 나타나, 범용 파지 정책이 **프리트레인 모델**로서 유용함을 보여주었습니다.

## 4. 실험 설계 및 구성 평가

논문의 실험은 주로 **시뮬레이션 환경에서의 다수 객체 파지 평가**로 이루어졌으며, 설계 면에서 비교적 철저하고 신중하게 구성되었습니다. 실험에 사용된 **로봇 손 플랫폼**은 앞서 언급한 4가지 훈련용 손(ShadowHand, Allegro Hand, Schunk SVH Hand, Ability Hand)과 2가지 테스트용 손(LEAP Hand, Inspire Hand)입니다. 이들은 손가락 개수(4~5개)와 자유도(10~22 DoF)가 상이하여, CrossDex의 일반화 성능을 검증하기에 충분히 다양한 사례를 포함합니다. 모든 손은 **RealMan RM65 6자유도 로봇팔** 끝단에 장착되어 있고, 로봇팔의 베이스는 책상 옆면에 고정되어 있습니다. 이는 실제 로봇 실험을 염두에 둔 구성으로, 시뮬레이션에서의 설정이 현실의 하드웨어 배치와 일치하도록 했습니다.

**학습 환경**으로는 NVIDIA의 물리 시뮬레이터인 **Isaac Gym**을 사용하였고, 한 번에 수천 개의 병렬 환경을 돌려 데이터 효율을 극대화했습니다. 예를 들어 PPO 기반 상태정책 학습 시 **8192개 환경**을 병렬 실행하여 4만 iteration을 수행했고, 비전 정책 학습 시는 **16,384개 환경**까지 활용하여 대용량의 데이터로 학습을 진행했습니다. 이렇게 대규모 환경을 사용하면서도, 한 대의 GPU에서 학습이 가능하도록(8192 환경의 경우 RTX 4090 한 장으로 가능) 시뮬레이션을 최적화했다고 합니다. 이는 **실험 결과의 신뢰성** 측면에서 중요합니다. 강화학습 결과는 보통 무작위성에 민감한데, 병렬 환경을 많이 사용하면 운에 따른 편차를 줄이고 더 안정적인 성능 추정을 할 수 있습니다. 또한 논문에서는 표와 그래프에 성공률의 평균과 표준편차(또는 표준오차)를 함께 제시하여 성능 차이가 통계적으로 유의미함을 보여주고 있습니다.

**평가 방식**은 YCB Object Set에 속한 **45개의 다양한 물체**에 대해 로봇 손이 물체를 들어올릴 수 있는지(성공/실패)를 측정하는 형태였습니다. 물체는 책상 위에 무작위로 놓여 있고, 로봇 손은 초기 자세로부터 물체를 파지해 들어올리는 **에피소드**를 반복하게 됩니다. 각 손-물체 조합에 대해 여러 시도를 수행해 성공률을 계산하며, 이를 모든 손에 대해 평균내어 종합 성능으로 사용했습니다.

Baseline(비교 방법)으로는 **다중작업(Multi-task) RL 방식**들을 설정하여 CrossDex의 설계 요소들을 하나씩 제거해본 변형들을 사용했습니다. 예를 들어, **MT-Raw-OA**라는 베이스라인은 **관측과 행동을 그대로(raw) 사용**하는 대신 손 종류를 구분하는 원-핫 임베딩을 추가하여 여러 손을 함께 학습시키는 방법입니다. 이때 손별 관절 상태 차원을 맞추기 위해 남는 부분을 0으로 패딩하고, 관절 증가 방향도 통일시키는 등의 정규화를 적용했다고 합니다. 또한 **MT-Raw-A**는 관측은 CrossDex처럼 통합하되 행동은 원래 각 손의 관절 명령을 쓰는 경우, **MT-Raw-O**는 그 반대로 행동은 eigengrasp로 통일하되 관측은 각 손의 원래 관절각을 모두 포함한 경우로 설정하여 비교했습니다. 이렇게 baseline을 구성함으로써, **공용 관측/행동 공간이 성능에 미치는 영향**을 정량적으로 분석하려는 의도가 엿보입니다.

전체적으로 실험 설계는 **적절한 대조군**을 갖추고 있고, **훈련에 사용되지 않은 새로운 손과 새로운 물체로의 일반화 평가**까지 포함하여 제안 방법의 효과를 다각도로 검증했습니다. 다만 아쉬운 부분을 꼽자면, 본 논문에서는 주된 결과가 시뮬레이션 상의 성공률에 집중되어 있어 **실제 로봇에의 적용 검증이 제한적**이었다는 점입니다. 저자들이 프로젝트 페이지를 통해 LEAP Hand 실물과 RealSense 카메라를 사용한 **Sim-to-Real 실험 영상**을 공개하긴 했지만, 논문 본문에서는 이 부분이 정량적인 평가로 다루어지진 않았습니다. 따라서 현실 세계에서 다양한 손으로 범용 정책을 적용할 때 발생할 수 있는 미지의 변수들(센서 노이즈, 모델링 오차 등)에 대한 검증은 향후 과제로 남아 있습니다. 그럼에도 불구하고 시뮬레이터 내에서 충분한 다양한 상황을 실험했고, 코드와 환경 설정을 공개(PKU-RL/CrossDex 깃허브)하여 **재현성**도 높인 점은 긍정적으로 평가할 수 있습니다.

## 5. 주요 결과 및 한계점 논의

CrossDex의 실험 결과는 **범용 정책의 가능성**을 뒷받침하는 인상적인 수치를 보여줍니다. 우선, 하나의 비전 기반 정책이 **훈련에 사용된 4가지 손**에 대해 평균 **약 80%의 파지 성공률**을 달성했습니다. 개별 객체별로 보면 45개 YCB 물체 중 42개에서 성공률이 거의 100%에 가깝게 나왔으며, 이는 여러 손을 동시에 제어하도록 학습했음에도 **각 손에 대한 성능 희생이 크지 않았다**는 뜻입니다. 또한 **동시학습의 이점**으로, 저자들은 동일 조건에서 손별로 따로 학습시킨 정책들과 비교해볼 때 **크게 뒤지지 않거나 오히려 학습 안정성이 높았다**고 보고합니다. Figure 3의 학습 곡선 비교를 보면 CrossDex와 개별 학습의 수렴 속도 및 최종 성능이 유사하거나 약간 더 나은데, 이는 서로 다른 손 간에 경험이 일부 공유되면서 학습 신호의 다양성이 증가한 덕분으로 해석할 수 있습니다.

**Baseline 대비 성능**을 살펴보면, CrossDex의 통합 관측/행동 공간 전략이 기존 방식들을 확연히 능가함을 알 수 있습니다. Table 1의 결과에 따르면, **CrossDex 정책은 훈련에 사용된 손들은 물론 보지 못한 새로운 손들에 대해서도 모든 baseline보다 높은 성공률**을 보였습니다. 흥미로운 점은, raw 관측/행동을 사용한 다중학습도 어느 정도 일반화 능력을 갖긴 했다는 것입니다. 예를 들어 MT-Raw-A (관측 통일/행동 원래값) 방법은 새로운 손에 대해 21% 정도의 성공률을 보였는데, 이는 개발자들이 **URDF 정렬** 등을 통해 손들 간 관절 정의를 신중히 맞춰준 덕분입니다. 즉, 손가락 순서나 관절 증감 방향 등을 수작업으로 통일한 결과 어느 정도 제로샷 일반화가 가능했지만, **인간 손 eigengrasp를 활용한 CrossDex의 성능(35% 이상 성공률)에는 크게 못 미쳤습니다**. 더욱이 다른 baseline인 MT-Raw-O나 MT-Raw-OA의 경우, 상태기반 학습 시 로봇 종류 원-핫벡터를 활용한 탓에 비전 정책으로 지식을 증류할 때 관측 정보가 감소하여 성능 하락이 두드러졌습니다. 이러한 분석은 **CrossDex의 설계 선택이 단순히 편의성뿐만 아니라 성능 면에서도 최적**임을 뒷받침합니다. 한편, CrossDex가 **새로운 손에 대해 달성한 제로샷 성공률 \~35% 수준**은 절대적으로 보면 낮게 느껴질 수 있습니다. 하지만 앞서 언급했듯 전혀 보지 못한 손의 경우 일반적인 정책은 거의 실패하는 반면, 3번 중 1번 이상 성공하게 만든 것만으로도 고무적인 결과입니다. 이는 **추가 학습(finetuning)을 통해 쉽게 끌어올릴 수 있는 출발점**으로 볼 수 있습니다.

CrossDex의 **한계점**도 짚고 넘어갈 필요가 있습니다. 우선, 현재의 범용 정책은 **파지(grasp) 동작**에 초점을 맞추고 있습니다. 즉, 테이블 위의 정적(static)인 물체를 집어 드는 과제에 특화되어 있으며, `물체를 쥔 후 섬세한 조작(in-hand manipulation)`이나 연속적인 작업으로의 일반화는 다루지 않습니다. 이는 연구 범위를 넘는 부분이지만, 결국 궁극적인 로봇 손 활용을 위해서는 파지 이후의 단계(예: 물체 회전시키기, 도구 사용 등)도 범용 정책으로 다룰 수 있어야 할 것입니다. 또한 **시뮬레이터 기반의 학습과 평가**라는 한계 때문에, 현실 세계에서 발생하는 마찰 모델 차이나 센서 오차, 카메라 인식 문제 등이 고려되지 않았습니다. 프로젝트 페이지에서 시도된 **Sim-to-Real 적용**에서는 어느 정도 성공을 거두었지만, 실제 실험에서는 **카메라에 손이 가려 물체가 보이지 않게 된다거나**, **로봇팔이 테이블과 충돌**하는 등의 **실패 사례**도 확인되었습니다. 이는 향후 **센서 융합이나 모션 플래닝을 결합**하여 개선할 부분입니다.

또 다른 한계로, **행동 공간을 인간 손 모드로 제한**한 것이 복잡한 손 동작을 표현하는 데 제약이 될 가능성이 있습니다. 저자들은 MANO 기반 eigengrasp를 6개, 12개 등 여러 개수로 시도한 결과 큰 성능 차이는 없었다고 보고하고 있어 현재 설정이 충분함을 시사하지만, 이 공간이 표현하지 못하는 특이한 손 자세가 필요한 작업이 있을 수 있습니다. 마찬가지로 손가락 끝 위치만으로 관측을 구성한 것도 파지에는 충분했으나, **손가락의 세부 굽힘 정도나 관절 토크 등의 정보가 배제**되었다는 점에서 향후 더 복잡한 상호작용에는 보완이 필요할 수 있습니다.

요약하면, CrossDex의 결과는 현재 수준에서 `범용 다지 손 파지의 개념 증명(proof of concept)`을 성공적으로 보여줬다고 평가할 수 있습니다. 동일한 정책으로 여러 형태의 로봇 손에 일관되게 물체 파지 동작을 수행하고, 새로운 손에도 일부 일반화가 가능함을 입증한 것은 큰 진전입니다. 동시에, 현실 적용까지 넘어서기 위해서는 몇 가지 난제들 – 예를 들어 **더 복잡한 작업으로의 확장**, **현실 세계의 불확실성 대응**, **더욱 다양한 손 구조 (예: 두 손가락 집게 그리퍼 등 극단적으로 형태가 다른 손)** – 이 남아 있음을 확인하게 해줍니다.

## 6. 향후 연구 방향 제언

이번 연구를 바탕으로 생각해볼 수 있는 향후 연구 방향은 여러 가지가 있습니다:

* **실세계 적용 및 검증 확대:** 시뮬레이션에서 입증된 CrossDex 정책을 실제 로봇 플랫폼에서 더욱 광범위하게 시험하는 것이 필요합니다. 다양한 센서 피드백(예: 촉각센서)과 카메라 환경에서 정책이 얼마나 강인한지 평가하고, Sim-to-Real 간 갭을 줄이기 위한 도구(도메인 랜덤화, 적대적 학습 등)를 결합하는 연구가 유망합니다.

* **다양한 작업과 목표로의 일반화:** 파지 외에 **조작(manipulation)** 작업에도 범용 정책을 확장하는 방향입니다. 예를 들어 하나의 정책이 여러 손으로 병뚜껑 열기, 공 돌리기, 도구 사용하기 등의 **다양한 조작 과제**까지 수행하도록 학습시킬 수 있다면 진정한 범용 손 사용 로봇에 가까워질 것입니다. 이를 위해 고차원 행동을 효율적으로 탐색할 수 있는 계층적 RL이나 모방학습 결합 기법 등을 고려해볼 수 있습니다.

* **정책 아키텍처의 진화:** 현재 CrossDex는 인간 손이라는 **정적 인터페이스**를 통해 손들을 묶었지만, 향후에는 **모든 형태의 로봇 손을 포괄할 수 있는 학습 기반 표현**을 탐색할 수 있습니다. 예컨대 그래프 신경망으로 로봇 손의 형태를 인코딩하고, 거기에 정책을 조건부 생성(condition)하는 방법이나, 트랜스포머를 이용해 **손 구조를 토큰화하여 in-context**로 제어 명령을 생성하는 방법 등이 대안이 될 수 있습니다. 이러한 접근은 새로운 손이 투입되었을 때 인간 손 공간에 맞추기 어려운 경우에도 적용할 수 있다는 장점이 있습니다.

* **휴먼 데몬스트레이션과의 결합:** CrossDex는 인간 손 공간을 활용했지만 정작 인간 시연 데이터는 강화학습 교사로 직접 쓰지 않았습니다. 향후에는 **인간 시연 데이터를 활용한 오프라인 RL**이나 **IL(모방학습)** 기법을 결합하여 학습 효율을 높이고, 정책이 더욱 인간스러운 동작을 하도록 유도할 수 있을 것입니다. 특히 teleoperation 시스템에서 얻은 **인간-로봇 손 동작 짝(pair)** 데이터를 활용하면 리타게팅 학습이나 초기 정책 구성에 도움이 될 것입니다.

* **추가적인 안전 및 제약 요소 통합:** 로봇 손이 다양한 환경에서 동작하려면, 단순 성공률뿐 아니라 **안전성**도 중요합니다. 향후 연구에서는 정책 학습 시 **로봇 손의 충돌 회피**나 **힘 제어 안정성** 등의 제약을 모델에 통합하는 방향도 고려될 수 있습니다. 예를 들어 손가락 끝에 과도한 힘이 가해지면 감지하여 놓아주도록 하거나, 테이블과 부딪히지 않도록 학습에 페널티를 주는 등 현실적인 제약을 넣으면, 실제 적용에 한층 가까워질 것입니다.

결론적으로, *Cross-Embodiment Dexterous Grasping with RL* 논문은 **다양한 로봇 손에 걸쳐 통합적으로 동작하는 강화학습 정책**의 가능성을 보여준 선구적인 연구입니다. 배경의 난제부터 제안 기법, 실험 검증까지 논리적이고 명확하게 전개되었으며, 로봇공학 및 강화학습 연구자들에게 새로운 관점을 제공하고 있습니다. 앞으로 이 방향의 연구가 지속되어, 사람 손처럼 유연하고 범용적인 로봇 손 조작 정책이 실현되기를 기대해봅니다.
