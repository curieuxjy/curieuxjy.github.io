---
title: "📃AnyTeleop 리뷰"
date: 2025-08-15
categories: [teleoperation, vision]
toc: true
number-sections: true
description: A General Vision-Based Dexterous Robot Arm-Hand Teleoperation System
---

- [Paper Link](https://arxiv.org/abs/2307.04577)
- [Project Link](http://anyteleop.com/)

1.  ✨ AnyTeleop은 다양한 로봇 팔과 손, 현실 환경, 카메라 설정에 사용 가능한 일반적인 비전 기반 로봇 원격 조작 시스템입니다.
2.  🌐 이 시스템은 여러 시뮬레이터와 실제 환경을 지원하며, 원격 및 협업 원격 조작 기능까지 제공하여 데이터 수집의 확장성을 높입니다.
3.  🚀 AnyTeleop은 범용적인 설계에도 불구하고 실제 및 시뮬레이션 실험에서 기존 시스템보다 뛰어난 성능을 달성하여, 모방 학습을 위한 고품질 데이터 수집을 용이하게 합니다.

---

# Brief Review

AnyTeleop는 다양한 로봇 팔, 로봇 손, 현실(시뮬레이터 또는 실제 세계), 카메라 설정 및 다중 운영자 협업을 지원하는 범용적인 비전 기반 로봇 팔-손 원격 조작 시스템입니다. 기존의 비전 기반 원격 조작 시스템은 특정 로봇 모델이나 배포 환경에 맞춰 설계되어 로봇 모델의 다양성이나 운영 환경이 증가함에 따라 확장성이 떨어진다는 문제를 해결합니다. AnyTeleop는 단일 시스템 내에서 이러한 다양한 시나리오를 지원하면서도 우수한 성능을 달성합니다. 실제 환경 실험에서 AnyTeleop는 동일한 로봇을 사용하여 특정 하드웨어에 맞게 설계된 이전 시스템보다 더 높은 성공률을 보였고, 시뮬레이션 원격 조작에서는 해당 시뮬레이터를 위해 특별히 설계된 이전 시스템보다 더 나은 Imitation Learning 성능을 이끌어냈습니다.

시스템은 모듈성, 통신 중심 설계, Containerization이라는 세 가지 핵심 원칙으로 설계되었습니다. 카메라는 Camera Driver를 통해 RGB 또는 RGB-D 스트림을 Teleoperation Server로 전송합니다. Teleoperation Server는 입력 데이터를 처리하여 로봇 제어 명령을 생성하며, 이 명령은 Teleoperation Client를 통해 시뮬레이터나 실제 로봇으로 전달됩니다. Web Visualizer는 로컬 또는 원격에서 원격 조작 과정을 시각적으로 피드백합니다.

Teleoperation Server는 다음 네 가지 주요 모듈로 구성됩니다.

1.  Hand Pose Detection: 카메라 스트림에서 사람 손의 손목(wrist) 및 손가락(finger) 자세를 예측합니다. RGB 또는 RGB-D 카메라, 단일 또는 다중 카메라 등 다양한 카메라 구성을 지원합니다. 손가락 키포인트(keypoint)는 MediaPipe를 사용하여 RGB 데이터로부터 검출하며, 손목 6D 자세는 RGB-D 데이터의 경우 키포인트의 Depth 값을 이용한 PnP 알고리즘으로, RGB 데이터만 있는 경우 FrankMocap에서 영감을 받은 추가 신경망을 사용하여 약한 투영(weak perspective) 변환 스케일을 예측하는 방식으로 추정합니다.
2.  Detection Fusion: 다중 카메라 감지 결과를 통합합니다. 외적 캘리브레이션 없이 사람 손을 자연 마커(marker)로 사용하는 자동 캘리브레이션 과정을 통해 카메라 간의 상대적인 회전(SO(3))을 계산합니다. 각 감지 결과의 신뢰도는 SMPL-X 손 형상 매개변수 예측값의 일관성(초기 참조 값과의 오차)을 기준으로 평가하며, 신뢰도가 가장 높은 카메라의 상대 동작을 선택하여 다음 모듈로 전달합니다.
3.  Hand Pose Retargeting: 사람 손 자세 데이터를 로봇 손 관절 위치로 매핑합니다. 이는 최적화 문제로 공식화됩니다. 사람 손과 로봇 손의 키포인트 벡터 간 차이를 최소화하고 시간적 부드러움을 개선하는 목적 함수를 최소화합니다.
    $$
    \min_{q_t} \sum_{i=0}^N ||\alpha v_i^t - f_i(q_t)||^2 + \beta ||q_t - q_{t-1}||^2 \\
    \text{s.t.} \quad q_l \leq q_t \leq q_u
    $$
    여기서 $q_t$는 시간 $t$에서의 로봇 손 관절 위치, $v_i^t$는 감지된 사람 손 키포인트에서 계산된 $i$번째 키포인트 벡터, $f_i(q_t)$는 로봇 손 관절 위치 $q_t$를 입력받아 $i$번째 로봇 손 키포인트 벡터를 계산하는 순방향 기구학 함수, $q_l$과 $q_u$는 관절 위치의 하한 및 상한, $\alpha$는 손 크기 차이를 고려한 스케일링 계수, $\beta$는 시간적 부드러움에 대한 가중치입니다. 다른 로봇 손 형태에 대해서는 사람과 로봇 손가락 사이의 키포인트 매핑을 수동으로 지정해야 합니다. 이 모듈은 로봇 손만 고려합니다.
4.  Motion Generation: 감지된 손목 및 손 자세(최종 End-effector의 Cartesian 자세)를 기반으로 로봇 팔의 부드럽고 충돌 없는 움직임을 생성합니다. 실시간 모션 생성을 위해 GPU 가속 병렬화 충돌 회피 로봇 모션 생성 라이브러리인 CuRobo를 사용합니다. Motion Generation 모듈은 낮은 주파수(25 Hz)로 End-effector의 Cartesian 자세를 받아 높은 주파수(120 Hz)로 충돌 없는 관절 공간 궤적을 생성합니다. 충돌 회피는 CUDA 기반 기하학적 질의를 사용하여 학습 기반 모델 없이 수행됩니다.

Web-based Teleoperation Viewer는 meshcat 및 Three.js를 기반으로 개발되었으며, 브라우저 기반의 접근성과 원격 및 협업 원격 조작을 위한 동기화된 시각화를 지원합니다. 다중 브라우저 창을 열어 다중 시점(multi-view)을 제공할 수 있습니다.

시스템 평가 결과, Profiling 분석에서 Hand Pose Detection이 가장 시간이 많이 소요되는 모듈이지만 요구 주파수(25Hz)를 충족합니다. 실제 로봇 원격 조작 실험에서는 XArm6 및 Allegro Hand를 사용하여 Robotic Telekinesis [54]의 10가지 조작 작업을 재현하여 비교했습니다. AnyTeleop는 10개 작업 중 8개에서 더 높은 성공률을 보였으며, 특히 컵 쌓기 등 얇은 구조의 객체 조작에서 더 안정적인 성능을 보여, 특정 하드웨어에 맞게 설계된 시스템보다 우수함을 입증했습니다. Imitation Learning 응용에서는 AnyTeleop로 수집된 데이터가 SAPIEN 환경에서 [43]의 데이터보다 더 부드럽고 자체 충돌 없는 궤적을 제공하여 더 나은 Imitation Learning 성능을 이끌어냈습니다. 또한, AnyTeleop는 다중 운영자가 서로 다른 로봇을 제어하여 협업 조작 작업을 수행하는 설정을 지원할 수 있음을 Human-to-robot Handover 작업 시연을 통해 보여주었습니다.

실패 모드로는 빠른 손 움직임 중 추적 손실 및 자체 가림(self-occlusion) 시 불안정한 손 자세가 있습니다. 전자는 운영자가 손 움직임을 늦추도록 지시하고, 후자는 다중 카메라를 사용하여 완화할 수 있습니다.

결론적으로 AnyTeleop는 다양한 로봇, 현실, 카메라 설정 및 운영자 수에 적용 가능한 다목적 원격 조작 시스템입니다. 뛰어난 일반성 및 유연성을 제공하며, 시뮬레이션 및 실제 환경 모두에서 이전 시스템보다 우수한 성능을 보여줍니다. 오픈 소스화를 통해 원격 조작 분야의 추가 연구에 기여할 것입니다.

---

# Detail Review

## 기술적 기여 및 주요 아이디어 분석

사람의 섬세한 손동작을 로봇 팔-손 시스템으로 원격 조작(teleoperation)하는 것은 로봇에게 인간 수준의 지능적인 물체 조작 능력을 부여하는 유망한 접근법입니다. 특히 **비전 기반** 원격 조작은 값비싼 장비 없이 카메라만으로 이러한 목표를 달성할 수 있어 큰 주목을 받고 있습니다. 그러나 기존의 비전 기반 원격 조작 시스템들은 대개 **특정 로봇 모델**이나 **정해진 환경**만을 염두에 두고 설계되어 왔습니다. 이에 따라 새로운 로봇이나 다양한 작업 환경이 추가되면 시스템을 확장하기 어려운 한계가 있었습니다. 또한 대부분 **한 가지 현실(시뮬레이션 또는 실제)**에만 국한되거나, **단일 조종자-로봇 상호작용**만 지원하여 다중 로봇 협업에는 대응하지 못했습니다. 이러한 한계를 극복하기 위해 이 논문에서는 **AnyTeleop**이라는 범용 비전 기반 원격 조작 시스템을 제안합니다. AnyTeleop의 핵심 아이디어와 기술적 기여는 다음과 같습니다:

* **범용성과 통합성:** AnyTeleop은 하나의 시스템으로 **다양한 로봇 팔 및 다지(多指) 로봇 손** 조합을 지원하고, **여러 종류의 현실**(여러 시뮬레이터 플랫폼 또는 실제 환경)에서 동작할 수 있으며, **원격지**에서도 웹 브라우저를 통한 **실시간 시각 피드백**으로 조작이 가능하고, **RGB 또는 RGB-D 카메라** 여러 대를 활용한 **유연한 카메라 구성**을 허용하며, 나아가 **다수의 운영자가 각기 다른 로봇을 동시에 조작**하여 협업할 수 있습니다. 즉 **하나의 통합 시스템**으로 “모든(Any) 테레오퍼레이션” 상황을 아우르는 유연성을 달성했습니다.

<center>
<img src="../../images/2025-08-15-anyteleop/image-20250817172035265.png" width="100%" />
</center>

> *AnyTeleop 시스템이 다양한 시나리오에서 조작 작업을 수행하는 예시.* 상단 행은 IsaacGym 등 가상 시뮬레이터 상에서의 로봇 조작, 중단 행은 또 다른 시뮬레이터(SAPIEN) 상의 조작, 하단 행은 실제 로봇(XArm6+Allegro)이 컵 쌓기 등 일상 물체를 다루는 모습입니다. 이러한 다양한 환경과 작업에서 **일관된 원격 조작 프레임워크**를 제공하는 것이 AnyTeleop의 목표입니다.

* **모듈화된 아키텍처:** AnyTeleop은 **카메라로부터 사람 손의 동작을 추적**하고 이를 로봇 동작으로 변환하는 일련의 파이프라인을 모듈화하여 설계했습니다. 구체적으로, **① 손 자세 인식 모듈**(Hand Pose Detection)은 단일 혹은 복수의 RGB/RGB-D 카메라 입력으로부터 사람 손의 3차원 관절 위치(keypoint)를 추정합니다. 여러 카메라를 사용할 경우 **② 다중 카메라 융합 모듈**(Detection Fusion)이 각 카메라에서 얻은 손 추적 결과를 통합하여, 한 카메라에서 손이 가려져도 다른 시점 정보를 활용해 **자기 가림(self-occlusion)** 문제를 완화합니다. 그리고 **③ 손 자세 리타게팅 모듈**(Hand Pose Retargeting)이 사람 손의 관절각을 로봇 손의 관절각으로 변환하는데, 로봇의 URDF(기구학 모델) 정보만 있다면 **학습된 모델 없이**도 임의의 로봇 손 구조에 대응할 수 있도록 **최적화 기반 알고리즘**을 사용했습니다. 마지막으로 **④ 모션 생성 모듈**(Motion Generation)은 목표로 한 손목(End-effector)의 위치와 자세를 로봇 팔이 부드럽고 **충돌 없이** 따라가도록 **실시간 경로 생성**을 수행합니다. 본 논문에서는 NVIDIA의 GPU 가속 모션 계획 라이브러리인 **CuRobo**를 활용하여 120Hz 이상의 고주파수로 자연스러운 로봇 팔 움직임을 생성하며, 로봇의 자기 충돌이나 관절 한계 위반이 없도록 제어합니다.

* **웹 기반 원격 조작:** AnyTeleop은 **웹 시각화 인터페이스**를 제공하여, 원격지에 있는 운영자도 손쉽게 브라우저를 통해 로봇의 3D 환경을 실시간으로 모니터링하고 조작할 수 있게 했습니다. MeshCat/Three.js 기반으로 구현된 뷰어는 시뮬레이터나 실제 로봇으로부터 주기적으로 장면 정보를 받아와 동기화된 영상을 제공합니다. 이로써 공간적으로 떨어진 다수의 사용자들도 동일한 가상 환경을 보며 **협동 작업**을 수행할 수 있습니다. 또한 이러한 웹 기반 접근은 특정 시뮬레이터에 종속되지 않는 **시뮬레이터 불가지론적** 시각화를 가능케 하여, IsaacGym이든 SAPIEN이든 동일한 방식으로 원격 조작을 지원합니다.

요약하면, AnyTeleop은 **모든 구성요소를 표준 인터페이스로 분리**하여 필요에 따라 손쉽게 교체하거나 확장할 수 있고, 소프트웨어적으로 도커(container)로 배포되어 복잡한 의존성 설정 없이 **손쉽게 설치 및 실행**할 수 있도록 구현되었습니다. 이러한 **범용적이면서도 성능을 놓치지 않은** 설계 철학 덕분에, AnyTeleop은 다양한 조건에서 안정적으로 동작하면서도 인간 조종자의 섬세한 동작을 로봇에 충실히 재현해냅니다.

## 기존 연구와의 차별점

과거의 로봇 원격 조작 연구들은 주로 **특수 장비**에 의존하거나 **한정된 기능**만을 제공했습니다. 예를 들어 초기의 섬세한 손 조작 원격 제어는 데이터 장갑, 모션 캡처 수트, VR 컨트롤러, 햅틱 장치 등의 고가 하드웨어를 착용한 상태에서 수행되는 경우가 많았습니다. 반면 AnyTeleop은 **카메라 하나만으로** 사람 손의 움직임을 인식함으로써 장비 비용과 사용자 부담을 크게 낮추었습니다. 최근 등장한 비전 기반 원격 조작 시스템들도 있었지만, 이들 역시 각자 특정 로봇에 맞게 튜닝되어 범용성이 떨어지는 단점이 있었습니다. 예컨대 **DexPilot (2020)**은 KUKA LBR 팔 + Allegro Hand 조합을 비전으로 조작했지만 특정 하드웨어에 맞춘 보정(calibration)과 심도 카메라 등을 필요로 했고, **Robotic Telekinesis (2023)**와 같은 시스템은 XArm6 + Allegro에 특화된 **학습된 리타게팅 모델**과 **충돌회피 모델**을 사용하여 다른 로봇에는 적용이 어려웠습니다. 또한 **Holo-Dex (2022)**나 **DIME (2022)** 등의 연구는 AR/VR 장치를 통해 몰입감 있게 조작하거나, 학습 기반으로 효율적인 모방을 꾀했지만, **로봇 손만 제어**하고 **로봇 팔은 고려하지 않는** 등 시스템 범위가 제한적이었습니다. 이처럼 기존 연구들은 각각 일부 기능에 초점을 맞추었을 뿐, **여러 로봇과 환경을 통합적으로 지원**하거나 **협업 시나리오**까지 포괄하지는 못했습니다.

이에 비해 AnyTeleop의 가장 큰 차별점은 **범용적 모듈성**과 **확장성**입니다. 사람 손 추적부터 로봇 제어까지의 전 과정을 모듈화하고 표준화함으로써, **새로운 로봇 팔-손 시스템이나 새로운 환경에도 즉각 대응**할 수 있습니다. 예를 들어, 다른 형태의 로봇 손을 사용하려면 해당 URDF 기구학 모델만 제공하면 되고, 별도의 신경망 재학습 없이 곧바로 리타게팅과 제어가 가능합니다. 충돌 회피 역시 특정 로봇에 특화된 모델 대신 **CUDA 기반 기하 계산**으로 처리하여, 로봇 모양만 주어지면 즉석에서 충돌을 검사하고 회피경로를 산출합니다. 이러한 학습 비의존적 접근 덕분에 **새로운 로봇 추가나 환경 변경 시 발생하는 데이터 수집 및 모델 재교육 비용이 거의 들지 않으며**, 시스템이 자동적으로 일반화될 수 있습니다. 반면 기존 시스템들은 새로운 로봇마다 모션 변환기를 다시 학습시키거나 환경별로 손동작 데이터셋을 다시 수집해야 했기에 확장성이 낮았습니다.

또 다른 차별화 지점은 **협업 및 멀티-로봇 지원**입니다. 이전의 비전 기반 원격 조작 연구들은 대부분 한 명의 인간 조작자가 한 대의 로봇만 다루는 시나리오에 머물렀습니다. AnyTeleop은 처음으로 **두 명 이상의 사람이 각자의 로봇을 동시에 조작**하여 **공동의 작업**을 수행하는 비전 기반 시스템을 선보였습니다. 예컨대 한 운영자가 물체를 집어 다른 로봇에게 건네주고, 다른 운영자가 그 로봇을 통해 이를 받아 최종 목표 지점에 두는 식의 **다중 로봇 협동**도 구현됩니다 (뒤의 실험 결과 참조). 이러한 기능은 이전 연구에서는 보고된 바 없어 AnyTeleop의 **독보적인 기능적 차별성**이라 할 수 있습니다.

종합하면, AnyTeleop은 기존의 원격 조작 연구들이 개별적으로 추구하던 목표들(예: 저비용 비전 기반 추적, 범용 로봇 지원, 시뮬레이션 활용, 협업 등)을 **단일 시스템에 통합**함으로써 실현했다는 점에서 의미가 큽니다. 이를 통해 성능 저하 없이도 **더 넓은 적용 범위**를 커버하며, 실제로 실험을 통해 이전 특수목적 시스템들보다 **더 나은 성능**을 입증하였습니다. 즉 AnyTeleop은 **범용성 vs. 성능**의 트레이드오프를 극복한 사례로 평가됩니다.

## 실험 설정 및 결과 평가

논문 저자들은 AnyTeleop의 성능을 **정량적 실험**을 통해 검증하기 위해, **실제 로봇 실험**과 **가상 시뮬레이션 실험**을 모두 수행했습니다.

* **실제 로봇 실험:** 

기존 **Robotic Telekinesis** 시스템과의 성능 비교를 위해, 동일한 로봇 하드웨어인 *HxArm6* 로봇 팔과 *Allegro* 로봇 손을 사용했습니다. 실험에 사용된 **작업(task)**들은 Telekinesis 논문에서 제시된 10가지 조작 작업으로, 작은 상자를 집어 옮기기, 천으로 만든 장난감 집기, 상자 회전시키기, 가위 잡기, 컵 포개 쌓기, 이중 컵 쌓기, 상자에 든 큐브를 접시로 붓기, 컵을 접시에 옮겨 담기, 서랍 열기, 서랍 열고 물건 집어내기 등이 포함됩니다. 각 작업에 대해 숙련된 조작자가 AnyTeleop을 통해 10번씩 시도를 하였고, 단일 **Intel RealSense RGB-D 카메라**로 손동작을 추적했습니다. Telekinesis (기존 시스템)의 경우 해당 논문에 보고된 성공률을 그대로 비교 대상으로 사용했습니다. **성공률 측정**은 10회 시도 중 작업 목표를 완수한 비율로 정의되었습니다.

**결과:** 

비교 결과 AnyTeleop은 **10개 중 8개 작업에서 더 높은 성공률**을 보였고, **나머지 2개 작업에서는 대등한 성능(동일 성공률)**을 나타냈습니다. 구체적으로, 예를 들어 *두 개의 컵 쌓기* 작업의 경우 기존 시스템 성공률이 30%에 그쳤던 반면 AnyTeleop은 **70%의 성공률**을 달성했고, *컵을 접시에 옮기기* 작업도 기존 80%에서 AnyTeleop은 **100% 성공**으로 향상되었습니다. 대부분의 작업에서 AnyTeleop이 향상된 성과를 보였으며, *상자 회전*이나 *큐브 붓기* 같은 작업에서는 두 시스템 모두 유사한 성공률을 보였습니다. 흥미롭게도 저자들은 AnyTeleop의 **우수한 성능 요인**으로, **얇은 벽 구조의 물체(컵 등)를 다루는 작업에서의 강점**을 꼽았습니다. AnyTeleop의 **최적화 기반 손 리타게팅 모듈**은 사람 손가락 사이 거리를 로봇 손가락도 최대한 좁혀서 섬세하게 재현하기 때문에, 얇은 컵을 단단히 쥐는 동작을 성공적으로 모사했습니다. 반면 Telekinesis의 **신경망 기반 리타게팅**은 이러한 미세한 그립(grip) 동작을 정확히 전달하지 못해 컵을 놓치는 경우가 있었다고 분석합니다. 요컨대 AnyTeleop은 **더 범용적인 시스템임에도 불구하고 특정 하드웨어에 맞춰진 기존 시스템보다 오히려 높은 조작 성공률**을 보여주었고, 특히 **정밀한 손가락 제어가 필요한 작업에서 두드러진 강점**을 입증했습니다.

**시뮬레이션 및 모방학습 실험:** 

AnyTeleop의 범용성은 가상 환경에서도 시험되었습니다. 저자들은 로봇 학습에서 중요한 **인간 시연 데이터 수집** 용도를 평가하기 위해, 시뮬레이터 상에서 원격 조작으로 모방 학습용 데이터를 모으고 그 품질을 비교했습니다. 비교 대상으로는 최근 발표된 비전 기반 원격 조작 시스템인 Qin 등 (2022)을 선정했는데, 이 시스템은 **단일 RGB-D 카메라**로 **부유하는 손(floating hand)**만 조작할 수 있는 제한이 있었습니다. 실험에 사용된 **조작 작업**들은 해당 기존 연구와 동일하게 3가지로 설정되었으며, (i) 탁자 위 물체를 집어 다른 위치로 옮기는 *위치 이동(Relocate)*, (ii) 뒤집혀 놓인 머그잔을 90도 회전시켜 올려놓는 *머그 뒤집기(Flip Mug)*, (iii) 문 손잡이를 돌려 잠금을 풀고 문을 당겨 여는 *문 열기(Open Door)* 입니다. 각 작업은 **부유 손(floating hand)** 버전과 **팔이 달린 손(arm-hand)** 버전 두 가지 변형으로 수행되었습니다. 부유 손은 로봇 팔이 없는 자유롭게 움직이는 손만으로 조작하는 경우이고, 팔-손 버전은 로봇 팔 끝에 손이 달려있어 고정된 기반(base)을 갖는 현실적인 경우입니다.

**데이터 수집 및 학습:** 

우선, 기존 시스템과 AnyTeleop을 이용하여 각 작업당 **50개 시연(trajactory)** 데이터를 수집했습니다. 기존 시스템은 부유 손 형태만 직접 조작 가능했기에, 팔-손 버전 시연을 얻기 위해 저자들이 별도의 **시연 변환 과정**을 적용했습니다. 반면 AnyTeleop은 처음부터 팔-손 조작을 지원하므로, 팔-손 형태로 50개 시연을 수집한 뒤 이를 부유 손 형태로 변환하여 두 방식 모두에 활용했습니다. 이렇게 얻은 시연 데이터로 **Demo Augmented Policy Gradient (DAPG)** 알고리즘을 이용해 모방 학습 정책을 훈련하였고, 비교를 위해 **순수 강화학습(RL)**으로만 훈련한 정책도 함께 평가했습니다. 각 방법에 대해 서로 동일한 신경망 구조와 보상 설계를 사용하여 공정성을 유지했습니다. 최종 정책들은 3개의 시드로 학습되어, **각각 100회 에피소드 평가**를 통해 성공률을 산출했습니다.

**결과:** 

**Table VI**에 보고된 결과에 따르면, AnyTeleop을 통해 수집한 시연으로 학습한 정책이 **6개의 평가 시나리오 중 5개에서** 기존 시스템 시연으로 학습한 정책보다 높은 성공률을 기록했습니다. 또한 이 정책들은 사람 시연 없이 강화학습만으로 훈련한 정책보다도 월등히 높은 성능을 보였습니다. 특히 **팔-손 조작이 포함된 과제들**에서 AnyTeleop 기반 정책의 성능 향상이 두드러졌는데, 그 이유는 다음과 같습니다. (i) AnyTeleop으로 얻은 시연 궤적은 **매우 부드럽고 자연스러운 연결 동작**으로 이루어져 있어, 상태-행동 쌍이 일관되고 학습하기 용이했습니다. 반면 기존 시스템의 시연 데이터는 손동작이 불연속적으로 추정되거나 팔 움직임 생성 시 다소 튐(jitter)이 있어 학습 난이도가 높았습니다. (ii) AnyTeleop은 **팔-손 시스템을 직접 조작**하면서 **자기 충돌이 발생하지 않도록 제약**을 거는 등 물리적으로 **유효한 시연만을 수집**합니다. 하지만 기존 시스템은 팔이 없는 손 시연을 사후에 로봇 팔에 재현(retargeting)하는 방식을 쓰다 보니, 그 과정에서 로봇 팔이 자기 자신을 때리는 등의 **충돌이 포함된 시연**이 여럿 발생했습니다. 이러한 차이로 인해 팔-손 조작 과제들에서 AnyTeleop 시연으로 학습한 정책이 훨씬 높은 성공률을 얻은 것입니다. 한 가지 예외적으로, *팔을 이용한 머그 뒤집기* 과제에서는 두 시스템의 학습 정책 성능이 비슷했는데, 저자들은 **팔이 있는 상태로 머그를 뒤집는 시연 자체가 매우 어렵고** AnyTeleop으로도 해당 데모의 품질이 떨어졌기 때문이라고 분석했습니다.

종합하면, **AnyTeleop을 통한 원격 조작 데모가 기존 시스템보다 질적으로 우수**하며, 이는 **다운스트림 로봇 학습 성능 향상**으로 직접 이어짐을 보였습니다. 이로써 AnyTeleop의 범용성이 **단순히 여러 로봇을 조작할 수 있다는 것뿐만 아니라**, **실제 활용 가치** 측면에서도 더 나은 데이터를 제공한다는 점이 입증되었습니다.

* **협동 조작 시연:** 앞서 언급한 **다중 운영자 협업** 시나리오의 가능성도 실험적으로 보여주었습니다. 저자들은 예시로 한 명의 사람 조종자가 로봇 팔 A를 이용해 물체를 집어 다른 로봇 팔 B에게 건네주고, 두 번째 조종자가 그 로봇 B를 제어해 물체를 받는 **“사람→로봇 핸드오버”** 과제를 수행했습니다. 구체적으로, **운영자 1**은 *UR10 팔 + Schunk 5지 로봇 손*을 AnyTeleop으로 조작하여 테이블 위 물체를 집어 들고, **운영자 2**는 *KUKA 팔 + Shadow 로봇 손*을 조작하여 그 물체를 건네받았습니다. 이 협업 시나리오는 두 운영자가 **같은 가상 환경**을 실시간으로 바라보면서 동작을 조율해야 했는데, AnyTeleop의 **웹 기반 동기화 뷰어**를 통해 두 사용자 화면에 항상 동일한 장면이 표시되고, 각자의 손동작이 중앙 **시뮬레이션 서버**에서 통합되어 두 로봇의 움직임으로 반영됨으로써 가능했습니다. **그림 4**는 이 핸드오버 작업의 한 장면을 보여주는데, 왼쪽의 흰색 UR10-슁크 로봇이 물체를 건네고 오른쪽 주황색 KUKA-섀도우 로봇이 그것을 받는 모습입니다.

<center>
<img src="../../images/2025-08-15-anyteleop/image-20250817172921843.png" width="70%" />
</center>

<center>
<img src="../../images/2025-08-15-anyteleop/image-20250817172900752.png" width="70%" />
</center>

> *두 운영자가 원격으로 각기 다른 로봇 팔-손을 조작하여 물체를 주고받는 협동 조작 예시.* AnyTeleop의 중앙 서버-다중 유닛 구조를 활용하면 물리적으로 떨어진 장소에 있는 사용자들도 마치 한 공간에서 두 로봇을 함께 조작하는 것처럼 **협응(coordinate)**할 수 있습니다. 해당 데모는 **정량적 수치**로 평가되지는 않았지만, AnyTeleop이 **협업 데이터 수집**에도 유용하게 쓰일 수 있음을 보여줍니다. 저자들은 사람이 둘 이상 필요한 로봇 작업(예: 무거운 물체 공동 운반, 조립 작업 등)의 시연 데이터를 얻는 것이 얼마나 어려운지 언급하며, AnyTeleop이 이러한 **복잡한 시나리오의 데이터 수집을 손쉽게 해주는 도구**가 될 수 있다고 강조합니다.

## 한계점 및 향후 연구 방향

AnyTeleop은 현재까지의 원격 조작 시스템 중 가장 범용적이고 성능도 뛰어났지만, 여전히 몇 가지 **한계점**과 **도전 과제**가 존재합니다.

* **비전 기반 손 추적의 한계:** AnyTeleop은 상업용 카메라와 딥러닝 기반 손 인식에 의존하기 때문에, **사람 손이 너무 빠르게 움직일 경우 추적이 따라가지 못해 손실**되는 문제가 있습니다. 실제 실험 중에도 조종자가 손을 급격히 휘두르면 시스템이 일시 정지한 뒤 손 위치를 재인식(re-detection)해야 하는 상황이 발생했습니다. 또한 단일 카메라로 손을 볼 때 **손의 일부가 자기 몸으로 가려지면** 인식된 손 자세의 정확도가 떨어지는 현상이 있습니다. 이러한 문제를 완화하기 위해 현재는 **“천천히 움직이라”**고 조종자에게 안내하거나, **여러 대의 카메라**를 배치하여 다양한 각도에서 손을 관찰하는 방식을 사용합니다. 그러나 이는 근본적인 해결책이 아니므로, **향후 연구**로 더 향상된 손 추적 비전 알고리즘을 도입하거나 딥러닝 모델을 개선하여 **속도와 정확도를 높이는 방향**이 필요합니다. 예컨대, 빠른 동작 예측을 위한 **추론 프레임 보간**이나, **자기 가림 상황을 극복할 수 있는 3D 모델 기반 추적** 기법 등을 적용해볼 수 있을 것입니다.

* **실시간 시스템 성능:** AnyTeleop이 원활한 조작을 위해 요구하는 일부 기술적 사양도 한계로 지적됩니다. 논문에 따르면 손 추적 모듈은 GPU 상에서 25Hz 정도로 동작하고, 로봇 팔 모션 생성은 100Hz 이상으로 구동되어야 가장 자연스러운데, **모든 모듈을 단일 PC에서 동시에 실행하면 성능이 저하**될 수 있다고 합니다. 이를 해결하기 위해 손 추적/제어 모듈과 시뮬레이터를 **별도의 컴퓨터로 분리**하여 운용하면 성능을 유지할 수 있었지만, 일반 사용자 입장에서는 여러 대의 고성능 장비를 요구하는 셈이어서 부담일 수 있습니다. 따라서 향후에는 **더 경량화된 추적 모델**이나 **최적화된 병렬 처리**를 통해 **단일 시스템에서도 충분한 주파수로 동작**할 수 있도록 개선하는 연구가 기대됩니다.

* **감각 피드백의 부재:** 현재 AnyTeleop 조종자는 **시각 정보**에만 의존하여 원격 환경을 느낄 수 있습니다. 반면 기존의 장갑 기반 시스템들은 일부 **촉각/힘 피드백**을 제공하여 사용자가 물체를 쥐는 힘이나 질감을 느낄 수 있게 했습니다. AnyTeleop은 이러한 햅틱 장비를 배제함으로써 간편성과 범용성을 얻었지만, 동시에 조종자가 **촉각적 단서 없이** 화면만 보고 조작해야 한다는 제약이 있습니다. 이는 매우 섬세한 힘 조절이 필요한 작업(예: 계란을 잡기 등)에서 어려움을 초래할 수 있습니다. 향후 연구로 **증강현실(AR)이나 햅틱 피드백 장치**를 선택적으로 결합하여, 필요한 경우 조종자에게 **추가적인 감각 피드백**을 제공하는 방향을 모색해볼 수 있습니다. 예를 들어 AR 헤드셋을 사용해 원격 장면을 1인칭으로 보여주거나, 로봇 손끝에 힘 센서를 달아 조종자 손에 진동 피드백을 주는 방안 등이考해볼 수 있습니다 (물론 이는 시스템 복잡도를 높여 AnyTeleop의 장점인 저비용성을 희석시킬 수 있으므로 균형이 필요합니다).

* **복잡한 작업 및 다중 로봇 시나리오:** AnyTeleop으로 시연된 과제들은 주로 단일 로봇 팔-손이 하나의 물체를 다루는 작업 또는 두 로봇 간의 간단한 물체 전달 작업이었습니다. 향후에는 **보다 복잡한 협업 시나리오** (예: 두 로봇이 동시에 서로 다른 도구를 사용해 조립 작업을 한다거나, 다수의 로봇과 인간이 섞인 팀 협업)으로 확장하는 도전이 있습니다. 이를 위해서는 여러 조종자의 동작 타이밍을 자동으로 조율해주는 **공동 제어 알고리즘**, 물체를 주고받는 이상의 **정교한 상호작용 프로토콜** 등이 필요할 것입니다. 또한 현 단계에서는 한 명의 조종자가 하나의 로봇만 제어하지만, 장차 **한 사람이 다수의 로봇을 제어**하거나 **한 로봇을 여러 명이 제어**하는 특별한 경우(예: 거대한 로봇을 두 사람이 각각 다른 부분을 조작)도 고려해볼 수 있습니다. 이러한 확장은 AnyTeleop의 모듈식 구조를 한층 발전시켜, **고차원의 공동 제어** 문제를 다루는 연구로 이어질 수 있습니다.

요약하면, AnyTeleop은 훌륭한 성능과 범용성을 달성했지만 **비전 센싱의 한계**, **시스템 성능 요구사항**, **피드백 부족**, **복잡한 확장 시나리오** 등의 면에서 향후 개선의 여지가 있습니다. 다행히 저자들은 이 시스템을 **오픈 소스**로 공개하여 연구자들이 자유롭게 개선·활용하도록 할 계획이며, 이를 통해 다양한 후속 연구가 활발히 전개될 것으로 기대됩니다.

## 종합 정리

“**AnyTeleop: A General Vision-Based Dexterous Robot Arm-Hand Teleoperation System**” 논문은 로봇 원격 조작 분야에서 **중요한 이정표**를 세운 연구로 평가할 수 있습니다. 이 연구는 하나의 통합된 시스템으로 다양한 로봇과 환경을 지원하면서도, 개별 특화 시스템에 비견하거나 뛰어넘는 성능을 입증하였습니다. **범용성**과 **성능**을 동시에 잡았다는 점에서 학술적 **독창성**이 돋보이며, 특히 다중 로봇 협업까지 아우르는 비전 기반 테레오퍼레이션은 처음 시도되었다는 점에서 **신규성**이 있습니다.

AnyTeleop이 미칠 **실용적 영향**도 상당합니다. 우선 로봇 학습을 위한 **데몬스트레이션 수집**을 규모 확장하는 데에 큰 기여를 할 것으로 보입니다. 과거에는 많은 로봇 시연 데이터를 모으기 위해 복잡한 장비와 인력이 필요했지만, AnyTeleop을 활용하면 저비용 카메라와 인터넷만으로도 어디서든 사람이 로봇에게 시연을 가르칠 수 있습니다. 이는 곧 대규모 **모방 학습 데이터셋 구축**이나, 원격 **크라우드소싱**을 통한 로봇 학습으로 이어질 수 있습니다. 또한 시뮬레이터와 실제를 한 시스템에서 모두 지원하므로, **시뮬레이션-현실 격차(sim-to-real gap)**를 줄이고 보다 빠르게 현실 로봇에 학습 결과를 적용하는 사이클을 구축할 수 있습니다.

산업 및 사회적으로도 응용 가능성이 무궁무진합니다. 예를 들어, **위험한 환경**에서 인간을 대신해 작업해야 하는 로봇을 AnyTeleop으로 조종한다면, 작업자는 안전한 장소에서 카메라 원격장비로 로봇을 제어할 수 있습니다. 이는 원자력 발전소 점검, 화학 물질 취급, 화재 진압, 재난 현장 수색 등의 분야에서 **원격 존재(telepresence)** 로봇의 활용을 용이하게 할 것입니다. 또한 **의료/복지 분야**에서도 숙련된 전문가가 멀리 떨어진 곳의 로봇 팔을 원격 조종해 수술을 보조하거나, 장애인이 집에 있는 로봇 손을 원격 조작해 일상 작업을 수행하는 등 **원격 협력** 시나리오가 실현될 수 있습니다. 두 명 이상의 사람이 협동하여 거리를 초월해 로봇들을 움직이는 기술은 향후 **원격 제조**나 **원격 협업 연구실** 등에도 적용되어, 공간의 제약 없이 인력이 협업하는 새로운 형태의 작업 환경을 만들어낼 수 있습니다.

마지막으로, AnyTeleop은 **오픈 소스로 공개**됨으로써 연구 커뮤니티와 산업계 모두에 **플랫폼**을 제공할 것입니다. 이제 개별 로봇 회사나 연구실이 각자 원격 제어 시스템을 처음부터 만들 필요 없이, AnyTeleop을 기반으로 자신들의 로봇에 맞춰 확장하고 개선하는 식으로 발전이 가속화될 것입니다. 이러한 **공유 인프라**의 등장은 로봇공학 분야에서 표준을 정립하고 중복 노력을 줄이는 효과도 기대됩니다.

결론적으로, AnyTeleop은 **비전 인식, 최적화 제어, 웹기술**을 융합하여 로봇 원격 조작의 새로운 지평을 연 시스템입니다. 사람의 능숙한 손동작을 전례 없이 다양한 로봇에 이식함으로써, 인간과 로봇의 거리를 한층 좁혔습니다. 향후 이 연구를 바탕으로 보다 향상된 추적 기술, 사용자 피드백 통합, 그리고 인간-로봇 협업의 복잡한 문제들에 대한 추가 연구가 이어진다면, **언제 어디서나 누구나 로봇을 조종할 수 있는 시대**가 성큼 다가올 것으로 기대됩니다. AnyTeleop이 구축한 탄탄한 토대 위에서, 로봇 원격 조작의 **스케일 업(scale-up)**과 실용화가 한층 가속화될 것입니다.
