---
draft: true
title: "ğŸ‘©â€ğŸ’»JAX"
description: 2023 Pycon JAX Tutorial
date: "2023-08-21"
categories: [context, rl, paper]
toc: true
number-sections: true
---


# JAX 101

- JAX is Autograd and XLA, brought together for high-performance numerical computing `JAXëŠ” XLAì™€ Autogradë¥¼ ì´ìš©í•´ ë¨¸ì‹ ëŸ¬ë‹ ì—°êµ¬ì™€ ê³ ì„±ëŠ¥ ì—°ì‚°ì‘ì—…ì„ ìœ„í•´ ë§Œë“  í”„ë ˆì„ ì›Œí¬`

- NumPyì™€ ë¹„ìŠ·í•˜ë‹¤
- ì—°ì‚°ì¥ì¹˜(CPU/GPU/TPU)ë³„ ì´ë™ì´ ììœ ë¡­ë‹¤?

1. Numpyì™€ ìœ ì‚¬í•˜ê²Œ ì‚¬ìš©

```python
import jax
import jax.numpy as jnp

import

```

**BUT** ì°¨ì´ì ì€ ìˆë‹¤.
- NumPyì˜ ArrayëŠ” **ê°€ë³€ì **
- JAX NumPy ArrayëŠ” **ë¶ˆë³€ì **: `y = x.at[idx].set(value)`ì™€ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ë³€ê²½í•´ì•¼ í•¨

1-1. Numpy case

```python
import numpy as np

x = np.array([1, 2, 3])

def in_place_modify(x):
    x[0] = 123
    return None

in_place_modify(x)
x
```

1-2. JAX case
```python
def jax_in_place_modify(x):
    return x.at[0].set(123)

y = jnp.array([1, 2, 3])
jax_in_place_modify(y)
```

2. JIT(Just-In-Time) Compilation

- Interpreterì™€ Compilerì˜ ì ˆì¶©ì•ˆ
- ëŸ°íƒ€ì„ ì‹¤í–‰ ì‹œ ì½”ë“œì˜ ì¼ë¶€ë¶„ì„ ì»´íŒŒì¼í•˜ì—¬ ì†ë„ë¥¼ í–¥ìƒ
- JAXì—ì„œëŠ” `@jit` ë˜ëŠ” `jsx.jit()`ìœ¼ë¡œ ì‚¬ìš©

**XLA** ë‹¤ì‹œë³´ê¸°!

```
XLA(Accelerated Linear Algebra)ëŠ” CPU, GPU ë° ë§ì¶¤í˜• ì•¡ì…€ëŸ¬ë ˆì´í„°(like TPU)ì™€ ê°™ì€ ì—°ì‚°ê¸°ê¸°ì— ëŒ€í•´ JIT ì»´íŒŒì¼ ê¸°ë²•ì„ ì‚¬ìš©í•˜ì—¬ ëŸ°íƒ€ì„ì— ì‚¬ìš©ìê°€ ìƒì„±í•œ ì—°ì‚° ê·¸ë˜í”„ë¥¼ ë¶„ì„í•˜ê³  ì‹¤ì œ ëŸ°íƒ€ì„ ì°¨ì›ê³¼ ìœ í˜•ì— ë§ê²Œ ìµœì í™”í•˜ë©°, ì—¬ëŸ¬ ì—°ì‚°ì„ í•¨ê»˜ í•©ì„±í•˜ê³  ì´ì— ëŒ€í•œ íš¨ìœ¨ì ì¸ ë„¤ì´í‹°ë¸Œ ê¸°ê³„ì–´ ì½”ë“œë¥¼ ë‚´ë³´ë‚¸ë‹¤.

```

JIT íš¨ê³¼ í™•ì¸í•˜ê¸°

```python
import jax
import jax.numpy as jnp

def selu(x, alpha=1.67, lambda_=1.05):
    return lambda_ * jnp.where(x>0, x, alpha * jnp.exp(x) - alpha)

x = jnp.arange(100000)
%timeit selu(x).block_until_ready() # JIT ì‚¬ìš©í•˜ì§€ ì•Šì„ ë•Œ

selu_jit = jax.jit(selu)

# Warm up
selu_jit(x).block_until_ready()

%timeit selu_jit(x).block_until_ready() # JIT ì‚¬ìš©í–ˆì„ ë•Œ

```

3. Autograd

- `jax.grad()`ë¡œ ìë™ ë¯¸ë¶„

```python
import jax

f = lambda x: x**3 + 2*x**2 - 3*x + 1

dfdx = jax.grad(f)
d2fdx = jax.grad(dfdx)
d3fdx = jax.grad(d2fdx)
d4fdx = jax.grad(d3fdx)

```

- `stop_gradient()`: ëª¨ë¸ì´ ì—¬ëŸ¬ê°œ ìˆì„ ë•Œ, íŠ¹ì • ëª¨ë¸ì´ í•™ìŠµí•˜ëŠ” ë™ì•ˆ ë‹¤ë¥¸ ëª¨ë¸ì€ í•™ìŠµì„ ë©ˆì¶”ê¸¸ ë°”ë„ ë•Œ
    - meta learning ê¸°ë²•ì—ì„œ ì‚¬ìš© ê°€ëŠ¥

```python 
class blabla(nn.Module):

class A(nn.Module):

class B(nn.Module):

def __cal__:
    a = A(input)
    b = B(a)
    return b
```

```python
def train_fn(A_params, B_params, x, y):
    a = model_A.apply({'params': A_params}, x)
    b = model_B.apply({'params': B_params}, a)
    return loss_fn(b, y)

def train_fn_A_learn(A_params, B_params, x, y):
    return train_fn(A_params, jax.lax.stop_gradient(B_params), x, y)

def train_fn_B_learn(A_params, B_params, )
```



