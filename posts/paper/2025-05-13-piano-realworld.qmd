---
draft: true
title: "📃Learning to play piano 리뷰"
description: Learning to Play Piano in the Real World
date: "2025-05-13"
categories: [paper, hand]
toc: true
number-sections: true
---

1.  이 연구는 학습 방식을 활용하여 실제 로봇이 피아노를 연주하는 최초의 시스템을 제시합니다.
2.  Sim2Real 강화 학습을 통해 시뮬레이션에서 정책을 학습시킨 후 실제 다지 로봇에 적용했으며, 하이브리드 실행 모드가 가장 효과적이었습니다.
3.  실험 결과, 이 시스템은 여러 간단한 곡을 성공적으로 연주하여 Sim2Real 접근 방식의 가능성을 확인했습니다.

---

# Brief Review

이 논문은 로봇의 인간 수준 조작 능력(human-level manipulation)을 달성하기 위한 중요한 도전 과제 중 하나로, 현실 세계에서 로봇이 피아노를 연주하는 데 학습 기반 접근 방식을 성공적으로 적용한 최초의 사례를 제시합니다. 피아노 연주는 전략적이고 정밀하며 유연한 움직임이 필요한 복잡하고 접촉이 많은(contact-rich) 작업입니다. 기존 연구들은 주로 수동으로 설계된 제어기(hand-designed controllers)를 사용하거나 시뮬레이션 환경에서 로봇 학습(robot learning)을 평가했지만, 본 논문은 정교한 다지(dexterous multi-finger) 로봇 손을 사용하여 Sim2Real 접근 방식을 통해 현실 세계 피아노 연주를 구현했습니다.

연구의 핵심 방법론은 다음과 같습니다. 먼저 강화 학습(Reinforcement Learning)을 사용하여 시뮬레이션 환경에서 로봇의 피아노 연주 정책(policy)을 훈련시킵니다. 훈련된 정책은 현실 세계 로봇에 배포됩니다. Sim2Real 갭을 극복하기 위해 시뮬레이션 매개변수(예: 관절 댐핑, 마찰 계수, 스프링 강성 등)에 대한 Domain Randomization 기법을 활용했습니다.

하드웨어 구성으로는 UFACTORY xArm7 로봇 팔에 Allegro hand v4.0 왼손 모델을 장착했습니다. Allegro 손은 각 손가락에 4개의 관절을 가지며 총 16개의 관절이 있지만, 엄지 관절은 피아노 건반에 닿기 어려워 사용하지 않았습니다. 따라서 총 12개의 관절이 활성 상태입니다. 각 손가락 끝은 표준 Allegro 손가락 끝보다 얇은 3D 프린팅된 손가락으로 교체하여 한 번에 하나의 건반만 정확히 누를 수 있도록 했습니다. 피아노로는 M-Audio Keystation 49e MIDI 키보드를 사용했으며, 이 키보드는 건반 누름 및 떼기 이벤트를 MIDI 신호로 전송하여 로봇이 현재 눌린 건반 상태를 관찰할 수 있도록 합니다 (부분적으로 눌린 상태는 관찰 불가).

시뮬레이션 환경은 Robopianist 환경 [18]을 기반으로 Mujoco 물리 엔진 [13]과 dm_control [14] 래퍼를 사용하여 구축되었습니다. 시뮬레이션 모델은 현실 세계와 유사하게 12개의 활성 관절과 피아노와 평행하게 손을 움직일 수 있는 슬라이더 관절을 가진 Allegro 손, 그리고 49개의 건반을 가진 피아노 모델로 구성됩니다.

문제는 확률적 부분 관측 마르코프 결정 과정(Stochastic, Partial Observable MDP)으로 모델링됩니다.
*   상태 공간($S$)과 전이 확률($p$)은 Mujoco 물리 엔진에 의해 정의됩니다.
*   행동 공간($A$)은 13개의 연속적인 값입니다. 로봇 손의 12개 관절 각각에 대한 행동과 피아노와 평행한 손목 이동을 위한 1개의 행동입니다.
*   관측 공간($O$)은 356차원입니다. 여기에는 Allegro 손의 사용된 관절 위치, 평행 이동 슬라이더 위치, 현재 눌린 건반 상태(원-핫 인코딩), 현재 목표 건반(원-핫 인코딩), 그리고 향후 여러 시간 단계의 목표 건반들(원-핫 인코딩)이 포함됩니다. MIDI 표준의 제약으로 인해 건반 상태는 이산적으로만 관측 가능하며, 이는 건반 위에서 손가락을 움직이며 정보를 얻는 것을 어렵게 만듭니다. 또한, 4개 손가락 구조로 인해 일반적인 인간 운지법을 적용하기 어려워 운지 정보가 관측 공간에 포함되지 않았습니다.

보상 함수($r_{\text{total}}$)는 네 가지 구성 요소의 합으로 정의됩니다: $r_{\text{total}} = r_{\text{energy}} + r_{\text{hand position}} + r_{\text{keypress}} + r_{\text{sliding}}$.
*   $r_{\text{energy}}$는 빠른 움직임, 큰 힘, 불필요한 떨림을 억제합니다. 수식은 $r_{\text{energy}} = -|\tau_{\text{joints}}|^T |v_{\text{joints}}| \cdot C_{\text{energy}}$이며, $\tau_{\text{joints}}$는 관절 토크, $v_{\text{joints}}$는 관절 속도, $C_{\text{energy}}$는 에너지 계수입니다.
*   $r_{\text{hand position}}$은 손바닥($p_{\text{palm}}$)과 목표 건반 위치($P_{\text{targets}}$) 사이의 거리를 줄여 손이 목표 건반으로 향하도록 유도합니다. 수식은 $r_{\text{hand position}} = \frac{1}{|P_{\text{targets}}|} \sum_{p_{\text{key}} \in P_{\text{targets}}} g(||p_{\text{key}} - p_{\text{palm}}||_2)$이며, $g$는 거리를 정규화하는 함수입니다.
*   $r_{\text{keypress}}$는 올바른 건반을 누르도록 장려하는 가장 중요한 보상입니다. 아무 건반도 누르지 않는 것보다 잘못된 건반을 누르는 것이 더 나쁘지 않아야 하고, 잘못된 건반을 누르는 것보다 올바른 건반을 누르는 것이 더 좋아야 한다는 원칙에 기반합니다. 이는 목표 건반 수($k_{\text{target}}$), 잘못 눌린 건반 수($k_{\text{wrong}}$), 올바르게 눌린 건반 수($k_{\text{correct}}$), 목표 건반의 평균 눌림 상태($\bar{\mu}_{\text{target}}$), 잘못 눌린 건반들의 눌림 상태 집합($\mu_{\text{wrong}}$)에 따라 다음과 같이 계산됩니다 (Appendix A 참조):
    *   $k_{\text{target}} > 0$일 때:
        *   $r_{\text{keypress}} = 0$, $k_{\text{wrong}} = 0 \land k_{\text{correct}} = 0$
        *   $r_{\text{keypress}} = 0.5 + 0.5 \cdot \bar{\mu}_{\text{target}}$, $k_{\text{wrong}} > 0$
        *   $r_{\text{keypress}} = 1 + 0.5 \cdot \bar{\mu}_{\text{target}}$, $k_{\text{wrong}} = 0 \land k_{\text{correct}} > 0$
    *   $k_{\text{target}} = 0$일 때:
        *   $r_{\text{keypress}} = 2 \cdot (1 - \max_{\mu' \in \mu_{\text{wrong}}}\mu')$
*   $r_{\text{sliding}}$은 건반이 눌린 상태에서 손이 빠르게 옆으로 움직이는 것을 억제합니다. 이는 슬라이더를 이용한 부자연스러운 연주를 막고, 인접한 건반이 동시에 눌린 경우 패널티를 두 배로 부과합니다. 수식은 $r_{\text{sliding}} = -\lambda \cdot v_{\text{hand}}^2 \cdot 3$이며, $\lambda \in \{0, 1, 2\}$는 인접 건반 눌림 상태에 따라 결정되고 $v_{\text{hand}}$는 손의 속도입니다.

모델 훈련에는 Soft Actor-Critic 알고리즘인 DroQ [2]를 사용했습니다. 
평가 지표로는 Precision, Recall, F1 점수($F1 = 2 \frac{\text{recall} \cdot \text{precision}}{\text{recall} + \text{precision}}$)를 사용하며, 이는 목표 건반 상태와 현재 건반 상태를 비교하여 계산됩니다. 
정책의 제어 주파수는 20 Hz입니다.

현실 세계 배포를 위해 세 가지 실행 모드를 테스트했습니다: Joint Mirroring (시뮬레이션 관측만 사용), Real World Execution (현실 세계 관측만 사용), Hybrid Execution (눌린 건반은 현실, 관절 위치 등은 시뮬레이션 관측 사용). 
다양한 복잡성의 네 가지 곡(Twinkle Twinkle Little Star, C-Major Scale, D-Major Scale, Chord Progression)으로 성능을 평가했습니다.

실험 결과, Hybrid Execution 모드가 전반적으로 가장 좋은 성능을 보였습니다. 
이는 현실 세계의 눌린 건반 정보를 통해 오류를 수정하면서도 시뮬레이션에서 학습된 안정적인 관절 움직임을 활용할 수 있기 때문입니다. 
Domain Randomization 강도에 대한 분석 결과, 시뮬레이션 성능은 강도가 증가할수록 떨어지지만, 현실 세계 성능은 중간 수준의 강도에서 가장 높게 나타났습니다. 
이는 적절한 Domain Randomization이 모델을 현실 세계의 불확실성에 대해 더 강건하게 만들지만, 너무 강하면 학습 자체가 어려워지기 때문입니다.

본 연구는 현실 세계에서 로봇이 피아노를 연주하도록 학습시키는 개념 증명(proof-of-concept)을 성공적으로 제시했지만, 몇 가지 한계점도 있습니다. 
특정 곡에 대한 정책의 일반화 능력 부족(새로운 곡마다 재훈련 필요), 촉각 센서의 부재, 로봇 손 자체의 민첩성 및 자연스러운 움직임 제약, 3D 프린팅 손가락 끝의 낮은 마찰 등이 있으며, 이는 향후 연구를 통해 개선될 수 있습니다. 또한, 현재는 단일 손으로 비교적 간단한 곡만 연주하지만, 향후 양손 연주(bimanual playing)로 확장하여 더 복잡한 곡에 도전하는 것을 목표로 합니다.

결론적으로, 이 논문은 Sim2Real 접근 방식과 Hybrid Execution 모드를 활용하여 현실 세계 로봇이 피아노를 학습하고 연주할 수 있음을 성공적으로 시연했습니다. 이는 정교하고 접촉이 많은 조작 작업에 대한 학습 기반 접근 방식의 잠재력을 보여주며, 피아노 연주가 이러한 연구를 위한 도전적이고 관련성 높은 벤치마크가 될 수 있음을 시사합니다.

# Detail Review

좋습니다. 머신러닝 연구자를 위한 수식 기반의 딥러닝 아키텍처 분석을 포함한 심층 기술 리뷰 블로그 글을 준비하겠습니다. 논문 \*"Learning to Play Piano in the Real World"\*의 핵심 기법, 시스템 구성 요소, 아키텍처 설계와 학습 전략 등을 자세히 다루고, 논문 내 그림은 그대로 활용할 수 있도록 지시하겠습니다. 준비되면 알려드릴게요.


# 실세계 로봇 피아노 연주 학습: *Learning to Play Piano in the Real World* 심층 리뷰

&#x20;*Figure 1.* **실세계 로봇 손의 피아노 연주** – 다수의 손가락을 가진 로봇 핸드(사진: Wonik’s Allegro Hand)가 실제 전자 키보드 건반을 누르는 모습. 본 논문은 오직 **시뮬레이션 학습만으로** 실제 로봇이 피아노를 연주하도록 만드는 최초의 시도이며, **Sim2Real** 기법을 통해 가상 환경에서 강화학습으로 훈련한 정책을 실세계 로봇에 이식하는 접근을 탐구한다.

## 소개 (배경 및 목표)

사람의 손으로 피아노를 연주하는 것은 고도의 **운동제어, 협응, 정밀성**이 요구되는 복잡한 과제다. 인간은 **촉각 및 청각 피드백**을 활용하여 많은 연습 끝에 이러한 능력을 습득한다. 반면 로봇에게 피아노 연주는 난제이며, 그동안 일부 연구들은 **규칙기반 제어기**를 설계하여 실제 로봇으로 피아노를 치게 했고, 다른 연구들은 **시뮬레이션 환경**에서 로봇 학습 접근을 평가해왔다. 이 논문의 목표는 **사람 수준의 섬세한 조작**을 향한 벤치마크로서 피아노 연주를 제시하고, **학습 기반 정책**을 실제 **다지(多指) 로봇 손**에 적용하여 **실세계에서 피아노 연주 학습**을 실증하는 것이다. 저자들은 이를 통해 향후 **사람처럼 자연스러운 로봇 손 동작**으로 복잡한 작업을 수행하는 방향의 연구를 촉진하고자 한다.

## 전체 시스템 구조 요약

이 연구의 전체 시스템은 **가상 시뮬레이션 환경**에서의 정책 학습과, 학습된 정책을 이용한 **실세계 로봇 제어** 두 단계로 구성된다. **그림 3**은 학습된 정책을 실환경에 적용하는 다양한 방식(실행 모드)을 개략적으로 보여준다. 실험에는 **Wonik’s Allegro Hand**라는 4개의 손가락을 가진 로봇 핸드와 7자유도 로봇 팔(UFactory xArm7)이 사용되었으며, **M-Audio Keystation 49e** MIDI 전자피아노(49개 건반)를 연주 대상으로 했다. 로봇 핸드의 손가락 끝은 키 하나만 누를 수 있도록 얇은 맞춤형 3D 프린트 재질로 교체되었다. 시뮬레이션 환경도 **MuJoCo** 물리엔진 기반으로 동일한 Allegro Hand 모델과 전자피아노 모델을 사용하여 구축되었고, 현실과의 차이를 줄이기 위해 실제 환경의 요소(예: 손가락 끝 형태, 키 배열)를 최대한 모사하였다. 특히 **수평 슬라이더 축**을 추가하여 손 전체가 피아노 건반을 따라 좌우로 이동할 수 있게 하였는데, 이는 사람의 팔꿈치-어깨 움직임에 해당하는 역할을 하여 음역을 바꿀 때 로봇 손 위치를 옮기는 것을 가능케 한다.

* **관찰 공간(Observation space)**: 에이전트가 인지하는 상태는 차원 수 356의 벡터로 구성된다. 여기에는 로봇 손의 **관절 각도 12개**(손가락 당 4개 관절 × 3개 손가락; 엄지는 사용 안 함)와 **슬라이더 위치 1개**로 이루어진 **프로프리오셉션**(자기몸 인지) 정보, 전자피아노로부터 읽은 **현재 눌린 건반들**(49차원 원-핫 벡터), 그리고 연주하려는 악보로부터 주어진 **현재 타겟 음표**(또는 코드)의 건반들(49차원 원-핫) 및 향후 필요할 건반들에 대한 정보 등이 포함된다. 참고로 상기 **MIDI 신호**는 건반이 일정 깊이 이상 눌리면 On(1), 떼면 Off(0)으로만 표시되므로, **부분적으로 누른 정도**는 관측할 수 없다. 또한 Allegro Hand는 인간의 손가락 5개보다 적은 4개 손가락만 있어서, 기존 시뮬레이터 (RoboPianist)에서 제공되던 **어떤 손가락으로 누를지(fingering)** 정보도 제외되었다. 이러한 제한으로 에이전트는 손가락이 건반 위에 살짝 올려져 있는 상태 등을 감지할 수 없으며, 이는 탐색을 어렵게 만들기에 **세심한 보상설계가 필수**적이다.

* **행동 공간(Action space)**: 에이전트의 행동은 **13차원 연속 실수 벡터** $a \in \mathbb{R}^{13}$로 표현되며, Allegro Hand의 12개 관절과 슬라이더 축 각각에 대한 **모터 제어 명령**을 포함한다. 시뮬레이션에서는 이 행동을 각 관절 모터의 목표위치/토크로 적용하고, 실세계에서는 ROS를 통해 해당 값을 로봇 팔과 손에 전송하여 동일한 동작을 유발한다. 행동 주기는 시뮬레이션에서 매 10 스텝마다 한 번씩 관측-결정하도록 설정되어, 실제 로봇 제어 주파수와 대응되는 약 **100 Hz 내외**의 정책 실행 빈도로 맞춰졌다 (예: 시뮬레이션 time-step이 0.002초이면 10스텝=0.02초 간격으로 행동 적용).

**정책(Policy)**은 위 관찰을 입력받아 행동을 출력하는 **신경망 컨트롤러**이다. 특별히 논문에서는 오프폴리시(off-policy) 강화학습 알고리즘인 **DroQ**(Dropout Q-functions 활용) 기반의 **소프트 액터-크리틱**(soft actor-critic, SAC) 변종을 사용해 정책을 학습시켰다. Actor-critic 계열에서 정책 신경망(Actor)은 확률적으로 행동을 생성하고, 두 개의 Q-함수 신경망(Critic)은 그 행동의 가치를 평가하며, DroQ는 Q함수에 드롭아웃을 적용하여 효율성을 높인 방법이다. **관측 인코더(Observation Encoder)**란 정책 신경망의 앞부분으로, 356차원 관측 벡터를 받아 여러 계층의 fully-connected 레이어를 거쳐 **잠재 표현**을 추출하는 부분을 의미한다. 반면 **행동 디코더(Action Decoder)**는 신경망의 출력부로서, 잠재 표현으로부터 13차원 연속 행동을 산출하는 역할을 한다. 논문에서는 각 구성요소에 대한 상세 구조(예: 레이어 수나 크기)를 명시하지 않았지만, 일반적으로 SAC에서는 MLP 구조의 인코더/디코더를 사용하고 Gaussian 정책 분포를 산출한다. 학습된 정책은 실행 시 매 시각 관측을 받아 인코더를 통과한 후 행동 디코더에서 각 관절/슬라이더에 대한 제어명령을 출력하게 된다.

한편, 본 논문에는 **키포인트 검출기(Keypoint Detector)**와 같은 별도의 시각 인지 모듈은 포함되어 있지 않다. 즉, 로봇이 **카메라나 외부 센서로 건반의 위치나 손가락 위치를 인식**하는 과정을 따로 두지 않았다. 대신, 환경의 기하학을 미리 알고 (시뮬레이터와 로봇 설정을 일치시켜) 슬라이더 위치와 관절각을 통해 **손가락 끝과 건반의 상대위치**를 내부적으로 계산할 수 있도록 하였고, **어떤 건반이 눌렸는지**는 전적으로 피아노의 MIDI 신호를 이용했다. 이러한 설계 덕분에, 시뮬레이션에서도 로봇 손가락과 건반 사이의 충돌로 눌림 여부를 바로 알 수 있어 굳이 영상 처리로 건반 특징점을 추적하지 않아도 된다. 요약하면, **정책 신경망**은 시각적인 입력 대신 **엔지니어드된 상태 표현**(관절각+건반 상태+타겟 음 정보)을 받아 동작을 결정하며, 이는 **키포인트 검출에 의존하지 않는 경량한 접근**이라 볼 수 있다.

## 핵심 구성 요소의 딥러닝 아키텍처 분석

**관측 인코더(Observation Encoder)**와 **행동 디코더(Action Decoder)**로 구성된 정책 신경망은 강화학습 프레임워크의 중심이다. 관측 인코더는 높은 차원의 상태를 효율적으로 표현하기 위해 설계되었다. 예를 들어 건반의 현재 상태와 목표음 정보는 49차원의 원-핫 벡터로 각각 주어지는데, 이러한 **이산 특성의 입력**은 신경망에서 연속값으로 임베딩되어 처리된다. 관측 인코더는 이러한 다양한 정보를 결합하여 **로봇이 현재 무엇을 하고 있으며, 무엇을 해야 하는지**에 대한 함축적 표현을 학습한다. 특히 목표로 하는 건반(one-hot 형태)은 조건(condition)으로서 정책에 주어지므로, **정책이 곡의 진행에 따라 다른 행동을 취하도록** 한다. 이는 마치 조건부 정책 $\pi(a|s, g)$에서 $g$가 목표(goal, 여기서는 다음 눌러야 할 건반)인 셈이며, 단일 정책이 여러 곡에 대응할 수 있는 일반화를 가능하게 한다는 점에서 유용한 아키텍처적 선택이다. (다만 본 연구에서는 실제로 곡마다 정책을 따로 훈련했는데, 이는 후술할 대로 곡들 간 난이도 차이와 탐색 안정성을 위한 것으로 보인다.)

행동 디코더는 인코더의 출력(잠재 특징)을 받아 최종적인 로봇 제어 명령을 생성한다. SAC 기반 정책인 만큼, 행동 디코더는 각 행동 차원마다 **확률분포의 모수**(예: 평균 $\mu\_i$와 분산 $\sigma\_i$)를 출력하고, 샘플링을 통해 실제 행동 벡터 $a$를 얻는다. 이때 출력 범위는 각 관절의 허용 각도/토크 등에 맞게 **적절한 스케일로 정규화**되도록 설계된다. 정책 신경망은 훈련 시에는 이 확률적 특성을 활용해 탐색을 하고, 실행 시에는 통상 평균값을 취하거나 노이즈를 줄인 샘플링으로 결정론적 행동처럼 사용할 수 있다.

**키포인트 검출(Keypoint Detection)** 측면에서, 이 연구는 관측 공간을 구성하는 단계에서 **건반의 키 좌표나 손가락-건반 거리** 등의 정보를 명시적으로 포함시키지 않았다. 그러나 시뮬레이터 안에서는 손바닥과 목표 건반들 사이의 거리값을 계산하여 보상의 일부로 사용하고 있어 (자세한 내용은 다음 섹션), 간접적으로 **환경이 목표 키포인트까지의 거리**를 인지하고 있다. 실세계에서는 로봇이 자체적으로 거리를 계산하지는 않지만, 시뮬레이터와 동일한 초기 정렬을 통해 이 부분을 상쇄했다. 만약 시각 정보를 활용했다면, 카메라 영상을 통해 건반, 손가락 끝 등의 키포인트를 추적하고 정책에 입력으로 줄 수도 있었겠지만, 본 논문은 **문제의 복잡도를 줄이기 위해 시각정보를 배제**하고 잘 정형화된 상태만 이용했다. 이는 학습해야 할 인공신경망의 규모를 줄이고, Sim2Real 차이를 줄이는 데도 유리한 결정이다 (카메라 영상은 시뮬레이터와 현실 간 차이가 크기 때문). 그 결과, **관측 인코더**는 비교적 **저차원 상태**를 다루므로 MLP 구조만으로 충분했고, **키포인트 검출기**에 해당하는 역할은 **환경 설계와 센서 활용**(MIDI 신호 및 알려진 기하)으로 대체되었다.

## 주요 수식 및 학습 방식

**마르코프 결정 프로세스(MDP)** 설정: 로봇 피아노 연주 문제는 상태, 행동, 보상으로 정의되는 MDP로 모델링된다. 시뮬레이션에서의 **상태 $s$**는 물리 엔진이 추적하는 로봇 손가락 관절, 건반 위치 등의 완전한 내부 상태이고, **관측 $o$**는 앞서 설명한 356차원의 부분관측 (partial observation)이다. 행동 공간 $\mathcal{A}$는 연속 공간이며 차원 13이다. 매 시뮬레이션 스텝에서 환경은 (상태 $s$, 행동 $a$) 쌍에 대해 다음 상태를 확률적으로 생성하고 (여기서는 결정론적 물리 시뮬레이션이지만 도메인 랜덤화로 인한 불확실성은 약간 존재), 설계된 보상 $r(s,a)$을 계산한다. 비할음(無割引) 에피소드의 누적보상을 최대화하는 정책 $\pi\_\theta(a|o)$를 학습하는 것이 목표이다.

**보상 함수(Reward)**는 효과적인 학습을 위해 **네 가지 구성요소**로 설계되었다. 총 보상 $R$은 아래와 같은 네 부분의 합으로 볼 수 있다 (상수 가중치는 편의상 생략):

1. **에너지/속도 페널티** $R\_{\text{energy}}$: 불필요하게 **빠르고 힘이 많이 드는 움직임**을 억제하기 위한 항목이다. 손가락 관절 토크 $\tau$와 속도 $\dot{q}$를 이용해 $-\sum\_i |\tau\_i \cdot \dot{q}\_i|$와 같은 형태로 계산하며, 이는 **진동하는 행동**도 줄여 로봇 동작을 **안정적이고 유연하게** 만든다.

2. **목표 접근 보상** $R\_{\text{distance}}$: 손이 목표 건반에 가까이 가도록 유도하는 항목이다. 손바닥 위치 $p\_{\text{hand}}$와 목표로 해야 할 건반들의 위치 집합 $G$ 사이의 평균 거리에 대해 **거리가 짧을수록 큰 보상**이 주어진다. 구현상 dm\_control 라이브러리의 tolerance 함수를 써서 거리를 0\~1 범위로 **정규화된 보상**으로 변환했다.

3. **정타(key press) 보상** $R\_{\text{press}}$: **올바른 건반을 누르는 것**을 가장 크게 장려하는 핵심 보상이다. 이 보상은 두 가지 설계 요구사항을 충족하도록 만들어졌다: **(i)** “아무 것도 누르지 않는 것” < “잘못된 건반이라도 누르는 것” (<는 보상이 더 낮음을 의미) 그리고 **(ii)** “잘못된 건반을 누르는 것” < “정확한 건반을 누르는 것”. 이를 위해 현재 눌러야 할 건반 수, 실제 누른 건반 중 맞는 건반 수 $n\_{\text{correct}}$, 틀린 건반 수 $n\_{\text{wrong}}$ 등을 고려하여 경우를 나누어 보상을 계산했다. 예를 들어 목표 건반이 있는데 하나도 누르지 않았으면 보상이 0이고, 틀린 건반이라도 누르면 소량의 양의 보상을 주며, 맞는 건반을 누르면 더 큰 보상을 주는 식이다. 이러한 설계로 **아무것도 안 하고 있는 상태**를 탈피하여 적극적으로 건반을 탐색하도록 유도하면서도, 최종적으로 **정타를 누르는 방향으로 수렴**하게 된다. 실제로 요구사항 (ii)는 수식으로 $R\_{\text{correct}} - R\_{\text{wrong}} > 0$ 형태로 구현되어 항상 만족되도록 하였다.

4. **슬라이딩 페널티** $R\_{\text{slide}}$: 건반을 누른 상태로 손을 옆으로 **슬라이딩하는 행동을 억제**한다. 로봇이 한 손가락으로 건반을 누른 채 손 전체를 옆으로 미끄러뜨리면, 원래 목표한 건반이 아닌 인접 건반을 눌러버리는 편법을 쓸 수 있는데, 이는 인간의 자연스러운 연주 동작과 동떨어져 있다. 따라서 어떤 건반이 눌린 상태에서 슬라이더가 움직이면 페널티를 주고, 만약 그로 인해 **인접한 두 건반이 동시에 눌리게 되면 페널티를 두 배**로 주어 이러한 현상을 최소화했다.

이상의 보상 설계로 에이전트는 **부드럽고 에너지 효율적인 움직임**으로 손을 목표 건반 쪽에 가져가 최대한 정확히 건반을 누르려고 하며, 비현실적인 슬라이딩 꼼수를 피하게 된다. 또한 평가 단계에서 에이전트의 **성공 여부**는 Information Retrieval 분야의 관점에서 **정밀도(Precision)**와 **재현율(Recall)**로 정의하였다. 매 시간 스텝마다 “눌러야 할 건반(target) 대비 누른 정도”와 “누르면 안 될 건반을 누른 정도”를 따져 Precision $P$과 Recall $R$을 계산하고, 그 **조화평균**인 $F1$ 점수를 최종 성능 지표로 사용하였다. (Precision은 잘못 누르지 않는 정도, Recall은 제대로 누르는 정도를 의미하며 $F1 = \frac{2PR}{P+R}$이다.)

**강화학습 알고리즘**: 학습은 시뮬레이션 상에서 진행되며, 앞서 언급한 **DroQ 알고리즘** (SAC 기반)을 사용했다. 이는 리플레이 버퍼를 활용하는 off-policy 방법으로, 비교적 **데이터 효율성이 높고 안정적인 학습**이 가능하다. 특히 **드롭아웃 Q-함수**를 활용하여 Q함수의 과적합과 **과도한 탐색 억제** 문제를 완화하는 기법을 포함하고 있어, 제한된 자원으로 훈련할 때 유리하다. 논문에 따르면 각 모델은 약 **수백만 스텝**에 해당하는 에피소드를 학습했고, 엔비디아 RTX 4070 GPU 및 32코어 CPU가 장착된 랩톱에서 모델 하나를 몇 시간 내로 훈련할 수 있었다고 한다 (정확한 에피소드 횟수나 시간은 명시돼 있지 않으나, 비교적 짧은 시간에 학습 가능함을 시사).

**도메인 랜덤화(Domain Randomization)**: Sim2Real의 핵심 기법 중 하나로, 시뮬레이션에서 학습할 때 환경의 물리 파라미터들을 무작위로 변화시켜 다양한 **랜덤 드롭아웃**을 경험하도록 하는 것이다. 이 논문에서도 시뮬레이션 훈련 시 **현실과 관련된 여러 물리 변수**를 에피소드마다 랜덤으로 변동시켰다. 랜덤화된 주요 파라미터로는 **피아노 높이**, **초기 손 위치**, **관절 감쇠 계수**, **관절 강성 계수**, **건반이 눌렸다고 간주되는 임계 깊이**, **건반 스프링 강성**, **손가락 끝과 건반 사이의 마찰계수** 등이 있다. 이러한 변수들을 폭넓게 바꿈으로써, 에이전트는 **모델 불확실성**이나 환경 변화를 견디는 강인한 정책을 학습하게 된다. 논문에서는 **랜덤화 강도**를 $0 \le \beta \le 1$의 매개변수로 두고, $\beta=0$이면 랜덤화를 꺼서 이상적인 정확한 시뮬레이션으로 학습하고 $\beta=1$이면 최대 범위로 변화시켜 학습하도록 실험을 설계하였다.

정리하면, 학습 단계에서는 잘 설계된 관측/행동 표현과 보상 함수, 그리고 SAC 기반 알고리즘과 도메인 랜덤화를 통해 **시뮬레이션에서 현실성 있는 피아노 연주 정책**을 얻어낸다. 다음으로는 이렇게 얻어진 정책을 **실제 로봇에 적용**하는 방법과, 실험 결과에 대한 분석을 살펴본다.

## 실제 환경에서의 학습 적용을 가능하게 한 핵심 기술 요소

시뮬레이터에서 성공적으로 학습한 정책이라도 바로 실제 로봇에 옮기면 여러 문제가 발생할 수 있다. **현실 세계의 오차와 불확실성**(모델링 오차, 센서 잡음, 지연 등)으로 인해 성능이 크게 저하되는 **Sim2Real 갭** 문제가 대표적이다. 본 연구에서는 이를 극복하기 위해 **(i)** 앞서 언급한 **도메인 랜덤화**를 활용해 정책의 **강인성**을 높였고, **(ii)** 현실에서 정책을 실행하는 방식을 세분화하여 고안하였다. 특히 후자와 관련하여 **세 가지 실행 모드(execution mode)**를 제시하고 비교하였는데, 이는 시뮬레이터와 실제 로봇 간 **관측 및 제어신호의 연결방식**을 다르게 설계한 것이다.

&#x20;*Figure 2.* **학습된 정책의 실행 모드 비교** – A: **관절 미러링 (Joint Mirroring)**, B: **하이브리드 실행 (Hybrid Execution)**, C: **실세계 실행 (Real World Execution)** 방법을 도식화. 노란색 박스는 관측(**Observations**)이 어디서 오는지, 행동(**Actions**)이 어디에 적용되는지를 보여준다. 음표 아이콘은 목표 악보(현재 눌러야 할 건반 정보)를 의미한다.

* **A) 관절 미러링 (Joint Mirroring)**: 정책은 **오로지 시뮬레이터로부터의 관측만** 받으며, 현실 세계로부터의 피드백은 전혀 사용하지 않는다. 학습 때와 동일한 방식으로 시뮬레이터 내에서 정책이 움직임을 결정하면, **그 관절 위치를 그대로 실제 로봇에 미러링**하여 따라하게 한다. 이렇게 하면 시뮬레이션에서의 동작 궤적을 현실에서 재현할 수 있지만, **현실에서 발생하는 오차** (예: 건반이 제대로 안 눌렸거나 손 위치가 약간 어긋났을 때)를 정책이 전혀 감지하지 못한다. 따라서 실행 도중에 잘못되어도 수정이 불가능하고, **폐루프 제어**가 아닌 **개루프 (open-loop)** 실행이 된다.

* **B) 하이브리드 실행 (Hybrid Execution)**: 시뮬레이터와 현실의 정보를 **조합하여 관측**을 구성하는 절충안이다. 구체적으로, 관측 중 **건반 눌림 상태**는 실제 전자피아노(MIDI)로부터 받아오고, 나머지 **관절 위치 등 상태**는 시뮬레이터의 값을 사용한다. 실행 과정에서는 매 타임스텝마다 정책이 **시뮬레이터 상의 로봇 상태**(관절 각도 등)와 **실제 건반 입력**을 보고 행동을 내면, **그 행동을 시뮬레이터와 실제 로봇에 동시에 적용**한다. 즉, **시뮬레이터는 현실을 모사하며 계속 정책과 상호작용**하고, 실제 로봇은 시뮬레이터의 로봇을 거울처럼 따라간다. 이렇게 하면 정책은 여전히 시뮬레이션으로부터 익숙한 형태의 관찰(관절 상태)을 얻으면서도, 건반이 실제로 눌렸는지 안 눌렸는지는 **현실 데이터를 통해 반영**하게 된다. 그 결과, 만약 실제로 건반을 놓쳤다면 관측에 반영되어 정책이 이를 인지하고 **교정 행동**을 취할 수 있다. 동시에, 관절 움직임 자체는 시뮬레이터 기준이라 **학습 당시와 동일한 분포**를 유지하므로, 정책이 과도한 현실적 불확실성에 노출되지 않는다. 하이브리드 모드는 일종의 **부분 폐루프 제어**라고 볼 수 있다.

* **C) 실세계 실행 (Real World Execution)**: 정책이 **모든 관측을 실제 로봇/피아노로부터** 얻고, **행동도 직접 실 로봇에 적용**하는 가장 직관적인 방식이다. 이 경우 시뮬레이터는 전혀 사용되지 않는다. 정책은 학습 시에 보지 못했던 현실 관측(예: 미세한 잡음이 섞인 관절값 등)에 그대로 노출되므로 **Sim2Real 갭을 정책 스스로 극복**해야 한다. 따라서 이 모드에서는 시뮬레이터의 도움 없이도 잘 동작하려면, 사전에 정책이 충분히 **강인하게 학습**되어 있어야 한다.

세 모드 중 어떤 것이 성능에 유리할지는 사전에 명확하지 않았기 때문에, 저자들은 모두 실험해보았다. 한 가지 흥미로운 점은, **Joint Mirroring**과 **Hybrid** 모드는 정책 학습 시 **도메인 랜덤화가 없어도** 현실 적용이 가능했다는 것이다. Joint Mirroring의 경우 어차피 현실 피드백을 무시하니 랜덤화가 큰 의미가 없고, Hybrid 모드도 관찰 대부분이 시뮬레이터 기반이라 비교적 안전했기 때문이다. 반면 **Real World Execution** 모드는 온전히 현실에 의존하므로, **학습 시 적절한 강도의 랜덤화**를 사용하지 않으면 실행이 매우 어려웠다. 실제 저자들은 Real World 모드로 정책을 이식할 때 **중간 정도 강도(moderate intensity)**의 랜덤화를 사용해 학습한 정책을 선택했고, 그렇지 않은 경우 실패함을 보고했다.

**도메인 랜덤화의 효과 분석**: 다양한 $\beta$ 값에 대해 훈련한 정책을 Hybrid/Real 모드로 실행한 결과, **너무 작거나 너무 큰 랜덤화 강도는 모두 성능을 저해**하고, **적절한 중간값에서 최고의 성능**이 나타났다. $\beta=0$ (랜덤화 없음)인 경우 시뮬레이션에서는 높은 성능을 보이나 현실에서는 작은 교란에도 복구를 못해 성능이 떨어졌다. 반대로 $\beta=1.0$에 가까운 과도한 랜덤화는 애초에 시뮬레이터에서 학습 난이도를 크게 높여버리기 때문에, 충분한 학습시간을 주지 않으면 오히려 성능이 낮아졌다. 결국 현실 성능 관점에서 $\beta \approx 0.5$ 주변에서 F1 스코어가 최고였으며, 이는 **시뮬레이션과 현실 간 갭을 메우면서도 학습 난이도를 너무 높이지 않는 균형점**임을 의미한다. 이러한 결과는 **Sim2Real을 위한 랜덤화 강도의 선택이 중요**함을 보여주며, 추후 적응적 랜덤화 조절이나 발전된 도메인 불확실성 모델링 등의 여지를 남긴다.

요약하면, **하드웨어적인 개선**(얇은 손가락 팁 장착 등)과 **시뮬레이션 환경 구성**, **도메인 랜덤화** 및 **실행 모드 설계**라는 다양한 기술 요소가 결합되어, 순전히 가상훈련만으로 실제 로봇이 피아노를 연주할 수 있게 되었다. 특히 **하이브리드 실행 모드**는 현실 적용 시 **안정성**과 **적응성**을 동시에 확보하게 해준 중요한 기법으로 밝혀졌다.

## 실험 결과 요약 및 한계점 평가

논문에서는 앞서 소개한 시스템을 이용해 **네 가지 곡목**에 대해 로봇의 연주 실험을 수행했다: **작은 별(Twinkle Twinkle Little Star)** 멜로디, **C 장조 음계**, **D 장조 음계**, 그리고 **코드 진행(chord progression)**. 각 곡마다 강화학습으로 별도의 정책을 훈련했고 (다른 곡으로의 전이는 고려하지 않음), 동일 곡에 대해 두 가지 시드로 학습한 모델을 3번씩 실행하여 성능을 측정했다. 주 성능 지표인 F1 스코어는 **시뮬레이션에서의 성능**과 **실제 로봇으로 연주한 성능**을 모두 구하여 비교했다.

**전반적인 성공률**: 네 곡 모두 로봇이 **끝까지 연주하는 데 성공**했으며, 이는 본 접근법의 **일반화 가능성**을 보여준다. 다만 **모든 경우에서 Sim2Real 갭이 뚜렷**하게 나타나, 현실의 F1 점수가 시뮬레이터에서보다 낮았다. 그래도 F1 값 자체는 대체로 0.5\~0.9 범위로 양호한 편이어서, **곡의 상당 부분을 정확히 연주**했음을 의미한다 (F1=1이면 완벽, 0이면 전혀 못 함).

**Precision vs Recall 경향**: 흥미롭게도 실제 연주에서 **정밀도(Precision)**가 일관되게 **재현율(Recall)**보다 높게 측정되었다. 즉, **틀린 건반을 누르지 않는 능력**은 비교적 좋은데 **정확한 건반을 모두 놓치지 않고 누르는 능력**은 상대적으로 떨어졌다. 저자들은 이를 두 가지 이유로 분석했다: (i) 로봇이 **틀린 건반을 눌렀을 때 즉시 손가락을 떼는 것은 비교적 쉽지만**, **놓친 건반을 다시 누르러 가는 것은 여러 동작(손가락 들기 -> 옆으로 이동 -> 누르기)이 연속되어 더 어렵고 시간 소모적**이라서, 빠른 곡 진행 중에는 놓친 키를 건너뛰게 될 가능성이 있다. (ii) 애초에 **Precision을 높이려면** 필요한 모든 올바른 키는 누르면서 동시에 **어떤 잘못된 키도 누르면 안 되는** 까다로운 조건이지만, **Recall을 높이는 가장 쉬운 방법은** 아예 **아무 키도 누르지 않는 것**이기 때문에 (물론 그러면 곡을 못하지만 이론적으로는 Recall=0% 달성 가능), 학습 시 Precision을 끝까지 높이는 것이 Recall을 높이는 것보다 어려운 경향이 있다. 결과적으로 로봇은 곡을 연주하다가 일부 키(특히 빠르게 바뀌는 부분)를 놓치는 경우가 있었지만, 웬만하면 틀린 키는 거의 누르지 않으려는 보수적인 전략을 취한 것으로 보인다.

**곡 난이도별 성능**: **Twinkle**과 **음계 연습곡**들은 단선율(한 번에 한 음)이라 비교적 높은 F1을 보였다. **D 장조 음계**의 경우 검은 건반(#)이 포함되어 있어 손가락 교체(fingering)가 필요한데, 로봇 손은 4손가락으로 엄지없이 연주해야 하므로 약간 어려움을 겪었다. **코드 진행 곡**은 한 번에 여러 개의 건반(화음)을 눌러야 해서 시뮬레이션상 학습 난이도가 가장 높았고, 시뮬레이터에서 얻은 F1도 가장 낮았다. 이유는 **여러 손가락을 동시에 제어**해야 하므로 정책 신경망에 더 큰 용량(capacity)이 필요할 수 있기 때문이다. 그러나 아이러니하게도 실제 로봇으로 실행한 결과는 코드 진행이 **다른 곡들에 비해 오히려 안정적**이었다. 저자들은 화음을 연주할 때 **각 코드당 누르고 있는 시간이 길어서** 로봇이 그 사이 오류를 교정할 여유가 생기기 때문으로 설명했다. 즉, 한 음 한 음이 짧게 진행되는 멜로디보다, 코드처럼 길게 눌러주는 음은 Sim2Real 갭이 상대적으로 작았다.

**실행 모드 비교**: 세 가지 실행 모드(A, B, C)에 대한 성능 비교에서는 **하이브리드 모드(B)**가 **전반적으로 가장 우수**한 성과를 보였다. Hybrid 모드는 **실시간 교정 능력**과 **시뮬레이터 기반 안정성**을 모두 갖추었기 때문에, 곡 전반에서 높은 F1과 안정적인 재생을 달성했다. 반면 **Joint Mirroring(A)**은 잘못 눌린 키를 끝까지 인지하지 못해 Recall이 크게 저하되었고, **Real-world 모드(C)**는 일부 곡에서 Hybrid에 비해 성능이 떨어졌는데, 이는 여전히 남은 Sim2Real 갭 영향으로 보인다. 전반적으로 “시뮬레이터 관절로 학습한 **버릇**”을 유지하면서도 실제 결과에 반응할 수 있는 hybrid 접근이 가장 실용적임을 확인한 셈이다.

**한계점 및 향후 과제**: 본 연구는 중요한 첫걸음을 뗐지만, 여러 **한계**도 존재한다. 우선, **Sim2Real 갭을 완전히 제거하지는 못했다**. F1 점수가 100%에 도달한 곡은 없으며, 특히 빠른 연주나 복잡한 곡으로 갈수록 놓치는 음이 발생할 수 있다. 이는 부분적으로는 **관찰의 제한(부분관찰)** 때문이기도 하다 – 로봇은 건반을 누르기 전까지는 손가락과 건반의 접촉 여부를 알 수 없고, 눌림 여부도 이진값으로만 알 수 있어 **미세한 조정**이 어렵다. 추후 **고해상도 촉각 센서**(예: DIGIT 센서 등)나 **비전 피드백**을 활용하면 이 문제를 완화할 수 있을 것이다. 둘째, **로봇 손 자체의 한계**로 사람같은 손가락 교체 기법을 쓰지 못해 일부 음계나 복잡한 패시지에서 비효율적인 동작을 하거나, 물리적으로 도달하기 어려운 음 조합이 있다. 향후 더 많은 자유도를 가진 손(5손가락)이나 피아노 전용 기계손으로 확장할 필요가 있다. 셋째, **정책의 일반화** 범위가 아직 제한적이다. 본 논문에서는 곡마다 개별 정책을 훈련했지만, 이상적으로는 **하나의 정책이 여러 곡을 조건부로 연주**하거나, 온라인으로 새로운 악보를 바로 연주할 수 있도록 일반화를 높이는 방향으로 나아가야 한다. 넷째, **음악적 표현력**은 고려되지 않았다. 사람이 연주할 때는 세게치기/여리게치기(건반 누르는 속도에 따른 음 세기)나 페달 사용 등이 중요한데, 사용된 키보드는 **건반 눌림 깊이에 따른 MIDI 벨로시티는 제공하지 않아서** 로봇은 모든 음을 동일한 세기로 냈다. 추후 MIDI velocity 정보나 실제 피아노의 음향 피드백을 보상에 포함하여 **음질 측면의 보상**(예: 목표 음향과의 유사도)까지 최적화하면 더욱 인간다운 연주가 가능할 것으로 기대된다. 마지막으로, **학습 효율과 안정성**도 향후 개선 여지다. DroQ로 비교적 적은 데이터로도 학습을 했지만, 여전히 시뮬레이터에서 수백만 스텝을 돌아야 했다. 더 효율적인 알고리즘이나 모방학습과의 병행, 인간 시연 데이터 활용 등으로 훈련 시간을 단축시킬 수 있을 것이다.

## 결론

*Learning to Play Piano in the Real World* 논문은 **강화학습을 통해 실제 로봇 핸드로 피아노 연주를 구현한 최초의 사례**로서 큰 의미를 지닌다. 비록 단순한 곡들로 시작했지만, **Sim2Real** 접근을 한층 발전시켜 **복잡한 휴머노이드 조작** 문제에 도전한 점, 그리고 **Hybrid 실행 모드** 등의 창의적인 아이디어로 현실 적용을 가능케 한 점이 돋보인다. 이 연구를 통해 **피아노 연주**가 로봇 조작 연구의 새로운 **표준 벤치마크**가 될 가능성이 제시되었고, 향후 인간 수준의 섬세한 연주에 도전하기 위한 기반이 마련되었다. 앞으로 더 많은 로봇 연구자들이 이 방향으로 연구를 확장시켜, **예술과 로봇공학의 만남**이라는 흥미로운 영역에서 지속적인 발전이 이루어지길 기대한다.

**참고 문헌** (논문 번호는 원문과 동일하게 유지):

1. Handa et al. (2024), DeXtreme: Transfer of Agile In-hand Manipulation from Simulation to Reality.
2. Hiraoka et al. (2022), **DroQ**: Dropout Q-Functions for Doubly Efficient Reinforcement Learning.
