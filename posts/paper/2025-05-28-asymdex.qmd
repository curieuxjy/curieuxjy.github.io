---
draft: true
title: "📃AsymDex 리뷰"
description: Asymmetry and Relative Coordinates for RL-based Bimanual Dexterity
date: "2025-05-28"
categories: [paper, hand]
toc: true
number-sections: true
---

1. ✨ AsymDex는 인간 양손 사용의 비대칭성에서 영감을 받아 양손 로봇 조작 기술을 효율적으로 학습하는 새로운 강화 학습 프레임워크입니다.
2. 🤖 이 프레임워크는 손에 보조적 역할과 주요 조작 역할을 할당하고, 손과 물체 간의 상대적 관측 및 행동 공간을 활용하여 차원 축소와 샘플 효율성을 달성합니다.
3. 👍 시뮬레이션 및 실제 실험에서 AsymDex는 다양한 양손 미세 조작 작업에서 기존 베이스라인보다 뛰어난 성능과 학습 효율을 보이며 성공적인 Sim2Real 전환을 입증했습니다.

---

# Brief Review

논문 "AsymDex: Asymmetry and Relative Coordinates for RL-based Bimanual Dexterity"는 멀티 핑거 핸드를 사용하는 이중 로봇 팔(bimanual) 조작(manipulation) 기술을 순수 강화 학습(Reinforcement Learning, RL)으로 효율적으로 학습하는 새로운 프레임워크인 Asymmetric Dexterity (AsymDex)를 제안합니다. 기존 RL 기반 이중 팔 조작 방법들은 샘플 효율성(sample efficiency)이 낮거나 특정 태스크에 한정되는 반면, AsymDex는 시연(demonstration) 없이 다양한 비대칭 이중 팔 능숙 조작(dexterous manipulation) 태스크를 학습할 수 있습니다.

AsymDex는 두 가지 핵심 아이디어에 기반합니다:

1.  **Asymmetry (비대칭성):** 인간이 대부분의 이중 팔 태스크에서 '손잡이(handedness)'를 보이는 것에서 영감을 받아, 각 손에 보완적인 역할을 부여합니다. Facilitating hand는 물체 하나를 잡고 재배치(reposition)하거나 방향을 재조정(reorient)하는 역할을 하고, Dominant hand는 복잡한 조작을 수행하여 원하는 결과를 달성합니다. 이러한 역할 분담은 관찰 공간(observation space)과 행동 공간(action space)의 차원을 줄이고 샘플 효율성을 향상시킵니다. Facilitating hand는 물체와 상대적인 움직임 없이 베이스(base)의 6DoF 포즈(pose)만 제어하도록 제한되며, dominant hand는 모든 DoF를 사용합니다.
2.  **Relative Coordinates (상대 좌표):** 양손 간의 협응(coordination)과 동기화(synchronization)에 중요한 상대적인 움직임을 제어하기 위해, 상대적인 관찰 및 행동 공간을 설계하고 상대 포즈 추적 컨트롤러(relative-pose tracking controller)를 활용합니다. 관찰 공간 $z_{\text{AsymDex}}$는 dominant hand의 조인트 상태 $\xi_h^d(t)$, facilitating hand가 잡고 있는 물체 기준 dominant hand 베이스의 6D 상대 포즈 $\xi_b^r(t)$, dominant hand가 조작하는 물체의 6D 상대 포즈 $\xi_{\text{obj}}^r(t)$로 구성됩니다. 행동 공간 $a_{\text{AsymDex}}(t) = (\hat{\xi}_b^r(t), \hat{\xi}_h^d(t))$는 목표 상대 베이스 포즈 $\hat{\xi}_b^r(t)$와 목표 dominant hand 조인트 설정 $\hat{\xi}_h^d(t)$으로 구성됩니다. 이는 절대 좌표를 사용하는 일반적인 대칭(symmetric) 정책 $\pi_{\text{sym}}$보다 상태 및 행동 공간 차원을 크게 줄입니다 (Table 4, 5 참조).

정책이 출력하는 목표 상대 포즈 $\hat{\xi}_b^r(t)$를 절대 좌표계상의 목표 베이스 포즈로 변환하기 위해 다음과 같은 상대 포즈 추적 컨트롤러를 사용합니다:
$\hat{\xi}_b^d(t) = \alpha R_{\text{ofworld}} \cdot \text{dist}(\hat{\xi}_b^r(t), \xi_b^r(t)) + \xi_b^d(t)$
$\hat{\xi}_b^f(t) = (\alpha - 1) R_{\text{ofworld}} \cdot \text{dist}(\hat{\xi}_b^r(t), \xi_b^r(t)) + \xi_b^f(t)$
여기서 $R_{\text{ofworld}}$는 facilitating hand가 잡고 있는 물체의 좌표계에서 월드 좌표계로의 회전 변환이고, $\text{dist}(\cdot, \cdot)$는 두 6D 포즈 간의 차이를 나타냅니다. $\alpha$는 각 손의 움직임 기여도를 조절하는 하이퍼파라미터입니다 (Alg. 1 참조).

실제 이중 팔 조작은 물체 획득 단계(acquisition phase)와 상호작용 단계(interaction phase)로 구성됩니다. AsymDex는 주로 상호작용 단계에 초점을 맞추지만, PDGM [50]과 같은 기존의 학습된 파지(grasping) 정책과 통합하여 획득 단계를 처리할 수 있음을 보여줍니다 (Fig. 3 참조). 이는 각 손이 독립적으로 물체를 파지한 후, AsymDex 정책이 제어를 넘겨받아 이중 팔 상호작용을 수행하는 방식입니다 (Appendix B 참조).

정책 학습에는 PPO [49] 알고리즘을 사용하며, 정책 및 가치 함수는 MLP로 모델링됩니다 (Appendix A 참조).

AsymDex의 효과는 4가지 시뮬레이션 태스크(Block in cup, Stack, Bottle cap, Switch)와 3가지 실제 태스크(Block in cup, Pour, Twist Lid)에서 평가되었습니다. 시뮬레이션 태스크는 BiDexHand [14]의 변형이며, 지지 표면 없이 양손 협응을 더 강하게 요구하도록 수정되었습니다 (Appendix C 참조). 실제 태스크는 Kinova Gen3 팔과 Allegro, Ability 핸드로 구성된 하드웨어에서 수행되었습니다. 물체 및 핸드 포즈 추정에는 Realsense 카메라와 AprilTag 또는 말단 장치(end-effector) 포즈 추정을 사용했습니다 (Appendix D 참조).

상호작용 단계 평가에서 AsymDex는 대칭 정책(Sym), 비대칭만 적용된 정책(Asym-w/o-rel), 상대 공간만 적용된 정책(Rel-w/o-asym) 등 강력한 베이스라인들을 모든 태스크에서 성공률 및 샘플 효율성 면에서 일관되게 능가했습니다 (Fig. 4 (a), Table 1 참조). AsymDex의 핵심 설계 요소인 비대칭성과 상대 공간 모두 성능에 필수적임을 확인했으며, 상대 공간이 개별적으로 더 큰 성능 향상을 가져오는 경향이 있었습니다. 획득 단계를 포함한 태스크 평가에서도 AsymDex(두 단계 분해 활용)는 단일 단계 정책(Monolithic) 및 대칭 두 단계 정책(2-stage-sym)보다 현저히 우수한 성능을 보였습니다 (Table 2 참조). 이는 단계 분해 자체도 유익하다는 것을 시사합니다.

실제 환경에서 AsymDex 정책은 도메인 무작위화(Domain Randomization)를 통해 학습된 시뮬레이션 정책의 제로샷 Sim2Real 이전(zero-shot Sim2Real transfer)에 성공하며 높은 성공률을 달성했습니다 (Table 3, Appendix D 참조). AsymDex는 기존 연구들과 달리 양손 베이스를 모두 재배치하고 방향을 재조정할 수 있어 더욱 유연한 이중 팔 협응이 가능함을 보여주었습니다.

결론적으로, AsymDex는 비대칭 역할 부여와 상대 관찰/행동 공간 사용이라는 핵심 요소를 통해 시연 데이터 없이도 복잡한 비대칭 이중 팔 능숙 조작 기술을 효율적으로 학습할 수 있음을 시뮬레이션 및 실제 환경 실험을 통해 입증했습니다.

향후 연구 방향으로는 양손 모두 복잡한 멀티 핑거 조작이 필요한 태스크(예: 동적 핸드오버)로 확장, 매니퓰레이터 팔의 동역학적 제약 고려, 그리고 더 자연스럽고 인간과 유사한 동작 생성을 위한 인센티브 설계 등이 있습니다.

---

# Detail Review

좋습니다. "AsymDex: Asymmetry and Relative Coordinates for RL-based Bimanual Dexterity" 논문을 심층 분석하고, 특히 제안된 방법론에 초점을 맞춘 기술 블로그 글을 준비하겠습니다. 대상 독자는 강화학습에 익숙한 개발자와 로봇공학자로 설정하겠습니다.

분석이 완료되면 블로그 형식의 초안을 공유드릴게요.


# AsymDex: 비대칭성과 상대 좌표계로 배우는 양손 로봇 조작

## 양손 조작의 난제와 기존 접근법의 한계

로봇의 **양손 조작(bimanual manipulation)**은 한 손만 사용하는 조작보다 훨씬 고차원적인 문제로, 두 손의 **상태 공간**과 **행동 공간**이 기하급수적으로 늘어나고 정밀한 **동작 동기화**까지 요구되기에 강화학습으로 배우기 매우 어렵습니다. 사람 손 모양의 **다지 로봇 손**(multi-fingered hand) 하나로도 섬세한 조작 기술을 학습하려면 엄청난 탐색과 학습 시간이 드는 것으로 알려져 있는데, 두 손을 동시에 학습시키면 관절 수와 행동 차원이 두 배 이상으로 증가하고 손 사이의 **협응(coordination)**까지 처리해야 하므로 문제는 훨씬 복잡해집니다. 이러한 어려움 때문에 기존 연구들은 흔히 몇 가지 우회로를 택했습니다.

첫째, **모방학습(IL)**에 의존하는 접근법이 많았습니다. 인간 또는 알고리즘 전문가의 **시演 데이터(demonstration)**를 수집하여 모델을 학습시키는 방식으로, 실제 로봇이나 가상 환경에서 사람이 VR 장비나 텔레오퍼레이션으로 두 손을 조작해 데이터를 모으거나, 사전에 잘 작동하는 정책을 마련해두는 것입니다. 이러한 방법들은 단기간에 성공적인 동작을 보여줄 수 있지만, **데이터 수집**이 매우 까다롭고(특히 다관절 손으로 복잡한 조작 시연을 반복 수집하는 것은 큰 인프라와 노력 필요), 수집된 시演에 과도하게 의존해 **일반화 제한**이 있었습니다. 또한 대부분 **병렬 두손 그리퍼**(엄지와 검지만 있는 단순 집게 형태) 같은 단순 말단-effector를 가정하거나, 과제 하나만을 위한 맞춤형 솔루션인 경우가 많았습니다.

둘째, **강화학습(RL)** 기반 접근도 시도되었으나, 별도의 구조나 도움 없이 **순수한 RL**로 두 손 조작을 학습하는 것은 거의 불가능에 가까웠습니다. 한 손 조작에서도 엄청난 탐색(step 수)이 필요하므로, 두 손의 경우 **탐색 공간**이 폭발적으로 커지고 두 손의 상호작용 때문에 학습이 불안정해집니다. 예컨대, 과거 연구에서 피아노 연주나 병 뚜껑 열기 등 특정 양손 과제를 RL로 풀어본 사례가 있지만, 이러한 방법들은 **특정 동작에 특화**된 설계(예: 보상함수 설계나 상태 표현) 덕에 겨우 성공한 경우로, 범용성이 떨어집니다. **전이학습**이나 **시뮬레이션-현실 전이** 등의 기법을 접목하지 않으면 실제 로봇에 적용하기 어려운 점도 한계로 지적됩니다.

정리하면, 다관절 로봇 양손의 섬세한 조작을 학습시키는 문제는 높은 차원과 두 손의 **실시간 협응** 요구 때문에 기존 접근법들이 **시演 데이터**에 의존하거나 **단순한 손**만을 대상으로 하거나 **과제 특화된 솔루션**에 머무르는 한계를 보여왔습니다. 이러한 맥락에서, 시演 없이 순수 RL로, 그리고 다섯 손가락을 가진 두 손으로, 다양한 양손 조작 기술을 **효율적**으로 학습할 새로운 방법론이 요구되어 왔습니다.

## AsymDex의 핵심 기여

**AsymDex** (Asymmetric Dexterity)은 이러한 문제를 해결하기 위해 고안된 **강화학습 기반 양손 조작 프레임워크**로, **비대칭성(asymmetry)**과 **상대 좌표계(relative coordinates)**라는 두 가지 핵심 아이디어를 도입함으로써 **학습 효율**과 **성능**을 크게 향상시켰습니다. 아래는 AsymDex의 주요 기여 내용입니다:

* **양손 역할의 비대칭적 분담:** 인간의 **손잡이(handedness)** 현상에서 영감을 받아, 한 손은 물체를 잡고 위치/방향을 잡아주는 **보조 손**(facilitating hand)으로, 다른 손은 세밀한 조작을 수행하는 **주 손**(dominant hand)으로 역할을 구분합니다. 이렇게 역할을 나누면 불필요하게 거대했던 상태·행동 공간이 줄어들고, 각 손이 맡은 역할에 집중하여 학습할 수 있습니다. 이는 마치 한 사람이 한 손으로 병을 들고 다른 손으로 병뚜껑을 여는 것과 같은 자연스러운 분업을 로봇에 적용한 것입니다.

* **상대적 관측 및 동작 공간:** 두 손의 움직임을 **절대 좌표계**가 아니라 **상대 좌표계**에서 표현함으로써, 두 손의 **동시 움직임**을 일관성 있게 조정합니다. 구체적으로, 보조 손이 잡은 물체에 좌표계를 붙이고, 주 손과 다른 객체의 위치를 그 좌표계로 측정하여 **상대적인 상태**로 사용합니다. 행동도 마찬가지로 상대 좌표계에서 정의된 **목표 자세**를 출력하게 하여, 한 손의 움직임에 다른 손이 자동으로 대응하는 구조를 취했습니다. 이를 통해 별도의 타이밍 신호 없이도 두 손의 움직임이 **동기화**되고, 관측/행동 차원도 더욱 감소합니다.

* **두 단계 학습 (파지 + 조작):** 대부분의 양손 조작 과제가 그렇듯이, 물체를 처음 잡아드는 **획득 단계**와 잡은 후에 다루는 **상호작용 단계**로 문제를 나누었습니다. AsymDex는 이러한 **2-단계 분해**를 통해, 먼저 **파지(grasp)** 정책으로 물체를 집게 한 후 곧바로 양손 조작 정책으로 전환하여 과제를 완수합니다. 이때 기존 연구의 최신 **파지 학습 기법**(예: PDGM)을 받아들여, 시뮬레이션에서 다관절 손의 파지 정책을 미리 학습하고 AsymDex와 통합했습니다. 그 결과, 초기 물체 획득부터 최종 조작 완료까지 **시작부터 끝까지 학습 가능**한 총합적인 양손 RL 프레임워크가 되었습니다.

* **범용성과 샘플 효율성:** AsymDex는 하나의 알고리즘으로 **여러 종류의 과제**에 적용되어 좋은 성능을 보였습니다. 병 마개 열기, 물체 쌓기, 컵 속에 블록 넣기, 스위치 누르기 등 **상호 비대칭적 역할**이 드러나는 다양한 작업에서, 추가 튜닝 없이도 높은 성공률을 기록했습니다. 특히 **표본 효율(sample efficiency)** 측면에서, 동일 조건의 다른 RL 방식들보다 훨씬 적은 학습 단계로 성공에 도달함을 보였습니다. 이는 비대칭성과 상대 좌표계 도입으로 **학습 난이도 자체를 낮춘 효과**로 해석됩니다.

* **시연 없는 학습:** 끝으로, AsymDex는 **오로지 강화학습**만으로 동작하며 **전이 학습이나 시演 데이터**에 의존하지 않습니다. 이는 구현과 재현 측면에서 실용적 이점이 있습니다. 복잡한 모션 캡처 장치나 휴먼 데모가 없어도 되고, 새로운 작업에 대해서도 보상 신호만 설계하면 학습을 시작할 수 있기 때문입니다.

이러한 기여 요소들을 바탕으로, AsymDex는 기존 방법들이 풀기 어려웠던 양손 다지 조작 문제에 **새로운 구조적 솔루션**을 제시했습니다. 다음으로 이 핵심 아이디어인 **비대칭성 전략**과 **상대 좌표계 설계**를 좀 더 상세히 살펴보겠습니다.

## 비대칭성 설계의 동기와 구현

인간은 대부분 한 손을 **기능적인 주 손**으로, 다른 손을 **보조 손**으로 사용합니다. 예를 들어 병을 열 때 한 손으로 병을 단단히 잡고(보조), 다른 손으로 뚜껑을 비틀어 엽니다(주); 옷을 단추 끼울 때도 한 손으로 옷감을 잡아 고정하고 다른 손으로 단추를 끼우죠. 이러한 **역할-분화된 양손 조작(role-differentiated bimanual manipulation)**의 이점은 인간 공학 및 진화론 연구에서도 중요성이 제시되어 왔습니다. **AsymDex**는 바로 이 점에 착안하여, **한 손은 잡고** **한 손은 조작하는** 비대칭 전략을 명시적으로 모델링했습니다.

**동기:** 비대칭성의 가장 큰 장점은 **문제 차원의 축소**입니다. 두 손이 모든 일을 다 할 수 있게 허용하면 관절 수, 행동 조합이 폭증하지만, 한 손은 **물체를 붙들고 위치를 잡는 역할**로 한정하면 그 손의 **세밀한 손가락 움직임(in-hand manipulation)**을 고려할 필요가 없어집니다. 인간 보조 손도 물체를 쥐는 데는 관여하지만 쥔 후엔 손가락을 이리저리 굴리지 않고 물체를 단단히 잡은 채 팔 움직임으로 위치만 바꾸는 경우가 많습니다 (예: 병뚜껑 열 때 병을 잡은 손은 그대로 잡고 팔로 병 위치만 돌립니다). 따라서 **보조 손**의 역할을 물체 **파지 및 재배치**로 국한하면, 그 손의 **불필요한 자유도**를 대폭 없앨 수 있습니다. 반대로 **주 손**은 **모든 자유도**를 활용해 정교한 작업을 수행하도록 하고, 대신 보조 손이 작업 대상을 좋은 위치로 가져다주는 식입니다. 이처럼 역할을 분리하면 각 손의 **담당 작업이 단순화**되고, 두 손이 서로 방해되기보다는 **협력**하게 되어 학습이 용이해집니다.

**구현:** AsymDex에서는 두 손을 사전에 지정하여 하나는 **“주 손”**, 다른 하나는 **“보조 손”**으로 설정합니다. **보조 손**은 물체를 잡는 역할이므로, **초기에 물체를 잡은 후 손가락 관절을 고정**합니다. 구체적으로, 보조 손의 모든 손가락 관절은 **파지한 자세**로 묶어 두고 더 이상 개별 제어하지 않으며, 대신 손목에 해당하는 **손 바닥(base)**의 6자유도(3D 위치 + 3D 방향)만 움직이도록 합니다. 이러면 보조 손은 물체를 떨어뜨리지 않는 한 **잡은 물체와 상대 움직임이 없게** 되고 (손가락을 펴거나 다시 잡지 않으므로 물체에 대해 항상 동일한 파지 자세), 사실상 보조 손과 물체를 하나의 결합된 객체로 볼 수 있게 됩니다. 반대로, **주 손**은 6자유도 손목 움직임에 더해 **손가락 관절**까지 **모두 제어**합니다. 주 손은 작업에 필요한 섬세한 조작(예: 물체 돌리기, 뚜껑 비틀기, 작은 부품 집기 등)을 수행하도록 설계되었습니다.

이 **비대칭 정책** 덕분에 **관측**과 **행동** 차원에서 큰 이득을 보았습니다. 기존 대칭 접근에서는 **상태**에 두 손의 모든 관절 각도와 두 손 바닥 위치, 그리고 두 손이 다루는 물체들의 상태를 다 포함해야 했지만, AsymDex의 **비대칭 관측** \$z\_{\text{asym}}\$에서는 **보조 손의 손가락 상태**와 **그 손이 잡은 물체의 상태**를 아예 뺄 수 있습니다. 왜냐하면 보조 손은 잡은 물체와 **상대적 변화가 없으므로** 해당 정보는 상수와 다름없기 때문입니다. 마찬가지로 **행동(action)**에서도 보조 손의 다수 관절 제어를 제거하여, 정책이 출력해야 할 변수 수가 크게 줄었습니다. 예를 들어 연구에 사용된 ShadowHand 로봇 손은 손가락에 24개 관절이 있는데, 기존 방식이었다면 양손 합쳐 **손가락 관절 48개 + 손목 12개 = 60차원 출력**을 매 시간-step마다 내야 했다면, AsymDex에서는 보조 손의 24개 관절을 제외하여 **36차원(주 손 24 + 두 손 손목 12)**만 제어합니다. 실제 구현에서는 여기서 더 나아가 **상대 좌표계**까지 적용하므로 손목 12개 중 6개도 상대적으로 표현되어 정책 출력 차원은 30까지 감소합니다. 이처럼 거대한 차원의 감소는 **표본 효율성** 향상에 결정적 역할을 합니다.

또 한 가지, 기존 비슷한 연구들은 보조 손을 그저 물체를 **고정**하는 데만 쓰고 거의 움직이지 않도록 제한했던 반면, AsymDex의 보조 손은 **물체를 재배치(reposition and reorient)**하는 적극적 역할을 맡습니다. 이는 두 손이 **동시에 작업**한다는 점에서 효율적입니다. 예를 들어 **블록을 컵에 넣는 작업**에서 보조 손(컵을 들고 있는 손)은 컵을 단단히 쥐고 있으면서도 살짝 기울이거나 위치를 조정하여 주 손(블록을 든 손)이 블록을 넣기 쉽게 도와줄 수 있습니다. 실제 AsymDex로 학습한 정책을 보면, 한 손이 물체를 잡고 그 물체를 적절한 각도로 **기울여 주거나(target orientation)** 대상 부품을 **가까이 가져가는(positioning)** 행동을 학습하였습니다. 이러한 협응은 사람이 두 손을 쓸 때 나타나는 행동과 유사하며, 결과적으로 주 손이 과제를 수행하기 더 쉽게 만들어 **성공률과 안정성**을 높여줍니다.

요약하면, 비대칭성 설계는 "**한 손은 물체를 안정적으로 잡고 전체 위치/방향을 맞추고, 다른 손은 섬세한 조작을 집중 수행한다**"라는 명제를 정책에 구조적으로 주입함으로써, 학습 난이도를 낮추고 두 손의 **상보적 협력**을 이끌어낸 것입니다.

## 상대 좌표계의 역할과 설계 이유

**상대 좌표계(relative coordinate frame)**의 도입은 AsymDex의 두 번째 핵심 아이디어입니다. 두 손이 유기적으로 협력하려면 **서로의 움직임에 즉각 대응**할 수 있어야 하는데, 이를 명시적으로 학습시키기란 어렵습니다. 일반적인 절대 좌표계에서 한 손의 움직임은 다른 손의 관측 값(상대 위치 등)을 복잡하게 변화시키므로, 정책이 이 상호작용을 스스로 배우게 두면 많은 데이터가 필요합니다. 이를 해결하기 위해 **AsymDex는 보조 손이 쥔 물체를 중심 좌표계로 삼아, 모든 관측과 행동을 그 좌표계로 변환**합니다. 이렇게 하면 보조 손과 함께 움직이는 좌표축이 생겨, 주 손의 관측은 "보조 손이 들고 있는 물체를 기준으로 어디에 있는가"를 나타내게 됩니다.

**구체적 구현:** 우선 **보조 손이 들고 있는 물체**에 좌표계 \$P\_f\$를 붙입니다. 원래 전역 월드 좌표계 \$P\_W\$에서 주 손의 손목 위치 \$\xi^d\_b(t)\$, 보조 손 손목 위치 \$\xi^f\_b(t)\$, 주 손 관절상태 \$\xi^d\_h(t)\$, 그리고 물체들의 위치 \$o\_f(t), o\_d(t)\$ 등을 관측했다면, 이를 \$P\_f\$ 좌표계로 변환합니다. 보조 손이 잡은 물체(\$o\_f\$)는 그 좌표계에서 **원점**에 해당하므로, \$P\_f\$상에서 \$o\_f(t)\$는 항상 0인 상수 벡터가 됩니다. 또한 보조 손 손목 \$\xi^f\_b(t)\$도 그 물체를 쥐고 있기 때문에 \$P\_f\$ 기준으로 변하지 않는 값(보조 손과 물체 사이에 상대 운동이 없으므로)이고, 따라서 이 둘은 **관측 공간에서 제거**해도 정보를 잃지 않습니다. 결과적으로 **상대 관측 상태** \$z\_{\text{AsymDex}}\$에는 **주 손의 손목 자세** (보조 손 물체 좌표계에서 본 위치/방향), **주 손의 관절 상태** (자체적인 값이니 좌표계 영향 없음), 그리고 **다른 대상 물체의 상대 위치**(예: 주 손이 조작하려는 두 번째 물체가 있다면 그것의 보조 손 물체 기준 위치)만 포함됩니다. 이러한 **객체 중심 관측**은 두 손의 **관계성**만을 남기고 기타 중복 정보를 덜어내기 때문에 훨씬 효율적입니다. 이전 절의 비대칭 설계와 결합하면, 상태 공간 차원이 월등히 줄어드는 효과가 있습니다 (예: 앞서 언급한 72차원의 관측이 상대 좌표 변환 후 약 절반 이하로 감소).

행동(action) 역시 **상대 공간**에서 정의됩니다. AsymDex 정책은 **절대 좌표의 두 손 목표**를 직접 내보내는 대신, **보조 손 물체 기준으로 주 손이 도달해야 할 목표 자세** \$\hat{\xi}^{r}\_{b}(t)\$를 출력합니다. 쉽게 말해 "주 손의 손목이 보조 손 물체 기준으로 어디에 어떻게 위치해야 하는가"를 내놓는 것입니다. 그리고 주 손의 손가락 목표 각도 \$\hat{\xi}^d\_h(t)\$도 함께 출력됩니다. 보조 손의 목표는 직접 출력하지 않는데, 대신 이 **상대 목표 자세**를 달성하기 위해 **두 손을 어떻게 움직일지**는 별도의 **상대 자세 제어기(controller)**가 담당합니다. 이 제어기는 현재 주 손의 실제 상대자세 \$\xi^r\_b(t)\$와 정책이 낸 목표 상대자세 \$\hat{\xi}^r\_b(t)\$를 비교하여 둘 사이의 **차이(dist)}**를 계산합니다. 그리고 이 차이를 **월드 좌표계**로 변환한 뒤, 그 차이를 양손에 적절히 분배하여 **실제 각 손의 목표 손목 pose** \$(\hat{\xi}^d\_b, \hat{\xi}^f\_b)\$를 산출합니다.

具体例로, 정책이 "주 손이 보조 손 물체에 비해 앞으로 10cm 가고 30도 회전해야 한다"는 명령을 내렸다면, 제어기는 이 10cm/30도 상대 변화량의 절반을 주 손에게, 나머지 절반은 보조 손에게 부여하는 식입니다 (연구에서는 \$\alpha=0.5\$로 설정하여 절반씩 움직이도록 했습니다). 그래서 주 손은 5cm 앞으로 가고 보조 손은 물체를 5cm 당겨와 둘이 만나게 되며, 회전도 절반씩 나눠서 주 손이 일부 돌리고 보조 손이 물체를 일부 돌려주는 형태로 **동시 달성**하게 됩니다. 이렇게 하면 한 손이 모든 걸 움직일 필요 없이 **두 손이 협조적으로 목표를 맞추는** 결과를 얻습니다. (물론 \$\alpha\$ 값을 바꾸면 한 손이 더 많이 움직이게 조절할 수도 있지만, 실험에서는 대칭적으로 하였습니다.)

상대 좌표계 도입으로 얻는 이점은 두 가지로 요약됩니다. **첫째**, 앞서 언급했듯 **차원 축소** 및 **중복 제거**입니다. 보조 손과 물체의 상태가 상수화되어 빠지고, 정책 출력도 상대 자세 하나면 되니 전체 변수가 줄어듭니다. **둘째**, 두 손의 **동기화된 움직임**을 자연스럽게 이끌어냅니다. 한 좌표계에서 목표를 표현하다 보니, 보조 손이 움직이면 주 손의 목표 좌표도 자동으로 변환되어 연동되고, 주 손이 다가가면 보조 손은 그만큼 덜 움직이는 식의 **반사적 조정**이 일어납니다. 이는 마치 두 손이 **하나의 팀처럼** 행동하게 만드는 효과가 있습니다. 그 결과 정책이 명시적으로 "먼저 왼손 움직이고 그 다음 오른손" 같은 타이밍을 배우지 않아도, **상대 관계**만 유지하도록 하면 자연히 동시협응이 달성됩니다.

이미 **선행 연구들**에서 단순한 로봇 팔 두 개를 협동시키는 경우 **상대 상태 표현**이 유용하다는 보고가 있었습니다. 다만 기존에는 두 팔 사이 거리 한 개 정도(1-자유도)만 상대적으로 쓰거나 제한적으로 활용했는데, AsymDex는 **풀 6자유도** 상대 pose를 활용하여 훨씬 복잡한 다지 손의 협응에 적용한 점이 차별화됩니다.

정리하면, AsymDex의 상대 좌표계 접근은 **"두 손이 서로를 기준으로 생각하도록 만들자"**는 철학으로, 복잡한 협응 문제를 **좌표 변환의 문제**로 치환하여 해결한 것입니다. 이를 통해 두 손의 관계 유지는 훨씬 수월해졌고, 결과적으로 학습 속도와 성공률 향상에 크게 기여했습니다.

## 강화학습 프레임워크: 정책 구조, 보상 설계, 환경 세팅

이제 AsymDex의 전체 **강화학습 프레임워크** 구성을 살펴보겠습니다. 앞서 설명한 비대칭성 및 상대 좌표 설계를 토대로, AsymDex는 **관측 → 정책 신경망 → 행동 출력**의 형태로 동작합니다. **정책 신경망**은 상태를 입력받아 행동을 출력하는 함수로, MLP 기반의 근사 함수를 사용했습니다. 입력으로는 위에서 정의한 **상대 좌표계 기반 비대칭 관측** \$z\_{\text{AsymDex}}\$ (주 손 손목의 상대 pose, 주 손 관절 상태, 주 손이 다룰 물체의 상대 pose 등) 벡터가 들어가고, 출력은 **주 손의 목표 상대 pose** \$\hat{\xi}^r\_b\$ 및 **주 손 손가락 목표 각도** \$\hat{\xi}^d\_h\$입니다. 이 정책은 하나의 **모델 (파라미터 세트)**로 두 손을 동시에 제어하게 되며, AsymDex 논문에서는 **Proximal Policy Optimization (PPO)** 알고리즘을 사용해 이 정책을 학습시켰습니다. PPO는 대표적인 on-policy 정책 최적화 알고리즘으로, 다수 에피소드를 병렬 수집하여 정책 갱신을 조금씩 진행하며 누적 보상을 높이는 방향으로 학습합니다.

**보상 설계:** AsymDex는 **시演 데이터가 없으므로 보상 함수 설계**가 매우 중요합니다. 각 과제별로 **성공 조건**이 정의되고, 이에 맞춰 보상 함수를 구성했습니다 (자세한 수식은 논문 Appendix C에 제공). 일반적으로 **최종 목표 달성**(예: 블록을 컵 안에 넣음, 병뚜껑 분리, 스위치 켜짐 등)에 큰 양의 보상을 주고, 그에 도달하도록 **단계별 성과 보상(shaping reward)**을 추가합니다. 예를 들어 *Block in cup* 과제의 경우, 블록과 컵의 거리/정렬 정도에 따른 점진적 보상, 블록이 컵에 들어갔을 때의 성공 보상, 중간에 블록이나 컵을 떨어뜨리면 큰 패널티 등을 설계합니다. **두 손의 협력**을 유도하기 위해, 한 손이라도 물체를 놓쳐 **작업이 불가능해지면** 즉시 에피소드를 종료하고 실패 보상을 주어 **안정적으로 물체를 잡도록** 하였습니다. 이러한 보상 항목들은 모두 **과제 성공과 효율**을 극대화하도록 설계되었으며, 잘 설계된 보상은 RL이 알아서 두 손의 협응을 **간접적으로 배우게** 만들어줍니다. (예컨대 블록을 컵에 넣으려면 두 손이 자연스럽게 협력할 수밖에 없으므로, 별도 협응 보상 없이도 성공 보상을 극대화하는 과정에서 협응이 향상됩니다.)

**환경 세팅:** 모든 실험은 NVIDIA의 **Isaac Gym** 시뮬레이터 상에서 진행되었습니다. 시뮬레이션 환경에는 인간 손과 유사한 **ShadowHand 로봇 손** 두 개가 위치하며, 각각 **30자유도**(24개 손가락 관절 + 6 DoF의 손목 기준 좌표 이동)로 모델링되었습니다. 두 손은 마주 보고 협력하도록 배치되었으며, 과제에 따라 하나 또는 두 개의 **물체**(예: 컵과 블록, 병과 뚜껑 등)가 초기 배치됩니다. 한 가지 중요한 설정은, **상호작용 단계**에서는 **바닥이나 테이블에 의존하지 않고** 순수히 두 손으로 물체를 다뤄야 한다는 점입니다. 예컨대 컵에 블록 넣기 과제에서, 컵을 책상 위에 내려놓고 블록을 넣는 것이 아니라 **한 손이 컵을 공중에 들고 있어야** 합니다. 이렇게 함으로써 두 손 간의 **진정한 협업** 없이는 성공할 수 없게 난이도를 높였고, 로봇 손이 물체를 놓치지 않고 끝까지 다루는지를 평가했습니다.

**학습 절차:** AsymDex의 학습은 크게 두 단계로 나뉩니다. 앞서 언급한 **물체 획득(acquisition) 단계**와 **상호작용(interaction) 단계**입니다. 많은 이전 연구들이 **획득 단계를 생략**하고, 물체가 이미 두 손에 쥐어진 상태로 시작하는 가정만 두었는데, AsymDex 연구진은 이를 현실적으로 확장하여 **처음부터 물체를 집는 것까지 학습**에 포함했습니다.

* **획득(파지) 단계:** 이 단계에서는 각 물체를 해당 손이 **단독으로 집는 것**만 목표로 합니다. 두 손 간 협응이 필요 없으므로, **한 손씩 따로** 학습시키거나 기존 기법을 활용할 수 있습니다. 논문에서는 **PDGM \[45]**이라는 알려진 파지 알고리즘을 사용하여 **다관절 손의 grasp 정책**을 훈련했습니다. 시뮬레이션에서 물체 주변의 **사전 파지 자세(pre-grasp pose)** 후보들을 제공하고 손이 그중 하나를 취해 물체를 붙잡도록 하는 방식으로 학습을 가속화했습니다. 그렇게 보조 손이 물체를 안정적으로 잡으면, 곧바로 **AsymDex의 양손 정책**으로 전환합니다. 만약 과제상 주 손도 다른 물체를 잡아야 한다면, 주 손에 대해서도 동일한 grasp 정책을 훈련해 두었다가 적용하고, 물체를 잡은 즉시 주 손의 **손가락 제어를 AsymDex 정책으로 이양**합니다. 이 단계 분리는 학습을 **두 부분으로 나눠서** 생각하게 해주므로, 난이도를 크게 낮추어 줍니다. (Grasp만 집중 학습 -> 이후 협동 조작 학습)

* **상호작용 단계:** 두 손에 필요한 물체들이 쥐어진 이후부터 시작됩니다. 여기서는 앞서 설명한 **비대칭+상대** 정책이 본격적으로 두 손을 동시에 제어하면서 과제를 해결합니다. 학습 시에는 두 단계가 연속으로 일어나지만, 분석을 위해 **두 손 협응 학습만 따로 평가**하기도 했습니다. 예컨대, **학습 실험 1**에서는 환경을 이미 파지된 상태 (보조 손이 물체 쥐고, 주 손도 필요시 물체 쥐고)에서 시작시켜서 AsymDex 정책이 **협응만 배우도록** 했습니다. 이는 파지 단계를 고려하지 않을 때 AsymDex의 구조적 이득(비대칭/상대)이 얼마나 효과적인지 확인하기 위한 실험입니다. 이후 **학습 실험 2**에서는 초기 상태를 책상 위 물체로 설정하여 **파지+협응 전체**를 AsymDex (+ grasp 정책)으로 풀도록 했습니다.

* **동작 제어:** 정책이 출력한 **목표 자세**(상대 pose 및 주 손 관절 각도)는 **PD 제어기**를 통해 로봇에 전달됩니다. 구체적으로, 매 시뮬레이션 step마다 정책은 다음 목표 위치/각도를 내놓고, PD(position + velocity gain) 제어기가 현재 위치에서 목표로 수렴하도록 토크를 걸어주는 방식입니다. 이 저수준 제어가 실제 로봇의 서보 모터 구동을 모사하며, 정책은 고수준 kinematic 목표만 내면 됩니다. PD 제어기의 이득은 실험적으로 튜닝되어, 손가락이 떨지 않고 부드럽게 따라가도록 설정했습니다.

요약하면, AsymDex의 RL 프레임워크는 \*"먼저 잡고, 그 다음 협력해서 원하는 작업 수행"\*이라는 순서를 취하며, **정책 신경망**은 비대칭/상대화된 상태를 보고 **다음 상대 움직임 지시**를 내리는 역할을 합니다. **PPO 알고리즘**으로 수백만 회 시뮬레이션 step을 학습한 결과, 정책은 두 손의 협응을 점차 익혀서 난해한 양손 조작 과제들을 해결할 수 있게 됩니다. 다음으로, 이런 학습 결과가 **얼마나 향상되었는지** 성능 지표와 함께 살펴보겠습니다.

## 성능 평가 및 실험 결과

연구진은 AsymDex의 효과를 검증하기 위해 **네 가지 시뮬레이션 과제**와 **세 가지 실제 로봇 과제**에 대해 실험을 수행했습니다. 시뮬레이션 과제들은 BiDexHand 연구에 소개된 환경들을 기반으로 하며, 모두 **두 손의 비대칭 협력**이 필요한 시나리오입니다:

* **Block in cup**: 한 손은 컵을 들고, 다른 손은 블록을 집어 **컵 안에 넣는** 과제. (컵이나 블록을 떨어뜨리지 않아야 함)
* **Stack (컵 쌓기)**: 각각의 손이 컵 하나씩 잡고, 두 컵을 맞대어 **포개듯이 쌓는** 과제. (두 컵의 구멍을 잘 정렬해야 성공)
* **Bottle cap**: 한 손이 병을 쥐고 적당히 기울이면, 다른 손이 병뚜껑을 **잡아서 분리(오프닝)** 하는 과제.
* **Switch**: 한 손이 스위치(혹은 버튼이 있는 장치)를 들고 방향을 맞추고 있으면, 다른 손이 그 **스위치를 눌러 켜는** 과제.

이들 과제는 하나같이 **보조 손이 물체를 들거나 고정**하고 **주 손이 작업**하는 형태입니다. 또한 BiDexHand의 원래 과제보다 난이도가 높게 설정되어 있는데, 예를 들어 *Block in cup*의 경우 원래는 테이블 위에서 컵을 고정할 수 있었지만, 본 연구에선 **두 손만 사용하여 공중에서 수행**하도록 했습니다. 이는 **양손 협응 없이는 불가능한 상황**을 만들어 AsymDex의 필요성을 부각시키기 위함입니다.

실험에서는 AsymDex의 성능을 검증하기 위해 여러 **비교 기준 정책(baseline)**들과 대조했습니다:

* **Monolithic (모놀리식)**: 아무 구조도 넣지 않은 **단일 정책**입니다. 두 손을 한꺼번에 제어하며, 비대칭 역할 구분도 없고 관측도 절대 좌표계 그대로 씁니다. 일종의 "*naive*"한 강화학습 접근으로, 이 방법이 잘 안 되는 이유를 실증하기 위한 기준입니다 (Sec 3.2의 \$\pi\_{\text{sym}}\$와 유사한 정책). 두 손이 대칭적으로 **똑같은 역할**을 할 수 있다고 가정하여, 양손의 모든 관절/손목을 다 쓰는 거대 공간에서 PPO로 학습시켰습니다.

* **Asym w/o rel (비대칭 but 절대좌표)**: AsymDex에서 **상대 좌표계만 뺀 버전**입니다. 즉, 두 손의 역할은 주/보조로 나누되 관측과 행동을 **월드 좌표계** 기준으로 학습합니다. 이 정책을 통해 **상대 좌표계의 효과**를 검증할 수 있습니다.

* **Rel w/o asym (상대좌표 but 대칭 역할)**: AsymDex에서 **비대칭 역할만 제외한 버전**입니다. 두 손 모두 풀 DOF로 조작할 수 있게 하되, **관측과 행동은 상대 좌표계**로 표현합니다. 이를 통해 **비대칭성의 효과**를 단독으로 살펴봅니다.

* *(추가로, 2단계 학습을 고려한 실험에서는)* **Two-stage-sym**: 파지 단계 + 상호작용 단계를 분리한다는 점은 AsymDex와 동일하지만, **상호작용에서 모놀리식 정책**(대칭)을 사용하는 베이스라인입니다. 즉, 두 단계로 나눈 건 같으나 두 손 역할 구분이나 상대 좌표계를 쓰지 않은 조합입니다. 이를 통해 **단계 분할의 이점만 가진 경우**를 비교합니다.

**성능 지표**로는 주요하게 **과제 성공률(%)**과 **학습 속도(표본 효율)**를 평가했습니다. 각 실험은 5개 시드로 반복하여 평균 및 표준편차를 기록했습니다.

### 시뮬레이션 결과 (상호작용 단계 중심)

먼저, **파지 단계를 배제**하고 순수 상호작용 학습만 수행한 경우(즉 환경을 이미 물체를 잡은 상태에서 시작) 결과를 보면, **AsymDex가 모든 과제에서 가장 높은 성공률**을 보였고 **학습도 가장 안정적**이었습니다. 구체적으로, AsymDex는 네 과제 모두에서 최종 성공률이 다른 방법들을 압도하거나 유사 수준으로 최고였으며, 어떤 과제는 두 번째로 좋은 방법 대비 성공률이 20~~30%p 높았습니다. 특히 **Switch** 과제의 경우 AsymDex만이 60% 이상의 높은 성공률을 기록했고, 다른 방법들은 한 자릿수~~10%대에 머물렀습니다.

**Rel w/o asym** (상대좌표만 사용) 정책은 일부 과제에서는 AsymDex에 버금가는 성능을 냈지만, 다른 과제에서는 뒤처졌습니다. 예를 들어 **Stack**(컵 쌓기) 과제에서는 Rel w/o asym도 AsymDex와 거의 비슷한 성공률을 냈으나, **Bottle cap**이나 **Switch**에서는 AsymDex보다 상당히 낮은 성공을 보였습니다. 반면 **Asym w/o rel** (비대칭만 사용) 정책은 **Bottle cap** 같은 특정 과제 한두 개에서만 그나마 성공을 거두고, 나머지에서는 좋은 정책을 끝내 찾지 못했습니다. 이는 상대 좌표계 없이 절대 좌표로 학습하니 두 손 협응을 맞추기 어려워진 것으로 추정됩니다. 가장 구조가 없는 **Monolithic** 정책은 전체적으로 **학습에 실패**했습니다. 네 과제 중 하나도 제대로 높은 성공률로 수렴하지 못했고, **Stack** 과제 정도에서만 오히려 Asym w/o rel보다 좀 나은 수준을 보였습니다. 연구진의 분석에 따르면, Monolithic이 *Stack*에서 약간 나았던 이유는 **비대칭 정책이 금지한 보조 손의 손가락 움직임을 Monolithic은 허용**하다 보니, 한 손의 손가락을 사용해 컵을 약간 기울여 끼우는 **예상밖의 전략**을 스스로 찾아냈기 때문이라고 합니다. 그러나 이는 어디까지나 특이 사례이고, 전반적으로 Monolithic은 거대한 탐색 공간 때문에 **아무 것도 배우지 못하거나 엉뚱한 행동**을 보이는 경우가 많았습니다.

**학습 곡선** 측면에서도 AsymDex의 우수성이 드러났습니다. 논문 Figure 4의 학습曲을 보면, AsymDex는 매우 **가파르게 보상과 성공률이 상승**하여, 동일 시간 내에 타 방법들보다 훨씬 빨리 목표 성능에 도달했습니다. 반면 Monolithic 등은 장기간 학습해도 보상이 불안정하거나 거의 0에 머물렀습니다. 이는 AsymDex가 **표본 효율**에서 탁월함을 증명합니다.

이 실험을 통해 얻은 통찰은 명확합니다: **비대칭성만으로도, 상대좌표계만으로도 부족하고 둘 다 필요**하다는 것입니다. 먼저, 개별적으로 보면 **Relative 좌표계**를 쓴 경우(비대칭 없음)가 **비대칭만 쓴 경우**보다 **일반적으로 더 성능이 좋았다**는 결과입니다. 이는 두 손 사이 관계를 맞추는 것이 손 하나를 단순화하는 것보다 과제 달성에 좀 더 결정적임을 시사합니다. 그러나 **두 가지를 모두 쓴 AsymDex가 가장 안정적**으로 모든 과제를 잘 풀었기에, **두 요소의 상호보완적 효과**를 확인할 수 있었습니다. 일부 과제에서는 상대좌표만으로 충분해 보이나 (Stack의 경우), 다른 과제에서는 반드시 비대칭 분업이 필요했고 (Switch의 경우), **결국 둘 다 갖춰야 모든 과제를 대응**할 수 있었습니다.

### 파지 + 조작 전체 학습 결과

다음으로, **물체 파지 단계까지 포함한 학습** 실험 결과를 살펴보겠습니다. 이 실험에서는 초기 물체들이 테이블 위에 놓인 상태에서 시작하여, 각 방법이 물체를 집고 이후 조작까지 **전체 작업을 완료**할 수 있는지 평가했습니다. 비교 대상으로는 **Monolithic (파지+조작 통합 단일정책)**과 **Two-stage-sym (파지 따로+조작 대칭정책)**을 설정했습니다. AsymDex는 **파지 정책 + 비대칭 상대 조작 정책**의 조합이고, Two-stage-sym은 **파지 정책 + 모놀리식 조작 정책**의 조합입니다. 공정한 비교를 위해 파지 단계에서의 성공률/시도 회수를 동일하게 맞추고 (사전에 충분히 파지 성공률을 높여둠) 평가했습니다.

결과는 AsymDex의 **압승**이었습니다. 논문 Table 2에 따르면, **전체 작업 성공률**에서 AsymDex가 다른 두 방법보다 **유의미하게 높았으며**, 두 가지 과제(Block in cup, Bottle cap 등) 모두에서 확연한 차이를 보였습니다. 특히 Monolithic은 앞서 협응 단계만 할 때도 저조했지만, 여기에 파지까지 함께 학습시키려 하니 더욱 어려워져 거의 성공하지 못했습니다 (성공률 0%에 수렴). Two-stage-sym은 파지 자체는 잘하지만 조작 단계에서 대칭 정책의 한계로 성공률이 낮았고, AsymDex보다 학습도 느렸습니다. 반면 AsymDex는 파지-조작 **두 단계 모두**를 효율적으로 수행하여 높은 **종단 간 성공률**을 보였습니다. 이는 **단계 분해의 장점과 구조적 정책 설계의 장점이 결합**되어야 최고의 성능을 낼 수 있음을 의미합니다. 재밌는 점으로, Two-stage-sym이 Monolithic보다는 성능이 좋았는데, 이는 **단계 분해만으로도** 학습 난이도를 완화시키는 효과를 입증합니다. 하지만 결국 **비대칭+상대좌표의 구조적 이득이 없으면** 높은 성능에 한계가 있음도 확인되었습니다.

또한, 연구진은 **정성적 관찰(qualitative analysis)**로, 두-단계 방법들은 학습된 정책이 **보다 사람같이 합리적인 동작 순서**를 보여주는 경향이 있다고 언급했습니다. 예컨대 Block in cup 작업에서, Two-stage 방식들은 먼저 컵을 안정적으로 잡고, 블록을 들어올려 컵에 넣는 **분리된 행동 단계**를 학습했지만, Monolithic은 한꺼번에 하려다가 오히려 컵을 떨어뜨리는 등 **혼란스러운 시도를 반복**했다고 합니다. 이는 **단계적 학습**의 현장 로봇 적용상 이점도 시사합니다 (한번에 복잡한 동작을 배우는 것보다, 단계별로 배운 정책을 잇는 것이 더 이해하기 쉽고 안정적이라는 것).

### 실제 로봇 실험

흥미롭게도, AsymDex 논문에서는 **실제 로봇으로의 검증**도 일부 시도되었습니다. 연구진은 **실물 ShadowHand 유사 손**인 **Allegro Hand** 두 개와 로봇 암(하나는 7-자유도 Kinova Gen3)을 활용하여, 앞서 시뮬레이션의 세 가지 작업을 현실에서 실행해 보았습니다. 과제는 **Block in cup**, **Pour (따르기)**, **Twist lid (뚜껑 비틀기)**의 세 가지입니다. **Block in cup**은 말 그대로 한 손으로 컵을 들고 다른 손으로 블록을 넣는 것, **Pour**는 한 손에 컵을 들고 작은 공이나 액체를 다른 컵에 **따르는** 작업, **Twist lid**는 한 손으로 병을 잡고 다른 손으로 뚜껑을 비트는 과제입니다 (BiDexHands의 Bottle cap과 유사).

현실 실험에서는 시뮬레이션에서 학습된 정책을 **그대로 이식(sim-to-real)**하거나, 약간의 **도메인 랜덤화**로 강인성을 높인 정책을 사용했습니다. 결과적으로, **AsymDex 정책은 현실에서도 높은 성공률**을 보였습니다 (Block in cup과 Pour 작업은 거의 매번 성공). 특히 AsymDex는 **제로샷(Zero-shot) 전이**에 가까운 형태로, 시뮬레이션에서 관측에 약간의 노이즈를 추가하고 물리 파라미터를 랜덤화하는 등으로 훈련했더니 실제에서도 별도 추가 학습 없이 작업을 수행할 수 있었습니다. 반면, **기존 방법들의 정책**은 시뮬레이션에서조차 성능이 낮았기에 현실에 가져올 수준에 이르지 못했습니다. 연구진은 AsymDex 정책이 현실 물리에서도 잘 동작하는 모습을 보이며, **난폭한 동작 없이** 비교적 안전하게 두 손을 사용했다고 보고했습니다. 다만 **Twist lid** 작업은 가장 난이도가 높아서, 학습 시에 인간스러운 회전력 프로필을 따르는 추가 보상 설계를 하는 등 신경을 써서 겨우 성공률을 끌어올렸다고 합니다. 이 또한 AsymDex 정책으로 현실에서 일부 성공 사례를 얻었으나, 아주 완벽하거나 빠른 동작은 아니었다고 합니다.

전반적으로, **시뮬레이션 결과와 실제 결과 모두** AsymDex의 구조적 아이디어들이 성능 향상에 기여함을 입증했습니다. 두 손 협조 문제가 제대로 풀리지 않던 기존 RL 방식들과 달리, **AsymDex는 어려운 양손 조작도 학습 가능하다는 것을 보여주었고**, 여러 상황에서 일관되게 우수한 성적을 거두었습니다.

## AsymDex의 실질적 장점과 한계

마지막으로, AsymDex 방법론의 **강점**과 **제한점**을 정리해 보겠습니다.

**장점:**

* **학습 효율 극대화:** 비대칭성과 상대 좌표계를 도입한 결과, 불필요한 상태·행동 변수를 대폭 줄여 **샘플 효율성**이 향상되었습니다. 동일 과제를 수행하는 다른 RL 방법들보다 훨씬 적은 경험으로 높은 성능에 도달할 수 있었으며, 이는 곧 **훈련 시간 단축** 및 **시뮬레이션 자원 절약**으로 이어집니다. 예를 들어, Monolithic 정책은 끝내 해결하지 못한 과제를 AsymDex는 비교적 짧은 에피소드 수로 마스터했습니다.

* **구조적 일반화:** AsymDex의 설계 원리 (한 손 고정·한 손 조작 + 상대 좌표 동기화)는 한두 특정 과제에만 통하는 것이 아니라 **여러 양손 조작에 범용적으로 적용 가능**한 **구조적 귀납 바이어스**입니다. 사람의 양손 사용 방식 자체를 모사한 것이기에, **붙였다 떼는 조립**, **뚜껑 열기/닫기**, **따르기**, **쌓기** 등 어떤 작업에서도 통했습니다. 이는 이전에 RL로 한 가지 작업을 풀면 다른 비슷한 작업에 다시 처음부터 학습해야 했던 것과 대비되는 점입니다.

* **무시演 자율 학습:** AsymDex는 **전적으로 보상신호에 의한 강화학습**으로 동작하여, 별도의 **전이학습이나 시演 데이터 준비가 불필요**합니다. 이는 실용성 측면에서 큰 이점인데, 새로운 로봇 손이나 새로운 과제에 대해 사람 시연을 일일이 모을 필요 없이, 시뮬레이터만 세팅하면 학습을 돌려볼 수 있기 때문입니다. 특히 다관절 손의 시연 데이터는 구하기 어려운데, 이를 효과적으로 우회했습니다.

* **파이프라인 통합:** 파지-조작의 **2단계 통합**은 학계 선행 연구들이 간과했던 부분을 짚고 넘어갔습니다. 실제 로봇에게는 물체를 집는 것 자체가 중요한 도전인데, AsymDex는 이를 포함하여 **처음 집기부터 최종 작업 완료까지 한 흐름으로** 솔루션을 제시했습니다. 파지 정책과의 결합도 유연하여, 향후 더 좋은 파지 알고리즘이 나오면 쉽게 교체해 쓸 수 있는 장점이 있습니다.

* **다관절 손을 활용:** 기존 양손 RL 연구 상당수가 두 손을 **단순 그리퍼** (두 손가락 집게) 정도로 가정한 것과 달리, AsymDex는 **사람 손처럼 복잡한 로봇 핸드**를 전제로 했습니다. 보조 손은 손가락을 움직이지 않지만 애초에 파지에 다관절 손의 능력을 활용했고, 주 손은 풀 24자유도 손가락 움직임을 사용합니다. 그럼에도 학습에 성공했다는 건 이 방법이 **고차원 제어기에도 통한다**는 의미이며, 향후 사람 수준의 섬세한 로봇 조작에 한 발 다가섰음을 보여줍니다.

* **강인한 협응과 안정성:** 상대 좌표계 컨트롤과 전용 제어기는 두 손 움직임의 **동시성**을 높이고 한 손이 움직일 때 다른 손이 제때 받쳐주는 식의 **안정적 협응**을 이끌었습니다. 결과적으로 물체를 떨어뜨리는 실수나 한쪽이 지나치게 먼저 움직여 실패하는 사례가 줄어들었습니다. 예를 들어 두 손이 컵과 블록을 맞추는 동작에서, 한 손이 움직이는 만큼 다른 손도 비례해 움직여 끝까지 **서로 놓치지 않고 협력**하는 모습을 보였습니다 (Fig. 4 그래프 및 시뮬레이션 영상 참조).

**한계:** (연구팀이 언급한 부분과 우리가 유추한 내용을 함께 포함합니다)

* **복잡한 **대칭 작업 미대응**: 현재 AsymDex는 **비대칭 역할**을 전제로 하기 때문에, **양손 모두 세밀한 조작이 필요한 작업**에는 바로 적용하기 어렵습니다. 예컨대 **무거운 물체를 두 손으로 함께 돌리는 작업**이나 **두 손이 동시에 정교하게 움직여야 하는 동작** (협력 운반, 혹은 동적 물체 주고받기 등)은 AsymDex 구조로는 표현이 힘듭니다. 이러한 작업에서는 두 손 모두가 주 손 역할을 부분적으로 수행해야 할 수 있는데, AsymDex는 애초에 한 손은 손가락을 고정해버리므로 한계가 있습니다. **Dynamic handover**(물체를 한 손에서 다른 손으로 넘겨주는 동작)도 논문에서 한계 사례로 언급했는데, 이 경우 처음에는 A손이 보조였다가 넘겨받은 후에는 B손이 주가 되는 등 **역할 전환**이 필요하지만 현재 프레임워크로는 대응되지 않습니다.

* **로봇 팔의 운동 한계 미고려:** 실험에서는 손목이 6자유도로 **자유롭게 움직일 수 있는** 가상 환경을 썼지만, 실제 로봇 손은 **팔에 부착**되어 있습니다. 때문에 **작업 영역(workspace)**이나 **관절 한계**, **두 팔 간 간섭** 등의 문제가 현실에는 존재합니다. AsymDex는 이러한 **키네마틱/다이내믹 제약**을 명시적으로 다루지 않았습니다. 시뮬레이션에서야 손목을 그냥 원하는 대로 이동시키면 되지만, 실제 로봇 팔은 서로 부딪힐 수 있고 관절 속도/가속도 제한도 있으므로, AsymDex 정책을 그대로 적용하면 충돌이나 범위 제한에 걸릴 수 있습니다. (물론 이는 추가적인 모션 플래너나 안전 레이어를 통해 보완 가능하나, 현 연구 범위 밖입니다.)

* **행동의 자연스러움**: 연구진은 AsymDex로 학습한 정책이 **항상 인간처럼 자연스럽진 않다**고 지적했습니다. 이는 보상 설계가 **성공률** 위주로 되어 있어, 로봇 움직임의 **우아함**이나 **안정성**을 별도로 보장하지 않았기 때문입니다. 예를 들어 컵을 기울일 때 사람이라면 천천히 기울일 텐데, 초기엔 갑자기 확 기울여 내용물을 쏟을 위험이 있는 식입니다. 실제 실험에서도 AsymDex 정책이 성공은 했지만 때때로 **급격한 동작이나 비직관적인 자세**를 보였다고 합니다. 이는 **추가적인 제약이나 페널티** (예: 너무 빠른 손목 회전은 벌점 등)를 주면 개선될 수 있을 부분이며, 현 구조의 한계라기보다 학습 목표 설정의 한계라 할 수 있습니다.

* **하드웨어 실증 제한:** 논문 시점 기준으로 AsymDex는 시뮬레이션 성능을 주로 다루었고, 하드웨어 실험은 제한적인 범위에서만 보여주었습니다. 완전히 동일한 정책을 다양한 실제 로봇 플랫폼에 적용한 검증이 아직 부족하며, 센서 노이즈나 미세한 모델 불일치 등에 얼마나 견딜 수 있는지 추가 연구가 필요합니다. (연구진은 향후 더 광범위한 **Sim2Real 검증**을 계획하고 있다고 서술했습니다.) 현실 세계에서는 예기치 못한 상황들이 많아, 현재 방법을 바로 쓰기보다는 약간의 **현장 미세조정(fine-tuning)**이나 **안전장치**가 요구될 것으로 보입니다.

* **역할 고정의 제약:** AsymDex는 학습 전에 어떤 손을 주로 쓸지 결정해야 합니다. **왼손이 항상 보조, 오른손이 항상 주**로 정해지므로, 만약 작업 중에 그 반대 역할이 잠시 요구되더라도 지원하지 못합니다. 사람은 상황 따라 양손 역할을 유연하게 바꾸지만, AsymDex 정책은 고정된 역할 분담만 학습하므로 **상황 적응성**이 떨어질 수 있습니다. (물론, 특정 작업에서는 어느 손이 주인지 처음부터 정하면 되지만, 환경 변화로 역할을 바꿔야 하면 다시 학습해야 할 수 있습니다.)

* **보상/파라미터 튜닝 필요:** 마지막으로, AsymDex라고 해도 **보상 함수 설계**와 **하이퍼파라미터 튜닝**의 손길을 완전히 벗어나진 못했습니다. 각 과제별로 성능을 끌어올리기 위해 연구진은 reward의 가중치를 조절하고, \$\alpha\$ 등의 파라미터(두 손 분배 비율)도 0.5로 설정하는 등 수동 조정이 있었습니다. 이런 부분은 범용 RL의 한계이기도 하지만, 궁극적으로 사용자 개입 없이도 학습될 수 있도록 하는 과제가 남아 있습니다.

요약하면, **AsymDex는 현재의 양손 RL 문제를 한 단계 전진시켰지만** 아직 모든 문제를 해결한 만능 솔루션은 아닙니다. 다만, 이러한 **한계들 자체가 향후 연구 방향**을 제시해 준다는 점에서 의미가 있습니다.

## 향후 확장 가능성 및 개선 방향

AsymDex의 등장으로 양손 강화학습의 새로운 가능성이 열렸지만, 동시에 앞서 열거한 한계들을 보완하며 더 발전시킬 여지가 많습니다. 앞으로의 연구 및 개선 방향을 몇 가지 짚어보겠습니다:

* **역할 유연성 및 대칭 작업 확장:** 가장 먼저, **비대칭성 가정 완화**가 도전과제로 꼽힙니다. 향후에는 AsymDex 구조를 확장하여 **두 손 모두 정교한 조작을 할 수 있는 상황**에도 대처할 수 있어야 합니다. 예를 들어, 두 손이 번갈아 주·보조 역할을 교대한다든지 (작업 중 동적으로 역할 변경), 혹은 애초에 두 손 다 부분적으로 물체를 움직이도록 허용하면서도 효율을 챙긴다든지 하는 방향입니다. 이는 정책 구조를 더 복잡하게 만들 수도 있지만, **옵션(policy) 교체** 같은 계층적 방법을 도입하면 한 정책이 상황에 따라 비대칭 모드 또는 대칭 모드로 전환하는 식으로 해결해 볼 수 있을 것입니다.

* **로봇 암(Arm) 제약 통합:** 현실 응용을 위해서는 손뿐 아니라 로봇 팔의 **운동학적 제약**도 고려해야 합니다. 향후 연구는 AsymDex와 **모션 플래너**를 결합하거나, 정책 자체가 **팔 관절 상태**까지 입력으로 받아 안전한 동작 영역 내에서 움직이도록 만들 수 있습니다. 예컨대 관절 한계나 충돌을 피하는 페널티를 추가하거나, 상대 좌표계 목표를 낼 때 물리적으로 실행 가능한지 검증하는 모듈을 둘 수 있습니다. 이렇게 하면 AsymDex의 아이디어를 실제 로봇 팔-손 시스템에 자연스럽게 녹여낼 수 있을 것입니다.

* **자연스럽고 인간다운 동작:** AsymDex로 얻은 정책이 때때로 **비직관적 움직임**을 보인다는 점을 개선하기 위해, **목표 함수를 다각화**할 수 있습니다. 예를 들어, **에너지 최소화**, **동작의 부드러움(smoothness)**, **동작 시간 최적화** 등의 요소를 보상에 추가하여, 사람처럼 효율적이고 매끄럽게 협업하도록 유도할 수 있습니다. 인간 데이터와의 **imitation regularizer**를 추가하는 것도 한 방법입니다. 궁극적으로 로봇 두 손이 인간 정도의 능숙함으로 물체를 다루려면, 성공률뿐 아니라 **품질**에 대한 학습도 필요할 것으로 보입니다.

* **광범위한 현실 검증:** 시뮬레이터를 넘어, 다양한 **현실 세계 시나리오**에서 AsymDex를 검증하는 연구가 예상됩니다. 다른 로봇 손이나 다른 물체로 실험해보면서, 얼마나 일반화되는지 파악할 필요가 있습니다. 또한 **센서 피드백** (예: 카메라 비전, 촉각 센서)을 통합하여, 완전한 **폐루프 제어**로 발전시키는 방향도 있습니다. 지금까지는 상태를 완벽히 안다고 가정했지만, 실제로는 로봇이 **관측의 불완전성**을 다루어야 하므로, 이에 맞는 POMDP 처리나 도메인 랜덤화 강화 등이 요구될 것입니다.

* **복잡한 작업으로의 스케일업:** 현재 과제들은 단일 작업(한 번의 조작)으로 완료되는 것들이었습니다. 앞으로는 AsymDex를 **계열의 작업**이나 더 복잡한 작업에 확장할 수 있습니다. 예를 들어, **조립 작업**에서도 여러 부품을 순차적으로 결합해야 하는 경우, 각 결합 단계마다 AsymDex를 활용하고 이를 상위 **계획 알고리즘**이 조직화하는 형태로 사용 가능할 것입니다. 또는 물체 **교환/handover** 시나리오에서, 먼저 AsymDex로 handover를 수행한 뒤 이어서 새로운 AsymDex 인스턴스를 통해 후속 조작을 하는 등 **모듈화**를 고려할 수 있습니다.

* **자동 보상 및 구조 검색:** 마지막으로, AsymDex가 수동 설계한 구조(비대칭+상대)를 사람이 아이디어로 준 것처럼, **메타학습**이나 **자동화된 구조 검색**을 통해 유사한 솔루션을 발견할 수 있을지도 모릅니다. 예컨대 강화학습 자체가 어떤 관절을 고정하고 어떤 좌표계를 쓰는 게 유리한지 탐색하게 하거나, 여러 후보 구조를 평가해 스스로 최적 구조를 채택하게 할 가능성입니다. 이는 난이한 연구 주제지만, 장기적으로 RL 에이전트가 **스스로 협응 전략을 발견**하도록 하는 궁극적 목표와 연결됩니다.

요약하자면, AsymDex는 **“강화학습으로 양손 다지 로봇의 섬세한 조작을 학습시킬 수 있다”**는 가능성을 힘있게 보여주었습니다. 동시에, 향후 연구자들이 파고들어야 할 여러 흥미로운 문제들도 부각시켰습니다. **비대칭성의 동적 활용, 상대 좌표계의 일반화, 실제 로봇 적용** 등의 방향으로 발전해 나간다면, 머지않아 로봇이 인간처럼 두 손을 능숙하게 사용하여 복잡한 작업을 해내는 모습을 볼 수 있을 것입니다. AsymDex는 그 토대가 되는 **구조적 접근법**을 제시한 의미있는 한 걸음이라고 평가할 수 있습니다.
