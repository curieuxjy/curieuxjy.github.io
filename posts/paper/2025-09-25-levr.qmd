---
title: "📃LeVR 리뷰"
date: 2025-09-25
categories: [residual, manipulation]
toc: true
number-sections: False
description: A Modular VR Teleoperation Framework for Imitation Learning in Dexterous Manipulation

---

- [Paper Link](https://arxiv.org/pdf/2509.14349)
- [Code Link](https://github.com/wengmister/LeFranX)

1. LeVR은 덱스터러스 핸드를 위한 가상 현실(VR) 텔레오퍼레이션과 강력한 LeRobot 모방 학습(IL) 프레임워크 간의 원활한 통합을 제공하는 모듈형 소프트웨어 프레임워크입니다.
2. 이 프레임워크의 오픈소스 구현인 LeFranX는 Franka FER 암과 RobotEra XHand에 적용되어 데이터 수집부터 정책 배포까지의 완벽한 엔드투엔드 워크플로우를 제공합니다.
3. 저지연 텔레오퍼레이션으로 수집된 공개 데이터셋을 통해 ACT 및 DP와 같은 최신 visuomotor policies를 성공적으로 fine-tuning하여 시스템의 효과를 입증했습니다.

<center>
<img src="../../images/2025-09-25-levr/01.png" width="70%" />
</center>

---

# Brief Review

LeVR은 로봇 모방 학습(imitation learning, IL)에서 두 가지 중요한 격차를 해소하기 위해 설계된 모듈형 소프트웨어 프레임워크입니다. 첫째, 섬세한 조작이 가능한 로봇 팔과 손을 위한 강력하고 직관적인 가상 현실(VR) 기반 원격 조작 데이터 수집 기능을 제공하여 기존 시스템의 한계를 극복합니다. 둘째, LeRobot IL 프레임워크와 기본적으로 통합되어 VR 기반 원격 조작 데이터를 활용하고 데모 수집 과정을 간소화합니다.

이 프레임워크는 일반적인 원격 조작 로직과 로봇 특정 확장 부분을 분리하는 모듈형 아키텍처를 특징으로 합니다.

1.  **Hand-tracking VR Interface**:
    *   Meta Quest의 **OpenXR Hand API**를 사용하여 손 움직임을 캡처합니다.
    *   27개의 랜드마크 골격 모델을 21개 관절의 **MediaPipe** 스타일 토폴로지로 변환합니다.
    *   초당 30Hz로 손목 포즈 ($T_{wrist,t} \in SE(3)$)와 21개 손 랜드마크 ($K_t = \{k_i \in R^3\}_{i=1}^{21}$)를 출력합니다.
    *   안정적인 데이터 스트림을 위해 유선 TCP 연결을 사용하며, 낮은 지연 시간으로 실시간 시각적 피드백(HUD 및 그림자 오버레이)을 제공합니다.

2.  **LeRobot Extension**:
    *   VR 인터페이스를 학습 스택과 통합하는 어댑터 레이어입니다.
    *   **Arm Branch - Differential Intent Inverse Kinematics**: 초기 및 현재 손목 포즈($T_{wrist,0}$, $T_{wrist,t}$)에서 차동 의도($\Delta p_{op}, \Delta q_{op}$)를 계산합니다. 이 의도는 로봇 베이스 프레임 변환 ($T_{base}^{op}$)을 통해 목표 말단 효과기 포즈($T_{ee}^{target}$)로 매핑됩니다. 이후 IK 솔버는 팔의 URDF와 관절 한계를 사용하여 관절 목표($q_{arm}^t$)를 계산합니다.
    *   **Hand Branch - Dexterous Retargeting**: 21개 키포인트 손 골격을 정규화한 후, DexPilot 스타일의 목적 함수를 사용하는 섬세한 리타겟팅 옵티마이저로 전달됩니다. 최적화 목적은 로봇 키네마틱스($v_{robot,i}(q_{hand}^t)$)와 인간 키네마틱스($v_{human,i}(K_t)$) 사이의 가중치 부여된 거리 차이를 최소화하는 것입니다. 이때 관절 한계와 모방 관절 제약이 고려됩니다. 수식은 다음과 같습니다.
        $$\min_{q_{hand}^t}\sum_i w_i \ell(v_{robot,i}(q_{hand}^t) - v_{human,i}(K_t)) \quad \text{s.t. } q_{min} \le q_{hand}^t \le q_{max}$$
        여기서 $\ell(\cdot)$은 Huber loss와 같은 강건한 손실 함수입니다. 이 과정은 관절 목표($q_{hand}^t$)를 출력하며, 선택적으로 EMA(Exponential Moving Average) 필터를 사용하여 궤적을 부드럽게 합니다.
    *   **LeRobot Integration**: 로봇 고유의 설정(예: 올바른 URDF 및 컨트롤러 인터페이스 로드)이 필요하지만, 로깅된 데이터셋과 정책의 관측-행동 인터페이스는 로봇 하드웨어를 변경해도 동일하게 유지됩니다.

3.  **Optional Robot I/O Server**: 로봇 SDK가 LeRobot과 직접 통합되지 않는 경우, 액션 명령을 전송하고 고유수용성 데이터를 수신하는 경량의 로봇 I/O 서버를 구현합니다. Franka FER 로봇의 경우 Franka Server가 30Hz 입력 명령과 FCI의 1kHz 제어 루프를 연결하기 위해 Ruckig[34]과 같은 온라인 궤적 생성기를 통합하여 부드럽고 동적으로 실현 가능한 관절 속도 명령을 생성합니다.

LeVR의 오픈소스 구현인 LeFranX는 Franka FER 로봇과 RobotEra XHand에 적용되었습니다. LeFranX는 Franka의 팔 움직임에 GeoFIK [32] 분석적 IK 솔버를 사용하여 빠르고 결정론적인 해를 얻고, 7자유도(DOF) 팔의 널 스페이스(null space)를 활용하여 중복성을 해결합니다. XHand의 경우, 적응형 스케일링 휴리스틱과 EMA 필터를 적용하여 자연스럽고 부드러운 움직임을 구현합니다.

저자들은 LeFranX 시스템을 사용하여 오렌지 큐브, 박스 파이, 빵 토스터 작업을 포함한 세 가지 복잡도 수준의 작업에 대한 공개 데이터셋(각 작업당 100개의 전문가 데모)을 수집했습니다. 이 데이터셋은 Apache Parquet 형식으로 저장되며, 시각 데이터는 동기화된 타임스탬프와 함께 압축된 MP4 비디오로 인코딩됩니다.

수집된 데이터셋을 사용하여** ACT (Action Chunking with Transformers) 및 DP (Diffusion Policy)** 두 가지 최신 **visuomotor 정책**을 LeRobot 프레임워크에서 미세 조정하고 평가했습니다. 실험 결과, LeFranX는 Open-Teach와 비교하여 작업 완료 시간을 크게 단축시키는 효율적인 원격 조작 성능을 보였으며, 수집된 데모 데이터는 단순한 작업에서 높은 성공률(ACT 80%, DP 60% - 오렌지 큐브 작업)을 달성하여 유능한 정책을 훈련하는 데 효과적임을 입증했습니다. 하지만 복잡한 작업에서는 성능이 저하되는 한계도 확인되었습니다.

결론적으로 LeVR은 현대 로봇 학습 파이프라인과 직접 통합되도록 설계된 모듈형 VR 원격 조작 프레임워크입니다. LeFranX 구현과 함께 공개된 데이터셋은 섬세한 조작 및 모방 학습 연구를 가속화하기 위한 강력한 플랫폼을 제공합니다.

---

# Detail Review

## 서론

LeVR는 로봇 모방 학습을 위한 모듈형 VR 텔레오퍼레이션 프레임워크이다. 기존 VR 조작 시스템이 복잡한 다지능 손(dexterous hand) 제어를 충분히 지원하지 못하고, 수집된 데모 데이터를 최신 IL(pytorch 기반 LeRobot 등) 파이프라인과 통합하기 어렵다는 두 가지 문제를 해결하고자 고안되었다. LeVR은 이 목표를 위해 VR 인터페이스와 로봇 제어를 분리된 모듈로 설계하였으며, Franka FER 로봇 팔과 RobotEra XHand를 대상으로 한 오픈소스 구현체 **LeFranX**를 공개했다. LeVR을 통해 VR 기반 시연(teleoperation) 데이터 수집에서부터 LeRobot 기반 정책 파인튜닝, 실제 로봇 배포까지 일관된 워크플로우를 제공하며, 실제 100회 이상의 전문가 시연 데이터를 공개하여 IL 연구 커뮤니티에 기여하고 있다.

### 시스템 구조 분석

LeVR 프레임워크는 모듈형 아키텍처로 설계되어, 일반 텔레오퍼레이션 로직과 로봇별 확장 부분을 명확히 분리한다. 전체 시스템 구성은 아래 그림과 같으며, 시연 수집 시 VR 인터페이스가 모션 데이터를 수집하여 LeRobot 확장 모듈로 전달하고, 로봇의 관절 위치/속도(프로프리오셉션)와 이미지 센서 스트림을 동기화하여 저장한다. 정책 수행 시에는 수집된 체크포인트를 불러와 직접 추론을 수행한다.

<center>
<img src="../../images/2025-09-25-levr/0.png" width="100%" />
</center>

- VR 핸드 트래킹 인터페이스: Meta Quest와 같은 소비자용 VR 디바이스의 OpenXR Hand API를 이용해 사용자의 손 동작을 추적한다. OpenXR API는 27개 랜드마크로 구성된 손 골격 모델을 출력하며, LeVR의 애플리케이션은 이를 21-자유도 핸드 토폴로지로 변환한다. 이때 매 프레임(30Hz)마다 손목 위치·쿼터니언 자세(T_wrist)와 21개 손 랜드마크(3D 좌표) 정보를 획득한다. 또한, 가상 헤드업 디스플레이(HUD)와 그림자 오버레이를 통해 사용자가 자신의 손 모션을 실시간으로 모니터링할 수 있도록 시각적 피드백을 제공한다. 데이터 전송은 유선 TCP 연결(ADB)을 통해 이루어지며, 무선 연결에 비해 낮은 지터로 고품질 데이터를 보장한다.

<center>
<img src="../../images/2025-09-25-levr/Screenshot from 2025-09-26 10-43-48.png" width="70%" />
</center>

- LeRobot 확장 모듈: LeVR은 LeRobot 학습 스택과의 원활한 통합을 위해 VR 입력을 LeRobot의 텔레오퍼레이터 인터페이스 형식으로 변환하는 어댑터 계층을 제공한다. 이 확장 모듈은 2개의 병렬 리타게팅(retargeting) 브랜치를 노출한다. 첫째 암(arm) 브랜치에서는 차분-의도(Differential Intent) 역기구학을 사용한다. 텔레오퍼레이션 시작 시 초기 손목 자세와 로봇 말단자(end-effector)의 자세를 기록하고, 매 타임스텝마다 사용자의 손 움직임 변화(∆p_op, ∆q_op)를 계산하여 이를 로봇 기준으로 변환한 목표 자세를 산출한다. 이를 ARM의 IK 솔버에 입력하여 팔 관절 각도(q_arm)를 산출한다. Franka 7-DOF 로봇의 경우, LeFranX 구현에서는 GeoFIK(Franka 전용 해석적 IK 솔버)을 사용하여 실시간 제어에 적합하도록 빠르게 해를 구하며, 중복 자유도(null space)를 활용하여 기구 조작성(manipulability), 중립 자세 근접성, 연속성 등을 최적화한다.

두 번째 **핸드(hand) 브랜치**에서는 Dexterous retargeting 최적화 문제를 풀이한다. VR 인터페이스로부터 얻은 21-키포인트 손 골격을 로봇 손 URDF와 관절 제한을 고려하여 최적화에 입력한다. DexPilot 방식과 유사하게, 사람 손의 중요 키포인트(예: 손가락 끝)의 위치가 로봇 손에서 최대한 보존되도록 하면서, 관절 범위(q_min≤q_hand≤q_max)를 만족시키며 손 관절 명령(q_hand)을 계산한다. 이렇게 계산된 12-DOF 로봇 손 관절 지시값은 EMA 필터 등을 거쳐 로봇에 전달된다.

<center>
<img src="../../images/2025-09-25-levr/Screenshot from 2025-09-26 10-43-21.png" width="70%" />
</center>


- 입출력 통합 및 데이터 파이프라인: LeRobot 확장 모듈은 일관된 관찰-행동(observation-action) 인터페이스를 제공한다. 관찰(observation)은 로봇 프로프리오셉션과 비전 카메라 영상 스트림으로 구성되며, 행동(action)은 {q_arm, q_hand} 형태의 19차원 관절 목표값으로 통합된다. 이 스키마는 로봇 하드웨어가 바뀌어도 데이터 형식을 유지하므로, 새로운 로봇 적용 시 URDF나 컨트롤러만 로드하면 된다. Franka 로봇의 경우, 실시간 제어를 위해 별도 로봇 I/O 서버를 사용한다. 이 서버는 클라이언트로부터 30Hz 명령을 받아 프랑카 1kHz 제어 루프에 적합한 토크/속도 명령으로 보간(예: Ruckig 사용)을 수행하며, 로봇의 관절 상태를 TCP로 반환한다.
- 데이터 수집 및 시뮬레이터: 본 연구에서는 실제 로봇 환경에서 시연 데이터를 수집하였다. Franka FER+XHand 시스템에 3대의 Intel RealSense D435 카메라(오버헤드, 3인칭, 손목 장착)를 배치하고, 30Hz 동기화하여 저장했다. 관절 상태 등 프로프리오셉션 정보는 Apache Parquet 형식으로, 고해상도 영상은 압축 MP4 영상으로 기록한다. 각 작업(task)마다 100회의 전문가 시연을 수집하였으며, 수집된 데이터는 LeRobot 프레임워크로 곧바로 파인튜닝에 사용되었다. 학습용 시뮬레이터 언급은 없으며, 정책 평가역시 실제 로봇에서 수행했다(LeRobot의 원격 서버를 통한 추론).

## 성능 평가

LeVR(LeFranX) 프레임워크는 **세 가지 대표 과제(Orange Cube, Boxed Pie, Bread Toaster)**에 대해 평가되었다. Orange Cube 과제는 주황색 큐브 집기-위치의 기본 동작, Boxed Pie는 상자 뚜껑 열기 포함 다단계 Pick and Place, Bread Toaster는 빵을 토스터에 넣고 레버를 누르는 복잡 과제다. 하드웨어 구성은 Franka FER+XHand, 로컬 워크스테이션과 원격 GPU 서버로 이루어졌다.

- **텔레오퍼레이션 효율성**: 숙련된 한 명의 조작자가 각 과제를 직접 수작업과 LeFranX VR 텔레오퍼레이션으로 수행한 후 평균 소요 시간을 비교했다(10회 반복). 결과(Table I)에서 LeFranX는 직접 조작 대비 약 2배의 시간이 소요되었지만, 기존 VR 방식(Open-Teach)보다 훨씬 빠른 속도를 보였다. 예를 들어 Orange Cube 과제에서 직접 조작은 3.2초인 반면 LeFranX는 6.3초에 완료되었으며, Open-Teach는 11.9초가 소요되었다. 복잡한 Boxed Pie와 Bread Toaster에서도 각각 5–6초(직접) vs 11–12초(LeFranX) vs 훨씬 긴 시간(기존)으로, LeFranX가 중간 성능을 보였다. 이는 LeFranX의 저지연(retargeting) 제어가 다지능 작업을 비교적 효율적으로 처리함을 시사한다.
- **모션 품질 및 학습 성능**: LeVR의 정밀한 리타게팅 덕분에 Boxed Pie 작업의 상자 뚜껑 열기 등 복잡한 동작이 가능해졌다. 반면 기존 Direct joint 맵핑 방식은 이런 작업을 수행하지 못했다. 수집된 3개 과제별 100회의 전문가 시연 데이터로 LeRobot 내의 최첨단 시각-운동 정책 ACT와 Diffusion Policy(DP)를 각각 100k 스텝 파인튜닝한 뒤, 무작위 초기상태에서 10회씩 자율 수행 실험을 진행했다. 그 결과 ACT 정책은 Orange Cube에서 10회 중 8회 성공, Boxed Pie 5회, Bread Toaster 4회를 달성했고, DP는 각각 6/10, 3/10, 1/10을 기록했다. 성공률은 과제 난이도 증가에 따라 감소하는 경향을 보였으며, ACT가 전반적으로 DP보다 안정적이었다. LeFranX에서 수집한 데이터는 비교적 양질의 학습 신호를 제공하여, 다소 제한적이나마 복잡한 작업까지 정책 학습에 활용할 수 있음을 확인했다. 단, 여전히 복잡 과제에서는 성공률이 낮아 모사 학습의 일반화 가능성에는 한계가 있음이 드러났다.

## 기존 연구와의 차별성

LeVR의 주요 혁신점은 다지능 손을 위한 **VR 조작과 IL 프레임워크 통합**을 동시에 추구했다는 점이다.

기존 VR 텔레오퍼레이션 연구(DexPilot[5], AnyTeleop[6] 등)는 정밀한 시각 기반 제어를 다루었으나, 대부분 엔드이펙터가 간단한 그리퍼 수준이거나 특정 하드웨어에 종속적이다. LeVR은 **Meta Quest 같은 저가 VR 장비와 OpenXR**으로 사람 손 전체를 추적하여 27개 랜드마크를 뽑아낸 뒤 이를 로봇의 21-DOF 손 동작으로 변환함으로써, 일반 소비자용 VR로도 다지능 조작이 가능함을 보여주었다. 또한 LeVR은 LeRobot 학습 파이프라인과의 완전 통합을 기본 설계로 삼아, VR 데이터로 직접 시각-운동 정책을 학습할 수 있도록 한다. 즉, 사용자 정의 전처리 없이 LeRobot 인터페이스에 바로 연결되며, 이는 기존 대부분의 방법들이 VR 데이터 형식을 수동으로 맞추어야 했던 점에 비해 큰 장점이다.

또한, LeVR은 모듈성과 오픈소스 구현을 내세웠다. LeFranX 구현에서는 Franka 팔과 XHand를 독립적인 로봇 클래스로 캡슐화하여 쉽게 조합할 수 있게 함으로써, 다양한 팔-손 조합을 지원할 수 있다. 코드는 공개되어 있어 연구자들이 즉시 LeRobot과 함께 사용하거나 자신들의 하드웨어에 맞게 수정할 수 있다. 이러한 일관된 워크플로우는, 기존 연구들이 여러 툴과 계층을 잇는 복잡한 과정이 필요했던 것과 달리, 비교 대상 없는 완성된 파이프라인을 제공한다는 점에서 차별적이다.

## 종합적 평가 및 한계점

LeVR/LeFranX는 원활한 데이터 수집에서 학습까지의 엔드-투-엔드 워크플로우를 제시함으로써, 로봇 모방 학습 연구에 유용한 리소스를 제공했다. 모듈형 설계와 공개된 데모 데이터셋(과제당 100회 시연) 덕분에, 다른 연구자들도 LeRobot 기반 정책 평가나 알고리즘 비교를 손쉽게 수행할 수 있다. 한편, LeVR에도 몇 가지 한계가 있다. 우선, 실험 결과 실제 시연에 비해 텔레오퍼레이션 시간이 약 2배 길어지는 것으로 나타났다. 이는 인간 손-로봇 팔의 기구학적 불일치, 팔과 손 관절의 관성·속도 제한, 그리고 햅틱 피드백이나 시점 카메라 부재에 기인한다. 특히 조작자가 물체와 접촉할 때 힘 피드백이 없어 정확한 컨트롤에 한계가 있었다. 또한, 상용 VR 기기의 추적 지연(latency)으로 인해 실시간 제어 품질이 일부 저하되고, 복잡 과제에서는 정책 수행 성공률이 크게 떨어졌다. 향후 연구에서는 이러한 문제를 보완할 수 있는 기능 추가가 필요하다. 예를 들어 햅틱 장갑이나 햅틱 장치를 도입해 조작자의 촉각 피드백을 제공하거나, 더 고성능의 추적 시스템(저지연 센서 또는 FPGA 기반 처리)으로 반응성을 높일 수 있다. 또한 다양한 작업과 다수 사용자에 대한 평가가 추가되어야 한다. 그럼에도 불구하고 LeVR의 오픈된 프레임워크와 데이터는 다지능 로봇 조작 연구를 촉진할 기반을 마련했다는 점에서 중요한 기여로 평가된다.
