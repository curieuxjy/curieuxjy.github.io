---
title: "📃Spin pens 리뷰"
date: 2025-08-01
categories: [in-hand, rl, spin-task]
toc: true
number-sections: false
description: Lessons from Learning to Spin “Pens”
---

- [Paper Link](https://arxiv.org/abs/2407.18902)
- [Project Link](https://penspin.github.io/)
- [Github Link](https://github.com/HaozhiQi/penspin)

1. 이 연구는 고품질 데모 부족과 시뮬레이션-실제(sim-to-real) 간의 큰 격차로 인해 어려웠던 펜과 유사한 물체를 로봇 손으로 연속적으로 회전시키는 최초의 학습 기반 시스템을 제안합니다.
2. ⚙️ 저자들은 먼저 강화 학습을 통해 시뮬레이션에서 오라클 정책(oracle policy)을 훈련하여 고충실도 궤적 데이터를 생성하고, 이 데이터를 실제 로봇의 오픈루프 제어(open-loop control)에 사용하여 고품질 실제 세계 데모를 수집했습니다.
3. 💡 수집된 실제 세계 궤적 50개 미만으로 사전 훈련된 센서모터 정책(sensorimotor policy)을 미세 조정하여 다양한 펜 유사 객체를 여러 번 회전시킬 수 있었으며, 이는 복잡한 인핸드 조작(in-hand manipulation)에서 시뮬레이션 사전 훈련과 실제 세계 미세 조정의 중요성을 입증했습니다.


<center>
<img src="../../images/2025-08-01-spin-pens/PenSpin.gif" width="80%" />
<img src="../../images/2025-08-01-spin-pens/teaser.gif" width="80%" />
</center>

---

# Brief Review

본 논문은 펜과 같은 물체를 손 안에서 회전시키는 인핸드 조작(in-hand manipulation) 기술을 학습하는 새로운 접근 방식을 제안한다. 기존 학습 기반 방법들은 고품질 데모 데이터의 부족과 시뮬레이션(Sim)과 실제(Real) 환경 간의 상당한 격차(Sim-to-Real gap)로 인해 이러한 동적이고 복잡한 작업 수행에 어려움을 겪었다. 본 연구는 강화 학습(Reinforcement Learning, RL)을 통해 시뮬레이션에서 '오라클 정책(oracle policy)'을 훈련하고, 이를 통해 고품질의 궤적 데이터셋을 생성한다. 이 데이터셋은 두 가지 목적으로 사용되는데, 첫째, 시뮬레이션에서 '센서모터 정책(sensorimotor policy)'을 사전 훈련하는 데 사용되고, 둘째, 실제 로봇에서 '오픈 루프 궤적 리플레이(open-loop trajectory replay)'를 수행하여 실제 데모 데이터를 수집하는 데 사용된다. 마지막으로, 실제 환경에 정책을 적응시키기 위해 이 실제 궤적들을 사용하여 센서모터 정책을 미세 조정(fine-tune)한다.

**핵심 방법론**

1.  **오라클 정책 훈련 (Oracle Policy Training):**
    고품질의 현실적인 궤적 데이터를 얻기 위해 시뮬레이션 환경에서 오라클 정책을 강화 학습으로 훈련한다. 이 정책은 실제 세계에서는 접근할 수 없는 '특권 정보(privileged information)'를 활용한다.
    *   **관측값($\mathbf{o}_t$)**: 로봇의 현재 관절 위치($\mathbf{q}_t$), 이전 관절 목표($\mathbf{a}_{t-1}$), 이진 촉각 신호($\mathbf{c}_t$), 손끝 위치($\mathbf{p}_t$), 펜의 현재 자세 및 각속도($\mathbf{w}_t$), 그리고 펜의 포인트 클라우드($\in \mathbb{R}^{100 \times 3}$)를 포함한다. 포인트 클라우드는 PointNet [58]을 사용하여 인코딩된다. 또한, 특권 정보로 펜의 질량(mass), 질량 중심(center of mass), 마찰 계수(coefficient of friction), 물체 크기(object size)와 같은 물리적 속성이 입력으로 포함된다. 과거 3단계의 관절 위치와 목표가 스택되어 입력으로 사용된다.
    *   **행동($\mathbf{a}_t$)**: 정책 네트워크 $f(\mathbf{o}_t)$가 제공하는 상대 목표 위치이며, $\mathbf{a}_t = \eta f(\mathbf{o}_t) + \mathbf{a}_{t-1}$와 같이 이전 목표에 스케일링된 행동을 더하여 계산된다. 이 목표는 저수준 PD 제어기(PD controller)를 통해 로봇에 토크로 변환되어 전달된다.
    *   **Reward Function($r$)**: 펜의 Z축을 중심으로 연속적인 회전을 목표로 하며, 다음과 같이 정의된다.
        $r = r_{rot} + \lambda_z r_z + \lambda_{energy} r_{energy}$
        여기서 $r_{rot}$는 펜의 회전 속도를 보상하고, $\lambda_z r_z$는 펜의 가장 높은 점과 낮은 점 사이의 높이 차이에 페널티를 부여하여 펜을 수평으로 유지하도록 유도한다. $\lambda_{energy} r_{energy}$는 물체의 선형 속도, 초기 관절 위치로부터의 편차, 기계적 작업량, 적용된 토크에 대한 페널티를 포함한다.
    *   **초기 상태 설계**: 기존 연구와 달리, 무작위 샘플링 대신 인간의 펜 돌리기에서 영감을 받은 6가지 '정형화된 초기 파지(canonical initial poses)'를 수동으로 설계하고, 잡음(noise)을 추가하여 안정적인 초기 상태를 생성 및 필터링한다. 이는 정책 훈련 시 유의미한 탐색을 촉진하는 데 중요하다.
    *   **정책 최적화**: PPO(Proximal Policy Optimization) 알고리즘을 사용하여 오라클 정책을 훈련한다. 정책 및 가치 네트워크는 MLP(Multi-Layer Perceptron)로 구성되며, 도메인 무작위화(domain randomization)가 적용된다.

2.  **센서모터 정책 사전 훈련 (Sensorimotor Policy Pre-training):**
    오라클 정책은 특권 정보 없이는 실제 환경에 직접 배포할 수 없다. 이전 연구의 DAgger와 같은 직접적인 증류(distillation) 방식은 본 작업의 동적이고 접촉이 많은 특성으로 인해 시뮬레이션과 실제 환경 간의 큰 불일치(mismatch)로 인해 제대로 작동하지 않았다. 특히 촉각-시각 피드백(visuotactile feedback)을 사용하는 정책은 시뮬레이션에서 합리적인 성능을 보였으나, 실제 환경에서는 큰 갭에 직면했다. 반면 고유수용성 피드백(proprioceptive feedback)만을 사용하는 정책은 시뮬레이션에서도 수렴하지 못했다.
    본 연구에서는 시뮬레이션에서 오라클 정책 $f$를 실행하여 고유수용성 상태와 행동의 데이터셋 $(\mathbf{s}_t, \mathbf{a}_t)$을 수집한다. 이 데이터셋은 시뮬레이션에서 고유수용성 정책을 사전 훈련하는 데 사용된다. 이 단계의 목표는 센서모터 정책을 다양한 훈련 데이터에 노출시켜 '움직임 사전 지식(motion prior)'을 제공하는 것이다. 이 사전 훈련은 정책이 부정확한 역학으로 인해 실제 세계로 직접 전이되지는 않지만, 실제 세계 궤적으로 효율적으로 미세 조정될 수 있는 기반을 마련한다. 고유수용성 정책은 30단계의 관절 위치($\mathbf{q}_{t-29:t}$)와 이전 관절 목표($\mathbf{a}_{t-30:t-1}$)를 입력으로 받으며, 시간적 트랜스포머(temporal transformer)를 사용하여 순차적 특징을 모델링하고 MLP를 정책 네트워크로 사용한다.

3.  **오라클 리플레이를 통한 센서모터 정책 미세 조정 (Fine-tuning Sensorimotor Policy with Oracle Replay):**
    큰 Sim-to-Real 갭을 극복하고 정책을 실제 역학에 적응시키기 위해 실제 궤적을 사용한다. 그러나 동적인 펜 회전 작업은 텔레오퍼레이션으로 고품질 데모를 수집하기 매우 어렵다. 본 연구의 핵심 관찰은 오라클 정책이 직접 전이되지는 않지만, 텔레오퍼레이션으로는 생성하기 어려운 움직임 시퀀스를 제공한다는 점이다. 이에 영감을 받아, 시뮬레이션에서 생성된 오라클 정책의 궤적을 실제 로봇에서 '오픈 루프 제어기(open-loop controller)'로 활용한다.
    구체적으로, 오라클 정책 훈련 후, 시뮬레이션 환경에서 15개의 800 타임스텝 이상 지속되는 궤적을 선택한다. 이 행동 시퀀스를 세 가지 훈련 물체(training objects)에 대해 실제 로봇에 리플레이한다. 이 오픈 루프 제어기가 물체를 $2\pi$ 라디안 이상 회전시키는 데 성공하면, 해당 궤적을 실제 데이터셋에 저장한다. 이 과정을 물체당 15개의 궤적(총 45개)이 수집될 때까지 반복한다. 이 방식으로 수집된 데이터는 정책의 보상 정의에 따른 부드러운 움직임과 행동 데이터를 모두 포함한다는 장점이 있다. 사전 훈련된 고유수용성 정책은 이 소량의 실제 데이터셋(50개 미만의 궤적)으로 미세 조정되어 실제 역학에 적응한다.

**실험 및 결과**

*   **오라클 정책 훈련**: 잘 설계된 초기 상태 분포(6가지 정형화된 파지)가 손가락 보행(finger gaiting)의 출현에 결정적이며, 단일 초기 자세로는 한 번의 회전도 달성하기 어렵다. 촉각 피드백, 포인트 클라우드, 물체 물리적 속성과 같은 특권 정보는 정책 수렴에 필수적이다. Z축 보상($r_z$)은 펜이 수평을 유지하며 안정적으로 회전하도록 돕고, 이는 실제 리플레이의 성공률을 높인다. $r_z$ 없이는 펜이 기울어져 불안정해진다.
*   **센서모터 정책 배포**: 오라클 리플레이 자체는 실제 환경에서 합리적인 성능을 보이나, 본 연구의 미세 조정된 정책이 훈련 물체에서 15-30% 더 높은 성공률을 보이며, 미학습 물체(unseen objects)에서도 10% 더 긴 회전량을 달성하는 등 더 우수한 성능을 나타낸다. 이는 시뮬레이션 사전 훈련을 통한 다양한 데이터 학습의 이점을 보여준다. DAgger 방식의 직접적인 증류는 Sim-to-Real 갭이 너무 커서 펜 회전 작업에는 실패한다. 사전 훈련과 미세 조정은 모두 필수적인 구성 요소이다. 사전 훈련만으로는 시뮬레이션-실제 간의 물리적 갭으로 인해 실제 환경에서 효과가 제한적이며, 미세 조정만으로는 제한된 실제 데이터에 과적합되어 일반화 능력이 떨어진다. 실제 데모 수를 15개에서 75개로 늘려도 사전 훈련을 대체할 수는 없으며, 특히 미학습 물체에 대한 일반화 성능은 여전히 본 연구 방법보다 훨씬 낮게 나타났다.

**결론 및 교훈**

본 연구는 학습 기반 시스템으로서는 최초로 펜과 유사한 물체를 연속적으로 회전시키는 데 성공했다. 이를 통해 다음과 같은 교훈을 얻었다.

*   시뮬레이션 훈련에서는 탐색(exploration)을 돕기 위한 적절한 초기 분포 설계와 정책 학습을 촉진하기 위한 특권 정보 사용 등 광범위한 설계가 필요하다.
*   접촉이 많고 고도로 동적인 작업의 경우, Sim-to-Real은 직접적으로 작동하지 않는다. 순수 물리적 Sim-to-Real 갭은 광범위한 도메인 무작위화만으로는 메우기 어렵다.
*   시뮬레이션은 인간의 텔레오퍼레이션으로는 거의 불가능한 펜 돌리기와 같은 동적인 기술을 탐색하는 데 매우 유용하다.
*   시뮬레이션에서 학습된 고유수용성 정책이 실제 환경에서 직접 작동하지 않더라도, 소수의 성공적인 실제 궤적만으로도 실제 역학에 적응하도록 효과적으로 미세 조정될 수 있다.

**한계점**: 현재 시스템은 Z축을 중심으로 한 회전만 가능하며, 물체가 안정적인 파지 위치에 놓여있음을 가정한다. 향후 연구에서는 다축 회전으로의 확장과 고급 파지(grasping) 기술의 통합을 목표로 한다.

---

# Detail Review

> 로봇이 펜을 돌리는 법을 배우다 – **"Lessons from Learning to Spin 'Pens'"** 논문 리뷰

## 들어가며

사람들은 손가락으로 펜을 돌리는 묘기를 종종 부리곤 합니다. 이는 겉보기엔 단순해 보여도, **로봇 손**에게는 상당히 어려운 정교한 **손 내 조작(in-hand manipulation)** 기술입니다. 펜과 같이 길쭉한 물체를 손 안에서 자유롭게 회전시키는 능력은 재미 이상의 의미를 갖습니다. 망치나 드라이버처럼 길쭉한 도구들을 다루는 동작과 유사하기 때문에, 로봇이 이러한 **펜 돌리기**를 해낼 수 있다는 것은 다양한 도구 사용의 기초 기술을 확보하는 셈이기도 합니다. 하지만 지금까지 **강화학습** 등 **학습 기반 방법**으로 이러한 작업을 달성하기란 매우 어려웠습니다. 무엇보다 사람처럼 시범을 보여주기도 힘들고, 시뮬레이션과 실제 로봇 사이의 물리 차이(시뮬레이션-현실 격차)도 커서, 가상훈련 성과를 현실에 옮기기 어려웠기 때문입니다. 이번 리뷰에서는 2024년 CoRL(로봇학습 콘퍼런스)에 발표된 **"Lessons from Learning to Spin 'Pens'"** 논문을 깊이 있게 살펴보고자 합니다. 이 연구는 **강화학습 기반의 새로운 접근법**으로 로봇 손이 펜과 유사한 물체를 손가락 사이에서 **여러 바퀴 회전**시키는 데 성공했고, 그 과정에서 얻은 통찰과 한계를 공유하고 있습니다. 본 포스트에서는 해당 논문의 주요 내용을 요약하고, 사용된 기법과 모델의 기술적 요소를 설명하며, 얻어진 교훈과 남은 과제를 함께 분석해보겠습니다.

## 배경 및 도전 과제

로봇의 **손 내 조작** 기술은 인간 수준의 섬세함을 달성하기 위해 필수적인 연구 주제입니다. 과거에도 로봇 손으로 물체를 돌리거나 재배열하는 시도가 없었던 것은 아니지만, **펜 돌리기처럼 연속적이고 역동적인 회전 동작**은 특히 난제가 되었습니다. 기존 학습 기반 기법들이 이 문제에 부딪혀온 주된 이유는 두 가지입니다. 첫째, 이러한 고난도 작업에 대한 **고품질 시演(데몬스트레이션)** 데이터를 얻기가 어렵습니다. 사람 손의 섬세한 움직임을 모방하거나 원격 조종(텔레오퍼레이션)을 통해 로봇에 시범을 가르치려 해도, 펜 돌리기의 복잡한 동작을 **정확히 재현**하기가 거의 불가능에 가깝습니다. 둘째, **시뮬레이션-현실 간의 차이(sim-to-real gap)**가 매우 크다는 문제입니다. 시뮬레이터 상에서 로봇 손가락이 펜을 돌리는 데 성공하더라도, 실제 로봇에 동일한 정책(policy)을 이식하면 마찰 계수, 물체의 미세한 물리 특성 차이, 센서 오차 등으로 인해 펜을 금세 떨어뜨리기 일쑤입니다. 특히 펜 돌리기처럼 **접촉이 연속적으로 발생하고 매우 역동적인 작업**의 경우 이 격차는 더욱 심해져서, 단순한 **도메인 랜덤화** 등으로는 메우기 어렵다는 것이 선행 연구들의 교훈이었습니다.

연구팀 역시 초기에 **시뮬레이션에서 학습한 정책을 바로 현실 로봇에 이식**해 보거나, 하드웨어 구조와 물체 재질을 바꿔보는 등 여러 시도를 했지만 번번이 실패를 겪었습니다. 펜은 이내 손가락 사이에서 미끄러지거나 튕겨 나가 떨어졌고, 시뮬레이터와 현실 사이의 **분포 차이(distribution shift)**를 실감해야 했습니다. 그렇다면 이런 어려운 문제를 어떻게 풀 수 있을까요? 이 논문의 핵심은 "**시뮬레이션의 힘을 최대한 활용하면서도, 최소한의 현실 데이터로 격차를 메우는 하이브리드 학습 전략**"에 있습니다. 다음 섹션에서는 저자들이 제안한 독창적인 접근 방법을 살펴보겠습니다.

## 접근 방법: 시뮬레이션 오라클과 현실 적응

**"Lessons from Learning to Spin 'Pens'"** 논문의 저자들은 **시뮬레이션**과 **현실 데이터**를 단계적으로 활용하는 학습 파이프라인을 고안하였습니다. 이 접근법은 크게 세 단계로 요약할 수 있습니다:

1. **시뮬레이션에서의 오라클 정책 학습** – 우선 가상 환경에서 펜 돌리기 문제를 충분히 탐색할 수 있는 **오라클 정책(oracle policy)**을 강화학습으로 훈련합니다. 여기서 오라클 정책이란 시뮬레이터가 제공하는 **특권 정보(privileged information)**를 모두 활용하는 **전지전능한 정책**입니다. 예를 들어 실제 로봇은 카메라나 촉각 센서로만 펜의 상태를 추정해야 하지만, 오라클 정책은 시뮬레이션이 제공하는 **펜의 정확한 위치와 속도 등 완전한 상태 정보**를 관측으로 사용할 수 있습니다. 이러한 추가 정보 덕분에 강화학습 에이전트는 탐색을 빠르게 진행하며 **성공 궤적**들을 만들어낼 수 있습니다. 오라클 정책은 결국 펜을 연속 회전시키는 훌륭한 전략을 익히게 되었고, 이를 통해 **정밀한 시뮬레이션 궤적 데이터셋**을 다수 확보할 수 있었습니다. 논문에 따르면, 이 단계에서 생성된 궤적들은 펜을 돌리는 손가락 동작의 **고해상도 시나리오**들을 담고 있어 이후 과정에 핵심적인 밑거름이 됩니다.

2. **학생 정책 학습 및 열린 루프 실행** – 다음으로, 시뮬레이션에서 수집된 궤적 데이터를 활용하여 **학생(sensorimotor) 정책**을 학습시킵니다. 학생 정책은 실제 로봇에 투입될 **센서 기반 정책**으로, 오라클과 달리 특권 정보 없이 **로봇이 실제로 사용할 수 있는 센서 신호**만으로 동작하도록 설계됩니다. 이를 위해 오라클의 시뮬레이션 궤적을 **모방 학습(behavior cloning)**이나 **지도학습**을 통해 학생 정책이 따라하도록 훈련합니다. 이렇게 초기화된 학생 정책은 시뮬레이션 상에서는 펜을 돌리는 방법을 알고 있지만, 여전히 현실 환경에서 바로 쓸 수 있을 정도로 **견고하지는 않은 상태**입니다. 따라서 연구팀은 한 걸음 더 나아가, 이 학생 정책을 **현실 로봇에 적용하여 열린 루프(open-loop)로 실행**해 보았습니다. 여기서 열린 루프 실행이란, **시뮬레이션에서 녹화한 동작 액션 시퀀스를 그대로 로봇에 재생**하는 것으로, 실행 도중에 별도의 피드백 보정 없이 **고정된 액션 궤적을 따라가는 것**을 의미합니다. 놀랍게도 오라클 정책으로부터 학습한 학생 정책은 실제 로봇에서도 **일정 수준의 펜 회전**을 만들어냈고, 특히 그 중 일부 시도에서는 펜을 여러 차례 회전시키는 **성공 사례**들도 얻을 수 있었습니다. 연구진은 이러한 성공적인 현실 궤적 데이터를 추가로 수집하여 다음 단계에 활용합니다.

3. **현실 데이터로 정책 미세조정** – 마지막 단계에서는, 방금 확보한 **현실 세계의 성공 궤적 데이터**(50개 미만의 비교적 적은 수라고 합니다)를 가지고 학생 정책을 **파인튜닝(fine-tuning)**합니다. 이는 일종의 **도메인 적응** 단계로, 시뮬레이션 전용 정책이 현실의 물리 법칙과 잡음에 적응하도록 도와줍니다. 구체적으로, 학생 정책을 현실 궤적 데이터에 대해 다시 한 번 모방 학습하거나, 필요에 따라 추가 강화학습을 진행하여 **현실 물리에 맞게 보정**한 것입니다. 이렇게 함으로써 최종 정책은 비로소 **현실 환경에서 펜을 안정적으로 돌릴 수 있는 능력**을 얻게 됩니다. 결과적으로 단 **50개 미만의 현실 궤적**만으로도, 시뮬레이터 속에서만 통하던 정책이 현실의 오차와 마찰을 견디며 펜을 돌릴 수 있게 된 것이죠. 더욱이 이 최종 정책은 **물리적 속성이 제각각인 10여 종의 펜 모양 도구**들에 대해서도 모두 수 차례 연속 회전을 성공적으로 구현해냈습니다.

<center>
<img src="../../images/2025-08-01-spin-pens/1.png" width="100%" />
</center>

> **그림 1:** 본 연구에서 제안된 학습 파이프라인 개략도. 왼쪽에서는 시뮬레이션에서 오라클 정책을 강화학습으로 훈련하여 **고품질 궤적 데이터**를 생성하고, 이 데이터로 학생 정책을 사전 학습합니다. 그 후 그 학생 정책을 실제 로봇에 **열린 루프 제어**로 실행하여 성공 사례 **현실 궤적**을 수집합니다. 마지막으로 해당 현실 궤적으로 학생 정책을 미세 조정하여 현실 환경에 적응된 최종 정책을 얻습니다. 이 파이프라인을 통해 **시뮬레이션의 탐색 능력**과 **현실 데이터의 정확성**을 결합함으로써, 순전히 인간 시범이나 순전한 시뮬레이션으로는 불가능했던 **펜 돌리기** 과제를 달성할 수 있었습니다.

## 실험 결과: 펜 돌리기의 달성 및 분석

그렇다면 이러한 접근법으로 얻은 **최종 로봇 정책**은 실제로 어느 정도 성과를 거두었을까요? 논문에 따르면, 불과 50개 미만의 현실 성공 사례 데이터로 미세조정한 정책임에도 불구하고, 로봇 손은 다양한 펜 모양의 물체를 손가락 사이에서 **여러 바퀴 연속으로 회전**시키는 데 성공했습니다. 여기에는 플라스틱 볼펜, 마커, 나무 막대 등 **물리적 특성(무게, 마찰, 균형)**이 서로 다른 10여 개의 물체들이 포함되어 있었는데, 정책은 이들 **펜-유사 물체(pen-like objects)** 각각에 대해 안정적으로 회전 동작을 구사했습니다. 이는 단순히 하나의 물체에 특화된 솔루션이 아니라, **일반화된 펜 돌리기 기술**을 습득했음을 보여줍니다.

또한 흥미로운 점은, **강화학습 단계에서의 설계 선택이 실제 결과에 큰 영향**을 미쳤다는 것입니다. 저자들은 시뮬레이션 상의 오라클 정책을 학습할 때 여러 가지 보상 설계와 제약 조건을 실험했는데, 이를 통해 **성공적인 펜 돌리기에는 어떤 요소가 중요한지**를 분석했습니다. 예를 들어, 오라클 정책을 훈련할 때 **손가락 자세를 하나의 고정된 포즈**로만 사용하도록 제한한 경우(일종의 **단일 자세 제약** 실험)에는 에이전트가 펜을 길게 돌리지 못했습니다. 이때는 손가락의 위치를 바꾸는 **핑거게이팅(finger gaiting)**이 나타나지 않아, 탐색이 비효율적이고 결국 연속 회전에 실패했습니다. 반면 **우리 방법(제안된 방법)**에서는 에이전트가 학습을 통해 **자발적으로 손가락을 끊어 움직이며(pingergaiting)** 펜을 계속 회전시키는 동작을 터득했고, 그 결과 **지속적인(spinning) 연속 회전**이 가능해졌습니다. **핑거게이팅**은 마치 사람이 손가락을 번갈아 가며 물체를 옮겨 쥐는 동작에 비유될 수 있는데, 로봇 정책이 이런 행동을 스스로 학습했다는 것은 매우 고무적인 성과입니다.

또 다른 분석 요소로는 **Reward Function의 구성**이 있었습니다. 연구팀은 펜을 돌리는 강화학습 보상에 특별한 항목을 하나 추가했는데, 바로 **"Z-축 보상(Z-reward)"**입니다. 이는 펜이 회전 중에 기울어지지 않고, 일정 수준 이상 높이를 유지하도록 유도하는 보상으로 해석됩니다. 이 보상의 중요성은, **Z-보상을 제외한 실험**에서 드러났습니다. 해당 실험에서는 시뮬레이션 상에서는 펜을 돌릴 수 있었지만, 일정 시간 이후 펜이 기울어지면서 결국 손가락 사이에서 빠져버리는 문제가 관찰되었습니다. 이러한 **기울어짐(tilt)** 현상은 특히 현실 로봇에서 더 치명적이어서, Z-보상이 없는 정책은 실제 테스트 시 펜을 쉽게 떨어뜨렸다고 합니다. 따라서 펜을 안정적으로 여러 바퀴 돌리려면, 회전 속도나 횟수뿐만 아니라 **자세 안정성(orientation stability)** 역시 중요하다는 교훈을 얻었습니다.

연구진은 제안한 방법의 효과를 검증하기 위해 여러 **비교 실험(베이스라인)**도 수행하였습니다. 첫째, **시뮬레이션에서 학습된 정책을 바로 사용(시뮬레이션 사전학습 only)**하는 방안을 시험했는데, 예상대로 **시뮬레이션-현실 간 물리 차이**를 극복하지 못해 펜 돌리기에 실패했습니다. 둘째, **시뮬레이션 궤적을 열린 루프**로만 재생하는 단순 모방 방식의 경우, 동작 자체는 그럴듯하지만 **센서 피드백이 전혀 없기 때문에** 작은 불확실성에도 금방 실패로 이어졌습니다. 즉, 이러한 오픈 루프 재생은 **재활용 불가능한 일회성 묘기**에 그치고 일반화된 정책이 될 수 없었습니다. 마지막으로, **비전(distillation) 기반 접근**도 검토되었는데, 이는 아마도 시뮬레이션에서 **시각적 관찰**로 학습한 정책을 distill하여 현실에 적용하려 한 시도였습니다. 그러나 이 경우 물체(펜)가 회전하며 움직일 때 **카메라 기반 인식에 오차**가 커지고, **훈련 분포에서 벗어난(OOD) 시각 정보**가 입력되면서 정책 성능이 불안정해졌습니다. 결과적으로 물체가 화면에서 **흔들려 보이는(oscillates)** 상황에서 비전 정책은 큰 에러를 일으켰고, 펜 돌리기를 유지하지 못했습니다. 이러한 비교 실험들은 본 논문의 **혼합 접근법**이 왜 필요한지 잘 뒷받침해줍니다. 요컨대, **시뮬레이션 학습만으로는 부족하지만**, 그렇다고 **현실 데이터만으로 처음부터 학습하기엔 탐색이 불가능한** 이 딜레마 상황에서, **시뮬레이션의 성공 경험을 최대한 활용하고 최소한의 현실 경험으로 보완**하는 것이 핵심이라는 점을 입증한 셈입니다.

## 배운 교훈 및 한계

이 연구를 통해 얻은 교훈(lessons)들은 펜 돌리기 과제에만 국한되지 않고, 일반적인 **로봇 강화학습과 시뮬레이션 활용**에 시사하는 바가 큽니다. 저자들은 논문에서 개발 과정에서 느낀 주요 교훈을 다음과 같이 정리하였습니다:

* **탐색을 위한 시뮬레이션 설계의 중요성:** 시뮬레이션에서 강화학습을 성공시키려면 **탐색이 충분히 이루어지도록 환경과 보상을 꼼꼼히 설계**해야 합니다. 초기 상태 분포를 적절히 다양하게 만들어 에이전트가 여러 상황을 접하게 하고, 학습을 돕는 **특권 정보**를 활용하는 등 세심한 디자인이 필요했습니다. 이러한 노력이 뒷받침되어야 비로소 시뮬레이터 상에서 어려운 기술이 발현될 수 있다는 것입니다.

* **단순한 Sim-to-Real은 통하지 않는다:** 접촉이 많은 고난이도 동작일수록, 시뮬레이션 결과를 그대로 현실에 가져오는 것은 거의 실패한다고 볼 수 있습니다. 연구진이 **촉각, 시각 센서 등을 개별적으로 배제해보는 등** 여러 방법을 시도해봤지만, **물리 엔진과 실제 세계의 근본적 차이**는 남았습니다. 광범위한 **도메인 랜덤화**조차도 이 격차를 완전히 메꾸지 못했고, 결국 **현실 데이터의 직접적인 활용**이 불가피하였습니다.

* **그래도 시뮬레이션은 유용하다:** 비록 시뮬레이션 결과만으로 완성품을 얻을 순 없지만, **시뮬레이터는 여전히 새로운 기술을 탐색하는 데 필수적**입니다. 펜 돌리기와 같은 **역동적 스킬**은 인간이 로봇을 원격으로 조종하며 가르치기에는 거의 불가능에 가깝습니다. 이런 경우 **강화학습을 통해 시뮬레이션에서 성공 사례를 찾아내는 과정**이 있었기에, 초기 정책과 궤적을 확보할 수 있었습니다. 이는 다른 복잡한 로봇 기술 학습에도 시사하는 바가 있으며, 시뮬레이션을 **탐색 도구**로 적극 활용하되 그 한계를 인지하는 균형 잡힌 접근이 필요합니다.

* **현실 데이터는 생각보다 적게 필요하다:** 희망적인 소식은, 시뮬레이터에서 학습한 정책을 잘 활용하면 **소량의 현실 성공 데이터로도 충분히 정책을 보정**할 수 있다는 점입니다. 이번 연구에서는 불과 50여 개 미만의 실행 궤적으로도 정책을 **현실에 적응(fine-tuning)**시킬 수 있음을 보여주었습니다. 즉, 시뮬레이션으로 기본기를 익혀 놓으면 이후 현실에서는 **몇십 차례의 실험만으로도** 높은 수준의 성능을 얻어낼 수 있다는 뜻입니다. 이는 로봇 학습 분야에서 **데이터 효율성** 측면으로 큰 의미가 있습니다.

이러한 교훈들과 더불어, 본 연구에서 드러난 몇 가지 **한계점**도 짚고 넘어가겠습니다.
- 첫째, 최종 정책은 **시각 센서에 의존하지 않는 proprioceptive(고유감각) 기반 정책**입니다. 이는 펜의 위치나 움직임을 **로봇 손의 관절 센서** 등으로만 추정한다는 의미인데, 이러한 접근은 물체의 형태나 주변 환경 변화를 인지하진 못하기 때문에 **범용성 면에서는 한계**가 있습니다.
- 둘째, 논문에서 보고된 **실패 사례**들을 보면 현 방법의 **하드웨어적 한계**도 나타납니다. 예를 들어, 로봇 손의 **제어 주파수(control frequency)**가 충분히 높지 않아 **빠르게 떨어지는 물체를 붙잡지 못하는 경우**가 있었습니다. 펜이 손가락에서 살짝 이탈할 때 재빨리 대응해야 하지만, 현재 시스템의 속도론 역부족이었다는 것입니다. 또한 펜이 돌면서 **무게중심(center of mass)**이 미세하게 변해 균형이 깨지는 경우도 있었는데, 이런 상황에서는 시스템이 불안정해져 실패가 발생했습니다. 이러한 문제들은 하드웨어 성능이나 제어 알고리즘을 개선해야만 극복할 수 있는 부분으로 보입니다.
- 마지막으로, 비록 여러 종류의 펜을 다뤘다고는 하나 **모든 형태의 도구**로 일반화되었다고 보긴 어렵습니다. 펜 돌리기는 비교적 대칭적이고 길쭉한 물체라 가능한 면도 있는데, **전혀 다른 모양이나 질감의 물체**를 다룰 때도 이 접근법이 유효할지는 추가 검증이 필요합니다. 향후에는 **더 복잡한 조작 과제**(예: 공중에서 던졌다 받기, 비대칭 물체 다루기 등)에도 이번 기법을 확장하여 테스트해볼 수 있을 것입니다.

## 결론

**"Lessons from Learning to Spin 'Pens'"** 논문은 로봇 강화학습 연구에서 하나의 이정표가 될 만한 흥미로운 성과를 보여주었습니다. 이들은 어려운 펜 돌리기 과제를 해결하는 과정에서, **시뮬레이션의 탐색 능력과 현실의 정확한 피드백을 결합**하는 창의적인 전략을 선보였습니다. 이를 통해 **소량의 현실 데이터**만으로도 복잡한 기술을 학습할 수 있음을 증명했고, 기존 방법들이 처했던 **시뮬레이션-현실 격차의 함정**을 효과적으로 극복했습니다. 특히 강화학습으로 로봇 손에 **핑거게이팅과 같은 인간 유사 전략**이 자발적으로 발현된 점, 그리고 **연속 회전**이라는 난제를 달성해냈다는 점은 주목할 만합니다. 물론 아직 해결해야 할 한계들도 존재하지만, 저자들이 공유한 교훈들은 향후 로봇 학습 연구에 귀중한 지침이 될 것입니다. 궁극적으로, 이 연구는 **“어렵다고 여겨진 로봇 기술도 올바른 학습 전략을 통해 극복 가능하다”**는 희망을 보여주었습니다. 앞으로 펜 돌리기를 넘어서, 로봇이 더욱 다양한 도구를 능숙하게 다루는 모습을 기대해 봐도 좋겠습니다.
