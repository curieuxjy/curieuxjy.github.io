---
title: "📃Open Tech 리뷰"
date: 2025-11-01
categories: [teleop, vr]
toc: true
number-sections: False
description: A Versatile Teleoperation System for Robotic Manipulation
---

- [Paper Link](https://arxiv.org/abs/2403.07870)
- [Project LInk](https://open-teach.github.io/)
- [Code](https://github.com/aadhithya14/Open-Teach)

1. 🤖 OPEN TEACH는 VR 헤드셋(예: Meta Quest 3)을 활용하여 다양한 로봇(다중 손가락 핸드, 양팔, 모바일 매니퓰레이터)을 직관적으로 제어하는 오픈 소스 범용 텔레오퍼레이션 시스템입니다.
2. 💡 이 시스템은 사용자의 자연스러운 손동작을 실시간(최대 90Hz)으로 로봇의 움직임으로 변환하며, 기존 텔레오퍼레이션 프레임워크보다 뛰어난 성능과 정책 학습을 위한 고품질 데이터 수집 능력을 입증했습니다.
3. 🌐 완전 오픈 소스인 OPEN TEACH는 저렴한 비용으로 복잡하고 장기적인 로봇 작업을 수행할 수 있게 하여 로봇 공학 연구의 접근성과 발전을 크게 촉진합니다.


<center>
<img src="../../images/2025-11-01-open-tech/intro_compressed.jpg" width="70%" />
</center>

---

# Brief Review

OPEN TEACH는 저렴한 VR 헤드셋(Meta Quest 3)을 활용하여 로봇 조작을 위한 범용적이고 사용자 친화적인 원격 조작 시스템입니다. 이 시스템은 여러 로봇 팔과 손, 이동 조작(mobile manipulation)을 지원하며, 캘리브레이션이 필요 없고, 시뮬레이션 및 실제 환경 모두에서 작동합니다.

핵심 방법론은 다음과 같습니다:

1.  **가상 환경 설정 (Placing an Operator in a Virtual World)**:
    *   $500 상당의 Meta Quest 3 VR 헤드셋을 사용하여 사용자를 가상 세계에 몰입시킵니다.
    *   헤드셋은 2064 × 2208 해상도와 90Hz 주사율로 가상 환경을 제공합니다.
    *   Quest 3의 풀 컬러 패스스루(full-color passthrough) 기능을 통해 사용자는 원격 조작 중 로봇 셋업을 직접 볼 수 있어 편안하고 직관적인 조작이 가능합니다.
    *   맞춤형 혼합 현실(mixed reality) 세계를 생성하여 로봇 시스템 및 진단 패널을 VR 내에서 시각화할 수 있습니다.
2.  **자세 추정 (Pose Estimation)**:
    *   Quest 3에 내장된 핸드 포즈 추정기(hand pose estimator) [23]를 직접 사용합니다. 이 추정기는 2개의 단색(monochrome) 카메라를 이용하며, 내부적으로 캘리브레이션되어 있어 별도의 캘리브레이션 루틴이 필요 없습니다.
    *   실시간으로 90Hz의 높은 주파수로 손과 손목의 포즈를 스트리밍하여, 높은 정확도와 주파수로 포즈 데이터를 얻습니다.
3.  **인간-로봇 포즈 리타겟팅 (Human to Robot Pose Retargeting)**:
    *   VR 헤드셋에서 얻은 인간의 손목 및 손가락 관절 위치 정보를 사용하여, 다양한 로봇 형태에 맞춰 인간의 손 포즈를 로봇 포즈로 매핑하는 래퍼(wrapper)를 설계합니다.
    *   **로봇 팔 (Robot Arm)**: 손목 키포인트(keypoint)와 검지 및 새끼손가락 너클(knuckle) 지점을 사용하여 3D 좌표계를 설정합니다. 손목 위치는 로봇 말단 효과기(end-effector) 위치에 매핑되고, 3D 좌표계의 시간별 변환은 말단 효과기 방향의 변화에 매핑됩니다.
    *   **로봇 손 (Robot Hand)**: 인간 손의 개별 관절 각도를 로봇 손의 해당 관절 각도로 직접 명령합니다. 특히 엄지손가락의 경우, Holo-Dex [4]의 한계를 개선했습니다. Holo-Dex는 3개의 별도 영역에서 2D 리타겟팅을 사용하여 떨림(jitters)과 정체(stagnancy)를 유발하고, 엄지손가락 끝의 높이가 고정되어 있었습니다. OPEN TEACH는 단일의 넓은 2D 작업 공간 영역을 사용하여 떨림 없이 엄지손가락 끝을 리타겟팅하며, 엄지손가락이 경계를 벗어날 경우 가장 가까운 경계 내 지점을 사용합니다. 또한, 엄지손가락이 손바닥에 수직인 2D 표면을 따라 움직일 수 있도록 하여 인간 엄지손가락 끝의 높이를 로봇 엄지손가락 끝에 최대/최소 높이 경계에 따라 매핑함으로써 3D 움직임을 가능하게 합니다.
    *   **두 손가락 그리퍼 (Two-fingered gripper)**: 새끼손가락과 엄지손가락 사이의 꼬집음(pinch) 거리를 계산하고 임계값을 설정하여 그리퍼의 열림/닫힘을 감지하며, 꼬집을 때마다 그리퍼 상태를 토글(toggle)합니다.
    *   **모바일 매니퓰레이터 (Mobile manipulator)**: 로봇 팔과 동일한 3D 좌표계를 사용하여 손목 움직임을 모바일 로봇의 행동에 매핑합니다. 손목을 앞으로 움직이면 로봇 팔이 확장되고, 수직 움직임은 로봇의 높이를 조정하며, 측면 움직임은 바퀴를 제어하여 로봇을 옆으로 이동시킵니다. 인덱스 핑거와 엄지손가락 사이의 꼬집음으로 그리퍼를 제어합니다.
4.  **로봇 제어 (Robot Control)**:
    *   ZeroMQ를 통한 노드 간 네트워킹을 사용하며, 원격 조작은 Detector, Keypoint Transformer, Operator, Controller, Visualizer의 5가지 구성 요소로 나뉩니다.
    *   비동기적으로 ROS [59] 통신 프레임워크를 사용합니다.
    *   리타겟팅 절차에서 계산된 로봇 관절 위치를 사용하여 PD 컨트롤러가 300Hz 주파수로 목표 토크를 출력하며, 정상 상태 오차를 완화하기 위해 중력 보상 모듈을 포함합니다.
    *   Allegro Hand는 60Hz, xArm은 90Hz, Franka Emika Panda 및 Kinova Jaco는 60Hz, Hello Stretch는 5Hz로 제어됩니다. 이러한 고주파 원격 조작은 실시간 로봇 움직임과 즉각적인 오류 수정을 가능하게 합니다.
    *   일시정지(pause) 기능과 정밀 작업을 위한 해상도 조정(resolution adjustment) 기능을 제공합니다.

OPEN TEACH는 Franka, xArm, Jaco, Allegro, Hello Stretch와 같은 다양한 플랫폼을 지원하며, 38가지 작업에 대한 광범위한 평가를 통해 시스템의 다용성을 입증했습니다. 사용자 연구에서는 AnyTeleop [47] 프레임워크보다 원격 조작 능력에서 상당한 개선을 보였으며, 수집된 데이터는 10가지 정교하고 접촉이 많은 조작 작업에서 정책 학습(policy learning)과 호환됨을 입증하여 평균 86%의 성공률을 달성했습니다. 전체 프로젝트는 오픈소스입니다.


<center>
<img src="../../images/2025-11-01-open-tech/0.png" width="100%" />
</center>

# Detail Review


> Open Teach: 유연한 원격조작 시스템 심층 리뷰

## 서론

Open Teach는 가상현실(VR) 헤드셋을 이용해 사용자를 혼합현실 환경에 배치함으로써 직관적인 로봇 원격조작을 가능하게 하는 프레임워크이다. Meta Quest 3 같은 저비용 VR 기기(약 $500)를 기반으로 구축된 Open Teach는 다중 손가락 핸드, 양팔 로봇, 모바일 매니퓰레이터 등을 *실시간*(최대 90Hz)으로 조작할 수 있으며, 장면 전경과 로봇의 카메라 영상을 시각적 피드백으로 제공한다. 논문에 따르면 Open Teach는 총 38개의 다양한 과제를 통해 다수의 로봇 플랫폼에서 평가되었고, AnyTeleop 같은 기존 범용 원격조작 시스템에 비해 새로운 사용자도 원격조작 효율을 유의미하게 향상시킬 수 있음을 보였다. 또한 수집된 데이터를 활용한 정책 학습 실험에서 10개 과제 평균 86%의 성공률을 보이며 데이터 품질을 검증하였다. 본 리뷰에서는 Open Teach 시스템의 전체 구조와 구성 요소(텔레오퍼레이션 인터페이스, 백엔드 서버, 로봇 제어 API 등)를 상세히 분석하고, Allegro Hand 플랫폼에 Open Teach를 적용했을 때의 호환성 및 기술적 고려사항을 논의한다.

## 관련 연구

원격조작(teleoperation)은 인간 조작자가 로봇을 원격으로 제어하며 데이터를 수집하거나 과업을 수행하는 중요한 수단이다. 기존의 원격조작 장치로는 조이스틱, 스페이스마우스, VR 컨트롤러, 운동보조(kinesthetic) 기법, 스마트폰 원격조작 등이 있다. 이러한 장치들은 설정이 간단하고 비용이 저렴하지만, 복잡한 동작을 정확히 수행하기 위해서는 사용자의 숙련과 반복적 트레이닝이 필요하다는 문제가 있다. 예를 들어, ALOHA, GELLO, AirExo 같은 외골격(엑소스켈레톤) 기반 시스템은 인간 조작자가 로봇 암과 운동학적으로 동형(isomorphic)인 보조 장비를 착용하여 로봇을 직관적으로 제어하도록 설계되었으나, 각 로봇마다 보조 로봇이나 하드웨어가 필요하고 초기 구축 비용이 크며, 특정 로봇 형태에 특화된다는 단점을 지닌다. 특히 고차원 조작 환경에서는 센서 장갑(gloves)을 사용하거나 다수의 카메라로 보정을 수행해야 하고, 시야 가림(occlusion) 문제가 발생할 수 있다.

이와 달리 다양한 로봇 플랫폼에 적용 가능한 범용 프레임워크에 대한 시도도 있었다. AnyTeleop 같은 시스템은 다수의 로봇 암과 핸드를 지원하려 하지만, 비전문가가 사용하기엔 인터페이스가 복잡하거나 개별 로봇의 제약을 완전히 해소하지 못한다. 또한, 이전 연구들은 대개 상업용 VR 헤드셋이나 마커 기반 외골격, 측방식(kinect) 트래킹 등 특정 입력 장치와 로봇 조합에 의존하는 경우가 많았다. 이와 비교하여 Open Teach는 **교정(calibration) 없이** 다양한 로봇 형태(양팔 로봇, 다중 손가락 핸드, 모바일 매니퓰레이터 등)를 지원하며, 모든 구성 요소를 오픈소스로 공개한 점이 특징이다.

## 시스템 설계

Open Teach는 크게 **텔레오퍼레이션 인터페이스(VR 애플리케이션)**, **핸드 포즈 추정 및 리타게팅 모듈**, **로봇 제어 모듈(백엔드)**로 구성된다. 사용자는 Meta Quest 3 헤드셋을 착용하고 로봇 환경을 3D 혼합현실로 경험하며, 온보드의 손 추적 기능으로 손 제스처를 이용해 로봇을 조작한다. 시스템 전체 아키텍처는 크게 사용자-서버-로봇의 파이프라인으로 나뉘며, 각 단계의 구성 요소가 유기적으로 연결된다.

- **VR 텔레오퍼레이션 인터페이스**: Meta Quest 3 헤드셋은 컬러 패스스루(color passthrough) 기능을 제공하여 사용자가 실제 로봇 환경을 사실적으로 볼 수 있도록 한다. 높은 해상도와 90Hz의 주사율은 몰입감을 높이며, Unity 기반 VR 애플리케이션은 로봇 장면과 진단 패널(상태표시기, 카메라 뷰 등)을 혼합 현실에 시각화한다. 사용자는 손을 움직이고 집게 쥐기와 같은 제스처를 수행하여 로봇 제어 명령을 발행하게 된다. 이때 Meta Quest 3 헤드셋의 내장 손 추정(hand-pose estimation) 모듈을 사용하여 약 90Hz 속도로 사람의 손 관절 위치와 자세를 검출한다. 이 모듈은 두 개의 흑백 카메라를 이용하며 단일 RGB 카메라 대비 손가락 간섭(occlusion)에도 비교적 강건하고, 내부 교정이 되어 있어 별도의 캘리브레이션 과정이 필요없다.

- **핸드 포즈 리타게팅(Human-to-Robot Mapping)**: VR 헤드셋으로부터 얻은 인간 손의 관절 포즈 정보를 로봇의 조인트 명령으로 변환(retargeting)하는 작업이 핵심이다. Open Teach에서는 *모듈화된 래퍼(wrapper)* 형태로 리타게팅 룰을 정의하며, 로봇 형태에 따라 유연하게 조작할 수 있다. 예를 들어, 로봇 암의 경우 휴먼 손의 손목(wrist) 위치와 검지/새끼손가락 관절(knuckle) 위치를 이용해 인간 손바닥 기준 좌표계를 정의하고, 손목 위치 변화로 로봇 엔드이펙터(end-effector)의 위치를 매핑한다. 방향 변화 또한 손바닥 평면 좌표계의 변환을 로봇 엔드이펙터 회전에 대응시킨다.

로봇 핸드(다수의 손가락을 지닌 로봇 손)의 경우, 인간 손 관절 각도를 로봇 손 관절 각도로 1:1 대응시키되 엄지손가락은 특수 처리한다. 다른 손가락은 관절값을 일대일로 명령하나, 엄지는 단순 대응만으로는 어색하거나 불가능한 자세가 생긴다. Open Teach는 이전 연구인 Holo-Dex의 엄지 리타게팅 방식을 개선하여, 인간 엄지 끝끝 좌표를 로봇 엄지 평면상으로 투영한 뒤 역운동학으로 관절을 계산한다. 특히 Open Teach는 엄지손가락의 전체 작업공간(workspace)을 아우르는 단일 변환 영역을 사용하여, 손가락이 영역 밖을 벗어나도 가까운 경계점으로 안정적으로 끌어당겨 매핑하도록 하여 모션의 끊김을 줄였다. **Allegro Hand**의 경우 논문에서 언급된 것처럼 로봇이 새끼손가락(pinky)을 지원하지 않으므로 인간 손의 새끼손가락 관절 정보는 무시하고 매핑한다. 이처럼 유연한 리타게팅 설계를 통해 사용자는 각 로봇 형태에 맞춘 매핑 함수를 손쉽게 정의할 수 있다.

<center>
<img src="../../images/2025-11-01-open-tech/001.png" width="70%" />
</center>


또한 손가락을 사용하지 않는 2-핀처(two-fingered) 그리퍼의 개폐는 인간의 엄지와 새끼손가락 사이의 *핀치(pinch)* 동작을 이용해 제어한다. 엄지와 새끼손가락 끝 사이 거리가 특정 임계값 이하면 그리퍼를 토글(toggle)식으로 열고 닫는다. 모바일 매니퓰레이터의 경우 인간의 손목 움직임을 로봇 베이스와 암의 동작으로 변환한다. 손목을 전진시키면 로봇 암이 앞으로 뻗고, 상/하 이동은 로봇의 높이를 조절하며, 좌우 움직임은 로봇의 주행으로 변환된다. 이 과정에서도 손목 좌표계의 3차원 변환을 로봇 엔드이펙터의 이동·회전으로 대응시킨다. 2-핀처의 개폐는 역시 손가락 핀치로 조작한다.

- **제어 백엔드(Backend & 로봇 API)**: 리타게팅된 로봇 목표 자세는 로봇 제어 모듈로 전달된다. 전체 통신은 ZeroMQ(ZMQ) 소켓과 ROS를 통해 이뤄지며, 그림으로 정리하면 '**데이터 수집기(Detector) → 키포인트 변환기(Keypoint Transformer) → 오퍼레이터(Operator) → 컨트롤러(Controller) → 로봇**'의 순서로 동작한다. 각 단계의 역할은 다음과 같다:
  - **Detector(감지 모듈)**: Meta Quest 3에서 추정된 인간 손 관절 좌표를 받아 ZMQ를 통해 서버로 발행한다.
  - **Keypoint Transformer(키포인트 변환기)**: Detector가 보낸 인간 손 키포인트를 구독한 뒤, 앞서 정의한 리타게팅 룰에 따라 로봇 목표 자세로 변환한다(위의 리타게팅 로직 수행).
  - **Operator(오퍼레이터)**: 키포인트 변환기로부터 받은 로봇 목표 자세와 현재 로봇 상태(센서/조인트 상태)를 사용하여 최종 명령(actions)을 산출한다. 예를 들어 목표 관절 각도와 현재 각도를 비교하여 제어 입력을 계산한다. 연산된 명령은 ZMQ 소켓으로 발행된다.
  - **Controller(컨트롤러)**: Operator의 출력을 받아 실제 환경에서 로봇을 구동하고, 로봇의 최신 상태(조인트 위치, 센서값 등)를 ZMQ를 통해 Operator에게 피드백한다. 실제로 Allegro 핸드와 xArm, Franka, Kinova Jaco 등의 로봇마다 별도의 제어기를 사용한다. 예를 들어 Franka는 Deoxys 컨트롤러, Jaco는 이전 연구에서 공개한 컨트롤러를 사용한다.
  - **Visualizer(시각화기)**: 로봇 장착 카메라나 시뮬레이터의 RGB 영상을 구독하여 VR 애플리케이션 화면에 표시함으로써 사용자가 로봇 관점의 실시간 영상을 볼 수 있게 한다.
  - **Data Collector(데이터 수집기)**: 관절 상태, 터치 센서, RGB-D 영상 등 각종 센서 및 로봇 정보를 기록하여 학습 데이터로 저장한다. 각 데이터 스트림은 타임스탬프에 따라 동기화된다.

이러한 구조로 인해 Open Teach는 낮은 지연(latency)과 고주파수 제어(최대 90Hz)로 로봇을 제어할 수 있다. 특히 Allegro 핸드의 경우 ROS를 통해 비동기(asynchronous) 제어를 수행하며, 리타게팅된 관절 각도를 PD(PD 게인 기반) 컨트롤러로 300Hz의 속도로 토크로 변환해 구동한다. PD 제어기에는 중력 보상(gravity compensation)을 추가하여 관절의 영구오차를 줄인다. XArm 암은 제조사 제공 Python SDK로, Franka Emika Panda는 Deoxys 컨트롤러로, Kinova Jaco는 공개된 컨트롤러로 구동하며, 각각 최대 90Hz, 60Hz 수준의 제어 빈도를 사용한다. Allegro 핸드는 60Hz로 스트리밍되며, Hello Stretch 로봇은 5Hz로 제어한다. 또한 Open Teach는 사용자가 필요할 때 즉시 원격조작을 일시 정지(pause)하거나 해상도(resolution)를 조정할 수 있는 기능을 제공하여 고정밀 작업의 안정성을 높인다(부록에 상세 설명).

전체적으로 Open Teach의 시스템 아키텍처는 `VR 인터페이스 - 서버 처리 - 로봇 제어`의 3단계로 구분되며, 각 구성요소(예: Detector, Transformer, Operator, Controller 등)는 네트워크 소켓으로 연결되어 실시간 데이터와 명령을 주고받는다. Figure 2(요약 그림)을 참고하면, 사용자 손 포즈 데이터가 로봇 서버로 전송되어 리타게팅된 뒤 로봇이 제어되며, 그 과정에서 로봇의 영상이 다시 VR 화면으로 전송된다.

<center>
<img src="../../images/2025-11-01-open-tech/00.png" width="100%" />
</center>


## 실험

Open Teach의 다목적성(versatility)을 평가하기 위해 다양한 로봇 구성에서 총 **6가지 세트업**으로 실험을 진행했다. 네 가지 실제 환경은 다음과 같다:

- **Franka+Allegro(프랑카암+알레그로 핸드)**: Franka Emika Panda 팔에 16자유도 Allegro Hand(촉각 센서 Xela 포함)를 결합한 조합.
- **Kinova+Allegro(키노바암+알레그로 핸드)**: Kinova Jaco 암에 Allegro Hand(Xela 포함)를 장착한 조합.
- **양팔(Bimanual)**: 두 대의 xArm7 암으로 구성된 양팔 로봇(각 암 끝단에는 2-핀처 그리퍼).
- **Hello Stretch(스트레치)**: 모바일 매니퓰레이터인 Hello Stretch(휠 기반 이동 로봇)에 2-핀처 그리퍼를 장착한 구성.

두 가지 시뮬레이션 환경은:

- **Allegro Sim**: 관성화된(floating) Allegro Hand만으로 구성된 시뮬레이션.
- **LIBERO Sim**: Franka 암과 2-핀처 그리퍼로 구성되며 다양한 장면에서 가정용 조작 과제를 수행하는 시뮬레이터(LIBERO 환경).

각 세트업에서 사용자들은 VR 헤드셋을 통해 로봇을 원격조작하며 총 38개의 과제(박스 열기, 스폰지 집기, 타자 뒤집기 등)를 수행했다. 예를 들어 Franka-Allegro 구성에서는 전자제품 개폐, 섬유 집기, 봉지 회전 등의 작업을 수행했다. 각 시나리오마다 카메라 센서를 사용하여 로봇의 시점 영상을 수집했고, Allegro 핸드 세트업에서는 시각 데이터와 함께 촉각(Xela) 센서 데이터도 기록되었다. 이렇게 수집된 시뮬레이션 및 실제 데이터는 이후 행동 클로닝(behavior cloning)과 역강화학습(inverse RL)을 위해 저장되었다.

실험의 목표는 (1) Open Teach의 다양한 로봇 구성 지원 범위 평가, (2) 수집 데이터로 학습한 정책의 성공률 검증, (3) Open Teach를 통한 복잡·장기 과제 수행 가능성 확인, (4) 신규 사용자의 직관성 검증이었다. 특히 Allegro Hand가 포함된 세트업(Franka-Allegro, Kinova-Allegro, Allegro Sim)에서는 16-DOF 다지능 손 조작 데이터의 수집이 이루어졌고, 이는 고차원 원격조작 성능 평가의 중요한 근거가 되었다.

## 결과

Open Teach를 통해 수집된 데이터와 사용자 평가 결과는 다음과 같다. 먼저 **수집된 데이터의 품질**은 정책 학습 실험을 통해 검증되었다. Frana-Allegro 구성에서는 시각 및 촉각 데이터를 이용해 TAVI 알고리즘으로 정책을 학습하였고, Allegro Sim에서는 FISH(비전 기반) 방식으로 학습했다. 이들 정책은 20분 이내의 학습으로 평균 82%의 성공률을 기록했고, LIBERO Sim에서는 Behavior Cloning으로 93% 성공률을 보였다. 결과적으로 모든 로봇과 과제에 걸쳐 평균 86%의 성공률을 달성하여, Open Teach로 수집된 관측 및 행동 데이터가 정책 학습에 매우 적합함을 확인했다.

**원격조작의 효율성 및 정확도**도 우수함을 보여주었다. 실험 결과, 사용자는 로봇별 전용 원격조작 시스템과 비교해 견줄 만한 속도로 조작을 수행하였고, AnyTeleop 같은 범용 시스템보다 훨씬 빠른 조작 속도를 달성했다. 예를 들어, 과제 시연 속도를 살펴보면 Open Teach는 로봇 전용장치 못지않은 수준으로 빠르게 조작했으며, AnyTeleop 대비 유의하게 높은 속도를 기록했다.

**장기·복합 과제 수행** 능력도 검증되었다. 논문 Fig.4에서는 다양한 실제 환경에서 USB 삽입, 노트북 여닫기, 차가운 조리도구 다루기 등 정밀도가 요구되는 작업부터 장기적인 조작까지, 2-핀처 및 다손가락 핸드 구성 모두에서 Open Teach가 성공적으로 수행하는 모습이 제시되었다. 이는 Open Teach가 고차원 조작에서도 사용자 입력을 신속히 반영하여 복잡한 연속 과제를 수행할 수 있음을 의미한다.

**사용자 연구(user study)** 결과도 눈에 띈다. 15명의 신규 사용자를 대상으로 Franka-Allegro 세트업에서 Holo-Dex, AnyTeleop, Open Teach를 비교 실험한 결과, Open Teach 사용 시 성공률이 가장 높고 과제 완료 시간도 가장 짧았다. 특히 큐브 뒤집기와 핀치 집기 같은 과제에서 Open Teach는 다른 두 시스템 대비 성공률이 높았고, 평균 미디언 완성시간 또한 유의하게 감소했다. 신규 사용자의 성능은 숙련자(전문가)에 비해 약 76% 정도였지만, 짧은 연습만으로도 상당한 성과를 냈다. 이는 Open Teach의 인터페이스가 직관적이며, 반복 학습을 통해 추가적인 향상이 가능함을 시사한다.

다만 **한계점**으로는 VR 손 추적의 정확도에 대한 의존성이 지적되었다. Oculus 기반 추정기가 손가락을 가리면(occlusion) 잘못 인식할 수 있는데, 이로 인해 그리퍼를 여닫는 동작처럼 정밀한 핀치 제스처가 어색해지기도 한다. 이러한 문제는 VR 자체의 센서 한계에서 기인하므로, 향후 더 정교한 손 추적 기술의 도입이 필요하다.

## 결론 및 Allegro Hand 적용 논의

Open Teach는 저렴한 VR 장치를 활용하여 **다양한 로봇 형태**의 원격조작을 실현한 유연한 시스템이다. 메타 퀘스트3 기반의 혼합현실 인터페이스와 고주파 제어 파이프라인 덕분에 별도의 캘리브레이션 없이 *프랑카, xArm, Kinova Jaco, Allegro Hand, Hello Stretch* 등 여러 로봇을 즉시 제어할 수 있음을 보였다. 이 프레임워크가 공개되어 있어 연구자 누구나 쉽게 적용·확장할 수 있다는 점도 장점이다.

특히 본 연구에서는 **Allegro Hand** 플랫폼 적용 가능성이 검증되었다. 실험에 Franka 및 Kinova 암에 장착된 Allegro Hand를 포함시켰으며, Open Teach로 Allegro의 16개 독립 관절을 제어할 수 있음을 보였다. 기능적으로 Allegro는 4개의 손가락(각 4DOF)과 촉각 센서를 갖춘 저비용 핸드로, ROS 기반 API를 통해 실시간 제어가 가능하다. Open Teach는 Quest의 손 관절 데이터를 Allegro 관절로 직접 매핑하며, Pinky(새끼손가락)가 없는 Allegro 구조는 Open Teach의 리타게팅 단계에서 단순히 무시했다. 즉, 시스템 자체가 Allegro의 16DOF 아키텍처에 근본적으로 호환되도록 설계되어 있다. 실제로 논문에서는 Allegro 핸드의 60Hz 스트리밍과 PD 제어(300Hz)로 고속의 응답성을 달성했고, 이는 Allegro의 강체 전기구조에도 안정적으로 적용되었다.

*인터페이스 통합* 관점에서 볼 때, Open Teach의 백엔드는 ROS와 ZMQ를 사용하므로 Allegro Hand의 기존 ROS 드라이버를 손쉽게 연결할 수 있다. 실제 코드에서는 Allegro를 ROS 노드로 설정하고, Open Teach의 Operator-Controller 파이프라인과 통합시켜 사용한다. 이 과정에서 Allegro 핸드는 단일 토크 제어모드로 동작하며, Open Teach가 계산한 목표관절을 주기적으로 업데이트한다. 따라서 별도의 복잡한 인터페이스 개발이 필요 없고, 오픈소스인 Allegro 드라이버와 원격조작 서버를 연계하여 실시간 제어가 가능하다.

**제어 정밀도 및 안정성** 측면에서는, PD 제어와 중력보상을 적용하여 스테디스테이트 오차를 줄이고자 하였다. Allegro와 같이 다수 관절을 가진 로봇의 경우, 각 관절에 적절한 PD 이득 튜닝이 필요하지만, 본 시스템에서는 이미 충분한 샘플링 빈도로(60Hz) 연속적 명령을 내려 모션을 부드럽게 하여 사용자에게 실시간 피드백을 제공했다. 이렇게 고주파(300Hz) 제어를 사용하므로 응답 속도와 정밀도는 우수하나, 통신 지연이나 네트워크 부하가 커지면 불안정성이 발생할 수 있다. 또한 VR 기반 입력의 잡음이나 가벼운 오차가 Allegro 관절 명령으로 바로 반영되기 때문에, 고관절 수를 가진 Allegro 핸드에서는 누적 오차와 떨림(jitter)에 주의해야 한다. 실제 테스트에서도 손가락 관절 추적이 불안정하면 작은 노이즈가 손동작 과민반응으로 이어지는 문제가 있었다. 따라서 Allegro 적용 시 제어 루프와 필터링, 필요한 경우 저역 필터 적용 등의 보완이 필요할 것이다.

이상의 분석을 바탕으로, Open Teach를 Allegro Hand에 적용할 때의 **예상 장점과 기술적 과제**는 다음과 같다. 첫째, 장점으로는 저렴한 시스템 비용($500 VR 장치)을 통해 Allegro와 같은 고다자유도 로봇을 쉬운 방식으로 원격제어할 수 있다는 점이다. 별도의 외골격이나 고가의 장갑 없이 자연스러운 인간 손동작으로 16DOF Allegro를 조작할 수 있으므로, 연구자·교육자들이 손쉬운 데모 수집 환경을 갖게 된다. 둘째, Open Teach의 오픈소스 특성상 Allegro에 맞춘 리타게팅 로직(예: 엄지 매핑 개선, 핑거 개수 차이 보정)을 커스터마이즈하여 추가 개발이 가능하다. 예를 들어 Allegro V5(4F) 모델에는 엄지 대신 네 손가락만 존재하므로, 세 손가락만 가진 모델에 맞춰 리타게팅 래퍼를 수정할 수 있다.

반면 **고려해야 할 과제**도 있다. Allegro는 손가락이 많아 인간 손보다 구속(kinematic constraint)이 느슨하고, 손가락 하나가 다른 손가락의 움직임에 비해 상대적으로 예민하게 움직일 수 있다. 따라서 VR 입력의 작은 오차가 Allegro 제어에서 증폭되어 원치 않는 움직임을 유발할 수 있다. 이를 해결하기 위해 사용자는 잡음 제거 필터, 관절 속도 제한, 또는 정밀도 모드 전환 기능(저속 모드 등)을 활용해야 한다. 또한 Allegro의 각 손가락에 부착된 360도 촉각 센서를 적극 활용할 수 있는데, 이를 Open Teach에 통합하려면 촉각 데이터의 실시간 스트리밍과 동기화 문제가 남는다. 마지막으로, Open Teach는 현재 로봇 암과 핸드의 포즈를 동시 제어하지만, Allegro를 장착한 양팔 로봇과 같은 복합 시스템에서는 각 손과 팔을 별도로 제어할 수 있도록 바운드드 컨트롤러(bound control)나 상태 머신이 필요할 수 있다. 예를 들어 두 핸드를 한 VR 환경에서 동시에 조작하거나, 팔과 손의 우선순위를 전환하는 제어 논리가 추가될 수 있다.

결론적으로, Open Teach는 기존의 Allegro 제어 방식을 보완하며 보다 직관적이고 저비용의 다지능 로봇 원격조작을 가능하게 한다. 향후 Allegro 핸드용 사용자 인터페이스 튜닝, 모션 분할 알고리즘, 촉각 피드백 통합 등의 기술 개발을 통해, 특히 정밀한 물체 조작과 촉각 기반 상호작용 같은 영역에서 높은 성능을 낼 수 있을 것으로 기대된다.

