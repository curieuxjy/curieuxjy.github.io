---
title: "📃From Imitation to Refinement 리뷰"
date: 2025-09-22
categories: [residual, manipulation]
toc: true
number-sections: False
description: Residual RL for Precise Assembly

---

- [Paper Link](https://arxiv.org/abs/2407.16677)
- [Homepage](https://residual-assembly.github.io/)

1. 기존 Behavior Cloning (BC) 방식은 오프라인 데이터의 분포 변화와 액션 청킹으로 인한 폐루프 제어 부족 때문에 정밀한 로봇 조립 작업에서 성능이 포화되고 신뢰성이 떨어진다는 한계를 보였습니다.
2. 본 논문은 미리 학습된 BC 모델에 강화 학습(RL)으로 훈련된 폐루프 잔여 정책(residual policy)을 결합하는 ResiP(Residual for Precise Manipulation)를 제안하여, BC가 궤적 "계획자" 역할을 하고 잔여 정책이 실시간으로 미세한 교정을 수행하게 합니다.
3. ResiP는 50개의 시연 데이터만으로도 정밀 조립 작업에서 기존 BC보다 훨씬 높은 성공률(예: peg-in-hole에서 5%→99%)을 달성하며, 분포 변화와 동적 교란에 대한 로봇의 견고성을 크게 향상시켰습니다.


<center>
<img src="../../images/2025-09-22-imitation-to-refine/residual_see_through.gif" width="70%" />
</center>

---

# Brief Review

이 논문은 정밀 조립과 같은 로봇 조작 작업에서 Behavior Cloning (BC) 정책의 신뢰성 부족과 성능 포화 문제를 다룹니다. 이러한 한계는 주로 오프라인 데이터 사용으로 인한 분포 변화(distribution shift)와 행동 덩어리(action chunking)를 통한 개방 루프(open-loop) 실행으로 인한 폐쇄 루프(closed-loop) 보정 제어의 부재에서 기인합니다.

저자들은 행동 덩어리를 예측하는 BC 정책이 반응형 컨트롤러보다는 궤적 "플래너(trajectory planner)"에 가깝다고 분석합니다. 이러한 문제를 해결하기 위해, ResiP(Residual for Precise Manipulation)이라는 간단하면서도 효과적인 방법을 제안합니다. ResiP는 고정된(frozen) 청크(chunked) BC 모델에 RL(Reinforcement Learning)로 훈련된 완전히 폐쇄 루프 방식의 잔여(residual) 정책을 추가합니다. 이 잔여 정책은 분포 변화를 해결하고 BC 궤적 플래너가 예측한 행동 청크의 개방 루프 실행에 대한 폐쇄 루프 보정을 도입합니다.

ResiP의 핵심 방법론은 다음과 같습니다:

1.  **Base Policy Learning via Behavior Cloning:**
    *   50개의 시뮬레이션 데모 데이터를 사용하여 기본 정책 $\pi_{base}$를 훈련합니다. Diffusion Policy (DP) 아키텍처를 사용하며, 이는 최신 BC 훈련 기법에 따라 여러 미래 행동을 예측하는 행동 청크(action chunks)를 활용합니다.
    *   정책은 $T_a$ 길이의 미래 행동 시퀀스를 예측하고, 그 중 $T_{exec}$ 길이의 부분 집합($[a^{base}_t, \dots, a^{base}_{t+T_{exec}-1}]$)만 실행합니다. 이 논문에서는 $T_a=32$, $T_{exec}=8$을 사용합니다.

2.  **Reactive Control via ResiP:**
    *   BC로 얻은 초기 청크 기본 정책 $\pi_{base}$를 사용하여 분포 변화 및 반응성 부족 문제를 개선합니다.
    *   잔여 정책 $\pi_{res}$는 PPO(Proximal Policy Optimization)로 훈련된 작은 단일 스텝 Gaussian Multi-Layer Perceptron (MLP)입니다.
    *   각 타임스텝 $i = 0, \dots, T_{exec}-1$에 대해 잔여 정책은 현재 시뮬레이션 상태(로봇 고유 정보 및 객체 자세)와 기본 정책이 예측한 행동 $a^{base}_{t+i}$를 연결한 $s^{res}_{t+i} = [s_{t+i}, a^{base}_{t+i}]$를 관측합니다.
    *   잔여 정책은 이 관측을 바탕으로 보정 행동 $a^{res}_{t+i} \sim \pi_{res}(\cdot|s^{res}_{t+i})$을 생성하여 기본 행동을 수정합니다. 최종 실행 행동은 $a_{t+i} = a^{base}_{t+i} + a^{res}_{t+i}$입니다. 이 per-timestep 보정 기능은 정밀 작업에 필수적입니다.
    *   훈련 중에는 탐색 노이즈를 조정하여 새로운 행동을 발견하고 작업 성공을 위한 충분한 정밀도를 유지합니다. 잔여 정책은 orthogonal initialization을 사용하여 초기 보정이 0 주변에 집중되도록 합니다.

3.  **Sim-to-Real Transfer:**
    *   시뮬레이션에서 훈련된 상태 기반 $\pi_{res}$ 정책을 교사(teacher) 정책 $\pi_{teacher}$로 활용하여 시각 기반(vision-based) 학생(student) 정책 $\pi_{student}$로 지식 증류(distillation)합니다.
    *   $\pi_{teacher}$로부터 성공적인 궤적 데이터셋 $D_{synth}$를 수집하고, 이를 실제와 같은 이미지 관측 $D_{synth-render}$로 다시 렌더링하여 시각적 다양성을 도입합니다.
    *   소량의 실제 데모 $D_{real}$와 합성 렌더링된 데이터셋 $D_{synth-render}$를 결합하여 $\pi_{student}$를 훈련합니다. $\pi_{student}$는 ResNet18 시각 인코더가 있는 Diffusion Policy 아키텍처를 사용합니다.

주요 결과는 다음과 같습니다:

*   ResiP는 "one leg" 작업에서 50개의 데모만으로 98%의 성공률을 달성하여, 100K 데모를 사용한 순수 BC 방식(54%)보다 18% 포인트 향상된 성능을 보입니다. "peg-in-hole"과 같은 정밀 작업에서는 5%에서 99%로 성공률이 크게 향상되었습니다.
*   BC 정책의 실패 원인이 작은 부정확성(small imprecision)임을 분석하고, ResiP가 이러한 오류를 미세한 조정으로 안정적으로 수정하며 새로운 그립 전략과 같은 질적으로 다른 행동을 발견하기도 합니다.
*   ResiP는 청크 기반 메서드(DP, DP-DAgger, ResiP-C)보다 동적 교란(dynamic disturbances)에 대해 더 강력한 로버스트니스(robustness)를 보여줍니다(성능 하락 12% vs. 19-26%).
*   잔여 정책의 아키텍처는 깊은 RL 훈련을 불안정하게 만드는 큰 정책 업데이트를 방지하여 안정적인 훈련과 높은 샘플 효율성을 제공합니다.
*   시뮬레이션 데이터와 실제 데모를 공동 훈련(co-training)하면 실제 배포 시 성능이 크게 향상되고, 부품 색상 변화와 같은 시각적 변화에 대한 로버스트니스를 제공합니다.

제한 사항으로는, ResiP의 지역적 특성으로 인해 초기 장면 무작위성이 매우 높은 경우에는 어려움을 겪을 수 있으며, 시각 기반 정책의 sim-to-real 전이 및 지식 증류 과정에서 여전히 성능 격차가 존재한다는 점이 언급되었습니다.

---

# Detail Review

## 서론: 문제 정의와 중요성

로봇 조립과 같은 다단계 조작 작업은 고도의 정밀도와 강건성을 요구하며, 카메라만으로 제어하려면 매우 까다로운 문제다. 예를 들어, 다수의 부품을 정해진 위치에 정확히 결합해야 하는 조립 작업은 시뮬레이션 기반 초기 위치 조정을 위한 장치 없이 수행하기 어렵다. 전통적으로 행동 복제(Behavior Cloning, BC)가 복잡한 시각 조작 정책 학습에 많이 사용되고 있다. 시연 데이터를 통해 직접 정책을 학습할 수 있어 간편하지만, 대량의 사람이 제공한 데모가 필요하고 , 인간 시연이 해결된 궤적만 학습하기 때문에 실행 중 발생하는 편차를 자체적으로 보정하는 능력은 제한적이다. 반면 강화학습(RL)은 탐색 기반 학습으로 보상을 통해 국소적 보정 행동을 익힐 수 있으나, 보상 함수 설계나 안정적인 훈련이 어려운 면이 있다. 본 논문은 정밀 조립 과제에서 BC 정책의 한계를 RL로 보완하는 방안을 제안한다. 즉, 데모로 쉽게 배운 BC 정책을 “잔차 학습” 방식으로 정제하여 정밀 제어 성능을 향상시키는 새로운 파이프라인을 제시한다. 이는 모방으로부터 보상의 활용으로 이행(Imitation to Refinement)한다는 점에서 중요한 기여다.

## 핵심 아이디어 요약

이 논문에서 제안하는 ResiP(Residual for Precise manipulation) 기법의 핵심은 동결된 BC 기반 정책 위에 RL기반 잔차 정책을 추가하는 것이다. 먼저 시뮬레이션에서 수십 개(50∼90)의 전문가 시연을 수집하여, 확산 정책(Diffusion Policy)과 행동 청크(Action Chunking)를 활용한 BC 기반 기본 정책($π_{base}$ )을 학습한다. 이 기본 정책은 일정 간격마다 미래 행동 일괄(청크) a_t^(base: t,…,t+T_a-1)를 예측하므로, 실행 시점마다 미리 계획된 궤적을 따르는 오픈 루프 행동 계획자 역할을 한다. 하지만 이러한 구조는 분포 편차(distribution shift)와 폐쇄 루프 제어 부족으로 인해 정밀 조작 성능에 한계를 보인다.

이를 보완하기 위해, 본 연구는 잔차 강화학습을 도입한다. 훈련된 BC 기반 정책을 고정한 채, 그 위에 작고 가벼운 잔차(residual) 정책($π_{res}$ )을 학습한다. $π_{res}$ 는 매 시점의 현재 상태($s_t$)와 BC 정책이 예측한 행동($a_t^{base}$)을 입력받아, 이를 미세 조정할 보정 행동Δa_t를 출력한다. 최종 행동은 $a_t = a_t^(base) + α·Δa_t$ 의 형태로 합성되며, α는 보정 규모를 조절하는 계수다. 이렇게 함으로써 원래의 확산 정책 파라미터는 전혀 변경하지 않고, 학습된 궤적에 폐쇄 루프 보정을 추가한다. 이 접근법은 분포 편차를 보정하고 작게 일탈한 경우를 바로잡아주기 때문에, BC 정책의 장기 예측 능력은 유지하면서 신뢰성을 크게 높인다. 학습은 표준 PPO를 사용한 온폴리시 방식으로 진행되며, 시뮬레이션에서 성공적 궤적을 생성한 후 이를 RGB 시뮬레이션 이미지와 결합하여 최종적으로 실제 카메라 영상을 통한 정책으로 증류한다(교사-학생 증류 및 시뮬→실 제도 일반화).

<center>
<img src="../../images/2025-09-22-imitation-to-refine/807fa95e-17e4-4337-ace1-224fc3de5ad8.png" width="100%" />
</center>

> 제안된 파이프라인 개요. (1) 시뮬레이션 데모를 수집하여 BC 기반 확산 정책($π_{base}$ )을 학습한다. (2) 해당 정책을 동결한 채 잔차 정책($π_{res}$ )을 강화학습으로 학습하여 성능을 향상시킨다. (3) 향상된 정책으로 시뮬레이션에서 고품질 궤적을 생성하고, (4) 소량의 실제 시연 데이터를 추가 수집한다. (5) 합성 데이터와 실제 데이터를 합쳐 최종 비전 기반 실세계 정책을 학습한다.

## 기술적 분석: 방법론과 기여

이 파이프라인은 크게 BC 학습, 잔차 RL, 증류의 세 단계로 구성된다.

- BC 기반 기본 정책($π_{base}$) 학습: 시뮬레이터에서 인간 조작으로 수집한 50개 안팎의 시연 데이터셋 $D_{sim}$ 으로 행동 복제(BC)를 수행한다. 이때 확산 정책(Diffusion Policy) 아키텍처를 사용하여 상대적으로 적은 데이터로도 복잡한 조작을 학습할 수 있게 했다. 또한 단일 행동 예측보다 여러 타임스텝의 행동 시퀀스를 묶어 예측하는 행동 청크(Action Chunking) 기법을 적용하여, 긴 시간 축에서의 계획 능력을 강화했다. 예를 들어 H=8의 청크를 예측하면, 실제로는 그중 선두 l개만 실행하도록 하여 탐색 안정성을 확보한다. 이 단계에서는 확산 모델과 청크 길이가 조작 성공률을 크게 좌우하는 요인임을 실험을 통해 확인했다.
- 잔차 정책($π_{res}$)을 통한 강화학습: BC 학습만으로는 조립 작업의 작은 오차조차 수정하지 못하기 때문에, $π_{base}$  위에 작은 잔차 정책을 온폴리시 PPO로 학습시킨다. 구체적으로 $π_{res}$ 는 현재 상태 s_t와 베이스 정책의 제안 행동 $a_t^{base}$을 입력으로 받아, 이 행동을 보정할 델타Δa_t를 생성한다. 즉, $a_t = a_t^{base} + αΔa_t$ 형태로 행동을 결정한다. 이때 $π_{base}$ 는 동결(frozen)되어 변경되지 않으며, $π_{res}$ 만 학습 대상이다. 이 설계에는 다음과 같은 장점이 있다: 우선 확산 모델 내부 구조를 수정할 필요 없이 일종의 블랙박스로 간주할 수 있다. 또한 $π_{base}$ 와 $π_{res}$ 의 행동 예측 간격이 다를 수 있는데, 통상적으로 RL은 한 스텝 단위의 행동 정책 최적화에 유리하다. 마지막으로, $π_{res}$ 는 작은 국소적 보정에 특화되도록 유도되므로, 베이스 정책의 작업 수행 능력 자체를 유지하며 잔차만 학습하기 때문에 파인튜닝 과정에서의 망각(forgotten capabilities) 문제도 줄어든다. 학습 시에는 상태 공간에 베이스 정책의 예측 행동을 추가로 포함시켜, $π_{base}$ 가 시뮬레이터에서 제안한 행동 궤적을 $π_{res}$ 가 각 시점에서 보정하도록 했다. 이 과정을 통해, 작은 위치 편차나 그랩 그립(gripper closing) 등의 성공/실패 조건에 민감한 “병목 구간”에서 $π_{res}$ 가 올바른 보정 동작을 학습하도록 유도한다.


<center>
<img src="../../images/2025-09-22-imitation-to-refine/4d3dafdc-d934-469a-952e-9c63e0f16c20.png" width="60%" />
</center>

> 그림 2. ResiP 잔차 정책 구조. 상단 회색 영역은 동결된 BC 기반 확산 정책($π_{base}$ )이 매 $T_a$ 간격으로 행동 청크 $a_t^(base)$를 예측하는 모습이고, 하단 보라색 영역은 잔차 정책( $π_{res}$ )이 매 시점마다 상태 s_{t+i}와 $π_{base}$ 의 행동 $a_{t+i}^{base}$ 를 입력받아 보정 행동을 생성하는 과정을 보여준다. 이러한 구조를 통해 미리 계획된 궤적을 따라가면서도 순간적인 목표 이탈을 보정할 수 있다.

- 교사-학생 증류와 도메인 랜덤화: 강화학습 단계가 수렴하면 $π_{res}$ 를 포함한 합성 정책 π_combined은 시뮬레이션에서 높은 성공률을 보인다. 이어서 RL로 학습된 정책을 기반으로 시뮬레이터에서 다양한 초기 조건하의 성공 궤적을 대량 생성한다. 이를 다시 카메라 시점의 RGB 영상으로 렌더링하여 합성 데이터셋 $D_{synth}$를 구축한다. 실제 데이터 부족 문제를 해결하기 위해, 소량의 실제 시연 데이터 D_real과 합쳐 최종 BC 학습용 데이터 D_real+synth로 사용한다. 이렇게 학습된 최종 비전 정책은 실제 환경의 전방/손목 카메라 영상만으로 조립 작업을 수행할 수 있다. 실험 결과, 시뮬레이션 데이터를 혼합할수록 실제 삽입 동작의 매끄러움과 성공률이 향상되었음을 확인했다. 또한 도메인 랜덤화를 통해 조명, 물체 색상·텍스처, 카메라 위치 등을 다양하게 변형하여, 시뮬→실 전이 차이를 줄였다.


이러한 방법론은 기존의 BC+RL 방식을 새롭게 재구성했다는 점에서 기여가 있다. 특히, 확산 정책이나 행동 청크와 같은 최신 정책 구조를 그대로 살리면서, 복잡한 RL 알고리즘 대신 잔차 MLP와 표준 PPO로 문제를 단순화한 점이 참신하다. 또한 시뮬레이터를 적극 활용한 교사-학생 프레임워크를 통해, 정밀 조립과 같은 실제 로봇 작업에서도 가능성을 보였다는 점도 의미 있다.

## 장단점 분석

- **성능 향상**: ResiP는 기본 BC 정책만으로는 달성하기 어려운 조립 과제에서 성공률을 극적으로 높였다. 예를 들어, one_leg 과제(초기 변동성 낮음)에서 BC 확산 정책의 성공률을 55%에서 95%로 끌어올렸고, 다른 과제들에서도 유의미한 증가를 보였다. 이는 BC만으로는 발견하지 못한 미세 조정을 RL 잔차가 보상했기 때문이다. 또한 직접 RL로 BC 정책을 파인튜닝하거나 Q-함수 기반 샘플링(IDQL)하는 대안들보다 안정적으로 성능을 높였다.
- **구조적 유연성**: 행동 청크와 확산 모델을 사용하더라도, 본 기법은 베이스 정책을 변경하지 않고 잔차만 학습하도록 설계되어 있다. 따라서 확산 정책·ACT 등 최신 BC 기법을 그대로 응용하면서도 RL 미세조정이 가능하다. 즉, 복잡한 확산 모델을 직접 강화학습할 때 요구되는 MDP로의 재정의나 로그 확률 계산 복잡도를 회피할 수 있다.
- **실제 적용 가능성**: 교사-학생 증류와 시뮬레이션-실제 데이터 혼합 덕분에, 최종적으로 RGB 영상만으로도 실제 로봇이 정밀 조립 작업을 수행할 수 있음을 보였다. 특히, 시뮬레이션 데이터를 포함시키면 삽입 동작의 소음(jerkiness)이 줄고 다양한 초기 부품 위치에서도 성능이 개선되었다. 이는 본 방법이 현실적 제약하에서도 일정 수준의 성능을 갖추었음을 시사한다.
- **제한점**: 그러나 ResiP의 잔차 정책은 국소적 보정에 최적화되어 있어, 큰 규모의 편차(예: 부품 낙하, 예상치 못한 기구 배치 변화)에는 대응하기 어렵다. 또한 초기 상태가 매우 무작위적이거나 시뮬레이터 분포 밖 상황에서는 베이스 정책과 잔차 모두 실패할 수 있다. 실험 결과, 랜덤성이 커질수록 성공률이 포화되는데, 이는 기본 정책 한계를 넘지 못한 잔차의 제약 때문으로 보인다. 아울러 실제 환경에서의 성공률은 아직 높지 않아(논문에서도 여전히 한계로 지적) , 보다 정교한 sim2real 전이나 추가 실험이 필요하다.
- **훈련 복잡도**: 마지막으로, 파이프라인이 여러 단계로 구성되어 있고 RL 학습을 포함하므로, 학습 과정이 비교적 긴 편이다. 특히 시뮬레이션 데이터 생성과 이미지 렌더링 등 추가적인 오버헤드가 있으며, 하이퍼파라미터 선택에도 주의가 필요하다. 그럼에도 불구하고 결과 성능 개선이 크다는 점은 장점으로 볼 수 있다.

## 결론 및 향후 방향

이 논문은 시연 기반 행동 복제의 한계를 보완하기 위해 잔차 강화학습(ResiP)을 제안했다. 동결된 BC 확산 정책 위에 학습되는 작은 보정 정책을 통해 정밀 조립 과제에서의 성공률을 크게 향상시켰으며, 별도의 복잡한 RL-확산 통합 방법 없이 표준 PPO만으로도 이러한 개선을 달성했다. 또한 시뮬레이터를 적극 활용한 교사-학생 증류 기법으로, 시뮬에서 획득한 정교한 제어 전략을 실제 RGB 비전 정책으로 이전할 수 있음을 보였다. 향후 연구로는 현재 잔차 정책이 해결하지 못하는 거시적 오류 복구 문제를 다룰 수 있다. 예를 들어, 드롭아웃이나 급격한 부품 배치 변화에 대응하기 위해 더 큰 스케일의 행동 수정, 혹은 추가적인 탐색 메커니즘이 필요하다. 또한 실제 환경에서 성공률을 높이기 위해 시뮬-실 전이 기법 개선 및 다양한 환경에 대한 일반화 연구가 기대된다. 종합하면, 본 논문은 BC와 RL을 효과적으로 결합하여 복잡한 조립 작업에 적용할 수 있는 실용적인 프레임워크를 제시했다는 점에서 큰 의의를 가진다. 꾸준한 후속 연구를 통해 로봇의 정밀 조립 능력이 한층 더 개선될 것으로 전망된다.

# Reference

- [Residual Off-Policy RL for Finetuning Behavior Cloning Policies](https://residual-offpolicy-rl.github.io/)
