---
title: "📃DrEureka 리뷰"
date: 2025-08-26
categories: [eureka, llm, domain randomization]
toc: true
number-sections: false
description: Language Model Guided Sim-To-Real Transfer
---

- [Paper Link](https://arxiv.org/pdf/2406.01967)
- [Project Link](https://eureka-research.github.io/dr-eureka/)
- [Code Link](https://github.com/eureka-research/DrEureka)

1. 🤖 이 논문은 시뮬레이션에서 학습된 로봇 정책을 실제 환경으로 전이시키는 과정에서 보상 함수와 시뮬레이션 물리 파라미터의 수동 설계 및 튜닝으로 인한 비효율성을 해결하기 위해 LLM 기반의 DrEureka를 제안합니다.
2. ⚙️ DrEureka는 LLM을 활용하여 안전 지침이 포함된 보상 함수를 자동 생성하고, 초기 정책을 통해 환경 물리 파라미터의 Reward-Aware Physics Prior (RAPP)를 구축한 뒤, 이 사전 정보를 바탕으로 Domain Randomization 구성을 최적화합니다.
3. 🚀 제안된 DrEureka는 사족 보행 및 정교한 조작 작업에서 기존의 수동 설계 방식과 대등하거나 더 나은 성능을 보였으며, 요가 볼 위에서 로봇 균형 잡기와 같은 새로운 도전적인 작업도 수동 개입 없이 성공적으로 해결하여 sim-to-real 전이를 가속화하는 잠재력을 입증했습니다.

---

# Brief Review

이 논문은 로봇 기술 습득을 가속화하기 위해 시뮬레이션에서 학습된 정책을 실제 환경으로 전이시키는 Sim-to-Real(시뮬레이션-실제 환경 전이) 과정에서 발생하는 수동 설계 및 튜닝의 문제점을 해결하고자 합니다. 특히, 보상 함수(reward function) 설계와 시뮬레이션 물리 파라미터의 도메인 랜덤화(Domain Randomization, DR) 분포 설정에 필요한 상당한 인적 노력과 시간을 줄이는 데 중점을 둡니다.

본 논문은 대규모 언어 모델(Large Language Models, LLMs)의 물리적 상식(physical common sense) 및 가설 생성 능력을 활용하여 이러한 Sim-to-Real 설계 과정을 자동화하는 DrEureka라는 새로운 접근 방식을 제안합니다. DrEureka는 대상 작업에 대한 물리 시뮬레이션만 주어지면, 실제 환경 전이를 지원하기에 적합한 보상 함수와 도메인 랜덤화 분포를 자동으로 구성합니다.


<center>
<img src="../../images/2025-08-26-dreureka/1.png" width="100%" />
</center>

DrEureka의 핵심 방법론은 세 가지 주요 단계로 구성됩니다:

1.  **LLM 기반 보상 함수 합성 (LLM-Guided Reward Function Synthesis):**
    *   이 단계는 Eureka [9]의 보상 함수 설계 원칙을 기반으로 하되, Sim-to-Real 설정에 맞게 개선되었습니다.
    *   LLM은 태스크 설명($l_{\text{task}}$)과 환경 코드($M$)에서 제공되는 환경 상태 및 액션 공간 요약을 입력으로 받습니다.
    *   특히, DrEureka는 프롬프트에 안정성(safety), 부드러움(smoothness), 바람직한 태스크별 속성 등을 명시적으로 고려하도록 "안전 지침(safety instruction)"($l_{\text{safety}}$)을 추가합니다. 이는 LLM이 실제 환경 전이에 더 적합한 안전 보장형(safety-regularized) 보상 함수를 생성하도록 유도합니다.
    *   LLM은 여러 보상 함수 후보를 코드로 생성하고, 각 후보는 강화 학습(Reinforcement Learning, RL)을 통해 정책을 학습하고 태스크 점수($F$)를 계산하여 평가됩니다.
    *   이러한 점수와 학습 통계(예: 학습 중 보상 구성 요소의 값)는 LLM에 피드백으로 제공되어 더 나은 보상 함수를 반복적으로 발전시킵니다.
    *   최종적으로, 이 단계에서는 최적의 보상 함수($R_{\text{DrEureka}}$)와 이에 상응하는 초기 정책($\pi_{\text{initial}}$)을 얻습니다:
        $R_{\text{DrEureka}}, \pi_{\text{initial}} := \text{Eureka}(M, l_{\text{task}} + l_{\text{safety}})$

2.  **보상 인지 물리 사전 생성 (Reward-Aware Physics Prior, RAPP):**
    *   안전한 보상 함수는 정책 행동을 규제하지만, Sim-to-Real 전이에는 충분하지 않습니다.
    *   DrEureka는 학습된 초기 정책($\pi_{\text{initial}}$)을 사용하여 도메인 랜덤화 파라미터에 대한 "보상 인지 물리 사전"(RAPP)을 구축합니다.
    *   RAPP의 목적은 $\pi_{\text{initial}}$이 여전히 성능을 발휘할 수 있는 환경 파라미터의 최대 다양성 범위를 찾는 것입니다. 이는 도메인 랜덤화가 태스크 보상 함수에 따라 달라져야 하며, 도메인 랜덤화 없이 학습된 정책 행동에 맞게 커스터마이징되어야 한다는 통찰에서 출발합니다.
    *   구체적으로, RAPP는 각 도메인 랜덤화 파라미터에 대해 "실행 가능한(feasible)" 값의 하한 및 상한을 계산합니다. 이는 광범위한 잠재적 값들을 탐색하고, 각 값을 시뮬레이션에 설정한 후($S.p = r$), $\pi_{\text{initial}}$을 이 수정된 시뮬레이션에서 실행합니다. 정책의 성능이 미리 정의된 성공 기준을 만족하면 해당 값은 해당 파라미터에 대해 실행 가능한 것으로 간주됩니다.
    *   각 파라미터에 대한 실행 가능한 모든 값 집합이 주어지면, 파라미터의 하한 및 상한은 최소 및 최대 실행 가능한 값으로 설정됩니다. 이 과정은 계산적으로 가볍고 병렬적으로 효율적으로 수행될 수 있습니다.

3.  **LLM 기반 도메인 랜덤화 구성 생성 (LLM for Domain Randomization):**
    *   이 최종 단계에서는 RAPP가 계산한 각 DR 파라미터에 대한 범위 정보를 LLM에 컨텍스트로 제공합니다.
    *   LLM은 이 컨텍스트를 바탕으로 (1) 랜덤화할 물리 파라미터의 부분집합을 선택하고, (2) 선택된 각 파라미터에 대한 랜덤화 범위를 결정하도록 지시받습니다.
    *   이는 LLM의 제로샷(zero-shot) 가설 생성 능력을 활용하여 여러 독립적인 DR 구성 후보($\mathcal{T}_1, \ldots, \mathcal{T}_m$)를 생성합니다.
    *   마지막으로, 생성된 보상 함수($R_{\text{DrEureka}}$)와 각 DR 구성($\mathcal{T}_i$)을 사용하여 RL을 통해 최종 정책($\pi_{\text{final},i} = \text{A}(M, \mathcal{T}_i, R_{\text{DrEureka}})$)을 학습하고 실제 환경에 배포합니다.

DrEureka는 quadrupedal locomotion 및 dexterous manipulation 태스크에 대한 평가에서 기존의 수동 설계 구성과 비교하여 경쟁력 있는 성능을 보였습니다. 예를 들어, quadruped locomotion 태스크에서 DrEureka로 학습된 정책은 인간 설계 보상 함수 및 DR 파라미터로 학습된 정책보다 전진 속도에서 34%, 이동 거리에서 20% 더 우수한 성능을 나타냈습니다. 또한, 요가 볼 위에서 균형을 잡고 걷는 것과 같은 새로운 로봇 태스크를 수동 설계 없이 해결할 수 있음을 입증했습니다. 이는 DrEureka가 로봇 기술 발견을 가속화할 잠재력을 가지고 있음을 시사합니다.


<center>
<img src="../../images/2025-08-26-dreureka/4.png" width="70%" />
</center>

요약하면, DrEureka는 LLM을 활용하여 Sim-to-Real 전이의 핵심 병목인 보상 설계와 도메인 랜덤화 파라미터 구성을 자동화함으로써, 인간의 개입 없이도 효과적이고 실제 환경에 강건한 로봇 정책을 학습할 수 있게 하는 혁신적인 프레임워크입니다.

---

# Detail Review

> DrEureka: LLM 활용 **Sim-to-Real** 전이 – 논문 리뷰 (RSS 2024)

**시뮬레이션에서 학습한 정책을 현실의 로봇에 그대로 이식(Sim-to-Real)** 하는 것은 로봇 기술 확장에 매우 유망한 접근입니다. 그러나 기존 **Sim-to-Real 전이** 기법들은 작업 **보상 함수** 설계와 시뮬레이터의 물리 **파라미터 튜닝**을 사람 손으로 반복 조정해야 했기에, 많은 시간과 노력이 들었습니다 . 이번 리뷰에서는 이러한 과정을 **대규모 언어 모델(LLM)**로 자동화하여 **시뮬레이션-현실 전이 설계**를 가속화한 연구 **“DrEureka: Language Model Guided Sim-To-Real Transfer”**를 살펴보겠습니다 . 이 논문은 2024년 Robotics: Science and Systems (RSS)에 발표되었으며, UPenn·UT Austin·NVIDIA 연구진이 협업한 결과물입니다. DrEureka는 **물리 시뮬레이터만으로** 인간 전문가 수준의 보상 함수와 도메인 랜덤화 구성요소를 자동 생성해, 별도 수작업 없이도 현실 로봇에 적용 가능한 정책을 학습시켰습니다 . 아래에서는 논문의 핵심 아이디어와 기여, 기술적 접근 방법, 실험 및 결과, 기존 대비 차별점, 한계와 향후 과제를 차례로 정리합니다.

## 1. 논문의 핵심 아이디어와 주요 기여
DrEureka의 핵심 아이디어는 **LLM을 활용해 Sim-to-Real 과정의 난점을 자동화**하는 것입니다. 구체적으로, 사람이 일일이 짜던 **보상 함수 설계**와 **도메인 랜덤화**(Domain Randomization) 파라미터 설정을 LLM이 대신 수행합니다  . 이를 통해 시뮬레이션에서 현실로 지식이나 정책을 옮길 때 필요한 까다로운 설계 작업을 대폭 줄이고, **자동화된 파이프라인**으로 신속하게 최적 구성을 찾아냅니다  .

### 주요 기여
- **LLM 기반 Sim-to-Real 자동화 기법 제안** – 보상 함수 설계와 물리 파라미터 **도메인 랜덤화**를 동시에 자동 구성하는 **DrEureka 알고리즘**을 제시  .
- **다양한 로봇 과제에 대한 실세계 검증** – 사족보행 로봇 *Unitree Go1*의 고속 전진 보행, 로봇 손(LEAP Hand)의 큐브 조작 등에서 **인간 전문가 설계보다 우수한 정책**을 달성  .
- **새로운 난제 과제에서의 성공적 전이** – *요가 공 위 걷기*라는 완전히 새로운 과제를 시뮬레이션 기반 자동 커리큘럼으로 학습하고, **현실 로봇에 곧장 적용**해 수 분간 균형을 유지하는 결과를 달성 .


## 2. 사용된 방법론과 기술적 접근 분석
DrEureka는 **세 단계 파이프라인**을 통해 환경과 보상 설계를 자동화합니다 .

1. **보상 함수 생성**
   - 과제 설명과 안전 지침을 LLM(GPT-4 등)에 제공.
   - LLM이 보상 함수 코드(예: 파이썬+NumPy)를 작성.
   - “안전 프롬프트”를 통해 **무리한 가속, 관절 손상**을 유발하는 보상을 피하고, **안전성과 효율성을 동시에 고려**하도록 유도  .
   - 여러 후보를 시뮬레이션에서 평가 후 성능이 가장 좋은 보상 함수를 선택 .

2. **RAPP (Reward-Aware Physics Prior)**
   - 선택된 정책을 시뮬레이터에 넣고, **물리 파라미터(마찰, 질량, 감쇠, 관성 등)를 하나씩 변화**시켜 정책 성능이 유지되는 범위를 찾음 .
   - 정책이 실패하지 않는 **최소–최대 허용 범위**를 기록하여, 이후 LLM에게 전달할 **물리 prior**로 활용  .
   - 이 단계는 **정책이 견딜 수 있는 물리적 한계**를 탐색하는 과정으로, 과도하게 넓은 랜덤화를 방지 .

3. **LLM 기반 도메인 랜덤화 생성**
   - LLM에 **랜덤화 가능한 파라미터와 RAPP 범위**를 제공 .
   - LLM이 어떤 파라미터를 어떤 분포로 샘플링할지 제안(예: 마찰 0.6–1.0 범위에서 균일 분포) .
   - LLM이 동시에 여러 후보안을 생성, 각각을 시뮬레이터에서 RL 훈련하여 최종적으로 **현실에서 가장 성능 좋은 정책** 선택  .

이 과정을 반복함으로써 DrEureka는 **LLM–시뮬레이터 공진화 루프**를 형성하고, **보상 설계 + 랜덤화 설계**를 인간 개입 없이 자동으로 최적화할 수 있습니다  .

## 3. 실험 설계 및 결과 평가
DrEureka는 두 가지 대표 과제와 하나의 도전적 과제를 통해 검증되었습니다.

- **사족보행 전진 (Go1 로봇)**
  - 목표: 2 m/s 속도로 빠르게 전진.
  - 비교: 인간 설계 보상+DR vs DrEureka 자동 설계.
  - 결과: DrEureka 정책이 **평균 속도 34%↑, 이동 거리 20%↑** 성능 향상 .
  - **지형 일반화** 평가(실내 바닥, 잔디, 보도블록, 양말 신긴 발)에서도 안정적 성능 유지 .

- **Dexterous Manipulation (LEAP Hand, 큐브 돌리기)**
  - 목표: 손바닥 위 큐브를 최대한 많이 회전시키기.
  - 결과: DrEureka 정책이 **인간 설계 대비 약 3배 더 많은 회전** 성공  .
  - 정책의 안정성이 높아 실제 로봇에서도 일관된 성능을 보임 .

- **요가 공 위 걷기 (Ball Balancing)**
  - 완전히 새로운 과제: 큰 요가 공 위에서 균형 잡으며 전진.
  - 결과: **시뮬레이션 학습 정책을 곧장 현실 적용**, 수 분간 균형 유지하며 걷기 성공 .
  - 교란(발로 공을 차거나 공의 공기압을 줄임)에도 로봇이 **스스로 균형을 회복**  .

이 실험들은 DrEureka가 **다양한 환경에서 학습한 정책이 시뮬레이션을 넘어 현실에서도 강건하게 동작**함을 보여주며, 기존의 수작업 기반 접근보다 뛰어난 성능과 일반화 능력을 입증했습니다 .


<center>
<img src="../../images/2025-08-26-dreureka/2.png" width="65%" />
</center>

<center>
<img src="../../images/2025-08-26-dreureka/3.png" width="65%" />
</center>


> *Unitree Go1이 다양한 실제 지면 위에서 DrEureka로 학습한 정책을 실행하는 모습. 기존 방법보다 **안정성과 속도 모두 우수**함을 보여줌  .*


<center>
<img src="../../images/2025-08-26-dreureka/5.png" width="100%" />
</center>

## 4. 기존 연구와의 차별점 및 기여
- **LLM in Sim-to-Real 설계**: 기존 LLM 기반 로봇 연구는 주로 **자연어를 고수준 계획으로 변환**하거나, 단순한 **환경 샘플링** 수준에 그쳤습니다  . DrEureka는 처음으로 **보상 함수와 도메인 랜덤화까지 포함한 Sim-to-Real 설계 전체**를 LLM이 주도할 수 있음을 보여주었습니다  .
- **인간 개입 최소화**: 기존 커리큘럼 설계나 도메인 랜덤화 연구는 대부분 **전문가가 파라미터 범위**를 손으로 지정해야 했습니다 . DrEureka는 **RAPP + LLM 조합**으로 이를 자동화하여 **효율성과 재현성**을 동시에 확보했습니다 .
- **실세계 성능 검증**: 많은 선행연구가 시뮬레이션에서만 결과를 보고한 것과 달리, DrEureka는 **실제 로봇 실험을 통해 직접 성능을 입증**했습니다  . 특히 **요가 공 위 걷기**와 같은 **새로운 과제**를 자동 설계하고 성공적으로 전이한 사례는 **최초의 결과**로 평가됩니다 .
- **물리 기반 LLM 활용**: LLM이 **물리적 상식(마찰, 중력, 관성 등)**을 활용하여 파라미터 범위를 합리적으로 설정하고, 그 근거까지 설명할 수 있다는 점은 **AI-로봇 공학 융합 연구**에서 중요한 전환점입니다  .


## 5. 한계점 및 향후 연구 과제에 대한 비판적 고찰
DrEureka는 강력한 결과를 보여주었지만, 몇 가지 **한계점**이 존재합니다  .

- **현실 피드백 부족**: 현재 DrEureka는 시뮬레이터만 사용하여 학습 후 현실에 곧장 이식하는 **Zero-Shot Sim-to-Real**을 목표로 합니다. 하지만 현실의 **노이즈·마찰·센서 오차**는 시뮬레이터에 완벽히 반영되지 않기 때문에, 일부 환경에서는 여전히 실패할 수 있습니다 . → **향후 과제**: 현실 실행 데이터를 LLM 프롬프트로 되먹임하는 **Sim-Real co-adaptation 루프** 필요.

- **감각 통합 한계**: 본 연구는 **proprioception 기반 제어**만 다루었고, **시각·촉각 정보**를 사용하지 않았습니다 . → **향후 과제**: **비전/멀티모달 정보**까지 통합해 더 복잡한 환경과 상호작용하는 정책 학습으로 확장해야 함.

- **시뮬레이터 품질 의존성**: DrEureka는 시뮬레이터의 물리 충실도가 낮으면 효과가 떨어질 수 있습니다 . → **향후 과제**: **시뮬레이터 신뢰성 개선** 또는 LLM이 **시뮬레이터 한계 자체를 인지/보완**할 수 있는 구조 필요.

- **LLM 비용 및 안정성**: GPT-4와 같은 LLM 사용에는 **비용(논문에서는 약 15달러/24시간)**과 **출력 품질 편차** 문제가 있습니다  . → **향후 과제**: **경량화된 LLM**이나 **프롬프트 최적화**, **출력 검증 체계**가 필요합니다.


## 결론
DrEureka는 **LLM의 코드 생성 및 물리 상식 활용 능력**을 Sim-to-Real 파이프라인에 접목하여, **보상 함수와 도메인 랜덤화 설계를 자동화**하는 최초의 프레임워크를 제시했습니다. 이를 통해 사족보행, 로봇 손 조작, 요가 공 걷기와 같은 **다양하고 난이도 높은 과제에서 실제 로봇 전이 성공**을 입증했으며, 기존 전문가 설계 기반 접근보다 **더 높은 성능과 일반화 능력**을 보여주었습니다  .

물론 아직 **현실 피드백의 부족**, **시뮬레이터 의존성**, **LLM 사용 비용** 등의 한계가 존재하지만, DrEureka는 **로봇 제어 학습 자동화**와 **LLM 기반 물리 시뮬레이션 디자인**의 가능성을 제시하며, 향후 로봇 연구의 **패러다임 전환**을 이끌 잠재력을 보여주고 있습니다.


# Related Works

- [Eureka 논문 리뷰](https://curieuxjy.github.io/posts/paper/2025-07-20-eureka.html)
