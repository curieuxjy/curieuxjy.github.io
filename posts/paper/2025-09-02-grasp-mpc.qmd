---
title: "📃GraspMPC 리뷰"
date: 2025-09-02
categories: [mpc, grasp]
toc: true
number-sections: true
description: Closed-Loop Visual Grasping via Value-Guided Model Predictive Control
---

- [Paper Link](https://arxiv.org/abs/2509.06201)
- [Homepage](https://grasp-mpc.github.io/)

1. Grasp-MPC는 복잡한 환경에서 새로운 물체를 견고하게 파지하기 위해 가치 함수(value function) 기반의 모델 예측 제어(MPC)를 활용하는 폐쇄 루프 6-DoF 시각 파지 정책을 제안합니다.
2. 이 가치 함수는 8천 개 이상의 Objaverse 객체에 대한 2백만 개 이상의 성공 및 실패 파지 궤적을 포함하는 대규모 합성 데이터셋으로 학습되어, 파지 성공 가능성을 예측하고 MPC 프레임워크 내에서 로봇을 안내합니다.
3. Grasp-MPC는 시뮬레이션 및 실제 환경에서 기존 개방 루프(open-loop) 및 폐쇄 루프(closed-loop) 방식을 최대 33.3% 능가하며, 예측 오류와 물체 자세 변화에 대한 견고성과 다양한 환경에서의 일반화 능력을 입증했습니다.

<center>
<img src="../../images/2025-09-02-grasp-mpc/overview.png" width="100%" />
</center>

---

# Brief Review

Grasp-MPC는 복잡한 환경에서 새로운 객체를 견고하고 반응적으로 파악하기 위해 설계된 폐쇄 루프 6-DoF 비전 기반 그립 정책입니다. 이 논문은 모델 예측 제어(MPC)와 데이터에서 학습된 가치 함수를 결합하여 개방 루프 및 폐쇄 루프 그립 방식의 장점을 통합합니다.

**핵심 방법론:**

1.  **대규모 합성 데이터셋 생성 (Data Generation):**
    *   8,515개의 Objaverse 객체를 사용하여 시뮬레이션 환경에서 2백만 개 이상의 그립 궤적(trajectory) 데이터셋을 생성합니다.
    *   궤적은 상용 그립 예측 모델에서 얻은 noisy한 pre-grasp 자세부터 ground-truth grasp 자세까지 모션 플래닝(CuRobo 사용)을 통해 생성됩니다.
    *   Pre-grasp 자세는 annotated grasp 자세에서 15cm의 고정 오프셋을 적용하고, $U(-0.04cm, 0.04cm)$의 병진 노이즈와 $U(-0.04\pi, 0.04\pi)$의 회전 노이즈를 추가하여 다양성을 확보합니다.
    *   생성된 궤적 중 물리적으로 그립 가능한 경우는 성공(successful)으로, 그 외는 실패(failed)로 레이블링되어 가치 함수 학습에 활용됩니다. 이 데이터셋은 총 1억 1,500만 개의 상태를 포함하며, 성공 궤적은 70.2%를 차지합니다.

2.  **가치 함수 학습 (Value Function Training):**
    *   가치 함수 $V(x_t)$는 주어진 상태에서 예상되는 잔여 비용(cost-to-go)을 근사하도록 학습됩니다. 여기서 상태 $x$는 분할된 객체 포인트 클라우드(segmented object point cloud)와 포인트 클라우드 중심에 대한 end-effector의 상대적인 포즈 $T_{\text{obj}}^{\text{EE}}$를 포함합니다. 포인트 클라우드는 입력을 표준화하기 위해 중심화됩니다.
    *   학습은 Bellman 에러 목적 함수를 사용하여 수행됩니다. 비용 $c_t$는 그립 목표 자세에 도달하고 물리적으로 가능한 경우 0으로, 그 외에는 1로 정의되는 sparse cost label입니다.
        $$c_t = \begin{cases} 0 & \text{if } |q_{\text{goal},i} - q_{t,i}| \le 5e^{-3}, \forall i, \text{ and } \mathbf{1}_{\text{feasible}} = 1 \\ 1 & \text{Otherwise} \end{cases}$$
        여기서 $q_{t,i}$는 시간 $t$에서의 $i$-번째 joint position, $q_{\text{goal},i}$는 목표 joint position, $\mathbf{1}_{\text{feasible}}$은 궤적이 가능한 그립에 해당하는지 여부를 나타냅니다.
    *   가치 함수는 다음 식을 최소화하도록 학습됩니다:
        $$\ell(\phi; x_t, c, x_{t+1}) = y_t - V_{\phi}(x_t)^2$$
        여기서 $y_t = c_t + \gamma V_{\phi'}(x_{t+1})$는 1-step target이며, $\gamma = 0.99$는 할인율(discount factor)입니다.
    *   가치 함수 네트워크는 PointNet++ 인코더(포인트 클라우드 처리)와 MLP(고유수용성, end-effector 포즈 처리)로 구성되며, 이들의 출력을 연결하여 최종 MLP 헤드로 값을 예측합니다. softplus 활성화 함수를 사용하여 항상 양의 값을 예측하도록 합니다.

3.  **MPC 내 가치 함수 통합 (Integrating a Value Function as a Grasp Cost within MPC):**
    *   학습된 가치 함수는 MPC의 비용 함수로 통합되어 온라인 배포 시 그립 비용을 최소화하도록 로봇을 안내합니다.
    *   MPC의 그립 비용은 예측 궤적의 각 상태에 대한 가치 함수의 할인된 합으로 정의됩니다:
        $$C_{\text{grasp}}(x_{h \in H}) = \sum_{t'=t}^{t+H} \gamma^{t'-t} V_{\theta}(x_{t'})$$
    *   최종 MPC 목적 함수는 CuRobo의 기본 비용(최소 jerk, 충돌 회피 등)에 가치 기반 그립 비용을 더한 것입니다:
        $$C_{\text{Grasp-MPC}} = C_{\text{curobo}} + \omega C_{\text{grasp}}$$
        여기서 $C_{\text{curobo}}$는 CuRobo의 기본 비용(world collision, self-collision, bounds cost), $\omega = 1000$은 가치 함수 비용의 가중치입니다.
    *   MPC는 GPU 가속 프레임워크인 CuRobo에 구현된 Model Predictive Path Integral (MPPI) 옵티마이저를 사용합니다.

**배포 (Deployment):**

Grasp-MPC는 상용 그립 예측 모델(M2T2) 및 모션 플래너와 결합하여 작동합니다. 로봇은 먼저 모션 플래너를 사용하여 pre-grasp 자세로 이동한 다음, Grasp-MPC를 통해 객체를 파악합니다. 이 과정에서 로봇 상태, 분할된 객체 포인트 클라우드, 그리고 환경의 Signed Distance Field(SDF, NVBlox로 표현) 피드백을 활용하여 충돌을 피하면서 그립 작업을 수행합니다.

**실험 결과:**

시뮬레이션(FetchBench) 및 실제 환경에서 광범위한 평가를 통해 Grasp-MPC는 개방 루프, Diffusion Policy, Transformer Policy, IQL과 같은 최첨단 방법론보다 우수한 성능을 보여줍니다. 특히, 시뮬레이션에서 최대 32.6%, 실제 환경의 노이즈가 많은 조건에서 최대 33.3%의 그립 성공률 향상을 달성했습니다. Grasp-MPC는 noisy한 그립 자세나 동적인 객체 교란에도 견고하게 작동하며, 학습 데이터셋이 빈 장면에서만 생성되었음에도 불구하고 복잡한 실제 환경(테이블 위, 선반 위 혼잡한 장면)에서 효과적인 그립 능력을 입증했습니다.

---

# Detail Review

> Grasp-MPC 논문 심층 리뷰 분석

## 1. 논문의 주요 기여 및 기술적 혁신 분석

*Grasp-MPC 프레임워크 개요: 다양한 객체 데이터셋(Objaverse의 8천여 개 객체)으로부터 파지(그립) 자세를 생성하고, 모션 플래닝을 통해 **200만 개 이상의 파지 궤적** 데이터를 시뮬레이션에서 수집한다. 이 궤적 데이터에는 성공한 시도와 실패한 시도가 모두 포함되며, 이를 활용해 **포인트 클라우드 관측** 기반의 **가치 함수**(value function)를 학습한다. 학습된 가치 함수는 파지 성공 확률을 예측하며 MPC의 비용 함수로 활용되어, 충돌 회피 등의 제약 조건 하에 로봇의 파지 동작을 실시간 생성한다. 오른쪽 그림은 Grasp-MPC를 사용하여 복잡한 환경의 새로운 물체를 안전하게 파지하는 UR10 로봇의 예시를 보여준다.*

**Grasp-MPC**는 복잡한 환경에서 새로운 물체들을 다룰 수 있는 **폐루프(closed-loop) 6-자유도 비전 기반 파지** 기법으로, 기존 접근법들의 한계를 극복하기 위해 제안되었다. 이 방법은 **open-loop 방식**(사전에 예측된 파지 자세로 이동)과 **closed-loop 제어**(실시간 피드백을 통한 조정)의 강점을 결합한 것이 핵심이다. 구체적으로, Grasp-MPC는 최신 **그립 포즈 예측 모델**을 이용해 로봇을 대략적인 **pre-grasp 자세**까지 먼저 이동시킨 후, 그 지점부터 **모델 예측 제어(MPC)**를 사용하여 폐루프 방식으로 파지를 완료한다. 이를 통해 기존 open-loop 기법이 겪는 **그립 위치 예측 오차**나 **물체 위치 변화**에 실시간으로 대응할 수 있으며, 로봇이 목표를 동적으로 조정하면서 파지를 성공시킬 수 있게 된다.

Grasp-MPC의 가장 큰 기술적 혁신은 **MPC의 비용 함수**로 **학습된 가치 함수**를 도입한 점이다. 전통적인 방법에서는 예측된 파지 자세까지의 거리 등의 **기하학적 비용함수**를 사용했지만, 이는 오차에 취약하고 MPC의 폐루프 잠재력을 충분히 활용하지 못했다. 반면 Grasp-MPC에서는 시뮬레이션을 통해 대규모로 수집된 **파지 궤적 데이터**(성공과 실패 사례 모두 포함)로부터 **비전 기반 가치 함수**를 학습하고 이를 **과제 비용(task cost)**으로 사용한다. 이 **가치 함수**는 주어진 물체의 포인트 클라우드와 로봇 말단Effector 자세를 입력으로 **파지 성공 가능성**을 예측하며, MPC 내부에서 **과제 성공도를 나타내는 비용 항**으로 작용한다. 결과적으로 로봇은 이 비용을 최소화하는 방향으로 상태 공간을 탐색하면서 파지 행동을 생성하게 되며, 이는 곧 파지 성공 확률을 최대화하도록 유도한다. 추가적으로, MPC의 최적화 과정에는 **충돌 회피(collision avoidance)**와 **동작의 부드러움(minimum jerk)**을 위한 비용 항도 포함되어 있어 복잡한 환경에서 **안전한 파지 경로**를 생성할 수 있다. 이러한 **학습된 가치 함수 기반 폐루프 제어**는 **동적이고 협소한 공간**에서도 파지를 **안정적**이고 **신뢰성 있게** 수행할 수 있게 해주는 본 논문의 핵심 기술적 기여이다.

논문에서 저자들이 강조한 Grasp-MPC의 **주요 기여**는 다음과 같이 정리된다:

* **안전한 폐루프 비전 기반 파지 정책 제안:** Grasp-MPC는 복잡한(cluttered) 환경에서 새로운 물체들을 다룰 수 있는 **안전한 폐루프 시각 파지 정책**을 제시한다. 즉, 실시간 센서 피드백을 통해 동적으로 조정하면서도, 충돌을 피하고 안정적으로 물체를 잡을 수 있는 정책이다.

* **모델 기반 제어와 데이터 기반 학습의 통합:** Grasp-MPC는 **모델 기반 제어 (MPC)**와 **데이터 기반 접근**을 통합하여, **학습된 그립 가치 함수**를 MPC 프레임워크에 결합한 새로운 구조를 구현했다. 이를 통해 **반응적이고 제약을 준수하는(grasping with reactive, constraint-aware)** 파지 실행이 가능해졌으며, 동적인 환경에서도 실시간 적응이 가능하다.

* **대규모 합성 파지 궤적 데이터셋 구축:** 저자들은 **대규모 시뮬레이션 파지 궤적 데이터셋**을 구축하였는데, **수백만 건(M=2,000,000+)의 파지 경로**와 상태, 그리고 **수천 개에 이르는 다양한 객체**를 포함한다. 이렇게 방대한 **Objaverse 객체**를 활용한 데이터셋은 일반화 가능한 가치 함수를 학습하는 데 기여하였으며, 이전 방법들보다 **훨씬 다양한 시나리오**를 포괄한다.

* **폭넓은 실험을 통한 성능 검증:** 시뮬레이션(FetchBench 벤치마크)과 실제 로봇 실험 모두에서 **광범위한 평가**를 수행한 결과, Grasp-MPC는 기존 최신 open-loop 및 closed-loop 방법들을 **의미 있게 능가하는 성능 향상**을 보여주었다. 특히 **시뮬레이션 환경에서 최대 32.6%**, **실제 복잡한 환경에서 33.3%**까지 파지 성공률을 개선하여, diffusion 정책, Transformer 기반 정책, IQL 등 다양한 기존 접근법 대비 **두 자릿수 이상의 향상**을 달성하였다. 이러한 결과는 본 기법의 **실용성**과 **일반화 성능**을 잘 뒷받침해준다.

요약하면, Grasp-MPC는 **대규모 학습된 가치 함수**를 **MPC 제어 루프**에 통합함으로써, 이전의 파지 방법들이 지니던 **실시간 피드백 부족 문제**, **일반화 한계**, **안전성 문제**를 모두 아우르는 **새로운 솔루션**을 제공한 것이다. 이러한 기술적 혁신은 향후 복잡한 로봇 조작 분야에서 **피드백 기반의 안전한 제어**와 **데이터 활용**을 접목시키는 데에 중요한 방향을 제시한다.

## 2. 실험 설정 및 결과 평가

논문에서는 **시뮬레이션**과 **실제 로봇** 환경에서 Grasp-MPC의 성능을 평가하여 그 **유효성**과 **우수성**을 입증했다. 실험은 다음과 같은 핵심 질문들을 중심으로 설계되었다:

1. **Ground Truth 파지 자세**가 주어졌을 때, Grasp-MPC는 새로운 물체를 얼마나 잘 파지하는가? (이론적으로 최적의 파지 위치가 주어진 경우 성능 평가)
2. **파지 자세에 오차/노이즈**가 있는 경우(예: 예측 오류로 인한 위치 편차), Grasp-MPC는 얼마나 견고하게 파지를 수행하는가?
3. **학습된 그립 포즈 예측 모델**을 사용하여 파지할 때(즉, 실제로는 이상적인 파지 자세가 아닌 **예측된 목표**로 파지 시도), Grasp-MPC의 성능은 어떠한가?

**시뮬레이션 환경 설정:** 저자들은 시뮬레이션 평가를 위해 FetchBench라는 표준 환경을 활용하였고, 로봇으로 **UR10 팔과 Robotiq 2F-140 그리퍼**를 사용했다. 평가 장면은 다양한 **복잡한(cluttered) 환경**으로 구성되었으며, 실험에 사용된 물체들은 모두 **새로운 객체**(학습 시 사용되지 않은 객체)로 채워졌다. 3개의 카메라로부터 **포인트 클라우드**를 수집하여 정책에 입력으로 사용하였고, 각 실험 장면마다 여러 가지 파지 시나리오를 구성하여 총 수천 회 이상의 시험을 진행했다. 성능 평가 척도로는 **파지 성공률**을 사용했는데, 이는 로봇이 물체를 집은 후 **일정 높이 이상 들어올리는지**로 정의된다. (모션 플래닝으로 pre-grasp 지점까지 이동하는 과정에서 실패한 경우는 파지 시도가 이루어지지 않았으므로 별도로 제외하여 성공률을 계산).

**비교 방법(베이스라인):** Grasp-MPC의 성능을 검증하기 위해 여러 기존 방법들과 비교 실험을 수행했다. **Open-loop** 접근의 대표로는 **OSC**(Operational Space Control) 기반 직선 이동 파지 방법을 사용하여, 이는 FetchBench에서도 oracle에 가까운 baseline으로 쓰이는 방식이다. 또한 **모방학습 기반** 폐루프 정책인 **Transformer Policy**도 포함되었는데, 이는 FetchBench 벤치마크에 사용된 Transformer 아키텍처의 정책이다. 더불어, **Diffusion Policy (확산 정책)** 방식의 폐루프 IL 정책도 평가에 포함되었다 – 이 방법은 포인트 클라우드를 입력으로 사용하는 **최신 상태의 모방학습 기반** 파지 정책 중 하나이다. 마지막으로, **오프라인 강화학습** 기반의 정책인 **IQL (Implicit Q-Learning)**을 비교에 포함시켰다. 모든 방법들은 Grasp-MPC와 동일하게 사전에 **모션 플래너(CuRobo)**를 이용해 로봇을 지정된 pre-grasp 위치까지 움직인 뒤, 그 지점부터 각자의 방법으로 파지를 수행하도록 설정되었다. (참고로 IL 기반 정책들은 학습 시 **성공한 파지 사례만**으로 훈련되었다고 명시되어 있다.)

**시뮬레이션 실험 결과:** 우선 **이상적인 파지 자세(ground-truth annotation)**가 주어지는 실험에서, Grasp-MPC는 **사전계획(open-loop) 기반의 Oracle 성능에 근접하는 높은 성공률**을 보였다. 구체적으로, Grasp-MPC의 파지 성공률은 약 73.6%에 달해, oracle에 해당하는 open-loop 방식(OSC)의 성능과 거의 유사한 수준을 달성했다. 더욱이 Grasp-MPC는 **다른 폐루프 기반 방법들보다 현저히 높은 성공률**을 기록했는데, 예를 들어 **IQL**의 경우 Grasp-MPC보다 훨씬 낮은 성공률(약 60%대)에 그쳤다. Transformer 기반 IL 정책과 Diffusion 정책 역시 Grasp-MPC보다 성능이 떨어졌으며, 이는 **모션 플래닝으로 수집된 제한적인 시演 데이터**로 학습한 데 따른 한계와, **훈련 환경과 평가 환경 간의 차이(domain mismatch)**로 인한 성능 저하 때문이라고 분석된다. (IL 모델들은 주로 빈 테이블 환경에서 데이터 수집이 이루어졌는데, 정작 평가 시에는 물체가 많은 복잡한 환경이라 **MDP 불일치**가 발생했고, 이로 인해 일반화 성능이 낮았다는 설명이다.)

다음으로 **파지 자세에 노이즈가 추가**된 실험(두 번째 질문)에서는 Grasp-MPC의 **강인함(robustness)**이 두드러졌다. Ground truth 파지 위치에 **무작위 위치 오차(수 센티미터 변위와 회전 노이즈)**를 섞어서 실행한 경우, **open-loop 방식(OSC)은 성능이 약 40%p 급락**하여 제대로 파지에 실패하는 반면, Grasp-MPC는 **약 14%p 정도의 경미한 성능 감소**만을 보이며 대부분의 시나리오에서 여전히 성공적으로 물체를 잡아냈다. 이는 폐루프 제어를 통해 피드백을 활용한 Grasp-MPC가 초기 목표 위치의 부정확함을 **실시간 보정**하며 대응하는 반면, open-loop은 한번 계획된 경로를 수정하지 못해 실패하기 때문이다. Grasp-MPC는 이 경우에도 다른 폐루프 baselines들보다 높은 성공률을 유지하여, **오프라인 RL이나 IL 기반 정책들보다 오차에 대한 내성이 높음**을 보여주었다.

세 번째로, **학습된 그립 포즈 예측 모델**을 실제로 활용하는 시나리오에서도 Grasp-MPC의 성능 우수성이 입증되었다. 저자들은 **M2T2**라는 최신 grasp pose 예측 모델을 사용하여 물체의 파지 목표 자세를 예측하고, 이를 각 방법들의 입력으로 사용했다. 예측된 파지 자세에는 필연적으로 오차와 노이즈가 존재하기 때문에, 이 설정은 실제 로봇 적용에 가까운 시나리오다. 그 결과 **IL 기반 방법들은 성공률 36.5% 수준에 그쳐 거의 파지에 실패**하였고, **open-loop 방식(OSC)은 약 63.6%**의 성공률을 보였다 (이 값은 ground truth 사용 시보다 약 **15%p 감소**한 수치이다). 반면 **Grasp-MPC는 67.2%의 성공률**로 가장 좋은 성능을 기록했으며, ground truth 대비 성능 감소폭도 **불과 8%p**에 그쳐 **예측 오차에 대한 견고함**을 뚜렷이 보여주었다. 즉, 학습된 그립 예측 모델의 출력이 완벽하지 않음에도 불구하고, Grasp-MPC는 폐루프 보정과 가치 함수 기반의 안정적인 제어를 통해 **최고의 파지 성공률**을 달성한 것이다. 이러한 결과는 Grasp-MPC가 **실제 로봇 현장에 투입**될 경우를 고려할 때, **예측 모델의 불확실성에 강인한 솔루션**임을 시사한다.

**실세계(real-world) 실험 설정:** 시뮬레이션에서 유의미한 성능을 보인 Grasp-MPC를 실제 로봇 환경에서도 검증하였다. 실제 실험에는 **UR10 로봇 팔**과 **Robotiq 2F-140 그리퍼**를 사용했고, **이질적인 3가지 환경**에서 평가가 이뤄졌다: (1) 물체가 거의 없는 **빈 테이블** 위, (2) 여러 새로운 물체들이 놓인 **복잡한 테이블 위(cluttered tabletop)**, (3) 물체들이 선반에 놓인 **복잡한 선반 환경(shelf clutter)**. 각 환경마다 서로 다른 객체 세트를 배치하여 다양성을 높였고, 각 물체에 대해 **세 가지 서로 다른 초기 자세**를 설정하여 반복 시험했다. 환경별로 수십 회 이상의 파지 시도를 통해 일관성과 신뢰성을 평가하였다. 로봇의 시각 센서는 **RealSense L515 깊이 카메라 2대**를 사용하여 실시간 **포인트 클라우드**를 생성했고, 목표 물체는 **SAM-Track** 기법으로 분리(segment)하여 인식했다. (SAM-Track은 Grounding DINO를 통한 객체 검출과 SAM(Segment Anything)으로 분할을 결합한 방법으로, 목표 물체의 포인트 클라우드를 추출해준다.) 또 주변 장애물을 다루기 위해 **NVBlox**를 이용해 환경의 장애물 맵을 생성하고, 이를 모션 플래닝과 MPC 모듈에서 고려하도록 설정하여 **충돌을 사전에 회피**하도록 했다. 파지 성공 기준은 시뮬레이션과 유사하게 **물체를 집어 올린 뒤 로봇의 홈 포지션까지 이동시키면서 한 번도 떨어뜨리지 않는 것**으로 정의되었다.

**실세계 비교 및 안전성:** 실제 환경에서는 폐루프 정책들의 안전성 문제가 있기 때문에, **비교 대상**으로는 **오픈 루프 기반**의 **CuRobo-GraspAPI** 방법만을 사용했다. 이 방법은 모션 플래닝으로 지정된 파지 자세까지 이동한 후 그대로 집는 **기존의 open-loop 파지 파이프라인**으로, 현실에서 비교적 안전하고 신뢰할 만한 기준으로 간주된다. 반면 앞서 시뮬레이션에 포함됐던 다른 폐루프 방식들(IL 기반 정책들 등)은 **충돌 회피 메커니즘이 없어 안전 문제**가 예상되므로 실제 로봇에는 적용하지 않았다. (예를 들어 선반이나 테이블에 로봇팔이 부딪칠 위험이 있어 제외했다는 설명이다.) 이에 비해 Grasp-MPC는 **MPC 최적화 자체에 충돌 회피 비용을 포함**하고 있어 주변 장애물이 있는 상황에서도 **안전하게 동작**할 수 있기 때문에, **실제 로봇 실험에 적합**하다는 점도 강조되었다.

**실세계 실험 결과:** 빈 테이블부터 복잡한 선반까지 **점진적으로 난이도가 증가하는 3가지 환경** 모두에서 Grasp-MPC는 **일관되게 open-loop 기준보다 높은 파지 성공률**을 거두었다. 구체적으로 Figure 8의 결과에 따르면, **어느 환경에서든 Grasp-MPC가 CuRobo(open-loop) 방법보다 성공률이 높았으며 복잡한 환경일수록 그 격차가 커졌다**고 보고된다. Open-loop 방식은 예측된 파지 자세가 이상적 위치에서 조금만 벗어나도 실행 중 경로를 수정하지 못해 **파지에 실패하는 사례가 잦았지만**, Grasp-MPC는 실행 도중 지속적으로 **그리퍼의 자세를 조정**하면서 **가치 함수 상의 비용을 최소화**하도록 동작하기 때문에, 장애물(예: 선반 가장자리 등)을 피하면서도 최종적으로 물체를 잡는 비율이 훨씬 높았다. 요약하면, **정적인 물체 파지 작업에서 조차도 Grasp-MPC가 open-loop 대비 뛰어난 적응력을 보여준** 것이다. 실제 예시로, Grasp-MPC는 선반 구석에 있거나 여러 물체 사이에 낀 목표 물체를 집을 때도 **중간에 그립 자세를 미세 조정**하여 성공적으로 파지하는 모습을 보였는데, 이러한 능력은 **기존 개방형 제어로는 불가능한 부분**이다.

한편, **동적인 변화에 대한 적응** 실험도 진행되었다. 이는 실제 로봇 환경에서 **폐루프 제어의 장점을 극대화**하는 시나리오로서, 로봇이 목표 파지 지점(pre-grasp)에 도달한 **후**에 의도적으로 물체의 위치를 이동(교란)시켜 보는 테스트이다. 이러한 돌발 상황은 일반적인 open-loop 접근으로는 대응이 불가능하므로, 해당 실험은 Grasp-MPC 단독으로 수행되었다. 실험에서는 여러 물체에 대해 각각 수 차례씩 큰 폭의 위치 교란을 주었는데, **Grasp-MPC는 물체가 갑자기 움직여도 즉각적으로 경로를 보정하여 끝내 파지에 성공**하는 높은 적응력을 보여주었다. 심지어 학습된 가치 함수는 주로 **5cm 이내의 비교적 작은 움직임**만 경험했음에도, **그 이상의 큰 물체 이동에도 전역적으로 파지를 재계획**하여 상당한 성공률을 달성했다는 점이 고무적이다. (정량적인 성공률 수치가 제시되지는 않았지만, 실행 예시를 통해 **Grasp-MPC가 실시간으로 움직이는 표적을 추적**하여 잡는 모습이 확인되었다고 한다.) 이는 Grasp-MPC의 폐루프 제어가 가지는 **실시간 적응성**을 잘 보여주는 대목으로, 실제 응용에서 물체가 떨어지거나 움직일 때도 **회복(graceful recovery)** 가능한 파지 시스템의 가능성을 시사한다.

결론적으로, 실험 결과들은 **시뮬레이션에서나 실제에서나** Grasp-MPC의 우수한 성능과 견고함을 입증한다. Grasp-MPC는 **대규모 데이터로 학습된 가치 함수** 덕분에 **새로운 물체에 대한 일반화 능력**이 뛰어나고, **MPC 기반 피드백 제어**를 통해 **환경 변화나 예측 오차에도 흔들리지 않는 파지 성공률**을 보여주었다. 특히 복잡한 실제 환경 (테이블, 선반 등)에서도 **추가 학습 없이** 곧바로 높은 성공률을 낸 점은, 이 접근법의 **실용적 가치**를 뒷받침하는 중요한 성과라 할 수 있다. Grasp-MPC는 센서 노이즈, 물리적 접촉 등의 현실 요인을 잘 견디며, 이는 **물리 기반 시뮬레이션 데이터에 의존하지 않고도** 달성된 것이라 더욱 주목된다.

## 3. 기존 연구와의 비교 분석

로봇 파지(grasping) 분야의 기존 연구들은 크게 **open-loop 방식**과 **closed-loop 방식**으로 양분된다. **Open-loop 파지 기법**들은 딥러닝으로 **그립 포즈(파지 자세)**를 예측한 후, 로봇팔을 해당 위치로 **모션 플래닝**해 이동시켜 파지하는 접근법을 사용해왔다. 대표적으로 물체의 3D 데이터베이스를 활용한 대규모 학습(예: Dex-Net 등)이나, 시뮬레이터에서 생성한 파지 annotation 데이터를 통한 학습 방법들이 이에 속한다. 이러한 open-loop 방법들은 비교적 **새로운 물체에 대한 파지 성공률이 높게 보고**되었으나, **실시간 피드백 부족**으로 인해 한 번 계획이 시작되면 경로를 유연하게 수정하지 못한다는 한계가 있다. 그 결과, **그립 포즈 예측 오류**나 실행 중 **물체의 움직임 변화**에 매우 취약하며, 복잡한 환경에서 물체가 서로 상호작용하거나 예측과 다른 위치에 있을 때 **실패율이 높아지는 문제**가 있었다.

**폐루프(closed-loop) 파지 기법**들은 이러한 문제를 극복하고자 **실시간 센서 피드백**을 제어에 도입한 방법들이다. **강화학습(RL)** 기반 접근과 **모방학습(IL)** 기반 정책학습이 이에 해당하며, 로봇이 카메라 등으로부터 주기적으로 관측을 받아 매 시점 행동을 결정하는 **정책(policy)**을 학습한다. 이러한 폐루프 방법들은 오픈 루프에 비해 **피드백으로 오차를 수정**할 수 있어 성공률 향상의 여지가 있지만, 현실적으로 **학습을 위한 데이터 수집이 어렵고 비싼** 문제가 있다. 많은 기존 연구들이 단순한 **테이블 위 단일 물체** 환경에서만 학습/평가되었고, 주어진 제한된 데이터로 학습된 정책은 **새로운 물체나 복잡한 장면으로 일반화**하기 어려웠다. 특히 다양한 물체에 대한 **대규모 파지 데이터셋 부족**이 병목이 되어, 폐루프 정책들의 성능은 제한적이었다. 또한 학습된 정책이 **충돌 회피**와 같은 **안전성**을 내재적으로 보장하지 못해, 복잡한 환경에 로봇을 투입하기에는 위험 요소가 많았다. 예컨대, 기존의 여러 RL/IL 기반 파지 논문들은 로봇과 주변 물체 간 충돌을 고려하지 않아서 실제 응용시 안전 문제가 지적되어 왔다.

이 논문의 Grasp-MPC 접근법은 **기존 대비 몇 가지 중요한 차별점**을 지닌다. 우선, **Open-loop와 Closed-loop의 장점 결합**이라는 관점에서, Grasp-MPC는 **사전 학습된 그립 예측 모델**과 **모션 플래닝**을 사용해 초기 파지 자세까지 접근하는 **모델 기반 단계**를 활용하면서도, 최종 파지 동작은 **MPC 폐루프 제어**로 수행함으로써 **두 접근법의 이점을 모두 취했다**. 이처럼 **모델 기반 + 데이터 기반**을 통합한 설계는 기존에 없던 새로운 틀로, open-loop 방식의 **빠른 초기 경로 설정** 능력과 closed-loop 방식의 **실시간 적응** 능력을 결합한 것이다. 특히 **MPC를 정책 실행기**로 사용한 점이 독특한데, 일반적인 RL과 달리 **정책 네트워크를 명시적으로 학습하지 않고** 가치 함수만으로도 MPC가 최적 행위를 찾아낼 수 있도록 했다. 이는 **오프라인 RL**의 철학과도 맞닿아 있는데, 기존 오프라인 RL 기법들은 대용량의 오프라인 데이터를 활용하면서도 **학습된 Q함수나 가치함수로부터 정책을 추출하는 과정**에서 어려움이 있었다. Grasp-MPC는 애초에 **MPC가 곧 정책**이므로 이러한 추출 과정이 불필요하며, **값 함수 학습 자체에 집중**할 수 있었다. 이러한 구조 덕분에 IQL과 같은 오프라인 RL 방법에 비해 **학습된 가치 함수를 효율적으로 활용**하여 더 높은 성능을 발휘한 것으로 보인다.

또한 **비용 함수 설계의 측면**에서, Grasp-MPC는 **학습 기반의 비용 함수(가치 함수)**를 도입하여 기존 방법들의 단점을 극복했다. 앞서 언급한 바와 같이, 기존에 Chen 등의 연구에서는 **예측된 그립 포즈와의 거리**를 기반으로 하는 값 함수를 MPC의 cost로 사용하려 했으나, 이러한 **단순 거리 기반 척도는 파지 성공에 중요한 여러 요인**(예: 손가락과 물체의 구체적인 접촉 관계나 물체의 무게 중심 등)을 반영하지 못해 결과적으로 **최적이 아닌 동작**을 유도하는 문제가 있었다. 또한 다른 시도 중 하나인 **CV-MPC**에서는 **소량의 데모**만으로 가치 함수 앙상블을 학습하였는데, 이는 **저차원 상태**(예: 로봇 joint 값 등)만 사용하여 학습하였기 때문에 **시각적 다양성**이나 **새로운 상황**에 적응하기 어려웠다. 반면 Grasp-MPC의 경우 **고차원 시각정보(포인트 클라우드)**와 **수백만 건의 다양한 시뮬레이션 궤적**으로부터 학습한 가치 함수를 사용함으로써, **파지 성공에 영향을 주는 미세한 요소들까지 비용에 반영**할 수 있었고 결과적으로 **일반화 성능**과 **성공률** 모두 향상시켰다. 이는 Grasp-MPC가 **MPC를 활용한 폐루프 제어** 분야에서 **처음으로 대규모 비전 기반 학습**을 결합한 사례로 평가할 수 있다.

**데이터셋 규모와 일반화** 측면에서도 Grasp-MPC는 기존 연구보다 앞선다. FetchBench 등 이전 연구들에서는 파지 데이터의 **양과 다양성의 한계**로 인해 성능이 제약되었는데, Grasp-MPC는 **Objaverse 기반으로 훨씬 큰 규모의 합성 데이터**를 생성하여 학습함으로써 이러한 한계를 돌파했다. 특히 FetchBench에서 사용된 Transformer IL 정책의 경우 제한된 시연 데이터로 학습돼 **복잡한 장애물 환경에서는 성능이 떨어졌으나**, Grasp-MPC는 더 **대규모·다양한 데이터**로 학습된 덕분에 **이질적인 환경에서도 견고한 성능**을 보였다. 실제 실험에서 Grasp-MPC가 **학습 때는 빈 환경만 경험했음에도 불구하고**, **복잡한 테이블이나 선반 환경**에서 별도 튜닝 없이 높은 성공률을 낸 점은 이러한 **일반화 능력 향상**을 잘 보여준다.

**안전성과 시스템 통합** 관점에서 보더라도, Grasp-MPC는 실용적인 우위를 갖는다. 기존의 폐루프 학습 정책들은 주로 **충돌 회피나 안전 제약을 고려하지 않고** 학습되었기 때문에, 실제 환경에서 로봇이 장애물과 충돌할 위험이 있었다. 하지만 Grasp-MPC는 **MPC 최적화 문제에 안전을 위한 제약(충돌 회피, 최소 jerk 등)**을 명시적으로 포함시켰기 때문에, **협소한 공간이나 장애물이 많은 상황에서도 안정적으로 동작**할 수 있었다. 이는 본 논문 실험에서도 입증되어, 다른 폐루프 방법들은 현실 환경에서 위험해 적용하지 못한 데 비해 Grasp-MPC는 선반 같은 복잡한 환경에서도 **무사고로 임무를 수행**했다. 나아가 이러한 모듈식 설계 덕분에, Grasp-MPC는 **새로운 제약 조건이나 환경 변화에 유연하게 대응**할 수 있다. 예를 들어 로봇의 작업 공간에 특정 **금지 영역**이나 **동역학적 제한**이 추가되더라도, MPC 문제에 해당 비용이나 제약을 넣으면 재학습 없이도 시스템에 반영될 수 있다. 이는 학습된 정책을 바꾸지 않고도 제어 단계에서 해결할 수 있기 때문에 **실제 응용에서의 편의성**을 높여준다.

마지막으로, **성능 측면에서의 비교**를 요약하면 다음과 같다. Grasp-MPC는 시뮬레이션 상에서 **5,400여 개의 다양한 파지 시나리오**를 실험한 결과, **모방학습(IL) 기반 방법들을 크게 앞서는 성공률**을 보였고, 기존 **계획 기반 방법(planning-based)**이 예측 오차나 센서 노이즈로 성능이 떨어지는 상황에서도 우수한 결과를 냈다. 오프라인 RL인 IQL과 비교해서도, IQL이 **정책 추출의 비효율**로 성능이 제한된 반면 Grasp-MPC는 더 높은 성공률로 그 격차를 보여주었다. 실제 로봇 실험에서도, **기존의 계획 기반 파지 파이프라인** 대비 Grasp-MPC가 **복잡한 테이블 및 선반 환경에서 더 높은 성공률**을 기록했으며, 이는 **학습 당시 접하지 않은 환경**에서도 통하는 일반화 능력을 입증한 것이다. 요컨대 Grasp-MPC는 현 시점에서 **개방형·폐루프 파지 접근법들 모두를 뛰어넘는 새로운 state-of-the-art 성능**을 달성한 것으로 평가된다.

한편, 저자들은 이러한 공헌에도 불구하고 남아있는 한계점도 언급한다. 예를 들어 **절대적인 성공률을 더 높이기 위해**서는 향후 **물리 시뮬레이션을 통한 더 정확한 성공/실패 레이블링**이나, **현실 데이터로의 파인튜닝** 등이 유효할 수 있다고 제안한다. 또한 현재 Grasp-MPC의 검증은 **파지(grasping)** 작업에 국한되어 있는데, 유사한 접근을 다른 조작(manipulation) 작업 (예: 도구 사용이나 비전 기반 위치 미세조정 등)에도 확장할 수 있을 것으로 기대하며, 이는 추후 연구과제로 남겨두었다고 밝혔다. 그럼에도 불구하고, Grasp-MPC는 기존 연구 대비 **데이터 규모, 알고리즘 구조, 실험 검증 면에서 새로운 기준을 세운 연구**로서 의의가 크다. 이는 로봇 파지 및 일반적인 **로봇 제어** 커뮤니티에서 **모델 예측 제어와 딥러닝 가치 함수의 융합** 가능성을 보여준 사례이며, 향후 더욱 **복잡한 조작 임무**에 폐루프 학습기반 제어를 적용하는 데에 밑거름이 될 것으로 전망된다.
