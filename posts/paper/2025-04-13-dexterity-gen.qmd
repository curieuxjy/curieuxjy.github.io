---
draft: true
title: "📃Dexterity-Gen 리뷰"
description: Foundation Controller for Unprecedented Dexterity
date: "2025-04-13"
categories: [paper, rl, hand]
toc: true
number-sections: true
image: ../../images/2025-04-13-dexterity-gen/idea.png
---

<center>
<img src="../../images/2025-04-13-dexterity-gen/1.png" width="100%" />
</center>

이번 포스팅에서 리뷰할 논문은 [**DEXTERITYGEN: Foundation Controller for Unprecedented Dexterity**](https://arxiv.org/pdf/2502.04307) 입니다. 

1. 🤖 다양한 작업에서 로봇 손의 안전하고 정교한 동작 생성.
2. 📊 인간의 텔레오퍼레이션을 통해 로봇의 고수준 정책을 보조.
3. 🛠️ 시뮬레이션 데이터셋을 통해 대규모 운동 기본(primitive) 요소를 학습.

# Brief Review

이 논문은 복잡한 손 조작 기술을 로봇에게 가르치는 데 있어 기존 방식들의 한계를 극복하고자 **DexterityGen (DexGen)**이라는 새로운 프레임워크를 제안합니다. DexGen은 강화 학습(RL)을 통해 대규모의 손 조작 동작 primitive를 사전 학습하고, 이를 바탕으로 튼튼한(robust) foundational controller를 학습합니다. 실제 환경에서는 인간의 텔레오퍼레이션(teleoperation)과 같은 외부 정책을 통해 DexGen을 프롬프트하여 고도의 손재주를 발휘할 수 있도록 합니다.

**핵심 방법론:**

DexGen의 핵심은 RL을 통해 생성된 다양한 시뮬레이션 데이터셋을 활용하여 행동 생성 모델을 사전 학습하는 것입니다. 이는 coarse한 동작 명령을 안전하고 유용한 로봇 동작으로 변환하는 것을 목표로 합니다. 

1.  **데이터셋 생성:** "Anygrasp-to-Anygrasp"라는 태스크를 중심으로 RL을 통해 다양한 물체 조작 궤적을 수집합니다. 이 태스크는 물체를 임의의 자세로 이동시키는 것을 목표로 하며, 다양한 손목 자세, 움직임, 도메인 랜덤화를 통해 데이터셋의 다양성을 확보합니다. RL 학습 과정에서 목표를 근접한 위치로 설정하여 학습의 효율성을 높입니다. 데이터셋은 목표 물체 자세와 손가락 관절 위치를 포함하는 goal-related reward, 스타일 관련 reward, 그리고 regularization term으로 구성된 보상 함수를 사용하여 생성됩니다.

    *   수식 표현:
        *   전체 보상 함수: $r = w_{goal}r_{goal} + w_{style}r_{style} + w_{reg}r_{reg}$
        *   Goal-related reward: $r_{goal} = exp(-\alpha_{pos}\|p_{obj} - p_{obj}^{target}\|^2 - \alpha_{orn}d(R_{obj}, R_{obj}^{target})) - \alpha_{hand}\|q - q^{target}\|^2 + \alpha_{bonus}\mathbb{1}(goal\ achieved)$
        *   Regularization term: $r_{reg} = -\alpha_{work}\|\dot{q}^T\|\|\tau\| - \alpha_{action}\|a\|^2 - \alpha_{\tau}\|\tau\|^2$
        *   Style reward: $r_{style} = \sum_{i} \alpha_{i} \|\dot{x}_i^{tip}\|$

2.  **모델 아키텍처:** DexGen은 두 개의 모듈로 구성됩니다.

    *   **Diffusion Model:** 로봇의 현재 상태가 주어졌을 때, 로봇 손가락 핵심점(keypoint)의 움직임 분포를 모델링합니다. 이는 UNet 기반의 모델을 사용하며, FiLM-conditioning을 통해 task/mode 정보를 주입합니다.
    *   **Inverse Dynamics Model:** 핵심점 움직임을 실행 가능한 로봇 동작(관절 목표 위치)으로 변환합니다. 이는 residual multilayer perceptron (MLP)으로 구현됩니다.

3.  **추론 (Inference):** 학습된 분포에서 안전한 동작을 샘플링하는 동시에, 입력된 참조 동작을 최대한 보존하는 방향으로 동작을 생성합니다. 이를 위해 gradient guidance를 diffusion sampling 과정에 적용하여 motion conditioning을 수행합니다. 이는 다음 수식으로 표현됩니다.
    *   $\Delta x \sim p_\theta(\Delta x|o) \exp(-Dist(\Delta x, \Delta x^{input}))$
    *   $Dist(\Delta x, \Delta x^{input}) = \sum_{i=1}^T \|\Delta x_i - \Delta x^{input}\|^2$

**결과:**

시뮬레이션 및 실제 환경 실험을 통해 DexGen의 효과를 검증합니다. 시뮬레이션에서는 DexGen이 suboptimal한 정책의 견고성(robustness)과 성공률을 크게 향상시키는 것을 보여줍니다. 실제 환경에서는 인간 텔레오퍼레이터를 통해 DexGen을 활용하여 다양한 물체 재정렬, 주사기 사용, 드라이버 사용과 같은 복잡한 손 조작 기술을 성공적으로 수행하는 것을 입증합니다. 특히, 물체를 떨어뜨리지 않고 유지하는 시간이 10배에서 100배까지 향상되는 결과를 보여줍니다.

**전반적으로, DexGen은 coarse한 명령을 섬세한 손 동작으로 변환하여 로봇의 손재주를 크게 향상시키는 foundational controller로서의 가능성을 보여줍니다.**

# Detail Review


> DEXTERITYGEN: 전례 없는 기교를 위한 **Foundation Controller** 리뷰

## 배경: Dexterous Manipulation과 Foundation Model 접근  
**Dexterous manipulation (정교한 물체 조작)**는 인간 손과 유사한 다자유도 로봇 손을 이용해 물체를 섬세하고 능숙하게 다루는 문제입니다. 
이러한 dexterous manipulation 기술을 로봇에게 가르치는 일은 매우 어렵습니다. 기존 접근법은 크게 두 갈래로 나뉩니다. 
- 첫째, **인간 원격조작 (teleoperation)**을 통한 **모방학습** 방법입니다. 사람 운영자가 VR 장갑 등으로 로봇 손을 원격 제어하며 시연 데이터를 모아 정책(policy)을 학습시키는 방식이지만, **촉각 피드백 부재**와 **다른 구조의 로봇 손 조작의 난해함** 때문에 **안전하고 정교한 동작**을 인간이 만들어내기가 어렵습니다. 
- 둘째, **시뮬레이션 강화학습 (sim-to-real RL)** 방법으로, 가상 환경에서 강화학습으로 정책을 훈련한 후 실제 로봇에 이전(sim-to-real)하는 접근입니다. 하지만 이 방법도 **시뮬레이션-현실 간 도메인 차이**로 인한 성능 저하와, 복잡한 작업에 필요한 **특수한 보상함수 설계** 등의 문제가 있습니다.  

이러한 한계 때문에, 최근 로봇 제어 분야에서는 **대규모 사전학습 모델 (foundation model)** 개념을 도입한 새로운 접근들이 주목받고 있습니다. 
**DeepMind의 Gato** 모델이 대표적인 예로, 하나의 거대 **Transformer** 네트워크로 **604가지에 달하는 다양한 과제**를 학습하여 **멀티모달** 정책을 구현했습니다. 
Gato는 동일한 파라미터로 아타리 게임 플레이, 이미지 자막 생성, 대화, **로봇 팔로 블록 쌓기** 등의 서로 다른 작업을 모두 수행해 **“Generalist Agent”**의 가능성을 보였지만, 주로 비교적 단순한 **平行 그리퍼 로봇 (parallel gripper)**의 조작 등에 한정되어 **정교한 손동작**까지 보여주지는 못했습니다. 

한편, **Google의 Robotics Transformer (RT-1)**은 13대의 로봇을 통해 **17개월간 130k 에피소드의 실세계 데이터**를 수집하여 **700가지 이상의 작업**을 학습한 거대 비전-모터 정책입니다 ([Robotics Transformer: RT-1](https://robotics-transformer1.github.io/#:~:text=To%20test%20RT,contain%20over%20700%20tasks)). 
RT-1은 카메라 영상과 작업 지시어를 입력 받아 **End-to-End**로 로봇 제어 명령을 출력하는 Transformer로, 새로운 물체나 환경에서도 일반화된 성능을 보였습니다. 
이후 버전인 **RT-2**에서는 거대 비전-언어 모델을 접목하여 시각적 개념과 언어지식을 활용하는 방향으로 확장되었지만, 이들 역시 **집게(gripper)** 형태 로봇 조작에 초점을 맞춰 **다손가락 섬세 조작**에는 직접 적용하기 어렵습니다.  

또 다른 흐름으로 **Diffusion Policy** 방법론이 있습니다. 이는 로봇의 정책을 **확률적 생성 모델**로 간주하여 **확산 모델 (diffusion model)**로 구현한 접근입니다. 
예를 들어 **Diffusion Policy**는 로봇의 **시각-운동 (visuomotor) 정책**을 조건부 확산 과정으로 표현하여 **단일 정책이 다모달 행동 분포를 학습**하도록 합니다. 
확산 모델은 **멀티모달 행동** 분포를 효과적으로 캡처하여, 시뮬레이션 로봇 조작 벤치마크에서 LSTM 기반 정책이나 강화학습 기반 방법들을 능가하는 성능을 보였습니다. 
특히, **10Hz의 저빈도 제어 출력**을 생성하고, 이미지 지연 등의 **latency 보정**을 위해 한 번에 여러 스텝의 액션을 예측하는 **receding horizon** 특성을 활용하는 등, **다단계 행동 생성(autoregressive)**과 **멀티모달 의사결정**에 강점을 보였습니다. 
하지만 기존 diffusion policy 연구들은 주로 단순한 팔-그리퍼 조작이나 짧은 수평 이동 등의 작업에 집중되어 있어, **고차원 다지 관절**을 가진 **로봇 손의 세밀한 제어**에 직접 적용된 사례는 드물었습니다.

요약하면, 대규모 **Foundation Model** 기반 접근(Gato, RT 계열)과 **Diffusion** 기반 정책 등이 로봇 제어에 도입되어왔지만, **다자유도 로봇 손의 섬세한 조작**이라는 영역에서는 **안전성**과 **정밀 제어** 측면의 난제가 여전히 남아 있습니다. 
**DexterityGen (DexGen)**은 이러한 격차를 해소하기 위해 고안된 새로운 **foundation 컨트롤러**로, 인간 원격조작과 강화학습의 장점을 결합해 전례없는 수준의 기교있는 로봇 손동작을 달성했습니다.

## DexterityGen의 주요 아이디어: RL로 학습한 Skill Prior + Teleop 프롬프트  

DexterityGen(이하 DexGen)은 “Foundation Controller”, 즉 범용적인 저수준 제어 모듈을 지향합니다. 
핵심 아이디어는 강화학습(RL)으로 다양한 low-level 손동작 스킬을 미리 학습해 **스킬 프라이어(prior)**를 만들고, 이를 기반으로 **인간 원격조작 명령**을 **안전하고 세밀한 동작으로 변환**시키는 **생성형 정책**을 구축하는 것입니다. 
저자들은 `“RL은 저수준 모션 프리미티브를 배우는 데 효과적이고, 인간은 고수준의 거친 동작 명령을 제공하는 데 뛰어나다”`는 통찰을 강조하며, 두 접근의 결합이 최적 해법임을 주장합니다. 
실제로 DexGen은 시뮬레이션에서 RL로 방대한 다중 작업 데이터셋을 생성하여 정책을 사전 학습하고(Training 단계), 실제 로봇 제어 시에는 인간 teleoperator의 조작 신호를 프롬프트로 받아 세밀한 액션을 생성합니다(Inference 단계). 
이렇게 함으로써, **사람이 직접 조작할 때 발생하는 위험하거나 불안정한 동작을 모델이 학습된 안전 동작 분포 내의 행동으로 투영(projection)하여 실행**하게 됩니다.  

* DexGen의 트레이닝/인퍼런스 개요. 
 
**왼쪽**: 시뮬레이션에서 다중 작업 데이터셋을 모아 DexGen **생성모델**을 학습한다. 모델은 주어진 상태에서 다양한 정교한 동작들을 확률적으로 생성하도록 훈련된다 (분포 예시: 회전, 병진 등). 

**오른쪽**: 실제 인퍼런스 시, 인간 **Teleop** 또는 상위 정책이 낸 거친 **Motion** 명령이 DexGen의 분포 상에 투영되어 안전한 행동을 산출한다. 이 과정을 통해 고차원 핸드의 복잡한 조작을 안정적으로 수행할 수 있다.*

**데이터 수집 및 사전학습:** DexGen의 학습을 위해 **대규모 시뮬레이션 데이터셋**인 **“Anygrasp-to-Anygrasp”**를 구축합니다. 이는 다양한 물체를 임의의 그립(grasp) 상태에서 다른 임의의 그립 상태로 옮기는 **범용 조작 과제**로, **다양한 손-물체 상호작용**의 핵심을 포괄하도록 설계되었습니다. 구체적으로, 각 물체에 대해 가능한 모든 잡기 구성을 탐색한 뒤 이를 **초기 상태**와 **목표 상태**로 설정하고, RL 에이전트가 한 그립에서 다른 그립으로 물체를 옮기도록 학습시킵니다. 학습된 RL 정책을 다수 rollout하여 **손-물체 이동 궤적 데이터**를 대량으로 모읍니다. 저자들은 **100,000개 이상의 고유 그립**을 샘플링하여 데이터 범위를 극대화했고, 추가로 **자유 손가락 운동**이나 **정밀 회전 조작** 등의 과제를 더해 **미세 동작** 데이터도 확보했습니다. 이렇게 모인 **멀티태스크 데이터**는 이후 **생성모델의 학습데이터**로 사용됩니다. 한편, 이러한 접근은 기존 **foundation model**들이 보통 **실세계 시演 데이터**에 의존하는 것과 대조적입니다. DexGen은 **시뮬레이션 데이터로 사전학습**함으로써 방대한 데이터 수집에 필요한 인간 노동을 줄이고, **도메인 랜덤화** 등을 통해 현실 적응력을 확보하고자 했습니다.

## DexGen의 모델 구성: Autoregressive 생성 정책과 Goal/State Network 구조  

DexGen의 컨트롤러는 크게 두 모듈로 구성됩니다: **(1) 생성 정책 모듈**과 **(2) 역기구학 모듈**입니다. 

첫 번째 **생성 정책**은 **확산 모델 (diffusion model)** 기반으로 구현된 **확률적 정책 생성기**로서, 현재 로봇 **상태(state)**를 입력받아 미래의 손동작 **분포**로부터 **샘플을 생성**합니다. 

두 번째 **역기구학(inverse dynamics)** 모듈은, 생성된 **손가락 움직임(motion)**을 실제 로봇의 **구동 명령(action)**으로 변환하는 역할을 합니다. 
이때 **기술적인 용어**를 빌리자면, 정책 모듈은 **현재 상태**를 처리하는 부분(이것을 편의상 **StateNet**이라 지칭)과, **외부 목표 동작** 정보를 반영하는 부분(마찬가지로 **GoalNet**이라 지칭)으로 나눠볼 수도 있습니다. 
DexGen의 구조에서 **StateNet**은 로봇 손의 과거 몇 스텝에 걸친 **프로프리오셉션(proprioception) 상태** 입력을 받아 **내재 상태 표현**으로 인코딩합니다. 한편, **GoalNet**에 해당하는 부분은 명시적인 네트워크 모듈로 존재하는 것은 아니지만, **외부에서 주어지는 목표 동작(Motion Conditioning)**을 생성 과정에 반영하는 **메커니즘**이 도입되어 있습니다. 즉, DexGen은 **인간 조작자가 의도하는 손가락 움직임**을 직접 네트워크 입력으로 취하지 않고, **생성 과정 중에 목표 방향으로 샘플을 유도하는 방식(guidance)**으로 반영합니다. 이러한 **Guided Sampling** 전략을 통해 모델은 학습 시 분포에서 벗어나는 **비안전한 명령**을 걸러내면서도, **사용자 명령의 의도**는 최대한 살린 행동을 생성할 수 있습니다.  

**Diffusion 기반 Autoregressive Policy:** DexGen의 정책 모듈은 **UNet 기반** 확산 모델로 구현되었으며, 한 번에 **Horizon H**만큼의 **미래 손가락 움직임 오프셋 시퀀스**를 예측합니다. 예컨대 0.1초 간격으로 5스텝 미래 (H=5, 0.5초) 정도의 **손가락 위치 변화**를 예측하도록 학습되고, 제어 주기는 10Hz로 동작합니다. 이는 **autoregressive**하게 정책을 생성하는 것과 유사한데, 매 시간스텝마다 현재 상태를 반영해 **단기 계획**을 출력하고, 실행 후 다시 업데이트된 상태로 다음 계획을 세우는 **receding horizon** 제어 구조를 가집니다. **Mode conditioning** 입력도 함께 제공하여, 특정 작업 모드(예: 드라이버로 나사를 조일 때의 정밀회전 모드)를 모델에 알려줄 수 있습니다. (대부분의 데이터는 “default” 모드로 학습되고, 특별한 경우에만 레이블링된 모드를 사용.) 이 생성 정책은 **확산모델 표준 학습목표**(노이즈 예측 손실)로 학습되며, 학습된 모델은 다양한 **회전, 이동, 재그립** 등의 동작을 **조건부 확률분포** 형태로 내재하게 됩니다. 중요한 점은, **외부 Motion 프롬프트**는 이 확산 정책 네트워크에 **직접 주입되지 않고**, 대신 **샘플링 단계에서의 Gradient Guidance**로 활용된다는 것입니다. 이는 학습 시 **분포 왜곡** 없이 안정적인 모션 분포를 익히게 하고, 추론 시에만 인간 명령을 **살짝 끌어당기는 역할**을 하도록 분리한 설계입니다.  

**Inverse Dynamics 모듈:** 위 확산 정책이 출력하는 **손가락 키포인트의 움직임**은 로봇 손가락의 **목표 관절각(position)**으로 변환되어야 실제 제어에 사용할 수 있습니다. DexGen에서는 이를 위해 **잔차(residual) 구조의 다층퍼셉트론(MLP)**으로 단순한 **역기구학 모델**을 구현했습니다. 이 MLP는 현재 로봇 상태와 생성된 **Motion**을 입력받아, 각 조인트의 목표 위치를 확률적으로 출력합니다 (평균과 분산을 내는 Gaussian 출력). 학습 시에는 **MSE 손실**로 해당 동작이 실제 데이터의 조인트 변위와 일치하도록 학습되었습니다. 이 모듈은 비교적 단순한 회귀망이지만, 상술한 **생성 정책의 출력 (미래 손끝 움직임)**을 물리적인 **로봇 명령**으로 바꿔주는 필수적인 역할을 합니다.  

*DexGen 컨트롤러의 **모듈 구조** (논문 Figure 4 재구성). 

**Diffusion Model**은 현재 **State** (로봇 상태)와 **Mode**를 입력받아 다다익선한 **Motion**(손가락 키포인트 움직임)을 생성한다. 이 때 **외부 Motion Conditioning** (사용자의 목표 동작)은 네트워크 입력으로 직접 들어가지 않고, **Gradient Guidance** 방식으로 확산 모델의 샘플을 적절히 유도한다. 이렇게 생성된 **Motion**은 **Inverse Dynamics Model**에 의해 로봇 관절 **Action** 명령으로 변환된다.*  

**Guided Sampling (모션 조건 유도):** DexGen이 **인간 텔레오퍼레이터의 명령**을 반영하는 방식은 확산 모델의 **Guided Sampling** 기법으로 구현되었습니다. 구체적으로, 모델이 예측한 다음 손가락 위치 시퀀스가 **입력된 목표 움직임**과 가까워지도록, 확산 과정의 각 스텝에서 **손가락 오프셋 차이**에 대한 **그래디언트**를 샘플에 가이드로 더해주는 방식입니다. 간단히 말해, **생성 중인 동작**이 **학습된 안전 동작 분포**를 유지하면서도 **사용자 명령 방향**으로 약간 편향되도록 힘을 주는 것입니다. 이때 가이드 세기인 λ 값에 따라 **DexGen의 보수성**이 조절됩니다. λ가 너무 작으면 모델이 안전하게만 굴러가서 사용자가 원하는 목표에 충분히 못 미치게 되고, λ가 너무 크면 오히려 사용자 명령(때로는 서투르거나 위험한 동작)이 모델의 안전장치를 이겨버릴 수 있습니다. 논문 실험에서는 모든 실험에 잘 맞는 **공통의 중간 세기**를 찾아 적용했다고 합니다. 결과적으로, DexGen은 **학습된 저수준 행동 분포**를 기반으로 **미세 조정**만 가해 안전성과 목표추종을 동시에 만족시키는 행동을 생성합니다. 이러한 접근은 인간 조작 신호를 **강화학습 정책의 관측으로 넣는** 기존 **Shared Autonomy** 방식들과 차별화되며, 고차원 손가락 제어에서 **사용자 의도에 따른 연속적 미세 행동 샘플링**을 가능케 한 점에서 의의가 있습니다.

## 실험 결과: 안정성 10~100배 향상, 인간-로봇 협업을 통한 전례없는 성과  

저자들은 DexGen의 성능을 시뮬레이션과 실제 로봇 실험으로 검증하였습니다. 

먼저 시뮬레이터에서, 앞서 생성한 다양한 RL 정책들과 결합하여 DexGen이 얼마나 성능을 향상시키는지 평가했습니다. 
Figure 8에서, **노이즈가 섞인 suboptimal 전문가 정책**에 DexGen을 적용한 경우와 아닌 경우를 비교하는데, DexGen 없이 원 시연 정책만 사용할 경우 물체를 제대로 잡지 못하고 계속 떨어뜨리는 **catastrophic failure**가 빈번했습니다. 
반면 DexGen이 보조해준 경우에는 **객체를 손에서 떨어뜨리지 않고 유지**하는 시간이 월등히 증가했고, 심지어 **대부분이 잡음뿐인 엉망인 정책**조차 DexGen이 일부 교정하여 **목표 동작을 어느 정도 성공**시키는 사례를 보였습니다. 논문에서는 DexGen이 다양한 상위 정책에 대해 **물체 홀딩 지속시간을 10배에서 최대 100배까지** 늘려주었고, **기존에 실패하던 경우에도 성공을 달성**하게 해주었다고 보고합니다. 이는 DexGen이 **외부 정책의 불안정성을 크게 완화**시켜준다는 것을 의미합니다. 특히 **지도 신호 λ**에 따른 영향도 분석했는데, λ=0 (guidance 없음)에서는 안전하지만 목표 달성은 안 되고, λ가 너무 크면 안전성이 떨어지는 현상을 확인하여, 적절한 trade-off 지점을 찾아 사용했습니다. 이 결과는 DexGen의 guided sampling 접근이 성공적으로 작동함을 보여줍니다.

**실제 로봇 실험**에서는 **Allegro 4-핑거 로봇 핸드 + Franka 암** 조합으로 다양한 난이도의 덱스터러스 작업을 수행했습니다. 
인간 조작자는 원격지에서 master 장치를 통해 **고수준 움직임**을 명령하고, DexGen 컨트롤러가 이를 받아 **손가락 세밀 조작**을 실시간 생성합니다. 
평가한 작업으로는 (1) **공중에서 물체 잡고 임의 방향으로 재배향하기** (In-hand Object Reorientation), (2) **툴 핸들 재그립** 후 **파워 그립 유지**하기 (Functional Grasping), (3) **물체를 손가락 사이에서 다른 그립으로 옮겨잡기** (In-hand Regrasping) 등이 있었습니다. 
또한 이러한 기본 스킬들을 조합해야 하는 **펜 잡고 쓰기**, **주사기 사용하기**, **드라이버로 나사 조이기** 같은 **장시간(long-horizon) 공구 사용 작업**도 도전하였습니다.  

그 결과, DexGen 없이는 대부분의 작업에서 인간 조작만으로는 실패하던 것이, DexGen의 도움으로 높은 성공률을 기록했습니다. 
예를 들어, “큰 물체 세로 방향 재배향” 작업에서 **Teleop 단독**으로는 20회 중 0회 성공, 평균 물체 홀딩 시간 5% (거의 바로 드롭)였으나, **Teleop + DexGen**의 경우 20회 중 **12회 성공**, 홀딩 시간 **75%**로 크게 향상되었습니다. 
작은 물체 재배향도 0→13회 성공으로 개선되었고, 공구 재그립 작업 또한 Teleop 단독 0%→ DexGen 보조 50~70%대로 성공률이 올랐습니다. 
아래 표는 대표적인 평가 과제들에 대한 **성공률(SR)**과 **물체 떨어뜨리기 전까지 유지 시간(TTF)**을 비교한 결과입니다:

| **작업**            | **Teleop (성공/시도)** | **Teleop TTF** | **Teleop + DexGen (성공/시도)** | **DexGen TTF** |
|-------------------|--------------------|----------------|-----------------------------|----------------|
| 큰 물체 재배향 (세로 방향)  | 0/20               | 5%             | 12/20                       | 75%            |
| 작은 물체 재배향 (세로 방향) | 0/20               | 5%             | 13/20                       | 79%            |
| 공구 수직 재그립 (파워그립)  | 0/10               | 5%             | 7/10                        | 87%            |
| 공구 수평 재그립 (파워그립)  | 1/10               | 10%            | 6/10                        | 80%            |
| 작은 공 공중 재그립       | 0/10               | 5%             | 5/10                        | 78%            |
| 실린더 공중 재그립        | 0/10               | 5%             | 5/10                        | 74%            |

*표 1: 주요 실제작업에서 **Teleop 단독** vs **DexGen 보조** 성능 비교 (성공률과 홀딩 시간). DexGen의 도움 없이는 인간이 물체를 거의 바로 떨어뜨려 성공할 수 없던 작업들도, DexGen을 통해 안정적으로 물체를 다루며 절반 이상 성공하는 것으로 나타났다.* 

특히 **펜으로 쓰기, 주사기 사용, 드라이버 사용**과 같은 복잡한 작업은 여러 단계를 차례로 수행해야 하는데, DexGen은 이러한 **장기간 조작에도 처음으로 도전하여 부분적인 성공**을 거두었습니다. 

- 예를 들어 `드라이버 작업`의 경우, “물체 재배향→재그립→나사 홈 정렬→조이기”의 4단계 중, **재배향은 16/20회**, **재그립 11/20회**, **드라이버 끝을 홈에 맞추기 5/20회**, 최종적으로 **나사 조이기 동작 완수 3/20회 성공**하였습니다. 
- `주사기 사용` 역시, “재배향→재그립→밀어 넣기” 3단계 중 최종 **밀어 넣기 4/20회** 성공으로 완전한 성공률은 아직 낮지만 **단계별로는 꽤 높은 성공 비율**을 보였습니다. 

이는 해당 작업의 난이도를 고려하면 유의미한 성과로, 저자들은 `“세계 최초로 이러한 장기간의 복잡한 기교 조작을 Teleop으로 실현했다”`고 강조합니다. 
비록 여러 단계를 연달아 성공적으로 수행하는 **연계 성공률**은 낮았지만, 이는 각 서브스킬의 성공률을 향상하고 통합하는 추가 연구로 극복 가능할 것으로 전망하고 있습니다.

한편, DexGen과 자주 비교될 수 있는 기존 방법들과의 차이를 정리하면 다음과 같습니다:

- **대규모 사전학습 데이터:** 기존의 Gato, RT-1 등 **로봇 Foundation 모델**들은 **실세계**의 거대한 시연 데이터로 학습된 반면, **DexGen은 전적으로 시뮬레이션 데이터**로 학습되었습니다. 이는 현실 데이터 수집의 어려움을 우회하면서도, Domain Randomization 등을 통해 현실 적용력을 확보하려 한 것으로, **sim-to-real**의 새로운 가능성을 제시합니다.  
- **고차원 다지 로봇 손 조작:** 이전의 일반ist 정책들은 주로 2-finger 그리퍼와 같은 **저차원 제어**에 집중되었습니다. DexGen은 **Degrees of Freedom 16**의 **Allegro Hand**를 효과적으로 제어함으로써, **정교한 손동작 분야에서 generative pretraining의 성공적인 사례**를 처음 보여주었습니다. 이는 Rubik’s Cube 풀기(OpenAI Dactyl) 등 단일 과제 RL 성공 이후로, **다양한 물체와 도구 사용**에까지 범용성을 확장한 의의가 있습니다.  
- **연속적 프롬프트에 의한 저수준 제어:** 대부분의 기존 **로봇 파운데이션 모델**은 **자연어 지시**나 **태스크 ID**처럼 **이산적이고 상위 수준**의 conditioning만을 받습니다. 반면 DexGen은 **연속적인 미세 동작**을 **프롬프트**로 받아들이고, 이를 실시간 행동으로 변환하는 **저수준 컨트롤러**라는 점에서 차별화됩니다. 이는 향후 고레벨 플래너나 언어 모델과 **결합하여 작동하는 기본 제어기**로 활용될 수 있음을 시사합니다.

## 결론 및 전망  

DexterityGen 프로젝트는 **인간-로봇 협업**과 **생성형 정책**의 결합으로 로봇 손의 잠재력을 크게 끌어올린 사례입니다. 
요약하면, DexGen은 **시뮬레이션에서 학습한 거대한 skill prior**를 활용하여, **현실에서 인간의 고수준 조작을 안전한 세부 행동으로 구현**하는 **foundation controller**를 제안했습니다. 
이를 통해 기존에 어려웠던 다양한 물체의 공중 조작, 도구 사용 등을 최초로 시연하며, **안정성 측면에서 10~100배 향상**이라는 놀라운 결과를 보였습니다. 
DexGen의 등장은 향후 로봇 연구에 여러 흥미로운 방향을 열어줍니다. 
예를 들어, 이 low-level dexterous controller를 high-level task planning이나 언어지시 정책과 결합하면, 더욱 복잡한 작업을 인간의 간단한 명령으로 수행하는 멀티레벨 로봇 제어 시스템을 구성할 수 있을 것입니다. 
또한 DexGen이 가능케 한 **안정적인 공구 사용** 데이터 수집을 바탕으로, 추후에는 **완전 자율적인 섬세 조작** 정책으로도 발전시킬 수 있을 것입니다. 
저자들도 DexGen이 `“미래 로봇 에이전트의 핵심 빌딩블록”`으로서 역할하며, 앞으로 끝없는 새로운 가능성을 열어줄 것으로 기대하고 있습니다.  

궁극적으로, **DexterityGen**은 **다자유도 로봇 손 제어** 분야에서 **파운데이션 모델** 개념을 성공적으로 적용한 첫 사례 중 하나로서, **복잡한 인간 수준의 기교**를 로봇에게 학습시키는 방향에 큰 이정표를 세웠습니다.
향후 연구를 통해 남아있는 한계점들 – 예를 들어 장기 연쇄 동작의 완성도 향상, 촉각센서 결합, 실시간 성능 최적화 등 – 이 개선된다면, 진정한 의미의 범용 조작 로봇에 한 걸음 더 다가설 수 있을 것으로 보입니다.


# Reference

- [Original Paper](https://arxiv.org/abs/2502.04307)
- [Project Homepage](https://zhaohengyin.github.io/dexteritygen/)