---
title: "MambaVision 리뷰"
date: 2025-07-19
categories: [mamba, transformer, vision]
toc: true
number-sections: true
description: A Hybrid Mamba-Transformer Vision Backbone
---


- [Paper Link](https://arxiv.org/pdf/2407.08083)
- [Github Link](https://github.com/NVlabs/MambaVision)

1.  ✨ MambaVision은 비전 애플리케이션을 위해 Mamba와 Transformer의 장점을 결합한 새로운 하이브리드 백본 아키텍처입니다.
2.  🧐 이 모델은 비전 작업에 맞게 Mamba 블록을 재설계하고, 특히 마지막 계층에 self-attention 블록을 전략적으로 통합하여 장거리 공간 의존성을 효과적으로 포착합니다.
3.  🚀 MambaVision은 ImageNet-1K 분류에서 SOTA 성능을 달성했으며, MS COCO 및 ADE20K 데이터셋의 객체 감지 및 분할과 같은 다운스트림 작업에서도 우수한 결과를 보이며 높은 처리량과 효율성을 제공합니다.


<center>
<img src="../../images/2025-07-19-mambavision/2.png" width="60%" />
</center>

---

# Brief Review

MambaVision은 시각 애플리케이션을 위해 특별히 설계된 새로운 하이브리드 Mamba-Transformer 백본입니다. 이 연구의 핵심 기여는 시각 특징의 효율적인 모델링 능력을 향상시키기 위해 Mamba 공식을 재설계한 것입니다. 포괄적인 ablation 연구를 통해 Vision Transformers (ViT)를 Mamba와 통합하는 것이 가능함을 입증합니다. 특히, Mamba 아키텍처의 최종 레이어에 Self-Attention 블록을 추가함으로써 장거리 공간 의존성(long-range spatial dependencies)을 포착하는 능력이 크게 향상됨을 보여줍니다. 이러한 발견을 바탕으로, 다양한 설계 기준을 충족시키는 계층적 아키텍처를 가진 MambaVision 모델군을 소개합니다.

**1. 주요 기여**

*   **시각 친화적인 Mamba 블록 재설계**: 원본 Mamba 아키텍처 대비 정확도와 이미지 처리량(throughput)을 향상시켰습니다.
*   **Mamba와 Transformer 블록의 통합 패턴 체계적 조사**: 최종 단계에 Self-Attention 블록을 통합하는 것이 모델의 전역적 맥락(global context) 및 장거리 공간 의존성 포착 능력을 크게 향상시킴을 입증했습니다.
*   **새로운 하이브리드 Mamba-Transformer 모델 MambaVision 도입**: ImageNet-1K 데이터셋에서 Top-1 정확도와 이미지 처리량 측면에서 새로운 SOTA Pareto frontier를 달성했습니다.

**2. 방법론**

**2.1. 매크로 아키텍처**

MambaVision은 4개의 다른 스테이지로 구성된 계층적 아키텍처를 가지고 있습니다.

*   **초기 두 스테이지 (Stage 1, 2)**: 고해상도 특징의 빠른 추출을 위해 CNN 기반 레이어를 사용합니다. Stem은 3x3 CNN 레이어 두 개로 구성되며, 스트라이드(stride) 2를 사용하여 입력 이미지를 오버랩 패치(overlapping patches)로 변환하고 채널 C의 임베딩 공간으로 투영합니다. 스테이지 사이의 Downsampler는 스트라이드 2의 3x3 CNN 레이어로 이미지 해상도를 절반으로 줄입니다. CNN 블록은 다음과 같은 일반적인 잔차 블록(residual block) 공식을 따릅니다:
    $$\hat{z} = \text{GELU}(\text{BN}(\text{Conv3x3}(z)))$$
    $$z = \text{BN}(\text{Conv3x3}(\hat{z})) + z$$
    - 여기서 GELU는 Gaussian Error Linear Unit 활성화 함수를, BN은 Batch Normalization을 나타냅니다.
*   **후기 두 스테이지 (Stage 3, 4)**: 제안된 MambaVision 및 Transformer 블록을 포함합니다. N개의 레이어가 주어진 경우, 첫 N/2개의 레이어는 MambaVision 믹서 블록을 사용하고, 나머지 N/2개의 레이어는 Self-Attention 블록을 사용합니다. 특히, Transformer 블록을 최종 스테이지에 배치함으로써 손실된 전역적 맥락을 복구하고 장거리 공간 의존성을 포착할 수 있도록 합니다.

**2.2. 마이크로 아키텍처**

**2.2.1. Mamba 기초**

Mamba에서 1D 연속 입력 $x(t) \in \mathbb{R}$는 학습 가능한 은닉 상태 $h(t) \in \mathbb{R}^M$와 매개변수 $A \in \mathbb{R}^{M \times M}$, $B \in \mathbb{R}^{M \times 1}$, $C \in \mathbb{R}^{1 \times M}$를 통해 $y(t) \in \mathbb{R}$로 변환됩니다:
$$h'(t) = Ah(t) + Bx(t)$$
$$y(t) = Ch(t)$$
이 연속 매개변수들은 계산 효율성을 위해 이산 매개변수로 변환됩니다. 시간 스케일 $\Delta$를 가정한 영차 보류(zero-order hold) 규칙이 적용되어 이산 매개변수 $\bar{A} \in \mathbb{R}^{M \times M}$, $\bar{B} \in \mathbb{R}^{M \times 1}$, $\bar{C} \in \mathbb{R}^{1 \times M}$를 얻습니다:
$$\bar{A} = \exp(\Delta A)$$
$$\bar{B} = (\Delta A)^{-1}(\exp(\Delta A) - I) \cdot (\Delta B)$$
$$\bar{C} = C$$
이산 매개변수를 사용한 수식은 다음과 같습니다:
$$h(t) = \bar{A}h(t-1) + \bar{B}x(t)$$
$$y(t) = \bar{C}h(t)$$
또한, 시퀀스 길이 T를 가진 입력 시퀀스에 대해, 커널(kernel) $K$를 가진 전역 컨볼루션(global convolution)이 Eq. 4의 출력을 계산하는 데 사용될 수 있습니다:
$$K = (CB, CAB, ..., CA^{T-1}B)$$
$$y = x * K$$
Mamba는 S4 공식을 확장하여 선택성(Selectivity) 메커니즘을 도입하는데, 이는 입력에 의존하는 시퀀스 처리를 가능하게 하여 모델의 매개변수 B, C, $\Delta$가 입력에 따라 동적으로 조정되고 관련 없는 정보를 필터링할 수 있게 합니다.

**2.2.2. 레이어 아키텍처**

입력 $X \in \mathbb{R}^{T \times C}$에 대해, Stage 3과 4의 레이어 $n$의 출력은 다음과 같이 계산됩니다:
$$\hat{X}_n = \text{Mixer}(\text{Norm}(X_{n-1})) + X_{n-1}$$
$$X_n = \text{MLP}(\text{Norm}(\hat{X}_n)) + \hat{X}_n$$
여기서 Norm은 Layer Normalization을, Mixer는 토큰 믹싱 블록(token mixing block)을 나타냅니다. N개의 레이어가 주어졌을 때, 첫 N/2개의 레이어는 MambaVision Mixer 블록을 사용하고, 나머지 N/2개의 레이어는 Self-Attention을 사용합니다.

**MambaVision Mixer**

MambaVision Mixer는 시각 작업에 더 적합하도록 원본 Mamba 믹서가 재설계되었습니다.

*   **인과적 컨볼루션(causal convolution) 대체**: 시각 작업에서는 불필요하고 제한적인 단일 방향 영향(single direction influence)을 제거하기 위해 인과적 컨볼루션을 일반 컨볼루션(regular convolution)으로 대체했습니다.
*   **대칭적 브랜치 추가**: SSM의 순차적 제약으로 인해 손실될 수 있는 콘텐츠를 보상하기 위해 SSM이 없는 대칭적 브랜치를 추가했습니다. 이 브랜치는 추가 컨볼루션과 SiLU (Sigmoid Linear Unit) 활성화로 구성됩니다.
*   **출력 연결**: 두 브랜치의 출력(SSM 및 비-SSM)을 연결한 다음 최종 Linear 레이어를 통해 투영합니다. 이 조합은 최종 특징 표현이 순차적 및 공간적 정보를 모두 통합하여 두 브랜치의 강점을 활용하도록 합니다.
입력 $X_{in}$에 대해 MambaVision Mixer의 출력 $X_{out}$은 다음과 같이 계산됩니다:
$$X_1 = \text{Scan}(\sigma(\text{Conv}(\text{Linear}(C, C/2)(X_{in}))))$$
$$X_2 = \sigma(\text{Conv}(\text{Linear}(C, C/2)(X_{in})))$$
$$X_{out} = \text{Linear}(C, C)(\text{Concat}(X_1, X_2))$$
여기서 $\text{Linear}(C_{in}, C_{out})(\cdot)$는 입력 및 출력 임베딩 차원이 각각 $C_{in}$ 및 $C_{out}$인 Linear 레이어를 나타냅니다. $\text{Scan}$은 선택적 스캔(selective scan) 연산이며, $\sigma$는 SiLU 활성화 함수입니다.

**Self-attention**

표준 Multihead Self-Attention 메커니즘을 사용하며, 다음 공식에 따라 계산됩니다:
$$\text{Attention}(Q, K, V) = \text{Softmax}( \frac{QK^T}{\sqrt{d_h}} )V$$
여기서 Q, K, V는 각각 쿼리(query), 키(key), 값(value)을 나타내며, $d_h$는 어텐션 헤드(attention head)의 수입니다. 이 프레임워크는 이전 연구와 유사하게 윈도우 방식(windowed manner)으로 어텐션을 계산할 수 있습니다.

**3. 실험 및 결과**

**3.1. 이미지 분류**

ImageNet-1K 데이터셋에서 MambaVision은 Conv-based, Transformer-based, Conv-Transformer, Mamba-based 모델 등 다양한 모델군과 비교하여 ImageNet Top-1 정확도와 이미지 처리량 측면에서 SOTA 성능을 달성합니다. 예를 들어, MambaVision-B는 ConvNeXt-B (83.8%) 및 Swin-B (83.5%)보다 높은 84.2%의 정확도를 달성하면서도 훨씬 더 높은 이미지 처리량을 보입니다. Mamba-based 모델과 비교해서도 MambaVision-B (84.2%)는 VMamba-B (83.9%)를 능가하며, 훨씬 높은 처리량을 제공합니다. 또한 MambaVision 모델은 유사한 크기의 모델에 비해 FLOPs가 훨씬 낮습니다.

**3.2. 객체 감지 및 분할**

MS COCO 데이터셋에서의 객체 감지 및 인스턴스 분할, ADE20K 데이터셋에서의 의미론적 분할 성능을 평가했습니다. MambaVision 백본을 사용하는 모델은 동급 크기의 경쟁 모델을 능가하는 성능을 보였습니다. 예를 들어, MS COCO에서 MambaVision-T는 ConvNeXt-T보다 box Average Precision (AP)에서 +0.7, mask AP에서 +0.6 향상되었고, Swin-T보다 box AP에서 +0.7, mask AP에서 +0.6 향상되었습니다. ADE20K에서 MambaVision-T, S, B는 Swin-T, S, B보다 mIoU에서 각각 +1.5, +0.6, +1.0 향상되었습니다. 이는 MambaVision이 다양한 비전 작업에 효과적이고 다용도 백본임을 검증합니다.

**3.3. Ablation 연구**

*   **ImageNet-21K 대규모 사전 훈련**: Mamba 기반 접근 방식 중 최초로 대규모 ImageNet-21K 데이터셋으로 훈련 스케일링을 성공적으로 수행했습니다. MambaVision-B의 Top-1 정확도가 84.2%에서 84.9%로 향상되었으며, MambaVision-L은 85%에서 86.1%로 향상되었습니다. MambaVision-L3 (739.6M 매개변수)는 256 해상도에서 87.3%, 512 해상도에서 88.1%의 Top-1 정확도를 달성하여 모델의 확장성을 입증했습니다.
*   **Token Mixer 설계**: MambaVision 토큰 믹서의 체계적인 설계를 위해 포괄적인 ablation 연구를 수행했습니다. 인과적 컨볼루션을 일반 컨볼루션으로 대체하고, SSM이 없는 대칭적 브랜치를 추가하며, 이 두 브랜치의 출력을 연결하는 것이 성능 향상에 결정적임을 입증했습니다. 특히, 연결(concatenation) 방식은 ImageNet Top-1 정확도, MS COCO의 box AP, mask AP, ADE20K의 mIoU에서 상당한 개선을 가져왔습니다.
*   **하이브리드 패턴**: Self-Attention과 MambaVision 토큰 믹서 간의 다양한 하이브리드 통합 패턴을 조사했습니다. Self-Attention 블록을 각 스테이지의 마지막 N/2 레이어에 배치하는 것이 가장 좋은 성능(82.3%)을 달성했으며, 이는 전역적 맥락 및 장거리 공간 의존성 포착을 위해 Self-Attention을 최종 레이어에 배치하는 것이 중요함을 시사합니다.
*   **해석 가능성 (Interpretability)**: MambaVision의 Self-Attention 레이어에서 생성된 어텐션 맵(attention map)을 시각화하여 모델이 명시적인 지도 없이도 의미론적으로 중요한 영역에 집중하는 것을 확인했습니다. 이는 Self-Attention 블록을 최종 단계에 사용하는 아키텍처 설계 선택을 뒷받침합니다.

**4. 결론**

이 연구는 시각 애플리케이션에 특화된 최초의 Mamba-Transformer 하이브리드 백본인 MambaVision을 소개합니다. 전역적 맥락 표현 학습 능력을 향상시키기 위한 Mamba 공식 재설계를 제안했습니다. MambaVision은 Top-1 정확도와 이미지 처리량 측면에서 새로운 SOTA Pareto frontier를 달성하며, Transformer 및 Mamba 기반 모델을 상당한 차이로 능가합니다. 분류, 감지, 분할을 포함한 여러 시각 작업에 대한 광범위한 실험을 통해 접근 방식의 다용도성과 효과를 입증했습니다. 통합 패턴에 대한 체계적인 분석은 Self-Attention 블록을 최종 레이어에 배치하는 것이 효율성을 유지하면서 모델의 장거리 의존성 포착 능력을 크게 향상시킨다는 것을 밝혔습니다. 또한, MambaVision을 ImageNet-21K 사전 훈련에 성공적으로 스케일링하여 SOTA 모델과 견줄 만한 강력한 성능을 달성함으로써 대규모 비전 애플리케이션의 잠재력을 보여주었습니다. MambaVision의 성공은 순수 Mamba 기반 아키텍처의 한계를 해결하면서 강점을 활용하여 비전 백본 설계에 새로운 가능성을 열었습니다.

---

## +

**Mamba의 장점**

- **선형 시간 복잡도 (Linear time complexity):** Mamba는 State Space Model(SSM) 기반으로, 입력 시퀀스를 효율적으로 처리하도록 설계되어 Transformer의 주된 병목인 self-attention의 쿼드러틱(제곱) 연산 복잡도를 선형으로 줄였습니다. 이는 긴 시퀀스나 고해상도 이미지 패치 처리 시 매우 효율적입니다.
- **하드웨어 친화적 설계와 선택적 처리 (Selective processing):** 입력 의존적인 동적 파라미터 조정을 통해 중요한 정보에 집중하면서 불필요한 정보를 걸러내, 효율성과 표현력을 동시에 높입니다.
- **우수한 시계열 및 연속 데이터 모델링 능력:** SSM 기반 특성상 연속적이고 순차적인 데이터 패턴을 잘 포착할 수 있습니다.

**Transformer의 장점**

- **전역 문맥 인식 (Global context modeling):** Self-attention 메커니즘 덕분에 이미지 내 모든 위치 간 상호작용을 동시다발적으로 모델링 가능하여, 장거리 공간적 의존성을 효과적으로 학습할 수 있습니다.
- **유연성:** 다양한 데이터 형식과 크기에 맞추어 손쉽게 조정 가능하며, 특히 비순차적이고 공간적인 분포가 중요한 비전 작업에 적합합니다.

**MambaVision에서 결합된 시너지**

- Mamba의 **효율적인 지역 및 연속 시퀀스 모델링** 능력과 Transformer의 **전역적이며 장거리 의존성 포착 능력**을 결합해, 이미지 내 로컬 특성과 글로벌 컨텍스트를 모두 효과적으로 학습합니다.
- 특히, MambaVision은 초반 고해상도 이미지 처리에는 CNN 기반 컨볼루셔널 레이어를 사용하고, 중간과 후반 단계에서 Mamba 기반 믹서와 Transformer의 self-attention 블록을 하이브리드로 조합하여, 효율성과 정확도의 트레이드오프를 훌륭히 달성합니다.
- Transformer 블록은 **최종 단계에 집중적으로 배치**되어 이미지의 전역 문맥을 보완, Mamba 블록으로 인해 제한된 전역 수용 영역 문제를 보완해줍니다.


> 결론적으로, Mamba는 **효율적이고 연속적 시퀀스 처리**에 뛰어나고, Transformer는 **강력한 전역 공간 의존성 학습능력**이 있어, 두 아키텍처가 만나면 각각의 단점은 완화되고 강점만 모아서, 이미지 인식 등 컴퓨터 비전 작업에서 더 탁월한 성능과 속도 조화를 이룰 수 있는 것입니다.

---

**1. MambaVision 아키텍처 개요**

- **4단계(Stages)로 구성된 계층형 구조**
  - **Stage 1, 2:** 고해상도 이미지에서 빠르게 특징을 추출하기 위해 Residual CNN 블록을 사용합니다.
  - **Stage 3, 4:** 저해상도 공간에서 MambaVision Mixer(개선된 Mamba 블록)와 Transformer의 self-attention 블록을 결합해 사용합니다.
- 이 구조는 높은 해상도에서는 효율적 지역 특성 추출에 집중하고, 점점 해상도가 낮아질수록 전역 컨텍스트 이해에 집중하도록 설계되어 있습니다.


**2. MambaVision Mixer (개선된 Mamba 블록)**

기존 Mamba (SSM 기반) 설계가 가지고 있던 **순차적이며 인과적인 causal convolution**의 한계를 보완해 다음과 같이 설계했습니다:

- **인과적(convolution) causal conv 대신 일반 conv 사용**:
  - 시계열 데이터처럼 좌->우 한 방향으로만 정보를 처리하지 않고, 시각 데이터를 위해 좌우 방향을 모두 고려하여 공간적 흐름에 대해 더 유연합니다.
- **두 개의 분기(branch) 도입:**
  - **SSM branch:** Mamba 고유의 상태공간 모델(Selective scan)을 통해 시퀀스적이고 구조적인 정보를 입력으로 처리합니다. 이는 시간적·순차적 특징 포착에 강점.
  - **비SSM branch (Symmetric branch):** 일반 1D convolution + SiLU (Sigmoid Linear Unit) 활성화 조합으로, SSM이 지나치게 한 방향성에 갇히는 문제를 보완하고, 컨텐츠 기반 전역 및 지역 특징을 보충적으로 학습합니다.
- **출력 합치기:**
  - 두 분기 출력은 절반 크기로 임베딩 후 연결(concatenation)하고, 마지막에 다시 선형 변환하여 원래 임베딩 크기로 되돌립니다.
- 이러한 구조는 **순차(SSM) + 공간(비SSM)** 정보가 결합되어 풍부한 시각 특성 표현을 가능하게 하고, Mamba의 효율성은 유지하면서 Vision에 더 적합하도록 개선된 것입니다.


**3. Transformer 블록과의 하이브리드 통합**

- MambaVision은 전체 Stage 3, 4 블록 중 뒷부분(N/2 층)을 Transformer의 multi-head self-attention 블록으로 구성합니다.
- 이유는 Transformer의 self-attention이 전역적 문맥 모델링에 특화되어 있어, MambaVision Mixer가 주로 지역단위 및 구조적인 특징을 뽑아낸 뒤 후반부에서 전역 정보를 보강하도록 역할을 분담하기 위함입니다.
- 성능 실험에서 Transformer 블록을 마지막 층(N/2, 혹은 N/4)에 두는 것이 임의적 혹은 초반부에 배치하는 것보다 훨씬 효과적임이 발견되었습니다.


**4. 모델 내부 처리 흐름 (수식 및 실험 코드 기준)**

- **입력:** $X \in \mathbb{R}^{T \times C}$ (시퀀스 길이 $T$, 임베딩 차원 $C$)
- **MambaVision Mixer 블록 동작:**


\begin{aligned}
X_1 &= \text{Scan} \big( \sigma(\text{Conv}(\text{Linear}(C \to C/2)(X))) \big) \\
X_2 &= \sigma \big(\text{Conv}(\text{Linear}(C \to C/2)(X)) \big) \\
X_{out} &= \text{Linear}(C, C)( \text{Concat}(X_1, X_2) )
\end{aligned}


- 여기서 Scan 함수는 Mamba 모델의 input-dependent selective state-space convolution이며, $\sigma$는 SiLU 활성화입니다.
- $X_1$에서 SSM branch가 입력 시퀀스를 Mamba 방식으로 처리하며, $X_2$는 일반 conv branch가 spatial 정보를 보완합니다.
- 이렇게 조합된 출력을 다음 MLP나 Self-Attention 블록에 넘겨 내부 표현을 진화시킵니다.


**5. 추가적으로 MambaVision의 설계 특징**

- **Downsampling CNN 블록:** 이미지 규모가 크기 때문에 전형적인 풀링 대신 3×3 stride=2 Conv를 사용해 공간 해상도를 조절하며, 이는 전형적 CNN의 특징 추출 방식을 계승합니다.
- **Layer Normalization 사용:** Mamba와 Transformer 블록 모두 안정적인 학습 및 표현을 위해 LayerNorm 기법을 사용합니다.
- **윈도우 기반 self-attention:** Transformer 단계에서 연산 비용을 줄이기 위해 지역 윈도우 내에서 self-attention을 수행하며, Stage 3에서는 window size 14, Stage 4에서는 7을 기본값으로 해 최적의 성능과 효율을 추구합니다.
- 이 윈도우 크기는 실험적으로도 최적값으로 확인되었으며, 윈도우를 크게 하면 전역성을 높일 수 있지만 속도가 약간 느려집니다.


**요약**

- MambaVision은 지역적인 효율적 시퀀스 모델링(Scan 된 SSM)과 공간적 비SSM 토큰 믹서를 두 개의 병렬 분기로 결합.
- 모든 입력을 연산하는데 있어 MambaVision Mixer가 먼저 지역/순차적 패턴을 효과적으로 파악.
- 후반부 Transformer self-attention 블록들이 멀리 떨어진 공간 정보와 전역 문맥을 포착.
- 전체적으로 효율적이면서도 장거리 및 다양한 공간적 의존성을 함께 포착하려는 복합적 설계를 통해 높은 정확도와 빠른 처리 속도를 달성.

---

# Detail Review

> MambaVision: Mamba-Transformer 하이브리드 비전 백본 분석

## 소개 (Introduction)

딥러닝 비전 분야에서는 이미지 인식 성능을 높이면서도 효율을 유지하기 위한 **백본 모델** 설계가 중요합니다. 과거에는 합성곱 신경망(CNN)이 주도했지만, 최근 **Transformer** 구조가 **Vision Transformer (ViT)** 등의 형태로 등장하여 뛰어난 성능을 보였습니다. Transformer는 자기어텐션(self-attention)을 통해 이미지 전역의 문맥 정보를 효과적으로 포착할 수 있다는 강점이 있지만, 입력 패치 수가 늘어날수록 연산량이 **\$O(N^2)\$** 수준으로 기하급수적으로 증가하는 단점이 있습니다. 한편, 2023년에 제안된 **Mamba**는 **상태 공간 모델**(State Space Model, SSM) 기반의 새로운 시퀀스 모델로, 입력 길이에 **선형 시간 복잡도**를 달성하면서도 자연어 등의 시퀀스 작업에서 Transformer에 필적하는 성능을 보였습니다. Mamba의 핵심 아이디어는 **입력 의존 동적 파라미터 선택 메커니즘**을 통해 불필요한 정보는 거르면서 긴 시퀀스를 효율적으로 처리하는 것이었죠.

이러한 장점에도 불구하고, Mamba와 같은 SSM 모델을 **컴퓨터 비전**에 직접 적용하는 데에는 한계가 존재했습니다. 이미지에서는 모든 픽셀이 순차적으로 의존하지 않고 **공간상의 국소적 관계**가 주로 중요하며, **글로벌 맥락**도 한 번에 고려되어야 정확한 판단을 내릴 수 있습니다. 그러나 기본 Mamba는 **자동회귀적 순차 처리** 특성 때문에 한 번에 한 방향으로만 정보를 흘려보내 전역 정보를 충분히 활용하지 못했고, 이를 극복하려 양방향 처리 등을 도입하면 지연 시간이 커지는 문제가 있었습니다. 결국 최신 Vision Transformer나 CNN 기반 모델들이 여전히 Mamba 기반 비전 모델보다 우수한 정확도를 보여주는 상황이었습니다.

**“MambaVision: A Hybrid Mamba-Transformer Vision Backbone”**은 이러한 배경에서 제안된 혁신적인 **하이브리드 백본**입니다. NVIDIA의 Ali Hatamizadeh와 Jan Kautz 연구진은 **Mamba** 구조를 비전에 맞게 개선하고, 그것을 **Transformer의 자기어텐션**과 결합함으로써 양쪽의 강점을 모두 살린 새로운 모델 **MambaVision**을 설계했습니다. 이 글에서는 해당 논문의 핵심 내용을 깊이 있게 분석하고자 합니다. **Mamba와 Transformer를 어떻게 결합하여 더 나은 비전 백본을 만들었는지**, 그리고 이러한 하이브리드 접근이 왜 중요한지를 알기 쉽게 풀어보겠습니다. 또한 모델 아키텍처의 세부 구성, 주요 실험 결과, 장단점과 시사점을 살펴봄으로써 MambaVision이 가져올 비전 모델 설계의 미래에 대해 논의하겠습니다.

## 배경 (Background)

### Transformer와 비전 모델

Transformer는 원래 자연어 처리에서 등장한 혁신적인 모델이지만, **ViT(Vision Transformer)**를 필두로 이미지 분야에도 빠르게 도입되었습니다. Transformer의 **자기어텐션** 메커니즘은 입력 토큰(이미지 패치)들 간 모든 쌍을 비교하여 **전역적인 상호 관계**를 학습할 수 있기 때문에, 복잡한 이미지에서도 장거리 의존 관계와 전체 맥락을 포착하는 데 유리합니다. 예를 들어 **Swin Transformer**는 윈도우 단위로 자기어텐션을 수행하고 계층적 구조를 도입하여 지역과 전역 패턴을 모두 잡아내려 했습니다. 이처럼 Transformer 기반 비전 모델들은 높은 표현력을 보여주었지만, **막대한 연산량**과 **대용량 데이터 학습 필요성**이라는 허들이 존재합니다. \$N \times N\$ 패치들 사이 모든 쌍을 처리하는 자기어텐션은 해상도가 커질수록 연산 비용이 급증하고, 대규모 데이터셋에서의 사전훈련 등이 요구되어 현실 적용에 부담이 될 수 있습니다. 이를 완화하기 위해 Swin Transformer는 국소 윈도우로 어텐션 범위를 제한하거나, **ConvNeXt**같은 현대적 CNN은 레이어 정규화 도입과 커널 크기 확대 등으로 Transformer의 일부 이점을 흡수하기도 했습니다. 그럼에도 **전역 receptive field**(수용 영역)를 효율적으로 확보하는 것은 여전히 순수 CNN에겐 어려운 과제이고, Transformer 기반 모델들에겐 계산 효율 측면의 도전 과제로 남아 있었습니다.

### Mamba와 상태 공간 모델(SSM)

한편, **상태 공간 모델 (State Space Model)**은 순차 데이터를 다루는 또 다른 패러다임으로, **연속 시간 역학 시스템**으로 입력 시퀀스를 처리하는 기법입니다. 2022년 등장한 S4 모델 등이 대표적이며, 매우 긴 시퀀스도 효율적으로 처리할 수 있다는 점 때문에 주목받았습니다. **Mamba**는 이러한 SSM 접근을 발전시킨 최신 모델로, 2023년 보고되었으며 Transformer 대비 **선형 시간**에 복잡도를 억제하면서 유사하거나 더 나은 성능을 달성해 화제가 되었습니다. Mamba의 작동 원리를 간략히 풀어보면, 입력 신호를 연속 미분 방정식 형태의 **은닉 상태** \$h(t)\$로 변환하여 처리한 뒤, 이를 다시 출력으로 변환하는 형태입니다. 이 과정에서 **A, B, C** 등의 행렬 파라미터가 시퀀스의 동특성을 결정하는데, Mamba는 **선택적 스캔(selective scan)** 메커니즘을 도입하여 입력에 따라 이 파라미터를 동적으로 조정함으로써 불필요한 부분은 거르고 **핵심 정보만 효율적으로 전달**하도록 했습니다. 또한 연속 모델을 이산화(discretization)하여 효율을 높이는 등 공학적 최적화도 포함하고 있습니다. 쉽게 비유하면, Transformer가 모든 토큰 쌍 사이를 비교하며 “**모든 정보를 한꺼번에 보는**” 방식이라면, Mamba는 **시간 흐름에 따라 정보를 전달**하며 “**중요한 정보만 골라 기억**하는” 방식이라 할 수 있습니다.

그러나 **이미지 처리를 위한 Mamba**에는 넘어야 할 산이 몇 가지 있었습니다. 우선, 이미지 픽셀들은 문장처럼 선형 순서가 있는 것이 아니기 때문에, Mamba가 텍스트에서 하던 것처럼 **한 방향으로 순차적**으로 픽셀 시퀀스를 처리하는 것은 비효율적입니다. 예를 들어 Mamba가 이미지를 왼쪽 위에서 오른쪽 아래로 한 줄씩 스캔한다고 상상해보면, 상하좌우로 인접한 픽셀들의 **지역적 패턴**을 놓칠 위험이 있고 전역적인 윤곽 파악도 어렵습니다. 실제로 **Vision Mamba (Vim)**라는 후속 연구에서는 이러한 문제를 보완하고자 **양방향 SSM**으로 이미지를 위-왼쪽에서 아래-오른쪽 등 **네 방향으로 반복 스캔**하는 기법을 도입하기도 했습니다. 하지만 이렇게 **모든 방향으로 순차 처리**를 하면 글로벌 문맥은 얻을 수 있어도, 모든 토큰을 다 처리할 때까지 출력이 지연되어 효율이 떨어지고 학습도 어려워지는 문제가 있습니다. 또 다른 시도로 **EfficientVMamba**는 **고해상도 구간에는 SSM, 저해상도 구간에는 CNN**을 사용하는 혼합 전략을 썼지만, 여전히 전역 컨텍스트를 온전히 반영하지 못해 정확도 면에서 한계가 있었습니다. 종합하면, Mamba류 SSM 기반 모델들은 **순차적 제약으로 인한 공간 이해 부족**과 **글로벌 컨텍스트 활용 미흡**이라는 약점을 보이고 있었던 것입니다.

이러한 배경에서 **하이브리드 접근**의 필요성이 대두되었습니다. CNN처럼 **초기 국소 특징 추출**은 빠르게 하고, Mamba의 **효율적 시퀀스 처리** 능력은 유지하되 부족한 **전역 문맥 이해**는 Transformer의 자기어텐션으로 보완하면 어떨까 하는 아이디어입니다. 즉, **“최고의 정확도를 가장 효율적으로”** 얻기 위해 한 가지 기법만 고집하기보다 서로 다른 장점을 가진 구조들을 조합하는 방향으로 연구가 발전한 것입니다. 이전에도 **CoAtNet**이나 **ConvNext** 등 CNN+Transformer 혼성 모델들이 있었지만, Mamba와 Transformer를 결합한 시도는 없었습니다. **MambaVision**은 바로 이러한 첫 도전으로서, Mamba 기반 토큰 믹서(token mixer)에 Transformer 블록을 결합한 새로운 비전 백본을 제안합니다.

## MambaVision이란? (What Is MambaVision?)

**MambaVision**은 Mamba와 Transformer를 **단일 아키텍처 안에서 융합**한 **계층형(hierarchical) 비전 백본 모델**입니다. 한 마디로 요약하면, **Mamba의 효율성과 Transformer의 전역 표현력을 모두 잡은 모델**이라 할 수 있습니다. 이 모델의 핵심 기여는 크게 두 부분으로 나뉩니다. 첫째, 기존 Mamba 블록을 이미지 처리에 적합하도록 **재설계(re-design)**하여 **“Vision-Friendly” Mamba** 토큰 믹서를 만들었다는 것입니다. 자동회귀 특성을 완화하고 추가 경로를 도입하는 등의 개선을 통해, 원본 Mamba 대비 **정확도와 처리 효율**을 모두 향상시킨 새로운 토큰 믹서가 탄생했습니다. 둘째, 이렇게 개선된 Mamba 토큰 믹서를 **Transformer의 자기어텐션 블록과 혼합**하여 **하이브리드 계층 구조**를 구현했다는 점입니다. 여러 가지 통합 방법을 실험한 끝에, **모델의 후반부 층들에 자기어텐션을 적용**하는 전략이 최적임을 발견했고, 이를 토대로 **MambaVision**이라는 **다중 해상도 단계형 모델**을 완성했습니다.

MambaVision은 전체적으로 **4개의 Stage(단계)**로 구성되며, 각 단계에서 입력 해상도를 점진적으로 줄여나가며 특징을 추출합니다. 앞단(Stage 1-2)에서는 **CNN 기반 레지듀얼 블록**들을 사용하여 고해상도의 입력을 신속히 처리하고 다운샘플링합니다. 후단(Stage 3-4)에서는 **MambaVision Mixer 블록**과 **Transformer 자기어텐션 블록**이 결합되어 동작합니다. 구체적으로, 각 Stage의 레이어들 중 **첫 절반은 MambaVision Mixer + MLP**로 구성되고, **나머지 절반은 Transformer Self-Attention + MLP**로 이루어져 있습니다. 이렇게 함으로써 **초반에는 Mamba 기반의 효율적 토큰 혼합**으로 특징을 추출하고, **후반에는 자기어텐션으로 전역 패턴을 파악**하도록 설계되었습니다. 논문에서는 “**최종 몇 개 층의 Transformer 블록이 잃어버린 전역 문맥을 회복하고 장거리 의존성을 캡처해준다**”고 설명합니다. 다시 말해, MambaVision은 **로컬-글로벌 처리의 균형**을 계층 구조 내에서 달성한 모델인 것입니다.

결과적으로 MambaVision 모델군(MambaVision-T, S, B, L 등 크기별 모델)은 ImageNet-1K 이미지 분류 기준으로 **이전까지 보고된 어떤 모델보다도 높은 정확도-속도 균형 성능**을 달성했습니다. 논문에서는 여러 경쟁 백본들과 Top-1 정확도 대 **추론 처리속도(throughput)**를 비교한 결과를 제시하며, **MambaVision이 새로운 Pareto 최적 선**을 그린다고 강조합니다. 특히 동일한 수준의 정확도를 내는 다른 모델들보다 월등히 빠르며, 비슷한 속도에서는 훨씬 높은 정확도를 보이는데, 이는 **하드웨어 효율성까지 고려한 Mamba의 장점과 Transformer의 표현력을 모두 활용**한 덕분입니다. 또한 **MS COCO 객체 검출**이나 **ADE20K 이미지 분할** 등의 다운스트림 과제에서도, MambaVision을 백본으로 사용한 모델이 기존 동급 백본을 쓴 모델보다 일관되게 우수한 성능을 보였다고 보고됩니다. 요약하면, MambaVision은 **비전 백본 설계의 새로운 가능성**을 보여준 첫 사례로서, Mamba와 Transformer의 만남이 실용적 가치가 있음을 입증한 것입니다.

## 아키텍처 상세 (Architecture Details)

<center>
<img src="../../images/2025-07-19-mambavision/1.png" width="100%" />
</center>

*MambaVision 백본의 계층적 아키텍처 개요. 입력 이미지는 합성곱 기반의 Stem과 Conv Block 단계를 거쳐 다운샘플링되고, Stage 3와 4에서 MambaVision Mixer 블록들과 Self-Attention 블록들이 조합되어 특징을 추출한다.*

MambaVision의 내부 구조를 단계별로 자세히 살펴보겠습니다. **그림 2**(위 다이어그램)은 MambaVision의 **4-Stage 계층 구조**를 보여줍니다. 우선 **Stem**이라 불리는 입력 처리 단계에서, 이미지가 작은 패치들로 분할되어 몇 차례의 \$3\times3\$ 합성곱과 스트라이드 2 다운샘플링을 거치며 **C 차원 임베딩**으로 변환됩니다. 이 Stem은 일종의 간단한 CNN 인코더 역할을 하여, 거대한 해상도의 이미지를 신경망이 다룰 수 있는 수준으로 줄여줍니다. 이어서 **Stage 1**과 **Stage 2**에서는 **잔차 연결(residual connection)을 갖는 CNN 블록들**이 사용됩니다. 각 Stage 사이에는 스트라이드 2의 다운샘플 합성곱이 있어 해상도를 단계적으로 낮추며, 채널 수는 늘려갑니다 (예: Stage 1 출력 채널 \$C\$에서 Stage 2 출력 \$2C\$로 증가). 이러한 디자인은 **ConvNeXt** 등 최신 CNN과 유사한 계층형 피라미드 구조로, **고해상도에서는 합성곱으로 국소 특징을 빠르게 추출**하여 효율을 높이기 위함입니다.

**Stage 3**와 **Stage 4**가 MambaVision의 핵심 혁신이 들어있는 부분입니다. 각 Stage에는 다수의 레이어가 있는데, **절반은 MambaVision Mixer 블록**, **절반은 Transformer Self-Attention 블록**으로 구성됩니다. 예를 들어 Stage 3에 8개의 레이어가 있다면 처음 4개는 MambaVision 토큰 믹서를, 나머지 4개는 멀티헤드 자기어텐션을 사용합니다. 각 레이어는 **Layer Normalization -> 토큰 믹싱(Mamba 또는 Self-Attention) -> 잔차합** 그리고 **Layer Normalization -> MLP (두 개의 선형층으로 이루어진 피드포워드) -> 잔차합**의 표준 Transformer 스타일 구조를 따릅니다. Stage 3 끝에서 다시 다운샘플링이 한 번 이루어지고, Stage 4에서도 동일한 패턴으로 진행됩니다. 최종적으로 Stage 4 출력 특성맵을 **2D 평균 풀링**으로 공간 차원을 줄이고, **최종 선형 분류기**를 통과시켜 예측을 얻습니다. 이처럼 MambaVision은 **CNN+SSM+Transformer가 유기적으로 통합된 형태**로, 각 구성 요소가 적재적소에 배치되어 있습니다. CNN은 **고해상도 특징 추출을 가속**하고, Mamba 기반 토큰 믹서는 **중간 단계에서 효율적으로 특징을 융합**하며, Transformer 블록은 **후반부에서 전역 상호작용을 모델링**합니다. 특히 **Transformer 블록을 최종 단계에 배치**한 것은, 앞 단계까지의 처리에서 놓쳤을 수 있는 **글로벌 문맥 정보를 복원**하고 **장거리 의존성을 포착**하기 위한 설계상의 선택입니다.

### MambaVision Mixer 블록 설계

MambaVision 아키텍처의 백미는 **MambaVision Mixer**라 명명된 토큰 믹싱 블록입니다. 이는 기존 **Mamba 블록을 Vision 용도로 개조**한 것으로, 간단히 말해 **SSM 기반 분지(branch)**와 **비-SSM 분지** 두 갈래로 입력을 처리한 뒤 통합하는 구조입니다. 원본 Mamba Mixer는 한 방향으로만 정보를 흘려보내는 **인과적(convolution causal) 1D 컨볼루션**과 SSM으로 구성되어 있었는데, 저자들은 이를 개선하기 위해 **두 가지 변경**을 가했습니다:

* **1)** **인과적 1D 컨볼루션을 일반 1D 컨볼루션으로 대체**하였습니다. 인과적 컨볼루션은 시퀀스 처리에서 현재 위치 이후의 정보를 차단하여 순방향 의존만 남기는 역할을 하는데, 이미지에서는 굳이 한쪽 방향으로 제한할 필요가 없기 때문입니다. 오히려 이러한 제약이 **한 방향으로만 국소 패턴을 전달하도록 만들어 비전에는 불필요하고 비효율적**입니다. 일반 컨볼루션으로 바꿈으로써 **양방향 문맥**을 모두 활용할 수 있게 됩니다.

* **2)** **SSM이 없는 병렬 분지(branch)를 추가**하였습니다. 구체적으로, 입력을 둘로 나누어 하나는 기존처럼 SSM 경로를 따르고 (Conv1D + SSM 연산), 다른 하나는 **추가된 1D 컨볼루션 연산 + SiLU 활성화**를 통과하도록 하였습니다. 이 두 번째 경로는 **순차적 상태 업데이트 없이** 즉각적인 공간 처리를 수행하므로, **SSM 경로가 잡아내지 못할 수 있는 정적인 정보나 글로벌 패턴**을 보완해주는 역할을 합니다. 쉽게 말해, SSM 분기가 **“시간적(순차적) 통합”**에 초점을 둔다면, 추가된 분기는 **“공간적 필터링”**을 수행하는 셈입니다.

이 두 경로의 출력은 **Concatenation(채널 축 연결)**으로 합쳐지고, 다시 **선형 결합**을 통해 원래 차원으로 투사(projection)됩니다. 이렇게 **출력 피처는 두 가지 분기의 정보를 모두 포함**하게 되며, 최종적으로 잔차 연결을 통해 입력과 더해집니다. 저자들은 이러한 대칭 분기 설계를 통해 **SSM의 순차 제약으로 인해 손실될 수 있는 콘텐츠를 보완**하고, **두 분기의 장점을 모두 살린 표현**을 얻을 수 있다고 설명합니다. 중요하게도, 새로 추가된 분기로 인해 파라미터 수가 크게 늘지 않도록 각 분기의 출력 채널을 절반으로 줄이는 등 크기 균형도 맞추었습니다.

이 MambaVision Mixer 블록의 구조를 그림으로 나타낸 것이 **논문 Figure 3**입니다. 해당 그림에는 SSM 경로(좌측, `SSM` 블록 및 관련 Conv1D)와 신규 경로(우측, `Conv1D` 및 SiLU)가 병렬로 그려져 있고, 최종 Linear로 합쳐지는 모습이 묘사되어 있습니다. 결과적으로 MambaVision Mixer는 **순차적 처리의 장점**과 **병렬 공간 처리의 장점**을 모두 활용하는 커스텀 토큰 믹서로 볼 수 있습니다. 이 Mixer 블록 하나만 놓고 봐도 **원본 Mamba 대비 큰 성능 향상**이 있었는데, 논문 부록에 공개된 실험을 보면:

* 아무 수정 없는 원래 Mamba 토큰 믹서는 ImageNet-1K Top-1 정확도 약 **80.9%**, ADE20K 분할 mIoU **44.2** 등에 그쳤지만,
* **인과 컨볼루션을 일반 컨볼루션으로 교체**하자 모든 지표가 소폭 상승했고,
* **대칭 Conv 분기(Conv2) 추가**까지 했더니 Top-1 **81.3%**, mIoU **45.7%**로 향상되었습니다,
* 최종적으로 **출력 결합 방식을 gating 대신 concat**으로 바꾸는 최종 설계에서 **Top-1 82.3%**, mIoU **46.0%**까지 성능이 도약했습니다.

이는 **MambaVision Mixer** 설계가 비전 작업에 매우 효과적임을 보여줍니다. 정리하면, **MambaVision의 성공**은 단순히 Mamba와 Transformer를 결합했다는 데 그치는 것이 아니라, **Mamba 자체를 비전에 최적화하여 업그레이드**한 덕분이라고 할 수 있습니다. 이 Mixer 블록을 통해 모델은 **순차적 장기 의존성과 공간적 글로벌 문맥**을 모두 포착하는 강력한 토큰 혼합을 수행하며, 이후 이어지는 **Transformer 자기어텐션 층**들과 시너지를 냅니다. Transformer 블록은 표준 멀티헤드 self-attention을 사용하되, 윈도우 크기 등을 적절히 조절하여 높은 해상도에서도 효율적으로 동작하도록 설계되었습니다. 최종 Stage의 self-attention은 resolution이 많이 내려간 (\$7\times7\$ 등) 상태이기에 부담이 크지 않고, 대신 이미지 전역의 정보를 통합해줍니다.

요컨대 MambaVision 아키텍처는 **“CNN + 개선된 Mamba Mixer + Transformer”**가 단계별로 배치된 형태로, 각 구성의 장점을 최대한 발휘하도록 정교하게 조합되어 있습니다. **초기 Stage**에서는 CNN이 로컬 패턴을 포착하고 다운샘플링하여 **효율성 극대화**, **중간 Stage**에서는 MambaVision Mixer가 **중/장기 의존성을 빠르게 통합**, **후반 Stage**에서는 Transformer 어텐션이 **전역 관계를 모델링**함으로써, 최종적으로 **고속추론에도 SOTA급 정확도**를 내는 백본을 완성한 것입니다.

## 실험 결과 (Experimental Results)

논문에서는 MambaVision의 우수성을 입증하기 위해 다양한 **비전 벤치마크**에서의 성능을 측정했습니다. **이미지 분류(ImageNet-1K)**부터 **객체 검출/분할(MS COCO)**, **장면 분할(ADE20K)**에 이르기까지 폭넓은 평가에서 **동일 세대 최고 모델들과 비교**가 이루어졌습니다. 주요 결과를 요약하면 다음과 같습니다.

### ImageNet-1K 분류 성능

ImageNet-1K 데이터셋에 대한 **Top-1 분류 정확도**와 **추론 속도** 비교에서, MambaVision 모델들은 현 시점 **최고의 균형 성능**을 보여주었습니다. 예컨대 **MambaVision-B** 모델(약 98M 파라미터)은 **Top-1 정확도 84.2%**를 기록하며, 비슷한 크기의 ConvNeXt-B(88M param, **83.8%**)나 SwinV2-S/B(50-88M param, **83.8\~84.6%**) 등을 앞질렀습니다. 놀라운 점은 **추론 속도**(이미지/초 기준)에서 MambaVision-B가 **3670 Img/sec**로, ConvNeXt-B의 1485 Img/sec 대비 **2배 이상 빠르고** Swin과 비교하면 수배에 달하는 속도를 냈다는 것입니다. 즉, **MambaVision-B는 더 정확하면서도 훨씬 빠른** 모델인 셈입니다. 작은 모델에서도 유사한 양상이 나타나, **MambaVision-T**(Tiny급 32M param)은 **82.3%** 정확도로 ConvNeXt-T(29M, 82.0%)와 거의 동등하지만 속도는 **6298 Img/sec**로 ConvNeXt-T(3196 Img/sec)의 두 배에 달합니다. 이는 EfficientFormer나 NextViT 같은 기존 효율 모델들을 크게 앞지르는 수치입니다. 요약하면, **MambaVision 시리즈는 모델 크기 전 범위에 걸쳐 최신 ConvNeXt, Swin 등의 정확도를 능가하거나 동등하면서도, 추론 처리량은 월등히 높아 새로운 Pareto 프론티어를 구축**했습니다.

이러한 경향을 한눈에 보여주는 것이 **논문 Figure 1**의 **정확도-처리량 분포 그래프**입니다. 해당 그래프에서 우측상단 방향으로 볼록하게 형성된 최선의 경계가 MambaVision 모델들로, 기존 EfficientNet, Swin, ViT 계열 및 다른 Mamba 기반 모델(Vim, VMamba 등)들이 그보다 아래쪽에 위치합니다. 특히 이전의 Mamba 기반 비전 모델들이 최고 83%대 정확도에 그쳤던 것에 비해, MambaVision은 이를 **최대 약 85%** 수준까지 끌어올렸습니다. 동시에 동일 정확도에서의 속도는 수배 향상되어, 예를 들어 **VMamba-B**(89M param, 83.9%)가 **645 Img/sec**에 불과했던 것과 대조적으로 MambaVision-B는 **3670 Img/sec**에 달합니다. 한 마디로 **더 적은 연산으로 더 높은 성능**을 얻는 데 성공한 것입니다.

또한 저자들은 Mamba 기반 모델로서는 최초로 **ImageNet-21K 거대 데이터셋**으로 MambaVision을 사전훈련해 보는 실험도 했습니다. 그 결과 사전훈련을 거친 MambaVision은 더욱 향상된 성능을 보였는데, 예를 들어 MambaVision-B의 Top-1 정확도가 **84.2% -> 84.9%**로 올라갔습니다. 대규모 데이터에서도 안정적으로 학습이 가능하며, **모델 스케일을 키워도 성능이 꾸준히 향상**함을 확인한 것입니다 (논문의 Figure 4 그래프 참고). 이는 MambaVision이 **확장성(scale-up)** 측면에서도 잠재력이 있음을 시사합니다.

### 객체 검출 및 분할 성능 (COCO, ADE20K)

다음으로 **다운스트림 비전 과제**에 대한 백본으로서의 성능입니다. 논문에서는 **MS COCO 데이터셋**의 객체 **검출 및 인스턴스 분할**에 **Cascade Mask R-CNN** 프레임워크를 사용하여 실험하였고, **ADE20K 데이터셋**의 **장면 분할**에는 **UPerNet** 모델로 실험하였습니다. 모든 경우에서 **동일한 조건(동일 헤드와 학습 스케줄)** 하에 **백본만 교체**하여 비교했는데, MambaVision 백본이 들어간 모델이 일관되게 더 나은 결과를 보였습니다.

* **COCO 검출/분할:** MambaVision-T/S/B 각각을 백본으로 썼을 때 **박스 mAP** 및 **마스크 mAP** 지표가, ConvNeXt-T/S/B 대비 **+0.1\~+0.7** 정도씩 높게 나타났습니다. 예를 들어, **MambaVision-B** 백본은 **박스 AP 52.8 / 마스크 AP 45.7**을 기록하여 ConvNeXt-B 백본(52.7 / 45.6)보다 약간 높고, Swin-B 백본(51.9 / 45.0)보다는 **박스 AP +0.9, 마스크 +0.7** 정도 개선되었습니다. 향상 폭이 아주 크지는 않지만 **일관되게 우세**하며, 파라미터나 FLOPs 등이 비슷한 조건에서 얻은 개선이라 의미가 있습니다. 무엇보다, **처리 속도를 높이면서도 정확도 손실이 없다는 것**이 중요한데, MambaVision 백본을 사용하면 동일 프레임워크에서 **추론 FPS도 올라갈 것으로 기대**되므로 실용적입니다.

* **ADE20K 분할:** Semantic segmentation 작업에서도 **mIoU** 지표 상의 개선이 확인되었습니다. MambaVision-T, S, B 백본은 각각 **46.0, 48.2, 49.1%** mIoU를 기록했는데, 이는 대응되는 Swin-T/S/B 백본 사용 대비 **+1.5, +0.6, +1.0%** 향상된 수치입니다. ConvNeXt 계열과 비교해서도 비슷하거나 더 나은 성능을 보였습니다. 특히 **MambaVision-B**의 **49.1%** mIoU는 동급 백본 중 최고 수준으로, 기존 Focal Transformer-B(49.0%)나 Twins-L(48.8%) 등을 근소하게 앞질렀습니다. 이러한 이득들은 **특별한 하이퍼파라미터 튜닝 없이** 기본 설정으로 얻은 것이어서 더욱 고무적입니다. 즉, MambaVision은 **분류뿐 아니라 검출/분할 같은 다운스트림 과제에도 범용적으로 강한 백본**임을 보여줍니다.

전반적으로 실험 결과들은 **MambaVision의 효율적 설계가 다양한 비전 작업의 성능을 향상**시킴을 입증합니다. 특히 **추론 속도 향상과 정확도 향상이 동시에 이루어졌다**는 점에서, 단순한 정확도 승부를 넘어 **실제 적용 효율**까지 고려한 큰 진전이라 평가할 만합니다.

## 분석 및 논의 (Analysis and Discussion)

MambaVision의 성공 요인을 조금 더 깊이 들여다보고, 그 의미를 분석해보겠습니다. 이 모델이 보여주는 바는 **“하이브리드 백본”** 접근이 얼마나 강력할 수 있는지입니다. MambaVision은 **Transformer류 모델의 세계**와 **SSM(Recurrent) 모델의 세계**를 절묘하게 결합함으로써, 양쪽의 장점을 취하고 단점을 보완했습니다. 그 결과 기존 **Conv-Transformer 하이브리드**(예: CoAtNet 등) 수준을 뛰어넘는 **새로운 SOTA 달성**이 가능했습니다.

먼저, **MambaVision Mixer 블록**의 도입은 **Mamba 구조의 한계였던 공간적 맥락 파악 문제**를 효과적으로 해결했습니다. SSM만으로 이미지를 처리할 때 문제가 되었던 **한 방향 제약**을 없애고, 별도 분기를 통해 **병렬적인 공간 특징 추출**을 함으로써 **글로벌 컨텍스트**를 확보한 것이 주효했습니다. 실제 ablation 연구에서도, **기존 Mamba 대비 제안한 변화(인과 컨브 제거, 대칭 분기 추가, concat 통합)**를 하나씩 적용할 때마다 **ImageNet, COCO, ADE20K 지표가 모두 상승**했음을 보여주었습니다. 최종적으로 **concat 방식 통합**이 가장 성능이 좋았는데, 이는 두 분기 출력을 **선형 결합하여 융합**하는 것이 gating 등보다 효과적임을 의미합니다. 이 덕분에 모델은 **순차적 특징과 공간적 특징의 “풍부한 표현”**을 얻게 되었고, 결과적으로 다양한 다운스트림 태스크에서 **경쟁력 있는 표현 학습**이 가능해졌습니다.

둘째, **Transformer 자기어텐션의 통합 위치**에 대한 저자들의 전략이 옳았다는 점입니다. 초기에는 “과연 Mamba와 Transformer 블록을 어떻게 섞는 것이 최선일까?” 하는 의문이 있었는데, 논문에서는 다양한 배치 패턴을 실험했습니다. 일부 레이어에만 랜덤하게 Transformer를 넣어보기도 하고, Stage 앞부분에 집중 배치하거나 교차 배치하는 등 시도를 했지만 대부분 최적이 아니었습니다. **Transformer 블록을 너무 이른 단계**(해상도가 높을 때)에 넣으면 연산량이 커져 비효율적일 뿐 아니라 성능 이득도 크지 않았습니다. **교차로 번갈아 배치**하는 패턴은 서로 다른 토큰 믹서간 상호 간섭이 생겨 성능이 떨어졌습니다. 반면, **마지막 몇 개 레이어에 연속 배치**하는 패턴이 가장 효과적이었는데, 특히 **각 Stage의 최후반 절반**을 Transformer로 할애했을 때 성능이 최고로 나타났습니다. 구체적으로, **Self-Attention을 Stage 후반 N/2 레이어에만 적용**한 최종 설계는 **Top-1 정확도 82.3%**로, 다른 패턴보다 0.5\~1% 가량 높았습니다. 이는 **“자기어텐션은 마지막에 몰아서 쓰는 것이 좋다”**는 논문의 가설과 일치하는 결과로, 결국 현재 MambaVision의 구조로 채택되었습니다. 이로써 모델은 **SSM의 효율**과 **어텐션의 전역성**을 **균형 있게** 획득한 것입니다.

한 가지 흥미로운 분석은 **MambaVision의 Self-Attention 레이어들이 실제로 무엇을 학습하였는가**입니다. 논문에서는 **최종 Transformer 블록들의 어텐션 맵**을 시각화하여, 이들이 **의미론적으로 중요한 영역에 집중**하고 있음을 보였습니다. 예를 들어, 비행기 이미지의 경우 어텐션 헤드가 비행기 전체 윤곽에 폭넓게 활성화되어 **대상 전체 형태를 포착**했고, 새(bird) 이미지에서는 한 헤드가 새의 머리와 꼬리 등 **독특한 부분을 집중적으로 바라보는** 등 **세밀한 부분까지 구분**했습니다. 사람 이미지에서는 손에 든 물체와 얼굴 등 **상호 작용하는 두 요소를 모두 강조**하여, **장면 내 요소들 간 관계**를 이해하고 있음을 보여줬습니다. 이러한 **시각적 해석**은, MambaVision의 자기어텐션이 단순히 형식적으로 추가된 것이 아니라 **실제로 글로벌 의존성과 의미 있는 특징들을 학습**하고 있음을 뒷받침합니다. 달리 말해, **하이브리드 구조의 시너지**가 내부 표현에서도 확인된 것입니다.

물론 MambaVision에도 몇 가지 고려할 점이 있습니다. 첫째, **Transformer 블록의 추가**로 순수 Mamba 모델에 비해서는 구조가 다소 복잡해졌고, **SSM과 어텐션 모두를 구현**해야 하므로 구현상의 부담이 있습니다. 하지만 논문에서 공개한 코드와 PyTorch 구현이 이를 잘 추상화하고 있어 실용적 사용에는 큰 지장이 없을 것으로 보입니다. 또한 MambaVision이 뛰어난 **처리량**을 보이긴 하지만, 여기에는 NVIDIA A100 GPU에서의 최적화가 작용한 면이 있습니다. 실제 임베디드 환경 등에서 SSM 연산이 최적화되지 않으면 이론적인 이점이 모두 나타나지 않을 수 있으므로, **플랫폼에 따른 성능 편차**는 추후 검증이 필요합니다. 그럼에도 불구하고, **동일한 하드웨어 조건에서 Transformer나 CNN 기반 모델보다 빠르다는 점**은 매우 고무적입니다.

또 하나, MambaVision의 **정확도 향상 폭**은 EfficientNet에서 ViT로 갈 때처럼 파격적이진 않지만, **효율 향상과 동반되었다는 점**을 기억해야 합니다. 예컨대 ConvNeXt-B 대비 +0.4%p 정확도 상승은 얼핏 작아 보일 수 있으나, 속도를 2배 이상 내면서 이뤄낸 성과입니다. 실제 대규모 서비스나 응용에서는 **처리 비용 절감이 곧 성능만큼 중요**하기 때문에, 이러한 **trade-off 개선**은 실질적인 가치가 큽니다. 더 나아가, MambaVision은 **크기 확장**을 통해 상한선에 도달하지 않고 더 개선될 여지도 있음을 보여주었습니다 (L, L2 모델에서 85%+ 정확도 달성). 따라서 **추가 데이터**나 **모델 확장**을 통해 향후 **86\~87%대**까지도 노려볼 수 있을 것입니다.

**하이브리드 접근의 시사점:** MambaVision의 등장은 향후 비전 모델 설계에 몇 가지 교훈을 줍니다. 하나는, **이질적인 모델 패러다임의 결합**이 생각以上의 시너지를 낼 수 있다는 점입니다. 그동안 Conv와 Transformer의 결합은 많이 시도됐지만, **시퀀스 모델(SSM)과 Transformer의 결합은 새 영역**입니다. 이 연구를 통해 **SSM 계열도 충분히 vision-friendly하게 개조 가능**하고, Transformer와 상호보완적으로 작동할 수 있음이 증명되었습니다. 이는 앞으로 **다른 SSM 변종**(예: S4, Linear RNN 등)과 Transformer를 결합하거나, 더 나아가 **CNN+SSM+Transformer 삼원 혼합 구조**를 탐색하는 등 새로운 **하이브리드 모델 연구**의 가능성을 엽니다. 또한 MambaVision은 **고해상도 입력 처리**에서 CNN 사용, **저해상도에서는 SSM/어텐션 사용**이라는 설계를 취했는데, 이는 **“어떤 해상도 레벨에서 어떤 토큰 믹서를 쓰는 게 최적인가”**에 대한 하나의 해답입니다. 향후 모델들은 해상도 단계별로 Conv, SSM, Attention을 **적절히 배치하는 네트워크 검색**을 통해 더욱 발전할 수 있을 것입니다.

## 결론 (Conclusion)

MambaVision은 **Mamba(SSM)와 Transformer를 결합한 최초의 하이브리드 비전 백본**으로서, **효율과 성능의 새 기준**을 세웠습니다. 이 모델은 **Mamba의 공식**을 비전 도메인에 맞게 재설계하고, 최적의 방식으로 Transformer 블록을 통합함으로써 **전역 문맥 표현 학습 능력**을 크게 향상시켰습니다. 그 결과 ImageNet-1K 분류에서 **Top-1 정확도와 처리량 측면의 새로운 SOTA Pareto 프론티어**를 달성했고, 다양한 비전 과제(분류, 검출, 분할)에서 동급 모델 대비 뛰어난 성능을 입증했습니다. 특히 **최종 단계에 자기어텐션을 배치하는 하이브리드 구조**를 통해 **장거리 의존성을 효과적으로 포착하면서도 효율을 유지**할 수 있음을 보였고, 대규모 데이터셋과 큰 모델에서도 경쟁력 있는 성능을 내며 **대규모 비전 응용 가능성**도 보여주었습니다.

MambaVision의 성공은 기존의 Mamba 기반 아키텍처의 한계를 극복하면서 그 장점을 살린 훌륭한 사례로, 미래의 비전 백본 설계에 새로운 방향을 제시합니다. 이제 연구자들은 **하이브리드 토대 위에서 더 다양한 조합과 변형**을 시도해볼 수 있게 되었습니다. 예를 들어, MambaVision을 토대로 **크로스모달(Vision+Language) 모델**이나 **비디오 처리**로 확장하는 연구, 또는 SSM 대신 다른 효율 시퀀스 모듈과의 결합 등이 기대됩니다. **결국 중요한 것은 장점은 극대화하고 단점은 보완하는 균형 잡힌 설계**이며, MambaVision이 그 성공 가능성을 보여준 만큼 앞으로 **이종 모델 융합을 통한 새로운 클래스의 비전 모델**들이 속속 등장할 것으로 보입니다. MambaVision 자체도 향후 실시간 서비스나 경량화 연구 등으로 발전하면서, 보다 넓은 범위의 비전 응용을 위한 든든한 기반이 될 것으로 전망됩니다.
