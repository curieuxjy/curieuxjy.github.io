---
title: "📃Twisting Lids Off리뷰"
date: 2025-09-26
categories: [bimanual, twist]
toc: true
number-sections: False
description: Manipulating objects with two multi-fingered hands

---

- [Paper Link](https://arxiv.org/abs/2403.02338)
- [Homepage](https://toruowo.github.io/bimanual-twist/)

1. 시뮬레이션에서 심층 강화 학습(RL)으로 훈련된 정책을 통해 로봇이 두 개의 다지형 손으로 다양한 물체의 뚜껑을 돌려 여는(lid-twisting) 작업을 실제 세계에서 제로-샷(zero-shot)으로 수행할 수 있음을 보여줍니다.
2. 핵심 기술로는 실제 역학을 모방하는 브레이크 기반(brake-based) 물리 모델링, 효율적인 실시간 인식을 위한 희소 객체 표현(sparse object representation), 그리고 자연스러운 동작을 유도하는 키포인트 기반(keypoint-based) 접촉 보상(contact reward)이 있습니다.
3. 제안된 시스템은 다양한 모양, 크기 및 재질의 가정용 물체에 대한 높은 일반화 능력과 외부 교란에 대한 강건함을 입증했으며, 심지어 뚜껑을 완전히 제거하는(lid-removal) 새로운 작업까지 성공적으로 수행했습니다.

---

# Brief Review

본 논문은 두 개의 다지(multi-fingered) 로봇 손을 사용하여 다양한 물체의 뚜껑을 돌리거나 제거하는 과제를 해결하는 심-투-리얼(sim-to-real) 강화 학습(RL) 시스템을 제안한다. 시뮬레이션에서 훈련된 단일 정책이 실제 환경의 다양한 물리적 특성(모양, 크기, 질량, 색상, 재료 등)을 가진 새로운 물체로 제로샷(zero-shot) 전이되는 것을 보여주며, 이는 다지 양손 시스템에서는 전례 없는 성과이다.

본 연구는 양손 조작의 높은 차원성과 접촉이 빈번한 작업의 본질적 복잡성으로 인해 발생하는 기존의 어려움을 해결하기 위해 세 가지 주요 통찰력을 제시한다.

1.  **물리 모델링:** 뚜껑과 몸체 사이의 정적 마찰을 정확하게 시뮬레이션하는 것이 어렵다는 점을 해결하기 위해, 논문은 '브레이크 링크(Brake Link)' 기반의 객체 모델을 도입한다. 이 모델은 리볼루트 조인트(revolute joint)와 나사산 구조로 연결된 두 개의 강체(몸체와 뚜껑) 사이에 프리즘형 조인트(prismatic joint)를 통해 지속적으로 압력을 가하는 특별한 '브레이크 링크'를 포함한다. 이는 뚜껑이 나사로 조여진 것과 유사하게 인공적으로 마찰력을 생성하여, 뚜껑과 몸체 사이의 상대적인 회전을 방지한다. 이 설계는 시뮬레이션 속도를 유지하면서 실제 물리 역학에 대한 높은 충실도를 제공하여 효율적인 정책 학습과 성공적인 심-투-리얼 전이를 가능하게 한다.

2.  **인지(Perception):** 미세한 접촉 조작에 정밀한 지각 정보가 필요할 것이라는 초기 가설과 달리, 본 연구는 '오브젝트 분할(object segmentation)' 및 '트래킹(tracking)' 도구(`Segment Anything Model (SAM)` 및 `XMem`)에서 추출한 '두 점의 희소(sparse) 객체 표현'만으로도 충분하다는 것을 발견했다. 실제 환경에서 RGBD 카메라를 사용하여 물체 마스크의 중심을 이미지 평면에서 얻고, 노이즈 있는 깊이 정보를 통해 3D 객체 키포인트(keypoint)를 추정한다. 이러한 최소한의 지각 정보와 '도메인 무작위화(domain randomization)' 기법은 폐색(occlusion) 및 카메라 노이즈에 강인한 정책 훈련을 가능하게 한다. 특히, 물체 위치 관측 노이즈, 관절 관측 노이즈, 액션 노이즈가 Sim-to-Real 전이에 가장 중요하다고 언급된다.

3.  **보상 설계:** 단일 부품의 강체 조작에 사용되던 기존 보상 설계는 다중 부품 조작에는 직접 적용하기 어렵다는 문제에 대응하여, 본 연구는 간단한 '키포인트 기반 접촉 보상(keypoint-based contact reward)'을 포함하는 다중 보상 항을 제안한다.
    *   **비틀기 보상 (Twisting Reward):** 뚜껑의 회전 각도($\Delta\theta$)에 비례하여 보상을 준다. 수식은 다음과 같다.
        $$r_{twisting} = \Delta\theta = q^{bottle}_{t+1} - q^{bottle}_t$$
    *   **손가락 접촉 보상 (Finger Contact Reward):** 손가락 끝이 병의 몸체($X_L$`)와 뚜껑($X_R$)에 부착된 참조 접촉 지점에 최대한 가깝게 유지되도록 유도한다. 이는 손가락 끝 위치($F^L_i, F^R_i$)와 참조 지점 간의 거리에 반비례하는 형태로 정의된다. 수식은 다음과 같다.
        $$r_{contact} = \sum_i \left( \frac{1}{1+\alpha d(X_L, F^L_i)} + \frac{1}{1+\alpha d(X_R, F^R_i)} \right)$$
        여기서 $\alpha$는 스케일링 하이퍼파라미터이고, $d(A, x) = \min_i \|A_i - x\|_2$는 점 $x$와 점 세트 $A$ 사이의 최소 거리를 나타낸다. 이 보상은 바람직한 행동과 작업 성공에 필수적인 것으로 확인되었다.
    *   **자세 보상 (Pose Reward):** 병의 주축($x_{axis}$)이 미리 정의된 방향($v$)과 정렬되도록 장려한다. 수식은 다음과 같다.
        $$r_{pose} = - \arccos(\langle x_{axis}, v \rangle)$$
    *   이 외에도 작업 패널티(work penalty) 및 액션 패널티(action penalty)와 같은 정규화 항이 포함된다.

정책 학습은 PPO(Proximal Policy Optimization) 알고리즘과 비대칭 비평가 관측(asymmetric critic observation)을 사용하여 수행된다. 관측 공간에는 로봇의 고유수용성 손 관절 위치, 병 몸체와 뚜껑의 추정된 3D 질량 중심 위치, 이전에 명령된 목표 관절 위치가 포함된다. 액션 공간은 PD 컨트롤러를 통해 생성된 상대적인 목표 관절 위치로 구성되며, 부드러운 움직임을 위해 EMA(Exponential Moving Average)가 적용된다. $\tilde{q}_{t+1} = \tilde{q}_t + \eta EMA(a_t)$

시뮬레이션 실험을 통해 키포인트 기반 접촉 보상과 시각 정보가 정책 학습 및 성능에 필수적임을 검증했다. 또한, 단일 물체 훈련보다 다중 물체 훈련이 약간 더 나은 성능을 보였는데, 이는 훈련 중 다양한 물체 인스턴스를 통해 탐색 과정을 용이하게 했기 때문으로 분석된다.

실제 환경 실험에서는 제안하는 정책이 모든 기준선(오픈 루프 리플레이, 비전 없음, 비대칭 훈련 없음, 대형 정책 네트워크)을 능가하며, 안정적인 파지와 효과적인 뚜껑 비틀기 성능을 보여주었다. 특히, 오픈 루프 정책의 낮은 성능은 이 과제가 물체 상태에 따라 매우 정밀한 액션을 요구함을 시사한다. 대형 정책이 실제 환경으로 전이되지 못한 것은 과적합(overfitting) 가능성을 보여주며, 접촉이 빈번한 작업의 심-투-리얼 전이를 위해서는 정책 네트워크 크기 제어가 중요함을 시사한다. 또한, 정책은 외부 힘에 대한 강인성을 보여주었으며, 훈련되지 않은 '뚜껑 제거(lid-removal)' 작업에 대해서도 새로운 가정용 물체에 대한 일반화 능력을 입증했다.

결론적으로, 본 연구는 양손 로봇의 복잡한 조작 능력 개발에 있어 중요한 진전을 이루었으며, 이는 실제 세계의 다양한 물체에 대한 조작 기술을 학습하는 데 새로운 가능성을 제시한다.

---

# Detail Review

## 서론

딥 러닝을 기반으로 두 손을 이용한 이중 손가락 로봇이 병뚜껑을 돌려 여는 것은 매우 난해한 문제로 간주되어 왔다. 복잡한 접촉 동역학과 높은 자유도의 조정이 필요하기 때문이다. Lin 등은 강화학습을 통해 시뮬레이션에서 학습한 정책을 실세계에 바로 적용하여, 다양한 병 모양에서 일반화할 수 있는 병뚜껑 비틀기 능력을 보여주었다. 이들은 기존에 없던 물리 모델링, 실시간 인지, 보상 설계 기법을 도입하여, 이중 다관절 로봇 손에 대한 강화학습 기반의 시뮬레이터-실세계 이전(sim-to-real)이 가능함을 증명했다. 본 리뷰에서는 이 논문이 제안하는 핵심 기법과 실험 결과를 심층 분석하고, 한계점 및 의의도 함께 논의한다.

## 시스템 구성 및 기술 개요

본 연구는 두 개의 16-자유도(DoF) Allegro 로봇 손을 사용한다. 각 손은 UR5e 로봇 암에 고정되어 있으며, Intel RealSense D435를 통해 물체 상태를 인식한다. 제어 주기는 약 30Hz 정도로 설정되었으며, 로봇에는 관절별 임피던스 PD 제어기가 적용된다. 강화학습 에이전트의 관측(observation)으로는 각 손의 관절 각도, 병 본체와 뚜껑의 3D 중심 위치, 이전에 명령된 관절 위치 등이 사용된다. 행동(action)은 각 관절의 목표 위치 변위로 표현되며, 이를 PD 제어기에 입력해 토크 명령을 생성한다. 에이전트 네트워크는 3-층 MLP(256-256-128)로 구성되며, 행동의 분포를 출력한다.

<center>
<img src="../../images/2025-09-26-twist-lids/1.png" width="70%" />
</center>


두 개의 Allegro 핸드로 구성된 실험 시스템. 상단: UR5e 암에 장착된 로봇 손들이 병뚜껑을 조작하는 모습. 하단: RGB 카메라 영상에서 병 몸체(붉은 색)와 뚜껑(초록색)의 마스크를 추정하고, 깊이 정보를 사용해 각 중심점을 3D로 계산한다. 본 연구에서는 RGBD 카메라에서 얻은 병의 분할(segmentation) 마스크 중심과 깊이 정보만으로 병 몸체와 뚜껑의 3D 위치를 계산하여 관측 정보로 사용한다.

시뮬레이션에서는 병과 뚜껑을 각각 강체 두 개로 모델링하고, 이 둘을 나사산이 달린 관절로 연결한다. 핵심 공학적 기여 중 하나는 브레이크 링크(brake link)를 도입한 물리 모델이다. 이는 병 몸체와 뚜껑 사이에 가압(prismatic joint)되는 추가 링크로, 뚜껑과 몸체 사이에 마찰력을 시뮬레이션한다. 이 브레이크 링크가 없으면 뚜껑이 손가락과 접촉하지 않아도 쉽게 회전하기에, 마치 실제로 나사산이 걸린 듯한 물리적 저항을 구현해 준다. 이를 통해 시뮬레이션 속도를 크게 희생하지 않으면서도 실제와 유사한 동역학을 얻을 수 있었다. 실제 실험을 위해 다양한 모양과 크기의 3D 프린팅 병 객체도 제작하였으며, 일부는 뚜껑이 무한히 회전하는 관절 구조를 가진다.

### 기여 요약

이 논문에서는 다음과 같은 핵심 기여를 제시한다:

- 물리 모델링: 두 파트(병 몸체와 뚜껑)로 구성된 관절 객체를 시뮬레이션하기 위해 브레이크 링크를 도입하여, 뚜껑과 몸체 사이의 마찰력을 효율적으로 모델링.
- 인지(Perception) 기법: 병체와 뚜껑을 구분한 분할(segmentation) 마스크의 중심점 두 개만을 이용하는 극히 희소한 객체 표현을 사용하며, 오차 내성 있는 학습을 위해 도메인 랜덤화 기법을 적용.
- 보상 설계: 두 손가락 각각이 병 몸체와 뚜껑의 특정 키포인트에 가까워지도록 유도하는 ‘키포인트 기반 접촉 보상’을 도입하여 자연스러운 그립 및 비틀림 동작을 유도. 회전 보상과 병 축 정렬 보상을 추가로 결합하여 세분화된 행동 지시를 구현했다.
- RL 기반 시뮬레이터-실세계 전이: 딥 RL(PPO)을 이용해 시뮬레이션으로부터 학습한 정책을 실세계 이중 손 조작에 직접 전이하는 시스템을 구축한 첫 사례로, 다양한 알 수 없는 병 객체에 일반화 가능한 동적이고 섬세한 행동을 보여주었다.

## 방법론 분석

### 강화학습 설정

문제는 부분 관측 마르코프 결정 과정(POMDP)으로 설정되었다. 상태(state)는 로봇 관절 각도, 병체와 뚜껑의 3D 위치 등의 관측(observation)을 포함한다. 행동(action)은 로봇 관절 각도 목표 값의 상대적 변위로 표현된다. 행동 출력을 부드럽게 하기 위해 지수 가중 이동 평균(EMA)으로 보정하며, PD 제어기를 통해 토크로 변환한다. PPO(Adam 최적화 포함)를 사용하여 정책을 학습하며, 정책 네트워크는 3층 MLP(256-256-128), 상태-독립 가우시안 분포, 값 함수 네트워크는 3층 MLP(512-512-512) 구조를 사용했다. 또한 가치함수 학습 시에만 물리 매개변수 등의 특권정보(privileged observations)를 추가 입력으로 사용하는 비대칭 학습(asymmetric PPO)을 통해 시뮬레이터-실세계 이전 성능을 높였다.

### 보상 함수 설계

과제 수행을 구체화하기 위해 세 가지 주요 보상 항(term)을 설계했다.

첫째, **회전 보상(Twisting Reward)**은 뚜껑이 한 타임스텝 동안 회전한 각도 Δθ 를 보상으로 한다 :

$$
r_{\text {twist }}=\Delta \theta
$$

둘째, **키포인트 기반 손가락 접촉 보상(Finger Contact Reward)**이다. 병 몸체와 뚜껑에 각각 사전에 정의된 키포인트 집합을 놓고, 왼손과 오른손의 특정 손가락 끝(예: 엄지 등) 위치가 해당 키포인트에 가까워지도록 거리에 반비례하는 보상을 준다. 구체적으로, 두 손가락 끝 위치 $\mathbf{p}_L, \mathbf{p}_R$ 에 대해 병체와 뚜껑 위의 네 개의 키포인트 $\left\{\mathbf{k}_i\right\}$ 와 $\left\{\mathbf{k}_j\right\}$ 중 가장 가까운 것과의 거리를 측정하여 보상을 계산한다. 이 보상이 클수록 손가락이 병뚜껑을 자연스럽게 움켜쥘 수 있다.

셋째, **자세 보상(Pose Reward)**은 병의 주요 축이 특정 방향(예: 수직)에 정렬되도록 유도한다. 추가로, 불필요한 큰 행동을 억제하기 위해 작업(work)과 행동(action) 크기 패널티를 도입했다. 이들을 가중합하여 전체 보상 함수가 구성된다. 실험에서 키포인트 접촉 보상이 제거되거나 약화되면 학습 효율과 최종 성능이 크게 떨어지는 것으로 나타났는데 , 이는 이 보상이 복잡한 두 손 조작 동작에서 원하는 자세 탐색을 강력히 유도함을 의미한다.


<center>
<img src="../../images/2025-09-26-twist-lids/f54d7a41-b980-4ce0-aaa9-7b2ef750ec2a.png" width="70%" />
</center>

> 서로 다른 보상 설계에 따른 시뮬레이션 행동 예시. 첫째 줄(“Ours 100% Contact Reward”)은 키포인트 기반 접촉 보상이 충분히 주어졌을 때의 안정적이고 자연스러운 그립과 비틀기 모습이다. 둘째 줄(“Gait Constraint Reward”)은 기존 손가락 보행 패턴 제약만을 쓴 경우로 손동작이 불안정하며 비틀림이 제대로 이루어지지 않는다. 셋째 줄(“50% Contact Reward”)은 접촉 보상이 절반으로 줄어든 경우로, 잡기는 다소 헐겁고 동작이 덜 매끄럽다. 이 예시는 키포인트 보상이 두 손 조작 동작 탐색에서 필수적인 역할을 함을 보여준다.

### 인지 및 도메인 랜덤화

실제 환경에서는 손가락이 병을 가린 복잡한 장면에서도 물체 위치를 추정해야 한다. 이를 위해 첫 프레임에서 Segment Anything 모델로 병 몸체와 뚜껑의 RGB 마스크를 얻고, XMem 트래커로 이후 프레임을 추적한다. 마스크 중심점의 2D 좌표와 깊이(Depth) 측정값을 결합해 3D 위치를 계산한다. 놀랍게도 이 극히 희소한 표현(두 점 위치)만으로도 복잡한 작업이 가능함을 실험적으로 확인했다.
시뮬레이션과 실제 환경 차이를 줄이기 위해 광범위한 도메인 랜덤화를 적용했다. 병 질량, 마찰 계수, 크기, 모양, 초기 위치/자세, 로봇 관절 마찰, PD 이득, 관측 노이즈, 프레임/액션 지연 등 여러 물리적·비물리적 파라미터를 랜덤하게 변화시켰다. 또한, 랜덤 외부 힘을 주기적으로 객체에 가해 시뮬레이터에 없는 동작을 모델링했다. 이러한 랜덤화는 제로샷 전이 성공에 중요한 역할을 했다.

## 시뮬레이션 실험

시뮬레이션에서는 다양한 병 모양(주로 실린더)으로 구성된 데이터셋을 사용했다. 두 가지 실험 구성을 두었다: **①단일 객체 학습(single-object, 중간 크기)과 ② 다중 객체 학습(multi-object, 다양한 크기)**.

성능 평가지표로는 **각도 변위(Angular Displacement, AD)와 실패 시간(Time-to-Fail, TTF)**을 사용했다. AD는 한 실험 동안 뚜껑이 회전한 총도를 나타내고, TTF는 병이 떨어지기 전까지 지속된 시간을 측정한다.

- 첫째, **보상 설계의 영향**을 분석했다. 키포인트 접촉 보상을 줄인 정책(“Reduced Contact Reward”)은 비틀기 능력을 전혀 학습하지 못하고 성능이 크게 저하되었다. 보상 강도와 학습 효율 및 최종 AD 점수는 양의 상관관계가 있었으며, 접촉 보상이 충분하지 않으면 RL의 탐색 공간이 지나치게 커져 학습이 실패한다.
- 둘째, **시각 정보의 유무**를 비교했다. 시각 입력 없이(단지 관절 정보만으로) 학습한 정책(No-Vis baseline)은, 단일/다중 객체 모두에서 우리 방법 대비 현저히 낮은 AD를 보였다. 이는 병의 위치 정보를 간접적으로 추정하기 위해 관절만 보는 것으론 이 복잡한 과제를 해결하기 어렵다는 것을 나타낸다. 반면 본 정책은 3D 위치를 관측에 포함시켜 안정적 비틀기 동작을 달성했다.
- 셋째, **단일 vs 다중 객체** 학습 결과를 살폈다. 일반적으로 단일 객체 학습은 특정 물체에 특화되나 일반화가 어렵고, 다중 객체 학습은 일반화에 유리하다고 알려져 있다. 놀랍게도 두 설정에서 다중 객체 학습이 단일 객체 학습보다 AD가 약간 더 높게 나타났다. 이는 다중 객체 학습이 난이도에 따른 자연스러운 커리큘럼 효과를 제공하여 탐색을 용이하게 하기 때문일 것으로 저자들은 추정했다.

## 실제 실험 및 평가

### 실험 환경

실험은 두 대의 Allegro 손과 UR5e 팔, 싱글 RealSense D435 카메라로 구성된다. 학습된 정책은 시뮬레이션에서 제로샷으로 바로 전이된다(추가적 학습 없음). 평가 대상 객체는 총 다섯 종의 실험용 병으로, 이 중 네 개는 훈련 분포와
유사한 원형 몸체, 하나는 네모 몸체를 가진 분포 외 객체이다. 각 정책마다 30초씩 20회 반복 실험하여 AD와 TTF를 측정했다. 최종적으로 10개의 무작위 시드로 학습한 정책 중 성능이 가장 우수한 상위 3개를 사용해 평균을 계산했다.

비교군은 다음과 같다:

1) 오픈루프 재생 정책(Replay): 시뮬레이션에서 성공 궤적을 녹화해 그대로 재생.
2) 시각 정보 제거(No-Vis): 기존 정책에서 병 정보 입력만 제거.
3) 비대칭학습 제거(No-Asym): 가치함수에 특권정보를 제공 하지 않고 학습.
4) 대형 신경망(Large): 네트워크 크기 증가(과적합 검증용).

## 실험 결과

본 정책은 모든 측정 지표에서 다른 비교군을 압도적으로 능가했다. 예를 들어 파란 병(BlueBottle)에서 본 정책은 평균 약 946°(≈2.6바퀴)의 회전을 수행한 반면, 나머지 정책은 거의 30° 이하의 회전을 기록했다. 또한 TTF(파란병 기준)에서 본 정책은 약 23.7초 동안 안정적으로 유지되었으나, 다른 정책은 대부분 7초 이하에서 병이 떨어졌다. 오픈루프 재생 정책은 거의 병을 떨어트리는 결과를 보였는데, 이는 미세한 접촉 시점에서 정책의 정밀성이 없으면 병이 손가락을 굴러떨어지는 것으로 보인다. 특히, 대형 네트워크(Large)는 시뮬레이션에서는 유사한 성능을 보였지만 실세계에선 실패해, 과적합의 위험성을 시사했다.

### 일반화 및 강인성

추가로 저자들은 실제 생활용 병(주류병, 땅콩버터 등) 10개를 테스트해 정책의 일반화력을 평가했다. 이들 물체는 모양·크기·재질·질량이 모두 달랐고, 실제로 대부분은 뚜껑이 나사처럼 잠기지 않는 구조였다. 성공 기준을 ‘뚜껑이 완전히 분리되는 것’으로 삼았을 때, 본 정책은 약 30% 성공률을 보였다. 이는 본 정책이 훈련 환경과 꽤 다른 상황에서도 제한적으로 비틀기 기술을 전이할 수 있음을 보여준다. 또한 정책의 외란 강인성을 검증하기 위해, 실험 중 임의 시간에 집게로 물체를 밀거나 건드려 힘을 가했다. 그럼에도 정책은 병을 재중심으로 이동시키고 비틀기를 계속하여 외란에 대한 적응력을 확인했다. 이 실험에서는 마커 기반 객체 추적(착용 마커)이 사용되었는데, 이는 시각적 마스크 추종과의 상호작용을 분리하여 분석하기 위함이다. 아래 그림은 외부 힘을 가하는 예시를 보여준다.

<center>
<img src="../../images/2025-09-26-twist-lids/5b9e6b19-3a7c-4ffe-8a65-d5a1ce5c3cbc.png" width="70%" />
</center>

학습된 정책이 외부 힘에 의해 병이 흔들릴 때의 모습. 상단 왼쪽부터 시계방향으로, 집게를 이용해 다양한 방향에서 힘을 가했다. 정책은 힘을 받은 뒤에도 손가락의 그립과 자세를 조절하여 병을 다시 중심으로 잡아올렸으며, 비틀기를 지속했다. 이는 학습된 정책이 예기치 않은 외란에도 일부분 적응할 수 있음을 보여준다.

### 한계 및 비판적 고찰

이 시스템은 복잡한 상자나 낮은 정확도의 객체 인식이 필요한 상황에는 아직 취약할 수 있다. 먼저, 보상 함수를 세밀하게 설계해야 했는데, 실제 환경에서의 추가적인 무작위성이나 예외 상황이 발생하면 정책의 동작이 예측 불가능해질 수 있다. 또한, 객체 분할과 추적을 위해 학습된 모델(SAM, XMem)을 사용하였으나, 이들은 뚜껑 분리 시 추적 목표가 변경되는 등 제한된 경우의 수에 의존한다. 학습에 소요된 계산량과 시뮬레이터-실시간 간 도메인 갭도 여전히 고려 사항이다. 그럼에도 이 연구는 이중 다관절 손을 사용한 복잡한 조작 동작을 RL로 해결할 수 있음을 보여준 첫 사례라는 점에서 의의가 크다. 기존의 로봇 팔 2개를 이용한 작업에서 간단한 그리퍼를 쓴 것과 달리, 이들은 고차원 손가락 제어 공간에서 인간과 유사한 미묘한 동작 패턴을 학습했다는 점에서 큰 진전이다. 특히, 특정 객체 모델을 알 필요 없이 다양한 병에 일반화된 정책을 얻었다는 점은 응용 가능성을 높여준다.

## 결론

Lin 등은 두 손을 사용하는 병뚜껑 비틀기 작업에 대하여, 물리 모델링(브레이크 링크), 희소한 인지 입력(분할 마스크 중심) 및 정교한 보상 설계를 결합한 딥 RL 프레임워크를 제안했다. 이로써 복잡한 이중 손 조작 과제를 시뮬레이터에서 학습해 실세계로 이전할 수 있음을 실증하였으며, 다양한 테스트에서 우수한 성능과 강인성을 보였다. 앞으로 이러한 접근은 더 많은 복잡 조작 과제로 확장될 수 있을 것이며, 인간 수작업 시연 없이도 다관절 로봇이 창의적 조작을 터득하는 길을 열어줄 것으로 기대된다.
