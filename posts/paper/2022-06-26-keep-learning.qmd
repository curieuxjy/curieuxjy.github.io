---
title: "ğŸ“ƒLegged Robots that Keep on Learning"
description: Fine-Tuning Locomotion Policies in the Real World
date: "2022-06-26"
categories: [quadruped, rl, redq, paper]
toc: true
execute:
  freeze: auto 
---

# 0. Abstract

> Legged robots are physically capable of traversing a wide range of challenging environments but designing controllers that are sufficiently robust to handle this diversity has been a long-standing challenge in robotics. Reinforcement learning presents an appealing approach for automating the controller design process and has been able to produce remarkably robust controllers when trained in a suitable range of environments. However, it is `difficult to predict all likely conditions the robot will encounter` during deployment and enumerate them at training-time. What if instead of training controllers that are robust enough to handle any eventuality, `we enable the robot to continually learn in any setting it finds itself in?` This kind of real-world reinforcement learning poses a number of challenges, including efficiency, safety, and autonomy. To address these challenges, we propose a practical robot reinforcement learning system for `fine-tuning locomotion policies in the real world.` We demonstrate that a modest amount of real-world training can substantially improve performance during deployment, and this enables `a real A1 quadrupedal robot` to autonomously fine-tune multiple locomotion skills in a range of environments, including `an outdoor lawn and a variety of indoor terrains.`

# I. Introduction

*ê°•í™”í•™ìŠµì´ ë¡œë´‡ ì œì–´ ë¶„ì•¼ì—ì„œ ê°ê´‘ ë°›ëŠ” ì´ìœ ê°€ ë¬´ì—‡ì¼ê¹Œ?* ê¸°ì¡´ì˜ ë¡œë´‡ ì œì–´ ì•Œê³ ë¦¬ì¦˜ë“¤ì€ ì •ë§ ë§ì€ engineering ì ì¸ ê³ ë ¤ì™€ ë³µì¡í•œ ìˆ˜í•™ì  ëª¨ë¸ë§ì´ í•„ìš”í•˜ë‹¤. ê·¸ëŸ°ë° ê·¸ë§ˆì € ì—”ì§€ë‹ˆì–´ê°€ ë¯¸ì²˜ ê³ ë ¤í•˜ì§€ ëª»í•œ ì‘ë™ì„ í•´ì•¼ í•  ë•ŒëŠ” ë°”ë¡œ ì‹¤íŒ¨í•œ controller ë””ìì¸ì´ ë˜ì–´ ë²„ë¦¬ê¸° ë•Œë¬¸ì— ë¡œë´‡ ì œì–´ëŠ” ì‰½ì§€ ì•Šì€ ë¬¸ì œì˜€ë‹¤. ì´ëŸ° ë©´ì—ì„œ ê°•í™”í•™ìŠµì€ controllerë¥¼ `trial-and-error`ë¡œ ë¡œë´‡ agentê°€ ì•Œì•„ì„œ ì–´ë–»ê²Œ ì‘ë™í•´ì•¼ í• ì§€ í•™ìŠµí•˜ê¸° ë•Œë¬¸ì— ê³µí•™ìì—ê²Œ controller ë””ìì¸ì— ëŒ€í•œ ë¶€ë‹´ì„ ì¤„ì—¬ì£¼ì—ˆê³  ì´ëŸ° ì ì— ê°•í™”í•™ìŠµì´ ë¡œë´‡ ì œì–´ ë¶„ì•¼ì—ì„œ ì£¼ëª© ë°›ëŠ” ì´ìœ ì˜€ë‹¤.

í•˜ì§€ë§Œ, ì•ˆíƒ€ê¹ê²Œë„ ê°•í™”í•™ìŠµì´ controllerë¥¼ ë§Œë“œëŠ” ê²ƒì˜ ë¶€ë‹´ì„ ì¤„ì—¬ì£¼ì—ˆì§€ë§Œ ê°•í™”í•™ìŠµì˜ environment ì„¤ê³„ì— ëŒ€í•œ ë¶€ë‹´ì´ì—ˆë‹¤. ìœ„ì—ì„œ ì„¤ëª…í•œ ëŒ€ë¡œ ê°•í™”í•™ìŠµì—ì„œ trial-and-errorë¡œ `ì•Œì•„ì„œ` í•™ìŠµí•œë‹¤ëŠ” ì ì´ ë§¤ë ¥ì ì´ì§€ë§Œ, ì´ëŸ° í•™ìŠµì˜ ì¡°ê±´ì—ëŠ” ì¢‹ì€ environmentê°€ í•„ìš”í•˜ë‹¤. ê°•í™”í•™ìŠµ ë¶„ì•¼ì—ì„œ ìì£¼ ì–¸ê¸‰ë˜ëŠ” `ì¢‹ì€ agentì˜ ë°°ê²½ì—ëŠ” ì¢‹ì€ environmentê°€ ìˆë‹¤.`ëŠ” ë§ì²˜ëŸ¼ agentê°€ environmentì—ì„œ ê²½í—˜í•˜ë©´ì„œ ì¢‹ì€ í•™ìŠµì„ í•˜ì§€ ëª»í•˜ë©´ ì¢‹ì€ ê²°ê³¼ë¥¼ ê¸°ëŒ€í•  ìˆ˜ ì—†ë‹¤. ë”°ë¼ì„œ ë§ˆì¹˜ controller ë””ìì¸ê³¼ environment ë””ìì¸ì€ trade-off ê´€ê³„ë¡œ ì—”ì§€ë‹ˆì–´ì—ê²Œ ê³¼ì œë¥¼ ë‚¨ê¸°ê²Œ ëœë‹¤.

agentê°€ í•™ìŠµí•˜ëŠ” ë™ì•ˆì— ê²½í—˜í•˜ê²Œ ë˜ëŠ” environmentì™€ í…ŒìŠ¤íŠ¸ ì‹œ(ì‹¤ì‚¬ìš© ì‹œ) ê²½í—˜í•˜ê²Œ ë˜ëŠ” environmentì˜ ì°¨ì´ê°€ í¬ë©´ í´ìˆ˜ë¡ agentëŠ” ì œëŒ€ë¡œ ì‘ë™í•  ìˆ˜ ì—†ë‹¤. í•™ìŠµë˜ì§€ ì•Šì€ ê²½í—˜ë“¤ì´ê¸° ë•Œë¬¸ì— í•™ìŠµëœ agentì˜ policyê°€ ì¢‹ì€ actionì„ í•  ìˆ˜ ì—†ë‹¤. ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ ê²½í—˜í•´ë³´ì§€ ëª»í•œ, ì¦‰ í•™ìŠµí•˜ì§€ ëª»í•œ ê²½ìš°ì— ëŒ€í•´ì„œë„ ì œëŒ€ë¡œ agentê°€ ë™ì‘í•˜ê¸° ìœ„í•´ `zero-shot generalization`(í•œë²ˆë„ ë³´ì§€ ëª»í•œ-zero shot ê²½í—˜ ë°ì´í„°ì— ëŒ€í•´ ì˜ ì¼ë°˜í™”-generalization í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥) ì´ í•„ìš”í•˜ì§€ë§Œ, ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì™„ë²½í•œ `zero-shot generalization`ì€ ì¼ì–´ë‚  ìˆ˜ ì—†ë‹¤ëŠ” ê°€ì •í•˜ì— ë¬¸ì œë¥¼ ì–´ë–»ê²Œ í’€ê²ƒì¸ê°€ ê³ ë¯¼í–ˆë‹¤.

ê·¸ë ‡ê²Œ í•´ì„œ ì œì•ˆëœ ë°©ë²•ì€ `í…ŒìŠ¤íŠ¸ í™˜ê²½ì—ì„œ ë¹ ë¥´ê²Œ fine-tuning í•´ì„œ agentê°€ ì˜ ë™ì‘í•˜ê²Œ ë§Œë“¤ì`ì˜€ê³ , ì´ ë°©ë²•ì´ ê°€ëŠ¥í•˜ë‹¤ë©´ ë¡œë´‡ì€ ì‹¤ì œë¡œ ë™ì‘í•˜ë©´ì„œ ì–¸ì œë“ ì§€ ë§ˆì£¼ì¹  ìˆ˜ ìˆëŠ” ìƒˆë¡œìš´ í™˜ê²½ì— ì ì‘í•´ì„œ(fine-tuned) ì˜ ë™ì‘í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤.

> ğŸ¯ ë³¸ ë…¼ë¬¸ì˜ ëª©í‘œëŠ” ì‹¤ì œ í™˜ê²½(real-world)ì—ì„œ ë¡œë´‡ì˜ locomotion policyë“¤ì´ fine-tuningí•  ìˆ˜ ìˆëŠ” ì™„ì „í•œ ì‹œìŠ¤í…œì„ ë””ìì¸ í•˜ëŠ” ê²ƒì´ë‹¤.


## System Process

![](https://i.imgur.com/vJJH1oF.png?1){fig-align="default"}

1. ìœ„ì˜ ì‚¬ì§„ì— ë³´ì´ëŠ” ê³µì›ê³¼ ê°™ì€ `ìƒˆë¡œìš´ í™˜ê²½`ì—ì„œ ë¨¼ì € ë¡œë´‡ agentê°€ ì²«ë²ˆì§¸ ì‹œë„ë¡œ locomotion taskë¥¼ ì§„í–‰í•œë‹¤.
2. ë§Œì•½ì— ë•…ì´ ê³ ë¥´ì§€ ëª»í•´ì„œ agentì˜ í•™ìŠµëœ policyë¥¼ í™œìš©í•  ìˆ˜ ì—†ëŠ” ìƒí™©ì´ ë˜ì–´ì„œ ë„˜ì–´ì§€ê²Œ ë˜ëŠ” ìƒí™©ì´ ë  ìˆ˜ ë„ ìˆë‹¤.
3. ì´ë•Œ `reset controller`ë¥¼ ì´ìš©í•´ì„œ ë¹ ë¥´ê²Œ ë‹¤ì‹œ ì¼ì–´ë‚œë‹¤.
4. ì‹¤ì œ taskì—ì„œ ì¢€ ë” ëª‡ ë²ˆ ì‹œë„ë¥¼ í•˜ë©´ì„œ 1~3ì˜ ê³¼ì •ì„ ëª‡ ë²ˆ ë°˜ë³µí•˜ê²Œ ë˜ê³  ì´ ê³¼ì •ì—ì„œ `policyê°€ ì—…ë°ì´íŠ¸` ë˜ê²Œ ëœë‹¤.
5. ì—…ë°ì´íŠ¸ê°€ ë˜ë©´ì„œ policyëŠ” `ìƒˆë¡œìš´ test í™˜ê²½ì—ì„œ ì œëŒ€ë¡œ ì‘ë™`í•  ìˆ˜ ìˆê²Œ ëœë‹¤.

## How

- ê°•í™”í•™ìŠµì˜ reward ê°€ robotì˜ `on-board ì„¼ì„œë¡œ ì¸¡ì •ë˜ëŠ” ê°’ë“¤ë¡œë§Œ` ë””ìì¸ ë˜ì–´ì•¼ ì‹¤ì œ Real-worldì—ì„œ ì‘ë™í•˜ë©´ì„œ fine tuningì„ í•  ìˆ˜ ìˆë‹¤.
- Agileí•œ behaviorë¥¼ í•™ìŠµí•˜ê¸° ìœ„í•´ì„œ `Motion imitation` ê¸°ë²•ì„ í™œìš©í–ˆë‹¤.
- ë¡œë´‡ì˜ ë„˜ì–´ì§€ê³  ë‚˜ì„œ ë¹ ë¥´ê²Œ ì •ìƒìì„¸ë¡œ íšŒë³µí•  ìˆ˜ ìˆë„ë¡ `Recovery policy`ë¥¼ í•™ìŠµí–ˆë‹¤.
- ê°•í™”í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ë“¤ ì¤‘ì—ì„œ [`REDQ(Randomized Ensembled Double Q-Learning)`](https://arxiv.org/abs/2101.05982) ë¼ëŠ” ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í–ˆëŠ”ë°, ì´ ì•Œê³ ë¦¬ì¦˜ì€ ì—¬ëŸ¬ê°œ Q-networkë“¤ì˜ ì•™ìƒë¸”ì„ í†µí•´ randomizationì„ í•´ì„œ Q-learning ê³„ì—´ì˜ ì•Œê³ ë¦¬ì¦˜ë“¤ì˜ sample-efficiencyì™€ ì•ˆì •ì„±ì„ í–¥ìƒì‹œí‚¨ ì•Œê³ ë¦¬ì¦˜ì´ë‹¤.

## Main Contribution

> ë³¸ ë…¼ë¬¸ì˜ ì£¼ìš” contributionì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

- 4ì¡± ë³´í–‰ ë¡œë´‡ì˜ agileí•œ locomotion skillì„ real-worldì—ì„œ í•™ìŠµí•˜ê¸° ìœ„í•œ `fine-tuning ìë™í™” ì‹œìŠ¤í…œ`ì„ ì œì•ˆí•˜ì˜€ë‹¤.
- ì²˜ìŒìœ¼ë¡œ `ìë™í™” reset`ê³¼ `on-board ìƒíƒœ ì¶”ì •`ì„ í†µí•´ real-worldì—ì„œ fine-tuningì´ ë  ìˆ˜ ìˆìŒìœ¼ë¡œ ë³´ì˜€ë‹¤.
- `A1` ë¡œë´‡ì„ ê°€ì§€ê³  dynamic skillë“¤ì„ í•™ìŠµí•´ì„œ ì™¸ë¶€ ì”ë””ì—ì„œ ì•ìœ¼ë¡œ, ë’¤ë¡œ pacingì„ í•˜ê³  3ê°€ì§€ ë‹¤ë¥¸ ì§€í˜• íŠ¹ì§•ì„ ê°€ì§„ í™˜ê²½ì—ì„œ side-steppingì„ í•  ìˆ˜ ìˆì—ˆë‹¤.

## Details with Hash tags
> ì› ë…¼ë¬¸ì˜ `II. Related Work` section ì°¸ê³ 

`#Cumbersome controller designs`

- ì´ì „ì˜ ë¡œë´‡ controllerë“¤ì€ footstep planning, trajectory optimization, model-predictive control (MPC) ë“±ì˜ ì¡°í•©ìœ¼ë¡œ ë§Œë“¤ì–´ì§€ê³  ìˆì—ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ëŸ° ë°©ë²•ë“¤ì€ ë¡œë´‡ì˜ ë™ì—­í•™ê³¼ ê° ë¡œë´‡ë§ˆë‹¤ ë‹¤ë¥´ê³  ê° skillë§ˆë‹¤ ë‹¤ë¥¸ ë§ì€ ìš”ì†Œë“¤ì„ ê³ ë ¤í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì— ì •ë§ ì–´ë ¤ì› ë‹¤.

`#Sim2Real`

- `trial-and-error`ë¼ëŠ” ë°ì´í„°ì— ë§¤ìš° ì˜ì¡´ì„±ì´ ë†’ì€ ê°•í™”í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì˜ íŠ¹ì„±ê³¼ í•˜ë“œì›¨ì–´ì˜ safety ì´ìŠˆ ë•Œë¬¸ì— ë³´í†µ ë¡œë´‡ ê°•í™”í•™ìŠµ agentëŠ” `ì‹œë®¬ë ˆì´ì…˜ ê¸°ë°˜`ìœ¼ë¡œ í•™ìŠµëœë‹¤. í•˜ì§€ë§Œ ì‹œë®¬ë ˆì´ì…˜ì—ì„œ í•™ìŠµí•˜ë©´ì„œ ì‹¤ì œë¡œ ë§Œë‚˜ë³´ì§€ ì•Šì€ real-worldì˜ ëª¨ë“  ì¡°ê±´ë“¤ì„ ì˜ˆìƒí•˜ê³  í•™ìŠµí•˜ê¸°ë€ ì‚¬ì‹¤ìƒ ë¶ˆê°€ëŠ¥í•˜ë©° ê°€ì¥ robustí•œ policyë¼ê³  í• ì§€ë¼ë„ ëª¨ë“  ìƒí™©ì— ëŒ€í•´ generalization ë˜ì—ˆë‹¤ê³  í•  ìˆ˜ ì—†ë‹¤.

`#Real-world`

- ì´ì „ì— ë³µì¡í•œ motionë“¤ì„ í•™ìŠµí•˜ê²Œ í•˜ê¸° ìœ„í•´ì„œ environmentì˜ ë‹¤ì–‘í•œ ì¥ì¹˜ë“¤ë¡œ ë‹¤ì–‘í•œ ìƒíƒœ ì •ë³´ë¥¼ ë§Œë“¤ì–´ì„œ ì‚¬ìš©í–ˆì§€ë§Œ ë³¸ ì—°êµ¬ì—ì„œëŠ” real-worldì—ì„œ ì‘ë™í•˜ê³  ìˆëŠ” ë¡œë´‡ì—ì„œ fine-tuningì„ í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì— `ë¡œë´‡ì˜ on-boardì—ì„œ ë°›ì„ ìˆ˜ ìˆëŠ” ëª¨ë“  state estimation ì •ë³´ë“¤ì„ ê°€ì§€ê³ ë§Œ` ì§„í–‰í–ˆìœ¼ë©° motion captureë‚˜ ì™¸ë¶€ ì¥ì¹˜ë“¤ì„ ë³„ë„ë¡œ ì‚¬ìš©í•˜ì§€ ì•Šì•˜ë‹¤.

- scratchë¶€í„° ì‹¤ì œ í™˜ê²½ì—ì„œ ë‹¨ìˆœí•œ êµ¬ì¡°ì˜ ë¡œë´‡ë“¤ë¡œ walking gaitsë“¤ì„ í•™ìŠµí•˜ëŠ”ê²Œ ì•„ë‹ˆë¼, `A1` ë¡œë´‡ìœ¼ë¡œ `pacing, side stepping` ë“± ë§¤ìš° ìì—°ìŠ¤ëŸ½ê³  ì¡°ê¸ˆì€ ë¶ˆì•ˆì •í•˜ê³  ì„¸ë°€í•œ balancingì´ ìš”êµ¬ë˜ëŠ” skillë“¤ì„ í•™ìŠµí•  ìˆ˜ ìˆì—ˆë‹¤. (ê¸°ì¡´ì˜ ì—°êµ¬ë“¤ì€ balancingì— ë§¤ìš° ì‹ ê²½ì“´ ë‚˜ë¨¸ì§€ ëŠë¦¬ê³  ë¶€ìì—°ìŠ¤ëŸ¬ìš´ walking gaits ì— ì¹˜ì¤‘í•œ ë©´ì´ ìˆì—ˆë‹¤.) ë³¸ ë…¼ë¬¸ì˜ ì—°êµ¬ì—ì„œ `motion imitationê³¼ ì‹¤ì œ í™˜ê²½ì—ì„œì˜ fine-tuning` ì´ ì´ëŸ° ë‹¤ì´ë‚˜ë¯¹í•œ taskë“¤ì„ ì„±ê³µì‹œí‚¤ëŠ”ë° ë§¤ìš° ì¤‘ìš”í•œ ì—­í• ì„ í–ˆë‹¤. ë˜í•œ ì‹¤ì œ í™˜ê²½ì—ì„œ ë¡œë´‡ì´ ì‘ë™í•˜ë©´ì„œ ë„˜ì–´ì§ˆ ë•Œ, manualí•˜ê²Œ ë¡œë´‡ì˜ resetí•˜ê±°ë‚˜ recoveryì‹œí‚¤ì§€ ì•Šê³  `ê°•í™”í•™ìŠµìœ¼ë¡œ ìë™ì ìœ¼ë¡œ reset` í•  ìˆ˜ ìˆëŠ” controllerë¥¼ ë§Œë“¤ì–´ì„œ ì‚¬ìš©í–ˆë‹¤.

`#Few-shot adaptation`

- ê¸°ì¡´ì˜ `Adaptation structure`ë¼ëŠ” êµ¬ì¡°ë¥¼ ë§Œë“¤ì–´ì„œ í•™ìŠµì‹œì¼œì„œ latent ë˜ëŠ” explicití•œ í™˜ê²½ì— ëŒ€í•œ descriptorë¡œ adaptiveí•œ policyë¥¼ ë§Œë“œëŠ” ì—°êµ¬ë“¤ì´ ìˆì—ˆìœ¼ë‚˜, ì´ ê¸°ë²•ë“¤ ë˜í•œ ê²°êµ­ trainingì—ì„œ ê²½í—˜í–ˆë˜ ê²ƒë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ adaptiveí•¨ì„ ë³´ì´ëŠ” ê²ƒì´ë¯€ë¡œ ì‹¤ì œ test í™˜ê²½ì´ ì´ í—ˆìš© ë²”ìœ„ì—ì„œ ë§ì´ ë²—ì–´ë‚  ê²½ìš° ì œëŒ€ë¡œ ì‘ë™ì•ˆë˜ëŠ” ê²ƒì€ ë˜‘ê°™ë‹¤. ë”°ë¼ì„œ ê°•í™”í•™ìŠµìœ¼ë¡œ ì§€ì†ì ì¸ ì ì‘ì ì¸ í•™ìŠµëŠ¥ë ¥ì„ ë³´ì¥í•´ì„œ `ì–´ë–¤ test í™˜ê²½ì—ì„œë“  ì˜ ì‘ë™í•  ìˆ˜ ìˆë„ë¡` í–ˆë‹¤.

`#RL Algorithm`

- ê°•í™”í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œëŠ” ê¸°ì¡´ì˜ vision ê¸°ë°˜ ë§¤ë‹ˆí“°ë ˆì´í„°ë“¤ì—ì„œ grasping ì‘ì—…ì„ í•˜ëŠ” taskë“¤ì—ì„œ ë§ì´ ì“°ì¸ `off-policy model-free` RL ê¸°ë²•ë“¤ì„ ì°¸ê³ í•˜ì—¬ fixedë˜ì–´ ìˆëŠ” ë§¤ë‹ˆí“°ë ˆì´í„°ë“¤ë³´ë‹¤ ë” challengingí•œ `floating-based ë³´í–‰ ë¡œë´‡ì˜ locomotion`ì— ì ìš©í•´ì„œ ì„±ê³µì‹œì¼°ë‹¤.


# II. Fine-tuning Locomotion in the Real World

ê°•í™”í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ `multi-tasking`ì„ í•  ìˆ˜ ìˆë„ë¡ í•™ìŠµì‹œì¼°ë‹¤.

- `REDQ` ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ì´ìš©í•´ì„œ sample efficiencyë¥¼ ë†’ì¼ ìˆ˜ ìˆì—ˆë‹¤.
- í•™ìŠµëœ `reset policy`ë¥¼ ì´ìš©í•´ì„œ ì—¬ëŸ¬ê°œì˜ episodeë“¤ì„ ì´ì–´ì„œ(stitch together) í•™ìŠµì‹œì¼°ë‹¤.

## a) Overview

![](https://i.imgur.com/2q4OYul.png?1){fig-align="default"}

![](https://i.imgur.com/kIL9RPT.png?1){fig-align="default"}

- ìœ„ ì‚¬ì§„ì˜ ì „ì²´ ì‹œìŠ¤í…œì˜ ê°œëµë„ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´ **ê°ê°ì˜ policy**ëŠ” **í•˜ë‚˜ì˜ desired skill**ì„ í•™ìŠµí•˜ê²Œ ëœë‹¤. (forward, backward, reset)
- Agentì˜ policyëŠ” **ì‹œë®¬ë ˆì´ì…˜ì—ì„œ pretrained** í•œë‹¤. (Algorithm2: line 2~7)
    - ê° ì—í”¼ì†Œë“œê°€ ëë‚  ë•Œë§ˆë‹¤ **í•™ìŠµëœ recovery policy**ê°€ ë¡œë´‡ì„ ë‹¤ìŒ rolloutì„ í•  ìˆ˜ ìˆë„ë¡ ì¤€ë¹„ì‹œì¼œì¤€ë‹¤.
    - ê° skillì„ ìœ„í•œ policyë“¤ì€ **ë…ë¦½ì ìœ¼ë¡œ í•™ìŠµ**ë˜ê³  recovery policyë„ ë§ˆì°¬ê°€ì§€ë¡œ ë…ë¦½ì ìœ¼ë¡œ í•™ìŠµëœë‹¤.
- **Fine-tuningì„ ì‹¤ì œ ë¬¼ë¦¬ì ì¸ í™˜ê²½ì—ì„œ** ì§„í–‰í•˜ë©´ì„œ training processë¥¼ ê³„ì† ì´ì–´ë‚˜ê°ˆ ìˆ˜ ìˆë‹¤. (Algorithm2: line 8~14)
    - ì‹œë®¬ë ˆì´ì…˜ê³¼ ì‹¤ì œ í™˜ê²½ì˜ ì°¨ì´ë¥¼ ê³ ë ¤í•˜ì—¬ **ê° policyë“¤ì˜ replay bufferëŠ” ì´ˆê¸°í™”** ì‹œì¼œì¤€ë‹¤.(Algorithm2: line 12)
- **Multitask framework**ë¥¼ ì‚¬ìš©í–ˆë‹¤.(Algorithm2 ì°¸ê³ )

## b) Motion Imitation

- `Motion Imiation` ë°©ë²•ì„ ì´ìš©í•˜ì—¬ reference motion clipë“¤ì˜ skillë“¤ì„ ëª¨ë°© í•™ìŠµí•˜ë„ë¡ í–ˆëŠ”ë° ì´ëŠ” [`Learning Agile Robotic Locomotion Skills by Imitating Animals`](https://arxiv.org/abs/2004.00784)ë¼ëŠ” ë…¼ë¬¸ì—ì„œ ì œì‹œí•œ ë°©ë²•ì„ ë”°ë¼í–ˆë‹¤. (Algorithm 1: line1~4)
- **Reference motion $M$**ì´ ì£¼ì–´ì§€ë©´ agentì˜ì¼ë ¨ì˜ poseë“¤ê³¼ ë¹„êµí•˜ì—¬ section `III-B`ì—ì„œ ì†Œê°œë  reward functionì„ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµí•œë‹¤.
    - ì´ ë°©ë²•ì„ í†µí•´ **reference motion data**ë§Œ ë°”ê¿”ì£¼ë©´ ë°”ë¡œ ë‹¤ë¥¸ ì—¬ëŸ¬ skillë“¤ì„ ë°°ìš¸ ìˆ˜ ìˆë‹¤.
    - **recovery policy**ë¥¼ í•™ìŠµí•˜ê¸° ìœ„í•´ì„œ **standing pose**ë¥¼ ëª¨ë°©í•˜ë„ë¡ í•  ìˆ˜ ìˆë‹¤.(`III-C` ì°¸ê³ )

## c) Off-policy RL

- off-policy ì•Œê³ ë¦¬ì¦˜ì¸ **REDQ algorithm** ì‚¬ìš©í–ˆë‹¤.(Algorithm 1: line5~9)
    - SAC ì•Œê³ ë¦¬ì¦˜ì„ ë” ë°œì „ì‹œí‚¨ ì•Œê³ ë¦¬ì¦˜
    - time stepì— ëŒ€í•œ gradient stepë¹„ìœ¨ì„ ì¦ê°€ì‹œì¼œì„œ ê°•í™”í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì˜sample efficiencyë¥¼ ë†’ì˜€ë‹¤.
    - ë„ˆë¬´ ë§ì€ gradient stepì„ í•  ê²½ìš°ì— ì¼ì–´ë‚  ìˆ˜ ìˆëŠ” overestimation issueë¥¼ ì•™ìƒë¸” ê¸°ë²•ì„ ì´ìš©í•´ì„œ ì™„í™”í•  ìˆ˜ ìˆì—ˆë‹¤.
    
(ìì„¸í•œ ëª¨ë°©í•™ìŠµê³¼ ê°•í™”í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì˜ training ê³¼ì • ì•Œê³ ë¦¬ì¦˜ì€ `Algorithm 1`ì„ ì°¸ê³ )

![](https://i.imgur.com/sBvIszd.png?1){fig-align="default"}

# III. System Design

> Setting
- A1 robot from Unitree
- PyBullet simulator
- **motion imitation skills**ì„ ì–»ê¸° ìœ„í•´ì„œ
    - ê³µê°œëœ ë°ì´í„° ì…‹ë“¤ ì¤‘ì— **dog pacing**ì˜ mocapì„ ë…¹í™”í•˜ê³  retargetting í•˜ì˜€ë‹¤.
    - ë¡œë´‡ì˜ ì—­ê¸°êµ¬í•™ì„ ì´ìš©í•´ì„œ A1 ë¡œë´‡ì˜ **side-step motion**ì„ ì• ë‹ˆë©”ì´ì…˜ìœ¼ë¡œ ë§Œë“¤ì–´ì„œ ì‚¬ìš©í–ˆë‹¤.
- REDQ ì•Œê³ ë¦¬ì¦˜
    - Adam optimizer
    - learning rate of 10âˆ’4
    - batch size of 256 transitions
    - TensorFlow

## A. State & Action Spaces

1. State space
    - StateëŠ” ì—°ì†ì ì¸ **3 timesteps**ì—ì„œ ì–»ì€ ì•„ë˜ ì •ë³´ë“¤ë¡œ ì •ì˜í–ˆë‹¤.
        - Root orientation (read from the IMU)
        - Joint angles
        - Previous actions
    - **Policy**ëŠ” ìœ„ì—ì„œ ë§í•œ **Proprioceptive input** ë¿ë§Œ ì•„ë‹ˆë¼ **a goal $g_t$**ì— ëŒ€í•œ ì •ë³´ë„ inputìœ¼ë¡œ ë°›ê²Œ ëœë‹¤.
        - $g_t$ëŠ” future timestepsì—ì„œì˜ reference motionì—ì„œ ê³„ì‚°ëœ **Target pose (root position, root rotation, joint angles)**ì˜ ì •ë³´ë¥¼ í¬í•¨í•œë‹¤.
        - **4 future target poses** ëŠ” í˜„ì¬ timestepì—ì„œ ì•½ 1ì´ˆ ì •ë„ ì´í›„ì˜ poseë“¤ì´ë‹¤.

2. Action space

    - Actionì€ **12 joints**ë“¤ì— ëŒ€í•œ **PD position targets** ì´ë‹¤.
    - **33Hz**ì˜ ì£¼íŒŒìˆ˜ë¡œ commandê°€ ì ìš©ëœë‹¤.
    - ìì—°ìŠ¤ëŸ¬ìš´ ì›€ì§ì„ì„ ìœ„í•´ PD targetsì„ **low-pass filter**ë¥¼ ë¡œë´‡ì— ì ìš©í•˜ê¸° ì „ì— í†µê³¼ì‹œì¼œì¤€ë‹¤.

## B. Reward Function

$$
\begin{gathered}r_{t}=w^{\mathrm{p}} r_{t}^{\mathrm{p}}+w^{\mathrm{v}} r_{t}^{\mathrm{v}}+w^{\mathrm{e}} r_{t}^{\mathrm{e}}+w^{\mathrm{rp}} r_{t}^{\mathrm{rp}}+w^{\mathrm{rv}} r_{t}^{\mathrm{rv}} \\w^{\mathrm{p}}=0.5, w^{\mathrm{v}}=0.05, w^{\mathrm{e}}=0.2, w^{\mathrm{rp}}=0.15, w^{\mathrm{rv}}=0.1\end{gathered}
$$

---

- $r_{t}^{\mathrm{p}}$ : ë¡œë´‡ì˜ **joint rotation** ê°’ë“¤ì„ reference motionì˜ joint rotationê³¼ ë§ì¶”ë„ë¡ í•˜ëŠ” reward term
    
    $$
    r_{t}^{\mathrm{p}}=\exp \left[-5 \sum_{j}\left\|\hat{q}_{t}^{j}-q_{t}^{j}\right\|^{2}\right]
    $$
    
    - $\hat{q}_{t}^{j}$ : ì‹œì  $t$ì— reference motionì˜ $j$ë²ˆì§¸ jointì˜ **local rotation**
    - $q_{t}^{j}$ : ë¡œë´‡ì˜ $j$ë²ˆì§¸ joint **local rotation**

- $r_{t}^{\mathrm{v}}$ : **joint velocities**
- $r_{t}^{\mathrm{e}}$ : **end-effector positions**
- ë¡œë´‡ì´ reference root motionì„ ì˜ tracking í•˜ê²Œ í•˜ê¸° ìœ„í•œ reward term
    - $r_{t}^{\mathrm{rp}}$ : **root pose reward**
    - $r_{t}^{\mathrm{rv}}$ : **root velocity reward**

---

ì´ì „ë¶€í„° ê°•ì¡°í•´ì™”ë“¯ì´, ì‹¤ì œ í™˜ê²½ì—ì„œ `fine-tuning`ê³¼ì •ì„ ì§„í–‰í•˜ê¸° ìœ„í•´ì„œ on-board ì„¼ì„œë“¤ì˜ ê°’ì„ ì´ìš©í•´ì„œ reward functionì„ ë””ìì¸í•˜ì˜€ê³  ì‹¤ì œ ë¬¼ë¦¬ì ì¸ í™˜ê²½ì—ì„œ êµ¬ë™í•  ë•Œ ì´ë¥¼ ìƒíƒœ ì¶”ì • ê¸°ë²•ì„ ì´ìš©í•´ì„œ rewardë¥¼ êµ¬í•˜ê²Œ ëœë‹¤. ë”°ë¼ì„œ ì•„ë˜ì˜ ìƒíƒœ ì¶”ì • ë°©ë²•(State Estimation)ì´ fine-tuningì˜ ì„±ëŠ¥ì„ ê²°ì •í•˜ëŠ” ì¤‘ìš”í•œ ë¶€ë¶„ì´ ëœë‹¤.

- Real-worldì—ì„œ ë¡œë´‡ì˜ **linear root velocity**ë¥¼ ì˜ ì¶”ì •í•˜ê¸° ìœ„í•´ì„œ **Kalman filter**ë¥¼ ì‚¬ìš©í–ˆë‹¤.
    - ì¹¼ë§Œ í•„í„°ëŠ” IMU ì„¼ì„œì—ì„œ accelerationê³¼ orientation ê°’ë“¤ì„ ì½ì–´ì„œ foot contact sensorsë¡œ ê°’ë“¤ì„ ë³´ì •í•œë‹¤.
    - ì²˜ìŒì— ë°œ ëì˜ ì†ë„ë¥¼ 0ìœ¼ë¡œ ìƒê°í•´ì„œ ê° ë‹¤ë¦¬ì˜ joint velocitiesë¥¼ ê³ ë ¤í•˜ì—¬ ëª¸ì²´ì˜ ì†ë„ë¥¼ ê³„ì‚°í•˜ê³  IMUìœ¼ë¡œë¶€í„° ì¶”ì •í–ˆë˜ ê°’ì„ ë³´ì •í•œë‹¤.
- ì´ë ‡ê²Œ ê³„ì‚°ëœ **linear velocity**ë¥¼ ë¡œë´‡ì˜ position ì¶”ì •ê°’ì— í†µí•©ì‹œí‚¨ë‹¤.

![](https://i.imgur.com/1DW4hsQ.png?1){fig-align="default"}

ìœ„ì˜ ê·¸ë˜í”„ë“¤ì— ë³¼ ìˆ˜ ìˆë“¯ì´(ì•„ë˜ì—ì„œ ìœ„ ë°©í–¥ìœ¼ë¡œ),

- **angular velocityì™€ orientation** ì„¼ì„œ ê°’ë“¤ì€ ë§¤ìš° ì •í™•í–ˆë‹¤.
- **linear velocity**ëŠ” ë§¤ìš° ì •í™•í•˜ì§„ ì•Šì•˜ì§€ë§Œ í—ˆìš©ê°€ëŠ¥í–ˆë‹¤.(reasonable)
- **position drifts**ëŠ” ìƒë‹¹íˆ ë²—ì–´ë‚˜ëŠ” ë¶€ë¶„ì´ ìˆì—ˆì§€ë§Œ, ê° ì—í”¼ì†Œë“œì—ì„œ reward functionì„ ê³„ì‚°í•  ì •ë„ë¡œì˜ ì í•©í•œ ê°’ë“¤ì„ ë³´ì—¬ì£¼ì—ˆë‹¤.

## C. Reset Controller

- **reset policy**ë¥¼ ì‹œë®¬ë ˆì´ì…˜ì—ì„œ í•™ìŠµí•˜ê¸° ìœ„í•´ ë‹¤ì–‘í•œ **initial states**ì—ì„œ ì‹œì‘í•˜ë„ë¡ í–ˆë‹¤. 

â†’ ë¡œë´‡ì„ randomí•œ height & orientationì—ì„œ ë–¨ì–´ëœ¨ë ¤ì„œ ì•„ë˜ ì‚¬ì§„ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´ ë‹¤ì–‘í•œ **initial states**ë¥¼ ì„¤ì •

![](https://i.imgur.com/2soOO1g.png?1){fig-align="default"}

- Motion imitation ëª©ì í•¨ìˆ˜ë¥¼ ìˆ˜ì •í•´ì„œ **single, streamlined reset policy**ë¥¼ í•™ìŠµì‹œì¼°ë‹¤.

- Reference motionì„ ê°€ì§€ê³  ë¡œë´‡ì´ ì •í™•íˆ ì–´ë–»ê²Œ ì¼ì–´ë‚˜ì•¼ í• ì§€ë¥¼ ì•Œë ¤ì£¼ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ì•„ë˜ì™€ ê°™ì€ ë°©ë²•ìœ¼ë¡œ reset policyë¥¼ í•™ìŠµì‹œì¼°ë‹¤.

1. policyê°€ **rolling right side up**ì„ ìœ„í•œ rewardë§Œì„ ê°€ì§€ê³  í•™ìŠµí•œë‹¤.
2. ë§Œì•½ ë¡œë´‡ì´ **upright**í•˜ëŠ”ë° ì„±ê³µí•˜ë©´ ì´í›„ì— **motion imitation reward**ë¥¼ ì¶”ê°€ì‹œì¼œì„œ í•™ìŠµë‹ˆë‹¤.
    - ì´ë•Œì˜ reference motionì€ **standing pose**ê°€ ë˜ê³  ë¡œë´‡ì´ ë˜‘ë°”ë¡œ ì„¤ ìˆ˜ ìˆë„ë¡ í•™ìŠµì‹œí‚¨ë‹¤.
- ì´ëŸ° ë°©ì‹ìœ¼ë¡œ í•™ìŠµëœ **reset policy**ëŠ” ë‹¤ì–‘í•œ test ì§€í˜•ì—ì„œ fine-tuning ì—†ì´ë„ ì˜ ë™ì‘í–ˆë‹¤.(tranfered well)

# IV. Experiments

ğŸ’¡ ì‹¤í—˜ ê²°ê³¼ì—ì„œ ì£¼ëª©í•´ì„œ ë´ì•¼í•  ì§ˆë¬¸ 3ê°€ì§€!

1. ë³¸ ë…¼ë¬¸ì—ì„œ ì œì‹œí•œ finetuning-based methodê°€ ì´ì „ì˜ ë°©ë²•ë“¤ì— ë¹„í•´ `ì‹œë®¬ë ˆì´ì…˜ trianingì„ ì¶©ë¶„íˆ í™œìš©`í•˜ê³  `ì‹¤ì œ ë¬¼ë¦¬ í™˜ê²½ì—ì„œ ì ì‘`í•  ìˆ˜ ìˆì—ˆëŠ”ê°€?

2. ë³¸ ë…¼ë¬¸ì—ì„œ ì œì‹œí•œ ì‹œìŠ¤í…œ ë””ìì¸ ìš”ì†Œë“¤ì´ `feasibility of real-world training`ì— ì–´ë–¤ ì˜í–¥ì„ ì£¼ì—ˆëŠ”ê°€?

3. `ì–¼ë§ˆë‚˜ ë‹¤ì–‘í•œ ì‹¤ì œ ë¬¼ë¦¬ì ì¸ í™˜ê²½ë“¤`ì—ì„œ autonomous, online fine-tuning ë°©ë²•ì´ ë¡œë´‡ì˜ skillì„ í–¥ìƒì‹œì¼°ëŠ”ê°€?

## A. Simulation Experiments

- agentì˜ policyë¥¼ ë¨¼ì € íŠ¹ì • ì‹œë®¬ë ˆì´ì…˜ ì…‹íŒ…ì—ì„œ í•™ìŠµì‹œí‚¨ í›„ì— **í•™ìŠµëœ ì‹œë®¬ë ˆì´ì…˜ê³¼ ë˜ ë‹¤ë¥¸** ì‹œë®¬ë ˆì´ì…˜ í™˜ê²½ ì…‹íŒ…ì— â€œdeployedâ€í•œ í›„ ê²°ê³¼ë¥¼ ì‚´í´ë³´ì•˜ë‹¤.
-  **Learned forward pacing gait**ê°€ í…ŒìŠ¤íŠ¸ í™˜ê²½ë“¤ì—ì„œ ì–¼ë§ˆë‚˜ ë¹¨ë¦¬ ì ìš©ë˜ëŠ”ì§€ í™•ì¸í•´ë³´ì•˜ë‹¤.
- **Standard dynamics randomization (mass, inertia, motor strength, friction, latency ë³€ë™)**ìœ¼ë¡œ Pre-trainì„ **flat** groundì—ì„œ ì§„í–‰í–ˆë‹¤.

- The test terrains : test í™˜ê²½ë“¤ë¡œëŠ” ì´ 3ê°€ì§€ë¡œ ì‹¤í—˜í•˜ì˜€ë‹¤.
    1. **a flat ground** : pre-training ê³¼ì •ì˜ ì‹œë®¬ë ˆì´ì…˜ ì…‹íŒ…ê³¼ ìœ ì‚¬í•œ test í™˜ê²½ 
    2. pre-training ê³¼ì •ì˜ ì‹œë®¬ë ˆì´ì…˜ ì…‹íŒ…ê³¼ ë‹¤ì†Œ ë‹¤ë¥¸ test í™˜ê²½ :
        1. **randomized heightfield** : ëœë¤í•˜ê²Œ ì§€í˜•ì˜ ë†’ì´ë¥¼ ì„¤ì •í•œ ìš¸í‰ë¶ˆí‰í•œ ì§€í˜•
        2. **a low friction surface** : ë‚®ì€ ë§ˆì°°ê³„ìˆ˜ë¥¼ ê°€ì§€ëŠ” ì§€í˜•, ë¹™íŒê¸¸ê³¼ ê°™ì€ ë¯¸ë„ëŸ¬ìš´ ì§€í˜•(Training ê³¼ì •ì—ì„œ ê²½í—˜í•œ ë§ˆì°°ê³„ìˆ˜ ë¶„í¬ì™€ í•œì°¸ ë™ë–¨ì–´ì§„ ë§ˆì°°ê³„ìˆ˜ë¥¼ ê°€ì§€ê³  ìˆìŒ)

-  ë¹„êµêµ°
    1. `latent space` : í˜¸ìœ¨ì ì¸ ë‹¤ì–‘í•œ dynamics parametersì— ëŒ€í•œ í•™ìŠµì„ í•˜ê¸° ìœ„í•´ latent spaceì— í‘œí˜„ëœ behaviorsì„ í•™ìŠµ
    2. `RMA`: dynamics randomizationí•œ ëª¨ë¸. ìœ„ì—ì„œ ì–¸ê¸‰í•œ `Adaptation Module`ì„ ê°€ì§€ê³  í•™ìŠµ
    3. `Vanilla SAC` : Soft Actor-Critic ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ í•™ìŠµ
    4. `Ours(REDQ)`: 10ê°œì˜ Q-functionsì„ ê°€ì§€ê³  randomly sample 2ë¡œ í•™ìŠµ

---

ì‹¤í—˜ ê²°ê³¼ëŠ” ì•„ë˜ì™€ ê°™ì•˜ë‹¤. 

- `RMA`ëŠ” training í™˜ê²½ì—ì„œë§Œ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì–´ Adaptation Moduleì˜ í•œê³„ì ì„ ëª…í™•íˆ ë³´ì—¬ì£¼ì—ˆë‹¤.
- `SAC`ì— ë¹„í•´ì„œ `Ours`ê°€ sample efficiencyê°€ ì¢‹ì„ ë¿ë§Œ ì•„ë‹ˆë¼ ìˆ˜ë ´í•˜ëŠ” Return ê°’ë„ ë†’ì•˜ë‹¤.

![](https://i.imgur.com/BCyV1Xk.png?1){fig-align="default"}


## B. Real-World Experiments

- ì‹œë®¬ë ˆì´ì…˜ì—ì„œ í•™ìŠµëœ Agentë¥¼ 4ê°œì˜ real-world í™˜ê²½(Outdoor 1ê°œ, Indoor 3ê°œ)ì—ì„œ test í–ˆë‹¤.

- ëª¨ë“  (real-world) test ì§€í˜• ì‹¤í—˜ì€ ì‹œë®¬ë ˆì´ì…˜ì˜ flat groundì—ì„œ pre-trainingëœ agentë¡œ ì‹¤í—˜í•œ ê²ƒì´ì—ˆìœ¼ë©°, ì²˜ìŒì— bufferë¥¼ 5000 samplesë¡œ ì´ˆê¸°í™” í•´ì£¼ê³  ì‹œì‘í•œ ë‹¤ìŒ test real world í™˜ê²½ì—ì„œ policyë¥¼ **fine-tuning** í•´ì£¼ì—ˆë‹¤.

1. Outdoor `grassy lawn`:
    - **slippery surface**ë¥¼ ê°€ì§€ê³  ìˆì–´ì„œ ë°œì´ ì”ë””ì—ì„œ ë¯¸ë„ëŸ¬ì§€ê±°ë‚˜ í™ì— ë¹ ì§ˆ ìˆ˜ ìˆë‹¤.
    - ì• í˜¹ì€ ë’¤ë¡œ ì›€ì§ì´ëŠ” **pacing gait**ë¥¼ fine-tuning í•˜ë„ë¡ í–ˆë‹¤.
        - pacing gait: ì¢Œë‚˜ ìš°ì˜ 2ê°œì˜ ë‹¤ë¦¬ê°€ í•œë²ˆì— ì›€ì§ì´ëŠ” ê±¸ìŒìƒˆ

    - Pre-trained **forward** pacing policyëŠ” ë§¤ìš° ì¡°ê¸ˆë§Œ ì•ìœ¼ë¡œ ê°ˆ ìˆ˜ ìˆì—ˆê³ , pre-trained **backward** pacing policyëŠ” ì˜ ë„˜ì–´ì§€ëŠ” ê²½í–¥ì´ ìˆì—ˆë‹¤. 
    
    - **ì‘ë™í•œ ì§€ ì•½ 2ì‹œê°„ ë§Œì—**, ë¡œë´‡ì€ (ì•„ì£¼ ì¡°ê¸ˆì˜ ë„˜ì–´ì§ì€ ìˆì—ˆì§€ë§Œ) ì§€ì†ì ì´ê³  ì•ˆì •ì ìœ¼ë¡œ ì• í˜¹ì€ ë’¤ë¡œ pacing gaitë¥¼ í•  ìˆ˜ ìˆì—ˆë‹¤.

    ![](https://i.imgur.com/EeTTRF8.png?2){fig-align="default"}
    
2. Indoor

    - `Carpeted room`: 
    ë†’ì€ ë§ˆì°°ê³„ìˆ˜ë¥¼ ê°€ì§€ëŠ” ì§€í˜•ìœ¼ë¡œ (ì¹´í«ì´ í‘¹ì‹ í•˜ë¯€ë¡œ) ë¡œë´‡ì˜ ê³ ë¬´ë¡œ ë§ˆê°ë˜ì–´ ìˆëŠ” ë°œì´ ì‹œë®¬ë ˆì´ì…˜ì—ì„œ í•™ìŠµëœ ê²ƒê³¼ ë‹¤ë¥´ê²Œ ì•ˆì •ì ì´ì§€ ì•Šì€ ì»¨íƒì„ í•˜ê²Œ ë  ìˆ˜ ìˆë‹¤.
    
    ![](https://i.imgur.com/Pviw7be.png?1){fig-align="default"}
    
    - `Doormat with crevices`: 
    ë§¤íŠ¸ í‘œë©´ì— ë°œì´ ë¹ ì§ˆ ìˆ˜ë„ ìˆëŠ” í™˜ê²½ì´ë‹¤.
    
    ![](https://i.imgur.com/865kXHv.png?1){fig-align="default"}
    
    - `Memory foam`: 
    4cm ì •ë„ì˜ ë‘ê»˜ì˜ ë©”ëª¨ë¦¬í¼ìœ¼ë¡œ ë°œì´ ë§¤íŠ¸ë¦¬ìŠ¤ì— ë¹ ì§€ê³  í‰í‰í•˜ê³  ë”±ë”±í•œ ë°”ë‹¥ê³¼ ë¹„êµí–ˆì„ ë•Œ ì´ í™˜ê²½ì—ì„œëŠ” gait(ê±¸ìŒìƒˆ)ê°€ ìƒë‹¹íˆ ë³€í™”ê°€ ë§ì´ ì¼ì–´ë‚  ìˆ˜ ìˆë‹¤.
    
    ![](https://i.imgur.com/EfZvlS5.png?1){fig-align="default"}
    
    - `Indoors`ì—ì„œëŠ”, pre-trained side stepping policyê°€ ì›€ì§ì¼ ë•Œ ë§¤ìš° ë¶ˆì•ˆì •í–ˆê³  motionì„ ëë‚´ê¸° ì „ì— ë„˜ì–´ì¡Œë‹¤.

    - ê·¸ëŸ¬ë‚˜ ê° ì§€í˜• ì…‹íŒ…ì—ì„œ **2.5 ì‹œê°„ ì´ë‚´ë¡œ** ë¡œë´‡ì´ ë¹„í‹€ê±°ë¦¼ ì—†ì´ skillì„ ìˆ˜í–‰í•  ìˆ˜ ìˆì—ˆë‹¤.

---

ì‹¤í—˜ ê²°ê³¼ëŠ” ì•„ë˜ì™€ ê°™ì•˜ë‹¤. 

![](https://i.imgur.com/NRNMAex.png?1){fig-align="default"}

## C. Semi-autonomous training
- ì „ë°˜ì ì¸ ëª¨ë“  ì‹¤í—˜ë“¤ì—ì„œ, the recovery policyëŠ” 100% ì„±ê³µì ì´ì—ˆë‹¤.
- ë³¸ ë…¼ë¬¸ì—ì„œ ì œì‹œëœ ë°©ë²•ìœ¼ë¡œ í•™ìŠµëœ reset controllerì™€ Unitreeì—ì„œ ì œê³µí•œ built-in rollover controllerë¥¼ ë¹„êµí•´ë³´ì•˜ë‹¤.
    - **On hard surfaces** : ë‘ ê°€ì§€ controllers ëª¨ë‘ íš¨ê³¼ì ìœ¼ë¡œ ì˜ ì‘ë™í–ˆì§€ë§Œ **built-in** ì»¨íŠ¸ë¡¤ëŸ¬ëŠ” **learned policy**ì— ë¹„í•´ ìƒë‹¹íˆ ëŠë ¸ë‹¤.
    - **On the memory foam** : **built-in** ì»¨íŠ¸ë¡¤ëŸ¬ëŠ” ë” ì„±ëŠ¥ì´ ì¢‹ì§€ ëª»í–ˆë‹¤.

# V. Conclusion

- grass, carpets, doormats and memory foamê³¼ ê°™ì€ ë‹¤ì–‘í•œ **real-world settings**ì—ì„œ **finetune locomotion policies**ì„ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ì‹œìŠ¤í…œì„ ì œì•ˆí•˜ì˜€ë‹¤.
- **autonomous data collection**ê³¼ **data-efficient model-free RL**ì˜ ê²°í•©ì„ ë³´ì—¬ì£¼ì—ˆë‹¤.
- ë¡œë´‡ì˜ ë„˜ì–´ì§ì—ì„œ **automated recoveries**ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆë„ë¡, ë¡œë´‡ì˜ **on-board sensors**ë“¤ì„ ê°€ì§€ê³  **state estimation**ì„ í–ˆìœ¼ë©°, ì´ ì •ë³´ë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ íš¨ê³¼ì ì¸ **reward calculation**ì„ ì œì•ˆí•˜ì˜€ë‹¤.
- ë‹¤ì–‘í•œ locomotion skillì— ëŒ€í•œ **data-efficient fine-tuning** ë°©ë²•ì„ ë³´ì—¬ì£¼ì—ˆë‹¤.
- ë³µì¡í•˜ê³  ë‹¤ì–‘í•˜ë©° **ëŠì„ì—†ì´** ë³€í™”í•˜ëŠ” **real-world environments**ì— ëŒ€ì‘í•  ìˆ˜ ìˆëŠ” `a lifelong learning system for legged robots`ë¥¼ future workë¡œ ë³´ê³  ìˆë‹¤. 

# Review
> ë…¼ë¬¸ ë¦¬ë·°í›„ì˜ ì£¼ê´€ì ì¸ ì¥ë‹¨ì ì„ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

- Pros ğŸ‘
    - ë¡œë´‡ operationì˜ ëª…í™•í•œ í•œê³„ì , ê²°êµ­ ë¡œë´‡ì´ ë™ì‘í•´ì•¼ í•˜ëŠ” í™˜ê²½ì´ ê³„ì† ë³€í™”í•  ìˆ˜ ë°–ì— ì—†ë‹¤ëŠ” ë¬¸ì œì  ì¸ì‹ì´ ì¢‹ì€ ê²ƒ ê°™ìŒ
    - ì‹¤ì œ ì‚°ì—…ì—ì„œë„ íš¨ìœ¨ì ì¼ ê²ƒ ê°™ì€ ë°©ë²•ì´ë¼ê³  ìƒê°ì´ ë“¤ì—ˆìŒ
    - rest policyì˜ ì„±ê³µë¥ ì´ ëŒ€ë‹¨í–ˆìŒ
    
- Cons ğŸ‘
    - Out door ì‹¤í—˜ì—ì„œëŠ” ì—¬ëŸ¬ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ë¹„êµí•´ë³´ì§„ ì•Šì•˜ìŒ
    - ì•Œê³ ë¦¬ì¦˜ì€ ë™ì¼í•˜ê²Œ í•˜ê³  3ê°œì˜ policyë¥¼ ë”°ë¡œ ë‘ì§€ ì•Šê³  1ê°œì˜ policyë¡œ ë§Œë“¤ì—ˆì„ ë•Œë„ ë¹„êµêµ°ìœ¼ë¡œ ë¹„êµí•´ì„œ ì‹¤í—˜ê²°ê³¼ê°€ ìˆì—ˆìœ¼ë©´ ë” ì¢‹ì•˜ì„ ê²ƒ ê°™ìŒ

# Reference

- [Original Paper](https://arxiv.org/abs/2110.05457)
- [Project Homepage](https://sites.google.com/berkeley.edu/fine-tuning-locomotion)
- [Randomized Ensembled Double Q-Learning: Learning Fast Without a Model](https://arxiv.org/abs/2101.05982)
- [REDQ REVIEW](https://github.com/utilForever/rl-paper-study/blob/main/4th/210510%20-%20Randomized%20Ensembled%20Double%20Q-Learning%20Learning%20Fast%20Without%20a%20Model%2C%20X.%20Chen%20et%20al%2C%202021.pdf)
