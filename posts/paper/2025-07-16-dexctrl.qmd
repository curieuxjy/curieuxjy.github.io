---
title: "üìÉDexCtrl Î¶¨Î∑∞"
date: 2025-07-16
categories: [sim2real, adaptive, rl, hand]
toc: true
number-sections: true
description: Towards Sim-to-Real Dexterity with Adaptive Controller Learning
---


- [Paper Link](https://arxiv.org/pdf/2505.00991)

1.  ü§ñ Dexterous manipulationÏùò sim-to-real transfer Î¨∏Ï†úÎäî Ï†ÄÏàòÏ§Ä controller dynamic Î∂àÏùºÏπòÎ°ú Ïù∏Ìï¥ Î∞úÏÉùÌïòÎ©∞, Í∏∞Ï°¥ Î∞©Î≤ïÏùÄ manual tuningÏù¥ÎÇò randomizationÏóê ÏùòÏ°¥ÌñàÏäµÎãàÎã§.
2.  üß† Î≥∏ ÎÖºÎ¨∏ÏùÄ historical informationÏùÑ ÌôúÏö©ÌïòÏó¨ actionÍ≥º controller parametersÎ•º ÎèôÏãúÏóê ÌïôÏäµÌïòÎäî adaptive controller ÌïôÏäµ ÌîÑÎ†àÏûÑÏõåÌÅ¨Ïù∏ DexCtrlÏùÑ Ï†úÏïàÌï©ÎãàÎã§.
3.  üöÄ DexCtrlÏùÄ Ïã§Ìñâ Ï§ë ÏûêÎèôÏúºÎ°ú control parametersÎ•º Ï°∞Ï†ïÌïòÏó¨ sim-to-real gapÏùÑ ÌÅ¨Í≤å Ï§ÑÏù¥Í≥† contact-rich dexterous taskÏóêÏÑú Ïö∞ÏàòÌïú real-world ÏÑ±Îä•ÏùÑ Îã¨ÏÑ±Ìï©ÎãàÎã§.

<center>
<img src="../../images/2025-07-16-dexctrl/1.png" alt="Overview" width="100%" />
    <figcaption>Overview</figcaption>
</center>

---

# Brief Review

DexCtrlÏùÄ ÏãúÎÆ¨Î†àÏù¥ÏÖò(sim)ÏóêÏÑú ÌõàÎ†®Îêú Î°úÎ¥á Ï†úÏñ¥ Ï†ïÏ±ÖÏùÑ Ïã§Ï†ú(real) ÌôòÍ≤ΩÏúºÎ°ú Ïù¥Ï†ÑÌï† Îïå Î∞úÏÉùÌïòÎäî ÎÇúÏ†ú, ÌäπÌûà Ï†ÄÏàòÏ§Ä(low-level) Ï†úÏñ¥Í∏∞ ÎèôÏó≠Ìïô Î∂àÏùºÏπò Î¨∏Ï†úÎ•º Ìï¥Í≤∞ÌïòÎäî Îç∞ Ï§ëÏ†êÏùÑ Îëî Ïó∞Íµ¨ÏûÖÎãàÎã§. Í∏∞Ï°¥ Ï†ëÍ∑º Î∞©ÏãùÎì§ÏùÄ ÏàòÎèô ÌäúÎãù(manual tuning)Ïù¥ÎÇò Ï†úÏñ¥Í∏∞ Î¨¥ÏûëÏúÑÌôî(controller randomization)Ïóê ÏùòÏ°¥ÌñàÎäîÎç∞, Ïù¥Îäî ÎÖ∏Îèô ÏßëÏïΩÏ†ÅÏù¥Í≥† ÌäπÏ†ï ÏûëÏóÖÏóêÎßå Ïú†Ìö®ÌïòÎ©∞ ÌïôÏäµ ÎÇúÏù¥ÎèÑÎ•º ÎÜíÏù¥Îäî Îã®Ï†êÏù¥ ÏûàÏóàÏäµÎãàÎã§. Î≥∏ ÎÖºÎ¨∏ÏùÄ Ïù¥Îü¨Ìïú ÌïúÍ≥ÑÎ•º Í∑πÎ≥µÌïòÍ∏∞ ÏúÑÌï¥ ÎèôÏûë(action)Í≥º Ï†úÏñ¥ Îß§Í∞úÎ≥ÄÏàò(controller parameter)Î•º ÎèôÏãúÏóê ÌïôÏäµÌïòÎäî ÏÉàÎ°úÏö¥ ÌîÑÎ†àÏûÑÏõåÌÅ¨Ïù∏ DexCtrlÏùÑ Ï†úÏïàÌï©ÎãàÎã§.

**ÌïµÏã¨ Î¨∏Ï†ú ÏãùÎ≥Ñ:**

Ïó∞Íµ¨Îäî ÏãúÎÆ¨Î†àÏù¥ÏÖò-Ïã§Ï†ú Î∂àÏùºÏπò(sim-to-real gap)Ïùò Ï§ëÏöîÌïú ÏõêÏù∏ Ï§ë ÌïòÎÇòÎ°ú Î°úÎ¥á Ï†úÏñ¥Í∏∞ Í∞ÑÏùò Ï∞®Ïù¥Î•º ÏßÄÎ™©Ìï©ÎãàÎã§. **ÎèôÏùºÌïú Í∂§Ï†Å(trajectory)Ïù¥ÎùºÎèÑ Ï†úÏñ¥ Îß§Í∞úÎ≥ÄÏàòÍ∞Ä Îã§Î•¥Î©¥ Ïã§Ï†ú Ï†ëÏ¥âÎ†•(contact force)Í≥º ÎèôÏûëÏù¥ ÌÅ¨Í≤å Îã¨ÎùºÏßà Ïàò ÏûàÍ∏∞ ÎïåÎ¨∏ÏûÖÎãàÎã§.** Í∏∞Ï°¥Ïùò Î∞©ÏãùÏùÄ Ï†úÏñ¥ Îß§Í∞úÎ≥ÄÏàòÎ•º Í≥†Ï†ïÌïòÍ±∞ÎÇò ÌõàÎ†® Ïãú Î¨¥ÏûëÏúÑÌôîÌïòÏó¨ robustnessÎ•º ÎÜíÏù¥Î†§ ÌñàÏßÄÎßå, Ïù¥Îäî Ïã§ÏßàÏ†ÅÏù∏ Î¨∏Ï†ú Ìï¥Í≤∞Ïóê ÌïúÍ≥ÑÍ∞Ä ÏûàÏóàÏäµÎãàÎã§.

**DexCtrlÏùò Î∞©Î≤ïÎ°†:**

DexCtrlÏùÄ Í≥ºÍ±∞Ïùò Í∂§Ï†Å Ï†ïÎ≥¥ÏôÄ Ï†úÏñ¥Í∏∞ Ï†ïÎ≥¥Î•º Í∏∞Î∞òÏúºÎ°ú ÎèôÏûëÍ≥º Ï†úÏñ¥ Îß§Í∞úÎ≥ÄÏàòÎ•º ÎèôÏãúÏóê ÏòàÏ∏°ÌïòÍ≥† Ï†ÅÏùëÏ†ÅÏúºÎ°ú Ï°∞Ï†ïÌïòÎäî Î©îÏª§ÎãàÏ¶òÏùÑ ÏÇ¨Ïö©Ìï©ÎãàÎã§. Ïù¥Î•º ÌÜµÌï¥ Ï†ïÏ±ÖÏù¥ Ïã§Ìñâ Ï§ëÏóê Ï†úÏñ¥ Îß§Í∞úÎ≥ÄÏàòÎ•º ÏûêÎèôÏúºÎ°ú ÌäúÎãùÌïòÏó¨ ÏãúÎÆ¨Î†àÏù¥ÏÖò-Ïã§Ï†ú Í∞ÑÍ∑πÏùÑ ÏôÑÌôîÌïòÍ≥†, Ï†ëÏ¥âÎ†• ÏÉÅÌò∏ÏûëÏö©Ïóê ÎåÄÌïú Ï∂îÎ°†ÏùÑ Í∞úÏÑ†ÌïòÏó¨ Ïã§Ï†ú ÏãúÎÇòÎ¶¨Ïò§ÏóêÏÑú Î°úÎ≤ÑÏä§Ìä∏ÎãàÏä§Î•º Ìñ•ÏÉÅÏãúÌÇµÎãàÎã§.

**ÌïµÏã¨ Î∞©Î≤ïÎ°† ÏÑ∏Î∂Ä ÏÇ¨Ìï≠:**

1.  **Îç∞Ïù¥ÌÑ∞ ÏàòÏßëÏùÑ ÏúÑÌïú Oracle Policy:**
    *   DexCtrlÏùÄ Î®ºÏ†Ä ÏãúÎÆ¨Î†àÏù¥ÏÖò ÌôòÍ≤ΩÏóêÏÑú Îã§ÏñëÌïú Î¨ºÏ≤¥ Î¨ºÎ¶¨ Îß§Í∞úÎ≥ÄÏàò(object physical parameters)Î•º ÏÇ¨Ïö©ÌïòÏó¨ ÌõàÎ†®Îêú "Ïò§ÎùºÌÅ¥ Ï†ïÏ±Ö(oracle policy)"ÏúºÎ°úÎ∂ÄÌÑ∞ Ï∂©Î∂ÑÌïú Îç∞Ïù¥ÌÑ∞Î•º ÏàòÏßëÌï©ÎãàÎã§.
    *   Ïò§ÎùºÌÅ¥ Ï†ïÏ±ÖÏùÄ Î™®Îç∏-ÌîÑÎ¶¨(model-free) Í∞ïÌôî ÌïôÏäµ(Reinforcement Learning) Í∏∞Î≤ïÏù∏ **PPO(Proximal Policy Optimization)**Î•º ÏÇ¨Ïö©ÌïòÏó¨ ÌïôÏäµÎê©ÎãàÎã§.
    *   Í∞Å ÏãúÍ∞Ñ Îã®Í≥Ñ $t$ÏóêÏÑú Ïò§ÎùºÌÅ¥ Ï†ïÏ±Ö $\pi(a_t, K_t | s_t)$Îäî ÌòÑÏû¨ ÏÉÅÌÉú $s_t$Î•º ÏûÖÎ†•Î∞õÏïÑ Ï°∞Ïù∏Ìä∏ ÎèôÏûë(joint action) $a_t$ÏôÄ Ï†úÏñ¥ Îß§Í∞úÎ≥ÄÏàò $K_t$Î•º ÎèôÏãúÏóê Ï∂úÎ†•Ìï©ÎãàÎã§. Ïó¨Í∏∞ÏÑú $a_t$Îäî $t$ ÏãúÏ†êÏùò Ï°∞Ïù∏Ìä∏ ÏúÑÏπò Î≥ÄÌôîÎüâÏùÑ ÎÇòÌÉÄÎÇ¥Î©∞, ÏõêÌïòÎäî Ï°∞Ïù∏Ìä∏ Í∂§Ï†ÅÏùÄ Ïù¥Ï†Ñ desired joint positionÏóê $a_t$Î•º ÎçîÌïòÏó¨ $q^d_t = q^d_{t-1} + a_t$Î°ú ÏñªÏñ¥ÏßëÎãàÎã§. desired joint velocityÎäî 0ÏúºÎ°ú ÏÑ§Ï†ïÎê©ÎãàÎã§.
    *   Î°úÎ¥á Ï†úÏñ¥Îäî Ï°∞Ïù∏Ìä∏ ÌÜ†ÌÅ¨ Ï†úÏñ¥(joint torque control) Î∞©ÏãùÏùÑ Îî∞Î•¥Î©∞, ÌÜ†ÌÅ¨ $\tau$Îäî Îã§ÏùåÍ≥º Í∞ôÏù¥ Í≥ÑÏÇ∞Îê©ÎãàÎã§:
        $$\tau = K_P (q_d - q_c) + K_D (\dot{q}_d - \dot{q}_c)$$
        Ïó¨Í∏∞ÏÑú $q_d$ÏôÄ $q_c$Îäî Í∞ÅÍ∞Å ÏõêÌïòÎäî(desired) Ï°∞Ïù∏Ìä∏ ÏúÑÏπòÏôÄ ÌòÑÏû¨(current) Ï°∞Ïù∏Ìä∏ ÏúÑÏπòÎ•º ÎÇòÌÉÄÎÇ¥Î©∞, $\dot{q}_d$ÏôÄ $\dot{q}_c$Îäî Í∞ÅÍ∞Å ÏõêÌïòÎäî Ï°∞Ïù∏Ìä∏ ÏÜçÎèÑÏôÄ ÌòÑÏû¨ Ï°∞Ïù∏Ìä∏ ÏÜçÎèÑÎ•º ÎÇòÌÉÄÎÉÖÎãàÎã§. $K_P$ÏôÄ $K_D$Îäî Í∞ïÏÑ±(stiffness) Î∞è Í∞êÏá†(damping) ÌñâÎ†¨ÏùÑ ÎÇòÌÉÄÎÇ¥Îäî Ï†úÏñ¥ Îß§Í∞úÎ≥ÄÏàòÏù¥Î©∞, Í∞ÑÎã®Ìï®ÏùÑ ÏúÑÌï¥ ÎåÄÍ∞Å ÌñâÎ†¨(diagonal matrices)Î°ú Í∞ÄÏ†ïÌï©ÎãàÎã§. $K = \{K_P, K_D\}$Îäî Ï†ÑÏ≤¥ Ï†úÏñ¥ Îß§Í∞úÎ≥ÄÏàò ÏßëÌï©ÏûÖÎãàÎã§.
    *   **ÏÉÅÌÉú(State) Íµ¨ÏÑ±:** ÏÉÅÌÉú $s_t \in \mathbb{R}^{219}$Îäî ÏßÄÎÇú ÏÑ∏ Îã®Í≥ÑÏùò Î¨ºÏ≤¥ Î∞è Î°úÎ¥á Í¥ÄÏ∏° Ï†ïÎ≥¥Î•º Ìè¨Ìï®Ìï©ÎãàÎã§.
        *   Î°úÎ¥á Ï†ïÎ≥¥ $s^r_t \in \mathbb{R}^{64}$: ÌòÑÏû¨ Ï°∞Ïù∏Ìä∏ ÏúÑÏπò $q^c_t$, ÏõêÌïòÎäî Ï°∞Ïù∏Ìä∏ ÏúÑÏπò $q^d_t$, Ï†úÏñ¥ Îß§Í∞úÎ≥ÄÏàò $K_t$.
        *   Î¨ºÏ≤¥ Ï†ïÎ≥¥ $s^{obj}_t \in \mathbb{R}^9$: Î¨ºÏ≤¥ ÏûêÏÑ∏(pose) $p^{obj}_t \in \mathbb{R}^6$, Î¨ºÏ≤¥ ÏÜçÏÑ± Î≤°ÌÑ∞ $\mu \in \mathbb{R}^3$ (Ïä§ÏºÄÏùº, ÏßàÎüâ, ÎßàÏ∞∞).
        $$s_t \triangleq (s^r_{t-2:t}, s^{obj}_{t-2:t})$$
        $$s^r_t \triangleq (q^c_t, q^d_t, K_t)$$
        $$s^{obj}_t \triangleq (p^{obj}_t, \mu)$$
    *   **Î≥¥ÏÉÅ(Reward) Ìï®Ïàò:** Î≥¥ÏÉÅ $r_t$Îäî Ï£ºÎ°ú ÎÑ§ Í∞ÄÏßÄ Î∂ÄÎ∂ÑÏúºÎ°ú Íµ¨ÏÑ±Îê©ÎãàÎã§: $$r_t = r_{rotation} + r_{contact} + r_{smoothness} + r_{terminate}$$

1.  **ÎèôÏûë ÏòàÏ∏° Î∞è Ï†úÏñ¥ Îß§Í∞úÎ≥ÄÏàò ÏòàÏ∏° Î™®Îìà:**
    *   Ïò§ÎùºÌÅ¥ Ï†ïÏ±ÖÏù¥ ÏàòÏßëÌïú Îç∞Ïù¥ÌÑ∞Î•º ÏÇ¨Ïö©ÌïòÏó¨, DexCtrlÏùÄ ÌïôÏÉù Ï†ïÏ±Ö(student policy)ÏùÑ Îëê Í∞úÏùò Î∂ÑÎ¶¨Îêú Î™®ÎìàÎ°ú ÌõàÎ†®Ìï©ÎãàÎã§: ÎèôÏûë ÏòàÏ∏° Î™®Îìà(Action Prediction Module)Í≥º Ï†úÏñ¥ Îß§Í∞úÎ≥ÄÏàò ÏòàÏ∏° Î™®Îìà(Control Parameters Prediction Module). Ïù¥Îäî Í∞Å Î™®ÎìàÏù¥ ÌÉúÏä§ÌÅ¨Ïùò Í∑ºÎ≥∏Ï†ÅÏúºÎ°ú Îã§Î•∏ Ï∏°Î©¥ÏùÑ Ïù∏ÏΩîÎî©ÌïòÎ©∞, Ï†úÏñ¥ Îß§Í∞úÎ≥ÄÏàò ÏòàÏ∏°Ïù¥ ÎèôÏûë ÏòàÏ∏°Ïóê ÏòÅÌñ•ÏùÑ ÎØ∏ÏπòÏßÄ ÏïäÎèÑÎ°ù ÌïòÍ∏∞ ÏúÑÌï®ÏûÖÎãàÎã§.
    *   **Historical Information ÏÇ¨Ïö©:** Ïò§ÎùºÌÅ¥ Ï†ïÏ±ÖÏùÄ Ïã§Ï†ú ÌôòÍ≤ΩÏóêÏÑú ÏßÅÏ†ë Ï†ëÍ∑ºÌïòÍ∏∞ Ïñ¥Î†§Ïö¥ Î¨ºÏ≤¥ ÏÜçÏÑ± Í∞ôÏùÄ ÏõêÏãú Ï†ïÎ≥¥(primitive information)Î•º ÏÇ¨Ïö©Ìï©ÎãàÎã§. Ïù¥Î•º Ìï¥Í≤∞ÌïòÍ∏∞ ÏúÑÌï¥ DexCtrlÏùÄ Î°úÎ¥áÏùò Í≥†Ïú†ÏàòÏö©ÏÑ±(proprioception) Ïù¥Î†• ÏÉÅÌÉú(last ten stepsÏùò ÌòÑÏû¨ Î∞è ÏõêÌïòÎäî Ï°∞Ïù∏Ìä∏ Í∂§Ï†Å, Ìï¥Îãπ Ï†úÏñ¥ Îß§Í∞úÎ≥ÄÏàò)Î•º ÌïôÏÉù Ï†ïÏ±ÖÏùò ÏûÖÎ†•ÏúºÎ°ú ÏÇ¨Ïö©Ìï©ÎãàÎã§.
    *   **Î™®Îìà ÏÑ§Í≥Ñ:**
        *   **ÎèôÏûë ÏòàÏ∏° Î™®Îìà:** temporal historical inputÏùÑ Î™®Îç∏ÎßÅÌïòÍ∏∞ ÏúÑÌï¥ self-attention Î©îÏª§ÎãàÏ¶òÏùÑ ÏÇ¨Ïö©Ìï©ÎãàÎã§. Ïù¥Îäî Ï°∞Ïù∏Ìä∏ Í∂§Ï†Å Î≥ÄÌôîÏùò Í≤ΩÌñ•(trend)ÏùÑ ÌååÏïÖÌï©ÎãàÎã§.
        *   **Ï†úÏñ¥ Îß§Í∞úÎ≥ÄÏàò ÏòàÏ∏° Î™®Îìà:** cross-attentionÏùÑ ÏÇ¨Ïö©ÌïòÎ©∞, ÌòÑÏû¨ ÎèôÏûë(current action)Ïù¥ ÏøºÎ¶¨(query) Ïó≠Ìï†ÏùÑ ÌïòÍ≥† historical inputÍ∞Ä ÌÇ§(key)ÏôÄ Í∞í(value) Ïó≠Ìï†ÏùÑ Ìï©ÎãàÎã§. Ïù¥Îäî ÌòÑÏû¨ Ï°∞Ïù∏Ìä∏ ÎèôÏûëÍ≥º Ïù¥Î†• Ï†ïÎ≥¥ Í∞ÑÏùò Í¥ÄÍ≥ÑÎ•º Î™®Îç∏ÎßÅÌïòÏó¨ Ï†úÏñ¥ Îß§Í∞úÎ≥ÄÏàòÎ•º Ï∂îÎ°†ÌïòÎäî Îç∞ ÌôúÏö©Îê©ÎãàÎã§.
    *   **ÌõàÎ†® Î∞è Ï∂îÎ°†:**
        *   **ÌõàÎ†®:** Îëê Î™®ÎìàÏùÄ Í∞úÎ∞© Î£®ÌîÑ(open-loop) Î∞©ÏãùÏúºÎ°ú ÌõàÎ†®Îê©ÎãàÎã§. Ï¶â, Î™®Îì† ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞Îäî ÏàòÏßëÎêú ÏãúÎÆ¨Î†àÏù¥ÏÖò Îç∞Ïù¥ÌÑ∞ÏÖãÏóêÏÑú ÏßÅÏ†ë Í∞ÄÏ†∏ÏòµÎãàÎã§.
        *   **Ï∂îÎ°†:** ÏãúÎÆ¨Î†àÏù¥ÏÖòÍ≥º Ïã§Ï†ú ÌôòÍ≤Ω Î™®ÎëêÏóêÏÑú ÌèêÏáÑ Î£®ÌîÑ(closed-loop) Î∞©ÏãùÏúºÎ°ú ÏàòÌñâÎê©ÎãàÎã§. Ï¶â, ÌòÑÏû¨ Í∂§Ï†Å Í∞íÏùÄ Ïã§Ï†ú Î°úÎ¥á ÏÑºÏÑúÎ°úÎ∂ÄÌÑ∞ ÏñªÏñ¥ÏßëÎãàÎã§.
        *   ÏãúÎÆ¨Î†àÏù¥ÏÖòÏóêÏÑú Ïã§Ï†ú ÏãúÏä§ÌÖúÏúºÎ°ú Ï†úÏñ¥ Îß§Í∞úÎ≥ÄÏàòÎ•º ÏÑ†ÌòïÏ†ÅÏúºÎ°ú Îß§ÌïëÌïòÎ©∞, Ïù¥Îäî ÎåÄÎûµÏ†ÅÏù∏ ÏÉÅÌïú Î∞è ÌïòÌïú Ï∂îÏ†ïÏπòÎßåÏúºÎ°úÎèÑ Ï∂©Î∂ÑÌï©ÎãàÎã§.
        *   ÌïôÏÉù Ï†ïÏ±Ö ÌõàÎ†® Ï§ë ÌòÑÏû¨ Í∂§Ï†Å Í∞íÏóê Í∞ÄÏö∞ÏãúÏïà ÎÖ∏Ïù¥Ï¶à(Gaussian noise)Î•º Ï∂îÍ∞ÄÌïòÎäî Í≤ÉÏù¥ ÏãúÎÆ¨Î†àÏù¥ÏÖò-Ïã§Ï†ú Ï†ÑÏù¥(sim-to-real transfer)Ïóê Ï∂©Î∂ÑÌïòÎã§Îäî Í≤ÉÏùÑ Î∞úÍ≤¨ÌñàÏäµÎãàÎã§.

**Ïã§Ìóò Î∞è Í≤∞Í≥º:**
Îëê Í∞ÄÏßÄ Ï†ëÏ¥â Í∏∞Î∞òÏùò Î≥µÏû°Ìïú Ï°∞Ïûë ÏûëÏóÖ(in-hand object rotation, flipping)ÏóêÏÑú DexCtrlÏùò ÏÑ±Îä•ÏùÑ ÌèâÍ∞ÄÌñàÏäµÎãàÎã§.

1.  **ÏãúÎÆ¨Î†àÏù¥ÏÖò ÏÑ±Îä• Ìñ•ÏÉÅ:**
    *   ÍµêÎûÄ(disturbance) Ïú†Î¨¥Ïóê Í¥ÄÍ≥ÑÏóÜÏù¥ ÏãúÎÆ¨Î†àÏù¥ÏÖò ÌôòÍ≤ΩÏóêÏÑú DexCtrlÏùÄ Í∏∞Ï°¥Ïùò "Manual Tuning" Î∞è "Ours w/o PD" (Ï†úÏñ¥ Îß§Í∞úÎ≥ÄÏàò ÏòàÏ∏° Î™®ÎìàÏù¥ ÏóÜÎäî Î≤ÑÏ†Ñ) Î≤†Ïù¥Ïä§ÎùºÏù∏ ÎåÄÎπÑ ÌòÑÏ†ÄÌûà Ïö∞ÏàòÌïú ÏÑ±Îä•ÏùÑ Î≥¥ÏòÄÏäµÎãàÎã§. ÌäπÌûà, ÌöåÏ†Ñ ÏÜçÎèÑ(RotR) Î∞è Ïã§Ìå®ÍπåÏßÄÏùò ÏãúÍ∞Ñ(TTF)Ïù¥ ÌÅ¨Í≤å Ìñ•ÏÉÅÎêòÏóàÍ≥†, ÌÜ†ÌÅ¨ Ìå®ÎÑêÌã∞(Torque) Î∞è Î¨ºÏ≤¥ ÏÑ†Ìòï ÏÜçÎèÑ(ObjVel)Îäî Í∞êÏÜåÌñàÏäµÎãàÎã§.
    *   Ïù¥Îäî DexCtrlÏù¥ Ï†úÏñ¥Í∏∞ Î∂àÏùºÏπòÍ∞Ä ÏóÜÎäî ÏÉÅÌô©ÏóêÏÑúÎèÑ ÎèôÏûëÍ≥º Ï†úÏñ¥ Îß§Í∞úÎ≥ÄÏàòÎ•º ÎèôÏãúÏóê Ï°∞Ï†ïÌï®ÏúºÎ°úÏç® ÏûëÏóÖ ÌîÑÎ°úÏÑ∏Ïä§Î•º Ìö®Í≥ºÏ†ÅÏúºÎ°ú ÏïàÏ†ïÌôîÌïòÍ≥† Í∞ÄÏÜçÌôîÌï† Ïàò ÏûàÏùåÏùÑ ÏûÖÏ¶ùÌï©ÎãàÎã§.

2.  **Sim-to-Real Gap ÏôÑÌôî:**
    *   Ïã§Ï†ú ÌôòÍ≤ΩÏóêÏÑú DexCtrlÏùÄ Ï†úÎ°úÏÉ∑(zero-shot) ÏãúÎÆ¨Î†àÏù¥ÏÖò-Ïã§Ï†ú Ï†ÑÏù¥Î•º ÌÜµÌï¥ Î≤†Ïù¥Ïä§ÎùºÏù∏Îì§ÏùÑ ÏïïÎèÑÏ†ÅÏúºÎ°ú Îä•Í∞ÄÌñàÏäµÎãàÎã§. ÌäπÌûà, ÏãúÎÆ¨Î†àÏù¥ÏÖòÏóêÏÑúÎ≥¥Îã§ Ïã§Ï†ú ÌôòÍ≤ΩÏóêÏÑú DexCtrlÍ≥º "Ours w/o PD" Í∞ÑÏùò ÏÑ±Îä• Í≤©Ï∞®Í∞Ä Ìõ®Ïî¨ Îçî ÌÅ¨Í≤å ÎÇòÌÉÄÎÇ¨ÏäµÎãàÎã§.
    *   Ïù¥ Í≤∞Í≥ºÎäî Ïã§Ï†ú Î°úÎ¥áÏóêÏÑú Îß§ Îã®Í≥ÑÎßàÎã§ Ï†úÏñ¥ Îß§Í∞úÎ≥ÄÏàòÎ•º Ï†ÅÏùëÏ†ÅÏúºÎ°ú Ï°∞Ï†ïÌïòÎäî Í≤ÉÏùò Ï§ëÏöîÏÑ±ÏùÑ Í∞ïÏ°∞ÌïòÎ©∞, DexCtrlÏù¥ ÏãúÎÆ¨Î†àÏù¥ÏÖò-Ïã§Ï†ú Î¨∏Ï†úÎ•º Ìï¥Í≤∞ÌïòÎäî Îç∞ ÌïµÏã¨Ï†ÅÏù∏ Ïó≠Ìï†ÏùÑ Ìï®ÏùÑ Î≥¥Ïó¨Ï§çÎãàÎã§.

3.  **Îã§ÏñëÌïú Î¨ºÎ¶¨ Îß§Í∞úÎ≥ÄÏàòÎ•º Í∞ÄÏßÑ Î¨ºÏ≤¥Ïóê ÎåÄÌïú ÏÑ±Îä•:**
    *   ÏßàÎüâ(mass)Í≥º ÎßàÏ∞∞(friction)Ïù¥ Îã§Î•∏ Î¨ºÏ≤¥Ïóê ÎåÄÌïú Ï∂îÍ∞Ä ÌÖåÏä§Ìä∏ÏóêÏÑú DexCtrlÏùÄ ÌäπÌûà Î¨¥Í±∞Ïö¥ Î¨ºÏ≤¥ÏóêÏÑú Î≤†Ïù¥Ïä§ÎùºÏù∏Î≥¥Îã§ Ìõ®Ïî¨ Îõ∞Ïñ¥ÎÇú ÏÑ±Îä•ÏùÑ Î≥¥ÏòÄÏäµÎãàÎã§. Ïù¥Îäî DexCtrlÏù¥ Îã§ÏñëÌïú Î¨ºÎ¶¨Ï†Å ÌäπÏÑ±ÏùÑ Í∞ÄÏßÑ Î¨ºÏ≤¥Ïóê Îçî Ïûò Ï†ÅÏùëÌï† Ïàò ÏûàÏùåÏùÑ ÎÇòÌÉÄÎÉÖÎãàÎã§.

4.  **Ï†úÏñ¥ Îß§Í∞úÎ≥ÄÏàò(ÌäπÌûà Í∞ïÏÑ±)Ïùò ÏòÅÌñ• Î∂ÑÏÑù:**
    *   ÌïôÏäµÎêú Í∞ïÏÑ±($K_P$)Ïù¥ Î¨ºÏ≤¥ ÏßàÎüâ Î∞è ÎßàÏ∞∞Í≥º Ïñ¥ÎñªÍ≤å Í¥ÄÎ†®ÎêòÎäîÏßÄ Î∂ÑÏÑùÌñàÏäµÎãàÎã§. ÏãúÎÆ¨Î†àÏù¥ÏÖò Îç∞Ïù¥ÌÑ∞ Î∂ÑÏÑù Í≤∞Í≥º, Í∞ïÏÑ±ÏùÄ ÏßàÎüâÍ≥º Îã®Ï°∞ Ï¶ùÍ∞Ä(monotonically increasing) Í¥ÄÍ≥ÑÎ•º Î≥¥ÏòÄÏäµÎãàÎã§ (Î¨¥Í±∞Ïö¥ Î¨ºÏ≤¥Îäî Îçî ÌÅ∞ Ìûò ÌïÑÏöî).
    *   ÎßàÏ∞∞Í≥ºÏùò Í¥ÄÍ≥ÑÎäî Îçî ÎØ∏Î¨òÌïòÏó¨, ÌäπÏ†ï Í≤ΩÏö∞ÏóêÎäî Ï¶ùÍ∞ÄÌïòÍ≥† Îã§Î•∏ Í≤ΩÏö∞ÏóêÎäî Í∞êÏÜåÌïòÎäî Îì± ÏûëÏóÖ ÏùòÏ°¥Ï†ÅÏù∏ ÎèôÏó≠ÌïôÏùÑ ÎÇòÌÉÄÎÉàÏäµÎãàÎã§.
    *   Ïã§Ï†ú ÌôòÍ≤Ω Î∂ÑÏÑùÏóêÏÑúÎäî Î¨¥Í±∞Ïö¥ Î¨ºÏ≤¥Ïóê ÎåÄÌï¥ Í∞ïÏÑ±Ïù¥ ÌäπÏ†ï ÏãúÍ∞Ñ Îã®Í≥ÑÏóêÏÑú Ï¶ùÍ∞ÄÌïòÍ±∞ÎÇò ÏµúÎåÄ Í∞íÏúºÎ°ú Îçî Ïò§Îûò Ïú†ÏßÄÎêòÎäî Í≤ΩÌñ•ÏùÑ, Î∂ÄÎìúÎü¨Ïö¥(smoother) Î¨ºÏ≤¥Ïóê ÎåÄÌï¥ÏÑúÎäî ÏùºÎ∂Ä Ï°∞Ïù∏Ìä∏ÏóêÏÑú Ïú†ÏÇ¨Ìïú Ìå®ÌÑ¥ÏùÑ, Îã§Î•∏ Ï°∞Ïù∏Ìä∏ÏóêÏÑúÎäî ÏÉÅÎ∞òÎêú Í≤ΩÌñ•ÏùÑ Î≥¥ÏòÄÏäµÎãàÎã§.
    *   Ïù¥Îäî ÌïôÏäµÎêú Ï†úÏñ¥ Îß§Í∞úÎ≥ÄÏàòÍ∞Ä ÌïÑÏöîÌïú Ï†ëÏ¥âÎ†•Ïùò Î≥ÄÌôîÎ•º Ïù∏ÏΩîÎî©ÌïòÎ©∞, Î¨ºÏ≤¥ Ï°∞Ïûë ÏÑ±Îä• Ìñ•ÏÉÅÏóê Í∏∞Ïó¨ÌïúÎã§Îäî Í∞ÄÏÑ§ÏùÑ Í≤ÄÏ¶ùÌï©ÎãàÎã§.

**Í≤∞Î°† Î∞è ÌïúÍ≥Ñ:**

- DexCtrlÏùÄ Ïù¥Î†• Ï†ïÎ≥¥Î•º Í∏∞Î∞òÏúºÎ°ú ÎèôÏûëÍ≥º Ï†úÏñ¥ Îß§Í∞úÎ≥ÄÏàòÎ•º ÎèôÏãúÏóê Ï∂úÎ†•ÌïòÏó¨ ÎØºÏ≤©Ìïú Ï°∞Ïûë(dexterous manipulation)Ïùò ÏãúÎÆ¨Î†àÏù¥ÏÖò-Ïã§Ï†ú Í∞ÑÍ∑πÏùÑ Ìö®Í≥ºÏ†ÅÏúºÎ°ú Ï§ÑÏòÄÏäµÎãàÎã§.
- Ìñ•ÌõÑ Ïó∞Íµ¨ÏóêÏÑúÎäî Ïó¨Îü¨ ÎØºÏ≤©Ìïú ÏûëÏóÖÏù¥ Îã®Ïùº Ï†úÏñ¥ Îß§Í∞úÎ≥ÄÏàò ÏòàÏ∏° Î™®ÎìàÏùÑ Í≥µÏú†ÌïòÍ±∞ÎÇò, ÌïòÎìúÏõ®Ïñ¥ ÏßÄÏõê Ïãú Ïã§ÏãúÍ∞Ñ Ìûò ÌîºÎìúÎ∞±(force feedback) Í∏∞Î∞òÏùò Ïò®ÎùºÏù∏ ÎØ∏ÏÑ∏ Ï°∞Ï†ï(fine-tuning)ÏùÑ ÏàòÌñâÌï† Í≥ÑÌöçÏûÖÎãàÎã§.
- ÌòÑÏû¨ Î∞©Î≤ïÏùò ÌïúÍ≥ÑÎäî ÌïòÎìúÏõ®Ïñ¥ Ï†úÏïΩÏúºÎ°ú Ïù∏Ìï¥ Ïã§Ï†ú Ìûò ÎòêÎäî Ï¥âÍ∞Å ÏÑºÏÑú(tactile sensing)Î•º ÌÜµÌï©ÌïòÏßÄ Î™ªÌïòÍ≥†, LeapHand ÌîåÎû´ÌèºÏóê ÌïúÏ†ïÎêú Ïã§Ï†ú ÌôòÍ≤Ω ÌèâÍ∞ÄÏóê ÏûàÏäµÎãàÎã§.

---

# Detail Review

> DexCtrl: Towards Sim-to-Real Dexterity with Adaptive Controller Learning ‚Äì A Deep Dive

## Introduction and the Sim-to-Real Challenge in Dexterous Manipulation


Dexterous robotic manipulation has made remarkable strides in simulation, achieving complex feats like in-hand object rotation, tool use, and even piano playing. However, **transferring** these learned manipulation skills **from simulation to real robots** remains notoriously difficult. One critical but often under-addressed culprit is the **low-level controller mismatch** between simulation and reality. In other words, even if a policy learns a perfect motion trajectory in sim, the way a real robot‚Äôs motors execute that trajectory (governed by controller parameters like stiffness and damping) can differ, leading to **vastly different forces and outcomes**. Identical motions can succeed in simulation but slip or fail on the real hardware due to these dynamics discrepancies.


Traditionally, researchers have tackled the sim-to-real gap through tedious **manual tuning** of controller gains (e.g. adjusting PD controller stiffness/damping so the real robot‚Äôs behavior matches sim) or by massive **domain randomization** of physics parameters during training. Unfortunately, **both approaches have drawbacks**: manual tuning is labor-intensive and often imprecise, while randomizing controller parameters can **significantly increase training difficulty** and still may not guarantee success in the real world. These methods rely on trial-and-error and human intuition rather than a principled solution, and they **demand extensive human effort** in choosing parameter schedules or noise ranges. Clearly, there is a need for a better way to bridge this ‚Äúcontroller gap‚Äù in a ***robust, automatic*** manner.

**Enter DexCtrl** ‚Äì a new framework proposed to address this very challenge. DexCtrl explicitly targets the controller mismatch by allowing the policy to **adapt its own low-level controller parameters on the fly**, rather than treating them as fixed or randomly perturbed constants. In essence, DexCtrl lets the robot policy not only decide *what* actions to take, but also *how* to execute those actions in terms of controller stiffness/damping. By doing so, it aims to **mitigate the sim-to-real gap at the source**: the torque-generation level. This deep-dive review will explore **how DexCtrl works**, its key contributions and novelty, and examine in detail the experimental evidence of its effectiveness in both simulation and real-world trials.

## Key Idea: Adaptive Controller Learning with DexCtrl

At its core, **DexCtrl is about jointly learning the control actions *and* the controller parameters** needed to carry out those actions. The key insight is that the policy can be trained to automatically tune its controller gains (like a robot adjusting its ‚Äúmuscle stiffness‚Äù) based on what it has felt and done in the recent past. This is achieved by giving the policy **historical context** and letting it output two things at each time step: (1) the next high-level action (e.g. desired joint positions), and (2) the appropriate controller parameters (e.g. stiffness $K$ and damping $D$ for a PD controller) to apply that action. By continuously adjusting these parameters in a closed loop, DexCtrl can compensate for modeling errors or unexpected force interactions *as they occur*, rather than hoping a fixed controller will work for all situations.

Crucially, DexCtrl‚Äôs policy **observes the controller state itself**. The current stiffness/damping values are fed into the observation space along with recent trajectory data, so the policy is aware of ‚Äúhow hard‚Äù it‚Äôs driving the motors at any given time. This enables better reasoning about contact forces ‚Äì if a task isn‚Äôt going as expected, the policy can sense if maybe the controller was too stiff or too compliant, and adjust accordingly. By explicitly including controller parameters in the observation, the agent can directly capture force-related feedback that would otherwise be hidden.

**Figure 1** below illustrates the concept. The top shows a conventional approach where the policy outputs only the action $a_t$ (like desired joint positions) and uses a fixed controller gain $K_{\text{const}}$ for the whole task. This often requires **extensive manual gain tuning** and still might be inaccurate if conditions change. The bottom shows DexCtrl‚Äôs approach: the policy outputs both $a_t$ and an adaptive gain $K_t$ at each step, allowing it to tweak the controller behavior continually. This adaptive scheme promises **reduced labor (no manual tuning)** and **better accuracy** in execution.

<center>
<img src="../../images/2025-07-16-dexctrl/2.png" width="100%" />
</center>
<b>
<figcaption>*Conceptual comparison. **Top:** Standard policy outputs action $a_t$ with fixed controller parameters ($K_{\text{const}}$), requiring manual tuning and risking performance mismatch. **Bottom:** DexCtrl outputs both action and adaptive controller parameters ($K_t$) each step, automatically adjusting the ‚Äútorque controller‚Äù to current needs (green check). The right shows dexterous manipulation tasks (in-hand rotation and flipping) where DexCtrl was evaluated.*</figcaption>
</b>

The **novelty** of DexCtrl‚Äôs adaptive controller learning lies in this **automatic, time-varying adjustment of control gains as part of the policy**. Previous works in dexterous manipulation sim-to-real mostly ignored this lever ‚Äì they would train with a fixed or randomly perturbed controller and hope for the best. While adaptive force control has been explored in simpler robotic tasks (like adjusting stiffness for wiping or pivoting tasks), it had **rarely been applied to complex dexterous hand manipulation**. DexCtrl is among the first to show that *learning to adjust control parameters* in tandem with the action policy can yield significant gains for dexterous hands, providing a more **principled approach to closing the sim-to-real gap** at the controller level. In summary, the key contributions of DexCtrl can be highlighted as follows:

* **Identifying Controller Mismatch:** It pinpoints the often-overlooked **mismatch in robot low-level controllers** as a critical factor in sim-to-real failure, and directly tackles it by making control parameters adaptable. This shifts sim-to-real transfer from being a manual tuning art to an automated learning problem.
* **Joint Action-Control Learning:** It introduces a **simple yet effective framework** that jointly learns the high-level actions and the low-level controller gains from historical data. By leveraging recent trajectories and past controller settings, the policy gains adaptivity to variations in force requirements and object dynamics.
* **Improved Performance Across Domains:** Through extensive experiments, DexCtrl demonstrates **significantly better performance than baseline approaches** (manual tuning and non-adaptive policies) both in simulation and in zero-shot real-world tests, across multiple contact-rich tasks. The results include not just task success metrics but also stability and efficiency measures, along with insightful analysis of *why* it works.

Next, we dive into **how DexCtrl is designed and trained**, before examining the experimental evidence in detail.

## DexCtrl Framework and Methodology

### How DexCtrl Works: Overview of the Architecture

The DexCtrl framework consists of a two-stage learning process: first training an *oracle policy* in simulation via reinforcement learning, and then **distilling that experience into two learned modules** ‚Äì one for action prediction and one for controller parameter prediction. **Figure 2** provides an overview of this pipeline. During an offline training phase (Fig. 2a), a high-performing oracle policy (trained with RL in diverse simulated settings) is used to generate a dataset of trajectories. This dataset contains sequences of states, actions, and the oracle‚Äôs chosen controller parameters at each step. Then, DexCtrl trains two separate neural networks (students) on this data via supervised learning: one network learns to predict the next action, and the other learns to predict the next controller parameters, both based on the history. At runtime (Fig. 2b), these two learned modules work together in a closed loop: the action module predicts the desired next joint motion, and the controller module immediately predicts the appropriate stiffness/damping to execute that motion, given what has happened in the recent past.


<center>
<img src="../../images/2025-07-16-dexctrl/3.png" width="100%" />
</center>
<b>
<figcaption>***DexCtrl framework.** (a) *Open-Loop Training:* An oracle policy (trained with PPO in simulation) generates rollouts of states $s_t$ with actions $a_t$ and controller gains $K_t$. A dataset of sequences $(q^c, q^d, a, K)$ is collected (where $q^c, q^d$ are current and desired joint angles). Two student networks are distilled on this data: an **Action Prediction Module** (blue) that learns to output $\hat{a}_t$ (desired joint positions) from historical trajectories, and a **Control Parameters Prediction Module** (purple) that learns to output $\hat{K}*t$ (controller gains) from the history *and* the current intended action. (Losses are mean-squared errors to the oracle‚Äôs outputs.) (b) *Closed-Loop Inference:* On the real robot or new simulation runs, the policy observes the actual robot state $q^c_t$ (and a window of past states $q^c*{t-1},‚Ä¶$), feeds the recent history into the action module to get $\hat{a}_t$, then feeds the history plus the proposed action into the control module to get $\hat{K}_t$. The robot‚Äôs low-level torque controller then executes, applying torque $\tau_t$ based on the chosen $K_t$ and the error between $q^d_t$ and $q^c_t$. This loop repeats every time step, continually adapting actions and controller parameters.*</figcaption>
</b>

Under the hood, DexCtrl‚Äôs observation at each decision step contains a **window of historical information**. In the implementation, the authors use the past 10 time steps of robot proprioceptive data ‚Äì specifically, the sequences of **actual joint positions $q^c$**, the **previous desired joint positions $q^d$**, and the **controller parameters $K$ (stiffness, damping)** applied at those steps. This rich history serves as the input for both student networks.

To effectively extract insights from this temporal data, DexCtrl employs an **attention-based architecture**. The action prediction module uses a **self-attention mechanism** over the time window of past states, allowing it to learn temporal patterns and trends in the trajectory (for example, to estimate object motion and infer what the next goal should be). In contrast, the controller prediction module uses a **cross-attention mechanism**: it takes the current planned action as a query and attends to the historical states as keys/values. Intuitively, this helps the controller module learn the relationship between *what action you‚Äôre about to do* and *what controller settings worked well in the past in similar situations*. The authors note that while both modules see the same form of input (the last 10 steps of $q^c, q^d, K$), their roles are different ‚Äì the action module is understanding the motion trajectory context, whereas the controller module is learning how past actions and gains led to outcomes, analogous to how a human might adjust their grip stiffness based on recent success or slip events.

It‚Äôs worth emphasizing that **DexCtrl decouples the learning of actions and controller parameters** into two separate prediction modules on purpose. This design ensures that the task of choosing the best motor commands isn‚Äôt entangled with the task of tuning gains ‚Äì which could otherwise make learning unstable. By training them separately (but on the same experiences), DexCtrl can achieve adaptive control without sacrificing the quality of the action policy.

### Training and Deployment Strategy

The training of DexCtrl‚Äôs neural modules is done in an **open-loop, supervised fashion** using the logged simulation data. That is, during training they directly feed in historical sequences from the oracle‚Äôs trajectories and train the networks to predict the oracle‚Äôs next action and gain, step by step. This avoids any compounding error during training and is effectively a form of **behavioral cloning/distillation** from the oracle RL policy. Because the oracle was trained with access to privileged simulation information (like object pose and properties) that may not be available on real robots, DexCtrl‚Äôs student networks rely purely on the observable history of joint states. Impressively, the authors show that the networks can infer necessary information (like how the object is moving or how heavy it might be) just from proprioceptive history, sidestepping the need for direct object state input. This is analogous to a robot feeling the weight of an object through the effort it needed over the last few moments.

When deploying to the real world (or for closed-loop testing), DexCtrl runs in **real-time closed-loop**: it uses the robot‚Äôs actual sensor readings at each step as the latest ‚Äúcurrent state‚Äù in the history, then produces an action and controller adjustment which are immediately applied. Notably, **no further training or fine-tuning was done on the real robot** ‚Äì the policy is deployed zero-shot after training in simulation. To account for units differences between sim and real, they apply a simple linear scaling of the controller parameters (based on approximate upper/lower bounds) rather than any painstaking system identification. Beyond that, no manual calibration was needed.

An interesting detail is how DexCtrl manages to transfer well without heavy randomization. The oracle policy was trained on a range of object physical properties (mass, friction) for generality, but the authors did **not** perform extreme randomization of sensor noise or controller gains when training the oracle. Instead, they added a small amount of Gaussian noise to the student networks‚Äô inputs during training, which turned out to be **sufficient for sim-to-real transfer**. This suggests that because DexCtrl‚Äôs policy can adjust to discrepancies on the fly, it doesn‚Äôt require as broad a training distribution to cover every possible gap ‚Äì a little bit of noise was enough to make the student robust, simplifying the training process for the teacher (oracle) policy. In other words, DexCtrl inherently handles some variability through adaptation, whereas a fixed-gain policy would have needed aggressive domain randomization to cope with the same variability.

## Experimental Evaluation and Results

The authors evaluated DexCtrl on **two challenging dexterous manipulation tasks** using a 16-degree-of-freedom *LEAP hand* (a sophisticated robotic hand platform). The tasks were designed to be **contact-rich** and sensitive to force execution, thus ideal for testing sim-to-real performance:

* **In-Hand Object Rotation:** The robot hand must rotate a grasped object (e.g. a cube, apple, etc.) around a specified axis using finger dexterity, without dropping it. This task stresses object-hand contacts and requires controlled forces to keep the object stable while spinning it.
* **Object Flipping on a Table:** The hand, starting with an object resting on a surface (table), must flip the object 180¬∞ (like flipping a block or a Rubik‚Äôs cube to its other face) using a coordinated push-lift motion. This involves contact between the object, the hand, *and* the table ‚Äì increasing sensitivity to dynamics (e.g. friction with the table and impact forces).

Both tasks were first learned in simulation (using PyBullet or a similar physics simulator), then the policies were deployed on a **real-world setup** with the LEAP hand performing the same tasks on real objects. To quantify performance, the authors defined several metrics:

* **Rotation Rate (RotR):** How fast the object rotates around the target axis. In simulation this was measured via a shaped reward; in real experiments it‚Äôs measured in radians of rotation achieved per trial. Higher RotR means better task success (faster rotation).
* **Time to Fail (TTF):** How long the hand can continue the task before failing (dropping the object in rotation, or the object falling off course in flipping). This reflects stability ‚Äì longer is better.
* **Object Linear Velocity (ObjVel):** The average translational speed of the object during the task (only measured in sim). Lower velocity implies the object isn‚Äôt rattling or moving erratically, hence indicating a *stable manipulation*.
* **Torque Penalty:** A measure of energy or effort, computed from the torques applied (sim only). Lower torque penalty means the controller is efficient and smooth, not applying excessive forces.

### Baselines for Comparison

DexCtrl was compared against two main baselines to isolate the benefits of adaptive controller tuning:

* **Manual Tuning Baseline:** This represents the *conventional approach*: First, manually adjust the simulator‚Äôs controller parameters (PD gains) and add slight randomness so that a policy trained in sim might transfer to real. In practice, the authors carefully tuned the stiffness/damping by comparing simulated vs real trajectory responses, and trained a new RL ‚Äúoracle‚Äù policy with those tuned gains (plus minor gain randomization for robustness). This oracle and its student policy **do not adapt gains at runtime** ‚Äì they output only fixed-gain actions. Essentially, this baseline is a well-tuned fixed controller approach, requiring significant human effort to set up.
* **Ours w/o PD (No Adaptive Gains):** This ablation is *DexCtrl without the core idea* ‚Äì the adaptive gain module is turned off. The policy is trained using the same oracle data as DexCtrl (which had varying gains in simulation), but during student training and deployment, the controller gains are fixed to the manually tuned values. The action module is still trained on the oracle‚Äôs trajectories, but it ignores the adaptive aspect. This baseline tests whether the advantage comes simply from our data collection and better action trajectories, versus the *addition of adaptive control*.

Both baselines and DexCtrl share the same training dataset size and neural network architecture for the action module, ensuring a fair comparison.

### Simulation Results: Performance Boost Even Without a Gap (Q1)

Before tackling sim-to-real transfer, the authors first asked: *Even if there is no sim-to-real discrepancy (i.e., using the same controllers in sim as in training), does jointly adapting controller parameters improve performance?* To test this, they evaluated the policies in simulation under ideal conditions (no controller mismatch) and also with some external disturbances (random forces applied to the object) to simulate a more challenging scenario.

The results were striking: **DexCtrl outperformed both baselines in simulation, even when all methods used the same simulator dynamics**. For the in-hand rotation task, DexCtrl achieved higher rotation speeds and longer stability than the manually tuned baseline, *and* used significantly less torque on average. For example, with random disturbances applied, DexCtrl‚Äôs rotation speed (RotR) was about 24% higher than the manual baseline, and it sustained the rotation for longer (TTF \~256 vs 239 time steps). It also expended only \~25% of the torque effort that the manual baseline did, indicating much more efficient control. Even the ‚ÄúOurs w/o PD‚Äù ablation (no adaptive gains) performed better than the manual baseline in rotation, thanks to the improved trajectories generated by our training approach, but **full DexCtrl still had the edge**.

On the flipping task, the difference was even more pronounced. DexCtrl nearly **doubled the rotation rate** of the flipped object compared to the manual-tuning baseline in simulation (RotR \~172 vs 91 with disturbances). It essentially solved the flipping with minimal failures (TTF almost maxed out at 297+ steps) and very low torque usage, whereas the fixed baseline struggled to get the object around as quickly. Interestingly, the ablation without adaptive control *did not outperform* the manual baseline on flipping at all ‚Äì it had lower RotR (\~82 vs 91 with disturbance). This highlights that flipping (which involves impacts with a table and complex contact) is **highly sensitive to controller parameters**, and a fixed PD policy couldn‚Äôt find a one-size-fits-all gain. **DexCtrl‚Äôs adaptive gains were key to its success in flipping**, adjusting stiffness on-the-fly to handle the transitions. In summary, even in simulation without any real-world discrepancies, letting the policy co-optimize *how* to control yields better stability and faster task completion than using a fixed controller. The authors also noted DexCtrl‚Äôs training was faster to converge than massive randomization approaches, since the policy doesn‚Äôt have to brute-force learn robustness ‚Äì it gets to sense and correct for variations directly.

### Zero-Shot Transfer to Real Robots: Narrowing the Gap (Q2)

The true test of DexCtrl is zero-shot transfer from simulation to a **real dexterous hand**. The team deployed the learned policies directly onto a real LEAP hand robot, with **no additional fine-tuning**, to see how well the skills carry over and whether DexCtrl indeed narrows the sim-to-real gap. They evaluated the in-hand rotation task on **12 different real objects** spanning a range of shapes, masses, and surface frictions (including a cube, bottle, apple, yogurt cup, baseball, Rubik‚Äôs cube, etc.), to ensure generality. Each object‚Äôs performance was measured over 10 test trials with different initial poses, and averages were compared.

The **real-world results** strongly favored DexCtrl. In fact, the performance gaps that were somewhat modest in simulation became **much larger in the real world**, underscoring how important adaptive control is outside the simulator. DexCtrl significantly outperformed the manually tuned baseline on all objects, achieving higher rotation speeds and similar or longer times before failure. For instance, across objects, the baseline might achieve only \~2‚Äì3 radians of rotation on average, whereas DexCtrl was often achieving **5‚Äì15+ radians**, essentially an order-of-magnitude improvement in some cases. It also maintained near-max stability (many objects never dropped within the 300-step trial limit, TTF = 300) whereas the baseline dropped some objects sooner. Even the non-adaptive variant (Ours w/o PD) did reasonably well in real ‚Äì better than the manual baseline, showing that the learned trajectories were inherently more robust ‚Äì but **DexCtrl‚Äôs adaptive module gave it a clear extra boost** in real conditions. The authors point out that the gap between DexCtrl and its no-PD ablation was **much larger in real-world tests than it had been in sim**, emphasizing that real hardware has many small unpredictabilities (friction, slight delays, minor calibration errors, etc.) that **require continual controller adjustment**. DexCtrl was uniquely capable of handling those, whereas a fixed-gain policy, even a strong one, was inevitably less optimal for some objects or moments.

They also demonstrated the flipping task on the real hand (with a Rubik‚Äôs cube being flipped on a table) to show generalization. Visual results (as seen in Fig. 1 and the paper‚Äôs videos) indicate that DexCtrl could perform the flip in reality as well. While detailed real-world numbers for flipping weren‚Äôt given for each baseline in the text, the success of DexCtrl in flipping further validates the approach‚Äôs generalizability to tasks involving external contacts (hand-object-table interactions). The overall takeaway is that **DexCtrl dramatically narrowed the sim-to-real gap** ‚Äì policies that would normally degrade when going to real instead retained high performance, thanks to adaptive control. The manual baseline, despite careful tuning, could not match this, highlighting the limitations of one-time tuning.

### Robustness to Object Variations (Q3)

One of the promises of adaptive controller learning is improved robustness to variations in object properties, since the policy can *tune its behavior on the fly* to suit the object being manipulated. The authors designed a set of experiments to explicitly test how DexCtrl handles changes in **mass** and **friction** of the object, in a controlled way. They took a **single object shape (a cube)** and created multiple versions: for mass variation, they used a hollow cube and inserted different weights to make it light, medium, or heavy; for friction variation, they used cubes of the same weight but with different surface textures (smooth, medium, rough) to alter the friction coefficient.

The results, summarized in Table 4 of the paper, show that **DexCtrl consistently outperformed the baselines across all mass and friction conditions**. Notably, the advantage of DexCtrl was **most pronounced for the most challenging cases** ‚Äì e.g. the heavy object and the very slippery object. For the heaviest cube, DexCtrl achieved a high rotation speed and didn‚Äôt drop the object (TTF \~300), whereas the fixed-gain baseline could barely rotate it (RotR near 0.6) and tended to fail much sooner. This indicates that DexCtrl‚Äôs policy learned to **increase stiffness/force output for heavier objects** to overcome their inertia, something a fixed controller tuned for lighter objects couldn‚Äôt do effectively. Similarly, for the low-friction (smooth) cube, DexCtrl spun it the fastest (since low friction makes rotation easier) but also managed stability adequately, whereas the baseline had lower speed and still occasional drops.

Interestingly, the pattern of performance across these variations aligned with intuitive **force-based reasoning**: lighter objects (or lower friction) can be flipped or rotated more easily (so they achieve higher speeds) but are also less stable (easier to slip), whereas heavier or rougher objects move more slowly but are inherently more stable once gripped. DexCtrl was able to handle both ends of these spectrums by adapting its control gains appropriately, confirming that its adaptive strategy generalizes to new physical properties. In contrast, the manual baseline had to pick one compromise setting of stiffness/damping ‚Äì it could neither apply enough force for heavy objects, nor gentle enough control for very light/smooth objects, leading to either weak performance or instability in those extremes.

### Understanding the Learned Adaptive Strategies (Q4)

A natural question arises: *What exactly is DexCtrl‚Äôs controller module learning to do?* To demystify this, the authors analyzed the **learned stiffness profiles** under different object conditions. Since the controller outputs a stiffness and damping for each joint at each time step, we can look at how those values change for, say, a heavy object vs a light object, or a high-friction vs low-friction scenario. The paper focuses on **stiffness (K)** since it had a more pronounced effect on performance than damping in their tasks.

Two notable analyses were presented:

* **Average Stiffness vs Object Properties:** They computed the average stiffness used (across time and joints) for different object masses and frictions. In simulation, they observed a **monotonic increase of stiffness with object mass** ‚Äì heavier objects induced the policy to choose higher stiffness values on average. This matches intuition: a heavier object requires more force (higher gain) to manipulate effectively, so DexCtrl ‚Äúcranks up the strength‚Äù accordingly. For friction, the relationship was not one-directional: in one axis of rotation task, stiffness increased with more friction (perhaps because more friction means more resistance to sliding, needing stronger control), while in another case stiffness decreased with more friction (possibly because friction helps hold the object, so less stiffness was needed). The takeaway is that **DexCtrl learned a nuanced strategy** where stiffness tuning depended on the specific context of how friction was affecting the task dynamics. This reflects the complexity of dexterous manipulation ‚Äì friction can both help and hinder movement depending on the situation, and the policy adjusted different joints differently to compensate.

* **Temporal Stiffness Patterns:** They also looked at how stiffness evolved over the course of a trial for different objects (Figure 7 in the paper). To isolate the effect of the controller module, they fed the *ground-truth actions* (from a successful trajectory) into the controller network and observed what gains it outputs over time for, say, a heavy vs light object. The trends showed that for **heavier objects, DexCtrl often raised stiffness at critical moments or maintained a higher stiffness for longer durations** (essentially keeping the ‚Äúmuscles flexed‚Äù). For **smoother (low-friction) objects**, certain joints showed *opposite stiffness trends* compared to rougher objects ‚Äì in some joints the stiffness would spike for smooth objects (to prevent slip), while in others it might reduce (perhaps to avoid pushing too hard and causing loss of contact). These joint-specific adjustments illustrate that the policy isn‚Äôt simply using a single scalar gain ‚Äì it‚Äôs tuning a *vector of gains* across the hand‚Äôs joints, tailoring the force distribution to the task. This level of adaptive control is very hard to achieve with manual tuning.

Overall, this analysis confirmed the hypothesis that **DexCtrl‚Äôs controller parameters encode meaningful adjustments to contact forces**, adapting systematically to the object‚Äôs mass and friction to improve performance. In essence, the policy learned to answer questions like ‚ÄúIs this object heavy? Then use a stiffer controller to grab it firmly,‚Äù or ‚ÄúIs this surface slippery? Maybe tighten grip in some fingers but loosen in others to keep balance.‚Äù This provides an insightful peek into *why* DexCtrl outperforms the fixed strategies ‚Äì it‚Äôs because it‚Äôs actively modulating the interaction forces to suit the situation, rather than hoping one fixed setting will work everywhere.

## Effectiveness, Limitations, and Outlook

**Effectiveness:** The combination of results from simulation, real-world tasks, and controlled variation tests builds a strong case that DexCtrl is an effective solution to narrow the sim-to-real gap in dexterous manipulation. By **adaptively tuning the robot‚Äôs controller each time step**, DexCtrl achieved higher task success rates, better stability, and more energy-efficient control compared to state-of-the-art non-adaptive approaches. It essentially offloads some of the burden of sim-to-real transfer from human engineers (who previously had to fine-tune gains or exhaustively randomize environments) to the learning algorithm itself. The approach proved its worth especially in real-world evaluations: tasks that normally required careful gain tuning were accomplished **zero-shot with minimal manual effort**, and even challenging objects were manipulated reliably. In short, DexCtrl demonstrates that **learning ‚Äúhow to control‚Äù is as important as learning ‚Äúwhat to do‚Äù** for robust robot dexterity.

**Limitations:** Despite its promising results, DexCtrl is not without limitations, as the authors openly discuss:

* **Lack of Explicit Force/Tactile Sensing:** Currently, DexCtrl relies solely on **proprioceptive data** (joint angles, etc.) and inferred object information from history. The real robot experiments did not use any tactile or force sensors on the hand. This was partly due to hardware limitations (the LEAP hand might not have rich tactile sensors). As a result, the policy might not observe some subtle aspects of contact (like the onset of slip or precise contact forces) directly. Integrating **real-time force feedback or tactile sensing** could further improve DexCtrl‚Äôs ability to adapt, essentially giving it an even clearer signal for how the interaction is going. The authors suggest that incorporating high-fidelity force/tactile data is a promising direction for future work. We can imagine a future DexCtrl that *feels* an object starting to slip and immediately boosts stiffness, even faster than it could infer from joint motions alone.

* **Hardware Generalization:** The study‚Äôs real-world tests were confined to the **LEAP hand platform** ‚Äì a specific 16-DOF dexterous hand at UC Berkeley. While the tasks and objects were varied, it remains to be proven that the DexCtrl approach generalizes to other robot hands (e.g., anthropomorphic hands with different kinematics or pneumatic actuators with different dynamics). The framework itself is general (it could, in principle, be applied to any torque-controlled system with adjustable gains), but practical differences in hardware could pose new challenges. The authors plan to extend evaluations to other dexterous hands to validate generalizability.

* **Task Generality and Shared Controllers:** DexCtrl was demonstrated on two tasks. Scaling to a wider range of tasks or more unstructured scenarios might require further development. One idea the authors mention is training a **single controller adaptation module that could be shared across multiple skills**. This could be useful if you want one system to perform many tasks but still benefit from adaptive control ‚Äì perhaps by fine-tuning gains appropriate for each task context.

* **Simulation Reliance:** Like most learning-based robotics approaches, DexCtrl still relies on a high-quality simulation (with diverse randomization of object properties) to learn the policy. If the simulator‚Äôs physics are very far off from reality, adaptive gains alone might not rescue performance. However, the need for less extreme domain randomization is a plus. Future work might explore if a small amount of **online learning or fine-tuning on real hardware** (e.g., a brief adaptation phase with real data) could further improve performance, especially as suggested if real force sensors are available.

In summary, **DexCtrl‚Äôs adaptive controller learning brings a new level of robustness to dexterous manipulation**, but incorporating richer sensing and testing its generality on other platforms are important next steps to fully realize its potential.

## Conclusion

DexCtrl represents an exciting development at the intersection of reinforcement learning, adaptive control, and robotics. By giving a dexterous robot hand the ability to **tune its own controller parameters on the fly**, the approach addresses a long-standing barrier in sim-to-real transfer. The method is conceptually elegant ‚Äì use past experience to inform both what action to take and how aggressively to take it ‚Äì and it proved remarkably effective in practice. Key contributions include highlighting the often-neglected role of low-level controller differences in sim-to-real gap and providing a principled learning-based solution to tackle it. The experiments showed improved performance over manual tuning even in simulation, and a dramatic reduction of performance loss when moving to real hardware, across various objects and conditions.

For a technical audience, DexCtrl offers insight into how **policy and controller design can be interwoven**: rather than treating the controller as a fixed black box, it becomes part of the learning problem. This blurs the line between high-level planning and low-level control, potentially leading to more integrated and resilient robotic systems. It also suggests that some challenges previously addressed by brute-force domain randomization might be better solved by model adaptation mechanisms like this, which can be more sample-efficient and targeted.

Looking forward, the idea of **adaptive controller learning** could be extended in many ways. As noted, adding direct force sensing could allow even finer-grained adaptation (imagine a robot that adjusts stiffness based on real-time tactile feedback, achieving human-like delicacy). Additionally, applying DexCtrl to more complex multi-step tasks or to legged robots and other domains could test the limits of its generality. If successful, these advances could bring us closer to robots that can reliably perform intricate tasks in unpredictable real-world environments ‚Äì a step change toward truly **robust, real-world dexterous manipulation**.

Overall, *DexCtrl: Towards Sim-to-Real Dexterity with Adaptive Controller Learning* demonstrates that bridging the sim-to-real gap is not only about better simulators or more data, but also about smarter control strategies. By **learning to adapt** at the control level, robots can achieve a new level of dexterity and reliability, which is an exciting prospect for the field of robotics and its real-world applications.

**Sources:** The analysis and results discussed are based on the paper by Zhao *et al.* (2025) and the references therein.
