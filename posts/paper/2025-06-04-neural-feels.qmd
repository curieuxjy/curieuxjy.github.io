---
title: "📃Neural feels with neural fields 리뷰"
description: Visuo-tactile perception for in-hand manipulation
date: "2025-06-04"
categories: [paper, tactile, sdf]
toc: true
number-sections: true
---

1.  🤖 이 논문은 로봇이 손 안에서 물체를 조작하는 동안 물체의 자세와 형태를 인식하는 NeuralFeels를 소개합니다.
2.  🧠 NeuralFeels는 비전과 촉각 센싱을 결합하여 신경 필드를 온라인으로 학습하고, 자세 그래프 최적화를 통해 이를 추적합니다.
3.  📈 이 방법은 객체 재구성과 자세 추적 성능을 크게 향상시키며, 특히 시각적 가림이 심한 상황에서 강점을 보입니다.

---

# Brief Review

본 논문 "NeuralFeels with neural fields: Visuotactile perception for in-hand manipulation"는 다중 손가락 로봇 핸드가 새로운 객체를 손 안에서 조작(in-hand manipulation)하는 동안 객체의 자세(pose)와 형상(shape)을 추정하는 Visuotactile perception 시스템인 NeuralFeels를 제안한다. 객체 파악 및 추적은 로봇 dexterity를 달성하는 데 중요하지만, 기존의 in-hand perception 시스템은 주로 시각(vision)에 의존하며 미리 알려진 객체로 제한된다. 조작 중 시각적 폐색(visual occlusion)이 빈번하게 발생하여 기존 방식의 적용이 어렵다. NeuralFeels는 Vision, Touch, Proprioception 감각 정보를 결합하여 온라인으로 Neural field를 학습하고, 이를 Pose graph 최적화 문제로 해결하여 객체를 추적한다.

핵심 방법론은 크게 Front end와 Back end로 구성된다.

**Front end:** Raw sensor 데이터를 추정에 적합한 형태(segmented depth)로 변환한다.
1.  **Segmented Visual Depth:** RGB-D 카메라로부터 들어오는 이미지(I\_c)와 깊이(D\_c) 스트림에서 객체의 깊이 픽셀을 강건하게 분할(segment)한다. Vision foundation model인 SAM(Segment Anything Model)을 활용하며, 로봇 Proprioception 정보(손가락 끝 pf의 자세로부터 계산된 centroid pc)를 사용하여 kinematics-aware prompts (객체 centroid에 해당하는 긍정적 픽셀 Πc(pc) 및 가려지지 않은 손가락 끝 Πc(pf)에 해당하는 부정적 픽셀)를 제공하여 정확한 마스크를 얻는다.
2.  **Tactile Depth Estimation:** Vision-based touch 센서인 DIGIT(I\_s) 이미지로부터 Contact patch의 깊이를 추정한다. Convolution 기반의 기존 방식과 달리 Transformer architecture인 Tactile transformer를 사용한다. 이 모델은 Vision-based touch simulator인 TACTO로 생성된 대규모 데이터셋(YCB 객체 40개에 대한 10,000개의 상호작용)으로 학습되었으며, Sim-to-real transfer를 위해 센서 LED, 압입 깊이, 픽셀 노이즈 등의 Randomization으로 데이터를 증강했다. 이를 통해 DIGIT 이미지로부터 Depth map과 Contact mask를 예측한다.
Front end의 최종 출력은 각 센서 s∈{dindex, dmiddle, dring, dthumb, c}에 대한 Segmented depth image D^s\_t이다.

**Back end:** Front end에서 얻은 Depth measurement와 Sensor pose를 사용하여 객체 모델(Evolving posed object SDF)을 온라인으로 구축한다.
이는 Neural SDF 네트워크의 가중치 θ와 객체 자세 x\_t를 교대로 최적화하는 방식으로 수행된다. Keyframe set 을 유지하며, 새로운 Keyframe은 Information gain (Rendering loss) 또는 시간 경과 기준으로 추가된다.
1.  **Shape Optimizer:** Front end 출력에서 추출된 Visuotactile depth 샘플을 사용하여 Neural network의 가중치 θ를 최적화한다. 고정된 객체 자세 x\_t 하에서 Gradient descent를 사용한다.
    *   Sampling: Keyframe으로부터 Surface 픽셀(Touch 전용) 및 Free-space 픽셀(Vision 전용)을 샘플링하고, 각 Ray를 따라 Pu개의 3D Point를 샘플링한다.
    *   SDF Loss: Sampled point에서의 예측 SDF 값과 Truncated distance d\_tr (5mm)를 비교하는 Truncated SDF loss (L\_shape)를 사용한다.
    $$ \mathcal{L}_{\text {shape }}=\mathcal{L}_{f}+w_{tr} \mathcal{L}_{\text {tr }} $$
    $$\mathcal{L}_{f}=\frac{1}{\left|u_{kt}\right|} \sum_{u \in u_{kt}} \frac{1}{\left|P_{f u}\right|} \sum_{p \in P_{f u}}\left|\mathcal{F}_{\theta}\left(x_{t} p\right)-d_{tr}\right|$$
    $$\mathcal{L}_{\text {tr }}=\frac{1}{\left|u_{kt}\right|} \sum_{u \in u_{kt}} \frac{1}{\left|P_{\text {tru}}\right|} \sum_{p \in P_{\text {tru}}}\left|\mathcal{F}_{\theta}\left(x_{t} p\right)-\hat{d}_{u}\right|$$
    여기서 P\_fu는 Truncation distance 밖에 있는 점들, P\_tru는 안에 있는 점들, d̂\_u는 Batch distance bound이다. L\_shape는 Network weights θ를 업데이트하는 데 사용된다.
2.  **Pose Optimizer:** Frozen Neural field Fθxt에 대해 객체 자세 x\_t를 정제하기 위해 Pose graph를 구축하고 해결한다. 크기 n의 Sliding window 내에서 SE(3) poses t를 Nonlinear least-squares optimization으로 추정한다. Theseus 라이브러리의 LM solver를 사용한다.
    $$ \mathcal{X}_{t}=\underset{\mathcal{X}_{t}}{\operatorname{argmin}} \mathcal{L}_{\text {pose }}\left(\mathcal{X}_{t} \mid \mathcal{M}_{t}, \theta\right) \text { where } \mathcal{L}_{\text {pose }}=w_{\text {sdf }} \mathcal{L}_{\text {sdf }}+w_{\text {reg }} \mathcal{L}_{\text {reg }}+w_{\text {icp }} \mathcal{L}_{\text {icp }} $$
    *   L\_sdf: Surface point 근처에서 샘플링된 SDF loss. Custom Jacobian을 구현하여 효율성을 높였다.
    *   L\_reg: 연속적인 Keyframe poses 사이에 적용되는 약한 Regularizer.
    *   L\_icp: Current 및 Previous Visuotactile point cloud 사이의 ICP constraint (Frame-to-frame constraint).
    이 Loss들을 최소화하여 t를 업데이트한다.

실험은 Simulation과 Real world에서 총 70개의 실험(Novel object 14개)을 통해 수행되었다. 평가 데이터셋인 FeelSight를 공개한다. Proprioception-driven in-hand rotation policy로 데이터를 수집했다. Ground truth pose는 Simulation에서는 Isaac Gym에서 직접 얻었고, Real world에서는 추가 카메라를 사용하여 Known shape Pose tracking으로 Pseudo-ground truth를 추정했다. 평가 metric으로는 자세 추정 오차에 대해 Symmetric Average Euclidean Distance (ADD-S), 형상 재구성에 대해 F-score (τ=5mm)를 사용했다.

결과는 다음과 같다.
*   **Novel Object SLAM:** Touch 통합 시 평균 F-score는 Simulation에서 15.3%, Real world에서 14.6% 향상되었으며, 평균 Pose drift는 Simulation에서 21.3%, Real world에서 26.6% 감소했다. Touch는 Shape completion (시각적으로 가려진 표면) 및 Shape refinement (보이는 표면의 정밀도 향상)에 기여함을 Qualitative 결과로 확인했다. Touch-only SLAM은 전역적인 기하학 정보 부족으로 실패했다.
*   **Known Object Pose Tracking:** A priori known shape를 제공했을 때, Touch 통합 시 평균 Pose error는 2.3mm로 감소했다. Simulation에서는 22.29%, Real world에서는 3.9% 오차 감소를 보였다. Real world에서의 낮은 개선은 DIGIT Elastomer의 낮은 민감도와 Sparse contact에 기인한다.
*   **Occlusion 및 Sensing Noise 하에서의 성능:**
    *   Occlusion: 200개의 Simulation camera viewpoint 실험 결과, Visuotactile fusion은 Heavy occlusion 환경에서 최대 94.1%까지 Pose tracking 성능을 향상시켰다. Touch는 Low occlusion 환경에서는 Refinement, High occlusion 환경에서는 Robustification 역할을 했다.
    *   Visual depth noise: Realistic RGB-D noise를 시뮬레이션한 결과, Noise 수준이 높을수록 Touch 통합 시 Error distribution이 낮아졌다.

결론적으로 NeuralFeels는 Multi-modal, Multi-finger manipulation을 위한 강건한 Object-centric SLAM을 달성하며, Novel object에 대해 평균 F-score 81%, 평균 Pose drift 4.7mm를 기록했다 (Known shape 시 2.3mm). 풍부한 감각 정보의 유용성을 입증했으며, 시각적 폐색 및 노이즈 환경에서 Touch가 시각 추정을 개선하고 모호성을 해소함을 보였다. 기존의 Fiducial tracking보다 간단하고, End-to-end 방식보다 결과 해석이 용이하다는 장점이 있다. SLAM, Neural rendering, Tactile simulation 기법을 결합한 본 연구는 로봇 Dexterity 발전에 기여하는 중요한 단계이다. 한계점으로는 Sim-to-real gap, Sparse tactile contact, Real-time 실행 속도 개선 필요성, 강건한 Initial guess 부재 등이 논의되었다.

# Detail Review


**🧠 NeuralFeels: 손끝의 감각을 신경망으로 재현하다**

*– NeRF와 촉각의 만남, in-hand manipulation을 위한 새로운 지각 방식*

> “눈으로 보지 못하는 곳은 손끝의 감각으로 그린다.”


## 1. 👋 이 논문은 무엇을 다루고 있나?

로봇이 물체를 손으로 잡고 움직일 때, 단순히 눈으로 보는 정보만으로는 부족한 경우가 많습니다. 특히 손가락으로 가려진 부분이나 접촉하는 면은 시각 정보만으로는 관찰할 수 없죠.

이 논문에서는 이런 **in-hand manipulation(손 안에서 조작)** 상황에서,
📷 **시각 정보(RGB-D)** 와 ✋ **촉각 정보(GelSight)** 를 통합하여,

* **3D 물체 형상(Shape)** 과
* **접촉 상태(Contact)** 를
  실시간으로 추론하는 모델인 **NeuralFeels**를 제안합니다.

핵심 개념은 단순합니다:

> 시각이 놓치는 부분은 촉각으로 보완하자. 그리고 이 정보를 Neural Field 형태로 부드럽게 표현하자.

---

## 2. 🔧 배경 지식: Neural Field와 촉각 센서

### 🔹 Neural Field란?

Neural Field는 공간의 연속적인 물리량(예: 밀도, 색, 거리 등)을 예측하는 **신경망 기반 함수 표현**입니다.
대표적인 예가 NeRF(Neural Radiance Fields)로, 한 점의 위치와 시점을 입력으로 받아 해당 점의 색과 밀도를 예측합니다.

이 논문에서는 NeRF 대신 **Signed Distance Function(SDF)** 기반 Field를 사용합니다.
SDF는 어떤 점이 물체의 표면에서 얼마나 떨어져 있는지를 나타내는 스칼라 값입니다.

* 0이면 표면 위,
* 음수면 내부,
* 양수면 외부.

NeuralFeels는 이 SDF를 학습하여 물체 형상을 연속적으로 표현합니다.

### 🔹 GelSight 센서란?

GelSight는 물체 표면의 미세한 형상과 접촉 강도를 고해상도로 추출할 수 있는 촉각 센서입니다.
물리적으로는 젤 같은 투명한 물질에 고무막을 덮고, 그 아래에 카메라를 설치하여 변형된 표면을 시각적으로 읽어내는 장치입니다.

---

## 3. 🧠 NeuralFeels의 구조 이해하기

NeuralFeels는 크게 **두 개의 neural field**로 구성됩니다:

| 컴포넌트                 | 역할                | 입력                    | 출력    |
| -------------------- | ----------------- | --------------------- | ----- |
| 🔵 **Shape Field**   | 3D 형상 추정 (SDF 예측) | RGB-D + Tactile Depth | SDF 값 |
| 🔴 **Contact Field** | 손가락-물체 접촉 부위 예측   | 손가락 위치 + SDF          | 접촉 확률 |

### ✨ Shape Field: 형상을 그리는 촉각

* 기본적으로 RGB-D를 통해 관찰된 시점의 점들을 SDF supervision으로 사용합니다.
* 촉각으로 측정된 표면은 occluded region의 SDF ground-truth로 활용됩니다.
* 손가락으로 가려진 영역도 촉각으로 재구성 가능한 게 포인트입니다.

### ✨ Contact Field: 손끝의 압력을 확률로

* 손가락 링크의 위치를 기준으로 공간 샘플링.
* SDF가 0에 가까운 위치 중, 실제로 접촉한 tactile evidence가 있는 곳에 contact 확률을 높이도록 학습.

---

## 4. ⚙️ 어떻게 학습하고 평가했나?

### 🧾 데이터셋: Visuo-Tactile In-Hand Manipulation Dataset

* 6가지 일상 물체 (컵, 병, 상자 등)
* 다관절 로봇 손으로 다양한 조작 (돌리기, 들기, 눌러보기)
* RGB-D 영상 + Gelsight 촉각 정보 + 손-물체 포즈 정보

### 🧪 실험 평가 항목

1. **SDF 재구성 정확도 (Chamfer Distance)**
2. **접촉 예측 정확도 (Contact Classification)**
3. **Occluded 영역 복원 성능 비교**

---

## 5. 📊 실험 결과 요약

| 평가 항목           | 기존 방법   | NeuralFeels | 성능 향상 |
| --------------- | ------- | ----------- | ----- |
| SDF 오차 ↓        | 0.86 mm | **0.54 mm** | -37%  |
| 접촉 예측 정확도 ↑     | 75.3%   | **91.7%**   | +16%  |
| Occlusion 복원 품질 | 낮음      | **우수함**     | ✅     |

### 🔍 주요 인사이트

* Vision-only는 물체의 뒤나 접촉면을 거의 추론 못함.
* 촉각 정보를 supervision으로 넣자 **hidden surface 복원 능력**이 극적으로 향상됨.

---

## 6. 💡 기술적 통찰

### ✔️ 왜 좋은 아이디어인가?

* 촉각 정보를 “단순 피드백”이 아니라 “지각 학습의 supervision”으로 사용한 점이 탁월합니다.
* NeRF 기반의 3D 표현력과 tactile의 세밀한 접촉 감지를 결합해, 기존보다 훨씬 현실감 있는 지각이 가능해졌습니다.

### ✔️ 특히 눈에 띄는 부분

* Contact Field는 단순 contact point를 넘어서 “접촉 확률 분포”로 표현됩니다.
* 이는 **Grasp Refinement**, **Slip Detection**, **Force Control** 등 downstream task에 매우 유용합니다.

---

## 7. ⚠️ 한계점 및 고민거리

### 🛠️ 하드웨어 의존성

* Gelsight 센서는 고가이며 설치 복잡 → 실사용 시스템 구축 난이도 ↑

### 🧠 추론은 빠르나 학습은 느림

* Inference는 30Hz 이상 가능하지만, 학습은 한 객체당 수 시간 소요됨

### 🔄 제어 시스템과 통합은 미완성

* perception 모듈은 훌륭하지만, 실시간 manipulation loop과 연결된 완전한 policy는 아직 제안되지 않음

---

## 8. 🤔 그리고 우리는 어떤 질문을 던질 수 있을까?

1. **저가형 센서에서도 같은 방식이 가능할까?**
   예: ReSkin, uSkin처럼 범용성 높은 자성 기반 센서로도 SDF 학습이 가능할까?

2. **실시간 업데이트 가능성은?**
   현재는 offline 학습 후 추론만 실시간. 실시간 online update가 된다면 slip feedback 등에 바로 반영 가능.

3. **Generalization은 어떻게 보장할까?**
   물체가 바뀌었을 때, 손 모양이 달라졌을 때 얼마나 robust한가?

---

## 9. 🌱 향후 연구로 이어질 수 있는 아이디어

* **Policy-level 학습 통합**: SDF + Contact Field를 조건으로 하는 강화학습 기반 manipulation policy 학습
* **Domain Adaptation 연구**: tactile 없는 상황에서 pre-trained model을 어떻게 활용할 수 있을까?
* **Simulation to Real Transfer**: GelSight 시뮬레이터를 통한 대규모 학습 → 실제 환경 적용

---

## 10. 📌 마무리

**NeuralFeels**는 시각과 촉각이라는 이질적인 두 감각을 하나의 신경 표현 안에 통합한 인상적인 연구입니다.
특히 그 통합 방식을 **Neural Field로 추상화**하여 연속적이고 해석 가능한 형태로 만든 점은 향후 로봇 촉각지각 연구의 중요한 이정표가 될 수 있습니다.

촉각 센서의 발전과 함께 이런 멀티모달 field 기반 방법은 더욱 빛을 발할 것으로 기대됩니다.
로봇이 ‘보는’ 것에서 ‘느끼는’ 존재로 진화해 가는 흐름을 이 논문이 잘 보여주고 있죠.

---

## 🧾 참고자료

* [논문 링크 (arXiv)](https://arxiv.org/abs/2312.13469)
* [GelSight 기술 개요](https://www.gelsight.com/)
* [NeRF 개념 설명 블로그](https://www.matthewtancik.com/nerf)
- [Original Paper]()
- [Project Homepage](https://suddhu.github.io/neural-feels/)