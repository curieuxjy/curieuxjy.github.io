---
title: "📃VT-Refine 리뷰"
date: 2025-10-21
categories: [visuo-tactile, bimanual]
toc: true
number-sections: False
description: Learning Bimanual Assembly with Visuo-Tactile Feedback via Simulation Fine-Tuning
---

- [Paper Link](https://arxiv.org/abs/2510.14930)
- [Homepage](https://binghao-huang.github.io/vt_refine/)
- [Code Link](https://github.com/NVlabs/vt-refine)


1. 🤖 VT-Refine은 실제 시연, 고충실도 촉각 시뮬레이션 및 강화 학습을 결합하여 정밀하고 접촉이 많은 양손 조립 작업을 위한 시각-촉각 정책 학습 프레임워크를 제시합니다.
2. 훈련은 소량의 실제 시각-촉각 시연으로 확산 정책을 사전 학습한 후, GPU 가속 촉각 시뮬레이션 환경에서 대규모 강화 학습을 통해 정책을 미세 조정하여 sim-to-real 전이를 가능하게 합니다.
3. 📈 실험 결과, VT-Refine은 시뮬레이션과 실제 환경 모두에서 조립 성능과 강건성을 크게 향상시켰으며, 특히 촉각 피드백이 정밀도와 탐색 능력 강화에 결정적인 역할을 함을 보여주었습니다.

<center>
<img src="../../images/2025-10-21-vt-refine/111.png" width="100%" />
</center>


> VT-Refine 프레임워크 개요: VT-Refine는 실제 사람 시연으로 학습한 시각-촉각 기반 확산 정책을 시뮬레이션으로 이전하여 대규모 강화학습으로 세밀하게 개선한 후 다시 현실에 적용하는 실세계-시뮬레이션-실세계(real-to-sim-toreal) 양팔 조립 학습 기법이다. 위 그림에서 보이듯, Stage 1에서는 사람 원격조작으로 수집한 약 30개의 시연 데이터를 활용해 시각 및 촉각 입력이 포함된 확산 정책(diffusion policy)을 사전 학습하고, Stage 2에서는 해당 정책을 가상 환경의 디지털 트윈으로 옮겨 동등한 시각/촉각 센서 입력을 모사하면서 병렬 강화학습(RL)으로 정책을 파인튜닝한다. 이렇게 강화학습으로 성능을 끌어올린 정책은 다시 실제 로봇으로 옮겨져 정교한 양팔 조립 작업을 수행하게 된다.

---

# Brief Review

VT-Refine는 시뮬레이션 미세 조정(Simulation Fine-Tuning)을 통해 Visuo-Tactile(시각-촉각) 피드백 기반의 양손 조립 작업을 학습하기 위한 프레임워크입니다. 이 연구는 사람이 촉각 피드백에 적응하며 양손 조립 작업을 수행하는 능력은 뛰어나지만, 로봇에게 행동 복제(Behavioral Cloning)만으로는 불가능하다는 문제에 주목합니다. 이는 사람 시연의 비최적성(suboptimality)과 제한된 다양성 때문입니다.

VT-Refine은 정밀하고 접촉이 많은 양손 조립 작업을 해결하기 위해 실제 시연, 고충실도 촉각 시뮬레이션, 그리고 강화 학습(Reinforcement Learning, RL)을 결합합니다. 핵심 방법론은 두 단계로 구성됩니다:

1.  **실제 세계 사전 학습(Real-World Pre-Training)**:
    *   소량의 실제 세계 시연(예: 30 에피소드)을 사용하여 Visuo-Tactile Diffusion Policy를 사전 학습합니다.
    *   이 시연 데이터는 동기화된 시각 및 촉각 입력을 포함합니다. 시각 입력은 로봇의 Ego-centric 카메라에서 얻은 컬러 없는 포인트 클라우드 $P_{\text{visual},t} \in \mathbb{R}^{N_{\text{vis}} \times 4}$이며, 촉각 입력은 센서 유닛의 3D 위치와 연속적인 센서 값을 나타내는 포인트 클라우드 $P_{\text{tactile},t} \in \mathbb{R}^{N_{\text{tac}} \times 4}$입니다.
    *   $N_{\text{tac}} = 384 \times N_{\text{finger}}$로 설정되는데, 이는 각 센서 패드가 $12 \times 32 = 384$개의 촉각 포인트를 가지기 때문입니다.
    *   로봇 고유 수용 감각(Proprioception) 정보(두 팔과 두 그리퍼의 조인트 위치)도 사용됩니다.
    *   시각 및 촉각 포인트 클라우드는 PointNet 인코더($\text{PointNetEncoderXYZTactile}$)에 의해 처리되며, 그 출력은 다층 퍼셉트론(MLP)으로 인코딩된 고유 수용 감각 특징과 연결되어 노이즈 제거 확산 네트워크(denoising diffusion network)의 컨디셔닝 입력으로 사용됩니다.
    *   사전 학습된 모델은 Diffusion Policy [1]의 행동 청크 예측 방식(action chunk prediction)을 따르며, 제한된 시연 데이터로 인해 작업 성공률이 높지 않을 수 있지만, 미세 조정 단계에서 RL을 위한 강력한 사전 지식(prior)을 제공합니다.

2.  **시뮬레이션 미세 조정(Simulation Fine-Tuning)**:
    *   사전 학습된 Diffusion Policy는 시뮬레이션 환경의 디지털 트윈으로 전이됩니다.
    *   여기서 강화 학습을 통해 정책을 추가로 미세 조정하여 강건성(robustness)과 일반화(generalization) 능력을 향상시킵니다.
    *   미세 조정은 Diffusion Policy Policy Optimization (DPPO) [6] 방법을 사용하며, 노이즈 제거 과정을 마르코프 결정 과정(MDP)으로 형식화하여 보상 신호가 노이즈 제거 체인을 통해 효과적으로 전파되도록 합니다.
    *   액터(Actor) 네트워크는 사전 학습된 가중치로 초기화되고, 크리틱(Critic) 네트워크는 무작위로 초기화됩니다. 크리틱은 로봇 및 객체 상태의 저차원 표현을 받는 비대칭 액터-크리틱 전략(Asymmetric Actor-Critic strategy) [53]을 채택합니다.
    *   보상 함수는 희소 보상(Sparse Reward)으로 설정됩니다. 부품이 성공적으로 조립되면 1의 보상을 받고, 그렇지 않으면 0의 보상을 받습니다. 사전 학습이 RL 탐색을 안내하는 강력한 사전 지식을 제공하므로 복잡한 보상 설계는 피할 수 있습니다.

VT-Refine은 정확한 Sim-to-Real 전이를 가능하게 하기 위해 GPU 가속 시뮬레이션을 사용하여 압저항식(piezoresistive) 촉각 센서를 사실적으로 모델링합니다. 이 센서("FlexiTac")는 12x32 센서 유닛 매트릭스로 구성되어 2mm 공간 해상도로 법선 방향 힘(normal force) 신호를 제공합니다. 촉각 시뮬레이션은 TacSL [12] 라이브러리를 기반으로 하며, 각 촉각 포인트와 강체 객체 간의 접촉을 Kelvin-Voigt 모델로 시뮬레이션합니다. 이는 선형 스프링과 점성 댐퍼가 병렬로 연결된 형태로, 접촉 법선 힘 $$f_n = -(k_n d + k_d \dot{d})n$$를 계산합니다. 여기서 $d$는 침투 깊이(interpenetration depth), $\dot{d}$는 접촉 법선을 따른 상대 속도, $n$은 외향 접촉 법선 벡터이며, $k_n$과 $k_d$는 각각 탄성률(compliance stiffness)과 점성 계수(damping coefficient)입니다. 시뮬레이션과 실제 센서 간의 응답을 일치시키기 위해 센서 캘리브레이션 절차를 거치며, 이는 시뮬레이션과 실제 데이터 간의 히스토그램 분포를 비교하여 검증됩니다.

이 프레임워크는 포인트 클라우드 기반 표현을 사용하여 시뮬레이션과 실제 환경 간의 견고한 전이를 가능하게 하며, 이는 시각 및 촉각 양식의 공간 관계를 보존하여 정책 효과를 높입니다. 실험 결과에 따르면, VT-Refine은 데이터 다양성을 높이고 보다 효과적인 정책 미세 조정을 통해 시뮬레이션과 실제 환경 모두에서 조립 성능을 향상시키는 것으로 나타났습니다. 특히, 사전 학습만으로는 달성하기 어려웠던 미세한 조정 동작(예: "wiggle-and-dock" maneuvers)이 RL 미세 조정을 통해 자연스럽게 나타나 정책의 성공률을 크게 높였습니다.

# Detail Review


## 기술적 기여
VT-Refine 연구의 핵심 기여는 정밀한 양팔 조립 작업을 위한 시각-촉각 통합 정책 학습 프레임워크를 제시한 것이다. 저자들은 세 가지 주요 기술적 혁신을 강조한다:

1. 시연 기반 확산 정책 + 강화학습 파인튜닝: 소량의 사람 시연으로 학습된 시각-촉각 확산 정책을 시뮬레이션에서의 강화학습 기반 미세조정으로 향상시켰다. 이는 기존 행동모방만으로는 부족했던 정책의 탐색 능력을 높여, 시연 데이터 주변의 상태-행동 공간을 적극 탐험하고 성능을 개선할 수 있게 한다. 다시 말해, 행동모방(BC) 단계에서 학습한 정책이 기본기를 제공하면, 이후 RL 단계에서 그 주변 영역을 탐색 및 최적화하여 사람 데모에 없던 미세 동작까지 습득하도록 만든 것이다.
2. 고충실도 촉각 시뮬레이션 모듈: GPU 가속 물리 시뮬레이터 내에 실제 촉각센서를 충실히 모사하는 병렬 촉각 시뮬레이션 모듈을 개발했다. 특히 피에조저항 기반 촉각 센서의 정규력(normal force) 신호를 정확히 재현하도록 함으로써, 시뮬레이션과 현실 간 촉각 정보의 차이를 크게 줄였다. 기존의 광학식 촉각센서가 조명 조건과 복잡한 질감 재현 문제로 시뮬레이션이 까다로운 반면, VT-Refine는 구조적 접촉 패턴과 정규력 분포에 초점을 맞춘 촉각센서를 선택하여 시뮬레이션 용이성과 이식성을 높였다. 그 결과 촉각 정보의 실-가상 전환 간 격차(sim-to-real gap)를 크게 좁혀, 대규모 시뮬레이션 데이터를 정책 학습에 활용할 수 있었다.
3. 시각-촉각 포인트 기반 표현 및 통합: 시뮬레이션과 현실을 자유롭게 오갈 수 있는 통합 상태 표현으로 포인트클라우드(point cloud) 기반 표현을 도입했다. 구체적으로, 카메라로 얻은 시각 정보를 3차원 점들로 표현하고, 로봇 손가락에 분포된 촉각 값들도 해당 위치의 점들로 변환하여, 두 정보를 하나의 점 구름으로 결합한다. 이렇게 하면 시각과 촉각의 공간적 관계가 보존되어, 동일한 인코더(예: PointNet)를 통해 일관되게 처리할 수 있고, 시뮬레이터와 현실 로봇 간에 동일한 형태의 입력으로 정책을 전이할 수 있다. 이러한 통합 멀티모달 표현은 양손 조작 정책의 양방향 이식성을 높여, 현실→시뮬레이터 사전학습 및 시뮬레이터→현실 파인튜닝 결과의 손실을 최소화하는 데 핵심적인 역할을 했다. 이상의 기술적 기여를 통해 VT-Refine는 높은 정밀도와 접촉이 풍부한 양팔 조립 문제에서 기존 방식들이 갖고 있던 한계를 효과적으로 돌파하였다. 특히 사람의 촉각 활용 능력을 로봇에 이식하기 위해 실제 데이터의 한계를 시뮬레이션 데이터로 보완하는 전략과, 학습 표현 및 센서 디자인까지 아우르는 총체적 접근을 선보인 점이 돋보인다.

## 방법론의 참신성

VT-Refine의 방법론은 시각-촉각 통합 정책을 시뮬레이션을 통해 학습 및 향상시키는 새로운 접근을 제시한다. 구체적으로, 이 방법론은 다음과 같은 참신한 구성 요소들로 이루어진다.

- **확산 모델 기반 정책 학습**: 초기 정책은 인공지능 생성 모델의 일종인 DDPM(Denoising Diffusion Probabilistic Model)을 이용해 학습된다. 연구진은 30회 정도의 사람 텔레오퍼레이션 시연을 모아 로봇의 시각(카메라 point cloud)과 촉각(센서 point cloud) 및 관절 상태(proprioception)를 입력으로, 다
단계 디퓨전 과정을 통해 미래 $H=16$ 스텝의 행동 시퀀스를 생성하는 정책 $\pi: O \rightarrow A$를 훈련시켰다. 확산 모델은 주어진 관측에서 노이즈를 점차 제거하며 최적 행동을 출력하도록 학습되는데, 이는 제한된 시연 데이터로도 다양한 행동 분포를 일반화하는 능력이 있어 최근 로봇 제어에 도입되고 있다. VTRefine는 이러한 확산 정책을 양팔 조작에 적용하여, 사람 시연의 분포를 학習하면서도 일정 수준의 탐색적 행동을 내재한 초기 정책을 얻는다.
- **고해상도 촉각 센서 설계 및 시뮬레이션**: VT-Refine의 또 다른 혁신은 현실과 시뮬레이션 모두에서 사용 가능한 맞춤형 촉각 센서(FlexiTac)를 설계한 것이다. 각 로봇 손가락에는 $12\times32$ 배열의 촉각 센서 패드가 부착되는데, 인접 센서 간 간격이 2mm인 고해상도 촉각 매트릭스이다. 이 패드는 유연한 인쇄회로 기판
(FPC) 두 장 사이에 압력 감응 필름(피에조 저항층)을 끼운 3중 레이어 구조로 제작되어, 힘을 가하면 해당 지점의 저항이 변해 정규력 분포를 읽어낼 수 있다 【30†】. 이러한 설계는 얇고 유연하여 로봇 그리퍼의 곡면에도 부착 가능하며, 무엇보다 광학식 촉각센서 대비 시뮬레이션이 용이하다는 장점이 있다. 광학 센서는 미세한 질감과 전단력까지 감지하지만 시뮬레이터에서 이를 정확히 재현하기 어렵다. 반면 FlexiTac 센서는 표면 접촉 패턴을 정규력 값들의 행렬로 제공하므로, 시뮬레이터 상에서 물체와 센서 격자 간 충돌 깊이를 계산해 대응하는 힘 신호를 생성하는 방식으로 비교적 정확히 모사할 수 있다. 본 논문에서는 Nvidia Isaac Gym 기반의 TacSL 라이브러리를 이용해 이러한 탄성 접촉 모델을 병렬 GPU 시뮬레이션으로 구현하였다. 그 결과 현실에서의 촉각 분포와 시뮬레이션 생성 촉각 분포가 통계적으로 유사하게 일치함을 확인하였고, 이를 통해 대규모 가상 촉각 데이터를 안심하고 생성하여 정책 강화학습에 활용할 수 있었다.


<center>
<img src="../../images/2025-10-21-vt-refine/00.png" width="80%" />
</center>


> FlexiTac 촉각 센서 설계: VT-Refine에서 사용한 유연한 촉각센서(FlexiTac)의 구조를 나타낸 그림이다. 얇은 상/하부 FPC층(0.2mm) 사이에 압력 감응 필름(0.1mm)이 놓여 있으며, 센서 패드 전체 크기는 약 가로 6.4cm, 세로 2.5cm이다. 이러한 센서 4개가 로봇 양팔 그리퍼의 손가락들에 부착되어, 접촉 시 각 지점의 정규력 변화를 384개 채널의 실시간 신호로 획득한다. 해당 신호는 시뮬레이터에서도 동일한 분해능으로 생성되며, 후처리를 통해 촉각 pointcloud 형태로 변환되어 시각 정보와 결합된다.

- **강화학습을 통한 정책 파인튜닝**: 시뮬레이터 내 디지털 트윈 환경에서는 상기 학습된 확산 정책을 초기화 값으로 활용하여, 정책경사 기반의 강화학습(RL)으로 추가 훈련을 진행한다. 논문에서는 확산 정책의 시간전개 과정을 MDP로 해석하여 DPPO(Diffusion Policy Policy Optimization) 알고리즘을 제안하는데, 간단히 말해 배치형 병렬 환경에서 다수의 로봇이 동시에 조립 작업을 시도하면서 성공 여부(완성 시 보상 1)를 가지고 정책망을 업데이트하는 방식이다. 이때 배우(Actor) 신경망은 사전학습된 확산 정책 가중치를 이어받고, 비대칭 학습을 위해 비평가(Critic)망에는 로봇 상태의 저차원 정보(물체 위치 등)가 주어져 효율적으로 가치를 평가한다. Sparse한 성공/실패 보상에도 불구하고, 이미 어느 정도 시연 데이터로 학습된 정책을 출발점으로 삼기에 학습이 원활하며, 수십만 회의 가상 조립 시도로 미세 조정된 정책을 얻을 수 있었다. 이 대규모 병렬 RL 파인튜닝 과정은 시연 데이터만으로 학습한 정책이 해내지 못했던 마지막 수 밀리미터의 정밀 조립 동작을 자동으로 터득하게 만드는 핵심 단계이다.
- **실세계 복귀 및 폐루프 개선**: 강화학습으로 향상된 정책은 최종적으로 다시 실제 로봇에 탑재되어 테스트된다. 이때 시뮬레이터와 현실의 차이(예: 깊이 카메라 pointcloud 잡음, 로봇 제어기의 미세 오차 등)로 성능 저하가 일부 발생할 수 있지만, VT-Refine에서는 촉각센서의 낮은 도메인 격차(low-gap tactile modality)와 포인트 기반 표현 덕분에 실→가상 전환 시 성공률 5~10% 하락, 가상→실 전환 시 0~5% 미만의 경미한 성능 저하만 관찰되었다. 반면 시뮬레이션 상 RL 파인튜닝으로 얻은 성공률 향상이 30%포인트 이상에 달했기 때문에, 이러한 전이 손실은 전체 성능 개선 효과에 비해 무시할 수준이었다고 보고된다. 요약하면, VT-Refine의 방법
론은 실세계 소량 학습 → 가상환경 대량 강화 → 실세계 검증의 순환 고리를 통해, 현실 데이터 획득의 비용과 위험을 줄이면서도 최종 성능을 극대화하는 독창적인 방식이라 할 수 있다.

## 실험 설계 및 결과 분석

이 논문에서는 5개의 난이도 높은 양팔 조립 작업을 선정하여 제안한 방법의 성능을 검증하였다. 실험에 사용된 작업들은 AutoMate 데이터셋에 포함된 과제들로서, 각기 다른 형상의 플러그-소켓 쌍을 양손으로 집어서 공중에서 결합(in-air insertion)해야 하는 문제들이다. 예를 들어 육각 기둥 모양 너트-볼트 결합, 카메라 렌즈 마운트와 유사한 베요넷 결합, 원통형 축을 슬리브에 끼우기, 전기 커넥터 삽입 등 다양한 형태의 정밀 접합 작업이 포함되었다. 이러한 작업들은 결합 시 시야가 가려지고 미세한 오차에도 조립이 불발되기 쉬워, 시각 정보만으로는 성공하기 어렵고 풍부한 촉각 활용이 필수적인 과제들이다.

**실험 환경 구성**: 연구진은 서로 다른 두 로봇 플랫폼에서 실험을 수행하여 제안 기법의 범용성도 평가하였다. 하나는 책상 위에 6자유도 로봇팔 2대를 배치한 테이블탑 양팔 로봇으로, 소형 WidowX 암과 소프트 그리퍼를 사용했다. 다른 하나는 사람 상반신을 모사한 세미-휴머노이드 양팔 로봇으로, 7자유도 Kinova Gen3 두 대와 Robotiq
2F-140 그리퍼를 사용한 보다 크고 무거운 시스템이다. 두 플랫폼 모두 각 그리퍼의 손가락 4개 면에 FlexiTac 촉각센서가 부착되었고, 인텔 RealSense D455 깊이 카메라로부터 로봇 중심 시점의 점군 시각정보를 입력받았다. 사람 시연 데이터는 Meta Quest 2 VR 장치를 이용한 원격 조작으로 수집되었는데, 사람이 가상현실 핸드컨트롤러로 물체를 잡고 끼우는 동작을 하면 로봇 팔이 이를 모방하도록 제어하는 방식이다. 이렇게 과제당 30회의 시연을 모아 앞서 설명한 Stage 1 확산 정책 학습을 진행한 후, 동일한 과제를 모사한 시뮬레이터 환경에서 Stage 2 RL 파인튜닝을 거쳐 정책을 최종 완성시켰다. 시뮬레이터에서는 매 에폭마다 수백 대의 병렬 로봇이 작업을 반복시도하면서 학습이 이루어져, 현실에서는 불가능한 대량의 시행착오를 안전하게 경험하게 했다. 마지막으로 이렇게 얻어진 정책을 두 실제 로봇 플랫폼에 이식하여 성공률(success rate) 및 조립 정밀도를 측정하였다. 각 작업마다 수십 회의 시도를 통해 성공 여부를 기록하고, 방법 간 성능을 비교하였다.

**비교 대상 및 평가 지표**: 검증을 위해 네 가지 방식의 정책이 비교되었다. (a) Pre-Train 전용: 사람 시연만으로 학습된 확산 정책(파인튜닝 없이 바로 실행). (b) Fine-Tune (w/ Pre-Train): 시연으로 학습된 정책을 초기화로 하여 시뮬레이터에서 RL 파인튜닝까지 거친 최종 정책(제안 방법). (c) Fine-Tune (w/o Pre-Train): 시연 데이터 없이 처음부터 RL로만 학습한 정책. (d) 시각 전용 정책: 위의 (a)와 (b)에 대하여 촉각 입력을 사용하지 않고 카메라 영상만으로 학습시킨 대응 실험. 이들은 곧 기존의 Vision-only BC 및 Vision-only RL 방법에 해당한다. 모든 정책에 대해 조립 성공 여부를 1/0의 보상으로 정의하여 성공률을 측정했고, 추가로 조립이 거의 완료되었으나 마지막 수 밀리미터 간격을 좁히지 못한 경우 등을 분석하여 정책의 정밀도를 평가하였다.

**실험 결과**: 전반적인 결과는 제안한 VT-Refine 방법의 뚜렷한 우수성을 보여주었다.

- 첫째, RL 파인튜닝의 효과가 두드러졌는데, 시뮬레이션 상의 강화학습을 거친 정책은 거치지 않은 정책에 비해 현실 성공률이 큰 폭으로 향상되었다. 예를 들어 테이블탑 양팔로봇 실험에서, 촉각을 포함한 정책의 경우 시연만으로 달성한 성공률이 과제에 따라 12~54% 수준이었으나, RL 파인튜닝 후에는 76~98%까지 상승하였다. 향상 폭은 과제에 따라 +30%p 이상에 달했으며, 특히 결합 간격이 매우 촘촘한 고정밀 작업일수록 파인튜닝 전후 성능 격차가 컸다. 흥미롭게도, 시연 데이터 없이 처음부터 RL로 학습한 정책은 어떤 경우도 성공적인 조립을 해내지 못해 성공률 0%에 머물렀는데, 이는 초기 시연을 통해 학습을 안정화하는 단계가 필수적임을 방증한다. 다시 말해 사람 데모의 기본기가 없다면, 복잡한 양팔 조립을 순전히 강화학습만으로는 탐색하지 못한다는 것이다.
- 둘째, 촉각 정보의 기여가 정량적으로 확인되었다. 촉각을 포함한 시각-촉각 정책은 촉각을 배제한 시각 전용 정책에 비해 일관되게 높은 성능을 보였다. 예컨대 시각 전용 정책은 플러그와 소켓의 미세한 접촉 단서들을 인지하지 못해 두 물체를 맞물리는 과정에서 공중에서 머뭇거리거나(hovering) 각도 불일치로 삽입을 시도하다 실패하는 경우가 많았다. 반면 촉각 기반 정책은 결합부에 닿는 순간의 힘 변화를 감지해 즉각적으로 미세 위치를 조정함으로써 끝까지 결합을 완료하는 비율이 높았다. 시뮬레이터 평가에서 촉각 정책은 초기 학습단계부터 시각 전용보다 높은 성공률로 시작해 최종에도 더 높은 정확도로 수렴하였는데, 이는 촉각이 초기 학습 가이드와 최종 미세 조정 모두에도움을 주었음을 시사한다. 요약하자면, “촉각을 사용한 우리 정책은 시작도 끝도 더 정밀했다”는 것이 저자들의 설명이다.
- 셋째, 세부 성능 분석 및 내성 평가도 설득력 있게 제시되었다. 시연 데이터의 양을 달리해본 실험에서, 10개 시연으로 학습한 정책은 거의 성공률 0%에 그쳤으나 RL 파인튜닝 후 약 30%까지 향상되었고, 30개 vs 50개 시연으로 학습한 정책은 둘 다 파인튜닝 후 거의 완벽에 가까운 성공률을 보였다. 시연을 30개에서 50개로 늘리는 것은 초기 성공률에 약간 기여했지만 결정적이지 않았으며, 결국 병목은 충분한 데모 수집보다 RL을 통한 미세동작 학습에 있었다고 분석된다. 또한 정책의 강인성(robustness) 측면에서, 저자들은 임의의 초기 물체 배치 변화(약 ±3cm 범위)나 잡는 과정에서의 미끄러짐 등이 발생해도 촉각 기반 파인튜닝 정책은 자연스럽게 “물체를 흔들며 다시 맞추는(wiggleand-dock)” 동작을 수행하며 성공에 이르는 것을 관찰했다. 이러한 동작은 사람이 촉각으로 삽입을 시도할때 보이는 전략과 유사하며, 초기 데모에는 없었던 것이 RL 단계를 통해 자발적으로 학습된 행동 패턴이었다고 한다. 반면 촉각이 없는 정책은 삽입이 어긋났을 때 무작정 힘을 줘 밀다 실패하거나 물체를 떨어뜨리는 등 섬세한 대응을 하지 못했다. 끝으로, 두 로봇 플랫폼 간 성능 비교에서는 대체로 일관된 향상 추이가 나타났으나, 테이블탑 소형 로봇에 비해 Kinova 기반 큰 로봇에서는 절대 성공률이 다소 낮았다. 이는 큰 로봇의 제어 미세 정확도 한계나 카메라 시야 등의 차이로 인한 것이며, 그럼에도 촉각 기반 파인튜닝으로 얻은 개선율은 두 플랫폼에서 유사하여 제안 기법의 일반성을 입증하였다.

종합하면, 실험 설계는 적절한 난이도의 과제를 선별하고, 다양한 통제 실험(유/무 촉각, 유/무 파인튜닝, 데모 수량 변화, 로봇 플랫폼 변화 등)을 통해 제안 방법의 효과를 다각도로 입증하였다. 성공률과 동작 패턴에 대한 정량/정성 분석이 모두 제시되어 설득력을 높였으며, 특히 “촉각을 사용한 시뮬레이션 파인튜닝이 없다면 불가능했을 조립을 가능하게 한다”는 주장을 실험 데이터로 뒷받침한 점이 돋보인다.


## 결론

VT-Refine는 시각과 촉각을 결합한 양팔 로봇 조작 분야에서 한 단계 도약을 이룬 연구로 평가된다. 이 논문은 사람이 손끝의 감각으로 수행하는 정밀 조립을 로봇이 학습하도록 하기 위해, 실제-가상 환경을 왕복하는 새로운 학습 사이클을 제안했다. 소수의 실제 시연으로 학습된 정책을 대규모 가상 데이터로 강화하여 현실에 다시 적용하는 이 접근법은, 데이터 부족과 시뮬레이터 간 격차라는 두 난제를 모두 해결한 점에서 의의가 크다. 또한 촉각센서 하드웨어 개발부터 표현 통합, 알고리즘 설계, 실험 검증까지 전 스택(full-stack)을 아우르는 통합 연구로서, 향후 유사한 멀티모달 로봇 학습에 좋은 청사진을 제시한다. 물론 완벽한 수준은 아니어서, 현재 방법도 일부 실패 케이스(예: 특정 각도로 삽입이 어려운 경우 등)가 존재하며 여전히 사전 시연 데이터에 성능이 의존하는 한계가 있다. 그러나 저자들이 공개한 FlexiTac 센서와 시뮬레이션 모듈은 관련 연구 커뮤니티에 활용 가치가 높고, 향후 시연 없이도 자체 탐색으로 학습하는 자율성이나 더 복잡한 조립 시나리오로의 확장 등 도전 과제들이 남아 있다. 그럼에도 결론적으로, VT-Refine는 “촉각이 있어야 마지막 2mm를 해낼 수 있다”는 교훈을 로봇 학습에 명확히 각인시킨 성공적인 사례로 남을 것이다.
