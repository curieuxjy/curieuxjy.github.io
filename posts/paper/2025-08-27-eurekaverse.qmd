---
title: "📃Eurekaverse 리뷰"
date: 2025-08-27
categories: [eureka, llm, curriculum]
toc: true
number-sections: false
description: Environment Curriculum Generation via Large Language Models
---

- [Paper Link](https://arxiv.org/abs/2411.01775)
- [Project Link](https://eureka-research.github.io/eurekaverse/)
- [Code Link](https://github.com/eureka-research/eurekaverse)


1. 🤖 Eurekaverse는 Large Language Model(LLM)을 활용하여 로봇 기술 훈련을 위한 점진적으로 어려운 환경을 자동으로 생성하는 비지도 환경 설계 알고리즘입니다.
2. ⚙️ 이 알고리즘은 LLM이 코드를 통해 환경을 생성하고, 로봇의 훈련 진행 상황에 맞춰 환경 난이도를 적응적으로 진화시키는 에이전트-환경 공동 진화(agent-environment co-evolution) 방식을 사용합니다.
3. 🏆 사족보행 로봇 파쿠르(parkour) 학습에 적용한 결과, Eurekaverse는 시뮬레이션 및 실제 환경 모두에서 수동으로 설계된 훈련 코스를 능가하는 성능을 보이며 뛰어난 일반화 능력을 입증했습니다.

---

# Brief Review

로봇에게 복잡한 기술을 가르치기 위한 유망한 전략은 점진적으로 난이도가 높아지는 환경 커리큘럼에서 로봇을 훈련시키는 것입니다. 하지만 효과적인 환경 커리큘럼을 개발하는 것은 상당한 전문 지식을 요구하며, 새로운 도메인마다 반복되어야 하는 번거로움이 있습니다. 이 논문은 환경이 종종 코드로 자연스럽게 표현될 수 있다는 핵심 통찰력을 바탕으로, 대규모 언어 모델(LLM)의 코드 생성 능력을 활용하여 효과적인 환경 커리큘럼 설계를 달성하고 자동화할 수 있는지 탐구합니다.

이러한 맥락에서, 본 논문은 사족보행 로봇 파쿠르 학습 도메인에서 `Eurekaverse`라는 비지도 환경 설계 알고리즘을 소개합니다. `Eurekaverse`는 LLM을 활용하여 로봇 기술 훈련을 위한 점진적으로 더 도전적이고, 다양하며, 학습 가능한 환경을 샘플링합니다. `Eurekaverse`가 자동으로 설계한 커리큘럼은 시뮬레이션에서 복잡한 파쿠르 기술의 점진적인 학습을 가능하게 하며, 사람이 수동으로 설계한 훈련 코스보다 뛰어난 성능으로 실제 세계에 성공적으로 전이될 수 있음을 보여줍니다.

<center>
<img src="../../images/2025-08-27-eurekaverse/2.png" width="100%" />
</center>


**핵심 방법론: 에이전트와 환경의 공동 진화 (Agent-Environment Co-Evolution)**

`Eurekaverse`는 크게 두 단계로 진행됩니다. 첫째, LLM을 사용하여 훈련을 위한 초기 환경 세트를 생성합니다. 둘째, 에이전트와 환경의 공동 진화(co-evolution)라고 불리는 과정을 사용하여 현재 환경 세트에서 에이전트를 훈련하고, 이 정보를 바탕으로 LLM이 환경 세트를 업데이트하여 에이전트의 현재 역량에 계속 도전하면서도 학습하기에 너무 어렵지 않도록 조정합니다.

1.  **초기 환경 생성 (Initial Environment Generation):**
    *   LLM(본 논문에서는 GPT-4o 사용)에게 태스크 설명과 단일 In-context 예시 환경 프로그램(Python 함수 형태)을 제공하여 초기 환경 세트를 쿼리합니다.
    *   LLM은 `height_field` (지면의 높이를 정의하는 2D 그리드)와 `goals` (목표 좌표 목록)를 포함하는 환경 프로그램 $\theta$를 Python 코드로 응답합니다.
    *   다양한 훈련 환경 세트를 얻기 위해, LLM은 0이 아닌 `temperature`로 여러 번 샘플링됩니다:
        $$\theta_j \sim \Lambda_{\text{init}}^{\text{LLM}}(\theta_{\text{incontext}})$$
        여기서 $\theta_{\text{incontext}}$는 In-context 예시 환경 프로그램입니다.
    *   생성된 프로그램은 최대 높이 및 최대 높이 차이 임계값과 같은 간단한 유효성 검사를 거쳐 실행 가능하고 타당한 지형을 보장합니다.

2.  **에이전트와 환경의 공동 진화 (Co-Evolution of Agents and Environments):**
    *   초기 환경은 LLM이 에이전트의 능력과 한계를 알지 못하므로 훈련에 비효율적일 수 있습니다. 또한, 이들은 독립적으로 동일하게 샘플링되므로 점진적으로 도전적인 커리큘럼을 형성하기 어렵습니다. 이 문제를 해결하기 위해, `Eurekaverse`는 정책 진화와 환경 진화 단계를 번갈아 수행합니다.
    *   **정책 진화 (Policy Evolution):**
        *   각 반복의 시작에서, $N$개의 RL 에이전트 $\{\pi_i\}_{i=1}^N$ 집합이 각각 $J$개의 환경으로 구성된 자체 훈련 라이브러리 $\{\theta_{ij}\}_{j=1}^J$에서 독립적이고 병렬적으로 훈련됩니다.
        *   모든 에이전트는 이전 반복을 포함하여 지금까지 생성된 모든 환경의 합집합인 프록시 환경 $\Theta_{\text{proxy}} = \cup_i\{\theta_{ij}\}_{j=1}^J$에 대해 평가됩니다.
        *   이 평가를 통해 가장 성능이 좋은 정책 $\pi_{\text{best}}$가 선택됩니다($\pi_{\text{best}} = \arg \max_{\pi_i} F(\pi_i, \Theta_{\text{proxy}})$). 후속 반복에서는 이 $\pi_{\text{best}}$가 초기 정책으로 사용되어 정책이 점진적으로 개선되도록 합니다.
        *   견고성을 위해 "소프트 선택(soft selection)" 방식이 사용되는데, 이는 최고 성능 정책에 $p_1=0.75$, 두 번째 정책에 $p_2=0.25$의 확률로 선택될 기회를 부여하는 방식입니다.
    *   **환경 진화 (Environment Evolution):**
        *   초기 반복 후, `Eurekaverse`는 $\pi_{\text{best}}$를 훈련하는 데 사용된 환경들 $\{\theta_{\text{best},j}\}_{j=1}^J$을 진화시켜 새로운 환경을 생성합니다.
        *   이 세트의 각 $\theta_j$에 대해, LLM은 해당 환경의 변형 $\tilde{\theta}_j$를 생성하도록 요청받습니다:
            $$\tilde{\theta}_j \sim \Lambda_{\text{evol}}^{\text{LLM}}(\theta_j, \theta_{\text{incontext}})$$
        *   LLM에게는 이전 LLM 응답인 $\theta_j$와 함께 환경 통계(예: 최대 지형 높이 차이) 및 정책 훈련 통계(예: 보상 항, 성공률)가 추가 정보로 제공되어 정책의 학습 진행 상황에 맞춰 환경을 조정할 수 있게 합니다. 또한, 이전 훈련에 사용된 지형들의 설명(`docstring`)도 제공됩니다. 이 과정은 다음 반복의 정책 훈련 실행을 위한 $N$개의 새로운 훈련 라이브러리를 생성하기 위해 독립적으로 $N$번 수행됩니다.

**실험 결과:**

이 방법론은 Unitree Go1 사족보행 로봇의 파쿠르 학습에 대해 광범위하게 시뮬레이션 및 실제 환경에서 검증되었습니다.

*   **시뮬레이션:** `Eurekaverse`는 사람이 설계한 환경(`Human-Designed`)이나 무작위로 생성된 환경(`Random`)에서 훈련된 정책보다 뛰어난 성능을 보였습니다. 특히, `Human-Designed` 환경에서 훈련된 정책이 학습 곡선이 빠르게 포화되는 반면, `Eurekaverse`는 환경이 현재 최고 정책에 적응적으로 진화하기 때문에 지속적인 성능 향상을 보이며 궁극적으로 테스트 벤치마크에서 Oracle 정책(테스트 환경에서 직접 훈련된 정책)에 근접한 결과를 달성했습니다. 초기 환경만 사용하거나, 단순히 다양성만 추구하거나, 정책 피드백 없이 난이도만 높이는 `Ablation` 모델들은 모두 성능이 저조하여 적응적인 커리큘럼과 정책 피드백의 중요성을 입증했습니다.
*   **실제 환경:** `Eurekaverse`로 훈련된 정책은 실제 세계에서 상자 오르기, 간격 넘기, 경사로 오르기, 계단 오르기 등 다양한 파쿠르 태스크에서 `Human-Designed` 정책보다 일반적으로 우수한 성능을 보였으며, 더 안전하고 안정적인 동작을 나타냈습니다. 또한, 훈련 중 보지 못했던 의자나 난간과 같은 환경적 특징에도 강건했으며, 새로운 복합 장애물 코스에서도 성공적으로 탐색하는 등 탁월한 일반화 능력과 회복 동작을 시연했습니다.

**결론:**

`Eurekaverse`는 LLM을 활용한 자동화된 환경 커리큘럼 설계의 가능성을 보여줍니다. 이 방법은 정책의 현재 훈련 진행 상황에 적응하는 효과적인 환경 프로그램을 생성하여 새로운 기술을 학습하고 기존 기술을 연마할 수 있도록 합니다. 이는 궁극적으로 개방형(open-ended)이고 범용적인 로봇 에이전트를 향한 길을 열 수 있는 LLM의 잠재력을 보여줍니다. 한계점으로는 LLM 샘플링의 효율성 향상 필요성 및 시각적 피드백을 활용한 공간 추론 능력 강화 가능성이 언급되었습니다.

---

# Detail Review

> LLM으로 자동화된 환경 커리큘럼 설계: *Eurekaverse* 리뷰

로봇에게 복잡한 기술을 가르치려면, 점진적으로 어려운 장애물 코스를 준비하는 **커리큘럼 학습(curriculum learning)**이 효과적이라는 사실이 알려져 있습니다. 하지만 이러한 코스를 사람이 직접 설계하려면 도메인 전문 지식이 필요하고, 새로운 환경마다 매번 작업을 반복해야 합니다. 본 논문 *Eurekaverse: Environment Curriculum Generation via Large Language Models*는 이 문제를 대규모 언어 모델(LLM)의 **코드 생성 능력**을 활용해 해결하고자 합니다. 환경 자체를 프로그램 코드로 표현함으로써, LLM이 점점 더 어려운 학습 환경(장애물 코스)을 **자동으로 생성**할 수 있을지 탐구한 것이죠  . 저자들은 이를 통해 **Eurekaverse**라는 알고리즘을 제안합니다. Eurekaverse는 LLM에게 초기 장애물 코스를 만들어보게 한 뒤, 강화학습(RL)으로 로봇 제어 정책을 훈련하고, 그 성능에 따라 코스를 계속 진화시켜 나가는 **공진화(agent-environment co-evolution)** 방식으로 작동합니다 .

이 논문의 주요 기여는 크게 두 가지로 요약됩니다: 첫째, **LLM 기반의 자율적 환경 디자인 알고리즘(Eurekaverse)**을 소개하여 환경 커리큘럼을 자동으로 생성할 수 있도록 한 점 . 둘째, 이를 4족 보행 로봇의 파쿠르(장애물 넘기) 과제에 적용해 시뮬레이션과 실제 로봇 실험을 통해 검증한 것입니다  . 특히 Eurekaverse가 자동 생성한 학습 코스는 사람이 만든 코스보다도 더 나은 일반화 성능을 보여주었으며, 실제 로봇 실험에서도 안정적이고 빠른 적응 능력을 보였습니다.


## 사용된 방법론과 기술적 접근

Eurekaverse의 핵심은 LLM으로 환경(장애물 코스)의 **프로그램 코드**를 생성하고, 이를 강화학습과 결합해 커리큘럼을 만들어 나가는 것입니다. 4족 보행 로봇 파쿠르 과제를 예로 들면, 환경은 땅의 높이를 나타내는 2D 그리드(높이장(height field))와 로봇이 차례로 도달해야 할 **목표(goal)** 좌표들의 리스트로 구성됩니다. 이 둘은 파이썬 코드(NumPy 연산 포함)로 구현되며, LLM은 이러한 코드 형태로 코스를 생성합니다 . 예를 들어 강아지 공원이나 놀이터에서 영감을 얻어 장애물을 설계하도록 프로프트(prompts)를 구성합니다. 이때 생성된 코스는 **최대 높이 제한**, **경사 차이 제한** 등의 간단한 검증 절차를 통해 물리적으로 불가능하거나 매우 위험한 코스는 걸러냅니다 .

방법의 전체 흐름은 다음과 같이 단계별로 이루어집니다   :

1. **초기 환경 생성**: 우선 LLM에 과제 설명과 간단한 예시 코스(인컨텍스트 예제)를 제공하고, 여러 개의 초기 환경 코드를 샘플링합니다 . (예: GPT-4o를 이용하여 8개의 환경을 무작위 온도로 생성)
2. **정책 훈련**: 생성된 환경 집합에서 각 환경을 포함하는 라이브러리(train library)별로 병렬로 복수의 RL 에이전트를 훈련합니다. 각 에이전트는 PPO와 같은 알고리즘으로 학습되며, 모든 환경에서의 도달 목표 개수 등의 성과 지표를 기록합니다 .
3. **정책 평가 및 선택**: 훈련이 끝나면 전체 환경(이전까지 생성된 모든 코스)에서 에이전트들의 성능을 평가하여 **가장 잘 수행한 정책(π\*)**을 선택합니다 . 여러 개의 에이전트를 유지하는 이유는, 나쁜 환경 설계에 영향을 받지 않고 일부는 유용한 환경에서 학습할 가능성을 높이기 위함입니다 .
4. **환경 진화 (LLM 재생성)**: 선택된 최고의 정책이 훈련된 환경들(효과적인 것으로 판명된 환경)을 바탕으로, LLM에게 해당 환경을 좀 더 어렵게 변형하도록 요청합니다. 즉, 기존 환경 코드를 LLM에 다시 보내면서 로봇의 현재 성능 지표(예: 성공률, 평균 보상)와 환경 통계(최대 높이 등)를 함께 제시하여 다음 세대 환경을 생성합니다 . 이 과정을 충분히 반복하여 새로운 환경 라이브러리를 만듭니다.
5. **반복 학습**: 이렇게 생성된 새 환경에서 다시 RL 정책을 학습시키고, 위 과정을 **반복(에이전트-환경 공진화)**하여 점진적으로 난이도를 높여 갑니다  .

이 과정은 전통적인 커리큘럼 학습처럼 점진적으로 어려운 환경을 제공하지만, **커리큘럼 자체를 LLM이 자동으로 설계**한다는 점이 다릅니다 . 저자들은 “소프트 선택(soft selection)” 방식을 도입하여 매 세대마다 성능 순위에 따라 정책을 무작위로 선택하기도 했으며, 최종적으로 가장 좋은 정책을 출력합니다. LLM으로는 GPT-4o를 사용하였으며, 전체 학습은 GPU 8대에서 약 24시간, OpenAI API 비용 약 15달러가 소요되었다고 합니다 .


## 실험 설계 및 결과 평가

저자들은 대규모 시뮬레이션과 실제 로봇 실험을 통해 Eurekaverse의 성능을 평가했습니다. **시뮬레이션 학습**은 Cheng et al.[21]의 프레임워크를 기반으로, Unitree Go1 로봇(4족 보행 로봇)에 대해 높이장(height field)과 목표 좌표로 정의된 파쿠르 코스를 사용했습니다. 난이도에 따라 장애물의 크기나 간격이 조정되며, PPO로 교사 정책을 학습한 뒤 깊이 이미지 기반 학생 정책으로 증류(distillation)하여 실제 로봇에 적용합니다. 비교 대상으로는 *사람이 설계한 코스*(Human-Designed)와 *무작위 배치 코스*(Random) 등이 있습니다. 특히 사람 설계 코스는 일반적인 파쿠르 기술을 가르치도록 잘 만들어진 것이지만, 적응 학습이 빠르게 포화하는 한계가 있습니다.

시뮬레이션 결과, **환경 설계가 필수적**임이 확인되었습니다. 무작위 환경(Random)에서는 성능 향상이 거의 없었으며, 이는 장애물들이 무작위로 배치되어 로봇이 제대로 학습하지 못했기 때문입니다 . 반면 Eurekaverse는 학습 곡선이 꾸준히 상승하며 지속적인 개선을 보였습니다. 최종적으로 Eurekaverse가 만든 커리큘럼으로 학습한 정책은, 사람 설계 코스의 정책보다 평균적으로 거의 두 개의 추가 목표(goal)를 더 달성할 정도로 뛰어난 성능을 보였습니다 . 사람 설계 코스도 초반에는 빨리 학습되지만 곧 성능이 포화되는 반면, Eurekaverse는 초반 학습이 다소 느려도 환경이 정책 성능에 맞춰 지속적으로 진화하기 때문에 마지막에는 더 높은 성능을 얻었습니다 . 실제로 20개의 테스트 코스(시뮬레이션)에서 평가했을 때, Eurekaverse 학습 정책은 인간 설계 코스를 따라 학습한 정책보다 목표 달성 개수가 평균 2개가량 더 높았습니다 . 또한, “초기 환경(Initial Envs)”이나 “최종 환경(Final Envs)”처럼 환경 변화를 고려하지 않은 경우, 정책은 다양한 기술을 익히지 못하고 성능이 저하되는 것으로 나타났습니다 . 이는 단순히 어려운 환경을 늘려주는 것이 아니라, 현재 정책 수준에 맞게 조정된 **적응적 커리큘럼**이 필요함을 시사합니다 .

실제 로봇 **실험**에서도 Eurekaverse의 우수성이 드러났습니다. 네 가지 대표적인 실제 과제(박스 오르기, 간격 뛰어넘기, 경사로 오르기, 계단 넘기)에서 Eurekaverse로 학습한 정책은 거의 모든 난이도에서 사람 설계 정책보다 성공률이 높았습니다 . 예를 들어 Go1 로봇에 대해 75cm 높이의 점프, 50cm 높이의 장애물 오르기, 30도 경사로 등 까다로운 조건에서도 성공했으며, 계단 과제에서는 전체 계단을 완주했습니다. 반면 사람 설계 코스로 학습한 정책은 모터 과부하로 넘어지는 등의 불안정한 동작이 잦았고, 실제 경사로 과제에서는 로봇이 손상될 정도여서 더 이상의 실험을 진행하지 못했습니다 .

<center>
<img src="../../images/2025-08-27-eurekaverse/1.png" width="100%" />
</center>

> 실제 4족 보행 로봇 실험에서 Eurekaverse로 학습한 정책(빨간 점선)이 사람 설계 정책(파란 점선)보다 전반적으로 높은 성공률을 보였다.

더욱 인상적인 것은 **학습 과정에서 한 번도 본 적 없는 완전히 새로운 코스**에서도 Eurekaverse 학습 정책이 뛰어난 범용성을 보인 점입니다. 논문에서는 박스+점프+경사+요가볼, 좁은 런웨이+65cm갭, 연속 점프+기울어진 경사면, 장애물 투척+유동형 갭 등 4가지 새로운 코스를 구성했는데, 우리 정책은 이들 모든 장애물 코스를 성공적으로 돌파했습니다 . 이는 높은 일반화 능력을 보여주며, 시뮬레이션에서 얻은 정책이 실제 및 예상치 못한 환경에서도 **강인한 적응력**을 가진다는 것을 의미합니다 .


## 기존 연구와의 차별점 및 기여

기존 연구들에서도 로봇 학습에 커리큘럼을 적용하려는 시도는 있었습니다. 다만 대부분은 환경 변형 범위를 사람이 직접 설계하거나, 물리 파라미터 범위를 무작위로 조절하는 수준에 머물렀습니다  . 예를 들어 이전 연구들은 2D 미로 배치 같은 단순 공간에서 강화학습 에이전트와 환경 생성자를 대결시키는 방식을 고안했지만, 이것도 설계자가 환경 생성 규칙을 만들어야 했고 파쿠르처럼 복잡한 지형 전체를 다루기엔 한계가 있었습니다 . LLM을 활용한 연구들도 있긴 하지만, 주로 상위 수준의 계획이나 픽앤플레이스 작업에서 환경 샘플링 정도로 사용되었으며 , 복잡한 3D 물리 환경 전체를 점진적으로 진화시키는 예는 없었습니다. 한편 파라미터 랜덤화(simulation randomization) 연구도 있었는데, 이는 중력이나 질량 같은 물리 수치를 무작위로 선정하는 단순한 방식이라 환경 변화의 폭이 제한적입니다 .

Eurekaverse는 **두 가지 측면에서 독창적**입니다. 첫째, 환경을 *프로그램 코드*로 취급하여 LLM이 직접 코스를 생성하고 수정할 수 있도록 했습니다  . 복잡한 장애물의 기하학적 구성을 명시적으로 코드로 표현함으로써 LLM이 유연하게 다양한 코스를 만들 수 있습니다. 둘째, LLM 생성과 정책 학습을 **adaptive co-evolution**으로 결합한 점입니다  . 기존 연구와 달리, 정책의 현재 성능을 반영하여 LLM에게 다음 세대 코스를 진화시켜 달라고 하는 피드백 루프가 추가되어 있습니다. 이를 통해 학습이 진행될수록 커리큘럼이 자연스럽게 어려워지고 다양해지며, 로봇의 기술이 점점 향상됩니다  . 이처럼 LLM을 이용한 **자동 커리큘럼 생성**은 이전에 없던 접근입니다. 특히 4족 보행 로봇 파쿠르와 같이 복잡한 실제 물리 환경에서도 성공적인 실험 결과를 보인 점이 기존 연구 대비 중요한 기여라고 할 수 있습니다  .


## 한계점 및 향후 연구 과제

Eurekaverse는 강력하지만, 몇 가지 한계와 과제도 지닙니다. 논문에서 저자들은 **LLM 샘플 효율**을 주요 문제로 지적합니다 . 매 반복마다 많은 환경을 GPT-4o로 생성해야 하므로, 보다 적은 호출로도 좋은 환경을 만들도록 LLM을 미세조정(fine-tuning)하는 연구가 필요합니다. 또한 현재 환경 변형 프롬프트는 텍스트 정보(보상, 성공률 등)만 활용하기 때문에, 앞으로는 환경의 3D 기하 구조나 실제 화면을 함께 제공하는 **멀티모달 입력**을 적용해 LLM의 공간적 추론 능력을 끌어올릴 여지도 있습니다 .

그 밖에 실용적 고려사항도 있습니다. 예를 들어 본 실험에서는 GPT-4o API를 사용하였고 24시간 학습에 걸려 비용이 발생했습니다. 따라서 실시간 로봇 학습이나 더 적은 계산 자원으로 적용하기 위해서는 효율화가 필요합니다. 또 현재는 파쿠르에 집중했으나, 다른 로봇 과제(예: 조작, 비행 등)에도 적합한 환경 생성이 가능한지 실험을 통해 확인해야 합니다. 마지막으로, LLM이 종종 비현실적인 코드를 생성할 수 있기 때문에 이를 걸러내는 안전 장치나 필터링 기법도 중요합니다.

요약하자면, Eurekaverse는 **언어 모델의 코드 생성 능력**을 로봇 환경 설계에 적용한 흥미로운 시도로서, 복잡한 장애물 코스 커리큘럼을 자동화했습니다. 이로써 로봇 학습에서의 환경 커리큘럼 제작 부담을 크게 덜어줄 수 있는 가능성을 보여주었지만, 동시에 LLM 활용의 효율성과 안정성 측면에서 후속 연구가 필요함도 드러냈습니다  . 향후 다양한 도메인으로의 확장과 시스템 최적화를 통해, 더 일반적이고 뛰어난 로봇 학습 환경을 만드는 연구로 이어질 것으로 기대됩니다.

# Reference

- [Eureka 논문 리뷰](https://curieuxjy.github.io/posts/paper/2025-07-20-eureka.html)
