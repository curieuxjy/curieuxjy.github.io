---
title: "📃DAPG 리뷰"
date: 2025-09-05
categories: [il, rl, dapg]
toc: true
number-sections: true
description: Learning Complex Dexterous Manipulation withDeep Reinforcement Learning and Demonstrations
---

- [Paper Link](https://arxiv.org/abs/1709.10087)
- [Homepage](https://sites.google.com/view/deeprl-dexterous-manipulation)

1.  이 논문은 고차원 로봇 손으로 복잡한 손 조작 작업을 학습하는 심층 강화 학습(DRL) 방법을 제시하지만, 기존 DRL은 샘플 효율성이 낮아 실제 시스템 적용에 어려움이 있습니다.
2.  저자들은 적은 수의 인간 데모를 활용하여 심층 강화 학습(DAPG)의 샘플 복잡성을 극적으로 줄이는 방법을 제안하며, 이는 몇 시간 분량의 로봇 경험만으로도 학습을 가능하게 합니다.
3.  이 방법은 물체 재배치, 도구 사용 등 복잡한 손 조작 작업을 학습할 수 있게 하며, 학습된 정책은 데모로부터 자연스럽고 강건한 움직임을 보여줍니다.

<center>
<img src="../../images/2025-09-05-dapg/natural_motion_dapg.gif" width="100%" />
</center>

<center>
<img src="../../images/2025-09-05-dapg/01.png" width="60%" />
</center>

---

# Brief Review

고차원 민첩한(dexterous) 다지 로봇 손 제어는 복잡하고 접촉이 많아 어렵습니다. 심층 강화 학습(DRL)은 모델에 구애받지 않는 접근 방식이지만, 고차원 민첩 조작에 확장되는 것을 보여주지 못했습니다. 또한, 샘플 비효율성 때문에 실제 시스템에 적용하기 어렵습니다. 이 연구는 24-DoF의 고차원 손으로 복잡한 조작 작업을 모델 프리(model-free) DRL로 스크래치부터 해결할 수 있음을 시뮬레이션에서 보여줍니다. 샘플 비효율성을 극복하기 위해 소수의 인간 데모를 사용합니다. 데모를 활용하면 샘플 복잡성을 크게 줄여 몇 시간 분량의 로봇 경험만으로 학습할 수 있게 됩니다. 또한 데모 사용은 자연스럽고 강건한(robust) 정책을 만듭니다. 이 연구는 객체 재배치(object relocation), 손 안 조작(in-hand manipulation), 도구 사용(tool use), 문 열기(door opening) 작업에 대한 성공적인 정책을 시뮬레이션에서 보여줍니다.

연구에서는 로봇이 숙달해야 할 4가지 대표적인 민첩 조작 작업 세트를 제안합니다.

(1) Object Relocation: 객체를 집어 목표 위치로 옮깁니다.
(2) In-hand Manipulation (Pen Repositioning): 손 안에서 객체(펜)의 방향을 조정합니다.
(3) Manipulating Environmental Props (Door Opening): 환경의 일부(문)를 조작합니다.
(4) Tool Use (Hammer): 도구(망치)를 사용하여 못을 박습니다.

실험에는 24-DoF ADROIT 손과 MuJoCo 시뮬레이터를 사용했습니다. 인간 데모는 VR 시스템을 통해 수집되었습니다.

핵심 방법론은 데모 강화 정책 경사법(Demo Augmented Policy Gradient, DAPG)입니다. 이는 강화 학습과 모방 학습을 결합합니다. 제어 문제는 MDP $\mathcal{M} = \{\mathcal{S}, \mathcal{A}, R, \mathcal{T}, \rho_0, \gamma\}$로 모델링됩니다. 정책 $\pi_\theta$는 기대 총 보상 $\eta(\pi) = E_{\pi, \mathcal{M}}[\sum_{t=0}^\infty \gamma^t r_t]$를 최대화하도록 최적화됩니다. 연구는 Natural Policy Gradient(NPG)를 기반으로 합니다.

NPG는 바닐라 정책 경사(vanilla policy gradient) $$\mathbf{g} = \frac{1}{N T} \sum_{i=1}^N \sum_{t=1}^T \nabla_\theta \log \pi_\theta(a^i_t|s^i_t) \hat{A}^\pi(s^i_t, a^i_t, t)$$를 계산하고, Fisher Information Matrix $$\mathbf{F}_\theta = \frac{1}{N T} \sum_{i=1}^N \nabla_\theta \log \pi_\theta(a^i_t|s^i_t) \nabla_\theta \log \pi_\theta(a^i_t|s^i_t)^T$$의 역행렬로 사전 조건화하여

업데이트 $$\theta_{k+1} = \theta_k + \sqrt{\frac{\delta}{\mathbf{g}^T \mathbf{F}_{\theta_k}^{-1} \mathbf{g}}} \mathbf{F}_{\theta_k}^{-1} \mathbf{g}$$를 수행합니다.

DAPG는 **데모 데이터셋 $\rho_D$를 활용하여 RL을 강화**합니다.

- 첫째, 행동 복제(Behavior Cloning, BC)를 사용하여 정책을 $\max_\theta \sum_{(s,a) \in \rho_D} \ln \pi_\theta(a|s)$를 통해 데모를 모방하도록 사전 학습(pretraining)하여 탐색에 도움을 줍니다.
- 둘째, 증강된 손실(augmented loss)을 사용한 RL 미세 조정(fine-tuning)을 수행합니다. 정책 경사 업데이트에 데모 데이터를 활용하는 항을 추가합니다. 증강된 경사 $$\mathbf{g}_{\text{aug}} = \sum_{(s,a) \in \rho_\pi} \nabla_\theta \ln \pi_\theta(a|s)A^\pi(s, a) + \sum_{(s,a) \in \rho_D} \nabla_\theta \ln \pi_\theta(a|s)w(s, a)$$를 사용하며, 여기서 $$w(s, a) = \lambda_0 \lambda_1^k \max_{(s',a') \in \rho_\pi} A^\pi(s',a')$$는 데모에 대한 어드밴티지 정보를 근사하기 위한 휴리스틱 가중치로, 학습이 진행됨에 따라 감소합니다.

실험 결과, sparse 보상으로는 NPG와 DDPG 모두 학습하지 못했습니다 (in-hand 작업 제외). shaped 보상으로는 NPG는 높은 성공률을 달성했지만 샘플 비효율적이고(몇백 로봇 시간 소요), 부자연스러우며 환경 변화에 강건하지 않았습니다. DDPG는 성공적인 정책 학습에 실패했습니다.

반면, **DAPG**는 희소한 보상 설정에서 다른 데모 활용 방법인 DDPGfD보다 훨씬 뛰어난 성능을 보였습니다. DAPG는 모든 작업에서 몇 로봇 시간 안에 정책을 학습할 수 있었으며, Object Relocation 작업에서 스크래치 학습보다 약 30배 빨랐습니다. DAPG로 학습된 정책은 데모를 통해 인간 전략의 강건함을 포착하여 환경 변화에 더 강건하며, 더 인간적인 동작을 보여주었습니다. 다양한 환경 앙상블(ensemble)에 대해 학습할 때도 DAPG가 스크래치 RL보다 더 강건한 정책을 학습할 수 있었습니다.

이 연구는 고차원 민첩 조작 작업을 위한 DRL 가능성을 보여주었습니다. DAPG는 데모를 통해 샘플 효율성, 정책의 강건성 및 자연스러움을 크게 향상시켰습니다. 몇 시간 안에 학습이 가능하여 실제 시스템 적용 가능성을 제시합니다.



{{< video https://youtu.be/KIuZs5rqN1c?si=y-7mNMUCpsh97Nyv >}}


---

# Detail Review

##소개 및 배경

다수의 관절을 가진 다지 손 로봇(multi-fingered dexterous hand)은 인간 환경에서 다양한 작업을 수행할 잠재력을 지니지만, 제어의 난이도가 매우 높습니다. 손가락이 여러 개인 로봇 손은 고차원 관절 공간과 복잡한 접촉 상호작용을 갖고 있으며, 물체를 쥐거나 조작하는 과정에서 접촉 지점이 수시로 바뀌고 동역학이 불연속적으로 변화합니다. 이런 이유로, 기존 연구에서는 손 로봇 제어를 쉽게 만들기 위해 구조적으로 단순한 손이나 제한된 동작에 집중하곤 했습니다.

예를 들어, 손가락을 두세 개로 제한하거나 특수 기계적 구조를 설계하여 문제를 단순화하거나, 파지(grasping)나 물체를 손 안에서 간단히 회전시키는 정도의 비교적 단순한 작업을 주로 다루었습니다. 모델 기반 최적화 방법으로 이러한 기본 동작들을 성공시킨 사례도 있었지만, 현실 세계의 복잡한 접촉이 있는 상황에선 정확한 모델링이 어려워 한계를 보였습니다.

강화학습(RL)은 동역학 모델 없이 시도-오차를 통해 정책을 학습하므로, 복잡한 로봇 제어 문제에 유연하게 적용할 수 있다는 장점이 있습니다. 하지만 딥 강화학습(DRL)을 다지 손 조작에 적용한 선행 연구들은 거의 없었고, 주로 7-DoF 로봇 팔 등 비교적 간단한 조작이나 보행 같은 다른 분야에 국한되어 있었습니다. 심지어 표준적인 RL 벤치마크 과제들은 차원이 낮아, 선형 정책으로도 쉽게 해결되는 경우가 있을 정도로 단순하여, 고차원 손 조작의 난제를 대변하지 못했습니다. 모델 프리 RL이 이렇게 복잡한 손 조작 작업에 직접 적용되어 성공한 사례는 논문 발표 시점까지 전례가 없었습니다.

본 리뷰의 대상인 Rajeswaran et al.(2018)의 논문은 이러한 간극을 메우기 위해 고차원 로봇 손의 복잡한 조작 작업을 딥 RL로 해결한 최초의 연구 중 하나입니다. 특히, 소수의 인간 시범(demonstrations) 데이터를 활용하여 강화학습의 탐색 문제와 표본 효율 문제를 크게 개선하였음을 보여주었습니다. 그 결과, 과거에는 시뮬레이션에서 수백 시간 걸리던 학습을 불과 몇 시간의 로봇 경험(몇 회의 에피소드)으로 단축하였고, 학습된 정책의 동작이 더 인간처럼 자연스럽고 환경 변화에 견고해지는 효과도 확인하였습니다. 이 리뷰에서는 해당 논문의 핵심 기여와 기술적 아이디어, 제안한 알고리즘 DAPG (Demo Augmented Policy Gradient)의 구체적인 동작 원리, 그리고 실험 설정 및 결과를 중점적으로 깊이 있게 분석합니다. 또한 기존 관련 연구들과 비교하여 본 논문의 차별점도 함께 짚어보겠습니다.

## 주요 기여 요약

이 논문에서 저자들은 복잡한 다지 손 조작 학습 분야에 다음과 같은 주요 기여를 하였습니다:

- **모델 프리 딥 RL로 고차원 손 조작 작업 성공**: 인간 손과 유사한 5-손가락 24자유도 로봇 손을 이용해, 물체 옮기기, 손안에서 물체 재배치, 도구 사용, 문 열기와 같은 다양한 접촉이 있는 복잡한 작업들을 모델 기반 사전지식 없이 RL만으로 학습하여 성공적으로 시연하였습니다. 이는 이론적으로나 실험적으로 최초의 성과로서, 고차원 손 조작에도 모델 프리 강화학습을 적용할 수 있음을 보였습니다.
- **시범 데이터 활용을 통한 학습 효율 비약적 향상**: 각 작업마다 *25개 정도의 인간 시범을 가상현실(VR) 인터페이스를 통해 수집하고 활용함으로써, 탐색 문제를 완화하고 학습에 필요한 샘플(데이터) 양을 드라마틱하게 감소시켰습니다. 그 결과, 시뮬레이터 상에서 몇 시간에 불과한 로봇 경험(예: 5시간 가량)만으로도 학습이 가능해졌습니다. 이는 해당 작업들을 실제 로봇에도 적용할 수 있을 만큼 실용적인 시간 내에 학습시킬 수 있는 수준으로 샘플 효율을 개선한 것입니다.
- **시범으로 얻은 자연스러움 및 강인성 향상**: 시범 데이터를 통해 학습한 정책들은 *보상함수 설계를 최소화해도 성공률이 높았을 뿐 아니라, 사람의 동작을 닮은 부드럽고 자연스러운 움직임을 보였습니다. 또한 동일한 작업이라도 환경 조건(예: 물체 질량이나 마찰 등)을 바꾸었을 때 성능 저하가 덜하고 견고하게 동작하여, 시범을 통해 인간 전략의 내재된 강인성이 정책에 스며들었음을 확인했습니다.
- **새로운 표준 과제 세트 제안**: 저자들은 향후 연구에 활용할 수 있도록, 앞서 언급한 4가지 *다양한 손 조작 작업 환경을 정식으로 구축하여 제시하였습니다. 이 작업들은 고차원, 풍부한 접촉 상호작용, 과업의 다양성 측면에서 실제 인간 환경의 과제를 잘 대표하며, 로봇 조작과 머신러닝 교차 분야 연구자들에게 도전적인 벤치마크로 활용될 수 있습니다.

요약하면 이 논문은, "고차원 로봇 손도 딥 RL로 학습시킬 수 있다. 그리고 소량의 시범을 더하면 학습 속도와 정책 품질 모두 극적으로 향상된다."는 것을 최초로 증명하며, 그 방법으로 DAPG 알고리즘을 제시한 것입니다.

**실험 환경:** 복잡한 다지 손 조작 과제들저자들은 인간형 5-손가락 로봇 손 (24 DoF)의 난이도를 충분히 체감할 수 있는 네 가지 대표 작업을 시뮬레이션 환경에 구현하였습니다. 이 작업들은 일상에서 사람이 손으로 하는 다양한 조작 기술을 반영하며, 가상 현실(VR) 장치를 통해 사람으로부터 시범 데이터를 수집할 수 있게 설계되었습니다. 각 작업에는 환경의 무작위성이 도입되어 초기 상태나 대상의 속성이 에피소드마다 달라지며, 최종 성공 여부만으로 보상이 주어지는 이진 성공 기준(sparsereward)을 갖습니다.

네 가지 과제는 다음과 같습니다:

1. **물체 옮기기 (Object Relocation)**: 테이블 위에 놓인 파란 공을 집어 들어서 초록색 목표 지점까지 옮기는 작업입니다. 에피소드마다 공과 목표 위치가 테이블 위 임의의 곳으로 설정되며, 공이 목표 지점의 반경 ε 이내에 놓이면 성공으로 간주됩니다. 이 과제는 기본적인 파지 및 이송 능력을 평가하며, 손-물체 간 다중 접촉이 필요한 대표적 작업입니다.
2. **손 안에서 물체 조작 (In-hand Manipulation**): 손에 쥔 펜의 방향을 재조정하여 주어진 목표 방향(녹색 표시)에 맞추는 작업입니다. 손목은 고정된 채 손가락들만 이용하며, 매 에피소드마다 펜의 초기 자세와 목표 자세가 무작위로 주어집니다. 펜의 방향이 목표와 일정 허용 오차 이내로 일치하면 성공입니다. 이 과제는 손가락들의 협조적인 미세 조작 능력을 필요로 하며, 연속적 접촉 및 재그립(re-grasping) 등이 요구됩니다.
3. **문 열기 (Door Opening)**: 손으로 문 손잡이의 걸쇠(latch)를 해제하고 문을 밀어 열기까지 수행하는 작업입니다. 문의 초기 닫힌 각도와 걸쇠의 상태가 무작위로 설정되며, 걸쇠에는 마찰력과 문을 닫히도록 잡아당기는 토크(스프링)가 존재해 손잡이를 어느 정도 힘줘 돌려야 열립니다. 문이 완전히 열려 문턱(stopper)에 닿으면 성공으로 판정됩니다. 이 과제는 복잡한 도구-환경 상호작용과 다단계 행동 시퀀스(걸쇠 풀기 → 밀기)를 포함하여, 손 조작의 탐색 난이도를 크게 높인 경우입니다.
4. **도구 사용 (Tool Use – Hammering)**: 바닥에 고정된 못을 망치로 내리쳐 박는 작업입니다. 테이블 위에 놓인 망치를 손으로 집어 들어 목표 못의 머리를 여러 차례 두드려서, 결국 못 전체를 나무 판자에 박아 넣으면 성공입니다. 못은 마찰로 약 15N의 힘까지 버티도록 설계되어, 충분한 타격 힘을 가해야 합니다. 이 과제는 물체 파지 → 근력 동작으로 이어지는 복잡한 활용이며, 비연속적인 충격 접촉과 도구의 정확한 조작이 요구되는 난도 높은 환경입니다.

<center>
<img src="../../images/2025-09-05-dapg/dapg1.png" width="80%" />
</center>


> DAPG 알고리즘으로 학습된 로봇 손의 도구 사용 (망치질) 과제 수행 장면. 로봇 손이 책상 위의 망치를 집어 들고 바닥의 못을 여러 차례 타격하여 끝까지 박는 복잡한 동작을 성공시켰다. 각 작업마다 약 25개의 인간 시범으로 초기정책을 학습시킨 후, 강화학습을 통해 몇 시간 만에 이러한 행동이 가능해졌다. 해당 정책은 사람 시범을 참고하여 학습되었기 때문에 동작이 비교적 자연스럽고, 환경 변화에도 강건한 특징을 보였다.


각 작업은 서로 다른 기술적 도전요소를 갖고 있어서, 하나의 알고리즘이 다양한 측면에서 성능을 검증받도록 합니다. 예를 들어, 물체 옮기기는 접근 및 파지(grasp & lift) 능력, 손내 조작은 정교한 자세 제어, 문 열기는 다단계 상호작용 및 힘 조절, 망치질은 공구 활용 및 충격력 제어라는 식으로, 손재주(hand dexterity)의 폭넓은 범위를 아우르도록 구성되었습니다. 특히 망치질이나 문 열기 등의 작업은 이전 연구들에서 다루지 않았던 복합 과제로서, 실제 가정환경에서 로봇 손이 해야 할 유용한 작업들에 가깝습니다. 저자들은 이러한 표준 과제 세트를 구축함으로써, 향후 연구자들이 로봇 손+강화학습 분야에서 공통으로 도전할 수 있는 벤치마크를 제공하였습니다.

### NPG의 한계

Demo Augmented Policy Gradient (DAPG) 알고리즘 핵심 아이디어는 강화학습(RL)과 모방학습(시범)을 효과적으로 결합하여, 탐색 어려움과 샘플 비효율 문제를 동시에 해결하는 것입니다. 기본적으로 저자들은 온-폴리시(on-policy) 정책 그래디언트 기반의 강화학습 알고리즘을 사용하였는데, 이는 Natural Policy Gradient (NPG) 방법으로 구현되었습니다.

NPG는 정책 파라미터 공간에서 Fisher Information Matrix로 그래디언트를 정규화하여 업데이트하는 기법으로, 고차원 연속 제어 문제에서 기존 방법들보다 안정적이고 성능이 좋은 것으로 알려져 있습니다. 저자들은 먼저 이 기본 강화학습 알고리즘을 사용하여 작업을 푸는 것을 시도해보았는데, 다음과 같은 한계를 확인했습니다:

- **보상 설계의 어려움**: 순수 RL로는 성공/실패에 대한 이진 보상(sparse reward)만 주었을 때 학습이 진행되지 않았습니다. 무작위 탐색으로는 성공 사례를 거의 못 찾아내기 때문입니다. 결국 작업별로 사람이 세밀한 shaping 보상(힌트성 중간 보상)을 설계해주어야 학습이 겨우 가능했습니다. 이러한 보상 설계는 많은 노력과 휴리스틱을 필요로 합니다.
- **샘플 요구량 문제**: 보상을 잘 설계해 주어 RL이 학습에 성공하더라도, 수백만 스텝에 달하는 경험이 필요하여 학습 시간이 매우 길었습니다. 논문 결과에 따르면 어떤 작업은 100 시간분에 해당하는 시뮬레이션 데이터가 필요했는데, 이는 실제 로봇에 적용하기엔 비현실적으로 많은 양입니다 (로봇을 100시간 연속 구동하며 학습시키는 것은 안전이나 비용 면에서 쉽지 않습니다).
- **학습된 정책 품질 문제**: 순수 RL로 겨우 얻어진 정책들조차 동작이 어색하고 비효율적이며, 환경 조건이 조금만 달라져도 실패할 정도로 취약성을 보였습니다. 예를 들어, 사람이라면 하지 않을 불필요한 손가락 움직임을 반복하거나 비정상적인 자세로 물체를 잡는 등 인간 공감대와 동떨어진 행동들이 나타났고, 약간 다른 물체 크기나 마찰조건에서는 성공률이 급격히 떨어졌습니다.

DAPG (Demo Augmented Policy Gradient) 알고리즘은 이러한 문제들을 해결하기 위해 인간 시범 데이터를 강화학습 과정에 체계적으로 통합합니다.

핵심 구성은 두 가지 단계로 이루어집니다: **(1) Behavior Cloning을 통한 정책 초기화와 (2) 시범 정보가 포함된 보조 손실로 강화학습 파인튜닝**입니다. 아래에서는 이 두 단계를 상세히 설명합니다.

### Behavior Cloning(BC)으로 초기 정책 학습

먼저 소량의 전문가 시범 데이터셋 $\rho_D$를 모아 Behavior Cloning으로 초기 정책을 학습시킵니다. 시범 데이터셋 $\rho_D={(s_t^{(i)}, a_t^{(i)}, \dots)}$는 여러 에피소드 $i$에서 시간 단계 $t$별로 상태 $s$, 전문가 행동 $a$ (그리고 보상 $r$, 다음 상태 등)을 모아둔 것입니다. Behavior Cloning은 이를 `지도학습 문제`로 보아, 주어진 상태에서 전문가의 행동을 모방하도록 정책 파라미터를 학습합니다. 수식으로 표현하면 다음과 같습니다:
$$ \max_{\theta} \sum_{(s,a)\in \rho_D} \ln \pi_\theta(a \mid s)\,. \tag{1} $$

위 식은 시범 데이터에서 정책 $\pi_\theta$의 로그 확률을 최대화하는 문제입니다. 즉, 시범 상태 $s$에서 전문가가 취한 행동 $a$를 가장 그럴듯하게 선택하도록 정책의 파라미터 $\theta$를 조정합니다. 이 과정을 통해 얻어진 모방 정책은 전문가 경로를 따라가는 초기 전략을 제공합니다. Behavior Cloning으로 초기화를 하면, `완전히 무작위 정책으로 시작하는 것에 비해 탐색을 훨씬 효율적으로 만들 수 있습니다.` 일반적인 정책 그래디언트 방법은 정책의 확률적 탐색 노이즈에만 의존하여 새로운 동작을 시도하는데, 고차원 문제에서는 무작위 시도로는 의미 있는 상태에 도달하기 어렵습니다. `반면, 시범 경로를 따라하도록 초기 정책을 잡아주면 초반부터 유망한 상태-행동 영역을 탐색하게 되어, 흔히 필요했던 복잡한 보상 shaping 없이도 학습을 시작할 수 있습니다.` 실제 논문 결과에서도, 시범을 활용하지 않은 경우에는 각 작업마다 사람이 일일이 추가 보상 신호를 설계해줘야 했지만, DAPG는 시범 덕분에 이런 보상 없이도 학습이 가능했음을 보여줍니다.

하지만, Behavior Cloning만으로 최종 정책을 얻기에는 **한계가 있습니다.** `시범 데이터의 양이 많지 않으면 모방 학습된 정책은 분포 이동(distributional shift) 문제로 새로운 상태에 대응하지 못해 실패하게 됩니다.` 실제로 저자들의 실험에 서도 BC만으로 학습한 정책은 대부분 작업을 끝까지 성공적으로 수행하지 못했습니다. 이는 시범 경로를 조금만 벗어나도 정책이 어떻게 행동해야 할지 모르기 때문입니다. 더욱이, `모방학습만으로는 전문가를 넘어서 성능을 향상시킬수도 없습니다` – 기본적으로 주어진 시범을 따라하기만 하므로 과업 성공에 대한 피드백이 없기 때문입니다. 따라서 모방으로 초기화한 후에도 강화학습을 통해 보상 신호로 정책을 향상시키는 단계가 필요합니다.

### 강화학습 파인튜닝 (시범 보조 손실 포함 정책 그래디언트)

초기 정책을 얻었다면, 이후에는 **온-폴리시 강화학습으로 정책을 계속 파인튜닝**합니다. 이때 일반적인 정책 그래디언트 방식과 차별화되는 점은, `시범 데이터로부터 추가로 얻는 그래디언트 항(term)을 손실 함수에 포함시킨다`는 것입니다. 이를 통해 학습 내내 정책이 시범의 유익한 행동들을 참고하도록 만들고, 중간 단계 행동들에 대한 가이던스를 제공합니다. 저자들은 이 접근을 **"데모 증강 정책 그래디언트(Demo Augmented Policy Gradient, DAPG)"**라 명명하였습니다.

구체적으로, 정책 그래디언트를 계산할 때 사용되는 목적 함수를 확장합니다. 기본이 되는 RL 목표는 정책의 기대 총 보상 $J(\pi_\theta) = \mathbb{E}_\pi \left[\sum_t \gamma^t r_t\right]$를 최대화하는 것으로, 이에 대응하는 정책 그래디언트는 REINFORCE 공식을 통해 아래처럼 주어집니다:

$$ g_{\text{RL}} \;=\; \mathbb{E}{(s,a)\sim \rho\pi}!\Big[ \nabla_\theta \ln \pi_\theta(a|s)\; A^\pi(s,a) \Big]
\,, $$

여기서 $\rho_\pi$는 현재 정책 $\pi$로부터 수집한 온-폴리시 데이터 분포이고, $A^\pi(s,a)$는 현재 정책에 대한 어드밴티지(advantage) 값입니다. $A^\pi(s,a)$는 해당 상태-행동이 평균적인 상태 대비 얼마나 더 좋은 결과를 내는지를 나타내는 값으로, 보상 $Q^\pi(s,a)$와 가치함수 $V^\pi(s)$의 차이로 정의됩니다. DAPG에서는 이 정책 그래디언트에 추가로 시범 데이터 방향의 그래디언트를 더해줍니다. 즉, 최종 증강 그래디언트 $g_{\text{aug}}$는 다음과 같이 구성됩니다:

$$ g_{\text{aug}} \;=\; \sum_{(s,a)\in \rho_\pi} \nabla_\theta \ln \pi_\theta(a|s)\; A^\pi(s,a)\;+\; \sum_{(s,a)
\in \rho_D} \nabla_\theta \ln \pi_\theta(a|s)\; w(s,a)\,. \tag{2} $$

첫 번째 항은 온-폴리시 RL 그래디언트(앞서 설명한 $g_{\text{RL}}$)이고, **두 번째 항이 시범 데이터로부터 오는 추가 그래디언트입니다.** 이 추가 항은 시범 데이터 분포 $\rho_D$에 대해, 그 상태에서 정책이 시범 행동 $a$를 취하도록 확률을 높이는 방향으로 작용합니다. 단, 모든 시범 데이터를 동일하게 사용하기보다는 각각에 가중치 $w(s,a)$를 부여하여 얼마나 신뢰할지 조절합니다.
이런 방식으로 시범을 활용하면 모방 학습과 강화학습을 연속적으로 접목할 수 있습니다.

몇 가지 극단적인 경우를 살펴보면, 만약 모든 $(s,a)\in\rho_D$에 대해 **$w(s,a)=0$이라면** 두 번째 항이 없어지고 **순수한 RL 학습과 동일**해집니다. 반대로 **$w(s,a)$가 매우 큰 상수**로 설정되어 시범 항이 지배적이라면, 이는 사실상 **Behavior Cloning만을 수행**하는 것과 가까워집니다. DAPG의 목표는 $w(s,a)$를 적절히 조절하여 RL의 성능 향상 효과와 시범의 가이드 효과를 동시에 얻는 것입니다.

---

저자들은 **이상적인 $w(s,a)$ 설계**에 대해 고찰하면서, **$\rho_\pi$와 $\rho_D$의 혼합 분포 관점에서 분석**합니다.

분석에 따르면, 이론적으로는 시범 데이터에서도 현재 정책의 Advantage를 계산하여 $w(s,a) = A^\pi(s,a)$로 두는 것이 합리적입니다. 즉, 시범에서의 행동이 현재 정책보다 얼마나 이득인지에 따라 가중치를 주는 것이 최선이라는 것이죠. 하지만 현실적으로 시범 각 상태의 $A^\pi(s,a)$ 값을 얻으려면 추가적인 시뮬레이션이나 가정이 필요하여 곧바로 계산하기 어렵습니다.
그래서 저자들은 경험적인 휴리스틱으로서 다음과 같은 단순한 형태의 가중치를 제안합니다 :

$$ w(s,a) \;=\; \lambda_0 \,\lambda_1^k \; \max_{(s',a')\in \rho_\pi} A^\pi(s',a') \qquad \forall (s,a) \in \rho_D\,, $$

여기서 $\lambda_0$는 초기 가중치 스케일, $\lambda_1$은 감쇄율(decay factor), $k$는 학습이 진행된 iteration 횟수를 나타냅니다. 즉, 현재 정책으로 모은 온-폴리시 데이터에서의 최대 advantage 값 (현 정책이 얻
은 가장 좋은 행동의 advantage)을 하나의 기준 상수로 삼아서, 모든 시범 샘플에 동일한 가치로 할당합니다. 그리고 학습이 거듭될수록 $\lambda_1^k$ 항을 통해 이 가중치를 지수적으로 감소시킵니다.

이 설계의 의도는 명확합니다: 초기 학습 단계에서는 시범이 정책보다 훨씬 우수한 행동들을 담고 있으므로 시범의 영향을 강하게 주어야 합니다. 반면 후기 단계에서는 정책이 이미 웬만한 성능에 도달했기 때문에 시범과 정책의 수준이 비슷해집니다. 이때까지도 시범에 끌려다니면 정책이 더 나아지기 어렵기 때문에, 후반으로 갈수록 시범의 비중을 줄여 정책이 스스로 성능을 최대로 끌어올리도록 합니다. 이렇게 함으로써 초반엔 시범 위주 학습 → 후반엔 RL 위주 학습으로 점진적으로 전환되어, 전체적으로 시범+RL의 시너지를 얻는 것이 DAPG의 목표입니다. 논문에서는 $\lambda_0=0.1$, $\lambda_1=0.95$로 설정하여 모든 실험을 진행했으며, 적절한 범위 내에서는 결과가 크게 민감하지는 않았다고 보고합니다.

이러한 개념을 종합하여 DAPG 알고리즘의 흐름을 단계별로 정리하면 다음과 같습니다:

1. 시범 수집: 전문가(휴먼)가 VR 장치를 통해 각 작업을 수행하여 성공 trajectories를 $N$개 (논문 실험에서는 약 25개) 기록합니다. 이로부터 시범 데이터셋 $\rho_D$를 구성합니다.
2. Behavior Cloning 초기화: 식 (1)의 최대우도 추정 문제를 풀어 시범을 모방하는 초기 정책 $\pi_{\theta_0}$를 얻습니다. (실제로는 딥러닝 옵티마이저를 활용하여 손실 함수를 최소화하는 방식으로 학습합니다.)
3. 강화학습 반복: $k=1,2,\dots$ 에 대해 다음을 수행합니다.
   1. 현재 정책 $\pi_{\theta_{k-1}}$을 사용하여 시뮬레이터에서 여러 에피소드의 on-policy 데이터 $\rho_\pi$를 샘플링합니다. (논문 구현은 한 iteration에 200 에피소드씩 수집하여 사용.)
   2. 수집한 데이터로부터 각 state-action의 advantage $\hat{A}^\pi(s,a)$ 값을 추정합니다 (가치함수 baseline 등을 이용). 정책 그래디언트 $g_{\text{RL}}$ (온-폴리시 부분)와 시범 그래디언트 항을 포함한 증강 그래디언트 $g_{\text{aug}}$를 식 (2)에 따라 계산합니다.
   3. $g_{\text{aug}}$를 사용하여 정책 파라미터 업데이트를 수행합니다. 이때 Natural Policy Gradient 방법을 사용하므로, 그래디언트에 Fisher 정보 행렬의 역을 곱해 스텝 크기를 조절한 업데이트를 적용합니다. (즉, $ \theta_k = \theta_{k-1} + \alpha\, F^{-1} g_{\text{aug}} $ 형태로, $\alpha$는 적절한 스텝 크기입니다.)
   4. 시범 그래디언트의 가중치 계수 $\lambda_1^k$를 다음 iteration을 위해 감소시킵니다.
4. 수렴 또는 충분한 성능 달성 시 종료: 정책을 평가하여 성공률 등이 기준을 넘으면 학습을 종료합니다.

이 전체 알고리즘에서, 시범 데이터는 초기 학습에서는 탐색을 크게 가속하고, 학습 중반에는 정책이 놓치고 있는 행동들을 보강하는 가이드 역할을 하며, 학습 후반에는 비중을 줄여 최종 성능을 RL이 주도하게 합니다. 저자들이 든 예를 다시 상기해 보면, Behavior Cloning 단계에서는 망치를 드는 것까지 겨우 배우지만 못을 치지는 못했습니다. 이후 강화학습이 망치 드는 부분을 자체적으로 개선하고 나면, 여전히 어려운 못 치는 동작을 시범 데이터가 뒷받침해주어 정책이 이를 습득하도록 만든 것입니다. 이렇게 시범의 정보가 전 학습 과정에 걸쳐 활용되기 때문에, 초기 시범에서 담지못한 복잡한 행동까지 최종 정책이 얻게 되는 것이 DAPG의 장점입니다.

> 참고: DAPG와 대비되는 접근으로, **off-policy 방법인 DDPGfD (DDPG from Demonstrations)**가 있습니다. DDPGfD는 Q러닝 기반 연속제어 알고리즘인 DDPG에 리플레이 버퍼 초기화 형태로 시범을 넣고, 우선순위 경험 재생(PER), n-step 보상, 네트워크 가중치 정규화 등 여러 기법을 조합하여 시범을 활용하는 방법입니다. off-policy 방법은 동일한 데이터로 반복 학습하므로 표본 효율은 높을 수 있지만, 훈련이 불안정하고 특히 고차원 환경에서는 민감도가 높다는 단점이 있습니다. 반면 DAPG는 on-policy 업데이트로 안정성을 확보하고 시범으로 효율 향상까지 얻은 방법으로, 논문 실험에서도 DDPGfD보다 우수한 성능을 보였습니다. (자세한 비교는 뒤의 실험 결과에서 다룹니다.)

## 실험 결과 및 분석

### 강화학습 단독으로는 한계

우선 시범을 사용하지 않고 순수 RL만으로 앞서 소개한 네 가지 작업을 학습시켜 본 결과, 여러 문제점이 나타났습니다. Sparse reward (최종 성공 여부만 보상) 설정에서는 대다수 작업에서 아무런 학습 진전이 없었는데, 이는 앞서 언급한대로 무작위 탐색으로는 성공 사례를 만나지 못해 정책이 옳은 방향으로 갱신되지 않기 때문이었습니다. 예외적으로 펜 돌리기 (in-hand) 작업은 매우 간단한 성공 조건 덕에 극히 드물게 성공 에피소드가 발생하여 조금이나마 학습이 진행되었으나, 다른 작업들은 전혀 성공을 경험하지 못한 채 보상이 0으로 머물렀습니다.

이 때문에 저자들은 RL을 학습시키기 위해 부득이하게 각 작업별로 세밀한 shaped reward를 설계하여 투입했습니다. 예컨대 문 열기 작업의 경우 "걸쇠를 어느 정도 돌리면 +보상, 문을 살짝 열면 +보상" 등의 중간 보상을 단계별로 주는 식입니다. 이러한 휴리스틱 보상 덕분에 NPG 알고리즘은 모든 작업에서 정책을 어느 정도 학습할 수 있었습니다. 최종 성능을 100회 시도 중 성공률(%)로 평가했을 때, NPG는 각 과제에서 상당히 높은 성공률을 보여주었습니다.

<center>
<img src="../../images/2025-09-05-dapg/dapg2.png" width="100%" />
</center>


Figure 7에 제시된 결과에 따르면 NPG의 경우 네 작업 모두 약 80~100%에 수렴하는 성능을 보인 반면, DDPG 알고리즘은 광범위한 튜닝에도 불구하고 어떠한 작업도 성공적으로 학습하지 못했습니다. (DDPG는 오프폴리시의 장점으로 데이터 효율은 높지만, 고차원 연속제어에서는 파라미터 민감도와 불안정성으로 학습 실패할 때가 많다는 지적이 있습니다. 이 실험에서도 복잡한 손 환경에 적합하지 않음이 확인되었습니다.) 비록 NPG로 shaped 보상 하에 학습이 되긴 했지만, 학습 속도와 정책 품질 면에서 문제가 남았습니다. 우선 학습에 요구되는 표본 수가 매우 컸는데, 논문 부속자료의 Table I에 정리된 바에 따르면 시범 없이 shaped 보상으로 학습한 경우 수백 회의 정책 업데이트(하나의 업데이트당 200 에피소드 샘플) 후에야 90% 성공률에 도달했습니다. 이를 실제 로봇 시간으로 환산하면 수십~수백 시간에 이르는 분량입니다. 아래 표는 각 작업에 대해 DAPG vs. 순수 RL의 학습 소요 시간을 비교한 것으로, DAPG가 얼마나 학습을 가속했는지 잘 보여줍니다:

|작업 (Task) | DAPG (시범 + sparse 보상) | RL (NPG) – shaped 보상 | RL (NPG) – sparse 보상 |
|:-:|:-:|:-:|:-:|
| Relocation (물체 옮기기) | 52회 업데이트 <br> (~5.8 시간) | 880회 <br> (~98 시간) | 실패 (학습 불가) |
| Hammer (망치질) | 55회 업데이트 <br> (~6.1 시간) | 448회 <br> (~50 시간) | 실패 (학습 불가) |
| Door (문 열기) | 42회 업데이트 <br> (~4.7 시간) | 146회 <br> (~16.2 시간) | 실패 (학습 불가) |
| Pen (펜 회전) | 30회 업데이트 <br> (~3.3 시간) | 864회 <br> (~96 시간) | 2900회 <br> (~322 시간) |

> 표 1: 시범 활용 여부에 따른 학습 소요 비교 (논문 Table I 기반 재구성).

DAPG는 시범 덕분에 sparse 보상만으로도 각 작업을 수 시간 내에 학습을 완료한 반면, 순수 RL (NPG)는 shaped 보상이 있어도 수십~수백 시간의 경험을 필요로 했습니다. 특히 sparse 보상만 주는 경우, Pen 작업을 제외하면 RL은 아예 학습 진행이 안 되어 무한대(∞)로 표시되었고, Pen도 300시간 이상의 방대한 경험을 쌓아야 겨우 성공률 기준을 만족시켰습니다. 또한 정책의 행동 품질도 큰 차이가 나타났습니다. 순수 RL로 학습된 정책들은 정의된 보상만 극대화하려다 보니 종종 엉뚱한 방식으로 과제를 수행했습니다. 예를 들어, 물체를 옮기는 작업에서 사람이라면 편하게 쥘 공을 매우 이상한 손가락 꼬임 자세로 쥔다든지, 망치질 작업에서 망치를 비틀어서 잡는 등 비효율적이거나 부자연스러운 동작이 관찰되었습니다 (논문 Figure 8 참조).

<center>
<img src="../../images/2025-09-05-dapg/dapg3.png" width="60%" />
</center>


이러한 정책은 사소한 변화에도 쉽게 실패했는데, 저자들이 정책의 강인성(robustness)을 실험한 결과 순수 RL 정책은 환경 파라미터가 조금만 달라져도 성공 확률이 급격히 떨어진 반면, DAPG로 학습한 정책은 변화된 상황에서도 높은 성능을 유지했습니다.

<center>
<img src="../../images/2025-09-05-dapg/dappg4.png" width="60%" />
</center>


Figure 9에 제시된 그래프에서, 예를 들어 DAPG 정책은 공의 질량이나 마찰계수가 달라져도 성공률 곡선이 완만하게 유지되지만, 순수 RL 정책은 기준 환경에서 벗어나면 성능이 크게 저하되거나 아예 학습 자체가 안 되는 모습이 확인되었습니다.

- DAPG의 성능:
  - 빠른 학습과 향상된 동작 품질 시범을 도입한 DAPG 알고리즘은 위의 문제들을 극적으로 개선하였습니다. 가장 큰 개선은 학습 효율로, Table 1에서 보듯이 모든 작업에서 RL 단독 대비 월등히 적은 시간 내에 정책을 학습했습니다. 특히 Pen (펜 돌리기)의 경우 shaped 보상으로 96시간 걸리던 것이 3.3시간으로 단축되어 약 30배의 가속이 달성되었습니다. 다른 작업들도 8배에서 20배 이상의 속도 향상을 보여, 전반적으로 "시범 + RL" 조합의 효과가 입증되었습니다. 저자들은 DAPG가 아니었다면 수 일(또는 수 주) 걸릴 학습을 몇 시간 수준으로 줄임으로써, 강화학습으로 복잡한 손 기술을 배우는 것이 현실적인 시간 스케일에서 가능함을 보였다고 강조합니다. 실제 논문에서 DAPG는 모든 작업을 5시간 이내에 학습시켰으며, 이는 곧 충분한 병렬화나 시뮬레이터 고속화를 통해 실제 로봇으로도 학습을 돌려볼 수 있는 수준입니다. (물론 아직 시뮬레이션에서만 검증되었지만, “few hours of robot experience”라는 표현에서 시사하듯이 저자들은 DAPG의 효율이라면 실제 로봇 학습도 도전해볼 만하다고 언급합니다.)
  - 다음으로 정책의 성공률과 견고성 측면에서도 DAPG는 뛰어난 성능을 보였습니다. 시범 덕분에 별도 보상 shaping 없이도 충분한 탐색이 이루어져, 저자들은 DAPG 실험에서는 최종 성공 여부에 대한 sparse 보상만 사용하여도 학습이 가능하도록 설정하였습니다. 그럼에도 불구하고 DAPG 정책은 최종 성공률 면에서 앞서 shaped 보상으로 학습한 NPG 정책에 버금가거나 더 나은 수준을 달성했습니다. 또한 흥미롭게도 정책의 강인성이 크게 향상되었는데, DAPG로 얻은 정책은 환경 변화(무게, 마찰, 초기 조건 등)에 훨씬 둔감하여 폭넓은 상황에서 성공을 거두었습니다. 논문에서는 이를 “인간 전략의 내재적 강인성(intrinsic robustness of human strategies)”이 시범을 통해 학습되었기 때문이라고 해석하고 있습니다. 인간 시범 제공자는 작업을 성공하기 위해 다양한 요인을 보정하며 행동하는데, 이런 휴리스틱 노하우가 정책 네트워크에 반영되어 특정 환경에 오버피팅되지 않는 일반적 해결책을 얻게 되었다는 것입니다.
  - 마지막으로 정책 동작의 자연스러움과 관련해, DAPG는 눈에 띄게 인간과 유사한 움직임을 보여주었습니다. 이는 정량적 지표로 측정하기는 어렵지만, 논문 저자들이 함께 공개한 비디오에서 DAPG 정책이 수행하는 동작은 시범 제공 자(인간)가 했을 법한 방식과 유사한 부분이 많음을 알 수 있습니다. 예를 들어, 공을 집어 옮길 때 손가락을 모아서 쥐는 모양새나, 망치를 휘두르는 속도와 궤적 등이 비교적 자연스럽습니다. 반면 보상 함수를 잘못 설계한 RL 정책은 때로 관절 가동 범위를 이상하게 쓰거나 목적에 맞지 않는 손가락 움직임을 보였는데, DAPG 정책에는 그런 엉뚱한 행동이 현저히 줄어든 것을 확인할 수 있습니다. 저자들은 “정책이 학습 과정에서 명시적으로 주지 않았던 ‘인간스러움’의 특성을 시범을 통해 얻게 되었다”고 평합니다. 물론 이는 사이드 이펙트이긴 하지만, 향후 인간과 함께 작업하는 로봇손이라면 이러한 사람다운 움직임이 주는 신뢰성과 안전성 이점도 무시할 수 없을 것입니다.


### DAPG vs. DDPGfD 등 다른 방법과의 비교

DAPG의 효과를 더 입증하기 위해, 저자들은 기존의 시범 활용 RL 기법들과 정량 비교를 수행했습니다. 그 중 한 가지 대표 비교 대상은 앞서 언급한 DDPGfD 알고리즘입니다. 실험 조건을 맞추기 위해 DAPG와 DDPGfD 모두 동일한 시범 데이터(25개)를 사용하고, sparse 보상만으로 각 작업을 학습시켰습니다.


<center>
<img src="../../images/2025-09-05-dapg/dapg5.png" width="100%" />
</center>


결과는 Figure 10에 학습 곡선으로 제시되어 있는데, DAPG가 모든 작업에서 DDPGfD보다 현저히 빠르고 높은 성능을 보였음을 알 수 있습니다. 구체적으로, DDPGfD는 학습 초반 거의 진전이 없다가 나중에서야 겨우 성공률이 오르는 양상을 보인 반면, DAPG는 매우 초기부터 급격히 성능이 향상되어 일정 에피소드 후에는 두 방법 간 큰 격차가 벌어졌습니다. 예를 들어 물체 옮기기의 경우, DAPG는 수십 회의 업데이트 내에 성공률 곡선이 가파르게 상승하여 목표 성능에 도달했지만, DDPGfD는 같은 시간 내 거의 0% 근처에 머물렀다가 한참 후에야 상승하는 모습을 보였습니다. 최종적으로 DAPG는 앞서 언급한 대로 5시간 내외로 모든 작업을 끝냈지만, DDPGfD는 문 열기 등의 몇몇 작업은 그보다 훨씬 느리거나 끝내 충분한 성능에 도달하지 못한 것으로 나타났습니다.


이러한 결과는 온-폴리시 정책 그래디언트 방식과 오프-폴리시 Q러닝 방식의 차이를 보여주는 것이기도 합니다. 저자들은 DDPGfD가 근본적으로 off-policy 수렴 불안정성과 고차원에서의 튜닝 어려움 때문에 초반 탐색에서 헤매는 경향을 보인다고 분석합니다. 반대로 DAPG는 on-policy의 안정된 개선에 시범의 도움까지 더해져 초반 탐색 난관을 빠르게 탈출할 수 있었다는 것입니다. 또한 DDPGfD는 시범을 활용하긴 해도 경험 재생 버퍼에 섞어주는 방식이기 때문에, 학습과정에서 시범이 정책에 미치는 영향력이 점차 희석됩니다. 반면 DAPG는 학습 내내 명시적인 시범 그래디언트를 줬다는 점도 성능 차이의 요인으로 볼 수 있습니다.

관련하여, 논문에서는 그 밖에도 몇 가지 시범+RL 방법들을 소개하고 차이점을 언급합니다. 예를 들어 Hester et al.(2018)의 DQfD (Deep Q-learning from Demonstrations) 는 값 함수 기반에서 시범을 활용한 초기 연구이고, 최근에는 시범을 보상 함수에 통합하거나 (IRL/보상 shaping 방식), 시범 데이터에 노이즈나 실패 사례가 포함되어 있을 때의 학습법 등도 연구되고 있습니다. Guided Policy Search (GPS) 기반으로 시범을 활용한 사례도 있는데, 주로 저차원 손 작업(예: 막대기 돌리기 등)에 한정되어 있고 모형 기반 기법이라 실제 적용에 제약이 있습니다. DAPG는 이러한 선행 연구들과 달리, 고차원 정책 신경망을 끝까지 end-to-end로 학습하면서도 샘플 효율과 안정성을 확보했다는 점에서 차별화됩니다. 또한 시범의 활용 방식에 있어서도 DAPG는 사전 학습 + 학습 중 보조신호라는 두 단계 결합을 명확히 제시하여, 복잡한 행동의 단계별 학습을 가능케 한 점이 특징입니다.

## 결론 및 향후 전망

Rajeswaran 등(2018)의 이 논문은 딥 강화학습과 인간 시범 데이터의 결합이 복잡한 로봇 손 조작 학습에 매우 효과적임을 보여준 사례로서, 이후 많은 관련 연구에 영감을 주었습니다. 저자들은 "DAPG"로 명명된 이 알고리즘이 30배에 달하는 샘플 효율 향상과 정책 품질 개선을 이루었음을 실험으로 입증하였고, 특히 인간 수준의 복잡한 작업들도 RL로 풀 수 있다는 가능성을 열어주었습니다. 이는 로봇 공학 및 강화학습 분야 모두에 의미 있는 성취로, 과거에는 어려움 때문에 시도되지 않던 고차원 다관절 조작 문제를 학습 기반 접근으로 다룰 수 있음을 보여준 것입니다.

물론 한계도 있습니다. 본 연구는 시뮬레이션에서 수행되었으며, 현실 로봇에 직접 적용하기까지는 여전히 넘어야 할 장애물이 있습니다. 예컨대 실제 로봇 손의 물리적 한계, 센서 노이즈, 충돌 처리, 그리고 무엇보다 실시간 학습에서의 안정성 등이 해결되어야 합니다. 그러나 논문 결과에 따르면 DAPG 알고리즘으로는 5시간 이내의 데이터로도 충분히 학습이 가능하므로, 이를 그대로 실제 로봇에 이식한다면 하루 작업으로 정책을 학습시킬 수도 있을 것입니다. 저자들도 결론에서, 시뮬레이션 결과와 샘플 효율 향상을 바탕으로 실제 복잡한 손 조작 학습에 한 발 다가섰다고 밝히고 있으며, 향후에는 실제 하드웨어 상에서 DAPG를 검증하는 것을 목표로 하고 있습니다.

이 논문의 발표 이후, 도메인 랜덤화(domain randomization) 등을 통해 시뮬레이터에서 학습한 손 정책을 실제 로봇에 적용한 사례(예: OpenAI의 Rubik's Cube 해법)나, 진화 전략과 모델 기반 강화학습으로 샘플 효율을 높이는 연구 등 다양한 후속 연구들이 진행되었습니다. 그 중심에는 "어떻게 하면 복잡한 로봇 행동을 현실적으로 학습시킬 것인가?"라는 큰 질문이 있습니다. DAPG는 그 질문에 대해 "우선 인간에게 배워라, 그리고 스스로 향상시켜라"라는 통찰을 준 방법이라 할 수 있습니다. 이는 인간과 로봇의 협력 학습이라는 관점에서도 흥미로운 방향입니다. 앞으로도 시범학습과 강화학습의 조합은 로봇에게 새로운 능력을 가르치는 강력한 수단으로 계속 연구될 것이며, 본 논문은 그 효과를 극적으로 보여준 선구적인 예시로 오래 회자될 것입니다.

**Reference**

- [NPG 논문](https://proceedings.neurips.cc/paper_files/paper/2001/file/4b86abe48d358ecf194c56c69108433e-Paper.pdf)
- [Natural Policy Gradients In Reinforcement Learning Explained](https://arxiv.org/abs/2209.01820)
- [CMU Material 1](https://www.andrew.cmu.edu/course/10-403/slides/S19_lecture13_NaturalPolicyGradients.pdf) + [Material 2](https://www.andrew.cmu.edu/course/10-403/slides/S19_lecture14_multigoalMCTSNPG.pdf)
