---
draft: true
title: "📃VisuoSkin 리뷰"
description: Learning Precise, Contact-Rich Manipulation through Uncalibrated Tactile Skins
date: "2025-05-06"
categories: [paper, tactile]
toc: true
number-sections: true
---


1.  🤖 Visuo-Skin(VISK)은 자기 스킨 센서를 사용하여 시각 정보와 함께 로봇 정책을 학습하는 간단한 프레임워크를 제안합니다.
2.  🦾 VISK는 신용 카드 스와이프, 플러그 삽입, USB 삽입, 책장 검색과 같은 실제 작업에서 시각 전용 정책 및 광학 촉각 감지 기반 정책보다 우수한 성능을 보입니다.
3.  ✔️ 촉각 및 시각 양식을 결합하면 정책 성능과 공간 일반화가 향상되어 작업 전반에서 평균 27.5%의 개선을 달성할 수 있습니다.

---

## Brief Review

이 논문은 시각 정보의 한계로 인해 정밀한 접촉 기반 조작 작업 수행이 어려운 문제를 해결하기 위해 자기 스킨 센서를 활용한 로봇 정책 학습 프레임워크인 VISK(Visuo-Skin)를 제시합니다. VISK는 트랜스포머 기반 정책을 사용하여 스킨 센서 데이터를 시각 정보와 함께 토큰으로 처리하며, 실제 로봇 작업(신용 카드 스와이프, 플러그 삽입, USB 삽입, 책장 검색)에서 시각 전용 및 광학 촉각 센서 기반 정책보다 우수한 성능을 보입니다.

**핵심 방법론:**

1.  **데이터 수집:** VR 기반 텔레오퍼레이션 프레임워크를 통해 실제 xArm 로봇 실험 데이터를 수집합니다. 로봇 그리퍼에 장착된 것을 포함하여 4개의 카메라 뷰에서 시각 데이터를 기록하고, AnySkin 센서의 자기장계 신호 또는 DIGIT 센서의 촉각 이미지를 촉각 데이터로 기록합니다. 정책 학습을 위해 데모에 노이즈를 추가하여 접촉 기반 신호의 다양성을 높입니다.
2.  **정책 아키텍처:** VISK 정책은 BAKU 아키텍처를 기반으로 하며, 이는 여러 카메라 뷰에서 시각적 정책을 학습하는 최첨단 트랜스포머 기반 정책 학습 아키텍처입니다. VISK는 다음과 같은 세 가지 주요 구성 요소를 포함합니다.
    *   **Sensory Encoder:** 카메라의 시각적 입력은 ResNet-18 모델을 수정한 시각적 인코더를 사용하여 인코딩됩니다. AnySkin 센서의 저차원 촉각 입력은 2계층 MLP(Multilayer Perceptron)를 사용하여 인코딩됩니다. DIGIT 센서에서 얻은 촉각 이미지는 시각적 데이터와 동일한 ResNet-18 인코더를 사용하여 인코딩됩니다.
    *   수식: 시각적 입력 $I$에 대해, 인코딩된 특징 $E_v = \text{ResNet-18}(I)$. AnySkin 촉각 입력 $T$에 대해, $E_t = \text{MLP}(T)$.
    *   **Observation Trunk:** 모든 카메라 뷰, 로봇 고유 수용성(proprioception), 촉각 신호에서 인코딩된 입력을 개별 관찰 토큰으로 처리하고 트랜스포머 디코더 네트워크를 통과시킵니다. 학습 가능한 액션 토큰이 관찰 토큰 목록에 추가되고, 액션 특징을 얻는 데 사용됩니다.
    *   수식: 관찰 토큰 $O = [E_{v1}, E_{v2}, E_{v3}, E_{v4}, E_t, A]$, 여기서 $E_{vi}$는 각 카메라 뷰의 인코딩된 특징이고, $A$는 학습 가능한 액션 토큰입니다. 트랜스포머 디코더의 출력은 $F = \text{TransformerDecoder}(O)$.
    *   **Action Head:** 액션 헤드는 관찰 트렁크의 액션 특징을 입력으로 받아 해당 액션을 예측합니다. 평균 제곱 오차 손실을 사용하여 학습된 결정적 액션 헤드를 사용합니다.
    *   수식: 액션 예측 $\hat{a} = \text{ActionHead}(F_A)$, 여기서 $F_A$는 액션 토큰에 해당하는 트랜스포머 디코더의 출력입니다. 손실 함수는 $\mathcal{L} = \frac{1}{N}\sum_{i=1}^{N} ||a_i - \hat{a}_i||^2$, 여기서 $a_i$는 실제 액션이고 $\hat{a}_i$는 예측된 액션입니다.
3.  **실험:** VISK의 효과를 입증하기 위해 실제 xArm 로봇을 사용하여 플러그 삽입, USB 삽입, 신용 카드 스와이프, 책장 검색의 네 가지 정밀 조작 작업에 대한 광범위한 실험을 수행합니다.

**주요 결과:**

*   스킨 기반 촉각 센서를 사용한 VISK로 학습된 정책은 4가지 정밀 조작 작업에서 시각 전용 모델에 비해 평균 27.5%의 절대적인 성능 향상을 보입니다.
*   다양한 양상이 정책 학습에 미치는 영향을 연구합니다. 특히 정밀 조작에 대한 시각적 정책과 시각 촉각 정책의 차이를 분석합니다.
*   AnySkin 촉각 센서를 사용하여 학습된 정책은 광학 촉각 센서(DIGIT)를 사용하는 정책보다 최소 43% 더 나은 성능을 보입니다. 이는 시각 촉각 정책 학습에 있어 스킨 기반 센서의 이점을 강조합니다.
*   학습된 VISK 정책은 공간적 변화에 대한 일반화 능력을 보여줍니다.

이 논문은 VISK가 로봇의 시각 촉각 정책 학습을 발전시키는 데 중요한 진전이며, 저차원 AnySkin 촉각 센서가 정밀 조작 작업에 효과적임을 보여줍니다.


# Detail Review

