---
title: "📃Sparsh 리뷰"
description: Self-supervised touch representations for vision-based tactile sensing
date: "2025-06-02"
categories: [paper, tactile, sdf]
toc: true
number-sections: true
---

1.  Sparsh는 시각 기반 촉각 센서를 위한 자기 지도 학습 기반의 일반적인 촉각 표현 모델을 제안합니다.
2.  본 연구는 힘 추정, 슬립 감지, 자세 추정 등을 포함하는 6가지 촉각 중심 작업으로 구성된 표준 벤치마크인 TacBench도 함께 제시합니다.
3.  평가 결과, 자기 지도 학습으로 사전 학습된 Sparsh는 제한된 라벨 데이터 환경에서 작업/센서별 모델보다 월등히 우수한 성능을 보이며 다양한 작업과 센서에 걸쳐 일반화됨을 입증했습니다.

---

# Brief Review

이 논문은 vision-based tactile sensor를 위한 일반적인 touch representation인 Sparsh와 이를 평가하기 위한 표준화된 benchmark인 TacBench를 제안합니다. 현재 vision-based tactile sensor를 로봇 작업에 통합하는 방식은 대부분 특정 task 및 sensor에 맞춤화된 perception model을 학습시키는 것으로, 이는 비효율적이며 force, slip과 같은 ground truth label을 포함하는 대규모 실세계 data를 수집하기 어렵다는 문제에 직면해 있습니다. Sensor마다 lighting, gel marking 등 특성이 다른 점도 일반화에 어려움을 더합니다.

이러한 한계를 극복하고자, 저자들은 computer vision에서 성공을 거둔 self-supervised learning (SSL)을 tactile sensing domain에 적용했습니다. Sparsh는 다양한 vision-based tactile sensor를 지원하며, pixel 및 latent space에서의 masking과 self-distillation 기법을 사용하여 460k+개의 unlabeled tactile image data로 pre-training됩니다.

**핵심 방법론 (Core Methodology)**

1.  **Dataset 구축 및 활용:** SSL pre-training을 위해 Touch-Slide (저자들이 새로 구축한 DIGIT sensor 데이터)와 기존의 YCB-Slide (DIGIT), Touch-and-Go (GelSight), ObjectFolder (GelSight Mini) dataset을 통합하여 총 약 661k개의 이미지를 수집했습니다. 이 중 70%인 약 462.7k개의 이미지를 SSL pre-training에 사용했으며, 이는 기존 tactile representation 연구보다 훨씬 큰 규모입니다.
2.  **데이터 처리:** Tactile image는 local information 특성이 강하고 sensor별 distractor (marker, lighting) 차이가 크므로, DIGIT와 GelSight Mini (markerless)의 경우 background subtraction을 수행하여 노이즈에 강건한 representation 학습을 유도했습니다. Slip detection 등 temporal reasoning이 필요한 task를 위해, image-based SSL의 경우 현재 시점 image It와 5 sample stride 전 image It-5를 channel 차원으로 concatenate한 $I_t \oplus I_{t-5} \in \mathbb{R}^{h \times w \times 6}$ 이미지를 입력으로 사용했습니다. Video-based SSL (V-JEPA)의 경우 [t, t-2, t-4, t-6] 시점의 4 frame clip을 사용했습니다. 입력 이미지는 Vision Transformer (ViT) 처리를 위해 224x224로 조정되었습니다.
3.  **SSL 알고리즘 적용:** Computer vision 분야의 최신 SSL 알고리즘들을 tactile domain에 맞게 수정하여 Sparsh를 학습시켰습니다. 모든 encoder backbone으로는 ViT-B/14를 사용했습니다.
    *   **Sparsh (MAE):** Masked Autoencoder (MAE) [23] 기반. 입력 이미지의 상당 부분을 masking하고, ViT encoder가 contextual representation을 학습한 후 lightweight decoder가 masked region의 raw pixel을 재구성하도록 학습합니다. Loss는 reconstruction L2 loss인 $L_{MAE} = \|I_{target} - I_{recon}\|_2^2$를 사용합니다.
    *   **Sparsh (DINO/DINOv2):** Self-distillation [48, 49] 기반. Student network가 EMA (Exponential Moving Average) teacher network의 output prediction을 모방하도록 학습합니다. 동일한 입력 data sample에 대해 다른 augmentation을 적용한 후, 각 network의 feature representation을 MLP head를 통과시켜 probability vector를 얻고, 이들 간의 cross-entropy loss ($L_{DINO} = - \sum p_t \log p_s$)를 최소화합니다. Gradient는 teacher network로 역전파되지 않습니다.
    *   **Sparsh (IJEPA/V-JEPA):** Joint-Embedding Predictive Architecture (JEPA) [50, 51] 기반. Masking 기반이지만 pixel space가 아닌 latent representation space에서 예측을 수행합니다. Context network (student)는 masked input에서 얻은 contextual feature를 predictor network를 통해 target network (EMA teacher)의 latent feature를 예측합니다. Loss는 feature space에서의 L2 loss $L_{JEPA} = \sum_{i \in M} \sum_{j \in B_i} \|\hat{s}_{yj} - s_{yj}\|_2^2$를 사용합니다. I-JEPA는 spatial block masking, V-JEPA는 temporal tube masking을 사용합니다.
4.  **TacBench 평가:** TacBench는 tactile properties (Force estimation, Force field visualization, Slip detection), physical perception (Pose estimation, Grasp stability, Textile recognition), manipulation planning (Bead maze)의 6가지 task로 구성됩니다. 평가 프로토콜은 SSL pre-training된 Sparsh encoder의 가중치를 freeze하고, 각 task의 labeled data (전체의 33%-50%만 사용)를 사용하여 task-specific decoder (attentive probe 또는 dense prediction을 위한 DPT decoder)를 학습시키는 방식입니다. Random initialization부터 encoder와 decoder를 모두 학습하는 End-to-End (E2E) baseline과 성능을 비교합니다.

**주요 결과**

TacBench 평가 결과, Sparsh pre-training은 task 및 sensor specific E2E training 대비 평균 95.1% 뛰어난 성능을 보였습니다. 특히 labeled data가 제한적인 환경에서 Sparsh의 이점이 두드러졌습니다.

*   Latent space에서 학습된 Sparsh (DINO)와 Sparsh (IJEPA)가 전반적으로 가장 우수한 성능을 기록하며, tactile image의 경우 pixel space reconstruction (Sparsh (MAE))보다 latent space 학습이 더 효과적임을 시사했습니다.
*   Sparsh는 DIGIT, GelSight 2017, GelSight Mini 등 다양한 sensor와 task에 걸쳐 일반화 성능을 보였습니다.
*   Force estimation ([T1]): Sparsh (DINO)가 적은 labeled data에서도 강건한 성능을 보였으며, Sparsh (DINOv2)가 full dataset에서 가장 낮은 RMSE를 달성했습니다. GelSight Mini에서는 E2E 모델의 성능이 낮았습니다.
*   Slip detection ([T2]): Sparsh (VJEPA)가 temporal perspective를 활용하여 가장 높은 F1 score를 기록했습니다.
*   Pose estimation ([T3]): labeled data가 적을 때 Sparsh가 E2E 대비 유의미하게 높은 정확도를 유지했습니다.
*   Grasp stability ([T4]) 및 Textile recognition ([T5]): Sparsh가 SOTA에 근접하거나 이를 능가하는 분류 성능을 보였습니다. Sparsh (MAE)는 Textile recognition에서 특히 강점을 보였습니다. Sparsh는 소량의 데이터만으로도 cross-sensor transferability를 입증했습니다.
*   Bead maze ([T6]): Sparsh representation을 활용한 policy (Diffusion Policy 기반)가 E2E policy 대비 낮은 trajectory error를 보였고, 실제 로봇 rollout에서도 더 긴 거리를 이동했습니다.

본 연구는 vision-based tactile sensor를 위한 일반 목적의 pre-trained representation 구축 가능성을 보여주며, tactile sensing 커뮤니티의 향후 연구를 위한 표준화된 benchmark를 제공합니다.

# Detail Review

> Sparsh: 자기 지도학습을 활용한 비전 기반 촉각 센싱 논문 리뷰

최근 로봇 공학에서 **촉각 센싱**(touch sensing)은 물체를 조작하고 환경과 상호작용하는 데 필수적인 모달리티로 주목받고 있습니다. 특히 **카메라 기반 촉각 센서**(vision-based tactile sensor)들은 젤 패드(elastomer)의 변형을 카메라로 촬영하여 접촉 지형과 힘 정보를 얻어내며, 시각으로는 얻기 어려운 미세한 접촉 정보를 제공합니다. 그러나 지금까지 이러한 촉각 센서를 활용한 로봇 시스템들은 센서 종류나 수행 작업에 따라 일일이 튜닝된 **맞춤형 모델**에 의존해 왔습니다. 새로운 작업을 수행하거나 새로운 형태의 촉각 센서를 사용할 때마다 대량의 데이터 수집과 레이블링이 필요했는데, 접촉 힘이나 미끄럼(slip) 같은 **접촉 특성**에 대한 정확한 레이블을 대규모로 얻는 것은 매우 어려운 일입니다.

**Sparsh**는 이러한 문제를 해결하기 위해 **자기 지도학습**(Self-Supervised Learning, SSL)을 활용한 **범용 촉각 표현 학습**을 제안한 최신 연구입니다. 이 논문은 다양한 비전 기반 촉각 센서들에 대해 **사전 학습된 공통 표현**을 만들어, 새로운 작업에 대한 레이블 부족 문제를 극복하고자 합니다. 또한 **TacBench**라는 벤치마크를 구축하여, 한 번 학습된 촉각 표현이 어느 정도 범용적으로 여러 과제에 통할 수 있는지 평가할 수 있는 표준 실험 환경을 제공합니다. 본 글에서는 해당 논문의 목표와 기여, 기술적 방법론, 실험 및 결과, 기존 연구와의 비교, 그리고 이 접근법의 장점과 한계를 살펴보고 향후 발전 방향에 대해 논의합니다.

&#x20;**그림 1:** Sparsh의 개요와 TacBench 벤치마크. 왼쪽에는 본 연구에서 다루는 다양한 **비전 기반 촉각 센서**들(DIGIT, GelSight 2017 등)의 예시가, 오른쪽에는 촉각 정보를 활용하는 여섯 가지 다운스트림 과제(T1–T6)가 묘사되어 있다. 가운데 그래프는 각 과제에서 Sparsh로 \*\*자기 지도 사전학습(SSL)\*\*한 모델과 \*\*엔드투엔드(E2E)\*\*로 해당 과제만 학습한 모델의 성능을 비교한 결과로, 제한된 양의 레이블만으로(대부분 33%, T6는 50%)도 Sparsh 사전학습 모델이 전반적으로 우수한 성능을 보임을 보여준다.

## 연구 목표 및 주요 기여

이 논문의 핵심 목표는 **여러 종류의 촉각 센서에 공통으로 적용 가능한 일반적인 촉각 표현**을 학습하는 것입니다. 즉, 한 번 거대한 **촉각 이미지** 데이터로 사전학습한 **백본 신경망**을 갖추면, 개별 작업별로 일일이 센서 특성에 맞춘 모델을 새로 설계하거나 대량의 레이블을 모으지 않아도, 다양한 로봇 조작 과제에 촉각 정보를 활용할 수 있게 하자는 것입니다. 저자들은 이 목표를 달성하기 위해 **자기 지도학습 기법**을 촉각 데이터에 적용하고, 학습된 표현의 효과를 체계적으로 검증할 벤치마크를 구축하였습니다.

논문에서 밝힌 **주요 기여**는 다음 세 가지로 요약됩니다:

* \*\*범용 촉각 표현 모델 (Sparsh)\*\*을 제시하였습니다. 다양한 비전 기반 촉각 센서 (예: DIGIT, GelSight 등)가 생성하는 이미지를 한데 모은 **460,000장 이상의 촉각 이미지** 데이터셋으로 **SSL 사전학습**한 **공통 인코더**를 개발하였습니다. 별도의 작업별 레이블 없이 **마스킹 복원**과 **자기 증류** 등의 자기 지도 학습 기법으로 학습된 이 모델들은 여러 센서에 걸쳐 사용할 수 있는 **일반적인 촉각 특징 표현**을 제공합니다.

* **TacBench 벤치마크**를 구축하였습니다. 이는 다양한 센서로부터 수집된 **6가지 촉각 관련 과제**에 대한 데이터와 평가 프로토콜로 구성된 벤치마크로, 촉각 표현의 성능을 표준화된 방식으로 평가할 수 있습니다. TacBench의 과제들은 **접촉 힘 추정, 미끄럼 감지** 등의 **촉각 물리량 이해 과제**부터 **물체 위치 추적, 그립 안정성 예측, 재질 인식** 등의 **물체 인지 과제**, 그리고 **비드 미로 조작** 같은 **로봇 조작 계획 과제**까지 폭넓게 포함하고 있습니다.

* **대규모 촉각 데이터셋의 수집 및 통합**을 수행하였습니다. 저자들은 기존에 공개된 여러 촉각 센서 데이터셋(YCB-Slide, Touch-and-Go, ObjectFolder 등)에 더해, 자체적으로 **Touch-Slide**라는 새 데이터셋을 수집하여 총 **66만장 이상의 촉각 이미지 데이터**를 확보했습니다. 이 중 \*\*약 46만장(70%)\*\*을 사전학습에 사용하고 나머지는 검증에 사용하였으며, 별도로 TacBench의 각 과제에 맞는 **레이블된 데이터셋**도 구축하여 공개하였습니다. 이러한 노력은 **촉각 표현 학습을 위한 데이터 규모**를 한층 끌어올려, 이전의 어떤 연구보다 **1자리수 이상 많**은 이미지로 모델을 훈련할 수 있게 하였다는 의의를 갖습니다.

요약하면 Sparsh 연구는 **(1)** 다양한 촉각 센서에 일반화되는 대규모 **사전학습 촉각 표현 모델**을 만들고, **(2)** 그 성능을 평가할 수 있는 **표준 벤치마크와 데이터셋**을 제시했으며, **(3)** 이를 통해 **한정된 레이블 데이터로도 높은 성능을 달성**할 수 있음을 보여준 것입니다. 실제로 TacBench 실험 결과, **사전학습한 Sparsh 모델이 작업별로 처음부터 끝까지 학습한 모델보다 평균 95.1% 높은 성능**을 보였으며(레이블 데이터 33\~50% 사용 시), 특히 **잠재 공간 표현**(latent representation)을 학습하는 \*\*Sparsh (DINO)\*\*와 **Sparsh (IJEPA)** 변형이 가장 우수했다고 합니다. 이는 촉각 이미지의 미묘한 변화와 노이즈를 직접 복원하는 **픽셀 공간 학습**보다, **잠재 특징 공간에서의 예측 학습**이 효과적임을 시사합니다.

## 방법론: 자기 지도학습 기반 촉각 표현 학습

Sparsh의 핵심은 \*\*자기 지도학습(SSL)\*\*을 통해 **레이블 없는 촉각 이미지 데이터**에서 유용한 표현을 학습하는 것입니다. 이를 위해 저자들은 **최신 컴퓨터 비전 SSL 기법**들을 촉각 도메인에 맞게 변형하여 활용하였습니다. 전체 프레임워크는 크게 **대규모 촉각 이미지 사전학습 단계**와 **다운스트림 과제 평가 단계**로 나뉩니다. 사전학습 단계에서 하나의 **비전 트랜스포머(ViT)** 인코더가 여러 센서의 촉각 이미지를 입력으로 공통된 잠재 표현을 학습하고, 이후 다운스트림 단계에서는 이 \*\*인코더를 고정(freeze)\*\*시킨 채 각 과제별로 간단한 **디코더**나 \*\*프로브(probe)\*\*를 학습시켜 표현의 품질을 평가합니다.

&#x20;**그림 2:** Sparsh 모델의 자기 지도학습 기법들. (a) 이번 연구에서 통합한 대규모 촉각 이미지 데이터셋의 구성: 새로운 **Touch-Slide**(인간이 장난감 부품을 문지르는 데이터)와 기존 **YCB-Slide**, **Touch-and-Go**, **ObjectFolder** 데이터셋 등으로 총 **약 66만 장**의 촉각 이미지가 사용되었다. 이 중 70%에 해당하는 **약 46만 장**을 자기 지도 사전학습에 활용하였다. (b) **Sparsh (MAE)** – **마스킹 자동인코더(Masked Autoencoder)** 방식: 입력 이미지의 상당 부분(예: 75%)을 무작위로 가린 후, 남은 일부분만을 보고 전체 이미지를 복원하도록 인코더-디코더를 학습시킨다. 인코더는 ViT 기반이며, 마스크된 영역 복원은 간단한 디코더를 통해 수행되고 L2 픽셀 손실로 학습된다. (c) **Sparsh (DINO)** – **자기 증류(self-distillation)** 방식: 동일한 구조의 **학생 네트워크**와 **EMA로 업데이트되는 교사 네트워크** 두 개를 두고, 하나의 촉각 이미지에 서로 다른 변환(crop 등)을 가한 두 입력을 통해 각각 특징을 추출한다. 각 특징을 별도의 MLP 헤드에 통과시켜 임의의 범주 분포(클러스터 확률)를 예측하고, **교사 출력에 학생 출력이 맞춰지도록** 크로스엔트로피 손실로 학습한다. 이렇게 하면 레이블 없이도 학생 네트워크가 교사의 **군집화된 표현**을 모방하며 의미 있는 잠재 표현을 얻게 된다. (d) **Sparsh (I-JEPA)** – **Joint Embedding Predictive Architecture** 방식: 학생(**컨텍스트** 네트워크)과 교사(**타겟** 네트워크)로 구성되며, 교사 네트워크는 EMA로 갱신된다. 학생 네트워크는 이미지의 **일부 영역만 관찰**하여 특징을 만들고 작은 **예측기**(predictor)를 통해 교사 네트워크의 특징을 맞춰보는 작업을 한다. 구체적으로, 학생은 이미지의 상당 부분을 마스킹한 **글로벌 시야**를 가지고, 교사는 이미지의 **국소 패치**들만을 본다. 학생의 컨텍스트 특징을 통해 교사의 지역 특징을 예측하도록 L1 손실로 학습함으로써, **부분 관찰을 기반으로 전체 정보를 추론**하는 능력이 향상된다. Sparsh에는 정적인 이미지에 적용한 I-JEPA뿐 아니라, 연속적인 **비디오 프레임**에 적용한 V-JEPA도 포함되어 있다.

사전학습에 사용된 **모델 아키텍처**는 기본적으로 **ViT-Base (패치 크기 14)** 구성입니다. 입력으로 들어오는 촉각 이미지는 **배경 차감**(background subtraction)을 거치는데, 이는 각 센서에서 **접촉이 없을 때의 기본 영상**을 빼줌으로써 조명이나 마커 패턴 차이 등 센서별 잡음을 줄이기 위함입니다. 이렇게 하면 같은 종류의 센서 내에서는 조금 다른 제조 공정으로 인한 배경 차이도 보정되어, 모델이 **진짜 접촉에 의한 변화에 집중**할 수 있게 됩니다. 또한 촉각 센싱은 시간적인 맥락이 중요한 경우가 많기 때문에, 저자들은 **시계열 정보**를 입력에 반영하였습니다. 구체적으로, 정적인 이미지 기반 SSL 기법들(MAE, DINO 등)에는 **한 센서 프레임에서 약 80ms 간격**으로 떨어진 **두 장의 이미지를 채널 차원으로 붙여서** 한 입력으로 사용하였습니다. 보통 촉각 센서가 60 FPS로 동작함을 감안하여 5프레임 차이를 둔 것으로, 이는 인간이 미끄러짐을 감지하고 힘을 조절하는 반응 시간과 비슷한 수준입니다. 한편 **비디오 기반** SSL 기법(V-JEPA)에는 **4프레임 길이**의 짧은 클립(동영상 조각)을 입력으로 사용하여 **시간에 따른 변화**까지 학습하도록 하였습니다. 이렇게 두 장 또는 여러 장의 연속 이미지를 활용함으로써, 촉각 변화의 **동적 패턴**(예: 미끄러지기 전의 미세한 움직임)을 포착할 수 있도록 한 것이 특징입니다.

SSL 학습은 **무엇을 예측하느냐**에 따라 픽셀 공간 또는 잠재 공간에서 이루어지는데, 앞서 언급했듯이 저자들은 **잠재 공간에서의 학습이 더 효율적**일 것으로 가설을 세웠습니다. 촉각 이미지는 센서의 국소적 접촉만 담고 있어 **한 장면만으로는 모호성**이 존재할 수 있고, 촬영 조건에 따라 **마커나 조명 변화** 같은 방해 요소도 큽니다. 따라서 모든 작은 디테일까지 복원하려는 픽셀 재구성보다는, **더 추상화된 특징에 집중하는 방법**(예: DINO, JEPA)이 이런 불확실성과 잡음을 무시하고 **유용한 패턴**을 학습하는 데 유리할 것이라 본 것입니다. 실제 실험에서도 나중에 보겠지만, **DINO나 I-JEPA처럼 잠재 표현 학습**을 한 모델이 **MAE같이 픽셀 복원을 한 모델보다** 전반적으로 **우수한 다운스트림 성능**을 보였습니다.

Sparsh 사전학습은 **150 에포크** 동안 진행되었으며, **AdamW 옵티마이저**와 코사인 학습률 스케줄 등이 사용되었습니다. 모든 모델은 학습 시 **\[CLS] 토큰을 사용하지 않고**, 패치 임베딩들의 풀링이나 별도 헤드를 통해 학습되었습니다. 특히 DINO의 경우 \[CLS] 토큰 대신 **ViT의 특정 레지스터**를 활용하여 클러스터 확률 출력을 얻는 등, 약간의 구조 변형이 있었습니다. 하이퍼파라미터로는 DINO와 JEPA 방식에서 **EMA 모멘텀 계수**(예: DINO 0.998, IJEPA 0.996)와 **학습률** 등이 조정되었고, 배치 크기는 MAE 100, 나머지 150으로 세팅되었습니다. 최종적으로 학습된 Sparsh 인코더는 약 **0.86억 개 파라미터**(ViT-Base 수준)이며, GPU 기준으로 **100 FPS 이상의 추론 속도**를 보여 **실시간 활용**에도 무리가 없음을 확인하였습니다 (예: Sparsh (DINO) \~112FPS).

## 실험: TacBench 벤치마크 평가

Sparsh의 효과를 검증하기 위해 저자들은 **TacBench**라 불리는 벤치마크를 구축하여 일련의 **다운스트림 과제 실험**을 수행했습니다. TacBench에는 앞서 소개한 대로 **6개의 대표적인 촉각 과제**(T1–T6)가 포함되어 있습니다. 각 과제는 서로 다른 센서와 데이터셋으로 구성되며, 구체적인 내용은 다음과 같습니다:

* **\[T1] 힘 추정 (Force Estimation):** 촉각 센서의 젤에 가해지는 \*\*3축 힘(수직+두 축의 전단력)\*\*을 추정하는 회귀 문제입니다. 저자들이 구축한 **Shear Load** 데이터셋을 사용하며, 로봇 팔에 장착된 촉각 센서(DIGIT 또는 GelSight Mini)로 **반구형, 뾰족한, 평평한** 모양의 인덴터(indenter)를 눌러가며 **동시에 힘 센서 값과 촉각 영상을 기록**하였습니다. DIGIT은 60fps, GelSight Mini는 25fps로 촬영되어 총 **각 7.5만 샘플**의 정렬된 **영상-힘** 데이터가 얻어졌습니다. 평가 지표는 \*\*3축 힘의 RMSE(평균 제곱근 오차)\*\*이며, 학습 시 실제 힘값을 정규화하여 L1 손실로 예측하도록 디코더(작은 MLP)를 학습시켰습니다.

* **\[T1A] 힘장 시각화 (Force Field Visualization):** T1의 부가 실험으로, 촉각 이미지로부터 **젤 표면 전체의 접촉 힘 분포** (정규력 분포 및 전단력 벡터장)를 추정하는 과제입니다. 마커가 있는 센서의 경우 마커 움직임 추적으로 전단 변형장을 얻는 것이 가능하지만, **마커가 없는 센서**에서는 전체 장(field)의 ground truth를 얻기 어려워 이 작업이 잘 수행되어오지 않았습니다. 저자들은 Sparsh 표현을 이용해 **마커 없는 센서의 힘장도 추정**이 qualitatively 가능함을 보였는데, \*\*정규력장은 깊이지도(depth map)\*\*로, **전단력장은 옵틱 플로우** 문제로 프레임 간 변위를 예측하도록 설정하여 **감독없이** 디코더(CNN+DPT 구조)를 학습시켰습니다. 그 결과 Sparsh (DINO) 모델을 활용하면, 접촉 패치의 움직임 방향, 예를 들어 **미끄럼 방향**이나 **비틀림(torsion)**, **접촉 시의 퍼져나가는 형태** 등을 유의미하게 보여주는 전단력 벡터장을 그려낼 수 있었습니다. 이는 \*\*그림 4 (vi)\*\*에 시각화 예시가 제시되어 있습니다.

* **\[T2] 미끄럼 감지 (Slip Detection):** 물체가 손가락에서 **미끄러지는지 여부**를 이진 분류하는 과제입니다. 실험을 위해 T1과 동일한 장비로 반구형 인덴터를 사용하되, 일부 구간에서는 **센서 표면이 미끄러지도록** 힘의 방향을 조절하여 **stick-slip 동작**을 만들어냈습니다. **마찰 원뿔 모델**에 따라, 정적 마찰 한계를 넘은 경우를 **미끄럼 발생**으로 레이블링 하였고 (정적 마찰계수는 실험적으로 추정), 총 **12.5만 샘플** 중 약 \*\*13%\*\*가 미끄럼으로 레이블되었습니다. 데이터 불균형이 있기 때문에 평가에는 **F1-score**를 주로 사용했습니다. 학습 시에는 **미끄럼 유무 분류** MLP와 **힘 변화량(regression)** MLP를 **멀티태스크**로 동시에 학습시켰는데, 힘 변화 (특히 전단력의 변화)가 미끄럼과 밀접한 관련이 있어 **두 값을 함께 예측**하면 성능이 좋아짐을 관찰했기 때문입니다.

* **\[T3] 물체 자세 추적 (Pose Estimation):** 촉각 이미지를 통해 **센서에 접촉한 물체의 상대 자세(SE(2) 변환)** 변화를 추정하는 과제입니다. 이는 손가락과 물체 사이의 **미끄러짐 정도**(평면 이동 및 회전)를 추적하는 것으로 볼 수 있습니다. 데이터셋은 DIGIT 센서로 사람이 물체(예: YCB 물체)를 살짝 미끄러뜨리는 장면을 촬영하고, 동기화된 모션 캡처 시스템으로 물체의 실제 자세 변화를 기록하여 만들었습니다. 총 **4.9만 쌍**의 (이전 이미지, 현재 이미지)와 그 사이의 물체 변환 레이블이 있으며, 학습 시 **분류를 통한 회귀(regression-by-classification)** 방식을 채택했습니다. 즉, 평면 상의 x, y 이동 및 회전 변화를 \*\*연속값 대신 여러 구간으로 양자화(binning)\*\*하여 **다중 클래스 분류** 문제로 변환한 것입니다. 이동은 수백 μm 정도의 해상도로, 회전은 수 도(degree) 단위로 구간화하여, 각 자유도에 대해 별도의 Softmax 출력을 내도록 하였습니다. 평가 지표로는 **분류 정확도**(정답 구간 맞춘 비율)를 사용했습니다.

* **\[T4] 그립 안정성 예측 (Grasp Stability):** 로봇 손가락으로 물체를 잡았을 때 **성공적으로 들었는지**(안정적으로 파지되었는지) 여부를 맞추는 이진 분류 과제입니다. 이는 촉각 연구에서 오랫동안 다뤄진 주제인데, 본 논문에서는 기존 **Feeling of Success** 데이터셋을 활용했습니다. 이 데이터셋은 병 따기용 로봇 손가락에 GelSight 센서를 부착하여 **100여 개 물체**를 잡은 후 성공/실패를 기록한 것으로, 하나의 시도마다 **잡기 전, 잡는 중, 잡은 후**의 **세 장의 촉각 이미지**가 제공됩니다. 총 **약 9300회 시도** 중 성공과 실패 사례가 모두 포함되어 있습니다. 본 연구에서는 한 손가락의 촉각 정보만 사용하므로, 세 개 이미지 중 **잡기 전**과 **잡는 중** 두 이미지를 **히스토리**로 입력으로 주었고, 레이블은 그립 성공 여부(예/아니오)입니다. 데이터셋에 공식 분할이 없어서 임의로 **8000여 개 시도**를 학습에, **1300여 개**를 테스트에 사용하였습니다. 성능 지표는 **분류 정확도**입니다.

* **\[T5] 직물 재질 인식 (Textile Recognition):** 촉각만으로 다양한 **직물의 재질을 분류**하는 과제입니다. 기존 연구에서 정의한 **Clothing Dataset**을 사용하였는데, 이는 GelSight 2017 센서(마커 패턴 있음)로 **가죽, 면, 폴리에스터** 등 **20가지 직물**을 잡았을 때의 촉각 영상이 담긴 **짧은 비디오 클립 4467개**로 이루어져 있습니다. 각 클립은 10\~25프레임 길이이며, 로봇이 옷감을 살짝 쥐었다 놓는 동작 등을 포함합니다. 우리는 제공된 메타데이터의 학습/시험 분할을 따랐습니다. 이 문제는 **20-class 분류**이며 평가 지표는 **정확도**입니다.

* **\[T6] 비드 미로 (Bead Maze) 조작:** 장난감인 **비드 미로**(고리 모양의 구슬을 막대 기둥을 따라 움직이는 퍼즐)를 로봇이 **촉각 피드백으로 풀도록** 하는 **강화 학습적 과제**입니다. 한쪽 끝에 구슬이 걸려 있는 구불구불한 막대 경로를 따라 구슬을 다른 끝까지 움직이는 것이 목표로, 로봇은 구슬을 집은 손가락에 달린 촉각 센서로부터 **저항을 느끼며** 힘을 조절해야 합니다. 시각적으로는 손가락에 가려 잘 안 보이고, 마찰로 인한 미세한 막힘 등을 감지해야 하기 때문에 **촉각에 의존**하는 문제 설정입니다. 저자들은 프랑카(Franka) 로봇팔과 DIGIT 센서를 이용하여 다양한 모양의 미로에서 **50회 데모 시연**을 모았습니다. 절반은 VR 장치를 활용한 원격 조작으로, 절반은 사람의 수동 조작으로 수행되었습니다. 총 **3.4만 개**의 (촉각 이미지 시퀀스, 로봇 관절 명령) 쌍이 데이터로 확보되었고, 이를 이용해 **Diffusion Policy** 라는 최신 Behavior Cloning 알고리즘으로 **정책(policy) 학습**을 합니다. 이때 **시각용으로 설계된 원래 Diffusion Policy의 인코더**를 **Sparsh 사전학습 인코더**로 교체하여, 촉각 관측을 처리하도록 만들었습니다. 학습된 정책은 **현재까지의 촉각 이미지 히스토리**(예: 2\~3장)와 **현재 로봇 관절각**을 입력으로 받아, **다음 관절 각 변화량**을 출력합니다. 평가에서는 **데모와 로봇의 실행 결과 간의 거리 차이**(trajectory error)를 짧은 구간별(3cm 이동당)로 측정하여 누적 오차를 계산하였으며, 최종적으로 **실제 로봇에 정책을 실행**하여 얼마나 먼 거리까지 구슬을 운반하는지도 측정했습니다.

각 과제마다, Sparsh로 **사전학습된 인코더는 동결**하고, 그 위에 작은 **태스크별 디코더/프로브**만 학습시켰습니다. 구체적으로, \[T1]\~\[T5] 대부분의 과제는 **어텐션 풀링 기반 디코더**를 사용했습니다. 이는 Sparsh 인코더의 패치별 출력에 \*\*교차 어텐션(cross-attention)\*\*을 적용해 전역적인 특징을 모은 후, **2계층 MLP**를 통해 최종 예측을 산출하는 소규모 네트워크입니다. 이렇게 하면 사전학습 표현이 얼마나 해당 과제의 정보를 담고 있는지 **선형 분류기 수준**에서 평가할 수 있습니다. 한편 힘 **분포 필드 재구성**(T1A) 같은 **Dense 예측**이 필요한 경우에는, Sparsh 인코더의 중간 특징들을 받아 **DPT(Dense Prediction Transformer)** 디코더로 픽셀 단위 출력을 복원하도록 설계했습니다. \[T6] 비드 미로의 경우는 강화학습 정책 특성상 **end-to-end**로 학습이 이뤄지지만, 비교를 위해 Sparsh 인코더를 고정한 버전과, 처음부터 정책과 인코더를 함께 학습한 버전을 모두 시험했습니다.

이제 **TacBench 실험 결과**를 살펴보겠습니다. 전체적인 결론부터 말하면, **Sparsh 사전학습 표현은 다양한 촉각 과제에서 매우 뛰어난 성능 향상을 보여주었습니다.** 우선 공통적으로, **레이블된 데이터가 적을수록** Sparsh의 효과는 더욱 두드러졌습니다. 그림 1 가운데 그래프에서 볼 수 있듯이, 사전학습 없이 \*\*개별 과제 전용으로 학습한 모델(E2E)\*\*들은 학습 데이터가 충분할 때는 일정 수준 성능을 내지만, 데이터가 줄어들수록 성능이 급격히 떨어졌습니다. 반대로 Sparsh 인코더를 사용한 모델들은 **레이블 1/3만으로도** 준수한 정확도를 유지했고, \*\*10%\*\*나 **1%** 수준의 극소량 레이블로도 어느 정도 유의미한 결과를 냈습니다. 예를 들어 힘 추정의 경우, **Sparsh (DINO)** 모델은 \*\*레이블의 10%\*\*만 써도 **0.1N 이하의 오차**를 유지했으며, 미끄럼 감지도 1% 데이터로 학습해도 F1-score가 꽤 유의미하게 나왔습니다. 반면 E2E 모델은 33% 이하에서는 아예 **출력이 한쪽으로 치우치는 등** 제대로 학습되지 않는 현상이 관찰되었습니다. 이러한 경향은 **정량적인 평균 지표**로도 확인되는데, 저자들이 명시하였듯이 **Sparsh 모델들은 레이블 33% 조건에서 E2E 대비 평균 95.1% 성능 향상**을 이루었습니다. 다시 말해 레이블이 부족할 때 **두 배 가까운 성능** 격차를 낸 것입니다. 심지어 **모든 과제가 레이블 충분 조건**(Full data)에서도 Sparsh가 E2E보다 대체로 우수했는데, 이는 사전학습 표현이 **학습 효율**뿐 아니라 **일반화 성능** 측면에서도 이점이 있음을 시사합니다.

각 과제별 **상세 결과**를 요약하면 다음과 같습니다:

* **힘 추정 \[T1]:** Sparsh 표현을 이용하면 **정규화된 힘 추정 오차**를 매우 낮게 유지할 수 있었습니다. 특히 **GelSight Mini** 센서의 경우 해상도가 높고 배경 대비 접촉 영역이 작아서 E2E 모델은 충분한 데이터가 있어도 과적합 등의 문제로 정확도가 낮았지만, Sparsh 인코더를 쓴 모델은 **안정적으로 작은 RMSE**를 기록했습니다. **DIGIT**의 경우는 데이터가 충분하면 E2E도 어느 정도 성능을 냈으나, 레이블이 적을 때는 Sparsh (특히 DINO 변형)이 훨씬 견고하게 낮은 오류를 보였습니다. Sparsh (DINO)는 **소량의 학습데이터로도** 강건하게 힘을 예측하여, **레이블 부족 상황**에서 두드러진 성능을 발휘했습니다.

* **힘장 시각화 \[T1A]:** 정량적 평가가 어려운 과제이지만, Sparsh 표현을 사용해 추정한 **전단력 벡터장**은 **마커 없는 GelSight 센서**에서도 접촉 패치의 움직임 방향을 잘 나타내주었습니다. 예컨대 **미끄러지는 방향**으로 화살표가 일정하게 그려진다든지, **비틀리는 경우 회전형 패턴**이 나타난다든지, **새로운 접촉이 일어날 때 퍼져나가는 형태**가 보이는 등, 사람의 촉각적 직관과 맞아떨어지는 시각화를 얻었습니다. 이는 기존에 마커가 없으면 얻기 힘들었던 정보를 **사전학습 표현**으로 보완한 흥미로운 결과입니다.

* **미끄럼 감지 \[T2]:** **불균형 데이터**(미끄럼 적음)와 **센서 노이즈**로 인해 DIGIT 센서에서는 특히 어려운 문제였지만, Sparsh의 **V-JEPA 변형**이 가장 뛰어난 성능(F1-score)을 달성했습니다. Sparsh (VJEPA)는 **비디오 클립 입력**을 활용해 시간에 따른 변화를 직접 학습한 덕분에, **정적인 방법들보다 미끄럼 징후 포착에 유리**했습니다. 그 결과 DIGIT처럼 표면에 마커 패턴이 없어 전단력 방향 파악이 어려운 센서에서도, 미끄럼 여부를 훨씬 정확히 분류해냈습니다. 특히 **학습데이터의 50% 이하**만 사용할 경우 Sparsh 모델과 E2E 사이 성능 격차가 크게 벌어졌는데, Sparsh (VJEPA)는 **50% 데이터로 학습해도** E2E의 **100% 데이터 성능을 능가**할 정도였습니다.

* **자세 추정 \[T3]:** 이 과제에서는 **클래스가 촘촘하게 양자화**되어 있어, 적은 데이터로 학습하면 인접한 각도나 이동 구간을 구별하기 어려워집니다. 실제로 E2E 모델은 **레이블이 줄어들수록** 혼동이 심해져, 예컨대 0도와 5도의 회전 차이도 맞히지 못하고 엉뚱한 큰 값만 예측하는 경향(극단값으로 수렴)이 나타났습니다. 반면 Sparsh 인코더 기반 모델은 **1/3 수준의 데이터**로도 꽤 높은 정확도를 유지했고, 적은 데이터에서 **극단값으로 치우치는 현상**도 덜했습니다. 충분한 데이터가 있을 때는 E2E와 큰 차이가 없었지만, **저데이터 시나리오**에서 Sparsh의 이점이 두드러져, **미세한 각도 차이**까지 비교적 잘 예측하였습니다.

* **그립 안정성 \[T4]:** **학습 데이터가 충분**한 (8000개 이상) 경우에는 Sparsh나 E2E나 모두 **유사한 높은 정확도**를 보였습니다. Sparsh (IJEPA)와 Sparsh (VJEPA)는 약 **90%에 가까운 정확도**를 달성하여, 시각+촉각 멀티모달 정보를 함께 쓴 이전 연구보다도 오히려 **높은 성능**을 나타냈습니다. 주목할 점은 여기서 Sparsh는 **한 손가락의 촉각 정보만** 사용하고도, 과거에 시각까지 동원한 모델을 앞질렀다는 것입니다. 또한 Sparsh 기반 모델은 \*\*데이터 33%\*\*로 학습해도 큰 성능 저하 없이 80% 이상 정확도를 유지했고, \*\*10% (800개 미만)\*\*만으로도 70%대 수준을 보여 **실용적인 성능**을 발휘했습니다. 그러나 **80개 (1%) 정도**로 극단적으로 줄이면 확연히 떨어져서, 저자들은 이 경우엔 무작위 추측 수준으로 전락함을 관찰했습니다. 그럼에도 불구하고 전반적으로 Sparsh 표현은 **촉각만으로 그립 성공 여부를 신뢰성 있게 예측**할 수 있음을 검증한 셈입니다.

* **직물 인식 \[T5]:** 이 과제는 **20-way 분류**로 난이도가 높고, 기존 연구에서도 **E2E 학습이 어렵다**고 보고된 바 있습니다. 본 연구 결과 역시 E2E로는 좋은 성능을 내기 힘들었지만, **사전학습 표현을 사용하자 성능이 크게 향상**되었습니다. 특히 **Sparsh (MAE)** 모델이 두드러졌는데, **픽셀 수준의 질감 특징**을 복원하도록 학습된 덕분에 촉감으로 재질을 구분하는 데 유리했던 것으로 보입니다. Sparsh (MAE)를 사용하면 **전체 데이터로 학습할 때** 정확도가 크게 높아졌을 뿐 아니라, **10% 이하의 데이터**로도 E2E의 풀데이터 성능에 맞먹는 결과를 얻었습니다. 흥미로운 추가 실험으로, 저자들은 **센서 간 전이 학습**을 시도했는데, 예컨대 GelSight로 사전학습한 Sparsh를 이용해 **DIGIT 센서의 직물 인식**을 몇 개 샘플로 파인튜닝하면 금방 적응한다는 것을 보였습니다. 단 **10-shot**(직물별 10장) 정도의 소량 학습으로도 새로운 센서에 맞게 분류기가 조정되어, **사전학습 표현의 범용성**을 확인시켜주었습니다.

* **비드 미로 \[T6]:** 강화학습(행동 클로닝) 문제에서도 Sparsh 표현이 도움을 주는지 검증했습니다. 우선 **시뮬레이션 환경**(데모 경로 추종)에서의 결과를 보면, \*\*Sparsh (DINO)\*\*와 **Sparsh (IJEPA)** 기반 정책이 E2E 정책보다 **궤적 오차**가 약 **16% 적게**나타났습니다. 같은 데이터로 학습해도 사전학습 인코더를 쓴 쪽이 훨씬 **데모를 정확히 재현**한다는 의미입니다. 또 학습에 쓰인 **데모 개수를 줄여보는 실험**에서도 Sparsh 모델은 적은 시연으로도 괜찮은 정책을 학습했지만, E2E는 데모가 줄면 성능 급하락을 보여 **표현 학습의 데이터 효율성**을 재확인했습니다. 마지막으로 **실제 로봇에 정책을 실행**하여 테스트한 결과, 모든 모델이 **미로 완주는 실패**했지만 Sparsh 기반 정책들이 E2E보다 **20\~53% 더 먼 거리**까지 구슬을 움직이는 데 성공했습니다. 즉, 완전한 성공률 측면에서는 한계가 있었으나, 사전학습 표현이 **부분적인 성능 향상**과 **안정적인 조작**에 기여함을 알 수 있습니다. 다만 실제 로봇 실험에서는, 시뮬레이션과 달리 로봇의 미세한 제어 한계, 한번 미끄러지면 재잡기 어려운 물리적 상황 등이 작용하여 사전학습 효과가 완전히 발휘되지 못했습니다. 그럼에도 Sparsh를 사용한 정책이 전반적으로 더 나은 결과를 보였다는 점에서, **촉각 표현 학습이 로봇 정책 학습에도 유용**할 수 있다는 가능성을 보여주었습니다.

以上의 실험들을 종합하면, Sparsh로 학습된 촉각 표현은 **여러 센서**와 **여러 과제**를 아우르며 성능을 향상시킵니다. 특히 레이블이 제한적인 현실에서, 사전학습 모델을 쓰면 훨씬 **안정적이고 높은 성능**을 얻을 수 있음을 확인했습니다. 또한 Sparsh의 다양한 변형들 중 **DINO 기반 모델**은 **힘/자세 추정** 같은 **물리량 회귀 과제**에 강했고, **I-JEPA 기반 모델**은 **미끄럼, 안정성, 재질 분류** 같은 **의미론적 분류 과제**에 좀 더 성능이 우수한 경향을 보였습니다. 이는 **픽셀 복원 vs 특징 예측**의 차이뿐 아니라, **학습 목표에 따른 특화 효과**도 일부 있음을 시사합니다. 재미있게도, 두 모델은 **비드 미로 정책**처럼 **복합적인 과제**에서는 비슷한 성능을 냈는데, 이는 그 과제가 **힘과 미끄럼에 대한 종합적 이해**를 요구하기 때문일 것입니다.

## 기존 연구와의 비교

Sparsh 이전의 **비전 기반 촉각 인지** 연구들은 주로 특정 작업에 초점을 맞춰 개별적인 모델을 개발하곤 했습니다. 초창기 접근법으로는, 촉각 이미지를 해석하기 위해 **마커 추적**이나 **물리 시뮬레이션(FEM)** 등을 활용하여 접촉 지형과 힘을 계산하는 방식이 있었습니다. 그러나 이러한 방식은 센서마다 **마커 패턴 보정**이 필요하거나 계산 비용이 크다는 한계가 있었습니다. 이후 **학습 기반 기법**들이 등장하면서, **개인별로 커스텀 디자인한 신경망**을 사용하여 촉각 정보를 처리하는 시도가 많았습니다. 예를 들어, 어떤 연구에서는 **특정 촉각 센서**에 맞춘 CNN 인코더를 설계해 **종이 질감 분류**를 학습하거나, **BioTac**이나 **GelSight** 등의 센서 데이터를 **전이학습**하여 패브릭의 여러 성분(실, 질감 등)을 알아내는 작업을 하기도 했습니다. 심지어 최근에는 **XELA**라는 센서의 출력을 **최근접 이웃 비교**만으로 조작에 활용한 사례도 있었는데, 이는 촉각 표현을 명시적으로 학습하지 않고 임베딩 공간에서 비슷한 촉각을 찾아내는 방식이었습니다. 하지만 이렇듯 **파편화된 접근**들로 인해, 서로 다른 연구 간에 결과를 비교하거나 **표준 모델**을 공유하기가 어려웠습니다. **표준화된 사전학습 모델**이 없었기 때문에, 매번 새로운 문제나 센서가 나오면 다시 데이터 수집에서 모델 디자인까지 처음부터 해야 하는 비효율이 존재했습니다.

**자기 지도 표현 학습**에 대한 관심도 점차 높아져, Sparsh 이전에도 몇 가지 관련 연구가 등장했습니다. 한 예로, **MAE 기법을 촉각에 적용**하여 재질 분류 등에 효과적임을 보인 초기 실험이 있었고, \*\*다양한 촉각 센서(BioTac 등)\*\*에 대해 **사전학습 후 미세조정**(fine-tuning)을 하여 성능을 높인 시도도 있었습니다. 그러나 이들 역시 **단일 센서** 혹은 **한정된 범위의 작업**에 초점을 맞추고 있어 일반성이 부족했습니다.

한편, **멀티모달 학습** 측면에서는 **시각-촉각** 간의 연관성을 학습하려는 노력들도 있어 왔습니다. 예를 들어, 동일한 물체를 만졌을 때의 촉각 이미지와 시각 이미지 쌍을 모아 \*\*대조 학습(contrastive learning)\*\*으로 **공통 임베딩 공간**에 매핑하거나, 촉각 정보로 시각적 스타일을 변환하는 등의 실험이 이루어졌습니다. 이러한 연구들은 **재질 분류**나 **그립 성공 예측** 등에 있어서 **촉각+시각 결합**의 가능성을 보여주었지만, 주로 **표면의 질감이나 물체의 시각적 특징**에 집중되었습니다. 즉, **힘, 미끄럼, 자세 변화**와 같은 **물리적 접촉 특성**까지 다루지는 못하였습니다. 이러한 부분은 여전히 멀티모달 접근법의 도전 과제로 남아 있습니다.

Sparsh와 **가장 유사하거나 병행된 연구**로 논문에서 언급한 것은 **T3**와 **UniT**라는 두 가지입니다. **T3**는 여러 촉각 센서에 대해 **공유 인코더**(shared trunk)를 학습하되, 센서별로 \*\*개별 가지(branch)\*\*를 둔 구조입니다. MAE 방식의 복원 과제와 함께 몇 가지 **레이블된 작업**을 혼합해 학습한 점이 특징이지만, 결국 **센서마다 별도 인코더 파라미터**를 유지해야 하므로 완전히 하나의 통합 모델이라고 보긴 어렵습니다. 또한 T3는 제한된 데이터로 학습되어 Sparsh만큼 광범위한 검증은 이루어지지 않은 것으로 보입니다. **UniT**는 GelSight Mini (마커 있음) 센서를 대상으로 한 연구로, **VQ-GAN**을 활용한 생성모델 기반 표현 학습을 시도했습니다. 이는 **패치 단위로 이미지를 압축**하며 디스크리미네이터로 훈련하는 방식인데, 특정 센서 한 종류에만 국한되어 있고 주로 정적 특성에 초점을 맞추었습니다.

이들에 비해 Sparsh의 차별점은 분명합니다. **첫째**, Sparsh는 **여러 대표적인 촉각 센서**(DIGIT 계열, GelSight 계열 등)의 데이터를 모두 활용하여 **하나의 공용 모델**로 학습했다는 점입니다. 이로써 **센서 교체나 추가**에 대한 **일반화 가능성**을 처음으로 본격 탐구했다는 의의가 있습니다. **둘째**, 최신 **SSL 알고리즘**들을 적극 도입하여, **픽셀 복원(MAE)**, **지식 증류(DINO)**, **잠재 예측(JEPA)** 등 다양한 패러다임을 비교·분석했습니다. 이는 이전 연구들이 주로 하나의 SSL 기법만 시험한 데 반해, **어떤 자기 지도 방식이 촉각에 가장 적합한가**라는 중요한 질문에 답을 제시하려 한 것입니다. **셋째**, Sparsh는 단순 모델 제시를 넘어 **TacBench라는 벤치마크**를 구축함으로써, **촉각 표현 학습의 평가 표준**을 마련했습니다. 이 벤치마크에는 물리적, 인지적, 조작 측면을 망라한 과제들이 포함되어, 앞으로 다른 연구들과 **직접 성능 비교**를 할 수 있는 장을 열었다는 점에서 큰 의미가 있습니다. **넷째**, **데이터 규모** 측면에서도 Sparsh는 이전보다 **압도적으로 큰** 데이터로 학습되었고, 이를 통해 **모델 용량을 키우고도** 과적합 없이 학습이 가능함을 보여주었습니다. 요컨대 Sparsh는 \*\*“촉각계의 Foundation Model”\*\*을 지향한 최초의 본격 시도로서, 이전 개별 연구들의 한계를 한 단계 뛰어넘은 통합적 접근이라 평가할 수 있습니다.

## 장점과 한계

**Sparsh의 성공 요인과 강점**을 정리하면 다음과 같습니다:

* **레이블 프리(self-supervised) 학습의 이점:** 거대한 **비지도 데이터**로 학습했기에, 기존에 힘들었던 **접촉 현상의 희귀 패턴**까지 모델이 학습할 수 있었습니다. 예컨대 **미끄럼**이나 **비틀림** 같은 이벤트는 레이블링하기 어렵고 드물지만, SSL로 방대한 데이터를 학습하면서 이런 특징까지 **잠재적으로 포착**하게 되었습니다. 그 결과 일반적인 E2E 모델이 놓치기 쉬운 부분에서도 Sparsh 표현은 의미 있는 신호를 내재화하고 있어, **적은 레이블로도 높은 성능**을 내는 밑바탕이 되었습니다.

* **범용성 (Generalization):** Sparsh 인코더 하나로 **여러 센서**와 **여러 과제**를 커버할 수 있다는 점이 입증되었습니다. 실제 TacBench 과제들을 보면, **센서 종류**(DIGIT vs GelSight 등)나 **출력 형태**(회귀 vs 분류 vs 강화학습)가 제각각인데, 동일한 Sparsh 표현이 모두에 적용되어 좋은 결과를 냈습니다. 이는 로봇 시스템에서 센서를 교체하거나 새로운 작업에 투입할 때 **기존 모델을 재사용**할 수 있는 길을 열어줍니다. 예를 들어, Sparsh로 학습된 표현을 쓰면 **새로운 GelSight 센서**를 장착하거나 **새로운 물체 잡기 과제**가 주어져도, 일부터 학습하지 않고 **10-shot 정도의 경미한 추가 학습**만으로 적응이 가능함을 보였습니다.

* **잠재 표현 학습의 효과:** Sparsh (DINO)와 (IJEPA)가 MAE보다 좋았듯이, **노이즈와 센서 특수성**을 걸러낸 **잠재 공간 표현**이 촉각 문제에 유리함이 확인되었습니다. 이는 촉각 센싱의 현실적인 문제, 즉 **조명 변화**, **카메라 잡음**, **마커 패턴** 등의 요소를 추상화하여, **핵심 접촉 정보에 집중**할 수 있게 해줍니다. 그 결과 DIGIT이나 GelSight Mini같이 **마커가 없는 센서**에서도 전에는 풀기 어려웠던 **전단력 분포 추정**이나 **미끄럼 감지** 등이 Sparsh 표현으로는 가능해졌습니다.

* **실시간성과 활용 가능성:** ViT-Base 수준의 Sparsh 모델은 고성능 GPU에서 **100 FPS 이상**의 추론 속도를 보였습니다. 이는 로봇 제어에 충분히 투입할 만한 속도이며, 실제로 비드 미로 과제에서도 **실시간 촉각 피드백**으로 정책을 실행할 수 있음을 시연했습니다. 따라서 Sparsh는 **실험실 프로토타입**을 넘어 **실시간 로봇 제어**에 투입 가능한 준비된 모델이라고 할 수 있습니다.

* **표준 벤치마크 제공:** TacBench의 등장은 촉각 연구 커뮤니티에 크게 기여하는 점입니다. 이제 연구자들은 새로운 촉각 표현 모델이 나오면 이 **6개 과제에서의 성능 지표**로 비교 평가할 수 있습니다. 논문 저자들도 TacBench를 \*\*“초기 벤치마크”\*\*로 칭하며, 향후 과제들을 더 추가하거나 데이터셋을 확장함으로써 지속적으로 발전시킬 수 있다고 언급했습니다. 이런 표준화된 플랫폼은 **공동 발전**을 가속할 것입니다.

물론 **한계나 개선점**도 존재합니다:

* **실제 로봇 응용에서의 과제:** Sparsh 표현을 사용한 정책도 **복잡한 실제 조작 문제**(비드 미로 완주 등)에서는 아직 완벽한 성과를 내지 못했습니다. 이는 꼭 Sparsh의 잘못이라기보다, **행동 복제 방식의 한계**와 **환경적 요인** 때문입니다. 실제 로봇에서는 **미세한 힘 제어**의 어려움, **오차가 누적되면 복구 불가** 등의 문제가 있어, 사전학습 표현이 있어도 **추가적인 대책**이 필요함이 드러났습니다. 이는 향후 **강화학습과의 연계**나 **실시간 보정 기법** 연구가 필요함을 시사합니다.

* **데이터의 편중:** 이번 연구에 사용된 공개 촉각 데이터들은 주로 **불연속적인 접촉**(물체를 뗐다 붙였다 하는) 위주입니다. **지속적으로 미끄러지는 상호작용**이나 **충격/진동** 같은 역동적 데이터는 부족한 편입니다. 저자들도 **전단이 풍부한 데이터**를 추가로 모은다면 표현 학습이 더 좋아질 것이라고 예상했습니다. 따라서 현 단계 Sparsh는 **저속, 준정적 접촉 상황**에 최적화되어 있을 수 있으며, **고속 충돌이나 진동 감지** 등에는 성능이 검증되지 않았습니다.

* **히스토리 길드에 대한 미실험:** Sparsh는 입력으로 두 프레임(또는 4프레임)을 사용하는 것으로 고정하였는데, 이 **이력 길이**를 늘리거나 줄이는 것에 대한 실험이 없습니다. 더 긴 시간 창의 입력이 잠재표현에 유리할지, 혹은 불필요한 잡음을 줄지에 대한 분석이 없어서, 현 설정이 최적인지는 추가 연구가 필요합니다.

* **타 센서 유형에의 일반화:** Sparsh는 **비전 기반 촉각 센서**들만 다룹니다. 압력 분포를 이미지로 변환하지 않는 **다른 종류의 촉각 센서**(예: 바이오태크의 전도도 변화, 힘 토크 센서 등)에 대해서는 이 접근법이 바로 통하지 않을 수 있습니다. 물론 **비전 기반**이라는 동일 원리에 속하는 센서들 사이에서는 일반화가 잘 되었지만, 그 외 방식까지 포괄하는 **멀티모달 촉각 표현**으로 확장하려면 추가 연구가 필요합니다.

* **대규모 자원 필요:** 장점으로 꼽았던 사전학습이란 과정은, 거꾸로 보면 **상당한 계산 자원과 데이터 수집 노력이 필요한 작업**입니다. 본 연구는 Meta 및 대학 협력을 통해 데이터를 모으고 수 주간 대용량 모델을 학습시켰는데, 모든 연구팀이 이를 수행하기는 어렵습니다. 다행히 **학습된 가중치 공개**(프로젝트 깃허브 등)가 이루어진다면 이를 활용하면 되겠지만, 향후 더 큰 모델들은 더 많은 데이터/연산이 필요할 것이기에 **규모의 경제**를 누릴 수 있는 기관 중심으로 연구가 진행될 우려도 있습니다.

* **부분 파인튜닝 효과 미미:** 본문에서는 다루지 않았지만, 부록에서 Sparsh 인코더를 **부분적으로 미세조정**(fine-tuning)해도 성능 향상은 크지 않고 거의 고정된 상태와 비슷했다고 합니다. **전체 파인튜닝**하면 더 성능이 좋아졌지만 특히 latent기반 모델이 그렇다는 것이고, **부분**(예: 마지막 몇 계층만)만 훈련하는 건 큰 의미가 없었는데요. 이는 Sparsh 표현이 이미 **충분히 일반적 특성**을 뽑아내 주기 때문일 수도 있고, 반대로 보면 **표현에 잠재된 한계**가 있어서 더 튜닝해도 극복하기 어렵다는 의미일 수도 있습니다. 이 부분은 추후 연구로 사전학습 표현을 **적절히 보완/미세조정하는 기법**이 개발될 여지가 있습니다.

## 향후 전망 및 응용

Sparsh 연구는 **촉각 센싱 분야의 새로운 장을 여는 출발점**으로 볼 수 있습니다. 저자들도 \*\*“시작에 불과”\*\*하다는 투으로 앞으로의 발전 방향을 제시하고 있습니다. 우선, **더 큰 규모의 촉각 데이터셋** 구축이 주요 과제로 언급됩니다. 현재까지는 연구팀이 자체 수집하거나 공개된 데이터를 모았지만, **전세계적으로 촉각 데이터를 모으는 노력**이 결집된다면 컴퓨터 비전이나 NLP처럼 **거대하고 다양한 데이터**로 모델을 학습시킬 수 있을 것입니다. 특히 다양한 **센서**(예: 새로운 저가 촉각센서, 질감 센서 등)와 다양한 **환경**(산업용, 의료용 등)의 데이터를 포함시키면 Sparsh의 후속 모델은 더욱 범용적으로 진화할 것입니다.

\*\*모델의 스케일 업(scale-up)\*\*도 기대해볼 수 있습니다. Vision Transformer 기반의 Sparsh는 Base 수준이지만, 향후 **ViT-Large나 Hybrid 모델**로 키운다면 더 정교한 표현을 얻을 수 있을 것입니다. 물론 데이터도 비례해서 늘어나야 하겠지만, **컴퓨터 비전 분야의 경험상** 모델이 커질수록 성능이 올라가는 추세가 촉각에도 적용될 가능성이 높습니다. 이러한 **대형 사전학습 촉각 모델**은 진정한 의미의 **촉각 Foundation Model**로 자리매김하여, 사람 손의 촉각처럼 범용적인 지능을 보여줄지도 모릅니다.

또 하나의 방향은 **다중 모달 통합**입니다. 앞서 비교한대로 Sparsh는 오직 촉각 모달리티만 사용했는데, 이후에는 **시각+촉각** 공동 학습이나 더 나아가 **언어**까지 결합한 멀티모달 학습으로 확장할 수 있습니다. 예를 들어, 동일한 상황에서 촉각과 시각 정보를 모두 사전학습하여 **공용 임베딩**을 만들면, 로봇이 보거나 만지는 것에 대해 **일관된 표현 공간**에서 이해할 수 있게 될 것입니다. 이는 인간이 **보고 느끼는 감각을 결합**하여 물체를 인지하는 방식과도 유사합니다. 실제로 이전 연구들에서 멀티모달 시도가 있던 만큼, Sparsh를 기반으로 그런 방향을 모색한다면 **촉각-시각 동시 활용 작업**(예: 물체 식별, 재질 탐색)에 혁신을 가져올 수 있습니다.

**강화학습 및 로봇 제어** 측면에서는, **표현 학습과 정책 학습의 접목**이 중요할 것입니다. Sparsh는 Behavior Cloning 예제로 비드 미로를 시도했지만, 앞으로는 **모델 기반 강화학습**이나 **온-정책(on-policy) 방법**에서 사전학습 표현을 활용하는 연구가 필요합니다. 특히 **실시간 상호작용** 중에 표현을 계속 적응시켜나가는 **표현 강강학습** 같은 개념도 생각해볼 수 있습니다. 이는 Sparsh 표현을 **동적으로 업데이트**하거나, 혹은 **프로브 네트워크에 피드백**을 주는 방식으로 이루어질 수 있을 것입니다. 궁극적으로는, 사전학습된 표현을 써서 **학습 효율을 높이면서도**, **실제 환경에서의 피드백**으로 지속 개선하는 방향이 바람직할 것입니다.

**응용 분야**를 살펴보면, Sparsh는 **다양한 산업 및 연구 도메인**에 파급효과를 가져올 수 있습니다. 예를 들어:

* **정밀 조립 및 삽입 작업:** 촉각 센서를 이용한 미세 조립(기계 부품 끼우기 등)에서는 힘과 위치를 섬세히 감지해야 합니다. Sparsh 표현이 있으면 **작은 접촉 변화**를 놓치지 않고도, 다양한 부품에 대한 **범용 조립 모듈**을 구축할 수 있을 것입니다.

* **의료 로봇 및 의수(의족):** 사람의 촉각을 대체하거나 보조하는 의수/의족에 Sparsh 같은 모델이 들어가면, 물건을 쥘 때 미끄러짐을 자동으로 감지해 힘을 조절하거나, 촉각으로 **물체의 재질을 식별**하여 사용자에게 피드백을 줄 수도 있습니다. 이는 촉각이 결여된 로봇이나 보조장치에 **사람 비슷한 촉감 능력**을 부여하는 방향입니다.

* **재료 및 품질 검사:** 촉각 센싱은 표면의 거칠기나 소재 특성을 파악하는 데 쓰일 수 있습니다. Sparsh 모델로 학습된 임베딩 공간은 **질감이나 마찰계수 등의 정보**를 내포하고 있으므로, 산업 공정에서 제품의 표면 품질 검사, 직물이나 종이의 분류 작업 등에 바로 활용될 수 있을 것입니다.

* **연구 플랫폼으로서의 활용:** TacBench의 공개로, 이제 연구자들은 새로운 촉각 센서만 개발되면 Sparsh에 **추가 학습**시키고 동일한 TacBench에서 시험해볼 수 있습니다. 이는 **새로운 센서 아키텍처 평가**에도 표준을 제공하여, 어떤 센서 설계가 더 다양한 촉각 정보를 잘 담아내는지 비교할 수도 있게 됩니다. 즉, Sparsh는 **촉각 센서 하드웨어 연구**에도 정량적 평가 도구로 기여할 수 있습니다.

마지막으로, Sparsh 연구진은 논문을 통해 \*“Sparsh는 범용 촉각 백본을 향한 중요한 발걸음이다”\*라고 강조합니다. 그리고 \*“우리의 목표는 더 큰 촉각 데이터셋과 더 큰 SSL 백본을 활용하는 것”\*이라고 밝히고 있습니다. TacBench는 시작일 뿐이며, **부족한 부분(예: 데이터 다양성, 시간 창 길이 등)은 향후 보완**하고, 예컨대 **액체를 따를 때 질량 변화 추적** 같은 더 흥미로운 촉각 과제도 추가할 수 있다고 제안합니다. 이러한 청사진을 볼 때, 앞으로 **촉각 연구 커뮤니티의 협업** 하에 Sparsh를 잇는 더 발전된 모델들이 나올 것으로 기대됩니다. \*\*시각과 언어에 이은 “촉각의 GPT”\*\*가 등장하는 날도 머지않을지 모릅니다. Sparsh는 그 첫걸음을 내딛었으며, 향후 로봇에게 **인간 수준의 촉각 지능**을 심어주는 여정에 중요한 이정표가 될 것입니다.
