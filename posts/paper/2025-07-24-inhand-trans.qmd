---
title: "📃In-Hand Translation Learning 리뷰"
date: 2025-07-24
categories: [rl, tactile, shear-force]
toc: true
number-sections: true
description: Learning In-Hand Translation Using Tactile Skin With Shear and Normal Force Sensing
---


- [Paper Link](https://arxiv.org/abs/2407.07885)
- [Project Link](https://jessicayin.github.io/tactile-skin-rl/)

1. 3축 촉각 피부 모델을 통해 이론적 시뮬레이션과 실제 환경간의 제로샷 전이 가능하게 함
2. 촉각 및 고유감각 피드백을 활용하여 다양한 물체 및 손의 동적 변화에 적응하는 RL 정책 학습
3. 190회 이상의 실제 환경에서의 평가를 통해 3축 촉각 정책이 기존 베이스라인에 비해 우수한 성능 달성


<center>
<img src="../../images/2025-07-24-inhand-trans/1.png" width="80%" />
</center>

---

# Brief Review


본 논문은 강화 학습(RL)과 촉각 감지 분야의 발전을 활용하여 로봇의 능숙한 손안 조작(in-hand manipulation)을 향상시키는 데 중점을 둡니다. 특히, 촉각 시뮬레이션과 실제 환경 간의 격차로 인해 발생하는 문제, 즉 시뮬레이션의 단순화된 촉각 신호 사용 문제를 해결합니다. 이를 위해 전단력과 법선력을 모두 감지하는 촉각 피부를 위한 새로운 센서 모델을 제안하며, 이 모델은 3축 촉각 신호의 zero-shot 시뮬레이션-실제 전이(sim-to-real transfer)를 가능하게 합니다.

**핵심 기여:**

*   **RL-학습 가능한 촉각 피부 센서 모델:** 바이너리 법선 신호와 3진수 전단 신호를 제로샷 시뮬레이션-실제 전이할 수 있는, 순응성 촉각 피부(compliant tactile skin)를 위한 최초의 RL-학습 가능한 센서 모델을 제안합니다. 이 모델은 두 개의 NVIDIA RTX-4090 GPU에서 초당 5000 프레임(FPS)의 훈련 속도를 달성하며, 이는 촉각 센서 없이 훈련하는 속도의 약 70%에 해당합니다.
*   **손안 조작을 위한 RL 제어 정책 학습:** 제안된 3축 센서 모델을 사용하여 접촉이 풍부하고 제어된 미끄러짐(controlled sliding)을 요구하는 손안 번역(in-hand translation) 작업을 위한 RL 제어 정책을 학습합니다.
*   **실험적 검증:** 3축 촉각 정책이 미지의 물체 특성, 새로운 손 방향 및 물체 역학에 대한 적응에서 기존의 기준선(전단력만 사용, 법선력만 사용, 고유 수용 감각만 사용)보다 일관되게 우수한 성능을 보임을 190개 이상의 실제 실험을 통해 입증합니다.

**방법론:**

본 연구는 Allegro Hand 로봇에 ReSkin이라는 자기 탄성체 촉각 피부를 부착하여 진행합니다. ReSkin은 팔레트에 부착되어 있으며, 내부의 자성 입자 변형을 통해 3축 힘을 자기장 변화로 측정합니다. 시뮬레이션과 실제 환경 모두에서 히스테리시스(hysteresis) 및 크로스토크(cross-talk) 효과를 최소화하기 위해 센서 출력을 이진화(binarize)합니다.

**1. 촉각 피부 모델 (Tactile Skin Model):**

IsaacGym 시뮬레이터에서 ReSkin 팔레트를 두 부분으로 모델링합니다. 첫째, ReSkin 팔레트 패드를 실제 센싱 피부와 동일한 물리적 치수를 가진 강체 볼륨으로 모델링하여 보다 정확한 동역학을 위한 연속적인 충돌 표면 역할을 하게 합니다. 둘째, ReSkin은 연속적이지만, 센싱 피부를 16개의 개별 택셀(taxel)로 이산화하며, 각 택셀은 기본 자기계의 위치에 해당합니다. 각 ReSkin 택셀은 원통형 충돌 형상(반경 $r = 1.5 \, \text{cm}$)으로 모델링되어 ReSkin 팔레트 표면에 배치됩니다.

*   **전단 신호(Shear Signal) 계산:**
    탄성 접촉 모델에서 영감을 받아 다음과 같이 계산합니다:
    $S_{x,t} = \left(\sum_{i=1}^n P_i\right) D (v_{x,t} + \omega_{x,t})$
    $S_{y,t} = \left(\sum_{i=1}^n P_i\right) D (v_{y,t} + \omega_{y,t})$
    여기서:
    *   $x$와 $y$는 센서 표면에 접하는 전역 좌표축을 나타냅니다.
    *   $S_{x,t}$와 $S_{y,t}$는 각 택셀이 생성하는 전단 신호입니다.
    *   $v_{x,t}$와 $v_{y,t}$는 물체의 선형 속도입니다.
    *   $\omega_{x,t}$와 $\omega_{y,t}$는 물체의 각속도입니다.
    *   $\sum_{i=1}^n P_i$는 센싱 범위 내에 있는 물체 점들의 침투 거리($P_i$)의 합계입니다. $P_i$는 센서 원점에서 물체 점까지의 거리를 센서 범위 $R$에서 뺀 값($R - l_i$)입니다.
    *   $D$는 물체 점 밀도(총 물체 점 수를 물체 부피로 나눈 값)입니다.

*   **법선력 신호(Normal Force Signal) 계산:**
    $S_{z,t} = \left(\sum_{i=1}^n P_i\right) D$
    물체 점 침투 거리의 합계를 물체 점 밀도로 나눈 값으로 법선력을 계산하며, 물체 점 침투 거리는 법선력의 대리(proxy) 값입니다. 물체 점 밀도로 나누는 것은 물체의 다양한 크기를 고려합니다.

*   **신호 이진화:**
    시뮬레이션-실제 간의 격차를 해소하기 위해 제어 정책은 모델로부터 임계값 처리된 센서 신호로 훈련됩니다:
    $S_{x,t} \in \{-1, 0, 1\}$, $S_{y,t} \in \{-1, 0, 1\}$, $S_{z,t} \in \{0, 1\}$
    부호 있는 3축 정책(S3-Axis)은 전단 신호의 양수 및 음수 부호를 유지하여 각 축에 대한 방향 정보를 제공합니다. 부호 없는 3축 정책(U3-Axis)은 전단 신호의 절대값을 취합니다.

*   **실제 환경 이진 촉각 처리:**
    ReSkin의 원시 신호는 상당한 히스테리시스를 가지므로, 단순한 임계값 처리만으로는 이진화에 충분하지 않습니다. 대신, 신호의 시간 미분(time derivative)에 대한 임계값을 사용합니다. 신호 이력과 현재 신호 버퍼 간의 차이가 임계값을 초과하면 1 또는 -1을 반환합니다(그렇지 않으면 0).

**2. RL 정책 학습 (RL Policy Training):**

정책 학습은 두 단계로 시뮬레이션에서 전적으로 수행됩니다.

*   **1단계: 오라클 정책 훈련:** 시뮬레이터에서 공개적으로 사용할 수 있는 특권 정보(privileged information)를 사용하여 오라클 정책($\pi$)을 훈련합니다. 특권 정보는 물체 위치, 속도, 크기, 질량, 질량 중심(COM), 마찰 계수 등을 포함하며, 8차원 벡터 $z_t$로 인코딩됩니다. 오라클 정책의 입력은 손의 고유 수용 감각(proprioception)에서 오는 손가락 관절 위치와 인코딩된 특권 정보 $z_t$입니다. 정책은 PD 제어기의 16개 관절 위치 목표 $a_t \in \mathbb{R}^{16}$를 출력합니다.
*   **2단계: 촉각 정책 훈련:** 오라클 정책($\pi$)은 고정하고, 제안된 시뮬레이션 센서 모델을 사용하는 관측 인코더(observation encoder)를 훈련합니다. 관측 인코더의 입력은 고유 수용 감각($q_t$)과 촉각 피부 모델($[S_{x,t}, S_{y,t}, S_{z,t}]$)에서 오는 시뮬레이션 센서 데이터입니다. 관측 인코더는 1) $z_t$와 $\hat{z}_t$ 사이의 L2 norm을 최소화하여 특권 정보를 재현하고, 2) $a_t$와 $\hat{a}_t$ 사이의 L2 norm을 최소화하여 오라클 제어 정책과 동일한 동작을 재현하도록 훈련됩니다.
*   **보상 함수:** 물체 상태에 대한 페널티(손안 번역 작업 정의)와 로봇 동작에 대한 페널티(손가락 보행 생성)를 사용하여 구성됩니다. 보상 함수는 주로 작업 보상($r_{iht}, r_{goal}$), 동작 페널티($r_{rotp}, r_{pose}$), 에너지 페널티($r_{work}, r_{torque}, r_{force}$)로 구성됩니다.
*   **알고리즘:** PPO(Proximal Policy Optimization)를 사용하여 오라클 정책을 최적화하고, 정책과 비평가 네트워크 간에 가중치를 공유합니다.

**실험 및 결과:**

실제 환경에서 190회 이상의 롤아웃을 통해 정책을 평가했습니다.

*   **내부 도메인(In-Domain) 성능:** 훈련 설정과 동일한 정방향 실린더 물체를 사용하여 S3-Axis와 Proprio-Only 정책을 비교했습니다. S3-Axis 정책은 Proprio-Only 정책에 비해 번역 거리가 38% 증가하고 물체 속도가 94% 빨라졌으며, 더 일관된 성능을 보였습니다.
*   **OOD(Out-of-Domain) 손 기울기 적응:** 손 기울기가 0~15도까지 변하는 조건에서 정책 적응을 테스트했습니다. 손 기울기가 증가함에 따라 작업 난이도가 증가하고 성능이 저하되었지만, S3-Axis는 모든 기울기 각도에서 Proprio-Only보다 우수한 성능을 유지했습니다.
*   **3축 촉각 감지 기반 OOD 적응:** 손 기울기 각도와 OOD 물체(드라이버, 망치, 물병)라는 두 가지 유형의 교란에 대한 정책 적응을 동시에 테스트했습니다.
    *   **드라이버 (어려운 형상, 0° 손 기울기):** U3-Axis 정책이 가장 효과적이었으며, 다음으로 좋은 정책에 비해 거리 6.3%, 속도 32% 향상을 달성했습니다.
    *   **망치 (불균형 COM, 직사각형 형상, 15-20° 손 기울기):** S3-Axis 정책이 가장 우수한 작업 지표를 달성했습니다. 20도 기울기에서 S3-Axis는 다음 정책에 비해 거리 149% 증가, 성공률 20% 증가를 보였습니다. U3-Axis는 이 경우 완전히 실패했습니다.
    *   **물병 (가변 COM, 불규칙 형상, 0-10° 손 기울기):** 물병의 가변 COM과 불규칙한 형상에도 불구하고 U3-Axis 정책이 다음으로 좋은 정책에 비해 거리 12.5%, 속도 153% 향상을 달성했습니다.
*   **실험 분석:**
    *   **잠재 공간 분석:** t-SNE를 사용하여 $\hat{z}_t$ 벡터를 분석한 결과, $\hat{z}_t$ 클러스터의 분산 정도가 정책 실패와 상관관계가 있음을 발견했습니다. 성공적인 S3-Axis 정책은 모든 롤아웃에서 $\hat{z}_t$의 조밀한 클러스터를 보여주었습니다.
    *   **더 많은 손가락 보행 탐색:** 촉각 정책들이 Proprio-Only 정책에 비해 평균적으로 더 다양한 관절 상태를 탐색하며, 특히 망치와 물병에서 이러한 경향이 두드러짐을 확인했습니다. S3-Axis의 포앙카레 단면(Poincaré section)을 통한 교차점의 표준 편차는 Proprio-Only에 비해 35% 증가했습니다. 이는 태스크 성공 및 보행 적응에 중요한 요소가 될 수 있습니다.

**결론 및 향후 연구:**

본 연구는 새로운 촉각 센서 모델을 제안하고, 이를 사용하여 도전적인 손안 조작 작업을 위한 RL 정책을 시뮬레이션에서 훈련할 수 있음을 보여주었습니다. ReSkin을 사용한 능숙한 손안 번역 정책은 실제 환경에 제로샷 배포될 수 있으며, ReSkin의 전단 및 법선력 감지가 OOD 손 방향과 물체 모두에 대한 최상의 적응 및 내부 도메인 성능을 일관되게 가능하게 함을 입증했습니다. 이는 일반적인 손안 조작을 위한 촉각 피드백의 핵심 단계입니다.

향후 연구는 현재의 경험론적(heuristic) 모델보다 물리적 기반 모델(특히 연속적인 촉각 신호에 대한)을 탐색하고, 여러 개의 개별 센서 간의 크로스토크 모델링과 같은 ReSkin 커버리지의 시뮬레이션-실제 전이 문제를 해결하는 데 집중할 것입니다. 또한, 시뮬레이션-실제 촉각 전이를 위한 도메인 적응(domain adaptation) 방법과 3축 촉각 감지가 다른 능숙한 기술의 정책 성능을 어떻게 향상시키는지 탐구할 계획입니다.

---


# Detail Review

> 손바닥 촉각 피부로 학습하는 손안 물체 이동

이 논문은 로봇 손바닥에 부착된 촉각 센서를 활용하여 손 안에서 물체를 **평행 이동**(in-hand translation)시키는 기술을 강화학습으로 학습한 연구입니다. 특히 **전단 힘**(shear force)과 **법선 방향 힘**(normal force)을 동시에 감지하는 **촉각 피부(tactile skin)** 모듈을 시뮬레이션하고, 이를 통해 시뮬레이션에서 학습한 정책을 실세계에 **zero-shot**으로 바로 적용하는 성과를 보였습니다. 아래에서는 이 논문의 기술적 배경과 방법론, 실험 설정, 주요 결과와 그 의미, 그리고 한계점과 향후 방향에 대해 심층적으로 분석합니다.

## 기술 개요: 배경과 목적

사람은 일상적인 물체 조작에서 **촉각**에 크게 의존하며, 이를 모방하여 로봇에 촉각 센서를 장착하려는 연구가 활발합니다. 그러나 정교한 촉각 센서를 만들고 이를 로봇 제어에 활용하는 데에는 어려움이 있습니다. 고해상도의 촉각 정보는 처리와 시뮬레이션에 시간이 많이 들고, 반대로 빠른 시뮬레이션은 촉각 정보를 지나치게 단순화하는 경향이 있어 실제와 차이가 큽니다. 이러한 문제로 현재 많은 강화학습 기반 조작 연구에서는 촉각 정보를 생략하거나 단순화된 접촉 신호만 사용하는 경우가 많았습니다.

본 논문에서는 이러한 **시뮬레이션-실세계 격차(sim-to-real gap)**를 줄이기 위해, 로봇 손바닥의 **자기 기반 탄성 촉각 피부(sensor)**인 **ReSkin**을 모델링하여 **이산화된 촉각 신호**를 출력하는 시뮬레이터를 개발했습니다. 이 모델은 각 촉각 센서 소자(taxel)에서 **법선 방향 힘은 2값(binary)**으로, **전단력은 3값(ternary)**으로 출력합니다. 즉, 물체가 센서를 누르고 있으면 법선 방향 접촉 신호가 1 (없으면 0)이며, 전단력은 미끄러지는 방향에 따라 +1 또는 -1 (미끄럼 없음은 0)으로 표시됩니다. 이렇게 단순화한 촉각 표현을 통해 시뮬레이터의 효율을 높였으며, 두 개의 GPU 상에서 **5000Hz 이상의 모의 센싱 속도**로 동작하여 촉각이 없는 경우 대비 약 70%의 속도로 학습을 진행할 수 있었습니다. 이 센서 모델의 가장 큰 장점은, **시뮬레이션에서 학습한 정책을 실세계로 별도 미세조정 없이 바로 옮길 수 있다는 점**(zero-shot transfer)으로, 논문 저자들은 이 모델로 훈련한 정책을 190회의 실험을 통해 검증하며 실제 로봇에서 안정적으로 동작함을 보였습니다.

**문제 정의:** 저자들은 이 센서 모델을 활용하여 **손바닥 위에서 물체를 한쪽에서 반대쪽으로 이동**시키는 **손안 평행이동(in-hand translation)** 과제를 수행했습니다. 이 과제는 물체를 손가락들 사이에서 **미끄러뜨리며 이동**시켜야 하기 때문에 지속적인 **슬라이딩 접촉 제어**가 필요하고, 따라서 **전단력에 대한 촉각 피드백**이 특히 중요하다고 가정했습니다. 기존 연구들은 주로 물체의 **자세 변경(회전)**에 초점을 맞추거나, 촉각을 사용하더라도 **법선 힘(접촉 여부)**만 활용한 경우가 많았습니다. 반면 본 연구는 회전에 국한되지 않은 **평행이동**이라는 새로운 조작 기술에 도전하고 있으며, 특히 **법선+전단 3축의 촉각 정보**를 활용해 **강화학습 정책**을 학습함으로써 복잡한 접촉 상황에서의 적응 능력을 높이고자 했습니다. 저자들의 목표는 이처럼 복잡한 촉각 정보를 다루는 **최초의 RL-활용 촉각 피부 모델**을 제시하고, 이를 통해 **섬세한 물체 조작(dexterous manipulation)** 기술을 한 단계 발전시키는 것이었습니다.

요약하면, 이 논문의 핵심 기여는 **(1)** 시뮬레이션에서 사용 가능한 고속 **촉각 피부 센서 모델**을 개발하여 전단/법선 촉각 신호의 sim-to-real 전이를 가능케 한 것, **(2)** 이를 활용해 **손안에서의 물체 평행이동**이라는 난이도 높은 조작 작업을 **강화학습 정책**으로 성공적으로 학습한 것, 그리고 **(3)** 실제 로봇 실험을 통해 **촉각 센싱(특히 3축 전단+법선)**이 있을 때 **없는 경우보다 월등히 높은 성능과 일반화 능력**을 발휘함을 실증한 것입니다.

## 실험 세팅: 로봇 손, 센서와 작업 환경

**하드웨어:** 실험에는 **SimLab Allegro Hand** 로봇 손이 사용되었습니다. 이 손은 사람 손과 유사하게 4개의 손가락(엄지 포함)을 가지고 각 손가락에 4개의 관절이 있어 총 16자유도를 갖습니다. 각 관절에는 **관절 위치 센서(proprioception)**가 있으며, 강화학습 정책은 매 30Hz마다 관절별 목표 위치 명령을 내고, 이 명령은 내부 PID 제어를 통해 토크로 변환되어 구동됩니다. 촉각 센서는 Meta AI에서 개발한 **ReSkin**이라는 자성 기반의 얇은 촉각 피부를 사용했으며, 약 **37×96 mm** 크기의 패치를 손바닥 중앙에 부착했습니다. ReSkin은 탄성 재질 안에 자성을 띤 미세 입자를 포함하고 있어, 변형될 때 그 분포 변화로 **자기장 센서(magnetometer)**가 3축 힘의 변화를 감지하는 원리입니다. 실험에서 ReSkin은 약 120Hz 속도로 데이터를 출력했고, 시뮬레이션 학습 단계에서는 이 센서의 동작을 가상 모델로 구현했습니다.

한 가지 실제 구현상의 문제는, **ReSkin 표면과 물체 사이 마찰력이 매우 높다**는 점이었습니다. 손바닥에 물체가 닿으면 쉽게 미끄러지지 않으므로 손가락에 과도한 힘이 필요하고, 이는 Allegro Hand의 모터 발열이나 토크 한계를 초래할 수 있습니다. 이를 완화하기 위해 연구진은 **ReSkin 위에 얇은 PET 필름(0.25mm)을 덮어서 마찰력을 감소**시켰습니다. 또한 손가락 일부에는 고무 테이프를 부착해 **물체가 손가락에서 잘 미끄러지지 않도록** 보조했습니다. 이러한 물리적 조치는 강화학습 정책의 성능에는 직접 영향하지 않지만, **하드웨어 한계로 인한 문제를 완화**하여 실험을 안정적으로 수행하기 위한 것입니다.

**훈련과 환경:** 정책 학습은 NVIDIA Isaac Gym 시뮬레이터에서 이루어졌습니다. 시뮬레이션 환경에는 Allegro Hand 모델과 손바닥의 촉각 센서 모델, 그리고 이동시킬 **원기둥 물체**가 포함됩니다. 학습시 물체는 일정한 크기의 원기둥이 아니라 **무작위로 크기(scale)를 바꾼 원기둥**을 사용하여, 정책이 다양한 크기에 일반화하도록 했습니다. 매 에피소드마다 물체의 질량, 마찰계수, 무게중심 위치 등 물리 속성도 랜덤으로 변화시켜 **도메인 무작위화(domain randomization)**를 적용했습니다. 물체는 항상 초기에 손바닥 쪽으로 단단히 쥐어진 상태(안정된 파지)에서 시작하며, 목표 위치는 손바닥을 가로질러 초기 위치로부터 x축 방향 일정 거리 떨어진 지점으로 주어졌습니다. 에이전트(로봇 손)의 목표는 **물체를 손에서 떨어뜨리지 않고 제한 시간 동안 목표 지점까지 이동**시키는 것입니다. 한 에피소드는 4초(시뮬레이션 400 스텝)에 걸쳐 진행되며, 도중에 물체를 떨어뜨리면 실패로 간주하고 에피소드를 종료합니다.

**테스트 환경:** 학습이 완료된 정책은 **실제 로봇 손**에 이식되어 검증되었습니다. 테스트에 사용된 물체는 총 4가지로, **훈련과 같은 원기둥**(직경 5cm, 길이 15cm)을 비롯하여, **망치**(머리 부분에 무게가 치우친 비대칭 물체), **드라이버**(손바닥에 여러 점으로 닿는 복잡한 형상), **물병**(안의 물이 움직여 무게중심이 변하는 물체) 등을 선택했습니다. 이들 중 원기둥 하나만이 **훈련(domain) 범위 내** 물체이고, 나머지 셋은 **훈련에 없던 새로운 물체(OOD)**로 간주되어 정책의 **일반화 성능**을 평가합니다. 또한 물체 이동 난이도를 조절하기 위해 로봇 손 자체를 기울이는 실험을 했습니다. 손바닥을 수평이 아니라 **중력에 대해 최대 20도까지 기울여서** 물체를 이동시키면, 중력에 의해 물체가 아래로 미끄러지려는 힘이 생겨 이동이 더 어려워집니다. 연구진은 0도(수평)에서 5도, 10도, 15도, 20도까지 기울인 상황을 시험하여, 기울기 변화(중력 영향)에 대한 정책의 **견실성(robustness)**도 평가했습니다. 각 조건(물체 종류 + 손 기울기)에 대해 약 5회의 실험 롤아웃을 수행했고, 총 190회의 실세계 실험 데이터를 수집하여 성능을 분석했습니다. 실험 중에는 OptiTrack 모션 캡처로 물체의 **실제 이동 거리(cm)**와 **이동 속도(cm/s)**를 측정하여 정량적 성과 지표로 사용했습니다. 또한 **성공 여부**는 제한 시간(최대 120초) 내에 **물체가 0cm 이상이라도 움직였는지**로 정의하여, 아예 물체를 떨어뜨리거나 전혀 못 움직인 경우만 실패로 간주했습니다. (※ 임계값을 0cm로 둔 이유는, 경사 환경에서는 일부 정책이 물체를 전혀 이동시키지 못하는 경우가 있어 이를 분리하기 위함입니다.)

**비교 대상:** 정책의 효과를 분석하기 위해 **여러 가지 입력 조합에 따른 정책들**을 학습시켜 비교했습니다. 모든 정책은 동일한 강화학습 알고리즘과 보상 함수를 사용하되, 관찰 입력에 따라 다음과 같이 나뉩니다:

* **Proprioception Only (고유감각만):** **촉각 없이** 관절각도 등 로봇의 상태만으로 학습한 정책 (기준 시나리오).
* **Normal Only (법선 힘만):** 촉각 중 **법선 접촉 신호만 2값** 입력으로 사용한 정책. 이는 기존 연구에서 흔히 사용하는 접촉 유무 센싱과 유사합니다.
* **Signed Shear Only (전단만, 부호 포함):** **전단력 신호만 3값**(방향성 포함) 입력으로 사용한 정책. 물체가 어디로 미끄러지는지 방향만 감지하고 접촉 유무는 고려하지 않습니다.
* **Unsigned 3-Axis (U3-Axis, 전단 절대값):** **법선+전단 모두 사용**하되 전단력의 부호는 무시하고 크기만 반영한 정책입니다. 즉 전단도 2값(binary) 형태로 간주하여, 촉각 센서가 미끄러짐을 감지하면 방향에 상관없이 1로 처리하는 방식입니다. 실제 ReSkin 센서의 이산화에도 유사하게 **전단 변화의 절대값**만 사용하기 때문에 도입된 변형입니다.
* **Signed 3-Axis (S3-Axis):** **본 논문의 제안 방법**, 전단력 방향까지 살린 **3축 촉각 (법선+전단)** 입력을 모두 사용하는 정책입니다.

이들 정책은 모두 동일한 환경에서 학습되어, **촉각 정보의 유무와 종류가 성능에 미치는 영향**을 평가하게 됩니다. 학습을 위한 알고리즘으로는 Proximal Policy Optimization (**PPO**)이 사용되었으며, 반복 학습을 통해 각 정책이 충분한 성능에 수렴하도록 했습니다. 특히 S3-Axis와 U3-Axis 정책은 **동일한 사전학습된 “오라클” 정책**으로부터 파생되는데, 이 상세 내용은 다음 방법론에서 설명합니다.

## 핵심 방법론: 촉각 센서 모델링과 2단계 학습



**1. 촉각 피부 시뮬레이션 모델:** 저자들은 시뮬레이터에서 ReSkin 촉각 피부의 동작을 근사하기 위해, **손바닥 면적 전체를 연속적인 접촉 면**으로 모사하고 그 위에 **16개의 가상 촉각 소자(taxel)**를 균일하게 배치했습니다. 각 촉각 소자는 실제 하드웨어의 자력 센서에 대응되며, 해당 위치의 피부 변형을 감지한다고 가정합니다. 구체적인 모델링 절차는 다음과 같습니다:

* 손바닥 면과 물체 표면 사이의 충돌을 감지하기 위해, 물체의 표면을 여러 개의 점(point)으로 **표면 점군(point cloud)**화 합니다. 이때 손바닥에 매우 가까이 있는 물체 점들을 찾기 위해, 각 촉각 소자 위치를 중심으로 작은 원통형 **감지 영역**(sensing range)을 정의합니다. 논문에서는 각 taxel마다 물체와 충돌을 계산하는 기본 지오메트리(원통)를 두고, 그 반경을 약간 확대하여 소프트 재질의 **탄성(compliance)** 효과를 모사했다고 설명합니다. 다시 말해, 실제 촉각 피부가 약간 눌려도 접촉을 느끼는 것처럼 시뮬레이션에서도 **충돌 판정 거리를 늘려**서 물체가 완전히 부딪히지 않아도 가까이 오면 힘으로 간주합니다. 이러한 **범위 확장과 다중 점 샘플링** 기법을 통해, 일반적인 물리엔진의 “한 접촉당 한 점” 모델 대신 **넓은 면적의 접촉으로 인한 힘 분포**를 근사할 수 있습니다.

<center>
<img src="../../images/2025-07-24-inhand-trans/4.png" width="80%" />
</center>

> 촉각 스킨의 시뮬레이션-실사 전달을 위한 모델링 접근 방식입니다. A) 손바닥을 16개의 개별 taxel이 있는 연속적인 표면으로 모델링하며, 각 taxel은 하부의 자력계에 해당합니다. B) 각 taxel에 대해 실린더를 사용하고 감지 범위 $(R)$을 충돌 지오메트리 너머로 확장합니다. C) 물체에서 점들을 샘플링하고 충돌 표면을 포인트 클라우드로 나타냅니다. 감지 범위 내의 점들은 $i$로 표시됩니다. D) 침투 거리 $P_i = R - l_i$를 합산하며, 여기서 $l_i$는 점 $i$에서 센서 원점까지의 거리입니다. 전단력 및 수직력에 대한 센서 신호는 $\sum_{i=1}^{n} P_i$, 물체 속도 및 물체 점 밀도를 사용하여 계산합니다.


* **법선 힘 (Normal force)**: 각 촉각 소자에 대해 감지 영역 내 들어온 물체 표면 점들의 **침투 깊이(penetration distance)**를 합산합니다. 이 합산 값이 일정 임계치 이상이면, 해당 소자에 **물체가 눌리고 있다(접촉 있음)**고 판단하여 **법선 접촉 신호 = 1**을 출력하고, 아니면 0을 출력합니다. 침투 깊이 합산값 자체는 물체가 누르는 **힘의 근사치**로 간주됩니다. 또한 물체 크기에 따라 이 값이 달라지는 것을 보정하기 위해, 물체의 표면 점 밀도(점 개수/부피)로 나눈 정규화를 수행합니다.

* **전단력 (Shear force)**: 전단의 경우 물체와 손바닥 사이에 **상대 운동**이 있어야 발생합니다. 저자들은 콜롬 마찰 모델에 착안하여, **물체의 순간 속도**(선형 및 각속도)와 위에서 계산된 **법선 힘 근사치**를 활용해 전단 신호를 계산했습니다. 쉽게 말해, 물체가 손바닥을 **미끄러지는 방향**으로 움직이고 있고 (속도 방향), 접촉력도 일정 수준으로 작용하면 그 방향으로 **전단 접촉 신호 = 1 (또는 -1)**을 출력하도록 했습니다. 구체적으로는 물체의 x, y축 방향 속도 성분을 가져와, 그 방향에 **법선 힘 규모에 비례하는 전단 값**을 산출한 뒤 이 역시 임계값 기반으로 이진화합니다. 전단의 부호(방향)는 속도 방향으로 정해지며, 예를 들어 물체가 +x 방향으로 미끄러지면 해당 taxel의 x축 전단 신호 = +1, -x로 미끄러지면 = -1, 거의 정지되어 있으면 0으로 표현됩니다. 이렇게 계산된 **전단력 신호 두 축(x, y)**과 **법선 신호(z)**, 총 3축의 이산 촉각 출력이 최종적으로 강화학습 정책의 입력으로 제공됩니다.

* **이산화 및 이력 처리:** 현실의 ReSkin 센서는 연속적인 아날로그 신호를 내지만, 이는 히스테리시스 등의 문제로 곧바로 사용하기 어렵습니다. 따라서 시뮬레이션에서는 처음부터 임계값을 넘겨 **0/1 (또는 ±1)**로 **이산화(discretization)** 하여 RL에 사용했고, 실제 센서에서도 시간에 따른 변화량을 보고 임계값을 넘을 때에만 상태가 바뀌는 방식으로 **신호를 이진 처리**했습니다. 예를 들어, 실제 실험에서 센서 출력이 한 방향으로 지속적으로 힘을 받고 변화가 없으면, 일정 시간이 지나 **출력 0(변화 없음)**으로 간주합니다. 이렇게 해야 ReSkin의 특성상 발생하는 **신호 잔류(hysteresis)**를 억제할 수 있었기 때문입니다. 다행히도 손안에서 물체를 움직이는 동안 센서 신호는 자주 변화하기 때문에, 이러한 방법이 크게 문제되지 않았다고 합니다.



**2. 강화학습 정책 학습 (두 단계):** 촉각 피부 모델이 준비되었지만, 곧바로 3축 촉각 정보를 넣어 강화학습을 시키면 학습 난이도가 매우 높을 수 있습니다. 특히 초기에는 랜덤한 정책으로 인해 물체를 쉽게 떨어뜨릴 것이고, Sparse한 보상 때문에 학습이 안 될 위험이 있습니다. 저자들은 이 문제를 해결하고자 **“교사-학생 학습”**과 유사한 **2단계 학습(framework)**을 도입했습니다. 이는 \[Chen et al., 2022] 등이 사용한 접근을 따랐다고 명시되어 있습니다:

* **1단계 – 오라클(Oracle) 정책 학습:** 여기서 오라클이란 학습에 **시뮬레이터의 “특권 정보(privileged information)”**를 활용한 이상적인 정책을 말합니다. 구체적으로, 시뮬레이션 내부의 물체 상태(물체의 위치, 속도, 크기, 질량, 무게중심, 마찰계수 등)를 모두 알고 있다고 가정하고, 이 정보를 8차원 벡터로 압축하여 정책의 입력으로 제공합니다. 물론 로봇의 관절 상태(proprioception)도 함께 입력됩니다. 이렇게 하면 정책이 **어떤 상황에서 어떤 동작이 필요한지 쉽게 학습**할 수 있습니다. 보상 함수는 물체를 목표 지점까지 이동시키는 정도(목표에 가까이 갈수록 큰 보상)와 물체를 떨어뜨리지 않는 것, 그리고 손가락 움직임의 효율성 등을 종합한 형태입니다. 저자들은 PPO 알고리즘으로 이 정책을 약 **1.68일**간 학습하여 높은 성능의 오라클 정책을 얻었습니다. 오라클 정책은 물체가 어디 있는지 정확히 아는 상태에서 동작하므로, 목표 달성에 매우 효과적이며 물체를 잡는 **손가락 gait 패턴**도 비교적 안정적으로 발견했습니다.

* **2단계 – 촉각 정책 학습:** 두 번째 단계에서는 실제 우리가 원하는 **촉각 기반 정책**을 학습합니다. 이때 1단계의 오라클 정책을 **고정된 교사**로 삼고, 학생인 촉각 정책은 **오라클의 행동을 모방**하도록 훈련됩니다. 구체적으로, 학생 정책은 관절 상태 + **시뮬레이터의 촉각 센서 출력(3축 이산 신호)**를 입력으로 받으며, **Transformer 인코더 구조**를 사용해 이 입력을 처리합니다. 인코더의 역할은, 촉각+관절 정보를 받아서 오라클 정책이 이용했던 8차원 **잠재 상태 정보(extrinsics 벡터)**를 최대한 재현하는 것입니다. 즉, 학생은 촉각을 통해 **물체의 상태를 간접적으로 추정**하도록 학습됩니다. 논문에 따르면, 이 단계에서는 두 가지 목표로 학습이 진행됩니다: **(a)** 학생 인코더의 출력이 오라클의 잠재 벡터와 가깝도록 하고, **(b)** 학생 정책의 행동이 오라클 정책의 행동과 유사하도록 합니다. 이를 통해 학생 정책은 촉각만 가지고도 마치 물체 상태를 알고 있는 것처럼 행동하게 됩니다. 이 두 번째 단계는 **지도학습적 손실**로 이루어지므로 비교적 빠르게 수렴하며, 약 **6.27시간**만에 완료되었다고 보고됩니다. 최종적으로 얻어진 정책은 **오라클 없이도** 촉각과 관절 정보만으로 동작하므로, 이를 그대로 실제 로봇에 이식하여 테스트를 진행합니다.

<center>
<img src="../../images/2025-07-24-inhand-trans/3.png" width="80%" />
</center>

이러한 2단계 학습 전략은 시뮬레이션 상의 풍부한 정보를 활용해 **학습 신호를 강화**하는 한편, 최종 정책은 **실제 센서 정보만으로 구동**되도록 보장하기 때문에 sim-to-real 시 유리합니다. 또한 오라클 정책과 동일한 보상 함수를 사용하므로, 학생 정책도 **물체 이동과 손가락 gait** 두 측면을 모두 고려한 동작을 하도록 유도됩니다. 요약하면, **오라클은 “이상적인 플레이”를 보여주는 역할**, **학생은 그 플레이를 촉각 감으로 보고 배우는 역할**을 수행한다고 이해할 수 있습니다.


## 실험 결과 및 해석


**실험 성능 비교:**

<center>
<img src="../../images/2025-07-24-inhand-trans/2.png" width="80%" />
</center>


- 왼쪽 그래프는 **손바닥 기울기(경사) 각도**에 따른 정책들의 **평균 이동 거리(cm)**와 **이동 속도(cm/s)**를 보여줍니다. 보라색 선은 3축 촉각 정책(S3-Axis), 주황색 선은 촉각 없는 정책(Proprio-Only)입니다. 경사가 증가할수록 두 정책 모두 성능이 떨어지지만, **S3-Axis 정책이 모든 각도에서 Proprio-Only보다 일관되게 더 나은 성능**을 보입니다. 0도 (평지) 환경에서는 S3-Axis가 Proprio-Only 대비 **약 38% 더 긴 이동거리(+2cm)**를 달성했고, **이동 속도도 94% 더 빠르게(+0.16cm/s)** 물체를 움직였습니다. 또한 S3-Axis는 시행 간 성능 변동성(표준편차)도 더 작아서, **더 안정적인 조작**을 함축했습니다. 경사 각도가 커지면(5도, 10도, 15도) 두 정책 모두 성능이 떨어졌지만, **S3-Axis가 항상 Proprio-Only보다 높은 거리와 속도**를 유지했습니다. 이는 **촉각 피드백 덕분에** 손이 기울어 물체가 미끄러지려는 상황에서도 **신속히 대응하여** 물체를 붙잡고 목표 방향으로 밀어내는 **손가락 조정 능력**이 향상되었음을 시사합니다.

- 오른쪽의 막대 그래프들은 **여러 정책들을 비교**한 결과로, **평균 성공률**, **평균 이동 거리**, **평균 속도**를 나타냅니다. 보라색 계열이 3축 촉각 정책들(S3-Axis 진한 보라, U3-Axis 옅은 보라), 분홍은 전단만(Signed Shear), 빨강은 법선만(Normal), 주황은 Proprio-Only입니다. **성공률**에서 S3-Axis 정책은 약 **93%**로 가장 높았고, 이는 Proprio-Only의 성공률을 크게 상회했습니다. **이동 거리** 역시 S3-Axis가 평균적으로 가장 길었으며(두 번째 좋은 정책보다 50% 이상 더 이동), **이동 속도**는 U3-Axis 정책이 약간 더 빠르긴 했지만 S3-Axis도 상위권을 유지했습니다. 한눈에 보면, **3축 촉각을 모두 활용**하는 두 정책(S3, U3)이 **단일 촉각 종류만 쓰는 경우나 촉각 없는 경우보다 전반적으로 우수**함을 알 수 있습니다. 이는 **법선+전단 촉각정보의 결합이 시너지**를 내어, 물체 이동 작업에서 **향상된 피드백 제어**를 가능케 한다는 연구의 주장을 뒷받침합니다.

**일반화 성능:** 새로운 물체(OOD)들에 대한 성능을 자세히 보면, 상황별로 촉각 정책들의 흥미로운 특징이 관찰됩니다:

* **드라이버 (복잡한 표면, 0도 경사):** 드라이버는 표면에 홈과 돌기가 있어 손바닥에 **불연속적인 접촉**을 만드는 물체입니다. 이 경우 모든 정책이 어느 정도 작업을 수행할 수 있었지만, **U3-Axis 정책이 가장 좋은 성능**을 보였습니다. S3-Axis와 큰 차이는 아니지만, U3-Axis가 두 번째로 좋은 정책보다도 **이동 거리 6.3% 길게, 속도 32% 빠르게** 목표에 도달했습니다. 전단 방향 정보를 무시하고 **크기 변화에만 반응**하는 U3-Axis가 오히려 유리했던 것은, 드라이버처럼 접촉 지점이 여러 개이고 복잡할 때 **전단 방향 신호가 잡음이 많아질 수 있기 때문**으로 해석됩니다. 이때는 차라리 **“미끄러짐 발생 여부”**만 보는 편이 더 안정적으로 물체 움직임을 감지한 것으로 보입니다.

* **망치 (무게중심 치우침, 15\~20도 경사):** 망치는 머리 부분에 무게가 집중되어 있고 손잡이는 직사각형 단면이라, 손바닥에 닿는 면적이 훈련때의 원기둥보다 넓습니다. 게다가 손을 기울이면 무거운 망치머리가 한쪽으로 쏠려 **강한 회전 토크**가 발생하기 때문에, 이 조합은 **가장 어려운 시나리오** 중 하나였습니다. 결과는 극적이어서, **15도 경사**까지는 대부분의 정책이 어떻게든 임무를 수행했지만 **20도 경사**에서는 S3-Axis를 제외한 거의 모든 정책이 실패했습니다. 특히 **U3-Axis 정책은 아예 물체를 전혀 옮기지 못하고 실패**했는데, 이는 전단 방향 정보를 버림으로써 **망치가 어느 쪽으로 기울어지는지 감지하지 못한 것**으로 추측됩니다. 반면 S3-Axis 정책과 (참고로 비교군인) Signed Shear Only 정책만이 간신히 망치를 붙잡고 이동을 수행할 수 있었습니다. 그 중에서도 **S3-Axis 정책은 가장 뛰어나서**, 다른 방법 대비 **약 149% 더 긴 이동 거리**를 기록했고 성공률도 **20%p 이상 높았습니다**. 이 결과는 **전단력의 방향성 정보가 물체의 난류한 동적 거동에 대응하는 데 필수적**임을 보여줍니다. 무게중심이 틀어진 물체가 미끄러질 때, 그 방향을 알아야 손가락들로 적절히 버티거나 밀어줄 수 있기 때문입니다.

* **물병 (유동 질량중심, 0\~10도 경사):** 물이 반쯤 든 물병은 이동 중에 **내용물이 움직여 무게중심이 바뀌는** 특성이 있습니다. 초기에는 물이 아래에 있다가, 손바닥을 가로질러 이동하는 동안 물이 흔들리며 점차 중앙으로, 끝으로 옮겨집니다. 이러한 상황에서도 3축 촉각 정책들은 비교적 잘 적응했는데, **U3-Axis 정책이 가장 우수한 이동 속도**를 보였습니다. U3-Axis는 다음으로 좋은 정책보다 **속도 153% 빠르게** 물체를 움직일 수 있었고, 이동 거리도 약간(12.5%) 길었습니다. S3-Axis도 실패하지는 않았지만 속도 면에서 U3보다 낮았는데, 이것은 **내용물 이동에 따른 센서 신호 변화**가 S3에 약간 혼선을 준 가능성이 있습니다. U3는 방향을 무시하고 변화의 크기만 보니, 물 sloshing에도 **민감하게 반응하여 빠르게 조절**한 것으로 보입니다. 비록 U3가 일부 시나리오에서 두각을 나타냈지만, **종합적으로 볼 때 S3-Axis 정책이 모든 실험을 통틀어 가장 안정적이고 높은 평균 성능**을 냈습니다. 특히 S3-Axis는 어떤 환경에서도 완전히 실패하지 않았지만, U3-Axis는 앞서 본 망치 20도 등 특정 조건에서 치명적인 실패를 겪었습니다.

**내부 행동 분석:** 흥미로운 것은 촉각 정책이 **어떻게 다르게 동작하는가**에 대한 분석입니다. 저자들은 정책 신경망 내부의 **잠재 상태 표현(extrinsics vector)**을 t-SNE로 저차원 시각화한 결과, **정책이 실패할 때 내부 표현들도 일관성을 잃고 흩어지는 경향**을 발견했습니다. 예컨대 U3-Axis가 망치 20도에서 실패한 실험들의 잠재 표현은 군집을 이루지 못하고 널리 퍼져 있었는데, 이는 정책이 상황을 제대로 파악하지 못하고 불안정하게 행동했음을 시사합니다. 반대로 S3-Axis 정책은 대부분의 실험에서 **잠재 표현 공간이 밀집된 하나의 군집**을 이루었는데, 이는 다양한 조건에서도 정책이 **일관된 전략**을 가지고 대응했음을 보여줍니다.

또한 손가락 **gait 패턴**에 대해서도 정량적 비교를 했습니다. 각 정책의 실험 로그에서 **손가락 관절 궤적**을 분석한 결과, **촉각을 사용하는 정책들이 촉각이 없는 정책보다 평균적으로 더 다양한 관절 상태를 탐색**함이 드러났습니다. 이는 촉각 피드백이 있을 때 로봇 손이 **더 적극적으로 손가락들을 재배치하고 움직이는 경향**을 보인다는 의미입니다. 예를 들어 S3-Axis 정책은 Proprio-Only에 비해 손가락 움직임의 **위상 공간에서 35% 더 넓은 범위**를 탐색했다고 합니다. 쉽게 말해, 촉각이 있다 보니 **손가락들을 이리저리 더 많이 써보면서** 최적의 움직임을 찾아가는 것입니다. 이러한 **탐색적 거동**은 특히 새로운 물체나 경사 조건에서 **유연한 대처**를 가능하게 해 주며, 결과적으로 더 높은 성공률로 이어졌습니다. 반면 촉각이 없는 정책은 제한된 정보로 인해 손가락 움직임 패턴이 반복적이고 경직되어, OOD 상황에서 적응력이 떨어졌음을 알 수 있습니다.

## 한계점 및 논의

이 연구는 촉각 센싱을 강화학습에 통합하여 인핸드 조작의 새로운 가능성을 열었지만, 몇 가지 **한계점**도 존재합니다:

* **시각 정보 부재:** 본 정책은 **오로지 촉각과 고유감각** 정보만으로 동작합니다. 이는 목표 위치가 미리 손안의 좌표로 정의된 평행이동 작업이기에 가능했습니다. 그러나 일반적인 조작 작업에서는 **시각(vision)**이 필요할 수밖에 없습니다. 예를 들어 물체를 정확한 위치나 자세로 옮기는 목표가 주어질 때, **카메라로 목표를 인지하고 조작을 미세조정**해야 합니다. 저자들도 해당 정책을 보다 **정밀한 작업에 적용하려면 시각 정보를 통합**하는 것이 필수적임을 한계이자 향후 과제로 언급했습니다.

* **정책의 고정 및 온라인 학습 부재:** 시뮬레이션에서 학습된 정책을 실세계에 **그대로 고정하여 적용**(zero-shot)한 점이 본 연구의 특징입니다. 하지만 이는 동시에 한계이기도 합니다. 실제 실험 중에 정책이 보완되어야 할 부분이 보여도, **온라인으로 추가 학습이나 파라미터 수정이 불가능**했습니다. 현실 환경에서 추가 학습이 된다면 더 나은 성능을 낼 여지가 있지만, 본 연구에서는 안전성과 일관성 때문에 정책을 고정한 채 실험했습니다. 저자들은 **실제 tactile 피드백을 이용한 파인튜닝**이나 **온-라인 학습으로의 확장**을 미래의 연구방향으로 제시했습니다.

* **이산 촉각 신호의 한계:** 촉각 센서 모델은 효율을 위해 **신호를 이진화(또는 3단계화)**했습니다. 이로 인해 세밀한 힘의 크기 변화는 정보를 잃게 됩니다. 예컨데 전단력이 약하게 작용하는지 매우 강하게 작용하는지는 (임계값만 넘으면) 동일하게 취급됩니다. 이러한 단순화가 정책 학습에는 오히려 도움이 되었지만, 궁극적으로 **연속적인 촉각 신호**를 제대로 활용하는 것이 이상적입니다. 실제 ReSkin 센서도 아날로그 출력을 가지고 있으므로, 장기적으로는 **연속 값 접촉 힘을 정확히 모사하는 시뮬레이터**로 개선할 필요가 있습니다. 저자들 역시 **연속 신호로의 시뮬레이션 개선**이 성능 향상에 기여할 수 있다고 보고, 이를 향후 연구로 제안했습니다.

* **테스트 범위:** 본 연구는 손바닥 평행이동이라는 단일 유형의 작업에 집중했습니다. 따라서 회전이나 복합 조작 등 **다른 인핸드 조작 스킬에 바로 적용될지는 미지수**입니다. 또한 OOD 객체들도 망치, 드라이버, 물병 정도였으며 **형상/크기 분포가 한정**되어 있습니다. 물론 이미 다양한 요소를 실험하긴 했지만, 예를 들어 더 큰 물체나 표면 재질이 다른 물체, 혹은 손가락에 닿는 경우 등에서는 추가 검증이 필요합니다. **손바닥에만 촉각 센서**가 있다는 제한도 있습니다. 물체가 손가락 끝쪽으로 치우치면 촉각 정보가 줄어들어 정책 성능이 저하될 수 있습니다. 실제로 저자들도 실험 시작 전 **항상 물체가 손바닥에 닿도록 안정된 파지**를 준비하여 초기화했는데, 이는 촉각 입력이 충분히 주어지는 조건에서만 정책이 작동함을 의미합니다. 향후에는 **손가락 측면이나 끝부분에도 촉각 센서**를 달거나, 초기 파지에 의존하지 않고도 물체를 다룰 수 있도록 해야 할 것입니다.

* **하드웨어 제약:** 앞서 언급한 대로, 실제 로봇 핸드의 마찰/토크 문제로 추가 장치(필름, 테이프)를 붙이는 등 **환경을 인위적으로 조정**해야 했습니다. 이는 연구의 핵심은 아니지만, **현재 로봇 손의 기계적 한계**를 보여주는 대목입니다. 더욱 강력한 구동기나 정교한 마찰 제어가 가능했다면, 이러한 조치는 불필요하고 더 다양한 환경을 다뤘을지도 모릅니다. 따라서 **로봇 플랫폼의 개선** 역시 이러한 촉각 조작 연구의 진전을 위해 병행되어야 할 과제입니다.

요약하면, 본 연구는 **촉각 기반 강화학습**의 가능성을 증명했으나, **시각과의 결합, 온라인 적응 학습, 연속센서 모델링, 다양한 작업 확장, 실제 로봇 성능 향상** 등 앞으로 해결해야 할 문제들이 남아 있습니다. 이는 동시에 이 분야의 **흥미로운 열린 문제들**이기도 합니다.

## 연구 의의 및 향후 방향

이 논문은 로보틱스에서 오랫동안 도전적이었던 **손안 조작(in-hand manipulation)** 분야에 촉각을 적극 활용한 성공 사례를 제시했습니다. 특히 **전단력 센싱**을 포함한 **3축 촉각 피부**를 시뮬레이션과 실제에 적용함으로써, **촉각 피드백이 다섯 손가락 로봇 손의 섬세한 동작을 향상**시킬 수 있음을 명확히 보여주었습니다. 기존에는 촉각 정보를 단순 접촉 여부로만 쓰거나, 비전 센싱에 의존하는 경우가 많았지만, 이 연구는 **“보이지 않아도 만져서 조작할 수 있다”**는 것을 멋지게 증명한 것입니다. 특히 학습된 정책이 **한번도 본 적 없는 물체나 상황에서도 높은 성공률로 적응**한 점은, 향후 범용 로봇 손 개발에 중요한 통찰을 줍니다. 촉각이 있으면 로봇이 **물체의 미끄러짐이나 무게 변화를 즉각 감지하고 대응**하여, 비전으로는 한참 늦을 피드백을 빠르게 반영할 수 있다는 것을 입증했습니다.

또한 시뮬레이션 분야에서는, 복잡한 촉각 센서를 **효율적으로 모델링**하여 **강화학습 훈련에 사용 가능**하게 한 공헌이 큽니다. 고속 시뮬레이션과 현실감의 균형을 맞춘 이 방법은 향후 다른 촉각 센서 (예: GelSight와 같은 비전 기반 촉각)에도 아이디어를 적용할 수 있을 것입니다. 논문 저자들이 밝힌 대로, 이 연구는 **“일반적인 손안 조작을 위한 촉각 피드백 활용의 중요한 한 걸음”**이며, 앞으로 **시각-촉각 멀티모달 통합**, **연속 신호 처리**, **실시간 학습** 등으로 확장해 갈 수 있습니다. 예를 들어, 추후 연구에서는 **시각으로 물체의 대략적 정보를 얻고 촉각으로 미세 조정을 학습**하는 체계를 만들 수도 있고, 현재 이진화된 촉각 출력을 **연속값으로 세분화**하여 더 부드러운 제어를 구현할 수도 있을 것입니다.

끝으로, 본 연구는 **모범적인 시뮬레이션-현실 연계 사례**로서도 의미가 있습니다. 시뮬레이터에서만 학습한 정책을 전혀 수정 없이 실제 로봇에서 190회나 실행하며, 그 과정에서 얻은 통찰을 분석했습니다. 이는 향후 강화학습 기반 로봇기술 개발에 있어 **시뮬레이션 활용의 가능성과 한계**를 잘 보여주며, 데이터 효율적인 로봇 학습을 위한 방향성을 제시합니다. 전반적으로, **촉각 피부를 장착한 로봇 손**이 어떻게 **학습을 통해 환경과 상호작용**할 수 있는지를 깊이 있게 탐구한 이 논문의 성과는, 로봇의 **감각 지능**을 한 단계 끌어올린 것으로 평가할 수 있습니다. 향후 이러한 접근들이 누적되면, 인간처럼 **보고 느끼고 조작하는 다재다능한 로봇 손**에 한층 가까워질 것으로 기대됩니다.

**참고:** 해당 연구의 프로젝트 웹사이트에는 실제 로봇 실험 영상들이 공개되어 있어, 정책이 물체를 어떻게 손가락으로 “걸어 옮기는지” 생생히 확인할 수 있습니다. 촉각 정보를 활용한 로봇의 새로운 능력을 보고 싶다면 한 번 살펴볼 가치가 있을 것입니다.
