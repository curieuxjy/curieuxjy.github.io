---
draft: true
title: "📃Robot Synesthesia 리뷰"
description: In-Hand Manipulation with Visuotactile Sensing
date: "2025-04-15"
categories: [paper, manipulation, visuotactile]
toc: true
number-sections: true
---

> NeurIPS 2023 Workshop on Touch Processing: a new Sensing Modality for AI

- []()


1.  🤖 이 논문에서는 시각 및 촉각 입력을 활용하여 로봇이 능숙한 손 안에서의 조작을 가능하게 하는 새로운 시스템인 Robot Synesthesia를 소개합니다.
2.  🎨 인간의 촉각-시각 공감각에서 영감을 받아, Force-Sensing Resistor (FSR)의 촉각 데이터를 카메라의 포인트 클라우드와 결합하여 통합된 3D 공간에 표현하는 새로운 포인트 클라우드 기반 촉각 표현을 제안합니다.
3.  🦾 시뮬레이터에서 학습된 정책이 실제 로봇 손으로 효과적으로 전달되어 복잡한 두 개의 공 회전과 새로운 물체에 대한 일반화가 가능함을 보여주며, 이는 시뮬레이션 환경 및 훈련 파이프라인에 대한 코드 공개로 이어질 것입니다.


# Brief Review

본 논문은 로봇 손의 정교한 조작을 위해 시각 및 촉각 센서 정보를 융합하는 새로운 접근 방식인 "Robot Synesthesia"를 제안합니다.  인간의 공감각에서 영감을 받아, 시각과 촉각 데이터를 통합된 3D 공간에 표현하여 로봇이 촉각적 상호 작용을 "볼" 수 있도록 합니다.

**핵심 방법론:**

1.  **촉각-시각 공감각 (Tactile-Visual Synesthesia):**

    *   기존의 방식처럼 시각과 촉각 정보를 개별적으로 처리하고 특징을 추출한 후 결합하는 대신, FSR(Force-Sensing Resistor) 센서의 촉각 데이터를 카메라의 point cloud와 결합하여 3D 공간에 표현합니다.  이를 통해 로봇 링크, FSR 센서, 조작 대상 객체 간의 공간적 관계를 유지합니다.
    *   구체적으로, 각 촉각 센서에서 신호($o_{t,i} = 1$)가 감지되면 센서 mesh에서 점들을 샘플링하여 촉각 point cloud $P_{touch}^t$ 를 생성합니다. 이 point cloud는 카메라 point cloud $P_c^t$ 및 로봇의 자기 수용 정보를 바탕으로 생성된 augmented point cloud $P_a^t$와 결합됩니다.
    *   각 point cloud의 유형을 구별하기 위해 one-hot vector가 각 점에 추가됩니다. point cloud의 크기는 $N_c = 512$, $N_a = 8n_{link}$, $N_t = 8n_{touch}$로 설정됩니다 ($n_{link}$: 로봇 링크 수, $n_{touch}$: 활성화된 촉각 센서 수).
    *   모든 point cloud는 로봇 손바닥 frame으로 변환되어 신경망에 입력됩니다.

2.  **교사-학생 (Teacher-Student) 학습 파이프라인:**

    *   고차원 입력 (point cloud)을 사용하는 강화 학습 (RL)의 데이터 비효율성 문제를 해결하기 위해 교사-학생 학습 방식을 사용합니다.
    *   **교사 정책 (Teacher Policy):** 낮은 차원의 상태 정보를 사용하여 PPO (Proximal Policy Optimization) 알고리즘으로 학습됩니다.  입력은 로봇 joint 위치 $q_t$, 이진 촉각 신호 $o_t$, 회전축 $k$, 이전 목표 위치 $\hat{q}_t$, 객체의 위치 $x_t$, 속도 $v_t$, 각속도 $w_t$, 그리고 객체 모양 특징 임베딩 $f$입니다.
    *   **학생 정책 (Student Policy):** 교사 정책의 행동을 모방하도록 학습됩니다. 입력은 로봇 joint 위치 $q_t$, 이진 촉각 신호 $o_t$, 회전축 $k$, 이전 목표 위치 $\hat{q}_t$, 카메라 point cloud $P_c^t$, augmented point cloud $P_a^t$, 그리고 제안된 촉각 point cloud $P_{touch}^t$ 입니다.  PointNet [3]을 사용하여 point cloud를 인코딩하고, latent vector를 다른 입력과 함께 MLP에 공급합니다.
    *   2단계 distillation 파이프라인을 사용합니다. 먼저 교사 데이터셋 $D$를 수집하고 BC (Behavior Cloning)를 사용하여 학생 정책 네트워크를 사전 학습합니다. 그 후 DAgger (Dataset Aggregation)를 사용하여 네트워크를 fine-tuning합니다.

3.  **보상 함수 (Reward Function):**

    *   견고하고 전이 가능한 in-hand 회전을 위해 여러 요소의 가중 조합으로 구성된 보상 함수를 사용합니다.
    *   $r_t = c_1r_{rot} + c_2r_{vel} + c_3r_{dist} + c_4r_{torq} + c_5r_{work} + c_6r_{ctrl}$
    *   $r_{rot}$: 객체의 회전 각도를 보상합니다.
    *   $r_{vel}$: 객체의 선형 속도를 penalize하여 객체를 이동시키는 움직임을 억제합니다.
    *   $r_{dist}$: 객체와 손가락 끝 사이의 거리에 대한 감소 함수입니다.
    *   $r_{torq}$: 큰 torque를 penalize합니다.
    *   $r_{work}$: 컨트롤러의 work를 penalize합니다.
    *   $r_{ctrl}$: 명령 목표와 실제 로봇 모션 간의 제어 오류를 penalize합니다.
    *   객체가 손에서 떨어질 경우 큰 penalty를 부여합니다.

본 논문에서는 다양한 실험을 통해 Robot Synesthesia의 효과를 검증하고, 시뮬레이션에서 학습된 정책이 실제 로봇에 성공적으로 전이될 수 있음을 보여줍니다.  특히, 복잡한 double-ball 회전 작업과 새로운 객체에 대한 일반화 능력을 입증했습니다. 또한, PointNet의 중간 레이어를 시각화하여 제안된 촉각 표현이 PointNet이 action 예측에 중요한 fingertip, 객체 표면, 촉각 점과 같은 중요 지점을 식별하는 데 도움이 된다는 것을 보여줍니다.


# Detail Review


# [논문 리뷰] Robot Synesthesia: In-Hand Manipulation with Visuotactile Sensing

## 소개 및 배경
현대 로봇 손으로 물체를 조작하는 **손내 조작(in-hand manipulation)** 과제에서는 **시각**과 **촉각**의 결합이 필수적입니다. 예를 들어 사람이 바늘에 실을 끼울 때, 먼저 **시각 정보**로 바늘 구멍 위치를 찾고 실의 방향을 맞추지만, 막상 실을 끼우는 순간에는 **촉각 정보**로 보이지 않는 실 끝 위치를 감지하여 안내합니다. 이처럼 사람은 시각과 촉각을 자연스럽게 통합하여 복잡한 작업을 수행하지만, 로봇에게 동일한 수준의 **시너지(synergy)**를 구현하는 것은 큰 도전입니다. 기존 로봇 연구에서는 시각 센서(예: 카메라)와 촉각 센서(예: 압력 센서)의 **데이터 형태 차이** 때문에 두 감각을 효과적으로 통합하기 어렵다는 한계가 있었습니다. 시각 데이터는 고해상도의 **풍부한 환경정보**를 제공하는 반면, 촉각 데이터는 국소 부위의 **희소한 접촉정보**만을 제공하기 때문에 하나의 **neural network**에 두 종류의 입력을 함께 학습시키기가 매우 까다롭습니다. 또한 복잡한 손내 조작 정책을 학습하려면 방대한 데이터가 필요한데, 이를 시뮬레이션으로부터 얻어 학습한 후 실제 로봇에 **이식(sim-to-real)** 하는 과정에서도 시각과 촉각 각각의 **도메인 차이(domain gap)**를 동시에 극복해야 하는 어려움이 존재합니다. 이러한 이유로 과거에는 시각과 촉각을 따로 처리한 후 나중에 결합하거나, 아예 한 가지 감각에 의존하는 등 제한적인 접근이 많았습니다.

이 논문 **"Robot Synesthesia: In-Hand Manipulation with Visuotactile Sensing"**은 이러한 배경에서 제안된 연구로, **시각-촉각 동시 통합**을 통해 로봇 손내 조작 능력을 향상시키고자 합니다. 특히 인간의 **공감각(synesthesia)** 개념에 착안하여, 촉각 신호를 시각적으로 **마치 하나의 감각처럼** 표현하는 새로운 표현 방법을 선보입니다. 저자들은 이를 **로봇 공감각(Robot Synesthesia)**이라고 부르며, 로봇이 자신의 **촉각을 눈으로 보듯이** 인식하게 만드는 것이 핵심 아이디어입니다. 이 방법을 통해 촉각과 시각 정보를 본질적으로 **하나의 통합된 형태**로 취급함으로써, 두 감각 간의 관계 학습을 용이하게 하고 시뮬레이션에서 실제로 지식 이전 시 발생하는 오류를 줄이는 효과를 얻습니다. 또한 별도의 실제 데이터 수집 없이 **시뮬레이션 학습만으로** 실제 로봇 손내 조작을 성공적으로 수행했다는 점에서 실용적인 의의도 있습니다. 본 포스트에서는 해당 논문의 동기와 배경, 제안된 방법론, 실험 설정과 결과, 그리고 그에 대한 논의 및 시사점을 석사 수준의 기술적 깊이로 정리합니다.

 *Figure 1: **Robot Synesthesia** 개념 데모. (좌) 시뮬레이션 학습 단계: Allegro 로봇 손이 인공 물체들을 다양한 축으로 회전시키며 훈련되고 있으며, 붉은 점으로 촉각 센서 접촉 지점을 나타낸다. (우) 학습된 정책을 실제 로봇에 적용한 결과: 휠-렌치(위 열), 동일한 두 공(가운데 열), 복잡한 형상의 물체들(아래 열)을 손 내에서 회전시키는 모습을 보여준다. 시뮬레이션에서 학습한 정책을 실제 로봇 손에 별도 추가학습 없이 이식했음을 강조한다.*

## 방법론 (Methodology)
### 1. **시각-촉각 공감각 표현** – Visuotactile Synesthesia Representation  
이 논문의 가장 큰 기여 중 하나는 시각과 촉각 데이터를 하나의 **공통 표현(common representation)**으로 합치는 **Visuotactile Synesthesia** 기법입니다. 일반적으로 로봇의 촉각 센서는 여러 지점에 부착되어 각 지점의 **이진 접촉 여부** 또는 압력을 출력합니다. 저자들은 이러한 촉각 정보를 로봇 손의 3차원 공간 상에 **점군(point cloud)**으로 투영함으로써 마치 **시각적인 형태**로 표현했습니다. 구체적으로, 로봇 손바닥과 손가락 마디 등에 장착된 **FSR(Force Sensing Resistor) 압력 센서** 16개로부터 접촉을 측정하고, **접촉이 발생한 센서 위치의 로봇 손 메쉬 표면 위에 점들을 샘플링**하여 **촉각 점군**을 생성합니다. 이렇게 생성된 촉각 점군은 해당 순간 로봇 손이 어디서 물체를 만지고 있는지 3D 좌표상에 표시해주는 역할을 합니다. 동시에 로봇 손 옆에 설치된 **깊이 카메라(depth camera)** (Microsoft Azure Kinect)를 통해 물체와 손 주변의 **환경 점군**을 얻습니다. 이 카메라로부터 얻은 **시각 점군**은 물체의 겉모습과 위치를 알려주며, 추가로 로봇의 현재 관절각 등 **자기센서(proprioception)** 정보를 이용해 로봇 손 자체의 3D 메쉬 표면에서도 일부 점들을 샘플링한 **증강 점군(augmented point cloud)**도 생성합니다. 이렇게 하면 로봇 손의 현재 형태와 물체의 상대적 위치 관계까지 공간적으로 표현할 수 있습니다. 최종적으로 **카메라 점군**, **로봇 손 증강 점군**, **촉각 점군**을 모두 하나의 **합쳐진 점군**으로 결합하여 네트워크에 입력합니다. 이때 각 점이 어떤 출처(카메라, 로봇, 촉각)에서 온 것인지 구분할 수 있도록 점마다 **원-핫(one-hot) 벡터 특징**을 추가하여 구별되게 표시합니다. 이러한 3차원 공간 통합 표현 덕분에, 로봇은 마치 **자신의 촉각 정보를 눈으로 보듯이** 전체적인 상태를 인식할 수 있게 됩니다. 저자들은 이처럼 촉각을 시각화하는 통합 감각 방식을 **“로봇 공감각(Robot Synesthesia)”**이라고 명명하였습니다.

이 접근법의 장점은 두 가지입니다. 첫째, 초기에 두 감각을 **자연스럽게 융합**된 형태로 입력받기 때문에, 별도의 late-fusion (나중 단계에서 특징 결합) 없이 **학습 단계부터** 네트워크가 시각-촉각 간 상호관계를 쉽게 학습합니다. 이는 단순히 “이미지 특징 + 촉각 값” 식으로 벡터를 이을 때 발생하는 학습상의 모호성을 줄여 줍니다. 둘째, 시뮬레이션에서 학습한 모델을 실제로 옮길 때 **모달리티 간 도메인 차이**가 누적되어 오류가 커지는 문제를 완화합니다. 즉, 촉각과 시각을 별개로 다루면 시뮬레이터의 촉각 모델 오차와 시뮬레이터의 렌더링 오차가 각각 존재하여 실제와 괴리가 생기는데, 두 감각을 하나로 **일체화된 형태**로 다루면 이러한 오차들이 어느 정도 상쇄되어 **sim-to-real 간격**이 줄어드는 효과가 있습니다. 실제로 저자들은 **RGB 카메라 영상** 대신 **깊이 카메라의 점군 데이터**를 시각 입력으로 선택했는데, 이는 시뮬레이션에서의 점군과 실제에서의 점군 모양이 매우 유사하여 도메인 갭이 작기 때문입니다. 아래 그림에서 볼 수 있듯이, 시뮬레이터 상의 로봇-물체 장면을 깊이 카메라로 찍어 얻은 점군과 실제 로봇에서 동일한 동작을 했을 때 얻은 점군은 형태적으로 거의 일치하지만, RGB 이미지의 경우 시뮬레이터와 실제 간 격차가 큽니다. 이러한 이유로 **point cloud 기반 시각 정보**를 선택함으로써 더욱 원활한 지식 이전을 도모했습니다.

 *Figure 2: 시뮬레이션 vs 실제의 관측 비교. (좌) 시뮬레이터 내부에서 본 장면(RGB 영상)과 그로부터 얻은 물체+손 점군. (우) 실제 로봇에서 동일한 동작을 실행할 때의 RGB 카메라 영상과 깊이 카메라 점군. RGB 이미지의 경우 시뮬레이터(왼쪽 사진)와 실제(오른쪽 사진) 배경 등이 확연히 다르지만, 점군 형태는 시뮬레이션과 실제가 거의 일치함을 알 수 있다. 점군 기반 관측을 활용하면 시뮬레이션 학습 결과를 실제에 옮길 때 이러한 차이를 최소화할 수 있다.*

### 2. **학습 구조 및 네트워크 구성**  
로봇 공감각 표현을 통해 시각-촉각 통합 데이터를 얻었다면, 이를 활용해 **강화학습(Reinforcement Learning, RL)**으로 손내 조작 정책을 학습해야 합니다. 그러나 점군 형태의 고차원 관측을 직접 사용하여 RL을 수행하면 학습이 매우 어려울 수 있습니다. 이를 해결하기 위해 저자들은 **교사-학생 학습(teacher-student training)** 구조를 도입했습니다. 이는 **“크로스 앰바디먼트 프리트레이닝(Cross-Embodiment Pretraining)”**의 일종으로 볼 수 있는데, 초기에는 **교사 정책(teacher policy)**이 **보다 쉬운 표현 공간(easier embodiment)**에서 학습되고, 이후 이를 **학생 정책(student policy)**이 **실제 사용할 센서 구성(actual embodiment)**으로 모사하도록 함으로써 두 다른 환경 간 지식을 이전합니다.

먼저 **교사 정책**은 시뮬레이터 상에서 비교적 접근하기 쉬운 **상태 표현**을 사용하여 학습됩니다. 구체적으로 교사 정책은 로봇의 **관절 상태(proprioception)**, 각 FSR 촉각 센서의 **이진 접촉 여부**, 물체의 **정확한 포즈(pose)**와 **형상 임베딩(shape feature)** 정보를 모두 관측으로 사용합니다. 즉, 시뮬레이터이기에 가능한 **물체의 위치와 모양에 대한 완전한 정보**까지 포함하여 RL로 최적 정책을 학습하는 것입니다. 이렇게 하면 높은 차원의 이미지나 점군을 직접 다루지 않아도 되므로 학습 난이도가 낮아지고, 비교적 적은 샘플로도 좋은 정책을 얻을 수 있습니다. 논문에서는 교사 정책을 **PPO 알고리즘**을 통해 훈련하였고, actor-critic 구조의 **MLP (Multi-Layer Perceptron)** 네트워크로 정책함수를 표현했습니다. 그 결과 교사 정책은 주어진 작업들에 대해 안정적인 성능을 얻었으며, 이 교사가 생성하는 시연 데이터를 다음 단계에 활용합니다.

*Figure 3: **교사-학생 정책 학습 파이프라인**. (상단) **교사(Teacher)** 정책은 로봇의 관절 상태, 이진 촉각 신호, 물체의 포즈와 사전 추출한 형상 특징 등을 입력으로 받는 actor-critic 신경망이다. 교사 정책은 시뮬레이터에서 강화학습(PPO)으로 훈련되며, 비교적 쉬운 입력을 사용하기 때문에 학습이 용이하다. (하단) 훈련된 교사 정책으로부터 **학생(Student)** 정책을 학습시킨다. 학생 정책은 실제와 동일한 센서 입력(프로프리오셉션, 이진 촉각, 카메라 점군+증강 점군+촉각 점군 통합)을 사용하며, 통합 점군 입력은 **PointNet** 기반 encoder로 임베딩 벡터를 추출한 후 다른 상태 입력과 결합하여 actor MLP에 전달된다. 교사 정책의 행동을 모방학습으로 전달받아 초기 학습을 하고, 이후 DAgger 알고리즘으로 지속적으로 교정한다. 붉은 색 실선 화살표는 교사 정책의 결정(action)을 학생이 모사하는 과정을 나타낸다.*

다음으로 **학생 정책**은 실제 로봇이 사용할 **시각-촉각 점군 관측**을 입력으로 하여 학습됩니다. 학생 정책의 입력은 위에서 설명한 **로봇 공감각 점군(시각+촉각 통합)**과 로봇 관절 상태, 이전 스텝의 명령 등의 정보로 구성됩니다. 학생 정책 네트워크는 **PointNet 기반의 점군 encoder**를 사용하여 수천 개의 점들을 저차원 임베딩 벡터로 변환하고, 여기에 로봇의 관절각, 접촉 센서 이진값 등의 추가 상태를 concatenate하여 **Actor MLP**에 입력합니다. (학생 정책 단계에서는 순수한 RL로 학습하는 것이 아니라, 이미 학습된 교사 정책을 모방하여 학습하므로 별도의 critic 네트워크는 두지 않았습니다.)

학생 정책의 학습은 **모방 학습(Imitation Learning)** 기법을 통해 이루어집니다. 먼저 시뮬레이터에서 학습된 교사 정책을 이용하여 대량의 **상태-행동 데이터셋** D를 수집합니다 (논문에서는 총 512만 **transition**을 교사로부터 모았습니다). 그런 다음 이 데이터로 학생 정책을 **Behavior Cloning (BC)**, 즉 **행동 복제 학습**으로 사전 학습시킵니다. 이렇게 1단계로 초기 학생 정책을 얻은 후, 2단계로는 **DAgger (Dataset Aggregation) 알고리즘**을 적용하여 학생 정책을 미세 조정합니다. DAgger 단계에서는 현재 학생 정책이 수행하다가 실패할 때 교사 정책으로부터 정답 행동을 다시 받아서 데이터에 추가하고 학생을 업데이트함으로써, 학생 정책이 자신의 실수에 대해 교정학습을 할 수 있게 합니다. 이러한 2단계 distillation(지식 증류) 과정으로 최종 **시각-촉각 기반 정책**이 완성됩니다. 정리하면, **교사-학생 파이프라인**을 통해 고차원 **visuotactile** 입력 공간에서의 학습을 간접적으로 수행함으로써 학습 효율과 성능을 높인 것입니다. 이 방식은 서로 다른 센서 구성(embodiment)을 연결하는 일종의 **교차 몸체 사전학습(cross-embodiment pretraining)**이라고 볼 수 있으며, 실제 로봇에서 필요한 **시각+촉각 정책을 효과적으로 얻는 핵심 기술**입니다.


## 네트워크 입출력 구조 상세 분석

이 논문에서는 강화학습(RL)을 통해 로봇이 물체를 손 안에서 조작하는 작업(in-hand manipulation)을 수행합니다. RL에서는 입력 관측 정보(observation)와 출력 행동(action)의 차원을 정확하게 설계하는 것이 중요합니다.

---

## 1. Observation의 구성 및 차원 (입력)

이 논문에서 제안하는 로봇 정책의 observation은 크게 다음의 4가지로 구성됩니다 :

### (1) 시각 점군 (Visual Point Cloud)

- **센서**: Azure Kinect RGB-D 카메라의 깊이 데이터로부터 추출된 점군.
- **데이터 형태**: 환경(물체+배경)을 나타내는 3D 점군 데이터.
- **차원**: 시각 점군의 경우 일반적으로 **약 300개의 점들**로 샘플링되어 사용됩니다 .
- 각 점의 특징 벡터는 총 **6차원**으로 구성됩니다:
  - **3D 좌표값**: (x, y, z)
  - **원핫 인코딩** (출처를 구분하기 위해): `[1, 0, 0]` → 시각 카메라 점군임을 나타냄.

따라서 시각 점군 데이터의 전체 차원은 약 **(300, 6)** 정도로 표현됩니다.

---

### (2) 촉각 점군 (Tactile Point Cloud)

- **센서**: Allegro 로봇 핸드의 16개의 FSR (Force Sensitive Resistor) 촉각 센서.
- **데이터 형태**: 이진 촉각 값 (접촉 여부)에서 생성한 3D 점군.  
  로봇 손 메쉬 표면에서 접촉이 발생한 센서 위치를 기준으로 점들을 샘플링하여 생성합니다.
- **차원**: 촉각 점군은 보통 **약 80개의 점들**로 샘플링됩니다 .
- 각 점의 특징 벡터 역시 **6차원**:
  - **3D 좌표값**: (x, y, z)
  - **원핫 인코딩**: `[0, 1, 0]` → 촉각 점군임을 나타냄.

따라서 촉각 점군의 전체 차원은 약 **(80, 6)**이 됩니다.

---

### (3) 로봇 손 증강 점군 (Augmented Robot Hand Point Cloud)

- **센서**: 로봇의 현재 관절 상태로부터 생성된 로봇 손의 자체 형태를 나타내는 점군.
- **데이터 형태**: 로봇 손 자체의 메쉬 모델에서 샘플링된 점군.
- **차원**: 이 역시 약 **80개의 점들**로 샘플링 .
- 각 점의 특징 벡터는 역시 **6차원**으로 구성:
  - **3D 좌표값**: (x, y, z)
  - **원핫 인코딩**: `[0, 0, 1]` → 로봇 손 점군임을 나타냄.

따라서 로봇 손 증강 점군의 전체 차원은 **(80, 6)**입니다.

---

### (4) 추가 상태 정보 (Additional State Vector)

점군 외에 별도로 제공되는 로봇 관절과 이전 액션에 대한 정보를 담은 추가 상태 벡터:

- **Proprioception (로봇 자기 감각)**:
  - 로봇 손의 관절각: Allegro 핸드는 **16개의 자유도**를 가집니다. 각 관절의 위치를 나타내는 **16차원 벡터** 사용 .
  - 손의 관절 각속도는 포함되지 않고 위치만 포함되었습니다 .
- **Binary Tactile Vector**:
  - 16개의 촉각 센서에서 오는 이진 접촉 여부: **16차원 벡터** .
- **이전 행동(previous action)**:
  - 이전 행동 정보를 네트워크가 참조할 수 있도록 제공되며, 이는 역시 Allegro 핸드의 16개 관절 각도의 목표 위치로 표현된 **16차원 벡터** .

결과적으로, 추가 상태 정보는:

- 관절 위치 (**16**) + 이진 촉각 신호 (**16**) + 이전 행동 (**16**)  
→ 총 **48차원**의 벡터로 구성됩니다.

---

### Observation 최종 정리:

종합하면, 네트워크에 입력되는 Observation은 다음과 같은 형태로 최종 정리됩니다:

| 입력 정보 (Observation)             | 점 개수 | 각 점의 특징 | 전체 차원   |
|-----------------------------------|-------|-------------|------------|
| 시각 점군 (Visual)                  | 300   | 6차원        | (300, 6)   |
| 촉각 점군 (Tactile)                 | 80    | 6차원        | (80, 6)    |
| 로봇 손 증강 점군 (Robot Augmented) | 80    | 6차원        | (80, 6)    |
| 추가 상태 정보 (Additional state)    | -     | -           | (48,)      |

**최종 Observation Dimension**: `(460개의 점, 각 6차원) + 48차원 상태 벡터`

---

## 2. Action의 구성 및 차원 (출력)

본 논문의 행동(action)은 Allegro 로봇 핸드의 **16개 관절의 목표 각도**입니다 .

- **차원**: 행동의 차원은 정확히 **16차원**입니다.
- 각 액션 값은 다음 스텝에서 로봇의 각 관절이 이동해야 할 목표 관절 각도를 지정합니다.
- 실제 로봇에서는 목표 관절 각도로 이동하는 방식으로 모터 컨트롤이 이루어지며, 매 step마다 약 10Hz로 새로운 액션을 출력합니다 .

### Action 최종 정리:

| 출력 행동 (Action)   | 차원 | 값의 의미                  |
|-------------------|-----|------------------------|
| 로봇 관절 목표 각도 | 16  | 로봇 핸드의 각 관절의 목표 위치 |

---

## 요약 정리 ✅

- **Observation**:
  - **점군 입력**: (총 460점, 각 6차원)
  - **추가 상태 입력**: 48차원 벡터
  - 점군은 PointNet 기반 Encoder를 거쳐 하나의 저차원 임베딩 벡터로 변환된 후, 상태 벡터와 결합됩니다.

- **Action**:
  - **16차원 벡터**: 각 값은 로봇의 16개 관절 목표 위치를 의미.

위와 같은 정확한 Observation 및 Action 차원을 이용해, 네트워크는 복잡한 in-hand manipulation 작업을 효율적으로 학습할 수 있습니다. 이를 통해 높은 Sim-to-Real 성능을 달성한 것이 이 논문의 핵심 기여입니다.

## 실험 설정 및 결과
### 1. 실험 환경 및 작업 구성  
**환경:** 실험에는 **유니버설로봇 XArm6 로봇 팔** 끝에 장착된 **알레그로(Allegro) 로봇 핸드**(4개의 손가락, 16자유도)가 사용되었습니다. 로봇 손바닥과 마디마디에는 16개의 FSR 촉각 센서가 부착되어 있으며, 이는 접촉시 아날로그 압력 값을 내지만 일정 임계값 이상이면 **이진 접촉**으로 간주합니다. 시각 센서는 로봇 손 앞을 향하도록 배치된 **Azure Kinect 깊이 카메라**로, RGB-D 데이터를 수집하되 알고리즘에는 **Depth로 얻은 점군만 사용**합니다. 시뮬레이션 환경은 NVIDIA IsaacGym으로 구현되어 실제와 동일한 로봇 모델과 물체 모델, 그리고 가상 깊이 카메라/촉각 센서를 갖춥니다. 시뮬레이터에서 접촉 센서는 실제와 동일하게 동작하도록 이진 신호로 모사되며, 제어 주기도 실제와 동일하게 10 Hz로 설정되었습니다. 학습된 정책은 **추가 파인튜닝 없이** 그대로 실제 로봇에 이식하여 검증되었습니다.

**작업(Task):** 논문에서는 손내 물체 회전과 관련된 세 가지 벤치마크 작업에 초점을 맞춥니다:

- *(i) Wheel-Wrench Rotation* – **십자 렌치 회전:** 자동차 바퀴 렌치처럼 십자형으로 교차된 막대를 손으로 쥐고, 한쪽 끝을 다 돌리면 다음 손잡이로 **재파지(re-grasp)**하여 연속 회전하는 과제입니다. 로봇은 현재 잡은 손잡이를 다 돌렸다면 **시각적으로 새 손잡이 위치를 찾아** 옮겨 잡아야 하며, 동시에 **촉각으로 놓치지 않고** 회전 힘을 가해야 합니다 ([](https://www.touchprocessing.org/2023/camera_ready/camera_ready_7.pdf#:~:text=successfully%20complete%20this%20task%2C%20the,Axis%20Rotation%3A%20This%20task)). 이 작업은 한 손으로 연속 회전하기에 난이도가 높으며, 시각과 촉각 둘 다가 필수적으로 요구됩니다.

- *(ii) Double-Ball Rotation* – **이중 공 회전:** 동일한 크기의 공 두 개를 손으로 동시에 잡고 서로의 주위를 돌도록 회전시키는 작업입니다. 두 공은 똑같이 생겼으므로 시각적으로 구분이 어려울 수 있지만, 촉각만으로는 어느 공이 어디에 있는지 **식별 불가**합니다. 따라서 **두 공의 상대적 위치를 파악하는 시각정보**와 **미끄러지지 않게 잡는 촉각정보**의 결합이 핵심입니다. 이 작업은 손으로 두 개의 물체를 한꺼번에 다루어야 하므로 난이도가 매우 높습니다 (로봇 손가락의 작은 움직임으로는 두 공을 회전시키기에 부족하고, 크게 움직이면 공을 떨어뜨릴 위험이 있습니다).

- *(iii) Three-Axis Rotation* – **3축 회전:** 물체를 z축뿐 아니라 고정된 x축 또는 y축을 중심으로도 회전시키는 일반적인 회전 조작 작업입니다. 이 작업을 통해 로봇 손이 특정 축으로 물체를 돌리는 능력을 평가하며, 특히 학습 단계에서 보지 못한 **다양한 모양의 물체들**에 대해서도 일반화할 수 있는지 테스트합니다. 학습 시에는 단순 기하학 형태의 물체들을 사용하고, 테스트 시에는 실제 일상 물체들(예: 마커펜, 토마토 등)을 줘서 얼마나 **모양 변화에 견고**한지 확인했습니다.

각 작업에 대해 시뮬레이터에서 교사 정책으로 충분히 학습한 후, 앞서 설명한 모방 학습을 거쳐 학생 정책을 얻었습니다. 최종 정책들을 아무 수정 없이 실제 로봇에 이식하여 평가를 진행했으며, **성공적으로 물체를 떨어뜨리지 않고 회전시키는 시간**(Time-to-Fall, TTF)과 **누적 회전 각도** 또는 **회전 횟수**(Cumulative Rotation) 등을 주요 성능 지표로 사용했습니다.

### 2. 주요 결과 및 성능 비교  
**시뮬레이션 단계 성능:** 먼저 시뮬레이터 상에서 교사/학생 정책들의 성능을 비교한 결과, 제안된 시각-촉각 공감각 정책이 **기존 방법들 대비 일관되게 우수한 성능**을 보였습니다. 예를 들어 학습 곡선을 보면, **Ours (제안 방식)**가 같은 시간 내 **Visual RL**(시각점군+촉각을 직접 RL로 학습)보다 훨씬 빠르게 높은 보상에 도달했고, **PS (Partial State)**라고 불리는 이전 상태기반 방법보다도 최종 성능이 높았습니다. 이는 물체의 정밀한 조작에는 **정확한 물체 상태 정보**(교사가 가진 위치/모양 정보에 대응)가 매우 중요하며, 우리 방법이 비록 직접 물체 pose를 알지는 못하지만 공감각 점군을 통해 이를 효과적으로 추론하고 있기 때문이라고 분석됩니다. 또한 동일한 조건에서 단일 단계로 RL을 한 경우(Visual RL 등)보다 교사-학생 2단계 학습이 **표현 학습 및 탐색 측면에서 유리**함을 보여주었습니다.

**실제 로봇 평가:** 가장 주목할 만한 결과는 **한번도 실제 데이터로 학습하지 않은 정책을 실제 로봇에서 바로 사용**했음에도 불구하고 높은 성능을 발휘했다는 점입니다. Table 3에 요약된 실제 실험 결과를 살펴보면, 제안된 **Touch+Cam+Aug+Syn (시각+촉각 공감각)** 정책이 모든 작업에서 다른 대조군들을 능가했습니다. 반면 카메라를 사용하지 않는 **Non-visual RL**이나 **Touch-only**와 같은 방식은 일부 간단한 경우 외에는 제대로 물체를 회전시키지 못했습니다. 대표적으로, **휠-렌치 회전 작업**의 경우 공감각 정책은 평균 **1.54회전**을 수행한 반면, 시각을 사용하지 않는 정책들은 **0.25회전**에 그쳤습니다. **이중 공 회전**에서는 공감각 정책이 약 **11.9회전**을 성공시켜 다른 방법들(~7~8회전)에 비해 확연히 더 많이 회전시켰으며, 물체를 놓치기까지 버틴 시간도 약 **17초**로 타 방법들(10~11초)에 비해 길었습니다. **3축 회전 작업**의 경우에도 z축 기준 회전에서 공감각 정책이 **평균 10.2회전**을 달성해 가장 우수했습니다. 일부 x축, y축 결과에서는 시각이 없는 방법들이 물체를 거의 돌리지 못하지만 오래 붙잡고 있는 반면(거의 움직이지 않으므로 떨어뜨릴 위험도 낮음), 시각이 있는 정책들은 **능동적으로 물체를 회전**시키려다 보니 때때로 더 빨리 놓치는 경우도 있었습니다. 그럼에도 불구하고 전반적으로 시각+촉각 결합 정책이 **회전 각도를 크게 향상**시켰으며, **실제 환경에서 그 우수성이 더욱 두드러졌다**는 것이 저자들의 분석입니다.

특히 재미있는 관찰은, **시각 정보가 있는 정책**은 실행 중에 물체가 손 바닥 중심에서 벗어나면 이를 **감지하고 교정 동작**을 취하는 반면, **시각이 없는 정책**은 물체가 어디로 가는지 모르기 때문에 학습된 반복 패턴대로만 움직여버려 물체가 이상한 위치에 걸려도 수정하지 못한다는 것입니다. 이러한 차이는 시각과 촉각의 통합이 왜 중요한지를 단적으로 보여줍니다. 시각이 있으면 로봇은 매 순간 물체의 **글로벌 위치**를 파악하여 전략을 미세하게 조정할 수 있고, 촉각이 있으면 **로컬 접촉 상태**를 느끼면서 힘 조절을 할 수 있기 때문에, 두 감각을 모두 가진 정책이 훨씬 **유연하고 견고**하게 동작합니다.

또한 **No-Synesthesia** 대조군과의 비교를 통해, 제안된 **공감각 점군 표현의 효과**를 검증했습니다. No-Synesthesia 정책은 시각 점군과 로봇 증강 점군은 사용하되 촉각 정보를 단순 이진값으로만 입력한 경우인데, 이 경우 공감각 정책 대비 성능이 떨어졌습니다. 이는 **촉각을 공간적으로 표시한 점군 표현(Synesthesia)**이 단순한 촉각 신호의 Concatenation보다 실제 조작에 유의미한 기여를 함을 시사합니다. 요컨대, **우리 정책(Ours)**은 모든 작업에서 다른 방식을 능가했고, 특히 **시뮬레이션에서 무리 없이 동작하던 시각 전용 정책이 실제로 오면 성능이 저하되는 반면**, 공감각 정책은 **시뮬레이션과 실제 간 격차가 매우 작아** 실제에서도 성능을 유지하거나 오히려 격차가 벌어지는 모습을 보였습니다. 이는 본 논문의 목표였던 **시각-촉각 융합을 통한 강인한 Sim2Real 성능**이 입증된 결과라고 볼 수 있습니다.

## 논의 및 분석
**의의:** *Robot Synesthesia*는 로봇이 복잡한 접촉-rich 작업에서 **시각과 촉각을 자연스럽게 통합**할 수 있는 새로운 방법론을 제시했습니다. 기존에는 두 감각의 데이터 형태 차이로 인해 병합이 쉽지 않았던 문제를, **점군이라는 공통 포맷**으로 변환함으로써 해결한 점이 특히 돋보입니다. 이를 통해 **학습 효율**과 **성능** 두 마리 토끼를 잡았는데, 교사-학생 구조를 적용하여 시뮬레이션에서 충분히 학습한 후 실제로 투입함으로써 **실제 데이터 없이도** 높은 성능을 달성했습니다. 이러한 접근은 향후 다양한 로봇 manipulation 분야에 응용될 수 있는 포괄적인 아이디어로서, 예를 들어 다른 형태의 촉각 센서나 로봇 플랫폼에도 적용하여 **멀티모달 정책학습**을 쉽게 할 수 있는 길을 열었습니다.

**강인성:** 본 연구의 정책은 훈련에 사용하지 않은 새로운 물체들에도 어느 정도 일반화하여 동작할 수 있음을 보여주었습니다. 실제 실험에서 동그란 공으로 학습한 정책이 토마토나 감자 같은 전혀 다른 물체 쌍에도 적용되어 회전을 시도하는 등, **모양 변화에 대한 강인성**을 확인했습니다. 이는 점군 표현 덕분에 물체의 형태적 차이를 임베딩 공간에서 학습하도록 한 효과로 해석됩니다. 또한 시각 정보의 도입으로 정책이 보다 **상황 적응적**으로 변해, 물체가 미끄러질 때 재조정하는 등의 행동이 관찰된 점도 유의미합니다. 다만 일부 경우에 시각 기반 정책이 지나치게 적극적으로 움직이다 보니 물체를 빨리 떨어뜨리는 현상도 있었는데, 이는 **안정성 vs. 민첩성** 사이의 트레이드오프로 볼 수 있습니다. 실제 응용에서는 작업 요구사항에 따라 시각/촉각 신호의 가중치를 조절하는 등이 필요할 수 있습니다.

**한계:** 현재 사용된 촉각 센서는 16개의 FSR로, 접촉 위치를 이진적으로 제공하기 때문에 **촉각 해상도**가 높지 않습니다. 복잡한 물체의 미세한 표면 질감이나 미끄러짐 방향까지 인지하려면 이보다 풍부한 촉각 정보가 필요할 수 있습니다. 논문에서도 후속 작업으로 **광학식 촉각 센서(optical tactile sensors)** 통합을 제시하고 있는데, 예를 들어 GelSight와 같은 비전 기반 촉각센서를 사용하면 **고해상도 촉각 이미지**를 점군 등과 함께 활용하는 방향도 가능할 것입니다. 또한 본 연구는 주로 **연속 회전** 작업에 집중하였는데, 향후에는 특정 목표 각도로 돌리는 **goal-conditioned 회전**이나, 회전 이외에 **이동/정렬 등의 복합 조작**으로 확장해 볼 여지도 있습니다. 마지막으로, 시각-촉각 이외의 모달리티(예: 힘-토크 센서나 음향 센서 등)도 함께 융합한다면 로봇의 환경에 대한 **멀티모달 이해**를 한층 높일 수 있을 것입니다.

**결론:** *Robot Synesthesia*는 **시각과 촉각의 경계**를 허물어 로봇이 마치 **“만져서 보는”** 새로운 방식으로 세상을 인식하게 함으로써, 난이도 높은 손내 물체 조작을 가능하게 한 흥미로운 연구입니다. 사람의 감각 통합에서 영감을 얻은 아이디어를 공학적으로 구현하여 실질적인 성능 향상을 입증했다는 점에서 의의가 큽니다. 시각-촉각 통합은 향후 인간과 상호작용하는 로봇, 서비스 로봇 등에서 필수적인 능력이 될 것이므로, 본 논문의 접근법은 그 중요한 한 걸음을 내딛은 것으로 평가됩니다. 앞으로 더 다양한 환경과 과제에 이 기법이 적용되고 발전되어, 로봇이 더욱 사람처럼 **다양한 감각을 자유자재로 활용**하는 미래를 기대해 봅니다.

# Reference

- [https://www.touchprocessing.org/2023/camera_ready/camera_ready_7.pdf](https://www.touchprocessing.org/2023/camera_ready/camera_ready_7.pdf)