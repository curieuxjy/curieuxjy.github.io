---
title: "📃VCGS"
description: Variational Constrained Grasp Sample
date: "2024-03-17"
categories: [grasp, pointcloud, vae, paper]
toc: true
number-sections: true
image: ../../images/2024-03-17-vcgs/2.png
---

<center>
<img src="../../images/2024-03-17-vcgs/1.png" width="67%" />
</center>

이번 포스팅은 [Variational Constrained Grasp Sample](https://arxiv.org/abs/2302.10745) 논문을 읽고 정리한 내용입니다. 해당 논문은 IROS 2023 학회에 Accept된 논문으로, 특정 대상 영역에 대한 제약을 가진 6자유도(DoF) Grasp을 샘플링하기 위한 새로운 생성적 그리핑 샘플링 네트워크, VCGS를 소개합니다. 뿐만 아니라 1,400만 개 이상의 훈련 샘플을 포함하는 새로운 데이터셋 CONG를 구축한 내용을 발표했습니다. 제안된 VCGS가 시뮬레이션 및 실제 테스트에서 비교 모델인 GraspNet보다 10-15% 높은 그리핑 성공률을 보이며, 2-3배 더 효율적인 것을 보여준 논문입니다. 

# Introduction

로봇 팔, Manipulator로 물체를 잡는 Task는 공정에서부터 가정에서 쓰일 수 있는 보조 역할까지 할 수 있는 기본적인 Task라고 볼 수 있습니다. 따라서 물체를 잡는다는 Task, 즉 Grasping(혹은 Gripping)을 인지 단계부터 제어 모션 단계까지 잘 할 수 있는 방법을 찾고자 하는 연구들이 많이 진행되어 왔습니다. 이러한 배경 하에, 특정 객체 부분과의 상호작용을 요구하는 작업에서 정밀한 그리핑 위치를 지정하는 것의 중요성이 부각되었습니다. 본 연구는 이 문제를 해결하기 위해 특정 대상 영역에 대한 제약을 가진 6자유도 그리핑을 샘플링하는 새로운 방법론을 제안합니다.

<center>
<img src="../../images/2024-03-17-vcgs/2.png" width="67%" />
    <figcaption>VCGS Process</figcaption>
</center>

6자유도 그리핑은 보통 top-down 방식의 4자유도 그리핑보다 더 자유도가 높은 제어를 요구하기 때문에 더 도전적인 Task라고 볼 수 있습니다. 물리 세계의 3d 물체를 인식하기 위해 point cloud data로 3차원 물체를 인식하고 물체를 안정적으로 잡을 grasp pose를 최종적으로 만들어서 Manipulator를 움직이는 것이 목표입니다.

<center>
<img src="../../images/2024-03-17-vcgs/3.png" width="67%" />
    <figcaption>CONG Dataset Comparison</figcaption>
</center>
해당 연구에서는 VCGS라는 안정적인 grasp pose를 제안할 뿐만 아니라 해당 모델을 잘 학습시킬 수 있고 Constrained Grasp에 대한 데이터셋을 만들어서 **CONG**라는 데이터셋을 만드는 부분에도 기여를 했습니다. Table 1에서도 볼 수 있듯이 다른 데이터셋에 비해서 절대적인 데이터셋의 크기가 클 뿐만 아니라 Task-agnostic한 Constraint에 대한 정보도 들어있는 데이터셋으로 유일하기 때문에 그 특성이 중요하다고 볼 수 있습니다.



# Problem Statement



Grasp pose를 생성하기 위해 다음과 같이 문제의 용어들을 정의합니다. 연구에서 사용한 Gripper는 단순한 2개의 손가락으로 집는 모션을 하는 그리퍼로 parallel-jaw grasp pose를 정하면 잡는 모션 제어를 할 수 있습니다.

- **G**: parallel-jaw grasp poses
  - 7차원: quaternion[4] + 3d-position[3]
  - Target area **A**에 있고 stable(**S**=1)한 조건이 만족될 때
- **O**: Object Point-cloud (차원: **N**x3)
- **A**: Target Area Point-cloud (차원: **M**x3)



<center>
<img src="../../images/2024-03-17-vcgs/4.png" width="67%" />
    <figcaption>Point cloud data와 Joint Probability Separation</figcaption>
</center>

VCGS가 학습해야 하는 목표는 $P(G, S | O, A)$ 를 최대화하는 것 입니다. 이 수식을 풀어서 생각해보면, object의 point cloud data와 잡아야 하는 부분인 target area point cloud data가 주어졌을 때 성공적인($S$) grasp $G$를 생성하는 확률을 높이는 것 입니다. 이를 Grasp Sampler와 Grasp Evaluator 각각으로 Approximation하는 네트워크로 두어 학습하는 과정을 진행합니다.

<center>
<img src="../../images/2024-03-17-vcgs/5.png" width="67%" />
    <figcaption>Grasp Pose 설명</figcaption>
</center>


# Method

이번 장에서는 앞서 설명한 대로 `Grasp Sampler`와 `Grasp Evaluator`로 구성되어 있는 VCGS 모델과 CONG dataset에 대해서 이야기해보려고 합니다.

## Grasp Sampler

VCGS의 Grasp Sampler의 기본적인 틀은 Conditional Variational Autoencoder (CVAE) 구조를 차용해서 아래와 같이 만들었습니다. Grasp pose를 다양하게 Sampling하기 위해 Encoder와 Decoder를 VAE 구조를 차용하여 Gaussian Prior Distribution을 이용해서 가능한 다양한 Grasp pose를 생성할 수 있도록 설계했습니다.

<center>
<img src="../../images/2024-03-17-vcgs/6.png" width="67%" />
    <figcaption>6</figcaption>
</center>

Encoder

<center>
<img src="../../images/2024-03-17-vcgs/7.png" width="67%" />
    <figcaption>7</figcaption>
</center>

## Grasp Evaluator

학습동안에 좋은 Grasp data만 학습하는 Encoder가 더 다양한 Grasp data를 경험할 수 있도록 Evaluator Network를 추가하여 Bad Grasp에 대한 경험도 할 수 있도록 만들었습니다.

<center>
<img src="../../images/2024-03-17-vcgs/8.png" width="67%" />
    <figcaption>8</figcaption>
</center>

## CONG Dataset

<center>
<img src="../../images/2024-03-17-vcgs/9.png" width="67%" />
    <figcaption>9</figcaption>
</center>

구성 요소
- $O$: object point cloud
- $G*$: target area A에서 랜덤하게 샘플링된 successful grasp

데이터셋 구축 과정

1. object를 원점에 랜덤한 orientation으로 놓고 O[N x 3] rendering
2. O에서 query point I[K x 3]를 샘플링(K << N) - Farthest Point Sampling 사용
3. 각 query point xi(∈I)에 대해서 반경 ri(~U[0, R]) 이웃한 point Ai들을 모두 찾음
    - 이때 R은 mesh bounding box의 대각선 길이
4. [grasp center point]와 [Ai의 어떤 점]이라도 최대 d인 모든 G를 찾아냄

<center>
<img src="../../images/2024-03-17-vcgs/10.png" width="67%" />
    <figcaption>10</figcaption>
</center>


# Experiment

실험에서 주목해봐야할 2가지 질문은 아래와 같습니다.

1. constrained grasping에서 grasp success rate로 나타내지는 성능
2. constrained grasp sampler가 unconstrained sampler보다 target-driven grasping에서 얼마나 sample efficient한지?

<center>
<img src="../../images/2024-03-17-vcgs/11.png" width="67%" />
    <figcaption>11</figcaption>
</center>

실험 셋팅은 다음과 같습니다.

- 비교군: GraspNet
- Simulation & Real Robot 둘 다 확인
- Evaluation Metric
    - successful/total
    - successful의 기준: 물체를 들고 predefined motion(linear acc + angular acc)을 진행한 후 물체가 gripper에 안정적으로 들려있는지

<center>
<img src="../../images/2024-03-17-vcgs/12.png" width="67%" />
    <figcaption>12</figcaption>
</center>

## A. Simulated Robotic Grasping

- best grasp, NOT the best reachable
- gripper와 object 둘 다 free-floating 상황
- IsaacGym simulator 사용
  - Acronym dataset에서 123개의 random object
  - 물체의 observation data로는 **depth sensor** 사용


- 시뮬레이터에서 2개 실험 진행
  - **Unconstrained sampling**: target area 없이 그냥 grasp을 샘플링. A=O
  - **Constrained sampling**: target area에서만 grasp 생성

- 비교군
  - **GraspNet**: SOTA
  - **GraspNetTaI**: Target as Input. target area만 grasp sampling network에 넣어준 모델 

<center>
<img src="../../images/2024-03-17-vcgs/13.png" width="67%" />
    <figcaption>13</figcaption>
</center>

- VCGS는 GraspNet보다 3배 이상의 Ratio of grasps kept %를 보여줌
  - 네트워크 입력으로 Constrained grasp sampling을 넣어주는 것의 이점에 대한 증거

- GraspNetTaI는 GraspNet보다 Success Rate가 낮음
  - 물체의 전체 정보(global)를 사용하는 것이 특정한 target area에 대한 정보(local)를 사용하는 것보다 좋음을 알 수 있음

- GraspNet은 Success Rate가 \# of grasps sampled에 영향을 받음
  - 만약 Unconstrained 경우라면 더 많은 sampling이 필요하다고 볼 수 있음
  - RGK가 #GS에 영향을 받지 않은 결과를 보고도 확인할 수 있는 가설임


<center>
<img src="../../images/2024-03-17-vcgs/14.png" width="67%" />
    <figcaption>14</figcaption>
</center>


<center>
<img src="../../images/2024-03-17-vcgs/15.png" width="67%" />
    <figcaption>15</figcaption>
</center>


마지막으로 해당 논문의 발표영상을 마지막으로 이번 포스팅을 마무리하도록 하겠습니다.

{{< video https://youtu.be/kTZD6cxTtv8?si=-WMLnODehN9f3rXR >}}


# Reference

- [Original Paper: VCGS](https://arxiv.org/abs/2302.10745)
- [Presentation Video](https://youtu.be/kTZD6cxTtv8?si=-WMLnODehN9f3rXR)
- [Baseline Model: GraspNet paper](https://arxiv.org/abs/1905.10520)