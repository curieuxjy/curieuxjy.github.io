---
title: "📃Touch in the Wild 리뷰"
date: 2025-07-18
categories: [touch, visuo-tactile, gripper]
toc: true
number-sections: true
description: Learning Fine-Grained Manipulation with a Portable Visuo-Tactile Gripper
---


- [Paper Link](https://binghao-huang.github.io/touch_in_the_wild//Touch_in_the_Wild_arxiv.pdf)
- [Github Link](https://github.com/YolandaXinyueZhu/touch_in_the_wild)
- [Project Link](https://binghao-huang.github.io/touch_in_the_wild/)

1. 이 연구는 휴대 가능하고 가벼운 촉각 센서가 통합된 그리퍼를 개발하여, 다양한 실제 환경에서 시각 및 촉각 데이터를 동기화하여 대규모로 수집할 수 있게 했습니다.
2. 수집된 데이터셋을 기반으로, 시각 및 촉각 신호의 고유한 특성을 보존하면서 이들을 통합하는 교차 모달 표현 학습 프레임워크를 제안하여 상호작용 관련 접촉 영역에 집중하는 해석 가능한 표현을 학습합니다.
3. 이 표현은 섬세한 조작 작업에서 로봇 정책 학습의 효율성과 정확성을 크게 향상시켜, 외부 교란에도 견고한 정밀 조작(예: 시험관 삽입, 피펫을 이용한 유체 이동)을 가능하게 합니다.


<center>
<img src="../../images/2025-07-18-touch-wild/2.png" width="100%" />
</center>


<center>
<img src="../../images/2025-07-18-touch-wild/1.png" width="100%" />
</center>

---

# Brief Review

이 논문은 정밀한 로봇 조작에 필수적인 촉각 피드백을 통합하기 위해 휴대 가능하고 가벼운 촉각-시각 그리퍼(visuo-tactile gripper)와 이에 기반한 크로스-모달(cross-modal) 표현 학습 프레임워크를 제안합니다. 기존의 핸드헬드 그리퍼는 시각 정보에만 의존하여 실제 환경(in-the-wild)에서의 세밀한 접촉 기반 조작에 한계가 있었습니다. 이를 해결하기 위해 저자들은 두 가지 주요 과제를 제시합니다:

(i) 휴대 가능한 견고한 촉각 하드웨어의 부재,

(ii) 이질적인 촉각 및 시각 데이터로부터 효과적인 표현을 학습하는 어려움.

**하드웨어 시스템:**

제안된 휴대용 촉각-시각 그리퍼는 소프트하고 핀 모양의 손가락에 유연한 압전저항(piezoresistive) 촉각 센서를 통합합니다. 이 센서는 3D-ViTac의 3중 레이어 디자인을 기반으로 하며, 다음과 같은 개선 사항을 포함합니다:

1.  **높은 공간 해상도:** 기존 스테인리스 스틸 전극 대신 FPC(Flexible Printed Circuits) 전극을 사용하여 패드당 $12 \times 32$ 텍셀(taxel)의 해상도(각 텍셀 $2 \times 2 \text{mm}^2$ 영역)를 달성하여 미세하고 동적인 접촉 패턴을 포착할 수 있습니다.
2.  **빠르고 확장 가능한 제조:** FPC를 사용하여 도구 없이 5분 이내에 각 패드를 제작하고 그리퍼에 부착할 수 있어 대규모 촉각 데이터 수집에 용이합니다.
이 그리퍼는 맞춤형 Arduino 기반 PCB와 함께 사용되며, 배터리를 포함하여 약 962g으로 장시간 사용에 적합합니다. 데이터 수집은 fisheye 카메라의 시각 정보와 촉각 센서의 데이터를 23Hz로 동기화하여 이루어집니다. 시각-촉각 데이터의 정밀한 정렬을 위해 하드웨어 없이 QR 코드를 통한 호스트 시간 동기화 전략을 사용합니다. 비디오 스트림은 각 데모 시작 전에 현재 호스트 시간을 표시하는 QR 코드를 30Hz로 카메라에 보여주고, 촉각 데이터는 ROS2를 통해 23Hz로 호스트 클록 타임스탬프와 함께 발행됩니다. 오프라인 후처리에서 이 공유 클록 참조를 통해 시각 및 촉각 기록을 정렬합니다.

**방법론:**

이 논문은 두 단계의 학습 프레임워크를 제안합니다.

**1단계: 촉각-시각 표현 학습 (Visuo-Tactile Representation Learning)**

이 단계의 목표는 촉각-시각 엔코더 $E_{\phi}$를 통해 시각 $I \in R^{3 \times 224 \times 224}$와 촉각 $T \in R^{1 \times 24 \times 32}$ 입력을 공동 표현 $z_{\text{fusion}} = E_{\phi}(I, T)$로 융합하는 것입니다. 이 과정은 마스킹된 오토인코딩(masked autoencoding) 방식으로 학습됩니다. 이는 대조 학습(contrastive learning)이 촉각 센서의 미세한 지오메트리 민감 신호를 억제할 수 있다는 점을 고려하여, 부분적으로 관측된 촉각 입력과 시각적 컨텍스트로부터 전체 촉각 이미지를 재구성하는 방식으로 진행됩니다. 최적화 목표는 다음과 같습니다:
$$(\phi^*, \psi^*) = \text{arg min}_{\phi, \psi} E_{(I,T) \sim D_{\text{pretrain}}} ||T - D_{\psi}E_{\phi}(I, T)||_2^2$$
여기서 $E_{\phi}$는 촉각-시각 엔코더이고 $D_{\psi}$는 촉각 재구성 디코더입니다.

*   **촉각 엔코더:**
    *   두 개의 손가락 끝 패드로부터의 촉각 판독값(각 $1 \times 12 \times 32$)을 수직으로 쌓아 $1 \times 24 \times 32$ 촉각 이미지를 형성합니다.
    *   단일 채널 맵은 고정된 컬러맵(colormap)을 통해 3채널 RGB 촉각 이미지로 변환됩니다.
    *   이 이미지는 $4 \times 4$ 패치로 분할되어 $6 \times 8$ 패치 그리드를 생성합니다.
    *   학습 중에는 95%의 샘플에서 60-80%의 패치를 학습 가능한 토큰 $T_{\text{mask}}$로 무작위 마스킹합니다.
    *   마스킹된 촉각 입력 $T_{\text{visible}}$은 다음과 같이 정의됩니다: $T_{\text{visible}} = M \odot T + (1 - M) \odot T_{\text{mask}}$ ($M \in \{0, 1\}^{6 \times 8}$은 이진 패치 마스크).
    *   $T_{\text{visible}}$은 3계층 CNN을 통해 768차원의 임베딩 $z_{\text{tac}}$를 생성합니다.
*   **시각 엔코더:**
    *   RGB 이미지 $I$는 CLIP으로 초기화된 ViT-B/16 엔코더에 의해 처리됩니다.
    *   모든 레이어를 미세 조정하며, 최종 [CLS] 토큰이 768차원의 시각 임베딩 $z_{\text{img}}$로 추출됩니다.
*   **크로스-모달 융합:**
    *   촉각 및 시각 특징을 통합하기 위해 두 라운드의 멀티-헤드 크로스-어텐션(Multi-Head Cross-Attention, MHAttn)을 적용합니다.
    *   $z'_{\text{tac}} = \text{MHAttn}(Q = z_{\text{tac}}, K = z_{\text{img}}, V = z_{\text{img}}) \xrightarrow{\text{LayerNorm}} z''_{\text{tac}}$
    *   $z'_{\text{img}} = \text{MHAttn}(Q = z_{\text{img}}, K = z''_{\text{tac}}, V = z''_{\text{tac}}) \xrightarrow{\text{LayerNorm}} z''_{\text{img}}$
    *   업데이트된 임베딩을 연결하여 융합된 표현 $z_{\text{fusion}} = [z''_{\text{tac}}; z''_{\text{img}}] \in R^{2d}$를 얻습니다.
*   **촉각 재구성 디코더:**
    *   융합된 특징 $z_{\text{fusion}}$은 2계층 MLP와 시그모이드 활성화 함수를 통과하여 재구성된 촉각 이미지 $\hat{T} \in R^{1 \times 24 \times 32}$를 생성합니다.
    *   손실 함수는 전체 이미지 재구성 손실 $L_{\text{stage1}}(\phi, \psi) = ||T - \hat{T}||_2^2$입니다.
*   EMA(Exponential Moving Average)를 통해 타겟 엔코더를 안정화합니다.

**2단계: 행동 복제(Behavior Cloning)를 통한 정책 학습 (Policy Learning)**

사전 학습된 촉각-시각 엔코더 $E_{\phi}$는 조건부 Diffusion Policy에 통합됩니다.

*   **관측 공간:** 각 타임스텝 $t$에서 로봇은 raw 센서 입력 $(I_t, T_t, p_t)$를 받습니다. $I_t$와 $T_t$는 사전 학습된 엔코더를 통해 촉각-시각 임베딩 $z_t = E_{\phi}(I_t, T_t)$를 생성하고, $p_t$는 고유 수용성 상태(end-effector pose, gripper width 등)를 나타냅니다. Diffusion Policy는 $o_t = (z_t, p_t)$에 따라 조건화됩니다.
*   **Diffusion Policy:** 액션을 직접 회귀하는 대신 노이즈 예측기 $\hat{\epsilon}_t^k = \epsilon_{\theta}(a_t^k, o_t, k)$를 학습합니다.
*   손실 함수: $L_{\text{stage2}} = E_{t,k}[||\epsilon_t^k - \hat{\epsilon}_t^k||_2^2]$.
*   추론 시에는 $a_t^K \sim N(0, I)$에서 시작하여 $K$ 스텝 동안 노이즈를 제거합니다: $a_t^{k-1} = \alpha a_t^k - \gamma \epsilon_{\theta}(a_t^k, o_t, k) + N(0, \sigma^2I)$.
*   모든 엔코더 구성 요소(CLIP 백본, 촉각 CNN, 크로스-어텐션 레이어)는 이 단계에서 미세 조정됩니다.

**실험 및 결과:**

이 시스템은 "test tube insertion"과 "pipette-based fluid transfer"와 같은 **네 가지 실제 환경 로봇 조작 작업**에서 검증되었습니다.

*   **대규모 촉각-시각 데이터셋:** 12개의 실내 및 실외 환경에서 수집된 2700개 이상의 데모와 260만 개 이상의 촉각-시각 쌍으로 구성된 데이터셋을 구축했습니다. 이 데이터셋은 핵심 작업, 기타 실내 작업, 그리고 30개 이상의 야외(in-the-wild) 작업을 포함합니다.
*   **정성적 분석:** 사전 학습된 엔코더는 부분적으로 마스킹된 촉각 및 RGB 이미지로부터 누락된 촉각 입력을 성공적으로 재구성합니다. 또한, ViT의 self-attention 맵은 배경이나 객체의 종류에 관계없이 그리퍼 접촉 영역에 일관되게 집중합니다. 데이터셋 크기가 클수록 재구성 손실이 낮아지고, 어텐션 맵이 더 선명하게 그리퍼 접촉 영역에 집중하며, 재구성된 촉각 이미지는 노이즈가 줄고 구조가 명확해집니다.
*   **정량적 평가:** "Vision-Only", "Ours w/o Cross Attention", "Ours w/o Pretraining" 등 여러 baseline과 비교했습니다. 제안하는 "Ours w/ Pretraining" 방법이 모든 세부 작업 및 전체 작업에서 일관되게 가장 높은 성공률을 보였습니다. 특히, "Test Tube Collection"과 "Fluid Transfer" 같은 미세 조작 작업에서 탁월한 성능을 입증했습니다.
*   **분석:**
    1.  **촉각 피드백은 명시적인 "in-hand" 상태 정보를 제공:** 시각 정보만으로는 객체 가려짐이나 모호한 시각적 단서(예: 시험관 재배치 중 코르크 색상 변화)로 인해 오류가 발생하기 쉽지만, 촉각 정책은 이러한 변화에 영향을 받지 않습니다.
    2.  **촉각 피드백은 중요한 상태 전환 감지를 향상:** "Fluid Transfer"와 같이 미세한 힘 제어가 필요한 작업에서 시각 정책은 압력 변화를 감지하기 어렵지만, 촉각 정책은 미묘한 힘 변화를 감지하여 정확한 단계 전환을 가능하게 합니다.
    3.  **공동 촉각-시각 엔코더는 시각과 촉각의 조화로운 사용 가능:** 단순한 특징 연결(크로스-어텐션 없는 정책)은 한 가지 모달리티에 과도하게 의존할 수 있지만(예: 화이트보드 지우기에서 과도한 힘 적용), 공동 학습된 엔코더는 시각적 맥락과 촉각 피드백에 따라 힘을 적절히 조절합니다.
*   **사전 학습 제거 연구(Ablation Study):** 낮은 데이터 및 낮은 에포크 환경에서 **사전 학습이 정책 성능에 상당한 이점을 제공함이 확인**되었습니다. 사전 학습된 정책은 더 부드러운 궤적을 따르며, 초기 환경 구성에 더 견고했습니다. 이는 사전 학습이 촉각-시각 상관관계를 조기에 학습하여, downstream 정책이 효과적인 액션 궤적 학습에 집중할 수 있게 돕기 때문입니다. 어텐션 맵 분석 결과, 사전 학습된 정책은 그리퍼-객체 접촉 영역에 집중하는 반면, 사전 학습 없는 정책은 관련 없는 배경 요소에 주의를 기울이는 경향이 있어 성능 저하를 야기합니다.

**결론:**
이 논문은 촉각 센서가 통합된 핸드헬드 그리퍼와 이를 통해 수집된 대규모 촉각-시각 데이터셋을 제시합니다. 사전 학습된 촉각-시각 공동 엔코더가 단일 팔 로봇의 정밀 조작 작업에서 유용함을 입증했습니다. 향후 연구에서는 이를 다지(multi-finger) 능숙한 손으로 확장하여 더욱 풍부하고 능숙한 조작 기술을 구현하는 것을 목표로 합니다.


---

# Detail Review

> Touch in the Wild – 휴대형 시각-촉각 그리퍼로 정밀 조작을 학습하다

## 1. 개요 및 기술적 기여

이 논문은 다음과 같은 세 가지 기술적 기여를 중심으로 전개됩니다:

1. **휴대형 시각-촉각 그리퍼 개발**
   논문에서는 290g에 불과한 가볍고 배터리로 구동되는 휴대형 그리퍼를 설계했습니다. 두 개의 손가락에 촘촘히 분포된 촉각 센서(12×32 텍셀)를 포함하고 있으며, 상단에 fisheye RGB 카메라가 부착되어 있어 촉각과 영상 정보를 동시에 수집할 수 있습니다. 이로 인해 실제 환경(in-the-wild)에서 **사람이 직접 다양한 작업을 시연하며 시각-촉각 데이터를 수집**할 수 있게 되었습니다.

2. **Cross-modal Masked Autoencoder 기반 Visuo-Tactile Representation Learning 프레임워크 제안**
   핵심은 시각 정보와 촉각 정보를 **단순히 결합(concatenation)하지 않고**, 각 모달리티의 특성을 보존한 채로 교차 주의 메커니즘(cross-attention)을 통해 학습하는 것입니다. 특히 **촉각 이미지를 무작위로 마스킹한 후, 이를 시각 정보로 보완하여 복원하는 방식**으로, 두 센서 간의 상호 보완 관계를 강제로 학습하게 됩니다.

3. **2.6M 프레임, 2700개 이상의 시연으로 구성된 대규모 Visuo-Tactile Dataset 구축**
   다양한 실내/외 환경(12곳)에서 43개 작업에 대한 촬영을 통해 **정밀 조작 작업을 포함한 대규모 데이터셋**을 수집하였으며, 실제 촉각/영상 동기화 방법으로 QR코드 기반 타임스탬프 정합 기법을 도입하여 고비용 장비 없이도 정밀한 멀티모달 수집이 가능했습니다.

> ✅ **정리하자면**, 이 논문은 **하드웨어, 데이터셋, 학습 구조**를 모두 포함한 **멀티모달 조작 학습의 end-to-end 전환점**으로 볼 수 있으며, 기존 연구들보다 한 단계 높은 현실성, 확장성, 성능을 보여줍니다.


## 2. Visuo-Tactile Fusion 방식

### 📌 핵심 구조

* **입력 분리 처리**

  * **시각**: CLIP ViT-B/16 백본을 사용해 768차원의 embedding 추출
  * **촉각**: 24×32 압력 행렬을 RGB 이미지로 인코딩 후, 작은 CNN을 통해 768차원의 embedding 생성

* **Cross-modal Attention**

  * 촉각이 시각을 쿼리하여 $z_{\text{tac}} \rightarrow z'_{\text{tac}}$로 업데이트
  * 다시 시각이 업데이트된 촉각을 쿼리하여 $z_{\text{img}} \rightarrow z'_{\text{img}}$ 생성
  * **양방향 교차 주의**를 통해 모달리티 간 정보를 **서로 보완**하고 조율

* **Masked Autoencoding 훈련 방식**

  * 입력 촉각 이미지의 60\~80%를 마스킹 후 시각 정보를 이용해 전체를 복원
  * reconstruction loss $L_{\text{recon}} = |T - \hat{T}|^2$을 사용
    → 촉각 정보를 직접 복원하게 하여 **단순한 피처 병합이 아닌 진정한 의미의 "융합" 학습**

### 🧠 해석 가능성과 장점

* 시각적 주의 맵을 보면 대부분 **접촉 위치나 물체와의 인터페이스 영역**에 집중되어 있음
* 이 attention은 unseen 환경에서도 일관되게 나타나며, **학습된 시각-촉각 표현이 일반화됨**을 보여줌
* 단순한 concat 방식보다 **훨씬 더 정밀한 접촉 인식과 위치 추론이 가능**

> 🔎 **핵심 요약**: cross-attention 구조 + reconstruction task의 조합은, 기존의 단순 early-fusion 기법보다 훨씬 정교하고 효과적인 멀티모달 표현을 가능하게 합니다.


## 3. 조작 정책 모델 구조

### 🔧 2단계 구조

1. **Visuo-Tactile Encoder $E_\phi(I, T)$**

   * 앞서 설명한 cross-modal encoder
   * 사전학습(pretraining) 후, 정책 학습 시에는 fine-tuning 가능

2. **Diffusion Policy**

   * 조건부 확률 기반 행동 생성: $p(a_t | z_t, p_t)$
   * 입력은 visuo-tactile embedding $z_t$와 proprioception $p_t$ (gripper 상태 등)
   * 정적인 MLP 대신 **확률 기반 U-Net 모델**로, 더 정교한 다중모드 행동 생성 가능

### 🤖 행동 생성 방식

* 행동 시퀀스를 직접 예측하는 것이 아닌 **noise → action**으로 변환하는 방식
* 학습 시 행동에 noise를 추가하고, 이를 역으로 제거하는 방식으로 학습 (Denoising Diffusion)
* 이로 인해 단일 행동 예측보다 **더 정교하고 부드러운 행동 시퀀스 생성 가능**

> ✅ 이 구조 덕분에 복잡한 조작에서도 행동이 한결 자연스럽고 신뢰성 있게 생성됩니다. 특히 접촉이 중요한 작업에서 **작은 감각 피드백 차이도 반영**할 수 있다는 점에서 큰 장점이 있습니다.


## 4. 학습 데이터 구성과 품질

### 📊 구성 개요

* 총 **2.6M 프레임, 2700개 이상의 시연, 43가지 작업**
* 분류:

  * **Main indoor tasks** (38%) – 논문 실험용 핵심 작업
  * **Other indoor tasks** (37%) – 다양한 보조 작업
  * **Outdoor tasks** (25%) – 시장, 거리, 공원 등 in-the-wild 환경에서 수행

### 📷 수집 방식

* **GoPro 카메라 + 촉각 센서 동기화**

  * QR코드 기반 타임스탬프 정합으로 저비용/고정밀 동기화 구현
* **사람이 손으로 조작하며 수집**

  * 더욱 섬세하고 정교한 조작 포함 가능 (ex. 피펫 액체 옮기기, 연필 깎기)

### 📉 한계점

* **병렬 조작이 어려움**: 2지 그리퍼 기준 수집되어 **멀티 핑거 조작에는 제약**
* **사람이 수집하고 로봇은 학습**하는 구조이므로 domain gap 존재
* **촉각 센서 주파수 제한** (23Hz): 고속 slip, texture 분류 등은 어려움

> 🌟 하지만, 해당 데이터셋은 촉각-시각 학습을 위한 현실적이고 확장 가능한 기반을 제공하며, 이는 기존 연구에서 보기 드문 강력한 장점입니다.


## 5. 실험 설계 및 한계

### 🧪 주요 실험 작업 (4개)

1. **Test Tube Insertion**: 집기 → 회전 → 슬롯 삽입
2. **Pencil Sharpening**: 연필 회전 및 정렬 후 구멍 삽입
3. **Fluid Transfer**: 피펫을 잡고, 부드럽게 짜서 액체 이동
4. **Whiteboard Erasing**: 일정한 힘으로 칠판 닦기

→ 공통점: **정밀 접촉 및 힘 조절이 필요한 작업**

### 🧪 실험 설계

* **20번의 반복 실험** per 작업
* 초기 상태 및 배경 변형을 통한 **일부 generalization 테스트**
* 세부 단계별(집기, 회전, 삽입 등) 성공률도 측정하여 분석의 정밀도를 높임

### 📊 성능 비교

| 방법                               | Tactile 사용 | Cross-attn | Pretrain | Test Tube 삽입 성공률 |
| -------------------------------- | ---------- | ---------- | -------- | ---------------- |
| Vision Only                      | ❌          | -          | ✅        | 25%              |
| Vision + Tactile (no cross-attn) | ✅          | ❌          | ✅        | 50%              |
| Vision + Tactile (no pretrain)   | ✅          | ✅          | ❌        | 70%              |
| **Ours (full)**                  | ✅          | ✅          | ✅        | **85%**          |

* Pretraining 및 Cross-attn이 **정밀 조작 성공률을 2\~3배 향상**시킴
* 특히 Vision만 사용하는 경우, 투명 물체/미세 접촉에서 **상황 판단 실패**가 자주 발생

### 🧩 한계

* 실험은 모두 **로봇 팔 기반 고정된 실내 환경**에서 수행 → "진짜 in-the-wild" 배치는 아님
* 멀티태스크 통합 정책은 없으며, 각 작업별 개별 정책 학습
* diffusion 기반 정책은 계산량이 크므로 실시간 제어에 한계 가능성

> ✅ 그럼에도 불구하고, 실험 설계는 각 구성요소의 기여도를 정량적으로 잘 보여주며, 이 방식이 촉각 기반 조작에 **의미 있는 성능 개선을 제공한다는 점을 설득력 있게 입증**합니다.


## ✍️ 결론 및 분석 요약

* **시각-촉각 통합 조작 학습**이라는 어려운 문제를 **하드웨어–데이터–학습 구조** 측면에서 풀어낸 훌륭한 논문
* 특히 **cross-modal fusion + reconstruction learning** 방식은 기존 멀티모달 학습에서 **모달리티 간의 정보 교환을 구조적으로 학습**할 수 있게 한 큰 기여
* 수집된 데이터의 **규모, 다양성, 품질** 또한 타 연구 대비 매우 우수
* 다만 **실제 배치 시 domain shift, 계산 비용, 멀티핑거 확장성** 등의 과제는 남아 있음
