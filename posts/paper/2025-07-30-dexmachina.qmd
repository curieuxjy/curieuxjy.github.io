---
title: "📃DexMachina 리뷰"
date: 2025-07-30
categories: [retargeting, quad]
toc: true
number-sections: false
description: Functional Retargeting for Bimanual Dexterous Manipulation
---

- [Paper Link](https://arxiv.org/abs/2505.24853)
- [Project Link](https://project-dexmachina.github.io/)
- [Github Link](https://github.com/MandiZhao/dexmachina)

1. 🤖 이 연구는 인간의 손-객체 시연을 로봇의 양손으로 관절형 객체를 조작하는 기능적 리타겟팅(functional retargeting) 정책으로 학습시키는 어려운 문제를 다룹니다.
2. 💡 핵심 아이디어는 가상 객체 컨트롤러(virtual object controllers)의 강도를 점진적으로 약화시키는 커리큘럼 기반 RL(강화 학습) 알고리즘인 DexMachina를 제안하여, 정책이 초기에는 객체를 자동으로 목표 상태로 이동시키면서 점차 조작을 인수하도록 훈련시키는 것입니다.
3. ✅ 이 알고리즘은 기존의 기준 방법론을 훨씬 능가하는 성능을 보였으며, 다양한 덱스터러스 핸드(dexterous hands)와 작업을 포함하는 시뮬레이션 벤치마크를 공개하여 하드웨어 설계의 기능적 비교를 가능하게 합니다.


<center>
<img src="../../images/2025-07-30-dexmachina/dexmachina-teaser-website.png" width="100%" />
</center>


---

# Brief Review

이 논문은 인체-객체 데모로부터 객체 상태를 추적하기 위한 능숙한 조작 정책을 학습하는 `functional retargeting` 문제를 다룬다. 특히, 높은 차원의 액션 공간, 시공간적 불연속성, 그리고 인간 손과 로봇 손 사이의 `embodiment gap`으로 인해 어려운 장기적이고 양손을 사용하는(bimanual) `articulated object` 작업에 중점을 둔다.

이러한 문제들을 해결하기 위해, 저자들은 가상 객체 컨트롤러(`virtual object controllers`, VOCs)를 기반으로 하는 새로운 커리큘럼 기반 `RL` 알고리즘인 `DexMachina`를 제안한다. `DexMachina`의 핵심 아이디어는 가상 객체 컨트롤러를 사용하여 객체를 목표 상태로 자동으로 움직이게 함으로써, 정책이 동작 및 접촉 지침 하에서 점진적으로 제어권을 인계받도록 학습하는 것이다. 초기에는 `VOCs`가 객체 움직임의 대부분을 담당하며, 정책은 작업을 망치지 않으면서 인간의 동작을 모방하는 것을 학습한다. 시간이 지남에 따라 `VOCs`의 강도가 약해지면서, 정책은 점차적으로 객체 조작을 스스로 수행하도록 학습한다.

**핵심 방법론: DexMachina**

`DexMachina`는 `RL` 환경에서 `functional retargeting` 작업을 수행하기 위해 정책을 학습시킨다.

1.  **RL 환경 및 Task Reward**:
    각 타임스텝 $t$에서 객체의 실제 달성 상태 $\hat{G}_t = \{\hat{g}_t^P, \hat{g}_t^R, \hat{g}_t^J\}$ (위치, 회전, 조인트 각도)와 데모의 목표 상태 $G_t = \{g_t^P, g_t^R, g_t^J\}$를 비교한다. `task reward` $r_{\text{task}}$는 각 상태 구성 요소의 정확도를 측정하는 세 항의 곱으로 정의되어 균형 잡힌 학습을 장려한다.
    -   위치 오차: $d_{\text{pos}} = || \hat{g}_t^P - g_t^P ||_2$
    -   회전 오차: $d_{\text{rot}} = 2 \cos^{-1}(|\langle \hat{g}_t^R, g_t^R \rangle|)$
    -   관절 오차: $d_{\text{ang}} = || \hat{g}_t^J - g_t^J ||_2$
    -   `Task reward`: $$r_{\text{task}} = \exp(-\beta_{\text{pos}}d_{\text{pos}}) \exp(-\beta_{\text{rot}}d_{\text{rot}}) \exp(-\beta_{\text{ang}}d_{\text{ang}})$$
    여기서 $\beta_{\text{pos}}, \beta_{\text{rot}}, \beta_{\text{ang}}$는 각 구성 요소에 대한 가중치이다.

2.  **액션 정형화 및 Aux Reward (`Auxiliary Rewards`)**:
    `task reward`만으로는 복잡한 장기 작업에서 정책이 효과적으로 학습하기 어렵다. 이를 보완하기 위해 두 가지 방법을 사용한다.
    -   **하이브리드 액션 정형화 (`Hybrid Action Formulation`)**: 인간 데모와 더 잘 일치하도록 손목(`wrist`) 액션 공간을 제한하고, 나머지 손가락(`finger`) 관절은 절대 액션을 사용한다. `kinematics-only retargeting` 알고리즘 (`Anyteleop` [3] 기반)을 통해 얻은 `retargeted joint` 값 $Q \in \mathbb{R}^{T \times J}$를 손목 관절의 기본 액션으로 사용하며, 정책은 그 위에 잔차(`residual`) 액션을 출력한다. 손가락 관절은 관절 한계에 의해 정규화된 절대 액션을 사용한다.
    -   **Aux Reward (`Auxiliary Rewards`)**: 정책이 인간의 손-객체 상호작용 전략을 따르도록 유도한다.
        -   **데이터 전처리**: 데모 데이터 $D_\eta$에서 `collision-aware kinematic retargeted joints` $Q$와 참조 키포인트(`reference keypoints`) $X \in \mathbb{R}^{T \times K \times 3}$를 추출한다. 또한, 손 링크와 객체 부품 사이의 근사 접촉 위치 $C \in \mathbb{R}^{(T \times N \times K \times 3)}$와 유효성 마스크 $M \in \mathbb{R}^{(T \times N \times K)}$를 추출한다.
        -   **동작 모방 Reward (`Motion Imitation Reward`)**: 인간과 유사한 손 동작을 장려하기 위해 키포인트 매칭 기반의 $r_{\text{imi}}$와 관절 각도 거리 기반의 행동 복제(`behavior-cloning`) Reward $r_{\text{bc}}$를 정의한다.
            $$r_{\text{imi}} = \frac{1}{K} \sum_{i=1}^K \exp(-\beta_{\text{imi}}||\hat{x}_i - x_i||_2)$$
            $$r_{\text{bc}} = \frac{1}{J} \sum_{i=1}^J \exp(-\beta_{\text{bc}}||\hat{q}_i - q_i||_2)$$
        -   **접촉 Reward (`Contact Reward`)**: 정책의 접촉을 데모의 접촉과 일치시켜 계산한다. 접촉 거리 $D \in \mathbb{R}^{N \times K}$는 유효성 마스크에 의해 가려진 `L2` 거리를 사용하여 계산된다.
            $$r_{\text{con}} = \frac{1}{2NK} (\sum_{i=1}^N \sum_{j=1}^K \exp(-\beta_{\text{con}}D(i,j)_{\text{left}}) + \sum_{i=1}^N \sum_{j=1}^K \exp(-\beta_{\text{con}}D(i,j)_{\text{right}}))$$
            최종 `RL` Reward $r_t$는 이 모든 항의 가중 합이다: $$r_t = \lambda_{\text{task}}r_{\text{task}} + \lambda_{\text{imi}}r_{\text{imi}} + \lambda_{\text{bc}}r_{\text{bc}} + \lambda_{\text{con}}r_{\text{con}}$$

3.  **가상 객체 컨트롤러를 이용한 자동 커리큘럼 (`Auto-Curriculum with Virtual Object Controllers`)**:
    정책이 복잡한 작업에서 초기 실패에 빠지는 것을 방지하기 위해 `VOCs`를 도입한다. `VOCs`는 데모 상태 $G$를 제어 목표로 삼아 가상 스프링-댐퍼 제약(`spring-damper constraints`)을 적용하여 객체를 목표 궤적을 따라 움직이게 한다.
    -   **구현**: 시뮬레이션에서 `privileged information`을 사용하여 구현된다. 각 객체는 기본 포즈에 대해 6-DoF, 관절 동작에 대해 1-DoF를 가진 가상 관절을 갖추고 있으며, 모든 관절은 `PD` 컨트롤러에 의해 구동된다.
    -   **커리큘럼 스케줄링**: 학습 시작 시에는 높은 `VOC` 이득(`gains`) $(k_p, k_v)$을 설정하고, 정책의 학습 진행에 따라 이 이득을 점진적으로 지수적으로 감소시킨다. 정책이 모든 Reward(`task`, `imi`, `bc`, `con`)에 대해 일정 임계값(`threshold`)을 초과하면 이득이 감소한다. 이를 통해 정책은 초기에 높은 `task reward`를 달성하면서 Aux Reward을 개선하는 방법을 학습하고, `VOCs`가 약해지면 높은 `task reward`를 유지하기 위해 자체 동작을 조정하는 법을 배운다.

**실험 및 결과**

저자들은 6개의 능숙한 로봇 손(`Inspire`, `Allegro`, `Xhand`, `Schunk`, `Ability`, `DexRobot Hand`)과 5개의 `articulated object`로 구성된 시뮬레이션 벤치마크를 구축했다. `Genesis` 물리 시뮬레이터와 `PPO` 알고리즘을 사용하여 `DexMachina`를 평가했다.

-   **주요 결과**: `DexMachina`는 모든 손과 작업에서 일관되게 성능을 향상시켰으며, 특히 복잡한 장기 작업에서 기준선(`baseline`) 방법들(`Kinematics Only`, `ObjDex`, `Task + Auxiliary Rewards without curriculum`, `ManipTrans`)을 크게 능가했다. `kinematic retargeting`만으로는 작업을 완료할 수 없었다.
-   **하드웨어 적응성**: `DexMachina`는 정책이 하드웨어 제약에 맞춰 작업 전략을 학습하도록 한다. 예를 들어, `Notebook-300` 작업에서 `XHand`는 인간 데모를 따랐지만, 더 작고 구동이 적은 `Inspire Hand`는 객체를 안정화하고 커버를 닫기 위해 양손을 사용하는 다른 전략을 학습했다. 이는 Aux Reward이 엄격한 지침이 아닌 유연한 안내 역할을 하여 정책이 더 나은 `task reward`를 위해 참조 동작에서 벗어날 수 있음을 보여준다.
-   **Ablation Study**:
    -   **액션 Ablation**: 제안된 하이브리드 액션 정형화(손목 움직임에 더 제한적인 경계를 사용)가 절대 액션이나 덜 제약적인 잔차 액션보다 전반적으로 더 나은 학습 성능을 보였다.
    -   **커리큘럼 Ablation**: `ManipTrans`의 커리큘럼(오차 임계값, 중력, 마찰 매개변수 감소)은 `DexMachina`의 `VOC` 기반 커리큘럼만큼 효과적이지 않았다. 물리 매개변수만 감소시키는 것으로는 장기적 `articulated object` 작업에 충분하지 않음을 시사한다.
-   **손 구현체 분석 (`Hand Embodiment Analysis`)**: `DexMachina`와 벤치마크를 사용하여 다른 능숙한 손 디자인을 기능적으로 비교했다. 더 크고 완전히 구동되는 손(`Allegro Hand` 등)이 더 높은 최종 성능과 더 나은 학습 효율을 보였다. 크기보다는 자유도(`degrees of freedom`, `DoF`)가 더 중요하며, `Schunk Hand`는 `Inspire`, `Ability`와 비슷한 크기임에도 더 많은 `DoF`와 유연한 디자인 덕분에 더 나은 성능을 달성했다.

<center>
<img src="../../images/2025-07-30-dexmachina/plot.png" width="100%" />
</center>

**결론 및 한계**

이 작업은 `functional retargeting`을 위한 `DexMachina` 알고리즘과 포괄적인 시뮬레이션 벤치마크를 제공한다. `DexMachina`는 기존 방법들을 뛰어넘는 성능을 보이며, 다양한 능숙한 손 디자인에 대한 기능적 비교를 가능하게 한다.
한계점으로는, 정책이 시뮬레이터의 `privileged information`에 의존하는 상태 기반 입력을 사용한다는 점(실제 세계에서의 취득 어려움), 고품질 인간 손-객체 데모 데이터의 필요성(수집 비용과 큐레이션), 시뮬레이션된 손 모델의 물리적 속성 추정으로 인한 실제 하드웨어와의 동역학 불일치 가능성, 그리고 실제 세계에서의 평가 부족 등이 있다.

---

# Detail Review

> DexMachina: 기능적 리타게팅을 통한 양손 섬세 조작

## Introduction

인간의 손재주는 로봇 공학에서 오랫동안 궁극적인 목표였지만, **인간 손과 로봇 손의 차이(embodiment gap)**로 인해 동일한 동작을 이식(retarget)하는 데 많은 어려움이 존재합니다. 2025년 5월 Arxiv에 공개된 **“DexMachina: Functional Retargeting for Bimanual Dexterous Manipulation”** 논문은 이러한 문제를 해결하기 위해 제안된 새로운 강화학습 기반 방법입니다. 이 글에서는 해당 논문의 핵심 아이디어와 방법론, 실험 결과, 그리고 기존 관련 연구들과의 비교를 **전문가의 시각**에서 깊이 있게 분석합니다. 특히 **① 방법론의 독창성** (기존 리타게팅/모션 트랜스퍼 기법 대비 차별점), **② 실험 결과 분석** (다양한 환경에서의 성능과 효과), **③ 기존 연구와의 비교** (유사 목적을 가진 프레임워크들과의 기술적 차이와 기여도)에 중점을 두어 살펴보겠습니다.

* **기능적 리타게팅의 개념:** DexMachina는 **기능적 리타게팅**(functional retargeting)이라는 개념을 정립합니다. 이는 **인간 시연의 “결과”에 초점을 맞춰** 로봇이 물체를 동일하게 조작하도록 학습하는 것으로, 단순히 인간의 손동작을 흉내내는 **운동학적 리타게팅**과 대비됩니다. 기존의 운동학적 리타게팅은 로봇 손가락 위치를 사람 손과 유사하게 따라하게 할 수는 있어도 물체 조작 성공을 보장하지 못하는 반면, DexMachina는 **물체의 목표 상태를 따라가도록** 정책을 학습시키는 점에서 근본적으로 다릅니다.

* **가상 객체 제어와 커리큘럼 학습:** DexMachina의 가장 독창적인 아이디어는 **“가상 객체 제어기(virtual object controller)”**를 활용한 자동 **커리큘럼 학습**입니다. 초기 학습 단계에서는 외부 힘(가상 제어기)이 물체를 자동으로 목표 위치까지 **밀어주면서** 정책 학습을 돕고, 점진적으로 그 도움을 줄여나감으로써 최종적으로 로봇 정책이 **스스로 물체를 조작**하도록 만듭니다. 이러한 **점감적 Aux** 방식은 초반 학습의 난이도를 크게 낮춰주어, 긴 시퀀스 작업에서도 **초기 실패를 방지**하고 안정적으로 탐색할 수 있게 합니다.

* **다양한 손과 작업에 대한 범용성:** 저자들은 **6종의 로봇 손**(Inspire Hand, Allegro Hand, X-Hand, Schunk Hand 등)과 **5종의 복잡한 물체**(노트북, 주방기구 등 관절부를 가진 물체들)로 구성된 **시뮬레이션 벤치마크**를 구축하여 DexMachina의 성능을 평가했습니다. 결과적으로 DexMachina는 **모든 손과 작업에 걸쳐 기존 방법들을 능가하는 성공률**을 보였으며, 특히 **복잡한 양손 장기 작업(long-horizon)**에서 두각을 나타냈습니다. 또한 하나의 알고리즘으로 다양한 로봇 손에 별도 튜닝 없이 적용 가능함을 보여주어, 향후 **로봇 손 하드웨어 설계 비교**에도 유용한 표준을 제시합니다.

이제 위 세 가지 주제에 대해 순서대로 자세히 살펴보겠습니다.

## 1. 방법론의 독창성: DexMachina만의 리타게팅 접근법

**인간 시연의 “기능”을 학습 목표로 삼다.** DexMachina는 **인간 손-물체 상호작용 시연**(예: 사람이 두 손으로 와플 기계를 들어 열었다 닫는 시연)을 입력으로 받아, 로봇의 두 손이 **물체의 동일한 기능적 결과를 재현**하도록 정책을 학습시킵니다. 여기서 **기능적 결과**란 물체의 상태 변화에 초점을 둔 것으로, 사람 시연과 **똑같은 동작 경로를 그리지 않더라도** 물체의 움직임이 같다면 성공으로 간주합니다. 이는 사람 움직임 자체를 따라하려는 기존의 접근과 근본적으로 다릅니다. 예를 들어 사람이 공을 던지는 시연이 주어졌을 때, DexMachina는 로봇이 **공을 같은 목표에 맞히는 것**에 집중하지, 인간의 모든 손가락 각도를 그대로 재현하려 하지는 않습니다. 이러한 발상의 전환 덕분에, 로봇과 인간 손 구조가 달라도 **Task 중심**으로 학습이 이루어질 수 있습니다.

**시연 데이터로 자동 Reward 설계:** DexMachina는 한 편의 **인간 데모**(모션 캡쳐된 손/물체 궤적)만으로도 추가 Reward 설계 없이 학습 목표를 정의합니다. 구체적으로, **데모의 물체 상태 궤적**을 추출하여 로봇이 따라가야 할 **Task Reward**(task reward)을 정하고, 인간 손동작을 로봇 손으로 **충돌 없게 변환한 기준 모션**을 계산하여 **모션 모방 Reward**으로 활용합니다. 또한 데모에서 **손-물체 접촉 지점**을 근사추정하여, 로봇 손도 비슷한 지점에 접촉하도록 **접촉 Reward**을 부여합니다. 흥미로운 점은, **로봇 손목(wrist)** 움직임은 데모 궤적을 최대한 따르도록 제한하고 손가락 관절은 절대 제어하게 하는 **하이브리드 방식**을 취한다는 것입니다. 이렇게 하면 큰 팔 동작은 인간과 비슷하게 유지하면서도, 세밀한 손가락 움직임은 로봇이 **자유롭게 조정**하여 자기 구조에 맞게 최적화할 수 있습니다. 요약하면, DexMachina는 **데모 → (Task + 모션 + 접촉) Reward**으로 변환하는 **객체 중심 학습 목표**를 자동 구축하여, 별도의 Reward 함수 설계 없이도 **효과적인 강화학습 환경**을 마련합니다.

**“Deus ex machina” – 가상 스프링으로 시작하는 커리큘럼 RL:** DexMachina라는 이름에는 “기계에서 내려온 신”이라는 뜻이 담겨 있는데, 이는 알고리즘이 **초반에 마치 보이지 않는 손처럼 물체를 움직여주는 모습**에 착안한 것입니다. 정책 학습이 특히 어려운 **장기간 양손 조작**의 경우, 두 손의 미세한 협응 실패로 초반에 곧잘 **임무가 좌초**되기 쉽습니다. 이를 해결하기 위해 DexMachina는 **자동 커리큘럼(auto-curriculum)** 전략을 도입했습니다. 초기 학습 단계에서는 **가상 객체 제어기(Virtual Object Controller)**가 일종의 **스프링 힘**으로 물체를 사람이 보여준 방향대로 움직여 줍니다. 정책은 이 때 **실패 위험 없이** 따라하는 법을 배우고, 점차 정책이 성과를 내기 시작하면 **가상 힘의 세기를 줄여** 정책이 **자율적으로** 물체를 조작하도록 만듭니다. 이러한 **점진적 난이도 상승**은 강화학습에서 흔히 쓰이는 커리큘럼 학습 개념을 응용한 것으로, 물체 물리 동역학을 제어하는 외력으로 난이도를 조절하는 점이 특징입니다.

<center>
<img src="../../images/2025-07-30-dexmachina/1.png" width="100%" />
</center>

> *DexMachina 알고리즘의 개요도.* 왼쪽은 인간 **양손 데모**에서 얻은 물체 상태 변화(예: 와플 기계 뚜껑의 각도)와 로봇 모션/접촉 **Reward 신호** 추출 과정을 보여준다. 오른쪽은 **자동 커리큘럼 학습** 과정으로, 초기에는 **강한 가상 스프링**(가상 객체 제어기)이 물체를 목표대로 움직여주어 로봇 정책이 실패 없이 모방학습을 하고, 중기에는 **약한 스프링**으로 Aux를 줄이면서, 최종적으로 **스프링 없이** 정책 혼자서 물체를 조작하도록 훈련된다. 이러한 방식으로 DexMachina는 **운동학적 리타게팅의 초기 가이드**와 **강화학습의 자율 탐색**을 자연스럽게 연결한다.

**기존 기법 대비 차별성:** DexMachina의 방법론은 기존 리타게팅 및 모션 트랜스퍼 기술과 몇 가지 중요한 차이를 보입니다. **첫째**, 사람 손동작을 로봇에 **단순 이식**하던 접근과 달리, DexMachina는 사람-로봇 간 **공통의 Task 공간**(object state trajectory)에서 문제를 정의하여 **물리적으로 실행 가능한 전략**을 학습합니다. 예컨대 Park 등(2025)은 인간-로봇-물체 움직임의 **공동 임베딩 공간**을 학습하여 거기서 로봇 동작을 추론하는 방식으로 **데이터 기반 리타게팅**을 수행하였는데, DexMachina는 이를 **강화학습 정책**으로 구현함으로써 새로운 손/환경에도 **온라인 적응 능력**을 부여합니다. **둘째**, 기존 많은 **텔레오퍼레이션 기반 시연** 방법들은 로봇 손마다 별도 시스템 구축이 필요하고 주로 단발적 그립 동작에 그쳤지만, DexMachina는 **단 하나의 시연**만으로 장시간의 복잡한 조작을 가능케 했습니다. 또한 **DeepMimic**이나 **DAPG**(Rajeswaran et al., 2018)처럼 예제 모션을 RL로 따라하는 선행 연구들이 있었지만, 대부분 **단일 로봇 손**에 **단일 작업**을 다루고 Reward 신호도 제한적이었습니다. 반면 DexMachina는 **Reward 구성의 다양화(Task+모션+접촉)**와 **외력 커리큘럼**이라는 새로운 요소로 이러한 모션 트랜스퍼 문제를 확장했고, 이를 통해 **두 손이 협응**해야 하는 복잡한 작업도 자동학습이 가능함을 보였습니다. 종합하면 DexMachina는 **“시연 데이터 + 강화학습 + 커리큘럼”**의 세 박자를 맞추어, 기존 방법들이 부분적으로만 해결했던 문제들을 하나의 프레임워크에 통합한 점에서 독창적입니다.

## 2. 실험 결과 분석: 성능, 적응성, 일반화 능력

**벤치마크 구성 및 평가 지표:** 저자들은 사람이 양손으로 수행한 긴 조작 시연 7개를 선택하여, 이를 모사해야 하는 시뮬레이션 환경을 만들었습니다. 해당 시연 데이터는 **ARCTIC**이라는 공개 데이터셋의 일부로, 예를 들어 **박스를 집어들어 뚜껑 열기**, **노트북 들어 덮기**, **믹서기 뚜껑 닫기**, **와플 기계 들어 열기** 등 **5가지 물체**에 대한 단기/장기 과제들을 포함합니다. 로봇 손으로는 오픈소스 **6종의 로봇 핸드**(Inspire, Allegro, X-Hand, Schunk, Ability, DexRobot 등 크기와 구조가 다양한 모델)를 활용했습니다. 성능 평가는 **물체 추적 오차** 기반 지표를 사용했는데, 데모의 물체 궤적 대비 로봇이 조작한 물체의 위치/자세가 얼마나 일치하는지를 **AUC-ADD**(평균 거리 오차에 대한 곡선 아래 면적) 형태의 점수로 환산하고, 이를 **성공률(%)**로 표현했습니다. 직관적으로 100%면 물체를 정확히 따라간 것이고, 0%면 전혀 따라가지 못한 것입니다.

**주요 비교 방법:** 실험에서는 DexMachina를 다음과 같은 방법들과 비교했습니다:

* **Kinematic Only:** 인간 **운동학적 리타게팅 결과**를 그대로 재생. (정책 학습 없음)
* **Task Rew Only:** **ObjDex**라 명명된 기준으로, **Task Reward만**으로 RL 정책 학습 (인간 데모의 물체 궤적만 목표, 모션/접촉 Reward 없음).
* **Task + Aux Reward:** DexMachina에서 제안한 **Task + 모션 + 접촉 Reward**은 쓰되, **커리큘럼 없이** 학습 (즉, 가상 객체 제어 미사용).
* **ManipTrans (2025)**: Li 등(2025)이 제안한 최신 방법으로, **사전 모방학습 + 잔여(residual) 정책 RL** 2단계로 인간 양손 시연을 이식하는 기법. (저자들이 공개한 코드/데이터를 이용한 결과)

<center>
<img src="../../images/2025-07-30-dexmachina/2.png" width="100%" />
</center>

> *여러 방법의 성공률 비교 (높을수록 우수).* 가로축은 작업 종류(예: *Ketchup-100*은 짧은 케첩 통 흔들기 과제, *Waffleiron-300*은 긴 와플기계 조작 과제)이며, 세로축은 해당 과제 완료 성공률(%)입니다. **노랑**은 학습 없이 **운동학적 리타게팅만**으로 실행한 경우, **갈색(ObjDex)**은 **Task Reward만**으로 RL한 경우, **회색**은 **Task+Aux Reward**으로 RL했으나 **커리큘럼 없는** 경우, **녹색(DexMachina)**은 제안 기법 (Aux Reward+커리큘럼)이며, **진갈색(ManipTrans)**은 최신 2단계 잔여학습 방식입니다. DexMachina(녹색)가 **대부분의 과제에서 최고 성능**을 보이며, 특히 난이도가 높은 *-300 장기 과제*들에서 두드러지게 앞서는 것을 알 수 있습니다. 또한 **ManipTrans** 대비해서도 DexMachina가 많은 경우 높은 성공률을 보이는데, 이는 제안 기법의 **탁월한 장기 탐색 능력**을 방증합니다. 실제로 ManipTrans 연구에서도 자사 방법이 성공률과 모션 재현 정확도에서 기존 기법들을 능가한다고 보고하였지만, DexMachina의 커리큘럼 전략이 동일 과제에서 한층 높은 성과를 낸 것을 확인할 수 있습니다.

**종합 성능:** 결과 그래프를 보면 DexMachina(녹색 막대)가 **모든 실험 환경에서 가장 높은 성공률**을 기록함을 알 수 있습니다.

- **단순 리타게팅 재생(Kinematic Only)**의 경우 사람과 로봇 손 구조 차이로 인해 물체를 제대로 다루지 못해 성공률이 거의 **0에 수렴**하며, **Reward만 준 RL(ObjDex)**도 **초반 탐색 실패**로 장기 작업을 끝내지 못하는 경우가 많았습니다.
- 반면 DexMachina는 **커리큘럼이 있는 경우** **없는 경우 대비 크게 향상**되어, **모든 손**과 **모든 작업**에서 일관되게 최고의 성능을 달성했습니다. 특히 각 작업 이름에 `-300`이 붙은 **장기 시나리오**(예: Notebook-300, Waffleiron-300 등)에서 그 격차가 두드러졌습니다.


**정량적 수치 및 분석:**

- DexMachina는 전반적인 평균 **약 85%** 수준의 높은 성공률을 기록하였는데, 이는 기존 방법들에 비해 크게 향상된 수치입니다.
- 특히 **단기 과제**의 경우 모든 로봇 손이 **70~90%**에 달하는 준수한 성과를 거두었고, **장기 과제**에서도 가장 어려운 시나리오(Waffleiron-300 등)조차 성공률 40~80% 범위를 달성하여 **난제 해결의 가능성**을 보여주었습니다.
- 반면 Aux Reward이나 커리큘럼이 없었던 RL 정책은 장기 과제에서 **0~30%** 수준에 머무는 등 불안정한 모습을 보였으며, 이는 **초기 탐색 실패**와 **접촉 타이밍 학습 미비** 등에 기인합니다.
- DexMachina는 **가상 제어기의 초기 개입** 덕분에 이러한 실패 구간을 건너뛰고 효과적으로 학습을 진행, **에피소드 말미까지 임무를 완수**하는 비율을 크게 끌어올린 것입니다.

**질적 결과: 적응적 전략의 학습**

- 흥미로운 것은, DexMachina로 학습된 정책이 **주어진 인간 시연을 맹목적으로 복제하지 않고**, 로봇 자신의 신체에 맞게 **전략을 재구성**했다는 점입니다.
- 예를 들어 **노트북 덮기** 작업에서, **XHand 로봇 손**은 인간 시연과 동일하게 **왼손으로 노트북을 들고 오른손으로 덮개를 닫는 전략**을 따라한 반면, **더 작고 자유도가 낮은 Inspire Hand**는 **양손 모두로 노트북을 지탱하면서 덮개를 닫는** 방식으로 임무를 완수했습니다.
- 동일한 인간 시연을 참고했음에도 각 로봇의 **크기와 관절 한계에 최적화된 동작**을 스스로 찾아낸 것입니다. 비슷하게 **믹서기 뚜껑 닫기** 작업에서는, **Allegro Hand**가 사람처럼 **긴 엄지손가락을 활용**해 뚜껑을 눌러 닫은 반면, **구조가 다른 Schunk Hand**는 **손바닥과 손목을 이용**해 뚜껑을 밀어 닫는 등 상이한 접근을 보였습니다.
- 이러한 사례들은 DexMachina의 정책이 **하드웨어 제약에 적응**하여 **기능적 목표를 달성하는 법**을 학습했음을 보여줍니다. 사람이 시연한 방식을 그대로 흉내내는 것이 아니라, **시연의 의도**를 이해해 로봇 자신의 방식으로 임무를 달성했다는 점에서 의미가 있습니다.

**로봇 손 설계 간 비교:**

- 저자들은 나아가 **제안한 벤치마크를 활용한 로봇 손 설계 비교 실험**도 수행했습니다.
- 모든 핸드는 동일한 인간 손 모션 참조를 사용하지만, 정책이 사람의 지침에서 벗어나는 정도는 핸드 크기와 운동학적 제약에 따라 달라집니다.
- 동일한 네 가지 장기 과제에 대해 서로 다른 로봇 손들이 DexMachina로 학습했을 때의 성능을 비교한 결과, **더 큰 크기이면서 모든 손가락이 능동 구동되는 손**일수록 **학습 효율과 최종 성공률이 높게** 나타났습니다.
  - 예를 들어 **Schunk Hand**나 **X-Hand**는 작은 Inspire Hand나 Ability Hand보다 **성공률과 학습속도 모두 우수**했는데, 이는 단순한 크기 차이뿐 아니라 **자유도의 차이**에서 기인한 것으로 분석됩니다.
- 크기보다 **자유도(degrees of freedom, DOF)가 더 중요한 성능 결정 요인**임을 발견했습니다.
- **큰 사이즈와 완전히 작동하는 핸드(fully-actuated hands)**는 학습 효율성과 최종 성능 면에서 뛰어나며, **긴 손가락을 가진 Allegro Hand가 특히 우수한 안정성을 제공**합니다.
  - 크기가 비슷한 Inspire, Ability, Schunk 핸드 중에서는 Schunk 핸드가 손가락 끝 부분이 작동하고 접히는 손바닥을 가지고 있어 평균적으로 더 나은 성능을 냈습니다.
- 적게 작동하는 핸드들은 인간 손과 더 닮아 보이지만 학습된 전략은 더 크고 기능적인 핸드보다 덜 인간적입니다.
- 결과적으로 Inspire와 Ability 핸드는 주어진 과제를 수행하기 위해 다른 전략을 자주 선택해야 합니다.
- **구동 가능한 관절 수(DoF)**가 많은 손은 물체를 다루는 대체 동작을 찾기 쉽기 때문에 학습에 유리하며, 반대로 인간 손 크기에 가깝더라도 **제약이 많은 손**은 학습 난이도가 높았습니다.
- 이러한 정량적 비교는 DexMachina가 제시한 하나의 중요한 활용 예로, **동일한 알고리즘 아래 여러 로봇 손의 기능적 성능을 객관적으로 평가**할 수 있음을 보여줍니다. 이는 향후 새로운 로봇 핸드 설계시 **어떤 구조가 실제 작업에 유리한지** 가늠하는 데에도 큰 도움을 줄 것으로 기대됩니다.

<center>
<img src="../../images/2025-07-30-dexmachina/3.png" width="60%" />
</center>


## 3. 기존 연구와의 비교: DexMachina의 기여도와 차별화

마지막으로, DexMachina를 **유사한 목표를 가진 기존 프레임워크/논문들과 비교**하여 그 기술적 위치를 살펴보겠습니다. 크게 **(a) 기존 리타게팅 기법**, **(b) 시연 기반 학습(RL/IL) 기법**, **(c) 최근 발표된 유사 연구** 세 범주로 나누어 논의합니다.

**(a) 전통적 리타게팅 vs. DexMachina:** 과거의 손 동작 **리타게팅 기술**은 주로 **인간 손가락 궤적을 로봇 손으로 대응시켜보는** 수준이었습니다. 예를 들어 VR 장갑이나 모캡으로 인간 손 움직임을 읽어 로봇 손가락 관절로 매핑하는데, 이는 **로봇과 인간의 형태 차이** 때문에 충돌을 일으키거나 물체를 제대로 쥐지 못하는 경우가 많았습니다. Park 등(2025)은 이러한 **기존 end-effector 정렬 기반 리타게팅**이 **비현실적인 동작**을 만들기 쉽다고 지적하며, 인간-로봇-물체 사이의 **공동 모션 manifold**를 학습하여 보다 **플라우저블(plausible)한 로봇 동작**을 얻는 방법을 제시하였습니다. 해당 방법은 대량의 인간 시연 데이터로 **인간-물체-로봇 사이의 관계 모델**을 훈련한 후, 주어진 새로운 시연에 대해 **로봇 행동을 직접 추론**하는 접근으로, 실제 로봇에 실험하여 **기존 단순 매핑보다 성공률 향상**을 시연했습니다. DexMachina 역시 **인간 시연→로봇 동작**이라는 큰 흐름은 같지만, 접근법은 사뭇 다릅니다. DexMachina는 **명시적으로 물리 시뮬레이션 환경에서 RL로 정책을 학습**하므로, 중간에 인간 모델을 로봇으로 변환하는 모션 생성기가 필요한 대신, **강화학습 자체가 모션을 만들어내는 역할**을 합니다. 이로써 한편으로는 **시연이 부족한 상황에서도** (정책이 탐색을 통해) 답을 찾아갈 수 있고, 다른 한편으로는 **학습된 정책이 온라인으로 물체 반응에 대응**할 수 있어 강인성을 얻습니다. 다만 DexMachina는 현재 **시뮬레이션 학습에 집중**하고 있어 즉시 실세계 로봇에 적용되지는 않았는데, Park 등의 접근은 애초에 **실물 로봇 대상 데이터**로 학습/검증되었다는 차이가 있습니다. 요약하면, 기존 리타게팅 기법들이 **“모델을 학습시켜 한 번에 매핑”**하는 경향이라면, DexMachina는 **“시뮬레이터 안에서 직접 배우게”** 하는 방식으로 문제를 푼다고 볼 수 있습니다. 이 과정에서 **커리큘럼을 통한 탐색 Aux**라는 혁신을 추가하여, 인간처럼 **양손 협응이 필요한 복잡한 작업도 실패 없이 학습**하게 만들었다는 점이 두드러집니다.

**(b) 시연 기반 강화학습(IL/RL) vs. DexMachina:** 인간 시연을 활용하는 기법들은 **모방학습(IL)**과 **Reward 강화학습(RL)**로 크게 나뉩니다. **모방학습**의 경우 시연 데이터만으로 정책을 모방하게 하는데, 로봇 손의 경우 **정확한 로봇행동-결과 페어 데이터 수집**이 어려워 제한적이었습니다. 이를 극복하고자 **텔레오퍼레이션 장비**(VR 장갑 등)를 이용해 **사람이 로봇 손을 직접 원격조작**하며 데이터를 모은 연구들이 다수 있었으나, 특정 로봇에 시스템을 특화해야 하고 주로 **단순 집기(grasp) 등 짧은 작업**에 머무는 한계가 있었습니다. 반면 DexMachina는 **단 한 번의 인간 시연**만으로도 긴 작업을 배울 수 있도록 했고, **인간-로봇 사이의 자세한 매핑 데이터 없이**도 학습이 가능하다는 점에서 **데이터 효율성**을 보여주었습니다. 한편 **강화학습+시연 혼합** 기법으로 2018년 **DAPG** 등이 인간 시演을 초기 정책으로 활용하고 추가 RL 훈련을 통해 성과를 낸 바 있습니다. 그러나 DAPG는 문 손잡이 돌리기 등 **단일 손의 비교적 짧은 작업**을 대상으로 했고, Reward도 **시연 모션 모방**과 **작업 완료 신호**를 수동 설정하는 식이었습니다. DexMachina는 이러한 선행들의 교훈 – 시연이 주는 **탐색 가이드** 효과 – 을 받아들이면서도, **Reward 신호를 자동으로 구성**하고 **외력 지원으로 탐색 효율을 높이는** 등 여러 개선을 통해 **문제 난이도를 한 단계 끌어올린 사례**라 할 수 있습니다. 특히 **장기간의 연속적인 물체 조작**이라는 새로운 영역에서 **시연+RL**의 위력을 입증한 점은 학술적으로 의미가 큽니다.

**(c) 최신 유사 연구들과의 비교:** DexMachina와 같은 시기에 발표된 몇몇 연구들도 **양손 조작 학습**에 도전하고 있어 흥미로운 비교가 됩니다. **ManipTrans (Li et al., CVPR 2025)**는 DexMachina와 동일하게 **인간의 양손 기술을 로봇에 전이**하는 목표를 갖되, **“두 단계”**로 접근한 것이 특징입니다. 먼저 인간 시연을 흉내내는 **트래젝토리 모방 모델**을 학습한 뒤, 이를 기반으로 **잔여 정책(residual policy)**을 강화학습으로 파인튜닝하는 구조입니다. 이러한 **사전학습+미세조정** 방식은 데이터 효율을 높이고 학습을 가속하는 효과가 있어, 저자들은 다양한 시연 데이터로 거대한 **DexManipNet**이라는 로봇 조작 데이터셋까지 구축하였습니다. 결과적으로 ManipTrans 역시 높은 성공률과 정확도를 보였지만, **정책 최종 성능** 측면에서는 DexMachina가 앞서는 것으로 DexMachina 논문 실험에서 확인되었습니다 (위 결과 그래프에서 갈색 막대 비교). 이는 **잔여 학습단계의 제한적 탐색**보다 DexMachina의 **초기부터 끝까지 RL로 최적화하는 접근**이 장기적으로 더 나은 솔루션을 찾았기 때문으로 추정됩니다. 다만 ManipTrans는 시연 모방 모델 덕분에 **3천 개 이상의 다양한 작업 에피소드**를 생성하여 **데이터 다양성**을 확보한 반면, DexMachina는 **각 작업별 한 개의 시연**에 집중합니다. 따라서 **일대일 전이 학습 효율**은 DexMachina가 높지만, **대량의 시연을 일반화**하는 측면은 ManipTrans 쪽이 방향성이 다르다고 볼 수 있습니다. 또 하나 주목할 것은 **실제로봇 적용**인데, ManipTrans 쪽은 시뮬레이션에서 학습한 정책을 실제 로봇으로 이식하는 실험을 시도하고 있습니다. DexMachina는 현재 결과가 모두 시뮬레이터 상이지만, **정밀한 상태 입력에 의존**하고 있어 시각센서 기반 정책으로의 확장은 과제로 남아 있습니다. 저자들도 향후 **비전 기반 RL 정책**이나 **고급 센서 데이터 통합**을 통해 실세계 적용을 모색할 수 있다고 언급하였습니다.

마지막으로, **Videodex (Shaw et al., 2022)**나 **XSkill (Xu et al., 2023)**처럼 **사전 녹화 동영상**이나 **다른 로봇의 경험**으로부터 간접적으로 기술을 학습하는 시도들도 있습니다. Videodex는 웹 비디오로부터 인간의 조작 시퀀스를 추출해 로봇에 학습시켰고, XSkill은 **교차 형태 간(skill transfer across embodiments)** 유용한 스킬을 발견하는 방법을 제안했습니다. 이러한 연구들은 **시연 데이터의 형태**가 다르지만, **로봇이 인간 수준의 다양한 조작을 배우는 방법**을 탐구한다는 점에서 DexMachina와 맥을 같이 합니다. DexMachina의 **가치**는 특히 **고품질의 한정된 시연을 최대한 활용**하여 복잡한 임무를 달성하는 쪽에 있는데, 이는 향후 비디오나 저해상도 데이터에도 응용될 수 있는 통찰을 줍니다. 또한 **AnyTeleop (Qin et al., 2023)** 같은 **원격 조작 시스템**들은 사람의 즉각적인 조작을 로봇으로 투영하여 복잡한 임무를 수행했는데, DexMachina는 **한 걸음 더 나아가** 이러한 인간 개입 없이도 **자율 정책으로 임무를 지속 수행**하게 만들었다는 점에서 **완전자율성**에 한층 가까워졌습니다.

## Conclusion

DexMachina는 **인간 시연으로부터 양손 로봇 조작 기술을 배우는 새로운 방법론**을 제시함으로써, 현재 활발한 **섬세 조작(dexterous manipulation)** 연구 분야에 큰 진전을 가져왔습니다. 방법론적으로 보면, **Reward 설계의 자동화**와 **커리큘럼을 통한 탐색 지원**을 결합하여 **강화학습의 취약점**을 효과적으로 보완한 점이 돋보입니다. 실험적으로는 다양한 로봇 손에 걸쳐 **일관된 성능 우위**를 증명함으로써, 제안 기법의 **범용성**과 **실용적 가치**를 입증했습니다. 특히 하나의 프레임워크로 **하드웨어 성능을 비교 평가**할 수 있다는 관점은, 향후 **로봇 손 개발자들이 디자인 선택을 최적화**하는 데에도 기여할 수 있을 것입니다.

물론 해결해야 할 과제도 남아 있습니다. 앞서 언급했듯이 DexMachina는 **시뮬레이션 상태정보에 크게 의존**하고 있어, 이를 **실세계 센서 입력(시각/촉각)**으로 옮기는 작업이 필요합니다. 또한 현실에서는 예기치 못한 교란이나 물체 모델의 불확실성 등이 존재하므로, 정책이 **오류 복구**나 **적응적 재계획**을 할 수 있도록 강화하는 연구도 중요할 것입니다. **데이터 수집** 측면에서는, 현재는 사람 시연을 별도로 캡쳐해야 하지만, 미래에는 **3D 비전**이나 **모션 캡쳐 자동화** 기술의 발전으로 보다 손쉽게 시연 데이터를 확보할 수 있을 것입니다. 이러한 보완이 이루어진다면 DexMachina의 접근법은 산업 현장이나 서비스 로봇에서 **사람처럼 도구를 다루고 협업하는 로봇**을 훈련하는 데 큰 역할을 할 것으로 기대됩니다.

**결론적으로**, DexMachina는 **기능적 리타게팅**이라는 개념을 통해 로봇에게 **“동작의 형태”보다 “동작의 목적”**을 가르치는 법을 보여주었습니다. 이는 향후 인간 수준의 다재다능한 로봇 조작을 실현하는 데 중요한 방향을 제시하며, 현재 진행 중인 많은 후속 연구들의 기반이 되고 있습니다. 인간이 시연하고 로봇이 배워서 자기만의 방식으로 임무를 수행하는 모습은, 궁극적으로 **휴먼-로봇 협업**과 **자율 기술 학습**의 접점에서 매우 유망한 패러다임이라 할 수 있습니다.
