---
title: "Sequential Dexterity 리뷰"
date: 2025-07-22
categories: [rl, assembly, hand]
toc: true
number-sections: true
description: Chaining Dexterous Policies for Long-Horizon Manipulation
---


- [Paper Link](https://arxiv.org/abs/2309.00987)
- [Project Link](https://sequential-dexterity.github.io/)

1. 🤖 본 논문은 복잡하고 장기적인 조작 작업을 위해 여러 Dexterous 정책들을 효율적으로 연결하는 "Sequential Dexterity" 시스템을 제안합니다.
2. ⚙️ 이 시스템은 전방 초기화(forward initialization)와 Transition Feasibility Function을 활용한 후방 미세 조정(backward fine-tuning)을 포함하는 양방향 최적화 과정을 통해 정책 간의 원활한 전환을 가능하게 합니다.
3. ✅ 제안된 프레임워크는 시뮬레이션 및 실제 로봇 환경에서 기존 단방향 스킬 체이닝 방법들보다 작업 성공률을 크게 향상시키고, 새로운 객체 모양에 대한 일반화 및 zero-shot transfer 능력을 입증했습니다.


<center>
<img src="../../images/2025-07-22-seqdex/1.gif" width="80%" />
</center>

---

# Brief Review

본 논문은 다양한 서브태스크들로 구성된 장기적(long-horizon)이고 복잡한 조작 작업을 수행하기 위한 RL(Reinforcement Learning) 기반의 일반 시스템인 Sequential Dexterity를 제안한다. 덱스터러스 핸드(dexterous hand)는 높은 자유도와 적응성으로 이러한 복잡한 작업에 적합하지만, 고차원적인 행동 공간과 복합적인 동역학으로 인해 활용이 어렵다는 문제가 있다. Sequential Dexterity는 여러 덱스터러스 정책(policy)을 연결(chain)하여 장기적 작업 목표를 달성하며, 시스템의 핵심은 전이 가능성 함수(Transition Feasibility Function, TFF)이다. 이 TFF는 정책 연결의 성공률을 높이기 위해 서브 정책(sub-policy)을 점진적으로 미세 조정(finetune)하고, 실패로부터의 복구 및 불필요한 단계를 건너뛰기 위한 자율적인 정책 전환(policy-switching)을 가능하게 한다. 시뮬레이션에서 소수의 객체로만 훈련되었음에도 불구하고, 본 시스템은 새로운 객체 형상에 대한 일반화 능력을 보이며, 실제 로봇 시스템으로 제로-샷 전이(zero-shot transfer)가 가능하다.

기존 연구들은 단일 스킬(skill)에 중점을 두거나, 인접한 스킬 간의 상태 공간 정규화(regularization)에 집중하여 장기적 목표가 초기 정책에 미치는 영향을 고려하지 않았다. 본 연구는 이와 달리 이중 방향 최적화(bi-directional optimization) 과정을 제안하여 전체 스킬 체인(skill chain)을 포괄적으로 최적화한다. 이는 순방향 초기화(forward initialization) 과정과 역방향 미세 조정(backward fine-tuning) 과정으로 구성된다.

핵심 방법론은 다음과 같다.
1.  **덱스터러스 서브 정책 학습(Learning dexterous sub-policies):**
    장기적 작업을 처음부터 학습하는 것은 매우 어렵기 때문에, 먼저 전체 작업을 $K$단계의 서브태스크 $G = (g_1, g_2, ..., g_K)$로 분해하고, 각 서브 정책 $\pi_i$를 PPO(Proximal Policy Optimization) 알고리즘으로 학습한다. 각 서브태스크는 MDP(Markov Decision Process) $M = (\mathcal{S}, \mathcal{A}, \pi, T, R, \gamma, \rho)$로 공식화된다.
    **순방향 초기화(Forward initialization):** 개별 서브 정책을 훈련하기 위한 초기 상태를 정확하게 샘플링하는 것은 어렵다. 본 연구는 이전 서브태스크 $\pi_{i-1}$의 성공적인 최종 상태($\mathbf{s}_T^{i-1}$)가 다음 서브 정책 $\pi_i$의 그럴듯한 초기 상태($\rho_i$)를 제공한다는 점에 착안한다. 따라서, 프레임워크는 태스크의 시간 순서에 따라 서브 정책들을 순차적으로 훈련한다. 각 서브 정책 $\pi_i$를 훈련한 후, 성공적인 종료 상태 집합 $\{\mathbf{s}_T^i\}$을 수집하고, 이를 다음 정책 $\pi_{i+1}$을 훈련하기 위한 초기 상태 분포 $\rho_{i+1}$로 사용한다. 이 순방향 훈련 방식은 초기 상태의 유효성을 보장하여 덱스터러스 정책 학습의 효율성을 높인다.

2.  **전이 가능성 함수를 이용한 정책 연결(Policy chaining with Transition Feasibility Function):**
    순방향 초기화만으로는 이전 정책 $\pi_{i-1}$이 다음 정책 $\pi_i$가 해결할 수 없는 종료 상태에 도달할 수 있으므로 성공을 보장하지 못한다. 이 문제를 해결하기 위해, 다음 정책 $\pi_i$의 가능성(feasibility)을 이전 정책 $\pi_{i-1}$에 역으로 전달하여, $\pi_{i-1}$이 $\pi_i$가 처리할 수 있는 상태로 최적화되도록 해야 한다. 이를 위해 전이 가능성 함수를 이용한 역방향 정책 미세 조정 메커니즘을 제안한다.
    **전이 가능성 함수 학습(Learning Transition Feasibility Function):** 주어진 상태에 대한 정책의 가능성은 해당 상태에서 시작할 때 정책이 최종적으로 성공할 수 있는 능력으로 정의된다. 이 개념은 전이 상태 $\mathbf{s}_0^i \in \rho_i$ (이는 $\mathbf{s}_T^{i-1}$와 동일)를 서브태스크 실행 내의 예상 보상 합 $E_{\pi_i} \left[ \sum_{t=0}^{T-1} r_t \right]$으로 매핑하는 함수 $F: \mathcal{S} \rightarrow \mathbb{R}$로 정형화된다. TFF는 단일 상태 $\mathbf{s}_T^{i-1}$만으로는 $\pi_i$의 성능을 구분하기에 불충분하므로, 이전 서브태스크로부터의 객체 속도와 같은 시간적 정보를 포함하여 10단계의 관찰 상태 시퀀스 $\mathbf{s}_{[0:T-1]}^{i-1}$를 입력으로 사용하고, Multi-head Attention Network를 활용하여 적절한 시간 정보를 추출한다. TFF $F^i$의 최종 학습 목표는 다음과 같다:
    $$L_i = \|F^i(\mathbf{s}_{[0:T-1]}^{i}) - E_{\pi_i} \left[ \sum_{t=0}^{T-1} r_t \right] \|_2^2$$
    **역방향 정책 미세 조정(Backward policy fine-tuning):** $F^i$가 훈련되면, $F^i$를 보조 보상 구성 요소로 통합하여 이전 정책 $\pi_{i-1}$을 미세 조정할 수 있다. 미세 조정은 마지막 서브태스크 정책 $\pi_{K-1}$부터 시작하여 첫 번째 정책 $\pi_1$이 업데이트될 때까지 각 이전 정책을 순차적으로 개선한다. 각 미세 조정 단계에서, $F^i$는 원본 서브태스크 보상 $R_{i-1}$과 결합되어 $\pi_{i-1}$을 미세 조정하는 추가 보상으로 사용된다. 최종 정책 미세 조정 보상 함수는 다음과 같다:
    $$R'_{i-1}(\mathbf{s}_t, \mathbf{a}_t, \mathbf{s}_{t+1}; F^i) = \lambda_1 R_{i-1}(\mathbf{s}_t, \mathbf{a}_t, \mathbf{s}_{t+1}) + \lambda_2 F^i_{\omega}(\mathbf{s}_{[0:T-1]}^i)$$
    여기서 $\lambda_1$과 $\lambda_2$는 가중치 계수이다. $\pi_{i-1}$이 개선되면, 데이터(초기 상태 $\mathbf{s}_{[0:T-1]}^{i-2}$에서 $\pi_{i-1}$이 받은 누적 보상 $E_{\pi_{i-1}} [\sum r_t]$까지 매핑)를 수집하여 새로운 전이 가능성 함수 $F^{i-1}$를 구성하고, 이는 다시 이전 정책 $\pi_{i-2}$를 미세 조정하는 데 사용된다.

3.  **전이 가능성 함수를 이용한 정책 전환(Policy switching with Transition Feasibility Function):**
    여러 덱스터러스 정책을 연결하는 데 중요한 과제는 다음 정책으로 언제, 어떤 정책으로 전환해야 할지 결정하는 것이다. 본 연구에서는 훈련된 TFF를 정책 전환 식별자로 활용한다. 각 시간 단계에서 다음 서브 정책의 전이 가능성 함수는 가능성 점수 $c_{i+1}^t = F^{i+1}(\mathbf{s}_{[0:t-1]}^t)/h_{i+1}$를 출력하며, $h_{i+1}$은 성공적인 태스크 실행의 보상에 기반한 임계값이다. 정책 전환의 이상적인 시점은 $c_{i+1}^t > 1$일 때로 정의된다. 또한, 로봇이 이전 정책을 사용하여 복구해야 하거나 이미 달성된 서브태스크를 건너뛰어야 할 수도 있으므로, 학습된 TFF 그룹 ($F^2, ..., F^K$)을 단계 추정기(stage estimator)로 활용한다. 매 정책 전환 단계에서, 전체 태스크의 최종 TFF부터 역방향으로 가능성 점수를 계산하여, 처음으로 $c_i^t > 1$인 서브 정책이 다음 실행 정책으로 선택된다. 어떤 점수도 만족하지 않으면, 로봇은 전체 태스크를 처음부터 재시작한다. 이러한 방식은 정책 실행 중 예상치 못한 실패에 대한 로봇의 견고성(robustness)을 향상시키고, 불필요한 단계를 건너뛰어 효율적인 태스크 실행을 촉진한다.

본 시스템은 Lego 블록 구조 구축 및 도구 위치 지정이라는 두 가지 장기적 덱스터러스 조작 태스크에 대해 평가되었다. 실험 결과, 본 연구의 이중 방향 최적화 방식은 기존의 단일 방향 스킬 연결 방법들(V-Chain, Policy-Seq, T-STAR)보다 태스크 성공률에서 20% 이상 크게 뛰어난 성능을 보였다. 특히, TFF는 PPO로 훈련된 가치 함수보다 다음 정책의 가능성을 더 잘 모델링하여 30% 이상 높은 성공률을 달성했다. 또한, TFF에 시간적(temporal) 정보를 입력으로 사용했을 때 정적 상태 정보만 사용했을 때보다 8% 더 나은 성능을 보여, 동적인 손가락 움직임을 포함하는 덱스터러스 정책 연결에 있어 시간 정보 추출의 중요성을 강조했다. 자율적인 정책 전환 능력은 30% 이상의 성공률 향상을 가져와 장기적 태스크 성공에 필수적임을 입증했다. 실제 로봇 실험에서도 본 접근 방식은 기존 방법들보다 30% 이상 높은 성공률을 보이며, 두 개의 블록을 쌓는 도전적인 8단계 태스크에서 33%의 성공률을 기록했다(다른 베이스라인은 0%).

본 연구의 한계점으로는 접촉이 많은 삽입 과정을 시뮬레이션하기 어렵고, 실제 로봇 배포 시 수동으로 설계된 누름 동작이 필요하다는 점과, 모터 촉각(motor tactile) 정보가 성능 향상에 큰 기여를 하지 못했다는 점이 있다. 향후 연구에서는 센서 기반 촉각 신호의 잠재력을 탐색할 수 있다.

결론적으로, Sequential Dexterity는 심층 강화 학습으로 학습된 다수의 덱스터러스 정책을 연결하는 이중 방향 최적화 과정을 통해 장기적 덱스터러스 조작 태스크를 해결하는 시스템이다. 시스템의 핵심인 전이 가능성 함수는 서브 정책의 점진적인 미세 조정과 동적 정책 전환을 가능하게 하여 정책 연결의 성공률을 크게 향상시킨다. 본 시스템은 실제 덱스터러스 로봇으로 제로-샷 전이가 가능하며, 새로운 객체 형상에 대한 일반화 능력을 보인다. 본 이중 방향 최적화 프레임워크는 덱스터러스 조작을 넘어 양팔 로봇(bimanual robots)을 위한 스킬 연결과 같은 잠재적 응용 분야에서도 일반적인 스킬 연결 방법으로 활용될 수 있다.

---

# Detail Review

> Sequential Dexterity: 다단계 정교한 조작을 위한 정책 연쇄 심층 분석

## 논문 개요 및 문제 제기

**Sequential Dexterity**는 긴 시퀀스로 이루어진 복잡한 조작 작업을 다중 **서브 정책**(sub-policy)을 **연쇄**하여 수행하는 강화학습 기반 프레임워크입니다. 현실 세계의 조작 과업들은 여러 **서브태스크**(예: 탐색, 정렬, 파지, 삽입)로 구성되는 경우가 많으며, 각 단계의 요구사항이 상이합니다. 사람의 손과 유사한 **다지(多指) 로봇 손**(dexterous hand)은 도구 교체나 재파지 없이 다양한 조작 모드를 전환할 수 있어 이러한 **장기 다단계 작업**에 잠재력이 크지만, 동시에 **자유도**와 **행동 공간**이 고차원이라 학습과 제어가 매우 어려운 도전과제를 제시합니다.

기존에는 전체 작업을 단일 정책으로 학습하기 어려워 태스크를 분할한 뒤 각 단계를 순차 실행하는 접근이 검토되었으나, **나이브한 순차 실행**은 앞 단계에서 도달한 상태가 다음 단계 정책이 학습된 분포를 벗어나는 경우 쉽게 실패하게 됩니다. 다시 말해, 한 단계의 종료 상태가 다음 단계 정책의 시작 상태 분포 밖에 있다면 연쇄에 실패하게 됩니다. 이러한 **상태 분포 불일치 문제**와 긴 호라이즌에 걸친 **복잡한 동적 상호작용**은 다단계 정책 연쇄의 핵심 난제로 지목됩니다. Sequential Dexterity 논문은 이러한 문제를 해결하고자, **정책 체이닝(policy chaining)**을 위한 새로운 **양방향 최적화 프레임워크**를 제안합니다. 이를 통해 시뮬레이션에서 학습한 복수의 손 동작 정책들을 한 데 묶어 복잡한 목표를 달성하고, 학습에 사용하지 않은 새로운 물체 형태에도 일반화하며, 나아가 **추가 학습 없이 실물 로봇에 적용(제로샷 전이)**할 수 있음을 시연하였습니다.

## 방법론 상세 분석

&#x20;*그림 1: Sequential Dexterity 프레임워크 개요. (a) 시뮬레이션에서 각 서브태스크별 정책 $\pi^1, \pi^2, \pi^3, \pi^4$를 순차 학습하는 **순방향 초기화**(파란색)와, 학습 완료 후 이전 정책을 미세조정하는 **역방향 파인튜닝**(빨간색) 과정. $\rho^i$는 정책 $\pi^i$의 상태 분포, $F^i$는 서브태스크 $i$와 $i!-!1$ 사이의 **전이 가능성 함수**. (b) 실 로봇 적용 시 정책 입력으로 6D 물체 자세와 로봇 상태가 주어지며, **정책 선택**은 최근 10스텝 상태를 고려한 $F^i$ 출력이 임계값 $h^i$를 넘는지에 따라 이루어진다.
단계 $\pi^2$ Orient, $\pi^3$ Grasp, $\pi^4$ Insert를 실 로봇에서 수행하는 모습.*

**Sequential Dexterity**의 핵심 아이디어는 긴 조작 작업을 구성하는 각 단계를 독립적인 **하위 정책**으로 정의하고 강화학습으로 학습한 뒤, 이들을 **체계적으로 연결**하여 전체 작업을 수행하도록 만드는 것입니다. 이를 위해 논문에서는 **양방향 최적화**(bi-directional optimization)라고 불리는 학습 절차를 도입하였습니다. 우선, **순방향 초기화 단계**에서는 각 서브태스크마다 개별 정책을 학습합니다. 이때 **PPO 알고리즘** 등의 강화학습 방법을 사용하여 각 서브태스크를 MDP로 정의하고 정책을 훈련하며, 이전 단계의 **종료 상태 분포**를 활용해 다음 단계 정책의 **초기 상태**로 샘플링함으로써 연쇄 시 발생할 상태 분포를 미리 포함시킵니다. 이렇게 하면 각 정책 $\pi^i$가 전 단계 $\pi^{i-1}$가 만들어낼 법한 상태에서도 동작하도록 **전방위 학습**을 수행할 수 있습니다.

이후 **역방향 파인튜닝 단계**에서는 **전이 가능성 함수**(Transition Feasibility Function, $F^i$)를 활용하여 이웃한 정책 사이의 접속을 더욱 최적화합니다. 전이 가능성 함수 $F^i(s)$란 **서브태스크 $i$의 초기 상태** $s$에 대해 해당 상태에서 정책 $\pi^i$를 시작했을 때 **성공 가능성이나 성과**를 출력하는 함수입니다. 논문에서는 각 $\pi^i$를 학습한 후, $\pi^{i-1}$의 종료 상태(즉 $\pi^i$의 시작 상태)들과 그로부터 $\pi^i$를 실행했을 때 얻은 누적 보상 또는 성공 여부를 모아서 $F^i$를 **지도 학습**으로 훈련합니다. 이렇게 학습된 $F^i$는 상태 $s$가 다음 단계 정책에 얼마나 **적합한지**를 예측하는 역할을 하며, 마치 다음 정책의 **가치 함수**처럼 동작하지만 일반적인 RL 가치 함수와 달리 **할인되지 않은 최종 성과**에 집중합니다. 실제 PPO 등으로 얻은 가치 함수는 할인의 영향으로 후속 정책의 성공 가능성을 제대로 평가하지 못하는데, 저자들은 별도의 $F^i$ 학습으로 이 문제를 해결하였다고 보고합니다. 특히 $F^i$에는 최근 몇 스텝 동안의 상태 변화를 입력으로 사용하여 **시간적 맥락**도 반영하였는데, 이렇게 **과거 상태 이력**을 통합함으로써 단순 단일 상태 기준보다 전이 성공 여부를 정확히 예측할 수 있었습니다.

이 전이 가능성 함수를 이용해 이전 단계 정책을 **역방향으로 미세조정**합니다. 구체적으로, 정책 $\pi^{i-1}$의 학습에 $F^i$를 통한 **추가 보상** 또는 **규제 항**을 부여하여, $\pi^{i-1}$이 종료 시 상태를 $F^i$ 기준으로 높은 **전이 점수**를 얻는 방향으로 유도합니다. 예를 들어 서브태스크 2 (Orient)의 전이 함수 $F^3$가 “이 상태에서 서브태스크 3 (Grasp)이 성공할 가능성”을 나타낸다면, $\pi^2$의 종료 상태가 $F^3$ 상 높은 점수를 받도록 $\pi^2$를 업데이트하는 식입니다. 이를 통해 각 하위 정책의 **종료 상태 분포를 다음 정책의 초기 상태 분포와 최대한 일치**시키며, 결과적으로 **정책 연쇄의 성공률을 극대화**합니다. Lee 등(2021)의 T-STAR와 같은 기존 **기술 연쇄 기법**들이 종료 상태 분포를 줄이는 **대항훈련(어드버서리) 방식**으로 접근한 것과 대비되며, Sequential Dexterity는 보다 **직접적으로 후속 성공 가능성을 예측**하여 활용한다는 차별점이 있습니다.

마지막으로, **정책 체이닝 실행 단계**에서 중요한 것이 **자율적인 정책 전환 메커니즘**입니다. 고정된 단계 순서로 한 번씩만 정책을 실행하는 대신, **전이 가능성 함수**를 활용하여 **실시간으로 적절한 정책을 선택**합니다. 구체적으로 로봇이 작업 도중 매 시점마다 현재 상태를 가지고 각 다음 가능 단계들의 $F$ 값을 평가하고, **임계값**을 넘겨 **실행 준비가 된** 것으로 판정된 **가장 후순위 단계**부터 실행합니다. 예를 들어 블록 쌓기 작업에서, 현재 상태에서 **삽입 단계**가 바로 가능하다고 판단되면 (예: 블록을 이미 손에 쥐고 있고 자세도 맞다면) 중간 단계를 건너뛰고 삽입을 수행합니다. 반대로, 만약 현재 단계 수행이 잘못되어 목표 상태에 도달하지 못하더라도, $F$가 알려주는 바에 따라 이전 단계로 **되돌아가 교정**할 수도 있습니다. 실제로 저자들은 **정책 전환을 역순으로 탐색**하는 방식을 취하는데, 삽입($\pi^4$)→파지($\pi^3$)→정렬($\pi^2$) 순으로 각각의 $F$ 값을 확인해 **가능한 가장 진전된 단계**로 점프하거나, 모든 $F$ 출력이 기준 미만이면 **처음부터 탐색($\pi^1$)**을 다시 실행하도록 설계했습니다. 이러한 **동적 스위칭**은 실패 복구와 불필요한 단계 생략을 모두 가능케 하여 체이닝의 **강인성**을 크게 높였습니다. 요컨대, Sequential Dexterity는 학습 단계에서 **각 정책이 연쇄에 적합하도록 양방향으로 최적화**되고, 실행 단계에서는 **전이 가능성 함수 기반의 지능형 전환**을 통해 긴 계획을 견고하게 수행합니다.

각 서브태스크의 **보상 설계**는 해당 단계의 목표 완료에 중점을 두고 이루어졌습니다. 예를 들어 “탐색” 정책에는 올바른 블록을 식별하거나 접근했을 때의 보상, “정렬” 정책에는 물체의 자세를 목표 자세에 가깝게 만들수록 부여되는 보상, “파지”에는 성공적으로 블록을 쥐었을 때의 보상 등이 주어집니다. **Sparse reward(희소 보상)**보다 **shaping 보상**을 적절히 추가하여 학습을 유도했을 것으로 추측되며, 최종 단계인 “삽입”도 블록이 목표 위치에 안착하면 보상을 얻도록 설계되었습니다. 특히 역방향 파인튜닝 시에는 앞서 언급한 대로 $F^i$의 출력을 일종의 **추가 보상**으로 취급하여 이전 정책의 종료 상태를 향상시키는 데 사용하였고, 이는 기존 보상과 합쳐져 에이전트가 **다음 단계의 성공까지 염두에 둔 행동**을 하도록 합니다. 이러한 보상 체계와 전이 함수 활용을 통해 **부분 최적해**에 머무르기 쉬운 개별 정책들을 **전체 작업의 성공**이라는 **글로벌 목표**에 수렴시키도록 유도한 것이 본 방법론의 특징입니다.

## 실험 설정 및 벤치마크

본 논문에서는 제안한 Sequential Dexterity 프레임워크를 **시뮬레이션 실험**과 **실 로봇 실험**을 통해 검증했습니다. 사용된 시뮬레이터는 대규모 병렬 환경을 지원하는 **Isaac Gym**으로, 각 서브태스크 정책을 학습할 때 최대 1024개의 환경에서 병렬로 데이터를 수집함으로써 학습 효율을 높였습니다. 실험은 두 가지 대표적인 **다단계 조작 과제**에 대해 이루어졌으며, 각 과제에 대해 제안 기법의 성능을 기존 방법들과 비교 평가했습니다.

* **과제 1: 블록 조립 (Building Blocks)** – 다양한 색상과 크기의 블록들이 섞여있는 상자에서 지정된 블록을 찾아 (**탐색**), 손가락을 이용해 적절한 자세로 **정렬**한 후, 블록을 **파지**하여 들어 올리고, 목표 구조물의 해당 위치에 **삽입**하는 일련의 작업입니다. 이 과제는 **Mega Bloks** 장난감을 사용한 간이 조립 작업으로 볼 수 있으며, 전체 구조를 완성하기 위해 이러한 네 가지 서브태스크를 블록 개수만큼 반복 수행해야 하는 **복합 장기과업**입니다. 시뮬레이션 환경에서는 블록들이 무작위로 쌓여있고 로봇은 손목 카메라 등의 감각을 통해 블록을 인식하게 됩니다. 실험에 사용된 로봇은 **다관절 로봇 손**(4-finger dexterous hand)을 장착한 로봇 팔로, 시뮬레이션에서 학습한 정책을 그대로 적용하여 실제 플라스틱 블록 조립을 수행했습니다. 실 로봇 환경에서는 상단에 장착된 카메라로 테이블 위 블록들의 대략적 위치를 파악하고, 손목에 장착된 RGB-D 카메라 영상에서 목표 블록을 분할(segmentation) 및 추적하여 실시간 6D 자세를 추정하는 **비전 파이프라인**을 구축했습니다. 이 정보를 실시간으로 정책에 입력하여 로봇이 블록을 찾아 집을 수 있도록 했습니다. 다만 시뮬레이터에서 블록을 홈에 끼워 넣는 **삽입** 동작의 아주 세밀한 부분(블록을 완전히 눌러 끼우는 과정)은 접촉 모델 한계로 구현이 어려웠기 때문에, 시뮬레이션에서는 **삽입 정책**이 블록을 목표 위치에 대략 놓는 것으로 학습되었습니다. 실 로봇 실험에서는 블록을 제자리에 올려놓는 것까지를 학습된 정책으로 수행하고, 마지막에 로봇 팔로 **블록을 아래로 눌러 완전히 끼우는 동작은 스크립트로 처리**하는 방식으로 실제 조립을 구현했습니다. 이러한 부분은 본 방법의 **물리 한계**를 보완하기 위한 조치로, 이후 한계점에서 추가 논의됩니다.

* **과제 2: 공구 자세 맞추기 (Tool Positioning)** – 테이블 위에 임의 자세로 놓인 공구를 **파지**하여 들어올린 뒤, 사용하기 편한 **준비 자세로 재정렬**(reorient)하는 작업입니다. 실험에서는 망치(hammer)를 대표적인 공구로 사용하여 학습하였고, 초기에는 망치가 바닥에 아무렇게나 놓여 있습니다. 로봇 손이 망치를 집어 드는 동작(Grasp)이 1단계, 이어서 공구를 회전시켜 손잡이 부분이 아래로 향하도록 세우는 동작(Orient)이 2단계로 구성됩니다. 이 과제는 **두 단계의 체이닝**이지만, 핵심은 **첫 파지의 방식이 두 번째 단계의 난이도에 큰 영향을 준다**는 점입니다. 예컨대 망치를 엉뚱한 각도로 집으면 회전 단계에서 균형을 잡기 어렵거나 충돌이 발생해 실패할 수 있습니다. 따라서 **초기 파지 단계부터 전체 과업을 고려한 최적의 방식**을 취하는 것이 중요합니다. 이 과제 역시 시뮬레이션으로 학습한 후 실제 로봇으로 **제로샷 검증**되었으며, 추가로 **일반화 성능** 평가를 위해 학습에 사용하지 않은 공구(주걱, 숟가락 등의 도구)를 대상으로도 실험이 수행되었습니다.

&#x20;*그림 2: 실험 환경 및 비교. (a) **블록 조립** 과제 – 시뮬레이터(왼쪽)와 실제 로봇(오른쪽)의 작업 환경. 로봇 팔 끝에 4손가락 로봇 핸드와 손목 카메라(빨간 상자), 상단에 전역 카메라(파란 상자)가 배치되어 블록을 탐색하고 조립한다. (b) **공구 자세 맞추기** 과제 – 초기 상태에 놓인 망치를 대상으로, **Baseline**(기존 순차 실행 방법) vs **Ours**(제안 방법)의 비교 예시. Baseline은 1단계 파지 이후 망치를 비스듬히 잡아 2단계 정렬에 실패(빨간 X)하지만, 제안 방법은 첫 파지부터 각도를 고려해 잡음으로써 최종 정렬에 성공(녹색 체크)한다.*

**벤치마크 방법**으로는 다음과 같은 비교 대상이 선정되었습니다. 첫째, 각 서브태스크를 개별적으로 학습한 후 **아무 조정 없이 순서대로 실행**하는 **기본 정책 연쇄** 방법이 있습니다. 이는 전이 가능성 함수를 사용하지 않고 단순히 정해진 순서로 한 번씩 정책을 적용하는 것으로, **전이 분포 불일치 문제**를 그대로 가지는 baseline입니다. 둘째, Lee 등(2021)이 제안한 **T-STAR** 기법 등 기존 **기술 체이닝** 알고리즘과의 비교가 수행되었습니다. T-STAR는 종료 상태 분포를 제한하기 위해 **적대적 학습**을 활용했던 선행 연구로, 본 논문의 문제 설정(특히 다지 로봇 손 사용)과는 차이가 있지만 아이디어상 인접한 기법입니다. 셋째, **V-Chain**이라고 명명된 비교 방법은 **다음 단계 정책의 RL 가치함수**를 전이 평가에 활용하여 이전 정책들을 파인튜닝하는 방식으로, 전이 가능성 함수를 쓰는 대신 PPO로 학습된 **가치망**으로 연쇄를 시도한 경우입니다. 저자들은 이를 통해 **전이 가능성 함수의 이점**을 검증하고자 했습니다. 넷째, **Ours w/o temporal**이라고 하는 **변형 모델**은 제안한 전이 가능성 함수에서 **시간적 이력(상태 시퀀스)** 입력을 제외하고 **현재 상태만으로** 전이 판정을 하도록 한 버전입니다. 이를 통해 전이 함수에 시간 정보를 포함시키는 것이 성능에 미치는 영향을 확인하였습니다. 마지막으로, 본 논문의 **Ours (제안 기법)**은 **양방향 최적화 + 전이 가능성 함수 + 자율 스위칭**의 풀셋을 사용한 완전한 Sequential Dexterity 시스템입니다.

**평가 지표**로는 각 과제에서 **최종 목표를 성공적으로 달성**했는지의 **성공률**이 주요하게 사용되었습니다. 블록 조립의 경우 주어진 구조물을 모두 완성하는 데 성공한 실험 에피소드 비율로 측정되며, 공구 자세 맞추기는 공구를 바른 자세로 세워서 안정적으로 유지하는 데 성공한 비율로 정의됩니다. 이 과정에서 서브태스크별 **단계 성공률**이나 수행 시간, 정책 전환 횟수 등도 부가적으로 관찰되었으나, 가장 중요한 평가지표는 **전체 작업의 완수 여부**였습니다. 또한 **정성적 평가**로 각 과정의 행동 궤적과 최종 상태를 비교하고, 정책 전환 전략에 따른 **성공/실패 사례**를 분석하였습니다. **일반화 평가**로는, 앞서 언급한 대로 공구 과제에서는 **학습 시 보지 못한 새로운 도구**(예: 주걱, 숟가락)를 투입하여 성공 여부를 측정하였고, 블록 조립 과제에서는 **새로운 구조 조립**(예: 다른 형태의 블록 조합 구조)에 대한 적용을 살펴보았습니다. 끝으로, **실제 로봇 검증**에서는 시뮬레이션에서 학습한 정책을 그대로 사용하여 실제 환경에서 작업을 수행하도록 하고, 그 **성공 사례 영상과 빈도**를 보고하였습니다. 성공률의 경우 시뮬레이션만큼 다수 반복하지는 못했지만, 몇 차례 시연에서 **지속적으로 작업 완수에 성공**하는 모습을 보여 **제로샷 학습 전이**의 가능성을 확인했습니다.

## 결과 분석

실험 결과, Sequential Dexterity 프레임워크는 제안한 요소들이 모두 결합되었을 때 **기존 방법 대비 현저한 성능 향상**을 이루는 것으로 나타났습니다. 특히 **전체 작업 성공률** 지표에서 두 과제 모두 유의미한 개선을 달성했습니다.

먼저 **블록 조립 과제**를 살펴보면, 제안 기법은 약 **80% 이상의 최종 성공률**을 기록하여, **기본 순차 실행 방식**(전이 최적화 없이 각 단계 한 번씩 수행) 대비 약 **30%포인트 이상 상승**한 성과를 보였습니다. 구체적으로, baseline의 경우 여러 번의 시행 중 절반도 안 되는 에피소드에서만 구조 완성에 성공한 반면, Sequential Dexterity를 적용하면 **10번 중 8번 이상**은 로봇이 모든 블록을 제자리에 정확히 쌓아올릴 수 있었습니다. **공구 자세 맞추기 과제**에서도 유사한 경향으로, 제안 기법은 약 **90%에 가까운 성공률**을 달성하여 baseline(약 60% 수준)보다 크게 앞섰습니다. 이로써 **정책 체이닝의 효과**가 정량적으로 입증되었는데, 특히 **전이 가능성 함수 기반의 자율 스위칭**이 없었던 경우(정책 전환 허용 횟수 0일 때) 성공률이 크게 낮았다가, 스위칭을 1회, 2회, 3회로 늘릴수록 성공률이 단계적으로 향상되는 양상을 보였습니다. **그림 3-(a)**에서 보이듯이, 정책 스위칭을 전혀 허용하지 않으면 블록 조립 성공률은 50% 미만에 머물렀으나, 최대 3회까지 재시도를 허용하자 약 80% 수준까지 높아졌습니다. 마찬가지로 공구 과제도 0회 스위칭 시 \~60%에서 3회 시 \~90%로 상승하여, **동적 정책 전환**이 복잡한 조작을 성공으로 이끄는 데 핵심적인 역할을 함을 알 수 있습니다.

&#x20;*그림 3: 성능 지표 및 전이 함수 효과 분석. (a) **정책 스위칭 허용 횟수에 따른 최종 성공률** – 스위칭을 늘릴수록 블록 조립(파란색)과 공구 조정(주황색) 과제 모두 성공률이 크게 향상됨을 보여준다. (b) **물체 자세 분포 비교(T-STAR vs Ours)** – 블록 조립 과제의 정렬 단계에서, 기존 방법(T-STAR)은 로봇이 다양한 각도로 블록을 배치하여 후속 파지 단계에 진입하지만(위, 파란점 분포), 제안 방법은 **블록의 돌기가 위로 향한 자세**에 대부분 모이도록 유도함으로써 이후 파지와 삽입을 쉽게 만들고 성공 확률을 높인다(아래). 이는 전이 가능성 함수가 **다음 단계 성공에 최적인 상태**를 학습하고 이전 단계 정책을 그 방향으로 유도한 결과이다.*

**전이 가능성 함수의 활용**은 단순한 성공률 상승뿐 아니라, 각 단계의 **정성적 동작 품질**도 향상시켰습니다. 예를 들어 블록 조립에서 **정렬(Orient) 정책**은 블록의 **방향을 어떻게 잡느냐**가 이후 **파지** 및 **삽입** 성공에 결정적인데, 제안 기법은 정렬 단계에서 블록의 **돌기가 위로 향하도록** 회전시키는 경향을 보였습니다. 이는 사람이 레고 블록을 끼울 때 윗면 돌기가 위로 오도록 맞추는 것과 유사한 전략으로, 이러한 **계획적 동작**은 **전이 가능성 함수가 “이 자세가 다음 단계에 유리하다”는 신호를 주었기 때문**입니다. 반면, 전이 함수를 쓰지 않은 방법이나 T-STAR 등의 기존 접근은 정렬 단계에서 다양한 각도로 블록을 놓아 후속 단계에 더 **넓은 초기 분포**를 허용하지만, 오히려 그 때문에 파지나 삽입 단계에서 **불필요한 어려움**이 생길 수 있습니다. **그림 3-(b)**는 이러한 차이를 잘 보여주는데, T-STAR 방식으로 최적화된 정책은 블록의 최종 자세 분포가 비교적 **넓게** 퍼져 있는 반면(Ours 이전의 참고용), Sequential Dexterity의 정책은 **한정된 범위**(거의 돌기가 위쪽을 향한 영역)에 몰려 있습니다. 이렇듯 **우리 방법이 목표 상태 공간을 효과적으로 제한**함으로써 **전체 연쇄의 성공 가능성을 높였음**을 알 수 있습니다.

**Baseline 대비 성능 개선**은 개별 사례 비교를 통해서도 드러납니다. 공구 자세 맞추기 과제의 경우, 기존 순차 실행 방식(baseline)은 첫 파지에서 망치를 옆으로 눕힌 채 잡는 경우가 많았고, 이 상태에서는 아무리 회전하려 해도 망치머리가 걸려 **자세 변경에 실패**하곤 했습니다 (위 그림 2-(b) 빨간 X 상황). 반면 Sequential Dexterity를 통해 학습된 정책은 첫 단계에서부터 망치를 세울 것을 염두에 두고 **손잡이 부분을 아래쪽으로 향하게 잡는 전략**을 취했습니다. 그 결과 두 번째 단계에서 **자연스럽게 망치를 똑바로 세울 수 있었고** 최종적으로 안정된 자세를 만들 수 있었습니다 (그림 2-(b) 녹색 체크 상황). 이처럼 **초기 단계에서의 적절한 결정이 후속 단계 성공으로 이어지는 사례**가 다수 관찰되었습니다.

**일반화 성능** 측면에서도 고무적인 결과가 보고되었습니다. 공구 과제에서 학습에 사용하지 않은 **주걱, 숟가락** 등의 도구에 대해서도 제안 정책은 큰 성능 저하 없이 작업을 수행해냈습니다. 예컨대 숟가락의 넓적한 부분을 잡을 때에도 학습된 망치 파지 정책을 응용하여 **비슷한 원리로 파지 및 세우기 동작**을 성공시켰습니다. 성공률 수치는 망치에 비해 다소 낮아질 수 있으나, baseline과 비교하면 여전히 **높은 성공 비율**을 유지하며 **우수한 일반화** 능력을 시현했습니다. 블록 조립의 경우 학습에는 몇 가지 크기와 색상의 블록만 사용되었지만, 테스트에서는 **처음 보는 형태의 블록**(예: 다른 모양의 조립 블록)을 투입해도 손쉽게 적응하는 모습을 보였습니다. 이는 제안한 정책이 물체의 정확한 CAD 모델에 의존하기보다는 **시각/촉각 피드백에 기반한 조작 스킬**을 학습했음을 시사합니다. 또한 블록의 초기 배치나 목표 구조의 형태가 달라져도, 주어진 **조립 설명서**만 바꾸면 (예: GUI로 원하는 구조를 지정) 로봇이 동일한 연쇄 정책으로 새로운 구조를 쌓아올렸습니다. 이러한 **태스크 적응력**은 본 기법의 **범용성**을 뒷받침하는 증거입니다.

끝으로, **실제 로봇 실험 결과** Sequential Dexterity의 실용 가능성을 보여주었습니다. 시뮬레이션에서 학습한 정책들을 그대로 이식한 로봇은, 카메라로 식별한 블록을 찾아 집어서 구조물을 쌓는 일련의 동작을 **성공적으로 수행**했습니다. 논문에 공개된 동영상을 보면 로봇이 여러 형태의 블록 구조 (예: 다층 탑, 특정 패턴 조립 등)를 사람의 도움 없이 완성하는 인상적인 시연이 포함되어 있습니다. 특히 **추가 학습이나 미세조정 없이** 시뮬레이션 정책을 바로 사용했다는 점에서, **시뮬레이션-현실 간 격차(sim2real gap)**를 최소화한 정책 학습의 효과를 실증했다고 볼 수 있습니다. 물론 완벽한 제로샷 전이를 위해 몇 가지 엔지니어링이 더해졌습니다. 예를 들어, 시뮬레이터에서는 손가락 움직임이 빠르게 튀는 **진동 현상**이 있었으나, 실제 로봇에서는 이를 **지수평활 필터**로 평탄화하여 제어의 안정성을 높였습니다. 또한 앞서 설명한 대로 마지막 삽입 동작은 약간의 스크립트 보조를 받았습니다. 그럼에도 불구하고, **학습된 부분(탐색~~정렬~~파지)**만으로도 로봇이 스스로 블록을 집어 정확한 자세로 위치시키는 건 기존에는 보기 드문 성과로, **모델 자유 강화학습**(model-free RL) 기법으로 이렇게 복잡한 실제 조립 작업을 성공한 **첫 사례 중 하나**로 평가됩니다.

## 한계점 및 향후 연구 방향

Sequential Dexterity는 긴 수평선(long-horizon)의 다단계 작업에 대한 새로운 접근법을 제시했지만, 여전히 몇 가지 **한계점**이 존재하며 향후 개선 여지가 있습니다.

첫째, **서브태스크의 분할과 정의가 사람에 의해 사전에 주어지는 점**입니다. 본 논문에서는 블록 탐색, 정렬, 파지, 삽입의 4단계와 공구 파지, 정렬의 2단계를 전제로 했습니다. 이러한 **서브태스크 분할**은 도메인 지식을 활용한 것으로, 복잡한 문제를 풀기 쉽게 만든 장점이 있지만 **범용 인공지능적 접근**과는 거리가 있습니다. 향후에는 로봇이 스스로 서브태스크를 발견하거나, 고수준 목표만 주어져도 내부적으로 적절한 정책 체인을 구성할 수 있는 **자동 계층화 학습**으로 확장될 수 있을 것입니다.

둘째, **학습 비용**의 문제입니다. 각 서브 정책을 강화학습으로 훈련하는 데 상당한 시뮬레이션 시간이 걸렸습니다. 논문에 따르면 블록 조립의 경우 서브태스크 하나를 학습하는 데 **하루\~이틀 정도** 소요되며, 모든 단계를 순차/역방향으로 최적화하는 데 상당한 계산자원이 필요했습니다. 이는 현재 기술 수준에서 불가피한 면도 있지만, 향후 더 효율적인 학습 알고리즘(PPO 외에 모델 기반 기법 등)이나 **지표함수 재사용** 등을 통해 훈련 시간을 단축할 수 있을 것입니다. 또한 현재는 시뮬레이터 상에서 대량의 병렬 환경을 사용했는데, **실제 로봇 학습**으로 이러한 기법을 확장하려면 표본 효율을 높이거나 인간 시연으로 초기 정책을 얻는 등의 추가 연구가 필요합니다.

셋째, **시뮬레이션과 현실의 차이**로 인한 한계가 있습니다. 논문에서도 지적했듯이 블록 삽입과 같은 **접촉이 많은 세밀한 동작**은 시뮬레이터에서 정확히 구현하기 어려웠고, 결국 일부를 스크립트에 의존했습니다. 이는 곧 **접촉력, 마찰, 변형 등의 물리적 상호작용을 정확히 다루는 어려움**을 뜻합니다. 향후 연구에서는 **전이 가능성 함수**에 이러한 **접촉 안정성**에 대한 평가를 포함시키거나, 시뮬레이션 단계에서부터 물리 파라미터를 다양화하는 **도메인ラン덤화(domain randomization)**를 통해 현실 적응력을 높일 수 있을 것입니다. 또한 **촉각 센서**나 **힘 제어**를 통합하여, 단순 시각 정보로 어려운 미세 접촉 동작의 성공률을 높이는 방향도 고려해볼 수 있습니다.

넷째, **센싱 및 시스템 복잡도**의 한계입니다. 본 연구는 시뮬레이션에서는 완전상태 관찰(ground-truth state)을 주로 사용하여 학습했고, 현실 실험에서는 이를 보완하기 위해 2대의 카메라와 세분화/추적 및 6D 포즈 추정 알고리즘(XMem, DenseFusion 등)을 사용했습니다. 이러한 **인지 시스템**은 추가적인 오류 가능성을 내포하며, 실제 적용 시 상당한 설정과 보정이 필요합니다. 향후에는 학습 단계에서부터 **영상 입력**을 받아 end-to-end로 정책을 학습하거나, 보다 경량의 센서 구성으로 동작할 수 있도록 단순화하는 연구가 필요합니다. 예를 들어, 손목 카메라 하나만으로도 목표 물체를 탐지하고 조작하도록 학습시키는 방향입니다. 이는 난이도가 높지만, 성공한다면 시스템 구성의 복잡도를 줄이고 실용성을 높일 수 있을 것입니다.

다섯째, **정책 연쇄의 범위**에 관한 한계입니다. Sequential Dexterity는 현재 **고정된 순서의 스킬 체인** 내에서만 동작합니다 (비록 스위칭으로 반복이나 생략은 가능하지만). 만약 작업 도중 전혀 새로운 하위 과제가 필요해지거나 분기(branch)가 발생하는 시나리오에서는 그대로 적용하기 어렵습니다. 예를 들어 조립 작업 중 블록이 떨어지거나 파손되는 등 **예외 상황**이 발생하면, 현재 체계로는 사전에 학습된 서브태스크로 대처할 수 없을 수 있습니다. 이러한 **비정형 상황**에 대응하려면 상위 계층의 **플래닝 알고리즘**이나 **온디맨드 학습**과 결합이 필요할 것입니다. 향후 연구는 체스와 같은 전략 게임에서 보듯이, 고수준 플래너가 서브 정책들을 호출하고 조합하는 **계층적 제어 구조**로 확장하는 방향을 제시할 수 있습니다. 또한 여러 대의 로봇 손이나 사람-로봇 협업 등의 **다중 에이전트** 상황에서도 이와 유사한 정책 체이닝 개념을 적용하려면 어떻게 할지 탐구해볼 수 있습니다.

마지막으로, **다지 로봇 손 자체의 한계**도 존재합니다. 사람 손과 유사한 로봇 핸드는 유연성이 높지만 제어가 어려워, 현재까지는 병렬 그리퍼 등에 비해 신뢰도가 낮은 편입니다. 논문에서도 평행그리퍼로는 수행이 어려운 작업이 본 과제였음을 강조하고 있으나, 반대로 말하면 다지 손이 반드시 필요하지 않은 작업에서는 체이닝 기법 없이도 성공할 수 있습니다. 그러므로 **어떤 유형의 작업에 다지 손 + 체이닝 접근이 유리한지**를 식별하고, 불필요한 경우에는 단순 그리퍼나 단일 정책으로도 충분한지를 구분하는 연구가 실용적 관점에서 중요합니다. 또한 다지 손의 고유한 문제인 **관절 제약, 내구성, 제어 지연** 등이 현실 적용에 걸림돌일 수 있어, 이러한 부분을 개선하거나 체이닝 기법이 그러한 하드웨어 한계를 완화할 수 있는지도 살펴봐야 합니다.

## 총평 및 기여도 평가

Sequential Dexterity 논문은 **로봇 강화학습 분야에서 장기 horizon 과업**을 풀기 위해 한 걸음 나아간 중요한 연구로 평가됩니다. 특히 **다관절 로봇 손**과 같이 제어가 까다로운 매니퓰레이터를 활용하여, 이전에는 사람이 개입하거나 실패율이 높았던 **다단계 조작 작업**을 **자율적으로 학습 수행**해낸 점에서 큰 의의가 있습니다. 이 논문의 주요 **기여도(contribution)**는 다음과 같이 정리할 수 있습니다:

* **다단계 섬세 조작을 위한 정책 체이닝의 개척**: 본 연구는 **장기 다단계 조작 작업에 정책 연쇄 기법을 적용한 최초의 사례** 중 하나로, 특히 다지 로봇 손을 이용한 정교한 작업에 초점을 맞추었습니다. 기존에는 주로 단순 그리퍼나 단일 단계 작업 위주로 연구되던 분야에, 복잡한 손 조작 시나리오를 제시하고 해결책을 모색했다는 점에서 학술적 가치를 지닙니다.

* **일반적인 양방향 최적화 프레임워크 제안**: 단순히 개별 실험 결과를 넘어서, **순방향 학습 + 전이 함수 기반 역방향 파인튜닝**이라는 일반화 가능한 프레임워크를 구축하였습니다. 이 프레임워크는 향후 다른 형태의 다단계 작업이나 다른 로봇 플랫폼에도 응용될 수 있는 설계로서, **정책 체이닝에 대한 보편적인 방법론**을 제시한 점이 높이 평가됩니다. 특히 value 함수가 아닌 별도의 **전이 가능성 함수**를 도입하고, 이를 **정책 전환 전략**까지 통합시킨 것은 본 논문의 기술적 창의성이라 할 수 있습니다.

* **다단계 조작에서의 SOTA 성능과 실환경 검증**: 제안한 방법을 통해 **기존 방법들이 실패하거나 성능이 낮았던 복합 작업을 성공적으로 해결**하였으며, 수치적으로도 높은 성공률을 입증했습니다. 예를 들어, 이전 강화학습 연구에서는 모델 프리 방식으로는 풀지 못했던 IKEA 가구 조립류의 작업을 본 연구와 유사한 접근으로 해결했고, 본 논문 역시 블록 조립 작업을 최초로 달성했습니다. 또한 **학습된 정책을 바로 실제 로봇에 이식하여 작업 성공**을 시연함으로써, 시뮬레이션 결과에 그치지 않고 현실 적용 가능성까지 보여주었습니다. 이는 로봇 학습 연구에서 상당히 까다로운 단계인데, 저자들의 통합적 노력(시뮬레이터 선정, 비전 모듈 개발, 제어 안정화 등)으로 이루어낸 성과입니다. 이러한 **sim-to-real 성공 사례**는 학계와 업계 모두에 영감을 주며, 향후 유사한 연구에 대한 **신뢰도**를 높이는 데 기여합니다.

이 논문을 **관련 문헌들과 비교**해보면, 이전의 **스킬 체이닝** 연구들은 주로 **상태 공간 확장**이나 **옵션 프레임워크** 등을 통해 정책 연결을 시도했는데, Sequential Dexterity는 **상태 분포의 폭발적 증가를 억제**하는 방향으로 접근한 점이 독특합니다. 특히 T-STAR(Lee et al., 2021) 등이 adversarial training으로 분포를 제한한 데 반해, 본 논문은 **학습된 전이 가능성 함수를 활용한 직접적인 보상 피드백**으로 문제를 해결하여 **안정적이면서도 효과적인 개선**을 이루었습니다. 또한 과거 연구들이 병렬 그리퍼 등 제한된 조작에서 단순 작업(예: 블록 쌓기 2\~3단계 정도)을 시연한 것에 비해, 본 연구는 **다지 손의 풍부한 조작 가능성**을 활용해 더 복잡한 작업을 했다는 점에서 **실용적 가치**가 있습니다. 사람의 손처럼 다양한 형태의 물체를 다뤄야 하는 과제에서 본 방법이 진가를 발휘할 수 있음을 보여준 셈입니다.

물론 Sequential Dexterity에도 앞서 논의한 개선점들이 있지만, **현재 시점 기준**으로 볼 때 이 논문은 **장기 강화학습 제어와 로봇 조작**의 교차점에서 상당한 진전을 이룬 것으로 평가됩니다. **학문적으로는**, 긴 시간범위의 의사결정 문제를 해결하기 위해 **계획(plan)**과 **학습(learning)**을 접목한 한 가지 성공 예를 제시했고, **실용적으로는** 향후 산업용 로봇이나 가정용 로봇의 복잡한 작업 학습에 적용될 수 있는 기술 방향을 제안했습니다. 특히, **전이 가능성 함수**라는 개념은 추후 다양한 **계층형 강화학습** 시나리오에서 활용될 수 있는 아이디어로, 다른 연구자들이 이를 변형·확장하여 새로운 알고리즘을 만들 가능성도 있습니다. 예를 들어, 사람의 시연을 활용한 정책 연쇄 학습이나, 다중 로봇 협업 과제에 전이 가능성 평가를 도입하는 방향 등이 떠오릅니다.

**결론적으로**, "Sequential Dexterity: Chaining Dexterous Policies for Long-Horizon Manipulation" 논문은 다단계 조작 문제에 도전한 혁신적인 연구로서, **정책 연쇄를 통한 장기 과업 해결**이라는 로봇학습 분야의 난제를 한 단계 전진시켰습니다. 이 연구의 성과는 향후 로봇이 인간 수준의 다양하고 긴 작업을 수행하기 위해 어떤 학습 프레임워크가 필요한지에 대한 **통찰**을 제공하며, 관련 분야 연구자 및 학생들에게 유용한 벤치마크와 아이디어의 **기준점**이 될 것입니다. 앞으로 이 방향의 연구가 더욱 발전하여, 로봇이 실제 세계에서 연속적인 복합 작업을 **안전하고 신뢰성 있게 수행**하는 날이 앞당겨지길 기대해봅니다.&#x20;
