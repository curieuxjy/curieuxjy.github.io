---
title: "📃AnyRotate 리뷰"
date: 2025-11-02
categories: [touch, in-hand, rotation]
toc: true
number-sections: False
description: Gravity-Invariant In-Hand Object Rotation with Sim-to-Real Touch
---

> 🔍 Ping. 🔔 Ring. ⛏️ Dig. A tiered review series: quick look, key ideas, deep dive.


- [Paper Link](https://arxiv.org/abs/2405.07391)
- [Project LInk](https://maxyang27896.github.io/anyrotate/)

1. ✋ 다자유도 로봇 손의 중력 불변 다축 물체 회전(gravity-invariant multi-axis in-hand object rotation)은 어려운 과제이며, 본 논문은 시뮬레이션-실제 환경 제로샷 전환이 가능한 고밀도 촉각 정보(dense featured sim-to-real touch)를 활용하는 AnyRotate 시스템을 제안합니다.
2. 🧠 이 시스템은 목표 조건 강화 학습과 접촉 자세 및 힘을 포함하는 고밀도 촉각 표현(dense tactile representation)을 사용하여, 중력 영향을 받지 않는 임의의 회전 축과 다양한 손 방향에서 동작하는 단일 정책을 성공적으로 학습했습니다.
3. 🚀 실제 실험에서 AnyRotate는 다양한 손 방향과 회전 축에서 미지의 객체에 대한 강력한 강건성(robustness)을 입증했으며, 풍부한 촉각 정보로 불안정한 파지(unstable grasp)를 감지하고 이를 복구하는 자율적 행동(emergent behavior)을 보여주었습니다.

<center>
<img src="../../images/2025-11-02-anyrotate/0.png" width="80%" />
</center>


---

# 🔍 Ping Review

> 🔍 Ping — A light tap on the surface. Get the gist in seconds.

본 논문 "AnyRotate: Gravity-Invariant In-Hand Object Rotation with Sim-to-Real Touch"는 로봇 손이 다양한 손의 움직임 속에서도 물체를 조작하는 데 있어 중력 불변(gravity-invariant)의 in-hand object rotation을 달성하기 위한 시스템 AnyRotate를 제안합니다. 기존 비전 기반 접근 방식은 self-occlusion과 같은 문제로 인해 in-hand manipulation에 한계가 있었으며, 촉각 센서(tactile sensor)는 상세한 접촉 정보를 제공할 수 있음에도 불구하고 sim-to-real gap으로 인해 고해상도 촉각 데이터가 충분히 활용되지 못했습니다. 본 연구는 이러한 문제를 해결하기 위해 dense featured sim-to-real touch를 활용하여 시뮬레이션에서 훈련된 policy를 실제 로봇에 zero-shot transfer하는 방법을 제시합니다.

**핵심 방법론:**

AnyRotate는 multi-axis gravity-invariant in-hand object rotation을 위해 다음과 같은 핵심 요소들을 통합합니다.

1.  **Goal-Conditioned Reinforcement Learning (RL) Formulation:**
    물체 회전 문제를 물체 재방향(object reorientation)으로 정형화하며, 회전 속도(angular velocity) 기반의 목표 설정이 비효율적임을 지적하고, 보조 목표(auxiliary goals) 방식을 도입합니다. 이 보조 목표는 현재 물체 자세(object orientation)를 원하는 회전 축(rotation axis)을 중심으로 일정한 간격으로 회전시켜 새로운 목표 자세를 생성합니다. Policy는 이러한 동적인 목표 자세를 추종하도록 훈련됩니다. 보상 함수 $r$는 물체 회전(rotation), 접촉(contact), 안정성(stability), 종료(termination)의 네 가지 요소로 구성됩니다.
    $$r = r_{\text{rotation}} + r_{\text{contact}} + r_{\text{stable}} + r_{\text{terminate}}$$
    $r_{\text{rotation}}$은 키포인트(keypoint) 거리 $K(||k_o^i - k_g^i||)$와 회전 보너스(goal bonus)를 통해 목표 자세 도달 및 연속적인 회전을 장려합니다. $r_{\text{contact}}$는 손끝(fingertip) 접촉을 최대화하고 다른 부위의 접촉을 패널티화하여 정밀한 파지(precision grasp)를 유도합니다. $r_{\text{stable}}$은 물체의 각속도(angular velocity), 손 자세(hand-pose), 제어기(controller)의 작업량 및 토크(torque)를 패널티화하여 안정적인 회전을 가능하게 합니다. 마지막으로 $r_{\text{terminate}}$는 물체가 파지에서 벗어나거나 회전 축이 목표에서 크게 벗어날 경우 패널티를 부여합니다. 학습 과정의 효율성을 위해, episode당 달성된 평균 회전 횟수에 따라 $r_{\text{contact}}$와 $r_{\text{stable}}$에 가중치 계수 $\lambda_{\text{rew}}$를 선형적으로 증가시키는 적응형 커리큘럼(adaptive curriculum)을 적용합니다.

2.  **Dense Tactile Representation:**
    물체와의 상호작용에 대한 풍부한 정보를 제공하기 위해, 국부적인 접촉 자세(local contact pose)와 접촉력(contact force)으로 구성된 dense tactile representation을 사용합니다. Contact pose는 극각(polar angle) $R_x$와 방위각(azimuthal angle) $R_y$로 정의되며, contact force는 3D 접촉력의 크기 $||F||$를 의미합니다. 시뮬레이션에서는 센서를 강체(rigid body)로 근사하여 접촉 정보를 직접 가져오고, 실제 센서의 지연과 노이즈를 시뮬레이션하기 위해 접촉력에 exponential moving average를 적용하고 값을 실제 센서의 범위에 맞춰 포화(saturate) 및 재조정(rescale)합니다.

3.  **Sim-to-Real Policy Transfer:**
    Policy 학습은 두 단계의 policy distillation 방식을 따릅니다.

    *   **Teacher Training:** 특권 정보(privileged information, 예: 물체 위치, 자세, 질량, 중력 벡터 등)가 제공된 시뮬레이션 환경에서 RL (Proximal Policy Optimization, PPO)을 통해 teacher policy를 훈련합니다.
    *   **Student Training:** 실제 환경에서 관측 가능한 정보(proprioception, 촉각 피드백)만을 사용하는 student policy를 훈련합니다. Student policy는 teacher policy와 동일한 actor-critic architecture를 가지며, 과거 $N$개의 관측 시퀀스(sequence of observations)로부터 TCN (Temporal Convolutional Network) encoder를 통해 저차원 잠재 벡터(latent vector) $z_t$를 예측합니다. Student는 teacher의 행동을 모방하도록 지도 학습(supervised learning) 방식으로 훈련되며, 이때 잠재 벡터와 행동 분포에 대한 MSE (Mean Squared Error) 및 NLL (Negative Log-Likelihood) 손실을 최소화합니다.
    *   **Tactile Perception Model:** Zero-shot sim-to-real transfer를 위해, 실제 촉각 이미지에서 dense contact feature를 추출하는 tactile perception model을 훈련합니다. 이 모델은 UR5 로봇 팔에 tactile sensor를 부착하고 force/torque sensor가 부착된 작업대 표면에서 센서를 무작위로 움직이며 접촉 깊이(contact depth), 접촉 자세, 접촉력을 레이블로 하여 데이터를 수집하여 CNN 기반으로 훈련됩니다. 실제 배포 시에는 SSIM (Structured Similarity Index)을 사용하여 이진 접촉(binary contact)을 계산하고, 이를 기반으로 contact pose 및 contact force 예측값을 마스킹(masking)합니다.

**실험 및 분석:**

실험은 Allegro Hand에 vision-based tactile sensor를 장착한 로봇 시스템에서 수행되었습니다.

*   **훈련 성능:** 보조 목표(auxiliary goal)를 사용하는 제안된 formulation이 angular rotation objective보다 multi-axis rotation 작업에서 훨씬 높은 정확도와 수렴 성공률을 보였습니다. 이는 특히 물체가 본질적으로 불안정한 configuration에서 작은 랜덤 액션이 회복 불가능한 상태로 이어질 수 있는 상황에서, 목표 지향적인 보상이 agent를 더 효과적으로 안내했음을 시사합니다. 적응형 커리큘럼 또한 학습에 긍정적인 영향을 주었습니다.
*   **시뮬레이션 결과:** Dense touch policy(contact pose 및 contact force)는 proprioception, binary touch, discrete touch 등 더 단순하고 덜 상세한 촉각 정보를 사용하는 policy들보다 우수한 성능을 보였습니다. 이는 상세한 촉각 정보가 다양한 mass 및 shape를 가진 unseen objects를 다루는 데 중요함을 입증합니다.
*   **실제 환경 결과:** Dense touch policy는 실제 환경에서 10가지 다양한 물체에 대해 성공적인 zero-shot transfer를 달성했으며, 다양한 hand orientation(손 방향) 및 rotation axis(회전 축)에서 가장 강력한 robustness를 보여주었습니다. 특히, 손이 중력에 대해 수평으로 위치할 때 성능 저하가 있었으나, dense touch policy는 이러한 noisy system에서도 안정적인 성능을 유지했습니다.
*   **Emergent Behavior:** Rich tactile sensing이 불안정한 파지(unstable grasp)를 감지하고, 물체 미끄러짐을 방지하는 반응적인 finger-gaiting 움직임을 유발하는 emergent behavior를 보여주었습니다. 이는 proprioception이나 binary touch만으로는 관찰되지 않았습니다.
*   **Gravity Invariance 및 Rotating Hand:** 훈련된 policy는 중력 벡터가 지속적으로 변하는 회전하는 손(rotating hand)에서도 효과적으로 물체 조작에 적응할 수 있음을 입증했습니다. 이는 물체의 6D 재방향과 동시에 파지 위치를 변경할 수 있는 새로운 수준의 dexterity를 제공합니다.

**결론 및 한계:**

본 연구는 rich tactile sensing을 활용하여 어떤 hand direction에서도 어떤 rotation axis로든 in-hand object rotation을 수행하는 일반적인 policy의 가능성을 보여주었습니다. 이는 multi-fingered robot hand의 촉각 dexterity 발전에 중요한 진전을 의미합니다. 하지만, 날카로운 기하학적 특징(sharp geometric features)을 가진 물체(예: 모서리)를 다루는 데 어려움이 있었으며, 이는 더 풍부한 촉각 표현(tactile representation)이나 시각 정보(visual information)를 통합하여 개선될 수 있습니다. 또한, Allegro Hand의 actuation이 특정 손 방향에서 약화되는 한계를 보여, 향후 low-cost면서도 더 강력한 하드웨어의 개발이 필요함을 시사합니다.

# 🔔 Ring Review

> 🔔 Ring — An idea that echoes. Grasp the core and its value.


> "거꾸로 들어도, 옆으로 들어도, 로봇 손이 물체를 자유자재로 회전시킨다"

안녕하세요! 오늘은 로봇 조작 분야에서 정말 흥미로운 논문을 소개해드리려고 합니다. Bristol 대학교의 Nathan Lepora 교수 연구팀이 발표한 **AnyRotate**라는 시스템인데요, 제목에서부터 야심차게도 "Gravity Invariant"(중력 불변)라는 단어를 사용하고 있습니다.

여러분, 손바닥을 위로 향한 채로 공을 돌리는 건 쉽죠? 그런데 손바닥을 아래로 뒤집은 채로 공을 떨어뜨리지 않고 계속 돌릴 수 있나요? 심지어 손을 옆으로 돌리거나 계속 움직이면서요? 사람도 어려운 이 동작을, 로봇이 해낸다는 겁니다. 그것도 **처음 보는 물체**로 말이죠!

---

## 1. 왜 이 연구가 중요할까?

### 로봇의 영원한 숙제: In-Hand Manipulation

로봇 팔이 물체를 집는 건 이제 어느 정도 해결됐습니다. 하지만 **손 안에서 물체를 자유롭게 조작하는 것**(in-hand manipulation)은 여전히 어려운 문제입니다. 우리는 손 안에서 연필을 돌리고, 동전을 손가락으로 굴리고, 루빅스 큐브를 맞추죠. 하지만 로봇에게 이런 일은 정말 어렵습니다.

**왜 어려울까요?**

1. **높은 자유도**: 사람 손은 27개의 뼈와 수많은 관절로 이루어져 있습니다. 로봇 손도 16개 이상의 관절을 제어해야 합니다.

2. **복잡한 물리**: 손가락과 물체 사이의 마찰, 미끄러짐, 접촉 등을 정확히 이해해야 합니다.

3. **부분 관측**: 손이 물체를 가리기 때문에 카메라로는 정확한 상태를 보기 어렵습니다.

4. **중력의 방해**: 손을 뒤집으면 물체가 떨어지려고 합니다.

### 기존 연구들의 한계

**OpenAI의 루빅스 큐브** (2019)

- 정말 인상적한 성과였지만...
- 엄청난 수의 카메라가 필요했습니다 (자기 폐색 문제)
- 손은 항상 위를 향했습니다
- 특정 물체(루빅스 큐브)에만 작동했습니다

**최근 촉각 기반 연구들**

- 이진 접촉(닿았다/안 닿았다)만 사용
- 손바닥 위 방향에서만 작동
- x, y, z축 중 하나의 축만 회전 가능
- 각 축마다 별도의 정책 필요

**이 논문이 해결하려는 것:**

- ✅ 풍부한 촉각 정보 활용
- ✅ 어떤 방향에서든 작동
- ✅ 임의의 축으로 회전
- ✅ 하나의 통합된 정책
- ✅ 다양한 미지의 물체

---

## 2. AnyRotate는 어떻게 작동할까?

### 시스템 구성: 로봇 손 + 촉각 센서

AnyRotate는 **Allegro Hand**라는 4개 손가락 로봇 손을 사용합니다. 그리고 핵심은 각 손가락 끝에 부착된 **DigiTac 촉각 센서**입니다.

**DigiTac 센서가 특별한 이유:**

이 센서는 사람의 지문처럼 작은 돌기들이 있는 부드러운 스킨으로 덮여 있습니다. 스킨 아래에는 작은 카메라가 있어서, 물체가 닿으면 돌기들이 움직이는 모습을 촬영합니다. 이를 통해:

- **접촉 위치**: 물체가 손가락의 어디에 닿았는지 (각도로 표현)
- **접촉 힘**: 얼마나 세게 누르는지 (힘의 크기)

이 두 가지 정보를 동시에 알 수 있습니다!

```
기존 방식: "물체가 닿았다" (1 bit 정보)
AnyRotate: "물체가 15도 각도로, 2.3N의 힘으로 닿았다" (연속적인 정보)
```

이 차이가 얼마나 중요한지는 뒤에서 실험 결과로 보여드리겠습니다.

### 학습 전략: 2단계 접근법

로봇을 학습시키는 과정이 정말 영리합니다. 마치 학생이 선생님께 배우는 것처럼 2단계로 나뉩니다.

#### 1단계: Teacher 정책 학습 (시뮬레이션)

**시뮬레이션의 장점:**

- 로봇을 실제로 수천 번 돌릴 필요 없음
- 물체를 떨어뜨려도 괜찮음
- 8,192개의 로봇을 동시에 학습시킬 수 있음!

**Teacher가 가진 특권:**

선생님(Teacher)은 물체의 정확한 위치, 방향, 무게, 중력 방향 등 모든 정보를 알고 있습니다. 이는 실제 세계에서는 불가능한 "치팅"이지만, 시뮬레이션에서는 가능하죠.

**강화학습으로 학습:**

Teacher는 PPO(Proximal Policy Optimization)라는 강화학습 알고리즘으로 학습됩니다. 보상 함수를 잘 설계하는 게 핵심인데, 이 논문의 보상 함수는 정말 정교합니다:

```
총 보상 = 회전 보상 + 목표 달성 보너스 + 접촉 보상
        - 나쁜 접촉 페널티 - 과도한 속도 페널티
        - 자세 페널티 - 에너지 소비 페널티
```

#### 2단계: Student 정책 증류 (실전 준비)

학생(Student)은 선생님이 하는 행동을 따라하면서 배웁니다. 하지만 중요한 차이가 있습니다:

**Student는 특권 정보가 없습니다!**
- 물체의 정확한 위치? 모름
- 목표 자세? 모름
- 오직 손가락의 관절 각도와 촉각 정보만 사용

**어떻게 가능할까?**
TCN(Temporal Convolutional Network)이라는 네트워크가 **과거 30 타임스텝의 관찰**을 받아서 압축된 표현(latent vector)으로 만듭니다. 이 표현에는 물체의 상태에 대한 암묵적인 정보가 담겨 있습니다.

```
[30개의 과거 관찰] → TCN → [8차원 잠재 벡터] → 정책 → [행동]
```

### 핵심 아이디어: 보조 목표(Auxiliary Goal)

이 부분이 정말 영리합니다! 기존 연구들은 "초당 30도씩 회전해!"라고 각속도를 직접 목표로 했습니다. 하지만 이건 학습이 정말 어렵습니다.

**AnyRotate의 접근:**
대신 "지금부터 1.5초 뒤에 이 자세가 되어야 해"라는 **중간 목표**를 계속 생성합니다. 목표에 도달하면 새로운 목표를 생성하고, 또 도달하면 또 생성하고... 이렇게 계속하면 자연스럽게 회전이 됩니다!

이게 왜 좋을까요?
- 각 목표는 달성 가능한 수준
- 연속적인 회전이 자연스럽게 발생
- 학습이 훨씬 안정적

**비유하자면:**
- 기존 방식: "100m를 10초에 뛰어!" (너무 어려워...)
- AnyRotate: "10m 앞으로 가! (달성) 또 10m 앞으로! (달성) 또 10m...!" (할 수 있어!)

### 적응형 커리큘럼: 단계적 학습

사람도 걷기 전에 기기를 배우듯이, 로봇도 단계적으로 배워야 합니다.

**초기 단계:**
- 보상: "일단 물체를 안정적으로 잡아!"
- 행동: 조심스럽게 물체를 파지
- 결과: 물체를 잘 잡지만 회전은 안 함

**중간 단계:**
- 보상의 가중치가 서서히 변화
- 회전 보상의 비중이 점점 증가
- 행동: 안정성을 유지하면서 조금씩 회전 시도

**후기 단계:**
- 보상: "이제 회전이 중요해!"
- 행동: 적극적으로 손가락을 움직여 회전
- 결과: 안정적이면서도 빠른 회전

이 과정은 자동으로 진행됩니다. 로봇이 평균적으로 얼마나 회전을 달성했는지에 따라 커리큘럼 계수 α가 0에서 1로 증가합니다.

---

## 3. 고밀도 촉각 표현: 게임 체인저

### 기존 촉각 표현들

**1. 이진 접촉 (Binary Touch)**
```
손가락 1: 접촉 O
손가락 2: 접촉 X
손가락 3: 접촉 O
손가락 4: 접촉 O
```
정보량: 4 bits

**2. 이산 촉각 (Discrete Touch)**
```
손가락 1: 영역 5번에서 접촉
손가락 2: 접촉 없음
손가락 3: 영역 12번에서 접촉
손가락 4: 영역 3번에서 접촉
```
정보량: 16 bits (16개 영역 중 하나)

### AnyRotate의 고밀도 촉각 (Dense Touch)

```
손가락 1:
  - 접촉 자세: θ=15.2°, φ=23.7°
  - 접촉 힘: 2.34 N
손가락 2: 접촉 없음
손가락 3:
  - 접촉 자세: θ=-8.1°, φ=45.3°
  - 접촉 힘: 1.87 N
...
```
정보량: **연속적인 실수 값들** (비교할 수 없을 만큼 풍부!)

### Sim-to-Real 전이: 시뮬레이션에서 현실로

여기서 큰 문제가 있습니다. 시뮬레이션에서는 접촉 정보를 직접 알 수 있지만, 실제 로봇은 촉각 **이미지**를 받습니다.

**해결책: 관찰 모델(Observation Model)**

1. **데이터 수집**: 실제 센서를 평평한 표면에 여러 각도와 힘으로 누르면서 3,000장의 이미지 수집

2. **CNN 학습**: 촉각 이미지 → (접촉 자세, 접촉 힘) 예측하도록 학습

3. **배포**: 학습된 모델로 실시간으로 촉각 특징 추출

```
[240×135 촉각 이미지]
    ↓ (CNN)
[θ, φ, Fx, Fy, Fz]
    ↓ (계산)
[접촉 자세, 접촉 힘]
```

놀라운 점은, 이렇게 학습한 정책을 **추가 학습 없이** 실제 로봇에 바로 적용할 수 있다는 겁니다! (제로샷 전이)

---

## 4. 실험 결과: 숫자가 말해주는 성능

### 시뮬레이션 실험: 촉각 정보의 중요성

첫 번째 실험은 "촉각 정보가 정말 중요한가?"를 확인합니다.

**테스트 환경:**
- 학습 때 보지 못한 물체들
- OOD Mass: 더 무거운 물체
- OOD Shape: 다른 형상의 물체

**결과 - OOD Mass (무거운 물체):**

| 관찰 유형 | 회전 수 | 시간(초) | 성능 |
|----------|---------|---------|------|
| 고유수용감각만 | 0.56 | 6.9 | ⭐ |
| 이진 촉각 | 1.03 | 11.1 | ⭐⭐ |
| 이산 촉각 | 1.26 | 13.1 | ⭐⭐⭐ |
| 고밀도 촉각 (힘 제외) | 1.55 | 15.4 | ⭐⭐⭐⭐ |
| 고밀도 촉각 (자세 제외) | 1.35 | 13.8 | ⭐⭐⭐ |
| **고밀도 촉각 (전체)** | **1.77** | **17.4** | ⭐⭐⭐⭐⭐ |

**결과 - OOD Shape (다른 형상):**

| 관찰 유형 | 회전 수 | 시간(초) |
|----------|---------|---------|
| 고유수용감각만 | 0.84 | 10.4 |
| 이진 촉각 | 1.35 | 14.9 |
| 이산 촉각 | 1.66 | 17.7 |
| **고밀도 촉각 (전체)** | **2.35** | **23.3** |

**분석:**

1. **촉각이 있으면 훨씬 좋다**: 고유수용감각만 쓰는 것보다 이진 촉각도 큰 도움

2. **상세할수록 좋다**: 촉각 정보가 상세할수록 성능이 계속 향상

3. **자세와 힘 모두 중요**: 둘 중 하나를 빼면 성능 하락
   - 힘 정보: 무거운 물체를 다룰 때 특히 중요
   - 자세 정보: 정밀한 손가락 움직임에 중요

4. **일반화 능력**: 학습 때 못 본 물체에도 잘 작동

### 보조 목표의 효과

"보조 목표가 정말 필요한가?" 테스트:

**비교 대상:**
- 제안 방법: 보조 목표 + 적응형 커리큘럼
- w/o 보조 목표: 각속도 직접 제어
- w/o 커리큘럼: 보조 목표는 있지만 고정된 보상

**결과:**

```
제안 방법:       ████████████████████ (20회 연속 목표 달성)
w/o 커리큘럼:    ██                    (물체만 잡고 회전 안 함)
w/o 보조 목표:   ████                  (단일 축만 가능, 다축 실패)
```

**결론:** 보조 목표와 적응형 커리큘럼 **둘 다 필수**입니다!

### 실제 로봇 실험: 진짜 세계에서의 도전

이제 진짜 로봇으로 실험합니다. 10가지 다양한 물체를 사용했는데, 플라스틱 과일부터 금속 실린더, 고무 장난감까지 정말 다양합니다.

#### 손 방향에 따른 성능

손을 여러 방향으로 돌려가며 테스트했습니다:

| 방향 | 설명 | 난이도 | 고밀도 촉각 성능 |
|------|------|--------|-----------------|
| Palm Up | 손바닥 위 | ⭐ 쉬움 | 6.2회/24.7초 |
| Palm Down | 손바닥 아래 | ⭐⭐ 중간 | 2.8회/18.3초 |
| Base Up | 손목 위 | ⭐⭐ 중간 | 3.5회/20.7초 |
| Base Down | 손목 아래 | ⭐⭐⭐ 어려움 | 2.0회/16.3초 |
| Thumb Up | 엄지 위 | ⭐⭐⭐⭐ 매우 어려움 | 1.5회/14.7초 |
| Thumb Down | 엄지 아래 | ⭐⭐⭐⭐⭐ 극악 | 1.2회/13.3초 |

**왜 이렇게 차이가 날까?**

- **Palm Up/Down**: 중력이 파지를 도와줌 (또는 적당히 방해)
- **Thumb Up/Down**: 손가락이 수평이라 중력이 구동력에 정면으로 반대
  - 손가락이 물체 무게를 온전히 버텨야 함
  - Allegro Hand의 구동력이 이 방향에서 약해짐

**놀라운 점:**
어려운 방향에서도 고밀도 촉각을 사용한 정책은 작동합니다! 고유수용감각이나 이진 촉각으로는 거의 불가능한 수준입니다.

#### 회전축에 따른 성능

x, y, z 세 축으로 회전을 시도했습니다:

| 회전축 | 특징 | 고밀도 촉각 성능 |
|-------|------|-----------------|
| z축 | 물체의 주축, 가장 자연스러움 | 4.2회/22.3초 |
| x축 | 손가락 2개는 고정, 2개는 회전 | 5.5회/25.3초 |
| y축 | 가장 복잡한 손가락 협응 필요 | 3.8회/21.7초 |

**흥미로운 발견:**

y축과 x축 회전은 **정교한 손가락 게이팅**이 필요합니다:
- 중지와 엄지(또는 검지와 새끼)가 물체를 고정
- 나머지 두 손가락이 회전력 제공
- 이런 복잡한 협응은 촉각 정보 없이는 거의 불가능

이진 촉각도 z축에서는 고유수용감각과 비슷한 성능이지만, x축과 y축에서는 확실히 더 좋습니다.

### 촉각 센서 분석: 로봇이 '느끼는' 것

실험 중 촉각 센서가 측정하는 값들을 분석했습니다. 그래프를 보면 두 가지 핵심 패턴이 보입니다:

**1. 미끄럼 감지**
```
시간 0초: 접촉 힘 = 2.5N, 접촉 자세 = 15°
시간 1초: 접촉 힘 = 2.3N, 접촉 자세 = 15° ← 안정
시간 2초: 접촉 힘 = 1.8N, 접촉 자세 = 18° ← 미끄러짐!
```

힘이 줄고 각도가 변하면 물체가 미끄러지는 중입니다.

**2. 반응적 게이팅**
미끄럼을 감지하면 정책이 즉시 반응합니다:
- 미끄러지는 손가락: 힘을 증가
- 다른 손가락들: 보상 동작 수행
- 결과: 물체를 다시 안정적으로 잡음

**놀라운 점:**
명시적인 "미끄럼 감지 모듈"이 없습니다! 고밀도 촉각 정보만으로도 정책이 **암묵적으로** 미끄럼을 감지하고 대응하는 법을 학습했습니다. 이는 이진 촉각이나 고유수용감각으로는 관찰되지 않은 창발적 행동입니다.

### 궁극의 테스트: 회전하는 손

가장 인상적인 실험입니다. 손 자체를 계속 회전시키면서 물체를 조작합니다!

**상상해보세요:**
- 손이 앞뒤로 흔들리면서
- 동시에 좌우로 회전하면서
- 그 와중에 손안의 공을 계속 돌림

사람도 어려운 이 동작을, 로봇이 해냅니다. 중력 방향이 손의 좌표계에서 계속 변하는데도 말이죠!

**세 가지 궤적 테스트:**

1. **단순 회전**: 손이 한 축으로 계속 회전 → 성공!

2. **복잡한 3D 궤적**: 손이 여러 축으로 복합 회전 → 성공!

3. **서보잉**: 손은 계속 움직이지만 물체는 공중에 거의 정지
   - 마치 저글링처럼 손이 물체 주위를 이동
   - 6DoF 재배향 + 파지 위치 재배치 동시 수행
   - 픽앤플레이스 같은 작업에 유용할 듯!

---

## 5. 기술적 깊이 파고들기

여기서는 좀 더 기술적인 세부사항을 살펴보겠습니다. 엔지니어링 측면에서 정말 잘 설계되었거든요.

### 보상 함수 해부

보상 함수는 로봇이 "무엇을 배워야 하는지" 정의합니다. AnyRotate의 보상 함수는 10개의 항으로 구성됩니다:

#### 긍정적 보상 (로봇이 이렇게 하면 좋아!)

**1. 회전 보상 (r_rot)**
```python
r_rot = exp(-β * d_keypoint)
```
- 물체의 키포인트가 목표에 가까울수록 높은 보상
- 키포인트: 물체에서 5cm 떨어진 6개 점
- β = 2.0 (가중치)

**2. 목표 달성 보너스 (r_bonus)**
```python
r_bonus = 10.0  # 목표 도달 시
```
- 키포인트 거리가 임계값(0.15) 미만이면 발동
- 희소 보상으로 중요한 이벤트 강조

**3. 델타 회전 보상 (r_delta)**
```python
r_delta = Δθ / Δt  # 실제 회전량
```
- 목표 축에 대한 실제 각도 변화
- 연속적인 회전 장려

**4. 좋은 접촉 보상 (r_contact)**
```python
r_contact = 1.0 if n_tip_contacts >= 2 else 0
```
- 손가락 끝 접촉이 2개 이상이면 보상
- 안정적인 파지 유도

#### 부정적 보상 (로봇이 이러면 안 돼!)

**5. 나쁜 접촉 페널티 (p_bad_contact)**
- 손바닥이나 손가락 옆면 접촉 시 페널티
- 손가락 끝만 사용하도록 유도

**6. 각속도 페널티 (p_ang_vel)**
```python
p_ang_vel = -1.0 if |ω| > 30 rad/s else 0
```
- 너무 빠른 회전 방지
- 제어 가능한 속도 유지

**7. 자세 페널티 (p_pose)**
- 표준 파지 자세에서 너무 멀어지면 페널티
- 극단적인 관절 각도 방지

**8. 일 페널티 (p_work)**
- 에너지 소비 최소화
- 효율적인 움직임 유도

**9. 토크 페널티 (p_torque)**
- 높은 토크 사용 억제
- 하드웨어 부담 감소

**10. 종료 페널티 (p_term)**
```python
p_term = -100  # 물체 낙하 또는 축 이탈 시
```
- 실패에 대한 강력한 불이익
- 안정성 최우선

**최종 보상 함수:**
```
r_total = r_rot + r_bonus + r_delta + r_contact
        - p_bad_contact - p_ang_vel - p_pose
        - p_work - p_torque - p_term
```

각 항의 가중치는 실험을 통해 세심하게 조정되었습니다.

### 네트워크 아키텍처 상세

#### Teacher Policy

```
[특권 정보 18차원]
    ↓
MLP Encoder [256 → 128 → 8]
    ↓
[잠재 벡터 8차원] + [고유수용+촉각 정보]
    ↓
Policy Network [512 → 256 → 128]
    ↓
[평균 μ, 표준편차 σ] (Gaussian policy)
    ↓
샘플링 → [행동 16차원]
```

**활성화 함수:**
- MLP: ReLU
- Policy: ELU (Exponential Linear Unit)
  - 음수 영역에서도 부드러운 그래디언트
  - 죽은 뉴런(dead neuron) 문제 해결

#### Student Policy

```
[30 타임스텝 × 관찰 차원]
    ↓
TCN Layer 1: Conv1D (kernel=9, stride=2) + ReLU
    ↓
TCN Layer 2: Conv1D (kernel=5, stride=1) + ReLU
    ↓
TCN Layer 3: Conv1D (kernel=5, stride=1) + ReLU
    ↓
[잠재 벡터 8차원]
    ↓
Policy Network [512 → 256 → 128] (Teacher와 동일)
    ↓
[행동]
```

**TCN의 장점:**
- 시간적 패턴 포착 (과거 30 프레임 = 1.5초)
- RNN보다 병렬화 효율적
- 긴 시퀀스에서도 안정적

**관찰 차원:**
- 고유수용감각만: 79차원
- 이진 촉각: 83차원 (+4)
- 고밀도 촉각: 91차원 (+12)

### 시스템 식별: 시뮬레이션을 현실과 맞추기

시뮬레이션이 아무리 좋아도 현실과 차이가 있습니다. 이를 줄이기 위해 **시스템 식별**을 수행합니다.

**최적화 대상:**
- 16개 관절 × 5개 파라미터 = **80개 파라미터**
  - 강성 (stiffness)
  - 감쇠 (damping)
  - 질량 (mass)
  - 마찰 (friction)
  - armature

**방법:**
1. 실제 로봇으로 여러 방향에서 궤적 기록
2. 같은 명령을 시뮬레이션에서 실행
3. CMA-ES 알고리즘으로 MSE 최소화
4. 최적 파라미터 발견

**결과:**
시뮬레이션과 실제 로봇의 움직임이 훨씬 유사해져서 sim-to-real 격차 감소!

### 도메인 무작위화: 다양성이 답이다

시뮬레이션 학습 중에 파라미터를 계속 변경합니다:

**물체 무작위화:**
```python
mass = random.uniform(0.025, 0.20)  # kg
friction = 10.0  # 고정
radius = random.uniform(0.025, 0.034)  # m
center_of_mass = random.uniform(-0.01, 0.01)  # m
```

**손 무작위화:**
```python
stiffness = random.uniform(35, 45)
damping = random.uniform(0.5, 2.5)
joint_noise = gaussian(0, 0.03)
position_noise = gaussian(0, 0.005)  # m
```

**촉각 무작위화:**
```python
pose_noise = gaussian(0, 0.0174)  # rad (약 1도)
force_noise = gaussian(0, 0.1)  # N
```

**외란:**
- 25% 확률로 랜덤 힘 적용
- 외란 스케일: 2.0
- 지수 감쇠: 0.99

이렇게 다양한 조건에서 학습하면 실제 세계의 예측 불가능한 상황에 강건해집니다!

### 실시간 제어 파이프라인

실제 로봇에서의 제어는 20Hz로 작동합니다:

```
┌─── 20 Hz 메인 루프 ───────────────────────┐
│                                           │
│  1. 촉각 이미지 수집 (비동기, 최대 30 FPS) │
│     ↓                                     │
│  2. 그레이스케일 변환 + 전처리             │
│     ↓                                     │
│  3. CNN으로 특징 추출 (4개 센서 병렬)      │
│     - 접촉 자세 (θ, φ)                    │
│     - 접촉 힘 (|F|)                       │
│     ↓                                     │
│  4. 관절 위치 읽기                         │
│     ↓                                     │
│  5. Forward Kinematics                    │
│     - 손가락 끝 위치                       │
│     - 손가락 끝 방향                       │
│     ↓                                     │
│  6. 관찰 벡터 구성                         │
│     ↓                                     │
│  7. 정책 추론                              │
│     - TCN으로 잠재 벡터 계산               │
│     - Policy로 행동 출력                  │
│     ↓                                     │
│  8. 지수 이동 평균                         │
│     q_target = 0.9*q_old + 0.1*q_new      │
│     ↓                                     │
│  9. 목표 관절 위치 전송                    │
│     ↓                                     │
│  ──→ PD 제어기 (300 Hz) ──→ 모터          │
│                                           │
└───────────────────────────────────────────┘
```

**병목 지점:**
- CNN 추론: ~5ms (GPU 사용)
- 정책 추론: ~3ms
- 나머지: ~42ms (여유 있음)

**최적화:**
- 촉각 센서들이 비동기로 스트리밍 (대기 시간 감소)
- 가장 최근 이미지 사용 (지연 최소화)
- SSIM으로 빠른 접촉 감지

---

## 6. 비교 분석: 기존 연구와 어떻게 다른가?

### OpenAI의 "Solving Rubik's Cube with a Robot Hand" (2019)와 비교

2019년 OpenAI가 발표한 "Solving Rubik's Cube with a Robot Hand" (Akkaya et al., 2019)와 후속 연구 "Learning Dexterous In-Hand Manipulation" (Andrychowicz et al., 2020)은 로봇 조작 분야에 큰 반향을 일으켰습니다.

**OpenAI가 보여준 것:**

이 프로젝트는 Shadow Hand라는 24자유도 로봇 손으로 루빅스 큐브를 푸는 놀라운 성과를 달성했습니다:

- **하드웨어**: Shadow Dexterous Hand (24 DoF, 20개 구동 관절)
- **센싱**: RGB 카메라 3개 (손을 둘러싼 위치에 배치)
- **학습 방법**:
  - ADR (Automatic Domain Randomization) - 자동으로 시뮬레이션 난이도 증가
  - Vision-based state estimation
  - 대규모 분산 학습 (6,144 CPU 코어 + 8 V100 GPU)
- **성과**: 루빅스 큐브를 60% 성공률로 해결 (심지어 고무 장갑 착용 상태에서도)

**하지만 한계가 명확했습니다:**

1. **과도한 비전 의존성**
   - 16-30개의 카메라 포인트 필요
   - 자기 폐색(self-occlusion) 문제 해결을 위한 복잡한 설정
   - 실험실 환경 밖에서는 적용 어려움

2. **고정된 손 방향**
   - 손은 항상 팔 위 (palm up)
   - 중력이 파지를 도와주는 상황
   - 다른 방향에서의 조작은 시도되지 않음

3. **작업 특화**
   - 루빅스 큐브라는 특정 물체에 최적화
   - 다른 물체로 일반화 검증 안 됨
   - 큐브의 특수한 기하학적 특성 활용

4. **막대한 컴퓨팅 자원**
   - 총 학습 시간: 100년 상당의 시뮬레이션
   - 13,000시간의 실제 시간
   - 일반 연구실에서 재현 어려움

**AnyRotate의 근본적 차이점:**

| 측면 | OpenAI (2019-2020) | AnyRotate (2024) |
|------|-------------------|------------------|
| **주요 센싱** | 비전 (다중 카메라) | 촉각 (4개 센서) |
| **자기 폐색** | 문제가 됨 | 문제 없음 |
| **손 방향** | 고정 (palm up) | 임의 (6+ 방향) |
| **중력 대응** | 중력이 도움 | 중력에 불변 |
| **회전축** | 작업 특화 | 임의 축 통합 정책 |
| **물체 일반화** | 루빅스 큐브 | 10+ 다양한 물체 |
| **설치 복잡도** | 매우 높음 | 상대적으로 낮음 |
| **컴퓨팅 자원** | 6,144 CPU + 8 GPU | GPU 시뮬레이터 (표준) |
| **배포 환경** | 실험실 전용 | 실용적 배포 가능 |

**철학적 차이:**

OpenAI의 접근은 "특정 작업에서 초인적 성능"을 목표로 했습니다. 루빅스 큐브를 푸는 것은 인상적이지만, 그 시스템을 다른 작업에 적용하기는 어렵습니다.

반면 AnyRotate는 "일반화 가능한 조작 능력"을 추구합니다. 처음 보는 물체도 다룰 수 있고, 어떤 손 방향에서도 작동하며, 복잡한 외부 센서 없이도 가능합니다.

**둘 중 무엇이 더 나은가?**

사실 이것은 "무엇이 더 나은가?"보다는 "무엇을 목표로 하는가?"의 문제입니다:

- **OpenAI**: "이것 봐! 로봇이 루빅스 큐브를 풀 수 있어!" → 기술의 한계를 보여주는 데모
- **AnyRotate**: "로봇이 다양한 물체를 실용적으로 다룰 수 있어" → 실제 응용을 향한 발걸음

두 접근 모두 중요하고, 서로 보완적입니다.

### 최근 촉각 기반 연구들과 비교

**Qi et al. (2023) - "General In-Hand Object Rotation with Vision and Touch" (RotateIt)**

UC Berkeley의 Jitendra Malik 교수 연구팀과 Meta AI가 공동으로 개발한 시스템으로, CoRL 2023에 발표되었습니다.

**연구 배경과 동기:**

Haozhi Qi를 비롯한 연구팀은 "시각과 촉각을 함께 사용하면 더 나을까?"라는 질문에서 출발했습니다. 인간도 눈과 손을 함께 사용하니까요. 그들의 답은 RotateIt 시스템이었습니다.

**시스템 구성:**

1. **하드웨어**
   - Allegro Hand (AnyRotate와 동일한 4-finger hand)
   - DIGIT 촉각 센서 (Meta에서 개발, 각 손가락에 부착)
   - 외부 RGB-D 카메라 (손 위에 설치)

2. **센싱 방식**
   - **비전**: 외부 카메라로 물체의 전체 형상과 깊이 정보 관찰
   - **촉각**: DIGIT 센서로 고해상도 접촉 정보 수집
     - 촉각 이미지를 16개 이산 영역으로 표현
     - 주로 접촉 위치에 집중
   - **고유수용감각**: 관절 각도와 속도

3. **핵심 기술: Visuotactile Transformer**

   이 부분이 정말 영리합니다:
   ```
   [RGB-D 이미지] → Vision Encoder (ResNet)
         ↓
   [Transformer Fusion]  ← [4개 DIGIT 이미지] → Tactile Encoder
         ↓
   [융합된 표현] → 물체 형상 + 물리 속성 추론
         ↓
   [정책 네트워크] → 회전 행동 출력
   ```

   Transformer가 cross-attention으로 시각과 촉각을 융합합니다:
   - 카메라가 물체의 전체 형상을 보면
   - 촉각이 접촉 지점의 세밀한 정보를 제공하고
   - "아, 이 물체는 이런 모양이고 이 무게라서 이렇게 잡아야겠다"

**학습 과정:**

AnyRotate와 유사하게 Teacher-Student 구조 사용:
- **Oracle Teacher**: GT 형상과 물리 속성 알고 있음
- **Visuotactile Student**: 노이즈가 있는 시각-촉각만으로 작동
- **특별한 점**: latent space에서 물체의 3D 형상을 재구성 가능!

**성능:**

시뮬레이션:
- X, Y, Z축 각각 80% 이상 성공률
- 비전만 vs 촉각만 vs 둘 다 → 둘 다가 최고

실제 로봇:
- 다양한 일상 물체(머그컵, 망치, 플라스틱 병 등)
- 처음 보는 물체에도 잘 일반화

**AnyRotate와의 상세 비교:**

| 차원 | RotateIt (Qi et al. 2023) | AnyRotate (2024) |
|------|--------------------------|------------------|
| **촉각 표현** | 16개 이산 영역 + 원본 이미지 | 연속적 자세(θ,φ) + 힘(\|F\|) |
| **정보 밀도** | 중간 (이산화로 일부 손실) | 높음 (연속 실수 값) |
| **비전 필요성** | 필수 (외부 카메라) | 불필요 (순수 촉각) |
| **설치 복잡도** | 높음 (카메라 캘리브레이션) | 낮음 (센서만) |
| **중력 불변성** | X (손바닥 위 고정) | O (6+ 방향) |
| **회전축 제어** | 다축 가능 (개별 학습?) | 통합 정책으로 임의 축 |
| **미끄럼 대응** | 명시적 모듈 필요 | 암묵적 학습 (창발) |
| **물체 이해** | 3D 형상 재구성 O | 직접 재구성 X |
| **센서** | DIGIT (Meta) | DigiTac (Bristol) |
| **주요 기여** | 멀티모달 융합 | 중력 불변 + 고밀도 촉각 |

**RotateIt의 강점:**

1. **형상 이해**:
   - RGB-D로 물체 전체 형상 파악
   - 특히 처음 보는 복잡한 물체에 유리
   - Learned representation으로 3D shape 재구성 가능

2. **Transformer 융합**:
   - Attention으로 시각-촉각 상호보완
   - 중요한 영역에 자동으로 집중

3. **멀티모달 프레임워크**:
   - 향후 연구를 위한 좋은 방법론
   - 다른 센서 추가도 가능한 확장성

**RotateIt의 한계:**

1. **외부 센서 의존**:
   - 카메라 설치와 캘리브레이션 필수
   - 실험실 밖 적용 어려움
   - 조명 변화에 민감

2. **고정된 손 방향**:
   - 여전히 palm up만
   - 중력 불변성 미달성

3. **일부 자기 폐색**:
   - 손이 물체를 가림
   - 완전한 해결은 아님

**결론:**

RotateIt는 "멀티모달"이 답이라고 주장합니다. 시각+촉각 > 각각의 합.

AnyRotate는 "고밀도 촉각만으로도 충분"하다고 보여줍니다. 그것도 임의 방향에서!

실용적 선택:
- 카메라 사용 가능 → RotateIt (더 많은 정보)
- 카메라 어려움 → AnyRotate (더 강건하고 단순)

---

**Khandate et al. (2022-2023) - 손가락 게이팅 학습 시리즈**

Columbia University의 Matei Ciocarlie 교수 연구실에서 진행한 연구입니다. Gagan Khandate 박사과정 학생이 주도했습니다.

**연구의 핵심 주제: 손가락 게이팅(Finger Gaiting)**

손가락 게이팅이란, 물체를 잡고 있는 손가락을 바꿔가며 조작하는 기술입니다. 마치 암벽등반처럼 손과 발을 번갈아 움직이는 것과 비슷하죠.

예를 들어:
1. 검지+중지로 물체 고정
2. 엄지+약지가 움직여서 물체 회전
3. 이제 엄지+약지로 고정
4. 검지+중지가 움직여서 더 회전
5. 반복...

---

**2022년 연구: "On the Feasibility of Learning Finger-gaiting In-hand Manipulation with Intrinsic Sensing" (ICRA 2022)**

"정말 강화학습만으로 이런 복잡한 기술을 배울 수 있을까?"

**시스템 구성:**

- **로봇 손**: 커스텀 제작 5-finger hand
  - 각 손가락: 1 roll joint + 2 flexion joints = 3 DoF
  - 총 15 DoF (5 fingers × 3 joints)
  - 모두 Dynamixel XM430 서보모터로 구동
  - 위치 제어 모드

- **센싱**:
  - 각 손가락 끝에 광학 기반 촉각 센서
  - 유연한 반사막 + LED + 포토다이오드 배열
  - 막이 눌리면 빛 반사 패턴 변화
  - **출력**: 이진 신호 (닿음 1 / 안 닿음 0)
  - 고유수용감각 (관절 각도, 속도)

**핵심 도전: 탐색 문제**

손가락 게이팅의 가장 큰 어려움:
```
문제: 무작위 행동으로는 거의 실패
- 손가락을 조금만 잘못 움직여도 → 물체 낙하
- 성공적인 게이팅은 극히 드물게 발생
- 학습 신호가 너무 희소 (sparse reward)
```

**해결책: Initial State Distribution**

그들의 영리한 아이디어:
```
1. 안정적인 파지 자세들을 사전 생성
   - 다양한 관절 각도 샘플링
   - 물리 시뮬레이션으로 안정성 검증
   - 안정한 자세만 저장

2. 학습 시 이 자세들을 초기 상태로 사용
   - 매 에피소드 시작을 안정한 자세에서
   - 거기서부터 게이팅 연습

3. 효과
   - 떨어뜨릴 확률 ↓↓
   - 유용한 경험 수집 ↑↑
   - 학습 속도 5-10배 향상
```

**학습 설정:**

- 알고리즘: PPO (Proximal Policy Optimization)
- 목표: Z축 회전 (수직축)
- 보상: 각속도 + 안정성 페널티
- 시뮬레이터: PyBullet

**성과:**

- Z축 손가락 게이팅 학습 성공!
- Palm up과 palm down 두 방향 작동
- 기존 방법보다 5-10배 빠른 학습
- 단순한 볼록 물체 (구, 실린더)

**한계:**

- 단일 축(Z축)만
- 시뮬레이션에서만 검증
- 실제 로봇 전이 안 됨

---

**2023년 연구: "Sampling-based Exploration for Reinforcement Learning of Dexterous Manipulation" (RSS 2023)**

"더 어려운 물체는 어떻게 다룰까? L자나 U자 같은?"

**새로운 도전:**

볼록한 공은 어디를 잡아도 비슷합니다. 하지만:

- **L자 물체**: 한쪽이 무거우면 균형 깨짐
- **U자 물체**: 잘못 잡으면 빠져나감
- **긴 막대기**: 회전 관성 높음

이런 물체들은 **좁은 통로(narrow passage)** 문제를 만듭니다:
```
상태 공간:
[안정 영역 A] ---- 좁은 통로 ---- [안정 영역 B]

문제: 무작위 탐색으로는 좁은 통로를 통과하기 극히 어려움
```

**해결책: Sampling-Based Planning + RL 하이브리드**

두 세계의 장점을 결합:

**1. Sampling-Based Planning (SBP):**
```python
# RRT* 같은 알고리즘 사용
def plan_path(start_pose, goal_pose):
    tree = initialize_tree(start_pose)

    for i in range(iterations):
        random_pose = sample_random_pose()
        nearest = find_nearest_in_tree(random_pose)
        new_pose = extend_towards(nearest, random_pose)

        if physics_check(new_pose):  # 떨어지지 않는지 확인
            tree.add(new_pose)

        if close_to_goal(new_pose, goal_pose):
            return extract_path(tree, new_pose)
```

이렇게 찾은 경로는:
- 물리적으로 실현 가능
- 좁은 통로도 통과
- 하지만 실시간은 아님 (계획에 시간 소요)

**2. Reinforcement Learning:**
```python
# SBP가 찾은 경로를 활용
def train_policy():
    # 1. SBP로 좋은 경로들 수집
    paths = sampling_based_planner.get_paths()

    # 2. 경로의 state들을 초기 상태로 활용
    for episode in training:
        init_state = sample_from(paths)
        # 여기서부터 RL 학습

    # 3. 결과: 경로를 넘어서 일반화된 정책
```

**하이브리드 접근의 장점:**

| 방법 | 장점 | 단점 | 하이브리드 |
|------|------|------|-----------|
| 순수 RL | 일반화 좋음 | 탐색 어려움 | ✓ 일반화 |
| 순수 Planning | 특정 목표 달성 | 실시간 어려움 | ✓ 효율 |
| | | 일반화 제한 | ✓ 실시간 |

**실험 물체:**

- **Easy**: 구, 실린더 (기본선)
- **Moderate**: 타원체, 두꺼운 막대
- **Hard**: L자, U자, 얇고 긴 막대

**성과:**

시뮬레이션:
- 모든 난이도의 물체에서 게이팅 성공
- Hard 물체도 80% 이상 성공률
- 순수 RL보다 3-5배 빠른 수렴

실제 로봇:
- 2023년 논문에서 드디어 실제 검증!
- L자, U자 물체 실제로 회전 성공
- IEEE Spectrum에 "Robot Hand Manipulates Complex Objects by Touch Alone" 기사화

**실제 로봇 하드웨어:**

"Highly Dexterous Robot Hand Can Operate in the Dark"
- 5개 손가락, 15 DoF
- 각 손가락 끝에 광학 촉각 센서
- 완전 암흑에서도 작동 (순수 촉각)
- 복잡한 형상 다룸

**AnyRotate와의 상세 비교:**

| 측면 | Khandate et al. (2022-2023) | AnyRotate (2024) |
|------|---------------------------|------------------|
| **촉각 해상도** | 이진 (1 bit/finger) | 연속 (θ,φ,F/finger) |
| **정보량** | 5 bits (5 fingers) | ~15 연속 변수 |
| **회전 자유도** | 주로 Z축 | 임의 축 통합 정책 |
| **손 방향** | Palm up/down (2가지) | 6+ 방향 자유롭게 |
| **중력 도전** | 부분적 해결 | 완전 해결 |
| **학습 접근** | SBP + RL 하이브리드 | 목표조건 RL + 커리큘럼 |
| **탐색 전략** | Planning으로 경로 제공 | Adaptive curriculum |
| **물체 복잡도** | 볼록 → 복잡 (L, U) | 다양한 일상 물체 |
| **실제 검증** | 2023년 일부 | 광범위 (10개 물체) |
| **주요 기여** | Planning-RL 결합 | 중력 불변 + 고밀도 촉각 |

**Khandate 연구의 강점:**

1. **이론적 체계성**:
   - Narrow passage 문제를 원리적으로 해결
   - SBP와 RL 결합의 이유를 명확히 설명
   - 로보틱스 커뮤니티에 방법론 제시

2. **복잡한 형상**:
   - L자, U자 등 기존 방법이 실패한 물체
   - 형상 복잡도의 한계를 넓힘
   - "이런 것도 학습으로 가능하다"

3. **점진적 발전**:
   - 2022: 기본 개념 검증 (시뮬레이션)
   - 2023: 어려운 물체 + 실제 로봇
   - 체계적이고 탄탄한 연구 진행

**Khandate 연구의 한계:**

1. **이진 촉각의 한계**:
   - "닿았다/안 닿았다"만 알 수 있음
   - 얼마나 세게? → 모름
   - 어느 각도로? → 모름
   - 미세한 미끄럼 감지 불가능

2. **제한된 중력 대응**:
   - Palm up/down은 가능
   - 하지만 Thumb up/down 같은 극한 자세는 여전히 어려움
   - 중력이 어느 정도 도와주는 상황

3. **회전축 제한**:
   - 주로 Z축 (수직축) 중심
   - X, Y축 회전은 명시적으로 다루지 않음
   - 임의 축은 미해결

4. **실제 배포 검증**:
   - 2022년은 시뮬레이션만
   - 2023년에 일부 실제 검증
   - 장시간 신뢰성, 다양한 조건 검증은 제한적

**연구 철학의 차이:**

**Khandate의 철학:**
```
"Planning(계획)과 Learning(학습)을 결합하면
 어려운 탐색 문제를 해결할 수 있다"

→ 방법론에 집중
→ 어려운 물체로 방법의 우수성 증명
→ 이론과 실전의 균형
```

**AnyRotate의 철학:**
```
"고밀도 센싱이 있으면
 극한 조건에서도 강건한 조작이 가능하다"

→ 센싱의 중요성 강조
→ 중력 불변이라는 새 차원 추가
→ 실용성에 집중
```

**상호 보완성:**

흥미롭게도, 두 접근을 결합하면 시너지가 날 것 같습니다:

```
Khandate의 SBP + AnyRotate의 고밀도 촉각
= 복잡한 형상을 임의 방향에서 다루는 시스템
```

예를 들어:
- L자 물체를 거꾸로 들고
- 임의의 축으로 회전시키면서
- 손가락 게이팅까지 수행

이것이 바로 과학의 발전 방식입니다. 각 연구가 특정 차원을 깊이 파고, 나중에 통합되어 더 강력한 시스템이 만들어지는 것이죠.

**핵심 차별점:**

1. **정보의 풍부함**:
   - 기존: 4 bits ~ 16 bits
   - AnyRotate: 연속적 실수 값 (무한 해상도)

2. **통합성**:
   - 기존: 조건별 별도 정책
   - AnyRotate: 하나의 정책으로 모든 조건

3. **강건성**:
   - 기존: 제한된 조건에서만
   - AnyRotate: 극한 조건(거꾸로, 옆으로)에서도

### Sievers et al. (2022) - 토크 제어 순수 촉각

**그들의 접근:**
- 토크 제어 DLR 손 사용
- 순수 촉각 (비전 없음)
- 모델 기반 + 강화학습 결합

**장점:**
- 물리 법칙 직접 활용
- 더 정밀한 힘 제어

**한계:**
- 고정된 손 방향
- 특정 물체에만 테스트
- 일반화 능력 제한적

**AnyRotate가 더 나은 점:**
- 다양한 물체로 일반화
- 중력 불변
- 완전 학습 기반 (물리 모델 불필요)

**Sievers가 더 나은 점:**
- 더 정밀한 힘 제어
- 물리적 직관 통합

---

## 7. 이론적 관점: 왜 이게 작동하는가?

기술적 디테일을 넘어서, 이 시스템이 왜 작동하는지 이론적으로 이해해봅시다.

### 정보 이론으로 바라보기

**채널 용량 관점:**

각 센싱 방식이 전달할 수 있는 정보량을 생각해봅시다.

**고유수용감각:**
- 16개 관절 각도
- 연속값이지만 **간접적** 정보
- 접촉 상태를 추론해야 함

**이진 촉각:**
- 4개 손가락 × 1 bit = 4 bits
- **직접적**이지만 **제한적**
- "어디에 닿았나"는 모름

**고밀도 촉각:**
- 4개 손가락 × (θ, φ, |F|) = 12개 연속 변수
- **직접적**이고 **풍부함**
- 위치와 힘을 동시에 앎

**상호 정보 (Mutual Information):**

물체 상태 S와 관찰 O 사이의 상호 정보:
```
I(S; O) = H(S) - H(S|O)
```

즉, "관찰을 통해 상태에 대한 불확실성이 얼마나 줄어드는가?"

```
I(S; O_dense) > I(S; O_discrete) > I(S; O_binary) > I(S; O_proprio)
```

고밀도 촉각이 더 많은 불확실성을 제거하므로, 더 좋은 의사결정이 가능합니다!

### POMDP 관점

In-hand 조작은 본질적으로 **부분 관측 마르코프 결정 과정**(POMDP)입니다.

**문제:**
- 물체의 정확한 자세는 모름
- 접촉 지점의 정확한 위치는 모름
- 마찰 계수는 모름

**해결책: Belief State**

에이전트는 "실제 상태가 무엇일까?"에 대한 확률 분포를 유지합니다:
```
b(s) = P(s | history)
```

**TCN의 역할:**
TCN은 지난 30 프레임의 관찰로부터 belief state의 **충분 통계량**을 계산합니다:
```
z_t = f_TCN(o_{t-29:t})
```

이 8차원 벡터 z_t는 belief state를 압축한 표현입니다!

**고밀도 촉각의 이점:**
더 많은 정보 → 더 정확한 belief → 더 좋은 결정
```
Uncertainty(b_dense) < Uncertainty(b_binary)
```

### 접촉 역학의 관점

**왜 미끄럼 감지가 중요한가?**

마찰 제약:
```
|F_tangential| ≤ μ * |F_normal|
```

미끄럼이 임박하면:
```
|F_tangential| / |F_normal| → μ
```

**고밀도 촉각으로 알 수 있는 것:**
- 접촉 자세 (θ, φ) → 법선 방향 추정
- 접촉 힘 |F| → 전체 힘의 크기

이 두 정보를 결합하면:
- 법선 성분 F_normal 추정
- 접선 성분 F_tangential 추정
- 마찰 여유도 계산!

**정책이 배우는 것:**
```python
if friction_margin < threshold:
    # 미끄럼 임박!
    increase_normal_force()
    adjust_finger_positions()
else:
    # 안전함
    continue_rotation()
```

이 모든 것이 명시적 프로그래밍 없이 **학습**으로 획득됩니다!

### 강화학습 관점: 탐색의 어려움

**왜 보조 목표가 필요한가?**

각속도를 직접 목표로 하면:
```
r = ω_desired - ω_actual
```

문제:
- 초기에 ω_actual ≈ 0 (물체가 안 돌아감)
- 보상이 계속 음수
- 어떻게 개선할지 모름 (희소 보상 문제)

**보조 목표 방식:**
```
r = exp(-distance_to_goal)
```

장점:
- 목표에 가까워질수록 점진적으로 보상 증가
- 밀집 보상 (dense reward)
- 학습 신호가 명확

**적응형 커리큘럼의 역할:**

초기 단계:
```
α = 0
r_total = r_stability  # 안정성만 집중
```

후기 단계:
```
α = 1
r_total = r_rotation + r_stability  # 회전도 중요
```

이는 **자동 과정 학습**(automatic curriculum learning)의 일종입니다. 로봇이 기본을 먼저 배우고, 준비되면 자동으로 다음 단계로!

### 창발적 행동: 미끄럼 감지

가장 흥미로운 발견은 **명시적으로 가르치지 않은 행동**이 나타났다는 것입니다.

**설계:**
- 미끄럼 감지 모듈 없음
- 미끄럼에 대한 명시적 보상 없음
- 단지 "회전하라, 떨어뜨리지 마라" 만 명시

**결과:**
- 정책이 스스로 미끄럼을 감지
- 미끄러질 때 반응적으로 대응
- 안정성 유지

**왜 이런 일이?**

1. **풍부한 정보**: 고밀도 촉각으로 미세한 변화 감지
2. **시간적 패턴**: TCN이 "정상 vs 비정상" 패턴 학습
3. **부정적 피드백**: 떨어뜨리면 큰 페널티 → 그 직전 신호 학습

이는 **암묵적 학습**(implicit learning)의 강력함을 보여줍니다. 모든 것을 명시적으로 가르칠 필요가 없습니다!

---

## 8. 한계점과 개선 방향

완벽한 시스템은 없습니다. AnyRotate도 한계가 있고, 연구팀도 솔직하게 인정합니다.

### 현재의 한계

#### 1. 상자 형태의 어려움

**문제:**
- 정육면체나 직육면체를 잘 못 다룸
- 큰 종횡비(aspect ratio) 물체도 어려움
- 예: 긴 막대, 납작한 판

**원인:**
다른 물체 상태에서 유사한 촉각 정보가 나타나서 **모호성** 발생

예를 들어:
- 상자의 모서리를 잡을 때
- 상자의 면을 잡을 때
- 두 경우의 촉각 정보가 비슷할 수 있음

**개선 방향:**
- 촉각 이미지 직접 사용 (현재는 특징으로 압축)
- 접촉 힘의 3D 분포 활용
- 비전 정보 추가 (멀티모달)
- 다중 접촉점의 관계 모델링

#### 2. 하드웨어 한계

**Allegro Hand의 약점:**
- Thumb Up/Down 방향에서 구동력 약화
- 손가락이 수평일 때 중력이 반대로 작용
- 모터가 물체 무게를 온전히 버텨야 함

**결과:**
- 어려운 방향에서 성능 저하
- 무거운 물체는 더 어려움

**개선 방향:**
- 더 강력한 액추에이터
- 저비용 고성능 하드웨어 설계
- 방향에 따른 적응형 제어 게인
- 중력 보상 알고리즘

#### 3. 작업의 복잡도

**현재:**
- 회전만 가능
- 단일 물체만

**확장 필요:**
- 다단계 조작 (예: 뚜껑 열기)
- 복잡한 조립 작업
- 도구 사용
- 양손 협업

**도전:**
- 계층적 강화학습 필요
- 장기 계획과 단기 제어 통합
- 스킬 라이브러리 구축

#### 4. 변형 물체

**현재:**
- 강체(rigid body)만 다룸
- 천, 종이, 고무 등은 못 다룸

**문제:**
- 시뮬레이터가 변형을 잘 모델링 못함
- 변형 상태 추정 어려움
- 접촉 역학이 훨씬 복잡

**개선 방향:**
- 소프트 바디 시뮬레이션
- 변형 추정 모듈
- 재료 속성 인식
- 적응형 파지 전략

### Sim-to-Real 격차

여전히 존재하는 차이들:

**1. 접촉 물리:**
- 시뮬레이션: 이상적인 마찰 모델
- 현실: 복잡한 비선형 마찰, 점착, 마모

**2. 센서 노이즈:**
- 시뮬레이션: 가우시안 노이즈
- 현실: 구조화된 노이즈, 드리프트, 조명 변화

**3. 시간 지연:**
- 시뮬레이션: 무시 가능
- 현실: 센서 지연, 통신 지연, 처리 지연

**개선 방향:**
- 실제 데이터로 시뮬레이터 개선
- 온라인 적응 메커니즘
- 메타 학습 (빠른 적응)
- 디지털 트윈 기술

### 학습 효율성

**현재 요구사항:**
- 8,192개 병렬 환경
- 고성능 GPU 필요
- 수일의 학습 시간

**개선 방향:**
- 샘플 효율적 알고리즘 (SAC, TD3, DreamerV3)
- 모델 기반 강화학습
- 오프라인 RL (사전 수집 데이터 활용)
- 인간 시연 활용 (imitation learning)
- 전이 학습

### 평가 메트릭의 제한

**현재:**
- 회전 수: 명확하지만 질을 반영 못함
- TTT: 주관적 판단 포함

**추가 필요:**
- 에너지 효율성
- 파지 안정성 지표
- 물체 손상 정도
- 성공률 신뢰 구간
- 일반화 능력 정량화

**제안:**
- 자동 추적 시스템 (모션 캡처)
- 힘-토크 센서 데이터
- 표준화된 벤치마크

---

# ⛏️ Dig Review

> ⛏️ Dig — Go deep, uncover the layers. Dive into technical detail.

## AnyRotate 논문 개요 및 시스템 구조

AnyRotate는 인간 수준의 다축 중력 무관(invariant) 인-핸드 객체 회전을 위해 고해상도 촉각 정보를 활용하는 로봇 시스템이다. 4-지간(Allegro) 로봇 핸드의 각 손가락 끝에 부착된 생체모방형 광학 촉각 센서(TacTip 기반 DigiTac)를 사용하며, 강화학습(RL)을 통해 어떤 축(axis)에 대해서도 임의의 방향으로 물체를 회전시킬 수 있는 단일 정책(policy)을 학습한다. 이 시스템의 핵심은 시뮬레이션에서 연속적 접촉 피처 표현(continuous contact feature representation) 을 학습하고, 실제 환경의 촉각 영상으로부터 이를 예측하여 제로샷(sim-to-real) 정책 이전을 달성하는 것이다. 특히, 촉각 센서의 연속적 접촉 포즈(contact pose)와 접촉력(contact force) 정보를 정책에 입력함으로써, 기존의 이진(binary) 또는 이산(discrete) 촉각 표현보다 풍부한 접촉 정보를 활용할 수 있음을 보였다.

**촉각 센서 및 핸드 구조:** Allegro 4-지간 핸드(16자유도)에 TacTip 기반 촉각 센서를 부착하고, UR5 로봇암에 장착하여 다양한 손 방향(palm up/down, thumb up/down 등)에서 객체를 회전시킨다 (그림 1, 2 참조). 촉각 센서는 커브드 표면을 가지며 밀착면 전체의 접촉 정보를 고해상도 영상으로 제공한다.

**목표 및 제약:** AnyRotate는 임의의 회전축, 임의의 손 방향에서 연속적이고 안정적인 객체 회전을 수행하도록 학습된다. 이를 위해 목표 지향(goal-conditioned) 강화학습 문제로 정의하고, 보조 서브골(auxiliary goal)과 점진적 커리큘럼(adaptive curriculum) 등을 통해 효율적인 학습을 설계했다. 교사-학생(Teacher-Student) 구조로, 시뮬레이션에서 특권 정보(privileged information)를 사용한 교사 정책을 학습하고, 학생 정책이 촉각-고유 감각만으로 이를 모방하도록 한다.

본 분석에서는 촉각 센서 데이터의 표현(Representation), 인코딩(Encoding), 제어 정책 통합 방법을 중심으로, 촉각 피드백의 시뮬레이션 구현과 시뮬-실제 전이 과정, 그리고 촉각 감지 모델링의 혁신점 및 과제를 상세히 살펴본다.

## 촉각 센서 데이터의 표현과 정책에서의 활용

촉각 센서가 생성하는 고해상도 영상(tactile image) 은 원시적으로 매우 고차원이므로, 이를 제어 정책이 활용 가능한 형태로 변환해야 한다. AnyRotate에서는 촉각 영상을 접촉 포즈와 접촉력이라는 저차원 연속적 피처로 인코딩한다. 구체적으로, 접촉 포즈(contact pose) 는 손가락 끝점(fingertip) 기준의 구형 좌표계에서 극각(polar angle) θ와 방위각(azimuthal angle) φ로 나타낸다. 접촉력(contact force) 은 접촉점에서 발생하는 힘 벡터의 크기(등급크기, magnitude)이다. 추가로, 이진 접촉 신호(binary contact) 는 실제로 센서가 물체에 접촉 중임을 나타내는 0/1 값으로, 깊이(z축) 변위나 힘의 크기가 일정 임계값 이상이면 참(1)으로 설정한다. 모든 손가락에 대하여 이들 값이 계산되어 정책 관측(observation) 벡터에 포함된다.

제어 정책의 관측 공간(observation space) 은 다음과 같은 정보들로 구성된다: 현재 및 목표 관절각, 이전 행동, 각 손가락 끝의 위치와 자세, 그리고 촉각 관련 변수로서 각 손가락의 이진 접촉 여부, 접촉 포즈(θ, φ), 접촉력 크기이다. 실제 환경에서 손가락 끝 위치/자세는 역기구학으로 계산하며, 촉각 영상처리 결과를 통해 접촉 포즈와 접촉력을 추출한다. 예를 들어, 시뮬레이션에서는 각 손가락 끝의 로컬 접촉 위치 벡터로부터 θ, φ를 계산하고, 접촉력의 정합(net force)을 $|\mathbf{F}|$로 취해 이를 관측값으로 사용한다. 이처럼 연속적이고 풍부한 촉각 변수들은 정책이 물체의 미세 움직임과 접촉 상태를 정밀하게 파악하도록 돕는다.

이렇게 추출된 촉각 피처는 정책의 입력으로 사용된다. 교사 정책은 이들 값을 포함한 관측 벡터를 받아 연속 동작(관절 상대 위치명령)을 출력하며, 실제 정책(학생)은 촉각 히스토리와 고유 감각(proprioception) 히스토리를 결합한 잠재벡터를 통해 행동을 선택한다. 학생 정책에서는 TCN(순환합성곱 신경망) 구조를 사용하여 시간축으로 연속된 촉각 관측 시퀀스를 인코딩한다. 즉, 과거 30스텝에 걸친 촉각 피처와 관절각 히스토리를 TCN에 통과시켜 잠재벡터(z)로 만들고, 이를 바탕으로 정책(액터)이 동작을 출력한다. 정책 학습 과정에서는 교사 정책이 생성한 잠재벡터와 학생의 잠재벡터 간 MSE(평균제곱오차) 및 행동 출력 간 NLL(부정로그우도) 손실을 최소화하여 학생을 지도학습 방식으로 훈련한다.


<center>
<img src="../../images/2025-11-02-anyrotate/01.png" width="100%" />
</center>

> AnyRotate의 촉각 예측 파이프라인. (a) 각 손가락 끝의 촉각 영상은 전처리(그레이스케일 변환, 리사이즈 등)를 거친다. (b) 각 촉각 영상은 학습된 관찰 모델(observation model, CNN)을 통과하여 접촉 포즈(θ, φ)와 접촉력 |F|을 예측한다. (c) 예측된 접촉 포즈(극각 θ, 방위각 φ)는 구면좌표계에서 표시되며, 접촉력은 색상의 면적으로 시각화된다. 이와 같이 추출된 촉각 피처가 강화학습 정책의 입력으로 사용된다.

위 그림에서 보듯이, 촉각 영상은 먼저 흐림/노이즈 제거 등의 전처리를 거친 뒤 CNN 관찰 모델에 입력된다. CNN은 학습을 통해 각 손가락 접촉의 극각(θ), 방위각(φ) 그리고 총 접촉력 크기 $|\mathbf{F}|$을 예측한다. 접촉 포즈 $(\theta,\phi)$와 접촉력 크기는 정책에 연속 변수로 제공되며, 손가락별로 $($이진 접촉, θ, φ, $|\mathbf{F}|)$ 값이 매 타임스텝의 관측(state)에 포함된다. 이러한 연속적 촉각 표현은 2D 이미지를 단순히 접촉 있음/없음으로 이진화한 기존 방식에 비해 훨씬 풍부한 정보를 제공한다. 실제로 실험 결과 연속 접촉 포즈와 힘 정보를 활용한 정책은 이산화(discrete) 촉각 표현보다 물체 회전 성능이 현저히 우수하였다.

## 촉각 피드백의 시뮬레이션 구현과 도메인 적응

AnyRotate에서는 촉각 센서를 시뮬레이션 환경에서 가상으로 구현하여 교사 정책을 학습시킨다. IsaacGym 물리엔진을 사용하여 충돌을 처리하며, 각 손가락 끝에서 발생한 실제 접촉 정보를 이용한다. 구체적으로, 시뮬레이션 단계에서 손가락 끝의 로컬 접촉 위치 $p_{\rm local}$를 추출하여 이를 극좌표 $(\theta,\phi)$로 변환한다. 또한, 접촉으로 인한 힘 벡터를 계산하여 그 크기 $|\mathbf{F}|$를 구하고, 이를 접촉력 관측값으로 사용한다. 이 때 실세계 센서의 특성을 모사하기 위해 몇 가지 처리를 추가한다. 먼저 부드러운(tactile) 센서가 가진 탄성 변형 딜레이를 모사하기 위해 힘 값에 지수평활(exponential moving average)을 적용하고, 시뮬레이터에서의 힘/포즈 값을 실제 센서의 측정 범위에 맞춰 포화(saturation) 및 재스케일링한다. 예를 들어, 힘 크기의 최대값을 제한하고 범위를 재조정하여 가상 센서가 실제와 유사한 범위 내에서 동작하도록 한다. 포즈 각도도 물리적 센서가 허용하는 최대 각도로 제한한다. 이러한 조정은 시뮬레이션과 실제 센서 간의 데이터 분포 차이를 줄이기 위한 일종의 도메인 적응(domain adaptation) 기법이다.

또한, 실제 촉각 센서에서는 접촉이 발생하지 않는 상태에서도 미약한 노이즈가 발생할 수 있는데, 이를 처리하기 위해 시뮬레이션에서도 이진 접촉 신호를 만들 때 일정 임계값($\theta_F$)을 사용한다. 즉, 힘 크기가 작은 경우에는 접촉 없음으로 간주하여 접촉 포즈와 힘을 0으로 마스킹한다. 이러한 절차로 시뮬레이션된 촉각 관측값(θ, φ, $|\mathbf{F}|$, 접촉 유무)이 RL 관측 공간에 공급되며, 교사 정책은 이를 포함한 완전 관측 상태를 사용하여 학습된다. 시뮬레이션 및 로봇 제어 주파수는 각각 60Hz, 20Hz로 실제 시스템과 동일하게 설정되었다. 이처럼 세심하게 조정된 시뮬레이션 촉각은 실제 센서와의 심도 차이(depth), 슈어(shear) 범위를 반영하도록 설계되었다.

도메인 랜덤화(domain randomization)의 관점에서 살펴보면, AnyRotate는 전통적인 이미지 노이즈/텍스처 랜덤화 대신, 물리적 파라미터 포화 및 스케일 매핑으로 시뮬레이션-실제 격차를 줄인다. 예를 들어, 실제 TacTip 센서는 깊이 방향 변위가 수 mm 범위 내에서만 민감하게 반응하므로, 시뮬레이션에서는 접촉 깊이도 약 ~ mm 범위로 제한한다. 이처럼 실험적으로 정해진 범위 내에서 촉각 데이터를 생성하여 관찰 모델을 학습함으로써 시뮬-실제 간의 특성 불일치를 최소화한다.

## Observation Model을 통한 시뮬-실제 전이

실제 로봇 환경에서는 시뮬레이션에서처럼 접촉 위치나 힘을 직접 계산할 수 없으므로, 이미지 기반 관찰 모델을 활용하여 실제 촉각 영상을 촉각 피처로 변환한다. 이를 위해 AnyRotate는 CNN 기반 관찰자 모델(Observation Model)을 학습한다. 학습 데이터는 6-자유도 UR5 로봇암에 탑재된 TacTip 센서와 힘/토크(F/T) 센서를 사용하여 수집된다. 구체적으로, 센서를 평평한 자극 표면에 무작위 방향·위치로 접촉(tap)시켜 얻은 촉각 이미지와 F/T 센서의 힘 정보를 함께 저장한다. 각 데이터 샘플은 로봇의 $x,y,z$ 위치와 접촉력 $(F_x,F_y,F_z)$을 포함하며, 이 중 접촉 포즈(θ, φ)는 로봇 암의 위치로부터, 접촉력 크기는 F/T 센서 값의 크기로 정답 레이블(label)로 사용한다. 데이터 수집 시 센서 자세(pose)는 실험적으로 다음 범위 내에서 랜덤 샘플링된다: 깊이 $\Delta z\in[-4,-1]$mm, $x,y$방향 $\pm2$mm, 회전 $\pm28^\circ$ 범위. 센서당 약 3000개의 이미지를 모아 CNN을 학습한다.

학습된 관찰 모델은 입력으로 원시 촉각 영상(그레이스케일, 240×135 픽셀)을 받아 6차원 출력을 예측한다. 이 6차원은 접촉 깊이 $d_z$, 접촉 포즈 $\theta,\phi$, 그리고 힘 벡터 성분 $F_x,F_y,F_z$이다. 이후 이 중에서 실제 강화학습 관측값으로는 ($\theta,\phi,|\mathbf{F}|$) 세 값만 사용된다. 즉, 모델이 예측한 $(F_x,F_y,F_z)$로부터 크기 $|\mathbf{F}|=\sqrt{F_x^2+F_y^2+F_z^2}$을 계산하여 접촉력을 얻고, 접촉 여부는 SSIM(구조적 유사성 지표)를 기반으로 결정한다. 구체적으로, 현재 입력 영상과 센서가 접촉하지 않은 기준 영상을 비교하여 SSIM 유사도를 계산하고, 이 값이 0.6 이상이면 접촉으로 간주하여 위에서 예측한 $(\theta,\phi,|\mathbf{F}|)$를 사용한다. 그렇지 않으면 접촉하지 않은 것으로 처리하여 모든 촉각 피처를 0으로 마스킹한다. 이 과정에서 SSIM 임계값은 경험적으로 0.6으로 설정하였고, 그레이스케일 영상에 블러와 어댑티브 임계처리(adaptive threshold) 등 전처리 필터를 적용하여 노이즈를 줄인다.

학습된 CNN 관찰 모델은 제로샷(sim-to-real) 정책 이전에 핵심 역할을 한다. 즉, 시뮬레이션에서 학습된 교사-학생 정책은 시뮬레이션상의 촉각 피처를 이용하지만, 실제 배치 시에는 센서가 제공하는 실시간 영상에서 관찰 모델이 추출한 촉각 피처를 입력으로 사용한다. 이 때 관찰 모델은 이전에 수집한 데이터로 충분히 학습되었기 때문에, 정책은 학습 시와 유사한 형태의 촉각 입력을 받게 되어 추가적인 재학습 없이(제로샷) 바로 실환경에 적용 가능하다.

## 교사-학생 정책 학습 파이프라인

AnyRotate의 학습은 크게 두 단계로 구성된다. 첫째, 교사 정책(Teacher Policy)은 시뮬레이션에서 특권 정보(privileged information)를 사용하여 PPO 같은 RL 기법으로 학습한다. 교사 정책의 입력 관측에는 앞서 언급한 촉각 피처 외에도 객체의 정확한 위치·자세, 각도 속도, 객체 크기, 중력 작용력 등 시뮬레이션에서만 얻을 수 있는 정보가 포함된다. 이렇게 학습된 교사 정책은 목표 축에 대해 물체를 안정적으로 회전시키는 정책을 만들어낸다.

둘째, 학생 정책(Student Policy)은 실제 상황에서의 실행을 위해 학습된다. 학생은 교사를 모방하기 위해 지도학습(policy distillation)을 사용한다. 학생 정책은 오로지 고유감각(관절각 등)과 앞서 추출된 촉각 히스토리만을 입력으로 받는다. 교사 정책은 매 시점마다 내부 잠재벡터(z)를 생성하는데, 학생 정책은 TCN 인코더로 입력 연속 관측을 잠재벡터로 압축하고, 이 잠재벡터가 교사의 것과 유사해지도록 학습된다. 또한 학생 정책의 출력 행동이 교사 정책의 행동과 가까워지도록 NLL 손실을 추가로 최적화한다. 이때 전체 손실은 잠재벡터 MSE와 행동 NLL의 합이며, 식으로 표현하면 다음과 같다: $$\mathcal{L} = \alpha |z_{\rm teacher} - z_{\rm student}|^2 + \beta\,\bigl(-\log p_{\rm student}(a_{\rm teacher}|s)\bigr),$$ 여기서 $z_{\rm teacher}$는 교사의 잠재벡터, $z_{\rm student}$는 학생의 인코더 출력, $p_{\rm student}(a_{\rm teacher}|s)$는 학생 정책이 교사 행동 $a_{\rm teacher}$를 취할 확률이다. 이 과정에서 학습된 관찰 모델을 활용하여 시뮬레이션 학생 학습 단계에서도 실제 촉각 피처를 모사하여 사용할 수 있다.


<center>
<img src="../../images/2025-11-02-anyrotate/00.png" width="100%" />
</center>

> AnyRotate의 학습 및 적용 파이프라인 개요. (왼쪽) 목표 회전축에 대한 물체 자세 재설정(auxiliary goal) 방식의 RL 문제 설정. (오른쪽) 교사-학생 정책(distillation) 구조. 교사 정책(Teacher)은 시뮬레이션의 특권 정보와 촉각 관측을 입력으로 RL 학습을 수행하고, 학생 정책(Student)은 촉각·고유감각 관측 히스토리를 입력으로 교사를 모방한다. 실세계 이전 시에는 CNN 관찰 모델이 실제 촉각 영상에서 접촉 피처를 추출하여 학생 정책에 공급한다.

그림에서 보듯, AnyRotate는 교사-학생 정책 구조와 보조 목표(auxiliary goal), 적응형 커리큘럼을 결합하여 학습을 진행한다. 교사 정책은 객체 자세를 목표로 설정하고, 객체의 6D 키포인트(keypoint) 거리 보상 등을 포함한 목표 지향 보상함수로 훈련된다. 학생 정책은 이렇게 학습된 교사의 안정적인 움직임을 실제 센서 데이터로 재현할 수 있도록, TCN 인코딩과 행동 모방을 통해 학습된다. 이 과정을 통해 AnyRotate는 시뮬레이션에서 학습한 정책을 추가 데이터 수집이나 재학습 없이 실환경에 전달할 수 있게 된다.

## 촉각 감지 모델링의 혁신성과 도전 과제

AnyRotate가 제시하는 주요한 혁신성 중 하나는 고해상도 촉각 정보를 활용한 시뮬-실제 RL 정책을 구현했다는 점이다. 기존의 촉각 기반 조작 연구에서는 종종 접촉 영상을 단순한 이진 신호나 이산 공간으로 축소하여 사용해 왔다. 반면 본 논문은 촉각 영상을 연속적이고 기하학적인 피처(θ, φ, |F|)로 표현함으로써, 훨씬 더 풍부한 접촉 정보를 정책 학습에 활용하였다. 이를 통해 미지의 물체나 무거운 물체에서도 유연하게 일반화할 수 있는 단일 정책을 달성했으며, 실제 다양한 물체로의 전이 실험에서 복수의 촉각 센서만으로도 성공적으로 회전 작업을 수행하였다. 또한, 복수 촉각 정보를 통해 물체의 미끄러짐(slippage)이나 잡기 실패를 암묵적으로 탐지하여 반응하는 행동을 구현할 수 있었다는 점도 주목할 만하다. 연구진은 명시적 미끄럼 검출기 없이도 “풍부한 다지간 촉각 센서가 객체 움직임을 감지하고 정책의 강인성(robustness)을 향상시키는 반응 행동을 유발”한다는 사실을 보고하였다.


<center>
<img src="../../images/2025-11-02-anyrotate/02.png" width="100%" />
</center>

그러나 촉각 센서 모델링에는 여러 도전 과제가 존재한다. 우선, 물리 기반 시뮬레이션의 한계다. TacTip과 같은 광학 촉각 센서는 유연한 겔 표면의 변형을 영상으로 포착하므로, 완전히 정확한 물리 시뮬레이션은 어려우며 매우 계산 집약적일 수 있다. AnyRotate에서는 단순히 충돌 지점을 기반으로 접촉 위치와 힘을 계산하는 방식으로 근사했지만, 실제 센서의 미세한 압력 분포나 센서 내부 렌즈 왜곡 등은 반영하지 못한다. 따라서 시뮬-실제 간의 데이터 분포 차이가 발생할 수 있으며, 이를 보정하기 위해 실험적으로 값의 포화, 시점 변경 등의 도메인 적응 기법을 활용하였다.

또한, 촉각 영상 자체가 매우 높은 차원을 가지므로 이를 유용한 피처로 압축하는 신경망 설계도 중요한 문제다. AnyRotate는 각 손가락마다 개별 CNN을 학습하여 접촉 피처를 추출했지만, 이 과정에서 과적합(overfitting) 방지와 정밀도(accuracy) 확보를 위해 많은 데이터와 적절한 네트워크 규모를 필요로 했다. 뿐만 아니라, 로봇 조작 중에는 센서 이미지에 동적 노이즈(손떨림, 광원 변화 등)가 발생할 수 있기 때문에, 관찰 모델은 다양한 환경 변화에 견고해야 한다. 이 연구에서는 SSIM 기반의 접촉 유무 검출과 영상 전처리로 노이즈를 감소시켰지만, 완전한 일반화를 위해서는 추가적인 도메인 랜덤화(예: 조명 색깔 변화, 노이즈 추가 등)가 필요할 수 있다.

끝으로, AnyRotate는 단일 정책으로 모든 회전축에 일반화한다는 점에서 기술적으로도 과제다. 서로 다른 축 회전은 중력의 영향과 접촉 기구학이 달라지므로, 정책이 충분히 모든 상황을 커버하도록 학습 데이터를 다양화해야 한다. 이를 위해 연구진은 에피소드마다 목표 회전축과 손 방향을 무작위로 설정하고, 적응형 보상 커리큘럼을 통해 학습 초기에는 쉬운 과제부터 시작했다. 이러한 접근은 시뮬레이션에서는 효과적이었으나, 실세계에서는 눈에 띄는 추가 구현 없이도 전이 가능해야 한다. AnyRotate는 관찰 모델을 통한 제로샷 전이로 이를 달성했으나, 아직도 “시뮬레이션에서 본 것과 완전히 다른 형태의 접촉”에는 민감할 수 있다. 예를 들어 이형적인 재질이나 윤곽을 가진 물체에서는 관찰 모델이 예측 오차를 낼 수 있어 정책 성능에 영향이 있을 수 있다. 따라서 고해상도 촉각 신호를 시뮬레이션에 정밀히 반영하는 것은 지속적인 연구 과제로 남아 있다.

이처럼 AnyRotate는 고해상도 촉각 정보의 시뮬-실제 통합이라는 측면에서 중요한 진전을 이뤘다. 연속적인 촉각 피처 표현과 관찰 모델을 결합하여 제로샷 이전을 가능하게 했다는 점은 촉각 기반 조작 연구의 새로운 방향을 제시한다. 동시에, 시뮬레이션에서의 센서 근사화 및 시뮬-실제 차이 극복이라는 어려운 문제를 다루었으며, 다가오는 연구에서는 더욱 정교한 물리 모델링과 데이터 증강 방법이 요구될 것으로 보인다.
