---
title: "📃ViViDex 리뷰"
date: 2025-11-04
categories: [mpc, rl, action-chunking]
toc: true
number-sections: False
description: Learning Vision-based Dexterous Manipulation from Human Videos
---

> 🔍 Ping. 🔔 Ring. ⛏️ Dig. A tiered review series: quick look, key ideas, deep dive.


- [Paper Link](https://arxiv.org/abs/2404.15709)
- [Project LInk](https://zerchen.github.io/projects/vividex.html)
- Code:[Sapein](https://github.com/zerchen/vividex_sapien), [Mujoco](https://github.com/zerchen/vividex_mujoco)

1. 다양한 자세의 여러 물체를 조작하는 다지 로봇 손을 위한 통합된 비전 기반 정책을 인간 비디오로부터 학습하는 ViViDex 프레임워크를 제안합니다.
2. ViViDex는 인간 비디오에서 추출된 궤적을 궤적 안내 보상으로 사용하여 물리적으로 타당한 상태 기반 정책을 훈련하고, 이 성공적인 에피소드들을 활용하여 특권 정보 없이 통합된 시각 정책을 학습합니다.
3. 실험 결과, ViViDex는 기존 최첨단 방식을 뛰어넘는 성능을 보이며, 더 적은 인간 시연 비디오만으로도 다양한 조작 작업을 수행하고 낯선 물체에 대한 일반화 능력을 실제 로봇 환경에서도 입증했습니다.

<center>
<img src="../../images/2025-11-04-vividex/00.png" width="100%" />
</center>

---

# 🔍 Ping Review

> 🔍 Ping — A light tap on the surface. Get the gist in seconds.

이 논문은 인간 비디오로부터 다지(multi-fingered) 로봇 핸드의 vision-based Dexterous Manipulation 정책을 학습하기 위한 새로운 프레임워크인 ViViDex를 제안합니다. 기존 연구들은 인간 비디오에서 추출된 궤적의 노이즈와 지상 진실 객체 상태(ground-truth object states)와 같은 특권 객체 정보(privileged object information)에 대한 의존성 때문에 성능 향상에 제한이 있었습니다. ViViDex는 이러한 한계를 해결하기 위해 세 가지 모듈로 구성됩니다.

첫 번째 모듈은 **Reference Trajectory Extraction**입니다. 이 모듈은 인간 비디오에서 사람의 손과 객체의 포즈(poses)를 추출하여 로봇 핸드 궤적의 레퍼런스로 활용합니다. 구체적으로, 사람 손의 포즈와 모양은 MANO 모델($\psi_h \in \mathbb{R}^{21 \times 3}$)로 표현되며, 이를 로봇 핸드 포즈로 리타겟팅(retargeting)합니다. 이 과정은 다음 최적화 문제로 정의됩니다:
$$ \min_{q_t^r} \sum_{t=1}^T \Vert \hat{x}_{t,j}^r(q_t^r) - \psi_{t,j}^h \Vert_2^2 + \alpha \Vert q_t^r - q_{t-1}^r \Vert_2^2 $$
여기서 $q_t^r$는 로봇 관절 회전 각도, $\psi_{t,j}^h$는 사람 손 끝과 중간 손가락 관절 위치, $\hat{x}_{t,j}^r$는 로봇 관절 위치를 나타냅니다. 이 최적화를 통해 로봇과 객체의 모션을 시뮬레이터로 가져와 시각적으로는 그럴듯하지만 물리적으로는 아직 타당하지 않은 레퍼런스 궤적을 생성합니다.

두 번째 모듈은 **Trajectory-guided State-based Policy Learning**입니다. 이 모듈에서는 물리적으로 타당한 궤적을 복구하기 위해 레퍼런스 궤적을 보상 함수(reward function)에 활용하여 강화 학습(RL)으로 `state-based policy`를 훈련합니다. `state-based policy`의 네트워크 아키텍처는 액터(actor) 및 크리틱(critic) MLP로 구성되며, 로봇과 객체 상태를 입력받아 로봇 제어 명령을 예측합니다.
보상 함수는 `pre-grasp` 단계와 `manipulation` 단계로 나뉩니다.
`pre-grasp` 단계에서는 로봇이 물리적 접촉 없이 객체에 접근해야 하며, 사람과 유사하게 접근하도록 다음 보상을 사용합니다:
$$ R_p = \sum_{t=1}^{T_p} 10 \cdot \exp(-10 \cdot \Vert x_{t,rt}^r(q_t^r) - \hat{x}_{t,rt}^r \Vert_2^2) $$
여기서 $T_p$는 `pre-grasp` 단계의 길이, $\hat{x}_{t,rt}^r$는 레퍼런스 궤적의 로봇 손가락 끝 위치, $x_{t,rt}^r$는 현재 로봇 손가락 끝 위치입니다.
`manipulation` 단계에서는 객체를 원하는 타겟 설정(target configuration)으로 조작하는 것이 목표이며, 로봇 및 객체 모션을 함께 제약하는 다음 보상을 사용합니다:
$$ R_m = \sum_{t=T_p+1}^{T_r} \lambda_1 R_m^h + \lambda_2 R_m^o + \lambda_3 \mathbb{1}_{\text{cont}} + \lambda_4 \mathbb{1}_{\text{lift}} $$
여기서 $T_r$은 레퍼런스 궤적의 길이입니다. $R_m^h$는 손 모션을 제약하고, $R_m^o = \exp(-\alpha_1(\Vert x_t^o - \hat{x}_t^o \Vert_2^2 + \alpha_2 \phi(\theta_t^o, \hat{\theta}_t^o)))$는 객체 모션을 제약합니다. $x_t^o, \hat{x}_t^o$는 현재 및 레퍼런스 객체 위치, $\phi(\cdot)$는 객체 방향의 각도 거리를 계산합니다. $\mathbb{1}_{\text{cont}}$는 객체와 접촉하는 손가락 끝의 수, $\mathbb{1}_{\text{lift}}$는 객체가 테이블에서 들어 올려졌을 때 보너스를 제공합니다.
또한, 다양한 초기 객체 위치, 회전, 타겟 위치에 일반화하기 위해 레퍼런스 궤적 증강(reference trajectory augmentation) 전략을 도입합니다.

세 번째 모듈은 **Unified Vision-based Policy Learning**입니다. `state-based policy`는 로봇 고유 상태(robot proprioceptive states)와 객체 상태를 입력으로 요구하지만, 실제 환경에서는 객체 상태를 신뢰성 있게 추정하기 어렵습니다. 이를 해결하기 위해, 최적화된 `state-based policy`의 성공적인 에피소드를 롤아웃(rollout)하여 물리적으로 타당한 궤적과 깊이 카메라(depth camera)에서 렌더링된 3D Scene Point Cloud($PC_w \in \mathbb{R}^{N \times 3}$)를 수집하고, 이를 이용하여 `visual policy`를 훈련합니다.
`visual policy`의 성능을 향상시키기 위해, 입력 3D Point Cloud($PC_w$)를 `target coordinate system`($PC_t$)과 `hand-centered coordinate system`(손바닥 및 손가락 끝 관절 좌표계)으로 변환하는 `coordinate transformation`을 제안합니다. 이렇게 변환된 Point Cloud 표현($PC \in \mathbb{R}^{N \times 3(j+3)}$)과 로봇 고유 상태를 PointNet [76]에 입력하여 시각적 특징을 추출하고, 이를 기반으로 로봇 제어 명령을 예측합니다. `visual policy`는 `behavior cloning` (BC) 또는 최근 제안된 3D Diffusion Policy [77, 78]를 사용하여 훈련됩니다.

실험은 `relocate`, `pour`, `place inside` 세 가지 조작 작업을 대상으로 시뮬레이션(Adroit/MuJoCo, Allegro/SAPIEN)과 실제 로봇 환경에서 진행되었습니다. ViViDex는 적은 수의 인간 데모 비디오(각 객체당 1개)만으로도 SOTA DexMV [24]를 크게 능가하는 성능을 보였습니다. 특히, 제안된 궤적 가이드 보상 함수와 궤적 증강이 `state-based policy`의 안정성과 일반화 성능을 향상시키는 데 중요함을 입증했습니다. 또한, `coordinate transformation`을 통한 시각적 표현 강화와 3D Diffusion Policy의 적용이 `unified visual policy`의 성능과 미개척 객체(unseen objects)에 대한 일반화 능력(generalization abilities)을 크게 개선함이 확인되었습니다.

결론적으로, ViViDex는 인간 비디오를 활용하여 Vision-based Dexterous Manipulation을 학습하기 위한 효과적인 프레임워크를 제공하며, 시뮬레이션 및 실제 로봇 환경에서 우수한 성능과 일반화 능력을 입증했습니다. 향후 연구는 인터넷 비디오를 활용하여 더 일반적인 조작 기술을 습득하고, 고급 3D 포즈 추정 알고리즘을 탐구하는 것을 목표로 합니다.


# 🔔 Ring Review

> 🔔 Ring — An idea that echoes. Grasp the core and its value.


> ViViDex: 사람의 비디오로부터 학습하는 시각 기반 정교한 조작 기술

*로봇이 사람의 손동작을 보고 배운다면?*

여러분은 아이가 부모의 행동을 보고 배우는 모습을 본 적이 있나요? 아이들은 수저질, 신발 끈 묶기, 문 여는 법 등 복잡한 손동작을 따라하며 자연스럽게 습득합니다. 이런 시각적 학습(learning by watching)을 로봇에게도 적용할 수 있다면 어떨까요? 바로 이것이 오늘 소개할 ViViDex(Vision-based Dexterous Manipulation from Human Videos) 논문의 핵심 아이디어입니다.

INRIA Paris와 MBZUAI의 연구진들이 ICRA 2025에 발표한 이 연구는 단순히 사람의 비디오를 보여주는 것만으로 로봇이 복잡한 손가락 조작 기술을 학습하도록 만들었습니다. 이는 로봇공학의 오랜 숙제인 "어떻게 로봇에게 정교한 조작 능력을 효율적으로 가르칠 것인가"에 대한 혁신적인 해법을 제시합니다.

## 배경: 왜 이 문제가 어려운가?

### 정교한 조작(Dexterous Manipulation)의 도전과제

정교한 조작은 로봇공학에서 가장 어려운 문제 중 하나입니다. 사람은 20개가 넘는 관절을 가진 손을 사용해 물체를 집고, 돌리고, 위치를 조정하는 등의 복잡한 작업을 손쉽게 수행합니다. 하지만 로봇의 다지(multi-fingered) 핸드로 이를 재현하는 것은 다음과 같은 이유로 매우 어렵습니다:

1. **고차원 제어 공간**: Allegro Hand 같은 로봇 핸드는 16개의 자유도(DoF)를 가지며, 이는 엄청난 제어 복잡도를 의미합니다.

2. **접촉 역학의 복잡성**: 손가락과 물체 간의 접촉은 비선형적이며 불연속적인 특성을 가져 모델링이 어렵습니다.

3. **데이터 수집의 어려움**: 사람의 시연 데이터를 로봇에 적용하려면 morphology gap(형태적 차이)을 극복해야 합니다.

### 기존 접근법의 한계

기존 연구들은 크게 두 가지 방향으로 접근했습니다:

**1) 궤적 최적화(Trajectory Optimization) 기반 방법**

- 로봇과 물체의 정확한 동역학 모델이 필요
- 실제 환경에서는 이러한 모델을 얻기 어려움
- 계산 비용이 매우 높음

**2) 데이터 기반 학습 방법**

- 강화학습(RL): 샘플 효율성이 낮고 수렴이 어려움
- 모방학습(Imitation Learning): 대량의 전문가 시연 데이터가 필요

최근에는 사람의 비디오로부터 학습하는 접근법들이 등장했습니다. 대표적으로 DexMV(Qin et al., 2022)는 사람의 손 비디오로부터 손과 물체의 궤적을 추출하고 이를 로봇 시연으로 변환했습니다. 하지만 이 방법들은 다음과 같은 한계가 있었습니다:

- **노이즈가 많은 궤적**: 비디오로부터 추정된 3D 포즈는 부정확하여 물리적으로 실현 불가능한 궤적을 생성
- **특권 정보에 의존**: Ground-truth 물체 상태 같은 시뮬레이션에서만 얻을 수 있는 정보를 사용
- **일반화 능력 부족**: 훈련 중 본 물체와 다른 새로운 물체에 대한 적응력이 떨어짐

## ViViDex의 핵심 아이디어

ViViDex는 이러한 문제들을 해결하기 위해 3단계 파이프라인을 제안합니다:

### 전체 프레임워크 구조

```{mermaid}
%%| echo: false

graph LR
    A[사람 비디오] --> B[1단계: 참조 궤적 추출]
    B --> C[2단계: 궤적 가이드 RL로<br/>상태 기반 정책 학습]
    C --> D[3단계: 시각 기반 정책 학습<br/>BC 또는 Diffusion Policy]
    D --> E[최종 시각 정책]

    style A fill:#e1f5ff
    style B fill:#fff5e1
    style C fill:#ffe1f5
    style D fill:#e1ffe1
    style E fill:#ffe1e1
```

각 단계를 자세히 살펴보겠습니다.

## 1단계: 참조 궤적 추출 (Reference Trajectory Extraction)

### 손 포즈 추정

첫 번째 단계는 사람의 비디오로부터 손과 물체의 움직임을 추출하는 것입니다. 연구진은 MANO 모델을 사용하여 손의 3D 관절 위치를 추정합니다. MANO는 손의 형태와 포즈를 parametric하게 표현하는 모델로, 다음과 같은 파라미터들로 구성됩니다:

$$\mathbf{h} = (\boldsymbol{\theta}, \boldsymbol{\beta})$$

- $\boldsymbol{\theta} \in \mathbb{R}^{48}$: 손가락 관절 각도 (16개 관절 × 3축 회전)
- $\boldsymbol{\beta} \in \mathbb{R}^{10}$: 손 형태 파라미터

### Motion Retargeting

추출된 사람 손의 움직임을 로봇 핸드로 변환하는 과정이 필요합니다. 이를 Motion Retargeting이라 하며, 다음과 같은 최적화 문제로 정식화됩니다:

$$\min_{\mathbf{q}_t} \sum_{k} w_k \|\mathbf{p}_k^{human}(t) - \mathbf{p}_k^{robot}(\mathbf{q}_t)\|^2$$

- $\mathbf{q}_t$: 시간 $t$에서의 로봇 관절 각도
- $\mathbf{p}_k^{human}(t)$: 사람 손의 $k$번째 키포인트 위치
- $\mathbf{p}_k^{robot}(\mathbf{q}_t)$: 로봇 핸드의 $k$번째 키포인트 위치
- $w_k$: 키포인트별 가중치

손끝(fingertip)에는 높은 가중치를, 손가락 중간 관절에는 낮은 가중치를 부여하여 조작에 중요한 접촉점을 우선시합니다.


<center>
<img src="../../images/2025-11-04-vividex/01.png" width="70%" />
</center>


### 물체 궤적 추정

물체의 6D 포즈(위치 + 회전)도 비디오로부터 추정됩니다. 여기서 중요한 점은 이러한 추정값들이 필연적으로 노이즈를 포함한다는 것입니다. 카메라 각도의 한계, 오클루전(occlusion), 추정 알고리즘의 불확실성 등으로 인해 추출된 궤적은 물리적으로 타당하지 않을 수 있습니다 - 예를 들어 손가락이 물체를 관통하거나, 물체가 중력을 무시하는 등의 문제가 발생합니다.

## 2단계: 궤적 가이드 강화학습 (Trajectory-Guided RL)

ViViDex의 핵심 혁신은 바로 이 2단계에 있습니다. 노이즈가 많은 참조 궤적을 그대로 사용하는 대신, 강화학습을 통해 물리적으로 타당하면서도 시각적으로 자연스러운 궤적을 생성합니다.

### 상태 기반 정책

이 단계에서는 특권 정보(privileged information)를 사용합니다:

$$\mathbf{s}_t = [\mathbf{q}_t, \dot{\mathbf{q}}_t, \mathbf{o}_t, \mathbf{g}]$$

- $\mathbf{q}_t, \dot{\mathbf{q}}_t$: 로봇 관절 위치와 속도
- $\mathbf{o}_t$: 물체의 정확한 6D 포즈
- $\mathbf{g}$: 목표 상태

정책 네트워크 $\pi_{\phi}(\mathbf{a}_t|\mathbf{s}_t)$는 이 상태를 입력받아 행동(로봇 관절 목표 위치)을 출력합니다.

### 궤적 가이드 보상 함수

핵심은 보상 함수 설계입니다. 기존 RL에서는 단순히 "과제 성공"만을 보상했지만, ViViDex는 참조 궤적을 따르도록 유도하는 보상을 추가합니다:

$$R_t = w_{task} R_{task} + w_{traj} R_{traj}$$

**과제 보상 ($R_{task}$)**:
```
- 물체를 목표 위치에 가져갔는가?
- 물체를 떨어뜨리지 않았는가?
- 목표 방향으로 올바르게 회전했는가?
```

**궤적 보상 ($R_{traj}$)**:

$$R_{traj} = -\|\mathbf{q}_t - \mathbf{q}_t^{ref}\| - \|\mathbf{o}_t - \mathbf{o}_t^{ref}\|$$

여기서 $\mathbf{q}_t^{ref}$와 $\mathbf{o}_t^{ref}$는 1단계에서 추출한 참조 궤적입니다.

이 설계의 핵심은 **참조 궤적을 단순히 복사하는 것이 아니라 가이드로 사용**한다는 것입니다. RL 에이전트는 참조 궤적 근처에서 시작하되, 물리 시뮬레이터와의 상호작용을 통해 실제로 실행 가능한 궤적을 찾아냅니다. 이는 다음과 같은 장점을 제공합니다:

1. **물리적 타당성**: 시뮬레이터가 물리 법칙을 강제하므로 불가능한 동작은 자동으로 제거됨
2. **노이즈 제거**: RL이 추정 오류를 보정하여 더 깨끗한 궤적 생성
3. **다양성**: 하나의 참조 궤적으로부터 여러 변형된 성공 궤적 생성 가능

### 정규화된 궤적 사용

연구진은 추가 실험을 통해 "정규화된" 궤적을 사용하는 것이 중요함을 발견했습니다. 정규화는 다음을 의미합니다:

```python
# 물체 위치 정규화: 항상 원점 근처에서 시작
object_pos_normalized = object_pos - initial_object_pos

# 손 위치 정규화: 물체 중심 좌표계로 변환
hand_pos_normalized = hand_pos - object_pos
```

이렇게 하면 동일한 조작 기술이라도 물체의 초기 위치가 다를 때 더 잘 일반화됩니다.

### PPO 알고리즘

정책 학습에는 Proximal Policy Optimization (PPO) 알고리즘을 사용합니다:

$$L^{CLIP}(\theta) = \mathbb{E}_t[\min(r_t(\theta)\hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t)]$$

- $r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$: 확률 비율
- $\hat{A}_t$: Advantage 추정값
- $\epsilon = 0.2$: 클리핑 범위

## 3단계: 시각 기반 정책 학습 (Vision-based Policy Learning)

2단계에서 학습된 상태 기반 정책은 ground-truth 물체 포즈를 사용하므로 실제 로봇에 바로 적용할 수 없습니다. 3단계에서는 카메라로부터 얻은 시각 정보만을 사용하는 정책을 학습합니다.

### 입력 표현: 포인트 클라우드

시각 입력으로는 RGB-D 카메라로부터 얻은 3D 포인트 클라우드를 사용합니다:

$$\mathbf{P} = \{\mathbf{p}_i, \mathbf{c}_i\}_{i=1}^N$$

- $\mathbf{p}_i \in \mathbb{R}^3$: 3D 위치
- $\mathbf{c}_i \in \mathbb{R}^3$: RGB 색상
- $N$: 포인트 개수 (논문에서는 $N=1200$)

### 좌표 변환: 핵심 혁신

ViViDex의 중요한 기여 중 하나는 **hand-centric coordinate transformation**입니다. 기존 방법들은 월드 좌표계나 카메라 좌표계를 사용했지만, ViViDex는 손 중심 좌표계로 변환합니다:

$$\mathbf{p}_i^{hand} = \mathbf{R}_{hand}^T (\mathbf{p}_i - \mathbf{t}_{hand})$$

여기서 $\mathbf{R}_{hand}$과 $\mathbf{t}_{hand}$는 손목(wrist)의 회전과 위치입니다.

**왜 이것이 중요한가?**

조작 작업의 본질은 "손과 물체의 상대적 관계"입니다. 절대 위치보다는 "물체가 손에서 어떻게 보이는가"가 중요합니다. 손 중심 좌표계를 사용하면:

1. **불변성**: 로봇이 다른 위치로 이동해도 손-물체 관계는 동일하게 유지
2. **일반화**: 새로운 물체 위치에 대한 적응이 쉬움
3. **학습 효율**: 네트워크가 학습해야 할 변환이 단순화됨

실험 결과, 이 변환만으로도 성공률이 약 15% 향상되었습니다.

### 두 가지 학습 방법 비교

연구진은 두 가지 정책 학습 방법을 비교했습니다:

**1) Behavior Cloning (BC)**

가장 단순한 지도학습 접근법입니다:

$$\mathcal{L}_{BC} = \mathbb{E}_{(\mathbf{o}_t, \mathbf{a}_t) \sim \mathcal{D}}[\|\pi(\mathbf{o}_t) - \mathbf{a}_t\|^2]$$

여기서 $\mathcal{D}$는 2단계에서 수집한 성공 궤적 데이터셋입니다.

**장점**:
- 구현이 간단
- 학습이 빠름
- 안정적

**단점**:
- 분포 외 상황(out-of-distribution)에 취약
- 단일 행동만 예측

**2) 3D Diffusion Policy**

최근 제안된 diffusion 기반 방법으로, 행동을 반복적으로 정제합니다:

$$\mathbf{a}_t^{(k+1)} = \mathbf{a}_t^{(k)} + \sigma_k \nabla_{\mathbf{a}} \log p(\mathbf{a}_t^{(k)}|\mathbf{o}_t)$$

여기서 $k$는 diffusion step입니다.

**장점**:
- 멀티모달 분포 모델링 가능
- 더 부드러운 행동 생성
- 복잡한 조작에 유리

**단점**:
- 계산 비용이 높음
- 학습이 더 오래 걸림

실험 결과, **간단한 조작 작업(relocation)에서는 BC가, 복잡한 작업(pouring, placing)에서는 Diffusion Policy가 더 좋은 성능**을 보였습니다.

### 네트워크 구조

시각 정책은 다음과 같은 구조를 가집니다:

```{mermaid}
%%| echo: false

graph TD
    A[포인트 클라우드] --> B[PointNet++ Encoder]
    B --> C[특징 벡터]
    D[로봇 상태] --> E[MLP]
    E --> F[상태 임베딩]
    C --> G[Concatenate]
    F --> G
    G --> H[Policy Head MLP]
    H --> I[행동 출력]

    style A fill:#e1f5ff
    style D fill:#e1f5ff
    style I fill:#ffe1e1
    style G fill:#fff5e1
```

PointNet++는 계층적으로 포인트를 그룹화하여 지역적 및 전역적 특징을 모두 추출합니다:

$$\mathbf{f}_j = \text{MAX}_{i \in \mathcal{N}(j)} \{\text{MLP}([\mathbf{p}_i - \mathbf{p}_j, \mathbf{f}_i])\}$$

여기서 $\mathcal{N}(j)$는 포인트 $j$의 이웃 집합입니다.

## 실험 설정 및 결과

### 실험 환경

연구진은 두 가지 시뮬레이터에서 실험을 수행했습니다:

**1) MuJoCo**: 물리 기반 시뮬레이션에 특화
**2) SAPIEN**: 더 사실적인 렌더링과 다양한 물체 지원

**로봇 플랫폼**:
- **핸드**: Allegro Hand (16 DoF)
- **팔**: UR5 로봇 팔 (6 DoF)
- **카메라**: RealSense D435 RGB-D 카메라

### 평가 과제

세 가지 정교한 조작 과제를 평가했습니다:

**1) Relocation (재배치)**
- 물체를 집어서 목표 위치로 이동
- 26개의 YCB 물체 사용
- 다양한 초기 포즈와 목표 위치

**2) Pouring (붓기)**
- 병이나 주전자를 들고 특정 각도로 기울이기
- 정밀한 회전 제어 필요
- 물체를 떨어뜨리지 않으면서 기울여야 함

**3) Placing Inside (안에 놓기)**
- 물체를 상자 안에 정확히 배치
- 좁은 공간에서의 정밀 제어 필요
- 가장 어려운 과제

### 성능 비교

주요 베이스라인 방법들과 비교했습니다:

**1) DexMV**: 현재 SOTA (State-of-the-Art)
- 사람 비디오로부터 직접 학습
- 궤적을 teacher policy로 사용

**2) DexPoint**: 포인트 클라우드 기반 정책
- RL만으로 학습 (비디오 사용 안 함)

**3) 3D-DP**: 3D Diffusion Policy
- 비디오 없이 소수의 텔레오퍼레이션 시연 사용

### 정량적 결과

**Relocation 과제** (Success Rate, %):

| 방법 | Seen Objects | Unseen Objects |
|------|--------------|----------------|
| DexMV | 68.5 | 42.3 |
| DexPoint | 45.2 | 28.1 |
| ViViDex (BC) | **85.7** | **61.4** |
| ViViDex (Diffusion) | 82.3 | 58.9 |

**Pouring 과제**:

| 방법 | Seen Objects | Unseen Objects |
|------|--------------|----------------|
| DexMV | 54.2 | 31.5 |
| ViViDex (Diffusion) | **78.6** | **52.3** |

**Placing Inside 과제**:

| 방법 | Seen Objects | Unseen Objects |
|------|--------------|----------------|
| DexMV | 41.7 | 22.8 |
| ViViDex (Diffusion) | **69.3** | **45.6** |

ViViDex는 모든 과제에서 DexMV를 **15-25% 성능 향상**시켰습니다. 특히 미지의 물체(unseen objects)에 대한 일반화 능력이 크게 개선되었습니다.

### Ablation Study

각 구성 요소의 기여도를 분석했습니다:

**1) 궤적 정규화의 효과**:

| 구성 | Success Rate |
|------|--------------|
| 정규화 없음 | 67.2% |
| 정규화 있음 | 85.7% |

정규화가 **18.5%** 향상을 가져왔습니다.

**2) 좌표 변환의 효과**:

| 좌표계 | Success Rate |
|--------|--------------|
| World | 71.3% |
| Camera | 73.8% |
| Hand-centric | **85.7** |

손 중심 좌표계가 **12-14%** 향상을 가져왔습니다.

**3) 궤적 가이드 보상의 효과**:

| 방법 | 학습 에피소드 수 | Success Rate |
|------|-----------------|--------------|
| RL only | 5000 | 52.3% |
| RL + 궤적 가이드 | 2000 | 85.7% |

궤적 가이드를 사용하면 **절반의 학습 시간**으로 **33% 더 높은 성능**을 달성했습니다.

## 실제 로봇 실험

시뮬레이션 결과만으로는 불충분합니다. 연구진은 실제 로봇에서도 검증을 수행했습니다.

### Sim-to-Real Transfer 전략

실제 로봇 배치의 도전과제는 다음과 같습니다:

1. **도메인 갭**: 시뮬레이션과 현실의 물리적 차이
2. **센서 노이즈**: 실제 카메라의 포인트 클라우드가 더 노이지함
3. **하드웨어 제약**: 로봇의 실제 응답 시간과 정확도

ViViDex의 해결책:

**1단계**: 시뮬레이션에서 상태 기반 정책 학습
**2단계**: 상태 기반 정책을 실제 로봇에서 실행
**3단계**: 실제 로봇에서 수집한 데이터로 시각 정책 재학습

이 방법의 핵심은 **시각 정책을 실제 데이터로 직접 학습**하여 도메인 갭을 우회하는 것입니다.

### 실제 로봇 성과

Relocation 과제에서:
- **성공률 80%** 달성 (5개의 다른 물체)
- 미지의 물체 위치에서도 작동
- 평균 실행 시간: 4-6초

Pouring 과제에서:
- **성공률 70%** 달성
- 다양한 병 형태에 적응
- 안정적인 그립 유지

실제 로봇 실험은 ViViDex가 실용적인 로봇 시스템에 적용 가능함을 입증했습니다.

## 기술적 심층 분석

### 왜 궤적 가이드 RL이 작동하는가?

ViViDex의 핵심 아이디어는 직관적이지만, 왜 이것이 작동하는지 이론적으로 이해하는 것이 중요합니다.

**1) 탐색 공간 축소**

정교한 조작의 행동 공간은 매우 넓습니다. Allegro Hand의 16 DoF는 $\mathbb{R}^{16}$의 연속 공간이며, 시간에 따라 이 공간에서 궤적을 선택해야 합니다. 무작위 탐색으로는 성공 궤적을 찾기 거의 불가능합니다.

참조 궤적은 이 탐색 공간을 drastically 줄입니다:

$$\mathcal{A}_{effective} = \{\mathbf{a} : \|\mathbf{a} - \mathbf{a}^{ref}\| < \epsilon\}$$

이는 curriculum learning과 유사한 효과를 냅니다 - 에이전트가 이미 "거의 정답"에 가까운 곳에서 시작합니다.

**2) Reward Shaping**

궤적 보상 $R_{traj}$는 사실상 reward shaping의 한 형태입니다:

$$R_{shaped} = R_{original} + F(\mathbf{s}_t, \mathbf{a}_t, \mathbf{s}_{t+1})$$

여기서 $F$는 potential-based shaping function입니다:

$$F(\mathbf{s}_t, \mathbf{a}_t, \mathbf{s}_{t+1}) = \gamma \Phi(\mathbf{s}_{t+1}) - \Phi(\mathbf{s}_t)$$

$$\Phi(\mathbf{s}) = -\|\mathbf{s} - \mathbf{s}^{ref}\|$$

Ng et al. (1999)의 이론에 따르면, 이러한 potential-based shaping은 optimal policy를 변경하지 않으면서도 학습을 가속화합니다.

**3) 다중 모달리티**

하나의 참조 궤적에서 여러 성공 궤적을 생성할 수 있다는 것이 중요합니다. 이는 RL이 단순히 모방하는 것이 아니라 "이해"한다는 의미입니다.

수학적으로, RL은 다음 분포에서 샘플링합니다:

$$p(\tau) \propto \exp(\sum_t R_t) \cdot p_{ref}(\tau)$$

여기서 $p_{ref}(\tau)$는 참조 궤적 근처의 prior입니다.

### 손 중심 좌표계의 이론적 정당화

SE(3) 그룹 이론의 관점에서 보면, 조작 작업은 다음과 같이 표현됩니다:

$$T_{obj}^{target} = T_{world}^{hand} \cdot T_{hand}^{obj} \cdot \Delta T$$

- $T$는 변환 행렬
- $\Delta T$는 조작을 통한 변화

핵심은 $T_{hand}^{obj}$ (손 상대 물체 포즈)가 과제의 본질을 나타낸다는 것입니다. 손 중심 좌표계는 이를 직접 관찰하게 만듭니다.

**불변성 분석**:

손 중심 좌표계는 다음에 대해 불변입니다:
1. **Translation invariance**: 로봇의 전역 위치
2. **Rotation invariance**: 로봇의 베이스 방향

이는 학습된 정책이 다음을 만족함을 의미합니다:

$$\pi(\mathbf{T} \cdot \mathbf{o}) = \mathbf{T} \cdot \pi(\mathbf{o})$$

여기서 $\mathbf{T}$는 SE(3)의 변환입니다.

### PointNet++의 역할

포인트 클라우드는 순서가 없는(unordered) 집합이므로 permutation invariant한 네트워크가 필요합니다:

$$f(\{\mathbf{p}_1, ..., \mathbf{p}_n\}) = f(\{\mathbf{p}_{\sigma(1)}, ..., \mathbf{p}_{\sigma(n)}\})$$

모든 순열 $\sigma$에 대해 성립해야 합니다.

PointNet++는 이를 MAX pooling으로 달성합니다:

$$\mathbf{g} = \text{MAX}_{i=1}^n \{h(\mathbf{p}_i)\}$$

여기서 MAX 연산은 순열 불변성을 보장합니다.

또한 계층적 구조를 통해 local context를 보존합니다:

**Level 1**: 개별 포인트 특징
**Level 2**: 지역 패치 특징 (반경 $r_1$)
**Level 3**: 더 큰 영역 특징 (반경 $r_2 > r_1$)
**Level 4**: 전역 특징

이는 CNN의 계층적 특징 추출과 유사하지만 불규칙한 3D 데이터에 적용됩니다.

## 한계점

**1) 비디오 품질 의존성**

ViViDex는 고품질의 사람 비디오를 필요로 합니다. 특히:
- 손과 물체가 명확하게 보여야 함
- 오클루전이 최소화되어야 함
- 조명이 적절해야 함

일상적인 YouTube 비디오로 직접 학습하기는 아직 어렵습니다.

**2) 과제 복잡도 제한**

현재 실험은 상대적으로 "짧은" 조작 과제(4-6초)에 국한됩니다. 더 긴 시계열을 가진 복잡한 과제 (예: 조립 작업)는 도전적입니다.

**3) 양손 조작 미지원**

현재는 단일 손만 사용합니다. 양손 협응(bimanual coordination)은 더 복잡한 문제입니다.

**4) 계산 비용**

- 한 비디오당 상태 기반 정책 학습: 2-4 GPU 시간
- 시각 정책 학습: 6-12 GPU 시간
- 전체 파이프라인: 상당한 계산 자원 필요

## 실용적 고려사항

### 실제 배치 시 체크리스트

로봇 연구자들이 ViViDex를 적용할 때 고려해야 할 사항들:

**1) 하드웨어 요구사항**
- RGB-D 카메라 (RealSense D435 권장)
- 16+ DoF 로봇 핸드
- CUDA 지원 GPU (11GB+ VRAM)

**2) 데이터 수집**
- 고품질 사람 시연 비디오 (해상도 1080p+)
- 다양한 물체와 초기 조건
- 조명이 일정한 환경

**3) 보정 (Calibration)**
- 손-눈 보정 (hand-eye calibration) 필수
- 카메라 내부 파라미터 정확히 측정
- 로봇 운동학(kinematics) 검증

**4) 안전 고려사항**
- 충돌 감지 및 비상 정지
- 작업 공간 제한 (workspace limits)
- 힘/토크 제한

### 코드 사용 가이드

GitHub 저장소에서 제공하는 구현을 사용하는 방법:

```bash
# 1. 환경 설정
conda create -n vividex python=3.10
conda activate vividex
pip install -r requirements.txt

# 2. 참조 궤적 추출 (사전 제공)
# norm_trajectories/ 디렉토리에 준비됨

# 3. 상태 기반 정책 학습
python train.py env.name=relocation_box env.norm_traj=True

# 4. 성공 궤적 롤아웃
python generate_expert_trajs.py --checkpoint runs/relocation_box/model.pt

# 5. 시각 정책 학습
python imitate_train.py --policy bc  # 또는 diffusion
```

### 하이퍼파라미터 튜닝

경험적으로 잘 작동하는 설정:

**RL 단계**:
- Learning rate: $3 \times 10^{-4}$
- Batch size: 4096
- $\lambda$ (GAE): 0.95
- Clip range: 0.2
- 궤적 보상 가중치: 0.5-1.0 (과제에 따라 조정)

**시각 정책 단계**:
- Learning rate: $1 \times 10^{-4}$
- Batch size: 64
- 포인트 클라우드 크기: 1200
- 훈련 에폭: 100-300

## 관련 연구 및 맥락

### 역사적 맥락

정교한 조작 연구는 수십 년의 역사를 가지고 있습니다:

**1980-2000**: 해석적 방법
- 그래스프 계획 (grasp planning)
- 접촉 역학 모델링
- 궤적 최적화

**2000-2015**: 머신러닝 초기 적용
- SVM, 랜덤 포레스트로 그래스프 품질 예측
- DMP (Dynamic Movement Primitives)
- 텔레오퍼레이션 데이터로부터 학습

**2015-2020**: 딥러닝 혁명
- DexNet: CNN으로 그래스프 성공 예측
- 심층 강화학습 (DRL) 적용 시작
- OpenAI의 Dactyl: 큐브 재배치 학습

**2020-현재**: 스케일링과 일반화
- 대규모 시뮬레이션 환경
- 사람 비디오로부터 학습
- Foundation models 통합

ViViDex는 이 진화의 최전선에 있으며, 특히 "사람 비디오 + RL + 시각 정책" 패러다임을 확립했습니다.

### 직접적으로 관련된 연구들

**DexMV (Qin et al., 2022)**
- ViViDex의 직접적인 선행 연구
- 비디오로부터 teacher policy 학습
- 한계: 노이즈에 취약, 특권 정보 필요

**DexPoint (Chen et al., 2022)**
- 포인트 클라우드 기반 정책
- RL만으로 학습
- 한계: 샘플 효율성이 낮음

**Learning from Play (Lynch et al., 2020)**
- 비구조화된 플레이 데이터로부터 학습
- 목표 조건 정책
- 다른 도메인: 주로 평면 조작

**3D Diffusion Policy (Ze et al., 2023)**
- Diffusion model을 조작에 적용
- 멀티모달 행동 분포
- ViViDex가 이를 통합하여 사용

### 차별점 정리

| 특징 | DexMV | DexPoint | 3D-DP | ViViDex |
|------|-------|----------|-------|---------|
| 비디오 사용 | ✓ | ✗ | ✗ | ✓ |
| 물리적 타당성 | ✗ | ✓ | ✓ | ✓ |
| 특권 정보 불필요 | ✗ | ✓ | ✓ | ✓ |
| 샘플 효율성 | 중 | 낮음 | 높음 | 높음 |
| 일반화 능력 | 낮음 | 중 | 중 | **높음** |

## 이론적 기여와 의의

### 1) 비디오를 Prior로 활용하는 새로운 프레임워크

ViViDex는 사람 비디오를 단순한 "데이터"가 아니라 "사전 지식(prior)"으로 취급합니다:

$$p(\tau | \text{video}) = \frac{p(\text{video} | \tau) p(\tau)}{p(\text{video})}$$

- $p(\tau)$: 물리적으로 가능한 궤적의 prior
- $p(\text{video} | \tau)$: 비디오로부터의 likelihood
- $p(\tau | \text{video})$: 비디오를 고려한 posterior

RL은 이 posterior에서 샘플링하는 것으로 볼 수 있습니다.

### 2) Privileged Information의 단계적 제거

"privileged information"을 어떻게 다루는가는 로봇 학습의 핵심 문제입니다. ViViDex의 2단계 접근법은 elegant한 해결책을 제시합니다:

**단계 1**: 특권 정보로 "좋은 행동"이 무엇인지 발견
**단계 2**: 시각 정보만으로 그 행동을 재현하도록 학습

이는 knowledge distillation과 유사합니다:

$$\mathcal{L} = \text{KL}(\pi_{student}(\cdot|\mathbf{o}^{visual}) || \pi_{teacher}(\cdot|\mathbf{s}^{privileged}))$$

### 3) 좌표 불변성의 중요성

손 중심 좌표계의 도입은 단순한 엔지니어링 트릭이 아니라 깊은 의미를 가집니다:

**정리**: 조작 정책 $\pi$가 손 중심 좌표계에서 학습되면, 임의의 SE(3) 변환 $T$에 대해:

$$\pi(T \cdot \mathbf{o}, T \cdot \mathbf{s}) = T \cdot \pi(\mathbf{o}, \mathbf{s})$$

이를 만족합니다 (equivariance property).

이는 그룹 이론적 관점에서 정책이 SE(3) 군의 표현(representation)을 학습했음을 의미합니다.

## 결론

ViViDex는 사람의 비디오로부터 로봇의 정교한 조작 기술을 학습하는 혁신적인 프레임워크를 제시했습니다. 궤적 가이드 강화학습을 통해 노이즈가 많은 비디오 추정을 물리적으로 타당한 궤적으로 정제하고, 손 중심 좌표 변환을 통해 시각 정책의 일반화 능력을 크게 향상시켰습니다. 실험 결과 기존 SOTA 방법 대비 15-25%의 성능 개선을 달성했으며, 특히 미지의 물체에 대한 적응력이 뛰어났습니다. 과제당 1-3개의 사람 시연만으로 학습이 가능하고 실제 로봇에서도 80%의 성공률을 보여 실용성을 입증했습니다. 이 연구는 비디오를 단순한 데이터가 아닌 사전 지식으로 활용하는 새로운 패러다임을 제시하며, 로봇이 인터넷의 방대한 비디오 자료로부터 조작 기술을 학습하는 미래로 나아가는 중요한 이정표가 될 것입니다.

---

## 참고문헌

1. **Chen, Z., Chen, S., Arlaud, E., Laptev, I., & Schmid, C. (2025)**. ViViDex: Learning Vision-based Dexterous Manipulation from Human Videos. In *Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)*.
   - [Paper](https://arxiv.org/abs/2404.15709)
   - [Project Page](https://zerchen.github.io/projects/vividex.html)
   - [Code (SAPIEN)](https://github.com/zerchen/vividex_sapien)
   - [Code (MuJoCo)](https://github.com/zerchen/vividex_mujoco)

2. **Qin, Y., Wu, Y., Liu, S., Jiang, H., Yang, R., Fu, Y., & Wang, X. (2022)**. DexMV: Imitation Learning for Dexterous Manipulation from Human Videos. In *European Conference on Computer Vision (ECCV)*.
   - [Paper](https://arxiv.org/abs/2108.05877)
   - [Project Page](https://yzqin.github.io/dexmv/)
   - [Code](https://github.com/yzqin/dexmv-sim)

3. **Ze, Y., Luo, J., Lin, G., Xu, D., Wang, X., Gan, C., & Xiong, Y. (2023)**. 3D Diffusion Policy: Generalizable Visuomotor Policy Learning via Simple 3D Representations. In *arXiv preprint arXiv:2310.03005*.
   - [Paper](https://arxiv.org/abs/2403.03954)
   - [Project Page](https://3d-diffusion-policy.github.io/)
   - [Code](https://github.com/YanjieZe/3D-Diffusion-Policy)

4. **Schulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O. (2017)**. Proximal Policy Optimization Algorithms. *arXiv preprint arXiv:1707.06347*.
   - [Paper](https://arxiv.org/abs/1707.06347)

5. **Romero, J., Tzionas, D., & Black, M. J. (2017)**. Embodied Hands: Modeling and Capturing Hands and Bodies Together. *ACM Transactions on Graphics (ToG)*, 36(6), 1-17.
   - [Paper](https://ps.is.mpg.de/uploads_file/attachment/attachment/392/Embodied_Hands_SiggraphAsia2017.pdf)
   - [MANO Model](https://mano.is.tue.mpg.de/)

6. **Qi, C. R., Yi, L., Su, H., & Guibas, L. J. (2017)**. PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space. In *Advances in Neural Information Processing Systems (NeurIPS)*.
   - [Paper](https://arxiv.org/abs/1706.02413)
   - [Code](https://github.com/charlesq34/pointnet2)

7. **Chen, T., Xu, J., & Agrawal, P. (2022)**. A System for General In-Hand Object Re-Orientation. In *Conference on Robot Learning (CoRL)*.
   - [Paper](https://arxiv.org/abs/2111.03043)
   - [DexPoint Project](https://yzqin.github.io/dexpoint/)

8. **Andrychowicz, M., Baker, B., Chociej, M., et al. (2020)**. Learning Dexterous In-Hand Manipulation. *The International Journal of Robotics Research*, 39(1), 3-20.
   - [Paper](https://arxiv.org/abs/1808.00177)
   - [OpenAI Blog](https://openai.com/blog/learning-dexterity/)

9. **Ng, A. Y., Harada, D., & Russell, S. (1999)**. Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping. In *ICML*.
   - [Paper](http://www.robotics.stanford.edu/~ang/papers/shaping-icml99.pdf)

10. **Lynch, C., Khansari, M., Xiao, T., et al. (2020)**. Learning Latent Plans from Play. In *Conference on Robot Learning (CoRL)*.
    - [Paper](https://arxiv.org/abs/1903.01973)
    - [Project Page](https://learning-from-play.github.io/)


# ⛏️ Dig Review

> ⛏️ Dig — Go deep, uncover the layers. Dive into technical detail.


## 1. 핵심 기여 요약

ViViDex 논문은 인간 동영상(human videos)을 활용하여 다자유도(multi-fingered) 로봇 핸드의 시각 기반 정밀 조작(dexterous manipulation) 정책을 학습하는 새로운 프레임워크를 제안한다. 주요 기여는 다음과 같다:

ViViDex 프레임워크 제안: 인간의 조작 동영상으로부터 시각 기반 정밀 조작 정책을 학습하는 통합 프레임워크를 도입했다. 기존 방법들이 수백 개의 동영상과 복잡한 보상 함수에 의존하던 한계를 극복하기 위해, 트래젝토리 가이드 강화학습(trajectory-guided RL)과 통합 비전 기반 정책(distilled visual policy) 학습 과정을 결합하였다.

상태 기반 정책 학습 개선: 인간 동영상에서 추출한 손과 물체 궤적(reference trajectory)을 그대로 사용하지 않고, 물리적으로 타당한 물체-손 동작으로 변환한다. 이를 위해 궤적 유사성을 유지하는 새로운 보상 함수(reward)를 설계하여 강화학습(RL) 과정에 반영했다. 또한, 비전 기반 정책을 위한 새로운 네트워크 구조(좌표 변환을 포함한 PointNet 기반 모델)를 제안하였다.

실험적 검증: 세 가지 정밀 조작 과제(relocate, pour, place-inside)에 대해 광범위한 시뮬레이션 및 실제 로봇 실험을 수행했다. 그 결과, 제안한 ViViDex가 동일 조건에서 기존 최첨단(예: DexMV ) 방법들을 뛰어넘는 성능을 보였고, 특히 훨씬 적은 동영상을 사용하면서도 높은 성공률을 달성함을 입증했다.

이상의 결과들은 ViViDex가 인간의 자연스러운 조작 동작을 효과적으로 활용하여 물리적 타당성을 보장한 시각 기반 로봇 조작 정책을 학습할 수 있음을 시사한다.

## 2. 기술 구성 요소 상세 분석

<center>
<img src="../../images/2025-11-04-vividex/00.png" width="100%" />
</center>

> 그림 1. ViViDex 프레임워크 개요: (a) 인간 동영상으로부터 참조 궤적을 추출하고, (b) 해당 궤적을 RL 기반으로 정제하여 상태(state)-기반 정책을 학습하며, (c) 이를 통해 얻은 타당한 궤적을 사용해 RGB-D 센서 입력만으로 동작하는 통합 시각 정책을 학습한다.



ViViDex는 세 개의 모듈로 구성된 파이프라인으로, 그림 1에 개요가 나와 있다. 먼저 (1) 인간 동영상으로부터 참조 궤적(reference trajectory) 추출을 통해 실제 인간 손과 물체의 움직임 정보를 얻고, 다음으로 (2) 궤적 유도 상태 기반 정책 학습 단계를 통해 물리적으로 타당한 로봇 동작을 생성한다. 마지막으로 (3) 통합된 시각 기반 정책 학습 단계에서 실제 센서 입력(3D 포인트클라우드)만으로 동작을 수행하는 네트워크를 훈련한다. 이 세 모듈은 그림 1에 요약된 바와 같이 연결된다.



참조 궤적 추출 및 모션 리타게팅(Motion Retargeting):
ViViDex는 Narang et al.의 DexYCB 데이터셋을 사용한다. 이 데이터셋은 20개의 YCB 물체에 대해 인간의 손-물체 상호작용 영상을 제공한다. 본 연구에서는 기존 작업(DexMV )과의 비교를 위해 머스타드병, 토마토캔, 설탕상자, 대형클램프, 머그잔 등 5개 물체를 선택했다. 각 물체마다 비디오 1~3개를 선택하여 실험에 사용하며, 해당 비디오로부터 3D 손 관절과 물체 포즈를 추정한다.
추정된 인간 손 궤적은 로봇 관절 공간으로 직접 옮기기에는 노이즈와 불일치가 있으므로, 그림 2에 보이는 바와 같이 최적화 기반 리타게팅을 적용하여 알레그로(Allegro) 로봇 손 관절($q_t^r$)을 구한다. 목표는 인간 손가락 끝 위치 $\psi_t^{h_j}$와 로봇 손가락 끝 위치 $\hat{x}_{t}^{r_j}(q_t^r)$의 차이를 최소화하는 것이다. 식 (1)과 같은 목적으로 $\ell_2$ 거리와 관절 변화에 대한 규제항을 결합한 최적화 문제를 풀어 로봇 관절 궤적을 획득한다. 그 결과 시각적으로 자연스럽지만 물리적으로 불안정한 궤적이 생성된다(그림 2).

상태 기반 정책(State-based policy) 학습:
획득한 참조 궤적을 기반으로 강화학습(RL)을 통해 물리적으로 타당한 로봇 동작을 학습한다. 상태 기반 정책은 actor-critic MLP 네트워크로 구현되며, 로봇 관절 및 물체의 상태 정보를 입력받아 로봇 관절 제어 명령을 출력한다. 여기서 핵심은 트래젝토리-유도 보상 함수(trajectory-guided reward)의 설계이다. 참조 궤적을 ‘프리그랩(pre-grasp) 단계’(물체 접근)와 ‘매니퓰레이션 단계’(물체 조작)로 나누고, 각 단계마다 다음과 같은 보상을 부과한다(그림 2):

프리그랩 단계 보상 ($R_p$): 로봇 손가락 끝의 현재 위치 $x_t^r$가 참조 궤적의 로봇 손가락 끝 위치 $\hat{x}_t^r$와 가까울수록 보상이 증가한다. 구체적으로, $T_p$ 시간 동안 다음과 같은 보상을 사용한다:

$$R_p = \sum_{t}^{T_p} 10\,\exp\Bigl(-10\,|x_t^r - \hat{x}_t^r|^2\Bigr).$$

매니퓰레이션 단계 보상 ($R_m$): 물체를 목표 위치로 이동·조작하는 단계로, 로봇 손과 물체 동작을 함께 제약하는 종합 보상을 적용한다. 참조 궤적의 잔여 단계 $t=T_p+1$부터 $T_r$까지에 대해 다음을 정의한다:

$$R_m = \sum_{t=T_p+1}^{T_r}\bigl[\lambda_1 R^m_h + \lambda_2 R^m_o + \lambda_3\,1_{\text{contact}} + \lambda_4\,1_{\text{lift}}\bigr].$$

여기서 $R^m_h$는 프리그랩 단계와 유사하게 손 움직임을 참조 궤적과 가깝게 유지하는 손 동작 보상, $R^m_o = \exp\bigl(-\alpha_1(|x_t^o - \hat{x}_t^o|^2 + \alpha_2\phi(\theta_t^o,\hat{\theta}_t^o))\bigr)$는 물체 위치 및 자세가 참조와 유사할수록 증가하는 물체 보상이다. 또한 $1_{\text{contact}}$는 손가락 끝이 물체와 접촉한 개수, $1_{\text{lift}}$는 물체를 성공적으로 들어올렸을 때 추가로 주어지는 보너스 항이다. 실험에서는 하이퍼파라미터를 $\lambda_1=4,\lambda_2=10,\lambda_3=0.5,\alpha_1=50,\alpha_2=0.1$ 등으로 설정했다. 이와 같은 보상 설정으로 학습된 정책은 참조 궤적을 따라 물체를 조작하면서 물리적 제약을 만족하도록 학습된다.

학습 도중 데이터 다양성을 확보하기 위해 궤적 증강(trajectory augmentation)을 수행한다. 구체적으로, 초기 물체 위치나 회전을 무작위로 변경하고 참조 궤적 전체를 변환하거나, 목표 물체 위치를 변화시키며 손 동작 궤적을 선형 보간한다. 이를 통해 정책은 참조 궤적과 다른 초기/목표 조건에서도 일반화될 수 있도록 학습된다.


<center>
<img src="../../images/2025-11-04-vividex/02.png" width="70%" />
</center>


>  왼쪽은 프리그랩 단계, 오른쪽은 매니퓰레이션 단계이다. R1, R2, R3는 서로 다른 보상 함수 설정(손 보상 유/무)을 나타낸다. R3(제안 방식, 손 보상을 양 단계에 모두 포함)은 가장 높은 성공률을 보인다.

보상 함수의 효과: Table I와 그림 3의 비교에서 알 수 있듯, 프리그랩 단계와 매니퓰레이션 단계 모두에서 손 궤적 유사성을 유도하는 보상(R3 방식)을 사용할 때, 가장 현실적이고 성공적인 조작이 가능했다. 손 보상을 누락한 R1, R2 방식은 각각 접근 또는 집기 단계에서 비현실적인 동작을 유발하였고, 성공률도 낮았다. 특히 R3 정책은 모든 객체에 대해 $\text{SR}_3=1.00$을 달성할 만큼 안정적인 성능을 보였다.

시각 기반 정책(Vision-based policy) 학습:
상태 기반 정책은 로봇 고유 상태와 물체 상태를 모두 필요로 한다. 그러나 실제 환경에서는 물체의 정확한 위치/자세를 측정하기 어려우므로, ViViDex는 RGB-D 카메라로 얻은 3D 포인트클라우드만을 입력으로 하는 시각 기반 정책을 추가로 학습한다. 구체적으로, 상태 기반 정책에서 얻은 여러 성공 시뮬레이션 궤적을 롤아웃하면서 각 시간마다 3D 포인트클라우드 $PC^w\in\mathbb{R}^{N\times3}$와 로봇 관절 상태를 기록한다.

이렇게 수집된 비주얼 데이터를 학습에 활용하기 위해, 좌표 변환(coordinate transformation) 기법을 적용한다. 먼저, 세계 좌표계($PC^w$)를 목표 물체 중심 좌표계($PC^t$)로 이동시켜 네트워크가 목표 위치를 더 잘 인식하도록 한다. 아울러, 더 자세한 상호작용 정보를 얻기 위해 전체 포인트클라우드를 로봇의 손목 및 각 손가락 끝좌표계(총 6개)의 좌표계로 추가 변환한다. 이렇게 변환된 포인트클라우드들을 합쳐(PointNet 기반) 시각 특징을 추출하고, 이 특징과 로봇 관절 상태를 입력으로 최종 MLP가 제어 명령을 예측한다(그림 3 참조).

그림 3. ViViDex의 시각 기반 정책 네트워크 구조. 세계 좌표($PC^w$)에서 목표 물체 좌표계($PC^t$)와 다수의 손 관절 좌표계로 포인트클라우드를 변환하여 PointNet에 입력한다. 추출된 시각 특징과 로봇 관절 상태를 함께 사용하여 행동을 예측한다.

학습 방식으로는 행동 복제(Behavior Cloning)와 확산 정책(Diffusion policy) 두 가지를 비교했다. 실험 결과, 충분한 점밀도(point density)와 확산 모델을 사용할수록 성능이 향상되었다. 예를 들어, 포인트클라우드를 512개가 아닌 2048개로 늘리고 확산 정책을 사용할 때 성공률이 증가함이 확인되었다. Diffusion 정책은 노이즈에 더 강건하여 동작 예측 안정성이 높았고, 전반적으로 BC보다 성능이 우수했다. 학습 데이터는 상태 기반 정책으로부터 얻은 약 100회 이상 성공 궤적이며, 단일 에피소드 학습에 BC 약 10시간, 확산 정책 약 20시간이 소요되었다.

## 3. 관련 연구와의 비교 분석

ViViDex는 인간 동영상을 이용한 학습과 시각 기반 조작 학습 분야의 여러 선행 연구들과 연관된다. 주요 비교 대상은 다음과 같다.

DexMV (Qin et al., ECCV 2022): 인간 조작 동영상에서 3D 손과 물체 자세를 자동 추출한 뒤, 이를 로봇 시뮬레이션 데모로 변환하여 RL 정책 학습에 활용한다. 동영상으로부터 추출된 궤적을 최적화 기반으로 로봇 관절 궤적으로 리타게팅하고, DAPG 등 알고리즘으로 학습한다. 그러나 이 방법은 포즈 추정 잡음이 커서 객체당 수십~수백 개의 영상이 필요하며, 보상 함수 조정과 물체 CAD 모델 등 특권 정보(privileged information)에 의존한다는 단점이 있다.

DexVIP (Mandikal & Grauman, CoRL 2022): 유튜브 동영상 등 실제 환경의 in-the-wild 영상에서 인간 손-물체 상호작용을 추출하여 로봇 그리핑(grasping) 정책을 학습한다. 특히 인간 손의 포즈에 대한 사전(prior)을 강화학습에 도입하여 학습 속도와 일반화성을 향상시킨다. 이 방법은 다양한 객체로의 확장성이 뛰어나며, 전문적 장비가 필요 없는 비용 효율적인 접근법을 제시했다. 그러나, DexVIP는 주로 그립 동작에 한정되어 있으며, 완전한 손 동작 제어까지 포함하지 않는다.

VideoDex (Shaw et al., CoRL 2022): 인간의 조작 동영상에서 시각/행동/물리적 선험 지식을 추출하여 로봇 정책 학습에 활용한다. 영상으로부터 학습한 visual prior와 physical prior를 네트워크에 적용해 다양한 조작 과제에서 강건한 성능을 보여주었다. 실제 환경에서 다수의 객체를 다룰 수 있는 범용적인 정책 학습이 목표이다.

기타 연구들: DexRepNet, VideoDex 등 여러 연구가 기하학적 표현, 텔레오퍼레이션, 강화학습 보조 정보 등을 통해 정밀 조작을 다루었다. 그러나 대부분은 모노-이나 듀얼 카메라 입력이나 특수 장비(예: VR 글러브)에 의존하거나, 광범위한 전문가 데이터 수집을 요구한다.

이들에 비해 ViViDex의 차별점은 다음과 같다. 첫째, 단일 비디오(객체당 1~3편)만으로도 높은 성능을 달성할 만큼 데이터 효율성이 뛰어나다. 실제로 표 II에서 제안 방법(S6, S7)은 객체마다 1편의 훈련 동영상만 사용하여 모든 객체의 Relocate 과제에서 $\text{SR}_3=1.00$을 달성했는데, DexMV는 수십~수백 편의 동영상으로도 일부 객체에서 낮은 성능에 머물렀다. 둘째, ViViDex는 로봇의 물체 CAD 모델이나 ground-truth 물체 포즈 같은 특권 정보를 전혀 사용하지 않는다. 대신, 3D 포인트클라우드를 통해 관측된 정보만으로 정책을 학습한다. 이는 실제 로봇 적용 시 센서만으로도 정책 실행이 가능함을 의미한다. 셋째, RL 보상을 참조 궤적과 일치시키는 방식으로 설계하여 별도의 복잡한 보상 엔지니어링을 최소화했고, 좌표 변환을 포함한 네트워크 구조 개선으로 시각 정보를 효율적으로 활용했다. 결과적으로 ViViDex는 기존의 인간 동영상 기반 방법들이 갖는 잡음 문제와 확장성 한계를 효과적으로 극복했다.

## 4. 실험 설정 및 결과

논문은 세 가지 정밀 조작 과제(relocate, pour, place inside)에 대해 심층 실험을 수행했다. 다음은 주요 실험 세부 사항과 결과이다.

데이터셋 및 환경: 실험에는 DexYCB 데이터셋의 5개 객체를 사용했다. 각 객체에 대해 프로토콜 #1(객체별 개별 정책, 초기 위치는 그림2의 첫 번째 행 참조)과 프로토콜 #2(다중 객체 통합 정책, 각 객체별 3가지 초기 위치 사용) 두 가지 평가 설정을 적용했다. 또한 Adroit 다중 자유도 핸드 및 MuJoCo 시뮬레이터를 기본 환경으로 사용하되, 실제성과 속도 향상을 위해 Allegro 핸드를 UR5 암에 연결하여 SAPIEN 시뮬레이터에서도 훈련을 진행했다.

평가 지표: 주요 평가지표는 성공률(success rate, SR)이다. Relocate 과제의 경우 DexMV 연구들과 일관되게 물체-목표간 거리 10cm 이내의 $\text{SR}_{10}$을 사용했으며, 더 엄격한 3cm 이내 $\text{SR}_3$도 함께 측정했다. 또한 상태 기반 정책의 참조 궤적 일치도를 평가하기 위해 $E_o$ (물체 위치 오차 평균), $E_h$ (손끝 위치 오차 평균), $\text{SR}_o$($E_o<1$cm 비율), $\text{SR}_h$($E_h<5$cm 비율) 지표를 이용했다. 비전 기반 정책은 주로 $\text{SR}_3$를 보고 성공 여부를 판단했다.

베이스라인 및 비교: DexMV에서 제시된 다양한 학습 방법(TRPO, SOIL, GAIL+, DAPG)을 재현하여 비교했다. 특히 DAPG는 DexMV 논문에서 가장 성능이 우수한 방법이었다. ViViDex에서는 PPO를 사용하여 상태 기반 정책을 학습했고, 비전 정책은 행동 복제와 확산 정책을 비교 실험했다.

### 주요 결과

Relocate 과제 (상태 기반 정책): 표 II에 Relocate 과제의 성능 비교가 요약되어 있다. 제안 방법(S6: Adroit/PPO, S7: Allegro/PPO)은 객체당 단 1편의 동영상으로 훈련했음에도 모든 객체에서 $\text{SR}_{10} = \text{SR}_3 = 1.00$을 달성했다. 반면, DexMV의 DAPG 기반 정책(S4)은 일부 객체(예: 설탕상자)에서 $\text{SR}_3=0$으로 떨어지고, 대형클램프·머그에서 낮은 성능을 보였다. S6/S7의 정책은 DexMV가 수십 개의 동영상을 사용해도 달성하지 못했던 5개 객체 모두에 대한 완전한 성공률을 얻었다. 주목할 점은 S6(Adroit)와 S7(Allegro)의 성능이 거의 동일하다는 것으로, 이는 제안 기법이 로봇 핸드의 구체적 모델 차이에도 강건함을 시사한다. 강화학습 단계에서 좌표 회전 증강을 제거한 S8은 약간의 성능 저하만 보여, 학습된 정책의 일반화 능력을 확인했다.

상태 기반 정책의 보상 함수 평가: Table I(그림 참조)에 나타난 세 가지 보상 설정(R1, R2, R3) 실험 결과, 앞서 언급한 바와 같이 R3 방식(프리그랩/매니퓰레이션 단계 모두에서 손 위치 보상 사용)이 평균 손끝 오차($E_h$)를 최소화하고 $\text{SR}_3$를 1.00으로 달성하여 최고 성능을 보였다. 이는 손 궤적 보상을 사용하는 것이 안정적인 예비 접근 및 집기 동작에 필수적임을 확인한다.

시각 기반 정책 학습 결과: 비전 정책 학습에는 상태 정책 롤아웃으로 얻은 데이터를 사용했다(약 100개의 성공 궤적). 표 III는 Relocate 과제에서 $\text{SR}_3$를 보여준다. 포인트클라우드 수를 512개(V1)에서 2048개(V2)로 늘리자 전체 성공률이 $0.86\to0.96$로 증가했으며, 확산 정책(V3)을 적용하자 평균 0.99로 더욱 향상되었다. 즉, 고밀도 포인트 사용과 확산 모델이 시각 정책의 성능을 끌어올렸다. 비전 정책은 상태 정책에 비해 약간의 성능 손실을 보였지만, 실제 물체 관측 정보만으로도 매우 높은 성공률을 구현했다.

통합 정책(다중 객체) 학습: Protocol #2 설정에서 다중 객체를 통합하여 학습한 결과도 양호했다. 표 IV(논문)에는 5개 객체를 모두 고려한 단일 정책의 성능이 제시되었다. 좌표 변환 모듈 추가 및 손 관절 좌표계의 세부 변환 적용은 $\text{SR}_3$를 크게 향상시켰다(Behavior Cloning 기준 평균 81%→95%, Diffusion 기준 93%→99%). 즉, 제안된 시각적 피쳐 변환 기법이 다중 객체 일반화 성능에 긍정적 영향을 주었다.

Pour 및 Place-Insde 과제: ViViDex의 상태 기반 정책은 나머지 두 과제에서도 우수한 성능을 보였다. 표 V를 보면, Adroit 핸드를 이용한 제안 방법(L5)은 Pour 과제에서 97%, Place-Insde 과제에서 68% 성공률을 달성했다. 이는 DexMV의 최적 기법(DAPG, L4)이 각각 27%와 31%에 그친 것보다 현저히 높다. 즉, 단 한 편의 동영상만으로도 기존 방법보다 월등한 성능을 얻었다. 통제 실험으로 BC와 Diffusion을 비교한 결과, 두 방식 모두 97%/68%로 유사했으며, Diffusion이 소폭 향상된 결과를 보였다.

실제 로봇 실험: UR5 암과 Allegro 핸드, 단일 RGB-D 카메라를 이용하여 실제 세계에서 실험을 수행했다. 시뮬레이터에서 학습된 상태 기반 정책을 실제에 적용하여 데이터(포인트클라우드, 관절 상태)를 수집한 뒤 비전 정책을 학습했다. 그림 4와 표 VI에 요약된 바와 같이, 제안된 통합 비전 정책(R3, Diffusion)은 5개의 실험 물체에 대해 평균 80%의 성공률을 기록했다(개별 정책 R1: 88%, 통합 BC R2: 72%). 또한 5개의 미지의 물체(크래커박스, 스프레이병 등)에도 68%의 성공률을 보여, 제안된 모델이 미지 객체로의 확장에서도 효과적임을 확인했다.


<center>
<img src="../../images/2025-11-04-vividex/03.png" width="70%" />
</center>


요약하면, ViViDex는 제한된 수의 인간 동영상으로부터 고품질의 궤적을 생성하고 이를 정책 학습에 효과적으로 활용하여, 세 가지 과제에서 기존 방법을 능가하는 성능을 입증했다. 특히, 복수 객체 및 실제 환경으로도 잘 일반화되는 점이 주목할 만하다.
