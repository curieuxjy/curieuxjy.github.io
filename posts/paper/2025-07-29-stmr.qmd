---
title: "📃SMTR 리뷰"
date: 2025-07-29
categories: [retargeting, quad]
toc: true
number-sections: false
description: Spatio-Temporal Motion Retargeting for Quadruped Robots
---


- [Paper Link](https://arxiv.org/abs/2404.11557)


1. 💡 본 논문은 Reinforcement Learning (RL)에서 보상 설계(reward design)의 중요성을 강조하며, Reward Engineering 및 보상 쉐이핑(reward shaping) 방법론과 기술을 포괄적으로 검토합니다.
2. 🛠️ 주요 내용은 보상 설계의 도전 과제(예: sparse/delayed rewards, reward hacking), 스칼라/벡터 보상 논쟁, 그리고 정책 경사(Policy Gradient), 잠재 기반(Potential-Based), 인간 피드백(Human Feedback) 등 다양한 보상 쉐이핑 기법의 상세 분류를 포함합니다.
3. 🚀 이 연구는 보상 설계가 로봇 공학, 자율 주행 등 실제 응용 분야에서 학습 효율성과 견고성을 향상시키며, Sim-to-Real 간극 해소에 필수적임을 보여주고 향후 자동화된 튜닝과 인간-로봇 협업 연구 방향을 제시합니다.


---

# Brief Review


---

# Detail Review

> 사족 보행 로봇의 시공간 모션 리타게팅: 논문 리뷰 및 손 로봇 적용 분석

## 연구의 독창성: 시공간 아키텍처, 모션 프라이어 통합, 리타게팅 전략

**「Spatio-Temporal Motion Retargeting (STMR)」**은 사족 보행 로봇에 동물의 움직임을 모방시키기 위한 새로운 모션 리타게팅 기법으로, **공간적 및 시간적 두 단계의 아키텍처**를 도입한 점이 특징입니다. 이 방법은 먼저 **공간적 모션 리타게팅(SMR)** 단계를 통해 원본 동작의 **운동학적 적합성**을 확보합니다. 구체적으로, **발 미끄러짐이나 발 관통**과 같은 운동학적 부조화를 제거하고 원본 동작의 **접촉 시퀀스(contact schedule)**를 유지하도록 발 움직임에 제약을 가함으로써, 대상 로봇의 관절 범위 내에서 **키네마틱하게 실행 가능한 전체 신체 모션**을 생성합니다. 다음으로 **시간적 모션 리타게팅(TMR)** 단계에서는 동작의 **동역학적 타당성**을 보장하기 위해 모션의 **시간적 파라미터를 최적화**합니다. 예를 들어, 점프 동작의 경우 로봇의 크기에 따라 **비행(flight) 단계의 지속시간**이 달라져야 하므로, TMR은 **모델 기반 제어기**(예: 차분 동적 프로그래밍, DDP)를 **내부 프로세스**로 활용하여 모션 타이밍을 조정하고 **중간 공중 체공시간 등 동역학적 일관성**을 확보합니다. 이러한 **이단계(공간+시간) 리타게팅 전략**을 통해 원본 생물체와 형태·크기가 다른 로봇이라도, **운동학적·동역학적으로 실행 가능한(reference) 모션**으로 변환할 수 있습니다. 이는 기존 방법들이 형태 차이는 보정하더라도 물리적으로 불가능한 동작을 생성하는 한계를 극복하고자 한 것으로, **모션 프라이어**(motion prior) 활용 면에서도 새로운 접근이라 할 수 있습니다. 본 논문에서 특별히 학습된 모션 프라이어(예: GAN 기반 생성기)를 직접 사용하지는 않지만, 대신 **모델 기반 최적화**와 **참조 모션의 물리적 타당성 확보**를 통해 **동작 사전 지식**을 통합한 효과를 냅니다. 이는 임의의 동작 데이터베이스에 한정되지 않고도 모사 학습을 유도할 수 있는 **내재적 모션 프라이어**를 제공하는 셈입니다. 마지막으로, 이렇게 얻어진 **kino-dynamically feasible** (운동학·동역학적으로 실행 가능한) 참조 모션을 토대로 **잔여 학습(residual learning)**을 적용한 **강화학습 정책**을 훈련하는 **리타게팅 전략**을 제시한 점도 독창적입니다. 구체적으로, **참조 모션을 기본 제어 신호로 활용하고 RL 에이전트가 보정 명령만 출력하도록** 함으로써, **피드백 제어 정책** 학습을 효과적으로 유도하고 최종적으로 **실세계 로봇에 배치 가능한 제어기**를 얻어낸 것입니다. 요약하면, 해당 연구의 STMR 접근법은 **공간적 최적화 + 시간적 최적화 + 잔여 RL 정책**으로 구성된 **새로운 시공간 모션 리타게팅 프레임워크**이며, 이는 기존 모션 모방 기법들의 한계를 넘어 **형태 차이 극복과 물리적 실행 가능성 보장**을 동시에 달성한 점에서 학술적 의의가 있습니다.

## 방법의 강점: 일반화 능력, 강인성, 학습 전략 및 설계

STMR 방법은 여러 측면에서 **뛰어난 강점**을 보였습니다. **일반화 능력** 측면에서, 이 기법은 **서로 다른 형태와 물리 특성을 지닌 로봇들**에 대해서도 적용 가능함을 보였습니다. 저자들은 크기와 질량이 서로 다른 세 종류의 사족 로봇(예: Unitree A1, Go1, AlienGo)에 대해, **6가지의 상이한 난이도의 동물 모션**들을 성공적으로 리타게팅 및 모방 학습시켰습니다. 그 결과 생성된 참조 모션은 **접촉 타이밍을 충실히 보존**하고 **발 미끄러짐이 거의 제거**되어, 크기가 다른 로봇이라도 원본 동작의 의미를 잃지 않도록 했습니다. 이처럼 STMR은 **동작 다양성**(여러 종류의 걸음걸이: 트롯, 페이스, 옆걸음, 점프회전 등)과 **로봇 플랫폼 다양성**에 모두 잘 대응하여, **우수한 일반화 성능**을 입증했습니다.

**강인성(Robustness)** 측면에서도 장점을 확인할 수 있습니다. RL 정책 학습 시 **잔여 정책(residual policy)** 접근을 도입함으로써, **기본 PD 제어기의 추종 성능에 학습 기반 보정 정책을 더하는 형태**를 취했는데, 이는 **학습의 안정성과 실환경 강건성**을 크게 높였습니다. 구체적으로, 참조 모션을 그대로 추종하도록 하는 **피드포워드 제어 신호**에, RL 에이전트가 **모델 불일치나 외란을 보정하는 미세 조정 신호**만 더하게 함으로써, 순수 RL에 비해 **훈련이 용이하고 수렴이 빠르며**, 실제 로봇 적용 시에도 **균형 유지 및 외란 견디기 성능**이 향상되었습니다. 실제로 시뮬레이션에서는 임의의 힘으로 로봇을 밀치는 등의 **도메인 랜덤화**를 적용해도 정책이 잘 작동했고, 이를 통해 **모델링 오차나 환경 불확실성에 대한 견디기 능력**을 강화하여 실험실 밖 **실세계 조건에서도 정책이 유효**함을 검증했습니다. 이러한 강인성은 **모델 기반 최적화로 생성된 물리적으로 타당한 참조**가 있었기에 가능했으며, 결과적으로 다른 방법보다 **높은 안정성으로 실제 로봇에 적용**할 수 있었습니다.

**학습 전략과 구조적 설계** 또한 STMR의 중요한 강점입니다. 이 방법은 **오프라인 최적화(모션 리타게팅)** 단계와 **온라인 학습(정책 RL)** 단계를 분리하여 효율을 높였습니다. 먼저 **비지도 최적화**를 통해 **참조 모션을 사전에 보정**해 둠으로써, RL 단계에서는 복잡한 동적 제약을 만족하는 모션을 **이미 확보**한 상태에서 학습이 시작됩니다. 이로써 RL은 보다 **용이한 보상 설계**(이미 물리적 타당성 확보)와 **향상된 모방 효율**을 보이며, 실제 실험에서도 **기존 기법 대비 적은 학습으로 높은 정확도 추종**을 달성했습니다. 예를 들어, 본 연구의 RL 정책은 약 5천만 스텝(약 1시간)의 훈련으로 높은 성능을 보였으며, **기존 AMP 기반 정책이 수배 더 많은 학습이 필요**했던 것에 비하면 효율적입니다. 또한 **공간-시간 단계별로 문제를 디컴포즈**한 설계는 **복잡한 모션 리타게팅 문제를 해결하기 쉽게 만든 구조적 이점**이 있습니다. 각각의 단계에서 **기성의 알고리즘**(예: IK 기반 **Unit Vector** 방법, DDP 기반 최적화)을 활용하여 신뢰성을 확보했고, 최종적으로 RL 제어기를 결합하는 **모듈식 설계**로서 각 구성요소의 역할이 명확합니다. 이러한 구조 덕분에 **새 동작 추가나 다른 로봇 적용 시에도 일부 모듈 교체/재학습으로 대응 가능**하며, **실시간 적용**을 위한 향후 확장도 모색하기 용이한 프레임워크라고 볼 수 있습니다. 정리하면, STMR의 학습전략은 **최적화된 참조로 학습 가이드**를 제공하고, 아키텍처 설계는 **문제 복잡도 감소와 모듈화**를 통해 **효율성과 확장성**을 모두 갖춘 점이 큰 강점입니다.

## 잠재적 약점: 제한된 상황, 복잡도 및 기타 고려사항

한편 이 연구에는 몇 가지 **약점이나 제한점**도 존재합니다. 첫째로, **접촉 추정에 대한 의존성**이 높다는 한계가 지적됩니다. STMR은 영상 등으로부터 얻은 **베이스 움직임이 없는 순수 키포인트 데이터**로도 모션을 재구성할 수 있다는 장점을 보여주었지만, 이를 위해서는 발의 지면 접촉 여부(접촉 스케줄)를 정확히 판단해야 합니다. 저자들에 따르면, **접촉 판정의 오류**가 있을 경우 비현실적인 모션(발이 갑자기 통과하거나 붕 뜨는 등)이 생성되어, 이후 전체 모션 최적화 과정에 **불규칙한 동작**이 전파될 위험이 있습니다. 실제로 키포인트 속도를 임계값으로 임의 thresholding 하여 접촉 여부를 결정할 경우, **연속된 프레임에서 접촉단계가 불연속적으로 바뀌는 문제**가 발생할 수 있고 이로 인해 **로봇이 따라갈 수 없는 빠른 접촉 전환**이 나타나 모방 실패로 이어질 수 있다고 지적합니다. 이를 완화하기 위해 **저역 통과 필터(low-pass filter)**로 접촉 신호를 **평활화**하였으나, 근본적으로 **신뢰성 있는 접촉 추정**이 보장되지 않으면 본 기법의 성능이 저하될 수 있다는 한계가 남습니다. 이러한 점은 **추후 접촉 오차에 대한 강인성 향상**이나 **학습을 통한 접촉 추정 보정** 등의 추가 연구가 필요함을 의미합니다.

둘째, **계산 복잡도와 실시간성**의 측면에서 제한이 있습니다. STMR은 **모션 하나를 리타게팅하기 위해 두 단계의 최적화(운동학 IK 및 동적 최적화)와 이후 강화학습 정책 훈련**까지 거치는 **비용 높은 파이프라인**입니다. 논문에서 명시하진 않았지만, 각 모션마다 이러한 과정을 수행해야 하므로 **새로운 동작마다 상당한 오프라인 계산과 학습 시간**이 필요합니다. 즉, 본 방법은 **사전 준비 단계**가 긴 편이며, 사람 시연을 즉석에서 바로 로봇에 모사시키는 **실시간 리타게팅**에는 부적합합니다. 반면 일부 기존 연구는 **사전 학습된 모델**이나 **모션 매핑 함수**를 이용하여 **실시간 온라인 리타게팅**을 지향하기도 하는데, STMR은 정확성과 물리적 타당성을 추구하는 대신 **속도를 양보**한 접근으로 볼 수 있습니다. 이로 인해 **실시간 상호작용 시나리오** (예: 사람이 즉흥적으로 시범을 보이는 것을 로봇이 바로 따라하는 경우)에는 사용하기 어렵고, **오프라인 모션 디자인 후 배치**하는 용도에 국한될 수 있습니다. 저자들도 향후 **실시간 제어 및 인지 모듈과의 통합**을 언급하며, 온라인 적용 가능성은 추후 연구과제로 남겨두고 있습니다.

셋째, **범용성의 범위**에 대한 한계입니다. 본 연구는 **사족보행 로봇의 동물 모션 모방**에 초점을 맞추었으며, 이에 최적화된 키포인트 정의(엉덩이, 허벅지, 무릎, 발 등 16개)와 제약 조건(네 발 접지 등)을 사용했습니다. 따라서 **휴머노이드나 비(非)사족형 로봇** 등에 동일 기법을 적용하려면 추가적인 고려가 필요합니다. 예를 들어, 인간형 로봇의 팔 동작이나 두 발 보행에는 상이한 키포인트와 접촉 방식이 있으므로, STMR의 **공간적/시간적 최적화 요소를 해당 플랫폼에 맞게 조정**해야 합니다. 논문에서도 STMR 개념을 **다른 형태의 로봇**(예: 휴머노이드)으로 **확장 적용**하는 방향을 제시하고 있으나, 아직 그러한 실증은 이루어지지 않았습니다. 따라서 현재로서는 **사족 보행 분야**에 국한된 솔루션이며, 완전히 다른 모폴로지에 대한 **범용적 보장**은 미지수입니다. 또한 시험된 동작들도 **달리기/점프 등 보행 동작에 한정**되어 있어, 이를 넘어선 **복잡한 상체 동작**이나 **환경과의 상호작용이 많은 동작**(예: 도약하여 착지, 물체 넘기기 등)에 대해서도 효과적일지는 추가 검증이 필요합니다.

마지막으로, **모션 프라이어 vs 물리적 타당성**에 관한 논의에서 오는 시사점도 있습니다. 저자들은 **기존의 Adversarial Motion Prior (AMP)**와 같은 기법이 **물리적으로 불가능한 참조 동작도 모방 가능하게 해주는 장점이 있지만**, 결국 **참조 모션 자체의 물리적 불일치로 인해 최종 성능이 제한**됨을 지적합니다. STMR의 결과가 AMP 대비 월등한 추종 정확도를 보인 것은, **물리적으로 일관된 참조**를 쓰는 것이 **학습 안정성과 성능에 중요**함을 보여줍니다. 다만 AMP와 같은 기법은 **스타일 상의 일반화나 데이터베이스 범위 밖 움직임 생성**에 강점이 있으므로, STMR처럼 **엄격한 물리 최적화 vs 모션 프라이어 기반 유연성** 사이에는 트레이드오프가 존재합니다. 이 논문에서는 물리적 타당성을 택했지만, **향후 두 접근의 결합**(예: 물리 제약을 지키면서도 모션 프라이어를 활용한 풍부한 모션 생성)도 가능할 것이며, 이는 본 연구가 직접 다루지는 않았지만 앞으로 탐구해볼 여지가 있는 부분입니다.

## 실험 결과의 타당성과 의미: 성능 검증 및 비교 분석

논문의 **실험 결과**는 제안한 방법의 **유효성**을 뒷받침하며, 여러 **성능 지표에서의 향상**을 명확히 보여줍니다. 저자들은 6가지 동물 모션(트롯 종류 2개, 페이스 2개, 옆걸음, 점프-회전 등)을 세 로봇에 걸쳐 실험하고, **3가지 기존 모방학습 기법**(DeepMimic, AMP, OptMimic)을 **baseline**으로 선정하여 비교했습니다. **모션 추종 정확도**는 **키포인트 경로의 DTW(dynamic time warping) L1 거리**로 측정되었는데, STMR 기반 정책은 **모든 동작에 걸쳐 가장 낮은 오차**를 기록했습니다. 평균적으로 STMR의 추종 오차는 다른 기법들에 비해 **크게 감소**하였으며, 특히 난이도가 높은 동작일수록 그 **격차가 더 벌어지는 경향**을 보였습니다. 예를 들어, **가장 복잡한 "HopTurn" (제자리 점프 회전)** 동작에서 STMR의 오차는 타 기법 대비 **현저히 낮아**, 어려운 공중 동작에서도 제안 방법이 **우수한 성능**을 발휘함을 보여주었습니다. 저자들은 이러한 결과를 통해 **물리적으로 정제된 참조 모션**을 사용한 것이 **학습 성능 향상**에 핵심임을 강조합니다. 키프레임 간의 시간 스케일을 늘리거나 줄이는 **TMR 단계**의 존재가, 특히 **비행\_phase가 있는 동작**에서 **로봇 크기에 맞는 체공시간을 부여**하여 모방 성공률을 높였음을 확인한 것입니다.

또 다른 중요한 지표로 **발 미끄러짐(foot sliding)**과 **접촉 일정 보존**이 있습니다. **발 미끄러짐**은 발이 지면에 닿은 채로 미끄러지는 정도를 나타내는 문제로, 리타게팅 과정에서 운동학적 불일치가 있을 때 흔히 발생합니다. 제안된 STMR의 **공간 최적화(SMR)** 단계에서 발끝의 속도와 위치 제약을 걸어주었기에, 결과 모션에서는 **발이 지면에 닿아있는 동안 거의 움직이지 않게** 되었습니다. 표에서 알 수 있듯이, 기존 단순 스케일링 방법(단위 벡터 UV 방법)의 경우 발 미끄러짐 지표가 매우 높았지만, STMR을 거친 모션에서는 동일 상황에서 **오차가 0에 수렴**할 정도로 미끄러짐이 제거되었습니다 (예: AlienGo 로봇의 HopTurn 동작에서 UV 대비 STMR의 발 이동량이 44.11→1.83mm로 대폭 감소). 동시에 **접촉 스케줄 보존도** 크게 향상되었습니다. 이는 원본 동작과 리타게팅된 동작의 **발 지면 접촉 on/off 시퀀스가 얼마나 일치하는지**를 IoU(교집합/합집합)로 측정한 것인데, STMR 결과 모션은 IoU ≈ 1.0에 가까워 **거의 완벽히 동일한 접촉 패턴**을 구현했습니다. 반면 기본 UV 방법의 경우 IoU가 평균 0.5 이하로 떨어져 접촉 타이밍이 어긋나는 경우가 많았습니다. 이 결과는 STMR이 **원본 동작의 의미적 맥락(언제 발을 디디고 떼는지)을 충실히 살려냈음**을 뜻하며, 단순히 모션을 비슷하게 흉내내는 데 그치지 않고 **동작의 의도와 접촉 상호작용까지 보존하는 정교한 리타게팅**이 이루어졌음을 보여줍니다. 요약하면, **정량적 성능 지표**들(추종 오차, 발 미끄러짐, 접촉 일정 등)에서 제안 방법은 **모든 기준선보다 우수**했으며, 특히 **동적이고 난이도 높은 동작**일수록 그 **이점이 두드러지는 유의미한 결과**를 얻었습니다.

**비교 기법 및 분석**을 살펴보면, 각 baseline과의 대비를 통해 STMR 구성 요소들의 **효과**를 확인할 수 있습니다. **DeepMimic**(Peng 등, 2018)는 별도의 모션 최적화 없이 **원본 모션을 바로 RL로 추종**하도록 한 경우인데, 이때 참조로 사용된 모션은 UV 스케일링으로 얻은 **운동학적 변환만 거친 모션**입니다. 그 결과 **발 미끄러짐이나 물리 불일치**가 교정되지 않아 RL 정책이 **불안정한 동작**을 보였고, 특히 **도약 동작에서 실패하거나 큰 오차**를 내는 등 한계를 보였습니다. 이는 **SMR/TMR 단계 없이** imitation learning만으로는 한계가 있다는 점을 뒷받침하며, STMR 대비 **추종 오차가 크게 높게** 나타났습니다. **AMP(Adversarial Motion Prior)** 기법은 **학습된 모션 판별자**를 통해 에이전트의 모션이 **데모의 분포와 유사**하도록 보상 신호를 주는 접근인데, 물리적으로 infeasible한 참조 동작도 어느 정도 모방이 가능하다는 장점이 있습니다. 본 논문 실험에서 AMP 기반 정책은 **DeepMimic보다는 개선된 추종**을 보였으나, **STMR만큼 정확한 추종에는 이르지 못했고** 특히 빠른 동작 전환이 필요한 경우 **정확도가 떨어지는 경향**을 보였습니다. 저자들은 **STMR 대비 AMP의 성능 열세**를 통해, **참조 모션의 물리적 일관성**이 모방 학습에 필수적임을 강조합니다. 끝으로, **OptMimic**(Fuchioka 등, 2023) 기법은 **물리 기반 최적화로 참조 모션을 개선**한 후 RL을 하는 방식으로 STMR과 유사하지만, **시간적 스케일 조정이 없는** 점이 다릅니다. 실험 결과 OptMimic은 DeepMimic보다는 낮은 오차를 내었지만, **STMR처럼 로봇 크기에 따른 동작 속도 조절이 안 되다 보니** 일부 동작에서는 **추종 오차가 여전히 크게 발생**했습니다. 예컨대 점프 후 착지 타이밍을 원본과 동일하게 가져가다 보니 로봇 크기가 달라 생기는 불균형을 완전히 해소하지 못한 것으로 볼 수 있습니다. 이러한 비교를 통해, **STMR의 공간+시간 이중 최적화 전략이 개별 구성요소들(SMR만 또는 TMR만)보다 뛰어난 성능**을 낸다는 점을 확인했습니다. 더불어 **영상 기반 모션 추출 실험**에서는, **손으로 들고 찍은 동물 영상**에서 얻은 **절대 위치 없는 상대적인 관절 움직임만으로도** STMR이 **전체 몸체 움직임을 재구성**하여 모션을 만들 수 있음을 보여주었습니다. 이는 **기존 방법들이 전제하는 '전신 모션 캡처 데이터' 없이도** 보다 폭넓은 소스에서 모션을 가져올 수 있다는 점에서 의의가 있습니다. 결국 실험 전반을 통해, 제안한 STMR 방법은 **모션 리타게팅의 품질**과 이를 활용한 **모방 제어 정책의 성능** 양면에서 **유의미한 개선**을 이루었고, 제시된 기법의 **실용적 가치**(시뮬레이션 및 실제 로봇에서의 성공)를 입증했습니다.

## 논문 기획 및 서술의 명확성, 재현 가능성, 추가적 제한사항

이 논문은 **전반적으로 명확하고 체계적인 구성**으로 작성되어 있어 읽는 이로 하여금 기법과 결과를 이해하기 쉽게 합니다. 서두에서 **문제의식**(동물 모션의 형태 차이, 기존 기법 한계)을 분명히 제시하고, 이를 해결하기 위한 **핵심 아이디어(STMR)**를 도식과 함께 직관적으로 설명하였습니다. 또한 **관련 연구**를 **모션 모방**과 **모션 리타게팅** 두 범주로 나누어 고찰함으로써, 본 연구가 어떤 지점에서 기여하는지 맥락을 분명히 했습니다. 방법론 부분에서는 **수학적 공식**과 알고리즘 절차를 상세히 기술하여, 제안 기법의 구현과 작동 방식을 이해할 수 있도록 했습니다. 예컨대, 공간적 리타게팅에 쓰인 **Unit Vector IK 알고리즘**과 시간적 최적화에 사용된 **DDP 기반 최적화**를 **별도의 전제(section)**에서 미리 설명하고, 이후 STMR 구성에 그것들이 어떻게 활용되는지 자연스럽게 연결했습니다. 이러한 **프레젠테이션의 논리적 흐름** 덕분에, 독자는 제안 기법이 **기존 기술의 어떤 부분을 차용하고 어디서 새로움을 제공하는지** 명확히 파악할 수 있습니다.

**실험 설계**와 **결과 제시** 역시 투명하게 이루어졌습니다. 비교 대상으로 삼은 각 기법의 설정(예: DeepMimic의 보상 설계, AMP의 판별기 구성, OptMimic의 최적화 세부사항 등)도 논문에 서술하고 있어, **재현 가능성**을 높였습니다. 또한 정량적 결과를 테이블과 도표로 제공하고, **표준편차 등 통계**도 기재하여 결과의 **신뢰도**를 보여줍니다. 특히 다양한 평가 지표(추종 오차, 정규화 오차, 발 미끄러짐, IoU 등)를 통해 여러 각도에서 성능을 평가한 점은, 제안 방법의 **다방면 성능 우위**를 뒷받침함과 동시에 **독자가 성능의 의미를 종합적으로 판단**하는 데 도움을 줍니다. 저자들은 **실험 동영상**도 웹에 공개하여 누구나 결과를 직접 확인할 수 있게 했고, **소스 코드** 역시 깃허브에 공개되어 있어 **재현성과 실용적 활용 가능성**을 높였습니다. 이러한 노력은 연구 공동체가 해당 기법을 따라 구현하거나 응용 연구를 진행하는 데 큰 도움을 줄 것으로 보입니다.

논문에서 **한계 및 향후 개선점**에 대한 언급도 찾아볼 수 있습니다. 마지막 부분에 **Limitation** 섹션을 두어, 접촉 추정의 불완전성으로 인한 문제와 그에 대한 임시 조치(저역필터 적용)를 솔직하게 기술하였고, 이는 앞서 지적한 바와 같이 본 기법의 현실적 제약을 독자가 이해하도록 돕습니다. 또한 **미래 연구 방향**으로, STMR을 휴머노이드 등 다른 형태의 로봇에 일반화하거나, 실시간 제어와 결합하는 방안을 제시하여, 해당 분야 발전에 대한 통찰을 제공하고 있습니다. 다만, 본문에서 직접 다루지 않은 몇 가지 잠재적 제한도 생각해볼 수 있습니다. 예를 들어, **환경과의 상호작용**이 있는 시나리오(물체를 밀면서 걷는다든지)에 대해서는 접촉 대상이 지면만 있는 현재 제약으로 충분하지 않을 수 있습니다. 또한 **복잡한 다관절 시스템**에서 최적화가 **지역해(local optimum)**에 빠질 위험이나, **동적 최적화의 계산 시간** 문제도 존재할 수 있지만 논문에서는 크게 다뤄지지 않았습니다. 그럼에도, 이러한 부분들은 해당 연구의 범위를 벗어나는 내용이기에 언급하지 않은 것으로 보이며, 전반적으로 논문의 서술과 정보 제공은 **충실하고 명확**하다고 평가됩니다. **재현성** 측면에서도 공개된 코드와 상세한 실험 파라미터 기술이 뒷받침하고 있으므로, 연구자들이 후속 연구를 이어가기 용이할 것입니다.

## 손 로봇 분야로의 기술 확장: 시공간 리타게팅의 응용

본 논문의 STMR에 담긴 **모션 리타게팅 기법과 시간적 동작 조정, 운동학적 제약 처리 기술**은 **다관절 로봇 손**의 동작 모방 및 학습에도 유용하게 응용될 수 있습니다. 사람의 섬세한 손동작을 로봇 손으로 전달하거나, 인간의 조작 기술을 로봇이 학습하도록 하는 문제는 사족 보행의 모션 모방과 유사하면서도 고유한 도전과제를 가집니다. 이하에서는 **(a) 인간 손의 자세 및 동작을 로봇 손으로 리타게팅하는 경우**와 **(b) 그런 리타게팅 모션을 활용해 로봇 손이 물체 조작을 학습/모방하는 경우**로 나누어, 해당 논문의 기법을 어떻게 적용하거나 변형할 수 있을지 논의합니다.

### 인간 손 동작의 로봇 손으로 리타게팅 (자세 및 궤적 매핑)

**사람 손의 복잡한 관절 움직임을 로봇 손에 대응시키는 리타게팅**에는 STMR의 **공간적 모션 리타게팅(SMR)** 개념이 핵심적으로 활용될 수 있습니다. 사람 손과 로봇 손은 **형태학적 차이**(모양, 크기, 관절 자유도 수) 때문에 직접적인 1:1 대응이 어려운데, 이는 사족 동물과 로봇의 다리 구조 차이와 유사한 문제입니다. STMR이 제시한 **“키포인트-기반 운동학적 매핑”**은 손의 경우에도 적용 가능합니다. 예를 들어, 인간 손의 **관절 마디 위치나 손가락 끝점(fingertip)** 등을 **키포인트**로 정의하고, **로봇 손의 대응 키포인트**와 **방향 단위 벡터**를 맞추는 방식으로 **초기 자세를 변환**할 수 있습니다. 이는 논문에서 SMR 단계에 사용한 **Unit Vector 방법**과 유사하게, **인접 마디 간 방향을 보존**하도록 로봇 손가락 관절을 설정하는 방식입니다. 이렇게 하면 로봇 손이 인간 손의 **손가락 뻗는 방향, 구부리는 각도 비율** 등을 크게 벗어나지 않게 되어 **자연스러운 초기 모션**을 얻을 수 있습니다. 이후, **역기구학(IK) 풀이**를 통해 로봇 손의 각 관절 각도를 찾아내면, 일단 **운동학적으로 유효한 로봇 손 자세 시퀀스**를 얻어낼 수 있을 것입니다. 이 과정에서 **로봇 손의 관절 가동 범위 제한**이나 **자기 충돌**(손가락 끼리 겹치지 않도록) 등의 **운동학적 제약**을 포함시키면, 마치 STMR의 SMR이 **발 관통을 방지**하듯이 **손가락 관통이나 비현실적 꺾임**을 방지할 수 있습니다. 또한 **접촉 관련 제약**도 중요한데, 이는 뒤에서 다룰 **조작 시나리오**와 겹치는 부분이 있지만, 기본적으로 **사람 손의 접촉 의도**(예: 어느 손가락이 어떤 지점에서 물체를 터치하는지)를 파악하여 로봇 손에서도 **해당 손가락(또는 대응 손가락)이 비슷한 시점에 비슷한 위치를 접촉**하도록 매핑해야 합니다. 이를 위해 **손가락 끝점**을 키포인트로 두고, 인간 손에서 접촉했던 시공간 정보를 **로봇 손 끝점의 위치/속도 제약**으로 활용하면, STMR의 **접촉 스케줄 보존** 개념을 구현할 수 있습니다. 예컨대, 사람 검지 손가락이 1초 시점에 물체 표면을 누르는 동작이 있었다면, 로봇 손에서도 대응되는 손가락(혹은 로봇의 검지에 해당하는 링크)이 그 시점에 그 위치를 누르도록 목표를 설정하는 식입니다. 이처럼 **공간적 리타게팅 단계**를 거치면, 비록 로봇 손의 형태가 달라도 **인간 손 동작의 형태적 특징과 접촉 의도**를 최대한 유지한 **전체 손가락 궤적**을 생성할 수 있습니다.

다음으로, **시간적 조정(TMR)** 개념을 손 동작에 적용할 수 있습니다. 인간과 로봇 손은 **관절 동작 속도, 토크 능력** 등이 다르기 때문에, 동일한 시간 프로파일로 움직이면 로봇이 따라가지 못하거나 과도한 힘을 내는 문제가 발생할 수 있습니다. STMR의 TMR 단계처럼, **모션의 시간축을 늘이거나 줄여서** 로봇 손에 **동역학적으로 적합한 속도와 가속도**를 찾는 최적화를 수행할 수 있습니다. 예를 들어, 사람은 순식간에 손가락을 펼쳐 물체를 놓을 수 있지만 로봇 손은 관절 속도가 한계가 있을 때, **놓는 동작의 시간을 늘려** 로봇이 충분히 따라할 수 있도록 합니다. 반대로, 사람 손이 천천히 움직이는 동작이라도 로봇에 매우 가벼운 부하라면 **시간을 압축**해도 되겠죠. 이러한 시간 최적화는 단순히 속도의 문제뿐 아니라, **동역학적 일관성**과 **접촉 안정성**을 고려해야 합니다. 예를 들어 물체를 쥐었다 놓는 시나리오에서, 로봇 손의 각 손가락이 **동시에 물체를 이격**해야 물체가 예상대로 떨어지지, 타이밍이 어긋나면 물체가 미끄러질 수 있습니다. 따라서 **손가락들 간의 타이밍 조율**, 그리고 **중력 및 물체 관성에 대응한 속도 조정** 등이 포함된 **시간적 파라미터 최적화**가 필요합니다. 이를 위해 STMR에서 활용한 방식처럼 **모델 기반 시뮬레이션**을 내부에 두고, 로봇 손+물체의 물리를 고려한 **최적 제어(예: DDP)**를 수행할 수 있습니다. 실제 논문에서는 로봇의 도약 높이와 체공시간을 DDP로 조정했듯이, 손의 경우 **물체가 들렸다 놓이는 동작**을 시뮬레이션하면서 **손가락 접촉력, 마찰 조건** 등을 만족하는 **시간 스케일**을 찾는 것입니다. 이러한 동적 리타게팅을 통해, 결과 모션은 로봇 손에게 **과도한 힘이나 충돌 없이** 실행 가능한 **물리적으로 타당한 조작 모션**이 될 것입니다. 기존 연구에서도 인간→로봇 손 리타게팅 시 **단순 위치 매핑은 관통 및 불안정한 접촉**을 초래하기 쉽고, 이를 개선하려 **에너지 함수 기반 최적화**로 관통을 줄였지만 **인간 동작의 풍부한 제약**(손가락 간 협조 움직임 등)을 충분히 반영하지 못한 문제가 지적됩니다. STMR의 접근은 **인간 손의 풍부한 kinematic 제약**을 보존하면서도 **물리적 안정성**까지 확보하도록 두 단계 최적화를 제안하므로, 로봇 손 리타게팅에서도 이러한 **일련의 공간-시간 분리 접근**이 효과적일 것으로 기대됩니다.

이 과정에서 **모션 프라이어**의 통합도 고려할 수 있습니다. 사족 보행의 경우 AMP 같은 방법이 참고될 수 있었던 것처럼, 손 동작의 경우도 **인간 손 움직임에 대한 사전 모델**을 활용하면 도움이 됩니다. 예를 들어 **인간 그립(grasp) 동작의 통계적 분포**나 **손가락 사이의 공조 움직임(예: 손가락의 synergy)**에 대한 모션 프라이어를 사용하면, 로봇 손이 더 **자연스러운 자세**를 취하도록 유도할 수 있습니다. STMR에서는 명시적으로 모션 프라이어를 쓰지 않았지만, 손 로봇에 적용 시에는 이러한 **인간 손의 선험적 지식**(예: 대부분의 그립에서 새끼손가락은 약간 굽혀진다든지)을 최적화 과정에 반영하면 리타게팅 품질을 향상시킬 여지가 있습니다. 요컨대, **공간적 정합 + 시간적 조정 + (필요시) 모션 프라이어**라는 세 가지 요소를 조합하는 것은 **복잡한 로봇 손 동작 리타게팅**에서도 핵심 원리로 적용될 수 있습니다.

### 시공간 모션 전이를 통한 조작 행동 학습 (모방 및 제어)

다음으로, 이렇게 리타게팅된 인간-로봇 손의 모션 데이터를 활용하여 **로봇 손의 조작 행동을 학습**시키는 방안을 STMR의 기법에 비추어 논의합니다. 기본 아이디어는 STMR이 **물리적으로 실행 가능한 참조 모션**을 만들어 RL 정책 학습을 **가이드**한 것처럼, 로봇 손의 경우에도 **인간 시연을 로봇 버전으로 변환한 “참조 조작 모션”**을 만들어 이를 **학습에 활용**하는 것입니다. 사람의 조작은 섬세하고 빠른 경우가 많아, 로봇이 이를 그대로 따라하려 하면 **동역학적 실패**(물체를 놓쳐버리거나 떨어뜨림, 충돌 등)로 이어질 수 있습니다. 따라서 앞서 언급한 리타게팅 단계를 거쳐 **로봇이 따라할 수 있는 수준으로 변환된 조작 시나리오**를 준비하면, 그 다음 **강화학습이나 모방 학습**으로 정책을 훈련시키는 것이 훨씬 수월해집니다.

예를 들어, 인간이 공을 집어 다른 곳에 놓는 시범 영상을 생각해봅시다. 이를 로봇 손에 모방시키고자 할 때, 먼저 **공을 집는 동작**의 **손가락 궤적**과 **접촉 타이밍**을 STMR 유사 방식으로 변환해 둡니다. 이렇게 얻은 **로봇 손의 기준 궤적**은 공을 안정적으로 잡고 옮기기에 적합하도록 **시간과 공간이 조율**되어 있겠지요. 이제 RL의 **보상 함수**를 이 참조 궤적과의 **유사도**로 설계하여 로봇이 이 동작을 익히게 할 수 있습니다. 구체적으로, STMR 논문에서와 마찬가지로 **관절 각도, 손목(기저) 위치와 자세, 손가락 키포인트 위치** 등이 참조 모션과 가까울수록 높은 보상을 주는 식입니다. 또한 **접촉 유지/해제의 시점**도 보상에 넣어, 예를 들어 공을 놓을 때 너무 늦게 손가락을 펴면 벌점을 주고, 참조와 같은 타이밍에 펴면 보상을 주는 형태로 설계할 수 있습니다. 이런 식의 **세밀한 모방 보상 설계**는 이미 참조 모션이 **물리적으로 타당**하기 때문에 가능해집니다. 만약 참조 모션이 부자연스럽거나 물리를 무시했다면 (예: 사람 손 동작을 그대로 써서 로봇 손엔 무리인 속도로 손가락을 편다든지), RL 단계에서 에이전트가 따라하려다 **실패하거나 학습이 불안정**해질 것입니다. STMR이 강조한 바와 같이, **물리적으로 일관된 레퍼런스가 있을 때 RL 모방 학습이 원활**해지므로, 로봇 손의 조작 학습에서도 이 원칙이 동일하게 적용됩니다.

**Residual Learning** 개념 역시 로봇 손 제어에 응용할 수 있습니다. 예를 들어, 로봇 손에 **기본 오픈루프 제어기**를 내장하여, 앞서 얻은 참조 궤적을 따라 **손가락 관절에 PD 제어**로 힘을 가하는 베이스를 깔아둡니다. 그런 다음, RL로 학습된 **잔여 정책**이 미세 조정 (예: “약간 더 강하게 쥐기”나 “마찰 증가를 위해 각도 보정” 등)을 출力하도록 합니다. 이는 STMR에서 **참조 joint 값 + RL 출력 = 최종 토크 명령**으로 구성한 것과 같은 방식입니다. 이렇게 하면 로봇 손이 물체를 쥐고 이동하는 동안, 기본 제어기는 **참조 궤적대로 손가락을 움직이려** 하고, RL 정책은 **물체의 미끄러짐을 막기 위해 필요한 추가 힘**이나 **모델 오차 보정 동작**을 학습하게 됩니다. 결과적으로, 처음부터 모든 것을 학습시키는 것보다 **안정적이고 샘플 효율적인 학습**이 가능하며, 실환경에서 **예기치 않은 상황**(물체 무게 변화, 마찰 계수 불확실성 등)에도 더 견고한 제어 정책을 얻을 수 있습니다. 실제 STMR 실험에서도 잔여 학습 기법이 **외란에 강한 정책**을 만들어냈고, 로봇 손의 미끄럼 방지나 **확률적 성공률** 향상에도 같은 이점을 기대할 수 있습니다.

또 하나 고려할 점은, **환경(대상 물체)과의 상호작용** 자체를 학습하는 부분입니다. 사족 로봇의 경우 지면은 고정되어 있고 발이 밀리지 않게 하는 것이 주 목표였지만, 로봇 손은 **잡은 물체를 안정적으로 다루는 것**이 핵심입니다. 따라서 RL 보상에는 **물체의 상태 안정성**(예: 물체의 목표 위치 도달 여부, 잡는 동안 떨어뜨리지 않기 등)을 포함해야 하고, 모션 리타게팅 단계에서도 **물체에 가해지는 힘**을 고려하는 것이 좋습니다. 이를테면, TMR 단계에서 **물체의 동역학까지 포함된 시뮬레이션**으로 로봇 손가락 궤적을 최적화하면, 단순히 손가락 움직임뿐 아니라 **물체가 미끄러지지 않는 경로**를 찾게 될 것입니다. 이와 관련하여, 최근 연구들은 **사람-로봇 손 동작 매핑** 시 **접촉력과 상호작용까지 모사**하려는 시도를 하고 있습니다. 예를 들어, **접촉맵(contact map)** 생성이나 **이중 임계값 기반 접촉 검출** 및 **프레임간 접촉 상태 스무딩** 같은 기법이 보고되는데, 이는 STMR에서 발 접촉을 부드럽게 처리한 것과 상통합니다. DexFlow 등 기존 연구에서는 이러한 **시간적 접촉 추적 및 스무딩**을 통해 손-물체 간 **접촉 불안정(따닥거림)**을 해소했다고 합니다. 로봇 손의 조작 모방에서도, **참조 모션 생성 시 접촉 사건을 안정적으로 추출 및 매핑**하고, RL 학습 시에도 **접촉 유지/이탈 여부**를 보상으로 다루어 **접촉 패턴을 제대로 모방**하도록 해야 합니다. STMR에서 **발바닥 접촉 IoU**로 평가를 했듯이, 손에서는 **물체와 손가락의 접촉 IoU**나 **미끄럼 정도** 등을 사용해 성능을 평가하고 최적화 방향을 설정할 수 있을 것입니다.

정리하면, STMR의 **“참조 모션 생성 + RL 정책 학습”** 투-스텝 전략은 로봇 손의 복잡한 조작 학습에도 유용한 청사진을 제공합니다. 사람의 시연 동작을 **운동학적으로 투영**하고 **동역학적으로 재조정**하여 로봇이 따라하기에 적합한 **시공간 모션**을 얻은 뒤, 이를 **모방 목표**로 삼아 **정책을 학습**함으로써, 단순 모방보다 **안정적이고 정확한 조작 정책**을 얻을 수 있습니다. 이때 **잔여 학습**을 포함한 제어 구조를 도입하면 학습된 정책의 **실제 적용성**을 높일 수 있습니다. 나아가, 필요하다면 **모션 프라이어**나 **전문가 시演 데이터**를 통합하여, 사람 손의 고차원 모션에서도 **스타일과 물리 타당성을 모두 살린 학습**을 추구할 수 있습니다. 이런 접근은 **테이블 위의 섬세한 조립 작업**, **수술용 로봇의 손 동작 모방**, **가상현실을 통한 로봇 손 원격 조작** 등 다양한 응용 분야에서 활용될 수 있을 것입니다. 실제로 **실시간 원격조작(teleoperation)**을 위한 연구들은 속도를 중시해 단순 매핑을 쓰는 경향이 있지만, 이는 미세 작업 시 **정밀도 저하**를 초래하기도 합니다. STMR에서 영감을 얻은 기법은 **오프라인에서 고품질의 모션 사전학습**을 통해 이러한 한계를 극복하고, 이후 **실시간 제어에 접목**하는 형태로도 발전할 수 있습니다.

### 손 로봇 적용을 위한 핵심 기법 요약

아래는 본 논문의 주요 기법들과 그 **로봇 손 분야로의 전이 가능성**을 정리한 표입니다. 이 표에서는 STMR에서 제안된 요소들이 로봇 손의 **자세 리타게팅** 및 **조작 행동 학습**에 어떻게 활용될 수 있는지 대비하여 나타내었습니다.

| **STMR의 핵심 기법**                                                        | **로봇 손 응용 방안**                                                                                                                                                                            |
| ---------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **공간 모션 리타게팅 (SMR)**<br>*- 키포인트 기반 자세 매핑*<br>*- 운동학적 제약 적용*            | 사람 손과 로봇 손의 **골격 구조 차이**를 극복하기 위해, 손가락 관절 마디와 끝점을 키포인트로 정의하여 **방향 벡터 보존 IK**로 초기 자세 변환. 로봇 손의 **관절 가동 범위**와 **자기 충돌** 제약을 포함해 **운동학적으로 실행 가능한 포즈** 생성.                                    |
| **시간 모션 리타게팅 (TMR)**<br>*- 모션 시퀀스의 시간 스케일 최적화*<br>*- 모델 기반 동적 검증*      | 로봇 손의 **속도/가속도 한계**와 **물체 동역학**을 고려하여, 인간 동작의 **타이밍을 조정**. DDP 등 **최적제어** 기법으로 손-물체 상호작용을 시뮬레이션하면서 **손가락 움직임의 속도를 늘이거나 줄여** **동역학적으로 안정적인** 궤적 생성. 예) 무거운 물체를 들 때 더 천천히 들어올려 **미끄러짐 방지**. |
| **운동학/동역학 제약 통합**<br>*- 접촉 유지 및 관통 방지*<br>*- 관절 한계 및 안정성 고려*           | 인간 손 시연의 **접촉 패턴**(어느 손가락이 언제 물체를 잡고 놓는지)을 분석하여 로봇 손에서도 **동일한 접촉 시퀀스**를 따르도록 제약 설정. 손가락 끝의 **미끄러짐 거리 최소화**, **관통 방지 에너지** 등을 최적화 목표에 포함시켜 **물리적으로 타당한 그립** 구현.                            |
| **잔여 정책 학습 (Residual RL)**<br>*- 기준 모션 + 보정 액션*<br>*- PD 제어기 기반 안정화*   | 리타게팅된 손 모션을 **피드포워드 참조**로 사용하고, RL 에이전트는 **보정용 미세 제스처**만 학습. 예를 들어, 참조 궤적을 따라가는 **PD 제어**를 기본으로, RL 정책이 물체가 미끄러지지 않도록 **추가 힘**을 보태는 구조. 이를 통해 **학습 안정성** 및 **실환경 강건성** 확보.                |
| **모션 프라이어 및 스타일 보존**<br>*- (Baseline: AMP)*<br>*- 인간 고유 손동작의 자연스러움 유지* | 필요한 경우 **인간 손동작 데이터베이스**로부터 학습된 **모션 프라이어**를 통합하여, 로봇 손의 동작이 **인간처럼 자연스러운 형태**를 유지하도록 유도. 예) GAN 기반 **스타일 디스크리미네이터**를 활용해 로봇 손의 모션이 인간 시演의 분포에 속하도록 보상 부여. 다만 **물리적 타당성과의 균형** 필요.        |
| **접촉 검출 및 시간적 스무딩**<br>*- 접촉 이벤트 추정*<br>*- Dual-threshold & smoothing* | 시연 데이터(예: 비디오나 센서 장갑 데이터)에서 **손가락-물체 접촉 여부**를 **이중 임계값** 등으로 검출하고, 프레임 간 **접촉 상태 변화를 저역통과 필터**로 평활화하여 **안정적인 접촉 시퀀스**를 확보. 이것을 리타게팅 및 학습에 사용해 **접촉 불안정 현상**(빠른 붙었다 떨어졌다 등) 제거.            |

上述한 기법들의 응용을 통해, 로봇 손은 인간의 섬세한 조작 기술을 보다 정확하고 안정적으로 모방할 수 있을 것으로 기대됩니다. STMR 논문의 접근법을 손 분야에 확장함으로써, **이기종 플랫폼 간 모션 모방**이라는 도전적인 문제를 해결하는 보편적인 프레임워크를 구축할 수 있습니다. 이는 향후 **인간과 로봇의 상호작용**, **원격 조작**, **로봇 기술 전수** 등의 영역에서 중요한 역할을 할 것입니다.
