---
title: "📃SoftMimic 리뷰"
date: 2025-10-23
categories: [humanoid, rl, il]
toc: true
number-sections: False
description: Learning Compliant Whole-body Control from Examples
---

- [Paper Link](https://arxiv.org/abs/2510.17792)
- [Homepage](https://gmargo11.github.io/softmimic/)


1. 예제 동작으로부터 휴머노이드 로봇의 유연한 전신 제어(compliant whole-body control) 정책을 학습하는 프레임워크로, 기존의 경직된 제어 방식이 야기하는 불안정성과 위험성을 해결합니다.
2. 이 방법은 역기구학(IK) 솔버를 활용해 외부 힘에 대한 로봇의 적절한 반응을 담은 '증강 데이터셋'을 생성하고, 이를 기반으로 강화 학습(RL) 정책을 훈련시켜 유연한 동작 추종을 가능하게 합니다.
3. 단일 동작 클립에서 다양한 태스크로의 일반화 능력과 외란 흡수력을 크게 향상시키며, 사용자가 정의하는 강성(stiffness)에 따라 환경과 안전하고 효과적으로 상호작용할 수 있음을 시뮬레이션 및 실제 로봇에서 입증했습니다.


<center>
<img src="../../images/2025-10-23-softmimic/00.png" width="100%" />
</center>

> SoftMimic 프레임워크 개요: 왼쪽은 컴플라이언트 동작 증강 단계로, 인간 시범 동작 데이터로부터 역운동학(IK) 해석을 통해 물리적으로 일관된 순응형 자세 예시들을 생성한다. 오른쪽은 강화학습을 통한 정책 학습 단계로, 로봇은 매 시각 기준 시범 동작만을 관찰하면서도 증강된 컴플라이언트 동작을 모방하도록 학습된다. 이때 정책은 외력에 따른 실제 몸의 어긋남을 스스로 감지하여 주어진 강성 계수에 맞는 부드러운 대응을 수행하도록 보상받는다.

<center>
<img src="../../images/2025-10-23-softmimic/01.png" width="100%" />
</center>


---

# Brief Review

SoftMimic은 예시 동작을 통해 휴머노이드 로봇을 위한 유연한 전신 제어(compliant whole-body control) 정책을 학습하는 프레임워크입니다. 기존의 강화 학습 기반 동작 모방(motion imitation) 방법론들은 로봇이 예상치 못한 외부 접촉에 직면했을 때, 기준 동작으로부터의 편차를 공격적으로 교정하여 경직되고 불안정한 행동을 유발했습니다. SoftMimic은 이러한 문제를 해결하기 위해 로봇이 균형과 자세를 유지하면서 외부 힘에 유연하게 반응할 수 있도록 합니다.

이 방법론의 핵심은 크게 두 단계로 나뉩니다:

**I. Compliant Motion Augmentation (유연 동작 증강)**

SoftMimic은 유연한 동작을 직접 강화 학습으로 탐색하는 대신, 학습 데이터셋을 사전에 증강(augment)하여 로봇이 외부 힘에 어떻게 반응해야 하는지에 대한 명시적인 기준을 제공합니다.

1.  **Inverse Kinematics (IK) Solver 활용:** 원본 기준 동작($q_{ref}$)과 특정 외부 힘($W_{ext}$), 그리고 원하는 로봇의 강성($K_{cmd}$)이 주어졌을 때, 오프라인 데이터 생성 단계에서 역기구학(Inverse Kinematics) 솔버를 사용하여 운동학적으로 실현 가능하고 동작 스타일이 일관된 유연 동작($q_{aug}$)을 생성합니다.
2.  **원하는 유연한 동작 목표 설정:** IK 솔버는 다음의 계층적 목표를 최적화하여 $q_{aug}$를 찾습니다.
    *   **Compliant Interaction (높은 우선순위):** 상호 작용하는 링크 $i$에 대해 다음의 목표 포즈를 설정하고, 현재 포즈 $T_i(q) = (R_i(q), p_i(q))$가 이에 근접하도록 페널티를 부여합니다.
        $$p_{i,des} = p_{i,ref}+ \frac{1}{K_{tcmd}}F_i$$
        $$R_{i,des} = R_{i,ref} \exp\left[\frac{\tau_i}{K_{rcmd}}\right]^{\times}$$
        여기서 $K_{tcmd}$는 병진 강성, $K_{rcmd}$는 회전 강성입니다.
    *   **Foot Placement (높은 우선순위):** 지지 발이 기준 동작의 접촉 스케줄과 일관되게 유지되도록 합니다.
    *   **CoM Stabilization (중간 우선순위):** 로봇의 무게 중심(Center of Mass, CoM)이 압력 중심(Center of Pressure, CoP)을 고려한 목표에 따라 균형을 유지하도록 합니다.
    *   **Keypoint Posture (낮은 우선순위):** 팔꿈치, 어깨, 몸통 등 주요 링크의 포즈가 원본 동작 스타일을 유지하도록 합니다.
    *   **Joint Posture (매우 낮은 우선순위):** 모든 관절 자유도를 기준 설정 $q_{ref}$에 가깝게 유지하여 중복성을 해결합니다.
3.  **Feasibility Validation 및 Rejection Sampling:** 생성된 IK 해는 사전에 정의된 운동학적 제약 조건(예: 최대 힘, 변위 한계, 링크 추적 오차, 발 위치 변위)을 충족해야 합니다. 만약 조건을 위반할 경우, 이벤트의 강도를 낮추고 다시 시뮬레이션하거나(이벤트 강도를 0.8로 스케일 다운) 아예 데이터셋에서 이벤트를 제거합니다. 이는 정책이 불가능한 과제를 학습하는 것을 방지하고, 최종적으로 생성된 $D_{aug}$ 데이터셋이 실현 가능한 유연 동작만을 포함하도록 합니다.
4.  **Force Field Dynamics:** 훈련 중 외부 힘은 'force field'로 구현되어, 로봇의 특정 링크를 무작위 환경 강성($K_{env}$)에 따라 움직이는 목표 지점으로 끌어당기는 방식으로 시뮬레이션됩니다. 강성 샘플링은 균일한 분포 대신 로그-균일 분포(log-uniform distribution)를 사용하여 광범위한 강성 및 유연성 값을 동등하게 탐색합니다.

**II. SoftMimic Training (강화 학습)**

1.  **관찰 공간 (Observation Space):** 정책은 로봇의 고유수용성 정보($[q_t, \dot{q}_t]$), 베이스 상태($[g_t^b, \omega_t^b]$), 이전 액션($a_{t-1}$), 그리고 원본 기준 자세($q_{ref_t}$)를 포함하는 상태를 관찰합니다. 또한 과거 3단계의 관찰 기록과 최대 1초 앞선 미래의 기준 동작 정보, 그리고 원하는 병진 및 회전 강성의 로그 값을 포함합니다. 외부 힘이나 변위 정보는 직접 관찰하지 않지만, 고유수용성 센싱을 통해 간접적으로 추론하도록 학습합니다.
2.  **보상 함수 (Reward Function):** 에이전트는 DeepMimic 스타일의 기준 동작 추적 보상($r_{ref} + r_{smooth}$)과 스프링과 유사한 유연성 보상($r_{spring} = r_{force} + r_{torque} + r_{pos} + r_{rot}$)의 합계를 받습니다. 여기서 유연성 보상은 현재의 외부 힘($W_i$)에 따라 달라집니다. 정책은 원본 $q_{ref}$를 관찰하지만, 증강된 $q_{aug}$ 목표를 따르도록 보상받습니다.
3.  **액션 공간 (Action Space):** 정책은 중간 게인(gains)을 가진 PD 컨트롤러에 대한 관절 공간(joint-space) 위치 목표를 출력하며, 위치 오차를 조절하여 토크 제어를 가능하게 합니다.
4.  **강성 범위 설정:** 정책이 과도하게 민감한 반응을 학습하는 것을 방지하고 실현 가능한 강성 범위를 설정하기 위해, 힘/위치 추정 노이즈 분석을 기반으로 강성 샘플링 범위를 제한합니다. 예를 들어 40 N/m에서 1000 N/m 사이로 설정합니다.
5.  **도메인 무작위화 (Domain Randomization):** Sim-to-real 전이를 강화하기 위해 로봇 역학 및 관찰 노이즈에 대한 표준 도메인 무작위화를 훈련 중에 적용합니다.

**결과 및 이점**

SoftMimic은 시뮬레이션 및 실제 Unitree G1 휴머노이드 로봇에서 검증되었으며, 다음과 같은 이점을 보여줍니다.

*   **작업 일반화:** 단일 동작 클립으로도 다양한 크기의 상자 줍기 등 새로운 조작 시나리오에 유연하게 대처할 수 있으며, 기존 방법보다 훨씬 낮은 충돌력을 유지합니다.
*   **외란 처리:** 벽 옆에서 팔을 들거나, 걷다가 장애물에 부딪히는 등 예기치 않은 접촉 상황에서 최대 충돌력을 현저히 낮춥니다.
*   **안전성:** 사용자 지정 강성 값에 따라 로봇이 환경과 상호 작용하는 방식이 달라지며, 낮은 강성에서는 부드럽게 밀고 높은 강성에서는 강하게 저항하는 등 안전한 상호 작용을 제어할 수 있습니다.
*   **강성 준수:** 정책은 훈련 범위 내에서 명령된 강성을 일관되게 준수하며, 증강된 데이터를 사용했을 때 특히 낮은 강성에서 변위 오차가 크게 감소합니다.
*   **동작 품질 유지:** 외부 방해가 없는 경우에도 기존의 최첨단 동작 추적 방법과 비교하여 유사한 추적 성능을 유지하면서도 훨씬 풍부하고 다재다능한 행동 레퍼토리를 학습합니다.
*   **세밀한 제어:** 오프라인 IK 솔버의 비용 함수를 조절하여 골반 방향 비용을 높이거나 낮추는 등, 유연한 반응의 전신 스타일을 세밀하게 제어할 수 있습니다.

SoftMimic은 유연한 전신 제어를 통해 휴머노이드 로봇이 복잡하고 예상치 못한 현실 세계 환경에서 안전하고 효과적으로 상호 작용할 수 있는 중요한 단계를 제시합니다.


# Detail Review

> SoftMimic: 예제 동작으로부터 전신 컴플라이언트 제어 학습 (리뷰)

## 1. 논문의 주요 기여점 요약

이 논문은 휴머노이드 로봇의 전신 제어 정책에 컴플라이언스(순응적 거동)를 도입하기 위한 새로운 강화학습 프레임워크 SoftMimic을 제안한다. 기존의 모션 모방 강화학습 기법들은 기준 동작(reference motion)을 엄격하게 추종하도록 로봇을 학습시키기 때문에, 작은 편차도 에러로 간주되어 공격적으로 수정되며 결과적으로 로봇이 매우 강직(stiff)하게 행동하게 된다. 이러한 강성 제어는 예기치 못한 접촉 상황에서 불안정하고 위험한 거동을 야기하는 문제점이 있었다.

SoftMimic의 주요 기여는 다음과 같다:

- **컴플라이언트 전신 제어 정책 학습 프레임워크 제안**: 강화학습 기반 모방학습에 컴플라이언스 개념을 통합함으로써, 로봇이 외부 힘이나 충돌에 대해 유연하게 반응하면서도 균형과 자세를 유지할 수 있는 정책을 학습할 수 있음을 보였다. 이로써 기존 방법들의 강직한 제어로 인한 취약성과 안전 문제를 해결하고, 휴머노이드가 사람과 환경과 안전하게 상호작용하도록 한다.
- **역운동학(IK)을 활용한 컴플라이언트 동작 증강 기법**: 단 하나의 기준 모션(예: 인간 시범 동작)으로부터 물리적으로 실행 가능한 다양한 순응적 동작 데이터를 오프라인 생성하는 Compliant Motion Augmentation 방식을 도입하였다. 구체적으로, 역운동학 솔버를 이용해 기준 동작 수행 중 외력이 가해진 상황을 모사한 변형 자세들을 계산함으로써, 넓은 범위의 외부 힘 및 다양한 강성 설정에 대해 로봇이 어떻게 반응해야 하는지를 상세히 시범으로 제공한다. 이러한 증강 데이터셋은 원본 시범의 스타일은 유지하면서도 다양한 교란 상황에서의 세밀한 목표 동작을 포함하므로, 정책 학습 시 추가적인 보상 튜닝 없이**도 모방과 컴플라이언스 간 균형을 달성할 수 있게 해준다.
- **모방학습 보상 구조의 혁신**: 정책은 학습 동안 오직 원본 기준 동작만을 관찰하지만, 보상은 증강된 컴플라이언트 목표 동작과의 일치 정도에 따라 주어지도록 설계하였다. 이 새로운 보상 설계를 통해 정책은 외력으로 인한 실제 동작 편차를 스스로 추론하도록 강제되며, 결과적으로 주어진 기준 동작을 경직되게 그대로 따라가기보다는 상황에 맞게 유연하게 움직이는 행동 전략을 학습한다. 이를 통해 정책 자체에 “강성(stiffness)” 수준을 조절할 수 있는 인자를 도입하여, 한 가지 정책으로 여러 가지 강직도의 동작 모드를 구현할 수 있게 한점도 중요한 기여이다. 예를 들어, 학습된 하나의 SoftMimic 정책이 낮은 강성 모드에서는 충돌을 흡수하고 사람과 부드럽게 상호작용할 수 있으며, 높은 강성 모드에서는 자세를 최대한 유지하며 외력에 강하게 저항하도록 할 수 있음을 보여주었다.
- **시뮬레이션 및 실험을 통한 유효성 검증**: 제안된 SoftMimic 기법을 물리 시뮬레이션뿐만 아니라 실제 휴머노이드 로봇(Unitree G1)에 적용하여 안전성과 범용성 측면에서 큰 향상을 달성하였다. SoftMimic으로 학습된 정책은 단 하나의 시범 동작으로부터 학습했음에도 다양한 작업 변형에 일반화할 수 있었고, 기존의 강직한 모방 제어와 비교하여 충돌 시 로봇과 물체에 가해지는 힘을 크게 감소시켰다. 이처럼 하나의 정책으로 여러 상황에 적응하며, 사람과 함께하는 환경에서 요구되는 안전한 상호작용을 실현한 것이 본 논문의 핵심적인 공헌이다.

## 2. 사용된 방법론 및 기술 설명

SoftMimic은 크게 (1)오프라인 컴플라이언트 동작 생성 단계와 (2)강화학습 정책 학습 단계의 두 부분으로 구성된다. 전체 프레임워크의 목표는 기준 모션을 최대한 모방하면서도 외부 힘에 대해 선형 스프링처럼 일정한 힘-변위 관계(강성)로 반응하도록 로봇을 학습시키는 것이다. 아래에서는 이러한 목표를 달성하기 위한 구체적인 방법론을 설명한다.


**① Compliant Motion Augmentation**: 우선 기존 모션 모방과 컴플라이언스 간 트레이드오프 문제를 풀기 위해, 오프라인에서 기준 시범 동작을 변형한 다양한 컴플라이언트 목표 동작들을 생성한다. 연구진은 역운동학(IK) 솔버를 활용하여, 기준 모션의 각 단계에서 임의의 외부 힘이나 환경 접촉이 있을 때 로봇이 균형을 잃지 않으며 허용할 수 있는 범위 내에서 자세를 얼마나 양보할지를 계산하였다. 예를 들어, 사람 팔 동작 모션을 모방하는 경우 일정한 힘으로 팔이 밀렸을 때 얼마나 팔을 뒤로 젖히고 몸을 기울이는지 등을 IK를 통해 산출하는 식이다. 이렇게 해서 얻은 컴플라이언트 동작 데이터셋은 원래의 시범 동작과 유사한 스타일과 목적 동작을 유지하면서도, 각 상황별 적절한 유연한 대응 자세를 명시적으로 포함한다. 또한, 이 과정에서 여러 수준의 가상 강성(stiffness) 계수를 고려하여 동작을 생성함으로써, 동일한 외부 힘에 대해 더 크게 양보하는 유연한 반응부터 미세하게만 움직이는 단단
한 반응까지 폭넓은 예시를 데이터셋에 담았다. 이러한 증강 데이터 생성 기법은 사전에 세밀하게 컴플라이언스 동작을 정의해두는 효과를 내며, 이후 학습에서 여러 보상 요소를 조율할 필요 없이도 모방과 안전 사이의 균형을 이루도록 해준다.

**② 강화학습 정책 학습**: 다음으로, 생성된 증강 데이터셋을 활용하여 강화학습으로 로봇의 전신 제어 정책을 훈련한다. 이때 중요한 설계 요소는, 정책의 관측(input)으로는 원본 기준 모션의 상태만을 제공하고 외력이나 변형된 목표 자세 정보를 직접 주지 않는 것이다. 대신에 보상 함수를 정의할 때, 매 시각에서 정책이 만들어낸 로봇 동작을 해당 시각의 증강된 “컴플라이언트 목표 자세”와 비교하여 얼마나 잘 일치하는지를 측정한다. 즉, 정책은 마치 원본 시범을 따라야 하는 줄 알지만, 실제로는 백그라운드에서 준비된 순응적 자세와의 오차에 따라 보상을 받게 되는 것이다. 이러한 비교 대상의 차별화로 인해 정책은 자기 몸의 상태 변화(관절각, 관성 센서 등)를 통해 외부 힘이 가해졌는지 여부를 스스로 추론하게 된다. 만약 기준 대비 실제 로봇의 자세가 어긋나기 시작하면, 이는 외력이 작용한다는 의미이고, 최적의 보상을 얻으려면 정책은 사전에 주어진 컴플라이언트 자세로 자연스럽게 “벗어나는” 방향으로 제어 명령을 출력하게 되는 원리이다. 그 결과 SoftMimic 정책은 기준 동작을 맹목적으로 따라가지 않고, 필요한 경우 참고 궤적에서 유연하게 이탈하여 환경을 받아들이는 행동을 보여준다.


또한 SoftMimic의 정책은 **동작의 강성(stiffness) 수준을 실시간으로 조절할 수 있는 입력을 포함**하도록 설계되었다. 학습 과정에서 다양한 강성 설정에 따른 동작 데이터를 함께 학습했기 때문에, 운용 시에 사용자가 원하는 강성 계수를 입력하면 정책이 이에 맞춰 더 단단하거나 더 유연한 거동을 보이도록 제어할 수 있다. 예를 들어, 원격 조종자가 조이스틱 입력으로 강성을 낮추면 로봇은 같은 목표 자세에 대해 더 많이 움직이며 충격을 흡수하고, 강성을 높이면 최대한 자세를 유지하며 외력을 억제하는 식이다. 이러한 강성 파라미터화 정책은 로봇이 작업 도중에 필요에 따라 적합한 순응성 수준을 발휘하도록 하는 유연성을 제공한다.

**③ 구현 및 알고리즘**: 논문에서는 위의 아이디어를 구현하기 위해 심층 강화학습 알고리즘(PPO 등)을 사용하여 정책을 학습시켰다. 관측 상태에는 로봇 관절각/각속도, 관성 센서 정보 등 고유감각(proprioceptive) 정보와 시간에 따른 기준 자세가 포함되며, 행動(action)은 각 관절 모터에 대한 목표 출력(토크 또는 목표각)으로 구성된다. 보상은 자세 모방
오차(정책 결과 vs 증강 목표 자세), 접지 안정성, 추가적인 규제항(너무 과격한 동작 방지) 등으로 이루어져 있으며, 증강 데이터에서 제공하는 목표 자세가 이미 안전성과 모션 스타일을 고려하고 있기 때문에 주로 모방 오차 중심으로 간단하게 설정되었다고 볼 수 있다. 학습은 시뮬레이터 상에서 다수 에피소드를 통해 진행되었고, 증강 데이터로부터 효과적
인 초기 행동 궤적을 학습함으로써 비교적 안정적인 학습이 가능하였다. 특히 논문에서는 SoftMimic의 학습 성능이 동일한 조건의 기존 강직 추종 정책 대비 훈련 수렴 속도와 데이터 효율 면에서도 개선됨을 보여주었는데, 이는 증강 데이터로 정책 탐색 공간을 안내한 덕분으로 해석된다 (관련 결과는 논문 Figure 8에 제시).


<center>
<img src="../../images/2025-10-23-softmimic/02.png" width="80%" />
</center>


요약하면, SoftMimic의 방법론은 “데이터에 의한 컴플라이언스 지정”과 “보상에 의한 컴플라이언스 유도”를 결합한 형태로 볼 수 있다. 오프라인에서 컴플라이언트 거동을 데이터로 명시하고, 온라인 학습에서는 그 데이터를 모방하도록 유도하는 두 단계 접근을 통해, 로봇이 복잡한 전신 동작 속에서도 내재적 탄성을 지니도록 만드는 기술적 해결책을 제시한 것이다.

## 3. 실험 결과 및 평가

논문에서는 SoftMimic의 효과를 검증하기 위해 여러 가지 시나리오에 대한 시뮬레이션 실험과 실제 로봇 실험을 수행하고, 그 결과를 기존의 강직한 모방 제어 정책(기준 정책)과 비교 평가하였다. 실험은 휴머노이드 로봇 (시뮬레이터 상의 모델 및 실제 Unitree G1 로봇)에게 사람의 동작을 모방시킨 후, 외부 힘이나 예기치 못한 상호작용 상황에서의 거동을 관찰하는 형식으로 진행되었다. 대표적인 평가 관찰 지표는 작업 성공률, 충돌 시 로봇/환경에 가해지는 힘의 크기, 자세 안정성(균형 유지), 동작의 자연스러움 등이었다. 주요 실험 결과는 다음과 같이 요약된다.

- **안전성 실험 (예기치 못한 접촉 흡수 능력)**: 우선 외부 충격에 대한 로봇의 거동을 평가하기 위해, 로봇이 팔을 들어 올리는 동작을 수행하던 중 주변의 깨지기 쉬운 구조물(예: 레고 블록 탑)과 불가피하게 접촉하는 상황을 실험했다. 기존 강직 추종 정책의 경우 이러한 접촉을 심각한 오류로 간주해 팔을 강하게 밀어내며 교정하려 했고, 그 결과 갑작스럽고 통제되지 않은 큰 힘이 구조물에 가해져 구조물이 무너질 위험이 나타났다. 반면 SoftMimic 정책은 접촉 발생 시 팔을 부드럽게 양보하며 충격을 흡수하여, 구조물에 거의 힘을 가하지 않고 안전하게 동작을 이어가는 모습을 보였다. 이처럼 SoftMimic은 예상치 못한 접촉 상황에서도 로봇과 환경 모두에 손상을 주지 않는 안정적인 거동을 실현하였다.
- **작업 일반화 실험 (단일 모션으로 다양한 대상 물체 다루기)**: SoftMimic의 동작 일반화 능력은 물체 조작 작업을 통해 검증되었다. 로봇에게 폭 20cm의 상자를 들어올리는 기준 모션을 모방하도록 학습시킨 후, 폭이 다른 여러 상자(예: 10cm, 15cm, 25cm 등)를 주어 제대로 들어올릴 수 있는지 시험하였다. 기존 강직 정책은 학습시 경험하지 않은 상자 크기에 직면하면, 여전히 기준 모션대로 손을 고정된 간격만큼 벌리고 잡으려 하다가 물체를 제대로 쥐지 못하거나 무리한 힘을 가해 상자나 로봇 손에 손상을 줄 위험이 있었다. 실제로 강직 정책은 자기 기준 자세를 맞추려다 보니 상자 폭이 다를 때 순간적으로 “큰 힘 스파이크”를 발생시키는 등 불안정한 동작을 보였다. 반면 SoftMimic 정책은 하나의 기준 동작만으로 학습되었음에도 불구하고, 상자의 폭 차이에 따라 팔과 손 위치를 순응적으로 조절하여 각각의 상자를 안정적으로 잡고 들어올릴 수 있었다. 로봇의 그립 힘은 상자마다 과도하지 않고 일정하고 부드러운 힘으로 적용되었고, 상자 크기가 달라져도 동작의 연속성과 안정성이 유지되었다. 이 결과는 SoftMimic이 단일 모션 시범으로부터 학습한 정책이 작업 조건의 변화(물체 크기 등)에 일반화할 수 있으며, 필요한 힘을 자동으로 조절함으로써 물체와 로봇의 안전을 동시에 보장함을 보여준다.
- **충돌력 감소 및 안정성 비교**: 추가로, 작업 도중 예기치 못한 장애물이나 물체에 부딪힐 때의 충돌 대응을 비교 평가하였다. 예를 들어 로봇이 걸어가다가 바닥에 놓인 상자를 못 보고 차거나 몸을 부딪치는 상황, 혹은 팔을 뻗었을 때 예상치 못한 물체가 그 궤적을 가로막는 상황 등을 시나리오로 실험하였다. 이런 충돌 이벤트에서 SoftMimic 정책은 최대 충돌력을 현저히 감소시켰다. 구체적으로, 동일한 충돌 상황에서 측정된 힘의 피크값이 SoftMimic의 경우 기존 강직 정책보다 크게 낮게 나타났으며, 충돌 이후 로봇의 균형 회복 시간도 단축되었다고 보고되었다. 반대로 기존 정책은 충돌 순간 큰 힘을 내며 뻣뻣하게 반응하고 이후 자세를 재정렬하는데에도 시간이 오래 걸렸다. 이는 SoftMimic 정책이 충돌 시 에너지 흡수를 잘 함으로써 로봇의 안정성을 높이고 2차 피해를 예방함을 의미한다.
- **다양한 복합 동작에의 적용**: SoftMimic의 효과는 보행과 조작이 결합된 복합 동작이나 이질적인 새로운 시나리오에서도 확인되었다. 하나의 예로, 로봇이 무거운 물체를 들고 걷는 동작(보행 모션과 상반신 들어올리기 모션 결합)을 학습한 후, 이동 중 사람과 가볍게 부딪히거나 사람이 로봇을 밀치는 상황을 실험하였다. SoftMimic 정책은 이러한 외력 간섭에도 불구하고 균형을 유지하면서도 들고 있는 물체를 떨어뜨리지 않고 계속 보행을 수행했다. 로봇은 밀림에 따라 몸을 유연하게 기울여 충격을 흡수하면서, 다리 조작으로 중심을 잡아 넘어지지 않았다. 다른 예로, 이중 팔을 사용하는 과제로 한 손으로 컵을 기울여 액체를 따르는(pouring) 동작을 학습시킨 후, 수행 중 다른 한 손을 외부에서 강제로 크게 변위시켜 보았다. 이 경우 기존 강직 정책은 한 쪽 팔의 예기치 못한 움직임에 의해 몸 전체가 균형을 잃고 따르던 액체를 쏟는 현상이 나타났다. 반면 SoftMimic 정책은 한 팔이 당황스러울 정도로 위치가 어긋나도 나머지 팔과 상체를 즉각적으로 재조정하여 따르는 동작을 거의 끊김 없이 지속했고, 결과적으로 액체를 매우 안정적으로 따를 수 있었다. 이 사례에서 기존 정책은 강체처럼 굳어있다 보니 예기치 않은 힘에 심하게 떨리는(jitter) 모습을 보인 반면, SoftMimic은 몸 전체를 활용해부드럽게 보상 동작을 취한 것이 큰 차이였다.
- **실제 로봇 실험**: 논문에서 특히 주목할 점은, 이러한 시뮬레이션 결과를 실제 휴머노이드 로봇(MIT Improbable AI Lab의 Unitree G1 휴머노이드 플랫폼)을 통해 재현했다는 것이다. 연구진은 학습된 정책을 실제 로봇에 이식하여, 사람과의 가벼운 접촉, 물체 들어올리기 등의 시나리오를 시험하였다. 그 결과 시뮬레이션에서와 유사하게 로봇이 충돌을 부드럽게 흡수하고 작업을 성공적으로 수행하는 모습을 보였다. 예를들어 사람 연구원이 로봇 팔을 밀어보는 실험에서, SoftMimic 로봇은 사람을 강하게 밀쳐내지 않고 가볍게 팔을 뒤로 움직이며 균형을 유지했고, 상자를 집어 옮기는 테스트에서도 크기가 다른 상자를 큰 힘 들이지 않고 다룰 수 있음을 실증했다. 이는 SoftMimic으로 학습된 정책이 시뮬레이터의 가정에 국한되지 않고 현실 세계의 불확실성 속에서도 충분한 견고성을 지님을 보여준다. 다만 실제 로봇 실험에서는 하드웨어 한계로 적용 가능한 외력의 크기나 동작 속도에 제약이 있었으며, 시뮬레이션만큼의 극단적인 상황 (예: 매우 무거운 하중이나 거친 충돌)은 다루지 못했다. 그럼에도 불구하고 강화학습으로 얻어진 소프트(compliant)한 거동을 실제 로봇에 구현한 것은 의미 있는 성과이며, 향후 사람과 함께 작업하는 로봇 적용에 희망적인 결과라고 평가된다.


전반적으로, 실험 결과는 SoftMimic이 강직 제어의 한계를 극복하여 안전하고 적응적인 휴머노이드 로봇 행동을 이끌어낼 수 있음을 입증한다. 충돌 시의 안정성 향상, 단일 정책의 다목적 활용(강성 조절 및 여러 작업 일반화), 그리고 시뮬레이션-현실 간 성능 이행 등의 측면에서 SoftMimic은 기존 방법 대비 뚜렷한 우위를 보였다. 이러한 결과는 컴플라
이언스 학습이 향후 휴머노이드의 실세계 활용에 필수적임을 강조하며, 본 논문이 제시한 방법론의 실효성을 뒷받침한다.

## 4. 기존 연구와의 비교

SoftMimic이 다루는 “모션 모방 + 컴플라이언스” 문제 설정은 로봇 제어 및 강화학습 분야에서 비교적 새로운 접근이지만, 관련되는 선행 연구와 개념들이 존재한다. 본 절에서는 모션 모방 강화학습 기법 및 컴플라이언트 제어 기법 측면에서 본 논문을 기존 연구들과 비교하여 그 차별점을 논의한다.

**① 모션 모방 강화학습 (Imitation Learning) 측면**: 기존의 대표적 모션 모방 RL 연구로 꼽히는 DeepMimic (Peng 등, 2018)이나 이후 다양한 후속 연구들은, 인간 모션 캡처 데이터를 그대로 따라하도록 로봇/캐릭터를 강화학습으로 훈련하여 사람과 유사한 움직임을 얻는 데 성공한 바 있다. 그러나 이러한 방법들은 대부분 환경과의 비상접(contactfree) 시나리오 또는 사전 정의된 지면 접촉만 존재하는 상황을 가정하고 있으며, 예기치 못한 외부 접촉이나 힘 작용을 고려하지 않는다. 따라서 기준 모션에서 벗어나는 모든 편차를 벌점으로 주는 구조가 일반적이며 , 그 결과 정책이 최대한 강직하게 동작하도록 학습되는 경향이 있다. 이는 시뮬레이션 캐릭터 동작 재현에는 문제가 없지만, 실제 로봇이 동적인 환경에서 사람이나 물체와 상호작용할 때는 취약하고 위험한 단점으로 드러난다. SoftMimic은 이러한 기존 모방 학습기의 한계를 정확히 짚어내고, 모방 학습에도 안전한 상호작용 능력을 부여했다는 점에서 의의가 크다. 특히 보상 설계의 측면 혁신으로서, 준수한 거동(compliant behavior)을 보이는 경우에만 보상을 높이는 SoftMimic의 방식은 기존 기법들과 확연히 구별된다. 이전의 방법들이 모션 정확도 하나만을 추구했다면, `SoftMimic은 모션 정확도와 안전한 힘 제어를 동시에 만족시키도록 학습 목표를 재정의`한 것이다. 또한 일부 선행 연구에서는 강성을 위해 외력 센서 입력을 활용하거나 충돌 시 제어 전환과 같은 추가적인 안전모드를 설계하기도 했으나, SoftMimic은 `이러한 명시적 장치 없이 정책 자체가 상황에 따른 적절한 유연성을 발휘하도록 학습적으로 유도한다는 점`에서 보다 세련되고 통합적인 접근이라 할 수 있다.

**② 컴플라이언트 제어 (Compliant/Impedance Control) 측면**: 전통적으로 로봇의 컴플라이언스는 물리 제어 계층에서 임피던스 제어나 어드미턴스 제어와 같은 방법으로 구현되어 왔다. 예를 들어, 미리 정의된 스프링-댐퍼 계수(강성, 감쇠)를 이용해 로봇 관절에 탄성 특성을 부여하거나, 힘 센서로 측정된 외력에 따라 목표 위치를 보정하는 등의 방식이 일반적이다. 이러한 고전 제어기법들은 즉각적으로 연속적인 힘 제어를 제공하고 안전성에 기여하지만, 로봇의 고차원 전신 동작에 적용하기 어렵고 작업별로 적절한 강성 파라미터 튜닝이 필요하다는 한계가 있다. 반면 SoftMimic은 `고차원 모션 학습의 맥락에서 컴플라이언스를 논한다는 점`에서 큰 차별성이 있다. 즉, 기존에는 모션 생성과 컴플라이언스 확보가 별개로 다루어졌다면, SoftMimic은 이를 `단일 학습 프레임워크에 통합`하였다. 예를 들어 SCAPE (2021) 등의 연구가 특정 작업에서의 강성 제어 정책을 학습시키기 위해 시도된 바 있지만, 주로 소규모 매니퓰레이터 작업에 국한되었다. SoftMimic은 `전신 제어 영역에서, 그것도 사람의 복잡한 시범 동작을 모방하는 동시에 컴플라이언스를 달성했다`는 점에서 새로운 지평을 연다고 볼 수 있다.

또한 SoftMimic은 `강성 수준을 연속적으로 조절 가능한 정책`이라는 점에서도 기존 연구에 찾아보기 힘든 특징을 지닌다. 전통 임피던스 제어에서는 강성을 높이거나 낮추려면 제어 규칙의 파라미터를 사용자가 사전에 설정해야 하고, 상황 변화에 따른 자동 조절이 제한적이었다. 반면 본 논문에서는 강성 값을 정책의 입력으로 사용하여, 학습된 정책이 주어진 강성 설정에 대응되는 동작 모드를 자연스럽게 구현하도록 하였다. 이러한 아이디어는 로봇에게 하나의 두뇌로 여러 성격(stiff or compliant)을 구현해주는 것으로, 향후 로봇이 작업 상황에 따라 스스로 강도를 조절하는 적응형 지능으로 발전시킬 수 있는 토대를 제공한다. 저자들도 “단일 정책으로 필요에 따라 단단해지거나 부드러워질 수 있다”는 점을 강조하고 있으며 , 이는 기존의 고정 강성 제어기들과 비교되는 SoftMimic만의 강점이라 할 수 있다.

정리하면, 기존 연구와의 비교를 통해 드러나는 SoftMimic의 특징은 다음과 같다:

- **기존 모션 모방 RL 대비**: 안전하고 유연한 모방을 달성 (기존엔 정확한 추종만, SoftMimic은 안전 고려).
- **기존 컴플라이언스 제어 대비**: 학습 기반으로 고차원 동작에서의 컴플라이언스 구현 (기존엔 저차원/고정 강성, SoftMimic은 학습으로 상황별 순응).
- **하나의 정책으로 강성 가변**: (기존엔 수동 파라미터 변경, SoftMimic은 정책 입력으로 실시간 가변).
- **시뮬레이션-현실 격차에서의 성능**: (기존 RL 모방 정책은 현실 적용 어려움, SoftMimic은 현실 로봇 적용 시 안전성 면에서 유리하여 실제 적용 가능성 증대).

이러한 차별화 요소들로 인해 SoftMimic은 휴머노이드 로봇의 상호작용 능력을 한 단계 끌어올린 접근으로 평가되며, 기존의 모션 모방 또는 컴플라이언스 연구들이 미처 해결하지 못한 공백을 메운 사례로 볼 수 있다.

## 5. 강점 및 약점 분석

**강점(strengths)**

- **안전성 & 상호작용 향상**: 가장 큰 강점은 로봇의 안전한 상호작용 능력을 비약적으로 향상시켰다는 점이다. SoftMimic 정책은 사람이나 환경과 접촉이 발생해도 큰 충돌력 없이 부드럽게 반응하도록 학습되었으며, 이는 로봇이 사람과 함께 작업할 때 필수적인 안전 요건을 충족시킨다. 기존에 휴머노이드가 조금만 접촉해도 균형을 잃거나 위험한 힘을 가하던 것과 달리, SoftMimic을 통해 “휴머노이드는 세상을 터치하면 쓰러진다”는 편견을 깰 수 있음을 보여주었다고 저자들은 강조한다. 실제 실험에서도 최대 충돌력이 유의미하게 감소했고 로봇이 자세를 유지한 채 접촉을 견뎌내는 결과는, 이 접근의 실용적 안전성을 뒷받침한다.
- **모션 범용성 & 적응력**: SoftMimic은 단일한 모션 클립으로부터 다수의 상황에 대응할 수 있는 일반화 능력을 입증하였다. 이는 로봇 학습 분야에서 데이터 효율성과 범용성 측면의 진전을 의미한다. 하나의 시범 동작을 가르쳐주면, 로봇이 그 주변의 변주된 과제들까지 해낼 수 있다는 것은, 향후 로봇에게 방대한 모든 상황을 일일이 가르치지 않아도 된다는 가능성을 시사한다. 예를 들어 상자 집기 동작 하나로 폭이 다양한 상자를 모두 다룰 수 있었고, 걷기 동작 하나로 약간의 방해물이나 하중 변화에 적응하며 걸을 수 있었다. 특히 강성 조절 기능 덕분에 하나의 정책이 “여러 성격”을 가져 상황별로 달라진 행동을 보일 수 있다는 점은 매우 혁신적이다. 이는 마치 사람 근육이 상황에 따라 힘을 조절하듯, 로봇 정책이 맥락에 맞게 연성(stiffness)을 변화시킬 수 있음을 보여준 것으로, 능동적 적응력의 증대라는 의의가 있다.
- **학습 설계의 단순성**: SoftMimic은 보상 함수를 복잡하게 설계하지 않고도 목표한 바를 달성한 점도 강점으로 꼽힌다. 기존에 모방 정확도와 안정성을 함께 확보하려면 보상에 여러 항을 넣고 가중치를 튜닝해야 하는 어려움이 있었다. 그러나 SoftMimic에서는 증강 데이터가 지닌 세밀한 정보 덕분에 사실상 모방 보상 하나만으로도 충분히 학습이 진행될 수 있었다. 이처럼 데이터로 문제를 풀어낸 접근은, 보상 설계로 인한 시행착오를 줄이고 학습 안정성을 높였다는 평가다. 또한 증강 데이터는 물리적으로 말이 되는 동작들로 구성되어 있으므로, 학습 중 비현실적 자세나 불안정한 동작으로 탐색이 새는 것을 방지하는 효과도 있어 결과적으로 표본 효율성이 향상되었다고 볼 수 있다.
- **실험으로 입증된 실효성**: 본 연구는 아이디어 제안에 그치지 않고 시뮬레이션 및 실제 로봇 실험을 통해 충분히 실효성을 검증하였다. 그 과정에서 다양한 정량/정성 지표로 SoftMimic의 우수성을 보여주었고, 재현 영상과 수치 결과를 통해 독자를 설득하고 있다. 특히 실제 로봇 실험을 병행한 점은, 시뮬레이션 성과에 그치는 다른 강화학습 연구들과 달리 현실 적용 가능성을 한층 끌어올린 부분이다. 이러한 종합적인 검증은 SoftMimic의 기여를 더욱 신뢰하게 만들며, 학계와 산업계에 주는 실용적 시사점을 크게 한다.
- **향후 확장 가능성**: SoftMimic의 접근은 개념적으로 다른 로봇 유형이나 작업에도 확장 가능하다는 점에서 매력적이다. 휴머노이드 외에 다른 로봇 팔, 이동 로봇 등에 대해서도 유사한 증강+모방 학습으로 컴플라이언스를 부여할 수 있을 것으로 예상된다. 또한 저자들이 제시하듯, 훗날 방대한 다양한 모션 데이터셋에 이 방법을 적용해 “컴플라이언트 전신 제어의 기본 정책”을 학습한다면, 일종의 파운데이션 모델처럼 여러 로봇에 이식 가능한 범용 정책으로 발전시킬 수도 있을 것이다. 이처럼 확장성과 응용의 폭이 넓다는 점도 SoftMimic의 강점으로 손꼽을 수 있다.

**약점(weaknesses)**

- **강성 조절의 자동화 부족**: 현재 SoftMimic 정책은 강성 수준을 외부에서 파라미터로 입력받아 동작을 조절한다. 즉, 로봇이 스스로 상황을 판단하여 강성을 높이거나 낮추는 것은 아니다. 예를 들어 무거운 물체를 들어올릴 때는 높은 강성이, 사람에게 물건을 건네줄 때는 낮은 강성이 적합한데, 이러한 맥락에 따른 최적 강성 선택을 정책이 자율적으로 하지는 못한다. 이는 여전히 사람이나 상위 계획기가 강성 입력을 적절히 조정해줘야 함을 의미하며, 완전한 지능적 순응 제어로 가기엔 남은 과제다. 저자들도 과제로서 “로봇이 스스로 동적으로 강성을 결정하는 방법”을 향후 연구 방향으로 제시하고 있다.
- **증강 데이터의 한계**: SoftMimic의 성능은 증강된 컴플라이언트 동작 데이터의 품질과 다양성에 크게 의존한다. 역운동학 기반으로 생성된 동작은 동역학적 효과(관성, 마찰 등)를 완전히 반영하지 못할 수 있다. 현재 방법은 순전히 기구학적(feasible posture) 관점에서 외력에 대한 자세를 산출하고 있는데, 실제 물리 환경에서는 동작 속도나 가속에 따른 관성력, 충돌 시 반발력 등이 중요한 역할을 한다. 예를 들어 빠르게 걷는 도중 밀렸을 때의 실제 거동은 단순 IK로 계산한 자세와 차이가 있을 수 있다. 데이터 증강에 동역학적 고려가 부족한 점은, 정책이 그런 상황을 학습하지 못하게 하여 한계로 작용할 수 있다. 논문의 미래 작업에서도 “증강 과정에 동역학을 포함”시키는 방향이 언급되어 있으며 , 이를 통해 현실 물리와의 오차를 줄일 필요가 있다.
- **증강 데이터 생성의 비용**: 다양한 외력 상황과 강성 수준을 망라하는 증강 데이터를 만드는 데 시간과 자원 비용이 많이 들 수 있다. 논문에서 “massive augmented dataset(거대 증강 데이터셋)”이라는 표현이 등장하는데, 이는 곧 매우 많은 시나리오별 IK 계산이 필요함을 의미한다. 복잡한 전신 모션의 IK 해는 계산이 어려울 수 있고, 제약 조건이 많을수록 실패하거나 비현실적인 자세가 나올 수 있다. 이러한 데이터를 만들고 검증하는 과정에 전문가의 개입이나 튜닝이 일부 요구될 수 있으며, 완전 자동화된 학습이라 보기 어려운 면도 있다. 또한 증강된 상황 이외의 완전히 새로운 상황이 나타나면 (예: 전혀 다른 방향의 충돌 등) 정책이 그 경우까지 적응할 지는 보장되지 않는다. 따라서 데이터 증강의 범위를 얼마나 잘 정하느냐가 성능을 좌우하며, 잘못하면 증강 범위를 벗어난 상황에서는 다시 취약해질 가능성이 있다.
- **정책의 복잡도 및 관찰 한계**: SoftMimic 정책은 외부 힘을 직접 관측하지 않고 내부 센서와 기준 모션 대비 오차로 추론하도록 되어 있다. 이는 센서 노이즈나 모델 오차에 민감할 수 있는 설정이다. 실제 로봇에서는 관절 센서 오차 등으로 인해 정책이 외력이 있다고 잘못 해석하거나, 반대로 작은 접촉을 감지 못하는 경우도 가능하다. 힘센서를 쓰지 않은 것은 설계 간소화 측면에서는 장점이지만, 동시에 정책이 추론해야 할 숨은 변수(hidden parameter)가 늘어난 셈이어서 학습 난이도를 높일 수도 있다. 특히 매우 급격한 충격이나 초당 여러 번 반복되는 진동 등의 경우, 정책이 이를 정확히 감지하여 빠르게 대응하는 데 한계가 있을 수 있다. 추후 이러한 부분을 보완하려면 정책에 외력 추정 모듈을 넣거나, 고주파 신호에 대한 로버스트니스를 높이는 훈련이c필요할 것이다.
- **작업 범위 및 일반성 한계**: SoftMimic의 실험들은 주로 일상적인 힘 상호작용 범위 내의 시나리오(가벼운 접촉, 수 kg 정도 물체, 비교적 느린 동작 등)에 한정되어 있다. 산업 현장처럼 매우 무거운 물체(수십 kg)를 다루거나, 충돌이 발생하면 안 되는 매우 정밀 작업, 혹은 달리기처럼 고속의 동적 동작 등에는 아직 적용 사례가 없다. 이러한 영역에서는 요구 강성이 극단적으로 높거나 낮아야 할 수 있는데, SoftMimic 접근이 그 만큼의 범위를 포괄할 수 있을지는 미지수다. 또한 본 논문은 하나의 모션에 대해서는 일반화를 보였지만, 서로 상이한 여러 모션간 (예: 걷기와 점프하기 두 가지를 모두) 동시에 학습한 하나의 정책으로 복합적인 스킬을 보이는 실험은 없다. 즉 멀티스킬 확장에 대한 검증은 남아 있다. 저자들이 미래 방향으로 언급한 “대규모 다양한 모션에 대한 전신 제어 학습” 은 아직 달성되지 않은 부분으로, 그 단계에서는 새로운 도전과제가 나타날 가능성이 있다.

---

결론적으로, SoftMimic은 기존 방법들이 간과했던 안전하고 유연한 로봇 제어의 중요성을 짚어내고, 이를 강화학습과 모방학습의 틀 안에서 우아하게 해결하려 한 걸출한 시도이다. 강력한 아이디어와 실증 결과를 통해 로봇이 사람처럼 부드럽게 움직이고 상호작용할 수 있다는 가능성을 보여주었고, 이는 휴머노이드 로봇의 실세계 활용을 앞당기는 의미 있는 진전으로 평가된다. 동시에, 현 단계에서 드러나는 몇 가지 한계들은 향후 연구의 방향성을 제시하는 과제로 남아 있다. 예를 들어, 로봇 스스로 상황에 맞게 강성을 선택하고, 동역학까지 아우르는 데이터 증강을 통해 현실 정확성을 높이며, 다양한 모션들의 통합 학습으로 일반성을 확보하는 등으로 연구가 발전될 필요가 있다. 이러한 도전과제를 해결해 나간다면 SoftMimic의 접근법은 더욱 견고해지고 폭넓게 적용될 것이며, 궁극적으로 안전하고 유연한 범용 로봇 제어기의 실현으로 이어질 것으로 기대된다.
