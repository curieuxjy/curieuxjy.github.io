---
title: "📃ManiSkill3 리뷰"
date: 2025-08-06
categories: [sapien, simulation]
toc: true
number-sections: true
description: Demonstrating GPU Parallelized Robot Simulation and Rendering for Generalizable Embodied AI with ManiSkill3
---

- [Paper Link](https://arxiv.org/abs/2410.00425)
- [Project Link](https://www.maniskill.ai/)
- [Github Link](https://github.com/haosulab/ManiSkill)


1. 🚀 ManiSkill3는 기존 시뮬레이터의 제약(속도, 메모리, 이질성)을 해결한 GPU 병렬화 로봇 시뮬레이터로, 최대 30,000+ FPS와 낮은 GPU 메모리 사용량을 자랑합니다.
2. 💡 이 플랫폼은 모바일 조작, 드로잉, 휴머노이드 등 12가지의 다양한 작업 도메인과 20개 이상의 로봇을 지원하며, 사용자 친화적인 API 및 이질적 시뮬레이션 기능을 통해 일반화 가능한 로봇 학습을 가속화합니다.
3. ✅ ManiSkill3는 로봇 강화 학습 시간을 크게 단축하고, 시뮬레이션에서 실제 환경으로의 안정적인 zero-shot 정책 배포를 입증하여 Sim2Real 및 Real2Sim 연구에 강력한 기반을 제공합니다.

<center>
<img src="../../images/2025-08-07-maniskill3/teaser.jpg" width="80%" />
</center>

---

# Brief Review

GPU 병렬 로봇 시뮬레이션 및 렌더링을 통한 일반화 가능한 체화형 AI 구현: ManiSkill3

ManiSkill3는 일반화 가능한 로봇 조작을 목표로 하는 가장 빠른 상태-시각 GPU 병렬 로봇 시뮬레이터로, 접촉이 많은 물리 시뮬레이션을 지원합니다. 이 프레임워크는 시뮬레이션+렌더링, 이종 시뮬레이션, 포인트 클라우드/복셀 시각 입력 등 다양한 측면에서 GPU 병렬화를 지원합니다. ManiSkill3의 GPU 시뮬레이션 및 렌더링은 다른 플랫폼보다 2~3배 적은 GPU 메모리를 사용하며, 최소한의 Python/PyTorch 오버헤드, GPU에서의 시뮬레이션, 그리고 SAPIEN 병렬 렌더링 시스템 덕분에 벤치마크 환경에서 최대 30,000+ FPS를 달성합니다. 이를 통해 수 시간 걸리던 훈련 시간이 몇 분으로 단축됩니다. 또한, 모바일 조작, 드로잉, 휴머노이드, 숙련된 조작 등 12가지 독특한 도메인에 걸쳐 가장 포괄적인 GPU 병렬화 환경/작업을 제공하며, 모션 플래닝, RL, 텔레오퍼레이션으로부터 수백만 프레임의 시연 데이터도 제공합니다. ManiSkill3는 인기 있는 RL 및 시연 기반 학습 알고리즘을 아우르는 포괄적인 기준선도 제공합니다.

기존 로봇 학습 연구는 비전 및 언어 연구와 달리 로봇 조작을 위한 적절한 데이터셋이 부족했습니다. 실세계 모방 학습은 방대한 데이터가 필요하고, 강화 학습(RL)은 광범위한 실제 환경 설정이 요구됩니다. Isaac Lab과 Mujoco의 MJX와 같은 GPU 병렬 시뮬레이션은 로봇 이동과 같은 문제 해결에 큰 진전을 가져왔지만, 조작 태스크에서는 이종 시뮬레이션(각 병렬 환경이 다른 장면을 포함하는 것) 및 빠른 병렬 렌더링 기능이 부족하여 시각 입력을 사용하는 RL 알고리즘의 훈련 속도가 느린 한계가 있었습니다. ManiSkill3는 이러한 한계를 해결하고 Apache-2.0 라이선스 하에 오픈 소스로 공개됩니다.

ManiSkill3의 핵심 기여는 다음과 같습니다.

1.  **최첨단 GPU 병렬 시뮬레이션 및 렌더링:** ManiSkill3는 빠른 병렬 렌더링과 낮은 시스템 오버헤드를 통해 GPU를 효율적으로 사용하여, 시각 태스크를 다른 시뮬레이터보다 빠르게 해결합니다. 시뮬레이션+렌더링 FPS는 최대 30,000+에 달하며, 시각 데이터 수집을 대규모로 가속화합니다. 또한, GPU 메모리 사용량이 2~3배 낮아 장치 내 시각 RL 및 대규모 신경망 훈련을 가능하게 합니다. 벤치마크에서 128개 병렬 환경의 경우 ManiSkill3는 3.5GB의 GPU 메모리를 사용하는 반면, Isaac Lab은 14.1GB를 사용합니다. ManiSkill3는 SAPIEN의 래스터화 렌더러를 사용하는 반면 Isaac Lab은 레이 트레이싱을 사용합니다.
2.  **가장 포괄적인 환경:** 테이블탑, 모바일 조작, 룸 스케일 장면, 사족보행/휴머노이드 이동, 휴머노이드/양손 조작, 다중 에이전트 로봇, 드로잉/클리닝, 숙련된 조작, 시각-촉각 조작, 고전 제어, 디지털 트윈, 소프트 바디 조작 등 12가지 독특한 카테고리의 환경과 20개 이상의 로봇을 기본 제공합니다. ReplicaCAD 및 AI2-THOR 장면을 지원하며, PhysX 기반의 시뮬레이션을 사용합니다.
3.  **이종(Heterogeneous) GPU 시뮬레이션:** ManiSkill3는 각 병렬 환경에서 완전히 다른 객체 형상, 객체 수, 그리고 서로 다른 자유도(DoF)를 가진 관절(articulation)을 시뮬레이션하고 렌더링할 수 있는 유일한 시뮬레이션 프레임워크입니다. 이를 통해 PPO와 같은 알고리즘이 YCB 데이터셋의 모든 객체나 PartNetMobility 데이터셋의 캐비닛에 대해 동시에 훈련하여 더 일반화 가능한 학습을 가능하게 합니다.
4.  **간소화된 통합 API:** 관절(articulation), 링크, 조인트 및 액터에 대한 완전한 객체 지향 API를 제공하여 복잡한 텐서 인덱싱 없이 로봇 환경을 쉽게 구축하고 관리할 수 있습니다. 포즈 정보는 배치(batched) `Pose` 객체로 저장되어, 예를 들어 두 포즈 $P_1, P_2$에 대해 $(P_1 P_2)^{-1} P_1^{-1}$와 같은 연산을 메서드 체이닝 패턴으로 간결하게 수행할 수 있습니다.
    `ManiSkill3.poses.Pose.inv_then_mul(p1, p2, p1_inv=True)`
    URDF 및 MJCF 정의 형식을 기본적으로 지원하며, GPU 병렬화된 관절 위치 제어 및 역운동학(IK) 제어를 위한 사전 구축된 제어기 옵션을 제공합니다.
5.  **확장 가능한 데이터셋 생성 파이프라인:** 모션 플래닝, RL 정책, 텔레오퍼레이션 등 다양한 방법을 통해 시연 데이터를 수집하며, 특히 복잡한 태스크의 경우 RLPD [2] 및 RFCL [47]과 같은 온라인 모방 학습 알고리즘을 사용하여 소수의 시연으로부터 일반화된 정책을 학습시킨 후 대규모 데이터셋을 생성합니다. 트랙토리 리플레이 도구는 CPU/GPU 시뮬레이션에서 수집된 데이터를 다른 설정(관측, 보상, 병렬 환경 수, RNG 시딩)으로 리플레이할 수 있도록 지원합니다.
6.  **VR 텔레오퍼레이션:** OpenVR 클라이언트 프로토콜을 구현하여 주류 VR 장치를 지원하며, 작업자의 손목 및 손 포즈를 실시간으로 로봇 동작으로 변환합니다. 4K 해상도의 스테레오 비디오 스트림을 60Hz로 VR 장치에 전송하여 몰입형 시야각을 제공합니다.
    *   **로봇 제어 모듈:**
        *   **팔 제어 모듈:** 인간의 손목 포즈를 로봇 팔 관절 위치로 변환합니다. Pinocchio 라이브러리 기반의 Closed-loop Inverse Kinematics (CLIK) 알고리즘의 수정된 버전을 사용하여 관절 각도를 계산하며, 이종 엔드 이펙터(end-effector)에 대한 IK 문제를 동시에 처리합니다. 특정 관절의 움직임을 조절하기 위해 소프트 마스크(soft mask)를 사용하고, 부드러운 팔 동작을 위해 SE(3) 그룹 필터를 적용합니다.
        *   **손 제어 모듈:** 인간 손가락 포즈를 로봇 손 관절 위치로 변환합니다. 이는 다음 목적 함수를 최소화하는 최적화 문제로 정식화됩니다.
            $$\min_{\mathbf{q}_t} \sum_{i=0}^N \|\alpha_i \mathbf{v}_i^t - f_i(\mathbf{q}_t)\|^2 + \beta\|\mathbf{q}_t - \mathbf{q}_{t-1}\|$$
            여기서 $\mathbf{q}_t$는 시간 $t$에서의 로봇 손 관절 위치, $\mathbf{v}_i^t$는 인간 손의 $i$번째 키포인트 벡터, $f_i(\mathbf{q}_t)$는 순방향 운동학을 사용한 로봇 손의 해당 키포인트 벡터입니다. $\alpha_i$는 손 크기 차이를 보상하고, $\beta$는 시간적 일관성을 위한 정규화 항의 가중치입니다. 이 최적화는 NLopt 솔버로 구현됩니다.
    *   **Sim-to-Real 인터페이스:** 깊이 카메라로 캡처된 포인트 클라우드를 VR 헤드셋에 투영하고 EasyHec으로 카메라 포즈를 보정하여 시뮬레이션 환경과 실제 환경을 "디지털 트윈" 방식으로 정렬합니다.

**벤치마크 및 결과:**

*   **강화 학습 (RL) 훈련 속도:** ManiSkill3의 GPU 시뮬레이션은 `PickCube` 태스크에서 PPO를 사용할 때 ManiSkill2의 CPU 시뮬레이션 대비 상태 기반 관측에서 약 15배, RGB 기반 관측에서 약 8배의 훈련 속도 향상을 보여줍니다. 상태 기반의 경우 1분, RGB 기반의 경우 10분 만에 거의 100% 성공률에 도달합니다.
*   **시연 기반 학습 (LfD) / 모방 학습:** 행동 복제(BC), Diffusion Policy (DP), Action Chunking Transformer (ACT)와 같은 오프라인 모방 학습 기준선과 RLPD, RFCL과 같은 온라인 모방 학습 기준선을 제공합니다. 특히 Diffusion Policy는 적은 수의 시연에서도 가장 좋은 성능을 보였습니다. PerAct와 같은 복셀 기반 VLA 모델도 지원하며, Multi-View 및 SE(3) 증강이 성능 향상에 기여함을 보여줍니다.
*   **시각 기반 Sim2Real 조작:** ManiSkill3는 Koch v1.1 로봇 팔과 휴대폰 카메라를 사용하여 시각 기반 조작 정책을 종단 간(end-to-end) 훈련하고 제로샷(zero-shot)으로 실제 세계에 배포하는 재현 가능한 설정을 제공합니다. 시뮬레이션 훈련 시 배경 녹색 스크리닝 및 광범위한 도메인 무작위화(카메라 포즈, 조명 방향, 로봇 포즈, 큐브 크기, 색상, 마찰)를 적용합니다. 약 1시간 훈련 후 최종 정책은 실제 세계에서 91.6%의 성공률을 달성했으며, 시뮬레이션과 실제 세계 성공률 간의 높은 상관관계(0.9284)를 입증했습니다.

**한계점:**
복잡한 환경(예: 룸 스케일 장면)에서는 단일 GPU에서 병렬 환경의 수가 제한될 수 있습니다. Sim2Real의 완전한 해결은 아니며, 특정 보상 설계가 필요할 수 있습니다. 현재 Sim2Real 데모는 정적 카메라로 제한되어 있고, 소프트 바디 환경과 같은 일부 태스크는 GPU 병렬화가 되지 않습니다.

**결론:**
ManiSkill3는 일반화 가능한 로봇 시뮬레이션 및 렌더링을 위한 최첨단 프레임워크를 제공합니다. 이 플랫폼은 낮은 GPU 메모리 사용량, 높은 FPS, 독특한 이종 GPU 시뮬레이션, 그리고 가장 다양한 로봇 태스크를 특징으로 합니다. Sim2Real 및 Real2Sim 환경을 신뢰성 있게 지원하고, 몰입형 VR 텔레오퍼레이션 시스템을 제공하며, 사용자 친화적인 객체 지향 API를 통해 확장 가능한 로봇 학습의 접근성을 높입니다.

---

# Detail Review
