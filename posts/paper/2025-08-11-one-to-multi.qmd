---
title: "From One Hand to Multiple Hands 리뷰"
date: 2025-08-11
categories: [il, rl, vision]
toc: true
number-sections: false
description: Imitation Learning for Dexterous Manipulation from Single-Camera Teleoperation
---


- [Paper Link](https://arxiv.org/abs/2204.12490)
- [Project Link](https://yzqin.github.io/dex-teleop-imitation/)
- [Code Link](https://github.com/yzqin/dex-hand-teleop)


1. 💡 이 연구는 단일 카메라 원격 조작 시스템을 통해 다중 손가락 로봇의 능숙한 조작을 위한 인간 시연 데이터를 효율적으로 수집하는 새로운 방법을 제안합니다.
2. 🤖 본 시스템은 iPad로 인간의 손을 촬영하여 시뮬레이터에 맞춤형 로봇 손을 생성한 후, 이 데이터를 Allegro, Schunk 등 여러 종류의 실제 로봇 손에 맞게 오프라인으로 리타겟팅하여 모방 학습에 활용합니다.
3. 🏆 제안된 방식으로 수집된 시연 데이터는 모방 학습 정책의 시뮬레이션 성능을 크게 향상시키고, Sim2Real 전환 시 더 견고하고 인간과 유사한 동작을 가능하게 함을 보여줍니다.


---

# Brief Review

본 논문은 단일 카메라 기반 텔레오퍼레이션(teleoperation)을 통해 숙련된 조작(dexterous manipulation)을 위한 인간 시범 데이터(human demonstration data)를 수집하고, 이를 이용하여 다지(multi-finger) 로봇 핸드에 대한 Imitation Learning 정책을 학습하는 새로운 프레임워크를 제안합니다. 궁극적으로 학습된 정책을 실제 로봇에 성공적으로 전이(transfer)시키는 것을 목표로 합니다.

**I. 소개**

다지 로봇 핸드를 이용한 Dexterous Manipulation은 로봇공학에서 가장 도전적이고 중요한 문제 중 하나입니다. 로봇 핸드와 조작 대상 간의 복잡한 접촉 패턴은 모델링하기 어렵고, 구조화되지 않은 환경에서 접촉이 많은 작업을 해결하는 제어기를 수동으로 설계하는 것은 매우 어렵습니다. 최근 Reinforcement Learning (RL)이 유망한 결과를 보여주지만, 높은 Degree-of-Freedom (DoF)과 불연속적인 접촉은 RL 정책 훈련의 샘플 복잡성(sample complexity)을 증가시킵니다. 또한, RL 보상을 통한 블랙박스 최적화는 예상치 못하거나 안전하지 않은 행동으로 이어질 수 있습니다. 텔레오퍼레이션을 통해 수집된 인간 시범으로부터 학습하는 것은 Dexterous Manipulation을 위한 자연스러운 해결책입니다. 그러나 대부분의 기존 텔레오퍼레이션 시스템은 Virtual Reality (VR) 장치나 유선 글러브(wired gloves)를 필요로 하여 유연성과 확장성(scalability)이 제한됩니다. Vision-based 텔레오퍼레이션은 특수 장비 착용의 필요성을 없애 비용을 절감하고 확장성을 높이지만, 인간 손 움직임을 로봇 손 움직임으로 변환하는 모션 리타겟팅(Motion Retargeting)이라는 새로운 과제를 제시합니다. 이 과정이 직관적이지 않으면 인간 작업자가 로봇을 제어하기 어렵고, 특정 로봇 손으로 수집된 시범은 동일한 로봇에서만 Imitation Learning에 사용될 수 있다는 한계가 있습니다.

본 논문은 이러한 도전 과제를 해결하기 위해 단일 카메라 텔레오퍼레이션 시스템을 도입합니다. 이 시스템은 iPad와 컴퓨터만으로 효율적으로 3D 시범을 수집할 수 있습니다. 핵심 기여 중 하나는 물리 시뮬레이터(physical simulator) 내에서 각 사용자에 대해 사용자 맞춤형 로봇 핸드(customized robot hand)를 생성한다는 점입니다. 이 핸드는 작업자 손과 동일한 구조를 모방하여 직관적인 인터페이스를 제공하고 데이터 수집 시 불안정한 인간-로봇 핸드 리타겟팅을 피하여 대규모의 고품질 데이터를 얻을 수 있게 합니다. 데이터 수집 후, 맞춤형 로봇 핸드의 궤적(trajectory)은 다양한 실제 로봇 핸드(예: Schunk Robot Hand, Adroit Robot Hand, Allegro Robot Hand)로 변환되어 훈련 시범으로 사용될 수 있습니다. 본 논문의 데이터와 Imitation Learning을 통해 여러 복잡한 조작 작업에서 기존 baseline 대비 큰 개선을 보였습니다. 특히, 학습된 정책이 실제 로봇으로 전이될 때 훨씬 더 강건함(robust)을 보여주었습니다.

**II. 제안하는 시스템 개요**

제안하는 프레임워크는 다음 세 단계로 구성됩니다 (그림 2 참조):
1.  **Customized Hand Teleoperation**: iPad에서 RGB-D 비디오 스트리밍을 통해 작업자의 손 모양을 추정하고, 이를 기반으로 물리 시뮬레이터에 사용자 맞춤형 로봇 핸드를 실시간으로 구성합니다. 인간 작업자는 이 맞춤형 로봇 핸드를 제어하여 Dexterous Manipulation 작업을 수행합니다. 이 시스템을 통해 시간당 약 60개의 성공적인 시범을 효율적으로 수집할 수 있습니다.
2.  **Multi-Robots Demonstration Translation**: 맞춤형 핸드로 수집된 시범 궤적을 오프라인에서 Motion Retargeting 최적화를 통해 실제 로봇 핸드(Schunk, Allegro, Adroit Robot Hand 등)의 상태-액션 궤적(joint position 및 motor command)으로 변환합니다. 이 변환은 손의 기하학적 구조, DoF, 심지어 손가락 개수가 다른 로봇에도 적용 가능합니다.
3.  **Demonstration-Augmented Policy Learning**: 변환된 시범 데이터를 사용하여 Dexterous Manipulation 정책을 훈련합니다. 이는 RL 목적 함수에 시범을 사용하여 Behavior Cloning을 결합하는 방식입니다 (Demo Augmented Policy Gradient, DAPG). 이 프레임워크는 RL 단독으로는 잘 해결되지 않는 복잡한 작업에서 효율적으로 인간과 유사한 기술을 학습할 수 있게 합니다.

학습된 정책은 XArm-6 로봇에 부착된 실제 Allegro Hand로 Sim2Real 전이 실험을 수행하였으며, 본 논문의 시범 데이터를 학습에 통합함으로써 Sim2Real 갭(gap)에 대한 정책의 강건함을 크게 향상시키는 것을 보여주었습니다.

**III. Customized Hand Teleoperation**

텔레오퍼레이션 시스템은 iPad와 노트북으로 구성됩니다 (그림 3 참조). iPad의 전면 카메라를 사용하여 25fps로 인간 작업자의 RGB-D 비디오를 스트리밍합니다. 시스템은 물리 시뮬레이터, 인간 동작을 포착하는 Hand Detector, 시뮬레이션 환경을 시각화하는 GUI로 이루어져 있습니다.

**A. Task Description**

시뮬레이션 환경은 SAPIEN [47]에서 구축되었으며, 세 가지 Dexterous Manipulation 작업이 설계되었습니다:

*   **Relocate**: 로봇이 물체(예: YCB dataset의 Tomato Soup Can, Potted Meat Can, Mustard Bottle)를 들어 목표 위치로 옮깁니다. 초기 및 목표 포즈가 무작위로 설정되는 목표-조건부(goal-conditioned) 작업입니다 (그림 1, 첫째 줄).
*   **Flip**: 로봇이 테이블 위의 머그컵을 뒤집습니다. 머그컵의 위치와 중력 방향을 따라 수평 회전이 무작위로 설정됩니다 (그림 1, 둘째 줄).
*   **Open Door**: 로봇이 문 손잡이를 돌려 문을 잠금 해제한 다음 당겨서 문을 엽니다. 문의 위치가 무작위로 설정됩니다 (그림 1, 셋째 줄).

**B. Hand Detector**

Hand Detector는 RGB-D 프레임을 입력받아 손목 포즈, 손 포즈 파라미터, 손 모양 파라미터를 출력합니다. MediaPipe [49]와 FrankMocap [50]을 기반으로 구현되었습니다. MediaPipe hand tracker로 손 영역의 바운딩 박스를 감지하고, 이를 FrankMocap 모델에 입력하여 SMPL-X [51] 모델 기반의 포즈 및 모양 파라미터를 추정합니다. SMPL-X 모델은 손의 기하학적 구조에 대한 모양 파라미터와 변형에 대한 포즈 파라미터로 손을 나타냅니다. 추정된 파라미터와 PnP (Perspective-n-Point) 알고리즘을 통해 손목의 카메라 변환을 계산합니다.

**C. Customized Robot Hand**

본 시스템은 각 사용자의 손 기하학적 구조를 기반으로 맞춤형 로봇 핸드를 구축합니다. 초기화 시 추정된 손 모양 파라미터로부터 인간 손의 관절 골격(joint skeleton)을 추출하고, 이를 기반으로 물리 시뮬레이터에 동일한 운동학적 구조(kinematics structure)를 가진 로봇 모델을 만듭니다 (그림 4 참조). 효율적인 충돌 감지 및 안정적인 시뮬레이션을 위해 손바닥은 상자, 손가락은 캡슐과 같은 기본 도형을 사용합니다. 맞춤형 핸드는 SMPL-X 모델과 일치하는 45 DoF (15 * 3)를 가지며, Motion Retargeting 없이 감지된 포즈 파라미터를 사용하여 직접 제어됩니다 (표 I 참조).

Customized Robot Hand의 관절 각도는 PD controller로 제어됩니다. 추정된 포즈는 저역 통과 필터(low-pass filter)를 거쳐 위치 목표(position target)로 설정됩니다. 시각 텔레오퍼레이션의 지각 오류(perception error) 문제를 해결하기 위해, 손 모양 추정 결과를 신뢰도 점수(confidence score)로 활용합니다. 초기화 시 최적의 시야에서 추정된 모양 파라미터를 ground-truth로 사용하고, 현재 프레임의 모양 파라미터와의 오차를 통해 포즈 정확도의 신뢰도를 계산합니다. 신뢰도 기반 PD 제어는 다음 수식으로 표현됩니다:

$u(t) = p(t)K_pe(t) + k_d\frac{de(t)}{dt}$

여기서 $u(t)$는 관절 토크(joint torque), $K_p$와 $K_d$는 PD 파라미터이며, $p(t)$는 정규화된 확률 밀도(normalized probability density)로 계산된 신뢰도 점수입니다. 지각 오류가 클 경우, 제어기의 강성(stiffness)을 줄여 원치 않는 갑작스러운 움직임을 제거합니다.

**IV. Multi-Robots Demonstration Translation**

**A. Hand Pose Retargeting**

맞춤형 핸드에서 수집된 시범을 특정 로봇 핸드로 변환하기 위해 Hand Pose Retargeting을 수행합니다. 본 시스템은 이를 오프라인 최적화 문제로 정의합니다.

$\min_{q_R}\sum_{i=0}^N|| f_C^i (q_C_t ) - f_R^i (q_R_t )||^2 + \alpha||q_R_t - q_R_{t-1}||^2$
s.t. $q_{R_{lower}} \le q_R_t \le q_{R_{upper}}$

여기서 $q_C_t$는 맞춤형 로봇의 시간 $t$에서의 관절 위치, $q_R_t$는 특정 로봇(예: Schunk Robot Hand)의 해당 관절 위치입니다. $f_C^i$와 $f_R^i$는 두 로봇의 $i$-번째 키포인트(예: 손가락 끝 위치)에 대한 전방 운동학(forward kinematics) 함수를 나타냅니다. 시간적 일관성(temporal consistency)을 개선하기 위해 관절 위치 변화를 패널티하는 정규화 항을 추가하고, $q_R_t$를 $q_R_{t-1}$의 값으로 초기화합니다. 이 최적화 문제를 해결하면 어떤 특정 로봇에 대해서도 관절 위치 궤적 $q_R_t$를 계산할 수 있습니다 (그림 5 참조).

**B. Action Computation**

Joint Pose Trajectory 외에도 Demo-augmented Policy Learning을 위해 각 손가락 관절에 대한 액션(action), 즉 joint torque 또는 motor control command가 필요합니다. DexMV [54]의 절차를 따라, 먼저 joint pose trajectory를 1차 저역 통과 필터에 통과시킨 후, 로봇 역동학(inverse dynamics)의 조작기 방정식(manipulator equation) $\tau = f_{inv}(q, q', q'')$를 통해 joint torque를 계산합니다.

**V. Demonstration-Augmented Policy Learning**

무작위로 초기화되거나 목표 포즈가 주어지는 작업에서는 단순한 Behavior Cloning만으로는 성공하기 어렵습니다. 따라서 본 논문은 시범을 RL에 통합하는 Imitation Learning 알고리즘인 Demo Augmented Policy Gradient (DAPG) [3]를 사용합니다. DAPG의 목적 함수는 다음과 같습니다:

$$g_{aug} = \sum_{(s,a)\in\rho^\pi}\nabla \ln \pi(a|s)A^\pi (s, a)+\sum_{(s,a)\in\rho^{\pi_{demo}}}\nabla \ln \pi_\theta (a|s)\lambda_0\frac{\lambda_1}{k} \max_{(s',a')\in\rho^\pi}A^\pi (s', a')$$

여기서 첫 번째 항은 RL의 일반적인 정책 기울기(policy gradient) 목적 함수이고, 두 번째 항은 시범을 이용한 Imitation 목적 함수입니다. 이는 Behavior Cloning과 온라인 RL의 조합으로 볼 수 있습니다. $\rho^\pi$는 정책 $\pi$ 하에서의 점유 측정(occupancy measure), $\lambda_0$와 $\lambda_1$은 하이퍼파라미터, $k$는 훈련 반복 횟수입니다. $A^\pi (s', a')$는 이점 함수(advantage function)입니다.

**VI. 실험**

**A. Teleoperation User Study**

제안된 Customized Robot Hand의 이점을 입증하기 위해 17명의 인간 작업자를 대상으로 텔레오퍼레이션 사용자 연구를 수행했습니다. 작업자들은 Customized Robot Hand, Schunk SVH Hand, Adroit Hand, Allegro Hand의 네 가지 로봇 핸드 모델을 사용하여 Relocate 및 Open Door 작업을 수행했습니다. Customised Robot Hand의 경우 인간 포즈 파라미터를 각 관절의 PD 목표로 직접 사용했지만, 나머지 세 로봇의 경우 온라인 Motion Retargeting이 필요했습니다.

결과는 표 II(Relocate)와 표 III(Open Door)에 나타나 있습니다. Customized Robot Hand는 다른 세 로봇 핸드의 온라인 Retargeting 방식에 비해 모든 작업에서 월등히 높은 성공률을 달성했습니다. 예를 들어, Relocate 작업의 경우 Customized Hand는 시간당 약 60개의 성공적인 데모를 수집할 수 있었지만, Allegro Hand를 직접 조작할 때는 10개에 불과했습니다. 사용자들은 Customized Hand가 다른 로봇 핸드보다 제어하기 쉽다고 보고했습니다. 이는 온라인 Motion Retargeting 단계에서 발생하는 제어 불가능한 시간 소모(평균 76 ± 65ms, 큰 편차) 때문인 것으로 분석되었습니다. 온라인 Retargeting을 제거함으로써 시스템은 더 부드럽고 즉각적인 피드백을 제공합니다.

**B. Task Learning Comparison**

Relocate (세 가지 다른 물체), Flip, Open Door 작업에서 처리된 시범 데이터를 사용하여 정책을 훈련하고 RL baseline과 비교했습니다. RL baseline으로는 Trust Region Policy Optimization (TRPO) [56]을 사용했습니다. 정책 및 가치 함수(value function)는 32 × 32의 2-layer Multi-Layer Perceptrons (MLPs)로 구성되었고, TRPO는 각 스텝마다 200개의 궤적을 사용했습니다. Imitation Learning 알고리즘은 DAPG를 사용했으며, 각 작업에 대해 50개의 시범 궤적을 수집하고 이를 특정 로봇으로 Retargeting했습니다.

훈련 곡선은 그림 6에, 세 가지 특정 로봇 핸드의 성공률은 표 IV에 요약되어 있습니다. Imitation Learning 방식인 DAPG는 대부분의 작업과 로봇에서 RL baseline을 능가했습니다. 이는 Motion Retargeting을 통해 생성된 시범이 정책 훈련을 크게 개선할 수 있음을 보여줍니다. 유일한 예외는 Allegro Hand를 사용한 Open Door 작업이었는데, DAPG는 자연스러운 행동으로 손잡이를 잡고 문을 열려고 시도하는 반면, RL 정책은 큰 힘으로 손잡이를 누르며 마찰에 의존하여 문을 여는 경향을 보였습니다 (그림 8). 이는 시범이 정책의 행동을 예상된(인간과 같은) 안전한 방향으로 조절하는 데 중요한 가치를 제공함을 강조합니다.

**C. Ablation Study**

다양한 동적 조건과 시범 데이터 수의 영향을 조사하기 위해, 물체 마찰, 제어기 파라미터, 물체 밀도, 시범 데이터 수를 대상으로 Relocate (tomato soup can, Schunk Robot) 작업에서 Ablation Study를 수행했습니다 (그림 7). 학습 곡선은 마찰 변화에 강건함을 보였는데, 이는 다지 핸드가 여러 접촉점을 통해 힘 폐쇄(force closure)를 형성하여 마찰에 덜 민감하기 때문입니다. 물체 밀도에 대해서도 유사한 결과가 나타났습니다. 제어기 파라미터의 경우, 강성이 클수록 더 빨리 목표에 도달했지만, 작은 PD 값으로도 작업을 해결할 수 있었습니다. 더 많은 시범 데이터를 사용할수록 더 나은 성능을 달성했으며, 20-30개의 데모에서는 분산(variance)이 더 크게 나타났습니다.

**D. Real-World Robot Experiments**

실제 로봇 실험에서는 XArm-6 로봇 암 [58]에 Allegro Hand를 부착했습니다 (그림 9 참조). Relocate 및 Flip 작업을 평가했으며, Sim2Real 전이를 용이하게 하기 위해 훈련 중 물체 포즈에 가산 가우시안 노이즈(additive Gaussian noise)를 적용하고 마찰, 밀도, PD 제어 파라미터와 같은 동적 파라미터를 무작위화했습니다. 관측 공간은 로봇 고유 상태(proprioceptive state), 물체 포즈(초기 포즈는 RealSense D435 카메라로 캡처된 포인트 클라우드를 ICP 알고리즘으로 추정), 그리고 Relocate의 경우 목표 위치를 포함했습니다.

Relocate 작업에서는 훈련에 사용된 물체(known object)와 훈련 중 보지 못한 새로운 물체(novel object) 그룹으로 나누어 정책을 평가했습니다 (그림 10). 정량적 결과(표 V)는 Imitation Learning (DAPG)이 순수 RL보다 실제 로봇 전이 시 훨씬 더 큰 성능 차이를 보임을 보여줍니다. 이는 인간과 유사한 조작 정책이 Sim2Real 갭에 더 강건하기 때문으로 추정됩니다. 더욱 흥미롭게도, 학습된 정책은 훈련 중 보지 못한 새로운 물체에도 일반화되었습니다. 이는 다지 핸드가 인간처럼 작동할 때 형상 변화에 대한 일정 수준의 강건함을 제공함을 시사합니다.

정책 시각화(그림 11)는 Sim2Real 갭에 대한 Imitation Learning 정책의 강건함을 설명합니다. Relocate 작업에서 RL 정책은 불안정한 접촉(두 손가락만 사용)으로 물체를 잡는 경향이 있었지만, 시범으로 훈련된 정책은 네 손가락 모두를 사용하여 안정적으로 잡았습니다. 이로 인해 실제 로봇에서는 RL 정책의 물체가 손에서 미끄러지는 반면, Imitation Learning 정책은 안정적으로 물체를 잡았습니다. Flip 작업에서도 순수 RL 정책은 시뮬레이터에서 컵을 빠르게 밀어 해결했지만, Imitation Learning 정책은 한 손가락을 컵 안에 넣고 손목을 회전시키는 인간과 유사한 행동을 보였습니다. 이러한 RL 정책의 행동은 실제 로봇 핸드에서는 거의 성공하지 못했습니다. 두 가지 예시 모두에서 순수 RL은 시뮬레이터의 물리적 특성을 '해킹'하여 부자연스러운 행동을 학습하는 경향이 있으며, 이는 실제 세계로 전이되기 어렵습니다. 반면, 본 논문의 시범을 사용하여 인간과 유사한 행동을 학습하는 Imitation Learning은 실제 세계 응용에 훨씬 더 강건하고 안정적인 정책을 가능하게 합니다.

**VII. 결론**

본 논문은 Imitation Learning을 위한 인간 손 조작 데이터를 수집하기 위해 새로운 단일 카메라 텔레오퍼레이션 시스템을 제안합니다. 특히, 다양한 인간 작업자가 데이터를 보다 직관적으로 수집할 수 있도록 맞춤형 로봇 핸드(customized robot hand) 개념을 도입했습니다. 수집된 시범 데이터가 여러 로봇에서의 Dexterous Manipulation 학습을 개선하고, 데이터 수집이 단 한 번만 필요함에도 불구하고 실제 세계에 배치될 때 강건성을 높임을 보여주었습니다.


---

# Detail Review


> 논문 리뷰: From One Hand to Multiple Hands – Single-Camera Teleoperation을 활용한 다지 로봇 손 모방학습

## 1. 방법론 (Methodology)

### 1.1 단일 카메라 텔레오퍼레이션을 통한 데모 수집

이 논문의 핵심은 **단일 RGB-D 카메라**(아이패드)에 기반한 텔레오퍼레이션 시스템을 통해 인간 손 시연(demonstration)을 효율적으로 수집하는 새로운 프레임워크다. 사용자는 특수 장비 없이 아이패드 한 대만으로 자신의 손 동작을 촬영하여 **3차원 손 자세와 형상**을 실시간으로 추정한다. 저자들은 MediaPipe와 FrankMocap 기반의 손 추적기를 사용하여, 입력 RGB-D 영상에서 **손목 위치, 손가락 관절 각도(포즈) 및 손 형태(shape) 파라미터**를 추정한다. 여기서 **SMPL-X 모델**을 활용하여 손의 형상과 포즈를 파라미터화하며, 초기 프레임에서 추정된 손 형태 파라미터를 통해 사용자의 손 크기와 모양을 파악한다. 이후 이 정보를 이용해 **물리 시뮬레이터(SAPIEN)** 상에 **사용자 손과 동일한 형태・크기의 맞춤형 로봇 손 모델**을 즉석에서 생성한다. 이 **커스텀 로봇 손**(customized robot hand)은 사용자의 손 골격과 운동학 구조를 그대로 반영하며, 예를 들어 사용자의 엄지손가락이 짧다면 생성된 로봇 손에서도 엄지가 짧게 구현된다. 이렇게 만들어진 로봇 손 모델은 약 **45자유도(DoF)**를 가져 인간 손의 섬세한 움직임을 모사한다.

사용자는 자신의 손을 움직이면, **PD 제어기**를 통해 해당 관절 목표각이 시뮬레이터의 커스텀 로봇 손에 전달되어 로봇 손이 따라 움직인다. 이때 영상 기반 추적의 오차로 인한 **잡음이나 튀는 동작**을 줄이기 위해, 저자들은 손 형태 추정값의 신뢰도를 **가중치로 활용한 PD 제어 기법**을 고안하였다. 구체적으로, 초기 캘리브레이션 시 얻은 **손 shape 파라미터**를 기준값으로 삼고, 매 프레임의 shape 추정치와의 차이를 계산하여 추적 정확도의 신뢰도로 활용한다. 신뢰도가 낮은 경우(손 추적 오차가 큰 경우) PD 제어의 강성을 낮추어 갑작스런 잘못된 동작 전송을 억제함으로써, **부드럽고 안정적인 원격조작**을 가능하게 하였다.

이 시스템은 **온라인 모션 리타기팅(motion retargeting)**을 필요로 하지 않는다는 점에서 기존 방식과 차별화된다. 일반적인 비전 기반 텔레오퍼레이션에서는 사람 손의 관절 움직임을 로봇 손의 관절로 **실시간 변환**해야 하는데, 사람 손과 로봇 손 구조 차이로 인해 제어가 어렵고 지연이 발생하곤 했다. 반면 본 논문에서는 **사용자별로 동일한 구조의 로봇 손을 사용**하므로 이러한 매핑 과정이 불필요하다. 그 결과 사용자는 마치 **자신의 손을 그대로 가상 공간에 옮겨놓은 듯한 자연스러운 방식**으로 물체 조작 시연을 할 수 있다. 저자들의 사용자 연구에 따르면, 제안 시스템으로는 **1시간에 약 60개의 성공 시연**을 모을 수 있는데, 이는 예컨대 사용자가 직접 Allegro 로봇 손(4손가락 로봇)을 조작해 시연을 모을 때의 약 10개에 비해 6배에 달하는 수치다. 실험에 참여한 사람들 역시 **커스텀 손이 기존 로봇 손보다 조작하기 훨씬 쉽다**고 평가하였다. 이러한 높은 데모 수집 효율은 **대규모의 양질의 시연 데이터**를 확보하게 해주며, 결과적으로 모방 학습 성능 향상으로 이어진다.

### 1.2 다중 로봇 손으로의 시연 데이터 변환 (Multi-Hand Demonstration Translation)

시뮬레이터에서 수집된 **맞춤형 로봇 손의 시연 trajectories**는, 오프라인 단계에서 임의의 타깃 로봇 손 모델로 변환(retargeting)된다. 이는 **한 번의 인간 시연 데이터 수집으로 여러 종류의 로봇 손 학습 데이터를 생성**할 수 있음을 의미한다. 논문에서는 **Schunk SVH 5손가락 로봇 손, Adroit 손(샌디에이고 대학 Adroit) 및 Allegro Hand(4손가락)**의 세 가지 서로 다른 상용 로봇 손을 대상으로 실험하였다. 이들 로봇 손들은 **형태(geometry)**와 **운동 범위(DOF)**뿐 아니라 손가락 개수까지 사람 손과 상이하지만, 커스텀 손 시연을 각 로봇 손의 데이터로 변환함으로써 **동일 과제에 대한 다종 로봇 시연 데이터셋**을 구축할 수 있다.

모션 리타기팅은 **각 로봇 손의 관절 움직임 궤적**을 찾는 **최적화 문제**로 공식화된다. 구체적으로, 커스텀 손과 타깃 로봇 손의 **중요 키포인트** (손가락 끝 위치 등)가 최대한 일치하도록 두 손의 관절각을 조정하는 방식을 취한다. 각 시점에서 **양 손 모델의 정방향 운동학 결과(손가락 키포인트 좌표)** 간 오차를 최소화하는 관절 구성을 찾아내며, 시간적 연속성을 위해 **이전 프레임의 해**로 초기화하고 관절 변화량에 대한 정규화 항을 추가하여 **부드러운 궤적**을 얻는다. 이렇게 계산된 **관절 위치 trajectory**는 해당 로봇 손에서의 시연으로 간주되며, 이후 학습 알고리즘에 투입될 수 있다. 특히 Allegro Hand의 경우 인간 손가락보다 하나 적은 **4개 손가락**만 있기 때문에, 최적화 과정에서 인간의 약지/소지 움직임을 나머지 손가락에 분산시키는 등 형태 차이를 보정한다. 저자들은 이러한 **오프라인 리타기팅**은 실시간으로 할 때보다 계산 비용이 크지만, **한 번만 수행하면 되므로 충분히 감내할 수 있는 수준**이라고 설명한다. 실제로 논문에 따르면 오프라인 리타기팅에 수 밀리초 단위의 계산 시간이 들지만, 온라인 리타기팅 시에는 반복 최적화로 인해 프레임 간 지연이 들쑥날쑥 커져 사람이 다음 동작을 예측하며 조작하기 어렵게 된다고 지적한다. 커스텀 손 접근법은 이러한 **온라인 retargeting에 따른 지연과 불안정성 문제**를 근본적으로 해소하였다.

리타기팅된 시연 데이터에는 각 로봇 손의 **상태(관절각 등)** 뿐 아니라 **행동(action) 데이터**도 필요하다. 모방 학습을 위해서는 시연 시퀀스의 상태-액션 쌍이 필요한데, 관절 위치 궤적으로부터 해당 로봇 손의 구동 명령(토크 또는 모터 입력)을 계산하는 절차가 추가된다. 이 부분에서 저자들은 이전 연구인 **DexMV** 방법론을 참고하여, 우선 관절각 시퀀스에 1차 저역통과 필터를 적용하고, 로봇 **역동역학(manipulator equation) 기반**으로 각 시점의 관절 토크를 추정하는 방식을 사용하였다. 이를 통해 **시연 궤적을 따라가는 데 필요한 근사 제어 신호**까지 계산함으로써, 최종적으로 *"(상태, 액션) 궤적"* 형태의 **학습용 시연 데이터**를 완성한다.

### 1.3 모방 학습 알고리즘 및 정책 학습

이렇게 준비된 다수 로봇 손의 인간 시연들을 활용하여, 최종적으로 **다이렉트 정책 학습**(policy learning)을 수행한다. 가장 단순한 접근인 **행동 클로닝(Behavior Cloning)**의 경우 시연 데이터를 바로 모방하도록 학습하면 되지만, 초기 상태나 목표 조건이 변하는 복잡한 작업에서는 순수 행동 클로닝만으로는 성공적인 정책을 얻기 어렵다. 따라서 논문에서는 **강화학습(RL)**에 시연 데이터를 활용하는 형태의 **모방 강화학습 알고리즘**을 채택하였다. 구체적으로, Rajeswaran 등이 제안한 **DAPG (Demo Augmented Policy Gradient)** 알고리즘을 사용하여 **강화학습 목표에 시연 모방 항(term)을 추가**하였다. DAPG는 전문가 시연으로 사전학습(behavior cloning)을 실시한 후, 이후 학습 과정에서도 **정책 그래디언트 계산 시 시연 데이터로부터 유도된 보상(or 정규화 항)**을 추가함으로써, **표준 RL과 BC의 조합**으로 볼 수 있는 방법이다. 본 연구에서는 TRPO(Trust Region Policy Optimization) 알고리즘을 기반 RL 기법으로 사용하고, 여기에 동일한 하이퍼파라미터 세팅으로 DAPG를 적용하여 학습을 진행하였다.

정책 학습은 시뮬레이션 환경에서 이루어지며, 관측 상태에는 **로봇 손 관절 상태**, **손바닥(팜)의 속도**, **물체의 3D 위치와 자세** 등이 포함된다. 과제에 따라 목표물의 위치나 문의 힌지 각도 등의 목표 조건도 관측에 주어지며, 에이전트의 행동은 **손바닥 이동**(자유 공간에서의 6-자유도 움직임은 6차원 속도 제어)과 **손가락 관절 제어**(PD 위치 제어 입력)로 구성된다. 이렇게 학습된 정책은 해당 로봇 손 모델의 시뮬레이션에서 동작을 익히게 되며, 이후 **실제 로봇 손**으로의 이식을 목표로 한다. 특히 실세계로의 일반화를 돕기 위해, 학습 중에 **도메인 랜덤화** 기법을 도입하였다. 예를 들어 물체의 초기 위치나 물리 속성(마찰 계수, 무게 등)을 다양하게 랜덤화하고, 관측되는 물체 상태에 **가우시안 잡음**을 추가하여 센서 노이즈와 환경 차이를 견디도록 훈련했다. 이러한 전략과 **인간 시演의 도입으로 정책이 사람과 유사한 동작 전략**을 학습하게 되어, **시뮬레이션-실세계 간 격차(sim2real gap)**를 극복하는 데 큰 도움이 되었다고 저자들은 밝히고 있다.

## 2. 실험 설정과 결과 평가 (Experiments and Results)

### 2.1 실험 환경과 과제 구성

저자들은 앞서 구축한 SAPIEN 시뮬레이터 환경에서 **세 가지 복잡한 다지 조작 과제**를 실험했다. 각 과제는 실제 인간 시연을 통해 데이터가 수집되고, 이후 정책 학습 및 평가에 활용되었다:

* **Relocate (물체 옮기기)**: 로봇 손이 탁자 위의 **물체를 집어서 임의의 목표 위치로 옮기는 작업**이다. 초기 물체의 자세와 목표 위치는 에피소드마다 무작위로 설정되며, 로봇은 물체를 들어올려 정해진 위치에 놓는 것을 목표로 한다. 이 과제에는 YCB 객체셋의 **토마토 수프 캔, 통조림(Potted Meat Can), 머스타드 병**의 세 가지 물체가 사용되어, **목표지향 다중 물체 조작** 능력을 평가한다.
* **Flip (머그컵 뒤집기)**: 평평한 테이블 위에 놓인 머그잔을 **90도 회전시켜 옆으로 눕히는 작업**이다. 로봇 손은 잔을 움켜쥐고 천천히 기울여 눕혀야 하며, 지나친 힘을 주면 물체가 미끄러지거나 튕겨나갈 수 있다. 이 과제는 **특정 방향으로 힘을 미세하게 가하여 물체를 조작하는 능력**을 평가하며, 매 에피소드마다 머그잔의 초기 위치와 회전 각도가 무작위로 변경된다.
* **Open Door (문 열기)**: 문에 달린 레버 형태의 손잡이를 **돌려서 문을 여는 이단계 작업**이다. 먼저 손잡이를 쥐고 회전시켜 **잠금을 해제**한 뒤, 계속 잡은 상태로 문을 **당겨서 연다**. 로봇 손은 손잡이를 단단히 파지하면서도 회전과 당기기 두 동작을 모두 수행할 수 있는 **적절한 손가락 구성**을 찾아야 한다. 매 시도마다 문의 위치(거리 등)가 약간씩 바뀌어, **일반화된 문 열기 동작**을 익혀야 한다.

각 과제의 **성공 기준**은 명확하게 정의되었다. 예를 들어 Relocate의 경우 에피소드 종료 시 **물체와 목표 지점 사이 거리가 일정 임계값 이하**이면 성공으로 판정하고, Flip은 **머그잔의 기울기 각도가 목표 범위**에 들어오면 성공으로 본다. Open Door는 **문 경첩의 회전각이 일정 각도 이상** 벌어져 실제로 문이 열렸을 때 성공 처리한다. 이러한 성능 지표를 통해 **성공률(success rate)**이나 **에피소드 리턴(return)** 등을 측정하였다.

### 2.2 사용자 원격조작 실험: 커스텀 손 vs. 기존 로봇 손

먼저 **데모 수집 단계의 효용성**을 확인하기 위해 수행된 **텔레오퍼레이션 사용자 연구(user study)** 결과를 살펴본다. 17명의 피험자들이 앞서 정의한 Relocate 과제와 Open Door 과제를 각기 수행하되, 네 가지 다른 로봇 손 모델을 사용하도록 했다. 비교 대상은 (1) 제안한 **맞춤형 로봇 손**, (2) **Schunk SVH** 5손가락 로봇 손, (3) **Adroit** 로봇 손, (4) **Allegro** 로봇 손이었다. 커스텀 손 이외의 세 경우에는 모두 **온라인 모션 리타기팅**을 통해 사용자의 손 동작이 해당 로봇 손으로 매핑되었고, 커스텀 손의 경우 앞서 설명한 대로 **1:1 직접 제어**가 이루어졌다.

평가 방법으로, 각 조합(작업 + 로봇 손)에 대해 **5회씩 연속 시도**하게 하고 그 **평균 성공률과 작업 완료 시간**을 측정하였다. Relocate와 Open Door 모두 **두 단계(stage)**로 나누어 세분화 평가하였는데, 예를 들어 Open Door의 경우 **손잡이 회전 성공 여부**를 1단계, **문 열기 완료 여부**를 2단계로 구분해 각각의 성공률과 소요 시간을 기록했다. 이는 작업 내 세부 단계별로 어느 부분에서 실패하거나 시간이 지연되는지 파악하기 위함이다.

**결과적으로, 커스텀 로봇 손을 이용한 경우 압도적으로 높은 성공률과 더 빠른 수행 시간**을 보였다. 예를 들어 Relocate 과제에서 1단계(물체 들어올리기) 성공률은 **커스텀 손 78.9%**로, Schunk(61.2%), Adroit(58.8%), Allegro(44.7%)에 비해 월등히 높았다. 2단계(물체 이동 완료) 역시 커스텀 손이 **55.3%**로 나머지(30.6%, 28.2%, 16.9%)보다 훨씬 높았다. Open Door 과제에서도 커스텀 손의 1단계(손잡이 돌리기) 성공률이 **95.3%**로 다른 손들(71~~83%)보다 높았고, 2단계(문 열기)도 **82.4%**로 타 로봇 손들(41~~61%) 대비 크게 앞섰다. 작업 완료 시간도 일관되게 커스텀 손이 짧아서, **더 빠르게 과제를 달성**했다. 이러한 사용자 실험을 통해, **맞춤형 손을 이용한 비전 기반 원격조작이 전통적인 로봇 손 직접 제어보다 훨씬 효율적**임이 검증되었다. 저자들은 특히 **Allegro 손의 성능이 가장 저조**했던 점을 지적했는데, 그 이유로 “Allegro는 손가락이 4개라 인간 손 동작을 충실히 맵핑하기 어렵고, 손 크기도 사람 손보다 훨씬 커 제어가 어색하다”고 분석했다. 반면 커스텀 손은 사용자 손과 크기/구조가 같고 추가 매핑 계산이 없기에 **즉각적이고 직관적인 피드백**을 주어 조작을 수월하게 만든 것이다.

결과적으로, 맞춤형 손 시스템은 **데모 수집 단계부터 질적으로 우수한 데이터**(높은 성공률의 시연)를 **다량 확보**하게 해주며, 이는 이후 학습 성능 향상의 기반이 된다.

### 2.3 정책 학습 성능 비교: RL vs. 모방 학습

다음으로, 이렇게 수집된 시연 데이터를 활용한 **정책 학습 결과**를 순수 강화학습과 비교 평가하였다. 각 과제(Relocate - 3가지 객체, Flip, Open Door)에 대해 **TRPO 기반 강화학습(RL)**으로 훈련한 정책과, **DAPG 기반 모방 학습(IL)**으로 훈련한 정책을 비교하였다. 여기서 **시연 데이터의 양은 과제당 50개 에피소드**로 동일하게 제한하였고, 정책 학습은 3개의 시드로 반복하여 **수렴 속도와 최종 성능의 평균**을 비교했다. Figure 4의 학습曲선 및 Table VI의 최종 성공률이 두 방법을 종합적으로 보여준다.

**학습曲선**을 보면, **모방 학습(DAPG)이 순수 RL보다 훨씬 빠르게 초기 성능을 끌어올리고 더 높은 수준에서 수렴**하는 경향이 명확하다. 특히 난이도가 높은 과제일수록 그 격차가 크게 벌어졌다. 최종 **성공률** 지표로 보아도, **대부분의 과제-로봇 조합에서 DAPG 정책이 RL 정책을 능가**했다. 예를 들어 Relocate-토마토캔, 머그 뒤집기 등에서 모든 로봇 손에 대해 모방 학습이 더 높은 성공률을 기록하였다. 이는 **사람 시연으로부터 생성한 데이터가 학습에 유의미한 신호**를 제공하여, 고차원 탐색 공간에서 RL 혼자 학습할 때 발생하는 시행착오를 크게 줄여주기 때문이다.

흥미로운 점은 **Open Door 과제에서 Allegro 손을 사용한 경우**였다. 이 한 가지 사례에서는 DAPG 정책의 최종 성공률이 RL과 큰 차이가 없었는데, 이는 Allegro 손의 구조적 한계와 과제의 난이도가 맞물려 나타난 결과로 보인다. 비록 정량적 성능 향상은 작았지만, 두 접근법의 **동작 양상은 크게 달랐다**. 저자들이 시각적으로 정책 행동을 관찰한 바로는, **DAPG로 학습된 정책은 사람처럼 손잡이를 쥐고 돌린 후 당기는 자연스러운 전략**을 구사한 반면, **순수 RL 정책은 손잡이를 제대로 쥐지 못하고 손바닥으로 강압적으로 누르면서 마찰력으로 문을 여는** 비교적 **비정상적인 방법**을 사용했다고 한다. 후자는 시뮬레이터 상에서는 우연히 성공할지 모르나 실제 세계에선 통하지 않을 가능성이 크다. 이 사례는, **모방 학습을 통해 얻은 정책이 보다 인간스러운 동작으로 안전하고 예상 가능한 범위 내에서 과제를 수행**함을 보여준다. 결국 **대부분의 상황에서 인간 데모 활용이 학습 성능과 행동 품질을 향상**시킴이 입증되었다.

### 2.4 추가 실험: Ablation 및 영향 요소 분석

저자들은 모방 강화학습의 성공에 기여하는 요소들을 분석하기 위해 몇 가지 **요인 별 Ablation 실험**도 수행하였다. 예를 들어, Relocate(토마토 수프 캔) + Schunk Hand 조합에 대해, **(a) 물체 마찰계수**, **(b) 물체 밀도(무게)**, **(c) PD 제어기의 강성/감쇠 계수**, **(d) 학습에 사용한 데모 개수** 등을 변화시켜 가며 DAPG 학습의 민감도를 관찰했다. Figure 5에 제시된 학습曲선을 통해 각 조건 변화가 학습 속도와 최종 성능에 미치는 영향을 비교하였는데, 전반적으로 **환경 물리 파라미터의 변화에도 시연 데이터가 포함된 학습은 안정적으로 동작**함을 보였다. 예컨대 물체 마찰이나 무게가 달라져도 데모를 포함한 정책은 비교적 **강인한 성능 유지**를 보였으며, 이는 시연을 통해 학습한 **인간 특유의 적응적 조작 전략**이 작용한 결과로 해석된다. 또한 데모 개수에 따른 실험에서는, **시연 데이터가 많을수록 학습 성능이 개선되다가 어느 정도 수렴**하는 양상이 나타났는데, 이는 추가 데모가 초기 학습에 도움은 되지만 과도한 경우 수확 체감이 있음을 시사한다. PD 제어 파라미터의 경우 너무 낮은 강성은 정확도 저하로, 너무 높은 강성은 진동 증가로 이어져, **적절한 튜닝이 필요함**을 실험으로 확인하였다.

### 2.5 실세계 로봇에의 적용 및 성능

최종적으로, 시뮬레이션에서 학습된 정책을 **현실의 로봇 손**에 이식하여 검증하였다. 하드웨어 플랫폼은 **Allegro Hand + XArm-6 로봇 팔** 조합으로, 시뮬레이션의 Allegro Hand 모델과 동일한 로봇 손을 6자유도 로봇 팔 끝에 장착한 구성이다. 저자들은 학습된 정책으로 실세계에서 위 과제들을 수행해보고 **성공률과 동작의 안정성**을 평가했다. 그 결과, **인간 시연을 포함하여 학습된 정책은 시뮬레이터에서뿐만 아니라 실제 환경에서도 높은 성공률과 강인한 성능**을 보였다. 더욱이 시뮬레이션에서는 보지 못했던 **새로운 객체나 변형된 상황에도 정책이 비교적 잘 적응**하는 모습을 보였는데, 이는 **학습 과정에서 인간 시연을 통해 얻은 일반적인 조작 원리와 자연스러운 힘 가하기 전략** 덕분으로 풀이된다. 반면, **동일 환경에서 순수 RL로 학습된 정책은 현실에서 거의 실패**하였는데, 시뮬레이터 상의 비현실적인 전략(예: 마찰로 문 밀기 등)이 현실에선 통하지 않고, 미세한 동작 오차에 대한 보정 능력도 부족했기 때문이다. 정량적으로 실험 횟수가 제한되어 구체적인 성공률 수치로 비교하진 않았지만, **데모 기반 정책이 월등히 안정적인 성과**를 보인 것은 분명하다. 요약하면, 본 논문의 접근법은 **시뮬레이션-현실 간 격차를 줄여주는 인간 데모의 힘**을 입증했으며, 복잡한 다관절 손 조작 작업을 실제 로봇으로 수행하는 데 있어 **모방 학습의 유용성**을 보여주었다.

## 3. 기존 연구와의 비교 (Comparison with Prior Work)

본 연구는 **다지 로봇 손의 정교한 조작**과 **인간 시연 학습** 분야에서 여러 기존 접근들과 구별되는 혁신점을 제시한다. 여기서는 관련 선행 연구들과 비교하여 이 논문의 차별성과 기여도를 분석한다.

* **VR 글러브 기반 데모 수집 vs. 카메라 기반 데모 수집**: 인간 시연을 로봇 학습에 활용하려는 시도는 이전부터 있어 왔다. 대표적으로 2018년 Rajeswaran 등은 **VR 장비와 데이터글러브**를 활용해 인간이 가상현실에서 로봇 손을 조작하며 데모를 모으고, 이를 활용해 **DAPG 알고리즘**으로 정책을 학습하는 연구를 수행했다. 이 방식은 성공적인 Dexterous 핸드 조작을 보여주었지만, **전용 장비가 필요하고 한정된 인원만 참여**할 수 있어 데이터 수집의 **확장성(scalability)**이 떨어진다는 단점이 있었다. 반면 본 논문은 **특수 장비 없이 카메라만으로 누구나 데모를 제공**할 수 있는 환경을 마련함으로써, 다양한 사용자로부터 **대량의 데모를 손쉽게 확보**할 수 있게 했다. 실제로 “VR을 통한 수집은 많은 인적 노력(human effort)을 요해 확장성이 낮지만, 단일 카메라 텔레오퍼레이션은 프로세스를 **더욱 손쉽고 대규모로 확장 가능**하게 만든다”고 저자들도 강조한다. 이러한 접근은 향후 **클라우드 로보틱스나 크라우드소싱**을 통해 방대한 인간 시연 데이터를 모을 수 있는 길을 열었다는 점에서 의의가 크다.

* **멀티캠 Vision 텔레오퍼레이션 vs. 단일캠 접근**: 비전 기반의 원격조작 자체는 과거에도 연구된 바 있는데, **DexPilot** 연구가 그 선구적 예다. DexPilot에서는 **4대의 깊이 카메라(RealSense)**를 테이블 주변에 배치하고 배경을 검은 천으로 막는 등 상당히 복잡한 세팅을 통해, 인간 손 영상을 인식해 Allegro 로봇 손을 제어하였다. 이와 비교하면 본 논문은 **카메라 한 대(iPad)**로 동일한 목표를 달성하면서, 추가로 **사용자별 커스텀 손 인터페이스**까지 제공하여 조작의 직관성을 높였다. 무엇보다도, DexPilot 등 이전 연구들은 **단일 특정 로봇 손**(예: Allegro)만을 대상으로 하였는데, 본 연구는 **동일한 인간 시연 데이터를 여러 로봇 손으로 변환하여 학습에 활용**하는 **범용성**을 선보였다. 이는 이전에는 볼 수 없었던 새로운 개념으로, 예컨대 한 사람의 시연으로 쉥크 손, 어드로이트 손, 알레그로 손 각각의 정책을 모두 학습시킬 수 있음을 처음으로 실증하였다. 이러한 **Multi-hand demonstration** 아이디어는 로봇 핸드 하드웨어가 다양한 현실에서 **데이터 효율적인 학습**을 가능케 하는 방향성을 제시한다.

* **단순 그리퍼(2-jaw) 작업 vs. 고차원 다지 조작**: 인간 시연을 비전으로부터 가져와 학습하는 연구 중에는, 비교적 간단한 병렬 그리퍼(집게 형태 로봇 핸드)로 **픽앤플레이스** 같은 작업을 가르치는 사례들이 있었다. 예를 들어 인간 동작 영상을 모방해 2-finger 그리퍼로 물체를 집는 정도의 과제는 3D 정보 없이도 가능하여, 2D 영상 기반 imitation이 시도되었다. 그러나 이러한 선행 연구들은 **저차원 제어(몇 개 관절)**와 **단순 작업**에 국한되어, 복잡한 손가락들을 활용한 3D 상호작용 과제에는 적용하기 어려웠다. 본 논문의 시스템은 **RGB-D를 통해 3D 손-물체 포즈 정보를 추출**하고, 이를 활용해 **복잡한 접촉이 있는 작업**들(예: 손잡이 돌려당기기)을 시연 및 학습하였다는 점에서, **기존 영상 모방학습 연구들을 한 단계 발전**시켰다. 요컨대 **보다 난이도 높은 다지 조작 작업에 비전 기반 모방학습을 확장**한 사례로 평가된다.

* **다수 로봇 손 간 정책 전이**: 다관절 로봇 손 연구에서 **로봇 구조가 바뀌면 새로 학습을 해야 하는 문제**는 오래 지속된 숙제였다. 예를 들어 5손가락 로봇에 맞춰 학습된 정책은 4손가락 로봇에는 바로 적용하기 어렵고, 심지어 손가락 길이 비율 차이만 있어도 성능에 큰 영향이 있었다. 이를 해결하려는 시도로 **meta-learning**이나 **이종 로봇 간 domain adaptation** 등의 연구가 일부 있었지만, 여전히 **사전 데이터 준비나 보정이 까다로웠다**. 본 연구는 사람 손이라는 **공통 참조 기준(human hand as common reference)**을 활용하여, 처음에는 사람 손과 같은 구조의 커스텀 손으로 시연을 모으고 나서 이를 각 로봇 손으로 변환함으로써 **자연스럽게 여러 형태의 로봇 시연 데이터**를 확보했다. 이렇게 함으로써 **여러 로봇 플랫폼에 걸쳐 학습 데이터를 공유**할 수 있게 되었고, 결과적으로 각 로봇별 최적 정책을 학습하면서도 **필요한 인간 시연 횟수는 최소화**하는 성과를 거두었다. 이는 기존의 multi-finger manipulation 연구에서 한 종류 로봇에 특화된 데이터를 썼던 관행과 달리, **범용적인 시연 데이터 활용** 가능성을 처음 보여준 것이다.

* **정책의 자연스러움과 안전성**: OpenAI의 유명한 **Rubik’s Cube 푸는 로봇 손**과 같은 사례에서 볼 수 있듯, 순수 강화학습을 통해 다관절 손 조작을 습득시키는 것은 가능하지만 종종 **비인간적인 해법**(예: 큐브 던졌다 받기 등)이 나타나거나, **현실 적용 시 예기치 못한 동작**을 보일 수 있다. 이에 비해 본 논문의 접근은 **인간의 데모를 통해 정책이 인간과 유사한 동작 분포를 따르도록 유도**하였고, 그 결과 RL만 사용했을 때 나타나는 위험하거나 이상한 행동을 억제하는 효과가 있었다. 논문에서 묘사된 것처럼, 문 열기 동작에서 순수 RL 정책은 문을 **마찰로 억지로 여는** 방법을 택했지만, 데모를 참고한 정책은 **손잡이를 제대로 잡고 트는** 보다 **안전하고 일반화 가능한 방법**을 구사했다. 이런 차이는 산업적 응용에서 매우 중요하다. 즉, 본 연구는 **기존 대비 안전하고 신뢰할 수 있는 다지 손 조작 정책**을 얻는 데 기여했다고 평가할 수 있다.

요약하면, *"From One Hand to Multiple Hands"* 논문은 **비전 기반 텔레오퍼레이션, 맞춤형 로봇 손 개념, 다중 로봇 간 시연 이식, 데모-강화 학습 통합** 등을 한 데 묶어 **다지 로봇 손 학습의 새로운 지평**을 연 연구라 할 수 있다. 이전 연구들에 비해 **데이터 수집의 용이성과 확장성**, **학습 정책의 성능과 현실 적합성** 측면에서 뛰어난 성과를 보였으며, 향후 다양한 형태의 로봇 손이나 복잡한 조작 작업에 **범용적으로 적용될 수 있는 모방학습 프레임워크**로서 큰 영향을 미칠 것으로 기대된다.
