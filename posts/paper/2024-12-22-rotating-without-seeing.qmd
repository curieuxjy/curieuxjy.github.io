---
title: "📃Rotating without Seeing 리뷰"
description: Towards In-hand Dexterity through Touch
date: "2024-12-22"
categories: [paper, tactile, rl, hand]
toc: true
number-sections: true
image: ../../images/2024-11-23-rotating-without-seeing/flow.png
format:
  html:
    code-fold: true
jupyter: python3
---

<center>
<img src="../../images/2024-11-30-rotating-without-seeing/1.png" alt="22" width="100%" />
</center>

이번 포스팅에서 리뷰할 논문은 [Rotating without Seeing: Towards In-hand Dexterity through Touch](https://arxiv.org/abs/2303.10880) 입니다. RSS(Robotics: Science and Systems) [2023 학회](https://roboticsconference.org/2023/)에서 발표된 해당 논문은 사람이 **시각 없이 촉각만으로** 손안에서 물체를 정교하게 조작하는 능력을 로봇 핸드에 구현하고자, 손바닥, 손가락 관절, 손끝 전체에 넓게 분포된 **저비용의 이진 촉각 센서를 활용**하여, 시뮬레이션에서 **강화학습으로 학습한 정책을 실제 로봇 손에 적용**하고, 이를 통해 학습한 물체뿐만 아니라 **학습하지 않은 새로운 물체까지 조작**할 수 있는 시스템인 **`Touch Dexterity`**를 제안합니다.

# Introduction

대다수의 기존 연구들은 점점 더 `고품질`의 센서를 활용하여 `정밀하고 세밀한 접촉`을 다루는 데 초점을 맞추었습니다. 하지만 이러한 연구들은 대개 `비싼 센서를 그리퍼나 손의 손가락 끝부분에만 부착`할 수 있어, **조작기 전체를 감지하지 못한다는 한계**를 가지고 있습니다. 이로 인해 수행할 수 있는 작업의 범위가 제한됩니다. 또한 복잡한 작업을 위해 대량의 학습 데이터를 필요로 하지만, 고정밀의 센서들을 사용하는 경우 더욱더 벌어지는 Sim2Real 간의 차이로 인해 시뮬레이터를 활용하기 어려운 문제가 존재합니다. 

Touch Dexterity는 물체를 "보고" 조작하는 대신 단순히 **접촉만으로 물체를 회전하거나 조작하는 기술**입니다. 이 접근법은 **손의 한쪽 면(손가락 끝, 링크, 손바닥)에 부착된 저비용의 binary force 센서를 활용**합니다. 이러한 **센서는 접촉 여부만을 판단**하며, 이를 통해 물체의 상태를 "느낄 수" 있게 합니다. 16개의 센서를 조합하면 최대 2¹⁶가지 상태를 표현할 수 있어 강력한 표현력을 가질 수 있고, 또한 Sim2Real 격차는 시뮬레이션을 통해 충분한 데이터를 확보함으로써 해결 가능하며, 이 때 binary 센서는 단순한 구조로 인해 노이즈에 덜 민감하다는 장점이 있습니다.

<center>
<img src="../../images/2024-11-30-rotating-without-seeing/3.png" alt="22" width="100%" />
<figcaption>Amazon에서 약 8달러 정도로 저렴하게 판매되고 있는 FSR 센서를 부착한 모습</figcaption>
</center>
Touch Dexterity는 다중 손가락 로봇 손을 사용하여 **"보이지 않는" 물체를 x, y, z 축으로 회전시키는 작업**에 초점을 맞추고 있으며, 이는 in-hand re-orientation task의 단순화된 버전으로 볼 수 있습니다. 여기서 `보이지 않는` 물체란 단순히 시각 센서가 없는 것을 의미할 뿐만 아니라, 학습 중에 보지 못한 물체들을 의미하기도 합니다. 강화 학습(RL) 정책은 binary touch sensing 정보와 로봇의 내부 상태를 입력으로 받아 각 시간 단계에서 폐루프(closed loop) 제어를 위한 행동을 예측합니다. 강화학습 Agent는 물체의 3D 구조와 자세를 암묵적으로 학습하여 이를 기반으로 회전을 수행하며 실제 로봇 시스템 테스트에서는 10개의 다양한 물체를 활용하여 실험을 진행했습니다. 아래의 그림은 Unseen 물체였던 오리 인형을 real world에서 in-hand manipulation을 하는 장면입니다.

<center>
<img src="../../images/2024-11-30-rotating-without-seeing/2.png" alt="22" width="100%" />
<figcaption>Rotate the rubber duck for two cycles without falling, even if it is never presented in training</figcaption>
</center>


**Dexterous Manipulation**



기존의 분석적 모델 기반 접근법은 물체와 컨트롤러에 대한 지나치게 많은 가정을 필요로 하여 복잡한 문제로 확장하는 데 한계가 있습니다. 모방 학습(imitation learning)은 주로 시각 입력에 의존하며, proprioception 데이터 내에 포함된 암묵적인 촉각 정보를 통해 물체의 정보를 추론할 수 있지만, 주로 손가락 끝에서의 물체 회전이나 제한된 물체 집합의 회전에만 초점을 맞춥니다. 반면, Touch Dexterity는 촉각 센서를 사용하여 손과 물체 간의 상호작용을 명시적으로 인지하고, 다양한 종류의 물체에 대해 손바닥 위에서의 물체 회전 문제를 풉니다. 이는 복잡한 물체의 움직임을 포함하며, 훨씬 더 도전적인 문제를 다룰 수 있고 학습되지 않은 새로운 물체에 대해서도 일반화가 가능하다는 장점을 가지고 있습니다.



**Tactile Robotic Manipulation**

`What type of touch information is essential?`

기존 연구들은 다양한 센서를 활용하여 조작을 지원하기 위해 국소적인 형상 정보(local geometry), 힘과 토크, 접촉 이벤트, 물질 특성을 추출하는 방법을 제안해왔습니다. 심지어 간단한 binary 접촉 신호를 sparse sensor array로 제공받는 경우에도 high-dimensional manipulation task에서 유용할 수 있습니다. 일례로, [Shadow Hand](https://en.wikipedia.org/wiki/Shadow_Hand)에 손바닥에 밀집된 센서 레이아웃을 활용한 [연구](https://ieeexplore.ieee.org/document/7353568)도 있었습니다. 

`How can tactile events be simulated to facilitate Sim2Real transfer?`

일반적으로 접촉 표면(contact surface)에서 normal & shear tactile force field을 시뮬레이션합니다. 반면, Touch Dexterity는 별도의 시뮬레이션 설계를 요구하지 않고, 기존 물리 시뮬레이터의 내장된 contact 시뮬레이션을 활용할 수 있다는 점에서 효율적이라고 볼 수 있습니다.



# Learning Touch Dexterity



논문에서 제안된 Touch Dexterity의 AI 모듈이 학습되는 과정에 대해 살펴보겠습니다.



## Problem Formulation

Touch Dexterity는 강화학습 방법으로 제어를 하기 때문에 강화학습의 문제 정의 방식인 MDP(Markov Decision Process)의 요소들, State, Action, Reward 순으로 확인해보겠습니다.

### State

State는 Hand Robot Agent의 상태를 나타내는 요소들로 이루어집니다. Allegro hand 로봇의의 joint position(16 차원), sensor observation(16차원), 이전 position target(16차원), 그리고 rotation axis(2차원)로 구성되어 있습니다. 핸드 로봇의 관절(joint) 부분들이 작은 모터들 16개로 이루어져 있고, FSR([Force Sensing Resistor](https://en.wikipedia.org/wiki/Force-sensing_resistor)) 센서들도 총 16개가 아래 그림처럼 손가락과 손바닥에 분포되어 있어 State 벡터의 차원들이 다음과 같이 구성되게 됩니다. 이렇게 구성된 State가 학습 시에 Policy Network의 Input으로 들어가게 되는데 1 time step 정보만으로는 학습하기에 부족한 정보량이기 때문에 현재 시점 기준 이전 스텝 2 time step을 합쳐(concatenation), 총 **3 time step 을 쌓아서** policy network에 input으로 넣어줍니다.

<center>
<img src="../../images/2024-11-30-rotating-without-seeing/4.png" alt="22" width="100%" />
<figcaption>State 구성요소</figcaption>
</center>


### Action

Hand Agent가 움직이는 Action은 로봇의 각 관절(joint) 모터들이 움직이는 것으로 생각할 수 있습니다. 따라서 Policy network에서는 16차원의 모터와 관련된 어떠한 command 정보가 나오게 됩니다. 하지만 Policy network의 output인 $a_t$를 바로 쓰는 것이 아닌 [PD Controller](https://en.wikipedia.org/wiki/Proportional%E2%80%93integral%E2%80%93derivative_controller)에 적용하기 위한 값으로 변환하는 과정을 거치게 됩니다. 결과적으로 PD Controller에서 사용하는 값은 $\tilde{q}_{t+1}$ (현재 time step이 $t$ 이므로 앞으로 제어할 position의 time step 첨자는 $t+1$)인 것 입니다.

하지만 여기서 $\tilde{q}_{t+1}$을 바로 적용할 경우 생기는 문제가 있습니다. policy network output 값들이 연속적인 시간 순으로 봤을때 갭이 큰 값들이 나타나게 되면 부드러운 움직임을 가질 수 없습니다. 따라서 해당연구에서는 [Exponential moving average](https://en.wikipedia.org/wiki/Moving_average) 방법을 사용하여 smoothing하는 과정을 거치게 됩니다.




<center>
<img src="../../images/2024-11-30-rotating-without-seeing/5.png" alt="22" width="100%" />
<figcaption>Action이 적용되는 과정</figcaption>
</center>
아래 그래프는 논문에서 제시한 파라미터($\eta$, 2 consecutive steps)로 랜덤한 포인트들을 가지고 smoothing하는 모습을 보여줍니다.

```{python}
import matplotlib.pyplot as plt
import numpy as np

# Parameters
eta = 0.8  # Smoothing factor
steps = 2  # Step size for x-axis
n_points = 200  # Number of points

# Generate data
x = np.arange(0, n_points, steps)
data = np.sin(x / 5) + np.random.normal(0, 0.3, len(x))  # Random data with noise
ema = []

# Calculate EMA
for i, point in enumerate(data):
    if i == 0:
        ema.append(point)  # Initialize EMA with the first data point
    else:
        ema.append(eta * point + (1 - eta) * ema[-1])

# Plot
plt.figure(figsize=(8, 3))
plt.plot(x, data, label="Data", marker="o", linestyle="--", alpha=0.6)
plt.plot(x, ema, label="Exponential Moving Average (EMA)", linewidth=2)
plt.xlabel("Step")
plt.ylabel("Value")
plt.title(f"Exponential Moving Average (eta={eta}, step={steps}) ")
plt.legend()
plt.grid(True)
plt.show()

```

이렇게 최종적으로 계산된 Action 값으로 Hand Agent의 모션이 만들어지게 됩니다.

### Reward

보상함수는 아래와 같이 6개의 term들로 구성되어 있습니다. 각 6개의 reward term들은 linear weighted sum이 되어 해당 timestep에서의 최종 reward가 됩니다.

<center>
<img src="../../images/2024-11-30-rotating-without-seeing/6.jpg" alt="22" width="100%" />
<figcaption>Reward Function</figcaption>
</center>


1. Reward of rotation $r_{rot}$
   - 회전 축 $k$의 법선 평면 $\Pi$에서 샘플링된 단위 벡터의 회전 각도 $\Delta \theta$로 정의된 회전 보상입니다.
<center>
<img src="../../images/2024-11-30-rotating-without-seeing/7.jpg" width="100%" />
<figcaption>Reward Function</figcaption>
</center>
   - 계산하는 과정
     - 법선 평면 $\Pi$에서 단위 벡터 $v$를 임의로 샘플링하며, 이 벡터는 object에 부착된 것으로 간주할 수 있습니다. 
     - 다음 상태에서의 해당 벡터 $v'$를 가져와 $\Pi$에 투영(projection)합니다: $v'_p = \text{Proj}(v', \Pi)$
     - $\Delta \theta \in [-\pi, \pi)$는 축 $k$에 대해 $v'_p$와 $v$ 사이의 부호 있는 거리로 정의됩니다.
   - object의 움직임이 매우 복잡한 경우, 시뮬레이터가 제공하는 각속도가 매우 노이즈가 심하기 때문에 보상에 이 각속도를 보상함수에 사용할 경우 특정 자세에서 진동하는 등 바람직하지 않은 object 움직임 패턴이 나타날 수 있습니다. 
   - 이 유한 차분(finite difference)을 보상으로 사용하는 것이 서로 다른 실행에서도 일관된 회전 동작을 생성할 수 있습니다.

2. Penalty of object's velocity $r_{vel}$
   - 손이 object를 안정적으로 회전시키도록 장려하며, 훈련된 정책의 transferability을 향상시킵니다.

3. Penalty of falling $r_{fall}$
   - object가 손바닥에서 떨어질 때 적용되는 negative penalty입니다.
4. Penalty of the work controller $r_{work}$
   - 컨트롤러의 일(work)의 양을 패널티로 부과합니다. 이 reward term의 torque $\tau$는 $t$에서 PD 컨트롤러가 출력한 토크입니다. 이 페널티는 손가락 움직임의 부드러움을 향상시키는 데 도움을 줍니다.

5. Penalty of torque $r_{torque}$
   - 큰 토크 출력값에 패널티를 부과합니다.

6. Reward of distance $r_{dist}$
   - 거리 보상으로, 손끝이 객체에 가까이 가서 상호작용하도록 장려합니다.
     - $d(x_{\text{tip}}, x_{\text{obj}})$는 손끝 위치 $x_{\text{tip}}$와 객체 위치 $x_{\text{obj}}$ 사이의 거리입니다.
     - $\epsilon$은 작은 양으로, 분모가 0이 되는 것을 방지합니다.
     - $c_2$와 $c_3$는 보상의 클리핑 범위를 정의하는 하한과 상한입니다.

### Reset

불필요한 exploration를 줄이고 학습 과정을 가속화하기 위해 object가 초기 위치(즉, 손바닥의 중심)에서 너무 많이 벗어날 경우 에피소드를 리셋합니다. 또한, object의 주요 축이 회전 축에서 너무 많이 벗어날 경우에도 에피소드를 리셋합니다.



## Domain Randomization

강화학습의 Sim2Real Gap을 줄이기 위해 학습 단계에서 Domain Randomization을 적용하는데 해당 연구에서는 2가지 Domain Randomization을 진행합니다.

1. **물리적 랜덤화**:

   - rotation하는 object의 초기 위치, 질량, 형태, 마찰을 랜덤화하여 학습된 정책이 다양한 종류의 객체를 처리할 수 있도록 합니다.
   - PD 컨트롤러의 게인을 랜덤화하여 실제 환경에서 PD 컨트롤러의 불확실성을 모델링합니다.
   - 각 촉각 센서를 랜덤화하는 것도 고려합니다. 활성화된 접촉 센서(출력이 1인 경우)에 대해, 확률 $p$로 출력을 0으로 뒤집습니다.
   - 지수 지연 모델(exponential delay)을 통해 접촉 센서의 신호 지연을 모델링합니다.

2. **비물리적 랜덤화**

   - policy의 observation과 출력된 action에 화이트 노이즈를 주입하여 작은 외란에도 강인하도록 만듭니다.

     

## Training Procedure

Proximal Policy Optimization (PPO) 알고리즘을 사용하며 policy 네트워크와 value 네트워크 모두에 다층 퍼셉트론(MLP)을 사용했습니다.

- **훈련 설정**:
  - 이점(advantage) 클립 임계값 ϵ=0.2\epsilon = 0.2와 KL 발산 임계값 0.020.02를 사용
  - 네트워크에서 활성화 함수로 ELU를 사용
  - 정책 네트워크는 학습 가능한 상태 독립적인 표준편차를 가지는 가우시안 분포를 출력
- **비대칭 관찰(asymmetric observation)**:
  - 정책 및 가치 네트워크의 학습 난이도를 줄이기 위해 asymmetric observation 을 사용
    - **가치 네트워크**: 입력에 접촉력, object의 ground-truth pose, 물리적 파라미터와 같은 특권 정보를 추가
    - **정책 네트워크**: 현재 상태와 함께 3개의 과거 상태를 입력으로 사용하며, 특권 정보는 접근할 수 없음
- **시뮬레이션 설정**:
  - IsaacGym 시뮬레이션에서 시간 간격($dt$)은 0.01667초로 설정하고, 2 sub step을 사용
  - 8192개의 병렬 환경에서 시뮬레이션을 실행
  - 정책 네트워크가 출력하는 행동(제어 목표)은 6단계 동안 실행되며, 이는 실제 환경에서 10Hz의 제어 주파수에 해당

<center>
<img src="../../images/2024-11-30-rotating-without-seeing/10.jpg" width="100%" />
<figcaption>Training Process</figcaption>
</center>


# Tactile Dexterous Manipulation System


## Real-world System Setup

<center>
<img src="../../images/2024-11-23-rotating-without-seeing/flow.png" width="70%" />
    <figcaption>Overview</figcaption>
</center>

하드웨어 구성은 XArm 로봇 팔과 16자유도(16-DOF)를 가진 Allegro Hand에 접촉 센서 배열을 장착한 형태로 이루어져 있습니다. 손바닥과 손가락 끝을 포함한 Allegro Hand의 여러 부위에 부착된 16개의 접촉 센서로 구성됩니다.

사용된 접촉 센서는 외부 힘이 표면에 가해질 때 저항이 변하는 Force-Sensing Resistor(FSR) 기반입니다. STM32F 마이크로컨트롤러를 사용하여 각 센서의 아날로그 전압 신호를 수집하고, 이를 디지털 신호로 변환하여 호스트로 전달합니다. 이 접촉 센서는 연속적인 접촉력 측정을 출력할 수 있지만, 신호는 일반적으로 비선형적이고 노이즈가 많습니다. 따라서 이를 제어에 사용하기 전에 적절한 전처리가 필요합니다. 선택된 임계값 $\theta_{\text{th}}$에 따라 이 측정값을 이진화(binarize)하고 이 신호를 사용합니다.

**이진 신호를 사용하는 장점**:

- 시뮬레이션과 실제 로봇 간의 차이를 줄이고, Sim2Real 전이 절차를 단순화할 수 있습니다.
- 이진화된 측정값은 임계값을 조정하여 쉽게 보정(calibrate)할 수 있습니다.

## Simulation Setup

이 논문에서는 IsaacGym 시뮬레이터  사용합니다. 각 접촉 센서는 손가락과 손바닥 링크의 고정된 링크로 시뮬레이션됩니다. 
시뮬레이터는 매 시뮬레이션 단계에서 각 센서 링크에 대한 순 접촉력 $F=[Fx,Fy,Fz]F = [F_x, F_y, F_z]$를 제공합니다. 
$\|F\|$을 시뮬레이션된 접촉력 측정값으로 사용한 다음, 이 측정값을 다른 임계값 $\tilde{\theta}_{\text{th}}$으로 이진화합니다.

<center>
<img src="../../images/2024-11-30-rotating-without-seeing/9.jpg" width="100%" />
</center>


중요한 점은 센서의 부모 링크에서 제공되는 힘은 순 접촉력에 기여하지 않는다는 것입니다. 
시뮬레이션에서 실제 환경과 유사한 동작을 보장하기 위해 이 센서들의 임계값 $\tilde{\theta}_{\text{th}}$을 조정합니다. 
시뮬레이션에서는 $\tilde{\theta}_{\text{th}} = 0.01N$을 사용합니다.


## Benchmark: In-hand Rotation

이 논문에서는 시스템의 손재주(dexterity)를 연구하기 위해, 시스템을 사용하여 손 안에서 회전하는 작업(`in-hand rotation task`)을 목표로 합니다. 이 task는 object가 손바닥에 초기화된 상태로 시작하며, 로봇 손은 주어진 회전 축을 따라 객체를 회전시켜야 합니다. 손 안에서 객체를 회전하는 동안, object의 움직임은 손끝 회전(`finger-tip rotation`)보다 훨씬 더 복잡하며 특히, 손 안에서 조작하는 동안 객체는 손바닥에서 미끄러지거나 구를 수 있습니다.

이와 같은 복잡한 움직임 패턴 때문에, 성공적인 조작을 위해 촉각 센서나 비전(vision) 시스템의 명시적인 피드백이 필요합니다. 그렇지 않으면, 현재 객체의 상태를 추론할 수 없으며, 객체를 안전하게 밀거나 회전시키는 데 실패할 수 있습니다.

<!--


## Discussion

# Experiments

-->

# Reference

- [Original Paper](https://arxiv.org/abs/2303.10880)
- [Project Homepage](https://touchdexterity.github.io/)