---
title: "📃IBRL 리뷰"
date: 2025-10-30
categories: [rl, il]
toc: true
number-sections: False
description: Imitation Bootstrapped Reinforcement Learning
---

- [Paper Link](https://arxiv.org/abs/2502.13406)
- [Code](https://github.com/hengyuan-hu/ibrl)


1. 모방 학습(IL)과 강화 학습(RL)의 장점을 결합한 IBRL(Imitation Bootstrapped Reinforcement Learning)은 샘플 효율성을 높이는 새로운 RL 프레임워크를 제안합니다.
2. 이 방법은 전문가 시연으로 훈련된 별도의 IL 정책을 활용하여 온라인 상호작용에서 더 나은 액션을 제안하고, RL 훈련 시 Q-함수의 목표 값 추정을 부트스트랩하여 탐색과 학습 효율을 크게 가속화합니다.
3. IBRL은 6가지 시뮬레이션 및 3가지 실제 로봇 작업에서 기존 방법들을 크게 능가하며, 특히 어려운 작업에서 탁월한 성능과 샘플 효율성을 입증했습니다.


<center>
<img src="../../images/2025-10-30-ibrl/0.png" width="100%" />
</center>

---

# Brief Review


본 논문은 모방 학습(Imitation Learning, IL)과 강화 학습(Reinforcement Learning, RL)의 장점을 결합하여 샘플 효율성을 높이는 새로운 프레임워크인 IBRL(Imitation Bootstrapped Reinforcement Learning)을 제안합니다. 로봇 제어 태스크에서 IL은 샘플 효율성 때문에 널리 사용되지만, 모든 시나리오에 일반화할 수 있는 포괄적인 전문가 시연(demonstrations) 데이터를 수집하는 것은 비용이 많이 들고, 분포 변화(distribution shift) 발생 시 데이터 재수집이 필요하다는 단점이 있습니다. 반면 RL은 자율적인 자기 개선 절차로서 IL을 기반으로 발전할 수 있다면 매력적입니다.

IBRL의 핵심 방법론은 다음과 같습니다.

1.  **독립적인 모방 정책 ($\mu_\psi$) 훈련**: 제공된 시연 데이터를 사용하여 별도의 독립적인 모방 학습 정책 $\mu_\psi$를 먼저 훈련합니다. 이 IL 정책은 온라인 RL에서 일반적으로 사용되는 것보다 더 깊고 강력한 신경망을 활용할 수 있습니다.
2.  **두 단계에서의 IL 정책 활용**: 훈련된 IL 정책은 RL 훈련을 가속화하기 위해 두 가지 주요 단계에서 명시적으로 사용됩니다.
    *   **온라인 상호작용 (Actor Proposal)**: 온라인 환경과의 상호작용 단계에서, IL 정책과 현재 훈련 중인 RL 정책 ($\pi_\theta$)은 각각 행동($a^{IL}, a^{RL}$)을 제안합니다. 에이전트는 학습 중인 Q-함수(Q-function)의 타겟 Q-함수 $Q_{\phi'}$에 따라 더 높은 Q-값을 가지는 행동을 실행합니다. 즉, 다음 행동 $a^*$는 다음과 같이 결정됩니다.
        $$a^* = \underset{a \in \{a^{IL}, a^{RL}\}}{\text{argmax}} Q_{\phi'}(s, a)$$
    *   **RL 훈련 (Bootstrap Proposal)**: RL의 Q-값 업데이트를 위한 타겟 값을 계산할 때, 단순히 RL 정책의 타겟 네트워크 $\pi_{\theta'}$에서 샘플링된 행동 $a^{RL}_{t+1}$만 사용하는 대신, IL 정책에서 샘플링된 행동 $a^{IL}_{t+1}$과 RL 정책에서 샘플링된 $a^{RL}_{t+1}$ 중 더 높은 Q-값을 가지는 행동을 사용하여 부트스트랩합니다.
        $$Q_\phi(s_t, a_t) \leftarrow r_t + \gamma \underset{a' \in \{a^{IL}_{t+1}, a^{RL}_{t+1}\}}{\text{max}} Q_{\phi'}(s_{t+1}, a')$$
    *   또한, 다른 선행 연구와 유사하게, RL 리플레이 버퍼(replay buffer)를 시연 데이터로 미리 채워서 정책이 첫 번째 온라인 성공을 거두기 전에 학습 신호를 제공합니다.

IBRL는 IL 정책을 RL 정책과 별도로 유지함으로써, 치명적인 망각(catastrophic forgetting)을 방지하기 위한 명시적인 정규화 손실(regularization loss)이나 복잡한 하이퍼파라미터 튜닝 없이 RL과 IL이 각자의 태스크에 가장 적합한 네트워크 아키텍처와 손실 함수를 사용할 수 있도록 합니다. 이를 통해 RL 정책이 초기 단계에서 미흡할 때 탐색(exploration) 품질과 가치 추정(value estimation)을 크게 향상시킬 수 있습니다.

본 논문은 또한 IBRL의 성능을 더욱 향상시키기 위한 아키텍처적 개선 사항들을 제안합니다:

*   **Actor Dropout**: 정책 네트워크(actor) $\pi_\theta$에 Dropout을 적용하여 안정성과 샘플 효율성을 개선합니다.
*   **개선된 Vision Encoder 및 Critic 디자인**: 이미지 입력으로부터 학습할 때, 기존의 얕은 ConvNet 대신 얕은 ViT(Vision Transformer) 기반 아키텍처를 사용하여 더 복잡한 태스크에서 성능 병목 현상을 해결합니다.

IBRL은 6개의 시뮬레이션 태스크(Meta-World 및 Robomimic)와 3개의 실제 로봇 태스크(Lift, Drawer, Hang)에 걸쳐 다양한 난이도 수준에서 평가되었습니다. 모든 태스크는 희소한 0/1 보상(sparse 0/1 reward)을 사용합니다. IBRL은 모든 태스크에서 기존의 강력한 방법들을 능가하거나 동등한 성능을 보였으며, 특히 어려운 태스크에서 그 개선 폭이 두드러졌습니다. 예를 들어, 가장 어려운 시뮬레이션 태스크에서는 두 번째로 좋은 방법보다 거의 두 배의 성능을 보였고, 까다로운 실제 천 걸기(deformable cloth hanging) 태스크에서는 두 번째로 좋은 RL 방법보다 2.4배 더 나은 성능을 달성했습니다.


# Detail Review

> Imitation Bootstrapped Reinforcement Learning (IBRL) 논문 심층 리뷰

## 1. 배경 및 동기

강화학습(RL)은 복잡한 제어 문제에서 뛰어난 성능을 보이지만, 샘플 효율성 및 탐색 어려움 때문에 실제 로봇 제어에는 널리 쓰이지 못해 왔다. 반면 모방학습(IL, 예: 행동 클로닝)은 전문가 시연 데이터를 통해 초기 정책을 효율적으로 학습할 수 있지만, 시연 데이터를 모두 커버하기 힘들고 배포 시 분포 차이 문제로 재수집이 필요하다. 따라서 소수의 시연만으로 시작해 자율적으로 성능을 개선할 수 있는 학습 기법이 요구된다.

기존 연구들은 대개 (1) 시연 데이터를 리플레이 버퍼에 삽입하여 학습 시 과대 샘플링하는 방식(RLPD: Reinforcement Learning from Prior Demonstrations), (2) 시연으로 RL 정책을 사전학습하고 이후 미세조정 시 추가 규제(loss)를 적용하는 방식, 또는 (3) 모델 기반 방법(MoDem)으로 시연을 통해 정책·비평자·모델을 모두 사전학습한 후, 모델 예측 제어로 강화학습하는 방식 등이 있다.

그러나 (1)의 방식은 IL이 일반화한 유익한 행동을 충분히 활용하지 못하고, (2)의 방식은 RL 과정에서 초기 지식을 잃지 않기 위한 하이퍼파라미터 튜닝이나 동일한 네트워크 구조 사용 제약이 필요하며, (3)의 모델 기반 접근은 계산 비용이 크다. 본 논문은 이러한 한계를 극복하기 위해 **모방 학습 정책(IL 정책)**을 강화학습에 직접 통합하여 샘플 효율을 높이는 새로운 프레임워크 **IBRL**을 제안한다.

## 2. IBRL 알고리즘 개요

IBRL의 핵심 아이디어는 (그림 1) 우선 전문가 시연으로 모방학습 정책($\mu_\psi$)을 학습하고, 이 정책을 RL 학습의 **두 단계**에 활용하는 것이다.

첫째, **온라인 상호작용 단계(Actor Proposal)**에서는 매 시점마다 IL 정책과 현재 학습 중인 RL 정책($\pi_\theta$)이 각각 행동 $$a_{IL} \sim \mu_{\psi}(s),$$ $$a_{RL} \sim \pi_{\theta}(s)$$를 제안한다. 이 두 후보 행동을 타깃 Q-네트워크 $Q_{\phi'}$로 평가하여 더 높은 Q값을 갖는 행동 $$a^{*} = \arg\max_{a \in \{ a_{IL},a_{RL}\}}Q_{\phi'}(s,a)$$를 실제 행동으로 선택한다(식 (1)). 이 방식으로 IL 정책이 초기 탐색에서 신뢰할 수 있는 행동을 지속적으로 제공함으로써, 희소 보상 환경에서 빠른 성공 경험을 얻을 수 있다.

둘째, **RL 학습 단계(Bootstrap Proposal)**에서는 Q-함수 업데이트 시 다음 상태에서의 최대 Q값을 계산할 때 IL 정책과 RL 정책이 제안하는 행동 중 더 높은 Q값을 갖는 쪽을 사용한다. 즉, 일반적인 TD 타깃 $$r + \gamma Q'\left( s',\pi'(s') \right)$$ 대신에 $$r + \gamma\max\{ Q'\left( s',a_{IL} \right),Q'\left( s',a_{RL} \right)\}$$ 형태로 값 함수를 부트스트래핑한다.

이를 통해 IL 정책이 제안하는 고품질 행동이 Q-값 학습에 직접 반영되어 학습 속도가 개선된다. 모듈화된 구조 덕분에 IL 정책과 RL 정책은 각자 최적화된 네트워크(예: ResNet-18 vs. ViT)로 독립적 학습이 가능하며, RL에 의한 초기 IL 지식 소실(catastrophic forgetting)을 우려하지 않아도 된다. 또한 초기 학습 신호를 제공하기 위해 리플레이 버퍼를 전문가 시연으로 미리 채우는 것은 종전 방법과 유사하게 적용된다.

## 3. 이론적 기초

IBRL은 표준 MDP $$\left( \mathcal{S},\mathcal{A},T,R,\gamma \right)$$를 가정하며, 오프-폴리시 RL(TD3/SAC)을 기반으로 한다. 비평자($Q_\varphi$) 네트워크는 강화학습 손실 $$L(\varphi) = \left( r_{t} + \gamma Q_{\phi'}\left( s_{t + 1},\pi_{\theta'}\left( s_{t + 1} \right) \right) - Q_{\varphi}\left( s_{t},a_{t} \right) \right)^{2}$$를 최소화하며, 정책(Actor) $\pi_\theta$는 $$L(\theta) = - Q_{\varphi}\left( s,\pi_{\theta}(s) \right)$$ 손실로 학습된다.

모방학습(IL) 정책 $\mu_\psi$는 전문가 데이터 $\mathcal{D}$에서 최대우도 또는 평균제곱오차로 행동을 복제하여 학습된다. IBRL은 먼저 이 데이터셋으로 $\mu_\psi$를 학습하고, 그 후 강화학습 과정에 $\mu_\psi$를 `참조 정책(reference policy)`으로 활용한다.

기존 연구 중 비슷한 아이디어로는, 사람이 만든 참조 정책을 온/오프-라인에서 사용하는 PEX, EfficientImitate 등이 있으나, 이들은 주로 탐색 보조에만 IL 정책을 사용하거나 저차원 관측에 제한되며 실험도 시뮬레이션에 한정되었다. IBRL은 IL 정책을 탐색과 학습 두 단계에 모두 적극적으로 활용하며 실제 로봇 태스크까지 평가한 점에서 차별화된다.

## 4. 실험 설정

저자들은 6가지 시뮬레이션 태스크와 3가지 실제 로봇 태스크에서 IBRL을 평가했다. 시뮬레이션에서는 희소 보상의 연속제어 문제를 다루며, 주로 **픽&플레이스** 계열 과제를 사용하는 Robomimic 벤치마크(Stanford)와 Meta-World 벤치마크 환경을 선택했다.

- **Robomimic 태스크 (Lift, PickPlaceCan, NutAssemblySquare)**: 블록을 들어올리기, 캔을 픽&플레이스, 너트 조립이라는 3단계로 난이도가 증가한다. Lift는 간단하여 1개, Can은 보통하여 10개, Square는 어려워 50개의 전문가 시연을 사용했다. 관측은 이미지(픽셀) 및 로우-스테이트(로봇 관절 상태)를 병행하여 실험했다.

- **Meta-World 태스크 (Assembly, BoxClose, CoffeePush, StickPull)**: 주어진 실험군에서 무작위로 4개를 선정했다. 각각 어셈블리, 박스 닫기, 커피 푸시, 막대 잡아당기기로, 태스크당 3개의 스크립트 기반 전문가 시연을 사용했다. 상태공간은 이미지(픽셀)이다.

비교 대상은 아래와 같다. Robomimic에서는 **RLPD+**(TD3 기반 구현, 데모 과대샘플링)와, BC 사전학습 후 규제 강화학습(BC+RLreg), SQIL(Synthetic Q-infilling) 등을 사용했다. Meta-World에서는 **MoDem**(모델 기반 RL with demonstrations) 및 RLPD+를 함께 비교했다. 모든 방법은 동일한 네트워크 구조와 하이퍼파라미터를 공유하여 공정 비교하였다.

실제 로봇 실험에서는 Franka 로봇팔을 사용하여 **Lift, Drawer, Hang** 과제를 수행했다. 각 과제별로 10~30회의 시연을 수집하였고, 매 방법에 동일한 상호작용 예산(스텝 수)과 정책 업데이트 횟수를 부여했다. 성공 여부는 룰 기반의 sparse 보상(성공 시 1, 아니면 0)으로 측정하였다.

## 5. 시뮬레이션 실험 결과

### 5.1 Robomimic 태스크

그림 1은 Robomimic의 Lift, PickPlaceCan(Can), NutAssemblySquare(Square) 과제에서 픽셀/스테이트 관측으로 IBRL과 RLPD+(기준선)를 비교한 학습 곡선이다. 그래프에서 빨간색 선이 IBRL, 파란색 선이 RLPD+를 나타내며, 점선은 IBRL 기본 변형(안티-사양)을 의미한다. Lift와 Can 환경에서는 IBRL이 훨씬 빠르게 성공률을 끌어올림을 볼 수 있다. 특히 단순한 Lift에서는 10K 단계 미만에서 100% 성공률에 도달하며, RLPD+보다 약 3배 빠른 수렴을 보인다.

그림 1의 오른쪽 상단 그래프(Can)는 IBRL이 20K 단계 내에 과제를 해결한 반면, RLPD+는 더 많은 단계가 필요함을 보여준다. Square는 가장 어려운 환경으로, 초기 학습이 매우 느리지만 IBRL이 계속 우세하다. 전반적으로 IBRL은 모든 Robomimic 과제에서 RLPD+를 크게 앞서며, 같은 시연 수로도 RLPD+보다 월등한 샘플 효율을 보인다.

<center>
<img src="../../images/2025-10-30-ibrl/x3.png" width="80%" />
</center>

> *그림 1:* Robomimic Lift, PickPlaceCan, NutAssemblySquare 과제에서 IBRL(빨간)과 RLPD+(파란)의 학습 성능 비교. 각 그래프는 성능(성공률)을 상호작용 스텝 수에 대해 보여준다. 모든 환경에서 IBRL이 더 빠르게 수렴하며 높은 성공률을 달성함을 확인할 수 있다.

또한 표 1에 나타난 바와 같이, IBRL로 학습된 정책은 인간 시연보다도 평균 에피소드 길이가 짧아지는 경향을 보였다. 예를 들어 IBRL은 Lift, Can, Square에서 인간 전문가(48.3, 116.0, 150.8 스텝)보다 각각 3~2.2배 빠르게 과제를 완료했으며, 평균적으로는 약 2.3 스텝을 단축했다. 이는 IBRL이 RL을 통해 시연에서 본 동작을 넘어 효율적인 행동을 학습했음을 시사한다.

한편, 픽셀 기반 학습에서는 랜덤 쉬프트 데이터 증강과 손목 카메라 활용 덕분에 Lift와 Can에서 오히려 스테이트 기반보다 빠른 수렴이 관찰되었다. Square는 시야가 제한되는 복잡도로 인해 픽셀 학습이 어려웠지만, 그나마 IBRL은 다른 방법들보다 더 빠르게 정책을 개선했다.

예를 들어 Robomimic의 PickPlaceCan에서는 단 10회의 시연과 10만 단계의 상호작용만으로도, IBRL은 RLPD 대비 성공률이 약 6.4배 높게 나타났다. 이처럼 IBRL은 적은 시연 데이터로도 강화학습을 효과적으로 진행하여, 특히 난이도가 높은 과제에서 큰 성능 향상을 보인다.

### 5.2 Meta-World 태스크

그림 2는 Meta-World의 4개 과제(Assembly, Box Close, Coffee Push, Stick Pull)에서 IBRL(빨간), IBRL Basic(빨간 점선), MoDem(초록), RLPD+(파랑)의 성능을 비교한 것이다. 모든 그래프에서 가로축은 상호작용 스텝, 세로축은 성공률을 나타낸다. 결과를 보면 IBRL(빨간 실선)이 네 가지 과제 모두에서 다른 방법들을 앞선다. 특히 어려운 과제인 Assembly나 Stick Pull에서는 IBRL이 거의 100% 성공률에 근접하는 반면, MoDem과 RLPD+는 상당히 낮은 성공률에 머물렀다.

IBRL Basic(점선)은 인코더를 간단화한 변형으로, 단순 환경에서는 오히려 IBRL보다 우수하나, 복잡한 작업에서는 깊은 구조의 IBRL이 더 안정적임을 보여준다. 전반적으로 IBRL과 그 변형은 모든 Meta-World 과제를 성공적으로 해결했으나, MoDem은 4개 중 3개 환경에서 신뢰할 수 있는 해결률을 달성하지 못했다. 아울러 MoDem은 모델 학습 및 계획 단계 때문에 시간 비용이 150시간 이상 소요되지만, IBRL은 단순히 정책 학습만으로 더 빠르게 수렴하였다.

<center>
<img src="../../images/2025-10-30-ibrl/1.png" width="100%" />
</center>

> *그림 2:* Meta-World 과제(Assembly, Box Close, Coffee Push, Stick Pull)에서의 IBRL, MoDem, RLPD+ 성능 비교. 빨간색 실선이 IBRL, 점선이 IBRL Basic, 초록이 MoDem, 파랑이 RLPD+이다. 모든 과제에서 IBRL(빨간)이 빠르고 높은 성능을 보이며, 특히 난이도 높은 환경에서 격차가 두드러진다.

### 5.3 주요 결과 요약

요약하면, IBRL은 6개 시뮬레이션 과제에서 두드러진 샘플 효율 개선을 보여준다. RLPD+와 같은 단순한 데모 과대샘플링 기법에 비해, IL 정책을 적극적으로 활용함으로써 초기 단계부터 고품질의 행동 후보를 얻고 더 빠른 탐색을 가능케 했다. 또한 IBRL Basic을 포함하여 다양한 구조에서도 안정적으로 우수한 성능을 내며, 모델 기반 MoDem보다 계산 효율성 측면에서도 유리하다. 이러한 결과는 *IBRL이 기존 기법 대비 더 높은 샘플 효율성과 최종 성능을 달성*했음을 뒷받침한다.

## 6. 실제 로봇 실험 및 적용성

IBRL의 실제 적용 가능성을 검증하기 위해 저자들은 3가지 실제 로봇 조작 과제를 설정했다. 과제는 Franka Panda 로봇팔로 수행되며, Lift(블록 들어올리기), Drawer(서랍 열기), Hang(천 걸기)으로 난이도가 증가한다. 초기 조건이나 로봇 시작 위치에 변이가 있으며, 각 과제별로 10~30회의 시연 데이터를 수집했다. Lift에서는 손목 카메라 시점을, Hang에서는 3인칭 카메라를 사용했다. 모든 방법은 동일한 네트워크 구조와 파라미터를 사용하고, 상호작용 예산도 과제 난이도에 맞추어 부여했다.


<div style="display: flex; justify-content: center; align-items: center; gap: 5px;">
  <img src="../../images/2025-10-30-ibrl/01.png" width="48%">
  <img src="../../images/2025-10-30-ibrl/02.png" width="48%">
</div>


그림 8 및 표 I에 따르면, IBRL은 세 과제 모두에서 RLPD(RFT) 및 행동 복제(BC) 대비 우수한 성능을 보였다. **Lift** 과제에서는 8K 상호작용 단계 만에 IBRL이 100% 성공률을 달성했고, RLPD와 RFT는 각각 95%, 90%로 뒤를 이었다. 더 어려운 **Lift Hard Eval** 상황(블록이 손목 카메라 시야 가장자리에 놓임)에서도 IBRL은 95%의 성공률을 유지한 반면, BC는 0%로 성능이 급락했다. 이는 IBRL이 학습 중 다양한 초기 상태를 경험하며 분포 차이를 극복했음을 의미한다.


**Drawer** 과제(서랍 열기)에서는 16K 단계 상호작용에서 IBRL이 95% 성공률로 가장 높았다. 실험을 조기 중단한 10K 단계 지점에서도 IBRL은 이미 100%에 도달했으나, RLPD와 RFT는 15% 이하로 극히 낮은 성공률이었다. IBRL은 데모가 충분하더라도 실제 환경의 세밀한 조작이 필요함을 빠르게 학습했음을 보여준다.

가장 어려운 **Hang** 과제(변형 천 걸기)에서는 IBRL만이 강건함을 증명했다. 30K 단계 상호작용 후 IBRL은 85% 성공률을 달성하여 BC(65%)보다 20%p 높았으나, RLPD와 RFT는 각각 15%, 25%에 머물렀다. 이는 천의 변형성을 예측하기 힘들어 랜덤 탐색이 거의 불가능한 상황에서, IBRL이 IL 정책으로부터 얻은 우수한 초기 행동을 꾸준히 활용해 정책을 빠르게 개선한 결과다. 그림 9의 롤아웃 예시에서도 IBRL은 더 적은 스텝 내에 성공했으며, BC가 실패하는 초기 조건에서도 성공함을 확인할 수 있다.

요약하면, 실제 환경에서도 IBRL은 단시간 내에 높은 성공률을 획득하여, 다른 RL 기법보다 상당히 높은 샘플 효율성을 입증했다. 특히 BC 기반 정책의 한계를 넘어서는 성능으로, 분포 변화나 노이즈에 의한 성능 저하 상황에서도 빠르게 회복할 수 있음을 보였다. 이는 IBRL이 실제 로봇 응용에서 *기존 IL 정책을 크게 뛰어넘는 성능 개선*을 신속히 가능케 한다는 것을 의미한다.

## 7. 기여, 한계 및 향후 연구 방향

IBRL은 로봇 강화학습 연구에 다음과 같은 기여를 한다. 첫째, *모방학습 정책을 명시적으로 강화학습에 통합*함으로써 시연 데이터의 이점을 극대화하고 RL 탐색 문제를 완화했다. 둘째, IL과 RL 정책의 분리로 각자 최적의 네트워크와 학습법을 사용할 수 있어, 보다 유연하고 효율적인 설계가 가능하다. 셋째, 폭넓은 시뮬레이션 및 실제 실험에서 SoTA 성능을 달성하여, 로봇 샘플 효율적 학습의 새로운 기준을 제시했다.

그러나 한계점도 존재한다. 본 연구의 실제 실험에서는 자동 초기화(autonomous reset)를 적용하지 않고 수동 리셋을 사용하여 안정적 평가를 보장했다. 실제 대규모 배포를 위해서는 자동 리셋 메커니즘이 필요하며, 이는 향후 과제로 남는다. 또한 본 연구에서는 BC를 사용한 단일 형태의 IL 정책을 실험했지만, IBRL 프레임워크는 이론상 어떠한 IL 기법과도 결합 가능하다. 향후에는 **확산 정책(diffusion policies)** 등 최신 IL 방법을 도입하거나, PEX/PILCO 등과의 비교 연구를 통해 성능을 더욱 개선할 수 있다.

## 8. 결론

IBRL은 제한된 전문가 시연 데이터를 바탕으로 *모방학습 정책을 참조 정책*으로 활용하여 샘플 효율적인 강화학습을 실현한 새로운 방법론이다. 실험 결과, IBRL은 기존 방법보다 적은 상호작용으로 높은 성공률을 달성했으며 특히 어려운 과제에서 효과가 두드러졌다. 실제 로봇 실험에서도 타 방법을 크게 앞서며, 로봇 제어 정책의 빠른 향상을 가능케 한다. 따라서 IBRL은 로봇 강화학습 연구에 있어 **시연 학습과 강화학습의 결합**을 새로운 방향으로 제시하며, 실제 로봇 적용 측면에서도 효율성과 성능을 크게 개선할 수 있는 방법으로 평가된다.

<!--
- [[논문리뷰] Imitation Bootstrapped Reinforcement Learning](https://ropiens.tistory.com/265)
-->
