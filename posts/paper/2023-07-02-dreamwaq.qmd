---
title: "📃DreamWaQ 리뷰"
description: Learning Robust Quadrupedal Locomotion With Implicit Terrain Imagination via Deep Reinforcement Learning
date: "2023-07-02"
categories: [context, rl, paper]
toc: true
number-sections: true
image: ../../images/2023-07-02-dreamwaq/dw1.png
---



이번 포스팅은 DeepMind에서 발표된 [DreamWaQ: Learning Robust Quadrupedal Locomotion With Implicit Terrain Imagination via Deep Reinforcement Learning](https://arxiv.org/abs/2301.10602) 논문을 읽고 정리한 내용입니다. 최근 ICRA 2023 런던에서 5월 30일부터 6월 1일까지 진행된 [Autonomous Quadruped Robot Challenge (QRC)](https://quadruped-robot-challenges.notion.site/)에서 KAIST 연구팀이 1등을 하여 큰 이슈가 되었었습니다. 이번 포스팅에서 리뷰하는 이 논문이 바로 대회에서 사용되었던 강화학습 기반의 보행제어 알고리즘에 대한 내용을 담고 있는 논문입니다. 

<center>
<img src="../../images/2023-07-02-dreamwaq/2023qrc.jpeg" width="67%" />
    <figcaption>2023 Autonomous Quadruped Robot Challenge</figcaption>
</center>


# Introduction

논문을 소개해드리면서 말씀드린바와 같이 오늘 리뷰할 DreamWaQ라는 논문에 나와있는 강화학습 알고리즘을 활용한 보행제어 알고리즘으로 KAIST 연구팀이 MIT 연구팀을 제치고 1등을 하여 다시한번 우리나라 기술력을 세계에 알린 기회가 되었다는 좋은 [뉴스](https://youtu.be/mhD5N6-SuGc)를 들을 수 있었습니다. 

**Finals Team DREAM STEP KAIST**

{{< video https://youtu.be/3ZFu7kyQyoY >}}

세계 로봇 연구팀들이 참여한 대회에서 KAIST의 DREAM STEP 팀의 결승 대회 영상입니다. 사족 보행로봇의 다양한 험지에서의 자율적인 보행을 테스트하기 위해 대회에서 사용된 지형은 정말 다양하고 로봇이 안정적으로 완주하기에 굉장히 어려운 코스임을 알 수 있습니다. 실제로 대회에 참가한 [다른 팀들의 영상들](https://www.youtube.com/@rise-lab-skku/videos)을 보시면 대회 코스의 한 부분 부분마다 각자 고군분투하며 걸어가는 로봇들의 모습을 다양하게 보실 수 있습니다. 뉴스에서 나왔던 것처럼 유명한 대학 연구팀들을 제치고 1등을 한 자랑스러운 KAIST 연구팀의 1등 비결을 담은 강화학습 알고리즘을 기반으로 한 DreamWaQ 알고리즘을 한번 살펴보겠습니다.

# DreamWaQ

## Key Contribution



<center>
<img src="../../images/2023-07-02-dreamwaq/dw1.png" width="67%" />
    <figcaption>Overview of DreamWaQ</figcaption>
</center>

DreamWaQ 알고리즘의 전체적인 흐름은 위의 사진과 같습니다. "Dream"이라는 워딩과 알고리즘 개괄도에서 생각 풍선 모양 표현에서 볼 수 있듯이 DreamWaQ 논문의 주요 Contribution으로는 **Implicit Terrain Imagination**을 할 수 있도록 **Context-Aided Estimator Network(CENet)**을 도입하였고 안정적으로 Policy가 학습될 수 있도록 **Adaptive Bootstrapping(AdaBoot)**방법을 제안하여 강화학습 보행 제어기를 설계한 점을 들 수 있습니다.



## Implicit Terrain Imagination

앞서 챌린지에서 사용된 환경에서 볼 수 있듯이 사족보행로봇은 다양한 지형(Terrain)을 극복하며 보행할 수 있는 능력이 중요합니다. 그럼 다양한 지형을 나타낼 수 있는 속성들에는 무엇이 있을까요? **지형의 마찰계수, 반발계수, 놓여져 있는 장애물, 울퉁불퉁한 정도 등등** 여러가지 속성들로 지형의 특징을 나타낼 수 있을 것입니다. 그리고 그러한 특징을 어떻게 4개의 다리를 이용하여 보행의 어려운 점들을 극복하며 원하는 방향으로 이동할 수 있도록 하는 것이 관건인 것입니다. 

이런 지형의 특징을 파악하기 위해 많은 연구들이 카메라나 라이다와 같은 비젼센서를 부착하여 환경을 인식한 뒤 극복하기 위한 방법을 고안하는 방향으로 연구되고 있습니다. 하지만 KAIST 연구진이 제안한 DreamWaQ에서는 지형을 인식할 수 있는 부차적인 비젼센서 없이 **로봇의 자체의 정보(proprioception)를 이용**하여 지형을 극복하기 위해 explicit한 환경 정보가 아닌, **implicit한 terrain imagination**을 할 수 있는 방법론은 제시했습니다.

사실 Implicit하게 로봇 주변의 지형이나 환경정보를 강화학습 로봇 에이전트가 인식할 수 있도록 하는 연구는 다양하게 진행되어왔었습니다. 앞선 주요 방법은로는 **Teacher-Student Network**를 이용하여 모든 환경정보를 학습한 `Teacher Network`로부터 `Student Network`가 추후에 따라 학습하는 방식이 있었지만, 해당 방법은 Teacher Network를 학습과 Student Network 학습을 따로 2개의 단계를 거쳐 학습을 해야한다는 데이터 비효율적인 학습 방법이라는 단점이 있었습니다. 따라서 DreamWaQ에서는 **Asymmetric Actor-Critic**이라는 기존의 Actor-Critic 강화학습 알고리즘에 약간 변형을 준 모델을 사용하여 Teacher-Student Network처럼 두 단계로 나누어서 학습하지 않고도 Implicit하게 Terrain 정보를 Actor-Critic 구조에 녹여들 수 있도록 했습니다.

## Asymmetric Actor-Critic

기존의 PPO, SAC와 같은 Policy Gradient의 강화학습 알고리즘들의 주요 구성요소로 **Actor Network**와 **Critic(Value) Network**가 있습니다. **Actor**는 강화학습 에이전트가 취해야하는 `action` 값을 출력하는 네트워크이며 **Critic**는 에이전트의 학습 방향을 보여주는 `value`값을 출력하여 이 2개의 네트워크들이 Policy Gradient 알고리즘의 목적식을 따라 `Return(누적 보상)`값을 최대화하는 방향으로 학습하게 되는 것입니다. 보통 2개의 네트워크 모두에게 같은 state(혹은 observation) 정보가 입력값으로 들어가게 되기 때문에 Actor 네트워크와 Critic 네트워크는 서로 **Symmetric**하다고 할 수 있습니다.

하지만 앞서 로봇이 센서 없이는 얻을 수 없는 지형 정보가 강화학습 알고리즘에 사용되는 네트워크의 인풋으로 들어간다면 실제 로봇에서 알고리즘이 돌아갈 때 넣어줄 지형정보가 없기 때문에 제어 알고리즘이 돌아갈 수 없을 것 입니다. 그래서 DreamWaQ에서는 Actor/Critic Network의 상호작용 과정에서 강화학습 에이전트가 얻을 수 있는 시간적 정보들을 기반으로 terrain 정보를 `상상`할 수 있도록, Actor 네트워크에 들어가는 입력값과 Critic 네트워크에 들어가는 입력값을 다르게 설계하였고 이를 **Asymmetric**한 구조라고 볼 수 있습니다.

<center>
<img src="../../images/2023-07-02-dreamwaq/dw2-1688297136432-3.png" width="67%" />
    <figcaption>Asymmetric Actor-Critic</figcaption>
</center>



위에 보이시는 것처럼 **Actor Network에는** Observation $o_t$, estimated velocity $v_t$, latent vector $z_t$가 입력으로 들어가게 됩니다. $v_t$와 $z_t$는 다음 파트에서 좀 더 살펴볼 예정이므로 여기에서는 우선 observation vecter인 $o_t$에 초점을 맞추어서 보겠습니다. observation 정보는 강화학습 MDP를 정의하는 한 요소로 강화학습 에이전트가 학습할 때 `관측(혹은 접근 가능한 정보)`하는 정보입니다. 따라서 로봇에 특별한 비젼 센서 추가 없이 로봇 자체 하드웨어에서 얻을 수 있는 정보인 proprioceptive 정보를 기반으로 몸체의 각속도 $\omega_t$, 중력방향 벡터 $g_t$ 등등의 정보가 observation vector의 요소로 들어가게 됩니다. 반면, **Critic Network**에는 State $s_t$가 입력값으로 들어가는 것을 알 수 있는데 이는 위에서 Observation과 State를 비교해놓은 것과 같이 state가 observation보다 많은 정보를 포함한 것을 알 수 있습니다. 여기에서 주목해서 볼 수 있는 점이 바로 지형에 대한 정보인 **heightmap scan $h_t$**가 한 요소임을 알 수 있고 이를 통해 implicit한 terrain imagination이 가능한 것 입니다. Heightmap scan에 대해 조금 더 설명을 덧붙이자면, 지형의 heightmap scan 정보는 실제 로봇에서 얻을 수 있는 정보는 아니고 강화학습 에이전트가 학습하게 되는 시뮬레이션에서만 얻을 수 있는 정보로 **지형의 z축 방향의 높이 정보**를 말합니다. 

환경을 정의하는 변수이고 시뮬레이션에서는 가상공간이기 때문에 프로그램에서 얻을 수 있는 물리적 정보이지만 실제로 로봇이 이용할 수 없는 정보를 **privileged observation**이라고 부르기도 합니다. 따라서 기존에 강화학습에서 State가 환경에서 에이전트가 놓여있는 상황을 설명할 수 있는 모든 정보를 말하고 Observation이 환경에 놓여있는 에이전트가 `관찰`할 수 있는 `일부` 상태 정보를 뜻하기 때문에 **State = Observation + Privileged Observation** 포함관계로 이해할 수 있습니다.(논문에서는 privileged observation이라는 표기를 state를 뜻하는 것으로 표기하고 있기 때문에 헷갈릴 수 있습니다.)



## Context-Aided Estimator Network

이번 파트에서 살펴보게 될 **Context-Aided Estimator Network**는 센서로 인식할 수 없는 지형 정보를 에이전트가 유추할 수 있도록 하는 일등공신 아이디어 입니다.

<center>
<img src="../../images/2023-07-02-dreamwaq/dw3.png" width="67%" />
    <figcaption>The architecture of CENet</figcaption>
</center>

CENet의 구조는 위와 같이 $\beta$-VAE구조를 활용하여 구성되어 있습니다. 일정 time horizon $H$동안 모은 observation이 Encoder에 들어가면 latent vector $z$와 몸체의 선속도 추정값인 $v_t$가 출력값으로 나오게 됩니다. Auto-Encoder의 일반적인 구조를 따라 이 값들이 Decoder의 인풋으로 들어가고 Decoder의 출력값으로는 time horizon을 지난 다음 observation vector $o_{t+1}$을 reconstruction할 수 있도록 학습하게 되는 것 입니다.

<center>
<img src="../../images/2023-07-02-dreamwaq/dw.png" width="67%" />
    <figcaption>The loss of CENet</figcaption>
</center>

그래서 CENet의 loss function은 크게 2개의 파트 $L_{est}$와 $L_{VAE}$로 나누어져 있는 것을 확인할 수 있습니다. 먼저 $L_{est}$는 보행하는 로봇 에이전트의 속도 추정을 CENet에서 할 수 있도록 학습하기 위한 부분으로, 로봇 몸체의 선속도 추정값 $\tilde{v}_t$는 실제 정답값 $v_t$는 시뮬레이션에서는 얻을 수 있는 값이기 때문에 Encoder에서 추정한 값 $\tilde{v}_t$와의 MSE(mean square error)를 구할 수 있습니다. 다음으로 $L_{VAE}$는 time horizon $H$동안 누적되 여러개의 observation 정보를 가지고 다음 observation $o_{t+1}$을 오토인코더 구조로 잘 reconstruction한지를 보는 첫번째 term과 추정 분포를 맞추는 부분인 KL-divergence 제약 조건 두번째 term으로 이루어져 있습니다. (VAE loss에 대해서 더 자세한 정보를 알고 싶으신 분은 이전에 VAE 논문을 리뷰한 [포스팅](https://curieuxjy.github.io/posts/paper/2022-10-02-vae.html)을 참고해주세요.)

이와 같은 loss 구성으로 학습된 CENet은 여러 타임 스텝동안 관찰된 observation 정보들을 기반으로 에이전트가 privileged observation을 유추할 것으로 기대할 수 있는 이유는 privileged observation을 기반으로 가치를 추정하는 Critic(Value) Network를 통해서 Actor Network가 업데이트 되는 Policy gradient과정을 거치기 때문입니다. 이러한 Asymmetric Actor-Critic구조와의 시너지 효과가 기존의 **Context RL** 분야에서도 사용되는 아이디어 인데(참조논문: [AACC](https://arxiv.org/abs/2208.02376)) 이와 비교해보았을 때, Critic Network가 deploy되는 과정에서 쓰이지 않기 때문에 Actor보다 더 많은 정보를 받아서 더 정확한 가치를 추정할 수 있게 한다는 기조는 비슷하지만 time-invarient한 context vector를 만드는 Context RL에서의 Asymmetric Actor-Critic과 다르게 DreamWaQ에서는 time-varient한 변수들을 추정하여 implicit하게 추정할 수 있도록 했다는 점이 다릅니다. 

**Adaptive Bootstrapping(AdaBoot)**

Adaptive bootstrapping은 policy 학습과정 중에 Estimator network인 CENet이 안정적으로 학습되도록 하기 위해 domain randomized로 다양화된 여러 환경요소에 대해 에피소드별 reward의 평균값에 대한 표준 편차의 비율인 변동 계수(CV)에 의해 제어되는 방법을 말합니다. 핵심 아이디어는 부정확한 가치 추정에 대한 정책을 보다 견고하게 만들기 위해 m개의 에이전트 reward의 CV가 작을 때 부트스트래핑을 하게됩니다. 반대로 에이전트가 충분히 학습하지 않은 경우에는 reward에서 큰 CV로 표시된 것처럼 부트스트랩을 해서는 안하도록 합니다.



<center>
<img src="../../images/2023-07-02-dreamwaq/dw5.png" width="67%" />
    <figcaption>Adaptive Bootstrapping Probability</figcaption>
</center>



# Experiments

DreamWaQ의 효과를 실험을 통해 살펴보기 위해 다음과 같은 비교 모델군을 설정했습니다.

**Compared Methods**

- **Baseline**: Adaptation을 하는 부분이 없는 기본 모델 구조
- **AdaptationNet**: Teacher-Student 구조를 활용하여 두 단계 학습을 거텨 implicit한 환경정보를 학습하는 모델
- **EstimatorNet**: Context 추정이 없이 explicit하게 환경정보를 추정하는 Estimator network가 있는 모델
- **DreamWaQ w/o AdaBoot**: AdaBoot를 하지 않은 DreamWaQ
- **DreamWaQ w/ AdaBoot**: `[proposed method]` AdaBoot를 한 DreamWaQ 



## Simultation Result

<center>
<img src="../../images/2023-07-02-dreamwaq/dw4.png" width="67%" />
    <figcaption>The loss of CENet</figcaption>
</center>

Isaac Gym 시뮬레이터를 이용하여 PPO 강화학습 알고리즘 이용하여 학습과정 동안의 Episodic Reward 그래프 변화를 살펴보면, EstimatorNet은 처음에는 AdaptationNet보다 평균 에피소드 보상이 높지만, 더 많은 training step 후에 더 어려운 지형과 마주치기 때문에 더 많은 반복 후에 성능이 저하됨을 알 수 있습니다. 반대로 DreamWaQ는 학습 지형이 점점 어려워 짐에도 다른 모든 방법들을 능가하는 퍼포먼스를 보여줍니다. 외부 인식 없이 걷는 것임에도 DreamWaQ는 주변 지형의 heightmap을 다 알 수 있는 오라클 policy만큼 성능이 좋은 것을 볼 수 있습니다.

**Explicit Estimation Comparison**

시뮬레이션에서 한번 지형정보를 Implicit가 아닌 **Explicit하게 알려주고 학습한다면 어떤 유의미한 차이가 있는지** 알아보는 실험도 진행했습니다. 

<center>
<img src="../../images/2023-07-02-dreamwaq/dw8.png" width="67%" />
    <figcaption>Adaptive Bootstrapping Probability</figcaption>
</center>

Timestep이 늘어날 수록 더 어려운 계단지형에서 보행하도록 학습시킨 결과 Explicit하게 지형정보를 학습한 Estimator는 지형이 어려워지자 Foot stumble 현상이 심하게 있었지만 DreamWaQ는 지형이 어려워져도 작은 foot stumble이 있음을 확인하여 오히려 Implicit하게 지형정보를 학습하는 것인 robust한 보행을 하는 것을 확인할 수 있었다고 합니다.

## Real-world Result

<center>
<img src="../../images/2023-07-02-dreamwaq/dw6.png" width="67%" />
</center>

실제 로봇 플랫폼을 가지고 Command tracking error를 plot 해보았을 때도 다른 비교 모델들과 비교해보았을 때 error 값이 적은 것을 확인할 수 있습니다. 특히나 AdaBoot방법이 있고 없고에 따라 error값의 크기가 다른 것을 통해 AdaBoot 방법이 policy 학습에 필요한 것을 확인할 수 있습니다.

# Conclusion

리뷰를 하면서 논문에서 애매하게 그려진 부분도 있어서 아쉽다는 생각이 들었지만 보행로봇이 비정형적이고 다양한 지형을 극복할 수 있도록 하는 것이 여전히 풀기 어려운 문제로 남아있는데 이를 추가적인 비젼센서 정보 없이 CENet과 AdaBoot라는 아이디어로 풀어내고 실제 학회에서 열린 대회에서도 좋은 퍼포먼스를 냈다는 점에서 좋은 연구라고 생각합니다.

{{< video https://youtu.be/J5wl0be5KQM >}}


# Reference

- [Original Paper: DreamWaQ](https://arxiv.org/abs/2301.10602)

- [ICRA 2023 Quadruped Robot Challenges](https://quadruped-robot-challenges.notion.site/)
- [카이스트 사족보행 로봇 ··· MIT 제치고 세계대회 우승 비결은?](https://youtu.be/mhD5N6-SuGc)
- [AACC: Asymmetric Actor-Critic in Contextual Reinforcement Learning](https://arxiv.org/abs/2208.02376)