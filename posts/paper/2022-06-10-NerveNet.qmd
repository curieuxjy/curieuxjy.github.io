---
title: "ğŸ“ƒNerveNet "
description: Learning Structured Policy with Graph Neural Networks
date: "2022-06-10"
categories: [gnn, rl, paper]
toc: true
---

# Abstract

> We address the problem of learning `structured policies` for `continuous` control. In traditional reinforcement learning, policies of agents are learned by multi-layer perceptrons (MLPs) which take the concatenation of all observations from the environment as input for predicting actions. In this work, we propose `NerveNet` to `explicitly model the structure of an agent`, which naturally takes the form of `a graph`. Specifically, serving as the agentâ€™s `policy network`, NerveNet `first propagates information over the structure of the agent` and then `predict actions for different parts of the agent`. In the experiments, we first show that our NerveNet is comparable to state-of-the-art methods on standard MuJoCo environments. We further propose our customized reinforcement learning environments for benchmarking two types of structure `transfer learning tasks`, i.e., size and disability transfer, as well as `multi-task learning`. We demonstrate that policies learned by NerveNet are significantly more transferable and generalizable than policies learned by other models and are able to transfer even in a `zero-shot setting`.

ë³´í†µ ê°•í™”í•™ìŠµì—ì„œ agentë“¤ì˜ policyëŠ” multi-layer perceptrons (MLPs)ìœ¼ë¡œ ë„¤íŠ¸ì›Œí¬ë¥¼ ë§Œë“¤ê¸° ë•Œë¬¸ì— agentê°€ environmentì—ì„œ ë°›ì€ observationë“¤ì„ ë‹¨ìˆœíˆ ìŒ“ì•„ì„œ(concatenation) policy networkì— ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°€ê²Œ ëœë‹¤. í•˜ì§€ë§Œ `ì†ì˜ ì†ë„ ì •ë³´`ì™€ `ë°œì˜ ì†ë„ ì •ë³´`ê°€ ê°™ì€ `ì†ë„` ë²”ì£¼ì´ì§€ë§Œ `ìœ„ì¹˜ê°€ ë‹¤ë¥´ê¸° ë•Œë¬¸ì—` êµ¬ë¶„ì´ ìˆì„ ìˆ˜ ìˆë“¯ì´ agentì˜ ì´ëŸ° êµ¬ì¡°ì ì¸ íŠ¹ì„±ì„ ë°˜ì˜í•´ì„œ policyë¥¼ ë§Œë“ ë‹¤ë©´ observation ì •ë³´ë“¤ê°„ì˜ êµ¬ë¶„ì„ í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤. ì´ëŸ° agentì˜ êµ¬ì¡°ì  ê´€ê³„ì„±ì„ ë‚˜íƒ€ë‚´ê¸° ìœ„í•´ì„œ MLPëŒ€ì‹  ê·¸ë˜í”„ë¥¼ í™œìš©í•˜ê²Œ ë˜ì—ˆê³  NerveNetì„ ê³ ì•ˆí•˜ê²Œ ë˜ì—ˆë‹¤. NerveNetì€ ê·¸ë˜í”„ êµ¬ì¡°ë¡œ ë˜ì–´ ìˆëŠ” policy networkì—ì„œ ê° ë…¸ë“œë“¤ì˜ ì •ë³´ë“¤ì´ ì „íŒŒ(propagation)ë˜ë©° agentì˜ ë¶€ë¶„ë“¤ì„ ë‚˜íƒ€ë‚´ëŠ” ë…¸ë“œë§ˆë‹¤ actionì„ prediction í•˜ê²Œ ëœë‹¤. MuJoCo í™˜ê²½ì—ì„œ MLP ê¸°ë°˜ì˜ ë²¤ì¹˜ë§ˆí¬ë“¤ê³¼ ë¹„ë“±í•œ í•™ìŠµê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ì—ˆìœ¼ë©°, `transfer learning task`ë¡œ agentì˜ í¬ê¸°(size)ì™€ agentì˜ ì¼ë¶€ íŒŒíŠ¸ê°€ ì‘ë™í•˜ì§€ ì•ŠëŠ”(disability) variationì„ ì£¼ì—ˆì„ ë•Œë„ ì˜ í•™ìŠµë˜ì—ˆìœ¼ë©° `multi-task learning`ìœ¼ë¡œ walker ê·¸ë£¹ì˜ ë‹¤ì–‘í•œ í™˜ê²½ì—ì„œì˜ í•™ìŠµ ê²°ê³¼ë“¤ë„ ì¢‹ì•˜ë‹¤. ì´ëŸ° ê²°ê³¼ë“¤ì„ í†µí•´ NerveNetì´ `transferable`í•  ë¿ë§Œ ì•„ë‹ˆë¼ `zero-shot setting`ë„ ê°€ëŠ¥í•¨ì„ ë³´ì—¬ì£¼ì—ˆë‹¤.

- `transferable` - A taskë¥¼ í•™ìŠµí•œ ë„¤íŠ¸ì›Œí¬(weights)ë¥¼ í™œìš©í•˜ì—¬ B task í•™ìŠµì—ë„ ì ìš©í•˜ì—¬ scratchì—ì„œ B taskë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒë³´ë‹¤ ë” ë¹ ë¥´ê³  íš¨ìœ¨ì ì¸ í•™ìŠµì„ ê°€ëŠ¥í•˜ê²Œ í•  ìˆ˜ ìˆë‹¤ëŠ” ì˜ë¯¸. A task í•™ìŠµì—ì„œ ìŠµë“í•œ ë…¼ë¦¬ì²´ê³„ë¥¼ B taskì—ë„ ì ìš©í•  ìˆ˜ ìˆìŒìœ¼ë¡œ ë³¼ ìˆ˜ ìˆë‹¤.
- [`zero-shot`](https://en.wikipedia.org/wiki/Zero-shot_learning) - Meta learningì—ì„œ ì‚¬ìš©ë˜ëŠ” ìš©ì–´ë¡œ A taskì— ëŒ€í•´ì„œ í•™ìŠµëœ ë„¤íŠ¸ì›Œí¬ê°€ fine tuningì´ ì—†ì´ ë°”ë¡œ **unseen** new task Bì— ëŒ€í•´ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ëŠ” ê²ƒì„ ì˜ë¯¸.

# Introduction

ë§ì€ ê°•í™”í•™ìŠµ ë¬¸ì œë“¤ì—ì„œ agentë“¤ì€ ì—¬ëŸ¬ê°œì˜ ë…ë¦½ì ì¸ controllerë“¤ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤. ì˜ˆë¥¼ë“¤ì–´ ë¡œë´‡ì˜ ì œì–´ì—ì„œ ê°•í™”í•™ìŠµì´ ë§ì´ ì ìš©ë˜ê³  ìˆëŠ”ë°, ë¡œë´‡ì€ ì—¬ëŸ¬ê°œì˜ ë§í¬(link)ë“¤ê³¼ ì¡°ì¸íŠ¸(joint)ë“¤ë¡œ ì´ë£¨ì–´ì ¸ ìˆê³  ì›€ì§ì„ì´ ì¼ì–´ë‚˜ëŠ” jointë“¤ì„ ê° ê°œë³„ì ì¸ controllerë¡œ ë³¼ ìˆ˜ ìˆë‹¤.  ì´ë•Œ linkëŠ” ë¡œë´‡ì˜ ë¬¼ë¦¬ì ì¸ í˜•íƒœë¥¼ êµ¬ì„±í•˜ëŠ” ë¼ˆëŒ€ì²˜ëŸ¼ ìƒê°í•˜ë©´ ë˜ê³  jointëŠ” ë¡œë´‡ì˜ ëª¨ì…˜ì„ ê²°ì •í•˜ëŠ” ê´€ì ˆë¡œ ìƒê°í•  ìˆ˜ ìˆë‹¤.(ë³´í†µ íšŒì „ì´ ê°€ëŠ¥í•œ revolute jointë¥¼ ì‚¬ìš©í•œë‹¤.) ë¡œë´‡ì€ `link-joint-link- ... -joint-link`ì™€ ê°™ì´ linkì™€ jointê°€ ì²´ì¸ì²˜ëŸ¼ ì—°ê²°ë˜ì–´ì„œ êµ¬ì„±ë˜ëŠ”ë°, ê° linkì™€ jointì˜ ì›€ì§ì„ì€ ìì‹ ì˜ ìƒíƒœì—ë§Œ ì˜ì¡´í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ì—°ê²°ëœ ì£¼ë³€ linkì™€ jointë“¤ì—ê²Œì„œë„ ì˜í–¥ì„ ë°›ì„ ìˆ˜ ë°–ì— ì—†ë‹¤. ë¡œë´‡ì„ ì›€ì§ì´ë„ë¡ `ì œì–´`ë¥¼ í•œë‹¤ëŠ” ê²ƒì€ ë°”ë¡œ ëª¨ì…˜ì„ ë§Œë“œëŠ” `jointì˜ ì›€ì§ì„ì„ ê²°ì •`í•˜ëŠ” ê²ƒì´ë‹¤. ë”°ë¼ì„œ robot agentëŠ” ê°•í™”í•™ìŠµì—ì„œì˜ actionì„ jointì„ ì œì–´í•˜ëŠ” ê²ƒìœ¼ë¡œ ìƒê°í•  ìˆ˜ ìˆê³  actionìœ¼ë¡œ ë¡œë´‡ì˜ jointì˜ ê°ë„ë¥¼ ë°”ê¿”ê°€ë©° ë¡œë´‡ì„ ì›€ì§ì´ê²Œ ëœë‹¤. 

ë³´í†µ ê°•í™”í•™ìŠµì—ì„œ agentì˜ policyëŠ” MLPë¡œ êµ¬ì„±í•œë‹¤. MLPê¸°ë°˜ì˜ policyëŠ” ê°€ì¥ ë‹¨ìˆœí•œ ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°ë¡œ inputìœ¼ë¡œ agentê°€ ì–»ëŠ” observation ì •ë³´ë¥¼ `concatenation`í•´ì„œ ë„£ì–´ì£¼ê²Œ ëœë‹¤. ë‹¤ì‹œ ë¡œë´‡ agentì˜ ì˜ˆì‹œë¡œ ëŒì•„ê°€ì„œ ìƒê°í•´ë³´ë©´, agentê°€ observationìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ì •ë³´ë¡œëŠ” ê° jointì˜ íšŒì „ê°ë„, íšŒì „ ê°ì†ë„, ìœ„ì¹˜ ì •ë³´ë“± ë‹¤ì–‘í•œ ì¢…ë¥˜ì˜ ì •ë³´ë“¤ì´ ìˆê³  ì´ ë‹¤ì–‘í•œ ì •ë³´ë“¤ì€ ê° jointë¡œë¶€í„° ì–»ê²Œ ë˜ë¯€ë¡œ `ê° jointì—ì„œ ì–»ì„ ìˆ˜ ìˆëŠ” ì •ë³´ x jointì˜ ìˆ˜` ê°€ ë³´í†µ observationì˜ ì°¨ì› ìˆ˜ê°€ ëœë‹¤. ë¡œë´‡ì˜ jointê°€ ë§ì•„ì§ˆ ìˆ˜ë¡ ì°¨ì›ì´ ë°°ë¡œ ì»¤ì§€ê²Œ ë˜ê³  ì´ëŸ° ì—¬ëŸ¬ ì •ë³´ë“¤ì„ ë‹¨ìˆœíˆ concatenateí•´ì„œ policy ë„¤íŠ¸ì›Œí¬ì— ë„£ì–´ì£¼ëŠ” ê²ƒì€ ë” ë§ì€ training timeì„ ìš”êµ¬í•˜ê²Œ ë˜ê³  ë” ë§ì€ í™˜ê²½ê³¼ agent ê°„ì˜ interaction ê³¼ì •ì´ í•„ìš”í•˜ê²Œ ëœë‹¤. ë”°ë¼ì„œ ë³¸ ë…¼ë¬¸ì—ì„œëŠ” agentê°€ ê°€ì§€ê³  ìˆëŠ” (linkì™€ jointë¡œ ì´ë£¨ì–´ì§„) êµ¬ì¡°ì ì¸ íŠ¹ì„±ì„ ì´ìš©í•´ì„œ policyë¥¼ ê·¸ë˜í”„ë¡œ ë§Œë“¤ì–´ observationì„ ë„£ì–´ì£¼ê³  í•™ìŠµì„ í•˜ëŠ” ê²ƒì„ ì œì•ˆí•˜ê²Œ ëœë‹¤.

ë¡œë´‡ì´ë‚˜ ë™ë¬¼ë“¤ì˜ ì‹ ì²´ì ì¸ êµ¬ì¡°ë¥¼ ë³´ë©´ ê·¸ë˜í”„ êµ¬ì¡°ì™€ ìœ ì‚¬í•˜ë‹¤. ìœ„ì—ì„œ ì„¤ëª…í•œ linkì™€ jointì˜ ì²´ì¸ê³¼ ê°™ì€ ì—°ê²°ì„±ì€ `Graph Neural Network`ë¥¼ ì ìš©í•˜ê¸°ì— ì¢‹ë‹¤. ê·¸ë˜ì„œ NerveNetì—ì„œ ì •ë³´ë“¤ì˜ propagationì€ ì´ëŸ° ê·¸ë˜í”„ êµ¬ì¡°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¼ì–´ë‚˜ê²Œ ë˜ê³  agentì˜ bodyì •ë³´ë¥¼ ì—¬ëŸ¬ ë‹¤ë¥¸ íŒŒíŠ¸ë“¤ì„ ê·¸ë˜í”„ì˜ nodeì™€ edgeë¡œ ì •ì˜í•˜ë©´ì„œ ì›€ì§ì„ì´ ì¼ì–´ë‚˜ëŠ” body nodeë“¤ì˜ actionì„ ê²°ì •í•˜ê²Œ ëœë‹¤. 

# NerveNet

ìš°ì„  ê°•í™”í•™ìŠµì˜ Notationì„ ì •ë¦¬í•´ë³´ë©´, ë³¸ ë…¼ë¬¸ì—ì„œëŠ” locomotion control problemë“¤ì„ ëª©í‘œë¡œ ì¡ì•˜ê¸° ë•Œë¬¸ì— `infinite-horizon discounted Markov decision process (MDP)` ë¡œ ì„¤ì •í–ˆë‹¤. ë³´í†µ `continuous`í•œ ì œì–´ ë¬¸ì œì—ì„œëŠ” ì‹œê°„ í• ì¸ìœ¨ì„ ê³ ë ¤í•œ ë¬´í•œ ì‹œê°„ ìŠ¤í…ì„ ê°€ì§€ê³  MDPë¥¼ êµ¬ì„±í•˜ê²Œ ëœë‹¤(ì‹¤ì œë¡œëŠ” max stepì„ ì„¤ì •í•˜ê¸´í•˜ë‚˜ ë§¤ìš° í° ìˆ˜ë¡œ ì¡ëŠ”ë‹¤).  

state í˜¹ì€ observation spaceë¡œ $S$ , action spaceë¡œ $A$, stochastic policy $\pi_{\theta}\left(a^{\tau} \mid s^{\tau}\right)$ ëŠ” íŒŒë¼ë¯¸í„° $\theta$ë¥¼ ê°€ì§€ê³  í˜„ì¬ ìƒíƒœ $s$ë¥¼ ê¸°ë°˜ìœ¼ë¡œ $a$ë¥¼ ë§Œë“¤ê²Œ ëœë‹¤. ì´ë ‡ê²Œ ë‚˜ì˜¨ agentì˜ $a$ì™€ $s$ë¥¼ ê°€ì§€ê³  í™˜ê²½ì—ì„œëŠ” reward $r\left(s^{\tau}, a^{\tau}\right)$ ë¥¼ ì£¼ê²Œ ë˜ê³  agentëŠ” ì´ë ‡ê²Œ ë°›ëŠ” rewardë¥¼ ìµœëŒ€í™”í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•˜ê²Œ ëœë‹¤. 

ì´ëŸ¬í•œ MDP êµ¬ì„±ì€ ê¸°ë³¸ì ì¸ ê°•í™”í•™ìŠµì˜ Notationì—ì„œ í¬ê²Œ ë²—ì–´ë‚˜ì§€ ì•ŠëŠ”ë‹¤.

## Graph Construction

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì‚¬ìš©í•œ MuJoCoì˜ agentë“¤ì€ ì´ë¯¸ êµ¬ì¡°ì ìœ¼ë¡œ tree êµ¬ì¡°ë¥¼ ê°€ì§€ê³  ìˆë‹¤. NerveNetì˜ í•µì‹¬ ì•„ì´ë””ì–´ì¸ ê·¸ë˜í”„ë¥¼ êµ¬ì„±í•˜ê¸° ìœ„í•´ `body`ì™€ `joint`, `root`ë¼ëŠ” 3ê°€ì§€ ì¢…ë¥˜ì˜ ë…¸ë“œë¥¼ ì„¤ì •í–ˆë‹¤. `body` ë…¸ë“œëŠ” ë¡œë´‡ê³µí•™ì—ì„œ ë§í•˜ëŠ” link ê¸°ì¤€ì˜ ì¢Œí‘œì‹œìŠ¤í…œì„ ë‚˜íƒ€ë‚´ëŠ” ë…¸ë“œì´ê³ , `joint` ë…¸ë“œëŠ” ëª¨ì…˜ì˜ ììœ ë„(freedom of motion)ì„ ë‚˜íƒ€ë‚´ë©° 2ê°œì˜ body ë…¸ë“œë“¤ì„ ì—°ê²°í•´ì£¼ëŠ” ë…¸ë“œì´ë‹¤. 

ì•„ë˜ëŠ” `Ant` í™˜ê²½ì˜ ì˜ˆì‹œì¸ë°, í•œ ê°€ì§€ ê·¸ë¦¼ì—ì„œ í—·ê°ˆë¦¬ì§€ ë§ì•„ì•¼ í•  ì ì€ ê·¸ë¦¼ì—ì„œëŠ” ë§ˆì¹˜ `body`ì™€ `root` ë…¸ë“œë§Œ ë…¸ë“œë¡œ ë§Œë“ ê²ƒ ì²˜ëŸ¼ ë³´ì´ì§€ë§Œ rootì™€ body, bodyì™€ bodyë¥¼ ì—°ê²°í•˜ëŠ” ì—£ì§€ë“¤ë„ ì‹¤ì œë¡œëŠ” `joint` ë…¸ë“œë“¤ì´ë‹¤.(*we omit the joint nodes and use edges to represent the physical connections of joint nodes*.)`root`ë¼ëŠ” ë…¸ë“œëŠ” agentì˜ ì¶”ê°€ì ì¸ ì •ë³´ë“¤ì„ ë‹´ì„ ë¶€ë¶„ìœ¼ë¡œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ì¶”ê°€í•œ ë…¸ë“œ ì¢…ë¥˜ë¡œ, ì˜ˆë¥¼ ë“¤ì–´ agentê°€ ë„ë‹¬í•´ì•¼ í•˜ëŠ” target positionì— ëŒ€í•œ ì •ë³´ ë“±ì´ ë‹´ê²¨ìˆë‹¤. 

![](https://i.imgur.com/pjCrsxt.png?1){fig-align="default"}


## NerveNet as Policy

í¬ê²Œ 3ê°€ì§€ íŒŒíŠ¸ë¡œ NerveNetì„ ì‚´í´ë³¼ ê²ƒì¸ë° ìš°ì„  (0) `Notation`ì„ ë³´ê³  ë‚œë’¤, (1) `Input model` (2) `Propagation model` (3) `Output model` ìˆœìœ¼ë¡œ ì‚´í´ë³¼ ì˜ˆì •ì´ë‹¤.

### 0. Notation

ê·¸ë˜í”„ì—ì„œì˜ ë…¸í…Œì´ì…˜ì€ ë‹¤ìŒê³¼ ê°™ì´ $G$ ë¼ëŠ” ê·¸ë˜í”„ëŠ” ë…¸ë“œ ì§‘í•© $V$ì™€ ì—£ì§€ ì§‘í•© $E$ë¡œ êµ¬ì„±ëœë‹¤.

$$
G=(V, E)
$$

Nervenet policyë¥¼ êµ¬ì„±í•˜ëŠ” ê·¸ë˜í”„ëŠ” `Directed graph(ìœ í–¥ ê·¸ë˜í”„)`ì´ê¸° ë•Œë¬¸ì— ê° ë…¸ë“œì—ì„œì˜ `in`ê³¼ `out`ì´ ë”°ë¡œ ëª…ì‹œë˜ê²Œ ëœë‹¤. 

- ë…¸ë“œ $u$ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ë…¸ë“œ $u$ë¡œ ë“¤ì–´ì˜¤ëŠ” ì´ì›ƒ ë…¸ë“œì´ë©´ $\mathcal{N}_{in}(u)$
- ë…¸ë“œ $u$ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ë…¸ë“œ $u$ì—ì„œ ë‚˜ê°€ëŠ” ì´ì›ƒ ë…¸ë“œì´ë©´ $\mathcal{N}_{out}(u)$ 

ê·¸ë˜í”„ì˜ ëª¨ë“  ë…¸ë“œ $u$ëŠ” íƒ€ì…ì„ ê°€ì§€ê²Œ ë˜ê³  ì´ë¥¼ $p_{u} \in\{1,2, \ldots, P\}$ (associated note type)ë¡œ ë‚˜íƒ€ë‚´ë©° ì—¬ê¸°ì—ì„œëŠ” ìœ„ì— ì„¤ëª…í•œ ê²ƒê³¼ ê°™ì´ `body`, `joint`, `root` 3ê°€ì§€ íƒ€ì…ì´ ìˆë‹¤. 

ë…¸ë“œë“¤ ë¿ë§Œ ì•„ë‹ˆë¼ ì—£ì§€ë“¤ë„ íƒ€ì…ì„ ì •í•  ìˆ˜ ìˆëŠ”ë°  $c_{(u, v)} \in\{1,2, \ldots, C\}$ (associate each edge)ë¡œ í‘œê¸°í•˜ì—¬ ë…¸ë“œìŒ $(u, v)$ ì‚¬ì´ì˜ ì—£ì§€ íƒ€ì…ì„ ì •ì˜í•  ìˆ˜ ìˆë‹¤.(í•˜ë‚˜ì˜ ì—£ì§€ì— ëŒ€í•´ì„œ ì—¬ëŸ¬ ì—£ì§€ íƒ€ì…ì„ ì •ì˜í•  ìˆ˜ ìˆì§€ë§Œ ì—¬ê¸°ì—ì„œëŠ” ì‹¬í”Œ ì´ì¦ˆ ë” ë² ìŠ¤íŠ¸ ì² í•™ìœ¼ë¡œ í•˜ë‚˜ì˜ ì—£ì§€ëŠ” í•˜ë‚˜ì˜ íƒ€ì…ë§Œ ê°€ì§€ë„ë¡ í–ˆë‹¤)

ì´ë ‡ê²Œ ë…¸ë“œë³„, ì—£ì§€ë³„ íƒ€ì…ì„ ë‚˜ëˆ”ìœ¼ë¡œì¨,

- `ë…¸ë“œ íƒ€ì…`ì€ ë…¸ë“œë“¤ê°„ì˜ ë‹¤ë¥¸ ì¤‘ìš”ë„ë¥¼ íŒŒì•…í•˜ëŠ”ë° ë„ì›€ì´ ë˜ê³ 
- `ì—£ì§€ íƒ€ì…`ì€ ë…¸ë“œë“¤ê°„ì˜ ì„œë¡œë‹¤ë¥¸ ê´€ê³„ë“¤ì„ ë‚˜íƒ€ë‚´ê³  ì´ ê´€ê³„ì˜ ì¢…ë¥˜ì— ë”°ë¼ ì •ë³´ë¥¼ ë‹¤ë¥´ê²Œ propagation í•˜ê²Œ ëœë‹¤.

ì´ì œ ì‹œê°„ ë…¸í…Œì´ì…˜ì— ëŒ€í•œ ë¶€ë¶„ì„ ì‚´í´ë³´ì. NerveNetì—ëŠ” ì‹œê°„(time step)ì˜ ê°œë…ì´ 2ê°€ì§€ ì¡´ì¬í•œë‹¤.

1. ê¸°ì¡´ ê°•í™”í•™ìŠµì—ì„œ í™˜ê²½ê³¼ agent ì‚¬ì´ì˜ interaction time stepì„ ë‚˜íƒ€ë‚´ëŠ” $\tau$
2. NerveNetì˜ ë‚´ë¶€ graph policyì—ì„œì˜ propagation stepì„ ë‚˜íƒ€ë‚´ëŠ” $t$

ë‹¤ì‹œ í’€ì–´ì„œ ìƒê°í•´ë³´ë©´, ê°•í™”í•™ìŠµì˜ ì‹œê°„ ê°œë… **$\tau$ ìŠ¤í…**ì—ì„œ í™˜ê²½ìœ¼ë¡œë¶€í„° observationì„ ë°›ê³ , ë°›ì€ observationì„ ê¸°ë°˜ìœ¼ë¡œ  **$t$ ìŠ¤í…**ë™ì•ˆ NerveNetì˜ ë‚´ë¶€ì˜ ê·¸ë˜í”„ì˜ propagationì´ ì¼ì–´ë‚œë‹¤.

### 1. Input model

ìœ„ì—ì„œ ë§í–ˆë“¯ì´ í™˜ê²½ê³¼ ìƒí˜¸ì‘ìš©ìœ¼ë¡œ observation $s^{\tau} \in \mathcal{S}$ì„ ë°›ê²Œ ëœë‹¤(time step $\tau$). ì´ $s^{\tau}$ëŠ” concatenationëœ ê° ë…¸ë“œì˜ observationì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. ì´ì œ ê°•í™”í•™ìŠµ interaction ìˆ˜ì¤€ì˜ $\tau$ ìŠ¤í…ì€ ì ì‹œ ë©ˆì¶°ë‘ê³  ê·¸ë˜í”„ ë‚´ë¶€ì˜ íƒ€ì„ ìŠ¤í…ì¸ $t$  ìˆ˜ì¤€ì—ì„œ ìƒê°í•´ë³´ì. observationì€ node $u$ì— í•´ë‹¹í•˜ëŠ” $x_{u}$ë¡œ í‘œí˜„í•  ìˆ˜ ìˆê³  $x_{u}$ëŠ” input network $F_{\mathrm{in}}$(MLP)ë¥¼ ê±°ì³ì„œ ê³ ì •ëœ í¬ê¸°ì˜ state vectorì¸ $h_{u}^{0}$ê°€ ëœë‹¤. $h_{u}^{0}$ì˜ ë…¸í…Œì´ì…˜ì„ í’€ì–´ì„œ í•´ì„í•˜ë©´ ë…¸ë“œ `u`ì˜ propagation step `0` ì—ì„œì˜ state vectorì¸ ê²ƒì´ë‹¤. ì´ë•Œ observation vector $x_{u}$ê°€ ë…¸ë“œë§ˆë‹¤ í¬ê¸°ê°€ ë‹¤ë¥¼ ê²½ìš° zero paddingìœ¼ë¡œ ë§ì¶°ì„œ input networkì— ë„£ì–´ì£¼ê²Œ ëœë‹¤.

$$
h_{u}^{0}=F_{\text {in }}\left(x_{u}\right)
$$



### 2. Propagation model

NerveNetì˜ propagation ê³¼ì • ë…¸ë“œë“¤ ê°„ì— ì£¼ê³  ë°›ëŠ” ì •ë³´ë¥¼ `message`ë¼ê³  í•˜ê²Œ ë˜ê³  ì´ëŠ” ë…¸ë“œë“¤ ê°„ì— ì£¼ê³  ë°›ëŠ” ìƒí˜¸ì‘ìš©ì´ë¼ê³  ìƒê°í•  ìˆ˜ ìˆë‹¤. Propagation modelì€ 3ê°€ì§€ ë‹¨ê³„ë¡œ ë‚˜ëˆ„ì–´ì„œ ë³¼ ìˆ˜ ìˆë‹¤.

1. Message Computation
    - ì „ë‹¬í•  ë©”ì„¸ì§€ë¥¼ ê³„ì‚°í•œë‹¤.
    - propagation stepì¸ $t$ì—, ëª¨ë“  ë…¸ë“œë“¤ $u$ì—ì„œ state vector $h_{u}^{t}$ë¥¼ ì •ì˜í•  ìˆ˜ ìˆë‹¤.
    - ë…¸ë“œ $u$ë¡œ ëª¨ì•„ì§€ëŠ”(in-coming) ëª¨ë“  ì—£ì§€ë“¤ì„ ê°€ì§€ê³  ë©”ì‹œì§€ë¥¼ êµ¬í•˜ê²Œ ë˜ëŠ”ë°, ì´ë•Œ $M$ì€ MLPì´ê³  Mì˜ ì•„ë˜ì²¨ì $c_{(u, v)}$ ë…¸í…Œì´ì…˜ì—ì„œ ì•Œ ìˆ˜ ìˆë“¯ì´ ê°™ì€ ì¢…ë¥˜ì˜ ì—£ì§€ì— ëŒ€í•´ì„œëŠ” ê°™ì€ message function $M$ì„ ì“´ë‹¤.
        
        $$
        m_{(u, v)}^{t}=M_{c_{(u, v)}}\left(h_{u}^{t}\right)
        $$
        
    - ì˜ˆë¥¼ ë“¤ì–´ ì•„ë˜ ê·¸ë¦¼ì€ `CentipedeEight` ì˜ ëª¨ìŠµì¸ë°, ì™¼ìª½ì€ ì‹¤ì œ agentì˜ ëª¨ìŠµì„ ë‚˜íƒ€ë‚´ê³  ìˆìœ¼ë©° ì˜¤ë¥¸ìª½ì€ agentë¥¼ ê·¸ë˜í”„ë¡œ ë‚˜íƒ€ëƒˆì„ ë•Œì˜ ëª¨ìŠµì´ë‹¤. ì—¬ê¸°ì—ì„œ 2ë²ˆì§¸ torsoì—ì„œ ì²«ë²ˆì§¸ ì„¸ë²ˆì§¸ torsoì—ì„œ ë³´ë‚¼ ë•Œ ê°™ì€ ë©”ì„¸ì§€ í‘ì…˜ $M_{1}$ ì„ ì‚¬ìš©í•˜ê³ , LeftHipê³¼ RightHipìœ¼ë¡œ ë³´ë‚´ëŠ” ë©”ì„¸ì§€ í‘ì…˜ $M_{2}$ë¥¼ ì‚¬ìš©í•˜ê²Œ ë˜ëŠ” ê²ƒì´ë‹¤.
        
        ![](https://i.imgur.com/QFOY9VH.png){fig-align="default"}
        
2. Message Aggregation
    - ì• ë‹¨ê³„ì—ì„œ ëª¨ë“  ë…¸ë“œë“¤ì— ëŒ€í•´ì„œ ë©”ì„¸ì§€ ê³„ì‚°ì´ ëë‚œ í›„ì— in-coming ì´ì›ƒ ë…¸ë“œë“¤ë¡œë¶€í„° ì˜¨(ê³„ì‚°ëœ) ë©”ì„¸ì§€ë¥¼ ëª¨ìœ¼ê²Œ ëœë‹¤. ì´ë•Œ summation, average, max-pooling ë“± ë‹¤ì–‘í•œ aggregation í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.
        
        $$
        \bar{m}_{u}^{t}=A\left(\left\{h_{v}^{t} \mid v \in \mathcal{N}_{i n}(u)\right\}\right)
        $$
        
3. States Update
    - ì´ì œ ëª¨ì€ ë©”ì„¸ì§€ë¥¼ ê¸°ë°˜ìœ¼ë¡œ state vectorë¥¼ ì—…ë°ì´íŠ¸ í•˜ë©´ ëœë‹¤!
        
        $$
        h_{u}^{t+1}=U_{p_{u}}\left(h_{u}^{t}, \bar{m}_{u}^{t}\right)
        $$
        
    - ì—¬ê¸°ì„œ ì—…ë°ì´íŠ¸ í•¨ìˆ˜ $U$ ëŠ” a gated recurrent unit (GRU), a long short term memory (LSTM) unit ë˜ëŠ” MLPê°€ ë  ìˆ˜ ìˆë‹¤.
    - Update functionì˜ ì•„ë˜ì²¨ì $p_{u}$ì—ì„œ ë³¼ ìˆ˜ ìˆë‹¤ì‹œí”¼ ê°™ì€ ë…¸ë“œ íƒ€ì…ì´ë©´ ê°™ì€ update function $U$ë¥¼ ì“°ê²Œ ëœë‹¤. ì´ë ‡ê²Œ ì—…ë°ì´íŠ¸ëœ state vectorëŠ” íƒ€ì„ ìŠ¤í… $t$ê°€ í•˜ë‚˜ ì˜¬ë¼ê°„ $t+1$ ì´ ëœ $h_{u}^{t+1}$ê°€ ëœë‹¤.

ì´ë ‡ê²Œ ë‚´ë¶€ propagation ê³¼ì • 3ë‹¨ê³„(Message Computation, Message Aggregation, States Update)ê°€ $T$ ìŠ¤í…ë™ì•ˆ ì¼ì–´ë‚˜ê²Œ ë˜ê³  ê° ë…¸ë“œì˜ ìµœì¢… state vectorëŠ” $h_{u}^{T}$ ê°€ ëœë‹¤.

### 3. Output model

ì „í˜•ì ì¸ RLì˜ MLP í´ë¦¬ì‹œì—ì„œëŠ” ë„¤íŠ¸ì›Œí¬ì—ì„œ ê° actionì˜ gaussian distributionì˜ meanì„ ë½‘ì•„ë‚´ê²Œ ëœë‹¤. stdëŠ” trainableí•œ ë²¡í„°ì´ë‹¤. NerveNetì—ì„œë„ stdëŠ” ë¹„ìŠ·í•˜ê²Œ ë‹¤ë£¨ì§€ë§Œ `ê° ë…¸ë“œì— ë§ˆë‹¤` action predictionì„ ë§Œë“¤ê²Œ ëœë‹¤. 

actuatorì™€ ì—°ê²°ë˜ì–´ ìˆëŠ” ë…¸ë“œë“¤ì˜ ì§‘í•©ì„ $O$ë¼ê³  í•˜ì. ì´ ì§‘í•©ì— ìˆëŠ” ë…¸ë“œë“¤ì˜ ìµœì¢… state vector $h_{u \in \mathcal{O}}^{T}$ëŠ” MLPì¸ Ouput model $O_{q_{u}}$ì— ì¸í’‹ìœ¼ë¡œ ë“¤ì–´ê°€ê²Œ ë˜ê³  ì•„ì›ƒí’‹ìœ¼ë¡œ ê° actuatorì˜ action distributionì¸ gaussian distributionì˜ mean $\mu$ì„ ì¶œë ¥í•˜ê²Œ ëœë‹¤. ì—¬ê¸°ì—ì„œ ìƒˆë¡œìš´ ë…¸í…Œì´ì…˜ $q_{u}$ë¥¼ ë³¼ ìˆ˜ ìˆëŠ”ë° $q_{u}$ëŠ” ì•„ì›ƒí’‹ íƒ€ì…, ì¦‰ ì•„ì›ƒí’‹ì„ ë‚´ë†“ëŠ” ë…¸ë“œ $u$ì˜ íƒ€ì…ìœ¼ë¡œ ì•„ì›ƒí’‹ í‘ì…˜ì˜ ì•„ë˜ì²¨ìì— $q_{u}$ì— ë”°ë¼ ì•„ì›ƒí’‹ ë…¸ë“œì˜ íƒ€ì…ì´ ê°™ìœ¼ë©´ Output functionì„ ê³µìœ í•  ìˆ˜ ìˆë‹¤. ë‹¤ì‹œë§í•´ ì•„ì›ƒí’‹ ë…¸ë“œ íƒ€ì…ì— ë”°ë¼ ì»¨íŠ¸ë¡¤ëŸ¬ë¥¼ ê³µìœ í•  ìˆ˜ë„ ìˆëŠ” ê²ƒì´ë‹¤. ìœ„ì˜ Centipedesì˜ ì˜ˆì‹œë¡œ ë³´ë©´, ê°™ì€ LeftHip ë¼ë¦¬ëŠ” ì»¨íŠ¸ë¡¤ëŸ¬ë¥¼ ê³µìœ í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤.

$$
\mu_{u \in \mathcal{O}}=O_{q_{u}}\left(h_{u}^{T}\right)
$$

ë…¼ë¬¸ì—ì„œ ì‹¤ì œë¡œ ì‹¤í—˜ì„ í•´ë´¤ì„ ë•Œ ë‹¤ë¥¸ íƒ€ì…ì˜ ì»¨íŠ¸ë¡¤ëŸ¬ë“¤ì„ í•˜ë‚˜ë¡œ í†µí•©í–ˆë”ë¼ë„($O$ functionì„ ë‹¤ ê°™ì€ MLPë¡œ ì‚¬ìš©) í¼í¬ë¨¼ìŠ¤ê°€ ê·¸ë ‡ê²Œ í•´ì³ì§€ì§€ ì•ŠìŒì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆë‹¤ê³  í•œë‹¤.

ì—¬ê¸°ê¹Œì§€í•´ì„œ ê·¸ë˜í”„ ë…¸í…Œì´ì…˜ì„ ë¹Œë ¤ ê·¸ë˜í”„ ê¸°ë°˜ ê°€ìš°ì‹œì•ˆ stochastic policyë¥¼ ë‚˜íƒ€ë‚´ë©´ ì•„ë˜ì˜ ìˆ˜ì‹ê³¼ ê°™ë‹¤. 

$$
\pi_{\theta}\left(a^{\tau} \mid s^{\tau}\right)=\prod_{u \in \mathcal{O}} \pi_{\theta, u}\left(a_{u}^{\tau} \mid s^{\tau}\right)=\prod_{u \in \mathcal{O}} \frac{1}{\sqrt{2 \pi \sigma_{u}^{2}}} e^{\left(a_{u}^{\tau}-\mu_{u}\right)^{2} /\left(2 \sigma_{u}^{2}\right)}
$$

---

ì—¬ê¸°ê¹Œì§€ NerveNetì˜ ê° ë‹¨ê³„ë¥¼ `Walker-Ostrich` í™˜ê²½ì—ì„œ ì˜ˆì‹œë¡œ í•œëˆˆì— ë³´ê¸° ì‰½ê²Œ ì •ë¦¬í•œ ê·¸ë¦¼ì€ ì•„ë˜ì™€ ê°™ë‹¤. 

![](https://i.imgur.com/lnrF2Pb.png){fig-align="default"}

## Learning Algorithm

ì´ì „ íŒŒíŠ¸ì—ì„œ NerveNetì˜ ë‚´ë¶€ì—ì„œ propagation ìŠ¤í… $t$ ë‹¨ìœ„ì—ì„œ ê° ë‹¨ê³„ë“¤ì„ ìì„¸íˆ ì‚´í´ë³´ì•˜ë‹¤ë©´ ì´ì œ ê°•í™”í•™ìŠµ íƒ€ì„ ìŠ¤í… $\tau$ ë‹¨ìœ„ì—ì„œ í•™ìŠµì˜ ëª©ì í•¨ìˆ˜ì™€ ì•Œê³ ë¦¬ì¦˜ì„ ì‚´í´ë³´ì. ëª©ì í•¨ìˆ˜ëŠ” ì „í˜•ì ì¸ RLê³¼ ë‹¤ë¥¸ ì ì´ ì—†ì´ policyì˜ íŒŒë¼ë¯¸í„° $\theta$ë¥¼ ê°€ì§€ê³  Return ê°’ì„ maximizationí•˜ëŠ” ê²ƒìœ¼ë¡œ í•œë‹¤.

$$
J(\theta)=\mathbb{E}{\pi}\left[\sum{\tau=0}^{\infty} \gamma^{\tau} r\left(s^{\tau}, a^{\tau}\right)\right]
$$

ê°•í™”í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œëŠ” [`PPO`](https://arxiv.org/abs/1707.06347)ê³¼ [`GAE`](https://arxiv.org/abs/1506.02438)ë¥¼ ì‚¬ìš©í–ˆìœ¼ë©° í•´ë‹¹ ì•Œê³ ë¦¬ì¦˜ë“¤ì˜ ë‚´ìš©ì€ ê°ê° ì•Œê³ ë¦¬ì¦˜ë“¤ì˜ ì›ë˜ ìˆ˜ì‹ê³¼ ë‚´ìš©ë“¤ê³¼ ìƒì´í•œ ì ì´ ì—†ìœ¼ë¯€ë¡œ ê° ë…¼ë¬¸ìœ¼ ì°¸ê³ í•˜ë©´ ë˜ê¸° ë•Œë¬¸ì— ì´ë²ˆ ë…¼ë¬¸ ë¦¬ë·°ì—ì„œëŠ” ìƒëµí•œë‹¤.

PPOì™€ GAE ì•Œê³ ë¦¬ì¦˜ì„ ì°¸ê³ í•˜ì—¬ ìœ„ì˜ ëª©ì í•¨ìˆ˜ $J$ë¥¼ ì •ë¦¬í•˜ë©´ NerveNetì˜ ëª©ì í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. 

$$
\begin{aligned}
\tilde{J}(\theta)=& J(\theta)-\beta L_{K L}(\theta)-\alpha L_{V}(\theta) \\
=& \mathbb{E}_{\pi_{\theta}}\left[\sum_{\tau=0}^{\infty} \min \left(\hat{A}^{\tau} r^{\tau}(\theta), \hat{A}^{\tau} \operatorname{clip}\left(r^{\tau}(\theta), 1-\epsilon, 1+\epsilon\right)\right)\right] \\
&-\beta \mathbb{E}_{\pi_{\theta}}\left[\sum_{\tau=0}^{\infty} \operatorname{KL}\left[\pi_{\theta}\left(: \mid s^{\tau}\right) \mid \pi_{\theta_{o l d}}\left(: \mid s^{\tau}\right)\right]\right]-\alpha \mathbb{E}_{\pi_{\theta}}\left[\sum_{\tau=0}^{\infty}\left(V_{\theta}\left(s^{\tau}\right)-V\left(s^{\tau}\right)^{\operatorname{target}}\right)^{2}\right]
\end{aligned}
$$

ìœ„ì˜ ìˆ˜ì‹ì—ì„œ ë³¼ ìˆ˜ ìˆëŠ” `value network Vë¥¼ ì–´ë–»ê²Œ ë””ìì¸í•  ê²ƒì¸ì§€`ê°€ ì´ë²ˆ ë…¼ë¬¸ì˜ ë‹¤ë¥¸ í¬ì¸íŠ¸ë¡œ ë³¼ ìˆ˜ ìˆë‹¤. ë…¼ë¬¸ì˜ ê¸°ë³¸ ì•„ì´ë””ì–´ëŠ” policy networkë¥¼ ê·¸ë˜í”„ë¡œ í‘œí˜„í•˜ëŠ” ê²ƒì´ê³ , value networkëŠ” ì–´ë–»ê²Œ í• ì§€ ì—¬ëŸ¬ ì„ íƒì§€ë“¤ì´ ë‚¨ì•„ìˆë‹¤. ê·¸ë˜ì„œ ë³¸ ë…¼ë¬¸ì—ì„œëŠ” value networkì˜ ë””ìì¸ì„ ë‘ê³  í¬ê²Œ 3ê°€ì§€ NerveNetì˜ ë³€í˜• ì•Œê³ ë¦¬ì¦˜ë“¤ì„ ì‹¤í—˜í•´ë³´ì•˜ë‹¤.

(1) **NerveNet-MLP** : policy networkë¥¼ 1ê°œì˜ GNNìœ¼ë¡œ êµ¬ì„±í•˜ê³  `value networkëŠ” MLPë¡œ` êµ¬ì„± 

(2) **NerveNet-2** : policy networkë¥¼ 1ê°œì˜ GNNìœ¼ë¡œ êµ¬ì„±í•˜ê³  `value networkëŠ” ë˜ ë‹¤ë¥¸ GNNìœ¼ë¡œ` êµ¬ì„±(ì´ GNN 2ê°œ - *without sharing the parameters of the two GNNs*)

(3) **NerveNet-1** : `policy networkì™€ value network ëª¨ë‘ 1ê°œì˜ GNNìœ¼ë¡œ` êµ¬ì„±(ì´ GNN 1ê°œ) 

# Experiments

ë¨¼ì € MuJoCo ì‹œë®¬ë ˆì´í„°ì—ì„œ NerveNetì˜ íš¨ê³¼ë¥¼ í™•ì¸í•˜ê³  ì¼ë¶€ ì»¤ìŠ¤í…€í•œ í™˜ê²½ë“¤ì—ì„œ NerveNetì˜ transferableê³¼ multi-task learning ëŠ¥ë ¥ì„ í™•ì¸í•œë‹¤.

## 1. Comparison on standard benchmarks of MuJoCo

- ë¹„êµêµ°ìœ¼ë¡œ `MLP`, `TreeNet(ëª¨ë“  ë…¸ë“œë“¤ì´ ì—°ê²° ë˜ì–´ ìˆëŠ” ê·¸ë˜í”„, depth 1)`ì„ ì‚¬ìš©
- ì´ 8ê°œì˜ í™˜ê²½ì—ì„œ ì‹¤í—˜ - `Reacher, InvertedPendulum, InvertedDoublePendulum, Swimmer, HalfCheetah, Hopper, Walker2d, Ant`
- ì¶©ë¶„íˆ í•™ìŠµí•˜ëŠ” ìŠ¤í…ì„ ì£¼ê¸° ìœ„í•´ì„œ 1 millionì„ maxë¡œ ë‘ 
- í•˜ì´í¼ íŒŒë¼ë¯¸í„°ì˜ ê²½ìš° ê·¸ë¦¬ë“œ ì„œì¹˜ë¡œ ì°¾ì•˜ìœ¼ë©°(Appendix ì°¸ê³ ) ê° ì•Œê³ ë¦¬ì¦˜ì˜ í¼í¬ë¨¼ìŠ¤ë¥¼ ì¸¡ì •í•  ë•Œ 3ë²ˆì˜ runì„ ëœë¤ ì‹œë“œë¥¼ ë°”ê¿”ê°€ë©° ì‹¤í–‰ì‹œí‚¨ í›„ í‰ê· ì„ êµ¬í•´ì„œ ê¸°ë¡
- ëŒ€ë¶€ë¶„ì˜ í™˜ê²½ì—ì„œ MLPê°€ ì˜ëê³  NerveNetë„ ì´ì™€ ë¹„ë“±í•œ í¼í¬ë¨¼ìŠ¤ë¥¼ ëƒˆë‹¤.

(3ê°€ì§€ ì¼€ì´ìŠ¤ì— ëŒ€í•œ learning curve, ë‹¤ë¥¸ ì¼€ì´ìŠ¤ë“¤ì—ì„œëŠ” ëŒ€ì²´ë¡œ NerveNetê³¼ MLPê°€ ë¹„ìŠ·í–ˆë‹¤.)

|`HalfCheetah`|`InvertedDoublePendulum`|`Swimmer`|
|-|-|-|
|![](https://i.imgur.com/0Y8Nw6H.png?1)|![](https://i.imgur.com/BrvwJpQ.png?1)|![](https://i.imgur.com/dmDNN1q.png?1)|
|MLPì™€ NerveNetì´ ë¹„ìŠ·í•˜ê³  TreeNetì´ ë§ì´ ì•ˆì¢‹ì•˜ìŒ|MLPê°€ ì¢€ë” ì¢‹ì€ ê²°ê³¼ë¥¼ ëƒ„|NerveNetì´ MLPë³´ë‹¤ ì¢‹ì€ ì„±ëŠ¥ì„ ëƒ„|

- ëŒ€ë¶€ë¶„ í™˜ê²½ë“¤ì—ì„œ `TreeNet`ì´ `NerveNet`ë³´ë‹¤ ì¢‹ì§€ ì•Šì•˜ê³  ì´ë¥¼ í†µí•´ì„œ ë¬¼ë¦¬ì ì¸ ê·¸ë˜í”„ êµ¬ì¡°ë¥¼ ê°€ì ¸ê°€ëŠ” ê²ƒì´ ì–¼ë§ˆë‚˜ ì¤‘ìš”í•œì§€ ì•Œ ìˆ˜ ìˆë‹¤.

## 2. Structure transfer learning

- MuJoCoì˜ í™˜ê²½ í•˜ë‚˜ë¥¼ ì»¤ìŠ¤í…€í•´ì„œ `size`ì™€ `disability`ì˜ ë³€í™”ê°€ ìˆì„ ë•Œ transferable í•¨ì„ ê²€ì¦
    - `size transfer` - ì‘ì€ ì‚¬ì´ì¦ˆì˜ ê·¸ë˜í”„ë¥¼ ê°€ì§„ agentë¥¼ í•™ìŠµ ì‹œí‚¨ í›„ ë” í° ì‚¬ì´ì¦ˆì˜ ê·¸ë˜í”„ë¥¼ ê°€ì§„ agentë¡œ transferable í•œì§€
    - `disability transfer` - ëª¨ë“  íŒŒíŠ¸ë“¤ì´ ì •ìƒì‘ë™í•˜ëŠ” agentë¡œ í•™ìŠµí•œ í›„ ì¼ë¶€ íŒŒíŠ¸ë“¤ì´ ì‘ë™í•˜ì§€ ì•ŠëŠ” ìƒí™©ì˜ agentë¡œ transferable í•œì§€
- 2ê°œ ì¢…ë¥˜ì˜ í™˜ê²½ì„ ì»¤ìŠ¤í…€í•˜ì—¬ ì‹¤í—˜ - `centipede`ì™€ `snake`
    1. centipede - ì§€ë„¤ì™€ ê°™ì´ ìƒê¸´ agentë¡œ torso bodyë“¤ì´ ì—¬ëŸ¬ê°œ ì²´ì¸ì²˜ëŸ¼ ì—°ê²° ë˜ì–´ ìˆê³  torsoë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì–‘ìª½ì— ë‹¤ë¦¬ê°€ 1ìŒìœ¼ë¡œ ë¶™ì–´ ìˆë‹¤. í•˜ë‚˜ì˜ ë‹¤ë¦¬ëŠ” thighì™€ shinìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆê³  hinge actuatorë¡œ êµ¬í˜„ë˜ì–´ ìˆë‹¤. ì»¤ìŠ¤í…€ì€ ë‹¤ë¦¬ì˜ ê°¯ìˆ˜ë¥¼ ë‹¤ì–‘í•˜ê²Œ í•´ì„œ ì—¬ëŸ¬ ì»¤ìŠ¤í…€ í™˜ê²½ë“¤ì„ ë§Œë“¤ì—ˆëŠ”ë°, ê°€ì¥ ì§§ì€ agentë¡œëŠ” `CentipedeFour` ë¶€í„° ê°€ì¥ ê¸´ agentë¡œëŠ” `CentipedeFourty` ë¡œ ë‹¤ë¦¬ê°€ 40ê°œê¹Œì§€(20ìŒ) ìˆëŠ” í™˜ê²½ì„ ë§Œë“¤ìˆ˜ ìˆì—ˆë‹¤. disabilityë¡œ ì¼ë¶€ íŒŒíŠ¸ê°€ ì‘ë™í•˜ì§€ ì•ŠëŠ” í™˜ê²½ì€ `Cp(Cripple)`ë¡œ ë”°ë¡œ í‘œê¸°í–ˆë‹¤. ì´ í™˜ê²½ì—ì„œ y-directionìœ¼ë¡œ ë¹¨ë¦¬ ì•ìœ¼ë¡œ ê°€ëŠ”ê²Œ ëª©í‘œë‹¤.
    2. snake - `swimmer` í™˜ê²½ì„ ê¸°ë°˜ìœ¼ë¡œ ì»¤ìŠ¤í…€í–ˆìœ¼ë©° ê°€ì¥ ë¹¨ë¦¬ ì§„í–‰ë°©í–¥ìœ¼ë¡œ ì›€ì§ì´ëŠ” ê²Œ ëª©í‘œë‹¤.
        
        ![](https://i.imgur.com/cKfht87.png){fig-align="default"}
        
    
- NerverNetê³¼ ë¹„êµêµ°
    - `NerveNet` : small agentê°€ í•™ìŠµí•œ ëª¨ë¸ì„ ë°”ë¡œ large agentì— ì ìš©í•  ìˆ˜ ìˆì—ˆë‹¤. agentì˜ êµ¬ì¡°ê°€ ë°˜ë³µì ì´ê¸° ë•Œë¬¸ì— ë°˜ë³µë˜ëŠ” ë¶€ë¶„ì„ ë” ëŠ˜ë¦¬ê¸°ë§Œ í•˜ë©´ ë˜ê¸° ë•Œë¬¸ì´ë‹¤.
    - `MLP Pre-trained (MLPP)`: agentì˜ í¬ê¸°ê°€ ì»¤ì§ì— ë”°ë¼ input sizeê°€ ë‹¬ë¼ì§€ë¯€ë¡œ ê°€ì¥ straightforwardí•˜ê²Œ ì²«ë²ˆì§¸ hidden layerë¥¼ ê·¸ëŒ€ë¡œ output layerë¡œ ì‚¬ìš©í•˜ê³  input layerì˜ ì‚¬ì´ì¦ˆë§Œ í‚¤ì›Œì„œ ì¶”ê°€í•˜ê³  ì´ input layerëŠ” ëœë¤ ì´ˆê¸°í™”ë¥¼ í•´ì¤€ë‹¤.
    - `MLP Activation Assigning (MLPAA)`: small agentì˜ weightë“¤ì„ ë°”ë¡œ large agentì˜ ëª¨ë¸ì— ë„£ì–´ì£¼ê³  weightë“¤ì˜ ë‚¨ëŠ” ë¶€ë¶„ë“¤ì„ 0ìœ¼ë¡œ ì´ˆê¸°í™” í•´ì¤€ë‹¤.
    - `TreeNet`: MLPAAì²˜ëŸ¼ ìŠ¤ì¼€ì¼ì„ í‚¤ì›Œì„œ 0ìœ¼ë¡œ ì´ˆê¸°í™” í•´ì¤€ë‹¤.
    - `Random` : action spaceì—ì„œ uniformlyí•˜ê²Œ ìƒ˜í”Œë§ì„ í•˜ëŠ” policyì´ë‹¤.
- ê²°ê³¼
    1. Centipede
        1. Pretraining
            - 6-ë‹¤ë¦¬ ëª¨ë¸ê³¼ 4-ë‹¤ë¦¬ ëª¨ë¸ë¡œ `NerveNet`, `MLP`, `TreeNet` ì—ì„œì˜ í¼í¬ë¨¼ìŠ¤ë¥¼ ë¹„êµí–ˆë‹¤. ì—¬ê¸°ì„œ 3ê°œì˜ ëª¨ë¸ì€ ì•ì„œ benchmark ë¹„êµ ì‹¤í—˜ì—ì„œ ì‚¬ìš©í•œ ë¹„êµêµ°ë“¤ê³¼ ë™ì¼í•˜ë‹¤.
                
                ![](https://i.imgur.com/M64FY71.png){fig-align="default"}
                
            - 4-ë‹¤ë¦¬ ëª¨ë¸ì—ì„œëŠ” NerveNetì´ ê°€ì¥ Rewardê°€ ë†’ê³ , 6-ë‹¤ë¦¬ ëª¨ë¸ì—ì„œëŠ” MLPê°€ ê°€ì¥ Rewardê°€ ë†’ìŒì„ ì•Œ ìˆ˜ ìˆë‹¤. TreeNetì€ ë‘ í™˜ê²½ ëª¨ë‘ì—ì„œ ê°€ì¥ ë‚®ë‹¤.
            - 6-ë‹¤ë¦¬ ëª¨ë¸ê³¼ 4-ë‹¤ë¦¬ ëª¨ë¸ë¡œ pretrainingì„ ì§„í–‰í•œ í›„ transferableì„ ì‹¤í—˜í–ˆë‹¤.
        2. Zero-shot
            - fine tuning ì—†ì´ í¼í¬ë¨¼ìŠ¤ë¥¼ ì¸¡ì •í–ˆë‹¤.
            - í¼í¬ë¨¼ìŠ¤ë¥¼ ì‰½ê²Œ ë¹„êµí•  ìˆ˜ ìˆë„ë¡ `average reward`ì™€ `average running-length`ë¥¼ normalizationí•´ì„œ ìƒ‰ìœ¼ë¡œ ì•„ë˜ì™€ ê°™ì´ í‘œí˜„í–ˆë‹¤.(green-good, red-bad)
                
                ![](https://i.imgur.com/qAbGhpb.png){fig-align="default"}
                
            - ëˆˆìœ¼ë¡œ í™•ì‹¤íˆ í™•ì¸í•  ìˆ˜ ìˆë“¯ì´ NerveNetì˜ í¼í¬ë¨¼ìŠ¤ê°€ ë‹¤ë¥¸ ë¹„êµêµ°ì— ë¹„í•´ ì›”ë“±íˆ transferableí•¨ì„ ì•Œ ìˆ˜ ìˆì—ˆë‹¤.
            - ë˜í•œ learning curveì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´ NerveNet+Pretrain ì´ ë‹¤ë¥¸ Pretrain ë¹„êµêµ°ë“¤ì— ë¹„í•´ í›¨ì”¬ ë†’ì€ reward ì‹œì‘ì ì—ì„œ ì‹œì‘í•˜ê³  ë” ì ì€ timestepìœ¼ë¡œ solved ì ìˆ˜ì— ë„ë‹¬í•˜ëŠ” ê²ƒì„ ë³´ì•„ ê·¸ë˜í”„ì˜ êµ¬ì¡°ì  ì´ì ì„ í™•ì‹¤íˆ í™œìš©í•˜ê³  ìˆìŒì„ ì•Œ ìˆ˜ ìˆë‹¤.
                
                ![](https://i.imgur.com/yU47fAC.png?1){fig-align="default"}
                
            - NerveNetì˜ agentë“¤ì€ ë‹¤ë¥¸ ë¹„êµêµ° agentë“¤ì—ì„œ ë³´ì´ì§€ ì•ŠëŠ” `walk-cycle`ì„ ê°€ì§€ê³  ìˆìŒì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆëŠ”ë°, ì´ëŠ” ë³´í–‰ ë¡œë´‡ë“¤ì€ ê±¸ìŒìƒˆì—ì„œ ë°˜ë³µì ì¸ ì›€ì§ì„ì„ í•˜ê²Œ ë˜ì–´ ìˆê¸° ë•Œë¬¸ì— ìì—°ìŠ¤ëŸ½ê²Œ cycleì„ ê°€ì§€ê²Œ ë˜ëŠ” ê²ƒì„ agentê°€ í•™ìŠµí–ˆìŒì„ ì•Œ ìˆ˜ ìˆë‹¤. (ë°˜ë©´ MLPëŠ” 8-ë‹¤ë¦¬ ëª¨ë¸ì—ì„œ ëª¨ë“  ë‹¤ë¦¬ë¥¼ ì›€ì§ì´ì§€ ì•ŠëŠ” ëª¨ìŠµì„ ë³´ì´ê¸°ë„ í–ˆë‹¤.)
    2. Snake
        - snakeí™˜ê²½ì—ì„œë„ NerveNetì´ ë‹¤ë¥¸ ë¹„êµêµ°ë“¤ì— ë¹„í•´ ë›°ì–´ë‚œ reward ì ìˆ˜ë¥¼ ë³´ì—¬ì£¼ë©° transferable í•¨ì„ ì•„ë˜ì˜ ë„í‘œì—ì„œì²˜ëŸ¼ ë³´ì—¬ì£¼ì—ˆë‹¤.
        - 350ì  ì •ë„ê°€ `snakeThree`ì—ì„œ solvedëœ ìƒíƒœë¼ê³  ë³¼ ìˆ˜ ìˆëŠ”ë° NerveNetì˜ ì‹œì‘ ì ìˆ˜ë“¤ì´ ëŒ€ë¶€ë¶„ 300ì ëŒ€ì—ì„œ ì‹œì‘í•œ ê²ƒìœ¼ë¡œ ë³´ì•„ ì´ëŠ” ìƒë‹¹í•œ zero-shot ì—­ëŸ‰ì´ ìˆìŒì„ ì•Œ ìˆ˜ ìˆë‹¤.
        - ë‹¤ë¥¸ ë¹„êµêµ°ë“¤ì€ overfittingì´ ì‹¬í•´ì„œ Randomë³´ë‹¤ ì•ˆì¢‹ì€ ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ëŠ” ì ë„ í¥ë¯¸ë¡­ë‹¤.
        
        ![](https://i.imgur.com/QvZG8di.png){fig-align="default"}
        
        - zero-shot ë¿ë§Œ ì•„ë‹ˆë¼ fine tuningì„ í•˜ëŠ” learning curveì—ì„œë„ NerveNetì€ Pretrainì˜ ì´ì ì„ ë‹¤ë¥¸ ë¹„êµêµ°ë“¤ì— ë¹„í•´ ì˜ í™œìš©í•˜ê³  ìˆìŒì„ ë³¼ ìˆ˜ ìˆì—ˆë‹¤. `NerveNet+Pretrain`ì˜ ì‹œì‘ rewardê°€ ë†’ìœ¼ë©°, íŠ¹ì • size transfer ì‹¤í—˜ì—ì„œëŠ” scratch NerveNetì´ ë„˜ì§€ ëª»í•œ MLP ì ìˆ˜ë¥¼ `NerveNet+Pretrain`ì´ ë”°ë¼ì¡ì•˜ë‹¤.
            
            ![](https://i.imgur.com/NLlKmqT.png?1){fig-align="default"}
            

## 3. Multi-task learning

NerveNetì€ ë„¤íŠ¸ì›Œí¬ì— structure priorë¥¼ í¬í•¨í•œ ê²ƒì´ê¸° ë•Œë¬¸ì— multi-task learningì— ìœ ë¦¬í•  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ ì´ë¥¼ ì‹¤í—˜í•˜ê¸° ìœ„í•´ `Walker` multi-task learningì„ ì§„í–‰í–ˆë‹¤.

- 2d-walker í™˜ê²½ë“¤ 5ê°œ - `Walker-HalfHumanoid`, `Walker-Hopper`, `Walker-Horse`, `Walker-Ostrich, Walker-Wolf`
- 1ê°œì˜ í†µí•©ëœ networkë¡œ í•™ìŠµ
- NerveNetê³¼ ë¹„êµêµ°
    - `NerveNet` : agentë“¤ì˜ í˜•íƒœê°€ ë‹¬ë¼ weightë“¤ì´ ë‹¤ë¥¼ ìˆ˜ ë°–ì— ì—†ê¸° ë•Œë¬¸ì— propagationê³¼ì •ì—ì„œì˜ weight matricesì™€ outputë§Œ ê³µìœ í–ˆë‹¤.
    - `MLP Sharing` : hidden layerë“¤ ê°„ì˜ weight matrices ë¥¼ ê³µìœ 
    - `MLP Aggregation` : ì°¨ì›ì´ ë‹¤ë¥¸ observationë“¤ì„ aggregationê³¼ì •ì„ í†µí•´ ì²«ë²ˆì§¸ hidden layerì˜ í¬ê¸°ë¡œ ë‹¤ ë§ì¶°ì£¼ì–´ì„œ inputìœ¼ë¡œ ë„£ì–´ì¤Œ
    - `TreeNet`: TreeNetë„ weightë¥¼ ê³µìœ ë¥¼ í•  ìˆ˜ ìˆì§€ë§Œ agentì˜ êµ¬ì¡°ì ì¸ ì •ë³´ëŠ” ì•Œ ìˆ˜ ì—†ë‹¤. ë‹¨ìˆœíˆ root nodeë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ëª¨ë“  ë…¸ë“œì˜ ì •ë³´ë‹¤ aggregation ë˜ê¸° ë•Œë¬¸ì´ë‹¤.
    - `MLPs`: ê° agentë§ˆë‹¤ ë”°ë¡œ MLP policyë¥¼ ë§Œë“¤ì–´ì„œ í•™ìŠµ(single-task)
- ê²°ê³¼
    - multi-task learning ì‹¤í—˜ì´ê¸° ë•Œë¬¸ì— í•œ ë‘ê°œ ëŸ¬ë‹ ê·¸ë˜í”„ë§Œ ë³¼ ìˆ˜ ì—†ê³  5ê°œì˜ ëŸ¬ë‹ ê·¸ë˜í”„ë¥¼ ê°™ì´ ë´ì•¼ í•œë‹¤.
    - Single-task policyë¥¼ ì œì™¸í•˜ê³  ëª¨ë“  í™˜ê²½ì—ì„œ NerveNetì˜ í¼í¬ë¨¼ìŠ¤ê°€ ì¢‹ìŒì„ ì•Œ ìˆ˜ ìˆë‹¤.
        
        ![](https://i.imgur.com/FpUeo5x.png){fig-align="default"}
        
    - í…Œì´ë¸”ì—ì„œ Ratioê°€ single-task policyì— ë¹„í•´ multi-task policyì˜ ì„±ëŠ¥ì„ percentageë¡œ ë‚˜íƒ€ë‚¸ ìˆ˜ì¹˜ì¸ë°, MLPì˜ í¼í¬ë¨¼ìŠ¤ê°€ single-taskì—ì„œ multi-taskë¡œ ë„˜ì–´ê°”ì„ ë•Œ 42%ë‚˜ í¼í¬ë¨¼ìŠ¤ê°€ ì¤„ì–´ë“œëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. (Average-58.6%) ë°˜ë©´ì— NerveNetì€ ì„±ëŠ¥ì´ ì „í˜€ ë–¨ì–´ì§€ì§€ ì•ŠëŠ” ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ì—ˆë‹¤.
        
        ![](https://i.imgur.com/HMSZPgc.png?1){fig-align="default"}
        

## 4. Robustness of learnt policies

ê°•í™”í•™ìŠµ ì œì–´ì—ì„œ robustnessëŠ” ì¤‘ìš”í•œ ì§€í‘œì¸ë° ì§ˆëŸ‰ì´ë‚˜ í˜ê³¼ ê°™ì€ ë¬¼ë¦¬ì ì¸ ê°’ë“¤ì˜ ì˜¤ì°¨ ë²”ìœ„ê°€ ì–´ëŠì •ë„ê¹Œì§€ policyê°€ í—ˆìš©í•˜ê³  ì˜ ì‘ë™í•˜ëŠ”ì§€ë¥¼ í™•ì¸í•´ì•¼ í•œë‹¤. 

- 5ê°œì˜ Walker ê·¸ë£¹ì˜ í™˜ê²½ì—ì„œ ì‹¤í—˜
- pretrained agentë¥¼ ê°€ì§€ê³  agentì˜ ì§ˆëŸ‰ê³¼ jointì˜ strengthì„ ë³€ê²½í•œ ë’¤ í¼í¬ë¨¼ìŠ¤ ì¸¡ì •
- ëŒ€ë¶€ë¶„ì˜ í™˜ê²½ê³¼ variationì—ì„œ NerveNetì˜ robustnessê°€ MLPë³´ë‹¤ ì¢‹ìŒì„ ì•Œ ìˆ˜ ìˆë‹¤.

![](https://i.imgur.com/9Rcl54E.png?1){fig-align="default"}

## 5. Interpreting the learned representations

ì‹¤ì œ í´ë¦¬ì‹œë“¤ì´ ì–´ë–¤ representationë“¤ì„ í•™ìŠµí–ˆëŠ”ì§€ ì•Œì•„ë³´ê¸° ìœ„í•´ `CentipedeEight` í™˜ê²½ì—ì„œ í•™ìŠµëœ agentì˜ final state vectorë¥¼ ê°€ì§€ê³  2D, 1D PCAë¥¼ ì§„í–‰í–ˆë‹¤.

ê° ë‹¤ë¦¬ìŒë“¤(Left Hip-Right Hip)ë“¤ì€ agentì˜ ì „ì²´ ëª¸ì²´ì—ì„œ ê°ê¸° ë‹¤ë¥¸ ìœ„ì¹˜ì— ìˆìŒì—ë„ ë¶ˆêµ¬í•˜ê³  invariant representationì„ ë°°ìš¸ ìˆ˜ ìˆì—ˆìŒì„ PCAë¥¼ í†µí•´ì„œ ì•Œ ìˆ˜ ìˆì—ˆë‹¤.

![](https://i.imgur.com/osZ7M6U.png?1){fig-align="default"}

ë˜í•œ ì•ì„œ Centipede transfer learning ì‹¤í—˜ ê²°ê³¼ì—ì„œë„ ì ê¹ ì–¸ê¸‰í–ˆë˜ `walk-cycle`ì´ ì£¼ê¸°ì„±ì´ ëšœë ·í•˜ê²Œ ë³´ì˜€ë‹¤.

![](https://i.imgur.com/Aw80MM1.png){fig-align="default"}

## 6. Comparison of model variants

Value Networkë¥¼ ì–´ë–»ê²Œ í•  ê²ƒì¸ì§€ì— ë”°ë¼ NerveNetì˜ ì—¬ëŸ¬ ë³€í˜•ì´ ìˆì„ ìˆ˜ ìˆëŠ”ë° `Swimmer`, `Reacher`, `HalfCheetah`ì—ì„œ ë¹„êµí•´ë³¸ ê²°ê³¼, Value NetworkëŠ” MLPë¡œ í•œ `NerveNet-MLP`ì˜ í¼í¬ë¨¼ìŠ¤ê°€ ê°€ì¥ ì¢‹ì•˜ê³  `NerveNet-1`ì˜ í¼í¬ë¨¼ìŠ¤ê°€ 2ë“±ìœ¼ë¡œ `NerveNet-MLP` ì™€ ë¹„ìŠ·í–ˆë‹¤. ì´ì— ëŒ€í•œ ì ì¬ì ì¸ ì´ìœ ë¡œ value networkì™€ policy networkê°€ weightë¥¼ ê³µìœ í•˜ëŠ” ê²ƒì´ PPO ì•Œê³ ë¦¬ì¦˜ì—ì„œì˜ trust-region based optimitaionì—ì„œì˜ weight $\alpha$ë¥¼ ë” sensitiveí•˜ê²Œ ë§Œë“¤ê¸° ë•Œë¬¸ì´ë¼ê³  ì¶”ë¡ í•  ìˆ˜ ìˆë‹¤.

![](https://i.imgur.com/W79cOCW.png){fig-align="default"}

# Conclusion

ë³¸ ë…¼ë¬¸ì„ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

- NerveNetì´ë¼ëŠ” ê·¸ë˜í”„ êµ¬ì¡°ë¥¼ í™œìš©í•œ policyë¥¼ ê°€ì§€ê³  RL agentì˜ body structureë¥¼ í™œìš©í•  ìˆ˜ ìˆëŠ” ì•Œê³ ë¦¬ì¦˜ì„ ì œì•ˆ
    - ê° bodyì™€ jointì˜ observationì„ ë°›ì•„ GNNì„ í†µí•´ non-linear messageë“¤ì„ ê³„ì‚°í•˜ê³  propagationí•˜ëŠ” ëª¨ë¸ë§
    - propagationì€ ì—£ì§€ë¡œ í‘œí˜„ëœ jointê°„ì˜ ë¬¼ë¦¬ì ìœ¼ë¡œ ì—°ê²°ì„±ì„ ê°€ì§€ê³  ë³¸ë˜ ìˆëŠ” ì˜ì¡´ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ ì´ë£¨ì–´ì§
- ì‹¤í—˜ì ìœ¼ë¡œ NerveNetì´ MoJoCo ì‹œë®¬ë ˆì´í„° ê¸°ë°˜ ì—¬ëŸ¬ í™˜ê²½ë“¤ì—ì„œ MLP ê¸°ë°˜ SOTA ì•Œê³ ë¦¬ì¦˜ë“¤ê³¼ ê²¬ì¤„ë§Œí•œ í¼í¬ë¨¼ìŠ¤ë¥¼ ë³´ì—¬ì¤Œo state-of-the-art methods on standard MuJoCo environments.
- ëª‡ê°€ì§€ í™˜ê²½ì„ ì»¤ìŠ¤í…€í•´ì„œ sizeì™€ disability transferë¥¼ ê²€ì¦í–ˆìœ¼ë©° zero-shot settingì—ì„œë„ transferableí•¨ì„ ë³´ì„

---

# Review

ë…¼ë¬¸ ë¦¬ë·°í›„ì˜ ì£¼ê´€ì ì¸ ì¥ë‹¨ì ì„ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

- Pros **ğŸ‘**
    - ë¡œë´‡ì˜ êµ¬ì¡°ì ì¸ íŠ¹ì§•ì„ ê¸°ë°˜ìœ¼ë¡œ íš¨ìœ¨ì ì¸ feature embeddingì´ì—ˆìŒì„ ë³´ì—¬ì¤Œ
    - ë‹¤ì–‘í•œ robot configurationì—ì„œë„ ì˜ ì‘ë™í•¨
    - ëª¨ë¸ì˜ í™•ì¥ì„±ì„ ì„¤ëª…í•  ìˆ˜ ìˆëŠ” Transfer learningê³¼ Multi-task learningì´ ì¸ìƒì ì´ì—ˆê³  í° ì¥ì ì´ë¼ê³  ìƒê°

- Cons **ğŸ‘**
    - ì‹œë®¬ë ˆì´ì…˜ì—ì„œë§Œ ì‹¤í—˜í–ˆë‹¤ëŠ” ì ì´ ì•„ì‰¬ì›€
    - ìƒê°ë³´ë‹¤ ê¸°ë³¸ì ì¸ gnnëª¨ë¸ì´ë¼ì„œ edgeì— ëŒ€í•œ í° ë””ìì¸ ìš”ì†Œê°€ ë“¤ì–´ê°€ì§€ ì•Šì€ ê²ƒ ê°™ìŒ
    - ë‹¤ì–‘í•œ RL ì•Œê³ ë¦¬ì¦˜ë“¤ê³¼ì˜ ì‹œë„ˆì§€ë¥¼ ë³´ê¸°ì—ëŠ” ì†”ì§íˆ ë…¼ë¬¸ì˜ ì–‘ì´ ë„ˆë¬´ ë°©ëŒ€í•´ì§ˆ ê²ƒ ê°™ê¸´í•˜ì§€ë§Œ ì´ì— ëŒ€í•œ ë¹„êµê°€ ìˆì—ˆìœ¼ë©´ ì¢‹ì•˜ì„ ê²ƒ ê°™ìŒ

# Reference
- Original Project Homepage: [http://www.cs.toronto.edu/~tingwuwang/nervenet.html](http://www.cs.toronto.edu/~tingwuwang/nervenet.html )
- Code
    - Official: [https://github.com/WilsonWangTHU/NerveNet](https://github.com/WilsonWangTHU/NerveNet)
    - Not official: [https://github.com/HannesStark/gnn-reinforcement-learning](https://github.com/HannesStark/gnn-reinforcement-learning)