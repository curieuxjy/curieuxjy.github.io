---
title: "📃Dextreme 리뷰"
description: Transfer of Agile In-hand Manipulation from Simulation to Reality 
date: "2025-04-18"
categories: [paper, in-hand, vision]
toc: true
number-sections: true
---

> CoRL 2021

- [프로젝트 홈페이지](https://dextreme.org/)
- [Code](https://github.com/isaac-sim/IsaacGymEnvs/tree/main/isaacgymenvs/tasks/dextreme)
- [Paper](https://arxiv.org/pdf/2210.13702)

---

1. 🤖 DeXtreme은 심층 강화 학습을 통해 인간형 로봇 손의 정교한 조작 정책을 훈련하고, 물체의 상태에 대한 신뢰성 있는 실시간 정보를 제공하는 강력한 자세 추정기를 개발했습니다.
2. 💪 시뮬레이션에서 다양한 조건에 적응하도록 훈련된 정책은 비전 기반 정책에서 기존 최고 성능을 능가하며 모션 캡처 시스템을 통해 특권적인 상태 정보를 제공받은 정책과 경쟁력을 갖습니다.
3. 💰 Allegro Hand 및 Isaac Gym GPU 기반 시뮬레이션을 통해 접근 가능한 로봇 손과 카메라로 유사한 결과를 얻을 수 있는 가능성을 제시하여 민첩한 조작 개발 및 배포 과정을 단순화했습니다.


# Brief Review

이 논문은 다지 로봇 손으로 민첩한 조작을 학습하고 시뮬레이션에서 현실로 이전하는 기술인 DeXtreme을 제시합니다. 핵심은 심층 강화 학습(RL)을 사용하여 로봇 손의 견고한 조작 정책을 훈련하고, 조작 대상 객체의 상태에 대한 신뢰할 수 있는 실시간 정보를 제공하는 강력한 자세 추정기를 개발하는 것입니다.

**핵심 방법론:**

1.  **과제 설정:** 인간형 로봇 손으로 물체 방향을 바꾸는 과제를 설정합니다. 손바닥 위에 놓인 물체를 원하는 목표 방향으로 회전시키는 것이 목표입니다. 목표 방향에 도달하면 새로운 목표 방향이 주어지고, 물체를 떨어뜨리거나 정지하지 않고 연속적으로 목표를 달성하는 횟수를 측정합니다.

2.  **하드웨어 구성:** Allegro Hand (손목 고정)와 3대의 Intel D415 RGB 카메라를 사용합니다. 카메라는 손바닥을 기준으로 외부 캘리브레이션됩니다. 마커 기반 시스템 대신 비전 기반 시스템을 사용하여 물체 자세를 추정합니다.

3.  **정책 학습:**
    *   **RL 공식화:** 물체 조작 과제를 부분적으로 관찰 가능한 마르코프 결정 프로세스(POMDP)로 모델링합니다. Proximal Policy Optimization (PPO) 알고리즘을 사용하여 관측값에서 행동으로의 매핑을 학습하는 확률적 정책 $\pi_\theta$ (actor)를 학습합니다. 또한 가치 함수 $V^{\pi}_\phi (s, o)$ (critic)를 학습하여 정책 가치를 추정합니다.
    *   **Actor와 Critic:** Critic은 Actor와 다른 관측값을 사용하며, 상태 $s \in S$에 대한 추가 정보를 받습니다. Actor의 입력은 물체 위치, 방향, 목표 위치, 방향, 상대적 목표 방향, 이전 행동, 손 관절 각도, 확률적 지연 등으로 구성됩니다 (총 50차원). Critic은 Actor의 입력 외에도 핑거팁 위치/회전/속도/힘, 손 관절 속도/힘, 물체 속성, 중력 벡터 등 추가 상태 정보를 활용합니다 (총 265차원).
    *   **신경망 구조:** 정책은 LSTM(Long Short-Term Memory) 네트워크를 사용하며, 이전 hidden state $h \in H$와 환경 관측값 $o$를 입력으로 받습니다. LSTM은 1024개의 hidden unit과 layer normalization을 사용하고, 그 뒤에 512 크기의 MLP(Multilayer Perceptron) 레이어 2개가 ELU 활성화 함수와 함께 연결됩니다. 액션 공간 $A$는 로봇 손의 16개 관절에 대한 PD 컨트롤러 목표입니다. 가치 함수 LSTM 레이어는 2048개의 hidden unit을 갖고 layer normalization을 사용하며, 1024 및 512 유닛의 MLP 레이어 2개와 ELU 활성화 함수가 뒤따릅니다. 정책의 출력은 지수 이동 평균(EMA) 평활화 필터를 통과합니다.

4.  **보상 함수:** Isaac Gym의 Shadow Hand 환경에서 영감을 받아 설계된 보상 함수를 사용합니다. 목표에 가까운 회전, 고정 목표 위치에 가까운 위치, 액션 페널티, 액션 변화 페널티, 관절 속도 페널티 등을 고려합니다.

5.  **시뮬레이션 환경:** GPU 기반 Isaac Gym 시뮬레이터를 사용하여 수천 개의 로봇을 병렬로 시뮬레이션합니다.

6.  **도메인 랜덤화:** 시뮬레이션과 실제 사이의 간극을 줄이기 위해 다양한 도메인 랜덤화 기법을 적용합니다.
    *   **Vectorized Automatic Domain Randomization (VADR):** 도메인 랜덤화 파라미터를 자동으로 조정하여 정책 성능을 유지하면서 랜덤화 범위를 최대화합니다. 각 파라미터에 대해 별도의 스텝 크기 $\Delta_n$을 설정하고, 환경의 40%를 평가에 사용합니다. 평가 환경에서는 ADR 랜덤화 차원 중 하나를 현재 상한 또는 하한 경계로 고정합니다. 에피소드 종료 시 연속 성공 횟수를 기록하고, 평균 연속 성공 횟수가 임계값을 초과하면 범위를 넓히고, 낮으면 범위를 좁힙니다. 여러 GPU에서 훈련할 때 각 GPU에서 VADR을 독립적으로 실행합니다.
    *   **물리 랜덤화:** 질량, 마찰, 반발 계수 등 기본 속성을 랜덤화합니다. 손과 물체의 크기를 랜덤하게 조정하고, 관절 강성, 감쇠, 한계를 랜덤화합니다. 또한 물체에 임의의 힘을 가합니다.
    *   **비물리 랜덤화:** 액션 및 관측에 확률적 지연, 액션 지연 시간, 상관/비상관 가우시안 노이즈를 추가합니다. 또한 RNA(Random Network Adversary)를 사용하여 구조화된 노이즈 패턴을 도입합니다.
    *   **Random Network Adversary (RNA):** 각 에피소드마다 무작위로 생성된 신경망을 사용하여 환경에 더 구조화된 상태 가변 노이즈 패턴을 도입합니다. RNA 네트워크의 액션은 $a = \alpha \cdot a_{RNA} + (1-\alpha) \cdot a_{policy}$의 공식으로 정책으로부터의 액션과 혼합됩니다. $\alpha$는 ADR에 의해 제어됩니다.

7.  **자세 추정:** NVIDIA Omniverse Isaac Sim과 Replicator를 사용하여 5백만 장의 이미지를 생성합니다. torchvision Mask-RCNN에서 영감을 얻은 네트워크를 사용하여 bounding box, segmentation, 큐브 모서리의 keypoint를 예측합니다. 세 대의 카메라에서 20Hz의 추론 속도로 네트워크를 실행하고, 각 카메라에서 PnP 알고리즘을 적용하여 자세를 추정한 다음, 필터링된 카메라에서 키포인트를 삼각 측량하여 최종 자세를 얻습니다.

**주요 결과:**

*   ADR을 사용하여 훈련된 정책이 수동으로 조정된 정책보다 성능이 우수합니다.

*   비전 기반 자세 추정을 사용한 정책이 이전 연구의 비전 기반 정책보다 성능이 향상되었고, 모션 캡처 시스템을 사용한 정책과도 경쟁력이 있습니다.

*   저렴한 하드웨어 (Allegro Hand)와 간단한 비전 시스템으로도 민첩한 조작이 가능하다는 것을 입증합니다.

**핵심 기여:**

*   저렴한 하드웨어를 사용하고 순수하게 비전 기반 파이프라인을 사용하는 학습 기반 민첩한 조작 시스템을 시연합니다.

*   다양한 로봇 설정에서 작동하는 강력한 자세 추정기를 개발합니다.

*   순수 비전 기반 상태 추정 결과가 기존 비전 기반 결과보다 성능이 우수하며 마커 기반 결과와 비슷합니다.

이 연구는 로봇 연구 커뮤니티가 최첨단 핸드 내 조작 시스템에 접근할 수 있도록 지원하고 추가 연구 및 발전을 촉진하는 것을 목표로 합니다.


# Detail Review

> DeXtreme: 시뮬레이션에서 현실로의 민첩한 손 안 조작 전이 – 논문 리뷰

## 배경: 손 안 조작과 Sim-to-Real의 도전과제
일반적인 로봇 그리퍼(집게)는 두 개의 손가락으로 물체를 잡는 방식이어서 한 번 쥔 물체의 **자세**(orientation)를 크게 바꾸기 어렵습니다. 
반면 **다지(多指) 로봇 손**은 인간 손처럼 복잡한 움직임으로 물체를 **손 안에서 조작(in-hand manipulation)**할 수 있어, 예를 들어 손바닥 위에서 큐브의 자세를 자유롭게 바꾸는 등 **고난도 조작**을 가능하게 합니다. 
그러나 이러한 **고자유도** 로봇 손의 제어는 난이도가 높고, 특히 시뮬레이션으로 학습한 제어 정책을 현실 로봇에 옮길 때 **시뮬레이션-현실 간 격차(sim-to-real gap)**로 인해 많은 문제가 발생합니다. 손가락과 물체 사이의 미세한 마찰, 접촉(dynamic contact) 등 물리 상호작용을 정확히 모사하기 어렵기 때문입니다. 

2018년 OpenAI 연구진은 **심층 강화학습**만으로 다지 로봇 손의 복잡한 손 안 조작을 처음 성공적으로 시현하여 큰 주목을 받았습니다. 
이들은 Shadow Hand라는 고가의 로봇 손과 모션 캡처 기반의 정밀 추적 시스템을 사용하고, 수백 대의 CPU 서버와 수십 대의 GPU를 동원한 복잡한 학습 구조를 도입했습니다. 
그 결과 Rubik's Cube와 같은 난제 해결이 가능해졌지만, **특수한 하드웨어와 막대한 연산 자원**에 의존했기에 **재현성과 확장성** 측면에서 한계가 있었습니다. 
이후 연구들에서도 강화학습을 통한 손 안 조작 성공 사례가 보고되었지만 대부분 **시뮬레이션 내의 성과**에 그쳤습니다.

**DeXtreme**은 이러한 맥락에서 등장한 연구로, **비용을 획기적으로 낮춘 장비**와 **효율적인 학습 기법**을 통해 **현실 세계에서 민첩한 손 안 조작**을 구현한 사례입니다. 이 연구에서는 약 10배 저렴한 Allegro Hand(4 finger, 관절 수 16)를 사용하고 손목을 고정했으며, 물체 추적을 위해 **전용 마커나 모캡 없이 3대의 범용 RGB 카메라**만 활용했습니다. 
또한 시뮬레이터로 **NVIDIA Isaac Gym**(GPU 가속 물리 시뮬레이션)을 사용하여 학습 인프라를 크게 간소화했습니다. 
그 결과 단 8개의 GPU로 학습을 완료할 수 있었으며 (OpenAI의 이전 작업은 400여 대의 CPU 서버와 32개의 GPU를 사용, 최종 정책은 **모션 캡처 없이도** OpenAI의 모캡 기반 성능에 필적하는 수준을 달성했습니다. 
요약하면, **DeXtreme**은 *“저렴한 로봇 손+비전 기반 상태 추정+대규모 병렬 시뮬레이션”* 조합으로 **손 안 조작의 sim-to-real** 가능성을 재확인하고, 관련 연구의 **진입장벽을 낮춘** 의미 있는 성과입니다.

## DeXtreme의 접근법: Privileged RL, Teacher-Student 구조, ADR 등

**DeXtreme**에서는 현실 로봇(hand + 카메라)으로 옮겨도 동작하는 **강화학습 정책**을 만들기 위해, 시뮬레이션 상에서 다음과 같은 기법들을 조합해 학습을 진행했습니다:

- **Privileged Learning과 비대칭 정책 학습**: 현실에서는 물체의 정확한 상태(예: 큐브의 3차원 자세)를 직접 알 수 없지만 시뮬레이션에선 쉽게 얻을 수 있습니다. 이를 활용해 학습 시 **정책(policy)**과 **가치함수(critic)**에 서로 다른 정보를 제공했습니다. 정책 네트워크(실제 로봇에 배치될 **학생(student)** 역할)는 **실제로 관측 가능한 정보**만을 입력으로 받도록 했습니다. 구체적으로 **손 관절 각도**, 목표 자세와 현재 추정된 큐브 자세(약간의 노이즈 추가) 등 **부분 관측 상태**만으로 행동을 결정하게 합니다. 반면 **가치함수 신경망(**교사** 역할)**에는 시뮬레이터가 제공하는 **privileged 정보**까지 추가로 입력했습니다. 예를 들어 물체의 실제 위치/자세, 물리 파라미터(마찰계수 등)와 임의로 가해진 외력 등의 정보를 **평가자**만 알고 있도록 함으로써, 학습 과정에서 **정책 평가**의 정확성을 높이고 안정적인 학습이 가능하게 했습니다. 이러한 **비대칭(actor-critic) 학습** 구조는 Pinto 등 선행연구의 방식을 따른 것으로, 현실에서 관측 불가능한 정보를 활용해 **교사가 학생 정책을 효과적으로 지도**하는 형태로 볼 수 있습니다. 또한 정책 네트워크는 **LSTM 기반의 순환신경망(recurrent policy)**으로 구성하여, 과거의 관측 정보를 내재화하고 **부분 관측** 문제를 완화했습니다. LSTM (은닉 차원 1024)을 사용함으로써 손 움직임의 **연속성**과 **시간 지연**에 대한 대응 능력을 갖춘 정책을 학습했습니다.

- **Domain Randomization (도메인 랜덤화)**과 **커리큘럼 학습**: 시뮬레이션-현실 격차를 극복하는 대표 기법으로 **환경 무작위화**가 사용되었습니다. **물리 파라미터 무작위화**로는 물체와 손의 마찰계수, 질량, 관성, 탄성계수 등을 임의로 변화시켰고, 손가락 관절 마다 마찰/마찰감쇠 계수나 구동 힘 한계 등을 랜덤하게 스케일 조정했습니다. 또한 매 timestep 임의의 작은 외력을 물체에 가해 **예기치 않은 충격**에도 견디도록 학습시켰습니다. 심지어 중력 가속도 값도 에피소드마다 바꾸어 가며 다양한 중력 환경을 경험시켰습니다. **비(非)물리적 랜덤화**로는 센서 지연이나 잡음 같은 현실적 요소를 추가했습니다. 예를 들어, **관측 지연**으로서 가상의 카메라 **프레임레이트 지연**을 도입하여, 정책이 매 시간 물체 상태를 업데이트 받지 못하고 낮은 주기의 정보에도 견인하도록 만들었습니다. 실제 시스템에서 제어 명령이 로봇에 전달될 때 통신 지연이 있으므로, **액션 지연** 또한 무작위로 발생시키고, 오래된 명령이 실행되는 상황을 섞었습니다. 이와 함께 각 관측값과 행동에 **가우시안 잡음**을 상관된 형태로 추가하여 (일부 환경은 노이즈 0으로 두기도 함) 센서/액츄에이터 노이즈에 대한 **내성**을 키웠습니다. 

- **Random Network Adversary**: 일반적인 가우시안 노이즈로는 한계가 있는 **구조적 교란**에 대응하기 위해, 무작위로 생성된 신경망을 **적대적 동작 신호**로 사용했습니다. 매 에피소드마다 임의의 파라미터로 초기화된 작은 신경망이 **가짜 행동** $a_{RNA}$을 생성하고, 이를 현재 정책의 행동 $a_{\pi}$와 일정 비율로 섞어서 최종 시뮬레이터에 적용할 행동 $a_{sim}$으로 삼았습니다. 이 비율 $\alpha$ 역시 학습 진행에 따라 조절되는데, 학습 초기에 $\alpha$를 높여 정책에 **예측 불가능한 교란**을 많이 섞어주고, 정책이 향상되면 $\alpha$를 낮추는 식입니다. 이렇게 하면 단순 랜덤 노이즈보다 **상황 의존적**인 교란을 정책이 견디도록 만들 수 있습니다 (OpenAI의 Rubik's Cube 학습에도 사용된 기법). 아래 그림은 이러한 **랜덤 신경망 적대기법**의 개념도입니다.

 **Figure 1:** 무작위 신경망 적대기법(Random Network Adversary)의 동작. 에피소드마다 랜덤 가중치로 생성된 신경망이 상태 $s_t$를 입력으로 **적대적 행동** $a_{RNA}$를 생성하고(왼쪽), 이를 현재 정책의 행동 $a_{\pi}$와 합쳐 최종 행동 $a_{sim}$로 적용한다(오른쪽). $\alpha$는 적대 행동의 반영 비율로, 학습 중 ADR 알고리즘에 의해 점진 조정된다.

- **Automatic Domain Randomization (ADR)**: 초기 학습 단계부터 너무 과도한 무작위화를 주면 정책이 **기본기 습득**도 못한 채 학습이 실패할 수 있습니다. DeXtreme은 이를 피하기 위해, OpenAI에서 제안된 **자동 도메인 랜덤화(ADR)**를 활용했습니다. **ADR 알고리즘**은 정책의 성능을 모니터링하며 **랜덤화 난이도를 자동 조절**합니다. 구체적으로, 각 랜덤화 변인의 범위(분포 폭)를 정책이 **어느 정도 견딜 수 있는지**를 평가하여, 성능이 좋으면 범위를 넓히고 성능이 떨어지면 범위를 줄이는 **적응형 커리큘럼**입니다. 이 작업을 가속하기 위해 전체 병렬 시뮬레이션 중 일부(40%) 환경을 **평가 모드**로 설정하고, 특정 랜덤화 변인을 극단값(최대/최소)으로 고정한 상태에서 에피소드를 실행합니다. 에피소드 종료 시 **연속 성공 횟수**(아래에서 설명)를 측정하여 임계값 이상이면 해당 변인의 무작위화 범위를 넓히고, 기준 이하이면 범위를 좁히는 식입니다. 이렇게 하면 학습이 진행될수록 정책이 더 **넓은 환경 변화**에 견디게끔 훈련 난이도가 상승합니다. **Vectorized ADR**은 Isaac Gym의 대량 병렬 환경을 활용해 이러한 범위 조정을 **동시에 여러 변인에 대해** 수행하는 최적화된 구현입니다. 요약하면, ADR을 통해 처음에는 **쉬운 환경**에서 시작해 점차 **어려운 환경**으로 난이도가 올라가는 **자동 커리큘럼 학습**이 달성됩니다.

 **Figure 2:** 시뮬레이션에서의 정책 학습 파이프라인. 여러 개의 병렬 환경에서 Allegro Hand가 큐브를 돌리는 장면이 보인다 (왼쪽 그림). 각 환경은 무작위화된 물리 파라미터를 가지며, 학습 알고리즘(PPO 기반)이 **순환 신경망 정책**을 개선한다. 동시에 **Vectorized ADR** 알고리즘이 정책 성능을 평가하여 무작위화 범위(오른쪽 아래 그래프 아이콘의 Δ 기호로 표시)를 조정한다. 이러한 과정으로 정책은 다양한 조건에서 **강인한 제어** 능력을 익히게 된다.

## Domain Randomisation (이하 **DR**) 디테일 파헤치기 🔍  

DeXtreme가 **저비용 Allegro Hand + RGB Vision** 만으로 실세계에서 민첩한 in‑hand manipulation을 달성할 수 있었던 결정적 비결은 **DR + Automatic Domain Randomisation(ADR)** 입니다. 이 절에서는 논문이 사용한 DR 구성, 매개변수 범위, ADR 알고리즘, 그리고 실제 성능 영향까지 깊이 들어가 보겠습니다.

---

### 1. 왜 Domain Randomisation인가? 
- **시뮬레이터 모델 오차**: 마찰·접촉·센서 지연 같은 요소를 완벽히 모델링하기 어렵다.  
- **하드웨어 가변성**: 온도·마모·배터리 전압 등의 요인으로 로봇 파라미터가 매일 변한다.  
- **예기치 못한 외란**: 손가락에 부딪히는 충격, 비전 tracking 오류 등.  

> 따라서 “시뮬레이터가 틀린 것을 전제로 하고”, 가능한 많은 조건을 **무작위화**하여 **강인한(policy)** 를 학습한다. 이때 DR은 크게 **물리·관측·제어·환경** 4 영역으로 나뉜다. citeturn6view0

---

### 2. 무작위화 파라미터 전체 목록  
아래 표는 논문의 **Table 3**을 발췌·정리한 것으로, **초기 범위**와 ADR이 발견한 **최종 범위**(↔ 더 넓어진 부분은 굵게 표기)를 보여준다.

| 카테고리            | 파라미터                     | 타입          | 초기 범위    | ADR 후 범위             |
|-----------------|--------------------------|-------------|----------|----------------------|
| **Hand**        | Mass Scaling             | uniform     | 0.4–1.5  | 0.4–1.5              |
|                 | Friction Scaling         | uniform     | 0.8–1.2  | **0.54–1.58**        |
|                 | Joint Stiffness Scaling  | log‑uniform | 0.3–3.0  | **0.3–3.52**         |
|                 | Effort Scaling           | uniform     | 0.9–1.1  | **0.9–2.49**         |
| **Object**      | Mass Scaling             | uniform     | 0.4–1.6  | 0.4–1.6              |
|                 | Friction Scaling         | uniform     | 0.3–0.9  | **0.01–1.60**        |
|                 | External Forces          | additive    | ‑        | OpenAI 방식 참조         |
| **Observation** | Pose Delay Prob.         | set         | 0–0.05   | **0–0.47**           |
|                 | Pose Frequency           | set         | 1 (=매스텝) | **1–6 스텝**           |
|                 | Correlated/Uncorr. Noise | gaussian    | 0–0.04   | **0–0.14 (uncorr.)** |
| **Action**      | Action Delay Prob.       | set         | 0–0.05   | **0–0.31**           |
|                 | Action Latency (스텝)      | set         | 0        | **0–1.5**            |
|                 | RNA (적대 신경망) 스케일 α       | set         | 0        | **0–0.16**           |
| **Environment** | Gravity Δg (m/s²)        | normal      | 0±0.5    | 동일                   |

*타입*: **Scaling**은 모수에 계수 곱셈, **Additive**는 값 더하기, **Set value**는 에피소드마다 상수를 샘플링한다. citeturn6view0

---

### 3. Automatic Domain Randomisation (**ADR**) 메커니즘
1. **초기화**: 각 파라미터  $p_i$ 는  $[l_i, u_i]$ 범위를 갖고 시작. 초기 값은 현실 계측치 ± 여유치.  
2. **병렬 환경 샘플링**: Isaac Gym의 8 GPU에서 **16 k envs**를 실행. 이 중 **40 %** 는 *evaluation env* 로 지정해 DR 범위의 **경계값**(  $l_i$ 또는  $u_i$ )만 고정한다.  
3. **성공률 측정**: 에피소드 종료 시 **consecutive success**(큐브 연속 회전 성공 수)를 경계별로 기록.  
4. **경계 업데이트** (VADR 알고리즘) citeturn6view0  
   - 평균 성공 > 상한  $\gamma_\text{high}$ → **범위를 확장**  
   - 평균 성공 < 하한  $\gamma_\text{low}$ → **범위를 축소**  
   - 파라미터마다 **개별 step size** 를 사용해 훈련 안정성 보장.  
   - 업데이트가 발생하면 해당 경계의 과거 통계 큐를 리셋.  

5. **반복**: 훈련 진행에 따라 대부분 파라미터가 **2–5 배** 이상 넓어짐(위 표 굵은 부분 참고). 이는 정책이 점진적으로 **더 험난한 난이도**를 경험하도록 하는 **자동 커리큘럼** 역할을 수행한다.

**RNA(Random Network Adversary)** 역시 ADR이 조정하는 파라미터 중 하나다. 무작위 MLP가 생성한 가짜 액션  $a_\text{RNA}$ 를 정책 액션  $a_\pi$ 와 혼합하여 **구조화된 교란**을 주입함으로써, 단순 가우시안 노이즈보다 강력한 일반화 효과를 얻는다.

---

### 4. 훈련 스케줄 & 실제 효과
| 모델           | DR 방식      | 평균 연속 성공(실로봇)  |
|--------------|------------|----------------|
| Non‑ADR      | 수동 튜닝 DR   | **14.8**       |
| ADR          | VADR + RNA | **27.8**       |
| ADR (최고 롤아웃) | 〃          | **112** (peak) |

- **수동 DR**은 파라미터가 고정되어 일부 상황에서 cube가 ‘stuck’되는 실패 모드를 보였다.  
- **ADR** 정책은 같은 하드웨어·연산 자원으로 **2 배 이상** 성공 횟수를 늘리고, stuck 현상도 제거했다. citeturn6view0  
- 훈련 커브(논문 Figure 6)는 ADR 활성화 후 **npd(무작위화 엔트로피)**가 지속적으로 상승하면서 성능도 동반 향상되는 양상을 보여준다.

---

### 5. 재현 팁 & 주의사항
1. **현실 계측 → 초기 범위 설정**: 실제 Allegro Hand의 관절 스프링·댐핑, 큐브 질량/마찰 값 등을 먼저 측정하고 ± 40–60 % 정도로 초기 범위를 넓히면 안정적인 초기 탐색이 가능하다.  
2. **Pose Frequency 랜덤화**: Vision tracking 주기가 제어 loop(30 Hz)보다 느리므로, `Obj.Pose Freq.` 를 1–6 스텝으로 랜덤화해 **센서 지연 내성**을 키워야 한다.  
3. **단계적 ADR 활성화**: 학습 초반엔 ADR off + 좁은 수동 DR로 **베이스 움직임**을 먼저 익히고, 5 M step 이상 성과가 안정화되면 ADR을 켜면 수렴 속도가 빠르다.  
4. **파라미터별 step size**: 마찰처럼 민감한 변수는 작은 step(예: ±0.02), 관절 effort scale 같이 비교적 완만한 변수는 큰 step(예: ±0.1)을 주면 학습 폭주를 막을 수 있다.  

---

### 6. 요약
- **DR**은 물리·관측·제어·환경 전 채널에 걸쳐 **70 개+** 파라미터를 무작위화.  
- **Vectorised ADR**이 **파라미터별 경계**를 자동 확장/축소 → **자동 커리큘럼**.  
- **RNA**가 구조화된 행동 교란을 가미해 정책 일반화 강화.  
- 그 결과, **모캡·고가 장비 없이도** Shadow Hand 수준의 연속 성공률을 Allegro Hand에서 달성.  

DeXtreme의 DR/ADR 설계는 “**얼마나 많이가 아니라, 어떻게 똑똑하게** 무작위화할 것인가?”에 대한 훌륭한 레퍼런스로, sim‑to‑real 로드맵을 그리는 연구자라면 꼭 참고해 볼 만하다.

## Vision 기반 상태 추정과 Teacher-Student 적용
현실 로봇에서 **큐브의 자세**를 알아내기 위해, 저자는 **별도의 비전 신경망**을 설계했습니다. 이 **물체 자세 추정 네트워크**는 **Mask R-CNN** 구조를 응용하여, 카메라 이미지에서 **큐브의 8개 모서리 점(keypoints)**를 검출하도록 학습되었습니다. 
먼저 RGB 영상에서 큐브의 **바운딩 박스와 마스크(segmentation)**를 예측하고, 그 내부에서 모서리 점들의 위치를 회귀합니다. 
이렇게 하면 복잡한 배경이나 큐브 미검출 문제를 줄이고, 검출된 키포인트들을 이용해 **PnP 알고리즘**으로 3D 자세를 산출할 수 있습니다. 
카메라는 Allegro Hand를 중심으로 배치된 Intel RealSense D415 RGB 카메라 3대를 사용했으며, 각 카메라의 출력으로 추정된 큐브 자세들을 **필터링 및 평균 결합**하여 최종 물체 자세를 계산합니다. 
이 방법은 전용 마커 없이도 비교적 정확한 (평균 5.3° 오차) 자세 추정이 가능했고, OpenAI의 end-to-end 비전 정책과 달리 **카메라 배치에 유연**하다는 장점이 있습니다.

비전 네트워크의 학습은 **전적으로 시뮬레이션 데이터**로 이루어졌습니다. 
Isaac Sim을 사용해 손과 큐브가 다양한 자세로 있는 **합성 이미지 500만 장**을 렌더링하여 대량의 학습 데이터를 확보했습니다. 
이 때 **조명, 카메라 각도, 배경** 등을 무작위화하여 다양한 상황을 망라했고, 추가로 **CutMix**, **모션 블러** 등의 **데이터 증강**을 적용하여 실제 카메라 영상과의 도메인 격차를 줄였습니다. 
아래 그림의 윗줄은 시뮬레이터로 생성한 다양한 카메라 시점의 이미지들이고, 아랫줄은 여기에 임의 배경 합성, 블러 등을 추가한 증강 결과들입니다. 
이렇게 학습된 네트워크는 실제 로봇에서 약 15Hz로 동작하며, 추정된 큐브 자세를 **강화학습 정책**에 실시간 제공해줍니다.

**Figure 3:** 비전 데이터 생성과 학습 파이프라인. (왼쪽) Omniverse Isaac Sim으로 무작위 환경에서 합성된 손+큐브 이미지들. 
이렇게 500만 장의 **합성 데이터셋**을 확보한 뒤, (오른쪽) **데이터 증강**을 거쳐 Mask R-CNN 기반 **물체 추적 네트워크**를 학습한다. 
이 네트워크는 큐브의 bounding box, segmentation, 8개 모서리 **키포인트**를 예측하며, 최종적으로 여러 카메라의 결과를 **PnP**로 통합해 3D 자세를 산출한다.

강화학습 **정책 네트워크**는 초기에 시뮬레이션에서 학습될 때 **정확한 큐브 자세(특권 정보)** 대신 위와 같은 **추정 정보**에 노이즈를 섞은 값을 관측으로 받도록 설계되었습니다. 
이를 통해 실제 비전 시스템의 오차와 지연에 정책이 미리 단련되었고, sim-to-real 시 겪는 **관측 불확실성**을 견딜 수 있었습니다. 
요약하면, 시뮬레이터 상의 **교사(teacher) 정책**은 완전 상태 정보를 활용해 높은 성능으로 학습되고, 이후 **학생(student) 정책**은 제한된 관측으로도 유사한 행동을 모방하도록 유도되었습니다. 
이러한 **teacher-student 네트워크 구조** 덕분에 최종 정책은 모캡 없이도 동작하면서, 마치 모캡이 있는 것처럼 높은 성공률을 보이는 결과를 얻었습니다.

## 결과: 현실에서의 민첩한 손 안 조작
**DeXtreme**의 최종 정책을 실제 Allegro Hand 로봇에 이식한 결과, 손바닥 위의 큐브를 **연속해서 여러 목표 자세로 빠르게 회전**시키는 데 성공했습니다. 평가 지표는 **연속 성공 횟수**(consecutive successes)로, 큐브를 떨어뜨리지 않고 몇 번 연속으로 목표 자세에 도달시키는지를 측정합니다. 
학습된 정책은 평균 수십 회 이상의 연속 성공을 달성했으며, 최대 수백 회에 달하는 연속 회전도 시현되었습니다. 
특히 **비전 기반**임에도 불구하고 성능이 매우 높아, 이전 최고 성능의 **모캡 기반 정책**에 버금가는 수준이고, OpenAI의 **최고 비전 정책**보다 **약 1.5배 많은 평균 연속 성공 횟수**를 기록했습니다. 
예를 들어, OpenAI의 카메라 기반 정책보다 **평균 연속 성공 횟수가 1.5배 향상**되었고, **모커(MoCap) 사용 정책과 유사한 범위**(약 100회 내외)의 성공을 보였습니다. 

또한 다양한 실험에서 **정책의 강인함**이 확인되었습니다. 
하루 간격으로 로봇을 동작시켜 **환경 변화(온도, 마모 등)**에 따른 성능 변화를 관찰한 결과, 약간의 변동은 있었지만 전반적으로 높은 성공률을 유지했습니다. 
한편, **ADR 기반 정책**과 그렇지 않은 정책을 비교한 실험에서는 ADR이 없는 경우 특정 조건에서 **명확한 실패 모드**가 나타난 반면, ADR을 거친 정책은 훨씬 **안정적 동작**을 보였습니다. 
요컨대, 시뮬레이션에서 도입한 다양한 기법들이 현실에서도 효과를 발휘하여, **손 안 조작 작업의 성공률과 안정성**을 크게 높인 것을 검증했습니다.

## 마무리 및 의의
DeXtreme 연구는 **저렴한 로봇 손과 카메라, 그리고 효율적인 학습 기법**만으로도 **난이도 높은 손 안 조작**을 현실에서 구현할 수 있다는 것을 보여주었습니다. 특히 **Privileged RL**과 **teacher-student 지식전이**, **대규모 도메인 랜덤화+ADR 커리큘럼**의 조합이 sim-to-real 문제를 극복하는 데 매우 효과적임을 입증했습니다. 이로써 복잡한 모캡 장비나 거대한 연산 자원 없이도 **최신 수준의 손 재주**를 로봇에게 학습시킬 수 있는 길을 제시했다는 점에서 의미가 큽니다. 저자들은 코드와 학습 파이프라인을 공개하여(**재현 가능성**) 추후 연구자들이 이를 기반으로 다양한 물체, 다양한 손으로 실험을 확장할 수 있도록 했습니다. DeXtreme은 향후 **일상적인 로봇 손 기술** 개발에 중요한 참고 사례가 될 것이며, **강화학습 기반 로봇 제어**의 실용화를 한 걸음 앞당긴 연구로 평가됩니다. 



# Reference

- [DeXtreme: Transfer of Agile In-Hand Manipulation from Simulation to Reality | Research](https://research.nvidia.com/publication/2023-06_dextreme-transfer-agile-hand-manipulation-simulation-reality#:~:text=Recent%20work%20has%20demonstrated%20the,the%20literature%20on%20the%20same)
- [[2210.13702] DeXtreme: Transfer of Agile In-hand Manipulation from Simulation to Reality](https://ar5iv.labs.arxiv.org/html/2210.13702#:~:text=This%20paper%20builds%20on%20top,us%20to%20reduce%20the%20amount)
