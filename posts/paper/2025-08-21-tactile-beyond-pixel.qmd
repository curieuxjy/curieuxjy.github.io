---
title: "📃Tactile Beyond Pixel 리뷰"
date: 2025-08-19
categories: [tactile, digit360, multlimodal]
toc: true
number-sections: false
description: Multisensory Touch Representations for Robot Manipulation
---

- [Paper Link](https://arxiv.org/abs/2506.14754)

1. 👉 이 논문은 로봇 조작을 위한 최초의 멀티모달 촉각 표현 학습 모델인 Sparsh-X를 제시하며, 이는 Digit 360 센서에서 얻은 이미지, 오디오, 모션, 압력 등 네 가지 촉각 양상을 자체 지도 학습을 통해 하나의 통합된 표현으로 융합합니다.
2. 📚 약 100만 건의 접촉 상호작용 데이터로 학습된 Sparsh-X는 물체의 물리적 특성 및 접촉 역학을 효과적으로 포착하며, 다양한 작업에서 이러한 정보를 활용할 수 있음을 입증합니다.
3. 🚀 실험 결과, Sparsh-X는 모방 학습 및 Sim-to-Real 촉각 적응을 통해 로봇 조작 성공률을 63% 향상시키고, 물리적 특성 추정 정확도를 기존 방식 대비 48% 개선하여 견고한 미세 조작 성능을 제공합니다.


---

# Brief Review

이 논문은 로봇 조작을 위한 일반 목적의 다중 감각 촉각 표현(multisensory touch representations) 모델인 Sparsh-X를 제안합니다. Sparsh-X는 Digit 360 센서에서 수집된 이미지, 오디오, 모션(IMU), 압력의 네 가지 촉각 모달리티를 활용하여 약 100만 건의 접촉 기반 상호작용 데이터로 자가 지도 학습(self-supervised learning)되었습니다. 이 모델은 다양한 시간적, 공간적 스케일의 상호 보완적인 촉각 신호들을 단일화된 표현으로 융합하여 물리적 특성을 포착합니다.


Sparsh-X의 핵심 방법론은 transformer 기반의 백본 아키텍처에 있습니다(그림 2 참조). 

<center>
<img src="../../images/2025-08-21-tactile-beyond-pixel/Screenshot 2025-08-21 at 11.27.27 AM.png" width="100%" />
</center>


입력 신호는 먼저 $L_f$개의 레이어($L_f = 8$)에서 각 모달리티별로 독립적인 self-attention을 통해 처리됩니다. 이후 $L_b$개의 블록($L_b = 4$)에서는 attention bottlenecks [35]를 사용하여 cross-modal 정보 흐름이 이루어집니다. 이를 위해 $B$개의 bottleneck fusion tokens($B = 4$)이 각 모달리티의 embedding에 연결되며, 각 cross-modal 업데이트 후 fusion tokens는 모달리티 전반에 걸쳐 평균화되어 정보 공유를 촉진합니다. 총 transformer 레이어 수는 $L = L_f + L_b = 12$로 설정되었습니다.

입력 모달리티는 다음과 같이 전처리되고 토큰화됩니다:

*   **Image**: 30fps로 샘플링된 촉각 이미지를 5의 temporal stride로 채널 차원에 따라 연결합니다. hyper-fisheye 이미지는 224x224x3 크기로 자르고 리사이즈되며, 16x16 크기의 패치(patch)로 분할된 후 linear projection layer를 통해 768차원의 embedding으로 토큰화됩니다.
*   **Audio**: 48kHz로 샘플링된 두 개의 접촉 마이크에서 얻은 0.55초의 오디오 신호는 5ms Hamming window와 2.5ms의 hop length로 128채널의 log-mel spectogram으로 변환됩니다. 두 마이크의 spectogram이 연결되어 224x256 오디오 입력이 되며, 16 크기의 패치로 토큰화됩니다.
*   **IMU (Accelerometer)**: 400Hz로 샘플링된 3축 가속도계 데이터는 0.55초 창으로 통합되어 224x3 temporal signal로 토큰화됩니다.
*   **Pressure**: 200Hz로 샘플링된 압력 신호는 1.1초 창으로 통합되어 224x1 temporal signal로 토큰화됩니다.

Sparsh-X는 자가 지도 학습을 위해 `teacher-student self-distillation` 접근 방식 [40, 11]을 사용합니다. 인코더와 예측 헤드로 구성된 두 브랜치에서, 학생 입력 토큰에 마스킹을 적용하고 (로컬 마스크의 경우 10-50%, 전역 마스크의 경우 50-100% 신호 유지), 교사 토큰을 pseudo-label로 사용하여 클러스터링 예측 작업을 수행합니다. 최적화 목표는 교사와 학생 네트워크의 softmax 출력 간 cross-entropy입니다.

평가 실험은 세 가지 주요 영역에서 진행되었습니다:

1.  **물리적 특성 추론 (Inferring physical properties)**: 객체-행동-표면 분류(object-action-surface classification), 재료-양 추정(material-quantity estimation), 법선력 추정(normal force estimation)과 같은 지도 학습(supervised learning) 작업을 통해 Sparsh-X 표현의 품질을 평가했습니다. Sparsh-X의 인코더 가중치는 고정된 상태로 task-specific attentive decoder를 훈련시켜 순수하게 표현의 품질을 측정했습니다. 그 결과, 다중 모달리티를 함께 사용했을 때 촉각 이미지 단독 사용 대비 분류 정확도가 현저히 향상되었으며, end-to-end 모델보다 데이터 효율성과 일반화 성능이 우수함을 보였습니다.
2.  **정책 학습을 위한 Sparsh-X 통합 (Sparsh-X for Policy Learning)**:
    *   **모방 학습(Imitation Learning)을 통한 플러그 삽입**: 로봇이 Allegro hand와 Digit 360 센서를 사용하여 플러그를 소켓에 삽입하는 작업에서, Sparsh-X를 활용한 다중 감각 촉각 피드백이 정책 성공률을 크게 향상시켰습니다. 외부 시각 정보만 사용했을 때보다 500%, 촉각 이미지만 사용한 end-to-end 정책보다 63%의 성공률 향상을 보였습니다.
    *   **시뮬레이션에서 실제 세계로의 촉각 적응(Sim-to-Real Tactile Adaptation)을 통한 손안 객체 회전**: Hora [51]와 같은 시뮬레이션 훈련된 기본 정책 위에 ControlNet [52]을 활용하여 촉각 적응 모듈을 학습시켰습니다. 이는 객체의 물리적 특성(질량, 마찰 등) 변화에 대한 정책의 견고성을 높여 수직 표류(vertical drift)를 90% 감소시키고, 객체 슬립을 줄여 회전 안정성을 향상시켰습니다.

Sparsh-X는 다중 감각 촉각 정보를 통합함으로써 로봇 조작의 정밀성과 견고성을 크게 향상시킬 수 있음을 입증하며, 촉각 센싱 분야의 foundation models 개발에 중요한 발걸음을 내디뎠습니다.

---

# Detail Review

## 1. 논문의 주요 내용 요약

<center>
<img src="../../images/2025-08-21-tactile-beyond-pixel/Screenshot 2025-08-21 at 11.27.55 AM.png" width="100%" />
</center>

> Sparsh-X의 멀티센서 촉각 표현 융합 구조 예시. 이 구조는 Digit 360 센서로부터 얻은 촉각 이미지, 진동(오디오), 관성모션, 압력 신호를 입력으로 받아, 트랜스포머 기반 백본에서 이들을 융합하여 물체의 물리적 특성 및 접촉 상태를 나타내는 통합 표현을 학습한다. 

본 논문에서는 Meta/FAIR가 개발한 초고해상도 촉각 센서 Digit 360을 활용하여 4가지 촉각 모달리티(이미지, 오디오, 관성, 압력)를 통합하는 Sparsh-X라는 표현 학습 백본을 제안한다. 저자들은 약 100만 건의 접촉-조작 데이터(삽입, 미끄럼, 두드림, 회전 등 다양한 조작 행동)를 수집하여 자기 지도 학습으로 Sparsh-X를 사전학습시켰다. 이렇게 학습된 표현은 물체의 질량, 마찰, 힘 등의 물리적 특성을 추론할 수 있는 정보를 포함한다.

* 제안 방식: Sparsh-X는 각 모달리티를 독립적인 트랜스포머 블록으로 인코딩한 후, 병목(Bottleneck) 토큰을 매개로 크로스-모달 퓨전을 수행하는 구조를 갖는다. 이를 통해 고해상도 촉각 이미지뿐 아니라 초음속 진동음, 가속도/자이로 센서 정보, 압력 신호 등 이질적인 데이터를 하나의 잠재 공간에 효과적으로 융합한다.
* 데이터: 양팔 로봇 손가락과 수동 집게에 장착한 Digit 360으로 다양한 환경에서 수집한 약 1M의 비라벨 접촉 데이터를 사용했다. 이 대규모 무라벨 데이터셋은 향후 유사한 연구를 위한 벤치마크로도 활용될 수 있다.
* 실험 과제: 학습된 표현은 두 가지 주요 실험에 적용되었다. 
    (1) 흉내 학습(imitation learning)을 통한 플러그 삽입 과제(Allegro 로봇 손에 플러그를 꽂는 작업)와, 
    (2) 시뮬레이션-현실 간 촉각 적응을 통한 손안 회전 과제(컵 모양 물체를 손 안에서 회전시키는 작업)이다. 
    * 또한 물리적 속성 추론 과제(물체-행동 분류, 재질·질량·양 추정, 가해진 힘 추정 등)도 병행하여 평가했다.
* 주요 결과: 
    * Sparsh-X를 활용한 정책은 (a) 외부 카메라 정보만 사용한 정책 대비 성공률이 약 500%(즉 5배) 높고, (b) 촉각 이미지만 단독으로 사용한 종단간(end-to-end) 학습 대비 63% 높은 성공률을 보였다. 
    * 특히, 플러그 삽입 실험에서는 Sparsh-X 기반 정책이 20회 시도 중 90%의 성공률을 기록했으며, 기존 비전·촉각 방식 대비 성능이 크게 향상되었다. 
    * 손안 회전 과제에서는 Sparsh-X를 이용해 시뮬레이터에서 훈련된 정책을 적응시킨 결과, 물체의 수직 이동이 약 90% 감소하는 등 안정성이 크게 개선되었다. 
    * 물리 속성 추론에서는 Sparsh-X 표현을 사용해 48% 높은 정확도를 기록해, 기존 종단 학습 방식 대비 물리적 특성 이해 능력이 크게 향상됨을 보였다. 예를 들어, 모든 모달리티를 결합하면 힘 추정 오차가 평균 35mN로 촉각 영상만 사용했을 때보다 17% 감소했다.

## 2. 기술적 기여 및 한계 분석

* 통합 멀티모달 백본: 본 논문의 핵심 기여는 Sparsh-X라는 최초의 멀티센서 촉각 표현 백본이다. 이전까지 촉각 표현 학습은 대부분 단일 모달리티(예: GelSight류 촉각 영상)나 모달리티별 개별 학습에 그쳤다. Sparsh-X는 네 가지 모달리티를 병목 기반 트랜스포머로 융합함으로써, 다양한 촉각 신호 간의 상호 보완적 정보를 포착한다. 특히, 단순 토큰 병합 방식을 사용한 기존 멀티모달 트랜스포머(MULSA)보다 계산 복잡도를 크게 낮추고, 네 가지 입력을 하나의 잠재 공간으로 압축할 수 있다.
* 대규모 자기 지도 학습: 약 100만 건의 실제 접촉 데이터를 활용한 SSL(자기 지도 학습)을 수행했다. 이를 통해 데이터 라벨링 비용 없이 일반화 가능한 촉각 표현을 학습하였다. 사전학습된 Sparsh-X 표현은 downstream 학습 시 데이터 효율을 크게 높여, 적은 레이블 샘플로도 안정적인 정책 학습이 가능하다.
* 정밀한 물리특성 학습: Sparsh-X는 물체의 질량, 마찰계수, 적용 힘 등 다양한 물리 속성을 포착하며, 이러한 속성 예측 성능이 크게 향상되었다. 실제로 본 논문에서는 물체-행동-면 분류, 재질·양 예측, 가해진 힘 추정 실험을 통해 Sparsh-X가 48% 더 높은 분류 정확도를 보임을 확인했다. 또한 정상(normal) 힘 추정 실험에서 모든 모달리티를 결합할 때 평균 오차가 17% 감소하여 힘 추정 정확도가 향상되었다.
* 정책 학습과 시뮬레이터 적응: Sparsh-X 표현은 실제 로봇 조작 정책 학습에도 적용되었다. 예를 들어, 플러그 삽입 과제에서 이미지+촉각의 조합으로 행동을 예측하는 종단간 모델에 Sparsh-X를 추가하자 성공률이 63% 증가했다. 또한, 시뮬레이터에서 훈련된 손안 물체 회전 정책에 Sparsh-X 기반의 촉각 적응 모듈(ControlNet)을 적용하자 물체의 미끄러짐이 현저히 줄어들었으며, 기존 방법 대비 수직 이동량 90% 감소 효과를 보였다. 이처럼 멀티센서 촉각 표현은 시뮬-실전 전이(sim-to-real) 문제 해결에도 기여함을 보였다.
* 한계점: 이 논문은 멀티센서 촉각의 잠재력을 보여주었으나 다음과 같은 한계도 지적한다. 첫째, 현재 사용된 데이터셋은 Digit 360 센서가 포함된 특정 플랫폼(예: Allegro 손, 수동 집게)에서 수집되었기 때문에 촉각 영상 모달리티의 다양성이 제한적일 수 있다. 센서별 광학적 특성 차이로 인해 일반화 성능이 제한될 우려가 있다. 둘째, 모든 실험에서 Sparsh-X 표현을 고정(frozen) 상태로 사용했으며, 다운스트림 과제별로 파인튜닝을 하지 않았다. 실제 적용 시 파인튜닝을 허용하면 개별 모달리티의 데이터 부족 문제를 보완하고 성능을 더욱 높일 수 있다. 셋째, 힘 추정 실험은 정상 방향 힘에 한정되었고, 다양한 접촉 기하나 전단력 추정은 다루지 않았다. 전단력은 Digit 360 구조(탄성돔) 때문에 모델링이 복잡하며, 본 논문에서는 별도 고려되지 않았다. 마지막으로, 대용량 트랜스포머를 학습하는 데 필요한 계산 자원과 데이터 수집 노력이 커서, 실제로 적용하는 데 비용 부담이 있다.

## 3. 관련 연구와의 비교

* 기존 촉각 기반 조작 연구: 전통적으로 로봇 촉각 연구에서는 GelSight, DIGIT 등 비전 기반 촉각 센서가 주로 사용되었다. 이런 센서는 고해상도 촉각 이미지를 제공해 물체 형상, 힘, 마찰 등을 추정할 수 있다. 그러나 대부분 작업은 단일 촉각 이미지에 의존하거나, 외부 카메라와 연계하는 방식이었다. 예를 들어, 다양한 조립, 표면 식별, 경로 추적 과제에서 GelSight류 센서가 활용되었지만, 영상 촉각만으로는 연속적인 접촉 동작의 미세한 변화를 완전히 포착하기 어렵다는 한계가 있다.
* 오디오 및 기타 모달리티 활용: 일부 연구에서는 접촉 시 발생하는 진동음(오디오)이나 외부 카메라 영상을 함께 이용하여 물체 특성을 추정하려 했다. 예컨대, 접촉 마이크를 이용해 재질을 식별하거나, 영상-오디오 합성 학습을 시도한 바 있다. 하지만 오디오 단일 모달로는 접촉과정의 복잡한 힘·변형 정보를 온전히 얻기 어렵고, 멀티모달 학습을 하더라도 주로 시각과 청각에만 국한되었다.
* 기존 멀티모달 접근: MULSA 등 최근 연구는 비전, 촉각 영상, 오디오를 함께 Transformer로 학습하는 방식으로 멀티모달 촉각 표현을 시도했다. 그러나 MULSA는 단순히 모든 토큰을 이어붙여(attention concatenation) 처리하기 때문에 계산 복잡도가 매우 커지는 문제가 있다. 또한 MimicTouch (Yu et al., 2024)와 같은 연구는 영상 촉각과 오디오를 개별적으로 SSL로 학습했지만, 모달리티 간 융합을 수행하지 않아 촉각 간 상호작용을 충분히 활용하지 못했다.
* 본 논문의 차별점: Sparsh-X는 네 가지 모달리티(이미지, 오디오, 관성, 압력)를 통합하는 점에서 기존 연구와 뚜렷한 차별성을 가진다. 특히, 병목 토큰 기반의 멀티모달 트랜스포머를 도입해 모달리티 간 정보를 효율적으로 융합하며, 종래 방식보다 계산 효율성과 표현력 모두 개선했다. 이는 기존 음성 기반 접근이나 단일센서 기반 모델이 다루지 못한 다양한 촉각 신호를 통합하여, 더 풍부한 촉각 표현을 학습할 수 있게 한다. 즉, 종전 연구가 해결하지 못했던 멀티센서 융합 방식을 제안함으로써, 로봇의 촉각 인지가 한층 발전되었다.

## 4. 실제 응용 가능성 평가

* 센서 및 하드웨어 측면: 제안된 방법은 Digit 360 센서를 전제로 한다. Digit 360은 Meta FAIR와 GelSight가 공개한 최첨단 촉각 센서로, 지름 14mm의 인조 손가락 모양이며 18개 이상의 센싱 기능을 통합한다[16]. 물리적 변형과 압력, 진동을 초고해상도로 감지하여 인간 수준의 정밀도로 터치를 디지털화할 수 있다[16]. GelSight 측은 이 센서를 내년부터 본격 공급할 예정이므로, 향후 산업용 로봇에도 장착이 가능해질 것으로 보인다. 그러나 현재는 비교적 실험실용 프로토타입 수준이므로, 실제 공장이나 서비스 환경에 배치하려면 추가적인 내구성 검증과 비용 고려가 필요하다.
* 시스템 요구 사항: Sparsh-X는 대규모 사전학습이 전제되므로 상당한 계산 자원과 데이터 수집이 필요하다. 산업 현장에서 도입하려면 개별 작업에 맞춰 추가 학습 또는 미세 조정(fine-tuning)이 필수적이다. 또한, Digit 360을 로봇 손가락에 부착하고 실시간으로 데이터를 처리하려면 고속 데이터 처리와 연산 하드웨어가 요구된다. 예를 들어 삽입 조립 작업에서는 센서-행동 반응 지연(latency)을 줄여야 하며, 산업용 로봇 암에 정확히 맞도록 센서 장착 방식을 고려해야 한다.
* 적용 가능성: 그럼에도 불구하고, Sparsh-X의 촉각 표현은 고정밀 작업에 유리하다. 플러그 삽입, 나사 체결 등 카메라로는 어려운 정밀 조립 작업이나, 조리 로봇의 섬세한 조작, 복잡 형상의 부품 검사 등에 활용할 수 있다. 또한, 센서 데이터와 학습을 병행하면 비전 정보가 불충분한 어둡거나 부분 가려진 환경에서도 안정적인 조작이 가능하다. 예컨대, 복잡한 회로기판 위 작은 부품을 집거나, 의료용 로봇이 미세한 조직을 다루는 작업 등에 응용될 수 있다. 실제로 본 논문에서도 플러그 삽입과 같은 산업적 의미가 있는 조작에서 큰 성능 향상이 관찰되었다.
* 제약 및 전망: 현재 연구 단계에서는 다양한 접촉 형태(경사진 표면, 전단력 등)에 대한 검증이 부족하며, 대량의 촉각 데이터 구축도 필요한 상태이다. 또한 실제 산업 환경에서는 센서의 내구성, 잡음·오염 문제, 모델의 추론 속도 등이 추가 과제가 될 수 있다. 그럼에도 불구하고 Sparsh-X는 촉각에 기반한 ‘기초 모델(Foundation Model)’ 접근의 가능성을 보여준다. 즉, 다양한 로봇 작업에 재사용 가능한 촉각 표현을 제공함으로써, 추후 도메인별 미세 조정으로 적용 범위를 확장할 수 있는 잠재력이 크다. 실제로 GelSight 측은 Digit 360을 로봇 촉각 연구의 다음 단계로 평가하며, 의료·가상현실·휴머노이드 등 다양한 분야에 응용할 수 있을 것으로 전망한다. 따라서 충분한 데이터와 연산 자원이 확보된다면, Sparsh-X 방식은 산업용·서비스용 로봇에서 섬세한 조작을 필요로 하는 다수 과제에 적용 가능할 것이다.
