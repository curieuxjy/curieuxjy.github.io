---
title: "ğŸ“ƒVAE"
description: Auto-Encoding Variational Bayes
date: "2022-10-02"
categories: [generative, variational inference, paper]
toc: true
execute:
  freeze: auto 
---

![](https://i.imgur.com/ASS0s9R.jpg)


ì´ë²ˆ í¬ìŠ¤íŠ¸ëŠ” ìƒì„±ëª¨ë¸ì—ì„œ ìœ ëª…í•œ Variational Auto-Encoder(VAE)ë¥¼ ë‹¤ë£¨ê³  ìˆëŠ” `Auto-Encoding Variational Bayes`ë¼ëŠ” ë…¼ë¬¸ ë¦¬ë·°ì…ë‹ˆë‹¤. ì´ë²ˆ í¬ìŠ¤íŠ¸ë¥¼ ì •ë¦¬í•˜ë©´ì„œ ê°€ì¥ ë§ì´ ì¸ìš©í•˜ê³  ë„ì›€ì„ ë°›ì€ [ì˜¤í†  ì¸ì½”ë”ì˜ ëª¨ë“  ê²ƒ](https://youtu.be/o_peo6U7IRM)ë¥¼ ë³´ì‹œë©´ í›¨ì”¬ ë” ìì„¸í•˜ê³  ê¹Šì€ ì´í•´ë¥¼ í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í¬ìŠ¤íŠ¸ì˜ ìˆœì„œëŠ” ì•„ë˜ì™€ ê°™ì´ ì§„í–‰ë©ë‹ˆë‹¤.

![](https://i.imgur.com/Dt8qaGu.jpg)

# Introduction

VAEëŠ” ìƒì„±ëª¨ë¸(Generative Model)ì—ì„œ ìœ ëª…í•œ ëª¨ë¸ì…ë‹ˆë‹¤. ê·¸ë ‡ë‹¤ë©´ ìƒì„± ëª¨ë¸ì´ë€ ë¬´ì—‡ì„ ë§í•˜ëŠ” ê±¸ê¹Œìš”? ì˜ˆë¥¼ ë“¤ì–´ ìš°ë¦¬ê°€ ì°ì€ ì ì´ ì—†ëŠ” ê°•ì•„ì§€ ì‚¬ì§„ì„ `ë§Œë“¤ì–´ë‚´ê³  ì‹¶ë‹¤`ê³  í•´ë´…ì‹œë‹¤. ê·¸ë ‡ì§€ë§Œ ê°•ì•„ì§€ ì‚¬ì§„ì´ ì‹¤ì œ ê°•ì•„ì§€ë“¤ì„ ì°ì€ ì‚¬ì§„ë“¤ê³¼ ë„ˆë¬´ ë™ë–¨ì–´ì ¸ì„œ ì´ì§ˆê°ì„ ëŠë¼ì§€ ì•Šì•„ì•¼ í•©ë‹ˆë‹¤. ì´ëŸ° ë§¥ë½ì—ì„œ ìš°ë¦¬ê°€ ì›í•˜ëŠ” ê²ƒì€ train databaseì— ìˆëŠ” ì‚¬ì§„ë“¤, ì¦‰ ì‹¤ì œë¡œ ê°•ì•„ì§€ë“¤ì„ ì°ì€ ì‚¬ì§„ë“¤ì˜ **ë¶„í¬**ë¥¼ ì•Œê³  ì‹¶ìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ **ë¶„í¬**ë¥¼ ì•Œê³  ì‹¶ì€ ì´ìœ ëŠ” ìš°ë¦¬ê°€ ë¶„í¬(distribution)ì„ ì•Œì•„ì•¼ ë¶„í¬ì—ì„œ dataë¥¼ ìƒ˜í”Œë§í•´ì„œ **ìƒì„±**í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ë‹¤ì‹œ ì •ë¦¬í•˜ìë©´, í˜„ì¬ ë°ì´í„°ë“¤ê³¼ **ë¹„ìŠ·í•œ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ìƒì„± í•˜ê¸° ìœ„í•´** í˜„ì¬ train DBì˜ ë°ì´í„°ë“¤ì˜ ë¶„í¬ **$p(x)$** ë¥¼ ì•Œê³  ì‹¶ìŠµë‹ˆë‹¤.

ë°ì´í„° $x$ë¥¼ ìƒì„±í•˜ëŠ” **Generator**ë¥¼ ì‘ë™ì‹œí‚¬ **controller**ê°€ í•„ìš”í•©ë‹ˆë‹¤. `ì–´ë–¤ ë°ì´í„°ë¥¼ ìƒì„±í•˜ë„ë¡ Generatorë¥¼ trigger` í•´ì£¼ëŠ” ë¶€ë¶„ì´ê¸° ë•Œë¬¸ì— ìš°ë¦¬ê°€ `ë‹¤ë£¨ê¸° ì‰½ê²Œ` ë§Œë“¤ì–´ ì¤˜ì•¼ ì´í›„ ìƒì„±ëª¨ë¸ì„ ì‚¬ìš©í•  ë•Œ í¸ë¦¬í•  ê²ƒ ì…ë‹ˆë‹¤. controller ì—­í• ì„ í•˜ëŠ” latent variable $z$ëŠ” $p(z)$ì—ì„œ ìƒ˜í”Œë§ë˜ë©° ë°ì´í„° $x$ë³´ë‹¤ ì°¨ì›ì´ ì‘ê²Œ ë§Œë“­ë‹ˆë‹¤.

![](https://i.imgur.com/xn7xVol.jpg)

<div style="text-align:center">
  Generative Model<sup>6</sup>
</div>

<p>

</p>

ë‹¤ì‹œ ëª©í‘œì˜€ë˜ $p(x)$ë¥¼ ìƒê°í•´ë³´ë©´, `prior probability` $p(z)$ì™€ conditional probababilityì˜ ê³± ì ë¶„ìœ¼ë¡œ ìƒê°í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì ë¶„ ì´ë•Œ ì ë¶„ì„ ë‹¨ìˆœíˆ samplingí•œ ì—¬ëŸ¬ ë°ì´í„°ë“¤ì„ summationí•´ì„œ maximum likelihood estimationì„ ë°”ë¡œ í•  ìˆ˜ ìˆì§€ ì•Šì„ê¹Œ? ìƒê°í•  ìˆ˜ë„ ìˆì§€ë§Œ ì´ëŠ” ìƒ˜í”Œë§í•˜ëŠ” ê³¼ì •ì—ì„œ ìš°ë¦¬ê°€ ì›í•˜ì§€ ì•ŠëŠ” ìƒ˜í”Œë“¤ì´ ë” ë§ì´ ë½‘í ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ì´ ë°©ë²•ì„ ì“¸ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

![](https://i.imgur.com/Nc4rQ75.jpg)

<div style="text-align:center">
  The reason why it is hard to measure the likelihood of images under a model using only sampling<sup>11</sup>
</div>

<p>

</p>


**ìš°ë¦¬ê°€ ì›í•˜ì§€ ì•ŠëŠ” ìƒ˜í”Œë“¤ì´ ë” ë§ì´ ë½‘íˆëŠ” í˜„ìƒ**ì„ ì˜ˆì‹œë¥¼ ë“¤ì–´ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤. MINST ë°ì´í„° ì¤‘ 2ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì´ë¯¸ì§€ (a)ê°€ ìˆê³ , (a)ë¥¼ ì¼ë¶€ ì§€ìš´ (b)ì™€ (a)ë¥¼ ì˜¤ë¥¸ìª½ìœ¼ë¡œ 1 pixel ë§Œí¼ ì˜®ê¸´ (c)ê°€ ìˆìŠµë‹ˆë‹¤. ì´ë•Œ ìš°ë¦¬ëŠ” (a)ì™€ ìœ ì‚¬í•œ ë°ì´í„°ë¥¼ ë” ë§ì´ ë½‘ê³  ì‹¶ê³ , (b)ë³´ë‹¤ëŠ” (c)ê°€ (a)ì™€ ë” ê°€ê¹ë‹¤ê³  ìƒê°í•˜ë¯€ë¡œ (c)ì™€ ê°™ì€ ë°ì´í„°ë“¤ì´ ë” ë§ì´ ë½‘íˆê¸°ë¦¬ ì›í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ë³´í†µ Generatorê°€ Normal distributionìœ¼ë¡œ ë””ìì¸ ë˜ê³  MSE ê±°ë¦¬ ê³„ì‚°ì„ í†µí•´ (a)ì™€ ë” ê°€ê¹Œìš´ ë°ì´í„° ìƒ˜í”Œë¡œ Normal distributionì˜ í‰ê· ì„ ì˜®ê²¨ê°„ë‹¤ê³  í–ˆì„ ë•Œ, (c)ë³´ë‹¤ (b)ê°€ (a)ì™€ì˜ MSEê°€ ì ê¸° ë•Œë¬¸ì— (b)ì™€ ë¹„ìŠ·í•œ ê°’ì´ ì •ê·œë¶„í¬ì˜ í‰ê· ì´ ë˜ê³  (b)ì™€ ë¹„ìŠ·í•œ ìƒ˜í”Œë“¤ì´ ë” ë§ì´ ë‚˜ì˜¤ê²Œ ë©ë‹ˆë‹¤. í•˜ì§€ë§Œ ê²°ê³¼ì ìœ¼ë¡œ (c)ê°€ (a)ì™€ ë¹„ìŠ·í•œ ê²ƒì´ ë” ì¢‹ì€ ìƒ˜í”Œë§ì´ ë˜ëŠ” ê²ƒì´ë¼ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì¢‹ì€ sampling functionì´ë€, ì•ì„  ì˜ˆì‹œì—ì„œ ë³¼ ìˆ˜ ìˆì—ˆë“¯ì´ train DBì— ìˆëŠ” data $x$ì™€ ìœ ì‚¬í•œ ìƒ˜í”Œì´ ë‚˜ì˜¬ ìˆ˜ ìˆëŠ” í™•ë¥ ë¶„í¬ë¼ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ê·¸ëƒ¥ ìƒ˜í”Œë§ í•¨ìˆ˜ë¥¼ ë§Œë“¤ê¸° ë³´ë‹¤ evidenceë¡œ xë¥¼ given(ì¡°ê±´)ìœ¼ë¡œ í•˜ì—¬ zë¥¼ ë½‘ì•„ë‚´ëŠ” í™•ë¥ ë¶„í¬ $p(z\|x)$ë¥¼ ë§Œë“¤ì–´ë‚´ëŠ” ê²ƒì´ ëª©ì ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ ì—¬ê¸°ì„œ ë˜ ë¬¸ì œì¸ ì ì€ true distridutionì¸ í•´ë‹¹ ë¶„í¬ë¥¼ ë§Œë“¤ì–´ë‚´ê¸°ìœ„í•´ Variational Inference ë°©ë²•ì„ ì´ìš©í•©ë‹ˆë‹¤. ë¶„í¬ ì¶”ì •ì„ ìœ„í•œ family, ì˜ˆë¥¼ ë“¤ë©´ guassian ë¶„í¬ë“¤ì„ Approximation Classë¡œ ë‘ê³  true distributionì„ ì¶”ì •í•©ë‹ˆë‹¤. ì´ë•Œ gaussian ë¶„í¬ì˜ íŒŒë¼ë¯¸í„°ì¸ *Ï•*ëŠ” meanê³¼ std ê°’ì´ ë  ê²ƒ ì´ê³  ì´ëŸ° ì—¬ëŸ¬ gaussian ë¶„í¬ë“¤ê³¼ true posterior ê°„ì˜ KL divergenceë¥¼ êµ¬í•˜ì—¬ ì¶”ì •í•´ê°‘ë‹ˆë‹¤. 

![](https://i.imgur.com/ksIIEL5.jpg)

<div style="text-align:center">
  Variational Inference<sup>12</sup>
</div>

<p>

</p>


ë”°ë¼ì„œ ì •ë¦¬í•´ë³´ë©´ ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ ìƒì„±ëª¨ë¸ì¸ Generatorë¥¼ í•™ìŠµí•˜ê¸° ìœ„í•´ Variational Inference ë°©ë²•ì„ ì‚¬ìš©í•˜ê²Œ ë˜ì—ˆê³  ê·¸ëŸ¬ë‹¤ ë³´ë‹ˆ AutoEncoderì™€ ë¹„ìŠ·í•œ ëª¨ë¸ êµ¬ì¡°ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ `ë°ì´í„° ì••ì¶•`ì´ ëª©í‘œì¸ AutoEncoderì™€ `ë°ì´í„° ìƒì„±`ì´ ëª©í‘œì¸ VAEëŠ” ê°ìì˜ ëª©í‘œì— ë§ì¶° í•„ìš”í•œ ë°©ë²•ë¡ ì„ ë”í•˜ê²Œ ë˜ë©´ì„œ ê·¸ ëª¨ë¸ êµ¬ì¡°ê°€ ë¹„ìŠ·í•´ë³´ì´ê²Œ ëœ ê²ƒì´ì§€ ê°™ì§€ì•ŠìŠµë‹ˆë‹¤.

![](https://i.imgur.com/AegDpTA.jpg)


VAEì˜ ì „ì²´ êµ¬ì¡°ëŠ” **[1] Decoder, Generator, Generation Network** ë¼ê³  ë¶€ë¥´ëŠ” ë¶€ë¶„ê³¼ **[2] Encoder, Posterior, Inference Network**ë¼ê³  ë¶€ë¥´ëŠ” ë¶€ë¶„, í¬ê²Œ 2ê°€ì§€ íŒŒíŠ¸ë¡œ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤.

![](https://i.imgur.com/1Jz0Hih.jpg)

<div style="text-align:center">
  VAE Structure<sup>6</sup>
</div>

<p>

</p>


# Variational Bound

ìœ„ì˜ íë¦„ì„ ì´ì–´ê°€ë³´ë©´, ì²˜ìŒì— ì•Œê³  ì‹¶ì—ˆë˜ ê²ƒì€ **(1) $p(x)$**ì˜€ìœ¼ë‚˜ ìš°ë¦¬ê°€ ì›í•˜ëŠ” ë°ì´í„°ë“¤ë¡œ ìƒ˜í”Œë§(ì»¨íŠ¸ë¡¤)í•˜ê¸° ìœ„í•´ **(2) $p(z\|x)$** (true posterior)ê°€ í•„ìš”í•´ì¡Œê³ , true posteriorë¥¼ ì•Œ ìˆ˜ ì—†ìœ¼ë‹ˆ ì´ë¥¼ ì¶”ì •(Variational Inference)í•˜ê¸° ìœ„í•´ì„œ **(3) $qÏ•(z\|x)$**ê°€ í•„ìš”í–ˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ìš°ë¦¬ëŠ” ì´ 3ê°œì˜ ë¶„í¬ë“¤ì˜ ê´€ê³„ë¥¼ ì¢€ ë” ì‚´í´ë³´ê³  ì–´ë–»ê²Œ ìƒì„±ëª¨ë¸ì„ í•™ìŠµí•´ë‚˜ê°ˆ ê²ƒì¸ì§€ ê³ ë¯¼í•´ë´ì•¼ í•©ë‹ˆë‹¤.

![](https://i.imgur.com/C8Gwxez.jpg)

ì²˜ìŒì˜ ëª©í‘œì˜€ë˜ $p(x)$ ì— logë¥¼ ì”Œì›Œì„œ ì•„ë˜ì™€ ê°™ì€ ì‹ ë³€í˜•ì„ ì§„í–‰í•˜ë©´ 2ê°œì˜ termìœ¼ë¡œ ë‚˜ëˆ ì§‘ë‹ˆë‹¤. ì²«ë²ˆì§¸ termì€ ì´ë²ˆ ì¥ì˜ ì£¼ì¸ê³µì¸ Evidence LowerBOundë¼ëŠ” ELBOì´ê³  ë‘ë²ˆì§¸ termì€ Variational Inferenceì—ì„œ ë´¤ì—ˆë˜ true posteriorì™€ approximator ì‚¬ì´ì˜ ê±°ë¦¬ë¥¼ ë‚˜íƒ€ë‚´ëŠ” KL ê°’ì…ë‹ˆë‹¤. ì—¬ê¸°ì„œ $log(p(x))$ ê°€ ì¼ì •í•  ë•Œ KL ê°’ì„ ì¤„ì´ëŠ” ê²ƒì´ ëª©í‘œ(=true posteriorë¥¼ ì˜ approximationí•˜ëŠ” ê²ƒ)ì´ê³  KLì€ í•­ìƒ ì–‘ìˆ˜ì´ê¸° ë•Œë¬¸ì—, ì—­ìœ¼ë¡œ ìƒê°í•´ë³´ë©´ ì²«ë²ˆì§¸ termì´ì—ˆë˜ ELBO ê°’ì„ ìµœëŒ€í™”í•˜ëŠ” ê²ƒì´ë¼ê³  ìƒê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ ê°„ë‹¨íˆ ê·¸ë˜í”„ë¡œ ë‚˜íƒ€ë‚´ë³´ë©´ ì˜¤ë¥¸ìª½ ê·¸ë¦¼ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´ ìš°ë¦¬ê°€ ì›í•˜ëŠ” ê²ƒì€ ELBOê°’ì´ ì»¤ì§ˆ ìˆ˜ ìˆëŠ” $Ï•$ë¥¼ ì°¾ì•„ê°€ëŠ” ê³¼ì •ì´ë¼ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

![](https://i.imgur.com/j6YYr7m.jpg)

<div style="text-align:center">
  Decomposition of $log(p(x))$<sup>6</sup>
</div>

<p>

</p>


ë”°ë¼ì„œ ELBOê°’ì´ ì»¤ì§ˆ ìˆ˜ ìˆëŠ” $Ï•$ë¥¼ ì°¾ì•„ê°€ëŠ” ìµœì í™”ë¥¼ ìˆ˜ì‹ì„ ë³€í˜•í•˜ì—¬ ë˜ ë‹¤ì‹œ 2ê°œì˜ term ì¦‰, (1) `Reconstructino Error`ì™€ (2) `Regularization` ìœ¼ë¡œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.


![](https://i.imgur.com/OnM9Zvj.jpg)

<div style="text-align:center">
  The relationship of $log(p(x))$, KL, and ELBO<sup>6</sup>
</div>

<p>

</p>

ë¨¼ì € **Reconstructino Error**ì€ í•™ìŠµ ë°ì´í„°ì¸ $x$ê°€ ë“¤ì–´ê°”ì„ ë•Œ $x$ê°€ ë‚˜ì˜¤ë„ë¡ í•˜ëŠ” `ë³µì›(Reconstruction)`ì„ í•˜ë„ë¡ í•˜ëŠ” termì´ë©°, **Regularization**ì€ prior distributionì¸ $q$ì˜ í˜•íƒœë¥¼ ì œí•œí•´ì£¼ëŠ” ì—­í• ì„ í•˜ëŠ” termì…ë‹ˆë‹¤.

![](https://i.imgur.com/l3GYAED.jpg)


## Regularization term

ELBO termì„ ë‚˜ëˆ„ì—ˆì„ ë•Œ ë‚˜ì™”ë˜ ì²«ë²ˆì§¸ Regularization termì— ëŒ€í•´ ë³´ê² ìŠµë‹ˆë‹¤. True posteriorë¥¼ ì¶”ì •í•˜ê¸° ìœ„í•œ $q_\phi(\mathrm{z} \mid \mathrm{x})$ì€ KL ê°’ì„ ê³„ì‚°í•˜ê¸° ì‰½ë„ë¡ í•˜ê¸° ìœ„í•´ Multivariate gaussian distributionìœ¼ë¡œ ì„¤ê³„í•©ë‹ˆë‹¤. ë˜í•œ ì•ì„œ ì´ì•¼ê¸°í–ˆë˜ ê²ƒ ì²˜ëŸ¼ controller ë¶€ë¶„ì¸ $p(z)$ëŠ” ë‹¤ë£¨ê¸° ì‰¬ìš´ ë¶„í¬ì´ì–´ì•¼ í•˜ê¸° ë•Œë¬¸ì— ì •ê·œë¶„í¬ë¡œ ë§Œë“¤ì–´ ì¤ë‹ˆë‹¤. ê·¸ëŸ¬ë©´ ë…¼ë¬¸ì˜ `Appendix F.1`ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´ ê°€ìš°ì‹œì•ˆ ë¶„í¬ë“¤ ì‚¬ì´ì˜ KL ê°’ì€ meanê³¼ stdë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒê³¼ ê°™ì´ ì‰½ê²Œ ê³„ì‚°ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 

![](https://i.imgur.com/YVzizMJ.jpg)

<div style="text-align:center">
  Regularization term<sup>6</sup>
</div>

<p>

</p>


## Reconstruction error term

ELBOì˜ ë‘ë²ˆì§¸ termì¸ Reconstruction errorì— ëŒ€í•´ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤. Reconstruction errorì˜ expectation í‘œí˜„ì„ integralë¡œ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ê³  ì´ëŠ” ëª¬í…Œì¹´ë¥¼ë¡œ ìƒ˜í”Œë§ì„ í†µí•´ $L$ê°œì˜ $z_{i,â€†l}$ë¥¼ ê°€ì§€ê³  í‰ê· ì„ ë‚´ì„œ êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì—ì„œ index $i$ëŠ” ë°ì´í„° $x$ì˜ ë„˜ë²„ë§ì´ê³  index $l$ì€ generatorì˜ distributionì—ì„œ ìƒ˜í”Œë§í•˜ëŠ” íšŸìˆ˜ì— ëŒ€í•œ ë„˜ë²„ë§ì…ë‹ˆë‹¤. VAEëŠ” í•œì •ëœ ëª¬í…Œì¹´ë¥¼ë¡œ ìƒ˜í”Œë§ì„ í†µí•´ íš¨ê³¼ì ìœ¼ë¡œ optimizationì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

![](https://i.imgur.com/9LLLgAo.jpg)

<div style="text-align:center">
  Recontruction error term<sup>6</sup>
</div>

<p>

</p>


### Reparametrization Trick

ìœ„ì—ì„œ Reconstruction errorë¥¼ êµ¬í•˜ê¸° ìœ„í•´ ìƒ˜í”Œë§í•˜ëŠ” ê³¼ì •ì—ì„œ backpropationì„ í•˜ê¸° ìœ„í•´ **Reparametrization trick**ì„ ì‚¬ìš©í•˜ê²Œ ë©ë‹ˆë‹¤. ë‹¨ìˆœíˆ ì •ê·œë¶„í¬ì—ì„œ ìƒ˜í”Œë§ í•˜ë©´ random nodeì¸ $z$ì— ëŒ€í•´ì„œ gradientë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ê¸° ë•Œë¬¸ì— randomì„±ì„ ì •ê·œë¶„í¬ì—ì„œ ìƒ˜í”Œë§ ë˜ëŠ” $Ïµ$ìœ¼ë¡œ ë§Œë“¤ì–´ì£¼ê³  ì´ë¥¼ reparametrizationì„ í•´ì£¼ì–´ì„œ deterministic nodeê°€ ëœ $z$ë¥¼ backpropagation í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤. 

![](https://i.imgur.com/9189AAx.jpg)

<div style="text-align:center">
  Reparametrization trick<sup>6</sup>
</div>

<p>

</p>

```python
# sampling by re-parameterization technique
z = mu + sigma * tf.random_normal(tf.shape(mu), 0, 1, dtype=tf.float32)
```

$z$ë¥¼ ìƒ˜í”Œë§í•˜ëŠ” generatorì˜ distributionì€ Bernoullië¡œ ë””ìì¸í•  ê²½ìš° NLLì´ Cross Entropyê°€ ë˜ë©° Gaussian ë¶„í¬ë¡œ ë””ìì¸í•  ê²½ìš° MSEê°€ ë˜ì–´ì„œ ë³´í†µ ê³„ì‚°í•˜ê¸° ìš©ì´í•œ 2ê°œì˜ ë¶„í¬ ì¤‘ í•˜ë‚˜ë¥¼ ì‚¬ìš©í•˜ê²Œ ë©ë‹ˆë‹¤. ëª¨ë¸ ë””ìì¸ì˜ ì¡°ê±´ì€ ë°ì´í„°ì˜ ë¶„í¬ì— ë”°ë¼ ê²°ì •ë˜ëŠ”ë° ë°ì´í„°ì˜ ë¶„í¬ê°€ continuous í•˜ë‹¤ë©´ Gaussian ë¶„í¬ì— ê°€ê¹ê¸° ë•Œë¬¸ì— Gaussianìœ¼ë¡œ ë””ìì¸í•˜ê³ , ë°ì´í„°ì˜ ë¶„í¬ê°€ discrete í•˜ë‹¤ë©´ Bernoullië¶„í¬ì— ê°€ê¹ê¸° ë•Œë¬¸ì— Bernoullië¡œ ë””ìì¸í•©ë‹ˆë‹¤.

![](https://i.imgur.com/ZKjm62G.jpg)

<div style="text-align:center">
  Types of generator distributions<sup>6</sup>
</div>

<p>

</p>


# VAE Structure

ì§€ê¸ˆê¹Œì§€ ì‚´í´ë³¸ VAE êµ¬ì¡°ëŠ” Encoderì™€ Decoderë¥¼ ê°ê° ì–´ë–¤ ë¶„í¬ë¡œ ë””ìì¸í•´ì£¼ëŠ” ëƒì— ë”°ë¼ Reconstruction errorì™€ Regularizationì„ ê³„ì‚°í•˜ëŠ” ì‹ë§Œ ì¡°ê¸ˆì”© ë‹¬ë¼ì§€ê²Œ ë©ë‹ˆë‹¤. Encoder ë¶€ë¶„ì€ Reconstruction errorì˜ ê³„ì‚°ì˜ ìš©ì´ì„± ë•Œë¬¸ì— ëª¨ë“  ìœ í˜•ì—ì„œ **ê°€ìš°ì‹œì•ˆ** ë¶„í¬ë¥¼ ì‚¬ìš©í•˜ê²Œ ë˜ê³  Decoder ë¶€ë¶„ë§Œ ë³€í˜•í•˜ì—¬ ì•„ë˜ì˜ ì—¬ëŸ¬ ìœ í˜•ë“¤ì´ ë‚˜íƒ€ë‚˜ê²Œ ë©ë‹ˆë‹¤. ìš°ì„  ëª¨ë“  VAEì—ì„œ ê³µí†µì ìœ¼ë¡œ ì‚¬ìš©í•˜ê³  ìˆëŠ” **Encoder**ë¥¼ ì½”ë“œë¡œ êµ¬í˜„í•˜ë©´ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

```python
# Gateway
def autoencoder(x_hat, x, dim_img, dim_z, n_hidden, keep_prob):

    # encoding
    mu, sigma = gaussian_MLP_encoder(x_hat, n_hidden, dim_z, keep_prob)

    # sampling by re-parameterization technique
    z = mu + sigma * tf.random_normal(tf.shape(mu), 0, 1, dtype=tf.float32)

    # decoding
    y = bernoulli_MLP_decoder(z, n_hidden, dim_img, keep_prob)
    y = tf.clip_by_value(y, 1e-8, 1 - 1e-8)

    # loss
    marginal_likelihood = tf.reduce_sum(x * tf.log(y) + (1 - x) * tf.log(1 - y), 1)
    KL_divergence = 0.5 * tf.reduce_sum(tf.square(mu) + tf.square(sigma) - tf.log(1e-8 + tf.square(sigma)) - 1, 1)

    marginal_likelihood = tf.reduce_mean(marginal_likelihood)
    KL_divergence = tf.reduce_mean(KL_divergence)

    ELBO = marginal_likelihood - KL_divergence

    loss = -ELBO

    return y, z, loss, -marginal_likelihood, KL_divergence

def decoder(z, dim_img, n_hidden):

    y = bernoulli_MLP_decoder(z, n_hidden, dim_img, 1.0, reuse=True)

    return y

```

**(1) Encoder: Gaussian / Decoder: Bernoulli**

![Encoder: Gaussian / Decoder: Bernoulli](https://i.imgur.com/JPOVTlK.jpg)

<div style="text-align:center">
  VAE type 1 <sup>6</sup>
</div>

<p>

</p>

ìœ„ ëª¨ë¸ì„ ì½”ë“œë¡œ êµ¬í˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

```python
# Bernoulli MLP as decoder
def bernoulli_MLP_decoder(z, n_hidden, n_output, keep_prob, reuse=False):

    with tf.variable_scope("bernoulli_MLP_decoder", reuse=reuse):
        # initializers
        w_init = tf.contrib.layers.variance_scaling_initializer()
        b_init = tf.constant_initializer(0.)

        # 1st hidden layer
        w0 = tf.get_variable('w0', [z.get_shape()[1], n_hidden], initializer=w_init)
        b0 = tf.get_variable('b0', [n_hidden], initializer=b_init)
        h0 = tf.matmul(z, w0) + b0
        h0 = tf.nn.tanh(h0)
        h0 = tf.nn.dropout(h0, keep_prob)

        # 2nd hidden layer
        w1 = tf.get_variable('w1', [h0.get_shape()[1], n_hidden], initializer=w_init)
        b1 = tf.get_variable('b1', [n_hidden], initializer=b_init)
        h1 = tf.matmul(h0, w1) + b1
        h1 = tf.nn.elu(h1)
        h1 = tf.nn.dropout(h1, keep_prob)

        # output layer-mean
        wo = tf.get_variable('wo', [h1.get_shape()[1], n_output], initializer=w_init)
        bo = tf.get_variable('bo', [n_output], initializer=b_init)
        y = tf.sigmoid(tf.matmul(h1, wo) + bo)

    return 
```


**(2) Encoder: Gaussian / Decoder: Gaussian**

![Encoder: Gaussian / Decoder: Gaussian](https://i.imgur.com/fNma1fZ.jpg)

<div style="text-align:center">
  VAE type 2 <sup>6</sup>
</div>

<p>

</p>

ìœ„ ëª¨ë¸ì„ ì½”ë“œë¡œ êµ¬í˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

```python 
# Gaussian MLP as encoder
def gaussian_MLP_encoder(x, n_hidden, n_output, keep_prob):
    with tf.variable_scope("gaussian_MLP_encoder"):
        # initializers
        w_init = tf.contrib.layers.variance_scaling_initializer()
        b_init = tf.constant_initializer(0.)

        # 1st hidden layer
        w0 = tf.get_variable('w0', [x.get_shape()[1], n_hidden], initializer=w_init)
        b0 = tf.get_variable('b0', [n_hidden], initializer=b_init)
        h0 = tf.matmul(x, w0) + b0
        h0 = tf.nn.elu(h0)
        h0 = tf.nn.dropout(h0, keep_prob)

        # 2nd hidden layer
        w1 = tf.get_variable('w1', [h0.get_shape()[1], n_hidden], initializer=w_init)
        b1 = tf.get_variable('b1', [n_hidden], initializer=b_init)
        h1 = tf.matmul(h0, w1) + b1
        h1 = tf.nn.tanh(h1)
        h1 = tf.nn.dropout(h1, keep_prob)

        # output layer
        # borrowed from https: // github.com / altosaar / vae / blob / master / vae.py
        wo = tf.get_variable('wo', [h1.get_shape()[1], n_output * 2], initializer=w_init)
        bo = tf.get_variable('bo', [n_output * 2], initializer=b_init)
        gaussian_params = tf.matmul(h1, wo) + bo

        # The mean parameter is unconstrained
        mean = gaussian_params[:, :n_output]
        # The standard deviation must be positive. Parametrize with a softplus and
        # add a small epsilon for numerical stability
        stddev = 1e-6 + tf.nn.softplus(gaussian_params[:, n_output:])

    return mean, stddev

```

**(3) Encoder: Gaussian / Decoder: Gaussian w/ Identity Covariance**

![Encoder: Gaussian / Decoder: Gaussian w/ Identity Covariance](https://i.imgur.com/Si4xjNg.jpg)

<div style="text-align:center">
  VAE type 3 <sup>6</sup>
</div>

<p>

</p>

MNIST dataì„ ì˜ˆì‹œë¡œ ë“¤ì–´ì„œ VAE êµ¬ì¡°ë¥¼ ë‚˜íƒ€ë‚´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

![MNIST VAE](https://i.imgur.com/FqjKwdF.jpg)

<div style="text-align:center">
  VAE with MNIST <sup>6</sup>
</div>

<p>

</p>


# Experiment

í•´ë‹¹ ë…¼ë¬¸ì—ì„œ ì‹¤í—˜ì€ ì´ 2ê°€ì§€ë¥¼ ì§„í–‰í–ˆëŠ”ë° ì•ì—ì„œëŠ” ê³„ì† VAEë¡œ ë‚˜íƒ€ëƒˆì§€ë§Œ ë…¼ë¬¸ì—ì„œëŠ” í•´ë‹¹ ì•Œê³ ë¦¬ì¦˜ì„ `AEVB`ë¡œ ì§€ì¹­í•˜ê¸° ë•Œë¬¸ì— ì´ë¥¼ VAE ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ìƒê°í•˜ê³  ì‹¤í—˜ ê²°ê³¼ë“¤ì„ ë³´ë©´ ë©ë‹ˆë‹¤. ìš°ì„  ì²«ë²ˆì§¸ë¡œ **MNIST ë°ì´í„°ì…‹**ê³¼ **Frey Face ë°ì´í„°ì…‹**ì„ ì‚¬ìš©í•˜ì—¬ ë² ì´ìŠ¤ë¼ì¸ìœ¼ë¡œ wake-sleep ì•Œê³ ë¦¬ì¦˜ê³¼ ì„±ëŠ¥ì„ ë¹„êµí–ˆìŠµë‹ˆë‹¤. 

![](https://i.imgur.com/nc6Sy7a.jpg)

ELBOê°’ì„ ìµœëŒ€í™”í•˜ëŠ” ê²ƒì´ ëª©í‘œì´ë¯€ë¡œ yì¶•ì˜ ê°’ì´ í´ìˆ˜ë¡ ì¢‹ì€ ê²ƒìœ¼ë¡œ í•´ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì˜ ê·¸ë˜í”„ë“¤ì—ì„œ ì‹¤ì„ ê³¼ ì ì„ ì€ ê°ê° trainê³¼ test ë°ì´í„°ì…‹ì— ëŒ€í•´ ELBO ê°’ì„ plottingí•œ ê²ƒìœ¼ë¡œ latent variableì¸ $z$ì˜ ì°¨ì›ì˜ í¬ê¸°ì— ë”°ë¼ ELBO ê°’ì´ ì–´ë–¤ ì–‘ìƒì„ ë‚˜íƒ€ë‚´ëŠ”ì§€ ë³´ì—¬ì¤ë‹ˆë‹¤. Experiment Iì„ ë³´ë©´ íŠ¸ë ˆì´ë‹ í¬ì¸íŠ¸ê°€ ë§ì„ ë•Œ ì¦‰ xì¶• ê°’ì´ í´ë•Œ testì™€ trainingì˜ ELBOê°’ì´ ì ì  ë²Œì–´ì§€ëŠ”ê²ƒì´ ê´€ì°°ì´ ë˜ëŠ”ë° ì´ëŠ” ì˜¤ë²„í”¼íŒ…ìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤. ì €ìëŠ” ì˜¤ë²„í”¼íŒ…ì„ ë°©ì§€í•˜ê¸°ìœ„í•´ ë°ì´í„°ì…‹ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ìˆ˜ì •í•˜ëŠ” ì‘ì—…ì„ í–ˆë‹¤ê³  í•©ë‹ˆë‹¤.

![](https://i.imgur.com/7l0FkhX.jpg)

ë‘ë²ˆì§¸ë¡œëŠ” MNIST ë°ì´í„°ì…‹ì— ëŒ€í•´ì„œ $z$ì˜ ì°¨ì›ì´ 1000, 50000ì¼ë•Œì˜ ê° ì•Œê³ ë¦¬ì¦˜ë“¤ì˜ ì„±ëŠ¥ì„ í•™ìŠµ ìƒ˜í”Œìˆ˜ì— ë”°ë¼ Marginal log-likelihoodë¥¼ plottingí•˜ì—¬ ë‚˜íƒ€ëƒˆìŠµë‹ˆë‹¤. ì´ ì‹¤í—˜ì—ì„œëŠ” ë² ì´ìŠ¤ë¼ì¸ìœ¼ë¡œ Wake-Sleepê³¼ MCEMì„ ì‚¬ìš©í–ˆìœ¼ë©° ì—¬ê¸°ì„œëŠ” AEVB(=VAE)ê°€ convergence speed ì¸¡ë©´ì—ì„œ ë² ì´ìŠ¤ë¼ì¸ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

![](https://i.imgur.com/ORzThE8.jpg)


# Conclusion

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì—°ì†ì ì¸ latent variableì„ íš¨ìœ¨ì ìœ¼ë¡œ inference í•˜ê¸°ìœ„í•´ Stochastic Gradient VBë¡œ variational lower boundì˜ estimation í•˜ëŠ” ë°©ë²•ë¡ ì„ ì œì‹œí•˜ì˜€ìŠµë‹ˆë‹¤. ê°€ìš°ì‹œì•ˆ ë¶„í¬ì—ì„œ ëœë¤í•˜ê²Œ ìƒ˜í”Œë§í•˜ëŠ” ë°©ë²•ì€ back propgataionì´ ë¶ˆê°€ëŠ¥í•˜ê¸° ë•Œë¬¸ì— VAEëŠ” Reparametrization  trickì„ ì´ìš©í•˜ì—¬ ê°€ìš°ì‹œì•ˆ ë¶„í¬ë¡œë¶€í„° ìƒ˜í”Œë§í•œ estimatorëŠ” ë¯¸ë¶„ ê°€ëŠ¥í•˜ê³  SGDë¡œ ìµœì í™” ë©ë‹ˆë‹¤. ë˜í•œ VAEëŠ” i.i.dì˜ ë°ì´í„°ì…‹ê³¼ ê°™ì´ ê° datapointê°€ ì—°ì†ì ì¸ latent variableë¥¼ ê°€ì§€ëŠ” high dimensional ë°ì´í„°ì— ëŒ€ë¹„í•´ Auto-Encoding VB ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ SGVB taskë¥¼ í•´ê²°í•˜ì˜€ìŠµë‹ˆë‹¤.
VAEëŠ” ì´ë¯¸ì§€ë¡œë¶€í„° ì €ì°¨ì›ì˜ ê°€ìš°ì‹œì•ˆ ë¶„í¬ë¥¼ í•™ìŠµí•œ ë‹¤ìŒ ì›ë³¸ ì´ë¯¸ì§€ë¡œ ë³µì›í•˜ëŠ” ìƒì„±ëª¨ë¸ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ ë³µì¡í•œ ë¶„í¬ë¡œ êµ¬ì„±ëœ ì´ë¯¸ì§€ë¥¼ ê°€ìš°ì‹œì•ˆ ë¶„í¬ë¡œ ì°¨ì›ì¶•ì†Œí•œ í•™ìŠµë°©ë²•ì€ ë¬¸ì œì ì´ ìˆìŠµë‹ˆë‹¤. í•µì‹¬ì ì¸ ì •ë³´ë¥¼ ê°€ìš°ì‹œì•ˆ ë¶„í¬ëª¨ì–‘ìœ¼ë¡œ ì••ì¶•í•˜ëŠ” ë°©ë²•ì€ ë§¤ìš° í˜ë“¤ë©° function lossëŠ” ì¡´ì¬í•  ìˆ˜ ë°–ì— ì—†ìŠµë‹ˆë‹¤. ì´ë¥¼ posterior collapseë¼ê³  í•©ë‹ˆë‹¤. VAE ì´í›„ Posterior collapseë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ë‹¤ì–‘í•œ ì—°êµ¬ë“¤ì´ ì§„í–‰ë˜ì—ˆëŠ”ë°ìš”, VQ-VAEì™€ ê°™ì´ VAEë¥¼ ì—…ê·¸ë ˆì´ë“œí•œ ë‹¤ì–‘í•œ latent variable ëª¨ë¸ë“¤ì„ ë§Œë‚˜ë³´ì„¸ìš”!

![](https://i.imgur.com/V65OdKY.jpg)


# Improved Work

VAEì™€ ìƒì„±ì  ì ëŒ€ ë„¤íŠ¸ì›Œí¬(GAN)ë¥¼ ì‚¬ìš©í•˜ì—¬ Auto-Encoderì˜ posteriorë¥¼ ì„ì˜ì˜ prior ë¶„í¬ì™€ matchingí•¨ìœ¼ë¡œì¨ variational inferenceë¥¼ í•˜ëŠ” [Advarsarial AutoEncoder(AAE)](https://arxiv.org/abs/1511.05644)ë¼ëŠ” ì—°êµ¬ê°€ ìˆì—ˆìŠµë‹ˆë‹¤. GANì˜ Discriminatorê°€ ìƒì„±ëœ ì´ë¯¸ì§€ì˜ ë¶„í¬ì™€ ì§„ì§œ ë°ì´í„°ì…‹ì—ì„œ ì˜¨ ì´ë¯¸ì§€ì˜ ë¶„í¬ë¥¼ íŒë³„í•˜ë©´ì„œ prior distributionì„ ë§¤ì¹­í•˜ê²Œ ë©ë‹ˆë‹¤. Posterior distributionì„ prior distributionê³¼ matching í•¨ìœ¼ë¡œì¨ latent space(prior distribution)ì—ì„œ ìƒ˜í”Œë“¤ì´ ì–´ë–¤ ì˜ë¯¸ë¥¼ ê°€ì§€ê³  ë¶„í¬ë˜ì–´ ìˆëŠ”ì§€ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. AAEì˜ encoderëŠ” ì›í•˜ëŠ” prior distributionì— data ë¶„í¬ë¥¼ ë§Œë“¤ê²Œ ë˜ê³  decoderëŠ” í•´ë‹¹ priorì—ì„œ ì˜ë¯¸ë¥¼ ì°¾ì„ ìˆ˜ ìˆëŠ” ìƒ˜í”Œë“¤ì„ ìƒì„±í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤.


![](https://i.imgur.com/2o85wBe.png)


<div style="text-align:center">
  Advarsarial AutoEncoder <sup>13</sup>
</div>

<p>

</p>


---

**Reference**

[1] original paper: [https://arxiv.org/abs/1312.6114](https://arxiv.org/abs/1312.6114)

[2] [https://di-bigdata-study.tistory.com/5](https://di-bigdata-study.tistory.com/5)

[3] [https://di-bigdata-study.tistory.com/4?category=848869](https://di-bigdata-study.tistory.com/4?category=848869)

[4] [https://ratsgo.github.io/generative%20model/2017/12/19/vi/](https://ratsgo.github.io/generative%20model/2017/12/19/vi/)

[5] [https://taeu.github.io/paper/deeplearning-paper-vae/](https://taeu.github.io/paper/deeplearning-paper-vae/)

[6] [https://medium.com/humanscape-tech/paper-review-vae-ac918509a9ba](https://medium.com/humanscape-tech/paper-review-vae-ac918509a9ba)

[7] [https://www.youtube.com/watch?v=o_peo6U7IRM](https://www.youtube.com/watch?v=o_peo6U7IRM)

[8] [https://youtu.be/SAfJz_uzaa8](https://youtu.be/SAfJz_uzaa8)

[9] [https://youtu.be/GbCAwVVKaHY](https://youtu.be/GbCAwVVKaHY)

[10] [https://youtu.be/7t_3dNs4QK4](https://youtu.be/7t_3dNs4QK4)

[11] [https://arxiv.org/abs/1606.05908](https://arxiv.org/abs/1606.05908)

[12] [https://cs.stanford.edu/~sunfanyun/talks/vi_discrete.pdf](https://cs.stanford.edu/~sunfanyun/talks/vi_discrete.pdf)

[13] [https://arxiv.org/abs/1511.05644](https://arxiv.org/abs/1511.05644)